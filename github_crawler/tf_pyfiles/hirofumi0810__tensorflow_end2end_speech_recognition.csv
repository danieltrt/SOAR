file_path,api_count,code
models/__init__.py,0,b''
models/model_base.py,37,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Base class for all models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nOPTIMIZER_CLS_NAMES = {\n    ""adagrad"": tf.train.AdagradOptimizer,\n    ""adadelta"": tf.train.AdadeltaOptimizer,\n    ""adam"": tf.train.AdamOptimizer,\n    ""rmsprop"": tf.train.RMSPropOptimizer,\n    ""sgd"": tf.train.GradientDescentOptimizer,\n    ""momentum"": tf.train.MomentumOptimizer,\n    ""nestrov"": tf.train.MomentumOptimizer\n}\n\n\nclass ModelBase(object):\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def _build(self, *args, **kwargs):\n        """"""Construct model graph.""""""\n        raise NotADirectoryError\n\n    def create_placeholders(self):\n        """"""Create placeholders and append them to list.""""""\n        raise NotImplementedError\n\n    def compute_loss(self, *args, **kwargs):\n        """"""Operation for computing loss.""""""\n        raise NotImplementedError\n\n    def _add_noise_to_inputs(self, inputs, stddev=0.075):\n        """"""Add gaussian noise to the inputs.\n        Args:\n            inputs: the noise free input-features.\n            stddev (float, optional): The standart deviation of the noise.\n                Default is 0.075.\n        Returns:\n            inputs: Input features plus noise.\n        """"""\n        # if stddev != 0:\n        #     with tf.variable_scope(""input_noise""):\n        #         # Add input noise with a standart deviation of stddev.\n        #         inputs = tf.random_normal(\n        #             tf.shape(inputs), 0.0, stddev) + inputs\n        # return inputs\n        raise NotImplementedError\n\n    def _add_noise_to_gradients(grads_and_vars, gradient_noise_scale,\n                                stddev=0.075):\n        """"""Adds scaled noise from a 0-mean normal distribution to gradients.\n        Args:\n            grads_and_vars:\n            gradient_noise_scale:\n            stddev (float):\n        Returns:\n        """"""\n        raise NotImplementedError\n\n    def _set_optimizer(self, optimizer, learning_rate):\n        """"""Set optimizer.\n        Args:\n            optimizer (string): the name of the optimizer in\n                OPTIMIZER_CLS_NAMES\n            learning_rate (float): A learning rate\n        Returns:\n            optimizer:\n        """"""\n        optimizer = optimizer.lower()\n        if optimizer not in OPTIMIZER_CLS_NAMES:\n            raise ValueError(\n                ""Optimizer name should be one of [%s], you provided %s."" %\n                ("", "".join(OPTIMIZER_CLS_NAMES), optimizer))\n\n        # Select optimizer\n        if optimizer == \'momentum\':\n            return OPTIMIZER_CLS_NAMES[optimizer](\n                learning_rate=learning_rate,\n                momentum=0.9)\n        elif optimizer == \'nestrov\':\n            return OPTIMIZER_CLS_NAMES[optimizer](\n                learning_rate=learning_rate,\n                momentum=0.9,\n                use_nesterov=True)\n        else:\n            return OPTIMIZER_CLS_NAMES[optimizer](\n                learning_rate=learning_rate)\n\n    def train(self, loss, optimizer, learning_rate):\n        """"""Operation for training. Only the sigle GPU training is supported.\n        Args:\n            loss: An operation for computing loss\n            optimizer (string): name of the optimizer in OPTIMIZER_CLS_NAMES\n            learning_rate (placeholder): A learning rate\n        Returns:\n            train_op: operation for training\n        """"""\n        # Create a variable to track the global step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        # Set optimizer\n        self.optimizer = self._set_optimizer(optimizer, learning_rate)\n\n        if self.clip_grad_norm is not None:\n            # Compute gradients\n            grads_and_vars = self.optimizer.compute_gradients(loss)\n\n            # Clip gradients\n            clipped_grads_and_vars = self._clip_gradients(grads_and_vars)\n\n            # Create operation for gradient update\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                train_op = self.optimizer.apply_gradients(\n                    clipped_grads_and_vars,\n                    global_step=global_step)\n\n        else:\n            # Use the optimizer to apply the gradients that minimize the loss\n            # and also increment the global step counter as a single training\n            # step\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                train_op = self.optimizer.minimize(\n                    loss, global_step=global_step)\n\n        return train_op\n\n    def _clip_gradients(self, grads_and_vars):\n        """"""Clip gradients.\n        Args:\n            grads_and_vars (list): list of tuples of `(grads, vars)`\n        Returns:\n            clipped_grads_and_vars (list): list of tuple of\n                `(clipped grads, vars)`\n        """"""\n        # TODO: Optionally add gradient noise\n\n        clipped_grads_and_vars = []\n\n        # Clip gradient norm\n        for grad, var in grads_and_vars:\n            if grad is not None:\n                clipped_grads_and_vars.append(\n                    (tf.clip_by_norm(grad, clip_norm=self.clip_grad_norm),\n                     var))\n\n        # Clip gradient\n        # for grad, var in grads_and_vars:\n        #     if grad is not None:\n        #         clipped_grads_and_vars.append(\n        #             (tf.clip_by_value(grad,\n        #                               clip_value_min=-self.clip_grad_norm,\n        #                               clip_value_max=self.clip_grad_norm),\n        #              var))\n\n        # TODO: Add histograms for variables, gradients (norms)\n        # self._tensorboard(trainable_vars)\n\n        return clipped_grads_and_vars\n\n    def _tensorboard(self, trainable_vars):\n        """"""Compute statistics for TensorBoard plot.\n        Args:\n            trainable_vars:\n        """"""\n        ##############################\n        # train\n        ##############################\n        with tf.name_scope(""train""):\n            for var in trainable_vars:\n                self.summaries_train.append(\n                    tf.summary.histogram(var.name, var))\n\n        # Mean\n        with tf.name_scope(""mean_train""):\n            for var in trainable_vars:\n                self.summaries_train.append(\n                    tf.summary.scalar(var.name, tf.reduce_mean(var)))\n\n        # Standard deviation\n        with tf.name_scope(""stddev_train""):\n            for var in trainable_vars:\n                self.summaries_train.append(\n                    tf.summary.scalar(var.name, tf.sqrt(\n                        tf.reduce_mean(tf.square(var - tf.reduce_mean(var))))))\n\n        # Max\n        with tf.name_scope(""max_train""):\n            for var in trainable_vars:\n                self.summaries_train.append(\n                    tf.summary.scalar(var.name, tf.reduce_max(var)))\n\n        # Min\n        with tf.name_scope(""min_train""):\n            for var in trainable_vars:\n                self.summaries_train.append(\n                    tf.summary.scalar(var.name, tf.reduce_min(var)))\n\n        ##############################\n        # dev\n        ##############################\n        with tf.name_scope(""dev""):\n            for var in trainable_vars:\n                self.summaries_dev.append(\n                    tf.summary.histogram(var.name, var))\n\n        # Mean\n        with tf.name_scope(""mean_dev""):\n            for var in trainable_vars:\n                self.summaries_dev.append(\n                    tf.summary.scalar(var.name, tf.reduce_mean(var)))\n\n        # Standard deviation\n        with tf.name_scope(""stddev_dev""):\n            for var in trainable_vars:\n                self.summaries_dev.append(\n                    tf.summary.scalar(var.name, tf.sqrt(\n                        tf.reduce_mean(tf.square(var - tf.reduce_mean(var))))))\n\n        # Max\n        with tf.name_scope(""max_dev""):\n            for var in trainable_vars:\n                self.summaries_dev.append(\n                    tf.summary.scalar(var.name, tf.reduce_max(var)))\n\n        # Min\n        with tf.name_scope(""min_dev""):\n            for var in trainable_vars:\n                self.summaries_dev.append(\n                    tf.summary.scalar(var.name, tf.reduce_min(var)))\n'"
utils/__init__.py,0,b''
utils/directory.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom os.path import join, isdir\n\n\ndef mkdir(path_to_dir):\n    """"""Make a new directory if the directory does not exist.\n    Args:\n        path_to_dir (string): path to a directory\n    Returns:\n        path (string): path to the new directory\n    """"""\n    if path_to_dir is not None and (not isdir(path_to_dir)):\n        os.mkdir(path_to_dir)\n    return path_to_dir\n\n\ndef mkdir_join(path_to_dir, *dir_name):\n    """"""Concatenate root path and 1 or more paths, and make a new direcory if\n    the direcory does not exist.\n    Args:\n        path_to_dir (string): path to a diretcory\n        dir_name (string): a direcory name\n    Returns:\n        path to the new directory\n    """"""\n    if path_to_dir is None:\n        return path_to_dir\n    for i in range(len(dir_name)):\n        if \'.\' not in dir_name[i]:\n            path_to_dir = mkdir(join(path_to_dir, dir_name[i]))\n        else:\n            path_to_dir = join(path_to_dir, dir_name[i])\n    return path_to_dir\n'"
utils/measure_time_func.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport time\n\n\ndef measure_time(func):\n    @functools.wraps(func)\n    def _measure_time(*args, **kwargs):\n        start = time.time()\n        func(*args, **kwargs)\n        elapse = time.time() - start\n        print(""Takes {} seconds."".format(elapse))\n    return _measure_time\n'"
utils/parallel.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Parallel computing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport multiprocessing as mp\n\n\ndef make_parallel(func, args, core=mp.cpu_count() - 1):\n    """"""\n    Args:\n        func (function):\n        args (tuple or dict): arguments for func\n    Returns:\n        result_tuple (tuple): tuple of returns\n    """"""\n    p = mp.Pool(core)\n\n    result_tuple = p.map(func, args)\n    # result_tuple = p.map_async(func, args).get(9999999)\n    # NOTE: for KeyboardInterrupt\n\n    # Clean up\n    p.close()\n    p.terminate()\n    p.join()\n\n    return result_tuple\n'"
utils/parameter.py,1,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef count_total_parameters(variables):\n    """"""\n    Args:\n        variables (list): tf.trainable_variables()\n    Returns:\n        parameters_dict (dict):\n            key => variable name\n            value => the number of parameters\n        total_parameters (float): total parameters of the model\n    """"""\n    total_parameters = 0\n    parameters_dict = {}\n    for variable in variables:\n        shape = variable.get_shape()\n        variable_parameters = 1\n        for dim in shape:\n            variable_parameters *= dim.value\n        total_parameters += variable_parameters\n        parameters_dict[variable.name] = variable_parameters\n    return parameters_dict, total_parameters\n'"
utils/progressbar.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tqdm import tqdm\n\n\ndef wrap_iterator(iterator, progressbar):\n    if progressbar:\n        iterator = tqdm(iterator)\n    return iterator\n\n\ndef wrap_generator(generator, progressbar, total):\n    if progressbar:\n        generator = tqdm(generator, total=total)\n    return generator\n'"
examples/timit/__init__.py,0,b''
models/attention/ListenAttendandSpell.py,0,b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n'
models/attention/__init__.py,0,b''
models/attention/attention_seq2seq.py,82,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Attention-based model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom models.model_base import ModelBase\nfrom models.encoders.load_encoder import load as load_encoder\nfrom models.attention.decoders.attention_layer import AttentionLayer\nfrom models.attention.decoders.attention_decoder import AttentionDecoder, AttentionDecoderOutput\nfrom models.attention.decoders.dynamic_decoder import _transpose_batch_time as time2batch\nfrom models.attention.bridge import InitialStateBridge\n\n# from models.attention.decoders.beam_search.util import choose_top_k\n# from models.attention.decoders.beam_search.beam_search_decoder import BeamSearchDecoder\n\nfrom collections import namedtuple\n\n\nHELPERS = {\n    ""training"": tf.contrib.seq2seq.TrainingHelper,\n    ""greedyembedding"": tf.contrib.seq2seq.GreedyEmbeddingHelper\n}\n\n\nclass EncoderOutput(\n    namedtuple(""EncoderOutput"",\n               [""outputs"", ""final_state"", ""seq_len""])):\n    pass\n\n\nclass AttentionSeq2Seq(ModelBase):\n    """"""Attention-based model.\n    Args:\n        input_size (int): the dimension of input vectors\n        encoder_type (string): blstm or lstm\n        encoder_num_units (int): the number of units in each layer of the\n            encoder\n        encoder_num_layers (int): the number of layers of the encoder\n        encoder_num_proj (int): the number of nodes in the projection layer of\n            the encoder. This is not used for GRU encoders.\n        attention_type (string): the type of attention\n        attention_dim: (int) the dimension of the attention layer\n        decoder_type (string): lstm or gru\n        decoder_num_units (int): the number of units in each layer of the\n            decoder\n        # decoder_num_proj (int): the number of nodes in the projection layer of\n            the decoder. This is not used for GRU decoders.\n        decoder_num_layers (int): the number of layers of the decoder\n        embedding_dim (int): the dimension of the embedding in target spaces\n        num_classes (int): the number of nodes in softmax layer\n        sos_index (int): index of the start of sentence tag (<SOS>)\n        eos_index (int): index of the end of sentence tag (<EOS>)\n        max_decode_length (int): the length of output sequences to stop\n            prediction when EOS token have not been emitted\n        lstm_impl (string): a base implementation of LSTM. This is\n            not used for GRU models.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n            Default is LSTMBlockCell.\n        use_peephole (bool, optional): if True, use peephole connection. This\n            is not used for GRU models.\n        splice (int, optional): the number of frames to splice. This is used\n            when using CNN-like encoder. Default is 1 frame.\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_grad_norm (float, optional): the range of clipping of gradient\n            norm (> 0)\n        clip_activation_encoder (float, optional): the range of clipping of\n            cell activation of the encoder (> 0). This is not used for GRU\n            encoders.\n        clip_activation_decoder (float, optional): the range of clipping of\n            cell activation of the decoder (> 0). This is not used for GRU\n            decoders.\n        weight_decay (float, optional): a parameter for weight decay\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        sharpening_factor (float, optional): a sharpening factor in the\n            softmax layer for computing attention weights\n        logits_temperature (float, optional): a parameter for smoothing the\n            softmax layer in outputing probabilities\n        sigmoid_smoothing (bool, optional): if True, replace softmax function\n            in computing attention weights with sigmoid function for smoothing\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 encoder_type,\n                 encoder_num_units,\n                 encoder_num_layers,\n                 encoder_num_proj,\n                 attention_type,\n                 attention_dim,\n                 decoder_type,\n                 decoder_num_units,\n                 #  decoder_num_proj,\n                 decoder_num_layers,\n                 embedding_dim,\n                 num_classes,\n                 sos_index,\n                 eos_index,\n                 max_decode_length,\n                 lstm_impl=\'LSTMBlockCell\',\n                 use_peephole=True,\n                 splice=1,\n                 parameter_init=0.1,\n                 clip_grad_norm=5.0,\n                 clip_activation_encoder=50,\n                 clip_activation_decoder=50,\n                 weight_decay=0.0,\n                 time_major=True,\n                 sharpening_factor=1.0,\n                 logits_temperature=1.0,\n                 sigmoid_smoothing=False,\n                 name=\'attention\'):\n\n        super(AttentionSeq2Seq, self).__init__()\n\n        assert input_size % 3 == 0, \'input_size must be divisible by 3 (+ delta, double delta features).\'\n        # NOTE: input features are expected to including \xce\x94 and \xce\x94\xce\x94 features\n        assert splice % 2 == 1, \'splice must be the odd number\'\n        assert clip_grad_norm > 0, \'clip_grad_norm must be larger than 0.\'\n        assert weight_decay >= 0, \'weight_decay must not be a negative value.\'\n\n        # Setting for the encoder\n        self.input_size = input_size\n        self.splice = splice\n        self.encoder_type = encoder_type\n        self.encoder_num_units = encoder_num_units\n        self.encoder_num_proj = encoder_num_proj\n        self.encoder_num_layers = encoder_num_layers\n        # self.downsample_list = downsample_list\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n\n        # Setting for the attention layer\n        self.attention_type = attention_type\n        self.attention_dim = attention_dim\n        self.sharpening_factor = sharpening_factor\n        # NOTE: sharpening_factor is good for narrow focus.\n        # 2 is recommended.\n        self.sigmoid_smoothing = sigmoid_smoothing\n        # NOTE: this alleviate sharp distribution\n\n        # Setting for the decoder\n        self.decoder_type = decoder_type\n        self.decoder_num_units = decoder_num_units\n        # self.decoder_num_proj = decoder_num_proj\n        self.decdoder_num_layers = decoder_num_layers\n        self.embedding_dim = embedding_dim\n        self.num_classes = num_classes + 2\n        # NOTE: add <SOS> and <EOS>\n        self.sos_index = sos_index\n        self.eos_index = eos_index\n        self.max_decode_length = max_decode_length\n        self.logits_temperature = logits_temperature\n        # self.beam_width = beam_width\n        self.use_beam_search = False\n\n        # Common setting\n        self.parameter_init = parameter_init\n        self.clip_grad_norm = clip_grad_norm\n        self.clip_activation_encoder = clip_activation_encoder\n        self.clip_activation_decoder = clip_activation_decoder\n        self.weight_decay = weight_decay\n        self.time_major = time_major\n        self.name = name\n\n        # Summaries for TensorBoard\n        self.summaries_train = []\n        self.summaries_dev = []\n\n        # Placeholders\n        self.inputs_pl_list = []\n        self.labels_pl_list = []\n        self.inputs_seq_len_pl_list = []\n        self.labels_seq_len_pl_list = []\n        self.keep_prob_encoder_pl_list = []\n        self.keep_prob_decoder_pl_list = []\n        self.keep_prob_embedding_pl_list = []\n        self.labels_st_true_pl_list = []\n        self.labels_st_pred_pl_list = []\n\n    def _build(self, inputs, labels, inputs_seq_len, labels_seq_len,\n               keep_prob_encoder, keep_prob_decoder, keep_prob_embedding):\n        """"""Define model graph.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            labels (placeholder): A tensor of size `[B, T]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            labels_seq_len (placeholder): A tensor of size `[B]`\n            keep_prob_encoder (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection of the encoder\n            keep_prob_decoder (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection of the decoder\n            keep_prob_embedding (placeholder, float): A probability to keep\n                nodes in the embedding layer\n        Returns:\n            logits: A tensor of size `[B, T_out, num_classes]`\n            decoder_outputs_train (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n            decoder_outputs_infer (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n            encoder_outputs.outputs: A tensor of size `[B, T_in, encoder_num_units]`\n        """"""\n        with tf.variable_scope(\'encoder\'):\n            # Encode input features\n            encoder_outputs = self._encode(\n                inputs, inputs_seq_len, keep_prob_encoder)\n\n        # Define decoder\n        decoder_train = self._create_decoder(\n            encoder_outputs=encoder_outputs,\n            labels=labels,\n            keep_prob_decoder=keep_prob_decoder,\n            mode=tf.contrib.learn.ModeKeys.TRAIN)\n        decoder_infer = self._create_decoder(\n            encoder_outputs=encoder_outputs,\n            labels=labels,\n            keep_prob_decoder=keep_prob_decoder,\n            mode=tf.contrib.learn.ModeKeys.INFER)\n\n        # Wrap decoder only when inference\n        # decoder_infer = self._beam_search_decoder_wrapper(\n        #     decoder_infer, beam_width=self.beam_width)\n\n        # Connect between encoder and decoder\n        bridge = InitialStateBridge(\n            encoder_outputs=encoder_outputs,\n            decoder_state_size=decoder_train.rnn_cell.state_size,\n            parameter_init=self.parameter_init)\n\n        # Call decoder (sharing parameters)\n        # Training stage\n        decoder_outputs_train, _ = self._decode_train(\n            decoder=decoder_train,\n            bridge=bridge,\n            encoder_outputs=encoder_outputs,\n            labels=labels,\n            labels_seq_len=labels_seq_len,\n            keep_prob_embedding=keep_prob_embedding)\n\n        # Inference stage\n        decoder_outputs_infer, _ = self._decode_infer(\n            decoder=decoder_infer,\n            bridge=bridge,\n            encoder_outputs=encoder_outputs)\n        # NOTE: decoder_outputs are time-major by default\n\n        # Convert from time-major to batch-major\n        if self.time_major:\n            decoder_outputs_train = self._convert_to_batch_major(\n                decoder_outputs_train)\n            decoder_outputs_infer = self._convert_to_batch_major(\n                decoder_outputs_infer)\n\n        # Calculate loss per example\n        logits = decoder_outputs_train.logits / self.logits_temperature\n        # NOTE: This is for better decoding. (T = 2 is recommended)\n        # See details in\n        #   https://arxiv.org/abs/1612.02695.\n        #   Chorowski, Jan, and Navdeep Jaitly.\n        #   ""Towards better decoding and language model integration in sequence\n        #    to sequence models."" arXiv preprint arXiv:1612.02695 (2016).\n\n        return logits, decoder_outputs_train, decoder_outputs_infer, encoder_outputs.outputs\n\n    def _encode(self, inputs, inputs_seq_len, keep_prob_encoder):\n        """"""Encode input features.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob_encoder (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n        Returns:\n            encoder_outputs (namedtuple): A namedtuple of\n                `(outputs, final_state, seq_len)`\n                outputs: Encoder states, a tensor of size\n                    `[B, T, num_units (num_proj)]` (always batch-major)\n                final_state: A final hidden state of the encoder\n                seq_len: equivalent to inputs_seq_len\n        """"""\n        # Define encoder\n        if self.encoder_type in [\'blstm\', \'lstm\']:\n            self.encoder = load_encoder(self.encoder_type)(\n                num_units=self.encoder_num_units,\n                num_proj=None,  # TODO: add the projection layer\n                num_layers=self.encoder_num_layers,\n                lstm_impl=self.lstm_impl,\n                use_peephole=self.use_peephole,\n                parameter_init=self.parameter_init,\n                clip_activation=self.clip_activation_encoder,\n                time_major=self.time_major)\n        else:\n            # TODO: add other encoders\n            raise NotImplementedError\n\n        outputs, final_state = self.encoder(\n            inputs=inputs,\n            inputs_seq_len=inputs_seq_len,\n            keep_prob=keep_prob_encoder,\n            is_training=True)\n        # TODO: fix this\n\n        if self.time_major:\n            # Convert from time-major to batch-major\n            outputs = tf.transpose(outputs, [1, 0, 2])\n\n        return EncoderOutput(outputs=outputs,\n                             final_state=final_state,\n                             seq_len=inputs_seq_len)\n\n    def _create_decoder(self, encoder_outputs, labels, keep_prob_decoder,\n                        mode):\n        """"""Create attention decoder.\n        Args:\n            encoder_outputs (namedtuple): A namedtuple of\n                `(outputs, final_state, seq_len)`\n            labels (placeholder): Target labels of size `[B, T_out]`\n            keep_prob_decoder (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection of the decoder\n            mode: tf.contrib.learn.ModeKeys\n        Returns:\n            decoder (callable): A callable function of `AttentionDecoder` class\n        """"""\n        # Define the attention layer (compute attention weights)\n        self.attention_layer = AttentionLayer(\n            attention_type=self.attention_type,\n            num_units=self.attention_dim,\n            parameter_init=self.parameter_init,\n            sharpening_factor=self.sharpening_factor,\n            sigmoid_smoothing=self.sigmoid_smoothing,\n            mode=mode)\n\n        # Define RNN decoder\n        cell_initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n        with tf.variable_scope(\'decoder_rnn_cell\',\n                               initializer=cell_initializer,\n                               reuse=True if mode == tf.contrib.learn.ModeKeys.TRAIN else False):\n            if self.decoder_type == \'lstm\':\n                if tf.__version__ == \'1.3.0\':\n                    rnn_cell = tf.contrib.rnn.LSTMBlockCell(\n                        self.decoder_num_units,\n                        forget_bias=1.0,\n                        clip_cell=self.clip_activation_decoder,\n                        use_peephole=self.use_peephole)\n                else:\n                    rnn_cell = tf.contrib.rnn.LSTMBlockCell(\n                        self.decoder_num_units,\n                        forget_bias=1.0,\n                        use_peephole=self.use_peephole)\n            elif self.decoder_type == \'gru\':\n                rnn_cell = tf.contrib.rnn.GRUCell(self.num_units)\n            else:\n                raise TypeError(\'decoder_type is ""lstm"" or ""gru"".\')\n\n            # Dropout for the hidden-hidden connections\n            rnn_cell = tf.contrib.rnn.DropoutWrapper(\n                rnn_cell, output_keep_prob=keep_prob_decoder)\n\n        # Define attention decoder\n        self.decoder = AttentionDecoder(\n            rnn_cell=rnn_cell,\n            parameter_init=self.parameter_init,\n            max_decode_length=self.max_decode_length,\n            num_classes=self.num_classes,\n            encoder_outputs=encoder_outputs.outputs,\n            encoder_outputs_seq_len=encoder_outputs.seq_len,\n            attention_layer=self.attention_layer,\n            time_major=self.time_major,\n            mode=mode)\n\n        return self.decoder\n\n    def _convert_to_batch_major(self, decoder_outputs):\n        """"""\n        Args:\n            decoder_outputs (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n        Returns:\n            decoder_outputs (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n        """"""\n        logits = time2batch(decoder_outputs.logits)\n        predicted_ids = time2batch(decoder_outputs.predicted_ids)\n        decoder_output = time2batch(decoder_outputs.decoder_output)\n        attention_weights = time2batch(\n            decoder_outputs.attention_weights)\n        context_vector = time2batch(\n            decoder_outputs.context_vector)\n        decoder_outputs = AttentionDecoderOutput(\n            logits=logits,\n            predicted_ids=predicted_ids,\n            decoder_output=decoder_output,\n            attention_weights=attention_weights,\n            context_vector=context_vector)\n        return decoder_outputs\n\n    def _decode_train(self, decoder, bridge, encoder_outputs, labels,\n                      labels_seq_len, keep_prob_embedding):\n        """"""Runs decoding in training mode.\n        Args:\n            decoder (callable): A callable function of `AttentionDecoder` class\n            bridge (callable): A callable function to connect between the\n                encoder and decoder\n            encoder_outputs (namedtuple): A namedtuple of\n                `(outputs, final_state, seq_len)`\n            labels: A tensor of size `[B, T_out]`\n            labels_seq_len: A tensor of size `[B]`\n            keep_prob_embedding (placeholder, float): A probability to keep\n                nodes in the embedding layer\n        Returns:\n            decoder_outputs (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n            decoder_final_state:\n        """"""\n        # Generate embedding of target labels\n        with tf.variable_scope(""output_embedding""):\n            output_embedding = tf.get_variable(\n                name=""W_embedding"",\n                shape=[self.num_classes, self.embedding_dim],\n                initializer=tf.random_uniform_initializer(\n                    -self.parameter_init, self.parameter_init))\n            labels_embedded = tf.nn.embedding_lookup(output_embedding,\n                                                     labels)\n            labels_embedded = tf.nn.dropout(labels_embedded,\n                                            keep_prob=keep_prob_embedding)\n\n        helper_train = tf.contrib.seq2seq.TrainingHelper(\n            inputs=labels_embedded[:, :-1, :],  # exclude <EOS>\n            sequence_length=labels_seq_len - 1,  # exclude <EOS>\n            time_major=False)  # TODO: self.time_major??\n        # labels_embedded: `[B, T_out, embedding_dim]`\n\n        # The initial decoder state is the final encoder state\n        with tf.variable_scope(""bridge""):\n            decoder_initial_state = bridge()\n\n        # Call decoder class\n        decoder_outputs, decoder_final_state = decoder(\n            initial_state=decoder_initial_state,\n            helper=helper_train)\n        # NOTE: These are time-major if self.time_major is True\n\n        return decoder_outputs, decoder_final_state\n\n    def _decode_infer(self, decoder, bridge, encoder_outputs):\n        """"""Runs decoding in inference mode.\n        Args:\n            decoder (callable): A callable function of `AttentionDecoder` class\n            bridge (callable): A callable function to connect between the\n                encoder and decoder\n            encoder_outputs (namedtuple): A namedtuple of\n                `(outputs, final_state, seq_len)`\n        Returns:\n            decoder_outputs (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n            decoder_final_state:\n        """"""\n        batch_size = tf.shape(encoder_outputs.outputs)[0]\n\n        # if self.use_beam_search:\n        #     batch_size = self.beam_width\n        # TODO: make this batch version\n\n        with tf.variable_scope(""output_embedding"", reuse=True):\n            output_embedding = tf.get_variable(\n                name=""W_embedding"",\n                shape=[self.num_classes, self.embedding_dim],\n                initializer=tf.random_uniform_initializer(\n                    -self.parameter_init, self.parameter_init))\n            # NOTE: dropout will not be performed when inference\n\n        helper_infer = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n            embedding=output_embedding,\n            start_tokens=tf.fill([batch_size], self.sos_index),\n            # start_tokens=tf.tile([self.sos_index], [batch_size]),\n            end_token=self.eos_index)\n        # ex.) Output tensor has shape [2, 3].\n        # tf.fill([2, 3], 9) ==> [[9, 9, 9]\n        #                         [9, 9, 9]]\n\n        # The initial decoder state is the final encoder state\n        with tf.variable_scope(""bridge"", reuse=True):\n            decoder_initial_state = bridge()\n\n        # Call decoder class\n        decoder_outputs, decoder_final_state = decoder(\n            initial_state=decoder_initial_state,\n            helper=helper_infer)\n        # NOTE: These are time-major if self.time_major is True\n\n        return decoder_outputs, decoder_final_state\n\n    def create_placeholders(self):\n        """"""Create placeholders and append them to list.""""""\n        self.inputs_pl_list.append(\n            tf.placeholder(tf.float32, shape=[None, None, self.input_size],\n                           name=\'input\'))\n        self.labels_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None, None], name=\'labels\'))\n        self.inputs_seq_len_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None], name=\'inputs_seq_len\'))\n        self.labels_seq_len_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None], name=\'labels_seq_len\'))\n\n        self.keep_prob_encoder_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob_encoder\'))\n        self.keep_prob_decoder_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob_decoder\'))\n        self.keep_prob_embedding_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob_embedding\'))\n\n        # These are prepared for computing LER\n        self.labels_st_true_pl = tf.SparseTensor(\n            tf.placeholder(tf.int64, name=\'indices_true\'),\n            tf.placeholder(tf.int32, name=\'values_true\'),\n            tf.placeholder(tf.int64, name=\'shape_true\'))\n        self.labels_st_pred_pl = tf.SparseTensor(\n            tf.placeholder(tf.int64, name=\'indices_pred\'),\n            tf.placeholder(tf.int32, name=\'values_pred\'),\n            tf.placeholder(tf.int64, name=\'shape_pred\'))\n        # TODO: remove this\n\n        self.labels_st_true_pl_list.append(tf.SparseTensor(\n            tf.placeholder(tf.int64, name=\'indices_true\'),\n            tf.placeholder(tf.int32, name=\'values_true\'),\n            tf.placeholder(tf.int64, name=\'shape_true\')))\n        self.labels_st_pred_pl_list.append(tf.SparseTensor(\n            tf.placeholder(tf.int64, name=\'indices_pred\'),\n            tf.placeholder(tf.int32, name=\'values_pred\'),\n            tf.placeholder(tf.int64, name=\'shape_pred\')))\n\n    def _beam_search_decoder_wrapper(self, decoder, beam_width=1,\n                                     length_penalty_weight=0.6):\n        """"""Wraps a decoder into a Beam Search decoder.\n        Args:\n            decoder: An instance of `RNNDecoder` class\n            beam_width (int): the number of beams to use\n            length_penalty_weight (float): weight for the length penalty factor.\n                0 disables the penalty.\n        Returns:\n            A callable BeamSearchDecoder with the same interfaces as the\n                attention decoder\n        """"""\n        assert isinstance(beam_width, int)\n        assert beam_width >= 1\n\n        if beam_width == 1:\n            # Greedy decoding\n            self.use_beam_search = False\n            return decoder\n        else:\n            self.use_beam_search = True\n            return BeamSearchDecoder(\n                decoder=decoder,\n                beam_width=beam_width,\n                vocab_size=self.num_classes,\n                eos_index=self.eos_index,\n                length_penalty_weight=length_penalty_weight,\n                choose_successors_fn=choose_top_k)\n\n    def compute_loss(self, inputs, labels, inputs_seq_len, labels_seq_len,\n                     keep_prob_encoder, keep_prob_decoder, keep_prob_embedding,\n                     scope=None):\n        """"""Operation for computing cross entropy sequence loss.\n        Args:\n            inputs: A tensor of `[B, T_in, input_size]`\n            labels: A tensor of `[B, T_out]`\n            inputs_seq_len: A tensor of `[B]`\n            labels_seq_len: A tensor of `[B]`\n            keep_prob_encoder (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection of the encoder\n            keep_prob_decoder (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection of the decoder\n            keep_prob_embedding (placeholder, float): A probability to keep\n                nodes in the embedding layer\n            scope (optional): A scope in the model tower\n        Returns:\n            total_loss: operation for computing total loss (cross entropy\n                sequence loss + L2).\n                This is a single scalar tensor to minimize.\n            logits: A tensor of size `[B, T_out, num_classes]`\n            decoder_outputs_train (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n            decoder_outputs_infer (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n        """"""\n        # Build model graph\n        logits, decoder_outputs_train, decoder_outputs_infer, _ = self._build(\n            inputs, labels, inputs_seq_len, labels_seq_len,\n            keep_prob_encoder, keep_prob_decoder, keep_prob_embedding)\n\n        # For prevent 0 * log(0) in crossentropy loss\n        epsilon = tf.constant(value=1e-10)\n        logits += epsilon\n\n        # Weight decay\n        if self.weight_decay > 0:\n            with tf.name_scope(""weight_decay_loss""):\n                weight_sum = 0\n                for var in tf.trainable_variables():\n                    if \'bias\' not in var.name.lower():\n                        weight_sum += tf.nn.l2_loss(var)\n                tf.add_to_collection(\'losses\', weight_sum * self.weight_decay)\n\n        with tf.name_scope(""sequence_loss""):\n            # batch_size = tf.cast(tf.shape(inputs)[0], tf.float32)\n            labels_max_seq_len = tf.shape(labels[:, 1:])[1]\n            loss_mask = tf.sequence_mask(tf.to_int32(labels_seq_len - 1),\n                                         maxlen=labels_max_seq_len,\n                                         dtype=tf.float32)\n            sequence_loss = tf.contrib.seq2seq.sequence_loss(\n                logits=logits,\n                targets=labels[:, 1:],  # exclude <SOS>\n                weights=loss_mask,\n                average_across_timesteps=True,\n                average_across_batch=True,\n                softmax_loss_function=None)\n            # sequence_loss /= batch_size\n\n            tf.add_to_collection(\'losses\', sequence_loss)\n\n        # Compute total loss\n        total_loss = tf.add_n(tf.get_collection(\'losses\', scope),\n                              name=\'total_loss\')\n\n        # Add a scalar summary for the snapshot of loss\n        if self.weight_decay > 0:\n            self.summaries_train.append(\n                tf.summary.scalar(\'weight_loss_train\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'weight_loss_dev\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_train.append(\n                tf.summary.scalar(\'total_loss_train\', total_loss))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'total_loss_dev\', total_loss))\n\n        self.summaries_train.append(\n            tf.summary.scalar(\'sequence_loss_train\', sequence_loss))\n        self.summaries_dev.append(\n            tf.summary.scalar(\'sequence_loss_dev\', sequence_loss))\n\n        return total_loss, logits, decoder_outputs_train, decoder_outputs_infer\n\n    def decode(self, decoder_outputs_train, decoder_outputs_infer):\n        """"""Operation for decoding.\n        Args:\n            decoder_outputs_train (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n            decoder_outputs_infer (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n        Return:\n            decoded_train: operation for decoding in training.\n                A tensor of size `[B, T_out]`\n            decoded_infer: operation for decoding in inference.\n                A tensor of size `[B, max_decode_length]`\n        """"""\n        decoded_train = decoder_outputs_train.predicted_ids\n\n        if self.use_beam_search:\n            # Beam search decoding\n            decoded_infer = decoder_outputs_infer.predicted_ids[0]\n\n            # predicted_ids = decoder_outputs_infer.beam_search_output.predicted_ids\n            # scores = decoder_outputs_infer.beam_search_output.scores[:, :, -1]\n            # argmax_score = tf.argmax(scores, axis=0)[0]\n            # NOTE: predicted_ids: `[time, 1, beam_width]`\n\n            # Convert to `[beam_width, 1, time]`\n            # predicted_ids = tf.transpose(predicted_ids, (2, 1, 0))\n\n            # decoded_infer = predicted_ids[argmax_score]\n            # decoded_infer = decoder_outputs_infer.predicted_ids[-1]\n        else:\n            # Greedy decoding\n            decoded_infer = decoder_outputs_infer.predicted_ids\n\n        return decoded_train, decoded_infer\n\n    def compute_ler(self, labels_true, labels_pred):\n        """"""Operation for computing LER (Label Error Rate).\n        Args:\n            labels_true: A SparseTensor of target labels\n            labels_pred: A SparseTensor of predicted labels\n        Returns:\n            ler_op: operation for computing LER\n        """"""\n        # Compute LER (normalize by label length)\n        ler_op = tf.reduce_mean(tf.edit_distance(\n            labels_pred, labels_true, normalize=True))\n        # TODO: consider <EOS>\n\n        # Add a scalar summary for the snapshot of LER\n        # with tf.name_scope(""ler""):\n        #     self.summaries_train.append(tf.summary.scalar(\n        #         \'ler_train\', ler_op))\n        #     self.summaries_dev.append(tf.summary.scalar(\n        #         \'ler_dev\', ler_op))\n        # TODO: feed_dict\xe3\x81\xae\xe3\x82\xbf\xe3\x82\xa4\xe3\x83\x9f\xe3\x83\xb3\xe3\x82\xb0\xe9\x81\x95\xe3\x81\x86\xe3\x81\x8b\xe3\x82\x89\xe3\x82\xa8\xe3\x83\xa9\xe3\x83\xbc\xe3\x81\xab\xe3\x81\xaa\xe3\x82\x8b\n        # global_step\xe3\x82\x92update\xe3\x81\x99\xe3\x82\x8b\xe5\x89\x8d\xe3\x81\xab\xe3\x81\x99\xe3\x82\x8b\xef\xbc\x9f\n\n        return ler_op\n'"
models/attention/bridge.py,8,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""A collection of bridges between encoder and decoder. A bridge defines\nhow encoder information are passed to the decoder.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom pydoc import locate\n\nimport six\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\n\ndef _total_tensor_depth(tensor):\n    """"""Returns the size of a tensor without the first (batch) dimension""""""\n    return np.prod(tensor.get_shape().as_list()[1:])\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Bridge(object):\n    """"""An abstract bridge class. A bridge defines how state is passed\n    between encoder and decoder.\n    All logic is contained in the `_create` method, which returns an\n    initial state for the decoder.\n    Args:\n        encoder_outputs (namedtuple): A namedtuple that corresponds to the the\n            encoder outputs.\n            `(outputs, final_state, seq_len)`\n        decoder_state_size: An integer or tuple of integers defining the\n            state size of the decoder.\n    """"""\n\n    def __init__(self, encoder_outputs, decoder_state_size):\n        self.encoder_outputs = encoder_outputs\n        self.decoder_state_size = decoder_state_size\n        self.batch_size = tf.shape(\n            nest.flatten(self.encoder_outputs.final_state)[0])[0]\n\n    def __call__(self):\n        """"""Runs the bridge function.\n        Returns:\n            An initial decoder_state tensor or tuple of tensors.\n        """"""\n        return self._create()\n\n    @abc.abstractmethod\n    def _create(self):\n        """""" Implements the logic for this bridge.\n        This function should be implemented by child classes.\n        Returns:\n            A tuple initial_decoder_state tensor or tuple of tensors.\n        """"""\n        raise NotImplementedError(""Must be implemented by child class"")\n\n\nclass ZeroBridge(Bridge):\n    """"""A bridge that does not pass any information between encoder and decoder\n    and sets the initial decoder state to 0. The input function is not\n    modified.\n    """"""\n\n    @staticmethod\n    def default_params():\n        return {}\n\n    def _create(self):\n        zero_state = nest.map_structure(\n            lambda x: tf.zeros([self.batch_size, x], dtype=tf.float32),\n            self.decoder_state_size)\n        return zero_state\n\n\nclass PassThroughBridge(Bridge):\n    """"""Passes the encoder state through to the decoder as-is. This bridge\n    can only be used if encoder and decoder have the exact same state size,\n    i.e. use the same RNN cell.\n    """"""\n\n    @staticmethod\n    def default_params():\n        return {}\n\n    def _create(self):\n        nest.assert_same_structure(self.encoder_outputs.final_state,\n                                   self.decoder_state_size)\n        return self.encoder_outputs.final_state\n\n\nclass InitialStateBridge(Bridge):\n    """"""A bridge that creates an initial decoder state based on the output\n    of the encoder. This state is created by passing the encoder outputs\n    through an additional layer to match them to the decoder state size.\n    The input function remains unmodified.\n    Args:\n        encoder_outputs (namedtuple): A namedtuple that corresponds to the\n            encoder outputs.\n            `(outputs, final_state, seq_len)`\n        decoder_state_size: An integer or tuple of integers defining the\n            state size of the decoder.\n    """"""\n\n    def __init__(self, encoder_outputs, decoder_state_size, parameter_init):\n        super(InitialStateBridge, self).__init__(encoder_outputs,\n                                                 decoder_state_size)\n\n        if not hasattr(encoder_outputs, ""final_state""):\n            raise ValueError(""Invalid bridge_input not in encoder outputs."")\n\n        self._bridge_input = getattr(encoder_outputs, ""final_state"")\n        self._activation_fn = locate(""tensorflow.identity"")\n        self.parameter_init = parameter_init\n\n    @staticmethod\n    def default_params():\n        return {\n            ""bridge_input"": ""final_state"",\n            ""activation_fn"": ""tensorflow.identity"",\n        }\n\n    def _create(self):\n        # Concat bridge inputs on the depth dimensions\n        bridge_input = nest.map_structure(\n            lambda x: tf.reshape(x, [self.batch_size, _total_tensor_depth(x)]),\n            self._bridge_input)\n        bridge_input_flat = nest.flatten([bridge_input])\n        bridge_input_concat = tf.concat(bridge_input_flat, axis=1)\n\n        state_size_splits = nest.flatten(self.decoder_state_size)\n        total_decoder_state_size = sum(state_size_splits)\n\n        # Pass bridge inputs through a fully connected layer layer\n        initial_state_flat = tf.contrib.layers.fully_connected(\n            bridge_input_concat,\n            num_outputs=total_decoder_state_size,\n            activation_fn=self._activation_fn,\n            weights_initializer=tf.truncated_normal_initializer(\n                stddev=self.parameter_init),\n            biases_initializer=tf.zeros_initializer(),\n            scope=None)\n\n        # Shape back into required state size\n        initial_state = tf.split(initial_state_flat, state_size_splits, axis=1)\n        return nest.pack_sequence_as(self.decoder_state_size, initial_state)\n'"
models/attention/joint_ctc_attention.py,61,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Attention model class based on BLSTM encoder and LSTM decoder.\n   This implemtentation is based on\n        https://arxiv.org/abs/1609.06773.\n          Kim, Suyoun, Takaaki Hori, and Shinji Watanabe.\n          ""Joint ctc-attention based end-to-end speech recognition using\n          multi-task learning."" arXiv preprint arXiv:1609.06773 (2016).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\n\n\nclass JointCTCAttention(AttentionSeq2Seq):\n    """"""Joint CTC-Attention model. Encoder is BLSTM as in the paper.\n    Args:\n        input_size (int): the dimension of input vectors\n        encoder_type (string): blstm or lstm\n        encoder_num_units (int): the number of units in each layer of the\n            encoder\n        encoder_num_layers (int): the number of layers of the encoder\n        encoder_num_proj (int): the number of nodes in the projection layer of\n            the encoder. This is not used for GRU encoders.\n        attention_type (string): the type of attention\n        attention_dim: (int) the dimension of the attention layer\n        decoder_type (string): lstm or gru\n        decoder_num_units (int): the number of units in each layer of the decoder\n        # decoder_num_proj (int): the number of nodes in the projection layer\n        # of\n            the decoder. This is not used for GRU decoders.\n        decoder_num_layers (int): the number of layers of the decoder\n        embedding_dim (int): the dimension of the embedding in target spaces\n        lambda_weight (float): weight parameter for multi-task training.\n            loss = lambda_weight * ctc_loss + \\\n                (1 - lambda_weight) * attention_loss\n        num_classes (int): the number of nodes in softmax layer\n        sos_index (int): index of the start of sentence tag (<SOS>)\n        eos_index (int): index of the end of sentence tag (<EOS>)\n        max_decode_length (int): the length of output sequences to stop\n            prediction when EOS token have not been emitted\n        lstm_impl (string): a base implementation of LSTM. This is\n            not used for GRU models.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n            Default is LSTMBlockCell.\n        use_peephole (bool, optional): if True, use peephole connection. This\n            is not used for GRU models.\n        splice (int, optional): the number of frames to splice. This is used\n            when using CNN-like encoder. Default is 1 frame.\n        parameter_init (float, optional): the ange of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_grad_norm (float, optional): the range of clipping of gradient\n            norm (> 0)\n        clip_activation_encoder (float, optional): the range of clipping of\n            cell activation of the encoder (> 0). This is not used for GRU\n            encoders.\n        clip_activation_decoder (float, optional): the range of clipping of\n            cell activation of the decoder (> 0). This is not used for GRU\n            decoders.\n        weight_decay (float, optional): a parameter for weight decay\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        sharpening_factor (float, optional): a sharpening factor in the\n            softmax layer for computing attention weights\n        logits_temperature (float, optional): a parameter for smoothing the\n            softmax layer in outputing probabilities\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 encoder_type,\n                 encoder_num_units,\n                 encoder_num_layers,\n                 encoder_num_proj,\n                 attention_type,\n                 attention_dim,\n                 decoder_type,\n                 decoder_num_units,\n                 #  decoder_num_proj,\n                 decoder_num_layers,\n                 embedding_dim,\n                 lambda_weight,\n                 num_classes,\n                 sos_index,\n                 eos_index,\n                 max_decode_length,\n                 lstm_impl=\'LSTMBlockCell\',\n                 use_peephole=True,\n                 splice=1,\n                 parameter_init=0.1,\n                 clip_grad_norm=5.0,\n                 clip_activation_encoder=50,\n                 clip_activation_decoder=50,\n                 weight_decay=0.0,\n                 time_major=True,\n                 sharpening_factor=1.0,\n                 logits_temperature=1.0,\n                 name=\'joint_ctc_attention\'):\n\n        super(JointCTCAttention, self).__init__(\n            input_size=input_size,\n            encoder_type=encoder_type,\n            encoder_num_units=encoder_num_units,\n            encoder_num_layers=encoder_num_layers,\n            encoder_num_proj=encoder_num_proj,\n            attention_type=attention_type,\n            attention_dim=attention_dim,\n            decoder_type=decoder_type,\n            decoder_num_units=decoder_num_units,\n            #  decoder_num_proj,\n            decoder_num_layers=decoder_num_layers,\n            embedding_dim=embedding_dim,\n            num_classes=num_classes,\n            sos_index=sos_index,\n            eos_index=eos_index,\n            max_decode_length=max_decode_length,\n            lstm_impl=lstm_impl,\n            use_peephole=use_peephole,\n            splice=splice,\n            parameter_init=parameter_init,\n            clip_grad_norm=clip_grad_norm,\n            clip_activation_encoder=clip_activation_encoder,\n            clip_activation_decoder=50,\n            weight_decay=0.0,\n            time_major=True,\n            sharpening_factor=1.0,\n            logits_temperature=1.0,\n            name=name)\n\n        # Setting for multi-task training\n        self.ctc_num_classes = num_classes + 1\n        self.lambda_weight = lambda_weight\n\n        self.ctc_labels_pl_list = []\n\n    def create_placeholders(self):\n        """"""Create placeholders and append them to list.""""""\n        self.inputs_pl_list.append(\n            tf.placeholder(tf.float32, shape=[None, None, self.input_size],\n                           name=\'input\'))\n        self.labels_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None, None], name=\'labels\'))\n        self.inputs_seq_len_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None], name=\'inputs_seq_len\'))\n        self.labels_seq_len_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None], name=\'labels_seq_len\'))\n\n        self.keep_prob_encoder_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob_encoder\'))\n        self.keep_prob_decoder_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob_decoder\'))\n        self.keep_prob_embedding_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob_embedding\'))\n\n        # These are prepared for computing LER\n        self.labels_st_true_pl = tf.SparseTensor(\n            tf.placeholder(tf.int64, name=\'indices_true\'),\n            tf.placeholder(tf.int32, name=\'values_true\'),\n            tf.placeholder(tf.int64, name=\'shape_true\'))\n        self.labels_st_pred_pl = tf.SparseTensor(\n            tf.placeholder(tf.int64, name=\'indices_pred\'),\n            tf.placeholder(tf.int32, name=\'values_pred\'),\n            tf.placeholder(tf.int64, name=\'shape_pred\'))\n\n        # Placeholder for multi-task training\n        self.ctc_labels_pl_list.append(tf.SparseTensor(\n            tf.placeholder(tf.int64, name=\'indices\'),\n            tf.placeholder(tf.int32, name=\'values\'),\n            tf.placeholder(tf.int64, name=\'shape\')))\n\n    def ctc_logits(self, encoder_outputs):\n        """"""\n        Args:\n            encoder_outputs:\n        Returns:\n            logits:\n        """"""\n\n        batch_size = tf.shape(encoder_outputs)[0]\n        max_time = tf.shape(encoder_outputs)[1]\n\n        # Reshape to apply the same weights over the timesteps\n        if \'lstm\' not in self.encoder_type or self.encoder_num_proj is None:\n            if \'b\' in self.encoder_type:\n                # bidirectional\n                outputs_2d = tf.reshape(\n                    encoder_outputs, shape=[-1, self.encoder_num_units * 2])\n            else:\n                # unidirectional\n                outputs_2d = tf.reshape(\n                    encoder_outputs, shape=[-1, self.encoder_num_units])\n        else:\n            if \'b\' in self.encoder_type:\n                # bidirectional\n                outputs_2d = tf.reshape(\n                    encoder_outputs, shape=[-1, self.encoder_num_proj * 2])\n            else:\n                # unidirectional\n                outputs_2d = tf.reshape(\n                    encoder_outputs, shape=[-1, self.encoder_num_proj])\n\n        with tf.variable_scope(\'ctc_output\') as scope:\n            logits_2d = tf.contrib.layers.fully_connected(\n                outputs_2d,\n                num_outputs=self.ctc_num_classes,\n                activation_fn=None,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n\n            if self.time_major:\n                # Reshape back to the original shape\n                logits = tf.reshape(\n                    logits_2d, shape=[max_time, batch_size, self.ctc_num_classes])\n            else:\n                # Reshape back to the original shape\n                logits = tf.reshape(\n                    logits_2d, shape=[batch_size, max_time, self.ctc_num_classes])\n\n                # Convert to time-major: `[T, B, ctc_num_classes]\'\n                logits = tf.transpose(logits, [1, 0, 2])\n\n        return logits\n\n    def compute_loss(self, inputs, labels, ctc_labels, inputs_seq_len,\n                     labels_seq_len,\n                     keep_prob_encoder, keep_prob_decoder, keep_prob_embedding,\n                     scope=None):\n        """"""Operation for computing cross entropy sequence loss.\n        Args:\n            inputs: A tensor of `[B, T_in, input_size]`\n            labels: A tensor of `[B, T_out]`\n            ctc_labels:\n            inputs_seq_len: A tensor of `[B]`\n            labels_seq_len: A tensor of `[B]`\n            keep_prob_encoder (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection of the encoder\n            keep_prob_decoder (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection of the decoder\n            keep_prob_embedding (placeholder, float): A probability to keep\n                nodes in the embedding layer\n            scope (optional): A scope in the model tower\n        Returns:\n            total_loss: operation for computing total loss (cross entropy\n                sequence loss + ctc_loss + L2).\n                This is a single scalar tensor to minimize.\n            logits: A tensor of size `[B, T_in, num_classes + 2 (<SOS> & <EOS>)]`\n            ctc_logits: A tensor of size `[B, T_in, num_classes + 1 (blank)]`\n            decoder_outputs_train (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n            decoder_outputs_infer (namedtuple): A namedtuple of\n                `(logits, predicted_ids, decoder_output, attention_weights,\n                    context_vector)`\n        """"""\n        # Build model graph\n        logits, decoder_outputs_train, decoder_outputs_infer, encoder_outputs = self._build(\n            inputs, labels, inputs_seq_len, labels_seq_len,\n            keep_prob_encoder, keep_prob_decoder, keep_prob_embedding)\n\n        # For prevent 0 * log(0) in crossentropy loss\n        epsilon = tf.constant(value=1e-10)\n        logits = logits + epsilon\n\n        # Weight decay\n        if self.weight_decay > 0:\n            with tf.name_scope(""weight_decay_loss""):\n                weight_sum = 0\n                for var in tf.trainable_variables():\n                    if \'bias\' not in var.name.lower():\n                        weight_sum += tf.nn.l2_loss(var)\n                tf.add_to_collection(\'losses\', weight_sum * self.weight_decay)\n\n        with tf.name_scope(""sequence_loss""):\n            # batch_size = tf.cast(tf.shape(inputs)[0], tf.float32)\n            labels_max_seq_len = tf.shape(labels[:, 1:])[1]\n            loss_mask = tf.sequence_mask(tf.to_int32(labels_seq_len - 1),\n                                         maxlen=labels_max_seq_len,\n                                         dtype=tf.float32)\n            sequence_loss = tf.contrib.seq2seq.sequence_loss(\n                logits=logits,\n                targets=labels[:, 1:],  # exclude <SOS>\n                weights=loss_mask,\n                average_across_timesteps=True,\n                average_across_batch=True,\n                softmax_loss_function=None)\n            # sequence_loss /= batch_size\n\n            tf.add_to_collection(\'losses\', sequence_loss *\n                                 (1 - self.lambda_weight))\n\n        with tf.name_scope(\'ctc_loss\'):\n\n            ctc_logits = self.ctc_logits(encoder_outputs)\n\n            ctc_losses = tf.nn.ctc_loss(\n                ctc_labels,  # NOTE: sparsetensor\n                ctc_logits,\n                # tf.cast(inputs_seq_len, tf.int32),\n                inputs_seq_len,\n                preprocess_collapse_repeated=False,\n                ctc_merge_repeated=True,\n                ignore_longer_outputs_than_inputs=False,\n                time_major=True)\n            ctc_loss = tf.reduce_mean(ctc_losses, name=\'ctc_loss_mean\')\n            tf.add_to_collection(\'losses\', ctc_loss * self.lambda_weight)\n\n        # Compute total loss\n        total_loss = tf.add_n(tf.get_collection(\'losses\', scope),\n                              name=\'total_loss\')\n\n        # Add a scalar summary for the snapshot of loss\n        if self.weight_decay > 0:\n            self.summaries_train.append(\n                tf.summary.scalar(\'weight_loss_train\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'weight_loss_dev\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_train.append(\n                tf.summary.scalar(\'total_loss_train\', total_loss))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'total_loss_dev\', total_loss))\n\n        self.summaries_train.append(\n            tf.summary.scalar(\'sequence_loss_train\', sequence_loss))\n        self.summaries_dev.append(\n            tf.summary.scalar(\'sequence_loss_dev\', sequence_loss))\n        self.summaries_train.append(\n            tf.summary.scalar(\'ctc_loss_train\', ctc_loss))\n        self.summaries_dev.append(\n            tf.summary.scalar(\'ctc_loss_dev\', ctc_loss))\n\n        return total_loss, logits, ctc_logits, decoder_outputs_train, decoder_outputs_infer\n'"
models/ctc/__init__.py,0,b''
models/ctc/bn_blstm_ctc.py,23,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Batch Normalized bidirectional LSTM-CTC model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom models.ctc.base import CTCBase\nfrom recurrent.layers.bn_lstm import BatchNormLSTMCell\nfrom recurrent.initializer import orthogonal_initializer\n\n\nclass BN_BLSTM_CTC(CTCBase):\n    """"""Batch Normalized Bidirectional LSTM-CTC model.\n    Args:\n        batch_size: int, batch size of mini batch\n        input_size: int, the dimensions of input vectors\n        num_units: int, the number of units in each layer\n        num_layers: int, the number of layers\n        num_classes: int, the number of classes of target labels\n            (except for a blank label)\n        parameter_init: A float value. Range of uniform distribution to\n            initialize weight parameters\n        clip_grad: A float value. Range of gradient clipping (> 0)\n        clip_activation: A float value. Range of activation clipping (> 0)\n        num_proj: int, the number of nodes in recurrent projection layer\n        weight_decay: A float value. Regularization parameter for weight decay\n        bottleneck_dim: int, the dimensions of the bottleneck layer\n        is_training: bool, set True when training\n        name: string, the name of the CTC model\n    """"""\n\n    def __init__(self,\n                 batch_size,\n                 input_size,\n                 num_units,\n                 num_layers,\n                 num_classes,\n                 parameter_init=0.1,\n                 clip_grad=None,\n                 clip_activation=None,\n                 dropout_ratio_input=1.0,\n                 dropout_ratio_hidden=1.0,\n                 num_proj=None,\n                 weight_decay=0.0,\n                 bottleneck_dim=None,\n                 is_training=True,\n                 name=\'bn_blstm_ctc\'):\n\n        CTCBase.__init__(self, batch_size, input_size, num_units, num_layers,\n                         num_classes, parameter_init,\n                         clip_grad, clip_activation,\n                         dropout_ratio_input, dropout_ratio_hidden,\n                         weight_decay, name)\n\n        self.bottleneck_dim = bottleneck_dim\n        self.num_proj = None if num_proj == 0 else num_proj\n        self._is_training = is_training\n\n    def _build(self, inputs, inputs_seq_len, keep_prob_input,\n               keep_prob_hidden):\n        """"""Construct model graph.\n        Args:\n            inputs: A tensor of `[batch_size, max_time, input_dim]`\n            inputs_seq_len:  A tensor of `[batch_size]`\n            keep_prob_input:\n            keep_prob_hidden:\n        Returns:\n            logits:\n        """"""\n        # Dropout for inputs\n        outputs = tf.nn.dropout(inputs,\n                                keep_prob_input,\n                                name=\'dropout_input\')\n\n        self.is_training = tf.placeholder(tf.bool)\n\n        # Hidden layers\n        for i_layer in range(self.num_layers):\n            with tf.name_scope(\'blstm_hidden\' + str(i_layer + 1)):\n\n                # initializer = tf.random_uniform_initializer(\n                #     minval=-self.parameter_init,\n                #     maxval=self.parameter_init)\n                initializer = orthogonal_initializer()\n\n                lstm_fw = BatchNormLSTMCell(self.num_units,\n                                            use_peepholes=True,\n                                            cell_clip=self.clip_activation,\n                                            initializer=initializer,\n                                            num_proj=self.num_proj,\n                                            forget_bias=1.0,\n                                            state_is_tuple=True,\n                                            is_training=self.is_training)\n\n                lstm_bw = BatchNormLSTMCell(self.num_units,\n                                            use_peepholes=True,\n                                            cell_clip=self.clip_activation,\n                                            initializer=initializer,\n                                            num_proj=self.num_proj,\n                                            forget_bias=1.0,\n                                            state_is_tuple=True,\n                                            is_training=self.is_training)\n                # num_proj=int(self.num_units / 2),\n\n                # Dropout for outputs of each layer\n                lstm_fw = tf.contrib.rnn.DropoutWrapper(\n                    lstm_fw,\n                    output_keep_prob=keep_prob_hidden)\n                lstm_bw = tf.contrib.rnn.DropoutWrapper(\n                    lstm_bw,\n                    output_keep_prob=keep_prob_hidden)\n\n                # _init_state_fw = lstm_fw.zero_state(self.batch_size,\n                #                                     tf.float32)\n                # _init_state_bw = lstm_bw.zero_state(self.batch_size,\n                #                                     tf.float32)\n                # initial_state_fw=_init_state_fw,\n                # initial_state_bw=_init_state_bw,\n\n                # Ignore 2nd return (the last state)\n                (outputs_fw, outputs_bw), final_state = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=lstm_fw,\n                    cell_bw=lstm_bw,\n                    inputs=outputs,\n                    sequence_length=inputs_seq_len,\n                    dtype=tf.float32,\n                    scope=\'blstm_dynamic\' + str(i_layer + 1))\n\n                outputs = tf.concat(axis=2, values=[outputs_fw, outputs_bw])\n\n        # Reshape to apply the same weights over the timesteps\n        if self.num_proj is None:\n            output_node = self.num_units * 2\n        else:\n            output_node = self.num_proj * 2\n        outputs = tf.reshape(outputs, shape=[-1, output_node])\n\n        # inputs: `[batch_size, max_time, input_size_splice]`\n        batch_size = tf.shape(inputs)[0]\n\n        if self.bottleneck_dim is not None and self.bottleneck_dim != 0:\n            with tf.name_scope(\'bottleneck\'):\n                # Affine\n                W_bottleneck = tf.Variable(tf.truncated_normal(\n                    shape=[output_node, self.bottleneck_dim],\n                    stddev=0.1, name=\'W_bottleneck\'))\n                b_bottleneck = tf.Variable(tf.zeros(\n                    shape=[self.bottleneck_dim], name=\'b_bottleneck\'))\n                outputs = tf.matmul(outputs, W_bottleneck) + b_bottleneck\n                output_node = self.bottleneck_dim\n\n        with tf.name_scope(\'output\'):\n            # Affine\n            W_output = tf.Variable(tf.truncated_normal(\n                shape=[output_node, self.num_classes],\n                stddev=0.1, name=\'W_output\'))\n            b_output = tf.Variable(tf.zeros(\n                shape=[self.num_classes], name=\'b_output\'))\n            logits_2d = tf.matmul(outputs, W_output) + b_output\n\n            # Reshape back to the original shape\n            logits = tf.reshape(\n                logits_2d, shape=[batch_size, -1, self.num_classes])\n\n            # Convert to time-major: `[max_time, batch_size, num_classes]\'\n            logits = tf.transpose(logits, (1, 0, 2))\n\n            return logits\n'"
models/ctc/ctc.py,54,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""CTC model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom models.model_base import ModelBase\nfrom models.encoders.load_encoder import load\n\n\nclass CTC(ModelBase):\n    """"""Connectionist Temporal Classification (CTC) network.\n    Args:\n        encoder_type (string): The type of an encoder\n            blstm: Bidirectional LSTM\n            lstm: Unidirectional LSTM\n            bgru: Bidirectional GRU\n            gru: Unidirectional GRU\n            vgg_blstm: VGG + Bidirectional LSTM\n            vgg_lstm: VGG + Unidirectional LSTM\n        input_size (int): the dimensions of input vectors\n        num_units (int): the number of units in each layer\n        num_layers (int): the number of layers\n        num_classes (int): the number of classes of target labels\n            (except for a blank label)\n        lstm_impl (string, optional): a base implementation of LSTM. This is\n            not used for GRU models.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n            Default is LSTMBlockCell.\n        use_peephole (bool, optional): if True, use peephole connection. This\n            is not used for GRU models.\n        splice (int, optional): the number of frames to splice. This is used\n            when using CNN-like encoder. Default is 1 frame.\n        num_stack (int, optional): the number of frames to stack\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_grad_norm (float, optional): the range of clipping of gradient\n            norm (> 0)\n        clip_activation (float, optional): the range of clipping of cell\n            activation (> 0). This is not used for GRU models.\n        num_proj (int, optional): the number of nodes in the projection layer.\n            This is not used for GRU models.\n        weight_decay (float, optional): a parameter for weight decay\n        bottleneck_dim (int, optional): the dimensions of the bottleneck layer\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n    """"""\n\n    def __init__(self,\n                 encoder_type,\n                 input_size,\n                 num_units,\n                 num_layers,\n                 num_classes,\n                 lstm_impl=\'LSTMBlockCell\',\n                 use_peephole=True,\n                 splice=1,\n                 num_stack=1,\n                 parameter_init=0.1,\n                 clip_grad_norm=None,\n                 clip_activation=None,\n                 num_proj=None,\n                 weight_decay=0.0,\n                 bottleneck_dim=None,\n                 time_major=True):\n\n        super(CTC, self).__init__()\n\n        assert input_size % 3 == 0, \'input_size must be divisible by 3 (+ delta, acceleration coefficients).\'\n        assert splice % 2 == 1, \'splice must be the odd number\'\n        if clip_grad_norm is not None:\n            assert float(\n                clip_grad_norm) > 0, \'clip_grad_norm must be larger than 0.\'\n        assert float(\n            weight_decay) >= 0, \'weight_decay must not be a negative value.\'\n\n        # Encoder setting\n        self.encoder_type = encoder_type\n        self.input_size = input_size\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_units = num_units\n        if int(num_proj) == 0:\n            self.num_proj = None\n        elif num_proj is not None:\n            self.num_proj = int(num_proj)\n        else:\n            self.num_proj = None\n        self.num_layers = num_layers\n        self.bottleneck_dim = bottleneck_dim\n        self.num_classes = num_classes + 1  # + blank\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n\n        # Regularization\n        self.parameter_init = parameter_init\n        self.clip_grad_norm = clip_grad_norm\n        self.clip_activation = clip_activation\n        self.weight_decay = weight_decay\n\n        # Summaries for TensorBoard\n        self.summaries_train = []\n        self.summaries_dev = []\n\n        # Placeholders\n        self.inputs_pl_list = []\n        self.labels_pl_list = []\n        self.inputs_seq_len_pl_list = []\n        self.keep_prob_pl_list = []\n\n        self.time_major = time_major\n        self.name = encoder_type + \'_ctc\'\n\n        if encoder_type in [\'blstm\', \'lstm\']:\n            self.encoder = load(encoder_type)(\n                num_units=num_units,\n                num_proj=self.num_proj,\n                num_layers=num_layers,\n                lstm_impl=lstm_impl,\n                use_peephole=use_peephole,\n                parameter_init=parameter_init,\n                clip_activation=clip_activation,\n                time_major=time_major)\n\n        elif encoder_type in [\'vgg_blstm\', \'vgg_lstm\', \'cldnn_wang\']:\n            self.encoder = load(encoder_type)(\n                input_size=input_size,\n                splice=splice,\n                num_stack=num_stack,\n                num_units=num_units,\n                num_proj=self.num_proj,\n                num_layers=num_layers,\n                lstm_impl=lstm_impl,\n                use_peephole=use_peephole,\n                parameter_init=parameter_init,\n                clip_activation=clip_activation,\n                time_major=time_major)\n\n        elif encoder_type in [\'bgru\', \'gru\']:\n            self.encoder = load(encoder_type)(\n                num_units=num_units,\n                num_layers=num_layers,\n                parameter_init=parameter_init,\n                time_major=time_major)\n\n        elif encoder_type in [\'vgg_wang\', \'resnet_wang\', \'cnn_zhang\']:\n            self.encoder = load(encoder_type)(\n                input_size=input_size,\n                splice=splice,\n                num_stack=num_stack,\n                parameter_init=parameter_init,\n                time_major=time_major)\n\n        elif encoder_type in [\'student_cnn_ctc\', \'student_cnn_compact_ctc\']:\n            self.encoder = load(encoder_type)(\n                input_size=input_size,\n                splice=splice,\n                num_stack=num_stack,\n                parameter_init=parameter_init,\n                time_major=time_major)\n\n        else:\n            raise NotImplementedError\n\n    def _build(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs: A tensor of size `[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            logits: A tensor of size `[T, B, num_classes]`\n        """"""\n        # inputs: `[B, T, input_size]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n\n        encoder_outputs, final_state = self.encoder(\n            inputs, inputs_seq_len, keep_prob, is_training)\n\n        # for debug\n        self.encoder_outputs = encoder_outputs\n\n        # Reshape to apply the same weights over the timesteps\n        output_dim = encoder_outputs.shape.as_list()[-1]\n        outputs_2d = tf.reshape(\n            encoder_outputs, shape=[batch_size * max_time, output_dim])\n\n        if self.bottleneck_dim is not None and self.bottleneck_dim != 0:\n            with tf.variable_scope(\'bottleneck\') as scope:\n                outputs_2d = tf.contrib.layers.fully_connected(\n                    outputs_2d,\n                    num_outputs=self.bottleneck_dim,\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=scope)\n\n            # Dropout for the hidden-output connections\n            outputs_2d = tf.nn.dropout(\n                outputs_2d, keep_prob, name=\'dropout_bottleneck\')\n\n        with tf.variable_scope(\'output\') as scope:\n            logits_2d = tf.contrib.layers.fully_connected(\n                outputs_2d,\n                num_outputs=self.num_classes,\n                activation_fn=None,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n\n            if self.time_major:\n                # Reshape back to the original shape\n                logits = tf.reshape(\n                    logits_2d, shape=[max_time, batch_size, self.num_classes])\n            else:\n                # Reshape back to the original shape\n                logits = tf.reshape(\n                    logits_2d, shape=[batch_size, max_time, self.num_classes])\n\n                # Convert to time-major: `[T, B, num_classes]\'\n                logits = tf.transpose(logits, [1, 0, 2])\n\n        return logits\n\n    def create_placeholders(self):\n        """"""Create placeholders and append them to list.""""""\n        self.inputs_pl_list.append(\n            tf.placeholder(tf.float32,\n                           shape=[None, None, self.input_size *\n                                  self.num_stack * self.splice],\n                           name=\'input\'))\n        self.labels_pl_list.append(\n            tf.SparseTensor(tf.placeholder(tf.int64, name=\'indices\'),\n                            tf.placeholder(tf.int32, name=\'values\'),\n                            tf.placeholder(tf.int64, name=\'shape\')))\n        self.inputs_seq_len_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None], name=\'inputs_seq_len\'))\n        self.keep_prob_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob\'))\n\n    def compute_loss(self, inputs, labels, inputs_seq_len,\n                     keep_prob, scope=None, softmax_temperature=1,\n                     is_training=True):\n        """"""Operation for computing CTC loss.\n        Args:\n            inputs: A tensor of size `[B, T, input_size]`\n            labels: A SparseTensor of target labels\n            inputs_seq_len: A tensor of size `[B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            scope (optional): A scope in the model tower\n            softmax_temperature (int, optional): temperature parameter for\n                ths softmax layer\n            is_training (bool, optional):\n        Returns:\n            total_loss: operation for computing total ctc loss (ctc loss + L2).\n                 This is a single scalar tensor to minimize.\n            logits: A tensor of size `[T, B, num_classes]`\n        """"""\n        # Build model graph\n        logits = self._build(inputs, inputs_seq_len, keep_prob,\n                             is_training=is_training)\n\n        # Weight decay\n        if self.weight_decay > 0:\n            with tf.name_scope(""weight_decay_loss""):\n                weight_sum = 0\n                for var in tf.trainable_variables():\n                    if \'bias\' not in var.name.lower():\n                        weight_sum += tf.nn.l2_loss(var)\n                tf.add_to_collection(\'losses\', weight_sum * self.weight_decay)\n\n        with tf.name_scope(""ctc_loss""):\n            ctc_losses = tf.nn.ctc_loss(\n                labels,\n                logits / softmax_temperature,\n                tf.cast(inputs_seq_len, tf.int32),\n                # inputs_seq_len,\n                preprocess_collapse_repeated=False,\n                ctc_merge_repeated=True,\n                ignore_longer_outputs_than_inputs=True,\n                time_major=True)\n            ctc_loss = tf.reduce_mean(ctc_losses, name=\'ctc_loss_mean\')\n            tf.add_to_collection(\'losses\', ctc_loss)\n\n        # Compute total loss\n        total_loss = tf.add_n(tf.get_collection(\'losses\', scope),\n                              name=\'total_loss\')\n\n        # Add a scalar summary for the snapshot of loss\n        if self.weight_decay > 0:\n            self.summaries_train.append(\n                tf.summary.scalar(\'weight_loss_train\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'weight_loss_dev\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_train.append(\n                tf.summary.scalar(\'total_loss_train\', total_loss))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'total_loss_dev\', total_loss))\n\n        self.summaries_train.append(\n            tf.summary.scalar(\'ctc_loss_train\', ctc_loss))\n        self.summaries_dev.append(\n            tf.summary.scalar(\'ctc_loss_dev\', ctc_loss))\n\n        return total_loss, logits\n\n    def decoder(self, logits, inputs_seq_len, beam_width=1):\n        """"""Operation for decoding.\n        Args:\n            logits: A tensor of size `[T, B, num_classes]`\n            inputs_seq_len: A tensor of size `[B]`\n            beam_width (int, optional): beam width for beam search.\n                1 disables beam search, which mean greedy decoding.\n        Return:\n            decode_op: A SparseTensor\n        """"""\n        assert isinstance(beam_width, int), ""beam_width must be integer.""\n        assert beam_width >= 1, ""beam_width must be >= 1""\n\n        # inputs_seq_len = tf.cast(inputs_seq_len, tf.int32)\n\n        if beam_width == 1:\n            decoded, _ = tf.nn.ctc_greedy_decoder(\n                logits, inputs_seq_len)\n        else:\n            decoded, _ = tf.nn.ctc_beam_search_decoder(\n                logits, inputs_seq_len,\n                beam_width=beam_width)\n\n        decode_op = tf.to_int32(decoded[0])\n\n        # TODO: chnage function name to `decode`\n\n        return decode_op\n\n    def posteriors(self, logits, blank_prior=1):\n        """"""Operation for computing posteriors of each time steps.\n        Args:\n            logits: A tensor of size `[T, B, num_classes]`\n            blank_prior (float): A prior for blank classes. posteriors are\n                divided by this prior.\n        Return:\n            posteriors_op: operation for computing posteriors for each class\n        """"""\n        # Convert to batch-major: `[B, T, num_classes]\'\n        logits = tf.transpose(logits, (1, 0, 2))\n\n        logits_2d = tf.reshape(logits, [-1, self.num_classes])\n\n        # TODO: Divide by blank prior\n        # mask = tf.one_hot(\n        #     indices=tf.shape(logits_2d),\n        #     depth=self.num_classes + 1,\n        #     on_value=1,\n        #     off_value=0,\n        #     axis=-1)\n        # mask /= blank_prior\n        # logits_2d = tf.multiply(logits_2d, mask)\n\n        posteriors_op = tf.nn.softmax(logits_2d)\n\n        return posteriors_op\n\n    def compute_ler(self, decode_op, labels):\n        """"""Operation for computing LER (Label Error Rate).\n        Args:\n            decode_op: operation for decoding\n            labels: A SparseTensor of target labels\n        Return:\n            ler_op: operation for computing LER\n        """"""\n        # Compute LER (normalize by label length)\n        ler_op = tf.reduce_mean(tf.edit_distance(\n            decode_op, labels, normalize=True))\n\n        # Add a scalar summary for the snapshot of LER\n        self.summaries_train.append(tf.summary.scalar(\'ler_train\', ler_op))\n        self.summaries_dev.append(tf.summary.scalar(\'ler_dev\', ler_op))\n\n        return ler_op\n'"
models/ctc/multitask_ctc.py,84,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Base class of the multi-task CTC model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom models.ctc.ctc import CTC\nfrom models.encoders.load_encoder import load\n\n\nclass MultitaskCTC(CTC):\n    """"""Hierarchical Connectionist Temporal Classification (CTC) network.\n    Args:\n        encoder_type (string): The type of an encoder\n            multitask_blstm: multitask bidirectional LSTM\n            multitask_lstm: multitask unidirectional LSTM\n        input_size (int): the dimensions of input vectors\n        num_units (int): the number of units in each layer\n        num_layers_main (int): the number of layers of the main task\n        num_layers_sub (int): the number of layers of the sub task\n        num_classes_main (int): the number of classes of target labels in the\n            main task (except for a blank label)\n        num_classes_second (int): the number of classes of target labels in the\n            second task (except for a blank label)\n        main_task_weight: A float value. The weight of loss of the main task.\n            Set between 0 to 1\n        lstm_impl (string, optional): a base implementation of LSTM. This is\n            not used for GRU models.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n            Default is LSTMBlockCell (the fastest).\n        use_peephole (bool, optional): if True, use peephole connection. This\n            is not used for GRU models.\n        splice (int, optional): the number of frames to splice. This is used\n            when using CNN-like encoder. Default is 1 frame.\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_grad_norm (float, optional): the range of clipping of gradient\n            norm (> 0)\n        clip_activation (float, optional): the range of clipping of cell\n            activation (> 0). This is not used for GRU models.\n        num_proj (int, optional): the number of nodes in the projection layer.\n            This is not used for GRU models.\n        weight_decay (float, optional): a parameter for weight decay\n        bottleneck_dim (int, optional): the dimensions of the bottleneck layer\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n    """"""\n\n    def __init__(self,\n                 encoder_type,\n                 input_size,\n                 num_units,\n                 num_layers_main,\n                 num_layers_sub,\n                 num_classes_main,\n                 num_classes_sub,\n                 main_task_weight,\n                 lstm_impl=\'LSTMBlockCell\',\n                 use_peephole=True,\n                 splice=1,\n                 parameter_init=0.1,\n                 clip_grad_norm=None,\n                 clip_activation=None,\n                 num_proj=None,\n                 weight_decay=0.0,\n                 bottleneck_dim=None,\n                 time_major=True):\n\n        super(MultitaskCTC, self).__init__(\n            encoder_type, input_size, num_units, num_layers_main,\n            num_classes_main, lstm_impl, use_peephole, splice,\n            parameter_init, clip_grad_norm, clip_activation, num_proj,\n            weight_decay, bottleneck_dim, time_major)\n\n        self.num_classes_sub = num_classes_sub + 1  # + blank label\n        if float(main_task_weight) < 0 or float(main_task_weight) > 1:\n            raise ValueError(\'Set main_task_weight between 0 to 1.\')\n        self.main_task_weight = main_task_weight\n        self.sub_task_weight = 1 - self.main_task_weight\n\n        # Placeholder for multi-task\n        self.labels_sub_pl_list = []\n\n        self.name = encoder_type + \'_ctc\'\n\n        if [\'multitask_blstm\', \'multitask_lstm\']:\n            self.encoder = load(encoder_type)(\n                num_units=num_units,\n                num_proj=self.num_proj,\n                num_layers_main=num_layers_main,\n                num_layers_sub=num_layers_sub,\n                lstm_impl=lstm_impl,\n                use_peephole=use_peephole,\n                parameter_init=parameter_init,\n                clip_activation=clip_activation,\n                time_major=time_major)\n        else:\n            raise NotImplementedError\n\n    def _build(self, inputs, inputs_seq_len, keep_prob):\n        """"""Construct model graph.\n        Args:\n            inputs: A tensor of size `[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n        Returns:\n            logits_main: A tensor of size `[T, B, num_classes]`\n            logits_sub: A tensor of size `[T, B, num_classes]`\n        """"""\n        # inputs: `[B, T, input_size]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n\n        encoder_outputs, final_state, encoder_outputs_sub, final_state_sub = self.encoder(\n            inputs, inputs_seq_len, keep_prob)\n\n        # Reshape to apply the same weights over the timesteps\n        if \'lstm\' not in self.encoder_type or self.num_proj is None or self.num_proj == 0:\n            if \'b\' in self.encoder_type:\n                # bidirectional\n                outputs_2d = tf.reshape(\n                    encoder_outputs, shape=[-1, self.num_units * 2])\n                outputs_sub_2d = tf.reshape(\n                    encoder_outputs_sub, shape=[-1, self.num_units * 2])\n            else:\n                # unidirectional\n                outputs_2d = tf.reshape(\n                    encoder_outputs, shape=[-1, self.num_units])\n                outputs_sub_2d = tf.reshape(\n                    encoder_outputs_sub, shape=[-1, self.num_units])\n        else:\n            if \'b\' in self.encoder_type:\n                # bidirectional\n                outputs_2d = tf.reshape(\n                    encoder_outputs, shape=[-1, self.num_proj * 2])\n                outputs_sub_2d = tf.reshape(\n                    encoder_outputs_sub, shape=[-1, self.num_proj * 2])\n            else:\n                # unidirectional\n                outputs_2d = tf.reshape(\n                    encoder_outputs, shape=[-1, self.num_proj])\n                outputs_sub_2d = tf.reshape(\n                    encoder_outputs_sub, shape=[-1, self.num_proj])\n\n        with tf.variable_scope(\'output_sub\') as scope:\n            logits_sub_2d = tf.contrib.layers.fully_connected(\n                outputs_sub_2d, self.num_classes_sub,\n                activation_fn=None,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n\n            if self.time_major:\n                # Reshape back to the original shape\n                logits_sub = tf.reshape(\n                    logits_sub_2d,\n                    shape=[max_time, batch_size, self.num_classes_sub])\n            else:\n                # Reshape back to the original shape\n                logits_sub = tf.reshape(\n                    logits_sub_2d,\n                    shape=[batch_size, max_time, self.num_classes_sub])\n\n                # Convert to time-major: `[T, B, num_classes]\'\n                logits = tf.transpose(logits_sub, [1, 0, 2])\n\n        if self.bottleneck_dim is not None and self.bottleneck_dim != 0:\n            with tf.variable_scope(\'bottleneck\') as scope:\n                outputs_2d = tf.contrib.layers.fully_connected(\n                    outputs_2d, self.bottleneck_dim,\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=scope)\n\n                # Dropout for the hidden-output connections\n                outputs_2d = tf.nn.dropout(\n                    outputs_2d, keep_prob, name=\'dropout_bottleneck\')\n\n        with tf.variable_scope(\'output_main\') as scope:\n            logits_2d = tf.contrib.layers.fully_connected(\n                outputs_2d, self.num_classes,\n                activation_fn=None,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n\n            if self.time_major:\n                # Reshape back to the original shape\n                logits = tf.reshape(\n                    logits_2d,\n                    shape=[max_time, batch_size, self.num_classes])\n            else:\n                # Reshape back to the original shape\n                logits = tf.reshape(\n                    logits_2d,\n                    shape=[batch_size, max_time, self.num_classes])\n\n                # Convert to time-major: `[T, B, num_classes]\'\n                logits = tf.transpose(logits, [1, 0, 2])\n\n        return logits, logits_sub\n\n    def create_placeholders(self):\n        """"""Create placeholders and append them to list.""""""\n        self.inputs_pl_list.append(\n            tf.placeholder(tf.float32, shape=[None, None, self.input_size],\n                           name=\'input\'))\n        self.labels_pl_list.append(\n            tf.SparseTensor(tf.placeholder(tf.int64, name=\'indices\'),\n                            tf.placeholder(tf.int32, name=\'values\'),\n                            tf.placeholder(tf.int64, name=\'shape\')))\n        self.labels_sub_pl_list.append(\n            tf.SparseTensor(tf.placeholder(tf.int64, name=\'indices_sub\'),\n                            tf.placeholder(tf.int32, name=\'values_sub\'),\n                            tf.placeholder(tf.int64, name=\'shape_sub\')))\n        self.inputs_seq_len_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None], name=\'inputs_seq_len\'))\n        self.keep_prob_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob\'))\n\n    def compute_loss(self, inputs, labels_main, labels_sub, inputs_seq_len,\n                     keep_prob, scope=None):\n        """"""Operation for computing ctc loss.\n        Args:\n            inputs: A tensor of size `[B, T, input_size]`\n            labels_main: A SparseTensor of target labels in the main task\n            labels_sub: A SparseTensor of target labels in the sub task\n            inputs_seq_len: A tensor of size `[B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            scope: A scope in the model tower\n        Returns:\n            total_loss: operation for computing total ctc loss\n            logits_main: A tensor of size `[T, B, input_size]`\n            logits_sub: A tensor of size `[T, B, input_size]`\n        """"""\n        # Build model graph\n        logits_main, logits_sub = self._build(\n            inputs, inputs_seq_len, keep_prob)\n\n        # Weight decay\n        if self.weight_decay > 0:\n            with tf.name_scope(""weight_decay_loss""):\n                weight_sum = 0\n                for var in tf.trainable_variables():\n                    if \'bias\' not in var.name.lower():\n                        weight_sum += tf.nn.l2_loss(var)\n                tf.add_to_collection(\'losses\', weight_sum * self.weight_decay)\n\n        with tf.name_scope(""ctc_loss_main""):\n            ctc_losses = tf.nn.ctc_loss(\n                labels_main,\n                logits_main,\n                # tf.cast(inputs_seq_len, tf.int32),\n                inputs_seq_len,\n                preprocess_collapse_repeated=False,\n                ctc_merge_repeated=True,\n                ignore_longer_outputs_than_inputs=False,\n                time_major=True)\n            ctc_loss_main = tf.reduce_mean(\n                ctc_losses, name=\'ctc_loss_mean_main\')\n            tf.add_to_collection(\n                \'losses\', ctc_loss_main * self.main_task_weight)\n\n        with tf.name_scope(""ctc_loss_sub""):\n            ctc_losses = tf.nn.ctc_loss(\n                labels_sub,\n                logits_sub,\n                # tf.cast(inputs_seq_len, tf.int32),\n                inputs_seq_len,\n                preprocess_collapse_repeated=False,\n                ctc_merge_repeated=True,\n                ignore_longer_outputs_than_inputs=False,\n                time_major=True)\n            ctc_loss_sub = tf.reduce_mean(\n                ctc_losses, name=\'ctc_loss_mean_sub\')\n            tf.add_to_collection(\n                \'losses\', ctc_loss_sub * self.sub_task_weight)\n\n        # Compute total loss\n        total_loss = tf.add_n(tf.get_collection(\'losses\', scope),\n                              name=\'total_loss\')\n\n        # Add a scalar summary for the snapshot of loss\n        if self.weight_decay > 0:\n            self.summaries_train.append(\n                tf.summary.scalar(\'weight_loss_train\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'weight_loss_dev\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_train.append(\n                tf.summary.scalar(\'total_loss_train\', total_loss))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'total_loss_dev\', total_loss))\n\n        self.summaries_train.append(\n            tf.summary.scalar(\'ctc_loss_main_train\',\n                              ctc_loss_main * self.main_task_weight))\n        self.summaries_dev.append(\n            tf.summary.scalar(\'ctc_loss_main_dev\',\n                              ctc_loss_main * self.main_task_weight))\n\n        self.summaries_train.append(\n            tf.summary.scalar(\'ctc_loss_sub_train\',\n                              ctc_loss_sub * self.sub_task_weight))\n        self.summaries_dev.append(\n            tf.summary.scalar(\'ctc_loss_sub_dev\',\n                              ctc_loss_sub * self.sub_task_weight))\n\n        return total_loss, logits_main, logits_sub\n\n    def decoder(self, logits_main, logits_sub, inputs_seq_len, beam_width=1):\n        """"""Operation for decoding.\n        Args:\n            logits_main: A tensor of size `[T, B, input_size]`\n            logits_sub: A tensor of size `[T, B, input_size]`\n            inputs_seq_len: A tensor of size `[B]`\n            beam_width (int, optional): beam width for beam search.\n                1 disables beam search, which mean greedy decoding.\n        Return:\n            decode_op_main: operation for decoding of the main task\n            decode_op_sub: operation for decoding of the sub task\n        """"""\n        assert isinstance(beam_width, int), ""beam_width must be integer.""\n        assert beam_width >= 1, ""beam_width must be >= 1""\n\n        # inputs_seq_len = tf.cast(inputs_seq_len, tf.int32)\n\n        if beam_width == 1:\n            decoded_main, _ = tf.nn.ctc_greedy_decoder(\n                logits_main, inputs_seq_len)\n            decoded_sub, _ = tf.nn.ctc_greedy_decoder(\n                logits_sub, inputs_seq_len)\n\n        else:\n            decoded_main, _ = tf.nn.ctc_beam_search_decoder(\n                logits_main, inputs_seq_len,\n                beam_width=beam_width)\n            decoded_sub, _ = tf.nn.ctc_beam_search_decoder(\n                logits_sub, inputs_seq_len,\n                beam_width=beam_width)\n\n        decode_op_main = tf.to_int32(decoded_main[0])\n        decode_op_sub = tf.to_int32(decoded_sub[0])\n\n        return decode_op_main, decode_op_sub\n\n    def posteriors(self, logits_main, logits_sub):\n        """"""Operation for computing posteriors of each time steps.\n        Args:\n            logits_main: A tensor of size `[T, B, input_size]`\n            logits_sub: A tensor of size `[T, B, input_size]`\n        Return:\n            posteriors_op_main: operation for computing posteriors for each\n                class in the main task\n            posteriors_op_sub: operation for computing posteriors for each\n                class in the sub task\n        """"""\n        # Convert to batch-major: `[B, T, num_classes]\'\n        logits_main = tf.transpose(logits_main, (1, 0, 2))\n        logits_sub = tf.transpose(logits_sub, (1, 0, 2))\n\n        logits_2d_main = tf.reshape(logits_main,\n                                    shape=[-1, self.num_classes])\n        posteriors_op_main = tf.nn.softmax(logits_2d_main)\n\n        logits_2d_sub = tf.reshape(logits_sub,\n                                   shape=[-1, self.num_classes_sub])\n        posteriors_op_sub = tf.nn.softmax(logits_2d_sub)\n\n        return posteriors_op_main, posteriors_op_sub\n\n    def compute_ler(self, decode_op_main, decode_op_sub,\n                    labels_main, labels_sub):\n        """"""Operation for computing LER (Label Error Rate).\n        Args:\n            decode_op_main: operation for decoding of the main task\n            decode_op_sub: operation for decoding of the sub task\n            labels_main: A SparseTensor of target labels in the main task\n            labels_sub: A SparseTensor of target labels in the sub task\n        Return:\n            ler_op_main: operation for computing LER of the main task\n            ler_op_sub: operation for computing LER of the sub task\n        """"""\n        # Compute LER (normalize by label length)\n        ler_op_main = tf.reduce_mean(tf.edit_distance(\n            decode_op_main, labels_main, normalize=True))\n        ler_op_sub = tf.reduce_mean(tf.edit_distance(\n            decode_op_sub, labels_sub, normalize=True))\n\n        # Add a scalar summary for the snapshot of LER\n        self.summaries_train.append(tf.summary.scalar(\n            \'ler_main_train\', ler_op_main))\n        self.summaries_train.append(tf.summary.scalar(\n            \'ler_sub_train\', ler_op_sub))\n        self.summaries_dev.append(tf.summary.scalar(\n            \'ler_main_dev\', ler_op_main))\n        self.summaries_dev.append(tf.summary.scalar(\n            \'ler_sub_dev\', ler_op_sub))\n\n        return ler_op_main, ler_op_sub\n'"
models/ctc/student_ctc.py,64,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Student CTC model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom models.model_base import ModelBase\n\nfrom models.encoders.core.student_cnn_ctc import StudentCNNCTCEncoder\nfrom models.encoders.core.student_cnn_compact_ctc import StudentCNNCompactCTCEncoder\nfrom models.encoders.core.student_cnn_xe import StudentCNNXEEncoder\nfrom models.encoders.core.student_cnn_compact_xe import StudentCNNCompactXEEncoder\n\n\nclass StudentCTC(ModelBase):\n    """"""Connectionist Temporal Classification (CTC) network.\n    Args:\n        encoder_type (string): The type of an encoder\n            student_cnn_xe:\n            student_cnn_compact_xe:\n            student_cnn_ctc:\n            student_cnn_compact_ctc:\n        input_size (int): the dimensions of input vectors\n        num_classes (int): the number of classes of target labels\n            (except for a blank label)\n        splice (int, optional): the number of frames to splice. This is used\n            when using CNN-like encoder. Default is 1 frame.\n        num_stack (int, optional): the number of frames to stack\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_grad_norm (float, optional): the range of clipping of gradient\n            norm (> 0)\n        weight_decay (float, optional): a parameter for weight decay\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n    """"""\n\n    def __init__(self,\n                 encoder_type,\n                 input_size,\n                 num_classes,\n                 splice=1,\n                 num_stack=1,\n                 parameter_init=0.1,\n                 clip_grad_norm=None,\n                 weight_decay=0.0,\n                 time_major=True):\n\n        super(StudentCTC, self).__init__()\n\n        assert input_size % 3 == 0, \'input_size must be divisible by 3 (+ delta, double delta features).\'\n        assert splice % 2 == 1, \'splice must be the odd number\'\n        if clip_grad_norm is not None:\n            assert float(\n                clip_grad_norm) > 0, \'clip_grad_norm must be larger than 0.\'\n        assert float(\n            weight_decay) >= 0, \'weight_decay must not be a negative value.\'\n\n        # Encoder setting\n        self.encoder_type = encoder_type\n        self.input_size = input_size\n        self.splice = splice\n        self.num_stack = num_stack  # NOTE: this is used for CNN-like encoders\n        self.num_classes = num_classes + 1  # + blank\n\n        # Regularization\n        self.parameter_init = parameter_init\n        self.clip_grad_norm = clip_grad_norm\n        self.weight_decay = weight_decay\n\n        # Summaries for TensorBoard\n        self.summaries_train = []\n        self.summaries_dev = []\n\n        # Placeholders\n        self.inputs_pl_list = []\n        self.labels_pl_list = []\n        self.inputs_seq_len_pl_list = []\n        self.keep_prob_pl_list = []\n\n        self.time_major = time_major\n        self.name = encoder_type + \'_ctc\'\n\n        if encoder_type == \'student_cnn\':\n            self.encoder = StudentCNNCTCEncoder(\n                input_size=input_size,\n                splice=splice,\n                num_stack=num_stack,\n                parameter_init=parameter_init,\n                time_major=time_major)\n\n        elif encoder_type == \'student_cnn_compact\':\n            self.encoder = StudentCNNCompactCTCEncoder(\n                input_size=input_size,\n                splice=splice,\n                num_stack=num_stack,\n                parameter_init=parameter_init,\n                time_major=time_major)\n\n        elif encoder_type == \'student_cnn_xe\':\n            self.encoder = StudentCNNXEEncoder(\n                input_size=input_size,\n                splice=splice,\n                num_stack=num_stack,\n                parameter_init=parameter_init)\n\n        elif encoder_type == \'student_cnn_compact_xe\':\n            self.encoder = StudentCNNCompactXEEncoder(\n                input_size=input_size,\n                splice=splice,\n                num_stack=num_stack,\n                parameter_init=parameter_init)\n\n        else:\n            raise NotImplementedError\n\n    def _build_ctc(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs: A tensor of size `[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            logits: A tensor of size `[T, B, num_classes]`\n        """"""\n        # inputs: `[B, T, input_size]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n\n        encoder_outputs, final_state = self.encoder(\n            inputs, inputs_seq_len, keep_prob, is_training)\n\n        # Reshape to apply the same weights over the timesteps\n        output_dim = encoder_outputs.shape.as_list()[-1]\n        outputs_2d = tf.reshape(\n            encoder_outputs, shape=[batch_size * max_time, output_dim])\n\n        with tf.variable_scope(\'output\') as scope:\n            logits_2d = tf.contrib.layers.fully_connected(\n                outputs_2d,\n                num_outputs=self.num_classes,\n                activation_fn=None,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n\n            if self.time_major:\n                # Reshape back to the original shape\n                logits = tf.reshape(\n                    logits_2d, shape=[max_time, batch_size, self.num_classes])\n            else:\n                # Reshape back to the original shape\n                logits = tf.reshape(\n                    logits_2d, shape=[batch_size, max_time, self.num_classes])\n\n                # Convert to time-major: `[T, B, num_classes]\'\n                logits = tf.transpose(logits, [1, 0, 2])\n\n        return logits\n\n    def _build_xe(self, inputs, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs: A tensor of size `[B, input_size]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            logits: A tensor of size `[B, num_classes]`\n        """"""\n        encoder_outputs = self.encoder(inputs, keep_prob, is_training)\n\n        with tf.variable_scope(\'output\') as scope:\n            logits = tf.contrib.layers.fully_connected(\n                encoder_outputs,\n                num_outputs=self.num_classes,\n                activation_fn=None,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n\n        return logits\n\n    def create_placeholders_ctc(self):\n        """"""Create placeholders for CTC training and append them to list.""""""\n        self.inputs_pl_list.append(\n            tf.placeholder(tf.float32,\n                           shape=[None, None, self.input_size * self.splice],\n                           name=\'input\'))\n        self.labels_pl_list.append(\n            tf.SparseTensor(tf.placeholder(tf.int64, name=\'indices\'),\n                            tf.placeholder(tf.int32, name=\'values\'),\n                            tf.placeholder(tf.int64, name=\'shape\')))\n        self.inputs_seq_len_pl_list.append(\n            tf.placeholder(tf.int32, shape=[None], name=\'inputs_seq_len\'))\n        self.keep_prob_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob\'))\n\n    def create_placeholders_xe(self):\n        """"""Create placeholders for XE training and append them to list.""""""\n        self.inputs_pl_list.append(\n            tf.placeholder(tf.float32,\n                           shape=[None, self.input_size],\n                           name=\'input\'))\n        self.labels_pl_list.append(\n            tf.placeholder(tf.float32,\n                           shape=[None, self.num_classes],\n                           name=\'label\'))\n        self.keep_prob_pl_list.append(\n            tf.placeholder(tf.float32, name=\'keep_prob\'))\n\n    def compute_ctc_loss(self, inputs, labels, inputs_seq_len,\n                         keep_prob, scope=None, softmax_temperature=1,\n                         is_training=True):\n        """"""Operation for computing CTC loss.\n        Args:\n            inputs: A tensor of size `[B, T, input_size]`\n            labels: A SparseTensor of target labels\n            inputs_seq_len: A tensor of size `[B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            scope (optional): A scope in the model tower\n            softmax_temperature (int, optional): temperature parameter for\n                ths softmax layer in the student training stage\n            is_training (bool, optional):\n        Returns:\n            total_loss: operation for computing total ctc loss (ctc loss + L2).\n                 This is a single scalar tensor to minimize.\n            logits: A tensor of size `[T, B, num_classes]`\n        """"""\n        # Build model graph\n        logits = self._build_ctc(inputs, inputs_seq_len, keep_prob,\n                                 is_training=is_training)\n\n        # Weight decay\n        if self.weight_decay > 0:\n            with tf.name_scope(""weight_decay_loss""):\n                weight_sum = 0\n                for var in tf.trainable_variables():\n                    if \'bias\' not in var.name.lower():\n                        weight_sum += tf.nn.l2_loss(var)\n                tf.add_to_collection(\'losses\', weight_sum * self.weight_decay)\n\n        with tf.name_scope(""ctc_loss""):\n            ctc_losses = tf.nn.ctc_loss(\n                labels,\n                logits / softmax_temperature,\n                tf.cast(inputs_seq_len, tf.int32),\n                # inputs_seq_len,\n                preprocess_collapse_repeated=False,\n                ctc_merge_repeated=True,\n                # ignore_longer_outputs_than_inputs=False,\n                ignore_longer_outputs_than_inputs=True,\n                time_major=True)\n            ctc_loss = tf.reduce_mean(ctc_losses, name=\'ctc_loss_mean\')\n            tf.add_to_collection(\'losses\', ctc_loss)\n\n        # Compute total loss\n        total_loss = tf.add_n(tf.get_collection(\'losses\', scope),\n                              name=\'total_loss\')\n\n        # Add a scalar summary for the snapshot of loss\n        if self.weight_decay > 0:\n            self.summaries_train.append(\n                tf.summary.scalar(\'weight_loss_train\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'weight_loss_dev\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_train.append(\n                tf.summary.scalar(\'total_loss_train\', total_loss))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'total_loss_dev\', total_loss))\n\n        self.summaries_train.append(\n            tf.summary.scalar(\'ctc_loss_train\', ctc_loss))\n        self.summaries_dev.append(\n            tf.summary.scalar(\'ctc_loss_dev\', ctc_loss))\n\n        return total_loss, logits\n\n    def compute_xe_loss(self, inputs, soft_targets, keep_prob,\n                        scope=None, softmax_temperature=1,\n                        is_training=True):\n        """"""Operation for computing XE loss.\n        Args:\n            inputs: A tensor of size `[B, input_size]`\n            soft_targets: A tensor of size `[B, num_classes]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            scope (optional): A scope in the model tower\n            softmax_temperature (int, optional): temperature parameter for\n                ths softmax layer in the student training stage\n            is_training (bool, optional):\n        Returns:\n            total_loss: operation for computing total ctc loss (XE loss + L2).\n                 This is a single scalar tensor to minimize.\n            logits: A tensor of size `[B, num_classes]`\n        """"""\n        # Build model graph\n        logits = self._build_xe(inputs, keep_prob,\n                                is_training=is_training)\n\n        # Weight decay\n        if self.weight_decay > 0:\n            with tf.name_scope(""weight_decay_loss""):\n                weight_sum = 0\n                for var in tf.trainable_variables():\n                    if \'bias\' not in var.name.lower():\n                        weight_sum += tf.nn.l2_loss(var)\n                tf.add_to_collection(\'losses\', weight_sum * self.weight_decay)\n\n        with tf.name_scope(""xe_loss""):\n            losses_soft = tf.nn.softmax_cross_entropy_with_logits(\n                labels=soft_targets,\n                # logits=logits / (softmax_temperature ** 2),\n                logits=logits)\n            loss_soft = tf.reduce_mean(losses_soft, name=\'xe_loss_mean\')\n            tf.add_to_collection(\'losses\', loss_soft)\n\n        # Compute total loss\n        total_loss = tf.add_n(tf.get_collection(\'losses\', scope),\n                              name=\'total_loss\')\n\n        # Add a scalar summary for the snapshot of loss\n        if self.weight_decay > 0:\n            self.summaries_train.append(\n                tf.summary.scalar(\'weight_loss_train\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'weight_loss_dev\',\n                                  weight_sum * self.weight_decay))\n            self.summaries_train.append(\n                tf.summary.scalar(\'total_loss_train\', total_loss))\n            self.summaries_dev.append(\n                tf.summary.scalar(\'total_loss_dev\', total_loss))\n\n        self.summaries_train.append(\n            tf.summary.scalar(\'xe_loss_train\', loss_soft))\n        self.summaries_dev.append(\n            tf.summary.scalar(\'xe_loss_dev\', loss_soft))\n\n        return total_loss, logits\n\n    def decoder(self, logits, inputs_seq_len, beam_width=1):\n        """"""Operation for decoding.\n        Args:\n            logits: A tensor of size `[T, B, num_classes]`\n            inputs_seq_len: A tensor of size `[B]`\n            beam_width (int, optional): beam width for beam search.\n                1 disables beam search, which mean greedy decoding.\n        Return:\n            decode_op: A SparseTensor\n        """"""\n        assert isinstance(beam_width, int), ""beam_width must be integer.""\n        assert beam_width >= 1, ""beam_width must be >= 1""\n\n        # inputs_seq_len = tf.cast(inputs_seq_len, tf.int32)\n\n        if beam_width == 1:\n            decoded, _ = tf.nn.ctc_greedy_decoder(\n                logits, inputs_seq_len)\n        else:\n            decoded, _ = tf.nn.ctc_beam_search_decoder(\n                logits, inputs_seq_len,\n                beam_width=beam_width)\n\n        decode_op = tf.to_int32(decoded[0])\n\n        # TODO: chnage function name to `decode`\n\n        return decode_op\n\n    def posteriors(self, logits, blank_prior=1):\n        """"""Operation for computing posteriors of each time steps.\n        Args:\n            logits: A tensor of size `[T, B, num_classes]`\n            blank_prior (float): A prior for blank classes. posteriors are\n                divided by this prior.\n        Return:\n            posteriors_op: operation for computing posteriors for each class\n        """"""\n        # Convert to batch-major: `[B, T, num_classes]\'\n        logits = tf.transpose(logits, (1, 0, 2))\n\n        logits_2d = tf.reshape(logits, [-1, self.num_classes])\n\n        posteriors_op = tf.nn.softmax(logits_2d)\n\n        return posteriors_op\n\n    def compute_ler(self, decode_op, labels):\n        """"""Operation for computing LER (Label Error Rate).\n        Args:\n            decode_op: operation for decoding\n            labels: A SparseTensor of target labels\n        Return:\n            ler_op: operation for computing LER\n        """"""\n        # Compute LER (normalize by label length)\n        ler_op = tf.reduce_mean(tf.edit_distance(\n            decode_op, labels, normalize=True))\n\n        # Add a scalar summary for the snapshot of LER\n        self.summaries_train.append(tf.summary.scalar(\'ler_train\', ler_op))\n        self.summaries_dev.append(tf.summary.scalar(\'ler_dev\', ler_op))\n\n        return ler_op\n'"
models/encoders/__init__.py,0,b''
models/encoders/load_encoder.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Select & load encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom models.encoders.core.blstm import BLSTMEncoder\nfrom models.encoders.core.lstm import LSTMEncoder\nfrom models.encoders.core.gru import GRUEncoder, BGRUEncoder\nfrom models.encoders.core.cnn_zhang import CNNEncoder\nfrom models.encoders.core.vgg_blstm import VGGBLSTMEncoder\nfrom models.encoders.core.vgg_lstm import VGGLSTMEncoder\nfrom models.encoders.core.vgg_wang import VGGEncoder\n# from models.encoders.core.resnet_wang import ResNetEncoder\nfrom models.encoders.core.multitask_blstm import MultitaskBLSTMEncoder\nfrom models.encoders.core.multitask_lstm import MultitaskLSTMEncoder\nfrom models.encoders.core.pyramidal_blstm import PyramidBLSTMEncoder\nfrom models.encoders.core.cldnn_wang import CLDNNEncoder\n\nfrom models.encoders.core.student_cnn_ctc import StudentCNNCTCEncoder\nfrom models.encoders.core.student_cnn_compact_ctc import StudentCNNCompactCTCEncoder\n\nENCODERS = {\n    ""blstm"": BLSTMEncoder,\n    ""lstm"": LSTMEncoder,\n    ""bgru"": BGRUEncoder,\n    ""gru"": GRUEncoder,\n    ""vgg_blstm"": VGGBLSTMEncoder,\n    ""vgg_lstm"": VGGLSTMEncoder,\n    ""cnn_zhang"": CNNEncoder,\n    ""vgg_wang"": VGGEncoder,\n    # ""resnet_wang"": ResNetEncoder,\n    ""multitask_blstm"": MultitaskBLSTMEncoder,\n    ""multitask_lstm"": MultitaskLSTMEncoder,\n    ""pyramid_blstm"": PyramidBLSTMEncoder,\n    ""cldnn_wang"": CLDNNEncoder,\n\n    ""student_cnn_ctc"": StudentCNNCTCEncoder,\n    ""student_cnn_compact_ctc"": StudentCNNCompactCTCEncoder\n}\n\n\ndef load(encoder_type):\n    """"""Select & load encoder.\n    Args:\n        encoder_type (string): name of the ctc model in the key of ENCODERS\n    Returns:\n        An instance of the encoder\n    """"""\n    if encoder_type not in ENCODERS.keys():\n        raise ValueError(\n            ""encoder_type should be one of [%s], you provided %s."" %\n            ("", "".join(ENCODERS), encoder_type))\n    return ENCODERS[encoder_type]\n'"
models/lm/__init__.py,0,b''
models/lm/base.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass RNNLM(object):\n    """"""\n    """"""\n\n    def __init__(self):\n        raise NotImplementedError\n'"
models/lm/char_rnnlm.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass CharRNNLM(object):\n    """"""\n    """"""\n\n    def __init__(self):\n        raise NotImplementedError\n'"
models/lm/word_rnnlm.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass WordRNNLM(object):\n    """"""\n    """"""\n\n    def __init__(self):\n        raise NotImplementedError\n'"
models/recurrent/__init__.py,0,b''
models/recurrent/initializer.py,7,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef identity_initializer(scale=1.0):\n    def _initializer(shape, dtype=tf.float32):\n        if len(shape) == 1:\n            return tf.constant(0., dtype=dtype, shape=shape)\n        elif len(shape) == 2 and shape[0] == shape[1]:\n            return tf.constant(scale * np.identity(shape[0], dtype))\n        elif len(shape) == 4 and shape[2] == shape[3]:\n            array = np.zeros(shape, dtype=float)\n            cx, cy = shape[0] / 2, shape[1] / 2\n            for i in range(shape[2]):\n                array[cx, cy, i, i] = 1\n            return tf.constant(scale * array, dtype=dtype)\n        else:\n            raise\n    return _initializer\n\n\ndef orthogonal_initializer(scale=1.0):\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        if partition_info is not None:\n            ValueError(""Do not know what to do with partition_info in BN_LSTMCell"")\n        flat_shape = (shape[0], int(np.prod(shape[1:])))\n        a = np.random.normal(0.0, 1.0, flat_shape)\n        u, _, v = np.linalg.svd(a, full_matrices=False)\n        # pick the one with the correct shape\n        q = u if u.shape == flat_shape else v\n        q = q.reshape(shape)  # this needs to be corrected to float32\n        # return tf.constant(scale * q[:shape[0], :shape[1]], dtype=dtype)\n        return tf.constant(scale * q, dtype=dtype)\n\n    return _initializer\n'"
models/test/__init__.py,0,b''
models/test/data.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom utils.io.inputs.splicing import do_splice\nfrom utils.io.inputs.frame_stacking import stack_frame\n\nfrom utils.io.inputs.feature_extraction import wav2feature\nfrom utils.io.labels.phone import Phone2idx\n\nSPACE = \'_\'\nSOS = \'<\'\nEOS = \'>\'\n\n\ndef _read_text(text_path):\n    """"""Read char-level transcripts.\n    Args:\n        text_path (string): path to a transcript text file\n    Returns:\n        transcript (string): a text of transcript\n    """"""\n    # Read ground truth labels\n    with open(text_path, \'r\') as f:\n        line = f.readlines()[-1]\n        transcript = SPACE.join(line.strip().lower().split(\' \')[2:])\n    return transcript\n\n\ndef _read_phone(text_path):\n    """"""Read phone-level transcripts.\n    Args:\n        text_path (string): path to a transcript text file\n    Returns:\n        transcript (string): a text of transcript\n    """"""\n    # Read ground truth labels\n    phone_list = []\n    with open(text_path, \'r\') as f:\n        for line in f:\n            line = line.strip().split(\' \')\n            phone_list.append(line[-1])\n    transcript = \' \'.join(phone_list)\n    return transcript\n\n\ndef generate_data(label_type, model, batch_size=1, num_stack=1, splice=1):\n    """"""\n    Args:\n        label_type (string): character or phone or multitask\n        model (string): ctc or attention or joint_ctc_attention\n        batch_size (int, optional): the size of mini-batch\n        num_stack (int, optional) the number of frames to stack\n        splice (int, optional): frames to splice. Default is 1 frame.\n    Returns:\n        inputs: `[B, T, input_size]`\n        labels: `[B]`\n        inputs_seq_len: `[B, frame_num]`\n        labels_seq_len: `[B]` (if model is attention)\n    """"""\n    # Make input data\n    inputs, inputs_seq_len = wav2feature(\n        [\'./sample/LDC93S1.wav\'] * batch_size,\n        feature_type=\'logfbank\', feature_dim=40,\n        energy=False, delta1=True, delta2=True)\n\n    # Frame stacking\n    inputs = stack_frame(inputs,\n                         num_stack=num_stack,\n                         num_skip=num_stack,\n                         progressbar=False)\n    if num_stack != 1:\n        for i in range(len(inputs_seq_len)):\n            inputs_seq_len[i] = len(inputs[i])\n\n    # Splice\n    inputs = do_splice(inputs,\n                       splice=splice,\n                       batch_size=batch_size,\n                       num_stack=num_stack)\n\n    phone2idx = Phone2idx(map_file_path=\'./phone61.txt\')\n\n    trans_char = _read_text(\'./sample/LDC93S1.txt\')\n    trans_char = trans_char.replace(\'.\', \'\')\n    trans_phone = _read_phone(\'./sample/LDC93S1.phn\')\n\n    # Make transcripts\n    if model == \'ctc\':\n        if label_type == \'character\':\n            labels = [alpha2idx(trans_char)] * batch_size\n            return inputs, labels, inputs_seq_len\n\n        elif label_type == \'phone\':\n            labels = [phone2idx(trans_phone.split(\' \'))] * batch_size\n            return inputs, labels, inputs_seq_len\n\n        elif label_type == \'multitask\':\n            labels_char = [alpha2idx(trans_char)] * batch_size\n            labels_phone = [phone2idx(trans_phone.split(\' \'))] * batch_size\n            return inputs, labels_char, labels_phone, inputs_seq_len\n\n    elif model == \'attention\':\n        if label_type == \'character\':\n            trans_char = SOS + trans_char + EOS\n            labels = [alpha2idx(trans_char)] * batch_size\n            labels_seq_len = [len(labels[0])] * batch_size\n            return inputs, labels, inputs_seq_len, labels_seq_len\n\n        elif label_type == \'phone\':\n            trans_phone = SOS + \' \' + trans_phone + \' \' + EOS\n            labels = [phone2idx(trans_phone.split(\' \'))] * batch_size\n            labels_seq_len = [len(labels[0])] * batch_size\n            return inputs, labels, inputs_seq_len, labels_seq_len\n\n        elif label_type == \'multitask\':\n            trans_char = SOS + trans_char + EOS\n            trans_phone = SOS + \' \' + trans_phone + \' \' + EOS\n            labels_char = [alpha2idx(trans_char)] * batch_size\n            labels_phone = [phone2idx(trans_phone.split(\' \'))] * batch_size\n            target_len_char = [len(labels_char[0])] * batch_size\n            target_len_phone = [len(labels_phone[0])] * batch_size\n            return (inputs, labels_char, labels_phone,\n                    inputs_seq_len, target_len_char, target_len_phone)\n\n    elif model == \'joint_ctc_attention\':\n        if label_type == \'character\':\n            att_trans_char = SOS + trans_char + EOS\n            att_labels = [alpha2idx(att_trans_char)] * batch_size\n            labels_seq_len = [len(att_labels[0])] * batch_size\n            ctc_labels = [alpha2idx(trans_char)] * batch_size\n        elif label_type == \'phone\':\n            att_trans_phone = SOS + \' \' + trans_phone + \' \' + EOS\n            att_labels = [phone2idx(att_trans_phone.split(\' \'))] * batch_size\n            labels_seq_len = [len(att_labels[0])] * batch_size\n            ctc_labels = [phone2idx(trans_phone.split(\' \'))] * batch_size\n        return inputs, att_labels, ctc_labels, inputs_seq_len, labels_seq_len\n\n\ndef alpha2idx(transcript):\n    """"""Convert from alphabet to number.\n    Args:\n        transcript (string): a sequence of characters\n    Returns:\n        index_list (list): indices of alphabets\n    """"""\n    char_list = list(transcript)\n\n    first_index = ord(\'a\')\n    index_list = []\n    for char in char_list:\n        if char == SPACE:\n            index_list.append(26)\n        elif char == SOS:\n            index_list.append(27)\n        elif char == EOS:\n            index_list.append(28)\n        else:\n            index_list.append(ord(char) - first_index)\n    return index_list\n\n\ndef idx2alpha(index_list):\n    """"""Convert from number to alphabet.\n    Args:\n        index_list (list): indices of alphabets\n    Returns:\n        transcript (string): a sequence of characters\n    """"""\n    first_index = ord(\'a\')\n    char_list = []\n    for num in index_list:\n        if num == 26:\n            char_list.append(SPACE)\n        elif num == 27:\n            char_list.append(SOS)\n        elif num == 28:\n            char_list.append(EOS)\n        else:\n            char_list.append(chr(num + first_index))\n    transcript = \'\'.join(char_list)\n    return transcript\n'"
models/test/test_attention.py,8,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nimport tensorflow as tf\n# from tensorflow.python import debug as tf_debug\n\nsys.path.append(os.path.abspath(\'../../\'))\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\nfrom models.test.data import generate_data, idx2alpha\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.parameter import count_total_parameters\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.measure_time_func import measure_time\n\n\nclass TestAttentionTraining(tf.test.TestCase):\n\n    def test(self):\n        print(""Attention Working check."")\n\n        # ok\n        # self.check(encoder_type=\'blstm\',attention_type=\'dot_product\')\n        # self.check(encoder_type=\'lstm\', attention_type=\'dot_product\')\n        # self.check(encoder_type=\'blstm\', attention_type=\'bahdanau_content\')\n        # self.check(encoder_type=\'lstm\', attention_type=\'bahdanau_content\')\n        # self.check(encoder_type=\'lstm\', attention_type=\'luong_dot\')\n        # self.check(encoder_type=\'blstm\', attention_type=\'luong_general\')\n        # self.check(encoder_type=\'lstm\', attention_type=\'luong_general\')\n        # self.check(encoder_type=\'blstm\', attention_type=\'luong_concat\')\n        # self.check(encoder_type=\'lstm\', attention_type=\'luong_concat\')\n        self.check(encoder_type=\'blstm\', attention_type=\'hybrid\')\n        # self.check(encoder_type=\'lstm\', attention_type=\'hybrid\')\n        # self.check(encoder_type=\'blstm\', attention_type=\'location\')\n        # self.check(encoder_type=\'lstm\', attention_type=\'location\')\n\n        # self.check(encoder_type=\'blstm\',attention_type=\'normed_bahdanau_content\')\n        # self.check(encoder_type=\'blstm\',attention_type=\'scaled_luong_dot\')\n        # self.check(encoder_type=\'blstm\',attention_type=\'baidu_attetion\')\n\n    @measure_time\n    def check(self, encoder_type, attention_type, label_type=\'character\'):\n\n        print(\'==================================================\')\n        print(\'  encoder_type: %s\' % encoder_type)\n        print(\'  attention_type: %s\' % attention_type)\n        print(\'  label_type: %s\' % label_type)\n        print(\'==================================================\')\n\n        tf.reset_default_graph()\n        with tf.Graph().as_default():\n            # Load batch data\n            batch_size = 4\n            inputs, labels, inputs_seq_len, labels_seq_len = generate_data(\n                label_type=label_type,\n                model=\'attention\',\n                batch_size=batch_size)\n\n            # Define model graph\n            num_classes = 27 if label_type == \'character\' else 61\n            model = AttentionSeq2Seq(input_size=inputs[0].shape[1],\n                                     encoder_type=encoder_type,\n                                     encoder_num_units=256,\n                                     encoder_num_layers=2,\n                                     encoder_num_proj=None,\n                                     attention_type=attention_type,\n                                     attention_dim=128,\n                                     decoder_type=\'lstm\',\n                                     decoder_num_units=256,\n                                     decoder_num_layers=1,\n                                     embedding_dim=64,\n                                     num_classes=num_classes,\n                                     sos_index=num_classes,\n                                     eos_index=num_classes + 1,\n                                     max_decode_length=100,\n                                     use_peephole=True,\n                                     splice=1,\n                                     parameter_init=0.1,\n                                     clip_grad_norm=5.0,\n                                     clip_activation_encoder=50,\n                                     clip_activation_decoder=50,\n                                     weight_decay=1e-8,\n                                     time_major=True,\n                                     sharpening_factor=1.0,\n                                     logits_temperature=1.0)\n\n            # Define placeholders\n            model.create_placeholders()\n            learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n            # Add to the graph each operation\n            loss_op, logits, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n                model.inputs_pl_list[0],\n                model.labels_pl_list[0],\n                model.inputs_seq_len_pl_list[0],\n                model.labels_seq_len_pl_list[0],\n                model.keep_prob_encoder_pl_list[0],\n                model.keep_prob_decoder_pl_list[0],\n                model.keep_prob_embedding_pl_list[0])\n            train_op = model.train(loss_op,\n                                   optimizer=\'adam\',\n                                   learning_rate=learning_rate_pl)\n            decode_op_train, decode_op_infer = model.decode(\n                decoder_outputs_train, decoder_outputs_infer)\n            ler_op = model.compute_ler(model.labels_st_true_pl,\n                                       model.labels_st_pred_pl)\n\n            # Define learning rate controller\n            learning_rate = 1e-3\n            lr_controller = Controller(learning_rate_init=learning_rate,\n                                       decay_start_epoch=20,\n                                       decay_rate=0.9,\n                                       decay_patient_epoch=10,\n                                       lower_better=True)\n\n            # Add the variable initializer operation\n            init_op = tf.global_variables_initializer()\n\n            # Count total parameters\n            parameters_dict, total_parameters = count_total_parameters(\n                tf.trainable_variables())\n            for parameter_name in sorted(parameters_dict.keys()):\n                print(""%s %d"" %\n                      (parameter_name, parameters_dict[parameter_name]))\n            print(""Total %d variables, %s M parameters"" %\n                  (len(parameters_dict.keys()),\n                   ""{:,}"".format(total_parameters / 1000000)))\n\n            # Make feed dict\n            feed_dict = {\n                model.inputs_pl_list[0]: inputs,\n                model.labels_pl_list[0]: labels,\n                model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n                model.labels_seq_len_pl_list[0]: labels_seq_len,\n                model.keep_prob_encoder_pl_list[0]: 0.8,\n                model.keep_prob_decoder_pl_list[0]: 1.0,\n                model.keep_prob_embedding_pl_list[0]: 1.0,\n                learning_rate_pl: learning_rate\n            }\n\n            idx2phone = Idx2phone(map_file_path=\'./phone61.txt\')\n\n            with tf.Session() as sess:\n                # Initialize parameters\n                sess.run(init_op)\n\n                # Wrapper for tfdbg\n                # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n\n                # Train model\n                max_steps = 1000\n                start_time_step = time.time()\n                for step in range(max_steps):\n\n                    # Compute loss\n                    _, loss_train = sess.run(\n                        [train_op, loss_op], feed_dict=feed_dict)\n\n                    # Gradient check\n                    # grads = sess.run(model.clipped_grads,\n                    #                  feed_dict=feed_dict)\n                    # for grad in grads:\n                    #     print(np.max(grad))\n\n                    if (step + 1) % 10 == 0:\n                        # Change to evaluation mode\n                        feed_dict[model.keep_prob_encoder_pl_list[0]] = 1.0\n                        feed_dict[model.keep_prob_decoder_pl_list[0]] = 1.0\n                        feed_dict[model.keep_prob_embedding_pl_list[0]] = 1.0\n\n                        # Predict class ids\n                        predicted_ids_train, predicted_ids_infer = sess.run(\n                            [decode_op_train, decode_op_infer],\n                            feed_dict=feed_dict)\n\n                        # Compute accuracy\n                        try:\n                            feed_dict_ler = {\n                                model.labels_st_true_pl: list2sparsetensor(\n                                    labels, padded_value=model.eos_index),\n                                model.labels_st_pred_pl: list2sparsetensor(\n                                    predicted_ids_infer, padded_value=model.eos_index)\n                            }\n                            ler_train = sess.run(\n                                ler_op, feed_dict=feed_dict_ler)\n                        except IndexError:\n                            ler_train = 1\n\n                        duration_step = time.time() - start_time_step\n                        print(\'Step %d: loss = %.3f / ler = %.3f (%.3f sec) / lr = %.5f\' %\n                              (step + 1, loss_train, ler_train, duration_step, learning_rate))\n                        start_time_step = time.time()\n\n                        # Visualize\n                        if label_type == \'character\':\n                            print(\'True            : %s\' %\n                                  idx2alpha(labels[0]))\n                            print(\'Pred (Training) : <%s\' %\n                                  idx2alpha(predicted_ids_train[0]))\n                            print(\'Pred (Inference): <%s\' %\n                                  idx2alpha(predicted_ids_infer[0]))\n                        else:\n                            print(\'True            : %s\' %\n                                  idx2phone(labels[0]))\n                            print(\'Pred (Training) : < %s\' %\n                                  idx2phone(predicted_ids_train[0]))\n                            print(\'Pred (Inference): < %s\' %\n                                  idx2phone(predicted_ids_infer[0]))\n\n                        if ler_train < 0.1:\n                            print(\'Model is Converged.\')\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=step,\n                            value=ler_train)\n                        feed_dict[learning_rate_pl] = learning_rate\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
models/test/test_ctc.py,9,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nimport tensorflow as tf\n# from tensorflow.python import debug as tf_debug\n\nsys.path.append(os.path.abspath(\'../../\'))\nfrom models.ctc.ctc import CTC\nfrom models.test.data import generate_data, idx2alpha\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.io.labels.sparsetensor import list2sparsetensor, sparsetensor2list\nfrom utils.parameter import count_total_parameters\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.measure_time_func import measure_time\n\n\nclass TestCTCTraining(tf.test.TestCase):\n\n    def test(self):\n        print(""CTC Working check."")\n\n        # CNN-like-CTC\n        # self.check(encoder_type=\'cnn_zhang\', label_type=\'phone\')\n        # self.check(encoder_type=\'vgg_wang\')\n        self.check(encoder_type=\'cldnn_wang\', lstm_impl=\'LSTMBlockCell\')\n\n        # BLSTM-CTC\n        self.check(encoder_type=\'blstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'blstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'blstm\', lstm_impl=\'LSTMBlockCell\')\n        self.check(encoder_type=\'blstm\', lstm_impl=\'LSTMBlockCell\',\n                   time_major=False)\n\n        # LSTM-CTC\n        self.check(encoder_type=\'lstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'lstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'lstm\', lstm_impl=\'LSTMBlockCell\')\n\n        # GRU-CTC\n        self.check(encoder_type=\'bgru\')\n        self.check(encoder_type=\'gru\')\n\n        # VGG-BLSTM-CTC\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'LSTMBlockCell\')\n\n        # VGG-LSTM-CTC\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'LSTMBlockCell\')\n\n    @measure_time\n    def check(self, encoder_type, label_type=\'character\',\n              lstm_impl=None, time_major=True, save_params=False):\n\n        print(\'==================================================\')\n        print(\'  encoder_type: %s\' % encoder_type)\n        print(\'  label_type: %s\' % label_type)\n        print(\'  lstm_impl: %s\' % lstm_impl)\n        print(\'  time_major: %s\' % str(time_major))\n        print(\'  save_params: %s\' % str(save_params))\n        print(\'==================================================\')\n\n        tf.reset_default_graph()\n        with tf.Graph().as_default():\n            # Load batch data\n            batch_size = 2\n            splice = 11 if encoder_type in [\'vgg_blstm\', \'vgg_lstm\', \'cnn_zhang\',\n                                            \'vgg_wang\', \'resnet_wang\', \'cldnn_wang\'] else 1\n            num_stack = 2\n            inputs, labels, inputs_seq_len = generate_data(\n                label_type=label_type,\n                model=\'ctc\',\n                batch_size=batch_size,\n                num_stack=num_stack,\n                splice=splice)\n            # NOTE: input_size must be even number when using CudnnLSTM\n\n            # Define model graph\n            num_classes = 27 if label_type == \'character\' else 61\n            model = CTC(encoder_type=encoder_type,\n                        input_size=inputs[0].shape[-1] // splice // num_stack,\n                        splice=splice,\n                        num_stack=num_stack,\n                        num_units=256,\n                        num_layers=2,\n                        num_classes=num_classes,\n                        lstm_impl=lstm_impl,\n                        parameter_init=0.1,\n                        clip_grad_norm=5.0,\n                        clip_activation=50,\n                        num_proj=256,\n                        weight_decay=1e-10,\n                        # bottleneck_dim=50,\n                        bottleneck_dim=None,\n                        time_major=time_major)\n\n            # Define placeholders\n            model.create_placeholders()\n            learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n            # Add to the graph each operation\n            loss_op, logits = model.compute_loss(\n                model.inputs_pl_list[0],\n                model.labels_pl_list[0],\n                model.inputs_seq_len_pl_list[0],\n                model.keep_prob_pl_list[0])\n            train_op = model.train(loss_op,\n                                   optimizer=\'nestrov\',\n                                   learning_rate=learning_rate_pl)\n            # NOTE: Adam does not run on CudnnLSTM\n            decode_op = model.decoder(logits,\n                                      model.inputs_seq_len_pl_list[0],\n                                      beam_width=20)\n            ler_op = model.compute_ler(decode_op, model.labels_pl_list[0])\n\n            # Define learning rate controller\n            learning_rate = 1e-4\n            lr_controller = Controller(learning_rate_init=learning_rate,\n                                       decay_start_epoch=50,\n                                       decay_rate=0.9,\n                                       decay_patient_epoch=10,\n                                       lower_better=True)\n\n            if save_params:\n                # Create a saver for writing training checkpoints\n                saver = tf.train.Saver(max_to_keep=None)\n\n            # Add the variable initializer operation\n            init_op = tf.global_variables_initializer()\n\n            # Count total parameters\n            if lstm_impl != \'CudnnLSTM\':\n                parameters_dict, total_parameters = count_total_parameters(\n                    tf.trainable_variables())\n                for parameter_name in sorted(parameters_dict.keys()):\n                    print(""%s %d"" %\n                          (parameter_name, parameters_dict[parameter_name]))\n                print(""Total %d variables, %s M parameters"" %\n                      (len(parameters_dict.keys()),\n                       ""{:,}"".format(total_parameters / 1000000)))\n\n            # Make feed dict\n            feed_dict = {\n                model.inputs_pl_list[0]: inputs,\n                model.labels_pl_list[0]: list2sparsetensor(labels, padded_value=-1),\n                model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n                model.keep_prob_pl_list[0]: 1.0,\n                learning_rate_pl: learning_rate\n            }\n\n            idx2phone = Idx2phone(map_file_path=\'./phone61.txt\')\n\n            with tf.Session() as sess:\n                # Initialize parameters\n                sess.run(init_op)\n\n                # Wrapper for tfdbg\n                # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n\n                # Train model\n                max_steps = 1000\n                start_time_step = time.time()\n                for step in range(max_steps):\n\n                    # for debug\n                    # encoder_outputs = sess.run(\n                    #     model.encoder_outputs, feed_dict)\n                    # print(encoder_outputs.shape)\n\n                    # Compute loss\n                    _, loss_train = sess.run(\n                        [train_op, loss_op], feed_dict=feed_dict)\n\n                    # Gradient check\n                    # grads = sess.run(model.clipped_grads,\n                    #                  feed_dict=feed_dict)\n                    # for grad in grads:\n                    #     print(np.max(grad))\n\n                    if (step + 1) % 10 == 0:\n                        # Change to evaluation mode\n                        feed_dict[model.keep_prob_pl_list[0]] = 1.0\n\n                        # Compute accuracy\n                        ler_train = sess.run(ler_op, feed_dict=feed_dict)\n\n                        duration_step = time.time() - start_time_step\n                        print(\'Step %d: loss = %.3f / ler = %.3f (%.3f sec) / lr = %.5f\' %\n                              (step + 1, loss_train, ler_train, duration_step, learning_rate))\n                        start_time_step = time.time()\n\n                        # Decode\n                        labels_pred_st = sess.run(\n                            decode_op, feed_dict=feed_dict)\n\n                        # Visualize\n                        try:\n                            labels_pred = sparsetensor2list(\n                                labels_pred_st, batch_size=batch_size)\n                            if label_type == \'character\':\n                                print(\'Ref: %s\' % idx2alpha(labels[0]))\n                                print(\'Hyp: %s\' % idx2alpha(labels_pred[0]))\n                            else:\n                                print(\'Ref: %s\' % idx2phone(labels[0]))\n                                print(\'Hyp: %s\' % idx2phone(labels_pred[0]))\n\n                        except IndexError:\n                            if label_type == \'character\':\n                                print(\'Ref: %s\' % idx2alpha(labels[0]))\n                                print(\'Hyp: %s\' % \'\')\n                            else:\n                                print(\'Ref: %s\' % idx2phone(labels[0]))\n                                print(\'Hyp: %s\' % \'\')\n                            # NOTE: This is for no prediction\n\n                        if ler_train < 0.1:\n                            print(\'Modle is Converged.\')\n                            if save_params:\n                                # Save model (check point)\n                                checkpoint_file = \'./model.ckpt\'\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=2)\n                                print(""Model saved in file: %s"" % save_path)\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=step,\n                            value=ler_train)\n                        feed_dict[learning_rate_pl] = learning_rate\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
models/test/test_ctc_decoder.py,7,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tensorflow as tf\n# from tensorflow.python import debug as tf_debug\n\nsys.path.append(os.path.abspath(\'../../\'))\nfrom models.ctc.ctc import CTC\nfrom models.test.data import generate_data, idx2alpha\nfrom utils.io.labels.sparsetensor import list2sparsetensor, sparsetensor2list\nfrom utils.evaluation.edit_distance import compute_cer\nfrom models.ctc.decoders.greedy_decoder import GreedyDecoder\nfrom models.ctc.decoders.beam_search_decoder import BeamSearchDecoder\nfrom utils.measure_time_func import measure_time\n\n\nclass TestCTCDecoder(tf.test.TestCase):\n\n    def test(self):\n        print(""CTC Working check."")\n\n        self.check(decoder_type=\'tf_greedy\')\n        self.check(decoder_type=\'tf_beam_search\')\n        self.check(decoder_type=\'np_greedy\')\n        self.check(decoder_type=\'np_beam_search\')\n\n    @measure_time\n    def check(self, decoder_type):\n\n        print(\'==================================================\')\n        print(\'  decoder_type: %s\' % decoder_type)\n        print(\'==================================================\')\n\n        tf.reset_default_graph()\n        with tf.Graph().as_default():\n            # Load batch data\n            batch_size = 2\n            num_stack = 2\n            inputs, labels, inputs_seq_len = generate_data(\n                label_type=\'character\',\n                model=\'ctc\',\n                batch_size=batch_size,\n                num_stack=num_stack,\n                splice=1)\n            max_time = inputs.shape[1]\n\n            # Define model graph\n            model = CTC(encoder_type=\'blstm\',\n                        input_size=inputs[0].shape[-1],\n                        splice=1,\n                        num_stack=num_stack,\n                        num_units=256,\n                        num_layers=2,\n                        num_classes=27,\n                        lstm_impl=\'LSTMBlockCell\',\n                        parameter_init=0.1,\n                        clip_grad_norm=5.0,\n                        clip_activation=50,\n                        num_proj=256,\n                        weight_decay=1e-6)\n\n            # Define placeholders\n            model.create_placeholders()\n\n            # Add to the graph each operation\n            _, logits = model.compute_loss(\n                model.inputs_pl_list[0],\n                model.labels_pl_list[0],\n                model.inputs_seq_len_pl_list[0],\n                model.keep_prob_pl_list[0])\n            beam_width = 20 if \'beam_search\' in decoder_type else 1\n            decode_op = model.decoder(logits,\n                                      model.inputs_seq_len_pl_list[0],\n                                      beam_width=beam_width)\n            ler_op = model.compute_ler(decode_op, model.labels_pl_list[0])\n            posteriors_op = model.posteriors(logits, blank_prior=1)\n\n            if decoder_type == \'np_greedy\':\n                decoder = GreedyDecoder(blank_index=model.num_classes)\n            elif decoder_type == \'np_beam_search\':\n                decoder = BeamSearchDecoder(space_index=26,\n                                            blank_index=model.num_classes - 1)\n\n            # Make feed dict\n            feed_dict = {\n                model.inputs_pl_list[0]: inputs,\n                model.labels_pl_list[0]: list2sparsetensor(labels,\n                                                           padded_value=-1),\n                model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n                model.keep_prob_pl_list[0]: 1.0\n            }\n\n            # Create a saver for writing training checkpoints\n            saver = tf.train.Saver()\n\n            with tf.Session() as sess:\n                ckpt = tf.train.get_checkpoint_state(\'./\')\n\n                # If check point exists\n                if ckpt:\n                    model_path = ckpt.model_checkpoint_path\n                    saver.restore(sess, model_path)\n                    print(""Model restored: "" + model_path)\n                else:\n                    raise ValueError(\'There are not any checkpoints.\')\n\n                if decoder_type in [\'tf_greedy\', \'tf_beam_search\']:\n                    # Decode\n                    labels_pred_st = sess.run(decode_op, feed_dict=feed_dict)\n                    labels_pred = sparsetensor2list(\n                        labels_pred_st, batch_size=batch_size)\n\n                    # Compute accuracy\n                    cer = sess.run(ler_op, feed_dict=feed_dict)\n                else:\n                    # Compute CTC posteriors\n                    probs = sess.run(posteriors_op, feed_dict=feed_dict)\n                    probs = probs.reshape(-1, max_time, model.num_classes)\n\n                    if decoder_type == \'np_greedy\':\n                        # Decode\n                        labels_pred = decoder(probs=probs,\n                                              seq_len=inputs_seq_len)\n\n                    elif decoder_type == \'np_beam_search\':\n                        # Decode\n                        labels_pred, scores = decoder(probs=probs,\n                                                      seq_len=inputs_seq_len,\n                                                      beam_width=beam_width)\n\n                    # Compute accuracy\n                    cer = compute_cer(str_pred=idx2alpha(labels_pred[0]),\n                                      str_true=idx2alpha(labels[0]),\n                                      normalize=True)\n\n                # Visualize\n                print(\'CER: %.3f %%\' % (cer * 100))\n                print(\'Ref: %s\' % idx2alpha(labels[0]))\n                print(\'Hyp: %s\' % idx2alpha(labels_pred[0]))\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
models/test/test_encoder.py,8,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\nimport tensorflow as tf\n\nsys.path.append(os.path.abspath(\'../../\'))\nfrom models.encoders.load_encoder import load\nfrom models.test.data import generate_data\nfrom utils.parameter import count_total_parameters\nfrom utils.measure_time_func import measure_time\n\n\nclass TestEncoder(unittest.TestCase):\n\n    def test(self):\n        print(""Encoder Working check."")\n\n        # CNNs\n        self.check(encoder_type=\'vgg_wang\', time_major=True)\n        self.check(encoder_type=\'cnn_zhang\', time_major=True)\n        self.check(encoder_type=\'cldnn_wang\', lstm_impl=\'LSTMBlockCell\',\n                   time_major=True)\n        # self.check(encoder_type=\'resnet_wang\')\n\n        ##############################\n        # time_major == True\n        ##############################\n        # BLSTM\n        self.check(encoder_type=\'blstm\', lstm_impl=\'BasicLSTMCell\',\n                   time_major=True)\n        self.check(encoder_type=\'blstm\', lstm_impl=\'LSTMCell\',\n                   time_major=True)\n        self.check(encoder_type=\'blstm\', lstm_impl=\'LSTMBlockCell\',\n                   time_major=True)\n\n        # LSTM\n        self.check(encoder_type=\'lstm\', lstm_impl=\'BasicLSTMCell\',\n                   time_major=True)\n        self.check(encoder_type=\'lstm\', lstm_impl=\'LSTMCell\',\n                   time_major=True)\n        self.check(encoder_type=\'lstm\', lstm_impl=\'LSTMBlockCell\',\n                   time_major=True)\n\n        # GRUs\n        self.check(encoder_type=\'bgru\', time_major=True)\n        self.check(encoder_type=\'gru\', time_major=True)\n\n        # VGG-BLSTM\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'BasicLSTMCell\',\n                   time_major=True)\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'LSTMCell\',\n                   time_major=True)\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'LSTMBlockCell\',\n                   time_major=True)\n\n        # VGG-LSTM\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'BasicLSTMCell\',\n                   time_major=True)\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'LSTMCell\',\n                   time_major=True)\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'LSTMBlockCell\',\n                   time_major=True)\n\n        ##############################\n        # time_major == False\n        ##############################\n        # BLSTM\n        self.check(encoder_type=\'blstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'blstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'blstm\', lstm_impl=\'LSTMBlockCell\')\n\n        # LSTM\n        self.check(encoder_type=\'lstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'lstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'lstm\', lstm_impl=\'LSTMBlockCell\')\n\n        # GRUs\n        self.check(encoder_type=\'bgru\')\n        self.check(encoder_type=\'gru\')\n\n        # VGG-BLSTM\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'vgg_blstm\', lstm_impl=\'LSTMBlockCell\')\n\n        # VGG-LSTM\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'vgg_lstm\', lstm_impl=\'LSTMBlockCell\')\n\n        # Multi-task BLSTM\n        self.check(encoder_type=\'multitask_blstm\',\n                   lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'multitask_blstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'multitask_blstm\',\n                   lstm_impl=\'LSTMBlockCell\')\n\n        # Multi-task LSTM\n        self.check(encoder_type=\'multitask_lstm\',\n                   lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'multitask_lstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'multitask_lstm\',\n                   lstm_impl=\'LSTMBlockCell\')\n\n        # Dynamic\n        # self.check(encoder_type=\'pyramid_blstm\')\n        # NOTE: this is under implementation\n\n    @measure_time\n    def check(self, encoder_type, lstm_impl=None, time_major=False):\n\n        print(\'==================================================\')\n        print(\'  encoder_type: %s\' % encoder_type)\n        print(\'  lstm_impl: %s\' % lstm_impl)\n        print(\'  time_major: %s\' % time_major)\n        print(\'==================================================\')\n\n        tf.reset_default_graph()\n        with tf.Graph().as_default():\n            # Load batch data\n            batch_size = 4\n            splice = 5 if encoder_type in [\'vgg_blstm\', \'vgg_lstm\',\n                                           \'vgg_wang\', \'resnet_wang\', \'cldnn_wang\',\n                                           \'cnn_zhang\'] else 1\n            num_stack = 2\n            inputs, _, inputs_seq_len = generate_data(\n                label_type=\'character\',\n                model=\'ctc\',\n                batch_size=batch_size,\n                num_stack=num_stack,\n                splice=splice)\n            frame_num, input_size = inputs[0].shape\n\n            # Define model graph\n            if encoder_type in [\'blstm\', \'lstm\']:\n                encoder = load(encoder_type)(\n                    num_units=256,\n                    num_proj=None,\n                    num_layers=5,\n                    lstm_impl=lstm_impl,\n                    use_peephole=True,\n                    parameter_init=0.1,\n                    clip_activation=5,\n                    time_major=time_major)\n            elif encoder_type in [\'bgru\', \'gru\']:\n                encoder = load(encoder_type)(\n                    num_units=256,\n                    num_layers=5,\n                    parameter_init=0.1,\n                    time_major=time_major)\n            elif encoder_type in [\'vgg_blstm\', \'vgg_lstm\', \'cldnn_wang\']:\n                encoder = load(encoder_type)(\n                    input_size=input_size // splice // num_stack,\n                    splice=splice,\n                    num_stack=num_stack,\n                    num_units=256,\n                    num_proj=None,\n                    num_layers=5,\n                    lstm_impl=lstm_impl,\n                    use_peephole=True,\n                    parameter_init=0.1,\n                    clip_activation=5,\n                    time_major=time_major)\n            elif encoder_type in [\'multitask_blstm\', \'multitask_lstm\']:\n                encoder = load(encoder_type)(\n                    num_units=256,\n                    num_proj=None,\n                    num_layers_main=5,\n                    num_layers_sub=3,\n                    lstm_impl=lstm_impl,\n                    use_peephole=True,\n                    parameter_init=0.1,\n                    clip_activation=5,\n                    time_major=time_major)\n            elif encoder_type in [\'vgg_wang\', \'resnet_wang\', \'cnn_zhang\']:\n                encoder = load(encoder_type)(\n                    input_size=input_size // splice // num_stack,\n                    splice=splice,\n                    num_stack=num_stack,\n                    parameter_init=0.1,\n                    time_major=time_major)\n                # NOTE: topology is pre-defined\n            else:\n                raise NotImplementedError\n\n            # Create placeholders\n            inputs_pl = tf.placeholder(tf.float32,\n                                       shape=[None, None, input_size],\n                                       name=\'inputs\')\n            inputs_seq_len_pl = tf.placeholder(tf.int32,\n                                               shape=[None],\n                                               name=\'inputs_seq_len\')\n            keep_prob_pl = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n            # operation for forward computation\n            if encoder_type in [\'multitask_blstm\', \'multitask_lstm\']:\n                hidden_states_op, final_state_op, hidden_states_sub_op, final_state_sub_op = encoder(\n                    inputs=inputs_pl,\n                    inputs_seq_len=inputs_seq_len_pl,\n                    keep_prob=keep_prob_pl,\n                    is_training=True)\n            else:\n                hidden_states_op, final_state_op = encoder(\n                    inputs=inputs_pl,\n                    inputs_seq_len=inputs_seq_len_pl,\n                    keep_prob=keep_prob_pl,\n                    is_training=True)\n\n            # Add the variable initializer operation\n            init_op = tf.global_variables_initializer()\n\n            # Count total parameters\n            parameters_dict, total_parameters = count_total_parameters(\n                tf.trainable_variables())\n            for parameter_name in sorted(parameters_dict.keys()):\n                print(""%s %d"" %\n                      (parameter_name, parameters_dict[parameter_name]))\n            print(""Total %d variables, %s M parameters"" %\n                  (len(parameters_dict.keys()),\n                   ""{:,}"".format(total_parameters / 1000000)))\n\n            # Make feed dict\n            feed_dict = {\n                inputs_pl: inputs,\n                inputs_seq_len_pl: inputs_seq_len,\n                keep_prob_pl: 0.9\n            }\n\n            with tf.Session() as sess:\n                # Initialize parameters\n                sess.run(init_op)\n\n                # Make prediction\n                if encoder_type in [\'multitask_blstm\', \'multitask_lstm\']:\n                    encoder_outputs, final_state, hidden_states_sub, final_state_sub = sess.run(\n                        [hidden_states_op, final_state_op,\n                         hidden_states_sub_op, final_state_sub_op],\n                        feed_dict=feed_dict)\n                elif encoder_type in [\'vgg_wang\', \'resnet_wang\', \'cnn_zhang\']:\n                    encoder_outputs = sess.run(\n                        hidden_states_op, feed_dict=feed_dict)\n                else:\n                    encoder_outputs, final_state = sess.run(\n                        [hidden_states_op, final_state_op],\n                        feed_dict=feed_dict)\n\n                # Convert always to batch-major\n                if time_major:\n                    encoder_outputs = encoder_outputs.transpose(1, 0, 2)\n\n                if encoder_type in [\'blstm\', \'bgru\', \'vgg_blstm\', \'multitask_blstm\', \'cldnn_wang\']:\n                    if encoder_type != \'cldnn_wang\':\n                        self.assertEqual(\n                            (batch_size, frame_num, encoder.num_units * 2), encoder_outputs.shape)\n\n                    if encoder_type != \'bgru\':\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[0].c.shape)\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[0].h.shape)\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[1].c.shape)\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[1].h.shape)\n\n                        if encoder_type == \'multitask_blstm\':\n                            self.assertEqual(\n                                (batch_size, frame_num, encoder.num_units * 2), hidden_states_sub.shape)\n                            self.assertEqual(\n                                (batch_size, encoder.num_units), final_state_sub[0].c.shape)\n                            self.assertEqual(\n                                (batch_size, encoder.num_units), final_state_sub[0].h.shape)\n                            self.assertEqual(\n                                (batch_size, encoder.num_units), final_state_sub[1].c.shape)\n                            self.assertEqual(\n                                (batch_size, encoder.num_units), final_state_sub[1].h.shape)\n                    else:\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[0].shape)\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[1].shape)\n\n                elif encoder_type in [\'lstm\', \'gru\', \'vgg_lstm\', \'multitask_lstm\']:\n                    self.assertEqual(\n                        (batch_size, frame_num, encoder.num_units), encoder_outputs.shape)\n\n                    if encoder_type != \'gru\':\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[0].c.shape)\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[0].h.shape)\n\n                        if encoder_type == \'multitask_lstm\':\n                            self.assertEqual(\n                                (batch_size, frame_num, encoder.num_units), hidden_states_sub.shape)\n                            self.assertEqual(\n                                (batch_size, encoder.num_units), final_state_sub[0].c.shape)\n                            self.assertEqual(\n                                (batch_size, encoder.num_units), final_state_sub[0].h.shape)\n                    else:\n                        self.assertEqual(\n                            (batch_size, encoder.num_units), final_state[0].shape)\n\n                elif encoder_type in [\'vgg_wang\', \'resnet_wang\', \'cnn_zhang\']:\n                    self.assertEqual(3, len(encoder_outputs.shape))\n                    self.assertEqual(\n                        (batch_size, frame_num), encoder_outputs.shape[:2])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
models/test/test_joint_ctc_attention.py,8,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nimport tensorflow as tf\n# from tensorflow.python import debug as tf_debug\n\nsys.path.append(os.path.abspath(\'../../\'))\nfrom models.attention.joint_ctc_attention import JointCTCAttention\nfrom models.test.data import generate_data, idx2alpha\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.parameter import count_total_parameters\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.measure_time_func import measure_time\n\n\nclass TestAttention(tf.test.TestCase):\n\n    def test(self):\n        print(""Joint CTC-Attention Working check."")\n\n        # self.check(label_type=\'phone\')\n        self.check(label_type=\'character\')\n\n    @measure_time\n    def check(self, label_type=\'phone\'):\n\n        print(\'==================================================\')\n        print(\'  label_type: %s\' % label_type)\n        print(\'==================================================\')\n\n        tf.reset_default_graph()\n        with tf.Graph().as_default():\n            # Load batch data\n            batch_size = 4\n            inputs, labels, ctc_labels, inputs_seq_len, labels_seq_len = generate_data(\n                label_type=label_type,\n                model=\'joint_ctc_attention\',\n                batch_size=batch_size)\n\n            # Define model graph\n            num_classes = 27 if label_type == \'character\' else 61\n            model = JointCTCAttention(input_size=inputs[0].shape[1],\n                                      encoder_type=\'blstm\',\n                                      encoder_num_units=256,\n                                      encoder_num_layers=2,\n                                      encoder_num_proj=None,\n                                      attention_type=\'dot_product\',\n                                      attention_dim=128,\n                                      decoder_type=\'lstm\',\n                                      decoder_num_units=256,\n                                      decoder_num_layers=1,\n                                      embedding_dim=64,\n                                      lambda_weight=0.5,\n                                      num_classes=num_classes,\n                                      sos_index=num_classes,\n                                      eos_index=num_classes + 1,\n                                      max_decode_length=100,\n                                      use_peephole=True,\n                                      splice=1,\n                                      parameter_init=0.1,\n                                      clip_grad_norm=5.0,\n                                      clip_activation_encoder=50,\n                                      clip_activation_decoder=50,\n                                      weight_decay=1e-8,\n                                      time_major=True,\n                                      sharpening_factor=1.0,\n                                      logits_temperature=1.0)\n\n            # Define placeholders\n            model.create_placeholders()\n            learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n            # Add to the graph each operation\n            loss_op, logits, ctc_logits, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n                model.inputs_pl_list[0],\n                model.labels_pl_list[0],\n                model.ctc_labels_pl_list[0],\n                model.inputs_seq_len_pl_list[0],\n                model.labels_seq_len_pl_list[0],\n                model.keep_prob_encoder_pl_list[0],\n                model.keep_prob_decoder_pl_list[0],\n                model.keep_prob_embedding_pl_list[0])\n            train_op = model.train(loss_op,\n                                   optimizer=\'adam\',\n                                   learning_rate=learning_rate_pl)\n            decode_op_train, decode_op_infer = model.decode(\n                decoder_outputs_train,\n                decoder_outputs_infer)\n            ler_op = model.compute_ler(model.labels_st_true_pl,\n                                       model.labels_st_pred_pl)\n\n            # Define learning rate controller\n            learning_rate = 1e-3\n            lr_controller = Controller(learning_rate_init=learning_rate,\n                                       decay_start_epoch=20,\n                                       decay_rate=0.9,\n                                       decay_patient_epoch=10,\n                                       lower_better=True)\n\n            # Add the variable initializer operation\n            init_op = tf.global_variables_initializer()\n\n            # Count total parameters\n            parameters_dict, total_parameters = count_total_parameters(\n                tf.trainable_variables())\n            for parameter_name in sorted(parameters_dict.keys()):\n                print(""%s %d"" %\n                      (parameter_name, parameters_dict[parameter_name]))\n            print(""Total %d variables, %s M parameters"" %\n                  (len(parameters_dict.keys()),\n                   ""{:,}"".format(total_parameters / 1000000)))\n\n            # Make feed dict\n            feed_dict = {\n                model.inputs_pl_list[0]: inputs,\n                model.labels_pl_list[0]: labels,\n                model.ctc_labels_pl_list[0]: list2sparsetensor(ctc_labels, padded_value=-1),\n                model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n                model.labels_seq_len_pl_list[0]: labels_seq_len,\n                model.keep_prob_encoder_pl_list[0]: 0.8,\n                model.keep_prob_decoder_pl_list[0]: 1.0,\n                model.keep_prob_embedding_pl_list[0]: 1.0,\n                learning_rate_pl: learning_rate\n            }\n\n            idx2phone = Idx2phone(map_file_path=\'./phone61.txt\')\n\n            with tf.Session() as sess:\n                # Initialize parameters\n                sess.run(init_op)\n\n                # Wrapper for tfdbg\n                # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n\n                # Train model\n                max_steps = 1000\n                start_time_step = time.time()\n                for step in range(max_steps):\n\n                    # Compute loss\n                    _, loss_train = sess.run(\n                        [train_op, loss_op], feed_dict=feed_dict)\n\n                    # Gradient check\n                    # grads = sess.run(model.clipped_grads,\n                    #                  feed_dict=feed_dict)\n                    # for grad in grads:\n                    #     print(np.max(grad))\n\n                    if (step + 1) % 10 == 0:\n                        # Change to evaluation mode\n                        feed_dict[model.keep_prob_encoder_pl_list[0]] = 1.0\n                        feed_dict[model.keep_prob_decoder_pl_list[0]] = 1.0\n                        feed_dict[model.keep_prob_embedding_pl_list[0]] = 1.0\n\n                        # Predict class ids\n                        predicted_ids_train, predicted_ids_infer = sess.run(\n                            [decode_op_train, decode_op_infer],\n                            feed_dict=feed_dict)\n\n                        # Compute accuracy\n                        try:\n                            feed_dict_ler = {\n                                model.labels_st_true_pl: list2sparsetensor(\n                                    labels, padded_value=model.eos_index),\n                                model.labels_st_pred_pl: list2sparsetensor(\n                                    predicted_ids_infer, padded_value=model.eos_index)\n                            }\n                            ler_train = sess.run(\n                                ler_op, feed_dict=feed_dict_ler)\n                        except IndexError:\n                            ler_train = 1\n\n                        duration_step = time.time() - start_time_step\n                        print(\'Step %d: loss = %.3f / ler = %.4f (%.3f sec) / lr = %.5f\' %\n                              (step + 1, loss_train, ler_train, duration_step, learning_rate))\n                        start_time_step = time.time()\n\n                        # Visualize\n                        if label_type == \'character\':\n                            print(\'True            : %s\' %\n                                  idx2alpha(labels[0]))\n                            print(\'Pred (Training) : <%s\' %\n                                  idx2alpha(predicted_ids_train[0]))\n                            print(\'Pred (Inference): <%s\' %\n                                  idx2alpha(predicted_ids_infer[0]))\n                        else:\n                            print(\'True            : %s\' %\n                                  idx2phone(labels[0]))\n                            print(\'Pred (Training) : < %s\' %\n                                  idx2phone(predicted_ids_train[0]))\n                            print(\'Pred (Inference): < %s\' %\n                                  idx2phone(predicted_ids_infer[0]))\n\n                        if ler_train < 0.1:\n                            print(\'Model is Converged.\')\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=step,\n                            value=ler_train)\n                        feed_dict[learning_rate_pl] = learning_rate\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
models/test/test_multitask_ctc.py,8,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nimport tensorflow as tf\n# from tensorflow.python import debug as tf_debug\n\nsys.path.append(os.path.abspath(\'../../\'))\nfrom models.ctc.multitask_ctc import MultitaskCTC\nfrom models.test.data import generate_data, idx2alpha\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.io.labels.sparsetensor import list2sparsetensor, sparsetensor2list\nfrom utils.parameter import count_total_parameters\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.measure_time_func import measure_time\n\n\nclass TestMultitaskCTC(tf.test.TestCase):\n\n    def test(self):\n        print(""Multitask CTC Working check."")\n\n        # BLSTM\n        self.check(encoder_type=\'multitask_blstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'multitask_blstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'multitask_blstm\', lstm_impl=\'LSTMBlockCell\')\n        self.check(encoder_type=\'multitask_blstm\', lstm_impl=\'LSTMBlockCell\',\n                   time_major=True)\n\n        # LSTM\n        self.check(encoder_type=\'multitask_lstm\', lstm_impl=\'BasicLSTMCell\')\n        self.check(encoder_type=\'multitask_lstm\', lstm_impl=\'LSTMCell\')\n        self.check(encoder_type=\'multitask_lstm\', lstm_impl=\'LSTMBlockCell\')\n\n    @measure_time\n    def check(self, encoder_type, lstm_impl, time_major=False):\n\n        print(\'==================================================\')\n        print(\'  encoder_type: %s\' % str(encoder_type))\n        print(\'  lstm_impl: %s\' % str(lstm_impl))\n        print(\'  time_major: %s\' % str(time_major))\n        print(\'==================================================\')\n\n        tf.reset_default_graph()\n        with tf.Graph().as_default():\n            # Load batch data\n            batch_size = 2\n            inputs, labels_char, labels_phone, inputs_seq_len = generate_data(\n                label_type=\'multitask\',\n                model=\'ctc\',\n                batch_size=batch_size)\n\n            # Define model graph\n            num_classes_main = 27\n            num_classes_sub = 61\n            model = MultitaskCTC(\n                encoder_type=encoder_type,\n                input_size=inputs[0].shape[1],\n                num_units=256,\n                num_layers_main=2,\n                num_layers_sub=1,\n                num_classes_main=num_classes_main,\n                num_classes_sub=num_classes_sub,\n                main_task_weight=0.8,\n                lstm_impl=lstm_impl,\n                parameter_init=0.1,\n                clip_grad_norm=5.0,\n                clip_activation=50,\n                num_proj=256,\n                weight_decay=1e-8,\n                # bottleneck_dim=50,\n                bottleneck_dim=None,\n                time_major=time_major)\n\n            # Define placeholders\n            model.create_placeholders()\n            learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n            # Add to the graph each operation\n            loss_op, logits_main, logits_sub = model.compute_loss(\n                model.inputs_pl_list[0],\n                model.labels_pl_list[0],\n                model.labels_sub_pl_list[0],\n                model.inputs_seq_len_pl_list[0],\n                model.keep_prob_pl_list[0])\n            train_op = model.train(\n                loss_op,\n                optimizer=\'adam\',\n                learning_rate=learning_rate_pl)\n            decode_op_main, decode_op_sub = model.decoder(\n                logits_main,\n                logits_sub,\n                model.inputs_seq_len_pl_list[0],\n                beam_width=20)\n            ler_op_main, ler_op_sub = model.compute_ler(\n                decode_op_main, decode_op_sub,\n                model.labels_pl_list[0], model.labels_sub_pl_list[0])\n\n            # Define learning rate controller\n            learning_rate = 1e-3\n            lr_controller = Controller(learning_rate_init=learning_rate,\n                                       decay_start_epoch=20,\n                                       decay_rate=0.9,\n                                       decay_patient_epoch=5,\n                                       lower_better=True)\n\n            # Add the variable initializer operation\n            init_op = tf.global_variables_initializer()\n\n            # Count total parameters\n            parameters_dict, total_parameters = count_total_parameters(\n                tf.trainable_variables())\n            for parameter_name in sorted(parameters_dict.keys()):\n                print(""%s %d"" %\n                      (parameter_name, parameters_dict[parameter_name]))\n            print(""Total %d variables, %s M parameters"" %\n                  (len(parameters_dict.keys()),\n                   ""{:,}"".format(total_parameters / 1000000)))\n\n            # Make feed dict\n            feed_dict = {\n                model.inputs_pl_list[0]: inputs,\n                model.labels_pl_list[0]: list2sparsetensor(labels_char, padded_value=-1),\n                model.labels_sub_pl_list[0]: list2sparsetensor(labels_phone, padded_value=-1),\n                model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n                model.keep_prob_pl_list[0]: 0.9,\n                learning_rate_pl: learning_rate\n            }\n\n            idx2phone = Idx2phone(map_file_path=\'./phone61.txt\')\n\n            with tf.Session() as sess:\n                # Initialize parameters\n                sess.run(init_op)\n\n                # Wrapper for tfdbg\n                # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n\n                # Train model\n                max_steps = 1000\n                start_time_step = time.time()\n                for step in range(max_steps):\n\n                    # Compute loss\n                    _, loss_train = sess.run(\n                        [train_op, loss_op], feed_dict=feed_dict)\n\n                    # Gradient check\n                    # grads = sess.run(model.clipped_grads,\n                    #                  feed_dict=feed_dict)\n                    # for grad in grads:\n                    #     print(np.max(grad))\n\n                    if (step + 1) % 10 == 0:\n                        # Change to evaluation mode\n                        feed_dict[model.keep_prob_pl_list[0]] = 1.0\n\n                        # Compute accuracy\n                        ler_train_char, ler_train_phone = sess.run(\n                            [ler_op_main, ler_op_sub], feed_dict=feed_dict)\n\n                        duration_step = time.time() - start_time_step\n                        print(\'Step %d: loss = %.3f / cer = %.3f / per = %.3f (%.3f sec) / lr = %.5f\' %\n                              (step + 1, loss_train, ler_train_char,\n                               ler_train_phone, duration_step, learning_rate))\n                        start_time_step = time.time()\n\n                        # Visualize\n                        labels_pred_char_st, labels_pred_phone_st = sess.run(\n                            [decode_op_main, decode_op_sub],\n                            feed_dict=feed_dict)\n                        labels_pred_char = sparsetensor2list(\n                            labels_pred_char_st, batch_size=batch_size)\n                        labels_pred_phone = sparsetensor2list(\n                            labels_pred_phone_st, batch_size=batch_size)\n\n                        print(\'Character\')\n                        try:\n                            print(\'  Ref: %s\' % idx2alpha(labels_char[0]))\n                            print(\'  Hyp: %s\' % idx2alpha(labels_pred_char[0]))\n                        except IndexError:\n                            print(\'Character\')\n                            print(\'  Ref: %s\' % idx2alpha(labels_char[0]))\n                            print(\'  Hyp: %s\' % \'\')\n\n                        print(\'Phone\')\n                        try:\n                            print(\'  Ref: %s\' % idx2phone(labels_phone[0]))\n                            print(\'  Hyp: %s\' %\n                                  idx2phone(labels_pred_phone[0]))\n                        except IndexError:\n                            print(\'  Ref: %s\' % idx2phone(labels_phone[0]))\n                            print(\'  Hyp: %s\' % \'\')\n                            # NOTE: This is for no prediction\n                        print(\'-\' * 30)\n\n                        if ler_train_char < 0.1:\n                            print(\'Modle is Converged.\')\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=step,\n                            value=ler_train_char)\n                        feed_dict[learning_rate_pl] = learning_rate\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
utils/dataset/__init__.py,0,b''
utils/dataset/attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Base class for loading dataset for the Attention model.\n   In this class, all data will be loaded at each step.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import basename\nimport random\nimport numpy as np\n\nfrom utils.dataset.base import Base\nfrom utils.io.inputs.frame_stacking import stack_frame\nfrom utils.io.inputs.splicing import do_splice\n\n\nclass DatasetBase(Base):\n\n    def __init__(self, *args, **kwargs):\n        super(DatasetBase, self).__init__(*args, **kwargs)\n\n    def __getitem__(self, index):\n        input_i = np.array(self.input_paths[index])\n        label_i = np.array(self.label_paths[index])\n        return (input_i, label_i)\n\n    def __next__(self, batch_size=None):\n        """"""Generate each mini-batch.\n        Args:\n            batch_size (int, optional): the size of mini-batch\n        Returns:\n            A tuple of `(inputs, labels, inputs_seq_len, labels_seq_len, input_names)`\n                inputs: list of input data of size\n                    `[num_gpu, B, T_in, input_size]`\n                labels: list of target labels of size\n                    `[num_gpu, B, T_out]`\n                inputs_seq_len: list of length of inputs of size\n                    `[num_gpu, B]`\n                labels_seq_len: list of length of target labels of size\n                    `[num_gpu, B]`\n                input_names: list of file name of input data of size\n                    `[num_gpu, B]`\n            is_new_epoch (bool): If true, 1 epoch is finished\n        """"""\n        if self.max_epoch is not None and self.epoch >= self.max_epoch:\n            raise StopIteration\n        # NOTE: max_epoch = None means infinite loop\n\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        # reset\n        if self.is_new_epoch:\n            self.is_new_epoch = False\n\n        if not self.is_test:\n            self.padded_value = self.eos_index\n        else:\n            self.padded_value = None\n        # TODO(hirofumi): move this\n\n        if self.sort_utt:\n            # Sort all uttrances by length\n            if len(self.rest) > batch_size:\n                data_indices = sorted(list(self.rest))[:batch_size]\n                self.rest -= set(data_indices)\n                # NOTE: rest is uttrance length order\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n                if self.epoch == self.sort_stop_epoch:\n                    self.sort_utt = False\n                    self.shuffle = True\n\n            # Shuffle data in the mini-batch\n            random.shuffle(data_indices)\n\n        elif self.shuffle:\n            # Randomly sample uttrances\n            if len(self.rest) > batch_size:\n                data_indices = random.sample(list(self.rest), batch_size)\n                self.rest -= set(data_indices)\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n\n                # Shuffle selected mini-batch\n                random.shuffle(data_indices)\n\n        else:\n            if len(self.rest) > batch_size:\n                data_indices = sorted(list(self.rest))[:batch_size]\n                self.rest -= set(data_indices)\n                # NOTE: rest is in name order\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n\n        # Load dataset in mini-batch\n        input_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.input_paths, data_indices, axis=0))))\n        label_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.label_paths, data_indices, axis=0))))\n\n        if not hasattr(self, \'input_size\'):\n            self.input_size = input_list[0].shape[1]\n            if self.num_stack is not None and self.num_skip is not None:\n                self.input_size *= self.num_stack\n\n        # Frame stacking\n        input_list = stack_frame(input_list,\n                                 self.num_stack,\n                                 self.num_skip,\n                                 progressbar=False)\n\n        # Compute max frame num in mini-batch\n        max_frame_num = max(map(lambda x: x.shape[0], input_list))\n\n        # Compute max target label length in mini-batch\n        max_seq_len = max(map(len, label_list)) + 2\n        # NOTE: + <SOS> and <EOS>\n\n        # Initialization\n        inputs = np.zeros(\n            (len(data_indices), max_frame_num, self.input_size * self.splice),\n            dtype=np.float32)\n        labels = np.array(\n            [[self.padded_value] * max_seq_len] * len(data_indices))\n        inputs_seq_len = np.zeros((len(data_indices),), dtype=np.int32)\n        labels_seq_len = np.zeros((len(data_indices),), dtype=np.int32)\n        input_names = list(\n            map(lambda path: basename(path).split(\'.\')[0],\n                np.take(self.input_paths, data_indices, axis=0)))\n\n        # Set values of each data in mini-batch\n        for i_batch in range(len(data_indices)):\n            data_i = input_list[i_batch]\n            frame_num, input_size = data_i.shape\n\n            # Splicing\n            data_i = data_i.reshape(1, frame_num, input_size)\n            data_i = do_splice(data_i,\n                               splice=self.splice,\n                               batch_size=1,\n                               num_stack=self.num_stack)\n            data_i = data_i.reshape(frame_num, -1)\n\n            inputs[i_batch, : frame_num, :] = data_i\n            if self.is_test:\n                labels[i_batch, 0] = label_list[i_batch]\n                # NOTE: transcript is saved as string\n            else:\n                labels[i_batch, 0] = self.sos_index\n                labels[i_batch, 1:len(label_list[i_batch]) +\n                       1] = label_list[i_batch]\n                labels[i_batch, len(label_list[i_batch]) + 1] = self.eos_index\n            inputs_seq_len[i_batch] = frame_num\n            labels_seq_len[i_batch] = len(label_list[i_batch]) + 2\n            # TODO: +2 ??\n\n        ###############\n        # Multi-GPUs\n        ###############\n        if self.num_gpu > 1:\n            # Now we split the mini-batch data by num_gpu\n            inputs = np.array_split(inputs, self.num_gpu, axis=0)\n            labels = np.array_split(labels, self.num_gpu, axis=0)\n            inputs_seq_len = np.array_split(\n                inputs_seq_len, self.num_gpu, axis=0)\n            labels_seq_len = np.array_split(\n                labels_seq_len, self.num_gpu, axis=0)\n            input_names = np.array_split(input_names, self.num_gpu, axis=0)\n        else:\n            inputs = inputs[np.newaxis, :, :, :]\n            labels = labels[np.newaxis, :, :]\n            inputs_seq_len = inputs_seq_len[np.newaxis, :]\n            labels_seq_len = labels_seq_len[np.newaxis, :]\n            input_names = np.array(input_names)[np.newaxis, :]\n\n        self.iteration += len(data_indices)\n\n        # Clean up\n        del input_list\n        del label_list\n\n        return (inputs, labels, inputs_seq_len, labels_seq_len,\n                input_names), self.is_new_epoch\n'"
utils/dataset/base.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Base class for all dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Base(object):\n\n    def __init__(self, *args, **kwargs):\n\n        self.epoch = 0\n        self.iteration = 0\n        self.is_new_epoch = False\n\n        self.map_dict = {}\n        if \'map_file_path\' in kwargs.keys():\n            # Read the mapping file\n            with open(kwargs[\'map_file_path\'], \'r\') as f:\n                for line in f:\n                    line = line.strip().split()\n                    self.map_dict[line[0]] = int(line[1])\n\n    def __len__(self):\n        return len(self.input_paths)\n\n    def __getitem__(self, index):\n        return (self.input_list[index], self.label_list[index])\n\n    def __iter__(self):\n        """"""Returns self.""""""\n        return self\n\n    @property\n    def sos_index(self):\n        return self.map_dict[\'<\']\n\n    @property\n    def eos_index(self):\n        return self.map_dict[\'>\']\n\n    def next(self, batch_size=None):\n        # For python2\n        return self.__next__(batch_size)\n\n    def reset(self):\n        """"""Reset data counter. This is useful when you\'d like to evaluate\n        overall data during training.\n        """"""\n        self.rest = set(range(0, len(self), 1))\n\n    @property\n    def epoch_detail(self):\n        # Floating point version of epoch.\n        return self.iteration / len(self)\n\n    def __next__(self):\n        raise NotImplementedError\n'"
utils/dataset/ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Base class for loading dataset for the CTC model.\n   In this class, all data will be loaded at each step.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import basename\nimport random\nimport numpy as np\n\nfrom utils.dataset.base import Base\nfrom utils.io.inputs.frame_stacking import stack_frame\nfrom utils.io.inputs.splicing import do_splice\n\n\nclass DatasetBase(Base):\n\n    def __init__(self, *args, **kwargs):\n        super(DatasetBase, self).__init__(*args, **kwargs)\n\n    def __getitem__(self, index):\n        input_i = np.array(self.input_paths[index])\n        label_i = np.array(self.label_paths[index])\n        return (input_i, label_i)\n\n    def __next__(self, batch_size=None):\n        """"""Generate each mini-batch.\n        Args:\n            batch_size (int, optional): the size of mini-batch\n        Returns:\n            A tuple of `(inputs, labels, inputs_seq_len, input_names)`\n                inputs: list of input data of size\n                    `[num_gpu, B, T_in, input_size]`\n                labels: list of target labels of size\n                    `[num_gpu, B, T_out]`\n                inputs_seq_len: list of length of inputs of size\n                    `[num_gpu, B]`\n                input_names: list of file name of input data of size\n                    `[num_gpu, B]`\n            is_new_epoch (bool): If true, 1 epoch is finished\n        """"""\n        if self.max_epoch is not None and self.epoch >= self.max_epoch:\n            raise StopIteration\n        # NOTE: max_epoch = None means infinite loop\n\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        # reset\n        if self.is_new_epoch:\n            self.is_new_epoch = False\n\n        if not self.is_test:\n            self.padded_value = -1\n        else:\n            self.padded_value = None\n        # TODO(hirofumi): move this\n\n        if self.sort_utt:\n            # Sort all uttrances by length\n            if len(self.rest) > batch_size:\n                data_indices = sorted(list(self.rest))[:batch_size]\n                self.rest -= set(data_indices)\n                # NOTE: rest is uttrance length order\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n                if self.epoch == self.sort_stop_epoch:\n                    self.sort_utt = False\n                    self.shuffle = True\n\n            # Shuffle data in the mini-batch\n            random.shuffle(data_indices)\n\n        elif self.shuffle:\n            # Randomly sample uttrances\n            if len(self.rest) > batch_size:\n                data_indices = random.sample(list(self.rest), batch_size)\n                self.rest -= set(data_indices)\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n\n                # Shuffle selected mini-batch\n                random.shuffle(data_indices)\n\n        else:\n            if len(self.rest) > batch_size:\n                data_indices = sorted(list(self.rest))[:batch_size]\n                self.rest -= set(data_indices)\n                # NOTE: rest is in name order\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n\n        # Load dataset in mini-batch\n        input_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.input_paths, data_indices, axis=0))))\n        label_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.label_paths, data_indices, axis=0))))\n\n        if not hasattr(self, \'input_size\'):\n            self.input_size = input_list[0].shape[1]\n            if self.num_stack is not None and self.num_skip is not None:\n                self.input_size *= self.num_stack\n\n        # Frame stacking\n        input_list = stack_frame(input_list,\n                                 self.num_stack,\n                                 self.num_skip,\n                                 progressbar=False)\n\n        # Compute max frame num in mini-batch\n        max_frame_num = max(map(lambda x: x.shape[0], input_list))\n\n        # Compute max target label length in mini-batch\n        max_seq_len = max(map(len, label_list))\n\n        # Initialization\n        inputs = np.zeros(\n            (len(data_indices), max_frame_num, self.input_size * self.splice),\n            dtype=np.float32)\n        labels = np.array(\n            [[self.padded_value] * max_seq_len] * len(data_indices))\n        inputs_seq_len = np.zeros((len(data_indices),), dtype=np.int32)\n        input_names = list(\n            map(lambda path: basename(path).split(\'.\')[0],\n                np.take(self.input_paths, data_indices, axis=0)))\n\n        # Set values of each data in mini-batch\n        for i_batch in range(len(data_indices)):\n            data_i = input_list[i_batch]\n            frame_num, input_size = data_i.shape\n\n            # Splicing\n            data_i = data_i.reshape(1, frame_num, input_size)\n            data_i = do_splice(data_i,\n                               splice=self.splice,\n                               batch_size=1,\n                               num_stack=self.num_stack)\n            data_i = data_i.reshape(frame_num, -1)\n\n            inputs[i_batch, :frame_num, :] = data_i\n            if self.is_test:\n                labels[i_batch, 0] = label_list[i_batch]\n            else:\n                labels[i_batch, :len(label_list[i_batch])\n                       ] = label_list[i_batch]\n            inputs_seq_len[i_batch] = frame_num\n\n        ###############\n        # Multi-GPUs\n        ###############\n        if self.num_gpu > 1:\n            # Now we split the mini-batch data by num_gpu\n            inputs = np.array_split(inputs, self.num_gpu, axis=0)\n            labels = np.array_split(labels, self.num_gpu, axis=0)\n            inputs_seq_len = np.array_split(\n                inputs_seq_len, self.num_gpu, axis=0)\n            input_names = np.array_split(input_names, self.num_gpu, axis=0)\n        else:\n            inputs = inputs[np.newaxis, :, :, :]\n            labels = labels[np.newaxis, :, :]\n            inputs_seq_len = inputs_seq_len[np.newaxis, :]\n            input_names = np.array(input_names)[np.newaxis, :]\n\n        self.iteration += len(data_indices)\n\n        # Clean up\n        del input_list\n        del label_list\n\n        return (inputs, labels, inputs_seq_len, input_names), self.is_new_epoch\n'"
utils/dataset/joint_ctc_attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Base class for laoding dataset for the Jont CTC-Attention model.\n   In this class, all data will be loaded at once.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import basename\nimport random\nimport numpy as np\n\nfrom utils.dataset.base import Base\nfrom utils.io.inputs.frame_stacking import stack_frame\nfrom utils.io.inputs.splicing import do_splice\n\n\nclass DatasetBase(Base):\n\n    def __init__(self, *args, **kwargs):\n        super(DatasetBase, self).__init__(*args, **kwargs)\n\n    def __next__(self, batch_size=None):\n        """"""Generate each mini-batch.\n        Args:\n            batch_size (int, optional): the size of mini-batch\n        Returns:\n            A tuple of `(inputs, labels, inputs_seq_len, labels_seq_len, input_names)`\n                inputs: list of input data of size\n                    `[num_gpu, B, T_int, input_size]`\n                att_labels: list of target labels for Attention, of size\n                    `[num_gpu, B, T_out]`\n                ctc_labels: list of target labels for CTC, of size\n                    `[num_gpu, B, T_out]`\n                inputs_seq_len: list of length of inputs of size\n                    `[num_gpu, B]`\n                att_labels_seq_len: list of length of target labels for Attention, of size\n                    `[num_gpu, B]`\n                input_names: list of file name of input data of size\n                    `[num_gpu, B]`\n            is_new_epoch (bool): If true, one epoch is finished\n        """"""\n        if self.max_epoch is not None and self.epoch >= self.max_epoch:\n            raise StopIteration\n        # NOTE: max_epoch = None means infinite loop\n\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        # reset\n        if self.is_new_epoch:\n            self.is_new_epoch = False\n\n        if not self.is_test:\n            self.att_padded_value = self.eos_index\n            self.ctc_padded_value = -1\n        else:\n            self.att_padded_value = None\n            self.ctc_padded_value = None\n        # TODO(hirofumi): move this\n\n        if self.sort_utt:\n            # Sort all uttrances by length\n            if len(self.rest) > batch_size:\n                data_indices = sorted(list(self.rest))[:batch_size]\n                self.rest -= set(data_indices)\n                # NOTE: rest is uttrance length order\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n                if self.epoch == self.sort_stop_epoch:\n                    self.sort_utt = False\n                    self.shuffle = True\n\n            # Shuffle data in the mini-batch\n            random.shuffle(data_indices)\n\n        elif self.shuffle:\n            # Randomly sample uttrances\n            if len(self.rest) > batch_size:\n                data_indices = random.sample(list(self.rest), batch_size)\n                self.rest -= set(data_indices)\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n\n                # Shuffle selected mini-batch\n                random.shuffle(data_indices)\n\n        else:\n            if len(self.rest) > batch_size:\n                data_indices = sorted(list(self.rest))[:batch_size]\n                self.rest -= set(data_indices)\n                # NOTE: rest is in name order\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n\n        # Load dataset in mini-batch\n        input_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.input_paths, data_indices, axis=0))))\n        label_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.label_paths, data_indices, axis=0))))\n\n        if not hasattr(self, \'input_size\'):\n            self.input_size = input_list[0].shape[1]\n            if self.num_stack is not None and self.num_skip is not None:\n                self.input_size *= self.num_stack\n\n        # Frame stacking\n        input_list = stack_frame(input_list,\n                                 self.num_stack,\n                                 self.num_skip,\n                                 progressbar=False)\n\n        # Compute max frame num in mini-batch\n        max_frame_num = max(map(lambda x: x.shape[0], input_list))\n\n        # Compute max target label length in mini-batch\n        max_seq_len = max(map(len, label_list))\n\n        # Initialization\n        inputs = np.zeros(\n            (len(data_indices), max_frame_num, self.input_size * self.splice),\n            dtype=np.float32)\n        att_labels = np.array(\n            [[self.att_padded_value] * (max_seq_len + 2)] * len(data_indices))\n        ctc_labels = np.array(\n            [[self.ctc_padded_value] * max_seq_len] * len(data_indices))\n        inputs_seq_len = np.zeros((len(data_indices),), dtype=np.int32)\n        att_labels_seq_len = np.zeros((len(data_indices),), dtype=np.int32)\n        input_names = np.array(list(\n            map(lambda path: basename(path).split(\'.\')[0],\n                np.take(self.input_paths, data_indices, axis=0))))\n\n        # Set values of each data in mini-batch\n        for i_batch in range(len(data_indices)):\n            data_i = input_list[i_batch]\n            frame_num, input_size = data_i.shape\n\n            # Splicing\n            data_i = data_i.reshape(1, frame_num, input_size)\n            data_i = do_splice(data_i,\n                               splice=self.splice,\n                               batch_size=1,\n                               num_stack=self.num_stack)\n            data_i = data_i.reshape(frame_num, -1)\n\n            inputs[i_batch, :frame_num, :] = data_i\n            if self.is_test:\n                att_labels[i_batch, 0] = label_list[i_batch]\n                ctc_labels[i_batch, 0] = label_list[i_batch]\n                # NOTE: transcript is saved as string\n            else:\n                att_labels[i_batch, 0] = self.sos_index\n                att_labels[i_batch, 1:len(\n                    label_list[i_batch]) + 1] = label_list[i_batch]\n                att_labels[i_batch, len(\n                    label_list[i_batch]) + 1] = self.eos_index\n                ctc_labels[i_batch, :len(\n                    label_list[i_batch])] = label_list[i_batch]\n            inputs_seq_len[i_batch] = frame_num\n            att_labels_seq_len[i_batch] = len(label_list[i_batch]) + 2\n            # TODO: +2 ??\n\n        ###############\n        # Multi-GPUs\n        ###############\n        if self.num_gpu > 1:\n            # Now we split the mini-batch data by num_gpu\n            inputs = np.array_split(inputs, self.num_gpu, axis=0)\n            att_labels = np.array_split(att_labels, self.num_gpu, axis=0)\n            ctc_labels = np.array_split(ctc_labels, self.num_gpu, axis=0)\n            inputs_seq_len = np.array_split(\n                inputs_seq_len, self.num_gpu, axis=0)\n            att_labels_seq_len = np.array_split(\n                att_labels_seq_len, self.num_gpu, axis=0)\n            input_names = np.array_split(input_names, self.num_gpu, axis=0)\n        else:\n            inputs = inputs[np.newaxis, :, :, :]\n            att_labels = att_labels[np.newaxis, :, :]\n            ctc_labels = ctc_labels[np.newaxis, :, :]\n            inputs_seq_len = inputs_seq_len[np.newaxis, :]\n            att_labels_seq_len = att_labels_seq_len[np.newaxis, :]\n            input_names = np.array(input_names)[np.newaxis, :]\n\n        self.iteration += len(data_indices)\n\n        # Clean up\n        del input_list\n        del label_list\n\n        return (inputs, att_labels, ctc_labels, inputs_seq_len,\n                att_labels_seq_len, input_names), self.is_new_epoch\n'"
utils/dataset/multitask_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the multitask CTC model.\n   In this class, all data will be loaded at each step.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import basename\nimport random\nimport numpy as np\n\nfrom utils.dataset.base import Base\nfrom utils.io.inputs.frame_stacking import stack_frame\nfrom utils.io.inputs.splicing import do_splice\n\n\nclass DatasetBase(Base):\n\n    def __init__(self, *args, **kwargs):\n        super(DatasetBase, self).__init__(*args, **kwargs)\n\n    def __getitem__(self, index):\n        input_i = np.array(self.input_paths[index])\n        label_main_i = np.array(self.label_main_paths[index])\n        label_sub_i = np.array(self.label_sub_paths[index])\n        return (input_i, label_main_i, label_sub_i)\n\n    def __next__(self, batch_size=None):\n        """"""Generate each mini-batch.\n        Args:\n            batch_size (int, optional): the size of mini-batch\n        Returns:\n            A tuple of `(inputs, labels_main, labels_sub, inputs_seq_len, input_names)`\n                inputs: list of input data of size\n                    `[num_gpu, B, T_in, input_size]`\n                labels_main: list of target labels in the main task, of size\n                    `[num_gpu, B, T_in]`\n                labels_sub: list of target labels in the sub task, of size\n                    `[num_gpu, B, T_in]`\n                inputs_seq_len: list of length of inputs of size\n                    `[num_gpu, B]`\n                input_names: list of file name of input data of size\n                    `[num_gpu, B]`\n            is_new_epoch (bool): If true, 1 epoch is finished\n        """"""\n        if self.max_epoch is not None and self.epoch >= self.max_epoch:\n            raise StopIteration\n        # NOTE: max_epoch = None means infinite loop\n\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        # reset\n        if self.is_new_epoch:\n            self.is_new_epoch = False\n\n        if not self.is_test:\n            self.padded_value = -1\n        else:\n            self.padded_value = None\n        # TODO(hirofumi): move this\n\n        if self.sort_utt:\n            # Sort all uttrances by length\n            if len(self.rest) > batch_size:\n                data_indices = sorted(list(self.rest))[:batch_size]\n                self.rest -= set(data_indices)\n                # NOTE: rest is uttrance length order\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n                if self.epoch == self.sort_stop_epoch:\n                    self.sort_utt = False\n                    self.shuffle = True\n\n            # Shuffle data in the mini-batch\n            random.shuffle(data_indices)\n\n        elif self.shuffle:\n            # Randomly sample uttrances\n            if len(self.rest) > batch_size:\n                data_indices = random.sample(list(self.rest), batch_size)\n                self.rest -= set(data_indices)\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n\n                # Shuffle selected mini-batch\n                random.shuffle(data_indices)\n\n        else:\n            if len(self.rest) > batch_size:\n                data_indices = sorted(list(self.rest))[:batch_size]\n                self.rest -= set(data_indices)\n                # NOTE: rest is in name order\n            else:\n                # Last mini-batch\n                data_indices = list(self.rest)\n                self.reset()\n                self.is_new_epoch = True\n                self.epoch += 1\n\n        # Load dataset in mini-batch\n        input_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.input_paths, data_indices, axis=0))))\n        label_main_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.label_main_paths, data_indices, axis=0))))\n        label_sub_list = np.array(list(\n            map(lambda path: np.load(path),\n                np.take(self.label_sub_paths, data_indices, axis=0))))\n\n        if not hasattr(self, \'input_size\'):\n            self.input_size = input_list[0].shape[1]\n            if self.num_stack is not None and self.num_skip is not None:\n                self.input_size *= self.num_stack\n\n        # Frame stacking\n        input_list = stack_frame(input_list,\n                                 self.num_stack,\n                                 self.num_skip,\n                                 progressbar=False)\n\n        # Compute max frame num in mini-batch\n        max_frame_num = max(map(lambda x: x.shape[0], input_list))\n\n        # Compute max target label length in mini-batch\n        max_seq_len_main = max(map(len, label_main_list))\n        max_seq_len_sub = max(map(len, label_sub_list))\n\n        # Initialization\n        inputs = np.zeros(\n            (len(data_indices), max_frame_num, self.input_size * self.splice),\n            dtype=np.float32)\n        labels_main = np.array(\n            [[self.padded_value] * max_seq_len_main] * len(data_indices))\n        labels_sub = np.array(\n            [[self.padded_value] * max_seq_len_sub] * len(data_indices))\n        inputs_seq_len = np.zeros((len(data_indices),), dtype=np.int32)\n        input_names = list(\n            map(lambda path: basename(path).split(\'.\')[0],\n                np.take(self.input_paths, data_indices, axis=0)))\n\n        # Set values of each data in mini-batch\n        for i_batch in range(len(data_indices)):\n            data_i = input_list[i_batch]\n            frame_num, input_size = data_i.shape\n\n            # Splicing\n            data_i = data_i.reshape(1, frame_num, input_size)\n            data_i = do_splice(data_i,\n                               splice=self.splice,\n                               batch_size=1,\n                               num_stack=self.num_stack)\n            data_i = data_i.reshape(frame_num, -1)\n\n            inputs[i_batch, :frame_num, :] = data_i\n            if self.is_test:\n                labels_main[i_batch, 0] = label_main_list[i_batch]\n                # NOTE: transcript is saved as string\n            else:\n                labels_main[i_batch, :len(\n                    label_main_list[i_batch])] = label_main_list[i_batch]\n            labels_sub[i_batch, :len(\n                label_sub_list[i_batch])] = label_sub_list[i_batch]\n            inputs_seq_len[i_batch] = frame_num\n\n        ###############\n        # Multi-GPUs\n        ###############\n        if self.num_gpu > 1:\n            # Now we split the mini-batch data by num_gpu\n            inputs = np.array_split(inputs, self.num_gpu, axis=0)\n            labels_main = np.array_split(labels_main, self.num_gpu, axis=0)\n            labels_sub = np.array_split(labels_sub, self.num_gpu, axis=0)\n            inputs_seq_len = np.array_split(\n                inputs_seq_len, self.num_gpu, axis=0)\n            input_names = np.array_split(input_names, self.num_gpu, axis=0)\n        else:\n            inputs = inputs[np.newaxis, :, :, :]\n            labels_main = labels_main[np.newaxis, :, :]\n            labels_sub = labels_sub[np.newaxis, :, :]\n            inputs_seq_len = inputs_seq_len[np.newaxis, :]\n            input_names = np.array(input_names)[np.newaxis, :]\n\n        self.iteration += len(data_indices)\n\n        # Clean up\n        del input_list\n        del label_main_list\n        del label_sub_list\n\n        return (inputs, labels_main, labels_sub, inputs_seq_len,\n                input_names), self.is_new_epoch\n'"
utils/dataset/xe.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Base class for loading dataset for the frame-wise model.\n   In this class, all data will be loaded at each step.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\nimport numpy as np\n\nfrom utils.dataset.base import Base\n\n\nclass DatasetBase(Base):\n\n    def __init__(self, *args, **kwargs):\n        super(DatasetBase, self).__init__(*args, **kwargs)\n\n    def __getitem__(self, index):\n        input_i = np.array(self.input_paths[index])\n        label_i = np.array(self.label_paths[index])\n        return (input_i, label_i)\n\n    def __len__(self):\n        if self.data_type == \'train\':\n            return 18088388\n        elif self.data_type == \'dev_clean\':\n            return 968057\n        elif self.data_type == \'dev_other\':\n            return 919980\n\n    def __next__(self, batch_size=None):\n        """"""Generate each mini-batch.\n        Args:\n            batch_size (int, optional): the size of mini-batch\n        Returns:\n            A tuple of `(inputs, labels, inputs_seq_len, labels_seq_len, input_names)`\n                inputs: list of input data of size\n                    `[num_gpu, B, input_size]`\n                labels: list of target labels of size\n                    `[num_gpu, B, num_classes]`\n                input_names: list of file name of input data of size\n                    `[num_gpu, B]`\n            is_new_epoch (bool): If true, 1 epoch is finished\n        """"""\n        if self.max_epoch is not None and self.epoch >= self.max_epoch:\n            raise StopIteration\n        # NOTE: max_epoch = None means infinite loop\n\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        # reset\n        if self.is_new_epoch:\n            self.is_new_epoch = False\n\n        # Load the first block at each epoch\n        if self.iteration == 0 or self.is_new_epoch:\n            # Randomly sample block\n            block_index = random.sample(list(self.rest_block), 1)\n            self.rest_block -= set(block_index)\n\n            # Load block\n            self.inputs_block = np.array(list(\n                map(lambda path: np.load(path),\n                    self.input_paths[block_index])))\n            # NOTE: `[1, num_frames_per_block, input_dim]`\n            self.inputs_block = self.inputs_block.reshape(\n                -1, self.inputs_block.shape[-1])\n\n            self.labels_block = np.array(list(\n                map(lambda path: np.load(path),\n                    self.label_paths[block_index])))\n            # NOTE: `[1, num_frames_per_block, num_classes]`\n            self.labels_block = self.labels_block.reshape(\n                -1, self.labels_block.shape[-1])\n\n            self.rest_frames = set(range(0, len(self.inputs_block), 1))\n\n        # Load block if needed\n        if len(self.rest_frames) < batch_size and len(self.rest_block) != 0:\n            # Randomly sample block\n            if len(self.rest_block) > 1:\n                block_index = random.sample(list(self.rest_block), 1)\n            else:\n                # Last block in each epoch\n                block_index = list(self.rest_block)\n            self.rest_block -= set(block_index)\n\n            # tmp\n            rest_inputs_pre_block = self.inputs_block[list(self.rest_frames)]\n            rest_labels_pre_block = self.labels_block[list(self.rest_frames)]\n\n            self.inputs_block = np.array(list(\n                map(lambda path: np.load(path),\n                    self.input_paths[block_index]))).reshape(-1, self.inputs_block.shape[-1])\n            self.labels_block = np.array(list(\n                map(lambda path: np.load(path),\n                    self.label_paths[block_index]))).reshape(-1, self.labels_block.shape[-1])\n\n            # Concatenate\n            self.inputs_block = np.concatenate(\n                (rest_inputs_pre_block, self.inputs_block), axis=0)\n            self.labels_block = np.concatenate(\n                (rest_labels_pre_block, self.labels_block), axis=0)\n\n            self.rest_frames = set(range(0, len(self.inputs_block), 1))\n\n        # Randomly sample frames\n        if len(self.rest_frames) > batch_size:\n            frame_indices = random.sample(\n                list(self.rest_frames), batch_size)\n        else:\n            # Last mini-batch in each block\n            frame_indices = list(self.rest_frames)\n\n            # Shuffle selected mini-batch\n            random.shuffle(frame_indices)\n        self.rest_frames -= set(frame_indices)\n\n        if len(self.rest_block) == 0 and len(self.rest_frames) == 0:\n            self.reset()\n            self.is_new_epoch = True\n            self.epoch += 1\n            self.rest_block = set(range(0, len(self.input_paths), 1))\n\n        # Set values of each data in mini-batch\n        inputs = self.inputs_block[frame_indices]\n        labels = self.labels_block[frame_indices]\n\n        ###############\n        # Multi-GPUs\n        ###############\n        if self.num_gpu > 1:\n            # Now we split the mini-batch data by num_gpu\n            inputs = np.array_split(inputs, self.num_gpu, axis=0)\n            labels = np.array_split(labels, self.num_gpu, axis=0)\n        else:\n            inputs = inputs[np.newaxis, :, :]\n            labels = labels[np.newaxis, :, :]\n\n        self.iteration += len(frame_indices)\n\n        return (inputs, labels), self.is_new_epoch\n'"
utils/evaluation/__init__.py,0,b''
utils/evaluation/edit_distance.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Functions for computing edit distance.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport Levenshtein as lev\n\n\ndef compute_edit_distance(session, labels_true_st, labels_pred_st):\n    """"""Compute edit distance per mini-batch.\n    Args:\n        session:\n        labels_true_st: A `SparseTensor` of ground truth\n        labels_pred_st: A `SparseTensor` of prediction\n    Returns:\n        edit_distances: list of edit distance of each uttearance\n    """"""\n    indices, values, dense_shape = labels_true_st\n    labels_pred_pl = tf.SparseTensor(indices, values, dense_shape)\n    indices, values, dense_shape = labels_pred_st\n    labels_true_pl = tf.SparseTensor(indices, values, dense_shape)\n\n    edit_op = tf.edit_distance(labels_pred_pl, labels_true_pl, normalize=True)\n    edit_distances = session.run(edit_op)\n\n    return edit_distances\n\n\ndef compute_per(ref, hyp, normalize=True):\n    """"""Compute Phone Error Rate.\n    Args:\n        ref (list): phones in the reference transcript\n        hyp (list): phones in the predicted transcript\n        normalize (bool, optional): if True, divide by the length of str_true\n    Returns:\n        per (float): Phone Error Rate between str_true and str_pred\n    """"""\n    # Build mapping of phone to index\n    phone_set = set(ref + hyp)\n    phone2char = dict(zip(phone_set, range(len(phone_set))))\n\n    # Map phones to a single char array\n    # NOTE: Levenshtein packages only accepts strings\n    phones_ref = [chr(phone2char[p]) for p in ref]\n    phones_hyp = [chr(phone2char[p]) for p in hyp]\n\n    per = lev.distance(\'\'.join(phones_ref), \'\'.join(phones_hyp))\n    if normalize:\n        per /= len(ref)\n    return per\n\n\ndef compute_cer(str_pred, str_true, normalize=True):\n    """"""Compute Character Error Rate.\n    Args:\n        str_pred (string): a sentence without spaces\n        str_true (string): a sentence without spaces\n        normalize (bool, optional): if True, divide by the length of str_true\n    Returns:\n        cer (float): Character Error Rate between str_true and str_pred\n    """"""\n    cer = lev.distance(str_pred, str_true)\n    if normalize:\n        cer /= len(list(str_true))\n    return cer\n\n\ndef compute_wer(ref, hyp, normalize=True):\n    """"""Compute Word Error Rate.\n        [Reference]\n            https://martin-thoma.com/word-error-rate-calculation/\n    Args:\n        ref (list): words in the reference transcript\n        hyp (list): words in the predicted transcript\n        normalize (bool, optional): if True, divide by the length of ref\n    Returns:\n        wer (float): Word Error Rate between ref and hyp\n    """"""\n    # Initialisation\n    d = np.zeros((len(ref) + 1) * (len(hyp) + 1), dtype=np.uint16)\n    d = d.reshape((len(ref) + 1, len(hyp) + 1))\n    for i in range(len(ref) + 1):\n        for j in range(len(hyp) + 1):\n            if i == 0:\n                d[0][j] = j\n            elif j == 0:\n                d[i][0] = i\n\n    # Computation\n    for i in range(1, len(ref) + 1):\n        for j in range(1, len(hyp) + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                d[i][j] = d[i - 1][j - 1]\n            else:\n                substitution = d[i - 1][j - 1] + 1\n                insertion = d[i][j - 1] + 1\n                deletion = d[i - 1][j] + 1\n                d[i][j] = min(substitution, insertion, deletion)\n\n    wer = d[len(ref)][len(hyp)]\n    if normalize:\n        wer /= len(ref)\n    return wer\n\n\ndef wer_align(ref, hyp):\n    """"""Compute Word Error Rate.\n        [Reference]\n            https://github.com/zszyellow/WER-in-python\n    Args:\n        ref (list): words in the reference transcript\n        hyp (list): words in the predicted transcript\n    Returns:\n        substitute (int): the number of substitution error\n        insert (int): the number of insertion error\n        delete (int): the number of deletion error\n    """"""\n    # Build the matrix\n    d = np.zeros((len(ref) + 1) * (len(hyp) + 1),\n                 dtype=np.uint8).reshape((len(ref) + 1, len(hyp) + 1))\n    for i in range(len(ref) + 1):\n        for j in range(len(hyp) + 1):\n            if i == 0:\n                d[0][j] = j\n            elif j == 0:\n                d[i][0] = i\n    for i in range(1, len(ref) + 1):\n        for j in range(1, len(hyp) + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                d[i][j] = d[i - 1][j - 1]\n            else:\n                substitute = d[i - 1][j - 1] + 1\n                insert = d[i][j - 1] + 1\n                delete = d[i - 1][j] + 1\n                d[i][j] = min(substitute, insert, delete)\n    result = float(d[len(ref)][len(hyp)]) / len(ref) * 100\n    result = str(""%.2f"" % result) + ""%""\n\n    # Find out the manipulation steps\n    x = len(ref)\n    y = len(hyp)\n    error_list = []\n    while True:\n        if x == 0 and y == 0:\n            break\n        else:\n            if d[x][y] == d[x - 1][y - 1] and ref[x - 1] == hyp[y - 1]:\n                error_list.append(""e"")\n                x = x - 1\n                y = y - 1\n            elif d[x][y] == d[x][y - 1] + 1:\n                error_list.append(""i"")\n                x = x\n                y = y - 1\n            elif d[x][y] == d[x - 1][y - 1] + 1:\n                error_list.append(""s"")\n                x = x - 1\n                y = y - 1\n            else:\n                error_list.append(""d"")\n                x = x - 1\n                y = y\n    error_list = error_list[::-1]\n\n    # Print the result in aligned way\n    print(""REF: "", end=\'\')\n    for i in range(len(error_list)):\n        if error_list[i] == ""i"":\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""d"":\n                    count += 1\n            index = i - count\n            print("" "" * (len(hyp[index])), end=\' \')\n        elif error_list[i] == ""s"":\n            count1 = 0\n            for j in range(i):\n                if error_list[j] == ""i"":\n                    count1 += 1\n            index1 = i - count1\n            count2 = 0\n            for j in range(i):\n                if error_list[j] == ""d"":\n                    count2 += 1\n            index2 = i - count2\n            if len(ref[index1]) < len(hyp[index2]):\n                print(ref[index1] + "" "" *\n                      (len(hyp[index2]) - len(ref[index1])), end=\' \')\n            else:\n                print(ref[index1], end=\' \')\n        else:\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""i"":\n                    count += 1\n            index = i - count\n            print(ref[index], end=\' \')\n\n    print(""\\nHYP: "", end=\'\')\n    for i in range(len(error_list)):\n        if error_list[i] == ""d"":\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""i"":\n                    count += 1\n            index = i - count\n            print("" "" * (len(ref[index])), end=\' \')\n        elif error_list[i] == ""s"":\n            count1 = 0\n            for j in range(i):\n                if error_list[j] == ""i"":\n                    count1 += 1\n            index1 = i - count1\n            count2 = 0\n            for j in range(i):\n                if error_list[j] == ""d"":\n                    count2 += 1\n            index2 = i - count2\n            if len(ref[index1]) > len(hyp[index2]):\n                print(hyp[index2] + "" "" *\n                      (len(ref[index1]) - len(hyp[index2])), end=\' \')\n            else:\n                print(hyp[index2], end=\' \')\n        else:\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""d"":\n                    count += 1\n            index = i - count\n            print(hyp[index], end=\' \')\n\n    print(""\\nEVA: "", end=\'\')\n    for i in range(len(error_list)):\n        if error_list[i] == ""d"":\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""i"":\n                    count += 1\n            index = i - count\n            print(""D"" + "" "" * (len(ref[index]) - 1), end=\' \')\n        elif error_list[i] == ""i"":\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""d"":\n                    count += 1\n            index = i - count\n            print(""I"" + "" "" * (len(hyp[index]) - 1), end=\' \')\n        elif error_list[i] == ""s"":\n            count1 = 0\n            for j in range(i):\n                if error_list[j] == ""i"":\n                    count1 += 1\n            index1 = i - count1\n            count2 = 0\n            for j in range(i):\n                if error_list[j] == ""d"":\n                    count2 += 1\n            index2 = i - count2\n            if len(ref[index1]) > len(hyp[index2]):\n                print(""S"" + "" "" * (len(ref[index1]) - 1), end=\' \')\n            else:\n                print(""S"" + "" "" * (len(hyp[index2]) - 1), end=\' \')\n        else:\n            count = 0\n            for j in range(i):\n                if error_list[j] == ""i"":\n                    count += 1\n            index = i - count\n            print("" "" * (len(ref[index])), end=\' \')\n\n    print(""\\nWER: "" + result)\n\n    substitute = error_list.count(\'s\')\n    insert = error_list.count(\'i\')\n    delete = error_list.count(\'d\')\n\n    return substitute, insert, delete\n'"
utils/io/__init__.py,0,b''
utils/training/__init__.py,0,b''
utils/training/learning_rate_controller.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decay learning rate per epoch.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Controller(object):\n    """"""Controll learning rate per epoch.\n    Args:\n        learning_rate_init: A float value, the initial learning rate\n        decay_start_epoch: int, the epoch to start decay\n        decay_rate: A float value,  the rate to decay the current learning rate\n        decay_patient_epoch: int, decay learning rate if results have not been\n            improved for \'decay_patient_epoch\'\n        lower_better: If True, the lower, the better.\n                      If False, the higher, the better.\n        worst_value: A flaot value, the worst value of evaluation\n    """"""\n\n    def __init__(self, learning_rate_init, decay_start_epoch, decay_rate,\n                 decay_patient_epoch=1, lower_better=True, worst_value=1):\n        self.learning_rate_init = learning_rate_init\n        self.decay_start_epoch = decay_start_epoch\n        self.decay_rate = decay_rate\n        self.decay_patient_epoch = decay_patient_epoch\n        self.not_improved_epoch = 0\n        self.lower_better = lower_better\n        self.best_value = worst_value\n\n    def decay_lr(self, learning_rate, epoch, value):\n        """"""Decay learning rate per epoch.\n        Args:\n            learning_rate: A float value, the current learning rete\n            epoch: int, the current epoch\n            value: A value to evaluate\n        Returns:\n            learning_rate_decayed: A float value, the decayed learning rate\n        """"""\n        if not self.lower_better:\n            value *= -1\n\n        if epoch < self.decay_start_epoch:\n            if value < self.best_value:\n                # Update\n                self.best_value = value\n            return learning_rate\n\n        if value < self.best_value:\n            # Improved\n            self.best_value = value\n            self.not_improved_epoch = 0\n            return learning_rate\n        elif self.not_improved_epoch < self.decay_patient_epoch:\n            # Not improved, but learning rate will be not decayed\n            self.not_improved_epoch += 1\n            return learning_rate\n        else:\n            # Not improved, and learning rate will be decayed\n            self.not_improved_epoch = 0\n            learning_rate_decayed = learning_rate * self.decay_rate\n            return learning_rate_decayed\n'"
utils/training/multi_gpu.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Utilities for mulit-GPU implementation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef average_gradients(total_grads_and_vars):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n        total_grads_and_vars: List of lists of (gradient, variable) tuples.\n            The outer list is over individual gradients. The inner list is over\n            the gradient calculation for each tower.\n    Returns:\n        average_grads_and_vars: List of pairs of (gradient, variable) where\n            the gradient has been averaged across all towers.\n    """"""\n    average_grads_and_vars = []\n    for tower_grads_and_vars in zip(*total_grads_and_vars):\n        # Note that each tower_grads_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        tower_grads = []\n        for grad, _ in tower_grads_and_vars:  # var is ignored\n            if grad is not None:\n                # Add 0 dimension to the gradients to represent the tower\n                expanded_grad = tf.expand_dims(grad, axis=0)\n\n                # Append on a \'tower\' dimension which we will average over\n                # below\n                tower_grads.append(expanded_grad)\n\n        # Average over the \'tower\' dimension.\n        tower_grads = tf.concat(axis=0, values=tower_grads)\n        mean_tower_grad = tf.reduce_mean(tower_grads, axis=0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        var = tower_grads_and_vars[0][1]  # var0_gpu0\n        grad_and_var = (mean_tower_grad, var)\n        average_grads_and_vars.append(grad_and_var)\n    return average_grads_and_vars\n'"
utils/training/plot.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nplt.style.use(\'ggplot\')\nimport seaborn as sns\n\nblue = \'#4682B4\'\norange = \'#D2691E\'\n\n\ndef plot_loss(train_losses, dev_losses, steps, save_path):\n    """"""Save history of training & dev loss as figure.\n    Args:\n        train_losses (list): train losses\n        dev_losses (list): dev losses\n        steps (list): steps\n    """"""\n    # Save as csv file\n    loss_graph = np.column_stack((steps, train_losses, dev_losses))\n    if os.path.isfile(os.path.join(save_path, ""ler.csv"")):\n        os.remove(os.path.join(save_path, ""ler.csv""))\n    np.savetxt(os.path.join(save_path, ""loss.csv""), loss_graph, delimiter="","")\n\n    # TODO: error check for inf loss\n\n    # Plot & save as png file\n    plt.clf()\n    plt.plot(steps, train_losses, blue, label=""Train"")\n    plt.plot(steps, dev_losses, orange, label=""Dev"")\n    plt.xlabel(\'step\', fontsize=12)\n    plt.ylabel(\'loss\', fontsize=12)\n    plt.legend(loc=""upper right"", fontsize=12)\n    if os.path.isfile(os.path.join(save_path, ""loss.png"")):\n        os.remove(os.path.join(save_path, ""loss.png""))\n    plt.savefig(os.path.join(save_path, ""loss.png""), dvi=500)\n\n\ndef plot_ler(train_lers, dev_lers, steps, label_type, save_path):\n    """"""Save history of training & dev LERs as figure.\n    Args:\n        train_lers (list): train losses\n        dev_lers (list): dev losses\n        steps (list): steps\n    """"""\n    if \'word\' in label_type:\n        name = \'WER\'\n    elif \'char\' in label_type or \'kana\' in label_type or \'kanji\' in label_type:\n        name = \'CER\'\n    elif \'phone\' in label_type:\n        name = \'PER\'\n    else:\n        name = \'LER\'\n\n    # Save as csv file\n    loss_graph = np.column_stack((steps, train_lers, dev_lers))\n    if os.path.isfile(os.path.join(save_path, ""ler.csv"")):\n        os.remove(os.path.join(save_path, ""ler.csv""))\n    np.savetxt(os.path.join(save_path, ""ler.csv""), loss_graph, delimiter="","")\n\n    # Plot & save as png file\n    plt.clf()\n    plt.plot(steps, train_lers, blue, label=""Train"")\n    plt.plot(steps, dev_lers, orange, label=""Dev"")\n    plt.xlabel(\'step\', fontsize=12)\n    plt.ylabel(name, fontsize=12)\n    plt.legend(loc=""upper right"", fontsize=12)\n    if os.path.isfile(os.path.join(save_path, name.lower() + \'.png\')):\n        os.remove(os.path.join(save_path, name.lower() + \'.png\'))\n    plt.savefig(os.path.join(save_path, name.lower() + \'.png\'), dvi=500)\n'"
examples/csj/data/load_dataset_attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the Attention model (CSJ corpus).\n   In addition, frame stacking and skipping are used.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.attention import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, train_data_size, label_type, batch_size,\n                 map_file_path, max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=True, sort_stop_epoch=None,\n                 progressbar=False, num_gpu=1):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or eval1 or eval2 or eval3\n            train_data_size (string): train_subset or train_fullset\n            label_type (string): kanji or kanji_divide or kana or\n                kana_divide\n            batch_size (int): the size of mini-batch\n            map_file_path (string): path to the mapping file\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        super(Dataset, self).__init__(map_file_path=map_file_path)\n\n        if data_type in [\'eval1\', \'eval2\', \'eval3\']:\n            self.is_test = True\n        else:\n            self.is_test = False\n\n        self.data_type = data_type\n        self.train_data_size = train_data_size\n        self.label_type = label_type\n        self.batch_size = batch_size * num_gpu\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = num_gpu\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/csj\',\n                        \'/n/sd8/inaguma/corpus/csj/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\',\n                          train_data_size, data_type)\n        # NOTE: ex.) save_path:\n        # csj_dataset_path/inputs/train_data_size/data_type/speaker/***.npy\n        label_path = join(dataset_root[0], \'labels\',\n                          train_data_size, data_type, label_type)\n        # NOTE: ex.) save_path:\n        # csj_dataset_path/labels/train_data_size/data_type/label_type/speaker/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\',\n                              train_data_size, data_type)\n            label_path = join(dataset_root[0], \'labels\',\n                              train_data_size, data_type, label_type)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for utt_name, frame_num in frame_num_tuple_sorted:\n            speaker = utt_name.split(\'_\')[0]\n            # ex.) utt_name: speaker_uttindex\n            input_paths.append(join(input_path, speaker, utt_name + \'.npy\'))\n            label_paths.append(join(label_path, speaker, utt_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/csj/data/load_dataset_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the CTC model (CSJ corpus).\n   In addition, frame stacking and skipping are used.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.ctc import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, train_data_size, label_type, batch_size,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=True, sort_stop_epoch=None,\n                 progressbar=False, num_gpu=1):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or eval1 or eval2 or eval3\n            train_data_size (string): train_subset or train_fullset\n            label_type (string): kanji or kanji_divide or kana or\n                kana_divide\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n            num_gpu (int, optional): if more than 1, divide batch_size by num_gpu\n        """"""\n        super(Dataset, self).__init__()\n\n        if data_type in [\'eval1\', \'eval2\', \'eval3\']:\n            self.is_test = True\n        else:\n            self.is_test = False\n\n        self.data_type = data_type\n        self.train_data_size = train_data_size\n        self.label_type = label_type\n        self.batch_size = batch_size * num_gpu\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = num_gpu\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/csj\',\n                        \'/n/sd8/inaguma/corpus/csj/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\',\n                          train_data_size, data_type)\n        # NOTE: ex.) save_path:\n        # csj_dataset_path/inputs/train_data_size/data_type/speaker/***.npy\n        label_path = join(dataset_root[0], \'labels\',\n                          train_data_size, data_type, label_type)\n        # NOTE: ex.) save_path:\n        # csj_dataset_path/labels/train_data_size/data_type/label_type/speaker/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\',\n                              train_data_size, data_type)\n            label_path = join(dataset_root[0], \'labels\',\n                              train_data_size, data_type, label_type)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for utt_name, frame_num in frame_num_tuple_sorted:\n            speaker = utt_name.split(\'_\')[0]\n            # ex.) utt_name: speaker_uttindex\n            input_paths.append(join(input_path, speaker, utt_name + \'.npy\'))\n            label_paths.append(join(label_path, speaker, utt_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/csj/data/load_dataset_multitask_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the multitask CTC model (CSJ corpus).\n   In addition, frame stacking and skipping are used.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.each_load.multitask_ctc_each_load import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, train_data_size, label_type_main,\n                 label_type_sub, batch_size,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=True, sort_stop_epoch=None,\n                 progressbar=False, num_gpu=1, is_gpu=True):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or eval1 or eval2 or eval3\n            train_data_size (string): train_subset or train_fullset\n            label_type_main (string): label type of the main task\n                kanji or kanji_wakachi or kana or kana_wakachi\n            label_type_sub (string): label type of the sub task\n                kana or kana_wakachi or phone\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n            num_gpu (int, optional): if more than 1, divide batch_size by num_gpu\n            is_gpu (bool, optional): if True, use dataset in the GPU server. This is\n                useful when data size is very large and you cannot load all\n                dataset at once. Then, you should put dataset on the GPU server\n                you will use to reduce data-communication time between servers.\n        """"""\n        super(Dataset, self).__init__()\n\n        if data_type in [\'eval1\', \'eval2\', \'eval3\'] and label_type_sub != \'phone\':\n            self.is_test = True\n        else:\n            self.is_test = False\n\n        self.data_type = data_type\n        self.train_data_size = train_data_size\n        self.label_type_main = label_type_main\n        self.label_type_sub = label_type_sub\n        self.batch_size = batch_size * num_gpu\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = num_gpu\n        self.padded_value = -1\n\n        if is_gpu:\n            # GPU server\n            root = \'/data/inaguma/csj\'\n        else:\n            # CPU server\n            root = \'/n/sd8/inaguma/corpus/csj/dataset\'\n\n        input_path = join(root, \'inputs\', train_data_size, data_type)\n        label_main_path = join(root, \'labels/ctc\', train_data_size,\n                               label_type_main, data_type)\n        label_sub_path = join(root, \'labels/ctc\', train_data_size,\n                              label_type_sub, data_type)\n\n        # Load the frame number dictionary\n        with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n            self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_main_paths, label_sub_paths = [], [], []\n        for utt_name, frame_num in frame_num_tuple_sorted:\n            # ex.) utt_name: speaker + _ + utt_index\n            input_paths.append(join(input_path, utt_name + \'.npy\'))\n            label_main_paths.append(join(label_main_path, utt_name + \'.npy\'))\n            label_sub_paths.append(join(label_sub_path, utt_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_main_paths = np.array(label_main_paths)\n        self.label_sub_paths = np.array(label_sub_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/csj/evaluation/eval_ctc.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the trained CTC model (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.csj.data.load_dataset_ctc import Dataset\nfrom experiments.csj.metrics.ctc import do_eval_cer\nfrom models.ctc.ctc import CTC\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=1,\n                    help=\'the size of mini-batch when evaluation\')\n\n\ndef do_eval(model, params, epoch, eval_batch_size, beam_width):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        eval_batch_size (int): the size of mini-batch when evaluation\n        beam_width (int): beam_width (int, optional): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n    """"""\n    # Load dataset\n    eval1_data = Dataset(\n        data_type=\'eval1\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    eval2_data = Dataset(\n        data_type=\'eval2\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    eval3_data = Dataset(\n        data_type=\'eval3\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                       model.labels_pl_list[0],\n                                       model.inputs_seq_len_pl_list[0],\n                                       model.keep_prob_pl_list[0])\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'Test Data Evaluation:\')\n        cer_eval1 = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op],\n            model=model,\n            dataset=eval1_data,\n            label_type=params[\'label_type\'],\n            train_data_size=params[\'train_data_size\'],\n            is_test=True,\n            # eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER (eval1): %f %%\' % (cer_eval1 * 100))\n\n        cer_eval2 = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op],\n            model=model,\n            dataset=eval2_data,\n            label_type=params[\'label_type\'],\n            train_data_size=params[\'train_data_size\'],\n            is_test=True,\n            # eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER (eval2): %f %%\' % (cer_eval2 * 100))\n\n        cer_eval3 = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op],\n            model=model,\n            dataset=eval3_data,\n            label_type=params[\'label_type\'],\n            train_data_size=params[\'train_data_size\'],\n            is_test=True,\n            # eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER (eval3): %f %%\' % (cer_eval3 * 100))\n\n        cer_mean = (cer_eval1 + cer_eval2 + cer_eval3) / 3.\n        print(\'  CER (mean): %f %%\' % (cer_mean * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type\'] == \'kana\':\n        params[\'num_classes\'] = 146\n    elif params[\'label_type\'] == \'kana_divide\':\n        params[\'num_classes\'] = 147\n    elif params[\'label_type\'] == \'kanji\':\n        if params[\'train_data_size\'] == \'train_subset\':\n            params[\'num_classes\'] = 2981\n        elif params[\'train_data_size\'] == \'train_fullset\':\n            params[\'num_classes\'] = 3385\n    elif params[\'label_type\'] == \'kanji_divide\':\n        if params[\'train_data_size\'] == \'train_subset\':\n            params[\'num_classes\'] = 2982\n        elif params[\'train_data_size\'] == \'train_fullset\':\n            params[\'num_classes\'] = 3386\n    else:\n        raise TypeError\n\n    # Modle setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'] * params[\'num_stack\'],\n                splice=params[\'splice\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, eval_batch_size=args.eval_batch_size,\n            beam_width=args.beam_width)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/csj/evaluation/eval_julius_cer.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport sys\nimport codecs\nfrom glob import glob\nimport numpy as np\nimport Levenshtein\n\nsys.path.append(\'../../../\')\nfrom experiments.utils.labels.character import kana2num, num2char\nfrom experiments.csj.data.load_dataset_ctc import Dataset\nfrom experiments.utils.progressbar import wrap_iterator\n\n\ndef main():\n\n    # Make mapping dictionary from kana to phone\n    phone2kana_dict = {}\n    with open(\'../metrics/mapping_files/kana2phone.txt\', \'r\') as f:\n        for line in f:\n            line = line.strip().split(\'+\')\n            kana, phone_seq = line\n            phone = re.sub(\' \', \'\', phone_seq)\n            if phone in phone2kana_dict.keys():\n                continue\n            phone2kana_dict[phone] = kana\n            phone2kana_dict[phone + \':\'] = kana + \'\xe3\x83\xbc\'\n\n    # Julius Results\n    for data_type in [\'eval1\', \'eval2\', \'eval3\']:\n        results_paths = [path for path in glob(\n            \'/home/lab5/inaguma/asru2017/csj_results_0710_kana/\' + data_type + \'/*.kana\')]\n        result_dict = {}\n\n        for path in results_paths:\n            with codecs.open(path, \'r\', \'euc_jp\') as f:\n                file_name = \'\'\n                output = \'\'\n                for line in f:\n                    line = line.strip()\n\n                    if \'wav\' in line:\n                        file_name = line.split(\': \')[-1]\n                        file_name = \'_\'.join(line.split(\'/\')[-2:])\n                        file_name = re.sub(\'.wav\', \'\', file_name)\n\n                    else:\n                        output = line\n                        output = re.sub(\'sp\', \'\', output)\n                        result_dict[file_name] = output\n\n        label_type = \'kana\'\n        dataset = Dataset(data_type=data_type,\n                          label_type=label_type,\n                          batch_size=1,\n                          train_data_size=\'large\',\n                          is_sorted=False,\n                          is_progressbar=True,\n                          is_gpu=False)\n\n        num_examples = dataset.data_num\n        cer_sum = 0\n\n        mini_batch = dataset.next_batch(batch_size=1)\n\n        def map_fn(phone): return phone2kana_dict[phone]\n\n        for _ in wrap_iterator(range(num_examples), False):\n            # Create feed dictionary for next mini batch\n            _, labels_true, _, input_names = mini_batch.__next__()\n\n            if input_names[0] not in result_dict.keys():\n                continue\n\n            output = result_dict[input_names[0]].split(\' \')\n            while \'\' in output:\n                output.remove(\'\')\n\n            str_pred = \'\'.join(list(map(map_fn, output)))\n\n            if input_names[0] in [\'A03M0106_0057\', \'A03M0016_0014\']:\n                print(str_pred)\n                print(labels_true[0])\n                print(\'-----\')\n\n            # Remove silence(_) & noise(NZ) labels\n            str_true = re.sub(r\'[_NZ\xe3\x83\xbc\xe3\x83\xbb]+\', """", labels_true[0])\n            str_pred = re.sub(r\'[_NZ\xe3\x83\xbc\xe3\x83\xbb]+\', """", str_pred)\n\n            # Compute edit distance\n            cer_each = Levenshtein.distance(\n                str_pred, str_true) / len(list(str_true))\n\n            cer_sum += cer_each\n\n        print(\'CER (\' + data_type + \'): %f\' % (cer_sum / dataset.data_num))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/csj/evaluation/eval_julius_fmeasure.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport sys\nimport codecs\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\n\nsys.path.append('../../../')\nfrom utils.io.labels.character import Char2idx\nfrom experiments.csj.data.load_dataset_ctc_ss import Dataset\n\n\ndef main():\n\n    # Julius Results\n    mean_f_f = 0\n    mean_f_d = 0\n    for data_type in ['eval1', 'eval2', 'eval3']:\n        results_paths = [path for path in glob(\n            '/home/lab5/inaguma/asru2017/csj_results_0710/' + data_type + '/*.log')]\n        result_dict = {}\n\n        for path in results_paths:\n            with codecs.open(path, 'r', 'euc_jp') as f:\n                start_flag = False\n                file_name = ''\n                output, output_pos = '', ''\n                for line in f:\n                    line = line.strip()\n                    if line == '----------------------- System Information end -----------------------':\n                        start_flag = True\n                    if start_flag:\n                        if 'input MFCC file' in line:\n                            file_name = line.split(': ')[-1]\n                            if data_type != 'dialog':\n                                file_name = '_'.join(file_name.split('/')[-2:])\n                            else:\n                                file_name = os.path.basename(file_name)\n                            file_name = re.sub('.wav', '', file_name)\n                            file_name = re.sub('.htk', '', file_name)\n\n                        if 'sentence1' in line:\n                            output = line.split(': ')[-1]\n                            output = re.sub('<s>', '', output)\n                            output = re.sub('</s>', '', output)\n                            output = re.sub('<sp>', '', output)\n                            output = re.sub(r'[\\s\xe3\x83\xbc]', '', output)\n\n                        if 'wseq1' in line:\n                            output_pos = line.split(': ')[-1]\n                            output_pos = re.sub('<s>', '', output_pos)\n                            output_pos = re.sub('</s>', '', output_pos)\n                            output_pos = re.sub('<sp>', '', output_pos)\n                            output_pos = re.sub('\xe6\x84\x9f\xe5\x8b\x95\xe8\xa9\x9e', 'f', output_pos)\n                            output_pos = re.sub('\xe8\xa8\x80\xe3\x81\x84\xe3\x82\x88\xe3\x81\xa9\xe3\x81\xbf', 'd', output_pos)\n                            result_dict[file_name] = [output, output_pos[1:]]\n\n        label_type = 'kana'\n        dataset = Dataset(data_type=data_type,\n                          label_type=label_type,\n                          ss_type='insert_left',\n                          batch_size=1,\n                          max_epoch=1,\n                          train_data_size='train_subset',\n                          shuffle=False)\n\n        tp_f, fp_f, fn_f = 0., 0., 0.\n        tp_d, fp_d, fn_d = 0., 0., 0.\n\n        for data, is_new_epoch in dataset:\n\n            # Create feed dictionary for next mini batch\n            inputs, labels_true, inputs_seq_len, input_names = data\n\n            if input_names[0][0] not in result_dict.keys():\n                continue\n\n            output, output_pos = result_dict[input_names[0][0]]\n\n            detected_f_num = output_pos.count('f')\n            detected_d_num = output_pos.count('d')\n\n            # if detected_d_num != 0:\n            #     print(output_pos)\n            #     print(output)\n            #     str_true = labels_true[0][0][0]\n            #     print(str_true)\n            #     print('-----')\n\n            true_f_num = np.sum(labels_true[0][0][0].count('f'))\n            true_d_num = np.sum(labels_true[0][0][0].count('d'))\n\n            # Filler\n            if detected_f_num <= true_f_num:\n                tp_f += detected_f_num\n                fn_f += true_f_num - detected_f_num\n            else:\n                tp_f += true_f_num\n                fp_f += detected_f_num - true_f_num\n\n            # Disfluency\n            if detected_d_num <= true_d_num:\n                tp_d += detected_d_num\n                fn_d += true_d_num - detected_d_num\n            else:\n                tp_d += true_d_num\n                fp_d += detected_d_num - true_d_num\n\n        r_f = tp_f / (tp_f + fn_f) if (tp_f + fn_f) != 0 else 0\n        p_f = tp_f / (tp_f + fp_f) if (tp_f + fp_f) != 0 else 0\n        f_f = 2 * r_f * p_f / (r_f + p_f) if (r_f + p_f) != 0 else 0\n        if data_type != 'eval3':\n            mean_f_f += f_f\n\n        r_d = tp_d / (tp_d + fn_d) if (tp_d + fn_d) != 0 else 0\n        p_d = tp_d / (tp_d + fp_d) if (tp_d + fp_d) != 0 else 0\n        f_d = 2 * r_d * p_d / (r_d + p_d) if (r_d + p_d) != 0 else 0\n        if data_type != 'eval3':\n            mean_f_d += f_d\n\n        acc_f = [p_f, r_f, f_f]\n        acc_d = [p_d, r_d, f_d]\n\n        df_acc = pd.DataFrame({'Filler': acc_f, 'Disfluency': acc_d},\n                              columns=['Filler', 'Disfluency'],\n                              index=['Precision', 'Recall', 'F-measure'])\n        print(df_acc)\n\n    print('Mean F-measure (filler): %f' % (mean_f_f / 2.))\n    print('Mean F-measure (disfluency): %f' % (mean_f_d / 2.))\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/csj/fine_tuning/finetune_ctc_dialog.py,12,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Fine tune CTC network (CSJ corpus, for dialog).""""""\n\nimport os\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\nimport tensorflow.contrib.slim as slim\n\nsys.path.append(\'../\')\nsys.path.append(\'../../\')\nsys.path.append(\'../../../\')\nfrom data.read_dataset_ctc import DataSet\nfrom data.read_dataset_ctc_dialog import DataSet as DataSetDialog\nfrom models.ctc.load_model import load\nfrom evaluation.eval_ctc import do_eval_per, do_eval_cer\nfrom evaluation.eval_ctc_dialog import do_eval_fmeasure\nfrom utils.data.sparsetensor import list2sparsetensor, sparsetensor2list\nfrom utils.util import mkdir, join\nfrom utils.parameter import count_total_parameters\nfrom utils.loss import save_loss\nfrom utils.labels.phone import num2phone\nfrom utils.labels.character import num2char\n\n\ndef do_fine_tune(network, optimizer, learning_rate, batch_size, epoch_num,\n                 label_type, num_stack, num_skip, social_signal_type,\n                 trained_model_path, restore_epoch=None):\n    """"""Run training.\n    Args:\n        network: network to train\n        optimizer: adam or adadelta or rmsprop\n        learning_rate: initial learning rate\n        batch_size: size of mini batch\n        epoch_num: epoch num to train\n        label_type: phone or character\n        num_stack: int, the number of frames to stack\n        num_skip: int, the number of frames to skip\n        social_signal_type: insert or insert2 or insert3 or remove\n        trained_model_path: path to the pre-trained model\n        restore_epoch: epoch of the model to restore\n    """"""\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n        # Read dataset\n        train_data = DataSetDialog(data_type=\'train\', label_type=label_type,\n                                   social_signal_type=social_signal_type,\n                                   num_stack=num_stack, num_skip=num_skip,\n                                   is_sorted=True)\n        dev_data = DataSetDialog(data_type=\'dev\', label_type=label_type,\n                                 social_signal_type=social_signal_type,\n                                 num_stack=num_stack, num_skip=num_skip,\n                                 is_sorted=False)\n        test_data = DataSetDialog(data_type=\'test\', label_type=label_type,\n                                  social_signal_type=social_signal_type,\n                                  num_stack=num_stack, num_skip=num_skip,\n                                  is_sorted=False)\n        # TODO\xef\xbc\x9a\xe4\xbd\x9c\xe3\x82\x8b\n        # eval1_data = DataSet(data_type=\'eval1\', label_type=label_type,\n        #                      social_signal_type=social_signal_type,\n        #                      num_stack=num_stack, num_skip=num_skip,\n        #                      is_sorted=False)\n        # eval2_data = DataSet(data_type=\'eval2\', label_type=label_type,\n        #                      social_signal_type=social_signal_type,\n        #                      num_stack=num_stack, num_skip=num_skip,\n        #                      is_sorted=False)\n        # eval3_data = DataSet(data_type=\'eval3\', label_type=label_type,\n        #                      social_signal_type=social_signal_type,\n        #                      num_stack=num_stack, num_skip=num_skip,\n        #                      is_sorted=False)\n\n        # Add to the graph each operation\n        loss_op = network.loss()\n        train_op = network.train(optimizer=optimizer,\n                                 learning_rate_init=learning_rate,\n                                 is_scheduled=False)\n        decode_op = network.decoder(decode_type=\'beam_search\',\n                                    beam_width=20)\n        per_op = network.ler(decode_op)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(network.summaries_train)\n        summary_dev = tf.summary.merge(network.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()), ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps = []\n        csv_train_loss = []\n        csv_dev_loss = []\n\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                network.model_dir, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Restore pre-trained model\'s parameters\n            ckpt = tf.train.get_checkpoint_state(trained_model_path)\n            if ckpt:\n                # Use last saved model\n                model_path = ckpt.model_checkpoint_path\n                if restore_epoch is not None:\n                    model_path = model_path.split(\'/\')[:-1]\n                    model_path = \'/\'.join(model_path) + \\\n                        \'/model.ckpt-\' + str(restore_epoch)\n            else:\n                raise ValueError(\'There are not any checkpoints.\')\n            exclude = [\'output/Variable\', \'output/Variable_1\']\n            variables_to_restore = slim.get_variables_to_restore(\n                exclude=exclude)\n            restorer = tf.train.Saver(variables_to_restore)\n            restorer.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n\n            # Train model\n            iter_per_epoch = int(train_data.data_num / batch_size)\n            if (train_data.data_num / batch_size) != int(train_data.data_num / batch_size):\n                iter_per_epoch += 1\n            max_steps = iter_per_epoch * epoch_num\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            fmean_best = 0\n            for step in range(max_steps):\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels, seq_len, _ = train_data.next_batch(\n                    batch_size=batch_size)\n                indices, values, dense_shape = list2sparsetensor(labels)\n                feed_dict_train = {\n                    network.inputs_pl: inputs,\n                    network.label_indices_pl: indices,\n                    network.label_values_pl: values,\n                    network.label_shape_pl: dense_shape,\n                    network.seq_len_pl: seq_len,\n                    network.keep_prob_input_pl: network.dropout_ratio_input,\n                    network.keep_prob_hidden_pl: network.dropout_ratio_hidden,\n                    network.lr_pl: learning_rate\n                }\n\n                # Create feed dictionary for next mini batch (dev)\n                inputs, labels, seq_len, _ = dev_data.next_batch(\n                    batch_size=batch_size)\n                indices, values, dense_shape = list2sparsetensor(labels)\n                feed_dict_dev = {\n                    network.inputs_pl: inputs,\n                    network.label_indices_pl: indices,\n                    network.label_values_pl: values,\n                    network.label_shape_pl: dense_shape,\n                    network.seq_len_pl: seq_len,\n                    network.keep_prob_input_pl: network.dropout_ratio_input,\n                    network.keep_prob_hidden_pl: network.dropout_ratio_hidden\n                }\n\n                # Update parameters & compute loss\n                _, loss_train = sess.run(\n                    [train_op, loss_op], feed_dict=feed_dict_train)\n                loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                csv_steps.append(step)\n                csv_train_loss.append(loss_train)\n                csv_dev_loss.append(loss_dev)\n\n                if (step + 1) % 10 == 0:\n                    # Change feed dict for evaluation\n                    feed_dict_train[network.keep_prob_input_pl] = 1.0\n                    feed_dict_train[network.keep_prob_hidden_pl] = 1.0\n                    feed_dict_dev[network.keep_prob_input_pl] = 1.0\n                    feed_dict_dev[network.keep_prob_hidden_pl] = 1.0\n\n                    # Compute accuracy & \\update event file\n                    ler_train, summary_str_train = sess.run([per_op, summary_train],\n                                                            feed_dict=feed_dict_train)\n                    ler_dev, summary_str_dev, labels_st = sess.run([per_op, summary_dev, decode_op],\n                                                                   feed_dict=feed_dict_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    # Decode\n                    # try:\n                    #     labels_pred = sparsetensor2list(labels_st, batch_size)\n                    # except:\n                    #     labels_pred = [[0] * batch_size]\n\n                    duration_step = time.time() - start_time_step\n                    print(\'Step %d: loss = %.3f (%.3f) / ler = %.4f (%.4f) (%.3f min)\' %\n                          (step + 1, loss_train, loss_dev, ler_train, ler_dev, duration_step / 60))\n\n                    # if label_type == \'character\':\n                    #     if social_signal_type == \'remove\':\n                    #         map_file_path = \'../evaluation/mapping_files/ctc/char2num_remove.txt\'\n                    #     else:\n                    #         map_file_path = \'../evaluation/mapping_files/ctc/char2num_\' + \\\n                    #             social_signal_type + \'.txt\'\n                    #     print(\'True: %s\' % num2char(labels[-1], map_file_path))\n                    #     print(\'Pred: %s\' % num2char(\n                    #         labels_pred[-1], map_file_path))\n                    # elif label_type == \'phone\':\n                    #     if social_signal_type == \'remove\':\n                    #         map_file_path = \'../evaluation/mapping_files/ctc/phone2num_remove.txt\'\n                    #     else:\n                    #         map_file_path = \'../evaluation/mapping_files/ctc/phone2num_\' + \\\n                    #             social_signal_type + \'.txt\'\n                    #     print(\'True: %s\' % num2phone(\n                    #         labels[-1], map_file_path))\n                    #     print(\'Pred: %s\' % num2phone(\n                    #         labels_pred[-1], map_file_path))\n\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if (step + 1) % iter_per_epoch == 0 or (step + 1) == max_steps:\n                    duration_epoch = time.time() - start_time_epoch\n                    epoch = (step + 1) // iter_per_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (epoch, duration_epoch / 60))\n\n                    # Save model (check point)\n                    checkpoint_file = os.path.join(\n                        network.model_dir, \'model.ckpt\')\n                    save_path = saver.save(\n                        sess, checkpoint_file, global_step=epoch)\n                    print(""Model saved in file: %s"" % save_path)\n\n                    start_time_eval = time.time()\n                    if label_type == \'character\':\n                        print(\'\xe2\x96\xa0Dev Evaluation:\xe2\x96\xa0\')\n                        fmean_epoch = do_eval_fmeasure(session=sess, decode_op=decode_op,\n                                                       network=network, dataset=dev_data,\n                                                       label_type=label_type,\n                                                       social_signal_type=social_signal_type)\n                        # error_epoch = do_eval_cer(session=sess,\n                        #                           decode_op=decode_op,\n                        #                           network=network,\n                        #                           dataset=dev_data,\n                        #                           eval_batch_size=batch_size)\n\n                        if fmean_epoch > fmean_best:\n                            fmean_best = fmean_epoch\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (F-measure)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            do_eval_fmeasure(session=sess, decode_op=decode_op,\n                                             network=network, dataset=test_data,\n                                             label_type=label_type,\n                                             social_signal_type=social_signal_type)\n                            # print(\'\xe2\x96\xa0eval1 Evaluation:\xe2\x96\xa0\')\n                            # do_eval_cer(session=sess, decode_op=decode_op,\n                            #             network=network, dataset=eval1_data,\n                            #             eval_batch_size=batch_size)\n                            # print(\'\xe2\x96\xa0eval2 Evaluation:\xe2\x96\xa0\')\n                            # do_eval_cer(session=sess, decode_op=decode_op,\n                            #             network=network, dataset=eval2_data,\n                            #             eval_batch_size=batch_size)\n                            # print(\'\xe2\x96\xa0eval3 Evaluation:\xe2\x96\xa0\')\n                            # do_eval_cer(session=sess, decode_op=decode_op,\n                            #             network=network, dataset=eval3_data,\n                            #             eval_batch_size=batch_size)\n\n                    else:\n                        print(\'\xe2\x96\xa0Dev Evaluation:\xe2\x96\xa0\')\n                        fmean_epoch = do_eval_fmeasure(session=sess, decode_op=decode_op,\n                                                       network=network, dataset=dev_data,\n                                                       label_type=label_type,\n                                                       social_signal_type=social_signal_type)\n                        # error_epoch = do_eval_per(session=sess,\n                        #                           per_op=per_op,\n                        #                           network=network,\n                        #                           dataset=dev_data,\n                        #                           eval_batch_size=batch_size)\n\n                        if fmean_epoch < fmean_best:\n                            fmean_best = fmean_epoch\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (F-measure)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            do_eval_fmeasure(session=sess, decode_op=decode_op,\n                                             network=network, dataset=test_data,\n                                             label_type=label_type,\n                                             social_signal_type=social_signal_type)\n                            # print(\'\xe2\x96\xa0eval1 Evaluation:\xe2\x96\xa0\')\n                            # do_eval_per(session=sess, per_op=per_op,\n                            #             network=network, dataset=eval1_data,\n                            #             eval_batch_size=batch_size)\n                            # print(\'\xe2\x96\xa0eval2 Evaluation:\xe2\x96\xa0\')\n                            # do_eval_per(session=sess, per_op=per_op,\n                            #             network=network, dataset=eval2_data,\n                            #             eval_batch_size=batch_size)\n                            # print(\'\xe2\x96\xa0eval3 Evaluation:\xe2\x96\xa0\')\n                            # do_eval_per(session=sess, per_op=per_op,\n                            #             network=network, dataset=eval3_data,\n                            #             eval_batch_size=batch_size)\n\n                    duration_eval = time.time() - start_time_eval\n                    print(\'Evaluation time: %.3f min\' %\n                          (duration_eval / 60))\n\n                    start_time_epoch = time.time()\n                    start_time_step = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Save train & dev loss\n            save_loss(csv_steps, csv_train_loss, csv_dev_loss,\n                      save_path=network.model_dir)\n\n            # Training was finished correctly\n            with open(os.path.join(network.model_dir, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, trained_model_path):\n\n    restore_epoch = None  # if None, restore the final epoch\n\n    # Read a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        corpus = config[\'corpus\']\n        feature = config[\'feature\']\n        param = config[\'param\']\n\n    if corpus[\'label_type\'] == \'phone\':\n        if corpus[\'social_signal_type\'] in [\'insert\', \'insert3\']:\n            output_size = 41\n        elif corpus[\'social_signal_type\'] == \'insert2\':\n            output_size = 44\n        elif corpus[\'social_signal_type\'] == \'remove\':\n            output_size = 38\n    elif corpus[\'label_type\'] == \'character\':\n        if corpus[\'social_signal_type\'] in [\'insert\', \'insert3\']:\n            output_size = 150\n        elif corpus[\'social_signal_type\'] == \'insert2\':\n            output_size = 153\n        elif corpus[\'social_signal_type\'] == \'remove\':\n            output_size = 147\n\n    # Load model\n    CTCModel = load(model_type=config[\'model_name\'])\n    network = CTCModel(batch_size=param[\'batch_size\'],\n                       input_size=feature[\'input_size\'] * feature[\'num_stack\'],\n                       num_cell=param[\'num_cell\'],\n                       num_layer=param[\'num_layer\'],\n                       output_size=output_size,\n                       clip_gradients=param[\'clip_grad\'],\n                       clip_activation=param[\'clip_activation\'],\n                       dropout_ratio_input=param[\'dropout_input\'],\n                       dropout_ratio_hidden=param[\'dropout_hidden\'],\n                       num_proj=param[\'num_proj\'],\n                       weight_decay=param[\'weight_decay\'])\n\n    network.model_name = config[\'model_name\'].upper()\n    network.model_name += \'_\' + str(param[\'num_cell\'])\n    network.model_name += \'_\' + str(param[\'num_layer\'])\n    network.model_name += \'_\' + param[\'optimizer\']\n    network.model_name += \'_lr\' + str(param[\'learning_rate\'])\n    if feature[\'num_stack\'] != 1:\n        network.model_name += \'_stack\' + str(feature[\'num_stack\'])\n    network.model_name += \'_transfer_\' + corpus[\'transfer_data_size\']\n\n    # Set save path\n    network.model_dir = mkdir(\'/n/sd8/inaguma/result/csj/dialog/\')\n    network.model_dir = join(network.model_dir, \'ctc\')\n    network.model_dir = join(network.model_dir, corpus[\'label_type\'])\n    network.model_dir = join(network.model_dir, corpus[\'social_signal_type\'])\n    network.model_dir = join(network.model_dir, network.model_name)\n\n    # Reset model directory\n    if not os.path.isfile(os.path.join(network.model_dir, \'complete.txt\')):\n        tf.gfile.DeleteRecursively(network.model_dir)\n        tf.gfile.MakeDirs(network.model_dir)\n    else:\n        raise ValueError(\'File exists.\')\n\n    # Set process name\n    setproctitle(\'ctc_csj_dialog_\' + corpus[\'label_type\'] + \'_\' +\n                 param[\'optimizer\'] + \'_\' + corpus[\'social_signal_type\'] +\n                 \'_transfer_\' + corpus[\'transfer_data_size\'])\n\n    # Save config file\n    shutil.copyfile(config_path, os.path.join(network.model_dir, \'config.yml\'))\n\n    sys.stdout = open(os.path.join(network.model_dir, \'train.log\'), \'w\')\n    print(network.model_name)\n    do_fine_tune(network=network,\n                 optimizer=param[\'optimizer\'],\n                 learning_rate=param[\'learning_rate\'],\n                 batch_size=param[\'batch_size\'],\n                 epoch_num=param[\'num_epoch\'],\n                 label_type=corpus[\'label_type\'],\n                 num_stack=feature[\'num_stack\'],\n                 num_skip=feature[\'num_skip\'],\n                 social_signal_type=corpus[\'social_signal_type\'],\n                 trained_model_path=trained_model_path,\n                 restore_epoch=restore_epoch)\n    sys.stdout = sys.__stdout__\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        ValueError(\n            \'Usage: python fine_tune_ctc.py path_to_config path_to_trained_model\')\n\n    main(config_path=args[1], trained_model_path=args[2])\n'"
examples/csj/metrics/__init__.py,0,b''
examples/csj/metrics/attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for the Attention-based model (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nfrom tqdm import tqdm\n\nfrom utils.io.labels.character import Idx2char\nfrom utils.evaluation.edit_distance import compute_cer\n\n\ndef do_eval_cer(session, decode_ops, model, dataset, label_type,\n                train_data_size,\n                is_test=False, eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        session: session of training model\n        decode_ops (list): operations for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kanji or kanji or kanji_divide or kana_divide\n        train_data_size (string): train_subset or train_fullset\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Return:\n        cer_mean (float): An average of CER\n    """"""\n    assert isinstance(decode_ops, list), ""decode_ops must be a list.""\n\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    if \'kanji\' in label_type:\n        map_file_path = \'../metrics/mapping_files/\' + \\\n            label_type + \'_\' + train_data_size + \'.txt\'\n    elif \'kana\' in label_type:\n        map_file_path = \'../metrics/mapping_files/\' + label_type + \'.txt\'\n    else:\n        raise TypeError\n\n    idx2char = Idx2char(map_file_path=map_file_path)\n\n    cer_mean = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, labels_seq_len,  _ = data\n\n        feed_dict = {}\n        for i_device in range(len(decode_ops)):\n            feed_dict[model.inputs_pl_list[i_device]] = inputs[i_device]\n            feed_dict[model.inputs_seq_len_pl_list[i_device]\n                      ] = inputs_seq_len[i_device]\n            feed_dict[model.keep_prob_encoder_pl_list[i_device]] = 1.0\n            feed_dict[model.keep_prob_decoder_pl_list[i_device]] = 1.0\n            feed_dict[model.keep_prob_embedding_pl_list[i_device]] = 1.0\n\n        labels_pred_list = session.run(decode_ops, feed_dict=feed_dict)\n        for i_device in range(len(labels_pred_list)):\n            for i_batch in range(len(inputs[i_device])):\n\n                # Convert from list of index to string\n                if is_test:\n                    str_true = labels_true[i_device][i_batch][0]\n                    # NOTE: transcript is seperated by space(\'_\')\n                else:\n                    str_true = idx2char(\n                        labels_true[i_device][i_batch][1:labels_seq_len[i_device][i_batch] - 1])\n                str_pred = idx2char(\n                    labels_pred_list[i_device][i_batch]).split(\'>\')[0]\n                # NOTE: Trancate by <EOS>\n\n                # Remove garbage labels\n                str_true = re.sub(r\'[_NZ\xe3\x83\xbc\xe3\x83\xbb<>]+\', \'\', str_true)\n                str_pred = re.sub(r\'[_NZ\xe3\x83\xbc\xe3\x83\xbb<>]+\', \'\', str_pred)\n\n                # Compute CER\n                cer_mean += compute_cer(str_pred=str_pred,\n                                        str_true=str_true,\n                                        normalize=True)\n\n                if progressbar:\n                    pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= len(dataset)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return cer_mean\n'"
examples/csj/metrics/ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for the CTC model (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nfrom tqdm import tqdm\n\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.sparsetensor import sparsetensor2list\nfrom utils.evaluation.edit_distance import compute_cer\n\n\ndef do_eval_cer(session, decode_ops, model, dataset, label_type,\n                train_data_size,\n                is_test=False, eval_batch_size=None, progressbar=False,\n                is_multitask=False, is_main=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        session: session of training model\n        decode_ops (list): operations for decoding\n        model: the model to evaluate\n        dataset: An instance of `Dataset` class\n        label_type (string): kanji or kanji or kanji_divide or kana_divide\n        train_data_size (string): train_subset or train_fullset\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize progressbar\n        is_multitask (bool, optional): if True, evaluate the multitask model\n        is_main (bool, optional): if True, evaluate the main task\n    Return:\n        cer_mean (float): An average of CER\n    """"""\n    # NOTE: add multitask version\n\n    assert isinstance(decode_ops, list), ""decode_ops must be a list.""\n\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    if \'kanji\' in label_type:\n        map_file_path = \'../metrics/mapping_files/\' + \\\n            label_type + \'_\' + train_data_size + \'.txt\'\n    elif \'kana\' in label_type:\n        map_file_path = \'../metrics/mapping_files/\' + label_type + \'.txt\'\n    else:\n        raise TypeError\n\n    idx2char = Idx2char(map_file_path=map_file_path)\n\n    cer_mean = 0\n    skip_data_num = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, _ = data\n\n        feed_dict = {}\n        for i_device in range(len(decode_ops)):\n            feed_dict[model.inputs_pl_list[i_device]] = inputs[i_device]\n            feed_dict[model.inputs_seq_len_pl_list[i_device]\n                      ] = inputs_seq_len[i_device]\n            feed_dict[model.keep_prob_pl_list[i_device]] = 1.0\n\n        labels_pred_st_list = session.run(decode_ops, feed_dict=feed_dict)\n        for i_device in range(len(labels_pred_st_list)):\n            batch_size_device = len(inputs[i_device])\n            try:\n                labels_pred = sparsetensor2list(labels_pred_st_list[i_device],\n                                                batch_size_device)\n                for i_batch in range(batch_size_device):\n\n                    # Convert from list of index to string\n                    if is_test:\n                        str_true = labels_true[i_device][i_batch][0]\n                        # NOTE: transcript may be seperated by space(\'_\')\n                    else:\n                        str_true = idx2char(labels_true[i_device][i_batch])\n                    str_pred = idx2char(labels_pred[i_batch])\n\n                    # Remove garbage labels\n                    str_true = re.sub(r\'[_NZ\xe3\x83\xbc\xe3\x83\xbb]+\', \'\', str_true)\n                    str_pred = re.sub(r\'[_NZ\xe3\x83\xbc\xe3\x83\xbb]+\', \'\', str_pred)\n\n                    # Compute CER\n                    cer_mean += compute_cer(str_pred=str_pred,\n                                            str_true=str_true,\n                                            normalize=True)\n\n                    if progressbar:\n                        pbar.update(1)\n            except:\n                print(\'skipped\')\n                skip_data_num += batch_size_device\n                # TODO: Conduct decoding again with batch size 1\n\n                if progressbar:\n                    pbar.update(batch_size_device)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= (len(dataset) - skip_data_num)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return cer_mean\n'"
examples/csj/training/train_attention.py,20,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the Attention-based model (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.csj.data.load_dataset_attention import Dataset\nfrom experiments.csj.metrics.attention import do_eval_cer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.training.multi_gpu import average_gradients\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\n\n\ndef do_train(model, params, gpu_indices):\n    """"""Run training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n        gpu_indices (list): GPU indices\n    """"""\n    if \'kanji\' in params[\'label_type\']:\n        map_file_path = \'../metrics/mapping_files/\' + \\\n            params[\'label_type\'] + \'_\' + params[\'train_data_size\'] + \'.txt\'\n    elif \'kana\' in params[\'label_type\']:\n        map_file_path = \'../metrics/mapping_files/\' + \\\n            params[\'label_type\'] + \'.txt\'\n\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'], map_file_path=map_file_path,\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'],\n        num_gpu=len(gpu_indices))\n    dev_data = Dataset(\n        data_type=\'dev\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'], map_file_path=map_file_path,\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False, num_gpu=len(gpu_indices))\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        # Create a variable to track the global step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        # Set optimizer\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n        optimizer = model._set_optimizer(\n            params[\'optimizer\'], learning_rate_pl)\n\n        # Calculate the gradients for each model tower\n        total_grads_and_vars, total_losses = [], []\n        decode_ops_infer, ler_ops = [], []\n        all_devices = [\'/gpu:%d\' % i_gpu for i_gpu in range(len(gpu_indices))]\n        # NOTE: /cpu:0 is prepared for evaluation\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i_gpu in range(len(all_devices)):\n                with tf.device(all_devices[i_gpu]):\n                    with tf.name_scope(\'tower_gpu%d\' % i_gpu) as scope:\n\n                        # Define placeholders in each tower\n                        model.create_placeholders()\n\n                        # Calculate the total loss for the current tower of the\n                        # model. This function constructs the entire model but\n                        # shares the variables across all towers.\n                        tower_loss, tower_logits, tower_decoder_outputs_train, tower_decoder_outputs_infer = model.compute_loss(\n                            model.inputs_pl_list[i_gpu],\n                            model.labels_pl_list[i_gpu],\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            model.labels_seq_len_pl_list[i_gpu],\n                            model.keep_prob_encoder_pl_list[i_gpu],\n                            model.keep_prob_decoder_pl_list[i_gpu],\n                            model.keep_prob_embedding_pl_list[i_gpu],\n                            scope)\n                        tower_loss = tf.expand_dims(tower_loss, axis=0)\n                        total_losses.append(tower_loss)\n\n                        # Reuse variables for the next tower\n                        tf.get_variable_scope().reuse_variables()\n\n                        # Calculate the gradients for the batch of data on this\n                        # tower\n                        tower_grads_and_vars = optimizer.compute_gradients(\n                            tower_loss)\n\n                        # Gradient clipping\n                        tower_grads_and_vars = model._clip_gradients(\n                            tower_grads_and_vars)\n\n                        # TODO: Optionally add gradient noise\n\n                        # Keep track of the gradients across all towers\n                        total_grads_and_vars.append(tower_grads_and_vars)\n\n                        # Add to the graph each operation per tower\n                        _, decode_op_tower_infer = model.decode(\n                            tower_decoder_outputs_train,\n                            tower_decoder_outputs_infer)\n                        decode_ops_infer.append(decode_op_tower_infer)\n                        # ler_op_tower = model.compute_ler(\n                        #     decode_op_tower, model.labels_pl_list[i_gpu])\n                        ler_op_tower = model.compute_ler(\n                            model.labels_st_true_pl_list[i_gpu],\n                            model.labels_st_pred_pl_list[i_gpu])\n                        ler_op_tower = tf.expand_dims(ler_op_tower, axis=0)\n                        ler_ops.append(ler_op_tower)\n\n        # Aggregate losses, then calculate average loss\n        total_losses = tf.concat(axis=0, values=total_losses)\n        loss_op = tf.reduce_mean(total_losses, axis=0)\n        ler_ops = tf.concat(axis=0, values=ler_ops)\n        ler_op = tf.reduce_mean(ler_ops, axis=0)\n\n        # We must calculate the mean of each gradient. Note that this is the\n        # synchronization point across all towers\n        average_grads_and_vars = average_gradients(total_grads_and_vars)\n\n        # Apply the gradients to adjust the shared variables.\n        train_op = optimizer.apply_gradients(average_grads_and_vars,\n                                             global_step=global_step)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" %\n                  (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        # NOTE: Start running operations on the Graph. allow_soft_placement\n        # must be set to True to build towers on GPU, as some of the ops do not\n        # have GPU implementations.\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                              log_device_placement=False)) as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize param\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            cer_dev_best = 1\n            not_improved_epoch = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels_train, inputs_seq_len, labels_seq_len, _ = data\n                feed_dict_train = {}\n                for i_gpu in range(len(gpu_indices)):\n                    feed_dict_train[model.inputs_pl_list[i_gpu]\n                                    ] = inputs[i_gpu]\n                    feed_dict_train[model.labels_pl_list[i_gpu]\n                                    ] = labels_train[i_gpu]\n                    feed_dict_train[model.inputs_seq_len_pl_list[i_gpu]\n                                    ] = inputs_seq_len[i_gpu]\n                    feed_dict_train[model.labels_seq_len_pl_list[i_gpu]\n                                    ] = labels_seq_len[i_gpu]\n                    feed_dict_train[model.keep_prob_encoder_pl_list[i_gpu]\n                                    ] = 1 - float(params[\'dropout_encoder\'])\n                    feed_dict_train[model.keep_prob_decoder_pl_list[i_gpu]\n                                    ] = 1 - float(params[\'dropout_decoder\'])\n                    feed_dict_train[model.keep_prob_embedding_pl_list[i_gpu]\n                                    ] = 1 - float(params[\'dropout_embedding\'])\n                feed_dict_train[learning_rate_pl] = learning_rate\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % int(params[\'print_step\'] / len(gpu_indices)) == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    inputs, labels_dev, inputs_seq_len, labels_seq_len, _ = dev_data.next()[\n                        0]\n                    feed_dict_dev = {}\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_dev[model.inputs_pl_list[i_gpu]\n                                      ] = inputs[i_gpu]\n                        feed_dict_dev[model.labels_pl_list[i_gpu]\n                                      ] = labels_dev[i_gpu]\n                        feed_dict_dev[model.inputs_seq_len_pl_list[i_gpu]\n                                      ] = inputs_seq_len[i_gpu]\n                        feed_dict_dev[model.labels_seq_len_pl_list[i_gpu]\n                                      ] = labels_seq_len[i_gpu]\n                        feed_dict_dev[model.keep_prob_encoder_pl_list[i_gpu]\n                                      ] = 1.0\n                        feed_dict_dev[model.keep_prob_decoder_pl_list[i_gpu]\n                                      ] = 1.0\n                        feed_dict_dev[model.keep_prob_embedding_pl_list[i_gpu]\n                                      ] = 1.0\n\n                    # Compute loss\n                    loss_train = sess.run(\n                        loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_train[model.keep_prob_encoder_pl_list[i_gpu]] = 1.0\n                        feed_dict_train[model.keep_prob_decoder_pl_list[i_gpu]] = 1.0\n                        feed_dict_train[model.keep_prob_embedding_pl_list[i_gpu]] = 1.0\n\n                    # Predict class ids\n                    predicted_ids_train_list, summary_str_train = sess.run(\n                        [decode_ops_infer, summary_train], feed_dict=feed_dict_train)\n                    predicted_ids_dev_list, summary_str_dev = sess.run(\n                        [decode_ops_infer, summary_dev], feed_dict=feed_dict_dev)\n\n                    # Convert to sparsetensor to compute LER\n                    feed_dict_ler_train = {}\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_ler_train[model.labels_st_true_pl_list[i_gpu]] = list2sparsetensor(\n                            labels_train[i_gpu],\n                            padded_value=train_data.padded_value),\n                        feed_dict_ler_train[model.labels_st_pred_pl_list[i_gpu]] = list2sparsetensor(\n                            predicted_ids_train_list[i_gpu],\n                            padded_value=train_data.padded_value)\n                    feed_dict_ler_dev = {}\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_ler_dev[model.labels_st_true_pl_list[i_gpu]] = list2sparsetensor(\n                            labels_dev[i_gpu],\n                            padded_value=dev_data.padded_value),\n                        feed_dict_ler_dev[model.labels_st_pred_pl_list[i_gpu]] = list2sparsetensor(\n                            predicted_ids_dev_list[i_gpu],\n                            padded_value=dev_data.padded_value)\n\n                    # Compute accuracy\n                    # ler_train = sess.run(ler_op, feed_dict=feed_dict_ler_train)\n                    # ler_dev = sess.run(ler_op, feed_dict=feed_dict_ler_dev)\n                    ler_train = 1\n                    ler_dev = 1\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n                    # TODO: fix this\n\n                    # Update even files\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        cer_dev_epoch = do_eval_cer(\n                            session=sess,\n                            decode_ops=decode_ops_infer,\n                            model=model,\n                            dataset=dev_data,\n                            label_type=params[\'label_type\'],\n                            train_data_size=params[\'train_data_size\'],\n                            eval_batch_size=1)\n                        print(\'  CER: %f %%\' % (cer_dev_epoch * 100))\n\n                        if cer_dev_epoch < cer_dev_best:\n                            cer_dev_best = cer_dev_epoch\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n                        else:\n                            not_improved_epoch += 1\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Early stopping\n                        if not_improved_epoch == params[\'not_improved_patient_epoch\']:\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=cer_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path, gpu_indices):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'label_type\'] == \'kana\':\n        params[\'num_classes\'] = 146\n    elif params[\'label_type\'] == \'kana_divide\':\n        params[\'num_classes\'] = 147\n    elif params[\'label_type\'] == \'kanji\':\n        if params[\'train_data_size\'] == \'train_subset\':\n            params[\'num_classes\'] = 2981\n        elif params[\'train_data_size\'] == \'train_fullset\':\n            params[\'num_classes\'] = 3385\n    elif params[\'label_type\'] == \'kanji_divide\':\n        if params[\'train_data_size\'] == \'train_subset\':\n            params[\'num_classes\'] = 2982\n        elif params[\'train_data_size\'] == \'train_fullset\':\n            params[\'num_classes\'] = 3386\n    else:\n        raise TypeError\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'],\n        sigmoid_smoothing=params[\'sigmoid_smoothing\'])\n\n    # Set process name\n    setproctitle(\'tf_csj_\' + model.name + \'_\' +\n                 params[\'train_data_size\'] + \'_\' + params[\'label_type\'] + \'_\' +\n                 params[\'attention_type\'])\n\n    model.name = \'en\' + str(params[\'encoder_num_units\'])\n    model.name += \'_\' + str(params[\'encoder_num_layers\'])\n    model.name += \'_att\' + str(params[\'attention_dim\'])\n    model.name += \'_de\' + str(params[\'decoder_num_units\'])\n    model.name += \'_\' + str(params[\'decoder_num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    model.name += \'_\' + params[\'attention_type\']\n    if params[\'dropout_encoder\'] != 0:\n        model.name += \'_dropen\' + str(params[\'dropout_encoder\'])\n    if params[\'dropout_decoder\'] != 0:\n        model.name += \'_dropde\' + str(params[\'dropout_decoder\'])\n    if params[\'dropout_embedding\'] != 0:\n        model.name += \'_dropem\' + str(params[\'dropout_embedding\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'wd\' + str(params[\'weight_decay\'])\n    if params[\'sharpening_factor\'] != 1:\n        model.name += \'_sharp\' + str(params[\'sharpening_factor\'])\n    if params[\'logits_temperature\'] != 1:\n        model.name += \'_temp\' + str(params[\'logits_temperature\'])\n    if bool(params[\'sigmoid_smoothing\']):\n        model.name += \'_smoothing\'\n    if len(gpu_indices) >= 2:\n        model.name += \'_gpu\' + str(len(gpu_indices))\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'attention\', params[\'label_type\'],\n        params[\'train_data_size\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params, gpu_indices=gpu_indices)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3 and len(args) != 4:\n        raise ValueError\n    main(config_path=args[1], model_save_path=args[2],\n         gpu_indices=list(map(int, args[3].split(\',\'))))\n'"
examples/csj/training/train_ctc.py,20,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the CTC model (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.csj.data.load_dataset_ctc import Dataset\nfrom experiments.csj.metrics.ctc import do_eval_cer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.training.multi_gpu import average_gradients\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.ctc import CTC\n\n\ndef do_train(model, params, gpu_indices):\n    """"""Run training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n        gpu_indices (list): GPU indices\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'],\n        num_gpu=len(gpu_indices))\n    dev_data = Dataset(\n        data_type=\'dev\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False, num_gpu=len(gpu_indices))\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        # Create a variable to track the global step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        # Set optimizer\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n        optimizer = model._set_optimizer(\n            params[\'optimizer\'], learning_rate_pl)\n\n        # Calculate the gradients for each model tower\n        total_grads_and_vars, total_losses = [], []\n        decode_ops, ler_ops = [], []\n        all_devices = [\'/gpu:%d\' % i_gpu for i_gpu in range(len(gpu_indices))]\n        # NOTE: /cpu:0 is prepared for evaluation\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i_gpu in range(len(all_devices)):\n                with tf.device(all_devices[i_gpu]):\n                    with tf.name_scope(\'tower_gpu%d\' % i_gpu) as scope:\n\n                        # Define placeholders in each tower\n                        model.create_placeholders()\n\n                        # Calculate the total loss for the current tower of the\n                        # model. This function constructs the entire model but\n                        # shares the variables across all towers.\n                        tower_loss, tower_logits = model.compute_loss(\n                            model.inputs_pl_list[i_gpu],\n                            model.labels_pl_list[i_gpu],\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            model.keep_prob_pl_list[i_gpu],\n                            scope)\n                        tower_loss = tf.expand_dims(tower_loss, axis=0)\n                        total_losses.append(tower_loss)\n\n                        # Reuse variables for the next tower\n                        tf.get_variable_scope().reuse_variables()\n\n                        # Calculate the gradients for the batch of data on this\n                        # tower\n                        tower_grads_and_vars = optimizer.compute_gradients(\n                            tower_loss)\n\n                        # Gradient clipping\n                        tower_grads_and_vars = model._clip_gradients(\n                            tower_grads_and_vars)\n\n                        # TODO: Optionally add gradient noise\n\n                        # Keep track of the gradients across all towers\n                        total_grads_and_vars.append(tower_grads_and_vars)\n\n                        # Add to the graph each operation per tower\n                        decode_op_tower = model.decoder(\n                            tower_logits,\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            beam_width=params[\'beam_width\'])\n                        decode_ops.append(decode_op_tower)\n                        ler_op_tower = model.compute_ler(\n                            decode_op_tower, model.labels_pl_list[i_gpu])\n                        ler_op_tower = tf.expand_dims(ler_op_tower, axis=0)\n                        ler_ops.append(ler_op_tower)\n\n        # Aggregate losses, then calculate average loss\n        total_losses = tf.concat(axis=0, values=total_losses)\n        loss_op = tf.reduce_mean(total_losses, axis=0)\n        ler_ops = tf.concat(axis=0, values=ler_ops)\n        ler_op = tf.reduce_mean(ler_ops, axis=0)\n\n        # We must calculate the mean of each gradient. Note that this is the\n        # synchronization point across all towers\n        average_grads_and_vars = average_gradients(total_grads_and_vars)\n\n        # Apply the gradients to adjust the shared variables.\n        train_op = optimizer.apply_gradients(average_grads_and_vars,\n                                             global_step=global_step)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        # NOTE: Start running operations on the Graph. allow_soft_placement\n        # must be set to True to build towers on GPU, as some of the ops do not\n        # have GPU implementations.\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                              log_device_placement=False)) as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            cer_dev_best = 1\n            not_improved_epoch = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels, inputs_seq_len, _ = data\n                feed_dict_train = {}\n                for i_gpu in range(len(gpu_indices)):\n                    feed_dict_train[model.inputs_pl_list[i_gpu]\n                                    ] = inputs[i_gpu]\n                    feed_dict_train[model.labels_pl_list[i_gpu]] = list2sparsetensor(\n                        labels[i_gpu], padded_value=train_data.padded_value)\n                    feed_dict_train[model.inputs_seq_len_pl_list[i_gpu]\n                                    ] = inputs_seq_len[i_gpu]\n                    feed_dict_train[model.keep_prob_pl_list[i_gpu]\n                                    ] = 1 - float(params[\'dropout\'])\n                feed_dict_train[learning_rate_pl] = learning_rate\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % int(params[\'print_step\'] / len(gpu_indices)) == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    inputs, labels, inputs_seq_len,  _ = dev_data.next()[0]\n                    feed_dict_dev = {}\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_dev[model.inputs_pl_list[i_gpu]\n                                      ] = inputs[i_gpu]\n                        feed_dict_dev[model.labels_pl_list[i_gpu]] = list2sparsetensor(\n                            labels[i_gpu], padded_value=dev_data.padded_value)\n                        feed_dict_dev[model.inputs_seq_len_pl_list[i_gpu]\n                                      ] = inputs_seq_len[i_gpu]\n                        feed_dict_dev[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_train[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute accuracy & update event files\n                    ler_train, summary_str_train = sess.run(\n                        [ler_op, summary_train], feed_dict=feed_dict_train)\n                    ler_dev, summary_str_dev = sess.run(\n                        [ler_op, summary_dev], feed_dict=feed_dict_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        cer_dev_epoch = do_eval_cer(\n                            session=sess,\n                            decode_ops=decode_ops,\n                            model=model,\n                            dataset=dev_data,\n                            label_type=params[\'label_type\'],\n                            train_data_size=params[\'train_data_size\'],\n                            eval_batch_size=1)\n                        print(\'  CER: %f %%\' % (cer_dev_epoch * 100))\n\n                        if cer_dev_epoch < cer_dev_best:\n                            cer_dev_best = cer_dev_epoch\n                            not_improved_epoch = 0\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model only (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n                        else:\n                            not_improved_epoch += 1\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Early stopping\n                        if not_improved_epoch == params[\'not_improved_patient_epoch\']:\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=cer_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.model_dir, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path, gpu_indices):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type\'] == \'kana\':\n        params[\'num_classes\'] = 146\n    elif params[\'label_type\'] == \'kana_divide\':\n        params[\'num_classes\'] = 147\n    elif params[\'label_type\'] == \'kanji\':\n        if params[\'train_data_size\'] == \'train_subset\':\n            params[\'num_classes\'] = 2981\n        elif params[\'train_data_size\'] == \'train_fullset\':\n            params[\'num_classes\'] = 3385\n    elif params[\'label_type\'] == \'kanji_divide\':\n        if params[\'train_data_size\'] == \'train_subset\':\n            params[\'num_classes\'] = 2982\n        elif params[\'train_data_size\'] == \'train_fullset\':\n            params[\'num_classes\'] = 3386\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\n        \'tf_csj_\' + model.name + \'_\' + params[\'train_data_size\'] + \'_\' + params[\'label_type\'])\n\n    model.name += \'_\' + str(params[\'num_units\'])\n    model.name += \'_\' + str(params[\'num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'num_proj\'] != 0:\n        model.name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'dropout\'] != 0:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n    if params[\'bottleneck_dim\'] != 0:\n        model.name += \'_bottle\' + str(params[\'bottleneck_dim\'])\n    if len(gpu_indices) >= 2:\n        model.name += \'_gpu\' + str(len(gpu_indices))\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'ctc\', params[\'label_type\'],\n        params[\'train_data_size\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params, gpu_indices=gpu_indices)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3 and len(args) != 4:\n        raise ValueError\n    main(config_path=args[1], model_save_path=args[2],\n         gpu_indices=list(map(int, args[3].split(\',\'))))\n'"
examples/csj/training/train_multitask_ctc.py,10,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the multi-task CTC model (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(\'../../../\')\nfrom experiments.csj.data.load_dataset_multitask_ctc import Dataset\nfrom experiments.csj.metrics.ctc import do_eval_cer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller.epoch import Controller\n\nfrom utils.directory import mkdir, mkdir_join\nfrom utils.parameter import count_total_parameters\nfrom utils.csv import save_loss, save_ler\nfrom models.ctc.load_model import load\n\n\ndef do_train(model, params):\n    """"""Run training.\n    Args:\n        model: model to train\n        params: A dictionary of parameters\n    """"""\n    # Load dataset\n    train_data = Dataset(data_type=\'train\',\n                         label_type_main=params[\'label_type_main\'],\n                         label_type_sub=params[\'label_type_sub\'],\n                         train_data_size=params[\'train_data_size\'],\n                         batch_size=params[\'batch_size\'],\n                         num_stack=params[\'num_stack\'],\n                         num_skip=params[\'num_skip\'],\n                         sort_utt=True)\n    dev_data_step = Dataset(data_type=\'dev\',\n                            label_type_main=params[\'label_type_main\'],\n                            label_type_sub=params[\'label_type_sub\'],\n                            train_data_size=params[\'train_data_size\'],\n                            batch_size=params[\'batch_size\'],\n                            num_stack=params[\'num_stack\'],\n                            num_skip=params[\'num_skip\'],\n                            sort_utt=False)\n    dev_data_epoch = Dataset(data_type=\'dev\',\n                             label_type_main=params[\'label_type_main\'],\n                             label_type_sub=params[\'label_type_sub\'],\n                             train_data_size=params[\'train_data_size\'],\n                             batch_size=params[\'batch_size\'],\n                             num_stack=params[\'num_stack\'],\n                             num_skip=params[\'num_skip\'],\n                             sort_utt=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders(gpu_index=0)\n\n        # Add to the graph each operation\n        loss_op, logits_main, logits_sub = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.labels_sub_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_input_pl_list[0],\n            model.keep_prob_hidden_pl_list[0],\n            model.keep_prob_output_pl_list[0])\n        train_op = model.train(loss_op,\n                               optimizer=params[\'optimizer\'],\n                               learning_rate=model.learning_rate_pl_list[0])\n        decode_op_main, decode_op_sub = model.decoder(\n            logits_main,\n            logits_sub,\n            model.inputs_seq_len_pl_list[0],\n            decode_type=\'beam_search\',\n            beam_width=20)\n        ler_op_main, ler_op_sub = model.compute_ler(\n            decode_op_main, decode_op_sub,\n            model.labels_pl_list[0], model.labels_sub_pl_list[0])\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=1,\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_main_train, csv_ler_main_dev = [], []\n        csv_ler_sub_train, csv_ler_sub_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Make mini-batch generator\n            mini_batch_train = train_data.next_batch()\n            mini_batch_dev = dev_data_step.next_batch()\n\n            # Train model\n            iter_per_epoch = int(train_data.data_num / params[\'batch_size\'])\n            train_step = train_data.data_num / params[\'batch_size\']\n            if (train_step) != int(train_step):\n                iter_per_epoch += 1\n            max_steps = iter_per_epoch * params[\'num_epoch\']\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            ler_main_dev_best = 1\n            learning_rate = float(params[\'learning_rate\'])\n            for step in range(max_steps):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels_main, labels_sub, inputs_seq_len, _ = mini_batch_train.__next__()\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs,\n                    model.labels_pl_list[0]: list2sparsetensor(labels_main, padded_value=-1),\n                    model.labels_sub_pl_list[0]: list2sparsetensor(labels_sub, padded_value=-1),\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n                    model.keep_prob_input_pl_list[0]: model.dropout_ratio_input,\n                    model.keep_prob_hidden_pl_list[0]: model.dropout_ratio_hidden,\n                    model.keep_prob_output_pl_list[0]: model.dropout_ratio_output,\n                    model.learning_rate_pl_list[0]: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % 200 == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    inputs, labels_main, labels_sub, inputs_seq_len, _ = mini_batch_dev.__next__()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs,\n                        model.labels_pl_list[0]: list2sparsetensor(labels_main, padded_value=-1),\n                        model.labels_sub_pl_list[0]: list2sparsetensor(labels_sub, padded_value=-1),\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n                        model.keep_prob_input_pl_list[0]: 1.0,\n                        model.keep_prob_hidden_pl_list[0]: 1.0,\n                        model.keep_prob_output_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_input_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_hidden_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_output_pl_list[0]] = 1.0\n\n                    # Compute accuracy & update event file\n                    ler_main_train, ler_sub_train, summary_str_train = sess.run(\n                        [ler_op_main, ler_op_sub, summary_train],\n                        feed_dict=feed_dict_train)\n                    ler_main_dev, ler_sub_dev, summary_str_dev = sess.run(\n                        [ler_op_main, ler_op_sub,  summary_dev],\n                        feed_dict=feed_dict_dev)\n                    csv_ler_main_train.append(ler_main_train)\n                    csv_ler_main_dev.append(ler_main_dev)\n                    csv_ler_sub_train.append(ler_sub_train)\n                    csv_ler_sub_dev.append(ler_sub_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(\'Step %d: loss = %.3f (%.3f) / ler_main = %.4f (%.4f) / ler_sub = %.4f (%.4f) (%.3f min)\' %\n                          (step + 1, loss_train, loss_dev, ler_main_train, ler_main_dev,\n                           ler_sub_train, ler_sub_dev, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if (step + 1) % iter_per_epoch == 0 or (step + 1) == max_steps:\n                    duration_epoch = time.time() - start_time_epoch\n                    epoch = (step + 1) // iter_per_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (epoch, duration_epoch / 60))\n\n                    # Save model (check point)\n                    checkpoint_file = join(model.save_path, \'model.ckpt\')\n                    save_path = saver.save(\n                        sess, checkpoint_file, global_step=epoch)\n                    print(""Model saved in file: %s"" % save_path)\n\n                    if epoch >= 5:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Evaluation ===\')\n                        ler_main_dev_epoch = do_eval_cer(\n                            session=sess,\n                            decode_op=decode_op_main,\n                            model=model,\n                            dataset=dev_data_epoch,\n                            label_type=params[\'label_type_main\'],\n                            eval_batch_size=params[\'batch_size\'],\n                            is_multitask=True,\n                            is_main=True)\n                        print(\'  CER (main): %f %%\' %\n                              (ler_main_dev_epoch * 100))\n\n                        ler_sub_dev_epoch = do_eval_cer(\n                            session=sess,\n                            decode_op=decode_op_sub,\n                            model=model,\n                            dataset=dev_data_epoch,\n                            label_type=params[\'label_type_sub\'],\n                            eval_batch_size=params[\'batch_size\'],\n                            is_multitask=True,\n                            is_main=False)\n                        print(\'  CER (sub): %f %%\' %\n                              (ler_sub_dev_epoch * 100))\n\n                        if ler_main_dev_epoch < ler_main_dev_best:\n                            ler_main_dev_best = ler_main_dev_epoch\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER main)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=epoch,\n                            value=ler_main_dev_epoch)\n\n                    start_time_epoch = time.time()\n                    start_time_step = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Save train & dev loss, ler\n            save_loss(csv_steps, csv_loss_train, csv_loss_dev,\n                      save_path=model.save_path)\n            save_ler(csv_steps, csv_ler_main_train, csv_ler_sub_dev,\n                     save_path=model.save_path)\n            save_ler(csv_steps, csv_ler_sub_train, csv_ler_sub_dev,\n                     save_path=model.save_path)\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Read a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type_main\'] == \'kanji\':\n        params[\'num_classes_main\'] = 3386\n    elif params[\'label_type_main\'] == \'kana\':\n        params[\'num_classes_main\'] = 147\n    else:\n        raise TypeError\n\n    if params[\'label_type_sub\'] == \'kana\':\n        params[\'num_classes_sub\'] = 147\n    elif params[\'label_type_sub\'] == \'phone\':\n        params[\'num_classes_sub\'] = 38\n    else:\n        TypeError\n\n    # Model setting\n    model = load(model_type=params[\'model\'])\n    model = model(batch_size=params[\'batch_size\'],\n                  input_size=params[\'input_size\'],\n                  splice=params[\'splice\'],\n                  num_stack=params[\'num_stack\'],\n                  num_units=params[\'num_units\'],\n                  num_layer_main=params[\'num_layer_main\'],\n                  num_layer_sub=params[\'num_layer_sub\'],\n                  #    bottleneck_dim=params[\'bottleneck_dim\'],\n                  num_classes_main=params[\'num_classes_main\'],\n                  num_classes_sub=params[\'num_classes_sub\'],\n                  main_task_weight=params[\'main_task_weight\'],\n                  parameter_init=params[\'weight_init\'],\n                  clip_grad_norm=params[\'clip_grad_norm\'],\n                  clip_activation=params[\'clip_activation\'],\n                  num_proj=params[\'num_proj\'],\n                  weight_decay=params[\'weight_decay\'])\n\n    model.model_name = params[\'model\']\n    model.model_name += \'_\' + str(params[\'num_units\'])\n    model.model_name += \'_main\' + str(params[\'num_layer_main\'])\n    model.model_name += \'_sub\' + str(params[\'num_layer_sub\'])\n    model.model_name += \'_\' + params[\'optimizer\']\n    model.model_name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'bottleneck_dim\'] != 0:\n        model.model_name += \'_bottoleneck\' + str(params[\'bottleneck_dim\'])\n    if params[\'num_proj\'] != 0:\n        model.model_name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'num_stack\'] != 1:\n        model.model_name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.model_name += \'_weightdecay\' + str(params[\'weight_decay\'])\n    model.model_name += \'_taskweight\' + str(params[\'main_task_weight\'])\n    if params[\'train_data_size\'] == \'large\':\n        model.model_name += \'_large\'\n\n    # Set save path\n    model.save_path = mkdir(model_save_path)\n    model.save_path = mkdir_join(model.save_path, \'ctc\')\n    model.save_path = mkdir_join(\n        model.save_path,\n        params[\'label_type_main\'] + \'_\' + params[\'label_type_sub\'])\n    model.save_path = mkdir_join(model.save_path, model.model_name)\n\n    # Reset model directory\n    if not isfile(join(model.save_path, \'complete.txt\')):\n        tf.gfile.DeleteRecursively(model.save_path)\n        tf.gfile.MakeDirs(model.save_path)\n    else:\n        raise ValueError(\'File exists.\')\n\n    # Set process name\n    setproctitle(\'csj_multictc_\' + params[\'label_type_main\'] + \'_\' +\n                 params[\'label_type_sub\'] + \'_\' + params[\'train_data_size\'])\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/csj/visualization/decode_ctc.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decode the trained CTC outputs (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.csj.data.load_dataset_ctc import Dataset\nfrom models.ctc.ctc import CTC\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.sparsetensor import sparsetensor2list\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\n\n\ndef do_decode(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    eval1_data = Dataset(\n        data_type=\'eval1\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    eval2_data = Dataset(\n        data_type=\'eval2\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    eval3_data = Dataset(\n        data_type=\'eval3\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                       model.labels_pl_list[0],\n                                       model.inputs_seq_len_pl_list[0],\n                                       model.keep_prob_pl_list[0])\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \\\n                    \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        decode(session=sess,\n               decode_op=decode_op,\n               model=model,\n               dataset=eval1_data,\n               label_type=params[\'label_type\'],\n               train_data_size=params[\'train_data_size\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n        decode(session=sess,\n               decode_op=decode_op,\n               model=model,\n               dataset=eval2_data,\n               label_type=params[\'label_type\'],\n               train_data_size=params[\'train_data_size\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n        decode(session=sess,\n               decode_op=decode_op,\n               model=model,\n               dataset=eval3_data,\n               label_type=params[\'label_type\'],\n               train_data_size=params[\'train_data_size\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n\ndef decode(session, decode_op, model, dataset, label_type,\n           train_data_size, is_test=True, save_path=None):\n    """"""Visualize label outputs of CTC model.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kanji or kanji or kanji_divide or kana_divide\n        train_data_size (string): train_subset or train_fullset\n        is_test (bool, optional): set to True when evaluating by the test set\n        save_path (string, optional): path to save decoding results\n    """"""\n    if \'kanji\' in label_type:\n        map_file_path = \'../metrics/mapping_files/\' + \\\n            label_type + \'_\' + train_data_size + \'.txt\'\n    elif \'kana\' in label_type:\n        map_file_path = \'../metrics/mapping_files/\' + label_type + \'.txt\'\n    else:\n        raise TypeError\n\n    idx2char = Idx2char(map_file_path=map_file_path)\n\n    if save_path is not None:\n        sys.stdout = open(join(model.model_dir, \'decode.txt\'), \'w\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        # Decode\n        batch_size = inputs[0].shape[0]\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n        no_output_flag = False\n        try:\n            labels_pred = sparsetensor2list(\n                labels_pred_st, batch_size=batch_size)\n        except IndexError:\n            # no output\n            no_output_flag = True\n\n        # Visualize\n        for i_batch in range(batch_size):\n\n            print(\'----- wav: %s -----\' % input_names[0][i_batch])\n            if is_test:\n                str_true = labels_true[0][i_batch][0]\n            else:\n                str_true = idx2char(labels_true[0][i_batch])\n            if no_output_flag:\n                str_pred = \'\'\n            else:\n                str_pred = idx2char(labels_pred[i_batch])\n\n            print(\'Ref: %s\' % str_true)\n            print(\'Hyp: %s\' % str_pred)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type\'] == \'kana\':\n        params[\'num_classes\'] = 146\n    elif params[\'label_type\'] == \'kana_divide\':\n        params[\'num_classes\'] = 147\n    elif params[\'label_type\'] == \'kanji\':\n        if params[\'train_data_size\'] == \'train_subset\':\n            params[\'num_classes\'] = 2981\n        elif params[\'train_data_size\'] == \'train_fullset\':\n            params[\'num_classes\'] = 3385\n    elif params[\'label_type\'] == \'kanji_divide\':\n        if params[\'train_data_size\'] == \'train_subset\':\n            params[\'num_classes\'] = 2982\n        elif params[\'train_data_size\'] == \'train_fullset\':\n            params[\'num_classes\'] = 3386\n    else:\n        raise TypeError\n\n    # Modle setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_decode(model=model, params=params,\n              epoch=args.epoch, beam_width=args.beam_width,\n              eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n\n    # if input_names[0] not in [\'A03M0106_0057\', \'A03M0016_0014\']:\n    #     continue\n'"
examples/csj/visualization/plot_ctc_posterior.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Plot the trained CTC posteriors (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tensorflow as tf\nimport yaml\n\nsys.path.append(\'../../../\')\nfrom experiments.csj.data.load_dataset_ctc import Dataset\nfrom experiments.csj.visualization.core.plot.ctc import posterior_test\nfrom models.ctc.load_model import load\n\n\ndef do_plot(network, param, epoch=None):\n    """"""Plot the CTC posteriors.\n    Args:\n        network: model to restore\n        param: A dictionary of parameters\n        epoch: epoch to restore\n    """"""\n    # Load dataset\n    eval1_data = Dataset(data_type=\'eval1\', label_type=param[\'label_type\'],\n                         batch_size=1,\n                         train_data_size=param[\'train_data_size\'],\n                         num_stack=param[\'num_stack\'],\n                         num_skip=param[\'num_skip\'],\n                         is_sorted=False, is_progressbar=True, is_gpu=False)\n    eval2_data = Dataset(data_type=\'eval2\', label_type=param[\'label_type\'],\n                         batch_size=1,\n                         train_data_size=param[\'train_data_size\'],\n                         num_stack=param[\'num_stack\'],\n                         num_skip=param[\'num_skip\'],\n                         is_sorted=False, is_progressbar=True, is_gpu=False)\n    eval3_data = Dataset(data_type=\'eval3\', label_type=param[\'label_type\'],\n                         batch_size=1,\n                         train_data_size=param[\'train_data_size\'],\n                         num_stack=param[\'num_stack\'],\n                         num_skip=param[\'num_skip\'],\n                         is_sorted=False, is_progressbar=True, is_gpu=False)\n\n    # Define placeholders\n    network.create_placeholders(gpu_index=None)\n\n    # Add to the graph each operation (including model definition)\n    _, logits = network.compute_loss(network.inputs_pl_list[0],\n                                     network.labels_pl_list[0],\n                                     network.inputs_seq_len_pl_list[0],\n                                     network.keep_prob_input_pl_list[0],\n                                     network.keep_prob_hidden_pl_list[0],\n                                     network.keep_prob_output_pl_list[0])\n    posteriors_op = network.posteriors(logits)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(network.model_dir)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch is not None:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        posterior_test(session=sess,\n                       posteriors_op=posteriors_op,\n                       network=network,\n                       dataset=eval1_data,\n                       label_type=param[\'label_type\'],\n                       # save_path=network.model_dir,\n                       save_path=None,\n                       show=True)\n        posterior_test(session=sess,\n                       posteriors_op=posteriors_op,\n                       network=network,\n                       dataset=eval2_data,\n                       label_type=param[\'label_type\'],\n                       # save_path=network.model_dir,\n                       save_path=None,\n                       show=True)\n        posterior_test(session=sess,\n                       posteriors_op=posteriors_op,\n                       network=network,\n                       dataset=eval3_data,\n                       label_type=param[\'label_type\'],\n                       # save_path=network.model_dir,\n                       save_path=None,\n                       show=True)\n\n\ndef main(model_path, epoch):\n\n    # Load config file\n    with open(os.path.join(model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        param = config[\'param\']\n\n    # Except for a blank label\n    if param[\'label_type\'] == \'kanji\':\n        param[\'num_classes\'] = 3386\n    elif param[\'label_type\'] == \'kana\':\n        param[\'num_classes\'] = 147\n    elif param[\'label_type\'] == \'phone\':\n        param[\'num_classes\'] = 38\n\n    # Model setting\n    model = load(model_type=param[\'model\'])\n    network = model(\n        batch_size=1,\n        input_size=params[\'input_size\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'],\n        num_units=param[\'num_units\'],\n        num_layer=param[\'num_layer\'],\n        bottleneck_dim=param[\'bottleneck_dim\'],\n        num_classes=param[\'num_classes\'],\n        parameter_init=param[\'weight_init\'],\n        clip_grad=param[\'clip_grad\'],\n        clip_activation=param[\'clip_activation\'],\n        dropout_ratio_input=param[\'dropout_input\'],\n        dropout_ratio_hidden=param[\'dropout_hidden\'],\n        num_proj=param[\'num_proj\'],\n        weight_decay=param[\'weight_decay\'])\n\n    network.model_dir = model_path\n    do_plot(network=network, param=param, epoch=epoch)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) == 2:\n        model_path = args[1]\n        epoch = None\n    elif len(args) == 3:\n        model_path = args[1]\n        epoch = args[2]\n    else:\n        raise ValueError(\n            (""Set a path to saved model.\\n""\n             ""Usase: python plot_ctc_posterior.py path_to_saved_model""))\n    main(model_path=model_path, epoch=epoch)\n'"
examples/erato/data/load_dataset_attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the Attention-based model (ERATO corpus).\nIn addition, frame stacking and skipping are used.\n  You can use only the single GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.attention import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, label_type, ss_type, batch_size,\n                 map_file_path, max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or test\n            label_type (string): kana\n            ss_type (string): remove or insert_left or insert_both or insert_right\n            batch_size (int): the size of mini-batch\n            map_file_path (string): path to the mapping file\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        super(Dataset, self).__init__(map_file_path=map_file_path)\n\n        self.is_test = True if data_type == \'test\' else False\n\n        self.data_type = data_type\n        self.label_type = label_type\n        self.ss_type = ss_type\n        self.batch_size = batch_size\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = 1\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/erato\',\n                        \'/n/sd8/inaguma/corpus/erato/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\', data_type)\n        # NOTE: ex.) save_path:\n        # erato_dataset_path/inputs/data_type/speaker/***.npy\n        label_path = join(dataset_root[0], \'labels\',\n                          ss_type, data_type, label_type)\n        # NOTE: ex.) save_path:\n        # erato_dataset_path/labels/ss_type/data_type/kana/speaker/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\', data_type)\n            label_path = join(dataset_root[0], \'labels\',\n                              ss_type, data_type, label_type)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for input_name, frame_num in frame_num_tuple_sorted:\n            speaker = \'_\'.join(input_name.split(\'_\')[:-1])\n            input_paths.append(join(input_path, speaker, input_name + \'.npy\'))\n            label_paths.append(join(label_path, speaker, input_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/erato/data/load_dataset_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the CTC model (ERATO corpus).\n   In addition, frame stacking and skipping are used.\n   You can use only the single GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.ctc import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, label_type, ss_type, batch_size,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or test\n            label_type (string): kana\n            ss_type (string): remove or insert_left or insert_both or insert_right\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        super(Dataset, self).__init__()\n\n        self.is_test = True if data_type == \'test\' else False\n\n        self.data_type = data_type\n        self.label_type = label_type\n        self.ss_type = ss_type\n        self.batch_size = batch_size\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = 1\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/erato\',\n                        \'/n/sd8/inaguma/corpus/erato/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\', data_type)\n        # NOTE: ex.) save_path:\n        # erato_dataset_path/inputs/data_type/speaker/***.npy\n        label_path = join(dataset_root[0], \'labels\',\n                          ss_type, data_type, label_type)\n        # NOTE: ex.) save_path:\n        # erato_dataset_path/labels/ss_type/data_type/kana/speaker/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\', data_type)\n            label_path = join(dataset_root[0], \'labels\',\n                              ss_type, data_type, label_type)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for input_name, frame_num in frame_num_tuple_sorted:\n            speaker = \'_\'.join(input_name.split(\'_\')[:-1])\n            input_paths.append(join(input_path, speaker, input_name + \'.npy\'))\n            label_paths.append(join(label_path, speaker, input_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/erato/evaluation/eval_attention.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the trained Attention-based model (ERATO corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.erato.data.load_dataset_attention import Dataset\nfrom experiments.erato.metrics.attention import do_eval_cer, do_eval_fmeasure\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_eval(model, params, epoch, beam_width, eval_batch_size):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam_width (int, optional): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    map_file_path = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'_\' + params[\'ss_type\'] + \'.txt\'\n\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, _, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n        model.inputs_pl_list[0],\n        model.labels_pl_list[0],\n        model.inputs_seq_len_pl_list[0],\n        model.labels_seq_len_pl_list[0],\n        model.keep_prob_encoder_pl_list[0],\n        model.keep_prob_decoder_pl_list[0],\n        model.keep_prob_embedding_pl_list[0])\n    _, decode_op_infer = model.decode(\n        decoder_outputs_train,\n        decoder_outputs_infer)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'=== Test Data Evaluation ===\')\n        cer_test = do_eval_cer(\n            session=sess,\n            decode_op=decode_op_infer,\n            model=model,\n            dataset=test_data,\n            label_type=params[\'label_type\'],\n            ss_type=params[\'ss_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER: %f %%\' % (cer_test * 100))\n\n        df_acc = do_eval_fmeasure(\n            session=sess,\n            decode_op=decode_op_infer,\n            model=model,\n            dataset=test_data,\n            label_type=params[\'label_type\'],\n            ss_type=params[\'ss_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(df_acc)\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'ss_type\'] == \'remove\':\n        params[\'num_classes\'] = 147\n    elif params[\'ss_type\'] in [\'insert_left\', \'insert_right\']:\n        params[\'num_classes\'] = 151\n    elif params[\'ss_type\'] == \'insert_both\':\n        params[\'num_classes\'] = 155\n    else:\n        TypeError\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, eval_batch_size=args.eval_batch_size,\n            beam_width=args.beam_width)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/erato/evaluation/eval_ctc.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the trained CTC model (ERATO corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.erato.data.load_dataset_ctc import Dataset\nfrom experiments.erato.metrics.ctc import do_eval_cer, do_eval_fmeasure\nfrom models.ctc.ctc import CTC\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_eval(model, params, epoch, beam_width, eval_batch_size):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam_width (int, optional): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                   model.labels_pl_list[0],\n                                   model.inputs_seq_len_pl_list[0],\n                                   model.keep_prob_pl_list[0])\n    decode_op = model.decoder(\n        logits,\n        model.inputs_seq_len_pl_list[0],\n        beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'=== Test Data Evaluation ===\')\n        cer = do_eval_cer(\n            session=sess,\n            decode_op=decode_op,\n            model=model,\n            dataset=test_data,\n            label_type=params[\'label_type\'],\n            ss_type=params[\'ss_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER: %f %%\' % (cer * 100))\n\n        df_acc = do_eval_fmeasure(\n            session=sess,\n            decode_op=decode_op,\n            model=model,\n            dataset=test_data,\n            label_type=params[\'label_type\'],\n            ss_type=params[\'ss_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(df_acc)\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'ss_type\'] == \'remove\':\n        params[\'num_classes\'] = 147\n    elif params[\'ss_type\'] in [\'insert_left\', \'insert_right\']:\n        params[\'num_classes\'] = 151\n    elif params[\'ss_type\'] == \'insert_both\':\n        params[\'num_classes\'] = 155\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, beam_width=args.beam_width,\n            eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/erato/evaluation/eval_julius.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport sys\nimport codecs\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\n\nsys.path.append('../../../')\nfrom experiments.erato.data.load_dataset_ctc import Dataset\n\n\ndef main():\n    results_paths = [path for path in glob(\n        '/home/lab5/inaguma/asru2017/erato_results_0710/test/*.log')]\n\n    # Julisu Rusults\n    for path in results_paths:\n        with codecs.open(path, 'r', 'euc_jp') as f:\n            start_flag = False\n            file_name = ''\n            output, output_pos = '', ''\n            result_dict = {}\n            for line in f:\n                line = line.strip()\n                if line == '----------------------- System Information end -----------------------':\n                    start_flag = True\n\n                if start_flag:\n                    if 'input MFCC file' in line:\n                        file_name = line.split(': ')[-1]\n                        file_name = '_'.join(file_name.split('/')[-2:])\n                        file_name = re.sub('.wav', '', file_name)\n\n                    if 'sentence1' in line:\n                        output = line.split(': ')[-1]\n                        output = re.sub('<s>', '', output)\n                        output = re.sub('</s>', '', output)\n                        output = re.sub('<sp>', '', output)\n                        output = re.sub(r'[\\s\xe3\x83\xbc]+', '', output)\n\n                    if 'wseq1' in line:\n                        output_pos = line.split(': ')[-1]\n                        output_pos = re.sub('<s>', '', output_pos)\n                        output_pos = re.sub('</s>', '', output_pos)\n                        output_pos = re.sub('<sp>', '', output_pos)\n                        output_pos = re.sub('\xe6\x84\x9f\xe5\x8b\x95\xe8\xa9\x9e', 'F', output_pos)\n                        output_pos = re.sub('\xe8\xa8\x80\xe3\x81\x84\xe3\x82\x88\xe3\x81\xa9\xe3\x81\xbf', 'D', output_pos)\n                        result_dict[file_name] = [output, output_pos[1:]]\n                        output, output_pos = '', ''\n\n    dataset = Dataset(data_type='test',\n                      label_type='kana',\n                      ss_type='insert_left',\n                      batch_size=1,\n                      max_epoch=1,\n                      shuffle=False,\n                      progressbar=True)\n\n    tp_f, fp_f, fn_f = 0., 0., 0.\n    tp_d, fp_d, fn_d = 0., 0., 0.\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, input_names = data\n\n        if input_names[0][0] not in result_dict.keys():\n            continue\n\n        output, output_pos = result_dict[input_names[0][0]]\n\n        detected_f_num = output_pos.count('F')\n        detected_d_num = output_pos.count('D')\n\n        if detected_f_num != 0 or detected_d_num != 0:\n            print(output_pos)\n            print(output)\n            str_true = labels_true[0][0][0]\n            print(str_true)\n            print('-----')\n\n        true_f_num = np.sum(labels_true[0][0][0].count('F'))\n        true_d_num = np.sum(labels_true[0][0][0].count('D'))\n\n        # Filler\n        if detected_f_num <= true_f_num:\n            tp_f += detected_f_num\n            fn_f += true_f_num - detected_f_num\n        else:\n            tp_f += true_f_num\n            fp_f += detected_f_num - true_f_num\n\n        # Disfluency\n        if detected_d_num <= true_d_num:\n            tp_d += detected_d_num\n            fn_d += true_d_num - detected_d_num\n        else:\n            tp_d += true_d_num\n            fp_d += detected_d_num - true_d_num\n\n        if is_new_epoch:\n            break\n\n    r_f = tp_f / (tp_f + fn_f) if (tp_f + fn_f) != 0 else 0\n    p_f = tp_f / (tp_f + fp_f) if (tp_f + fp_f) != 0 else 0\n    f_f = 2 * r_f * p_f / (r_f + p_f) if (r_f + p_f) != 0 else 0\n\n    r_d = tp_d / (tp_d + fn_d) if (tp_d + fn_d) != 0 else 0\n    p_d = tp_d / (tp_d + fp_d) if (tp_d + fp_d) != 0 else 0\n    f_d = 2 * r_d * p_d / (r_d + p_d) if (r_d + p_d) != 0 else 0\n\n    acc_f = [p_f, r_f, f_f]\n    acc_d = [p_d, r_d, f_d]\n\n    df_acc = pd.DataFrame({'Filler': acc_f, 'Disfluency': acc_d},\n                          columns=['Filler', 'Disfluency'],\n                          index=['Precision', 'Recall', 'F-measure'])\n    print(df_acc)\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/erato/metrics/attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for the Attention-based model (ERATO corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom utils.io.labels.character import Idx2char\nfrom utils.evaluation.edit_distance import compute_cer\n\n\ndef do_eval_cer(session, decode_op, model, dataset, label_type, ss_type,\n                is_test=False, eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kana\n        ss_type (string): remove or insert_left or insert_both or insert_right\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Return:\n        cer_mean (float): An average of CER\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type + \'_\' + ss_type + \'.txt\')\n\n    cer_mean = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, labels_seq_len,  _ = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        labels_pred = session.run(decode_op, feed_dict=feed_dict)\n\n        for i_batch in range(batch_size):\n\n            # Convert from list of index to string\n            if is_test:\n                str_true = labels_true[0][i_batch][0]\n                # NOTE: transcript is seperated by space(\'_\')\n            else:\n                str_true = idx2char(\n                    labels_true[0][i_batch][1:labels_seq_len[0][i_batch] - 1])\n            str_pred = idx2char(labels_pred[i_batch]).split(\'>\')[0]\n            # NOTE: Trancate by <EOS>\n\n            # Remove garbage labels\n            str_true = re.sub(r\'[_NZLFBDlfbd\xe3\x83\xbc<>]+\', \'\', str_true)\n            str_pred = re.sub(r\'[_NZLFBDlfbd\xe3\x83\xbc<>]+\', \'\', str_pred)\n\n            # Compute CER\n            cer_mean += compute_cer(str_pred=str_pred,\n                                    str_true=str_true,\n                                    normalize=True)\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= len(dataset)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return cer_mean\n\n\ndef do_eval_fmeasure(session, decode_op, model, dataset, label_type, ss_type,\n                     is_test=False, eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by F-measure.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kana\n        ss_type (string): remove or insert_left or insert_both or insert_right\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Returns:\n        f_mean (float): An average of F-measure of each social signal\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type + \'_\' + ss_type + \'.txt\')\n\n    tp_lau, fp_lau, fn_lau = 0., 0., 0.\n    tp_fil, fp_fil, fn_fil = 0., 0., 0.\n    tp_bac, fp_bac, fn_bac = 0., 0., 0.\n    tp_dis, fp_dis, fn_dis = 0., 0., 0.\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, labels_seq_len,  _ = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        labels_pred = session.run(decode_op, feed_dict=feed_dict)\n\n        for i_batch in range(batch_size):\n\n            # Convert from list of index to string\n            if is_test:\n                str_true = labels_true[0][i_batch][0]\n                # NOTE: transcript is seperated by space(\'_\')\n            else:\n                # Convert from list of index to string\n                str_true = idx2char(\n                    labels_true[0][i_batch][1:labels_seq_len[0][i_batch] - 1],\n                    padded_value=dataset.padded_value)\n            str_pred = idx2char(labels_pred[i_batch]).split(\'>\')[0]\n            # NOTE: Trancate by <EOS>\n\n            detected_lau_num = str_pred.count(\'L\')\n            detected_fil_num = str_pred.count(\'F\')\n            detected_bac_num = str_pred.count(\'B\')\n            detected_dis_num = str_pred.count(\'D\')\n\n            true_lau_num = str_true.count(\'L\')\n            true_fil_num = str_true.count(\'F\')\n            true_bac_num = str_true.count(\'B\')\n            true_dis_num = str_true.count(\'D\')\n\n            # Laughter\n            if detected_lau_num <= true_lau_num:\n                tp_lau += detected_lau_num\n                fn_lau += true_lau_num - detected_lau_num\n            else:\n                tp_lau += true_lau_num\n                fp_lau += detected_lau_num - true_lau_num\n\n            # Filler\n            if detected_fil_num <= true_fil_num:\n                tp_fil += detected_fil_num\n                fn_fil += true_fil_num - detected_fil_num\n            else:\n                tp_fil += true_fil_num\n                fp_fil += detected_fil_num - true_fil_num\n\n            # Backchannel\n            if detected_bac_num <= true_bac_num:\n                tp_bac += detected_bac_num\n                fn_bac += true_bac_num - detected_bac_num\n            else:\n                tp_bac += true_bac_num\n                fp_bac += detected_bac_num - true_bac_num\n\n            # Disfluency\n            if detected_dis_num <= true_dis_num:\n                tp_dis += detected_dis_num\n                fn_dis += true_dis_num - detected_dis_num\n            else:\n                tp_dis += true_dis_num\n                fp_dis += detected_dis_num - true_dis_num\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    p_lau = tp_lau / (tp_lau + fp_lau) if (tp_lau + fp_lau) != 0 else 0\n    r_lau = tp_lau / (tp_lau + fn_lau) if (tp_lau + fn_lau) != 0 else 0\n    f_lau = 2 * r_lau * p_lau / (r_lau + p_lau) if (r_lau + p_lau) != 0 else 0\n\n    r_fil = tp_fil / (tp_fil + fn_fil) if (tp_fil + fn_fil) != 0 else 0\n    p_fil = tp_fil / (tp_fil + fp_fil) if (tp_fil + fp_fil) != 0 else 0\n    f_fil = 2 * r_fil * p_fil / (r_fil + p_fil) if (r_fil + p_fil) != 0 else 0\n\n    p_bac = tp_bac / (tp_bac + fp_bac) if (tp_bac + fp_bac) != 0 else 0\n    r_bac = tp_bac / (tp_bac + fn_bac) if (tp_bac + fn_bac) != 0 else 0\n    f_bac = 2 * r_bac * p_bac / (r_bac + p_bac) if (r_bac + p_bac) != 0 else 0\n\n    r_dis = tp_dis / (tp_dis + fn_dis) if (tp_dis + fn_dis) != 0 else 0\n    p_dis = tp_dis / (tp_dis + fp_dis) if (tp_dis + fp_dis) != 0 else 0\n    f_dis = 2 * r_dis * p_dis / (r_dis + p_dis) if (r_dis + p_dis) != 0 else 0\n\n    acc_lau = [p_lau, r_lau, f_lau]\n    acc_fil = [p_fil, r_fil, f_fil]\n    acc_bac = [p_bac, r_bac, f_bac]\n    acc_dis = [p_dis, r_dis, f_dis]\n    mean = [(p_lau + p_fil + p_bac + p_dis) / 4., (r_lau + r_fil + r_bac + r_dis) / 4.,\n            (f_lau + f_fil + f_bac + f_dis) / 4.]\n\n    df_acc = pd.DataFrame({\'Laughter\': acc_lau,\n                           \'Filler\': acc_fil,\n                           \'Backchannel\': acc_bac,\n                           \'Disfluency\': acc_dis,\n                           \'Mean\': mean},\n                          columns=[\'Laughter\', \'Filler\',\n                                   \'Backchannel\', \'Disfluency\', \'Mean\'],\n                          index=[\'Precision\', \'Recall\', \'F-measure\'])\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return df_acc\n'"
examples/erato/metrics/ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for the CTC model (ERATO corpus).""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.sparsetensor import sparsetensor2list\nfrom utils.evaluation.edit_distance import compute_cer\n\n\ndef do_eval_cer(session, decode_op, model, dataset, label_type, ss_type,\n                is_test=False, eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kana\n        ss_type (string): remove or insert_left or insert_both or insert_right\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Return:\n        cer_mean (float): An average of CER\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type + \'_\' + ss_type + \'.txt\')\n\n    cer_mean = 0\n    skip_data_num = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, _ = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n\n        try:\n            labels_pred = sparsetensor2list(labels_pred_st, batch_size)\n\n            for i_batch in range(batch_size):\n\n                # Convert from list of index to string\n                if is_test:\n                    str_true = labels_true[0][i_batch][0]\n                    # NOTE: transcript is seperated by space(\'_\')\n                else:\n                    # Convert from list of index to string\n                    str_true = idx2char(labels_true[0][i_batch],\n                                        padded_value=dataset.padded_value)\n                str_pred = idx2char(labels_pred[i_batch])\n\n                # Remove garbage labels\n                str_true = re.sub(r\'[_NZLFBDlfbd\xe3\x83\xbc]+\', \'\', str_true)\n                str_pred = re.sub(r\'[_NZLFBDlfbd\xe3\x83\xbc]+\', \'\', str_pred)\n\n                # Compute CER\n                cer_mean += compute_cer(str_pred=str_pred,\n                                        str_true=str_true,\n                                        normalize=True)\n\n                if progressbar:\n                    pbar.update(1)\n        except:\n            print(\'skipped\')\n            skip_data_num += batch_size\n            # TODO: Conduct decoding again with batch size 1\n\n            if progressbar:\n                pbar.update(batch_size)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= (len(dataset) - skip_data_num)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return cer_mean\n\n\ndef do_eval_fmeasure(session, decode_op, model, dataset, label_type, ss_type,\n                     is_test=False, eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by F-measure.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kana\n        ss_type (string): remove or insert_left or insert_both or insert_right\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Returns:\n        f_mean (float): An average of F-measure of each social signal\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type + \'_\' + ss_type + \'.txt\')\n\n    tp_lau, fp_lau, fn_lau = 0., 0., 0.\n    tp_fil, fp_fil, fn_fil = 0., 0., 0.\n    tp_bac, fp_bac, fn_bac = 0., 0., 0.\n    tp_dis, fp_dis, fn_dis = 0., 0., 0.\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, _ = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n\n        try:\n            labels_pred = sparsetensor2list(labels_pred_st, batch_size)\n\n            for i_batch in range(batch_size):\n\n                # Convert from list of index to string\n                if is_test:\n                    str_true = labels_true[0][i_batch][0]\n                    # NOTE: transcript is seperated by space(\'_\')\n                else:\n                    # Convert from list of index to string\n                    str_true = idx2char(labels_true[0][i_batch],\n                                        padded_value=dataset.padded_value)\n                str_pred = idx2char(labels_pred[i_batch])\n\n                detected_lau_num = str_pred.count(\'L\')\n                detected_fil_num = str_pred.count(\'F\')\n                detected_bac_num = str_pred.count(\'B\')\n                detected_dis_num = str_pred.count(\'D\')\n\n                true_lau_num = str_true.count(\'L\')\n                true_fil_num = str_true.count(\'F\')\n                true_bac_num = str_true.count(\'B\')\n                true_dis_num = str_true.count(\'D\')\n\n                # Laughter\n                if detected_lau_num <= true_lau_num:\n                    tp_lau += detected_lau_num\n                    fn_lau += true_lau_num - detected_lau_num\n                else:\n                    tp_lau += true_lau_num\n                    fp_lau += detected_lau_num - true_lau_num\n\n                # Filler\n                if detected_fil_num <= true_fil_num:\n                    tp_fil += detected_fil_num\n                    fn_fil += true_fil_num - detected_fil_num\n                else:\n                    tp_fil += true_fil_num\n                    fp_fil += detected_fil_num - true_fil_num\n\n                # Backchannel\n                if detected_bac_num <= true_bac_num:\n                    tp_bac += detected_bac_num\n                    fn_bac += true_bac_num - detected_bac_num\n                else:\n                    tp_bac += true_bac_num\n                    fp_bac += detected_bac_num - true_bac_num\n\n                # Disfluency\n                if detected_dis_num <= true_dis_num:\n                    tp_dis += detected_dis_num\n                    fn_dis += true_dis_num - detected_dis_num\n                else:\n                    tp_dis += true_dis_num\n                    fp_dis += detected_dis_num - true_dis_num\n\n                if progressbar:\n                    pbar.update(1)\n        except:\n            print(\'skipped\')\n\n            if progressbar:\n                pbar.update(batch_size)\n\n        if is_new_epoch:\n            break\n\n    p_lau = tp_lau / (tp_lau + fp_lau) if (tp_lau + fp_lau) != 0 else 0\n    r_lau = tp_lau / (tp_lau + fn_lau) if (tp_lau + fn_lau) != 0 else 0\n    f_lau = 2 * r_lau * p_lau / (r_lau + p_lau) if (r_lau + p_lau) != 0 else 0\n\n    r_fil = tp_fil / (tp_fil + fn_fil) if (tp_fil + fn_fil) != 0 else 0\n    p_fil = tp_fil / (tp_fil + fp_fil) if (tp_fil + fp_fil) != 0 else 0\n    f_fil = 2 * r_fil * p_fil / (r_fil + p_fil) if (r_fil + p_fil) != 0 else 0\n\n    p_bac = tp_bac / (tp_bac + fp_bac) if (tp_bac + fp_bac) != 0 else 0\n    r_bac = tp_bac / (tp_bac + fn_bac) if (tp_bac + fn_bac) != 0 else 0\n    f_bac = 2 * r_bac * p_bac / (r_bac + p_bac) if (r_bac + p_bac) != 0 else 0\n\n    r_dis = tp_dis / (tp_dis + fn_dis) if (tp_dis + fn_dis) != 0 else 0\n    p_dis = tp_dis / (tp_dis + fp_dis) if (tp_dis + fp_dis) != 0 else 0\n    f_dis = 2 * r_dis * p_dis / (r_dis + p_dis) if (r_dis + p_dis) != 0 else 0\n\n    acc_lau = [p_lau, r_lau, f_lau]\n    acc_fil = [p_fil, r_fil, f_fil]\n    acc_bac = [p_bac, r_bac, f_bac]\n    acc_dis = [p_dis, r_dis, f_dis]\n    mean = [(p_lau + p_fil + p_bac + p_dis) / 4., (r_lau + r_fil + r_bac + r_dis) / 4.,\n            (f_lau + f_fil + f_bac + f_dis) / 4.]\n\n    df_acc = pd.DataFrame({\'Laughter\': acc_lau,\n                           \'Filler\': acc_fil,\n                           \'Backchannel\': acc_bac,\n                           \'Disfluency\': acc_dis,\n                           \'Mean\': mean},\n                          columns=[\'Laughter\', \'Filler\',\n                                   \'Backchannel\', \'Disfluency\', \'Mean\'],\n                          index=[\'Precision\', \'Recall\', \'F-measure\'])\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return df_acc\n'"
examples/erato/training/train_attention.py,9,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the Attention-based model (ERATO corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.erato.data.load_dataset_attention import Dataset\nfrom experiments.erato.metrics.attention import do_eval_cer, do_eval_fmeasure\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\n\n\ndef do_train(model, params):\n    """"""Run training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n    """"""\n    map_file_path = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'_\' + params[\'ss_type\'] + \'.txt\'\n\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path,\n        max_epoch=params[\'num_epoch\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'])\n    dev_data = Dataset(\n        data_type=\'dev\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders()\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        # Add to the graph each operation (including model definition)\n        loss_op, logits, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.labels_seq_len_pl_list[0],\n            model.keep_prob_encoder_pl_list[0],\n            model.keep_prob_decoder_pl_list[0],\n            model.keep_prob_embedding_pl_list[0])\n        train_op = model.train(loss_op,\n                               optimizer=params[\'optimizer\'],\n                               learning_rate=learning_rate_pl)\n        _, decode_op_infer = model.decode(\n            decoder_outputs_train,\n            decoder_outputs_infer)\n        ler_op = model.compute_ler(model.labels_st_true_pl,\n                                   model.labels_st_pred_pl)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total param\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M param"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize param\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            cer_dev_best = 1\n            not_improved_epoch = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels_train, inputs_seq_len, labels_seq_len, _ = data\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs[0],\n                    model.labels_pl_list[0]: labels_train[0],\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                    model.labels_seq_len_pl_list[0]: labels_seq_len[0],\n                    model.keep_prob_encoder_pl_list[0]: 1 - float(params[\'dropout_encoder\']),\n                    model.keep_prob_decoder_pl_list[0]: 1 - float(params[\'dropout_decoder\']),\n                    model.keep_prob_embedding_pl_list[0]: 1 - float(params[\'dropout_embedding\']),\n                    learning_rate_pl: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % params[\'print_step\'] == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels_dev, inputs_seq_len,\n                     labels_seq_len, _), _ = dev_data.next()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs[0],\n                        model.labels_pl_list[0]: labels_dev[0],\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                        model.labels_seq_len_pl_list[0]: labels_seq_len[0],\n                        model.keep_prob_encoder_pl_list[0]: 1.0,\n                        model.keep_prob_decoder_pl_list[0]: 1.0,\n                        model.keep_prob_embedding_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_encoder_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_decoder_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_embedding_pl_list[0]] = 1.0\n\n                    # Predict class ids & update even files\n                    predicted_ids_train, summary_str_train = sess.run(\n                        [decode_op_infer, summary_train], feed_dict=feed_dict_train)\n                    predicted_ids_dev, summary_str_dev = sess.run(\n                        [decode_op_infer, summary_dev], feed_dict=feed_dict_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    # Convert to sparsetensor to compute LER\n                    feed_dict_ler_train = {\n                        model.labels_st_true_pl: list2sparsetensor(\n                            labels_train[0], padded_value=train_data.padded_value),\n                        model.labels_st_pred_pl: list2sparsetensor(\n                            predicted_ids_train, padded_value=train_data.padded_value)\n                    }\n                    feed_dict_ler_dev = {\n                        model.labels_st_true_pl: list2sparsetensor(\n                            labels_dev[0], padded_value=dev_data.padded_value),\n                        model.labels_st_pred_pl: list2sparsetensor(\n                            predicted_ids_dev, padded_value=dev_data.padded_value)\n                    }\n\n                    # Compute accuracy\n                    ler_train = sess.run(ler_op, feed_dict=feed_dict_ler_train)\n                    ler_dev = sess.run(ler_op, feed_dict=feed_dict_ler_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        cer_dev_epoch = do_eval_cer(\n                            session=sess,\n                            decode_op=decode_op_infer,\n                            model=model,\n                            dataset=dev_data,\n                            label_type=params[\'label_type\'],\n                            ss_type=params[\'ss_type\'],\n                            eval_batch_size=1)\n                        print(\'  CER: %f %%\' % (cer_dev_epoch * 100))\n\n                        if cer_dev_epoch < cer_dev_best:\n                            cer_dev_best = cer_dev_epoch\n                            not_improved_epoch = 0\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n\n                            print(\'=== Test Data Evaluation ===\')\n                            ler_test = do_eval_cer(\n                                session=sess,\n                                decode_op=decode_op_infer,\n                                model=model,\n                                dataset=test_data,\n                                label_type=params[\'label_type\'],\n                                ss_type=params[\'ss_type\'],\n                                is_test=True,\n                                eval_batch_size=1)\n                            print(\'  CER: %f %%\' % (ler_test * 100))\n\n                            if params[\'ss_type\'] != \'remove\':\n                                df_acc = do_eval_fmeasure(\n                                    session=sess,\n                                    decode_op=decode_op_infer,\n                                    model=model,\n                                    dataset=test_data,\n                                    label_type=params[\'label_type\'],\n                                    ss_type=params[\'ss_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(df_acc)\n                        else:\n                            not_improved_epoch += 1\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Early stopping\n                        if not_improved_epoch == params[\'not_improved_patient_epoch\']:\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=cer_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'ss_type\'] == \'remove\':\n        params[\'num_classes\'] = 147\n    elif params[\'ss_type\'] in [\'insert_left\', \'insert_right\']:\n        params[\'num_classes\'] = 151\n    elif params[\'ss_type\'] == \'insert_both\':\n        params[\'num_classes\'] = 155\n    else:\n        TypeError\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'],\n        sigmoid_smoothing=params[\'sigmoid_smoothing\'])\n\n    # Set process name\n    setproctitle(\'tf_erato_\' + model.name + \'_\' +\n                 params[\'label_type\'] + \'_\' + params[\'ss_type\'] + \'_\' + params[\'attention_type\'])\n\n    model.name = \'en\' + str(params[\'encoder_num_units\'])\n    model.name += \'_\' + str(params[\'encoder_num_layers\'])\n    model.name += \'_att\' + str(params[\'attention_dim\'])\n    model.name += \'_de\' + str(params[\'decoder_num_units\'])\n    model.name += \'_\' + str(params[\'decoder_num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    model.name += \'_\' + params[\'attention_type\']\n    if params[\'dropout_encoder\'] != 0:\n        model.name += \'_dropen\' + str(params[\'dropout_encoder\'])\n    if params[\'dropout_decoder\'] != 0:\n        model.name += \'_dropde\' + str(params[\'dropout_decoder\'])\n    if params[\'dropout_embedding\'] != 0:\n        model.name += \'_dropem\' + str(params[\'dropout_embedding\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'wd\' + str(params[\'weight_decay\'])\n    if params[\'sharpening_factor\'] != 1:\n        model.name += \'_sharp\' + str(params[\'sharpening_factor\'])\n    if params[\'logits_temperature\'] != 1:\n        model.name += \'_temp\' + str(params[\'logits_temperature\'])\n    if bool(params[\'sigmoid_smoothing\']):\n        model.name += \'_smoothing\'\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'attention\', params[\'label_type\'],\n        params[\'ss_type\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError(\'Length of args should be 3.\')\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/erato/training/train_ctc.py,9,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the CTC model (ERATO corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.erato.data.load_dataset_ctc import Dataset\nfrom experiments.erato.metrics.ctc import do_eval_cer, do_eval_fmeasure\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.ctc import CTC\n\n\ndef do_train(model, params):\n    """"""Run training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'])\n    dev_data = Dataset(\n        data_type=\'dev\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders()\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        # Add to the graph each operation (including model definition)\n        loss_op, logits = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0])\n        train_op = model.train(\n            loss_op,\n            optimizer=params[\'optimizer\'],\n            learning_rate=learning_rate_pl)\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=params[\'beam_width\'])\n        ler_op = model.compute_ler(decode_op, model.labels_pl_list[0])\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            cer_dev_best = 1\n            not_improved_epoch = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels, inputs_seq_len, _ = data\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs[0],\n                    model.labels_pl_list[0]: list2sparsetensor(\n                        labels[0], padded_value=train_data.padded_value),\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                    model.keep_prob_pl_list[0]: 1 - float(params[\'dropout\']),\n                    learning_rate_pl: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % params[\'print_step\'] == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels, inputs_seq_len, _), _ = dev_data.next()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs[0],\n                        model.labels_pl_list[0]: list2sparsetensor(\n                            labels[0], padded_value=dev_data.padded_value),\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                        model.keep_prob_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_pl_list[0]] = 1.0\n\n                    # Compute accuracy & update event files\n                    ler_train, summary_str_train = sess.run(\n                        [ler_op, summary_train], feed_dict=feed_dict_train)\n                    ler_dev, summary_str_dev = sess.run(\n                        [ler_op, summary_dev], feed_dict=feed_dict_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        cer_dev_epoch = do_eval_cer(\n                            session=sess,\n                            decode_op=decode_op,\n                            model=model,\n                            dataset=dev_data,\n                            label_type=params[\'label_type\'],\n                            ss_type=params[\'ss_type\'],\n                            eval_batch_size=1)\n                        print(\'  CER: %f %%\' % (cer_dev_epoch * 100))\n\n                        if cer_dev_epoch < cer_dev_best:\n                            cer_dev_best = cer_dev_epoch\n                            not_improved_epoch = 0\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model only (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n\n                            print(\'=== Test Data Evaluation ===\')\n                            ler_test = do_eval_cer(\n                                session=sess,\n                                decode_op=decode_op,\n                                model=model,\n                                dataset=test_data,\n                                label_type=params[\'label_type\'],\n                                ss_type=params[\'ss_type\'],\n                                is_test=True,\n                                eval_batch_size=1)\n                            print(\'  CER: %f %%\' % (ler_test * 100))\n\n                            if params[\'ss_type\'] != \'remove\':\n                                df_acc = do_eval_fmeasure(\n                                    session=sess,\n                                    decode_op=decode_op,\n                                    model=model,\n                                    dataset=test_data,\n                                    label_type=params[\'label_type\'],\n                                    ss_type=params[\'ss_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(df_acc)\n                        else:\n                            not_improved_epoch += 1\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Early stopping\n                        if not_improved_epoch == params[\'not_improved_patient_epoch\']:\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=cer_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'ss_type\'] == \'remove\':\n        params[\'num_classes\'] = 147\n    elif params[\'ss_type\'] in [\'insert_left\', \'insert_right\']:\n        params[\'num_classes\'] = 151\n    elif params[\'ss_type\'] == \'insert_both\':\n        params[\'num_classes\'] = 155\n    else:\n        TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\'tf_erato_\' + model.name + \'_\' +\n                 params[\'label_type\'] + \'_\' + params[\'ss_type\'])\n\n    model.name += \'_\' + str(params[\'num_units\'])\n    model.name += \'_\' + str(params[\'num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'num_proj\'] != 0:\n        model.name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'dropout\'] != 0:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'ctc\', params[\'label_type\'],\n        params[\'ss_type\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError(\'Length of args should be 3.\')\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/erato/visualization/decode_attention.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decode the trained Attention outputs (ERATO corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.erato.data.load_dataset_attention import Dataset\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\nfrom utils.io.labels.character import Idx2char\nfrom utils.evaluation.edit_distance import wer_align\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_decode(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the Attention outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    map_file_path = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'_\' + params[\'ss_type\'] + \'.txt\'\n\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=eval_batch_size, map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, _, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n        model.inputs_pl_list[0],\n        model.labels_pl_list[0],\n        model.inputs_seq_len_pl_list[0],\n        model.labels_seq_len_pl_list[0],\n        model.keep_prob_encoder_pl_list[0],\n        model.keep_prob_decoder_pl_list[0],\n        model.keep_prob_embedding_pl_list[0])\n    _, decode_op_infer = model.decode(\n        decoder_outputs_train,\n        decoder_outputs_infer)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        decode(session=sess,\n               decode_op=decode_op_infer,\n               model=model,\n               dataset=test_data,\n               label_type=params[\'label_type\'],\n               ss_type=params[\'ss_type\'],\n               is_test=True,\n               eval_batch_size=1,\n               save_path=None)\n        # save_path=model.save_path)\n\n\ndef decode(session, decode_op, model, dataset, label_type, ss_type,\n           is_test=False, eval_batch_size=None, save_path=None):\n    """"""Visualize label outputs of Attention-based model.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kana\n        ss_type (string): remove or insert_left or insert_both or insert_right\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        save_path (string): path to save decoding results\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type + \'_\' + ss_type + \'.txt\')\n\n    if save_path is not None:\n        sys.stdout = open(join(model.model_dir, \'decode.txt\'), \'w\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, labels_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n        labels_pred = session.run(decode_op, feed_dict=feed_dict)\n        for i_batch in range(batch_size):\n            print(\'----- wav: %s -----\' % input_names[0][i_batch])\n            if is_test:\n                str_true = labels_true[0][i_batch][0]\n            else:\n                str_true = idx2char(\n                    labels_true[0][i_batch][1:labels_seq_len[0][i_batch] - 1])\n            str_pred = idx2char(labels_pred[i_batch]).split(\'>\')[0]\n            # NOTE: Trancate by <EOS>\n\n            print(\'Ref: %s\' % str_true)\n            print(\'Hyp: %s\' % str_pred)\n\n        if is_new_epoch:\n            break\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'ss_type\'] == \'remove\':\n        params[\'num_classes\'] = 147\n    elif params[\'ss_type\'] in [\'insert_left\', \'insert_right\']:\n        params[\'num_classes\'] = 151\n    elif params[\'ss_type\'] == \'insert_both\':\n        params[\'num_classes\'] = 155\n    else:\n        raise TypeError\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'])\n\n    model.save_path = args.model_path\n    do_decode(model=model, params=params,\n              epoch=args.epoch, beam_width=1,\n              eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/erato/visualization/decode_ctc.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decode the trained CTC outputs (ERATO corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.erato.data.load_dataset_ctc import Dataset\nfrom models.ctc.ctc import CTC\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.sparsetensor import sparsetensor2list\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_decode(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                   model.labels_pl_list[0],\n                                   model.inputs_seq_len_pl_list[0],\n                                   model.keep_prob_pl_list[0])\n    decode_op = model.decoder(\n        logits,\n        model.inputs_seq_len_pl_list[0],\n        beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        decode(session=sess,\n               decode_op=decode_op,\n               model=model,\n               dataset=test_data,\n               label_type=params[\'label_type\'],\n               ss_type=params[\'ss_type\'],\n               is_test=True,\n               eval_batch_size=1,\n               save_path=None)\n        #    save_path=model.save_path)\n\n\ndef decode(session, decode_op, model, dataset, label_type, ss_type,\n           is_test=False, eval_batch_size=None, save_path=None):\n    """"""Visualize label outputs of CTC model.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kana\n        ss_type (string): remove or insert_left or insert_both or insert_right\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        save_path (string, optional): path to save decoding results\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type + \'_\' + ss_type + \'.txt\')\n\n    if save_path is not None:\n        sys.stdout = open(join(model.model_dir, \'decode.txt\'), \'w\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n        try:\n            labels_pred = sparsetensor2list(\n                labels_pred_st, batch_size=batch_size)\n        except IndexError:\n            # no output\n            labels_pred = [\'\']\n\n        for i_batch in range(batch_size):\n            print(\'----- wav: %s -----\' % input_names[0][i_batch])\n            if \'char\' in label_type:\n                if is_test:\n                    str_true = labels_true[0][i_batch][0]\n                else:\n                    str_true = idx2char(labels_true[0][i_batch])\n                str_pred = idx2char(labels_pred[i_batch])\n            else:\n                if is_test:\n                    str_true = labels_true[0][i_batch][0]\n                else:\n                    str_true = idx2char(labels_true[0][i_batch])\n                str_pred = idx2char(labels_pred[i_batch])\n\n            print(\'Ref: %s\' % str_true)\n            print(\'Hyp: %s\' % str_pred)\n\n        if is_new_epoch:\n            break\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'ss_type\'] == \'remove\':\n        params[\'num_classes\'] = 147\n    elif params[\'ss_type\'] in [\'insert_left\', \'insert_right\']:\n        params[\'num_classes\'] = 151\n    elif params[\'ss_type\'] == \'insert_both\':\n        params[\'num_classes\'] = 155\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_decode(model=model, params=params,\n              epoch=args.epoch, beam_width=args.beam_width,\n              eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/erato/visualization/plot_ctc_posterior.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Plot the trained CTC posteriors (ERATO corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath, isdir\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\nimport shutil\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nplt.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set_style(""white"")\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.erato.data.load_dataset_ctc import Dataset\nfrom models.ctc.ctc import CTC\nfrom utils.directory import mkdir_join, mkdir\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_plot(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        ss_type=params[\'ss_type\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                   model.labels_pl_list[0],\n                                   model.inputs_seq_len_pl_list[0],\n                                   model.keep_prob_pl_list[0])\n    posteriors_op = model.posteriors(logits)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        plot(session=sess,\n             posteriors_op=posteriors_op,\n             model=model,\n             dataset=test_data,\n             label_type=params[\'label_type\'],\n             ss_type=params[\'ss_type\'],\n             num_stack=params[\'num_stack\'],\n             #  save_path=mkdir_join(model.save_path, \'ctc_output\'))\n             save_path=None)\n\n\ndef plot(session, posteriors_op, model, dataset, label_type, ss_type,\n         num_stack=1, save_path=None, show=False):\n    """"""Plot posteriors of phones.\n    Args:\n        session: session of training model\n        posteriois_op: operation for computing posteriors\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): kana\n        ss_type (string):\n        num_stack (int): the number of frames to stack\n        save_path (string, string): path to save ctc outputs\n        show (bool, optional): if True, show each figure\n    """"""\n    # Blank class is set to the last class in TensorFlow\n    if label_type == \'kana\':\n        if ss_type == \'remove\':\n            blank_index = 147\n        elif ss_type == \'insert_both\':\n            blank_index = 155\n        else:\n            blank_index = 151\n        laughter_index = 147\n        filler_index = 148\n        backchannel_index = 149\n        disfluency_index = 150\n\n    # Clean directory\n    if isdir(save_path):\n        shutil.rmtree(save_path)\n        mkdir(save_path)\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, _, inputs_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs,\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        # Visualize\n        batch_size, max_frame_num = inputs.shape[:2]\n        probs = session.run(posteriors_op, feed_dict=feed_dict)\n        probs = probs.reshape(-1, max_frame_num, model.num_classes)\n\n        # Visualize\n        for i_batch in range(batch_size):\n            prob = probs[i_batch][:int(inputs_seq_len[0]), :]\n\n            plt.clf()\n            plt.figure(figsize=(10, 4))\n            frame_num = int(inputs_seq_len[i_batch])\n            times_probs = np.arange(frame_num) * num_stack / 100\n\n            # NOTE: Blank class is set to the last class in TensorFlow\n            for i in range(0, prob.shape[-1] - 1, 1):\n                plt.plot(times_probs, prob[:, i])\n            plt.plot(times_probs, prob[:, -1],\n                     \':\', label=\'blank\', color=\'grey\')\n            plt.xlabel(\'Time [sec]\', fontsize=12)\n            plt.ylabel(\'Posteriors\', fontsize=12)\n            plt.xlim([0, frame_num * num_stack / 100])\n            plt.ylim([0.05, 1.05])\n            plt.xticks(list(range(0, int(frame_num * num_stack / 100) + 1, 1)))\n            plt.yticks(list(range(0, 2, 1)))\n            plt.legend(loc=""upper right"", fontsize=12)\n\n            if show:\n                plt.show()\n\n            # Save as a png file\n            if save_path is not None:\n                plt.savefig(join(save_path, input_names[0] + \'.png\'), dvi=500)\n\n        if is_new_epoch:\n            break\n\n        # # Plot\n        # if ss_type != \'remove\':\n        #     plt.subplot(211)\n        #     plt.plot(times_probs, probs[:, laughter_index],\n        #              label=\'Laughter\', color=orange, linewidth=2)\n        #     plt.plot(times_probs, probs[:, filler_index],\n        #              label=\'Filler\', color=green, linewidth=2)\n        #     plt.plot(times_probs, probs[:, backchannel_index],\n        #              label=\'Backchannel\', color=blue, linewidth=2)\n        #     plt.plot(times_probs, probs[:, disfluency_index],\n        #              label=\'Disfluency\', color=\'magenta\', linewidth=2)\n        #     plt.xlabel(\'Time[sec]\', fontsize=12)\n        #     plt.ylabel(\'social signals\', fontsize=12)\n        #     plt.xlim([0, duration])\n        #     plt.ylim([0.05, 1.05])\n        #     plt.xticks(list(range(0, int(len(probs) / 100) + 1, 1)))\n        #     plt.yticks(list(range(0, 2, 1)))\n        #     plt.legend(loc=""upper right"", fontsize=12)\n        #\n        #     # not social signals\n        #     plt.subplot(212)\n        #\n        #     plt.plot(times_probs, probs[:, 0],\n        #              label=\'Silence\', color=\'black\', linewidth=2)\n        #     for i in range(1, blank_index, 1):\n        #         plt.plot(times_probs, probs[:, i], linewidth=2)\n        #     plt.plot(times_probs, probs[:, blank_index], \':\',\n        #              label=\'Blank\', color=\'grey\', linewidth=2)\n        #     plt.xlabel(\'Time[sec]\', fontsize=12)\n        #     plt.ylabel(label_type, fontsize=12)\n        #     plt.xlim([0, duration])\n        #     plt.ylim([0.05, 1.05])\n        #     plt.xticks(list(range(0, int(len(probs) / 100) + 1, 1)))\n        #     plt.yticks(list(range(0, 2, 1)))\n        #     plt.legend(loc=""upper right"", fontsize=12)\n        #\n        #     if show:\n        #         plt.show()\n        #\n        #     # Save as a png file\n        #     if save_path is not None:\n        #         save_path = join(save_path, wav_index + \'.png\')\n        #         plt.savefig(save_path, dvi=500)\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'ss_type\'] == \'remove\':\n        params[\'num_classes\'] = 147\n    elif params[\'ss_type\'] == \'insert_left\':\n        params[\'num_classes\'] = 151\n    elif params[\'ss_type\'] == \'insert_both\':\n        params[\'num_classes\'] = 155\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_plot(model=model, params=params,\n            epoch=args.epoch, beam_width=args.beam_width,\n            eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/librispeech/data/load_dataset_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the CTC model (Librispeech corpus).\n   In addition, frame stacking and skipping are used.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.ctc import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, train_data_size, label_type, batch_size,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False, num_gpu=1):\n        """"""A class for loading dataset.\n        Args:\n            data_type (stirng): train or dev_clean or dev_other or\n                test_clean or test_other\n            train_data_size (string): train100h or train460h or train960h\n            label_type (stirng): character or character_capital_divide or word\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n            num_gpu (int, optional): if more than 1, divide batch_size by num_gpu\n        """"""\n        super(Dataset, self).__init__()\n\n        self.data_type = data_type\n        self.train_data_size = train_data_size\n        self.label_type = label_type\n        self.batch_size = batch_size * num_gpu\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = num_gpu\n\n        self.is_test = True if \'test\' in data_type else False\n        self.padded_value = -1 if not self.is_test else None\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/librispeech\',\n                        \'/n/sd8/inaguma/corpus/librispeech/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\',\n                          train_data_size, data_type)\n        # NOTE: ex.) save_path:\n        # librispeech_dataset_path/inputs/train_data_size/data_type/speaker/***.npy\n        label_path = join(dataset_root[0], \'labels\',\n                          train_data_size, data_type, label_type)\n        # NOTE: ex.) save_path:\n        # librispeech_dataset_path/labels/train_data_size/data_type/label_type/speaker/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\',\n                              train_data_size, data_type)\n            label_path = join(dataset_root[0], \'labels\',\n                              train_data_size, data_type, label_type)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for utt_name, frame_num in frame_num_tuple_sorted:\n            speaker = utt_name.split(\'-\')[0]\n            # ex.) utt_name: speaker-book-utt_index\n            input_paths.append(join(input_path, speaker, utt_name + \'.npy\'))\n            label_paths.append(join(label_path, speaker, utt_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/librispeech/data/load_dataset_multitask_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the multitask CTC model (Librispeech corpus).\n   In addition, frame stacking and skipping are used.\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.multitask_ctc import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, train_data_size, label_type_main,\n                 label_type_sub, batch_size,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False, num_gpu=1, is_gpu=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (stirng): train or dev_clean or dev_other or\n                test_clean or test_other\n            train_data_size (string): train100h or train460h or train960h\n            label_type_main (stirng): word\n            label_type_sub (stirng): character or character_capital_divide\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n            num_gpu (int, optional): if more than 1, divide batch_size by num_gpu\n        """"""\n        super(Dataset, self).__init__()\n\n        self.data_type = data_type\n        self.train_data_size = train_data_size\n        self.label_type_main = label_type_main\n        self.label_type_sub = label_type_sub\n        self.batch_size = batch_size * num_gpu\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = num_gpu\n        self.padded_value = -1\n\n        self.is_training = True if data_type == \'train\' else False\n        self.is_test = True if \'test\' in data_type else False\n        self.padded_value = -1 if not self.is_test else None\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/librispeech\',\n                        \'/n/sd8/inaguma/corpus/librispeech/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\',\n                          train_data_size, data_type)\n        label_main_path = join(dataset_root[0], \'labels\',\n                               train_data_size, data_type, label_type_main)\n        label_sub_path = join(dataset_root[0], \'labels\',\n                              train_data_size, data_type, label_type_sub)\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\', train_data_size,\n                              data_type)\n            label_main_path = join(dataset_root[0], \'labels\',\n                                   train_data_size, data_type, label_type_main)\n            label_sub_path = join(dataset_root[0], \'labels\',\n                                  train_data_size, data_type, label_type_sub)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_main_paths, label_sub_paths = [], [], []\n        for utt_name, frame_num in frame_num_tuple_sorted:\n            speaker = utt_name.split(\'-\')[0]\n            # ex.) utt_name: speaker-book-utt_index\n            input_paths.append(join(input_path, speaker, utt_name + \'.npy\'))\n            label_main_paths.append(\n                join(label_main_path, speaker, utt_name + \'.npy\'))\n            label_sub_paths.append(\n                join(label_sub_path, speaker, utt_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_main_paths = np.array(label_main_paths)\n        self.label_sub_paths = np.array(label_sub_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/librispeech/data/load_dataset_xe.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the frame-wise model (Librispeech corpus).\n   You can use the multi-GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join\nimport numpy as np\nfrom glob import glob\n\nfrom utils.dataset.xe import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, model_path, data_type, batch_size, max_epoch=None,\n                 num_gpu=1):\n        """"""A class for loading dataset.\n        Args:\n            model_path (string): path to the saved model\n            data_type (string): character\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            num_gpu (int, optional): if more than 1, divide batch_size by num_gpu\n        """"""\n        super(Dataset, self).__init__()\n\n        self.model_path = model_path\n        self.data_type = data_type\n        self.batch_size = batch_size * num_gpu\n        self.max_epoch = max_epoch\n        self.num_gpu = num_gpu\n\n        input_path = join(model_path, data_type, \'inputs\')\n        label_path = join(model_path, data_type, \'labels\')\n        # NOTE: block0.npy, block1.npy ...\n\n        # Sort paths to input & label\n        input_paths, label_paths = [], []\n        for file_name in glob(join(input_path, \'*.npy\')):\n            input_paths.append(file_name)\n        for file_name in glob(join(label_path, \'*.npy\')):\n            label_paths.append(file_name)\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest_block = set(range(0, len(self.input_paths), 1))\n        # NOTE: len(self.rest_block) == num_blocks\n\n        self.rest_frames = set([])\n'"
examples/librispeech/evaluation/eval_ctc.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the trained CTC model (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom experiments.librispeech.metrics.ctc import do_eval_cer, do_eval_cer2, do_eval_wer\nfrom models.ctc.ctc import CTC\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=-1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\n\nDECODER_TYPE = 2\n# NOTE:\n# DECODER_TYPE == 1: tensorflow native beam search decoder\n# DECODER_TYPE == 2: numpy implementation of beam search decoder\n\n\ndef do_eval(model, params, epoch, eval_batch_size, beam_width):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        eval_batch_size (int): the size of mini-batch when evaluation\n        beam_width (int): beam_width (int, optional): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n    """"""\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                       model.labels_pl_list[0],\n                                       model.inputs_seq_len_pl_list[0],\n                                       model.keep_prob_pl_list[0],\n                                       is_training=False)\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=beam_width)\n        posteriors_op = model.posteriors(logits)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'Test Data Evaluation:\')\n        if \'char\' in params[\'label_type\']:\n            if DECODER_TYPE == 1:\n                cer_clean_test, wer_clean_test = do_eval_cer(\n                    session=sess,\n                    decode_ops=[decode_op],\n                    model=model,\n                    dataset=test_clean_data,\n                    label_type=params[\'label_type\'],\n                    is_test=True,\n                    # eval_batch_size=eval_batch_size,\n                    progressbar=True)\n                print(\'  WER (clean): %f %%\' % (wer_clean_test * 100))\n                print(\'  CER (clean): %f %%\' % (cer_clean_test * 100))\n\n                cer_other_test, wer_other_test = do_eval_cer(\n                    session=sess,\n                    decode_ops=[decode_op],\n                    model=model,\n                    dataset=test_other_data,\n                    label_type=params[\'label_type\'],\n                    is_test=True,\n                    # eval_batch_size=eval_batch_size,\n                    progressbar=True)\n                print(\'  WER (other): %f %%\' % (wer_other_test * 100))\n                print(\'  CER (other): %f %%\' % (cer_other_test * 100))\n\n            elif DECODER_TYPE == 2:\n                cer_clean_test, wer_clean_test = do_eval_cer2(\n                    session=sess,\n                    # beam_width=beam_width,\n                    beam_width=20,\n                    posteriors_ops=[posteriors_op],\n                    model=model,\n                    dataset=test_clean_data,\n                    label_type=params[\'label_type\'],\n                    is_test=True,\n                    eval_batch_size=20,\n                    # eval_batch_size=eval_batch_size,\n                    progressbar=True)\n                print(\'  WER (clean): %f %%\' % (wer_clean_test * 100))\n                print(\'  CER (clean): %f %%\' % (cer_clean_test * 100))\n\n                cer_other_test, wer_other_test = do_eval_cer2(\n                    session=sess,\n                    # beam_width=beam_width,\n                    beam_width=20,\n                    posteriors_ops=[posteriors_op],\n                    model=model,\n                    dataset=test_other_data,\n                    label_type=params[\'label_type\'],\n                    is_test=True,\n                    eval_batch_size=20,\n                    # eval_batch_size=eval_batch_size,\n                    progressbar=True)\n                print(\'  WER (other): %f %%\' % (wer_other_test * 100))\n                print(\'  CER (other): %f %%\' % (cer_other_test * 100))\n\n        else:\n            wer_clean_test = do_eval_wer(\n                session=sess,\n                decode_ops=[decode_op],\n                model=model,\n                dataset=test_clean_data,\n                train_data_size=params[\'train_data_size\'],\n                is_test=True,\n                # eval_batch_size=eval_batch_size,\n                progressbar=True)\n            print(\'  WER (clean): %f %%\' % (wer_clean_test * 100))\n\n            wer_other_test = do_eval_wer(\n                session=sess,\n                decode_ops=[decode_op],\n                model=model,\n                dataset=test_other_data,\n                train_data_size=params[\'train_data_size\'],\n                is_test=True,\n                # eval_batch_size=eval_batch_size,\n                progressbar=True)\n            print(\'  WER (other): %f %%\' % (wer_other_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes\'] = 72\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes\'] = 77\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes\'] = 77\n    elif params[\'label_type\'] == \'word_freq10\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes\'] = 7213\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes\'] = 18641\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes\'] = 26642\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, eval_batch_size=args.eval_batch_size,\n            beam_width=args.beam_width)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/librispeech/evaluation/eval_ctc_temp.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the CTC model (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(os.path.abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom experiments.librispeech.metrics.ctc import do_eval_cer, do_eval_wer\nfrom models.ctc.ctc import CTC\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=-1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\nparser.add_argument(\'--temperature\', type=int, default=1,\n                    help=\'temperature parameter\')\n\n\ndef do_eval(model, params, epoch, beam_width, eval_batch_size, temperature):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n        temperature (int):\n    """"""\n    print(\'=\' * 30)\n    print(\'  frame stack %d\' % int(params[\'num_stack\']))\n    print(\'  splice %d\' % int(params[\'splice\']))\n    print(\'  beam width: %d\' % beam_width)\n    print(\'  temperature (training): %d\' % params[\'softmax_temperature\'])\n    print(\'  temperature (inference): %d\' % temperature)\n    print(\'=\' * 30)\n\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\') as scope:\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0],\n            scope,\n            softmax_temperature=temperature,  # this is for training\n            # is_training=False)\n            is_training=True)\n        logits /= temperature\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'Test Data Evaluation:\')\n        cer_clean_test, wer_clean_test = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op],\n            model=model,\n            dataset=test_clean_data,\n            label_type=params[\'label_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER (clean): %f %%\' % (cer_clean_test * 100))\n        print(\'  WER (clean): %f %%\' % (wer_clean_test * 100))\n\n        cer_other_test, wer_other_test = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op],\n            model=model,\n            dataset=test_other_data,\n            label_type=params[\'label_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER (other): %f %%\' % (cer_other_test * 100))\n        print(\'  WER (other): %f %%\' % (wer_other_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(os.path.join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n\n    # Model setting\n    model = CTC(\n        encoder_type=params[\'encoder_type\'],\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        splice=params[\'splice\'],\n        num_units=params[\'num_units\'],\n        num_layers=params[\'num_layers\'],\n        num_classes=params[\'num_classes\'],\n        lstm_impl=params[\'lstm_impl\'],\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation=params[\'clip_activation\'],\n        num_proj=params[\'num_proj\'],\n        weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, beam_width=args.beam_width,\n            eval_batch_size=args.eval_batch_size,\n            temperature=args.temperature)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/librispeech/evaluation/eval_ensemble2_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the ensemble of 2 CTC models (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom experiments.librispeech.evaluation.eval_ensemble4_ctc import do_eval_cer\nfrom utils.io.labels.character import Idx2char, Char2idx\nfrom utils.evaluation.edit_distance import compute_cer, compute_wer, wer_align\nfrom models.ctc.decoders.beam_search_decoder import BeamSearchDecoder\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--result_save_path\', type=str, default=None,\n                    help=\'path to save results of ensemble\')\n\nparser.add_argument(\'--model1_path\', type=str,\n                    help=\'path to the 1st model to evaluate\')\nparser.add_argument(\'--model2_path\', type=str,\n                    help=\'path to the 2nd model to evaluate\')\n\nparser.add_argument(\'--epoch_model1\', type=int, default=-1,\n                    help=\'the epoch of 1st model to restore\')\nparser.add_argument(\'--epoch_model2\', type=int, default=-1,\n                    help=\'the epoch of 2nd model to restore\')\n\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--temperature_infer\', type=int, default=1,\n                    help=\'temperature parameter in the inference stage\')\n\n\ndef do_eval(save_paths, params, beam_width, temperature_infer,\n            result_save_path):\n    """"""Evaluate the model.\n    Args:\n        save_paths (list):\n        params (dict): A dictionary of parameters\n        epoch_list (list): list of the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n        temperature_infer (int): temperature in the inference stage\n        result_save_path (string, optional):\n    """"""\n    if \'temp1\' in save_paths[0]:\n        temperature_train = 1\n    elif \'temp2\' in save_paths[0]:\n        temperature_train = 2\n    else:\n        raise ValueError\n\n    if result_save_path is not None:\n        sys.stdout = open(join(result_save_path,\n                               \'2models_traintemp\' + str(temperature_train) +\n                               \'_inftemp\' + str(temperature_infer) + \'.log\'), \'w\')\n\n    print(\'=\' * 30)\n    print(\'  frame stack %d\' % int(params[\'num_stack\']))\n    print(\'  beam width: %d\' % beam_width)\n    print(\'  ensemble: %d\' % len(save_paths))\n    print(\'  temperature (training): %d\' % temperature_train)\n    print(\'  temperature (inference): %d\' % temperature_infer)\n    print(\'=\' * 30)\n\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=1, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=1, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True)\n\n    print(\'Test Data Evaluation:\')\n    cer_clean_test, wer_clean_test = do_eval_cer(\n        save_paths=save_paths,\n        dataset=test_clean_data,\n        data_type=\'test_clean\',\n        label_type=params[\'label_type\'],\n        num_classes=params[\'num_classes\'] + 1,\n        beam_width=beam_width,\n        temperature_infer=temperature_infer,\n        is_test=True,\n        progressbar=True)\n    print(\'  CER (clean): %f %%\' % (cer_clean_test * 100))\n    print(\'  WER (clean): %f %%\' % (wer_clean_test * 100))\n\n    cer_other_test, wer_other_test = do_eval_cer(\n        save_paths=save_paths,\n        dataset=test_other_data,\n        data_type=\'test_other\',\n        label_type=params[\'label_type\'],\n        num_classes=params[\'num_classes\'] + 1,\n        beam_width=beam_width,\n        temperature_infer=temperature_infer,\n        is_test=True,\n        progressbar=True)\n    print(\'  CER (other): %f %%\' % (cer_other_test * 100))\n    print(\'  WER (other): %f %%\' % (wer_other_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model1_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    else:\n        raise TypeError\n\n    save_paths = [args.model1_path, args.model2_path]\n\n    do_eval(save_paths=save_paths, params=params,\n            beam_width=args.beam_width,\n            temperature_infer=args.temperature_infer,\n            result_save_path=args.result_save_path)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    main()\n'"
examples/librispeech/evaluation/eval_ensemble4_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the ensemble of 4 CTC models (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport yaml\nimport argparse\nimport re\nfrom tqdm import tqdm\nimport numpy as np\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom utils.io.labels.character import Idx2char, Char2idx\nfrom utils.evaluation.edit_distance import compute_cer, compute_wer, wer_align\nfrom models.ctc.decoders.beam_search_decoder import BeamSearchDecoder\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--result_save_path\', type=str, default=None,\n                    help=\'path to save results of ensemble\')\n\nparser.add_argument(\'--model1_path\', type=str,\n                    help=\'path to the 1st model to evaluate\')\nparser.add_argument(\'--model2_path\', type=str,\n                    help=\'path to the 2nd model to evaluate\')\nparser.add_argument(\'--model3_path\', type=str,\n                    help=\'path to the 3rd model to evaluate\')\nparser.add_argument(\'--model4_path\', type=str,\n                    help=\'path to the 4th model to evaluate\')\n\nparser.add_argument(\'--epoch_model1\', type=int, default=-1,\n                    help=\'the epoch of 1st model to restore\')\nparser.add_argument(\'--epoch_model2\', type=int, default=-1,\n                    help=\'the epoch of 2nd model to restore\')\nparser.add_argument(\'--epoch_model3\', type=int, default=-1,\n                    help=\'the epoch of 3rd model to restore\')\nparser.add_argument(\'--epoch_model4\', type=int, default=-1,\n                    help=\'the epoch of 4th model to restore\')\n\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--temperature_infer\', type=int, default=1,\n                    help=\'temperature parameter in the inference stage\')\n\n\ndef do_eval(save_paths, params, beam_width, temperature_infer,\n            result_save_path):\n    """"""Evaluate the model.\n    Args:\n        save_paths (list):\n        params (dict): A dictionary of parameters\n        epoch_list (list): list of the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n        temperature_infer (int): temperature in the inference stage\n        result_save_path (string, optional):\n    """"""\n    if \'temp1\' in save_paths[0]:\n        temperature_train = 1\n    elif \'temp2\' in save_paths[0]:\n        temperature_train = 2\n    else:\n        raise ValueError\n\n    if result_save_path is not None:\n        sys.stdout = open(join(result_save_path,\n                               \'4models_traintemp\' + str(temperature_train) +\n                               \'_inftemp\' + str(temperature_infer) + \'.log\'), \'w\')\n\n    print(\'=\' * 30)\n    print(\'  frame stack %d\' % int(params[\'num_stack\']))\n    print(\'  beam width: %d\' % beam_width)\n    print(\'  ensemble: %d\' % len(save_paths))\n    print(\'  temperature (training): %d\' % temperature_train)\n    print(\'  temperature (inference): %d\' % temperature_infer)\n    print(\'=\' * 30)\n\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=1, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=1, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True)\n\n    print(\'Test Data Evaluation:\')\n    cer_clean_test, wer_clean_test = do_eval_cer(\n        save_paths=save_paths,\n        dataset=test_clean_data,\n        data_type=\'test_clean\',\n        label_type=params[\'label_type\'],\n        num_classes=params[\'num_classes\'] + 1,\n        beam_width=beam_width,\n        temperature_infer=temperature_infer,\n        is_test=True,\n        progressbar=True)\n    print(\'  CER (clean): %f %%\' % (cer_clean_test * 100))\n    print(\'  WER (clean): %f %%\' % (wer_clean_test * 100))\n\n    cer_other_test, wer_other_test = do_eval_cer(\n        save_paths=save_paths,\n        dataset=test_other_data,\n        data_type=\'test_other\',\n        label_type=params[\'label_type\'],\n        num_classes=params[\'num_classes\'] + 1,\n        beam_width=beam_width,\n        temperature_infer=temperature_infer,\n        is_test=True,\n        progressbar=True)\n    print(\'  CER (other): %f %%\' % (cer_other_test * 100))\n    print(\'  WER (other): %f %%\' % (wer_other_test * 100))\n\n\ndef do_eval_cer(save_paths, dataset, data_type, label_type, num_classes,\n                beam_width, temperature_infer,\n                is_test=False, progressbar=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        save_paths (list):\n        dataset: An instance of a `Dataset` class\n        data_type (string):\n        label_type (string): character\n        num_classes (int):\n        beam_width (int): the size of beam\n        temperature (int): temperature in the inference stage\n        is_test (bool, optional): set to True when evaluating by the test set\n        progressbar (bool, optional): if True, visualize the progressbar\n    Return:\n        cer_mean (float): An average of CER\n        wer_mean (float): An average of WER\n    """"""\n    if label_type == \'character\':\n        idx2char = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n        char2idx = Char2idx(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n    else:\n        raise TypeError\n\n    # Define decoder\n    decoder = BeamSearchDecoder(space_index=char2idx(str_char=\'_\')[0],\n                                blank_index=num_classes - 1)\n\n    ##################################################\n    # Compute mean probabilities\n    ##################################################\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    cer_mean, wer_mean = 0, 0\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, input_names = data\n\n        batch_size = inputs[0].shape[0]\n        for i_batch in range(batch_size):\n            probs_ensemble = None\n            for i_model in range(len(save_paths)):\n\n                # Load posteriors\n                speaker = input_names[0][i_batch].split(\'-\')[0]\n                prob_save_path = join(\n                    save_paths[i_model], \'temp\' + str(temperature_infer),\n                    data_type, \'probs_utt\', speaker,\n                    input_names[0][i_batch] + \'.npy\')\n                probs_model_i = np.load(prob_save_path)\n                # NOTE: probs_model_i: `[T, num_classes]`\n\n                # Sum over probs\n                if probs_ensemble is None:\n                    probs_ensemble = probs_model_i\n                else:\n                    probs_ensemble += probs_model_i\n\n            # Compute mean posteriors\n            probs_ensemble /= len(save_paths)\n\n            # Decode per utterance\n            labels_pred, scores = decoder(\n                probs=probs_ensemble[np.newaxis, :, :],\n                seq_len=inputs_seq_len[0][i_batch: i_batch + 1],\n                beam_width=beam_width)\n\n            # Convert from list of index to string\n            if is_test:\n                str_true = labels_true[0][i_batch][0]\n                # NOTE: transcript is seperated by space(\'_\')\n            else:\n                str_true = idx2char(labels_true[0][i_batch],\n                                    padded_value=dataset.padded_value)\n            str_pred = idx2char(labels_pred[0])\n\n            # Remove consecutive spaces\n            str_pred = re.sub(r\'[_]+\', \'_\', str_pred)\n\n            # Remove garbage labels\n            str_true = re.sub(r\'[\\\']+\', \'\', str_true)\n            str_pred = re.sub(r\'[\\\']+\', \'\', str_pred)\n\n            # Compute WER\n            wer_mean += compute_wer(ref=str_pred.split(\'_\'),\n                                    hyp=str_true.split(\'_\'),\n                                    normalize=True)\n            # substitute, insert, delete = wer_align(\n            #     ref=str_true.split(\'_\'),\n            #     hyp=str_pred.split(\'_\'))\n            # print(\'SUB: %d\' % substitute)\n            # print(\'INS: %d\' % insert)\n            # print(\'DEL: %d\' % delete)\n\n            # Remove spaces\n            str_true = re.sub(r\'[_]+\', \'\', str_true)\n            str_pred = re.sub(r\'[_]+\', \'\', str_pred)\n\n            # Compute CER\n            cer_mean += compute_cer(str_pred=str_pred,\n                                    str_true=str_true,\n                                    normalize=True)\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= (len(dataset))\n    wer_mean /= (len(dataset))\n    # TODO: Fix this\n\n    return cer_mean, wer_mean\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model1_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    else:\n        raise TypeError\n\n    save_paths = [args.model1_path, args.model2_path,\n                  args.model3_path, args.model4_path]\n\n    do_eval(save_paths=save_paths, params=params,\n            beam_width=args.beam_width,\n            temperature_infer=args.temperature_infer,\n            result_save_path=args.result_save_path)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    main()\n'"
examples/librispeech/evaluation/eval_ensemble8_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the ensemble of 8 CTC models (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom experiments.librispeech.evaluation.eval_ensemble4_ctc import do_eval_cer\nfrom utils.io.labels.character import Idx2char, Char2idx\nfrom utils.evaluation.edit_distance import compute_cer, compute_wer, wer_align\nfrom models.ctc.decoders.beam_search_decoder import BeamSearchDecoder\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--result_save_path\', type=str, default=None,\n                    help=\'path to save results of ensemble\')\n\nparser.add_argument(\'--model1_path\', type=str,\n                    help=\'path to the 1st model to evaluate\')\nparser.add_argument(\'--model2_path\', type=str,\n                    help=\'path to the 2nd model to evaluate\')\nparser.add_argument(\'--model3_path\', type=str,\n                    help=\'path to the 3rd model to evaluate\')\nparser.add_argument(\'--model4_path\', type=str,\n                    help=\'path to the 4th model to evaluate\')\nparser.add_argument(\'--model5_path\', type=str,\n                    help=\'path to the 5th model to evaluate\')\nparser.add_argument(\'--model6_path\', type=str,\n                    help=\'path to the 6th model to evaluate\')\nparser.add_argument(\'--model7_path\', type=str,\n                    help=\'path to the 7th model to evaluate\')\nparser.add_argument(\'--model8_path\', type=str,\n                    help=\'path to the 8th model to evaluate\')\n\nparser.add_argument(\'--epoch_model1\', type=int, default=-1,\n                    help=\'the epoch of 1st model to restore\')\nparser.add_argument(\'--epoch_model2\', type=int, default=-1,\n                    help=\'the epoch of 2nd model to restore\')\nparser.add_argument(\'--epoch_model3\', type=int, default=-1,\n                    help=\'the epoch of 3rd model to restore\')\nparser.add_argument(\'--epoch_model4\', type=int, default=-1,\n                    help=\'the epoch of 4th model to restore\')\nparser.add_argument(\'--epoch_model5\', type=int, default=-1,\n                    help=\'the epoch of 5th model to restore\')\nparser.add_argument(\'--epoch_model6\', type=int, default=-1,\n                    help=\'the epoch of 6th model to restore\')\nparser.add_argument(\'--epoch_model7\', type=int, default=-1,\n                    help=\'the epoch of 7th model to restore\')\nparser.add_argument(\'--epoch_model8\', type=int, default=-1,\n                    help=\'the epoch of 8th model to restore\')\n\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--temperature_infer\', type=int, default=1,\n                    help=\'temperature parameter in the inference stage\')\n\n\ndef do_eval(save_paths, params, beam_width, temperature_infer,\n            result_save_path):\n    """"""Evaluate the model.\n    Args:\n        save_paths (list):\n        params (dict): A dictionary of parameters\n        epoch_list (list): list of the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n        temperature_infer (int): temperature in the inference stage\n        result_save_path (string, optional):\n    """"""\n    if \'temp1\' in save_paths[0]:\n        temperature_train = 1\n    elif \'temp2\' in save_paths[0]:\n        temperature_train = 2\n    else:\n        raise ValueError\n\n    if result_save_path is not None:\n        sys.stdout = open(join(result_save_path,\n                               \'8models_traintemp\' + str(temperature_train) +\n                               \'_inftemp\' + str(temperature_infer) + \'.log\'), \'w\')\n\n    print(\'=\' * 30)\n    print(\'  frame stack %d\' % int(params[\'num_stack\']))\n    print(\'  beam width: %d\' % beam_width)\n    print(\'  ensemble: %d\' % len(save_paths))\n    print(\'  temperature (training): %d\' % temperature_train)\n    print(\'  temperature (inference): %d\' % temperature_infer)\n    print(\'=\' * 30)\n\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=1, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=1, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True)\n\n    print(\'Test Data Evaluation:\')\n    cer_clean_test, wer_clean_test = do_eval_cer(\n        save_paths=save_paths,\n        dataset=test_clean_data,\n        data_type=\'test_clean\',\n        label_type=params[\'label_type\'],\n        num_classes=params[\'num_classes\'] + 1,\n        beam_width=beam_width,\n        temperature_infer=temperature_infer,\n        is_test=True,\n        progressbar=True)\n    print(\'  CER (clean): %f %%\' % (cer_clean_test * 100))\n    print(\'  WER (clean): %f %%\' % (wer_clean_test * 100))\n\n    cer_other_test, wer_other_test = do_eval_cer(\n        save_paths=save_paths,\n        dataset=test_other_data,\n        data_type=\'test_other\',\n        label_type=params[\'label_type\'],\n        num_classes=params[\'num_classes\'] + 1,\n        beam_width=beam_width,\n        temperature_infer=temperature_infer,\n        is_test=True,\n        progressbar=True)\n    print(\'  CER (other): %f %%\' % (cer_other_test * 100))\n    print(\'  WER (other): %f %%\' % (wer_other_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model1_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    else:\n        raise TypeError\n\n    save_paths = [args.model1_path, args.model2_path,\n                  args.model3_path, args.model4_path,\n                  args.model5_path, args.model6_path,\n                  args.model7_path, args.model8_path]\n\n    do_eval(save_paths=save_paths, params=params,\n            beam_width=args.beam_width,\n            temperature_infer=args.temperature_infer,\n            result_save_path=args.result_save_path)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    main()\n'"
examples/librispeech/evaluation/eval_multitask_ctc.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the multi-task CTC model (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_multitask_ctc import Dataset\nfrom experiments.librispeech.metrics.ctc import do_eval_cer, do_eval_wer\nfrom models.ctc.multitask_ctc import MultitaskCTC\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=-1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\n\n\ndef do_eval(model, params, epoch, beam_width, eval_batch_size):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'dev_clean\', train_data_size=params[\'train_data_size\'],\n        label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_other_data = Dataset(\n        data_type=\'dev_other\', train_data_size=params[\'train_data_size\'],\n        label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits_word, logits_char = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.labels_sub_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0])\n        decode_op_word, decode_op_char = model.decoder(\n            logits_word, logits_char,\n            model.inputs_seq_len_pl_list[0],\n            beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'Test Data Evaluation:\')\n        wer_clean_test = do_eval_wer(\n            session=sess,\n            decode_ops=[decode_op_word],\n            model=model,\n            dataset=test_clean_data,\n            train_data_size=params[\'train_data_size\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            is_multitask=True,\n            progressbar=True)\n        print(\'  WER (clean, word CTC): %f %%\' % (wer_clean_test * 100))\n\n        wer_other_test = do_eval_wer(\n            session=sess,\n            decode_ops=[decode_op_word],\n            model=model,\n            dataset=test_other_data,\n            train_data_size=params[\'train_data_size\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            is_multitask=True,\n            progressbar=True)\n        print(\'  WER (other, word CTC): %f %%\' % (wer_other_test * 100))\n\n        cer_clean_test, wer_clean_test = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op_char],\n            model=model,\n            dataset=test_clean_data,\n            label_type=params[\'label_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            is_multitask=True,\n            progressbar=True)\n        print(\'  WER (clean, char CTC): %f %%\' % (wer_clean_test * 100))\n        print(\'  CER (clean, char CTC): %f %%\' % (cer_clean_test * 100))\n\n        cer_other_test, wer_other_test = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op_char],\n            model=model,\n            dataset=test_other_data,\n            label_type=params[\'label_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            is_multitask=True,\n            progressbar=True)\n        print(\'  WER (other, char CTC): %f %%\' % (wer_other_test * 100))\n        print(\'  CER (other, char CTC): %f %%\' % (cer_other_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type_main\'] == \'word_freq10\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes_main\'] = 7213\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes_main\'] = 18641\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes_main\'] = 26642\n    else:\n        raise TypeError\n\n    if params[\'label_type_sub\'] == \'character\':\n        params[\'num_classes_sub\'] = 28\n    elif params[\'label_type_sub\'] == \'character_capital_divide\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes_sub\'] = 72\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes_sub\'] = 77\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes_sub\'] = 77\n    else:\n        raise TypeError\n\n    # Model setting\n    model = MultitaskCTC(encoder_type=params[\'encoder_type\'],\n                         input_size=params[\'input_size\'],\n                         splice=params[\'splice\'],\n                         num_stack=params[\'num_stack\'],\n                         num_units=params[\'num_units\'],\n                         num_layers_main=params[\'num_layers_main\'],\n                         num_layers_sub=params[\'num_layers_sub\'],\n                         num_classes_main=params[\'num_classes_main\'],\n                         num_classes_sub=params[\'num_classes_sub\'],\n                         main_task_weight=params[\'main_task_weight\'],\n                         lstm_impl=params[\'lstm_impl\'],\n                         use_peephole=params[\'use_peephole\'],\n                         parameter_init=params[\'weight_init\'],\n                         clip_grad_norm=params[\'clip_grad_norm\'],\n                         clip_activation=params[\'clip_activation\'],\n                         num_proj=params[\'num_proj\'],\n                         weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, beam_width=args.beam_width,\n            eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/librispeech/evaluation/eval_student.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the student model (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(os.path.abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom experiments.librispeech.metrics.ctc import do_eval_cer, do_eval_wer\nfrom models.ctc.student_ctc import StudentCTC\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=-1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\nparser.add_argument(\'--temperature\', type=int, default=1,\n                    help=\'temperature parameter\')\n\n\ndef do_eval(model, params, epoch, beam_width, eval_batch_size, temperature):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n        temperature (int):\n    """"""\n    if \'temp1\' in params[\'teacher_model_path\']:\n        teacher_train_temperature = 1\n    elif \'temp2\' in params[\'teacher_model_path\']:\n        teacher_train_temperature = 2\n\n    print(\'=\' * 40)\n    print(\'  frame stack %d\' % int(params[\'num_stack\']))\n    print(\'  splice %d\' % int(params[\'splice\']))\n    print(\'  beam width: %d\' % beam_width)\n    print(\'  temperature (teacher, training): %d\' % teacher_train_temperature)\n    print(\'  temperature (teacher, inference): %d\' %\n          params[\'teacher_temperature\'])\n    print(\'  temperature (training): %d\' % params[\'student_temperature\'])\n    print(\'  temperature (inference): %d\' % temperature)\n    print(\'=\' * 40)\n\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\') as scope:\n        # Define placeholders\n        model.create_placeholders_ctc()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_ctc_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0],\n            scope,\n            softmax_temperature=temperature,  # this is for training\n            # is_training=False)\n            is_training=True)\n        logits /= temperature\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'Test Data Evaluation:\')\n        cer_clean_test, wer_clean_test = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op],\n            model=model,\n            dataset=test_clean_data,\n            label_type=params[\'label_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER (clean): %f %%\' % (cer_clean_test * 100))\n        print(\'  WER (clean): %f %%\' % (wer_clean_test * 100))\n\n        cer_other_test, wer_other_test = do_eval_cer(\n            session=sess,\n            decode_ops=[decode_op],\n            model=model,\n            dataset=test_other_data,\n            label_type=params[\'label_type\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True)\n        print(\'  CER (other): %f %%\' % (cer_other_test * 100))\n        print(\'  WER (other): %f %%\' % (wer_other_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(os.path.join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n\n    if params[\'encoder_type\'] == \'student_cnn_compact_xe\':\n        params[\'encoder_type\'] = \'student_cnn_compact\'\n    elif params[\'encoder_type\'] == \'student_cnn_xe\':\n        params[\'encoder_type\'] = \'student_cnn\'\n\n    # Model setting\n    model = StudentCTC(\n        encoder_type=params[\'encoder_type\'],\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'],\n        num_classes=params[\'num_classes\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, beam_width=args.beam_width,\n            eval_batch_size=args.eval_batch_size,\n            temperature=args.temperature)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/librispeech/evaluation/save_ctc_prob.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Save the trained CTC posteriors (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport yaml\nimport argparse\nfrom tqdm import tqdm\nimport random\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom models.ctc.ctc import CTC\nfrom utils.directory import mkdir_join\nfrom utils.io.inputs.splicing import do_splice\nfrom utils.parallel import make_parallel\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=-1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\nparser.add_argument(\'--temperature\', type=int, default=1,\n                    help=\'temperature parameter\')\n\nTOTAL_NUM_FRAMES_DICT = {\n    ""train"": 18088388,\n    ""dev_clean"": 968057,\n    ""dev_other"": 919980,\n    ""test_clean"": 970953,\n    ""test_other"": 959587\n}\n\nNUM_POOLS = 10\n\n\ndef do_save(model, params, epoch, eval_batch_size, temperature):\n    """"""Save the CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        eval_batch_size (int): the size of mini-batch in evaluation\n        temperature (int):\n    """"""\n    print(\'=\' * 30)\n    print(\'  frame stack %d\' % int(params[\'num_stack\']))\n    print(\'  splice %d\' % int(params[\'splice\']))\n    print(\'  temperature (training): %d\' % temperature)\n    print(\'=\' * 30)\n\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        max_epoch=3, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=1)\n    dev_clean_data = Dataset(\n        data_type=\'dev_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        max_epoch=3, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=1)\n    dev_other_data = Dataset(\n        data_type=\'dev_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        max_epoch=3, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=1)\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        max_epoch=3, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=1)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        max_epoch=3, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=1)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0])\n        logits /= temperature\n        posteriors_op = model.posteriors(logits, blank_prior=1)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        #########################\n        # Save soft targets\n        #########################\n        # train100h\n        # save(session=sess,\n        #      posteriors_op=posteriors_op,\n        #      model=model,\n        #      dataset=train_data,\n        #      data_type=\'train\',\n        #      num_stack=params[\'num_stack\'],\n        #      save_prob=False,\n        #      save_soft_targets=True,\n        #      save_path=mkdir_join(model.save_path, \'temp\' + str(temperature), \'train\'))\n\n        # dev\n        # save(session=sess,\n        #      posteriors_op=posteriors_op,\n        #      model=model,\n        #      dataset=dev_clean_data,\n        #      data_type=\'dev_clean\',\n        #      num_stack=params[\'num_stack\'],\n        #      save_prob=False,\n        #      save_soft_targets=True,\n        #      save_path=mkdir_join(model.save_path, \'temp\' + str(temperature), \'dev_clean\'))\n        # save(session=sess,\n        #      posteriors_op=posteriors_op,\n        #      model=model,\n        #      dataset=dev_other_data,\n        #      data_type=\'dev_other\',\n        #      num_stack=params[\'num_stack\'],\n        #      save_prob=False,\n        #      save_soft_targets=True,\n        #      save_path=mkdir_join(model.save_path, \'temp\' + str(temperature), \'dev_other\'))\n\n        # test\n        save(session=sess,\n             posteriors_op=posteriors_op,\n             model=model,\n             dataset=test_clean_data,\n             data_type=\'test_clean\',\n             num_stack=params[\'num_stack\'],\n             save_prob=True,\n             save_soft_targets=False,\n             save_path=mkdir_join(model.save_path, \'temp\' + str(temperature), \'test_clean\'))\n        save(session=sess,\n             posteriors_op=posteriors_op,\n             model=model,\n             dataset=test_other_data,\n             data_type=\'test_other\',\n             num_stack=params[\'num_stack\'],\n             save_prob=True,\n             save_soft_targets=False,\n             save_path=mkdir_join(model.save_path, \'temp\' + str(temperature), \'test_other\'))\n\n\ndef save(session, posteriors_op, model, dataset, data_type,\n         save_prob=False, save_soft_targets=False,\n         num_stack=1, save_path=None):\n\n    # Initialize\n    pbar = tqdm(total=len(dataset))\n    total_num_frames = 0\n    pool_input_frames = None\n    pool_prob_frames = None\n    num_frames_per_block = 1024 * 100\n    frame_counter = 0\n    block_counter = 0\n    pool_counter = 0\n    accumulated_total_num_frames = 0\n\n    ########################################\n    # Count total frame number\n    ########################################\n    # for data, is_new_epoch in dataset:\n    #\n    #     # Create feed dictionary for next mini batch\n    #     inputs, _, inputs_seq_len, input_names = data\n    #\n    #     batch_size = inputs[0].shape[0]\n    #     for i_batch in range(batch_size):\n    #         total_num_frames += inputs_seq_len[0][i_batch]\n    #\n    #         pbar.update(1)\n    #\n    #     if is_new_epoch:\n    #         print(total_num_frames)\n    #         break\n\n    ########################################\n    # Save probabilities per utterance\n    ########################################\n    pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, _, inputs_seq_len, input_names = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size, max_time = inputs[0].shape[:2]\n\n        probs = session.run(posteriors_op, feed_dict=feed_dict)\n        probs = probs.reshape(batch_size, max_time, model.num_classes)\n\n        if pool_input_frames is None:\n            # Initialize\n            total_num_frames = TOTAL_NUM_FRAMES_DICT[data_type]\n\n            pool_num_frames = total_num_frames // NUM_POOLS + 1\n            pool_capacity = pool_num_frames\n\n            pool_input_frames = np.zeros(\n                (pool_num_frames, 120 * 2 * 5))\n            # NOTE: input_size == 120 * 2 (num_stack == 2), splice == 5\n            pool_prob_frames = np.zeros(\n                (pool_num_frames, model.num_classes))\n\n        for i_batch in range(batch_size):\n            speaker = input_names[0][i_batch].split(\'-\')[0]\n\n            # Mask\n            inputs_seq_len_i = inputs_seq_len[0][i_batch]\n            inputs_i = inputs[0][i_batch][:inputs_seq_len_i]\n            probs_i = probs[i_batch][:inputs_seq_len_i]\n\n            # Save probabilities as npy file per utterance\n            if save_prob:\n                prob_save_path = mkdir_join(\n                    save_path, \'probs_utt\', speaker, input_names[0][i_batch] + \'.npy\')\n                np.save(prob_save_path, probs_i)\n                # NOTE: `[T, num_classes]`\n\n            if dataset.splice == 1:\n                # NOTE: teacher is expected to be BLSTM\n                # Splicing\n                inputs_i = do_splice(inputs_i.reshape(1, inputs_seq_len_i, -1),\n                                     splice=5,\n                                     batch_size=1,\n                                     num_stack=dataset.num_stack)\n                inputs_i = inputs_i.reshape(inputs_seq_len_i, -1)\n\n            else:\n                # NOTE: teahcer is expected to be VGG (use features as it is)\n                pass\n\n            # Register\n            if pool_capacity > inputs_seq_len_i:\n                pool_input_frames[frame_counter:frame_counter +\n                                  inputs_seq_len_i] = inputs_i\n                pool_prob_frames[frame_counter: frame_counter +\n                                 inputs_seq_len_i] = probs_i\n                frame_counter += inputs_seq_len_i\n                pool_capacity -= inputs_seq_len_i\n            else:\n                # Fulfill pool\n                pool_input_frames[frame_counter:frame_counter +\n                                  pool_capacity] = inputs_i[:pool_capacity]\n                pool_prob_frames[frame_counter:frame_counter +\n                                 pool_capacity] = probs_i[:pool_capacity]\n\n                ##################################################\n                # Shuffle frames, divide into blocks, and save\n                ##################################################\n                num_blocks = pool_num_frames // num_frames_per_block\n                data_indices = list(range(pool_num_frames))\n                random.shuffle(data_indices)\n\n                for i_block in range(num_blocks):\n                    block_indices = data_indices[:num_frames_per_block]\n                    data_indices = data_indices[num_frames_per_block:]\n\n                    # Pick up block\n                    block_inputs_frames = pool_input_frames[block_indices]\n                    # NOTE: `[1024 * 100, input_size]`\n                    block_probs_frames = pool_prob_frames[block_indices]\n                    # NOTE\xef\xbc\x9a`[1024 * 100, num_classes]`\n\n                    # Save block\n                    if save_soft_targets:\n                        print(\' ==> Saving: block%d\' % block_counter)\n                        input_save_path = mkdir_join(\n                            save_path, \'inputs\', \'block\' + str(block_counter) + \'.npy\')\n                        label_save_path = mkdir_join(\n                            save_path, \'labels\', \'block\' + str(block_counter) + \'.npy\')\n                        np.save(input_save_path, block_inputs_frames)\n                        np.save(label_save_path, block_probs_frames)\n\n                    block_counter += 1\n                    accumulated_total_num_frames += len(block_indices)\n\n                pool_carry_over_num_frames = pool_num_frames - num_frames_per_block * num_blocks\n                utt_carry_over_num_frames = inputs_seq_len_i - pool_capacity\n                carry_over_num_frames = pool_carry_over_num_frames + utt_carry_over_num_frames\n\n                pool_carry_over_input_frames = pool_input_frames[data_indices]\n                pool_carry_over_prob_frames = pool_prob_frames[data_indices]\n\n                # Initialize\n                if pool_counter != NUM_POOLS - 1:\n                    pool_num_frames = total_num_frames // NUM_POOLS + 1 + carry_over_num_frames\n                else:\n                    # last pool\n                    pool_num_frames = total_num_frames - accumulated_total_num_frames\n\n                pool_input_frames = np.zeros(\n                    (pool_num_frames, 120 * 2 * 5))\n                # NOTE: input_size == 120 * 2 (num_stack == 2), splice == 5\n                pool_prob_frames = np.zeros(\n                    (pool_num_frames, model.num_classes))\n                frame_counter = 0\n                pool_counter += 1\n\n                # Register carry over frames\n                pool_input_frames[:pool_carry_over_num_frames] = pool_carry_over_input_frames\n                pool_prob_frames[:pool_carry_over_num_frames] = pool_carry_over_prob_frames\n                frame_counter += pool_carry_over_num_frames\n                pool_input_frames[frame_counter:frame_counter +\n                                  utt_carry_over_num_frames] = inputs_i[-utt_carry_over_num_frames:]\n                pool_prob_frames[frame_counter:frame_counter +\n                                 utt_carry_over_num_frames] = probs_i[-utt_carry_over_num_frames:]\n                frame_counter += utt_carry_over_num_frames\n\n                pool_capacity = pool_num_frames - carry_over_num_frames\n                print(\'=== next pool ===\')\n\n        pbar.update(batch_size)\n\n        if is_new_epoch:\n            ##################################################\n            # Save last pool\n            ##################################################\n            # Pick up block\n            block_inputs_frames = pool_input_frames[:frame_counter]\n            # NOTE: `[1024 * 100, input_size]`\n            block_probs_frames = pool_prob_frames[:frame_counter]\n            # NOTE\xef\xbc\x9a`[1024 * 100, num_classes]`\n\n            # Save last lock\n            if save_soft_targets:\n                print(\' ==> Saving: block%d\' % block_counter)\n                np.save(mkdir_join(save_path, \'inputs\', \'block\' +\n                                   str(block_counter) + \'.npy\'), block_inputs_frames)\n                np.save(mkdir_join(save_path, \'labels\', \'block\' +\n                                   str(block_counter) + \'.npy\'), block_probs_frames)\n\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n\n    # Model setting\n    model = CTC(\n        encoder_type=params[\'encoder_type\'],\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        splice=params[\'splice\'],\n        num_units=params[\'num_units\'],\n        num_layers=params[\'num_layers\'],\n        num_classes=params[\'num_classes\'],\n        lstm_impl=params[\'lstm_impl\'],\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation=params[\'clip_activation\'],\n        num_proj=params[\'num_proj\'],\n        weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_save(model=model, params=params, epoch=args.epoch,\n            eval_batch_size=args.eval_batch_size,\n            temperature=args.temperature)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/librispeech/metrics/ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for CTC model (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nfrom tqdm import tqdm\n\nfrom utils.io.labels.character import Idx2char, Char2idx\nfrom utils.io.labels.word import Idx2word\nfrom utils.io.labels.sparsetensor import sparsetensor2list\nfrom utils.evaluation.edit_distance import compute_cer, compute_wer, wer_align\nfrom models.ctc.decoders.beam_search_decoder import BeamSearchDecoder\n\n\ndef do_eval_cer(session, decode_ops, model, dataset, label_type,\n                is_test=False, eval_batch_size=None, progressbar=False,\n                is_multitask=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        session: session of training model\n        decode_ops: list of operations for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): character or character_capital_divide\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n        is_multitask (bool, optional): if True, evaluate the multitask model\n    Return:\n        cer_mean (float): An average of CER\n        wer_mean (float): An average of WER\n    """"""\n    assert isinstance(decode_ops, list), ""decode_ops must be a list.""\n\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    if label_type == \'character\':\n        idx2char = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n    elif label_type == \'character_capital_divide\':\n        idx2char = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character_capital_divide.txt\',\n            capital_divide=True,\n            space_mark=\'_\')\n    else:\n        raise TypeError\n\n    cer_mean, wer_mean = 0, 0\n    skip_data_num = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        if is_multitask:\n            inputs, _, labels_true, inputs_seq_len, _ = data\n        else:\n            inputs, labels_true, inputs_seq_len, _ = data\n\n        feed_dict = {}\n        for i_device in range(len(decode_ops)):\n            feed_dict[model.inputs_pl_list[i_device]] = inputs[i_device]\n            feed_dict[model.inputs_seq_len_pl_list[i_device]\n                      ] = inputs_seq_len[i_device]\n            feed_dict[model.keep_prob_pl_list[i_device]] = 1.0\n\n        labels_pred_st_list = session.run(decode_ops, feed_dict=feed_dict)\n        for i_device, labels_pred_st in enumerate(labels_pred_st_list):\n            batch_size_device = len(inputs[i_device])\n            try:\n                labels_pred = sparsetensor2list(labels_pred_st,\n                                                batch_size_device)\n                for i_batch in range(batch_size_device):\n\n                    # Convert from list of index to string\n                    if is_test:\n                        str_true = labels_true[i_device][i_batch][0]\n                        # NOTE: transcript is seperated by space(\'_\')\n                    else:\n                        str_true = idx2char(labels_true[i_device][i_batch],\n                                            padded_value=dataset.padded_value)\n                    str_pred = idx2char(labels_pred[i_batch])\n\n                    # Remove consecutive spaces\n                    str_pred = re.sub(r\'[_]+\', \'_\', str_pred)\n\n                    # Remove garbage labels\n                    str_true = re.sub(r\'[\\\']+\', \'\', str_true)\n                    str_pred = re.sub(r\'[\\\']+\', \'\', str_pred)\n\n                    # Compute WER\n                    wer_mean += compute_wer(ref=str_true.split(\'_\'),\n                                            hyp=str_pred.split(\'_\'),\n                                            normalize=True)\n                    # substitute, insert, delete = wer_align(\n                    #     ref=str_pred.split(\'_\'),\n                    #     hyp=str_true.split(\'_\'))\n                    # print(\'SUB: %d\' % substitute)\n                    # print(\'INS: %d\' % insert)\n                    # print(\'DEL: %d\' % delete)\n\n                    # Remove spaces\n                    str_true = re.sub(r\'[_]+\', \'\', str_true)\n                    str_pred = re.sub(r\'[_]+\', \'\', str_pred)\n\n                    # Compute CER\n                    cer_mean += compute_cer(str_pred=str_pred,\n                                            str_true=str_true,\n                                            normalize=True)\n\n                    if progressbar:\n                        pbar.update(1)\n\n            except IndexError:\n                print(\'skipped\')\n                skip_data_num += batch_size_device\n                # TODO: Conduct decoding again with batch size 1\n\n                if progressbar:\n                    pbar.update(batch_size_device)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= (len(dataset) - skip_data_num)\n    wer_mean /= (len(dataset) - skip_data_num)\n    # TODO: Fix this\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return cer_mean, wer_mean\n\n\ndef do_eval_cer2(session, posteriors_ops, beam_width, model, dataset,\n                 label_type, is_test=False, eval_batch_size=None,\n                 progressbar=False, is_multitask=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        session: session of training model\n        posteriors_ops: list of operations for computing posteriors\n        beam_width (int):\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): character or character_capital_divide\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n        is_multitask (bool, optional): if True, evaluate the multitask model\n    Return:\n        cer_mean (float): An average of CER\n        wer_mean (float): An average of WER\n    """"""\n    assert isinstance(posteriors_ops, list), ""posteriors_ops must be a list.""\n\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    if label_type == \'character\':\n        idx2char = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n        char2idx = Char2idx(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n    elif label_type == \'character_capital_divide\':\n        raise NotImplementedError\n    else:\n        raise TypeError\n\n    # Define decoder\n    decoder = BeamSearchDecoder(space_index=char2idx(\'_\')[0],\n                                blank_index=model.num_classes - 1)\n\n    cer_mean, wer_mean = 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        if is_multitask:\n            inputs, _, labels_true, inputs_seq_len, _ = data\n        else:\n            inputs, labels_true, inputs_seq_len, _ = data\n\n        feed_dict = {}\n        for i_device in range(len(posteriors_ops)):\n            feed_dict[model.inputs_pl_list[i_device]] = inputs[i_device]\n            feed_dict[model.inputs_seq_len_pl_list[i_device]\n                      ] = inputs_seq_len[i_device]\n            feed_dict[model.keep_prob_pl_list[i_device]] = 1.0\n\n        posteriors_list = session.run(posteriors_ops, feed_dict=feed_dict)\n        for i_device, labels_pred_st in enumerate(posteriors_list):\n            batch_size_device, max_time = inputs[i_device].shape[:2]\n\n            posteriors = posteriors_list[i_device].reshape(\n                batch_size_device, max_time, model.num_classes)\n\n            for i_batch in range(batch_size_device):\n\n                # Decode per utterance\n                labels_pred, scores = decoder(\n                    probs=posteriors[i_batch:i_batch + 1],\n                    seq_len=inputs_seq_len[i_device][i_batch: i_batch + 1],\n                    beam_width=beam_width)\n\n                # Convert from list of index to string\n                if is_test:\n                    str_true = labels_true[i_device][i_batch][0]\n                    # NOTE: transcript is seperated by space(\'_\')\n                else:\n                    str_true = idx2char(labels_true[i_device][i_batch],\n                                        padded_value=dataset.padded_value)\n                str_pred = idx2char(labels_pred[0])\n\n                # Remove consecutive spaces\n                str_pred = re.sub(r\'[_]+\', \'_\', str_pred)\n\n                # Remove garbage labels\n                str_true = re.sub(r\'[\\\']+\', \'\', str_true)\n                str_pred = re.sub(r\'[\\\']+\', \'\', str_pred)\n\n                # Compute WER\n                wer_mean += compute_wer(ref=str_true.split(\'_\'),\n                                        hyp=str_pred.split(\'_\'),\n                                        normalize=True)\n                # substitute, insert, delete = wer_align(\n                #     ref=str_pred.split(\'_\'),\n                #     hyp=str_true.split(\'_\'))\n                # print(\'SUB: %d\' % substitute)\n                # print(\'INS: %d\' % insert)\n                # print(\'DEL: %d\' % delete)\n\n                # Remove spaces\n                str_true = re.sub(r\'[_]+\', \'\', str_true)\n                str_pred = re.sub(r\'[_]+\', \'\', str_pred)\n\n                # Compute CER\n                cer_mean += compute_cer(str_pred=str_pred,\n                                        str_true=str_true,\n                                        normalize=True)\n\n                if progressbar:\n                    pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= (len(dataset))\n    wer_mean /= (len(dataset))\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return cer_mean, wer_mean\n\n\ndef do_eval_wer(session, decode_ops, model, dataset, train_data_size,\n                is_test=False, eval_batch_size=None, progressbar=False,\n                is_multitask=False):\n    """"""Evaluate trained model by Word Error Rate.\n    Args:\n        session: session of training model\n        decode_ops: list of operations for decoding\n        model: the model to evaluate\n        dataset: An instance of `Dataset` class\n        train_data_size (string): train100h or train460h or train960h\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize progressbar\n        is_multitask (bool, optional): if True, evaluate the multitask model\n    Return:\n        wer_mean (bool): An average of WER\n    """"""\n    assert isinstance(decode_ops, list), ""decode_ops must be a list.""\n\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    idx2word = Idx2word(\n        map_file_path=\'../metrics/mapping_files/word_\' + train_data_size + \'.txt\')\n\n    wer_mean = 0\n    skip_data_num = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        if is_multitask:\n            inputs, labels_true, _, inputs_seq_len, _ = data\n        else:\n            inputs, labels_true, inputs_seq_len, _ = data\n\n        feed_dict = {}\n        for i_device in range(len(decode_ops)):\n            feed_dict[model.inputs_pl_list[i_device]] = inputs[i_device]\n            feed_dict[model.inputs_seq_len_pl_list[i_device]\n                      ] = inputs_seq_len[i_device]\n            feed_dict[model.keep_prob_pl_list[i_device]] = 1.0\n\n        labels_pred_st_list = session.run(decode_ops, feed_dict=feed_dict)\n        for i_device, labels_pred_st in enumerate(labels_pred_st_list):\n            batch_size_device = len(inputs[i_device])\n            try:\n                labels_pred = sparsetensor2list(labels_pred_st,\n                                                batch_size_device)\n\n                for i_batch in range(batch_size_device):\n\n                    if is_test:\n                        str_true = labels_true[i_device][i_batch][0]\n                        # NOTE: transcript is seperated by space(\'_\')\n                    else:\n                        str_true = \'_\'.join(\n                            idx2word(labels_true[i_device][i_batch]))\n                    str_pred = \'_\'.join(idx2word(labels_pred[i_batch]))\n\n                    # if len(str_true.split(\'_\')) == 0:\n                    #     print(str_true)\n                    #     print(str_pred)\n\n                    # Compute WER\n                    wer_mean += compute_wer(ref=str_true.split(\'_\'),\n                                            hyp=str_pred.split(\'_\'),\n                                            normalize=True)\n                    # substitute, insert, delete = wer_align(\n                    #     ref=str_true.split(\' \'),\n                    #     hyp=str_pred.split(\' \'))\n                    # print(\'SUB: %d\' % substitute)\n                    # print(\'INS: %d\' % insert)\n                    # print(\'DEL: %d\' % delete)\n\n                    if progressbar:\n                        pbar.update(1)\n\n            except IndexError:\n                print(\'skipped\')\n                skip_data_num += batch_size_device\n                # TODO: Conduct decoding again with batch size 1\n\n                if progressbar:\n                    pbar.update(batch_size_device)\n\n        if is_new_epoch:\n            break\n\n    wer_mean /= (len(dataset) - skip_data_num)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return wer_mean\n'"
examples/librispeech/training/train_ctc.py,20,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the CTC model with multiple GPUs (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom experiments.librispeech.metrics.ctc import do_eval_cer, do_eval_wer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.training.multi_gpu import average_gradients\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.ctc import CTC\n\n\ndef do_train(model, params, gpu_indices):\n    """"""Run CTC training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n        gpu_indices (list): GPU indices\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'],\n        num_gpu=len(gpu_indices))\n    dev_clean_data = Dataset(\n        data_type=\'dev_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=len(gpu_indices))\n    dev_other_data = Dataset(\n        data_type=\'dev_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=len(gpu_indices))\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        # Create a variable to track the global step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        # Set optimizer\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n        optimizer = model._set_optimizer(\n            params[\'optimizer\'], learning_rate_pl)\n\n        # Calculate the gradients for each model tower\n        total_grads_and_vars, total_losses = [], []\n        decode_ops, ler_ops = [], []\n        all_devices = [\'/gpu:%d\' % i_gpu for i_gpu in range(len(gpu_indices))]\n        # NOTE: /cpu:0 is prepared for evaluation\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i_gpu in range(len(all_devices)):\n                with tf.device(all_devices[i_gpu]):\n                    with tf.name_scope(\'tower_gpu%d\' % i_gpu) as scope:\n\n                        # Define placeholders in each tower\n                        model.create_placeholders()\n\n                        # Calculate the total loss for the current tower of the\n                        # model. This function constructs the entire model but\n                        # shares the variables across all towers.\n                        tower_loss, tower_logits = model.compute_loss(\n                            model.inputs_pl_list[i_gpu],\n                            model.labels_pl_list[i_gpu],\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            model.keep_prob_pl_list[i_gpu],\n                            scope)\n                        tower_loss = tf.expand_dims(tower_loss, axis=0)\n                        total_losses.append(tower_loss)\n\n                        # Reuse variables for the next tower\n                        tf.get_variable_scope().reuse_variables()\n\n                        # Calculate the gradients for the batch of data on this\n                        # tower\n                        tower_grads_and_vars = optimizer.compute_gradients(\n                            tower_loss)\n\n                        # Gradient clipping\n                        tower_grads_and_vars = model._clip_gradients(\n                            tower_grads_and_vars)\n\n                        # TODO: Optionally add gradient noise\n\n                        # Keep track of the gradients across all towers\n                        total_grads_and_vars.append(tower_grads_and_vars)\n\n                        # Add to the graph each operation per tower\n                        decode_op_tower = model.decoder(\n                            tower_logits,\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            beam_width=params[\'beam_width\'])\n                        decode_ops.append(decode_op_tower)\n                        ler_op_tower = model.compute_ler(\n                            decode_op_tower, model.labels_pl_list[i_gpu])\n                        ler_op_tower = tf.expand_dims(ler_op_tower, axis=0)\n                        ler_ops.append(ler_op_tower)\n\n        # Aggregate losses, then calculate average loss\n        total_losses = tf.concat(axis=0, values=total_losses)\n        loss_op = tf.reduce_mean(total_losses, axis=0)\n        ler_ops = tf.concat(axis=0, values=ler_ops)\n        ler_op = tf.reduce_mean(ler_ops, axis=0)\n\n        # We must calculate the mean of each gradient. Note that this is the\n        # synchronization point across all towers\n        average_grads_and_vars = average_gradients(total_grads_and_vars)\n\n        # Apply the gradients to adjust the shared variables.\n        train_op = optimizer.apply_gradients(average_grads_and_vars,\n                                             global_step=global_step)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        # NOTE: Start running operations on the Graph. allow_soft_placement\n        # must be set to True to build towers on GPU, as some of the ops do not\n        # have GPU implementations.\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                              log_device_placement=False)) as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            ler_dev_best = 1\n            not_improved_epoch = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels, inputs_seq_len, _ = data\n                feed_dict_train = {}\n                for i_gpu in range(len(gpu_indices)):\n                    feed_dict_train[model.inputs_pl_list[i_gpu]\n                                    ] = inputs[i_gpu]\n                    feed_dict_train[model.labels_pl_list[i_gpu]] = list2sparsetensor(\n                        labels[i_gpu], padded_value=train_data.padded_value)\n                    feed_dict_train[model.inputs_seq_len_pl_list[i_gpu]\n                                    ] = inputs_seq_len[i_gpu]\n                    feed_dict_train[model.keep_prob_pl_list[i_gpu]\n                                    ] = 1 - float(params[\'dropout\'])\n                feed_dict_train[learning_rate_pl] = learning_rate\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % int(params[\'print_step\'] / len(gpu_indices)) == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    if params[\'train_data_size\'] in [\'train100h\', \'train460h\']:\n                        inputs, labels, inputs_seq_len, _ = dev_clean_data.next()[\n                            0]\n                    else:\n                        inputs, labels, inputs_seq_len, _ = dev_other_data.next()[\n                            0]\n                    feed_dict_dev = {}\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_dev[model.inputs_pl_list[i_gpu]\n                                      ] = inputs[i_gpu]\n                        feed_dict_dev[model.labels_pl_list[i_gpu]] = list2sparsetensor(\n                            labels[i_gpu], padded_value=dev_clean_data.padded_value)\n                        feed_dict_dev[model.inputs_seq_len_pl_list[i_gpu]\n                                      ] = inputs_seq_len[i_gpu]\n                        feed_dict_dev[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_train[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute accuracy & update event files\n                    ler_train, summary_str_train = sess.run(\n                        [ler_op, summary_train], feed_dict=feed_dict_train)\n                    ler_dev, summary_str_dev = sess.run(\n                        [ler_op, summary_dev], feed_dict=feed_dict_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        if \'char\' in params[\'label_type\']:\n                            print(\'=== Dev Data Evaluation ===\')\n                            # dev-clean\n                            cer_dev_clean_epoch, wer_dev_clean_epoch = do_eval_cer(\n                                session=sess,\n                                decode_ops=decode_ops,\n                                model=model,\n                                dataset=dev_clean_data,\n                                label_type=params[\'label_type\'],\n                                eval_batch_size=1)\n                            print(\'  CER (clean): %f %%\' %\n                                  (cer_dev_clean_epoch * 100))\n                            print(\'  WER (clean): %f %%\' %\n                                  (wer_dev_clean_epoch * 100))\n\n                            # dev-other\n                            cer_dev_other_epoch, wer_dev_other_epoch = do_eval_cer(\n                                session=sess,\n                                decode_ops=decode_ops,\n                                model=model,\n                                dataset=dev_other_data,\n                                label_type=params[\'label_type\'],\n                                eval_batch_size=1)\n                            print(\'  CER (other): %f %%\' %\n                                  (cer_dev_other_epoch * 100))\n                            print(\'  WER (other): %f %%\' %\n                                  (wer_dev_other_epoch * 100))\n\n                            if params[\'train_data_size\'] in [\'train100h\', \'train460h\']:\n                                metric_epoch = cer_dev_clean_epoch\n                            else:\n                                metric_epoch = cer_dev_other_epoch\n\n                            if metric_epoch < ler_dev_best:\n                                ler_dev_best = metric_epoch\n                                not_improved_epoch = 0\n                                print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                                # Save model (check point)\n                                checkpoint_file = join(\n                                    model.save_path, \'model.ckpt\')\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=train_data.epoch)\n                                print(""Model saved in file: %s"" % save_path)\n\n                                print(\'=== Test Data Evaluation ===\')\n                                # test-clean\n                                cer_test_clean_epoch, wer_test_clean_epoch = do_eval_cer(\n                                    session=sess,\n                                    decode_ops=decode_ops,\n                                    model=model,\n                                    dataset=test_clean_data,\n                                    label_type=params[\'label_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(\'  CER (clean): %f %%\' %\n                                      (cer_test_clean_epoch * 100))\n                                print(\'  WER (clean): %f %%\' %\n                                      (wer_test_clean_epoch * 100))\n\n                                # test-other\n                                cer_test_other_epoch, wer_test_other_epoch = do_eval_cer(\n                                    session=sess,\n                                    decode_ops=decode_ops,\n                                    model=model,\n                                    dataset=test_other_data,\n                                    label_type=params[\'label_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(\'  CER (other): %f %%\' %\n                                      (cer_test_other_epoch * 100))\n                                print(\'  WER (other): %f %%\' %\n                                      (wer_test_other_epoch * 100))\n                            else:\n                                not_improved_epoch += 1\n\n                        else:\n                            print(\'=== Dev Data Evaluation ===\')\n                            # dev-clean\n                            wer_dev_clean_epoch = do_eval_wer(\n                                session=sess,\n                                decode_ops=decode_ops,\n                                model=model,\n                                dataset=dev_clean_data,\n                                train_data_size=params[\'train_data_size\'],\n                                eval_batch_size=1)\n                            print(\'  WER (clean): %f %%\' %\n                                  (wer_dev_clean_epoch * 100))\n\n                            # dev-other\n                            wer_dev_other_epoch = do_eval_wer(\n                                session=sess,\n                                decode_ops=decode_ops,\n                                model=model,\n                                dataset=dev_other_data,\n                                train_data_size=params[\'train_data_size\'],\n                                eval_batch_size=1)\n                            print(\'  WER (other): %f %%\' %\n                                  (wer_dev_other_epoch * 100))\n\n                            if params[\'train_data_size\'] in [\'train100h\', \'train460h\']:\n                                metric_epoch = wer_dev_clean_epoch\n                            else:\n                                metric_epoch = wer_dev_other_epoch\n\n                            if metric_epoch < ler_dev_best:\n                                ler_dev_best = metric_epoch\n                                not_improved_epoch = 0\n                                print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (WER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                                # Save model (check point)\n                                checkpoint_file = join(\n                                    model.save_path, \'model.ckpt\')\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=train_data.epoch)\n                                print(""Model saved in file: %s"" % save_path)\n\n                                print(\'=== Test Data Evaluation ===\')\n                                # test-clean\n                                cer_test_clean_epoch = do_eval_wer(\n                                    session=sess,\n                                    decode_ops=decode_ops,\n                                    model=model,\n                                    dataset=test_clean_data,\n                                    train_data_size=params[\'train_data_size\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(\'  WER (clean): %f %%\' %\n                                      (cer_test_clean_epoch * 100))\n\n                                # test-other\n                                ler_test_other_epoch = do_eval_wer(\n                                    session=sess,\n                                    decode_ops=decode_ops,\n                                    model=model,\n                                    dataset=test_other_data,\n                                    train_data_size=params[\'train_data_size\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(\'  WER (other): %f %%\' %\n                                      (ler_test_other_epoch * 100))\n                            else:\n                                not_improved_epoch += 1\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Early stopping\n                        if not_improved_epoch == params[\'not_improved_patient_epoch\']:\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=metric_epoch)\n\n                    start_time_step = time.time()\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path, gpu_indices):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes\'] = 72\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes\'] = 77\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes\'] = 77\n    elif params[\'label_type\'] == \'word_freq10\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes\'] = 7213\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes\'] = 18641\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes\'] = 26642\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\n        \'tf_libri_\' + model.name + \'_\' + params[\'train_data_size\'] + \'_\' + params[\'label_type\'])\n\n    model.name += \'_\' + str(params[\'num_units\'])\n    model.name += \'_\' + str(params[\'num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'num_proj\'] != 0:\n        model.name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'dropout\'] != 0:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n    if params[\'bottleneck_dim\'] != 0:\n        model.name += \'_bottle\' + str(params[\'bottleneck_dim\'])\n    if len(gpu_indices) >= 2:\n        model.name += \'_gpu\' + str(len(gpu_indices))\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'ctc\', params[\'label_type\'],\n        params[\'train_data_size\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params, gpu_indices=gpu_indices)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3 and len(args) != 4:\n        raise ValueError\n    main(config_path=args[1], model_save_path=args[2],\n         gpu_indices=list(map(int, args[3].split(\',\'))))\n'"
examples/librispeech/training/train_ctc_temp.py,20,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the CTC model with multiple GPUs (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom experiments.librispeech.metrics.ctc import do_eval_cer, do_eval_wer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.training.multi_gpu import average_gradients\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.ctc import CTC\n\n\ndef do_train(model, params, gpu_indices):\n    """"""Run CTC training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n        gpu_indices (list): GPU indices\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'],\n        num_gpu=len(gpu_indices))\n    dev_clean_data = Dataset(\n        data_type=\'dev_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False, num_gpu=len(gpu_indices))\n    dev_other_data = Dataset(\n        data_type=\'dev_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False, num_gpu=len(gpu_indices))\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        # Create a variable to track the global step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        # Set optimizer\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n        optimizer = model._set_optimizer(\n            params[\'optimizer\'], learning_rate_pl)\n\n        # Calculate the gradients for each model tower\n        total_grads_and_vars, total_losses = [], []\n        decode_ops, ler_ops = [], []\n        all_devices = [\'/gpu:%d\' % i_gpu for i_gpu in range(len(gpu_indices))]\n        # NOTE: /cpu:0 is prepared for evaluation\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i_gpu in range(len(all_devices)):\n                with tf.device(all_devices[i_gpu]):\n                    with tf.name_scope(\'tower_gpu%d\' % i_gpu) as scope:\n\n                        # Define placeholders in each tower\n                        model.create_placeholders()\n\n                        # Calculate the total loss for the current tower of the\n                        # model. This function constructs the entire model but\n                        # shares the variables across all towers.\n                        tower_loss, tower_logits = model.compute_loss(\n                            model.inputs_pl_list[i_gpu],\n                            model.labels_pl_list[i_gpu],\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            model.keep_prob_pl_list[i_gpu],\n                            scope,\n                            softmax_temperature=params[\'softmax_temperature\'])\n                        # NOTE: tower_logits have NOT been divided by\n                        # softmax_temperature\n                        tower_loss = tf.expand_dims(tower_loss, axis=0)\n                        total_losses.append(tower_loss)\n\n                        # Reuse variables for the next tower\n                        tf.get_variable_scope().reuse_variables()\n\n                        # Calculate the gradients for the batch of data on this\n                        # tower\n                        tower_grads_and_vars = optimizer.compute_gradients(\n                            tower_loss)\n\n                        # Gradient clipping\n                        tower_grads_and_vars = model._clip_gradients(\n                            tower_grads_and_vars)\n\n                        # TODO: Optionally add gradient noise\n\n                        # Keep track of the gradients across all towers\n                        total_grads_and_vars.append(tower_grads_and_vars)\n\n                        # Add to the graph each operation per tower\n                        decode_op_tower = model.decoder(\n                            tower_logits,\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            beam_width=params[\'beam_width\'])\n                        decode_ops.append(decode_op_tower)\n                        ler_op_tower = model.compute_ler(\n                            decode_op_tower, model.labels_pl_list[i_gpu])\n                        ler_op_tower = tf.expand_dims(ler_op_tower, axis=0)\n                        ler_ops.append(ler_op_tower)\n\n        # Aggregate losses, then calculate average loss\n        total_losses = tf.concat(axis=0, values=total_losses)\n        loss_op = tf.reduce_mean(total_losses, axis=0)\n        ler_ops = tf.concat(axis=0, values=ler_ops)\n        ler_op = tf.reduce_mean(ler_ops, axis=0)\n\n        # We must calculate the mean of each gradient. Note that this is the\n        # synchronization point across all towers\n        average_grads_and_vars = average_gradients(total_grads_and_vars)\n\n        # Apply the gradients to adjust the shared variables.\n        train_op = optimizer.apply_gradients(average_grads_and_vars,\n                                             global_step=global_step)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        # NOTE: Start running operations on the Graph. allow_soft_placement\n        # must be set to True to build towers on GPU, as some of the ops do not\n        # have GPU implementations.\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                              log_device_placement=False)) as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            ler_dev_best = 1\n            not_improved_epoch = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels, inputs_seq_len, _ = data\n                feed_dict_train = {}\n                for i_gpu in range(len(gpu_indices)):\n                    feed_dict_train[model.inputs_pl_list[i_gpu]\n                                    ] = inputs[i_gpu]\n                    feed_dict_train[model.labels_pl_list[i_gpu]] = list2sparsetensor(\n                        labels[i_gpu], padded_value=train_data.padded_value)\n                    feed_dict_train[model.inputs_seq_len_pl_list[i_gpu]\n                                    ] = inputs_seq_len[i_gpu]\n                    feed_dict_train[model.keep_prob_pl_list[i_gpu]\n                                    ] = 1 - float(params[\'dropout\'])\n                feed_dict_train[learning_rate_pl] = learning_rate\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % int(params[\'print_step\'] / len(gpu_indices)) == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    if params[\'train_data_size\'] in [\'train100h\', \'train460h\']:\n                        inputs, labels, inputs_seq_len, _ = dev_clean_data.next()[\n                            0]\n                    else:\n                        inputs, labels, inputs_seq_len, _ = dev_other_data.next()[\n                            0]\n                    feed_dict_dev = {}\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_dev[model.inputs_pl_list[i_gpu]\n                                      ] = inputs[i_gpu]\n                        feed_dict_dev[model.labels_pl_list[i_gpu]] = list2sparsetensor(\n                            labels[i_gpu], padded_value=dev_other_data.padded_value)\n                        feed_dict_dev[model.inputs_seq_len_pl_list[i_gpu]\n                                      ] = inputs_seq_len[i_gpu]\n                        feed_dict_dev[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_train[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute accuracy & update event files\n                    ler_train, summary_str_train = sess.run(\n                        [ler_op, summary_train], feed_dict=feed_dict_train)\n                    ler_dev, summary_str_dev = sess.run(\n                        [ler_op, summary_dev], feed_dict=feed_dict_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        # dev-clean\n                        cer_dev_clean_epoch, wer_dev_clean_epoch = do_eval_cer(\n                            session=sess,\n                            decode_ops=decode_ops,\n                            model=model,\n                            dataset=dev_clean_data,\n                            label_type=params[\'label_type\'],\n                            eval_batch_size=params[\'batch_size\'])\n                        print(\'  CER (clean): %f %%\' %\n                              (cer_dev_clean_epoch * 100))\n                        print(\'  WER (clean): %f %%\' %\n                              (wer_dev_clean_epoch * 100))\n\n                        # dev-other\n                        cer_dev_other_epoch, wer_dev_other_epoch = do_eval_cer(\n                            session=sess,\n                            decode_ops=decode_ops,\n                            model=model,\n                            dataset=dev_other_data,\n                            label_type=params[\'label_type\'],\n                            eval_batch_size=params[\'batch_size\'])\n                        print(\'  CER (other): %f %%\' %\n                              (cer_dev_other_epoch * 100))\n                        print(\'  WER (other): %f %%\' %\n                              (wer_dev_other_epoch * 100))\n\n                        if params[\'train_data_size\'] in [\'train100h\', \'train460h\']:\n                            metric_epoch = cer_dev_clean_epoch\n                        else:\n                            metric_epoch = cer_dev_other_epoch\n\n                        if metric_epoch < ler_dev_best:\n                            ler_dev_best = metric_epoch\n                            not_improved_epoch = 0\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n\n                            print(\'=== Test Data Evaluation ===\')\n                            # test-clean\n                            cer_test_clean_epoch, wer_test_clean_epoch = do_eval_cer(\n                                session=sess,\n                                decode_ops=decode_ops,\n                                model=model,\n                                dataset=test_clean_data,\n                                label_type=params[\'label_type\'],\n                                is_test=True,\n                                eval_batch_size=params[\'batch_size\'])\n                            print(\'  CER (clean): %f %%\' %\n                                  (cer_test_clean_epoch * 100))\n                            print(\'  WER (clean): %f %%\' %\n                                  (wer_test_clean_epoch * 100))\n\n                            # test-other\n                            cer_test_other_epoch, wer_test_other_epoch = do_eval_cer(\n                                session=sess,\n                                decode_ops=decode_ops,\n                                model=model,\n                                dataset=test_other_data,\n                                label_type=params[\'label_type\'],\n                                is_test=True,\n                                eval_batch_size=params[\'batch_size\'])\n                            print(\'  CER (other): %f %%\' %\n                                  (cer_test_other_epoch * 100))\n                            print(\'  WER (other): %f %%\' %\n                                  (wer_test_other_epoch * 100))\n                        else:\n                            not_improved_epoch += 1\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Early stopping\n                        if not_improved_epoch == params[\'not_improved_patient_epoch\']:\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=metric_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path, gpu_indices):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\n        \'tf_libri_\' + model.name + \'_\' + params[\'train_data_size\'] + \'_\' + params[\'label_type\'])\n\n    model.name += \'_\' + str(params[\'num_units\'])\n    model.name += \'_\' + str(params[\'num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'num_proj\'] != 0:\n        model.name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'dropout\'] != 1:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n    if len(gpu_indices) >= 2:\n        model.name += \'_gpu\' + str(len(gpu_indices))\n    model.name += \'_temp\' + str(params[\'softmax_temperature\'])\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'ctc\', params[\'label_type\'],\n        params[\'train_data_size\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params, gpu_indices=gpu_indices)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3 and len(args) != 4:\n        raise ValueError\n    main(config_path=args[1], model_save_path=args[2],\n         gpu_indices=list(map(int, args[3].split(\',\'))))\n'"
examples/librispeech/training/train_multitask_ctc.py,23,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the multi-task CTC model with multiple GPUs (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_multitask_ctc import Dataset\nfrom experiments.librispeech.metrics.ctc import do_eval_cer, do_eval_wer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.training.multi_gpu import average_gradients\nfrom utils.directory import mkdir, mkdir_join\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.multitask_ctc import MultitaskCTC\n\n\ndef do_train(model, params, gpu_indices):\n    """"""Run multi-task CTC training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n        gpu_indices (list): GPU indices\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', train_data_size=params[\'train_data_size\'],\n        label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'],\n        num_gpu=len(gpu_indices))\n    dev_clean_data = Dataset(\n        data_type=\'dev_clean\', train_data_size=params[\'train_data_size\'],\n        label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=len(gpu_indices))\n    dev_other_data = Dataset(\n        data_type=\'dev_other\', train_data_size=params[\'train_data_size\'],\n        label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True, num_gpu=len(gpu_indices))\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        # Create a variable to track the global step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        # Set optimizer\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n        optimizer = model._set_optimizer(\n            params[\'optimizer\'], learning_rate_pl)\n\n        # Calculate the gradients for each model tower\n        total_grads_and_vars, total_losses = [], []\n        decode_ops_word, decode_ops_char = [], []\n        wer_ops, cer_ops = [], []\n        all_devices = [\'/gpu:%d\' % i_gpu for i_gpu in range(len(gpu_indices))]\n        # NOTE: /cpu:0 is prepared for evaluation\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i_gpu in range(len(all_devices)):\n                with tf.device(all_devices[i_gpu]):\n                    with tf.name_scope(\'tower_gpu%d\' % i_gpu) as scope:\n\n                        # Define placeholders in each tower\n                        model.create_placeholders()\n\n                        # Calculate the total loss for the current tower of the\n                        # model. This function constructs the entire model but\n                        # shares the variables across all towers.\n                        tower_loss, tower_logits_word, tower_logits_char = model.compute_loss(\n                            model.inputs_pl_list[i_gpu],\n                            model.labels_pl_list[i_gpu],\n                            model.labels_sub_pl_list[i_gpu],\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            model.keep_prob_pl_list[i_gpu],\n                            scope)\n                        tower_loss = tf.expand_dims(tower_loss, axis=0)\n                        total_losses.append(tower_loss)\n\n                        # Reuse variables for the next tower\n                        tf.get_variable_scope().reuse_variables()\n\n                        # Calculate the gradients for the batch of data on this\n                        # tower\n                        tower_grads_and_vars = optimizer.compute_gradients(\n                            tower_loss)\n\n                        # Gradient clipping\n                        tower_grads_and_vars = model._clip_gradients(\n                            tower_grads_and_vars)\n\n                        # TODO: Optionally add gradient noise\n\n                        # Keep track of the gradients across all towers\n                        total_grads_and_vars.append(tower_grads_and_vars)\n\n                        # Add to the graph each operation per tower\n                        decode_op_tower_word, decode_op_tower_char = model.decoder(\n                            tower_logits_word, tower_logits_char,\n                            model.inputs_seq_len_pl_list[i_gpu],\n                            beam_width=params[\'beam_width\'])\n                        decode_ops_word.append(decode_op_tower_word)\n                        decode_ops_char.append(decode_op_tower_char)\n                        wer_op_tower, cer_op_tower = model.compute_ler(\n                            decode_op_tower_word,\n                            decode_op_tower_char,\n                            model.labels_pl_list[i_gpu],\n                            model.labels_sub_pl_list[i_gpu])\n                        wer_op_tower = tf.expand_dims(wer_op_tower, axis=0)\n                        wer_ops.append(wer_op_tower)\n                        cer_op_tower = tf.expand_dims(cer_op_tower, axis=0)\n                        cer_ops.append(cer_op_tower)\n\n        # Aggregate losses, then calculate average loss\n        total_losses = tf.concat(axis=0, values=total_losses)\n        loss_op = tf.reduce_mean(total_losses, axis=0)\n        wer_ops = tf.concat(axis=0, values=wer_ops)\n        wer_op = tf.reduce_mean(wer_ops, axis=0)\n        cer_ops = tf.concat(axis=0, values=cer_ops)\n        cer_op = tf.reduce_mean(cer_ops, axis=0)\n\n        # We must calculate the mean of each gradient. Note that this is the\n        # synchronization point across all towers\n        average_grads_and_vars = average_gradients(total_grads_and_vars)\n\n        # Apply the gradients to adjust the shared variables.\n        train_op = optimizer.apply_gradients(average_grads_and_vars,\n                                             global_step=global_step)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_wer_train, csv_wer_dev = [], []\n        csv_cer_train, csv_cer_dev = [], []\n        # Create a session for running operation on the graph\n        # NOTE: Start running operations on the Graph. allow_soft_placement\n        # must be set to True to build towers on GPU, as some of the ops do not\n        # have GPU implementations.\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                              log_device_placement=False)) as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            wer_dev_best = 1\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels_word, labels_char, inputs_seq_len, _ = data\n                feed_dict_train = {}\n                for i_gpu in range(len(gpu_indices)):\n                    feed_dict_train[model.inputs_pl_list[i_gpu]\n                                    ] = inputs[i_gpu]\n                    feed_dict_train[model.labels_pl_list[i_gpu]] = list2sparsetensor(\n                        labels_word[i_gpu], padded_value=train_data.padded_value)\n                    feed_dict_train[model.labels_sub_pl_list[i_gpu]] = list2sparsetensor(\n                        labels_char[i_gpu], padded_value=train_data.padded_value)\n                    feed_dict_train[model.inputs_seq_len_pl_list[i_gpu]\n                                    ] = inputs_seq_len[i_gpu]\n                    feed_dict_train[model.keep_prob_pl_list[i_gpu]\n                                    ] = 1 - float(params[\'dropout\'])\n                feed_dict_train[learning_rate_pl] = learning_rate\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % int(params[\'print_step\'] / len(gpu_indices)) == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels_word, labels_char,\n                     inputs_seq_len, _), _ = dev_other_data.next()\n                    feed_dict_dev = {}\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_dev[model.inputs_pl_list[i_gpu]\n                                      ] = inputs[i_gpu]\n                        feed_dict_dev[model.labels_pl_list[i_gpu]] = list2sparsetensor(\n                            labels_word[i_gpu], padded_value=dev_other_data.padded_value)\n                        feed_dict_dev[model.labels_sub_pl_list[i_gpu]] = list2sparsetensor(\n                            labels_char[i_gpu], padded_value=dev_other_data.padded_value)\n                        feed_dict_dev[model.inputs_seq_len_pl_list[i_gpu]\n                                      ] = inputs_seq_len[i_gpu]\n                        feed_dict_dev[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_train[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute accuracy & update event files\n                    wer_train, cer_train, summary_str_train = sess.run(\n                        [wer_op, cer_op, summary_train], feed_dict=feed_dict_train)\n                    wer_dev, cer_dev, summary_str_dev = sess.run(\n                        [wer_op, cer_op, summary_dev], feed_dict=feed_dict_dev)\n                    csv_wer_train.append(wer_train)\n                    csv_wer_dev.append(wer_dev)\n                    csv_cer_train.append(cer_train)\n                    csv_cer_dev.append(cer_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / wer = %.3f (%.3f) / cer = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, wer_train, wer_dev,\n                           cer_train, cer_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_wer_train, csv_wer_dev, csv_steps,\n                             label_type=params[\'label_type_main\'],\n                             save_path=model.save_path)\n                    plot_ler(csv_cer_train, csv_cer_dev, csv_steps,\n                             label_type=params[\'label_type_sub\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        # dev-clean\n                        wer_main_dev_clean_epoch = do_eval_wer(\n                            session=sess,\n                            decode_ops=decode_ops_word,\n                            model=model,\n                            dataset=dev_clean_data,\n                            train_data_size=params[\'train_data_size\'],\n                            eval_batch_size=1,\n                            is_multitask=True)\n                        print(\'  WER (clean, main): %f %%\' %\n                              (wer_main_dev_clean_epoch * 100))\n                        cer_sub_dev_clean_epoch, wer_sub_dev_clean_epoch = do_eval_cer(\n                            session=sess,\n                            decode_ops=decode_ops_char,\n                            model=model,\n                            dataset=dev_clean_data,\n                            label_type=params[\'label_type_sub\'],\n                            eval_batch_size=1,\n                            is_multitask=True)\n                        print(\'  WER (clean, sub): %f %%\' %\n                              (wer_sub_dev_clean_epoch * 100))\n                        print(\'  CER (clean, sub): %f %%\' %\n                              (cer_sub_dev_clean_epoch * 100))\n\n                        # dev-other\n                        wer_main_dev_other_epoch = do_eval_wer(\n                            session=sess,\n                            decode_ops=decode_ops_word,\n                            model=model,\n                            dataset=dev_other_data,\n                            train_data_size=params[\'train_data_size\'],\n                            eval_batch_size=1,\n                            is_multitask=True)\n                        print(\'  WER (other, main): %f %%\' %\n                              (wer_main_dev_other_epoch * 100))\n                        cer_sub_dev_other_epoch, wer_sub_dev_other_epoch = do_eval_cer(\n                            session=sess,\n                            decode_ops=decode_ops_char,\n                            model=model,\n                            dataset=dev_other_data,\n                            label_type=params[\'label_type_sub\'],\n                            eval_batch_size=1,\n                            is_multitask=True)\n                        print(\'  WER (other, sub): %f %%\' %\n                              (wer_sub_dev_other_epoch * 100))\n                        print(\'  CER (other, sub): %f %%\' %\n                              (cer_sub_dev_other_epoch * 100))\n\n                        if wer_main_dev_other_epoch < wer_dev_best:\n                            wer_dev_best = wer_main_dev_other_epoch\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (WER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n\n                            print(\'=== Test Data Evaluation ===\')\n                            # test-clean\n                            wer_main_test_clean_epoch = do_eval_wer(\n                                session=sess,\n                                decode_ops=decode_ops_word,\n                                model=model,\n                                dataset=test_clean_data,\n                                train_data_size=params[\'train_data_size\'],\n                                is_test=True,\n                                eval_batch_size=1,\n                                is_multitask=True)\n                            print(\'  WER (clean, main): %f %%\' %\n                                  (wer_main_test_clean_epoch * 100))\n                            cer_sub_test_clean_epoch, wer_sub_test_clean_epoch = do_eval_cer(\n                                session=sess,\n                                decode_ops=decode_ops_char,\n                                model=model,\n                                dataset=test_clean_data,\n                                label_type=params[\'label_type_sub\'],\n                                is_test=True,\n                                eval_batch_size=1,\n                                is_multitask=True)\n                            print(\'  WER (clean, sub): %f %%\' %\n                                  (wer_sub_test_clean_epoch * 100))\n                            print(\'  CER (clean, sub): %f %%\' %\n                                  (cer_sub_test_clean_epoch * 100))\n\n                            # test-other\n                            wer_main_test_other_epoch = do_eval_wer(\n                                session=sess,\n                                decode_ops=decode_ops_word,\n                                model=model,\n                                dataset=test_other_data,\n                                train_data_size=params[\'train_data_size\'],\n                                is_test=True,\n                                eval_batch_size=1,\n                                is_multitask=True)\n                            print(\'  WER (other, main): %f %%\' %\n                                  (wer_main_test_other_epoch * 100))\n                            cer_sub_test_other_epoch, wer_sub_test_other_epoch = do_eval_cer(\n                                session=sess,\n                                decode_ops=decode_ops_char,\n                                model=model,\n                                dataset=test_other_data,\n                                label_type=params[\'label_type_sub\'],\n                                is_test=True,\n                                eval_batch_size=1,\n                                is_multitask=True)\n                            print(\'  WER (other, sub): %f %%\' %\n                                  (wer_sub_test_other_epoch * 100))\n                            print(\'  CER (other, sub): %f %%\' %\n                                  (cer_sub_test_other_epoch * 100))\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=wer_main_dev_other_epoch)\n\n                    start_time_step = time.time()\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path, gpu_indices):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type_main\'] == \'word_freq10\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes_main\'] = 7213\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes_main\'] = 18641\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes_main\'] = 26642\n    else:\n        raise TypeError\n\n    if params[\'label_type_sub\'] == \'character\':\n        params[\'num_classes_sub\'] = 28\n    elif params[\'label_type_sub\'] == \'character_capital_divide\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes_sub\'] = 72\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes_sub\'] = 77\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes_sub\'] = 77\n    else:\n        raise TypeError\n\n    # Model setting\n    model = MultitaskCTC(encoder_type=params[\'encoder_type\'],\n                         input_size=params[\'input_size\'],\n                         splice=params[\'splice\'],\n                         num_stack=params[\'num_stack\'],\n                         num_units=params[\'num_units\'],\n                         num_layers_main=params[\'num_layers_main\'],\n                         num_layers_sub=params[\'num_layers_sub\'],\n                         num_classes_main=params[\'num_classes_main\'],\n                         num_classes_sub=params[\'num_classes_sub\'],\n                         main_task_weight=params[\'main_task_weight\'],\n                         lstm_impl=params[\'lstm_impl\'],\n                         use_peephole=params[\'use_peephole\'],\n                         parameter_init=params[\'weight_init\'],\n                         clip_grad_norm=params[\'clip_grad_norm\'],\n                         clip_activation=params[\'clip_activation\'],\n                         num_proj=params[\'num_proj\'],\n                         weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\n        \'libri_\' + model.name + \'_\' + params[\'train_data_size\'] + \'_\' +\n        params[\'label_type_main\'] + \'_\' + params[\'label_type_sub\'])\n\n    model.name += \'_\' + str(params[\'num_units\'])\n    model.name += \'_main\' + str(params[\'num_layers_main\'])\n    model.name += \'_sub\' + str(params[\'num_layers_sub\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'num_proj\'] != 0:\n        model.name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'dropout\'] != 0:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n    model.name += \'_main\' + str(params[\'main_task_weight\'])\n    if len(gpu_indices) >= 2:\n        model.name += \'_gpu\' + str(len(gpu_indices))\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'ctc\',\n        params[\'label_type_main\'] + \'_\' + params[\'label_type_sub\'],\n        params[\'train_data_size\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params, gpu_indices=gpu_indices)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3 and len(args) != 4:\n        raise ValueError\n    main(config_path=args[1], model_save_path=args[2],\n         gpu_indices=list(map(int, args[3].split(\',\'))))\n'"
examples/librispeech/training/train_student_cnn_xe.py,17,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the student model with multiple GPUs (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_xe import Dataset\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss\nfrom utils.training.multi_gpu import average_gradients\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.student_ctc import StudentCTC\n\n\ndef do_train(model, params, gpu_indices):\n    """"""Run CTC training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n        gpu_indices (list): GPU indices\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        model_path=join(params[\'teacher_model_path\'],\n                        \'temp\' + str(params[\'teacher_temperature\'])),\n        data_type=\'train\',\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        num_gpu=len(gpu_indices))\n    dev_clean_data = Dataset(\n        model_path=join(params[\'teacher_model_path\'],\n                        \'temp\' + str(params[\'teacher_temperature\'])),\n        data_type=\'dev_clean\',\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        num_gpu=len(gpu_indices))\n    dev_other_data = Dataset(\n        model_path=join(params[\'teacher_model_path\'],\n                        \'temp\' + str(params[\'teacher_temperature\'])),\n        data_type=\'dev_other\',\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        num_gpu=len(gpu_indices))\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        # Create a variable to track the global step\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        # Set optimizer\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n        optimizer = model._set_optimizer(\n            params[\'optimizer\'], learning_rate_pl)\n\n        # Calculate the gradients for each model tower\n        total_grads_and_vars, total_losses = [], []\n        all_devices = [\'/gpu:%d\' % i_gpu for i_gpu in range(len(gpu_indices))]\n        # NOTE: /cpu:0 is prepared for evaluation\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i_gpu in range(len(all_devices)):\n                with tf.device(all_devices[i_gpu]):\n                    with tf.name_scope(\'tower_gpu%d\' % i_gpu) as scope:\n\n                        # Define placeholders in each tower\n                        model.create_placeholders_xe()\n\n                        # Calculate the total loss for the current tower of the\n                        # model. This function constructs the entire model but\n                        # shares the variables across all towers.\n                        tower_loss, tower_logits = model.compute_xe_loss(\n                            model.inputs_pl_list[i_gpu],\n                            model.labels_pl_list[i_gpu],\n                            model.keep_prob_pl_list[i_gpu],\n                            scope,\n                            softmax_temperature=params[\'student_temperature\'],\n                            is_training=True)\n                        tower_loss = tf.expand_dims(tower_loss, axis=0)\n                        total_losses.append(tower_loss)\n\n                        # Reuse variables for the next tower\n                        tf.get_variable_scope().reuse_variables()\n\n                        # Calculate the gradients for the batch of data on this\n                        # tower\n                        tower_grads_and_vars = optimizer.compute_gradients(\n                            tower_loss)\n\n                        # Gradient clipping\n                        tower_grads_and_vars = model._clip_gradients(\n                            tower_grads_and_vars)\n\n                        # TODO: Optionally add gradient noise\n\n                        # Keep track of the gradients across all towers\n                        total_grads_and_vars.append(tower_grads_and_vars)\n\n        # Aggregate losses, then calculate average loss\n        total_losses = tf.concat(axis=0, values=total_losses)\n        loss_op = tf.reduce_mean(total_losses, axis=0)\n\n        # We must calculate the mean of each gradient. Note that this is the\n        # synchronization point across all towers\n        average_grads_and_vars = average_gradients(total_grads_and_vars)\n\n        # Apply the gradients to adjust the shared variables.\n        train_op = optimizer.apply_gradients(average_grads_and_vars,\n                                             global_step=global_step)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        # Create a session for running operation on the graph\n        # NOTE: Start running operations on the Graph. allow_soft_placement\n        # must be set to True to build towers on GPU, as some of the ops do not\n        # have GPU implementations.\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n                                              log_device_placement=False)) as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            loss_dev_best = 10000\n            not_improved_epoch = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels = data\n                feed_dict_train = {}\n                for i_gpu in range(len(gpu_indices)):\n                    feed_dict_train[model.inputs_pl_list[i_gpu]\n                                    ] = inputs[i_gpu]\n                    feed_dict_train[model.labels_pl_list[i_gpu]\n                                    ] = labels[i_gpu]\n                    feed_dict_train[model.keep_prob_pl_list[i_gpu]\n                                    ] = 1 - float(params[\'dropout\'])\n                feed_dict_train[learning_rate_pl] = learning_rate\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % int(params[\'print_step\'] / len(gpu_indices)) == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    if params[\'train_data_size\'] in [\'train100h\', \'train460h\']:\n                        inputs, labels = dev_clean_data.next()[0]\n                    else:\n                        inputs, labels = dev_other_data.next()[0]\n                    feed_dict_dev = {}\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_dev[model.inputs_pl_list[i_gpu]\n                                      ] = inputs[i_gpu]\n                        feed_dict_dev[model.labels_pl_list[i_gpu]\n                                      ] = labels[i_gpu]\n                        feed_dict_dev[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    for i_gpu in range(len(gpu_indices)):\n                        feed_dict_train[model.keep_prob_pl_list[i_gpu]] = 1.0\n\n                    # Compute accuracy & update event files\n                    summary_str_train = sess.run(\n                        summary_train, feed_dict=feed_dict_train)\n                    summary_str_dev = sess.run(\n                        summary_dev, feed_dict=feed_dict_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n\n                    # Save model (check point)\n                    checkpoint_file = join(\n                        model.save_path, \'model.ckpt\')\n                    save_path = saver.save(\n                        sess, checkpoint_file, global_step=train_data.epoch)\n                    print(""Model saved in file: %s"" % save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        # dev-clean\n                        loss_dev_clean_epoch = do_eval_loss(\n                            session=sess,\n                            loss_op=loss_op,\n                            model=model,\n                            dataset=dev_clean_data,\n                            label_type=params[\'label_type\'],\n                            eval_batch_size=params[\'batch_size\'])\n                        print(\'  LOSS (clean): %f\' % loss_dev_clean_epoch)\n\n                        # dev-other\n                        # loss_dev_other_epoch = do_eval_loss(\n                        #     session=sess,\n                        #     loss_op=loss_op,\n                        #     model=model,\n                        #     dataset=dev_other_data,\n                        #     label_type=params[\'label_type\'],\n                        #     eval_batch_size=params[\'batch_size\'])\n                        # print(\'  LOSS (other): %f\' % loss_dev_other_epoch)\n\n                        if params[\'train_data_size\'] in [\'train100h\', \'train460h\']:\n\n                            metric_epoch = loss_dev_clean_epoch\n                        else:\n                            metric_epoch = loss_dev_other_epoch\n\n                        if metric_epoch < loss_dev_best:\n                            loss_dev_best = metric_epoch\n                            not_improved_epoch = 0\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (LOSS)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n                        else:\n                            not_improved_epoch += 1\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Early stopping\n                        if not_improved_epoch == params[\'not_improved_patient_epoch\']:\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=metric_epoch)\n\n                    start_time_step = time.time()\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef do_eval_loss(session, loss_op, model, dataset, label_type,\n                 eval_batch_size=None,):\n\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    loss_sum = 0\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels = data\n\n        feed_dict = {}\n        for i_device in range(dataset.num_gpu):\n            feed_dict[model.inputs_pl_list[i_device]] = inputs[i_device]\n            feed_dict[model.labels_pl_list[i_device]] = labels[i_device]\n            feed_dict[model.keep_prob_pl_list[i_device]] = 1.0\n\n        loss_sum += session.run(loss_op, feed_dict=feed_dict)\n\n        if is_new_epoch:\n            break\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return loss_sum\n\n\ndef main(config_path, model_save_path, gpu_indices):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    params[\'num_classes\'] = 28\n\n    # Model setting\n    model = StudentCTC(\n        encoder_type=params[\'encoder_type\'],\n        input_size=params[\'input_size\'] *\n        params[\'num_stack\'] * params[\'splice\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'],\n        num_classes=params[\'num_classes\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\n        \'tf_libri_\' + model.name + \'_\' + params[\'train_data_size\'] + \'_\' + params[\'label_type\'])\n\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'dropout\'] != 0:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n    if len(gpu_indices) >= 2:\n        model.name += \'_gpu\' + str(len(gpu_indices))\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'student_ctc\', params[\'label_type\'],\n        params[\'train_data_size\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params, gpu_indices=gpu_indices)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3 and len(args) != 4:\n        raise ValueError\n    main(config_path=args[1], model_save_path=args[2],\n         gpu_indices=list(map(int, args[3].split(\',\'))))\n'"
examples/librispeech/visualization/decode_ctc.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decode the trained CTC outputs (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom models.ctc.ctc import CTC\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.word import Idx2word\nfrom utils.io.labels.sparsetensor import sparsetensor2list\nfrom utils.evaluation.edit_distance import wer_align\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\n\n\ndef do_decode(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\',\n        train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_other_data = Dataset(\n        data_type=\'test_other\',\n        train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                       model.labels_pl_list[0],\n                                       model.inputs_seq_len_pl_list[0],\n                                       model.keep_prob_pl_list[0])\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        decode(session=sess,\n               decode_op=decode_op,\n               model=model,\n               dataset=test_clean_data,\n               label_type=params[\'label_type\'],\n               train_data_size=params[\'train_data_size\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n        decode(session=sess,\n               decode_op=decode_op,\n               model=model,\n               dataset=test_other_data,\n               label_type=params[\'label_type\'],\n               train_data_size=params[\'train_data_size\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n\ndef decode(session, decode_op, model, dataset, label_type,\n           train_data_size, is_test=True, save_path=None):\n    """"""Visualize label outputs of CTC model.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): character or character_capital_divide or word\n        train_data_size (string, optional): train100h or train460h or\n            train960h\n        is_test (bool, optional): set to True when evaluating by the test set\n        save_path (string, optional): path to save decoding results\n    """"""\n    if label_type == \'character\':\n        map_fn = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n    elif label_type == \'character_capital_divide\':\n        map_fn = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character_capital_divide.txt\',\n            capital_divide=True)\n    elif label_type == \'word\':\n        map_fn = Idx2word(\n            map_file_path=\'../metrics/mapping_files/word_\' + train_data_size + \'.txt\')\n\n    if save_path is not None:\n        sys.stdout = open(join(model.model_dir, \'decode.txt\'), \'w\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        # Decode\n        batch_size = inputs[0].shape[0]\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n        no_output_flag = False\n        try:\n            labels_pred = sparsetensor2list(\n                labels_pred_st, batch_size=batch_size)\n        except IndexError:\n            # no output\n            no_output_flag = True\n\n        # Visualize\n        for i_batch in range(batch_size):\n\n            print(\'----- wav: %s -----\' % input_names[0][i_batch])\n            if \'char\' in label_type:\n                if is_test:\n                    str_true = labels_true[0][i_batch][0]\n                else:\n                    str_true = map_fn(labels_true[0][i_batch])\n                if no_output_flag:\n                    str_pred = \'\'\n                else:\n                    str_pred = map_fn(labels_pred[i_batch])\n            else:\n                if is_test:\n                    str_true = labels_true[0][i_batch][0]\n                else:\n                    str_true = \'_\'.join(map_fn(labels_true[0][i_batch]))\n                if no_output_flag:\n                    str_pred = \'\'\n                else:\n                    str_pred = \'_\'.join(map_fn(labels_pred[i_batch]))\n\n            print(\'Ref: %s\' % str_true)\n            print(\'Hyp: %s\' % str_pred)\n            # wer_align(ref=str_true.split(), hyp=str_pred.split())\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes\'] = 72\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes\'] = 77\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes\'] = 77\n    elif params[\'label_type\'] == \'word_freq10\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes\'] = 7213\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes\'] = 18641\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes\'] = 26642\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\']\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_decode(model=model, params=params,\n              epoch=args.epoch, beam_width=args.beam_width,\n              eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/librispeech/visualization/decode_multitask_ctc.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decode the trained multi-task CTC outputs (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_multitask_ctc import Dataset\nfrom models.ctc.multitask_ctc import MultitaskCTC\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.word import Idx2word\nfrom utils.io.labels.sparsetensor import sparsetensor2list\nfrom utils.evaluation.edit_distance import wer_align\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=-1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\n\n\ndef do_decode(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\', train_data_size=params[\'train_data_size\'],\n        label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_other_data = Dataset(\n        data_type=\'test_other\', train_data_size=params[\'train_data_size\'],\n        label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits_word, logits_char = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.labels_sub_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0])\n        decode_op_word, decode_op_char = model.decoder(\n            logits_word, logits_char,\n            model.inputs_seq_len_pl_list[0],\n            beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        decode(session=sess,\n               decode_op_main=decode_op_word,\n               decode_op_sub=decode_op_char,\n               model=model,\n               dataset=test_clean_data,\n               label_type_main=params[\'label_type_main\'],\n               label_type_sub=params[\'label_type_sub\'],\n               train_data_size=params[\'train_data_size\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n        decode(session=sess,\n               decode_op_main=decode_op_word,\n               decode_op_sub=decode_op_char,\n               model=model,\n               dataset=test_other_data,\n               label_type_main=params[\'label_type_main\'],\n               label_type_sub=params[\'label_type_sub\'],\n               train_data_size=params[\'train_data_size\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n\ndef decode(session, decode_op_main, decode_op_sub, model,\n           dataset, train_data_size, label_type_main,\n           label_type_sub, is_test=True, save_path=None):\n    """"""Visualize label outputs of Multi-task CTC model.\n    Args:\n        session: session of training model\n        decode_op_main: operation for decoding in the main task\n        decode_op_sub: operation for decoding in the sub task\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type_main (string): word\n        label_type_sub (string): character or character_capital_divide\n        train_data_size (string, optional): train100h or train460h or\n            train960h\n        is_test (bool, optional): set to True when evaluating by the test set\n        save_path (string, optional): path to save decoding results\n    """"""\n    idx2word = Idx2word(\n        map_file_path=\'../metrics/mapping_files/word_\' + train_data_size + \'.txt\')\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type_sub + \'.txt\')\n\n    if save_path is not None:\n        sys.stdout = open(join(model.model_dir, \'decode.txt\'), \'w\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true_word, labels_true_char, inputs_seq_len, input_names = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_hidden_pl_list[0]: 1.0\n        }\n\n        # Decode\n        batch_size = inputs[0].shape[0]\n        labels_pred_st_word, labels_pred_st_char = session.run(\n            [decode_op_main, decode_op_sub], feed_dict=feed_dict)\n        try:\n            labels_pred_word = sparsetensor2list(\n                labels_pred_st_word, batch_size=batch_size)\n        except IndexError:\n            # no output\n            labels_pred_word = [\'\']\n        try:\n            labels_pred_char = sparsetensor2list(\n                labels_pred_st_char, batch_size=batch_size)\n        except IndexError:\n            # no output\n            labels_pred_char = [\'\']\n\n        # Visualize\n        for i_batch in range(batch_size):\n            print(\'----- wav: %s -----\' % input_names[0][i_batch])\n            if is_test:\n                str_true_word = labels_true_word[0][i_batch][0]\n                str_true_char = labels_true_char[0][i_batch][0]\n            else:\n                str_true_word = \'_\'.join(\n                    idx2word(labels_true_word[0][i_batch]))\n                str_true_char = idx2char(labels_true_char[0][i_batch])\n\n            str_pred_word = \'_\'.join(idx2word(labels_pred_word[0]))\n            str_pred_char = idx2char(labels_pred_char[0])\n\n            print(\'Ref (word): %s\' % str_true_word)\n            print(\'Ref (char): %s\' % str_true_char)\n            print(\'Hyp (word): %s\' % str_pred_word)\n            print(\'Hyp (char): %s\' % str_pred_char)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type_main\'] == \'word_freq10\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes_main\'] = 7213\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes_main\'] = 18641\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes_main\'] = 26642\n    if params[\'label_type_sub\'] == \'character\':\n        params[\'num_classes_sub\'] = 28\n    elif params[\'label_type_sub\'] == \'character_capital_divide\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes_sub\'] = 72\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes_sub\'] = 77\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes_sub\'] = 77\n\n    # Model setting\n    model = MultitaskCTC(encoder_type=params[\'encoder_type\'],\n                         input_size=params[\'input_size\']\n                         splice=params[\'splice\'],\n                         num_stack=params[\'num_stack\'],\n                         num_units=params[\'num_units\'],\n                         num_layers_main=params[\'num_layers_main\'],\n                         num_layers_sub=params[\'num_layers_sub\'],\n                         num_classes_main=params[\'num_classes_main\'],\n                         num_classes_sub=params[\'num_classes_sub\'],\n                         main_task_weight=params[\'main_task_weight\'],\n                         lstm_impl=params[\'lstm_impl\'],\n                         use_peephole=params[\'use_peephole\'],\n                         parameter_init=params[\'weight_init\'],\n                         clip_grad_norm=params[\'clip_grad_norm\'],\n                         clip_activation=params[\'clip_activation\'],\n                         num_proj=params[\'num_proj\'],\n                         weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_decode(model=model, params=params,\n              epoch=args.epoch, beam_width=args.beam_width,\n              eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/librispeech/visualization/plot_ctc_prob.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Plot the trained CTC posteriors (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath, isdir\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport yaml\nimport argparse\nimport shutil\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nplt.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set_style(""white"")\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom models.ctc.ctc import CTC\n# from models.ctc.distillation_ctc import CTC\nfrom utils.directory import mkdir_join, mkdir\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--eval_batch_size\', type=int, default=-1,\n                    help=\'the size of mini-batch when evaluation. \' +\n                    \'If you set -1, batch size is the same as that when training.\')\n\n\ndef do_plot(model, params, epoch, eval_batch_size):\n    """"""Plot the CTC posteriors.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        eval_batch_size (int): the size of mini-batch in evaluation\n    """"""\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\',\n        train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True)\n    test_other_data = Dataset(\n        data_type=\'test_other\',\n        train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'] if eval_batch_size == -\n        1 else eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_hidden_pl_list[0],\n            # softmax_temperature=params[\'softmax_temperature\'])\n            softmax_temperature=10)\n        posteriors_op = model.posteriors(logits, blank_prior=1)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        plot(session=sess,\n             posteriors_op=posteriors_op,\n             model=model,\n             dataset=test_clean_data,\n             label_type=params[\'label_type\'],\n             num_stack=params[\'num_stack\'],\n             #    save_path=None)\n             save_path=mkdir_join(model.save_path, \'ctc_output\', \'test-clean\'))\n\n        plot(session=sess,\n             posteriors_op=posteriors_op,\n             model=model,\n             dataset=test_other_data,\n             label_type=params[\'label_type\'],\n             num_stack=params[\'num_stack\'],\n             #    save_path=None)\n             save_path=mkdir_join(model.save_path, \'ctc_output\', \'test-other\'))\n\n\ndef plot(session, posteriors_op, model, dataset, label_type,\n         num_stack=1, save_path=None, show=False):\n    """"""Visualize label posteriors of CTC model.\n    Args:\n        session: session of training model\n        posteriois_op: operation for computing posteriors\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): phone39 or phone48 or phone61 or character or\n            character_capital_divide\n        num_stack (int): the number of frames to stack\n        save_path (string, string): path to save ctc outputs\n        show (bool, optional): if True, show each figure\n    """"""\n    # Clean directory\n    if isdir(save_path):\n        shutil.rmtree(save_path)\n        mkdir(save_path)\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, _, inputs_seq_len, input_names = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_hidden_pl_list[0]: 1.0\n        }\n\n        batch_size, max_frame_num = inputs[0].shape[:2]\n        probs = session.run(posteriors_op, feed_dict=feed_dict)\n        probs = probs.reshape(-1, max_frame_num, model.num_classes)\n\n        # Visualize\n        for i_batch in range(batch_size):\n            prob = probs[i_batch][:int(inputs_seq_len[0][i_batch]), :]\n\n            plt.clf()\n            plt.figure(figsize=(10, 4))\n            frame_num = int(inputs_seq_len[0][i_batch])\n            times_probs = np.arange(frame_num) * num_stack / 100\n\n            # NOTE: Blank class is set to the last class in TensorFlow\n            for i in range(0, prob.shape[-1] - 1, 1):\n                plt.plot(times_probs, prob[:, i])\n            plt.plot(times_probs, prob[:, -1], \':\',\n                     label=\'blank\', color=\'grey\')\n            plt.xlabel(\'Time [sec]\', fontsize=12)\n            plt.ylabel(\'Posteriors\', fontsize=12)\n            plt.xlim([0, frame_num * num_stack / 100])\n            plt.ylim([0.05, 1.05])\n            plt.xticks(list(range(0, int(frame_num * num_stack / 100) + 1, 1)))\n            plt.yticks(list(range(0, 2, 1)))\n            plt.legend(loc=""upper right"", fontsize=12)\n\n            plt.show()\n\n            # Save as a png file\n            if save_path is not None:\n                plt.savefig(join(\n                    save_path, input_names[0][i_batch] + \'.png\'), dvi=500)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes\'] = 72\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes\'] = 77\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes\'] = 77\n    elif params[\'label_type\'] == \'word_freq10\':\n        if params[\'train_data_size\'] == \'train100h\':\n            params[\'num_classes\'] = 7213\n        elif params[\'train_data_size\'] == \'train460h\':\n            params[\'num_classes\'] = 18641\n        elif params[\'train_data_size\'] == \'train960h\':\n            params[\'num_classes\'] = 26642\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\']\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                splice=params[\'splice\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad=params[\'clip_grad\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_plot(model=model, params=params, epoch=args.epoch,\n            eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/svc/data/__init__.py,0,b''
examples/svc/data/load_dataset_attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the Attention-based model (SVC corpus).\n   In addition, frame stacking and skipping are used.\n   You can use only the single GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.attention import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, label_type, batch_size, map_file_path,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or test\n            label_type (string): stirng, phone39 or phone48 or phone61 or\n                character or character_capital_divide\n            batch_size (int): the size of mini-batch\n            map_file_path (string): path to the mapping file\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        if data_type not in [\'train\', \'dev\', \'test\']:\n            raise TypeError(\'data_type must be ""train"" or ""dev"" or ""test"".\')\n        if label_type not in [\'phone3\', \'phone4\', \'phone43\']:\n            raise TypeError(\n                \'label_type must be ""phone3"" or ""phone4"" or ""phone43""\')\n\n        super(Dataset, self).__init__(map_file_path=map_file_path)\n\n        self.is_test = False\n\n        self.data_type = data_type\n        self.label_type = label_type\n        self.batch_size = batch_size\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = 1\n\n        input_path = join(\n            \'/n/sd8/inaguma/corpus/svc/dataset\', \'inputs\', \'htk\', data_type)\n        # NOTE: ex.) save_path: svc_dataset_path/inputs/data_type/***.npy\n        # 123-dim features\n        label_path = join(\n            \'/n/sd8/inaguma/corpus/svc/dataset\', \'labels\', label_type, data_type)\n        # NOTE: ex.) save_path:\n        # svc_dataset_path/labels/label_type/data_type/***.npy\n\n        # Load the frame number dictionary\n        with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n            self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for input_name, frame_num in frame_num_tuple_sorted:\n            input_paths.append(join(input_path, input_name + \'.npy\'))\n            label_paths.append(join(label_path, input_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/svc/data/load_dataset_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the CTC model (SVC corpus).\n   In addition, frame stacking and skipping are used.\n   You can use only the single GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.ctc import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, label_type, batch_size,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or test\n            label_type (string): stirng, phone3 or phone4 or phone43\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        if data_type not in [\'train\', \'dev\', \'test\']:\n            raise TypeError(\'data_type must be ""train"" or ""dev"" or ""test"".\')\n        if label_type not in [\'phone3\', \'phone4\', \'phone43\']:\n            raise TypeError(\n                \'label_type must be ""phone3"" or ""phone4"" or ""phone43""\')\n\n        super(Dataset, self).__init__()\n\n        self.is_test = False\n\n        self.data_type = data_type\n        self.label_type = label_type\n        self.batch_size = batch_size\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = 1\n\n        input_path = join(\n            \'/n/sd8/inaguma/corpus/svc/dataset\', \'inputs\', \'htk\', data_type)\n        # NOTE: ex.) save_path: svc_dataset_path/inputs/data_type/***.npy\n        # 123-dim features\n        label_path = join(\n            \'/n/sd8/inaguma/corpus/svc/dataset\', \'labels\', label_type, data_type)\n        # NOTE: ex.) save_path:\n        # svc_dataset_path/labels/label_type/data_type/***.npy\n\n        # Load the frame number dictionary\n        with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n            self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for input_name, frame_num in frame_num_tuple_sorted:\n            input_paths.append(join(input_path, input_name + \'.npy\'))\n            label_paths.append(join(label_path, input_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/svc/evaluation/__init__.py,0,b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n'
examples/svc/evaluation/eval_framewise.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""define evaluation method for frame-wise classifiers (SVC corpus).""""""\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nsys.path.append(\'../\')\nfrom . import metric\nfrom plot import probs\n\n\ndef do_eval_uaauc(session, posteriors_op, network, dataset, rate=1.0, is_training=True):\n    """"""Evaluate trained model by UAAUC.\n    Args:\n        session: session of trained model\n        posteriors_op: operation for computing posteriors\n        network: network to evaluate\n        dataset: set of input data and labels\n        rate: rate of data to use\n        is_training: if True, evaluate during training, else during restoring\n    Returns:\n        auc_l: AUC of laughter\n        auc_f: AUC of siller\n        uaauc: UAAUC between laughter and filler\n    """"""\n    num_examples = dataset.data_num * rate\n    iteration = max(1, int(num_examples / network.batch_size) + 1)\n    iter_laughter = iteration\n    iter_filler = iteration\n    auc_l_sum = 0\n    auc_f_sum = 0\n\n    # setting for progressbar\n    if is_training:\n        iterator = range(iteration)\n    else:\n        iterator = tqdm(range(iteration))\n\n    for step in iterator:\n        # create feed dictionary for next mini batch\n        inputs, labels = dataset.next_batch(batch_size=network.batch_size)\n\n        feed_dict = {\n            network.inputs_pl: inputs,\n            network.labels_pl: labels\n        }\n\n        for i in range(len(network.keep_prob_pl_list)):\n            feed_dict[network.keep_prob_pl_list[i]] = 1.0\n\n        posteriors = session.run(posteriors_op, feed_dict=feed_dict)\n\n        # low pass filter\n\n        # logistic regression\n\n        # check if positive label is included in mini batch\n        labels_tmp = np.copy(labels)\n        auc_l_batch = metric.compute_auc(labels, posteriors, label_index=1)\n        auc_f_batch = metric.compute_auc(labels_tmp, posteriors, label_index=2)\n\n        # nan check\n        if auc_l_batch != auc_l_batch:\n            iter_laughter -= 1\n        else:\n            auc_l_sum += auc_l_batch\n        if auc_f_batch != auc_f_batch:\n            iter_filler -= 1\n        else:\n            auc_f_sum += auc_f_batch\n\n    auc_l = auc_l_sum / iter_laughter\n    auc_f = auc_f_sum / iter_filler\n    uaauc = (auc_l + auc_f) / 2.\n\n    acc_l = [auc_l, uaauc]\n    acc_f = [auc_f, uaauc]\n\n    df_auc = pd.DataFrame({\'Laughter\': acc_l, \'Filler\': acc_f},\n                          columns=[\'Laughter\', \'Filler\'],\n                          index=[\'AUC\', \'UAAUC\'])\n\n    print(df_auc)\n\n    return auc_l, auc_f, uaauc\n\n\ndef do_eval_fmeasure(session, decode_op, network, dataset, rate=1.0, is_training=True):\n    """"""Evaluate trained model by F-measure.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        network: network to evaluate\n        dataset: `Dataset\' class\n        rate: A float value. Rate of evaluation data to use\n        is_training: if True, evaluate during training, else during restoring\n    Returns:\n        acc_l: list of [precision, recall, f_measure] of laughter\n        acc_f: list of [precision, recall, f_measure] of filler\n        fmean: mean of f-measure between laughter and filler\n    """"""\n    num_examples = dataset.data_num * rate\n    iteration = max(1, int(num_examples / network.batch_size) + 1)\n    tp_l, fp_l, fn_l = 0, 0, 0\n    tp_f, fp_f, fn_f = 0, 0, 0\n\n    iter_laughter = iteration\n    iter_filler = iteration\n\n    # setting for progressbar\n    if is_training:\n        iterator = range(iteration)\n    else:\n        p = ProgressBar(max_value=iteration)\n        iterator = p(range(iteration))\n\n    for step in iterator:\n        # create feed dictionary for next mini batch\n        inputs, labels = dataset.next_batch(batch_size=network.batch_size)\n\n        feed_dict = {\n            network.inputs_pl: inputs,\n            network.labels_pl: labels\n        }\n\n        batch_size = len(labels)\n\n        for i in range(len(network.keep_prob_pl_list)):\n            feed_dict[network.keep_prob_pl_list[i]] = 1.0\n\n        posteriors = session.run(posteriors_op, feed_dict=feed_dict)\n\n        # HMM processing\n\n        for i_batch in range(batch_size):\n            detected_l_num = np.sum(np.array(labels_pred[i_batch]) == 1)\n            detected_f_num = np.sum(np.array(labels_pred[i_batch]) == 2)\n            true_l_num = np.sum(labels[i_batch] == 1)\n            true_f_num = np.sum(labels[i_batch] == 2)\n\n            # laughter\n            if detected_l_num <= true_l_num:\n                tp_l += detected_l_num\n                fn_l += true_l_num - detected_l_num\n            else:\n                tp_l += true_l_num\n                fp_l += detected_l_num - true_l_num\n\n            # filler\n            if detected_f_num <= true_f_num:\n                tp_f += detected_f_num\n                fn_f += true_f_num - detected_f_num\n            else:\n                tp_f += true_f_num\n                fp_f += detected_f_num - true_f_num\n\n    p_l = tp_l / (tp_l + fp_l) if (tp_l + fp_l) != 0 else 0\n    r_l = tp_l / (tp_l + fn_l) if (tp_l + fn_l) != 0 else 0\n    f_l = 2 * r_l * p_l / (r_l + p_l) if (r_l + p_l) != 0 else 0\n\n    r_f = tp_f / (tp_f + fn_f) if (tp_f + fn_f) != 0 else 0\n    p_f = tp_f / (tp_f + fp_f) if (tp_f + fp_f) != 0 else 0\n    f_f = 2 * r_f * p_f / (r_f + p_f) if (r_f + p_f) != 0 else 0\n\n    confusion_l = [tp_l, fp_l, fn_l, tp_l + fp_l + fn_l]\n    confusion_f = [tp_f, fp_f, fn_f, tp_f + fp_f + fn_f]\n    acc_l = [p_l, r_l, f_l]\n    acc_f = [p_f, r_f, f_f]\n    mean = [(p_l + p_f) / 2., (r_l + r_f) / 2., (f_l + f_f) / 2.]\n\n    df_confusion = pd.DataFrame({\'Laughter\': confusion_l, \'Filler\': confusion_f},\n                                columns=[\'Laughter\', \'Filler\'],\n                                index=[\'TP\', \'FP\', \'FN\', \'Sum\'])\n    df_acc = pd.DataFrame({\'Laughter\': acc_l, \'Filler\': acc_f, \'Mean\': mean},\n                          columns=[\'Laughter\', \'Filler\', \'Mean\'],\n                          index=[\'Precision\', \'Recall\', \'F-measure\'])\n\n    # print(df_confusion)\n    print(df_acc)\n\n    return acc_l, acc_f, mean[2]\n'"
examples/svc/evaluation/metric.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluation metrics.""""""\n\nimport numpy as np\nfrom sklearn.metrics import roc_curve, auc\n\n\ndef compute_auc(y_true, y_pred, label_index):\n    """"""Compute Area Under the Curve (AUC) metric.\n    Args:\n        y_true: true class\n        y_pred: probabilities for a class\n        label_index:\n            label_index == 1 => laughter (class1) vs. others (class0)\n            label_index == 2 => filler (class1) vs. others (class0)\n    Returns:\n        auc_val: AUC metric accuracy\n    """"""\n    for i in range(y_true.shape[0]):\n        y_true[i] = 0 if y_true[i] != label_index else 1\n\n    y_true = np.reshape(y_true, (-1,))\n    y_pred = np.reshape(y_pred[:, label_index], (-1,))\n\n    try:\n        fpr, tpr, _ = roc_curve(y_true, y_pred, pos_label=1)\n    except UndefinedMetricWarning:\n        pass\n    auc_val = auc(fpr, tpr)\n    return auc_val\n'"
examples/svc/metrics/attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for Attention-based model (SVC corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom experiments.svc.metrics.ctc import read_trans\n\n\ndef do_eval_fmeasure(session, decode_op, model, dataset,\n                     eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by F-measure.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset\' class\n        label_type (string): phone39 or phone48 or phone61\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Return:\n        fmean (float): mean of f-measure of laughter and filler\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    tp_l, fp_l, fn_l = 0, 0, 0\n    tp_f, fp_f, fn_f = 0, 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, labels_seq_len, _ = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        # Decode\n        labels_pred = session.run(decode_op, feed_dict=feed_dict)\n\n        for i_batch in range(batch_size):\n\n            detected_l_num = np.sum(np.array(labels_pred[i_batch]) == 1)\n            detected_f_num = np.sum(np.array(labels_pred[i_batch]) == 2)\n            true_l_num = np.sum(labels_true[0][i_batch] == 1)\n            true_f_num = np.sum(labels_true[0][i_batch] == 2)\n\n            # Laughter\n            if detected_l_num <= true_l_num:\n                tp_l += detected_l_num\n                fn_l += true_l_num - detected_l_num\n            else:\n                tp_l += true_l_num\n                fp_l += detected_l_num - true_l_num\n\n            # Filler\n            if detected_f_num <= true_f_num:\n                tp_f += detected_f_num\n                fn_f += true_f_num - detected_f_num\n            else:\n                tp_f += true_f_num\n                fp_f += detected_f_num - true_f_num\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    # Compute F-measure\n    p_l = tp_l / (tp_l + fp_l) if (tp_l + fp_l) != 0 else 0\n    r_l = tp_l / (tp_l + fn_l) if (tp_l + fn_l) != 0 else 0\n    f_l = 2 * r_l * p_l / (r_l + p_l) if (r_l + p_l) != 0 else 0\n\n    r_f = tp_f / (tp_f + fn_f) if (tp_f + fn_f) != 0 else 0\n    p_f = tp_f / (tp_f + fp_f) if (tp_f + fp_f) != 0 else 0\n    f_f = 2 * r_f * p_f / (r_f + p_f) if (r_f + p_f) != 0 else 0\n\n    # confusion_l = [tp_l, fp_l, fn_l, tp_l + fp_l + fn_l]\n    # confusion_f = [tp_f, fp_f, fn_f, tp_f + fp_f + fn_f]\n    acc_l = [p_l, r_l, f_l]\n    acc_f = [p_f, r_f, f_f]\n    mean = [(p_l + p_f) / 2., (r_l + r_f) / 2., (f_l + f_f) / 2.]\n\n    # df_confusion = pd.DataFrame({\'Laughter\': confusion_l, \'Filler\': confusion_f},\n    #                             columns=[\'Laughter\', \'Filler\'],\n    #                             index=[\'TP\', \'FP\', \'FN\', \'Sum\'])\n    # print(df_confusion)\n\n    df_acc = pd.DataFrame({\'Laughter\': acc_l, \'Filler\': acc_f, \'Mean\': mean},\n                          columns=[\'Laughter\', \'Filler\', \'Mean\'],\n                          index=[\'Precision\', \'Recall\', \'F-measure\'])\n    # print(df_acc)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return mean[2], df_acc\n\n\ndef do_eval_fmeasure_time(session, decode_op, attention_weights_op, model, dataset,\n                          eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by F-measure.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        attention_weights_op: operation for computing attention weights\n        model: the model to evaluate\n        dataset: An instance of a `Dataset\' class\n        label_type (string): phone39 or phone48 or phone61\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Returns:\n        fmean (float): mean of f-measure of laughter and filler\n    """"""\n    threshold_l = threshold_f = 0.5\n\n    # Load ground truth labels\n    utterance_dict = read_trans(\n        label_path=\'/n/sd8/inaguma/corpus/svc/data/labels.txt\')\n\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    tp_l, fp_l, fn_l = 0, 0, 0\n    tp_f, fp_f, fn_f = 0, 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, labels_seq_len, input_names = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        max_frame_num = inputs.shape[1]\n        attention_weights_list = session.run(\n            [attention_weights_op], feed_dict=feed_dict)\n\n        raise NotImplementedError\n\n        for i_batch in range(batch_size):\n\n            # posteriors of each class\n            posteriors_index = np.array([i_batch + (batch_size * j)\n                                         for j in range(max_frame_num)])\n            posteriors_each = posteriors[posteriors_index]\n            posteriors_l = posteriors_each[:, 1]\n            posteriors_f = posteriors_each[:, 2]\n            predict_frames_l = np.where(posteriors_l >= threshold_l)[0]\n            predict_frames_f = np.where(posteriors_f >= threshold_f)[0]\n\n            # summarize consecutive frames in each spike\n            predict_frames_l_summary = []\n            predict_frames_f_summary = []\n            for i_frame in range(len(predict_frames_l)):\n                # not last frame\n                if i_frame != len(predict_frames_l) - 1:\n                    # not consecutive\n                    if predict_frames_l[i_frame] + 1 != predict_frames_l[i_frame + 1]:\n                        predict_frames_l_summary.append(\n                            predict_frames_l[i_frame])\n                else:\n                    predict_frames_l_summary.append(predict_frames_l[i_frame])\n            for i_frame in range(len(predict_frames_f)):\n                # not last frame\n                if i_frame != len(predict_frames_f) - 1:\n                    # not consecutive\n                    if predict_frames_f[i_frame] + 1 != predict_frames_f[i_frame + 1]:\n                        predict_frames_f_summary.append(\n                            predict_frames_f[i_frame])\n                else:\n                    predict_frames_f_summary.append(predict_frames_f[i_frame])\n\n            # compute true interval of each class\n            utt_info_list = utterance_dict[input_names[i_batch]]\n            true_frames_l = np.zeros((max_frame_num,))\n            true_frames_f = np.zeros((max_frame_num,))\n            for i_label in range(len(utt_info_list)):\n                start_frame = utt_info_list[i_label][1]\n                end_frame = utt_info_list[i_label][2]\n                if utt_info_list[i_label][0] == \'laughter\':\n                    true_frames_l[start_frame:end_frame] = 1\n                elif utt_info_list[i_label][0] == \'filler\':\n                    true_frames_f[start_frame:end_frame] = 1\n\n            detect_l_num = len(predict_frames_l_summary)\n            detect_f_num = len(predict_frames_f_summary)\n            true_l_num = np.sum(labels_true[i_batch] == 1)\n            true_f_num = np.sum(labels_true[i_batch] == 2)\n\n            ####################\n            # laughter\n            ####################\n            for frame in predict_frames_l_summary:\n                # prediction is true\n                if true_frames_l[frame] == 1:\n                    # TODO: \xe3\x81\xbe\xe3\x81\xa0\xe4\xba\x88\xe6\xb8\xac\xe3\x81\x97\xe3\x81\xa6\xe3\x81\xaa\xe3\x81\x84\n                    tp_l += 1\n                    # TODO: \xe3\x81\x99\xe3\x81\xa7\xe3\x81\xab\xe4\xba\x88\xe6\xb8\xac\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x9f\xe3\x82\x89\xe7\x84\xa1\xe8\xa6\x96\n                else:\n                    fp_l += 1\n            # could not predict\n            if true_l_num > detect_l_num:\n                fn_l += true_l_num - detect_l_num\n\n            ####################\n            # filler\n            ####################\n            for frame in predict_frames_f_summary:\n                # prediction is true\n                if true_frames_f[frame] == 1:\n                    # TODO: \xe3\x81\xbe\xe3\x81\xa0\xe4\xba\x88\xe6\xb8\xac\xe3\x81\x97\xe3\x81\xa6\xe3\x81\xaa\xe3\x81\x84\n                    tp_f += 1\n                    # TODO: \xe3\x81\x99\xe3\x81\xa7\xe3\x81\xab\xe4\xba\x88\xe6\xb8\xac\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x9f\xe3\x82\x89\xe7\x84\xa1\xe8\xa6\x96\n                else:\n                    fp_f += 1\n            # could not predict\n            if true_f_num > detect_f_num:\n                fn_f += true_f_num - detect_f_num\n\n            if progressbar:\n                pbar.update(1)\n\n    p_l = tp_l / (tp_l + fp_l) if (tp_l + fp_l) != 0 else 0\n    r_l = tp_l / (tp_l + fn_l) if (tp_l + fn_l) != 0 else 0\n    f_l = 2 * r_l * p_l / (r_l + p_l) if (r_l + p_l) != 0 else 0\n\n    r_f = tp_f / (tp_f + fn_f) if (tp_f + fn_f) != 0 else 0\n    p_f = tp_f / (tp_f + fp_f) if (tp_f + fp_f) != 0 else 0\n    f_f = 2 * r_f * p_f / (r_f + p_f) if (r_f + p_f) != 0 else 0\n\n    # confusion_l = [tp_l, fp_l, fn_l, tp_l + fp_l + fn_l]\n    # confusion_f = [tp_f, fp_f, fn_f, tp_f + fp_f + fn_f]\n    acc_l = [p_l, r_l, f_l]\n    acc_f = [p_f, r_f, f_f]\n    mean = [(p_l + p_f) / 2., (r_l + r_f) / 2., (f_l + f_f) / 2.]\n\n    # df_confusion = pd.DataFrame({\'Laughter\': confusion_l, \'Filler\': confusion_f},\n    #                             columns=[\'Laughter\', \'Filler\'],\n    #                             index=[\'TP\', \'FP\', \'FN\', \'Sum\'])\n    # print(df_confusion)\n\n    df_acc = pd.DataFrame({\'Laughter\': acc_l, \'Filler\': acc_f, \'Mean\': mean},\n                          columns=[\'Laughter\', \'Filler\', \'Mean\'],\n                          index=[\'Precision\', \'Recall\', \'F-measure\'])\n    # print(df_acc)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return mean[2], df_acc\n\n\ndef do_eval_ler(session, ler_op, model, dataset, progressbar=False):\n    """"""Evaluate trained model by Label Error Rate.\n    Args:\n        session: session of training model\n        ler_op: operation for computing label error rate\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        progressbar (bool, optional): if True, visualize the progressbar\n    Returns:\n        ler_mean (float): An average of LER\n    """"""\n    ler_mean = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, _, _ = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        ler_batch = session.run(ler_op, feed_dict=feed_dict)\n        ler_mean += ler_batch * batch_size\n\n        if progressbar:\n            pbar.update(batch_size)\n\n    ler_mean /= dataset.data_num\n\n    return ler_mean\n'"
examples/svc/metrics/ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for CTC model (SVC corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom utils.io.labels.sparsetensor import sparsetensor2list\n\n\ndef do_eval_fmeasure(session, decode_op, model, dataset,\n                     eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by F-measure.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset\' class\n        label_type (string): phone39 or phone48 or phone61\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Return:\n        fmean (float): mean of f-measure of laughter and filler\n    """"""\n    # Reset data counter\n    dataset.reset()\n\n    if eval_batch_size is None:\n        batch_size = dataset.batch_size\n    else:\n        batch_size = eval_batch_size\n\n    tp_l, fp_l, fn_l = 0, 0, 0\n    tp_f, fp_f, fn_f = 0, 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, _ = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        # Decode\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n        labels_pred = sparsetensor2list(labels_pred_st, batch_size)\n\n        for i_batch in range(batch_size):\n\n            detected_l_num = np.sum(np.array(labels_pred[i_batch]) == 1)\n            detected_f_num = np.sum(np.array(labels_pred[i_batch]) == 2)\n            true_l_num = np.sum(labels_true[0][i_batch] == 1)\n            true_f_num = np.sum(labels_true[0][i_batch] == 2)\n\n            # Laughter\n            if detected_l_num <= true_l_num:\n                tp_l += detected_l_num\n                fn_l += true_l_num - detected_l_num\n            else:\n                tp_l += true_l_num\n                fp_l += detected_l_num - true_l_num\n\n            # Filler\n            if detected_f_num <= true_f_num:\n                tp_f += detected_f_num\n                fn_f += true_f_num - detected_f_num\n            else:\n                tp_f += true_f_num\n                fp_f += detected_f_num - true_f_num\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    # Compute F-measure\n    p_l = tp_l / (tp_l + fp_l) if (tp_l + fp_l) != 0 else 0\n    r_l = tp_l / (tp_l + fn_l) if (tp_l + fn_l) != 0 else 0\n    f_l = 2 * r_l * p_l / (r_l + p_l) if (r_l + p_l) != 0 else 0\n\n    r_f = tp_f / (tp_f + fn_f) if (tp_f + fn_f) != 0 else 0\n    p_f = tp_f / (tp_f + fp_f) if (tp_f + fp_f) != 0 else 0\n    f_f = 2 * r_f * p_f / (r_f + p_f) if (r_f + p_f) != 0 else 0\n\n    # confusion_l = [tp_l, fp_l, fn_l, tp_l + fp_l + fn_l]\n    # confusion_f = [tp_f, fp_f, fn_f, tp_f + fp_f + fn_f]\n    acc_l = [p_l, r_l, f_l]\n    acc_f = [p_f, r_f, f_f]\n    mean = [(p_l + p_f) / 2., (r_l + r_f) / 2., (f_l + f_f) / 2.]\n\n    # df_confusion = pd.DataFrame({\'Laughter\': confusion_l, \'Filler\': confusion_f},\n    #                             columns=[\'Laughter\', \'Filler\'],\n    #                             index=[\'TP\', \'FP\', \'FN\', \'Sum\'])\n    # print(df_confusion)\n\n    df_acc = pd.DataFrame({\'Laughter\': acc_l, \'Filler\': acc_f, \'Mean\': mean},\n                          columns=[\'Laughter\', \'Filler\', \'Mean\'],\n                          index=[\'Precision\', \'Recall\', \'F-measure\'])\n    # print(df_acc)\n\n    return mean[2], df_acc\n\n\ndef do_eval_fmeasure_time(session, decode_op, posteriors_op, model, dataset,\n                          eval_batch_size=None, progressbar=False):\n    """"""Evaluate trained model by F-measure.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        posteriors_op: operation for computing posteriors\n        model: the model to evaluate\n        dataset: An instance of a `Dataset\' class\n        label_type (string): phone39 or phone48 or phone61\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n    Returns:\n        fmean (float): mean of f-measure of laughter and filler\n    """"""\n    threshold_l = threshold_f = 0.5\n\n    # Load ground truth labels\n    utterance_dict = read_trans(\n        label_path=\'/n/sd8/inaguma/corpus/svc/data/labels.txt\')\n\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    tp_l, fp_l, fn_l = 0, 0, 0\n    tp_f, fp_f, fn_f = 0, 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, input_names = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        max_frame_num = inputs.shape[1]\n        posteriors = session.run([posteriors_op], feed_dict=feed_dict)\n        for i_batch in range(batch_size):\n\n            # posteriors of each class\n            posteriors_index = np.array([i_batch + (batch_size * j)\n                                         for j in range(max_frame_num)])\n            posteriors_each = posteriors[posteriors_index]\n            posteriors_l = posteriors_each[:, 1]\n            posteriors_f = posteriors_each[:, 2]\n            predict_frames_l = np.where(posteriors_l >= threshold_l)[0]\n            predict_frames_f = np.where(posteriors_f >= threshold_f)[0]\n\n            # summarize consecutive frames in each spike\n            predict_frames_l_summary = []\n            predict_frames_f_summary = []\n            for i_frame in range(len(predict_frames_l)):\n                # not last frame\n                if i_frame != len(predict_frames_l) - 1:\n                    # not consecutive\n                    if predict_frames_l[i_frame] + 1 != predict_frames_l[i_frame + 1]:\n                        predict_frames_l_summary.append(\n                            predict_frames_l[i_frame])\n                else:\n                    predict_frames_l_summary.append(predict_frames_l[i_frame])\n            for i_frame in range(len(predict_frames_f)):\n                # not last frame\n                if i_frame != len(predict_frames_f) - 1:\n                    # not consecutive\n                    if predict_frames_f[i_frame] + 1 != predict_frames_f[i_frame + 1]:\n                        predict_frames_f_summary.append(\n                            predict_frames_f[i_frame])\n                else:\n                    predict_frames_f_summary.append(predict_frames_f[i_frame])\n\n            # compute true interval of each class\n            utt_info_list = utterance_dict[input_names[i_batch]]\n            true_frames_l = np.zeros((max_frame_num,))\n            true_frames_f = np.zeros((max_frame_num,))\n            for i_label in range(len(utt_info_list)):\n                start_frame = utt_info_list[i_label][1]\n                end_frame = utt_info_list[i_label][2]\n                if utt_info_list[i_label][0] == \'laughter\':\n                    true_frames_l[start_frame:end_frame] = 1\n                elif utt_info_list[i_label][0] == \'filler\':\n                    true_frames_f[start_frame:end_frame] = 1\n\n            detect_l_num = len(predict_frames_l_summary)\n            detect_f_num = len(predict_frames_f_summary)\n            true_l_num = np.sum(labels_true[i_batch] == 1)\n            true_f_num = np.sum(labels_true[i_batch] == 2)\n\n            ####################\n            # laughter\n            ####################\n            for frame in predict_frames_l_summary:\n                # prediction is true\n                if true_frames_l[frame] == 1:\n                    # TODO: \xe3\x81\xbe\xe3\x81\xa0\xe4\xba\x88\xe6\xb8\xac\xe3\x81\x97\xe3\x81\xa6\xe3\x81\xaa\xe3\x81\x84\n                    tp_l += 1\n                    # TODO: \xe3\x81\x99\xe3\x81\xa7\xe3\x81\xab\xe4\xba\x88\xe6\xb8\xac\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x9f\xe3\x82\x89\xe7\x84\xa1\xe8\xa6\x96\n                else:\n                    fp_l += 1\n            # could not predict\n            if true_l_num > detect_l_num:\n                fn_l += true_l_num - detect_l_num\n\n            ####################\n            # filler\n            ####################\n            for frame in predict_frames_f_summary:\n                # prediction is true\n                if true_frames_f[frame] == 1:\n                    # TODO: \xe3\x81\xbe\xe3\x81\xa0\xe4\xba\x88\xe6\xb8\xac\xe3\x81\x97\xe3\x81\xa6\xe3\x81\xaa\xe3\x81\x84\n                    tp_f += 1\n                    # TODO: \xe3\x81\x99\xe3\x81\xa7\xe3\x81\xab\xe4\xba\x88\xe6\xb8\xac\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x9f\xe3\x82\x89\xe7\x84\xa1\xe8\xa6\x96\n                else:\n                    fp_f += 1\n            # could not predict\n            if true_f_num > detect_f_num:\n                fn_f += true_f_num - detect_f_num\n\n            if progressbar:\n                pbar.update(1)\n\n    p_l = tp_l / (tp_l + fp_l) if (tp_l + fp_l) != 0 else 0\n    r_l = tp_l / (tp_l + fn_l) if (tp_l + fn_l) != 0 else 0\n    f_l = 2 * r_l * p_l / (r_l + p_l) if (r_l + p_l) != 0 else 0\n\n    r_f = tp_f / (tp_f + fn_f) if (tp_f + fn_f) != 0 else 0\n    p_f = tp_f / (tp_f + fp_f) if (tp_f + fp_f) != 0 else 0\n    f_f = 2 * r_f * p_f / (r_f + p_f) if (r_f + p_f) != 0 else 0\n\n    # confusion_l = [tp_l, fp_l, fn_l, tp_l + fp_l + fn_l]\n    # confusion_f = [tp_f, fp_f, fn_f, tp_f + fp_f + fn_f]\n    acc_l = [p_l, r_l, f_l]\n    acc_f = [p_f, r_f, f_f]\n    mean = [(p_l + p_f) / 2., (r_l + r_f) / 2., (f_l + f_f) / 2.]\n\n    # df_confusion = pd.DataFrame({\'Laughter\': confusion_l, \'Filler\': confusion_f},\n    #                             columns=[\'Laughter\', \'Filler\'],\n    #                             index=[\'TP\', \'FP\', \'FN\', \'Sum\'])\n    # print(df_confusion)\n\n    df_acc = pd.DataFrame({\'Laughter\': acc_l, \'Filler\': acc_f, \'Mean\': mean},\n                          columns=[\'Laughter\', \'Filler\', \'Mean\'],\n                          index=[\'Precision\', \'Recall\', \'F-measure\'])\n    # print(df_acc)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return mean[2], df_acc\n\n\ndef read_trans(label_path):\n    """"""Read labels.\n    Args:\n        label_path (list): list of paths to target labels\n    Returns:\n        utterance_dict (dict):\n            key (string) => speaker + \'_\' + utt_index\n            value (list) => [label, start_frame, end_frame, label, ...]\n    """"""\n    utterance_dict = {}\n    with open(label_path, \'r\') as f:\n        for line in f.readlines()[1:]:\n\n            # Extract utterance information\n            line = line.strip(\'\\n\').split(\',\')\n            utt_index = line[0]\n            speaker = line[1]\n            # gender = line[2]\n            # global_start_time = line[3]\n            line = line[4:]\n\n            # Extract laughter and filler\n            labels_utt = []\n            for i in range(int(len(line) / 3)):\n                label = line[0]\n                start = float(line[1])\n                end = float(line[2])\n\n                labels_utt.append(\n                    [label, int(round(start * 100)), int(round(end * 100))])\n\n                line = line[3:]\n\n            utterance_dict[speaker + \'_\' + utt_index] = labels_utt\n\n    return utterance_dict\n\n\ndef do_eval_ler(session, ler_op, model, dataset, progressbar=False):\n    """"""Evaluate trained model by Label Error Rate.\n    Args:\n        session: session of training model\n        ler_op: operation for computing label error rate\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        progressbar (bool, optional): if True, visualize the progressbar\n    Returns:\n        ler_mean (float): An average of LER\n    """"""\n    ler_mean = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, _ = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        ler_batch = session.run(ler_op, feed_dict=feed_dict)\n        ler_mean += ler_batch * batch_size\n\n        if progressbar:\n            pbar.update(batch_size)\n\n    ler_mean /= dataset.data_num\n\n    return ler_mean\n'"
examples/svc/training/train_attention.py,9,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the Attention-based model (SVC corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.svc.data.load_dataset_attention import Dataset\nfrom experiments.svc.metrics.attention import do_eval_fmeasure, do_eval_fmeasure_time\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\n\n\ndef do_train(model, params):\n    """"""Run training. If target labels are phone, the model is evaluated by PER\n    with 39 phones.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n    """"""\n    map_file_path = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'.txt\'\n\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path,\n        max_epoch=params[\'num_epoch\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True)\n    dev_data = Dataset(\n        data_type=\'dev\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        batch_size=1, map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders()\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        # Add to the graph each operation (including model definition)\n        loss_op, logits, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.labels_seq_len_pl_list[0],\n            model.keep_prob_encoder_pl_list[0],\n            model.keep_prob_decoder_pl_list[0],\n            model.keep_prob_embedding_pl_list[0])\n        train_op = model.train(loss_op,\n                               optimizer=params[\'optimizer\'],\n                               learning_rate=learning_rate_pl)\n        _, decode_op_infer = model.decode(\n            decoder_outputs_train,\n            decoder_outputs_infer)\n        ler_op = model.compute_ler(model.labels_st_true_pl,\n                                   model.labels_st_pred_pl)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=False)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total param\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M param"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize param\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            fmean_dev_best = 0\n            fmean_time_dev_best = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels_train, inputs_seq_len, labels_seq_len, _ = data\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs[0],\n                    model.labels_pl_list[0]: labels_train[0],\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                    model.labels_seq_len_pl_list[0]: labels_seq_len[0],\n                    model.keep_prob_encoder_pl_list[0]: 1 - float(params[\'dropout_encoder\']),\n                    model.keep_prob_decoder_pl_list[0]: 1 - float(params[\'dropout_decoder\']),\n                    model.keep_prob_embedding_pl_list[0]: 1 - float(params[\'dropout_embedding\']),\n                    learning_rate_pl: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % params[\'print_step\'] == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels_dev, inputs_seq_len,\n                     labels_seq_len, _), _ = dev_data.next()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs[0],\n                        model.labels_pl_list[0]: labels_dev[0],\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                        model.labels_seq_len_pl_list[0]: labels_seq_len[0],\n                        model.keep_prob_encoder_pl_list[0]: 1.0,\n                        model.keep_prob_decoder_pl_list[0]: 1.0,\n                        model.keep_prob_embedding_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_encoder_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_decoder_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_embedding_pl_list[0]] = 1.0\n\n                    # Predict class ids & update even files\n                    predicted_ids_train, summary_str_train = sess.run(\n                        [decode_op_infer, summary_train], feed_dict=feed_dict_train)\n                    predicted_ids_dev, summary_str_dev = sess.run(\n                        [decode_op_infer, summary_dev], feed_dict=feed_dict_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    # Convert to sparsetensor to compute LER\n                    feed_dict_ler_train = {\n                        model.labels_st_true_pl: list2sparsetensor(\n                            labels_train[0], padded_value=train_data.padded_value),\n                        model.labels_st_pred_pl: list2sparsetensor(\n                            predicted_ids_train, padded_value=train_data.padded_value)\n                    }\n                    feed_dict_ler_dev = {\n                        model.labels_st_true_pl: list2sparsetensor(\n                            labels_dev[0], padded_value=dev_data.padded_value),\n                        model.labels_st_pred_pl: list2sparsetensor(\n                            predicted_ids_dev, padded_value=dev_data.padded_value)\n                    }\n\n                    # Compute accuracy\n                    ler_train = sess.run(ler_op, feed_dict=feed_dict_ler_train)\n                    ler_dev = sess.run(ler_op, feed_dict=feed_dict_ler_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        fmean_dev_epoch, df_acc = do_eval_fmeasure(\n                            session=sess,\n                            decode_op=decode_op_infer,\n                            model=model,\n                            dataset=dev_data,\n                            eval_batch_size=params[\'batch_size\'])\n                        print(df_acc)\n                        print(\'  F-measure: %f %%\' % (fmean_dev_epoch))\n\n                        if fmean_dev_epoch > fmean_dev_best:\n                            fmean_dev_best = fmean_dev_epoch\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (F-measure)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model only when best accuracy is\n                            # obtained (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n\n                            print(\'=== Test Data Evaluation ===\')\n                            fmean_test_epoch, df_acc = do_eval_fmeasure(\n                                session=sess,\n                                decode_op=decode_op_infer,\n                                model=model,\n                                dataset=test_data,\n                                eval_batch_size=params[\'batch_size\'])\n                            print(df_acc)\n                            print(\'  F-measure: %f %%\' % (fmean_test_epoch))\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=fmean_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'feature\'] == \'fbank\':\n        input_size = 123\n    elif params[\'feature\'] == \'is13\':\n        input_size = 141\n\n    if params[\'label_type\'] in [\'original\', \'phone3\']:\n        params[\'num_classes\'] = 3\n    elif params[\'label_type\'] == \'phone4\':\n        params[\'num_classes\'] = 4\n    elif params[\'label_type\'] == \'phone43\':\n        params[\'num_classes\'] = 43\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=input_size * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'])\n\n    # Set process name\n    setproctitle(\'tf_svc_\' + model.name + \'_\' +\n                 params[\'label_type\'] + \'_\' + params[\'attention_type\'])\n\n    model.name += \'_en\' + str(params[\'encoder_num_units\'])\n    model.name += \'_\' + str(params[\'encoder_num_layers\'])\n    model.name += \'_att\' + str(params[\'attention_dim\'])\n    model.name += \'_de\' + str(params[\'decoder_num_units\'])\n    model.name += \'_\' + str(params[\'decoder_num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    model.name += \'_\' + params[\'attention_type\']\n    if params[\'dropout_encoder\'] != 0:\n        model.name += \'_dropen\' + str(params[\'dropout_encoder\'])\n    if params[\'dropout_decoder\'] != 0:\n        model.name += \'_dropde\' + str(params[\'dropout_decoder\'])\n    if params[\'dropout_embedding\'] != 0:\n        model.name += \'_dropem\' + str(params[\'dropout_embedding\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'wd\' + str(params[\'weight_decay\'])\n    if params[\'sharpening_factor\'] != 1:\n        model.name += \'_sharp\' + str(params[\'sharpening_factor\'])\n    if params[\'logits_temperature\'] != 1:\n        model.name += \'_temp\' + str(params[\'logits_temperature\'])\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'attention\', params[\'label_type\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError(\'Length of args should be 3.\')\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/svc/training/train_ctc.py,9,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the CTC model (SVC corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.svc.data.load_dataset_ctc import Dataset\nfrom experiments.svc.metrics.ctc import do_eval_fmeasure, do_eval_fmeasure_time\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.ctc import CTC\n\n\ndef do_train(model, params):\n    """"""Run training.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=True)\n    dev_data = Dataset(\n        data_type=\'dev\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_data = Dataset(\n        data_type=\'dev\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders()\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        # Add to the graph each operation (including model definition)\n        loss_op, logits = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0])\n        train_op = model.train(\n            loss_op,\n            optimizer=params[\'optimizer\'],\n            learning_rate=learning_rate_pl)\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=params[\'beam_width\'])\n        ler_op = model.compute_ler(decode_op, model.labels_pl_list[0])\n        posteriors_op = model.posteriors(logits, blank_prior=1)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=False)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            fmean_dev_best = 0\n            fmean_time_dev_best = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels, inputs_seq_len, _ = data\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs[0],\n                    model.labels_pl_list[0]: list2sparsetensor(\n                        labels[0], padded_value=train_data.padded_value),\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                    model.keep_prob_pl_list[0]: 1 - float(params[\'dropout\']),\n                    learning_rate_pl: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % params[\'print_step\'] == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels, inputs_seq_len, _), _ = dev_data.next()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs[0],\n                        model.labels_pl_list[0]: list2sparsetensor(\n                            labels[0], padded_value=dev_data.padded_value),\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                        model.keep_prob_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_pl_list[0]] = 1.0\n\n                    # Compute accuracy & update event files\n                    ler_train, summary_str_train = sess.run(\n                        [ler_op, summary_train], feed_dict=feed_dict_train)\n                    ler_dev, summary_str_dev = sess.run(\n                        [ler_op, summary_dev], feed_dict=feed_dict_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        fmean_dev_epoch, df_acc = do_eval_fmeasure(\n                            session=sess,\n                            decode_op=decode_op,\n                            model=model,\n                            dataset=dev_data,\n                            eval_batch_size=params[\'batch_size\'])\n                        print(df_acc)\n                        print(\'  F-measure: %f %%\' % (fmean_dev_epoch))\n\n                        if fmean_dev_epoch > fmean_dev_best:\n                            fmean_dev_best = fmean_dev_epoch\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (F-measure)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model only when best accuracy is\n                            # obtained (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n\n                            print(\'=== Test Data Evaluation ===\')\n                            fmean_test_epoch, df_acc = do_eval_fmeasure(\n                                session=sess,\n                                decode_op=decode_op,\n                                model=model,\n                                dataset=test_data,\n                                eval_batch_size=params[\'batch_size\'])\n                            print(df_acc)\n                            print(\'  F-measure: %f %%\' % (fmean_test_epoch))\n\n                        # fmean_time_dev_epoch, df_acc = do_eval_fmeasure_time(\n                        #     session=sess,\n                        #     decode_op=decode_op,\n                        #     posteriors_op=posteriors_op,\n                        #     model=model,\n                        #     dataset=dev_data,\n                        #     eval_batch_size=params[\'batch_size\'])\n                        # print(df_acc)\n                        # print(\'  Time F-measure: %f %%\' %\n                        #       (fmean_time_dev_epoch))\n\n                        # if fmean_time_dev_best < fmean_time_dev_epoch:\n                        #     fmean_time_dev_best = fmean_time_dev_epoch\n                        #     print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (Time F-measure)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                        # fmean_time_test_epoch, df_acc = do_eval_fmeasure_time(\n                        #     session=sess,\n                        #     decode_op=decode_op,\n                        #     posteriors_op=posteriors_op,\n                        #     model=model,\n                        #     dataset=test_data,\n                        #     eval_batch_size=params[\'batch_size\'])\n                        # print(df_acc)\n                        # print(\'  Time F-measure: %f %%\' %\n                        #       (fmean_time_test_epoch))\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=fmean_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'feature\'] == \'fbank\':\n        input_size = 123\n    elif params[\'feature\'] == \'is13\':\n        input_size = 141\n\n    if params[\'label_type\'] in [\'original\', \'phone3\']:\n        params[\'num_classes\'] = 3\n    elif params[\'label_type\'] == \'phone4\':\n        params[\'num_classes\'] = 4\n    elif params[\'label_type\'] == \'phone43\':\n        params[\'num_classes\'] = 43\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=input_size * params[\'num_stack\'],\n                splice=params[\'splice\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\'tf_svc_\' + model.name + \'_\' + params[\'label_type\'])\n\n    model.name += \'_\' + str(params[\'num_units\'])\n    model.name += \'_\' + str(params[\'num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'num_proj\'] != 0:\n        model.name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'dropout\'] != 0:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'ctc\', params[\'label_type\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError(\'Length of args should be 3.\')\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/svc/visualization/__init__.py,0,b''
examples/svc/visualization/decode_ctc.py,0,"b'def decode_test(session, decode_op, network, dataset, label_type, rate=1.0):\n    """"""Visualize label outputs.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        network: network to evaluate\n        dataset: Dataset class\n        label_type: original or phone1 or phone2 or phone41\n        rate: rate of evaluation data to use\n    """"""\n    batch_size = 1\n    num_examples = dataset.data_num * rate\n    if batch_size == 1 or batch_size % 2 == 0:\n        iteration = int(num_examples / batch_size)\n    else:\n        iteration = int(num_examples / batch_size) + 1\n\n    for step in range(iteration):\n        # create feed dictionary for next mini batch\n        inputs, labels, seq_len, input_names = dataset.next_batch(\n            batch_size=batch_size)\n        indices, values, dense_shape = list2sparsetensor(labels)\n\n        feed_dict = {\n            network.inputs_pl: inputs,\n            network.label_indices_pl: indices,\n            network.label_values_pl: values,\n            network.label_shape_pl: dense_shape,\n            network.seq_len_pl: seq_len,\n            network.keep_prob_input_pl: 1.0,\n            network.keep_prob_hidden_pl: 1.0\n        }\n\n        # visualize\n        batch_size_each = len(labels)\n        labels_st = session.run(decode_op, feed_dict=feed_dict)\n        labels_pred = dataset.sparsetensor2list(labels_st, batch_size_each)\n        for i_batch in range(batch_size_each):\n            print(\'-----wav: %s-----\' % input_names[i_batch])\n            print(\'Pred: \', end="""")\n            print(np.array(labels_pred[i_batch]))\n            print(\'True: \', end="""")\n            print(labels[i_batch])\n'"
examples/svc/visualization/probs.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Plot posteriors of CTC outputs (SVC corpus).""""""\n\nimport os\nimport numpy as np\nimport scipy.io.wavfile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\'ggplot\')\nsns.set_style(""white"")\n\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\n# candidate = [\'S2399\', \'S1633\', \'S1627\', \'S2470\', \'S1582\', \'S2485\', \'S1608\', \'S1607\']\ncandidate = [\'S2399\', \'S2171\', \'S2470\']\n\n\ndef posterior_test(session, posteriors_op, network, dataset, label_type, rate=1.0):\n    """"""Visualize posteriors.\n    Args:\n        session: session of training model\n        posteriois_op: operation for computing posteriors\n        network: network to evaluate\n        dataset: Dataset class\n        label_type: original or phone1 or phone2 or phone41\n        rate: rate of evaluation data to use\n    """"""\n    # load ground truth labels\n    label_dict, _, _ = label.read(\n        label_path=\'/n/sd8/inaguma/corpus/svc/data/labels.txt\')\n\n    batch_size = 1\n    num_examples = dataset.data_num * rate\n    if batch_size == 1 or batch_size % 2 == 0:\n        iteration = int(num_examples / batch_size)\n    else:\n        iteration = int(num_examples / batch_size) + 1\n\n    for step in range(iteration):\n        # create feed dictionary for next mini batch\n        inputs, labels, seq_len, input_names = dataset.next_batch(\n            batch_size=batch_size)\n        indices, values, dense_shape = list2sparsetensor(labels)\n\n        feed_dict = {\n            network.inputs_pl: inputs,\n            network.label_indices_pl: indices,\n            network.label_values_pl: values,\n            network.label_shape_pl: dense_shape,\n            network.seq_len_pl: seq_len,\n            network.keep_prob_input_pl: 1.0,\n            network.keep_prob_hidden_pl: 1.0\n        }\n\n        # visualize\n        batch_size_each = len(labels)\n        max_frame_num = inputs.shape[1]\n        posteriors = session.run(posteriors_op, feed_dict=feed_dict)\n        for i_batch in range(batch_size_each):\n            posteriors_index = np.array([i_batch + (batch_size_each * j)\n                                         for j in range(max_frame_num)])\n            label_each = label_dict[input_names[i_batch]]\n\n            # if input_names[i_batch] in candidate:\n            #     # np.save(input_names[i_batch], posteriors[posteriors_index])\n            #     np.save(\'label_\' + input_names[i_batch], label_each)\n\n            if label_type == \'phone1\':\n                probs.plot_probs_ctc_phone1(probs=posteriors[posteriors_index],\n                                            save_path=network.model_dir,\n                                            input_name=input_names[i_batch],\n                                            ground_truth_list=label_each)\n            elif label_type == \'phone2\':\n                probs.plot_probs_ctc_phone2(probs=posteriors[posteriors_index],\n                                            save_path=network.model_dir,\n                                            input_name=input_names[i_batch],\n                                            ground_truth_list=label_each)\n            elif label_type == \'phone41\':\n                probs.plot_probs_ctc_phone41(probs=posteriors[posteriors_index],\n                                             save_path=network.model_dir,\n                                             input_name=input_names[i_batch],\n                                             ground_truth_list=label_each)\n\n\ndef plot_probs_framewise(probs, save_path, input_name):\n    """"""Plot posteriors of frame-wise classifiers.\n    Args:\n        probs: posteriors of each class\n        save_path: path to save graph\n    """"""\n\n    # plot probs\n    save_path = os.path.join(save_path, \'{0:04d}\'.format(input_name) + \'.png\')\n    times = np.arange(len(probs)) * 0.01\n    plt.clf()\n    # plt.figure(figsize=(10, 10))\n    plt.subplot(211)\n    # plt.plot(times, probs[:, 0],  label=\'garbage\')\n    plt.plot(times, probs[:, 1], label=\'laughter\')\n    plt.plot(times, probs[:, 2], label=\'filler\')\n    # plt.plot(probs[3][0:10], label=\'blank\')\n    plt.title(\'Probs: \' + save_path)\n    plt.ylabel(\'Probability\', fontsize=12)\n    plt.xlim([0, times[-1]])\n    plt.ylim([0, 1])\n    plt.legend(loc=\'best\')\n    plt.grid(True)\n    plt.tight_layout()\n\n    # plot smoothed probs\n\n    # plot waveform\n    wav_path = \'/n/sd8/inaguma/dataset/SVC/wav/S\' + \\\n        \'{0:04d}\'.format(input_name) + \'.wav\'\n    sampling_rate, waveform = scipy.io.wavfile.read(wav_path)\n    sampling_interval = 1.0 / sampling_rate\n    waveform = waveform / 32768.0  # normalize\n    times = np.arange(len(waveform)) * sampling_interval\n    plt.subplot(212)\n    plt.plot(times, waveform, color=\'grey\')\n    plt.xlabel(\'Time[sec]\', fontsize=12)\n    plt.ylabel(\'Amplitude\', fontsize=12)\n    plt.savefig(save_path)\n    # plt.show()\n\n\ndef plot_probs_ctc_phone1(probs, save_path, input_name, ground_truth_list):\n    """"""Plot posteriors of phone1.\n    Args:\n        probs: posteriors of each class\n        save_path: path to save graph\n        input_name: input name\n        ground_truth_list: list of ground truth labels\n    """"""\n    if input_name not in candidate:\n        return 0\n\n    ####################\n    # waveform\n    ####################\n    wav_path = \'/n/sd8/inaguma/corpus/svc/data/wav/\' + input_name + \'.wav\'\n    # wav_path = os.path.join(os.path.abspath(\'../../../../../\'), input_name + \'.wav\')\n    sampling_rate, waveform = scipy.io.wavfile.read(wav_path)\n    sampling_interval = 1.0 / sampling_rate\n    waveform = waveform / 32768.0\n    times_waveform = np.arange(len(waveform)) * sampling_interval\n    plt.clf()\n    # plt.figure(figsize=(10, 5))\n    plt.subplot(311)\n    plt.title(input_name + \'.wav\')\n    plt.tick_params(labelleft=\'off\')\n    plt.tick_params(labelbottom=\'off\')\n    plt.plot(times_waveform, waveform, color=\'grey\')\n    # plt.xlabel(\'Time[sec]\', fontsize=12)\n    plt.ylabel(\'Input speech\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n\n    ####################\n    # ground truth\n    ####################\n    times_probs = np.arange(len(probs)) * 0.01\n    laughter = np.array([-1] * len(times_probs))\n    filler = np.array([-1] * len(times_probs))\n    for i in range(len(ground_truth_list)):\n        start_frame = int(ground_truth_list[i][1])\n        end_frame = int(ground_truth_list[i][2])\n        if ground_truth_list[i][0] == \'laughter\':\n            laughter[start_frame:end_frame] = 1\n        elif ground_truth_list[i][0] == \'filler\':\n            filler[start_frame:end_frame] = 1\n    plt.plot(times_probs, laughter, label=\'laughter (ground truth)\',\n             color=orange, linewidth=2)\n    plt.plot(times_probs, filler, label=\'filler (ground truth)\',\n             color=green, linewidth=2)\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    ####################\n    # social signals\n    ####################\n    plt.subplot(312)\n    plt.tick_params(labelbottom=\'off\')\n    plt.plot(times_probs, probs[:, 1],\n             label=\'laughter (prediction)\', color=orange, linewidth=2)\n    plt.plot(times_probs, probs[:, 2],\n             label=\'filler (prediction)\', color=green, linewidth=2)\n    plt.ylabel(\'Social signals\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n    plt.ylim([0.05, 1.05])\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    ####################\n    # garbage\n    ####################\n    plt.subplot(313)\n    plt.tick_params(labelbottom=\'off\')\n    line_garbage, = plt.plot(\n        times_probs, probs[:, 0], label=\'garbage\', color=\'black\', linewidth=2)\n    line_blank, = plt.plot(\n        times_probs, probs[:, 3], \':\', label=\'blank\', color=\'grey\')\n    plt.ylabel(\'Other classes\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n    plt.ylim([0.05, 1.05])\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(handles=[line_blank, line_garbage],\n               loc=""upper right"", fontsize=12)\n\n    save_path = os.path.join(save_path, input_name + \'.png\')\n    plt.savefig(save_path, dvi=500)\n    # plt.show()\n\n\ndef plot_probs_ctc_phone2(probs, save_path, input_name, ground_truth_list):\n    """"""Plot posteriors of phone2.\n    Args:\n        probs: posteriors of each class\n        save_path: path to save graph\n        input_name: input name\n        ground_truth_list: list of ground truth labels\n    """"""\n    if input_name not in candidate:\n        return 0\n\n    ####################\n    # waveform\n    ####################\n    wav_path = \'/n/sd8/inaguma/corpus/svc/data/wav/\' + input_name + \'.wav\'\n    # wav_path = os.path.join(os.path.abspath(\'../../../../../\'), input_name + \'.wav\')\n    sampling_rate, waveform = scipy.io.wavfile.read(wav_path)\n    sampling_interval = 1.0 / sampling_rate\n    waveform = waveform / 32768.0\n    times_waveform = np.arange(len(waveform)) * sampling_interval\n    plt.clf()\n    # plt.figure(figsize=(10, 5))\n    plt.subplot(311)\n    plt.title(input_name + \'.wav\')\n    plt.tick_params(labelleft=\'off\')\n    plt.tick_params(labelbottom=\'off\')\n    plt.plot(times_waveform, waveform, color=\'grey\')\n    # plt.xlabel(\'Time[sec]\', fontsize=12)\n    plt.ylabel(\'Input speech\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n\n    ####################\n    # ground truth\n    ####################\n    times_probs = np.arange(len(probs)) * 0.01\n    laughter = np.array([-1] * len(times_probs))\n    filler = np.array([-1] * len(times_probs))\n    for i in range(len(ground_truth_list)):\n        start_frame = int(ground_truth_list[i][1])\n        end_frame = int(ground_truth_list[i][2])\n        if ground_truth_list[i][0] == \'laughter\':\n            laughter[start_frame:end_frame] = 1\n        elif ground_truth_list[i][0] == \'filler\':\n            filler[start_frame:end_frame] = 1\n    plt.plot(times_probs, laughter, label=\'laughter (ground truth)\',\n             color=orange, linewidth=2)\n    plt.plot(times_probs, filler, label=\'filler (ground truth)\',\n             color=green, linewidth=2)\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    ####################\n    # social signals\n    ####################\n    plt.subplot(312)\n    plt.tick_params(labelbottom=\'off\')\n    plt.plot(times_probs, probs[:, 1],\n             label=\'laughter (prediction)\', color=orange, linewidth=2)\n    plt.plot(times_probs, probs[:, 2],\n             label=\'filler (prediction)\', color=green, linewidth=2)\n    plt.ylabel(\'Social signals\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n    plt.ylim([0.05, 1.05])\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    ####################\n    # speech & silence\n    ####################\n    plt.subplot(313)\n    line_sil, = plt.plot(\n        times_probs, probs[:, 0], label=\'silence\', color=\'black\', linewidth=2)\n    plt.plot(times_probs, probs[:, 3], label=\'speech\', color=blue, linewidth=2)\n    line_blank, = plt.plot(\n        times_probs, probs[:, 4], \':\', label=\'blank\', color=\'grey\')\n    plt.ylabel(\'Other classes\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n    plt.ylim([0.05, 1.05])\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(handles=[line_blank, line_sil], loc=""upper right"", fontsize=12)\n\n    save_path = os.path.join(save_path, input_name + \'.png\')\n    plt.savefig(save_path, dvi=500)\n    # plt.show()\n\n\ndef plot_probs_ctc_phone41(probs, save_path, input_name, ground_truth_list):\n    """"""Plot posteriors of phone41.\n    Args:\n        probs: posteriors of each class\n        save_path: path to save graph\n        input_name: input name\n        ground_truth_list: list of ground truth labels\n    """"""\n    if input_name not in candidate:\n        return 0\n\n    ####################\n    # waveform\n    ####################\n    wav_path = \'/n/sd8/inaguma/corpus/svc/data/wav/\' + input_name + \'.wav\'\n    # wav_path = os.path.join(os.path.abspath(\'../../../../../\'), input_name + \'.wav\')\n    sampling_rate, waveform = scipy.io.wavfile.read(wav_path)\n    sampling_interval = 1.0 / sampling_rate\n    waveform = waveform / 32768.0\n    times_waveform = np.arange(len(waveform)) * sampling_interval\n    plt.clf()\n    # plt.figure(figsize=(10, 5))\n    plt.subplot(311)\n    plt.title(input_name + \'.wav\')\n    plt.tick_params(labelbottom=\'off\')\n    plt.tick_params(labelleft=\'off\')\n    plt.plot(times_waveform, waveform, color=\'grey\')\n    # plt.xlabel(\'Time[sec]\', fontsize=12)\n    plt.ylabel(\'Input speech\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n\n    ####################\n    # ground truth\n    ####################\n    times_probs = np.arange(len(probs)) * 0.01\n    laughter = np.array([-1] * len(times_probs))\n    filler = np.array([-1] * len(times_probs))\n    for i in range(len(ground_truth_list)):\n        start_frame = int(ground_truth_list[i][1])\n        end_frame = int(ground_truth_list[i][2])\n        if ground_truth_list[i][0] == \'laughter\':\n            laughter[start_frame:end_frame] = 1\n        elif ground_truth_list[i][0] == \'filler\':\n            filler[start_frame:end_frame] = 1\n    plt.plot(times_probs, laughter, color=orange,\n             linewidth=2, label=\'laughter (ground truth)\')\n    plt.plot(times_probs, filler, color=green,\n             linewidth=2, label=\'filler (ground truth)\')\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    ####################\n    # social signals\n    ####################\n    plt.subplot(312)\n    plt.tick_params(labelbottom=\'off\')\n    plt.plot(times_probs, probs[:, 1],\n             label=\'laughter (prediction)\', color=orange, linewidth=2)\n    plt.plot(times_probs, probs[:, 2],\n             label=\'filler (prediction)\', color=green, linewidth=2)\n    plt.ylabel(\'Social signals\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n    plt.ylim([0.05, 1.05])\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    ####################\n    # 41 phones\n    ####################\n    plt.subplot(313)\n    line_sil, = plt.plot(\n        times_probs, probs[:, 0], label=\'silence\', color=\'black\', linewidth=2)\n    for i in range(3, 43, 1):\n        plt.plot(times_probs, probs[:, i])\n    line_blank, = plt.plot(\n        times_probs, probs[:, 43], \':\', label=\'blank\', color=\'grey\')\n    plt.ylabel(\'Other classes\', fontsize=12)\n    plt.xlim([0, times_waveform[-1]])\n    plt.ylim([0.05, 1.05])\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(handles=[line_blank, line_sil], loc=""upper right"", fontsize=12)\n\n    save_path = os.path.join(save_path, input_name + \'.png\')\n    plt.savefig(save_path, dvi=500)\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    for file in os.listdir(os.path.abspath(\'./class2\')):\n        posterior = np.load(os.path.abspath(\'./class2/\' + file))\n        input_name = os.path.basename(file).split(\'.\')[0]\n        ground_truth_list = np.load(os.path.abspath(\'./label_\' + file))\n        plot_probs_ctc_phone2(posterior, save_path=os.path.abspath(\'./\'),\n                              input_name=input_name, ground_truth_list=ground_truth_list)\n'"
examples/timit/data/__init__.py,0,b''
examples/timit/data/load_dataset_attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the Attention-based model (TIMIT corpus).\n   In addition, frame stacking and skipping are used.\n   You can use only the single GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.attention import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, label_type, batch_size, map_file_path,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or test\n            label_type (string): phone39 or phone48 or phone61 or\n                character or character_capital_divide\n            batch_size (int): the size of mini-batch\n            map_file_path (string): path to the mapping file\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        super(Dataset, self).__init__(map_file_path=map_file_path)\n\n        self.is_test = True if data_type == \'test\' else False\n\n        self.data_type = data_type\n        self.label_type = label_type\n        self.batch_size = batch_size\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = 1\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/timit\',\n                        \'/n/sd8/inaguma/corpus/timit/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\', data_type)\n        # NOTE: ex.) save_path: timit_dataset_path/inputs/data_type/***.npy\n        label_path = join(dataset_root[0], \'labels\', data_type, label_type)\n        # NOTE: ex.) save_path:\n        # timit_dataset_path/labels/data_type/label_type/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\', data_type)\n            label_path = join(dataset_root[0], \'labels\', data_type, label_type)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for input_name, frame_num in frame_num_tuple_sorted:\n            input_paths.append(join(input_path, input_name + \'.npy\'))\n            label_paths.append(join(label_path, input_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/timit/data/load_dataset_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the CTC model (TIMIT corpus).\n   In addition, frame stacking and skipping are used.\n   You can use only the single GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.ctc import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, label_type, batch_size,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or test\n            label_type (string): phone39 or phone48 or phone61 or\n                character or character_capital_divide\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        super(Dataset, self).__init__()\n\n        self.is_test = True if data_type == \'test\' else False\n\n        self.data_type = data_type\n        self.label_type = label_type\n        self.batch_size = batch_size\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = 1\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/timit\',\n                        \'/n/sd8/inaguma/corpus/timit/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\', data_type)\n        # NOTE: ex.) save_path: timit_dataset_path/inputs/data_type/***.npy\n        label_path = join(dataset_root[0], \'labels\', data_type, label_type)\n        # NOTE: ex.) save_path:\n        # timit_dataset_path/labels/data_type/label_type/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\', data_type)\n            label_path = join(dataset_root[0], \'labels\', data_type, label_type)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for input_name, frame_num in frame_num_tuple_sorted:\n            input_paths.append(join(input_path, input_name + \'.npy\'))\n            label_paths.append(join(label_path, input_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/timit/data/load_dataset_joint_ctc_attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the Joint CTC-Attention model (TIMIT corpus).\n   In addition, frame stacking and skipping are used.\n   You can use only the single GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.joint_ctc_attention import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, label_type, batch_size, map_file_path,\n                 max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or test\n            label_type (string): phone39 or phone48 or phone61 or\n                character or character_capital_divide\n            batch_size (int): the size of mini-batch\n            map_file_path (string): path to the mapping file\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        super(Dataset, self).__init__(map_file_path=map_file_path)\n\n        self.is_test = True if data_type == \'test\' else False\n\n        self.data_type = data_type\n        self.label_type = label_type\n        self.batch_size = batch_size\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = 1\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/timit\',\n                        \'/n/sd8/inaguma/corpus/timit/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\', data_type)\n        # NOTE: ex.) save_path: timit_dataset_path/inputs/data_type/***.npy\n        label_path = join(dataset_root[0], \'labels\', data_type, label_type)\n        # NOTE: ex.) save_path:\n        # timit_dataset_path/labels/data_type/label_type/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\', data_type)\n            label_path = join(dataset_root[0], \'labels\', data_type, label_type)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_paths = [], []\n        for input_name, frame_num in frame_num_tuple_sorted:\n            input_paths.append(join(input_path, input_name + \'.npy\'))\n            label_paths.append(join(label_path, input_name + \'.npy\'))\n        self.input_paths = np.array(input_paths)\n        self.label_paths = np.array(label_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/timit/data/load_dataset_multitask_ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Load dataset for the multitask CTC model (TIMIT corpus).\n   In addition, frame stacking and skipping are used.\n   You can use only the single GPU version.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile\nimport pickle\nimport numpy as np\n\nfrom utils.dataset.multitask_ctc import DatasetBase\n\n\nclass Dataset(DatasetBase):\n\n    def __init__(self, data_type, label_type_main, label_type_sub,\n                 batch_size, max_epoch=None, splice=1,\n                 num_stack=1, num_skip=1,\n                 shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                 progressbar=False):\n        """"""A class for loading dataset.\n        Args:\n            data_type (string): train or dev or test\n            label_type_main (string): character or character_capital_divide\n            label_type_sub (stirng): phone39 or phone48 or phone61\n            batch_size (int): the size of mini-batch\n            max_epoch (int, optional): the max epoch. None means infinite loop.\n            splice (int, optional): frames to splice. Default is 1 frame.\n            num_stack (int, optional): the number of frames to stack\n            num_skip (int, optional): the number of frames to skip\n            shuffle (bool, optional): if True, shuffle utterances. This is\n                disabled when sort_utt is True.\n            sort_utt (bool, optional): if True, sort all utterances by the\n                number of frames and utteraces in each mini-batch are shuffled.\n                Otherwise, shuffle utteraces.\n            sort_stop_epoch (int, optional): After sort_stop_epoch, training\n                will revert back to a random order\n            progressbar (bool, optional): if True, visualize progressbar\n        """"""\n        super(Dataset, self).__init__()\n\n        self.is_test = True if data_type == \'test\' else False\n\n        self.data_type = data_type\n        self.label_type_main = label_type_main\n        self.label_type_sub = label_type_sub\n        self.batch_size = batch_size\n        self.max_epoch = max_epoch\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_skip = num_skip\n        self.shuffle = shuffle\n        self.sort_utt = sort_utt\n        self.sort_stop_epoch = sort_stop_epoch\n        self.progressbar = progressbar\n        self.num_gpu = 1\n\n        # paths where datasets exist\n        dataset_root = [\'/data/inaguma/timit\',\n                        \'/n/sd8/inaguma/corpus/timit/dataset\']\n\n        input_path = join(dataset_root[0], \'inputs\', data_type)\n        # NOTE: ex.) save_path: timit_dataset_path/inputs/data_type/***.npy\n        label_main_path = join(\n            dataset_root[0], \'labels\', data_type, label_type_main)\n        label_sub_path = join(\n            dataset_root[0], \'labels\', data_type, label_type_sub)\n        # NOTE: ex.) save_path:\n        # timit_dataset_path/labels/data_type/label_type/***.npy\n\n        # Load the frame number dictionary\n        if isfile(join(input_path, \'frame_num.pickle\')):\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n        else:\n            dataset_root.pop(0)\n            input_path = join(dataset_root[0], \'inputs\', data_type)\n            label_main_path = join(\n                dataset_root[0], \'labels\', data_type, label_type_main)\n            label_sub_path = join(\n                dataset_root[0], \'labels\', data_type, label_type_sub)\n            with open(join(input_path, \'frame_num.pickle\'), \'rb\') as f:\n                self.frame_num_dict = pickle.load(f)\n\n        # Sort paths to input & label\n        axis = 1 if sort_utt else 0\n        frame_num_tuple_sorted = sorted(self.frame_num_dict.items(),\n                                        key=lambda x: x[axis])\n        input_paths, label_main_paths, label_sub_paths = [], [], []\n        for input_name, frame_num in frame_num_tuple_sorted:\n            input_paths.append(join(input_path, input_name + \'.npy\'))\n            label_main_paths.append(join(label_main_path, input_name + \'.npy\'))\n            label_sub_paths.append(join(label_sub_path,  input_name + \'.npy\'))\n        if len(label_main_paths) != len(label_sub_paths):\n            raise ValueError(\'The numbers of labels between \' +\n                             \'character and phone are not same.\')\n        self.input_paths = np.array(input_paths)\n        self.label_main_paths = np.array(label_main_paths)\n        self.label_sub_paths = np.array(label_sub_paths)\n        # NOTE: Not load dataset yet\n\n        self.rest = set(range(0, len(self.input_paths), 1))\n'"
examples/timit/evaluation/__init__.py,0,b''
examples/timit/evaluation/eval_attention.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the trained Attention-based model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom examples.timit.data.load_dataset_attention import Dataset\nfrom examples.timit.metrics.attention import do_eval_per, do_eval_cer\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=5,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_eval(model, params, epoch, beam_width, eval_batch_size):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam_width (int, optional): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    map_file_path = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'.txt\'\n\n    dev_data = Dataset(\n        data_type=\'dev\', label_type=\'phone61\',\n        batch_size=eval_batch_size,  map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Load dataset\n    if \'phone\' in params[\'label_type\']:\n        test_data = Dataset(\n            data_type=\'test\', label_type=\'phone39\',\n            batch_size=eval_batch_size,  map_file_path=map_file_path,\n            splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            shuffle=False, progressbar=True)\n    else:\n        test_data = Dataset(\n            data_type=\'test\', label_type=params[\'label_type\'],\n            batch_size=eval_batch_size,  map_file_path=map_file_path,\n            splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, _, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n        model.inputs_pl_list[0],\n        model.labels_pl_list[0],\n        model.inputs_seq_len_pl_list[0],\n        model.labels_seq_len_pl_list[0],\n        model.keep_prob_encoder_pl_list[0],\n        model.keep_prob_decoder_pl_list[0],\n        model.keep_prob_embedding_pl_list[0])\n    _, decode_op_infer = model.decode(\n        decoder_outputs_train,\n        decoder_outputs_infer)\n    per_op = model.compute_ler(\n        model.labels_st_true_pl, model.labels_st_pred_pl)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'Test Data Evaluation:\')\n        if \'char\' in params[\'label_type\']:\n            cer_test, wer_test = do_eval_cer(\n                session=sess,\n                decode_op=decode_op_infer,\n                model=model,\n                dataset=test_data,\n                label_type=params[\'label_type\'],\n                is_test=True,\n                eval_batch_size=eval_batch_size,\n                progressbar=True)\n            print(\'  CER: %f %%\' % (cer_test * 100))\n            print(\'  WER: %f %%\' % (wer_test * 100))\n        else:\n            per_test = do_eval_per(\n                session=sess,\n                decode_op=decode_op_infer,\n                per_op=per_op,\n                model=model,\n                dataset=test_data,\n                label_type=params[\'label_type\'],\n                is_test=True,\n                eval_batch_size=eval_batch_size,\n                progressbar=True)\n            print(\'  PER: %f %%\' % (per_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n    else:\n        raise TypeError\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, eval_batch_size=args.eval_batch_size,\n            beam_width=args.beam_width)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/evaluation/eval_ctc.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the trained CTC model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom examples.timit.data.load_dataset_ctc import Dataset\nfrom examples.timit.metrics.ctc import do_eval_per, do_eval_cer\nfrom models.ctc.ctc import CTC\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_eval(model, params, epoch, beam_width, eval_batch_size):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam_width (int, optional): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    if \'phone\' in params[\'label_type\']:\n        test_data = Dataset(\n            data_type=\'test\', label_type=\'phone39\',\n            batch_size=eval_batch_size, splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            shuffle=False, progressbar=True)\n    else:\n        test_data = Dataset(\n            data_type=\'test\', label_type=params[\'label_type\'],\n            batch_size=eval_batch_size, splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                   model.labels_pl_list[0],\n                                   model.inputs_seq_len_pl_list[0],\n                                   model.keep_prob_pl_list[0])\n    decode_op = model.decoder(logits,\n                              model.inputs_seq_len_pl_list[0],\n                              beam_width=beam_width)\n    per_op = model.compute_ler(decode_op, model.labels_pl_list[0])\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                # Use the best model\n                # NOTE: In the training stage, parameters are saved only when\n                # accuracies are improved\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'Test Data Evaluation:\')\n        if \'char\' in params[\'label_type\']:\n            cer_test, wer_test = do_eval_cer(\n                session=sess,\n                decode_op=decode_op,\n                model=model,\n                dataset=test_data,\n                label_type=params[\'label_type\'],\n                is_test=True,\n                eval_batch_size=eval_batch_size,\n                progressbar=True)\n            print(\'  CER: %f %%\' % (cer_test * 100))\n            print(\'  WER: %f %%\' % (wer_test * 100))\n        else:\n            per_test = do_eval_per(\n                session=sess,\n                decode_op=decode_op,\n                per_op=per_op,\n                model=model,\n                dataset=test_data,\n                label_type=params[\'label_type\'],\n                is_test=True,\n                eval_batch_size=eval_batch_size,\n                progressbar=True)\n            print(\'  PER: %f %%\' % (per_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, eval_batch_size=args.eval_batch_size,\n            beam_width=args.beam_width)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/evaluation/eval_multitask_ctc.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate the trained multi-task CTC model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom examples.timit.data.load_dataset_multitask_ctc import Dataset\nfrom examples.timit.metrics.ctc import do_eval_per, do_eval_cer\nfrom models.ctc.multitask_ctc import MultitaskCTC\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_eval(model, params, epoch, beam_width, eval_batch_size):\n    """"""Evaluate the model.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        batch_size (int): the size of mini-batch when evaluation\n        beam_width (int): beam_width (int, optional): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type_main=params[\'label_type_main\'],\n        label_type_sub=\'phone39\',\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation\n    _, logits_main, logits_sub = model.compute_loss(\n        model.inputs_pl_list[0],\n        model.labels_pl_list[0],\n        model.labels_sub_pl_list[0],\n        model.inputs_seq_len_pl_list[0],\n        model.keep_prob_pl_list[0])\n    decode_op_main, decode_op_sub = model.decoder(\n        logits_main, logits_sub,\n        model.inputs_seq_len_pl_list[0],\n        beam_width=beam_width)\n    _, per_op = model.compute_ler(\n        decode_op_main, decode_op_sub,\n        model.labels_pl_list[0], model.labels_sub_pl_list[0])\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                # Use the best model\n                # NOTE: In the training stage, parameters are saved only when\n                # accuracies are improved\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'=== Test Data Evaluation ===\')\n        cer_test, wer_test = do_eval_cer(\n            session=sess,\n            decode_op=decode_op_main,\n            model=model,\n            dataset=test_data,\n            label_type=params[\'label_type_main\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True,\n            is_multitask=True)\n        print(\'  CER: %f %%\' % (cer_test * 100))\n        print(\'  WER: %f %%\' % (wer_test * 100))\n\n        per_test = do_eval_per(\n            session=sess,\n            decode_op=decode_op_sub,\n            per_op=per_op,\n            model=model,\n            dataset=test_data,\n            label_type=params[\'label_type_sub\'],\n            is_test=True,\n            eval_batch_size=eval_batch_size,\n            progressbar=True,\n            is_multitask=True)\n        print(\'  PER: %f %%\' % (per_test * 100))\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type_main\'] == \'character\':\n        params[\'num_classes_main\'] = 28\n    elif params[\'label_type_main\'] == \'character_capital_divide\':\n        params[\'num_classes_main\'] = 72\n    else:\n        raise TypeError\n    if params[\'label_type_sub\'] == \'phone61\':\n        params[\'num_classes_sub\'] = 61\n    elif params[\'label_type_sub\'] == \'phone48\':\n        params[\'num_classes_sub\'] = 48\n    elif params[\'label_type_sub\'] == \'phone39\':\n        params[\'num_classes_sub\'] = 39\n    else:\n        raise TypeError\n\n    # Model setting\n    model = MultitaskCTC(encoder_type=params[\'encoder_type\'],\n                         input_size=params[\'input_size\'],\n                         splice=params[\'splice\'],\n                         num_stack=params[\'num_stack\'],\n                         num_units=params[\'num_units\'],\n                         num_layers_main=params[\'num_layers_main\'],\n                         num_layers_sub=params[\'num_layers_sub\'],\n                         num_classes_main=params[\'num_classes_main\'],\n                         num_classes_sub=params[\'num_classes_sub\'],\n                         main_task_weight=params[\'main_task_weight\'],\n                         lstm_impl=params[\'lstm_impl\'],\n                         use_peephole=params[\'use_peephole\'],\n                         parameter_init=params[\'weight_init\'],\n                         clip_grad_norm=params[\'clip_grad_norm\'],\n                         clip_activation=params[\'clip_activation\'],\n                         num_proj=params[\'num_proj\'],\n                         weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_eval(model=model, params=params,\n            epoch=args.epoch, beam_width=args.beam_width,\n            eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/metrics/__init__.py,0,b''
examples/timit/metrics/attention.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for the Attention-based model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nfrom tqdm import tqdm\n\nfrom examples.timit.metrics.mapping import Map2phone39\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.evaluation.edit_distance import compute_per, compute_cer, compute_wer, wer_align\n\n\ndef do_eval_per(session, decode_op, per_op, model, dataset, label_type,\n                is_test=False, eval_batch_size=None, progressbar=False,\n                is_multitask=False, is_jointctcatt=False):\n    """"""Evaluate trained model by Phone Error Rate.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        per_op: operation for computing phone error rate\n        model: the model to evaluate\n        dataset: An instance of a `Dataset\' class\n        label_type (string): phone39 or phone48 or phone61\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n        is_multitask (bool, optional): if True, evaluate the multitask model\n        is_jointctcatt (bool, optional): if True, evaluate the joint\n            CTC-Attention model\n    Returns:\n        per_mean (float): An average of PER\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    train_label_type = label_type\n    eval_label_type = dataset.label_type_sub if is_multitask else dataset.label_type\n\n    idx2phone_train = Idx2phone(\n        map_file_path=\'../metrics/mapping_files/\' + train_label_type + \'.txt\')\n    idx2phone_eval = Idx2phone(\n        map_file_path=\'../metrics/mapping_files/\' + eval_label_type + \'.txt\')\n    map2phone39_train = Map2phone39(\n        label_type=train_label_type,\n        map_file_path=\'../metrics/mapping_files/phone2phone.txt\')\n    map2phone39_eval = Map2phone39(\n        label_type=eval_label_type,\n        map_file_path=\'../metrics/mapping_files/phone2phone.txt\')\n\n    per_mean = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini-batch\n        if is_multitask:\n            inputs, _, labels_true, inputs_seq_len, labels_seq_len, _ = data\n        elif is_jointctcatt:\n            inputs, labels_true, _, inputs_seq_len, labels_seq_len, _ = data\n        else:\n            inputs, labels_true, inputs_seq_len, labels_seq_len, _ = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        # Evaluate by 39 phones\n        labels_pred = session.run(decode_op, feed_dict=feed_dict)\n\n        for i_batch in range(batch_size):\n            ###############\n            # Hypothesis\n            ###############\n            # Convert from index to phone (-> list of phone strings)\n            str_pred = idx2phone_train(labels_pred[i_batch]).split(\'>\')[0]\n            # NOTE: Trancate by <EOS>\n\n            # Remove the last space\n            if str_pred[-1] == \' \':\n                str_pred = str_pred[:-1]\n\n            phone_pred_list = str_pred.split(\' \')\n\n            ###############\n            # Reference\n            ###############\n            if is_test:\n                phone_true_list = labels_true[0][i_batch][0].split(\' \')\n            else:\n                # Convert from index to phone (-> list of phone strings)\n                phone_true_list = idx2phone_eval(\n                    labels_true[0][i_batch][1:labels_seq_len[0][i_batch] - 1]).split(\' \')\n                # NOTE: Exclude <SOS> and <EOS>\n\n            # Mapping to 39 phones (-> list of phone strings)\n            phone_pred_list = map2phone39_train(phone_pred_list)\n            phone_true_list = map2phone39_eval(phone_true_list)\n\n            # Compute PER\n            per_mean += compute_per(ref=phone_true_list,\n                                    hyp=phone_pred_list,\n                                    normalize=True)\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    per_mean /= len(dataset)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return per_mean\n\n\ndef do_eval_cer(session, decode_op, model, dataset, label_type,\n                is_test=False, eval_batch_size=None, progressbar=False,\n                is_multitask=False, is_jointctcatt=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): character or character_capital_divide\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n        is_multitask (bool, optional): if True, evaluate the multitask model\n        is_jointctcatt (bool, optional): if True, evaluate the joint\n            CTC-Attention model\n    Return:\n        cer_mean (float): An average of CER\n        wer_mean (float): An average of WER\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type + \'.txt\')\n\n    cer_mean, wer_mean = 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini-batch\n        if is_multitask:\n            inputs, labels_true, _, inputs_seq_len, labels_seq_len, _ = data\n        elif is_jointctcatt:\n            inputs, labels_true, _, inputs_seq_len, labels_seq_len, _ = data\n        else:\n            inputs, labels_true, inputs_seq_len, labels_seq_len, _ = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        labels_pred = session.run(decode_op, feed_dict=feed_dict)\n        for i_batch in range(batch_size):\n\n            # Convert from list of index to string\n            if is_test:\n                str_true = labels_true[0][i_batch][0]\n                # NOTE: transcript is seperated by space(\'_\')\n            else:\n                # Convert from list of index to string\n                str_true = idx2char(\n                    labels_true[0][i_batch][1:labels_seq_len[0][i_batch] - 1],\n                    padded_value=dataset.padded_value)\n            str_pred = idx2char(labels_pred[i_batch]).split(\'>\')[0]\n            # NOTE: Trancate by <EOS>\n\n            # Remove consecutive spaces\n            str_pred = re.sub(r\'[_]+\', \'_\', str_pred)\n\n            # Remove garbage labels\n            str_true = re.sub(r\'[<>\\\'\\"":;!?,.-]+\', \'\', str_true)\n            str_pred = re.sub(r\'[<>\\\'\\"":;!?,.-]+\', \'\', str_pred)\n\n            # Compute WER\n            wer_mean += compute_wer(hyp=str_pred.split(\'_\'),\n                                    ref=str_true.split(\'_\'),\n                                    normalize=True)\n            # substitute, insert, delete = wer_align(\n            #     ref=str_pred.split(\'_\'),\n            #     hyp=str_true.split(\'_\'))\n            # print(\'SUB: %d\' % substitute)\n            # print(\'INS: %d\' % insert)\n            # print(\'DEL: %d\' % delete)\n\n            # Remove spaces\n            str_pred = re.sub(r\'[_]+\', \'\', str_pred)\n            str_true = re.sub(r\'[_]+\', \'\', str_true)\n\n            # Compute CER\n            cer_mean += compute_cer(str_pred=str_pred,\n                                    str_true=str_true,\n                                    normalize=True)\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= len(dataset)\n    wer_mean /= len(dataset)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return cer_mean, wer_mean\n'"
examples/timit/metrics/ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Define evaluation method for the CTC model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nfrom tqdm import tqdm\n\nfrom examples.timit.metrics.mapping import Map2phone39\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.io.labels.sparsetensor import sparsetensor2list\nfrom utils.evaluation.edit_distance import compute_per, compute_cer, compute_wer, wer_align\n\n\ndef do_eval_per(session, decode_op, per_op, model, dataset, label_type,\n                is_test=False, eval_batch_size=None, progressbar=False,\n                is_multitask=False):\n    """"""Evaluate trained model by Phone Error Rate.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        per_op: operation for computing phone error rate\n        model: the model to evaluate\n        dataset: An instance of a `Dataset\' class\n        label_type (string): phone39 or phone48 or phone61\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n        is_multitask (bool, optional): if True, evaluate the multitask model\n    Returns:\n        per_mean (float): An average of PER\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    train_label_type = label_type\n    eval_label_type = dataset.label_type_sub if is_multitask else dataset.label_type\n\n    idx2phone_train = Idx2phone(\n        map_file_path=\'../metrics/mapping_files/\' + train_label_type + \'.txt\')\n    idx2phone_eval = Idx2phone(\n        map_file_path=\'../metrics/mapping_files/\' + eval_label_type + \'.txt\')\n    map2phone39_train = Map2phone39(\n        label_type=train_label_type,\n        map_file_path=\'../metrics/mapping_files/phone2phone.txt\')\n    map2phone39_eval = Map2phone39(\n        label_type=eval_label_type,\n        map_file_path=\'../metrics/mapping_files/phone2phone.txt\')\n\n    per_mean = 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        if is_multitask:\n            inputs, _, labels_true, inputs_seq_len, _ = data\n        else:\n            inputs, labels_true, inputs_seq_len, _ = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        # Evaluate by 39 phones\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n        labels_pred = sparsetensor2list(labels_pred_st, batch_size)\n\n        for i_batch in range(batch_size):\n            ###############\n            # Hypothesis\n            ###############\n            # Convert from index to phone (-> list of phone strings)\n            phone_pred_list = idx2phone_train(labels_pred[i_batch]).split(\' \')\n\n            # Mapping to 39 phones (-> list of phone strings)\n            phone_pred_list = map2phone39_train(phone_pred_list)\n\n            ###############\n            # Reference\n            ###############\n            if is_test:\n                phone_true_list = labels_true[0][i_batch][0].split(\' \')\n            else:\n                # Convert from index to phone (-> list of phone strings)\n                phone_true_list = idx2phone_eval(\n                    labels_true[0][i_batch]).split(\' \')\n\n            # Mapping to 39 phones (-> list of phone strings)\n            phone_true_list = map2phone39_eval(phone_true_list)\n\n            # Compute PER\n            per_mean += compute_per(ref=phone_pred_list,\n                                    hyp=phone_true_list,\n                                    normalize=True)\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    per_mean /= len(dataset)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return per_mean\n\n\ndef do_eval_cer(session, decode_op, model, dataset, label_type,\n                is_test=False, eval_batch_size=None, progressbar=False,\n                is_multitask=False):\n    """"""Evaluate trained model by Character Error Rate.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): character or character_capital_divide\n        is_test (bool, optional): set to True when evaluating by the test set\n        eval_batch_size (int, optional): the batch size when evaluating the model\n        progressbar (bool, optional): if True, visualize the progressbar\n        is_multitask (bool, optional): if True, evaluate the multitask model\n    Return:\n        cer_mean (float): An average of CER\n        wer_mean (float): An average of WER\n    """"""\n    batch_size_original = dataset.batch_size\n\n    # Reset data counter\n    dataset.reset()\n\n    # Set batch size in the evaluation\n    if eval_batch_size is not None:\n        dataset.batch_size = eval_batch_size\n\n    if label_type == \'character\':\n        idx2char = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n    elif label_type == \'character_capital_divide\':\n        idx2char = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character_capital_divide.txt\',\n            capital_divide=True,\n            space_mark=\'_\')\n\n    cer_mean, wer_mean = 0, 0\n    if progressbar:\n        pbar = tqdm(total=len(dataset))\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        if is_multitask:\n            inputs, labels_true, _, inputs_seq_len, _ = data\n        else:\n            inputs, labels_true, inputs_seq_len, _ = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n        labels_pred = sparsetensor2list(labels_pred_st, batch_size)\n        for i_batch in range(batch_size):\n\n            # Convert from list of index to string\n            if is_test:\n                str_true = labels_true[0][i_batch][0]\n                # NOTE: transcript is seperated by space(\'_\')\n            else:\n                # Convert from list of index to string\n                str_true = idx2char(labels_true[0][i_batch],\n                                    padded_value=dataset.padded_value)\n            str_pred = idx2char(labels_pred[i_batch])\n\n            # Remove consecutive spaces\n            str_pred = re.sub(r\'[_]+\', \'_\', str_pred)\n\n            # Remove garbage labels\n            str_true = re.sub(r\'[\\\'\\"":;!?,.-]+\', \'\', str_true)\n            str_pred = re.sub(r\'[\\\'\\"":;!?,.-]+\', \'\', str_pred)\n\n            # Compute WER\n            wer_mean += compute_wer(hyp=str_pred.split(\'_\'),\n                                    ref=str_true.split(\'_\'),\n                                    normalize=True)\n            # substitute, insert, delete = wer_align(\n            #     ref=str_pred.split(\'_\'),\n            #     hyp=str_true.split(\'_\'))\n            # print(\'SUB: %d\' % substitute)\n            # print(\'INS: %d\' % insert)\n            # print(\'DEL: %d\' % delete)\n\n            # Remove spaces\n            str_pred = re.sub(r\'[_]+\', \'\', str_pred)\n            str_true = re.sub(r\'[_]+\', \'\', str_true)\n\n            # Compute CER\n            cer_mean += compute_cer(str_pred=str_pred,\n                                    str_true=str_true,\n                                    normalize=True)\n\n            if progressbar:\n                pbar.update(1)\n\n        if is_new_epoch:\n            break\n\n    cer_mean /= len(dataset)\n    wer_mean /= len(dataset)\n\n    # Register original batch size\n    if eval_batch_size is not None:\n        dataset.batch_size = batch_size_original\n\n    return cer_mean, wer_mean\n'"
examples/timit/metrics/mapping.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Functions for phone mapping.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Map2phone39(object):\n    """"""Map from 61 or 48 phones to 39 phones.\n    Args:\n        label_type (string): phone48 or phone61\n        map_file_path: path to the mapping file\n    """"""\n\n    def __init__(self, label_type, map_file_path):\n        self.label_type = label_type\n\n        # Read the mapping file\n        self.map_dict = {}\n        with open(map_file_path) as f:\n            for line in f:\n                line = line.strip().split()\n                if label_type == \'phone61\':\n                    if line[1] != \'nan\':\n                        self.map_dict[line[0]] = line[2]\n                    else:\n                        self.map_dict[line[0]] = \'\'\n                elif label_type == \'phone48\':\n                    if line[1] != \'nan\':\n                        self.map_dict[line[1]] = line[2]\n\n    def __call__(self, phone_list):\n        """"""\n        Args:\n            phone_list (list): list of phones (string)\n        Returns:\n            phone_list (list): list of 39 phones (string)\n        """"""\n        if self.label_type == \'phone39\':\n            return phone_list\n\n        # Map to 39 phones\n        for i in range(len(phone_list)):\n            phone_list[i] = self.map_dict[phone_list[i]]\n\n        # Ignore q (only if 61 phones)\n        while \'\' in phone_list:\n            phone_list.remove(\'\')\n\n        return phone_list\n'"
examples/timit/training/train_attention.py,9,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the Attention-based model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom examples.timit.data.load_dataset_attention import Dataset\nfrom examples.timit.metrics.attention import do_eval_per, do_eval_cer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\n\n\ndef do_train(model, params):\n    """"""Run training. If target labels are phone, the model is evaluated by PER\n    with 39 phones.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n    """"""\n    map_file_path_train = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'.txt\'\n    if \'phone\' in params[\'label_type\']:\n        map_file_path_eval = \'../metrics/mapping_files/phone39.txt\'\n    else:\n        map_file_path_eval = \'../metrics/mapping_files/\' + \\\n            params[\'label_type\'] + \'.txt\'\n\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path_train,\n        max_epoch=params[\'num_epoch\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'])\n    dev_data = Dataset(\n        data_type=\'dev\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path_train,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n    if \'char\' in params[\'label_type\']:\n        test_data = Dataset(\n            data_type=\'test\', label_type=params[\'label_type\'],\n            batch_size=1, map_file_path=map_file_path_eval,\n            splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            sort_utt=False)\n    else:\n        test_data = Dataset(\n            data_type=\'test\', label_type=\'phone39\',\n            batch_size=1, map_file_path=map_file_path_eval,\n            splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            sort_utt=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders()\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        # Add to the graph each operation (including model definition)\n        loss_op, logits, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.labels_seq_len_pl_list[0],\n            model.keep_prob_encoder_pl_list[0],\n            model.keep_prob_decoder_pl_list[0],\n            model.keep_prob_embedding_pl_list[0])\n        train_op = model.train(loss_op,\n                               optimizer=params[\'optimizer\'],\n                               learning_rate=learning_rate_pl)\n        _, decode_op_infer = model.decode(\n            decoder_outputs_train,\n            decoder_outputs_infer)\n        ler_op = model.compute_ler(model.labels_st_true_pl,\n                                   model.labels_st_pred_pl)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total param\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M param"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize param\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            ler_dev_best = 1\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels_train, inputs_seq_len, labels_seq_len, _ = data\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs[0],\n                    model.labels_pl_list[0]: labels_train[0],\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                    model.labels_seq_len_pl_list[0]: labels_seq_len[0],\n                    model.keep_prob_encoder_pl_list[0]: 1 - float(params[\'dropout_encoder\']),\n                    model.keep_prob_decoder_pl_list[0]: 1 - float(params[\'dropout_decoder\']),\n                    model.keep_prob_embedding_pl_list[0]: 1 - float(params[\'dropout_embedding\']),\n                    learning_rate_pl: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % params[\'print_step\'] == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels_dev, inputs_seq_len,\n                     labels_seq_len, _), _ = dev_data.next()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs[0],\n                        model.labels_pl_list[0]: labels_dev[0],\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                        model.labels_seq_len_pl_list[0]: labels_seq_len[0],\n                        model.keep_prob_encoder_pl_list[0]: 1.0,\n                        model.keep_prob_decoder_pl_list[0]: 1.0,\n                        model.keep_prob_embedding_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_encoder_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_decoder_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_embedding_pl_list[0]] = 1.0\n\n                    # Predict class ids & update even files\n                    predicted_ids_train, summary_str_train = sess.run(\n                        [decode_op_infer, summary_train], feed_dict=feed_dict_train)\n                    predicted_ids_dev, summary_str_dev = sess.run(\n                        [decode_op_infer, summary_dev], feed_dict=feed_dict_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    # Convert to sparsetensor to compute LER\n                    feed_dict_ler_train = {\n                        model.labels_st_true_pl: list2sparsetensor(\n                            labels_train[0], padded_value=train_data.padded_value),\n                        model.labels_st_pred_pl: list2sparsetensor(\n                            predicted_ids_train, padded_value=train_data.padded_value)\n                    }\n                    feed_dict_ler_dev = {\n                        model.labels_st_true_pl: list2sparsetensor(\n                            labels_dev[0], padded_value=dev_data.padded_value),\n                        model.labels_st_pred_pl: list2sparsetensor(\n                            predicted_ids_dev, padded_value=dev_data.padded_value)\n                    }\n\n                    # Compute accuracy\n                    ler_train = sess.run(ler_op, feed_dict=feed_dict_ler_train)\n                    ler_dev = sess.run(ler_op, feed_dict=feed_dict_ler_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        if \'char\' in params[\'label_type\']:\n                            print(\'=== Dev Data Evaluation ===\')\n                            ler_dev_epoch, wer_dev_epoch = do_eval_cer(\n                                session=sess,\n                                decode_op=decode_op_infer,\n                                model=model,\n                                dataset=dev_data,\n                                label_type=params[\'label_type\'],\n                                eval_batch_size=1)\n                            print(\'  CER: %f %%\' % (ler_dev_epoch * 100))\n                            print(\'  WER: %f %%\' % (wer_dev_epoch * 100))\n\n                            if ler_dev_epoch < ler_dev_best:\n                                ler_dev_best = ler_dev_epoch\n                                print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                                # Save model only when best accuracy is\n                                # obtained (check point)\n                                checkpoint_file = join(\n                                    model.save_path, \'model.ckpt\')\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=train_data.epoch)\n                                print(""Model saved in file: %s"" % save_path)\n\n                                print(\'=== Test Data Evaluation ===\')\n                                ler_test, wer_test = do_eval_cer(\n                                    session=sess,\n                                    decode_op=decode_op_infer,\n                                    model=model,\n                                    dataset=test_data,\n                                    label_type=params[\'label_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(\'  CER: %f %%\' % (ler_test * 100))\n                                print(\'  WER: %f %%\' % (wer_test * 100))\n\n                        else:\n                            print(\'=== Dev Data Evaluation ===\')\n                            ler_dev_epoch = do_eval_per(\n                                session=sess,\n                                decode_op=decode_op_infer,\n                                per_op=ler_op,\n                                model=model,\n                                dataset=dev_data,\n                                label_type=params[\'label_type\'],\n                                eval_batch_size=1)\n                            print(\'  PER: %f %%\' % (ler_dev_epoch * 100))\n\n                            if ler_dev_epoch < ler_dev_best:\n                                ler_dev_best = ler_dev_epoch\n                                print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (PER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                                # Save model only when best accuracy is\n                                # obtained (check point)\n                                checkpoint_file = join(\n                                    model.save_path, \'model.ckpt\')\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=train_data.epoch)\n                                print(""Model saved in file: %s"" % save_path)\n\n                                print(\'=== Test Data Evaluation ===\')\n                                ler_test = do_eval_per(\n                                    session=sess,\n                                    decode_op=decode_op_infer,\n                                    per_op=ler_op,\n                                    model=model,\n                                    dataset=test_data,\n                                    label_type=params[\'label_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(\'  PER: %f %%\' % (ler_test * 100))\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=ler_dev_epoch)\n\n                    start_time_step = time.time()\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n    else:\n        raise TypeError\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'],\n        sigmoid_smoothing=params[\'sigmoid_smoothing\'])\n\n    # Set process name\n    setproctitle(\'tf_timit_\' + model.name + \'_\' +\n                 params[\'label_type\'] + \'_\' + params[\'attention_type\'])\n\n    model.name = \'en\' + str(params[\'encoder_num_units\'])\n    model.name += \'_\' + str(params[\'encoder_num_layers\'])\n    model.name += \'_att\' + str(params[\'attention_dim\'])\n    model.name += \'_de\' + str(params[\'decoder_num_units\'])\n    model.name += \'_\' + str(params[\'decoder_num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    model.name += \'_\' + params[\'attention_type\']\n    if params[\'dropout_encoder\'] != 0:\n        model.name += \'_dropen\' + str(params[\'dropout_encoder\'])\n    if params[\'dropout_decoder\'] != 0:\n        model.name += \'_dropde\' + str(params[\'dropout_decoder\'])\n    if params[\'dropout_embedding\'] != 0:\n        model.name += \'_dropem\' + str(params[\'dropout_embedding\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'wd\' + str(params[\'weight_decay\'])\n    if params[\'sharpening_factor\'] != 1:\n        model.name += \'_sharp\' + str(params[\'sharpening_factor\'])\n    if params[\'logits_temperature\'] != 1:\n        model.name += \'_temp\' + str(params[\'logits_temperature\'])\n    if bool(params[\'sigmoid_smoothing\']):\n        model.name += \'_smoothing\'\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'attention\', params[\'label_type\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError(\'Length of args should be 3.\')\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/timit/training/train_ctc.py,9,"b'# ! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the CTC model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom examples.timit.data.load_dataset_ctc import Dataset\nfrom examples.timit.metrics.ctc import do_eval_per, do_eval_cer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.ctc import CTC\n\n\ndef do_train(model, params):\n    """"""Run training. If target labels are phone, the model is evaluated by PER\n    with 39 phones.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'])\n    dev_data = Dataset(\n        data_type=\'dev\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n    if \'char\' in params[\'label_type\']:\n        test_data = Dataset(\n            data_type=\'test\', label_type=params[\'label_type\'],\n            batch_size=1, splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            sort_utt=False)\n    else:\n        test_data = Dataset(\n            data_type=\'test\', label_type=\'phone39\',\n            batch_size=1, splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            sort_utt=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders()\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        # Add to the graph each operation (including model definition)\n        loss_op, logits = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0])\n        train_op = model.train(\n            loss_op,\n            optimizer=params[\'optimizer\'],\n            learning_rate=learning_rate_pl)\n        decode_op = model.decoder(logits,\n                                  model.inputs_seq_len_pl_list[0],\n                                  beam_width=params[\'beam_width\'])\n        ler_op = model.compute_ler(decode_op, model.labels_pl_list[0])\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            ler_dev_best = 1\n            not_improved_epoch = 0\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels, inputs_seq_len, _ = data\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs[0],\n                    model.labels_pl_list[0]: list2sparsetensor(\n                        labels[0], padded_value=train_data.padded_value),\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                    model.keep_prob_pl_list[0]: 1 - float(params[\'dropout\']),\n                    learning_rate_pl: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % params[\'print_step\'] == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels, inputs_seq_len, _), _ = dev_data.next()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs[0],\n                        model.labels_pl_list[0]: list2sparsetensor(\n                            labels[0], padded_value=dev_data.padded_value),\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                        model.keep_prob_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_pl_list[0]] = 1.0\n\n                    # Compute accuracy & update event files\n                    ler_train, summary_str_train = sess.run(\n                        [ler_op, summary_train], feed_dict=feed_dict_train)\n                    ler_dev, summary_str_dev = sess.run(\n                        [ler_op, summary_dev], feed_dict=feed_dict_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        if \'char\' in params[\'label_type\']:\n                            print(\'=== Dev Data Evaluation ===\')\n                            ler_dev_epoch, wer_dev_epoch = do_eval_cer(\n                                session=sess,\n                                decode_op=decode_op,\n                                model=model,\n                                dataset=dev_data,\n                                label_type=params[\'label_type\'],\n                                eval_batch_size=1)\n                            print(\'  CER: %f %%\' % (ler_dev_epoch * 100))\n                            print(\'  WER: %f %%\' % (wer_dev_epoch * 100))\n\n                            if ler_dev_epoch < ler_dev_best:\n                                ler_dev_best = ler_dev_epoch\n                                not_improved_epoch = 0\n                                print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                                # Save model only when best accuracy is\n                                # obtained (check point)\n                                checkpoint_file = join(\n                                    model.save_path, \'model.ckpt\')\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=train_data.epoch)\n                                print(""Model saved in file: %s"" % save_path)\n\n                                print(\'=== Test Data Evaluation ===\')\n                                ler_test, wer_test = do_eval_cer(\n                                    session=sess,\n                                    decode_op=decode_op,\n                                    model=model,\n                                    dataset=test_data,\n                                    label_type=params[\'label_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(\'  CER: %f %%\' % (ler_test * 100))\n                                print(\'  WER: %f %%\' % (wer_test * 100))\n                            else:\n                                not_improved_epoch += 1\n\n                        else:\n                            print(\'=== Dev Data Evaluation ===\')\n                            ler_dev_epoch = do_eval_per(\n                                session=sess,\n                                decode_op=decode_op,\n                                per_op=ler_op,\n                                model=model,\n                                dataset=dev_data,\n                                label_type=params[\'label_type\'],\n                                eval_batch_size=1)\n                            print(\'  PER: %f %%\' % (ler_dev_epoch * 100))\n\n                            if ler_dev_epoch < ler_dev_best:\n                                ler_dev_best = ler_dev_epoch\n                                not_improved_epoch = 0\n                                print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (PER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                                # Save model only when best accuracy is\n                                # obtained (check point)\n                                checkpoint_file = join(\n                                    model.save_path, \'model.ckpt\')\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=train_data.epoch)\n                                print(""Model saved in file: %s"" % save_path)\n\n                                print(\'=== Test Data Evaluation ===\')\n                                ler_test = do_eval_per(\n                                    session=sess,\n                                    decode_op=decode_op,\n                                    per_op=ler_op,\n                                    model=model,\n                                    dataset=test_data,\n                                    label_type=params[\'label_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1)\n                                print(\'  PER: %f %%\' % (ler_test * 100))\n                            else:\n                                not_improved_epoch += 1\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Early stopping\n                        if not_improved_epoch == params[\'not_improved_patient_epoch\']:\n                            break\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=ler_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n    else:\n        raise TypeError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\'tf_timit_\' + model.name + \'_\' + params[\'label_type\'])\n\n    model.name += \'_\' + str(params[\'num_units\'])\n    model.name += \'_\' + str(params[\'num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'num_proj\'] != 0:\n        model.name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'dropout\'] != 0:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'ctc\', params[\'label_type\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError(\'Length of args should be 3.\')\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/timit/training/train_joint_ctc_attention.py,9,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the Joint CTC-Attention model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom examples.timit.data.load_dataset_joint_ctc_attention import Dataset\nfrom examples.timit.metrics.attention import do_eval_per, do_eval_cer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.attention.joint_ctc_attention import JointCTCAttention\n\n\ndef do_train(model, params):\n    """"""Run training. If target labels are phone, the model is evaluated by PER\n    with 39 phones.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n    """"""\n    map_file_path_train = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'.txt\'\n    map_file_path_eval = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'.txt\'\n\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path_train,\n        max_epoch=params[\'num_epoch\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'])\n    dev_data = Dataset(\n        data_type=\'dev\', label_type=params[\'label_type\'],\n        batch_size=params[\'batch_size\'], map_file_path=map_file_path_train,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n    if params[\'label_type\'] in [\'character\', \'character_capital_divide\']:\n        test_data = Dataset(\n            data_type=\'test\', label_type=params[\'label_type\'],\n            batch_size=1, map_file_path=map_file_path_eval,\n            splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            sort_utt=False)\n    else:\n        test_data = Dataset(\n            data_type=\'test\', label_type=\'phone39\',\n            batch_size=1, map_file_path=map_file_path_eval,\n            splice=params[\'splice\'],\n            num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n            sort_utt=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders()\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        # Add to the graph each operation (including model definition)\n        loss_op, logits, ctc_logits, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.ctc_labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.labels_seq_len_pl_list[0],\n            model.keep_prob_encoder_pl_list[0],\n            model.keep_prob_decoder_pl_list[0],\n            model.keep_prob_embedding_pl_list[0])\n        train_op = model.train(loss_op,\n                               optimizer=params[\'optimizer\'],\n                               learning_rate=learning_rate_pl)\n        _, decode_op_infer = model.decode(\n            decoder_outputs_train,\n            decoder_outputs_infer)\n        ler_op = model.compute_ler(model.labels_st_true_pl,\n                                   model.labels_st_pred_pl)\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total param\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M param"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_ler_train, csv_ler_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize param\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            ler_dev_best = 1\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels_train, ctc_labels, inputs_seq_len, labels_seq_len, _ = data\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs[0],\n                    model.labels_pl_list[0]: labels_train[0],\n                    model.ctc_labels_pl_list[0]: list2sparsetensor(\n                        ctc_labels[0], padded_value=train_data.ctc_padded_value),\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                    model.labels_seq_len_pl_list[0]: labels_seq_len[0],\n                    model.keep_prob_encoder_pl_list[0]: 1 - float(params[\'dropout_encoder\']),\n                    model.keep_prob_decoder_pl_list[0]: 1 - float(params[\'dropout_decoder\']),\n                    model.keep_prob_embedding_pl_list[0]: 1 - float(params[\'dropout_embedding\']),\n                    learning_rate_pl: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % params[\'print_step\'] == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels_dev, ctc_labels, inputs_seq_len,\n                     labels_seq_len, _), _ = dev_data.next()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs[0],\n                        model.labels_pl_list[0]: labels_dev[0],\n                        model.ctc_labels_pl_list[0]: list2sparsetensor(\n                            ctc_labels[0], padded_value=train_data.ctc_padded_value),\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                        model.labels_seq_len_pl_list[0]: labels_seq_len[0],\n                        model.keep_prob_encoder_pl_list[0]: 1.0,\n                        model.keep_prob_decoder_pl_list[0]: 1.0,\n                        model.keep_prob_embedding_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_encoder_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_decoder_pl_list[0]] = 1.0\n                    feed_dict_train[model.keep_prob_embedding_pl_list[0]] = 1.0\n\n                    # Predict class ids & update even files\n                    predicted_ids_train, summary_str_train = sess.run(\n                        [decode_op_infer, summary_train], feed_dict=feed_dict_train)\n                    predicted_ids_dev, summary_str_dev = sess.run(\n                        [decode_op_infer, summary_dev], feed_dict=feed_dict_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    # Convert to sparsetensor to compute LER\n                    feed_dict_ler_train = {\n                        model.labels_st_true_pl: list2sparsetensor(\n                            labels_train[0], padded_value=train_data.padded_value),\n                        model.labels_st_pred_pl: list2sparsetensor(\n                            predicted_ids_train, padded_value=train_data.padded_value)\n                    }\n                    feed_dict_ler_dev = {\n                        model.labels_st_true_pl: list2sparsetensor(\n                            labels_dev[0], padded_value=dev_data.padded_value),\n                        model.labels_st_pred_pl: list2sparsetensor(\n                            predicted_ids_dev, padded_value=dev_data.padded_value)\n                    }\n\n                    # Compute accuracy\n                    ler_train = sess.run(ler_op, feed_dict=feed_dict_ler_train)\n                    ler_dev = sess.run(ler_op, feed_dict=feed_dict_ler_dev)\n                    csv_ler_train.append(ler_train)\n                    csv_ler_dev.append(ler_dev)\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / ler = %.3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, ler_train, ler_dev,\n                           learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_ler_train, csv_ler_dev, csv_steps,\n                             label_type=params[\'label_type\'],\n                             save_path=model.save_path)\n\n                    # if train_data.epoch >= params[\'eval_start_epoch\']:\n                    if train_data.epoch >= 5:\n                        start_time_eval = time.time()\n                        if \'char\' in params[\'label_type\']:\n                            print(\'=== Dev Data Evaluation ===\')\n                            ler_dev_epoch, wer_dev_epoch = do_eval_cer(\n                                session=sess,\n                                decode_op=decode_op_infer,\n                                model=model,\n                                dataset=dev_data,\n                                label_type=params[\'label_type\'],\n                                eval_batch_size=1,\n                                is_jointctcatt=True)\n                            print(\'  CER: %f %%\' % (ler_dev_epoch * 100))\n                            print(\'  WER: %f %%\' % (wer_dev_epoch * 100))\n\n                            if ler_dev_epoch < ler_dev_best:\n                                ler_dev_best = ler_dev_epoch\n                                print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                                # Save model only when best accuracy is\n                                # obtained (check point)\n                                checkpoint_file = join(\n                                    model.save_path, \'model.ckpt\')\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=train_data.epoch)\n                                print(""Model saved in file: %s"" % save_path)\n\n                                print(\'=== Test Data Evaluation ===\')\n                                ler_test, wer_test = do_eval_cer(\n                                    session=sess,\n                                    decode_op=decode_op_infer,\n                                    model=model,\n                                    dataset=test_data,\n                                    label_type=params[\'label_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1,\n                                    is_jointctcatt=True)\n                                print(\'  CER: %f %%\' % (ler_test * 100))\n                                print(\'  WER: %f %%\' % (wer_test * 100))\n\n                        else:\n                            print(\'=== Dev Data Evaluation ===\')\n                            ler_dev_epoch = do_eval_per(\n                                session=sess,\n                                decode_op=decode_op_infer,\n                                per_op=ler_op,\n                                model=model,\n                                dataset=dev_data,\n                                label_type=params[\'label_type\'],\n                                eval_batch_size=1,\n                                is_jointctcatt=True)\n                            print(\'  PER: %f %%\' % (ler_dev_epoch * 100))\n\n                            if ler_dev_epoch < ler_dev_best:\n                                ler_dev_best = ler_dev_epoch\n                                print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (PER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                                # Save model only when best accuracy is\n                                # obtained (check point)\n                                checkpoint_file = join(\n                                    model.save_path, \'model.ckpt\')\n                                save_path = saver.save(\n                                    sess, checkpoint_file, global_step=train_data.epoch)\n                                print(""Model saved in file: %s"" % save_path)\n\n                                print(\'=== Test Data Evaluation ===\')\n                                ler_test = do_eval_per(\n                                    session=sess,\n                                    decode_op=decode_op_infer,\n                                    per_op=ler_op,\n                                    model=model,\n                                    dataset=test_data,\n                                    label_type=params[\'label_type\'],\n                                    is_test=True,\n                                    eval_batch_size=1,\n                                    is_jointctcatt=True)\n                                print(\'  PER: %f %%\' % (ler_test * 100))\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=ler_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n    else:\n        raise TypeError\n\n    # Model setting\n    model = JointCTCAttention(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        lambda_weight=params[\'lambda_weight\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'])\n\n    # Set process name\n    setproctitle(\'tf_timit_\' + model.name + \'_\' +\n                 params[\'label_type\'] + \'_\' + params[\'attention_type\'])\n\n    model.name += \'_en\' + str(params[\'encoder_num_units\'])\n    model.name += \'_\' + str(params[\'encoder_num_layers\'])\n    model.name += \'_att\' + str(params[\'attention_dim\'])\n    model.name += \'_de\' + str(params[\'decoder_num_units\'])\n    model.name += \'_\' + str(params[\'decoder_num_layers\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    model.name += \'_\' + params[\'attention_type\']\n    if params[\'dropout_encoder\'] != 0:\n        model.name += \'_dropen\' + str(params[\'dropout_encoder\'])\n    if params[\'dropout_decoder\'] != 0:\n        model.name += \'_dropde\' + str(params[\'dropout_decoder\'])\n    if params[\'dropout_embedding\'] != 0:\n        model.name += \'_dropem\' + str(params[\'dropout_embedding\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'wd\' + str(params[\'weight_decay\'])\n    if params[\'sharpening_factor\'] != 1:\n        model.name += \'_sharp\' + str(params[\'sharpening_factor\'])\n    if params[\'logits_temperature\'] != 1:\n        model.name += \'_temp\' + str(params[\'logits_temperature\'])\n    model.name += \'_lambda\' + str(params[\'lambda_weight\'])\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'joint_ctc_attention\', params[\'label_type\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError(\'Length of args should be 3.\')\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/timit/training/train_multitask_ctc.py,9,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Train the multi-task CTC model (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, isfile, abspath\nimport sys\nimport time\nimport tensorflow as tf\nfrom setproctitle import setproctitle\nimport yaml\nimport shutil\n\nsys.path.append(abspath(\'../../../\'))\nfrom examples.timit.data.load_dataset_multitask_ctc import Dataset\nfrom examples.timit.metrics.ctc import do_eval_per, do_eval_cer\nfrom utils.io.labels.sparsetensor import list2sparsetensor\nfrom utils.training.learning_rate_controller import Controller\nfrom utils.training.plot import plot_loss, plot_ler\nfrom utils.directory import mkdir_join, mkdir\nfrom utils.parameter import count_total_parameters\nfrom models.ctc.multitask_ctc import MultitaskCTC\n\n\ndef do_train(model, params):\n    """"""Run multi-task CTC training. The target labels in the main task is\n    characters and those in the sub task is 61 phones. The model is\n    evaluated by CER and PER with 39 phones.\n    Args:\n        model: the model to train\n        params (dict): A dictionary of parameters\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'], max_epoch=params[\'num_epoch\'],\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True, sort_stop_epoch=params[\'sort_stop_epoch\'])\n    dev_data = Dataset(\n        data_type=\'dev\', label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=params[\'batch_size\'], splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n    test_data = Dataset(\n        data_type=\'test\', label_type_main=params[\'label_type_main\'],\n        label_type_sub=\'phone39\', batch_size=1, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False)\n\n    # Tell TensorFlow that the model will be built into the default graph\n    with tf.Graph().as_default():\n\n        # Define placeholders\n        model.create_placeholders()\n        learning_rate_pl = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        # Add to the graph each operation\n        loss_op, logits_main, logits_sub = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.labels_sub_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_pl_list[0])\n        train_op = model.train(\n            loss_op,\n            optimizer=params[\'optimizer\'],\n            learning_rate=learning_rate_pl)\n        decode_op_character, decode_op_phone = model.decoder(\n            logits_main, logits_sub, model.inputs_seq_len_pl_list[0],\n            beam_width=params[\'beam_width\'])\n        cer_op, per_op = model.compute_ler(\n            decode_op_character, decode_op_phone,\n            model.labels_pl_list[0], model.labels_sub_pl_list[0])\n\n        # Define learning rate controller\n        lr_controller = Controller(\n            learning_rate_init=params[\'learning_rate\'],\n            decay_start_epoch=params[\'decay_start_epoch\'],\n            decay_rate=params[\'decay_rate\'],\n            decay_patient_epoch=params[\'decay_patient_epoch\'],\n            lower_better=True)\n\n        # Build the summary tensor based on the TensorFlow collection of\n        # summaries\n        summary_train = tf.summary.merge(model.summaries_train)\n        summary_dev = tf.summary.merge(model.summaries_dev)\n\n        # Add the variable initializer operation\n        init_op = tf.global_variables_initializer()\n\n        # Create a saver for writing training checkpoints\n        saver = tf.train.Saver(max_to_keep=None)\n\n        # Count total parameters\n        parameters_dict, total_parameters = count_total_parameters(\n            tf.trainable_variables())\n        for parameter_name in sorted(parameters_dict.keys()):\n            print(""%s %d"" % (parameter_name, parameters_dict[parameter_name]))\n        print(""Total %d variables, %s M parameters"" %\n              (len(parameters_dict.keys()),\n               ""{:,}"".format(total_parameters / 1000000)))\n\n        csv_steps, csv_loss_train, csv_loss_dev = [], [], []\n        csv_cer_train, csv_cer_dev = [], []\n        csv_per_train, csv_per_dev = [], []\n        # Create a session for running operation on the graph\n        with tf.Session() as sess:\n\n            # Instantiate a SummaryWriter to output summaries and the graph\n            summary_writer = tf.summary.FileWriter(\n                model.save_path, sess.graph)\n\n            # Initialize parameters\n            sess.run(init_op)\n\n            # Train model\n            start_time_train = time.time()\n            start_time_epoch = time.time()\n            start_time_step = time.time()\n            cer_dev_best = 1\n            learning_rate = float(params[\'learning_rate\'])\n            for step, (data, is_new_epoch) in enumerate(train_data):\n\n                # Create feed dictionary for next mini batch (train)\n                inputs, labels_char, labels_phone, inputs_seq_len, _ = data\n                feed_dict_train = {\n                    model.inputs_pl_list[0]: inputs[0],\n                    model.labels_pl_list[0]: list2sparsetensor(\n                        labels_char[0], padded_value=train_data.padded_value),\n                    model.labels_sub_pl_list[0]: list2sparsetensor(\n                        labels_phone[0], padded_value=train_data.padded_value),\n                    model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                    model.keep_prob_pl_list[0]: 1 - float(params[\'dropout\']),\n                    learning_rate_pl: learning_rate\n                }\n\n                # Update parameters\n                sess.run(train_op, feed_dict=feed_dict_train)\n\n                if (step + 1) % params[\'print_step\'] == 0:\n\n                    # Create feed dictionary for next mini batch (dev)\n                    (inputs, labels_char, labels_phone,\n                     inputs_seq_len, _), _ = dev_data.next()\n                    feed_dict_dev = {\n                        model.inputs_pl_list[0]: inputs[0],\n                        model.labels_pl_list[0]: list2sparsetensor(\n                            labels_char[0], padded_value=dev_data.padded_value),\n                        model.labels_sub_pl_list[0]: list2sparsetensor(\n                            labels_phone[0], padded_value=dev_data.padded_value),\n                        model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                        model.keep_prob_pl_list[0]: 1.0\n                    }\n\n                    # Compute loss\n                    loss_train = sess.run(loss_op, feed_dict=feed_dict_train)\n                    loss_dev = sess.run(loss_op, feed_dict=feed_dict_dev)\n                    csv_steps.append(step)\n                    csv_loss_train.append(loss_train)\n                    csv_loss_dev.append(loss_dev)\n\n                    # Change to evaluation mode\n                    feed_dict_train[model.keep_prob_pl_list[0]] = 1.0\n\n                    # Compute accuracy & update event files\n                    cer_train, per_train, summary_str_train = sess.run(\n                        [cer_op, per_op, summary_train],\n                        feed_dict=feed_dict_train)\n                    cer_dev, per_dev, summary_str_dev = sess.run(\n                        [cer_op, per_op,  summary_dev],\n                        feed_dict=feed_dict_dev)\n                    csv_cer_train.append(cer_train)\n                    csv_cer_dev.append(cer_dev)\n                    csv_per_train.append(per_train)\n                    csv_per_dev.append(per_dev)\n                    summary_writer.add_summary(summary_str_train, step + 1)\n                    summary_writer.add_summary(summary_str_dev, step + 1)\n                    summary_writer.flush()\n\n                    duration_step = time.time() - start_time_step\n                    print(""Step %d (epoch: %.3f): loss = %.3f (%.3f) / cer = %.3f (%.3f) / per = % .3f (%.3f) / lr = %.5f (%.3f min)"" %\n                          (step + 1, train_data.epoch_detail, loss_train, loss_dev, cer_train, cer_dev,\n                           per_train, per_dev, learning_rate, duration_step / 60))\n                    sys.stdout.flush()\n                    start_time_step = time.time()\n\n                # Save checkpoint and evaluate model per epoch\n                if is_new_epoch:\n                    duration_epoch = time.time() - start_time_epoch\n                    print(\'-----EPOCH:%d (%.3f min)-----\' %\n                          (train_data.epoch, duration_epoch / 60))\n\n                    # Save fugure of loss & ler\n                    plot_loss(csv_loss_train, csv_loss_dev, csv_steps,\n                              save_path=model.save_path)\n                    plot_ler(csv_cer_train, csv_cer_dev, csv_steps,\n                             label_type=params[\'label_type_main\'],\n                             save_path=model.save_path)\n                    plot_ler(csv_per_train, csv_per_dev, csv_steps,\n                             label_type=params[\'label_type_sub\'],\n                             save_path=model.save_path)\n\n                    if train_data.epoch >= params[\'eval_start_epoch\']:\n                        start_time_eval = time.time()\n                        print(\'=== Dev Data Evaluation ===\')\n                        cer_dev_epoch, wer_dev_epoch = do_eval_cer(\n                            session=sess,\n                            decode_op=decode_op_character,\n                            model=model,\n                            dataset=dev_data,\n                            label_type=params[\'label_type_main\'],\n                            eval_batch_size=1,\n                            is_multitask=True)\n                        print(\'  WER: %f %%\' % (wer_dev_epoch * 100))\n                        print(\'  CER: %f %%\' % (cer_dev_epoch * 100))\n                        per_dev_epoch = do_eval_per(\n                            session=sess,\n                            decode_op=decode_op_phone,\n                            per_op=per_op,\n                            model=model,\n                            dataset=dev_data,\n                            label_type=params[\'label_type_sub\'],\n                            eval_batch_size=1,\n                            is_multitask=True)\n                        print(\'  PER: %f %%\' % (per_dev_epoch * 100))\n\n                        if cer_dev_epoch < cer_dev_best:\n                            cer_dev_best = cer_dev_epoch\n                            print(\'\xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0 \xe2\x86\x91Best Score (CER)\xe2\x86\x91 \xe2\x96\xa0\xe2\x96\xa0\xe2\x96\xa0\')\n\n                            # Save model only when best accuracy is obtained\n                            # (check point)\n                            checkpoint_file = join(\n                                model.save_path, \'model.ckpt\')\n                            save_path = saver.save(\n                                sess, checkpoint_file, global_step=train_data.epoch)\n                            print(""Model saved in file: %s"" % save_path)\n\n                            print(\'=== Test Data Evaluation ===\')\n                            cer_test, wer_test = do_eval_cer(\n                                session=sess,\n                                decode_op=decode_op_character,\n                                model=model,\n                                dataset=test_data,\n                                label_type=params[\'label_type_main\'],\n                                is_test=True,\n                                eval_batch_size=1,\n                                is_multitask=True)\n                            print(\'  CER: %f %%\' % (cer_test * 100))\n                            print(\'  WER: %f %%\' % (wer_test * 100))\n                            per_test = do_eval_per(\n                                session=sess,\n                                decode_op=decode_op_phone,\n                                per_op=per_op,\n                                model=model,\n                                dataset=test_data,\n                                label_type=params[\'label_type_sub\'],\n                                is_test=True,\n                                eval_batch_size=1,\n                                is_multitask=True)\n                            print(\'  PER: %f %%\' % (per_test * 100))\n\n                        duration_eval = time.time() - start_time_eval\n                        print(\'Evaluation time: %.3f min\' %\n                              (duration_eval / 60))\n\n                        # Update learning rate\n                        learning_rate = lr_controller.decay_lr(\n                            learning_rate=learning_rate,\n                            epoch=train_data.epoch,\n                            value=cer_dev_epoch)\n\n                    start_time_epoch = time.time()\n\n            duration_train = time.time() - start_time_train\n            print(\'Total time: %.3f hour\' % (duration_train / 3600))\n\n            # Training was finished correctly\n            with open(join(model.save_path, \'complete.txt\'), \'w\') as f:\n                f.write(\'\')\n\n\ndef main(config_path, model_save_path):\n\n    # Load a config file (.yml)\n    with open(config_path, ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type_main\'] == \'character\':\n        params[\'num_classes_main\'] = 28\n    elif params[\'label_type_main\'] == \'character_capital_divide\':\n        params[\'num_classes_main\'] = 72\n    else:\n        raise TypeError\n    if params[\'label_type_sub\'] == \'phone61\':\n        params[\'num_classes_sub\'] = 61\n    elif params[\'label_type_sub\'] == \'phone48\':\n        params[\'num_classes_sub\'] = 48\n    elif params[\'label_type_sub\'] == \'phone39\':\n        params[\'num_classes_sub\'] = 39\n    else:\n        raise TypeError\n\n    # Model setting\n    model = MultitaskCTC(encoder_type=params[\'encoder_type\'],\n                         input_size=params[\'input_size\'],\n                         splice=params[\'splice\'],\n                         num_stack=params[\'num_stack\'],\n                         num_units=params[\'num_units\'],\n                         num_layers_main=params[\'num_layers_main\'],\n                         num_layers_sub=params[\'num_layers_sub\'],\n                         num_classes_main=params[\'num_classes_main\'],\n                         num_classes_sub=params[\'num_classes_sub\'],\n                         main_task_weight=params[\'main_task_weight\'],\n                         lstm_impl=params[\'lstm_impl\'],\n                         use_peephole=params[\'use_peephole\'],\n                         parameter_init=params[\'weight_init\'],\n                         clip_grad_norm=params[\'clip_grad_norm\'],\n                         clip_activation=params[\'clip_activation\'],\n                         num_proj=params[\'num_proj\'],\n                         weight_decay=params[\'weight_decay\'])\n\n    # Set process name\n    setproctitle(\'tf_timit_\' + model.name + \'_\' +\n                 params[\'label_type_main\'] + \'_\' + params[\'label_type_sub\'])\n\n    model.name += \'_\' + str(params[\'num_units\'])\n    model.name += \'_main\' + str(params[\'num_layers_main\'])\n    model.name += \'_sub\' + str(params[\'num_layers_sub\'])\n    model.name += \'_\' + params[\'optimizer\']\n    model.name += \'_lr\' + str(params[\'learning_rate\'])\n    if params[\'num_proj\'] != 0:\n        model.name += \'_proj\' + str(params[\'num_proj\'])\n    if params[\'dropout\'] != 0:\n        model.name += \'_drop\' + str(params[\'dropout\'])\n    if params[\'num_stack\'] != 1:\n        model.name += \'_stack\' + str(params[\'num_stack\'])\n    if params[\'weight_decay\'] != 0:\n        model.name += \'_wd\' + str(params[\'weight_decay\'])\n    model.name += \'_main\' + str(params[\'main_task_weight\'])\n\n    # Set save path\n    model.save_path = mkdir_join(\n        model_save_path, \'ctc\', \'char_\' + params[\'label_type_sub\'], model.name)\n\n    # Reset model directory\n    model_index = 0\n    new_model_path = model.save_path\n    while True:\n        if isfile(join(new_model_path, \'complete.txt\')):\n            # Training of the first model have been finished\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        elif isfile(join(new_model_path, \'config.yml\')):\n            # Training of the first model have not been finished yet\n            model_index += 1\n            new_model_path = model.save_path + \'_\' + str(model_index)\n        else:\n            break\n    model.save_path = mkdir(new_model_path)\n\n    # Save config file\n    shutil.copyfile(config_path, join(model.save_path, \'config.yml\'))\n\n    sys.stdout = open(join(model.save_path, \'train.log\'), \'w\')\n    # TODO(hirofumi): change to logger\n    do_train(model=model, params=params)\n\n\nif __name__ == \'__main__\':\n\n    args = sys.argv\n    if len(args) != 3:\n        raise ValueError(\'Length of args should be 3.\')\n    main(config_path=args[1], model_save_path=args[2])\n'"
examples/timit/visualization/__init__.py,0,b''
examples/timit/visualization/decode_attention.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decode the trained Attention outputs (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.timit.data.load_dataset_attention import Dataset\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.evaluation.edit_distance import wer_align\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_decode(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the Attention outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    map_file_path = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'.txt\'\n\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        batch_size=eval_batch_size, map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, _, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n        model.inputs_pl_list[0],\n        model.labels_pl_list[0],\n        model.inputs_seq_len_pl_list[0],\n        model.labels_seq_len_pl_list[0],\n        model.keep_prob_encoder_pl_list[0],\n        model.keep_prob_decoder_pl_list[0],\n        model.keep_prob_embedding_pl_list[0])\n    _, decode_op_infer = model.decode(\n        decoder_outputs_train,\n        decoder_outputs_infer)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        decode(session=sess,\n               decode_op=decode_op_infer,\n               model=model,\n               dataset=test_data,\n               label_type=params[\'label_type\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n\ndef decode(session, decode_op, model, dataset, label_type,\n           is_test=False, save_path=None):\n    """"""Visualize label outputs of Attention-based model.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): phone39 or phone48 or phone61 or character or\n            character_capital_divide\n        is_test (bool, optional):\n        save_path (string): path to save decoding results\n    """"""\n    if label_type == \'character\':\n        map_fn = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n    elif label_type == \'character_capital_divide\':\n        map_fn = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character_capital_divide.txt\',\n            capital_divide=True)\n    else:\n        map_fn = Idx2phone(\n            map_file_path=\'../metrics/mapping_files/\' + label_type + \'.txt\')\n\n    if save_path is not None:\n        sys.stdout = open(join(model.model_dir, \'decode.txt\'), \'w\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, labels_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n        labels_pred = session.run(decode_op, feed_dict=feed_dict)\n        for i_batch in range(batch_size):\n            print(\'----- wav: %s -----\' % input_names[0][i_batch])\n            if is_test:\n                str_true = labels_true[0][i_batch][0]\n            else:\n                str_true = map_fn(\n                    labels_true[0][i_batch][1:labels_seq_len[0][i_batch] - 1])\n                # NOTE: Exclude <SOS> and <EOS>\n            str_pred = map_fn(labels_pred[i_batch]).split(\'>\')[0]\n            # NOTE: Trancate by <EOS>\n\n            if \'phone\' in label_type:\n                # Remove the last space\n                if str_pred[-1] == \' \':\n                    str_pred = str_pred[:-1]\n\n            print(\'Ref: %s\' % str_true)\n            print(\'Hyp: %s\' % str_pred)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n    else:\n        TypeError\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'])\n\n    model.save_path = args.model_path\n    do_decode(model=model, params=params,\n              epoch=args.epoch, beam_width=1,\n              eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/visualization/decode_ctc.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decode the trained CTC outputs (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.timit.data.load_dataset_ctc import Dataset\nfrom models.ctc.ctc import CTC\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.io.labels.sparsetensor import sparsetensor2list\nfrom utils.evaluation.edit_distance import wer_align\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_decode(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                   model.labels_pl_list[0],\n                                   model.inputs_seq_len_pl_list[0],\n                                   model.keep_prob_pl_list[0])\n    decode_op = model.decoder(\n        logits,\n        model.inputs_seq_len_pl_list[0],\n        beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        decode(session=sess,\n               decode_op=decode_op,\n               model=model,\n               dataset=test_data,\n               label_type=params[\'label_type\'],\n               is_test=True,\n               save_path=None)\n        # save_path=model.save_path)\n\n\ndef decode(session, decode_op, model, dataset, label_type,\n           is_test=True, save_path=None):\n    """"""Visualize label outputs of CTC model.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): phone39 or phone48 or phone61 or character or\n            character_capital_divide\n        is_test (bool, optional):\n        save_path (string, optional): path to save decoding results\n    """"""\n    if label_type == \'character\':\n        map_fn = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n    elif label_type == \'character_capital_divide\':\n        map_fn = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character_capital_divide.txt\',\n            capital_divide=True)\n    else:\n        map_fn = Idx2phone(\n            map_file_path=\'../metrics/mapping_files/\' + label_type + \'.txt\')\n\n    if save_path is not None:\n        sys.stdout = open(join(model.model_dir, \'decode.txt\'), \'w\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n        labels_pred_st = session.run(decode_op, feed_dict=feed_dict)\n        try:\n            labels_pred = sparsetensor2list(\n                labels_pred_st, batch_size=batch_size)\n        except IndexError:\n            # no output\n            labels_pred = [\'\']\n\n        for i_batch in range(batch_size):\n            print(\'----- wav: %s -----\' % input_names[0][i_batch])\n            if \'char\' in label_type:\n                if is_test:\n                    str_true = labels_true[0][i_batch][0]\n                else:\n                    str_true = map_fn(labels_true[0][i_batch])\n                str_pred = map_fn(labels_pred[i_batch])\n            else:\n                if is_test:\n                    str_true = labels_true[0][i_batch][0]\n                else:\n                    str_true = map_fn(labels_true[0][i_batch])\n                str_pred = map_fn(labels_pred[i_batch])\n\n            print(\'Ref: %s\' % str_true)\n            print(\'Hyp: %s\' % str_pred)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_decode(model=model, params=params,\n              epoch=args.epoch, beam_width=args.beam_width,\n              eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/visualization/decode_multitask_ctc.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Decode the trained multi-task CTC outputs (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath\nimport sys\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.timit.data.load_dataset_multitask_ctc import Dataset\nfrom models.ctc.multitask_ctc import MultitaskCTC\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.io.labels.sparsetensor import sparsetensor2list\nfrom utils.evaluation.edit_distance import wer_align\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--beam_width\', type=int, default=20,\n                    help=\'beam_width (int, optional): beam width for beam search.\' +\n                    \' 1 disables beam search, which mean greedy decoding.\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_decode(model, params, epoch, beam_width, eval_batch_size):\n    """"""Decode the Multi-task CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        beam_width (int): beam width for beam search.\n            1 disables beam search, which mean greedy decoding.\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, logits_main, logits_sub = model.compute_loss(\n        model.inputs_pl_list[0],\n        model.labels_pl_list[0],\n        model.labels_sub_pl_list[0],\n        model.inputs_seq_len_pl_list[0],\n        model.keep_prob_pl_list[0])\n    decode_op_main, decode_op_sub = model.decoder(\n        logits_main, logits_sub,\n        model.inputs_seq_len_pl_list[0],\n        beam_width=beam_width)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        decode(session=sess,\n               decode_op_main=decode_op_main,\n               decode_op_sub=decode_op_sub,\n               model=model,\n               dataset=test_data,\n               label_type_main=params[\'label_type_main\'],\n               label_type_sub=params[\'label_type_sub\'],\n               is_test=True,\n               save_path=None)\n        #   save_path=model.save_path)\n\n\ndef decode(session, decode_op_main, decode_op_sub, model,\n           dataset, label_type_main, label_type_sub,\n           is_test=True, save_path=None):\n    """"""Visualize label outputs of Multi-task CTC model.\n    Args:\n        session: session of training model\n        decode_op_main: operation for decoding in the main task\n        decode_op_sub: operation for decoding in the sub task\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type_main (string): character or character_capital_divide\n        label_type_sub (string): phone39 or phone48 or phone61\n        is_test (bool, optional):\n        save_path (string, optional): path to save decoding results\n    """"""\n    idx2char = Idx2char(\n        map_file_path=\'../metrics/mapping_files/\' + label_type_main + \'.txt\')\n    idx2phone = Idx2phone(\n        map_file_path=\'../metrics/mapping_files/\' + label_type_sub + \'.txt\')\n\n    if save_path is not None:\n        sys.stdout = open(join(model.model_dir, \'decode.txt\'), \'w\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true_char, labels_true_phone, inputs_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size = inputs[0].shape[0]\n        labels_pred_char_st, labels_pred_phone_st = session.run(\n            [decode_op_main, decode_op_sub], feed_dict=feed_dict)\n        try:\n            labels_pred_char = sparsetensor2list(\n                labels_pred_char_st, batch_size=batch_size)\n        except:\n            # no output\n            labels_pred_char = [\'\']\n        try:\n            labels_pred_phone = sparsetensor2list(\n                labels_pred_char_st, batch_size=batch_size)\n        except:\n            # no output\n            labels_pred_phone = [\'\']\n\n        for i_batch in range(batch_size):\n            print(\'----- wav: %s -----\' % input_names[0][i_batch])\n\n            if is_test:\n                str_true_char = labels_true_char[0][i_batch][0].replace(\n                    \'_\', \' \')\n                str_true_phone = labels_true_phone[0][i_batch][0]\n            else:\n                str_true_char = idx2char(labels_true_char[0][i_batch])\n                str_true_phone = idx2phone(labels_true_phone[0][i_batch])\n\n            str_pred_char = idx2char(labels_pred_char[i_batch])\n            str_pred_phone = idx2phone(labels_pred_phone[i_batch])\n\n            print(\'Ref (char): %s\' % str_true_char)\n            print(\'Hyp (char): %s\' % str_pred_char)\n            print(\'Ref (phone): %s\' % str_true_phone)\n            print(\'Hyp (phone): %s\' % str_pred_phone)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type_main\'] == \'character\':\n        params[\'num_classes_main\'] = 28\n    elif params[\'label_type_main\'] == \'character_capital_divide\':\n        params[\'num_classes_main\'] = 72\n    if params[\'label_type_sub\'] == \'phone61\':\n        params[\'num_classes_sub\'] = 61\n    elif params[\'label_type_sub\'] == \'phone48\':\n        params[\'num_classes_sub\'] = 48\n    elif params[\'label_type_sub\'] == \'phone39\':\n        params[\'num_classes_sub\'] = 39\n\n    # Model setting\n    model = MultitaskCTC(\n        encoder_type=params[\'encoder_type\'],\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        num_units=params[\'num_units\'],\n        num_layers_main=params[\'num_layers_main\'],\n        num_layers_sub=params[\'num_layers_sub\'],\n        num_classes_main=params[\'num_classes_main\'],\n        num_classes_sub=params[\'num_classes_sub\'],\n        main_task_weight=params[\'main_task_weight\'],\n        lstm_impl=params[\'lstm_impl\'],\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation=params[\'clip_activation\'],\n        num_proj=params[\'num_proj\'],\n        weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_decode(model=model, params=params,\n              epoch=args.epoch, beam_width=args.beam_width,\n              eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/visualization/plot_attention_weights.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Plot attention weights (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath, isdir\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport yaml\nimport argparse\nimport shutil\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nplt.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set_style(""white"")\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.timit.data.load_dataset_attention import Dataset\nfrom models.attention.attention_seq2seq import AttentionSeq2Seq\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.directory import mkdir_join, mkdir\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_plot(model, params, epoch, eval_batch_size):\n    """"""Decode the Attention outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        eval_batch_size (int): the size of mini-batch when evaluation\n    """"""\n    map_file_path = \'../metrics/mapping_files/\' + \\\n        params[\'label_type\'] + \'.txt\'\n\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        batch_size=eval_batch_size, map_file_path=map_file_path,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, _, decoder_outputs_train, decoder_outputs_infer = model.compute_loss(\n        model.inputs_pl_list[0],\n        model.labels_pl_list[0],\n        model.inputs_seq_len_pl_list[0],\n        model.labels_seq_len_pl_list[0],\n        model.keep_prob_encoder_pl_list[0],\n        model.keep_prob_decoder_pl_list[0],\n        model.keep_prob_embedding_pl_list[0])\n    _, decode_op_infer = model.decode(\n        decoder_outputs_train,\n        decoder_outputs_infer)\n    attention_weights_op = decoder_outputs_infer.attention_weights\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        plot(session=sess,\n             decode_op=decode_op_infer,\n             attention_weights_op=attention_weights_op,\n             model=model,\n             dataset=test_data,\n             label_type=params[\'label_type\'],\n             is_test=True,\n             save_path=mkdir_join(model.save_path, \'attention_weights\'),\n             #  save_path=None,\n             show=False)\n\n\ndef plot(session, decode_op, attention_weights_op, model, dataset,\n         label_type, is_test=False, save_path=None, show=False):\n    """"""Visualize attention weights of Attetnion-based model.\n    Args:\n        session: session of training model\n        decode_op: operation for decoding\n        attention_weights_op: operation for computing attention weights\n        model: model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string, optional): phone39 or phone48 or phone61 or character or\n            character_capital_divide\n        is_test (bool, optional):\n        save_path (string, optional): path to save attention weights plotting\n        show (bool, optional): if True, show each figure\n    """"""\n    # Clean directory\n    if save_path is not None and isdir(save_path):\n        shutil.rmtree(save_path)\n        mkdir(save_path)\n\n    if label_type == \'character\':\n        map_fn = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character.txt\')\n    elif label_type == \'character_capital_divide\':\n        map_fn = Idx2char(\n            map_file_path=\'../metrics/mapping_files/character_capital_divide.txt\',\n            capital_divide=True)\n    else:\n        map_fn = Idx2phone(\n            map_file_path=\'../metrics/mapping_files/\' + label_type + \'.txt\')\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, labels_true, inputs_seq_len, _, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_encoder_pl_list[0]: 1.0,\n            model.keep_prob_decoder_pl_list[0]: 1.0,\n            model.keep_prob_embedding_pl_list[0]: 1.0\n        }\n\n        # Visualize\n        batch_size, max_frame_num = inputs.shape[:2]\n        attention_weights, labels_pred = session.run(\n            [attention_weights_op, decode_op], feed_dict=feed_dict)\n\n        for i_batch in range(batch_size):\n\n            # t_out, t_in = attention_weights[i_batch].shape\n\n            # Check if the sum of attention weights equals to 1\n            # print(np.sum(attention_weights[i_batch], axis=1))\n\n            # Convert from index to label\n            str_pred = map_fn(labels_pred[i_batch])\n            if \'phone\' in label_type:\n                label_list = str_pred.split(\' \')\n            else:\n                raise NotImplementedError\n\n            plt.clf()\n            plt.figure(figsize=(10, 4))\n            sns.heatmap(attention_weights[i_batch],\n                        cmap=\'Blues\',\n                        xticklabels=False,\n                        yticklabels=label_list)\n\n            plt.xlabel(\'Input frames\', fontsize=12)\n            plt.ylabel(\'Output labels (top to bottom)\', fontsize=12)\n\n            if show:\n                plt.show()\n\n            # Save as a png file\n            if save_path is not None:\n                plt.savefig(join(save_path, input_names[0] + \'.png\'), dvi=500)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a <SOS> and <EOS> class\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n    else:\n        raise TypeError\n\n    # Model setting\n    model = AttentionSeq2Seq(\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        encoder_type=params[\'encoder_type\'],\n        encoder_num_units=params[\'encoder_num_units\'],\n        encoder_num_layers=params[\'encoder_num_layers\'],\n        encoder_num_proj=params[\'encoder_num_proj\'],\n        attention_type=params[\'attention_type\'],\n        attention_dim=params[\'attention_dim\'],\n        decoder_type=params[\'decoder_type\'],\n        decoder_num_units=params[\'decoder_num_units\'],\n        decoder_num_layers=params[\'decoder_num_layers\'],\n        embedding_dim=params[\'embedding_dim\'],\n        num_classes=params[\'num_classes\'],\n        sos_index=params[\'num_classes\'],\n        eos_index=params[\'num_classes\'] + 1,\n        max_decode_length=params[\'max_decode_length\'],\n        lstm_impl=\'LSTMBlockCell\',\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation_encoder=params[\'clip_activation_encoder\'],\n        clip_activation_decoder=params[\'clip_activation_decoder\'],\n        weight_decay=params[\'weight_decay\'],\n        time_major=True,\n        sharpening_factor=params[\'sharpening_factor\'],\n        logits_temperature=params[\'logits_temperature\'])\n\n    model.save_path = args.model_path\n    do_plot(model=model, params=params,\n            epoch=args.epoch, eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/visualization/plot_ctc_prob.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Plot the trained CTC posteriors (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath, isdir\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport yaml\nimport argparse\nimport shutil\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nplt.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set_style(""white"")\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.timit.data.load_dataset_ctc import Dataset\nfrom models.ctc.ctc import CTC\nfrom utils.directory import mkdir_join, mkdir\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_plot(model, params, epoch, eval_batch_size):\n    """"""Plot the CTC posteriors.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): epoch to restore\n        eval_batch_size (int): the size of mini-batch in evaluation\n    """"""\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type=params[\'label_type\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, logits = model.compute_loss(model.inputs_pl_list[0],\n                                   model.labels_pl_list[0],\n                                   model.inputs_seq_len_pl_list[0],\n                                   model.keep_prob_pl_list[0])\n    posteriors_op = model.posteriors(logits, blank_prior=1)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        plot(session=sess,\n             posteriors_op=posteriors_op,\n             model=model,\n             dataset=test_data,\n             label_type=params[\'label_type\'],\n             num_stack=params[\'num_stack\'],\n             #    save_path=None)\n             save_path=mkdir_join(model.save_path, \'ctc_output\'))\n\n\ndef plot(session, posteriors_op, model, dataset, label_type,\n         num_stack=1, save_path=None, show=False):\n    """"""Visualize label posteriors of CTC model.\n    Args:\n        session: session of training model\n        posteriois_op: operation for computing posteriors\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): phone39 or phone48 or phone61 or character or\n            character_capital_divide\n        num_stack (int): the number of frames to stack\n        save_path (string, string): path to save ctc outputs\n        show (bool, optional): if True, show each figure\n    """"""\n    # Clean directory\n    if isdir(save_path):\n        shutil.rmtree(save_path)\n        mkdir(save_path)\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, _, inputs_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs,\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        # Visualize\n        batch_size, max_frame_num = inputs.shape[:2]\n        probs = session.run(posteriors_op, feed_dict=feed_dict)\n        probs = probs.reshape(-1, max_frame_num, model.num_classes)\n\n        # Visualize\n        for i_batch in range(batch_size):\n            prob = probs[i_batch][:int(inputs_seq_len[0]), :]\n\n            plt.clf()\n            plt.figure(figsize=(10, 4))\n            frame_num = int(inputs_seq_len[i_batch])\n            times_probs = np.arange(frame_num) * num_stack / 100\n\n            # NOTE: Blank class is set to the last class in TensorFlow\n            for i in range(0, prob.shape[-1] - 1, 1):\n                plt.plot(times_probs, prob[:, i])\n            plt.plot(times_probs, prob[:, -1],\n                     \':\', label=\'blank\', color=\'grey\')\n            plt.xlabel(\'Time [sec]\', fontsize=12)\n            plt.ylabel(\'Posteriors\', fontsize=12)\n            plt.xlim([0, frame_num * num_stack / 100])\n            plt.ylim([0.05, 1.05])\n            plt.xticks(list(range(0, int(frame_num * num_stack / 100) + 1, 1)))\n            plt.yticks(list(range(0, 2, 1)))\n            plt.legend(loc=""upper right"", fontsize=12)\n\n            if show:\n                plt.show()\n\n            # Save as a png file\n            if save_path is not None:\n                plt.savefig(join(save_path, input_names[0] + \'.png\'), dvi=500)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type\'] == \'phone61\':\n        params[\'num_classes\'] = 61\n    elif params[\'label_type\'] == \'phone48\':\n        params[\'num_classes\'] = 48\n    elif params[\'label_type\'] == \'phone39\':\n        params[\'num_classes\'] = 39\n    elif params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 72\n    else:\n        raise ValueError\n\n    # Model setting\n    model = CTC(encoder_type=params[\'encoder_type\'],\n                input_size=params[\'input_size\'],\n                splice=params[\'splice\'],\n                num_stack=params[\'num_stack\'],\n                num_units=params[\'num_units\'],\n                num_layers=params[\'num_layers\'],\n                num_classes=params[\'num_classes\'],\n                lstm_impl=params[\'lstm_impl\'],\n                use_peephole=params[\'use_peephole\'],\n                parameter_init=params[\'weight_init\'],\n                clip_grad_norm=params[\'clip_grad_norm\'],\n                clip_activation=params[\'clip_activation\'],\n                num_proj=params[\'num_proj\'],\n                weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_plot(model=model, params=params,\n            epoch=args.epoch, eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/visualization/plot_multitask_ctc_prob.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Plot the trained multi-task CTC posteriors (TIMIT corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join, abspath, isdir\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nplt.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set_style(""white"")\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\nsys.path.append(abspath(\'../../../\'))\nfrom experiments.timit.data.load_dataset_multitask_ctc import Dataset\nfrom experiments.timit.visualization.core.plot.ctc import plot\nfrom models.ctc.multitask_ctc import MultitaskCTC\nfrom utils.directory import mkdir_join\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_plot(model, params, epoch, eval_batch_size):\n    """"""Plot the multi-task CTC posteriors.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        eval_batch_size (int): the size of mini-batch in evaluation\n    """"""\n    # Load dataset\n    test_data = Dataset(\n        data_type=\'test\', label_type_main=params[\'label_type_main\'],\n        label_type_sub=params[\'label_type_sub\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=False, progressbar=True)\n\n    # Define placeholders\n    model.create_placeholders()\n\n    # Add to the graph each operation (including model definition)\n    _, logits_main, logits_sub = model.compute_loss(\n        model.inputs_pl_list[0],\n        model.labels_pl_list[0],\n        model.labels_sub_pl_list[0],\n        model.inputs_seq_len_pl_list[0],\n        model.keep_prob_pl_list[0])\n    posteriors_op_main, posteriors_op_sub = model.posteriors(\n        logits_main, logits_sub)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        plot(session=sess,\n             posteriors_op_main=posteriors_op_main,\n             posteriors_op_sub=posteriors_op_sub,\n             model=model,\n             dataset=test_data,\n             label_type_main=params[\'label_type_main\'],\n             label_type_sub=params[\'label_type_sub\'],\n             num_stack=params[\'num_stack\'],\n             save_path=mkdir_join(save_path, \'ctc_output\'),\n             show=False)\n\n\ndef plot(session, posteriors_op_main, posteriors_op_sub,\n         model, dataset, label_type_main, label_type_sub,\n         num_stack=1, save_path=None, show=False):\n    """"""Visualize label posteriors of the multi-task CTC model.\n    Args:\n        session: session of training model\n        posteriois_op_main: operation for computing posteriors in the main task\n        posteriois_op_sub: operation for computing posteriors in the sub task\n        model: model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type_main (string): character or character_capital_divide\n        label_type_sub (string): phone39 or phone48 or phone61\n        num_stack (int): the number of frames to stack\n        save_path (string): path to save ctc outpus\n        show (bool, optional): if True, show each figure\n    """"""\n    # Clean directory\n    if isdir(save_path):\n        shutil.rmtree(save_path)\n        mkdir(save_path)\n\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, _, _, inputs_seq_len, input_names = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs,\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len,\n            model.keep_prob_pl_list[0]: 1.0\n        }\n\n        batch_size, max_frame_num = inputs.shape[:2]\n        probs_char = session.run(posteriors_op_main, feed_dict=feed_dict)\n        probs_phone = session.run(posteriors_op_sub, feed_dict=feed_dict)\n        probs_char = probs_char.reshape(-1, max_frame_num, model.num_classes)\n        probs_phone = probs_phone.reshape(-1, max_frame_num, model.num_classes)\n\n        # Visualize\n        for i_batch in range(batch_size):\n            prob_char = probs_char[i_batch][:int(inputs_seq_len[i_batch]), :]\n            prob_phone = probs_phone[i_batch][:int(inputs_seq_len[i_batch]), :]\n\n            plt.clf()\n            plt.figure(figsize=(10, 4))\n            frame_num = int(inputs_seq_len[i_batch])\n            times_probs = np.arange(frame_num) * num_stack / 100\n\n            # NOTE: Blank class is set to the last class in TensorFlow\n            # Plot characters\n            plt.subplot(211)\n            for i in range(0, prob_char.shape[-1] - 1, 1):\n                plt.plot(times_probs, prob_char[:, i])\n            plt.plot(\n                times_probs, prob_char[:, -1], \':\', label=\'blank\', color=\'grey\')\n            plt.xlabel(\'Time [sec]\', fontsize=12)\n            plt.ylabel(\'Characters\', fontsize=12)\n            plt.xlim([0, frame_num * num_stack / 100])\n            plt.ylim([0.05, 1.05])\n            plt.xticks(list(range(0, int(frame_num * num_stack / 100) + 1, 1)))\n            plt.yticks(list(range(0, 2, 1)))\n            plt.legend(loc=""upper right"", fontsize=12)\n\n            # Plot phones\n            plt.subplot(212)\n            for i in range(0, prob_phone.shape[-1] - 1, 1):\n                plt.plot(times_probs, prob_phone[:, i])\n            plt.plot(\n                times_probs, prob_phone[:, -1], \':\', label=\'blank\', color=\'grey\')\n            plt.xlabel(\'Time [sec]\', fontsize=12)\n            plt.ylabel(\'Phones\', fontsize=12)\n            plt.xlim([0, frame_num])\n            plt.ylim([0.05, 1.05])\n            plt.xticks(list(range(0, int(frame_num * num_stack / 100) + 1, 1)))\n            plt.yticks(list(range(0, 2, 1)))\n            plt.legend(loc=""upper right"", fontsize=12)\n\n            if show:\n                plt.show()\n\n            # Save as a png file\n            if save_path is not None:\n                plt.savefig(\n                    join(save_path, input_names[i_batch] + \'.png\'), dvi=500)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(os.path.join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank label\n    if params[\'label_type_main\'] == \'character\':\n        params[\'num_classes_main\'] = 28\n    elif params[\'label_type_main\'] == \'character_capital_divide\':\n        params[\'num_classes_main\'] = 72\n\n    if params[\'label_type_sub\'] == \'phone61\':\n        params[\'num_classes_sub\'] = 61\n    elif params[\'label_type_sub\'] == \'phone48\':\n        params[\'num_classes_sub\'] = 48\n    elif params[\'label_type_sub\'] == \'phone39\':\n        params[\'num_classes_sub\'] = 39\n\n    # Model setting\n    model = MultitaskCTC(\n        encoder_type=params[\'encoder_type\'],\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        num_units=params[\'num_units\'],\n        num_layers_main=params[\'num_layers_main\'],\n        num_layers_sub=params[\'num_layers_sub\'],\n        num_classes_main=params[\'num_classes_main\'],\n        num_classes_sub=params[\'num_classes_sub\'],\n        main_task_weight=params[\'main_task_weight\'],\n        lstm_impl=params[\'lstm_impl\'],\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad_norm=params[\'clip_grad_norm\'],\n        clip_activation=params[\'clip_activation\'],\n        num_proj=params[\'num_proj\'],\n        weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_plot(model=model, params=params, epoch=args.epoch)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
experiments/librispeech/evaluation/save_ctc_prob.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Save the trained CTC posteriors (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport yaml\nimport argparse\n\nsys.path.append(os.path.abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom models.ctc.vanilla_ctc import CTC\nfrom utils.directory import mkdir_join\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_save(model, params, epoch, eval_batch_size):\n    """"""Save the CTC outputs.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        eval_batch_size (int): the size of mini-batch in evaluation\n    """"""\n    # Load dataset\n    train_data = Dataset(\n        data_type=\'train\', train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=eval_batch_size,\n        splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        sort_utt=True)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_input_pl_list[0],\n            model.keep_prob_hidden_pl_list[0],\n            model.keep_prob_output_pl_list[0],\n            softmax_temperature=params[\'softmax_temperature\'])\n        posteriors_op = model.posteriors(logits, blank_prior=1)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        for data, is_new_epoch in train_data:\n\n            # Create feed dictionary for next mini batch\n            inputs, _, inputs_seq_len, input_names = data\n            feed_dict = {\n                model.inputs_pl_list[0]: inputs[0],\n                model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n                model.keep_prob_input_pl_list[0]: 1.0,\n                model.keep_prob_hidden_pl_list[0]: 1.0,\n                model.keep_prob_output_pl_list[0]: 1.0\n            }\n\n            batch_size, max_frame_num = inputs[0].shape[:2]\n            posteriors = sess.run(posteriors_op, feed_dict=feed_dict)\n            posteriors = posteriors.reshape(-1,\n                                            max_frame_num, model.num_classes)\n\n            for i_batch in range(batch_size):\n                prob = posteriors[i_batch][:int(inputs_seq_len[0][i_batch]), :]\n\n                # Save as a npy file\n                np.save(mkdir_join(model.save_path, \'probs\',\n                                   input_names[0][i_batch]), prob)\n\n            if is_new_epoch:\n                break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(os.path.join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 77\n    elif params[\'label_type\'] == \'word\':\n        if params[\'train_data_size\'] == \'train_clean100\':\n            params[\'num_classes\'] = 7213\n        elif params[\'train_data_size\'] == \'train_clean360\':\n            params[\'num_classes\'] = 16287\n        elif params[\'train_data_size\'] == \'train_other500\':\n            params[\'num_classes\'] = 18669\n        elif params[\'train_data_size\'] == \'train_all\':\n            params[\'num_classes\'] = 26642\n\n    # Model setting\n    model = CTC(\n        encoder_type=params[\'encoder_type\'],\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        num_units=params[\'num_units\'],\n        num_layers=params[\'num_layers\'],\n        num_classes=params[\'num_classes\'],\n        lstm_impl=params[\'lstm_impl\'],\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad=params[\'clip_grad\'],\n        clip_activation=params[\'clip_activation\'],\n        num_proj=params[\'num_proj\'],\n        weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_save(model=model, params=params, epoch=args.epoch)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
experiments/librispeech/visualization/plot_ctc_prob.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Plot the trained CTC posteriors (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport yaml\nimport argparse\nimport matplotlib\n\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nplt.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set_style(""white"")\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\nsys.path.append(os.path.abspath(\'../../../\'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\n# from models.ctc.vanilla_ctc import CTC\nfrom models.ctc.distillation_ctc import CTC\nfrom utils.directory import mkdir_join\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--epoch\', type=int, default=-1,\n                    help=\'the epoch to restore\')\nparser.add_argument(\'--model_path\', type=str,\n                    help=\'path to the model to evaluate\')\nparser.add_argument(\'--eval_batch_size\', type=str, default=1,\n                    help=\'the size of mini-batch in evaluation\')\n\n\ndef do_plot(model, params, epoch, eval_batch_size):\n    """"""Plot the CTC posteriors.\n    Args:\n        model: the model to restore\n        params (dict): A dictionary of parameters\n        epoch (int): the epoch to restore\n        eval_batch_size (int): the size of mini-batch in evaluation\n    """"""\n    # Load dataset\n    test_clean_data = Dataset(\n        data_type=\'test_clean\',\n        train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n    test_other_data = Dataset(\n        data_type=\'test_other\',\n        train_data_size=params[\'train_data_size\'],\n        label_type=params[\'label_type\'],\n        batch_size=eval_batch_size, splice=params[\'splice\'],\n        num_stack=params[\'num_stack\'], num_skip=params[\'num_skip\'],\n        shuffle=False)\n\n    with tf.name_scope(\'tower_gpu0\'):\n        # Define placeholders\n        model.create_placeholders()\n\n        # Add to the graph each operation (including model definition)\n        _, logits = model.compute_loss(\n            model.inputs_pl_list[0],\n            model.labels_pl_list[0],\n            model.inputs_seq_len_pl_list[0],\n            model.keep_prob_input_pl_list[0],\n            model.keep_prob_hidden_pl_list[0],\n            model.keep_prob_output_pl_list[0],\n            softmax_temperature=params[\'softmax_temperature\'])\n        posteriors_op = model.posteriors(logits, blank_prior=1)\n\n    # Create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(model.save_path)\n\n        # If check point exists\n        if ckpt:\n            # Use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch != -1:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # Visualize\n        posterior_test(session=sess,\n                       posteriors_op=posteriors_op,\n                       model=model,\n                       dataset=test_clean_data,\n                       label_type=params[\'label_type\'],\n                       num_stack=params[\'num_stack\'],\n                       #    save_path=None)\n                       save_path=mkdir_join(model.save_path, \'ctc_output\', \'test-clean\'))\n\n        posterior_test(session=sess,\n                       posteriors_op=posteriors_op,\n                       model=model,\n                       dataset=test_other_data,\n                       label_type=params[\'label_type\'],\n                       num_stack=params[\'num_stack\'],\n                       #    save_path=None)\n                       save_path=mkdir_join(model.save_path, \'ctc_output\', \'test-other\'))\n\n\ndef posterior_test(session, posteriors_op, model, dataset, label_type,\n                   num_stack=1, save_path=None, show=False):\n    """"""Visualize label posteriors of CTC model.\n    Args:\n        session: session of training model\n        posteriois_op: operation for computing posteriors\n        model: the model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type (string): phone39 or phone48 or phone61 or character or\n            character_capital_divide\n        num_stack (int): the number of frames to stack\n        save_path (string, string): path to save ctc outputs\n        show (bool, optional): if True, show each figure\n    """"""\n    for data, is_new_epoch in dataset:\n\n        # Create feed dictionary for next mini batch\n        inputs, _, inputs_seq_len, input_names = data\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_input_pl_list[0]: 1.0,\n            model.keep_prob_hidden_pl_list[0]: 1.0,\n            model.keep_prob_output_pl_list[0]: 1.0\n        }\n\n        batch_size, max_frame_num = inputs[0].shape[:2]\n        posteriors = session.run(posteriors_op, feed_dict=feed_dict)\n        posteriors = posteriors.reshape(-1,\n                                        max_frame_num, model.num_classes)\n\n        # Visualize\n        for i_batch in range(batch_size):\n            prob = posteriors[i_batch][:int(inputs_seq_len[0][i_batch]), :]\n\n            plt.clf()\n            plt.figure(figsize=(10, 4))\n            frame_num = int(inputs_seq_len[0][i_batch])\n            times_probs = np.arange(frame_num) * num_stack / 100\n\n            # NOTE: Blank class is set to the last class in TensorFlow\n            for i in range(0, prob.shape[-1] - 1, 1):\n                plt.plot(times_probs, prob[:, i])\n            plt.plot(times_probs, prob[:, -1],\n                     \':\', label=\'blank\', color=\'grey\')\n            plt.xlabel(\'Time [sec]\', fontsize=12)\n            plt.ylabel(\'Posteriors\', fontsize=12)\n            plt.xlim([0, frame_num * num_stack / 100])\n            plt.ylim([0.05, 1.05])\n            plt.xticks(list(range(0, int(frame_num * num_stack / 100) + 1, 1)))\n            plt.yticks(list(range(0, 2, 1)))\n            plt.legend(loc=""upper right"", fontsize=12)\n\n            plt.show()\n\n            # Save as a png file\n            if save_path is not None:\n                plt.savefig(os.path.join(\n                    save_path, input_names[0][i_batch] + \'.png\'), dvi=500)\n\n        if is_new_epoch:\n            break\n\n\ndef main():\n\n    args = parser.parse_args()\n\n    # Load config file\n    with open(os.path.join(args.model_path, \'config.yml\'), ""r"") as f:\n        config = yaml.load(f)\n        params = config[\'param\']\n\n    # Except for a blank class\n    if params[\'label_type\'] == \'character\':\n        params[\'num_classes\'] = 28\n    elif params[\'label_type\'] == \'character_capital_divide\':\n        params[\'num_classes\'] = 77\n    elif params[\'label_type\'] == \'word\':\n        if params[\'train_data_size\'] == \'train_clean100\':\n            params[\'num_classes\'] = 7213\n        elif params[\'train_data_size\'] == \'train_clean360\':\n            params[\'num_classes\'] = 16287\n        elif params[\'train_data_size\'] == \'train_other500\':\n            params[\'num_classes\'] = 18669\n        elif params[\'train_data_size\'] == \'train_all\':\n            params[\'num_classes\'] = 26642\n\n    # Model setting\n    model = CTC(\n        encoder_type=params[\'encoder_type\'],\n        input_size=params[\'input_size\'] * params[\'num_stack\'],\n        num_units=params[\'num_units\'],\n        num_layers=params[\'num_layers\'],\n        num_classes=params[\'num_classes\'],\n        lstm_impl=params[\'lstm_impl\'],\n        use_peephole=params[\'use_peephole\'],\n        parameter_init=params[\'weight_init\'],\n        clip_grad=params[\'clip_grad\'],\n        clip_activation=params[\'clip_activation\'],\n        num_proj=params[\'num_proj\'],\n        weight_decay=params[\'weight_decay\'])\n\n    model.save_path = args.model_path\n    do_plot(model=model, params=params, epoch=args.epoch,\n            eval_batch_size=args.eval_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
models/attention/decoders/__init__.py,0,b''
models/attention/decoders/attention_decoder.py,31,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""A basic sequence decoder that performs a softmax based on the RNN state.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom models.attention.decoders.dynamic_decoder import dynamic_decode\n\n\nclass AttentionDecoderOutput(namedtuple(\n        ""DecoderOutput"",\n        [\n            ""logits"",\n            ""predicted_ids"",\n            ""decoder_output"",\n            ""attention_weights"",\n            ""context_vector""\n        ])):\n    pass\n\n\nclass AttentionDecoder(tf.contrib.seq2seq.Decoder):\n    """"""An RNN Decoder that uses attention over an input sequence.\n    Args:\n        rnn_cell: An instance of ` tf.contrib.rnn.RNNCell`\n        parameter_init (float): Range of uniform distribution to\n            initialize weight parameters\n        max_decode_length (int): the length of output sequences to stop\n            prediction when EOS token have not been emitted\n        num_classes (int): Output vocabulary size,\n             i.e. number of units in the softmax layer\n        encoder_outputs: The outputs of the encoder. A tensor of shape\n            `[B, T_in, encoder_num_units]`.\n        encoder_outputs_seq_len: Sequence length of the encoder outputs.\n            An int32 Tensor of shape `[B]`.\n        attention_layer: The attention function to use. This function map from\n            `(state, inputs)` to `(attention_weights, context_vector)`.\n            For an example, see `decoders.attention_layer.AttentionLayer`.\n        time_major (bool): if True, time-major computation will be performed\n            in the dynamic decoder.\n        mode: tf.contrib.learn.ModeKeys\n    """"""\n\n    def __init__(self,\n                 rnn_cell,\n                 parameter_init,\n                 max_decode_length,\n                 num_classes,\n                 encoder_outputs,\n                 encoder_outputs_seq_len,\n                 attention_layer,\n                 time_major,\n                 mode,\n                 name=\'attention_decoder\'):\n\n        super(AttentionDecoder, self).__init__()\n\n        self.rnn_cell = rnn_cell\n        self.parameter_init = parameter_init\n        self.max_decode_length = max_decode_length\n        self.num_classes = num_classes\n        self.encoder_outputs = encoder_outputs\n        self.encoder_outputs_seq_len = encoder_outputs_seq_len\n        self.attention_layer = attention_layer  # AttentionLayer class\n        self.time_major = time_major\n        self.mode = mode\n        self.reuse = False if mode == tf.contrib.learn.ModeKeys.TRAIN else True\n        self.name = name\n\n        # NOTE: Not initialized yet\n        self.initial_state = None\n        self.helper = None\n\n    @property\n    def output_size(self):\n        return AttentionDecoderOutput(\n            logits=self.num_classes,\n            predicted_ids=tf.TensorShape([]),\n            decoder_output=self.rnn_cell.output_size,\n            attention_weights=tf.shape(self.encoder_outputs)[1:-1],\n            context_vector=self.encoder_outputs.get_shape()[-1])\n\n    @property\n    def output_dtype(self):\n        return AttentionDecoderOutput(\n            logits=tf.float32,\n            predicted_ids=tf.int32,\n            decoder_output=tf.float32,\n            attention_weights=tf.float32,\n            context_vector=tf.float32)\n\n    @property\n    def batch_size(self):\n        return tf.shape(nest.flatten([self.initial_state])[0])[0]\n\n    def __call__(self, initial_state, helper):\n        """"""\n        Args:\n            initial_state: A tensor or tuple of tensors used as the initial\n                rnn_cell state. Set to the final state of the encoder by default.\n            helper: An instance of `tf.contrib.seq2seq.Helper` to assist\n                decoding\n        Returns:\n            A tuple of `(outputs, final_state)`\n                outputs: A tensor of `[T_out, B, num_classes]`\n                final_state: A tensor of `[T_out, B, decoder_num_units]`\n        """"""\n        with tf.variable_scope(\'attention_decoder\', reuse=self.reuse):\n            # Initialize\n            if self.initial_state is None:\n                self._setup(initial_state, helper)\n            # NOTE: ignore when attention_decoder is wrapped by\n            # beam_search_decoder\n\n            if self.mode == tf.contrib.learn.ModeKeys.TRAIN:\n                maximum_iterations = None\n            else:\n                maximum_iterations = self.max_decode_length\n\n            # outputs, final_state, final_seq_len =\n            # tf.contrib.seq2seq.dynamic_decode(\n            outputs, final_state = dynamic_decode(\n                decoder=self,\n                output_time_major=self.time_major,\n                impute_finished=True,\n                maximum_iterations=maximum_iterations,\n                scope=None)\n\n            # tf.contrib.seq2seq.dynamic_decode\n            # return self.finalize(outputs, final_state, final_seq_len)\n\n            # ./dynamic_decoder.py\n            return self.finalize(outputs, final_state, None)\n\n    def initialize(self):\n        """"""Initialize the decoder.\n        Returns:\n            finished: A tensor of size `[B, ]`\n            first_inputs: The first inputs to the decoder. A tensor of size\n                `[B, embedding_dim]`\n            initial_state: The initial decoder state. A tensor of size\n                `[B, ]`\n        """"""\n        # Create inputs for the first time step\n        finished, first_inputs = self.helper.initialize()\n        # finished: `[1]`\n        # first_inputs: `[B]`\n\n        # Concat empty attention context (Input-feeding approach)\n        batch_size = tf.shape(first_inputs)[0]\n        encoder_num_units = self.encoder_outputs.get_shape().as_list()[-1]\n        context_vector = tf.zeros(shape=[batch_size, encoder_num_units])\n        first_inputs = tf.concat([first_inputs, context_vector], axis=1)\n\n        # Initialize attention weights\n        self.attention_weights = tf.zeros(\n            shape=[batch_size, tf.shape(self.encoder_outputs)[1]])\n        # `[B, T_in]`\n\n        return finished, first_inputs, self.initial_state\n        # TODO: self.initial_state\xe3\x81\xaf\xe3\x81\x93\xe3\x81\xae\xe6\x99\x82\xe7\x82\xb9\xe3\x81\xa7\xe3\x81\xafNone??\n\n    def _compute_output(self, decoder_output, attention_weights):\n        """"""Computes the decoder outputs at each time.\n        Args:\n            decoder_output: The previous state of the decoder\n            attention_weights: A tensor of size `[B, ]`\n        Returns:\n            attentional_vector: A tensors of size `[B, ]`\n            logits: A tensor of size `[B, ]`\n            attention_weights: A tensor of size `[B, ]`\n            context_vector: A tensor of szie `[B, ]`\n        """"""\n        # Compute attention weights & context vector\n        attention_weights, context_vector = self.attention_layer(\n            encoder_outputs=self.encoder_outputs,\n            decoder_output=decoder_output,\n            encoder_outputs_length=self.encoder_outputs_seq_len,\n            attention_weights=attention_weights)\n\n        # Input-feeding approach, this is used as inputs for the decoder\n        attentional_vector = tf.contrib.layers.fully_connected(\n            tf.concat([decoder_output, context_vector], axis=1),\n            num_outputs=self.rnn_cell.output_size,\n            activation_fn=tf.nn.tanh,\n            weights_initializer=tf.truncated_normal_initializer(\n                stddev=self.parameter_init),\n            biases_initializer=None,  # no bias\n            scope=""attentional_vector"")\n        # NOTE: This makes the softmax smaller and allows us to synthesize\n        # information between decoder state and attention context\n        # see https://arxiv.org/abs/1508.04025v5\n\n        # Softmax computation\n        logits = tf.contrib.layers.fully_connected(\n            attentional_vector,\n            num_outputs=self.num_classes,\n            activation_fn=None,\n            weights_initializer=tf.truncated_normal_initializer(\n                stddev=self.parameter_init),\n            biases_initializer=tf.zeros_initializer(),\n            scope=""output_layer"")\n\n        return attentional_vector, logits, attention_weights, context_vector\n\n    def _setup(self, initial_state, helper):\n        """"""Sets the initial state and helper for the decoder.\n        Args:\n            initial_state:\n            helper:\n        """"""\n        self.initial_state = initial_state\n        self.helper = helper\n\n        def _att_next_inputs(time, outputs, state, sample_ids, name=None):\n            """"""Wraps the original decoder helper function to append the\n               attention context.\n            Args:\n                time:\n                outputs:\n                state:\n                sample_ids:\n                name (string, optional): Name scope for any created operations\n            Returs:\n                Returns:\n                    finished: A tensor of size `[B, ]`\n                    next_inputs: A tensor of size `[B, ]`\n                    next_state: A tensor of size `[B, ]`\n            """"""\n            finished, next_inputs, next_state = helper.next_inputs(\n                time=time,\n                outputs=outputs,\n                state=state,\n                sample_ids=sample_ids,\n                name=name)\n\n            # Input-feeding approach\n            next_inputs = tf.concat(\n                [next_inputs, outputs.context_vector], axis=1)\n\n            return finished, next_inputs, next_state\n\n        # Wrap helper function for the attention mechanism\n        self.helper = tf.contrib.seq2seq.CustomHelper(\n            initialize_fn=helper.initialize,\n            sample_fn=helper.sample,\n            next_inputs_fn=_att_next_inputs)\n\n    def step(self, time, inputs, state, name=None):\n        """"""Perform a decoding step.\n        Args:\n           time:\n           inputs:\n           state:\n           name (string, optional): Name scope for any created operations\n        Returns:\n            outputs: An instance of AttentionDecoderOutput\n            next_state: A state tensors and TensorArrays\n            next_inputs: The tensor that should be used as input for the\n                next step\n            finished: A boolean tensor telling whether the sequence is\n                complete, for each sequence in the batch\n        """"""\n        cell_output, cell_state = self.rnn_cell(inputs, state)\n        attentional_vector, logits, attention_weights, context_vector = self._compute_output(\n            decoder_output=cell_output,\n            attention_weights=self.attention_weights)\n\n        # Update attention weights\n        self.attention_weights = attention_weights\n\n        sample_ids = self.helper.sample(time=time,\n                                        outputs=logits,\n                                        state=cell_state)\n\n        outputs = AttentionDecoderOutput(logits=logits,\n                                         predicted_ids=sample_ids,\n                                         decoder_output=attentional_vector,\n                                         attention_weights=attention_weights,\n                                         context_vector=context_vector)\n\n        finished, next_inputs, next_state = self.helper.next_inputs(\n            time=time,\n            outputs=outputs,\n            state=cell_state,\n            sample_ids=sample_ids)\n\n        return outputs, next_state, next_inputs, finished\n\n    def finalize(self, outputs, final_state, final_seq_len):\n        """"""Applies final transformation to the decoder output once decoding is\n           finished.\n        Args:\n            outputs:\n            final_state:\n            final_seq_len:\n        Returns:\n            outputs:\n            final_state:\n        """"""\n        return outputs, final_state\n'"
models/attention/decoders/attention_layer.py,69,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Attention layer for computing attention weights.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nATTENTION_TYPE = [\n    \'bahdanau_content\', \'normed_bahdanau_content\',\n    \'location\', \'hybrid\', \'dot_product\',\n    \'luong_dot\', \'scaled_luong_dot\', \'luong_general\', \'luong_concat\',\n    \'baidu_attetion\']\n\n\nclass AttentionLayer(object):\n    """"""Attention layer.\n    Args:\n        attention_type (string): content or location or hybrid or layer_dot\n        num_units (int): the number of units used in the attention layer\n        parameter_init (float, optional): the ange of uniform distribution to\n            initialize weight parameters (>= 0)\n        sharpening_factor (float): a sharpening factor in the\n            softmax layer for computing attention weights\n        sigmoid_smoothing (bool, optional): if True, replace softmax function\n            in computing attention weights with sigmoid function for smoothing\n        mode: tf.contrib.learn.ModeKeys\n        name (string): the name of the attention layer\n    """"""\n\n    def __init__(self, attention_type, num_units, parameter_init,\n                 sharpening_factor, sigmoid_smoothing,\n                 mode, name=\'attention_layer\'):\n        self.attention_type = attention_type\n        self.num_units = num_units\n        self.parameter_init = parameter_init\n        self.sharpening_factor = sharpening_factor\n        self.sigmoid_smoothing = sigmoid_smoothing\n        self.reuse = False if mode == tf.contrib.learn.ModeKeys.TRAIN else True\n        self.name = name\n\n    def __call__(self, encoder_outputs, decoder_output,\n                 encoder_outputs_length, attention_weights):\n        """"""Computes attention scores and outputs.\n        Args:\n            encoder_outputs: The outputs of the encoder. A tensor of size\n                `[B, T_in, encoder_num_units]`\n            decoder_output: The current state of the docoder. A tensor of size\n                `[B, decoder_num_units]`\n            encoder_outputs_length: An int32 tensor of size `[B]` defining\n                the sequence length of `encoder_outputs`\n            attention_weights: This is used for the location-based\n                attention. A tensor of size `[B, T_in]`\n        Returns:\n            A tuple `(attention_weights, context_vector)`.\n                attention_weights: vector of length `T_in` where each\n                    element is the normalized ""score"" of the corresponding\n                    `inputs` element. A tensor of size `[B, T_in]`\n                context_vector: the final attention layer output\n                    corresponding to the weighted inputs.\n                    A tensor of size `[B, encoder_num_units]`.\n        """"""\n        with tf.variable_scope(self.name, reuse=self.reuse):\n            # Compute attention scores over encoder outputs\n            energy = self._compute_attention_score(\n                attention_type=self.attention_type,\n                encoder_outputs=encoder_outputs,\n                decoder_output=decoder_output,\n                attention_weights=attention_weights)\n\n            # Replace all scores for padded inputs with tf.float32.min\n            max_time_input = tf.shape(energy)[1]\n            scores_mask = tf.sequence_mask(\n                lengths=tf.to_int32(encoder_outputs_length),\n                maxlen=tf.to_int32(max_time_input),\n                dtype=tf.float32)\n            # ex.) tf.sequence_mask\n            # tf.sequence_mask([1, 3, 2], 5) = [[True, False, False, False, False],\n            #                                   [True, True, True, False, False],\n            #                                   [True, True, False, False, False]]\n            energy = energy * scores_mask + \\\n                ((1.0 - scores_mask) * tf.float32.min)\n            # NOTE: For underflow\n\n            # Smoothing\n            energy *= self.sharpening_factor\n\n            # Compute attention weights\n            if self.sigmoid_smoothing:\n                attention_weights = tf.sigmoid(energy) / tf.reduce_sum(\n                    tf.sigmoid(energy),\n                    axis=-1,\n                    keep_dims=True)\n            else:\n                attention_weights = tf.nn.softmax(\n                    energy, name=""attention_weights"")\n                # attention_weights = tf.exp(energy) / tf.reduce_sum(\n                #     tf.exp(energy),\n                #     axis=-1,\n                #     keep_dims=True)\n\n            # Compute context vector\n            context_vector = tf.expand_dims(\n                attention_weights, axis=2) * encoder_outputs\n            context_vector = tf.reduce_sum(\n                context_vector, axis=1, name=""context_vector"")\n            encoder_num_units = encoder_outputs.get_shape().as_list()[-1]\n            context_vector.set_shape([None, encoder_num_units])\n\n            return attention_weights, context_vector\n\n    def _compute_attention_score(self, attention_type, encoder_outputs,\n                                 decoder_output, attention_weights):\n        """"""An attention layer that calculates attention scores.\n        Args:\n            attention_type (string): the type of attention\n            encoder_outputs: The sequence of encoder outputs\n                A tensor of shape `[B, T_in, encoder_num_units]`\n            decoder_output: The current state of the docoder\n                A tensor of shape `[B, decoder_num_units]`\n            attention_weights: A tensor of size `[B, T_in]`\n        Returns:\n            energy: The summation of attention scores\n                A tensor of shape `[B, T_in]`\n        """"""\n        if attention_type not in ATTENTION_TYPE:\n            raise ValueError(\n                ""attention type should be one of [%s], you provided %s."" %\n                ("", "".join(ATTENTION_TYPE), attention_type))\n\n        encoder_num_units = encoder_outputs.shape.as_list()[-1]\n        decoder_num_units = decoder_output.shape.as_list()[-1]\n\n        if attention_type not in [\'luong_dot\', \'scaled_luong_dot\',\n                                  \'luong_general\', \'luong_concat\']:\n\n            # Fully connected layers to transform both encoder_outputs and\n            # decoder_output into a tensor with `num_units` units\n            W_query = tf.contrib.layers.fully_connected(\n                decoder_output,\n                num_outputs=self.num_units,\n                activation_fn=None,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=None,  # no bias\n                scope=""W_query"")\n            # NOTE: `[B, num_units]`,\n            W_keys = tf.contrib.layers.fully_connected(\n                encoder_outputs,\n                num_outputs=self.num_units,\n                activation_fn=None,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(\n                ) if self.attention_type != \'dot_product\' else None,\n                scope=""W_keys"")\n            # NOTE: `[B, T_in, num_units]`\n\n            if attention_type == \'dot_product\':\n                ############################################################\n                # layer-dot attention\n                # energy = dot(W_keys(h_enc), W_query(h_dec))\n                ############################################################\n                # Calculates a batch- and time-wise dot product\n                energy = tf.matmul(W_keys, tf.expand_dims(W_query, axis=2))\n\n                # `[B, T_in, 1]` -> `[B, T_in]`\n                energy = tf.squeeze(energy, axis=2)\n\n            elif attention_type in [\'bahdanau_content\', \'normed_bahdanau_content\']:\n                ############################################################\n                # bahdanau\'s content-based attention\n                # energy = dot(v_a, tanh(W_keys(h_enc) + W_query(h_dec)))\n                ############################################################\n                v_a = tf.get_variable(\n                    ""v_a"", shape=[self.num_units], dtype=tf.float32)\n\n                # Calculates a batch- and time-wise dot product with a variable\n                energy = v_a * \\\n                    tf.tanh(W_keys + tf.expand_dims(W_query, axis=1))\n\n                # `[B, T_in, num_units]` -> `[B, T_in]`\n                energy = tf.reduce_sum(energy, axis=2)\n\n                if attention_type == \'normed_bahdanau_content\':\n                    raise NotImplementedError\n\n            elif attention_type == \'hybrid\':\n                ############################################################\n                # hybrid attention (content-based + location-based)\n                # f = F * \xce\xb1_{i-1}\n                # energy = dot(v_a, tanh(W_keys(h_enc) + W_query(h_dec) + W_fil(f)))\n                ############################################################\n                with tf.control_dependencies(None):\n                    F = tf.Variable(tf.truncated_normal(\n                        shape=[200, 1, 10],\n                        # shape=[100, 1, 10],\n                        stddev=self.parameter_init),\n                        name=\'filter\')\n                # `[B, T_in]` -> `[B, T_in, 1]`\n                attention_weights = tf.expand_dims(attention_weights, axis=2)\n                f = tf.nn.conv1d(attention_weights,\n                                 F,\n                                 stride=1,\n                                 padding=\'SAME\',\n                                 #  use_cudnn_on_gpu=None,\n                                 #  data_format=None,\n                                 name=\'conv_features\')\n                W_fil = tf.contrib.layers.fully_connected(\n                    f,\n                    num_outputs=self.num_units,\n                    activation_fn=None,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=""W_filter"")\n\n                v_a = tf.get_variable(\n                    ""v_a"", shape=[self.num_units], dtype=tf.float32)\n\n                # Calculates a batch- and time-wise dot product with a variable\n                energy = v_a * \\\n                    tf.tanh(W_keys + tf.expand_dims(W_query, axis=1) + W_fil)\n\n                # `[B, T_in, num_units]` -> `[B, T_in]`\n                energy = tf.reduce_sum(energy, axis=2)\n\n            elif attention_type == \'location\':\n                ############################################################\n                # location-based attention\n                # f = F * \xce\xb1_{i-1}\n                # energy = dot(v_a, tanh(W_query(h_dec) + W_fil(f)))\n                ############################################################\n                with tf.control_dependencies(None):\n                    F = tf.Variable(tf.truncated_normal(\n                        shape=[201, 1, 10],\n                        # shape=[100, 1, 10],\n                        stddev=0.1), name=\'filter\')\n                # `[B, T_in]` -> `[B, T_in, 1]`\n                attention_weights = tf.expand_dims(attention_weights, axis=2)\n                f = tf.nn.conv1d(attention_weights, F,\n                                 stride=1, padding=\'SAME\',\n                                 #  use_cudnn_on_gpu=None,\n                                 #  data_format=None,\n                                 name=\'conv_features\')\n                W_fil = tf.contrib.layers.fully_connected(\n                    f,\n                    num_outputs=self.num_units,\n                    activation_fn=None,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=""W_filter"")\n\n                v_a = tf.get_variable(\n                    ""v_a"", shape=[self.num_units], dtype=tf.float32)\n\n                # Calculates a batch- and time-wise dot product with a variable\n                energy = v_a * tf.tanh(tf.expand_dims(W_query, axis=1) + W_fil)\n\n                # `[B, T_in, num_units]` -> `[B, T_in]`\n                energy = tf.reduce_sum(energy, axis=2)\n\n            elif attention_type == \'baidu_attetion\':\n                raise NotImplementedError\n\n        else:\n            if attention_type in [\'luong_dot\', \'scaled_luong_dot\']:\n                ############################################################\n                # luong\'s dot product attention\n                # energy = dot(h_enc, h_dec)\n                ############################################################\n                if encoder_num_units != decoder_num_units:\n                    raise ValueError(\n                        \'encoder_num_units and decoder_num_units must be the same size.\')\n\n                # Calculates a batch- and time-wise dot product\n                energy = tf.matmul(\n                    encoder_outputs, tf.expand_dims(decoder_output, axis=2))\n\n                # `[B, T_in, 1]` -> `[B, T_in]`\n                energy = tf.squeeze(energy, axis=2)\n\n                if attention_type == \'scaled_luong_dot\':\n                    raise NotImplementedError\n\n            elif attention_type == \'luong_general\':\n                ############################################################\n                # luong\'s general attention\n                # energy = dot(h_dec, W_keys(h_enc))\n                ############################################################\n                # Fully connected layers to transform both encoder_outputs into\n                # a tensor with `decoder_num_units` units\n                W_keys = tf.contrib.layers.fully_connected(\n                    encoder_outputs,\n                    num_outputs=decoder_num_units,\n                    activation_fn=None,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=None,  # no bias\n                    scope=""W_keys"")\n                # NOTE: `[B, T_in, decoder_num_units]`\n\n                # Calculates a batch- and time-wise dot product\n                energy = tf.matmul(\n                    W_keys, tf.expand_dims(decoder_output, axis=2))\n\n                # `[B, T_in, 1]` -> `[B, T_in]`\n                energy = tf.squeeze(energy, axis=2)\n\n            elif attention_type == \'luong_concat\':\n                ############################################################\n                # luong\'s content-based attention\n                # energy = dot(v_a, tanh(W_concat([h_enc;h_dec])))\n                ############################################################\n                max_time = tf.shape(encoder_outputs)[1]\n                concated_states = tf.concat(\n                    [encoder_outputs,\n                     tf.tile(tf.expand_dims(decoder_output, axis=1), [1, max_time, 1])],\n                    axis=2)\n\n                # Fully connected layers to transform both concatenated\n                # encoder_outputs and decoder_output into a tensor with\n                # `num_units` units\n                W_concat = tf.contrib.layers.fully_connected(\n                    concated_states,\n                    num_outputs=self.num_units,\n                    activation_fn=None,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=None,  # no bias\n                    scope=""W_concat"")\n                # NOTE: `[B, T_in, num_units]`,\n\n                v_a = tf.get_variable(\n                    ""v_a"", shape=[self.num_units], dtype=tf.float32)\n\n                # Calculates a batch- and time-wise dot product with a variable\n                energy = v_a * tf.tanh(W_concat)\n\n                # `[B, T_in, num_units]` -> `[B, T_in]`\n                energy = tf.reduce_sum(energy, axis=2)\n\n        return energy\n'"
models/attention/decoders/beam_search_decoder_from_tensorflow.py,55,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""A decoder that performs beam search.\nThis code is taken directly from Tensorflow\n(https://github.com/tensorflow/tensorflow) and is copied temporarily\nuntil it is available in a packaged Tensorflow version on pypi.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\n# from . import beam_search_ops\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.layers import base as layers_base\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_tf\nfrom tensorflow.python.util import nest\n\n\nclass BeamSearchDecoderState(\n    collections.namedtuple(""BeamSearchDecoderState"", (""cell_state"", ""log_probs"",\n                                                      ""finished"", ""lengths""))):\n    pass\n\n\nclass BeamSearchDecoderOutput(\n    collections.namedtuple(""BeamSearchDecoderOutput"",\n                           (""scores"", ""predicted_ids"", ""parent_ids""))):\n    pass\n\n\ndef _tile_batch(t, multiplier):\n    """"""Core single-tensor implementation of tile_batch.""""""\n    t = ops.convert_to_tensor(t, name=""t"")\n    shape_t = tf.shape(t)\n    if t.shape.ndims is None or t.shape.ndims < 1:\n        raise ValueError(""t must have statically known rank"")\n    tiling = [1] * (t.shape.ndims + 1)\n    tiling[1] = multiplier\n    tiled_static_batch_size = (\n        t.shape[0].value * multiplier if t.shape[0].value is not None else None)\n    tiled = tf.tile(tf.expand_dims(t, 1), tiling)\n    tiled = tf.reshape(\n        tiled, tf.concat(([shape_t[0] * multiplier], shape_t[1:]), 0))\n    tiled.set_shape(\n        tensor_shape.TensorShape(\n            [tiled_static_batch_size]).concatenate(t.shape[1:]))\n    return tiled\n\n\ndef tile_batch(t, multiplier, name=None):\n    """"""Tile the batch dimension of a (possibly nested structure of) tensor(s) t.\n    For each tensor t in a (possibly nested structure) of tensors,\n    this function takes a tensor t shaped `[batch_size, s0, s1, ...]` composed of\n    minibatch entries `t[0], ..., t[batch_size - 1]` and tiles it to have a shape\n    `[batch_size * multiplier, s0, s1, ...]` composed of minibatch entries\n    `t[0], t[0], ..., t[1], t[1], ...` where each minibatch entry is repeated\n    `multiplier` times.\n    Args:\n      t: `Tensor` shaped `[batch_size, ...]`.\n      multiplier: Python int.\n      name: Name scope for any created operations.\n    Returns:\n      A (possibly nested structure of) `Tensor` shaped\n      `[batch_size * multiplier, ...]`.\n    Raises:\n      ValueError: if tensor(s) `t` do not have a statically known rank or\n      the rank is < 1.\n    """"""\n    flat_t = nest.flatten(t)\n    with tf.name_scope(name, ""tile_batch"", flat_t + [multiplier]):\n        return nest.map_structure(lambda t_: _tile_batch(t_, multiplier), t)\n\n\ndef _check_maybe(t):\n    if isinstance(t, tensor_tf.TensorArray):\n        raise TypeError(\n            ""TensorArray state is not supported by BeamSearchDecoder: %s"" % t.name)\n    if t.shape.ndims is None:\n        raise ValueError(\n            ""Expected tensor (%s) to have known rank, but ndims == None."" % t)\n\n\nclass BeamSearchDecoder(decoder.Decoder):\n    """"""BeamSearch sampling decoder.""""""\n\n    def __init__(self,\n                 cell,\n                 embedding,\n                 start_tokens,\n                 end_token,\n                 initial_state,\n                 beam_width,\n                 output_layer=None,\n                 length_penalty_weight=0.0):\n        """"""Initialize BeamSearchDecoder.\n        Args:\n          cell: An `RNNCell` instance.\n          embedding: A callable that takes a vector tensor of `ids` (argmax ids),\n            or the `params` argument for `embedding_lookup`.\n          start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n          end_token: `int32` scalar, the token that marks end of decoding.\n          initial_state: A (possibly nested tuple of...) tensors and TensorArrays.\n          output_layer: (Optional) An instance of `tf.layers.Layer`, i.e.,\n            `tf.layers.Dense`.  Optional layer to apply to the RNN output prior\n            to storing the result or sampling.\n          length_penalty_weight: Float weight to penalize length. Disabled with 0.0.\n        Raises:\n          TypeError: if `cell` is not an instance of `RNNCell`,\n            or `output_layer` is not an instance of `tf.layers.Layer`.\n          ValueError: If `start_tokens` is not a vector or\n            `end_token` is not a scalar.\n        """"""\n        if not rnn_cell_impl._like_rnncell(cell):\n            raise TypeError(\n                ""cell must be an RNNCell, received: %s"" % type(cell))\n        if (output_layer is not None\n                and not isinstance(output_layer, layers_base.Layer)):\n            raise TypeError(\n                ""output_layer must be a Layer, received: %s"" % type(output_layer))\n        self._cell = cell\n        self._output_layer = output_layer\n\n        if callable(embedding):\n            self._embedding_fn = embedding\n        else:\n            self._embedding_fn = (\n                lambda ids: embedding_ops.embedding_lookup(embedding, ids))\n\n        self._start_tokens = ops.convert_to_tensor(\n            start_tokens, dtype=dtypes.int32, name=""start_tokens"")\n        if self._start_tokens.get_shape().ndims != 1:\n            raise ValueError(""start_tokens must be a vector"")\n        self._end_token = ops.convert_to_tensor(\n            end_token, dtype=dtypes.int32, name=""end_token"")\n        if self._end_token.get_shape().ndims != 0:\n            raise ValueError(""end_token must be a scalar"")\n\n        self._batch_size = tf.size(start_tokens)\n        self._beam_width = beam_width\n        self._length_penalty_weight = length_penalty_weight\n        self._initial_cell_state = nest.map_structure(\n            self._maybe_split_batch_beams,\n            initial_state, self._cell.state_size)\n        self._start_tokens = tf.tile(\n            tf.expand_dims(self._start_tokens, 1), [1, self._beam_width])\n        self._start_inputs = self._embedding_fn(self._start_tokens)\n\n    def _rnn_output_size(self):\n        size = self._cell.output_size\n        if self._output_layer is None:\n            return size\n        else:\n            # To use layer\'s compute_output_shape, we need to convert the\n            # RNNCell\'s output_size entries into shapes with an unknown\n            # batch size.  We then pass this through the layer\'s\n            # compute_output_shape and read off all but the first (batch)\n            # dimensions to get the output size of the rnn with the layer\n            # applied to the top.\n            output_shape_with_unknown_batch = nest.map_structure(\n                lambda s: tensor_shape.TensorShape([None]).concatenate(s),\n                size)\n            layer_output_shape = self._output_layer._compute_output_shape(  # pylint: disable=protected-access\n                output_shape_with_unknown_batch)\n            return nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n    @property\n    def output_size(self):\n        # Return the cell output and the id\n        return BeamSearchDecoderOutput(\n            scores=tensor_shape.TensorShape([self._beam_width]),\n            predicted_ids=tensor_shape.TensorShape([self._beam_width]),\n            parent_ids=tensor_shape.TensorShape([self._beam_width]))\n\n    @property\n    def output_dtype(self):\n        # Assume the dtype of the cell is the output_size structure\n        # containing the input_state\'s first component\'s dtype.\n        # Return that structure and int32 (the id)\n        dtype = nest.flatten(self._initial_cell_state)[0].dtype\n        return BeamSearchDecoderOutput(\n            scores=nest.map_structure(\n                lambda _: dtype, self._rnn_output_size()),\n            predicted_ids=dtypes.int32,\n            parent_ids=dtypes.int32)\n\n    def initialize(self, name=None):\n        initial_state = BeamSearchDecoderState(\n            cell_state=self._initial_cell_state,\n            log_probs=tf.zeros(\n                [self._batch_size, self._beam_width],\n                dtype=nest.flatten(self._initial_cell_state)[0].dtype),\n            finished=finished,\n            lengths=tf.zeros(\n                [self._batch_size, self._beam_width], dtype=dtypes.int32))\n\n        return (finished, start_inputs, initial_state)\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        """"""Finalize and return the predicted_ids.\n        Args:\n          final_state: An instance of BeamSearchDecoderState. Passed through to the\n            output.\n          sequence_lengths: An `int32` tensor shaped `[batch_size, beam_width]`.\n            The sequence lengths determined for each beam during decode.\n        Returns:\n          outputs: An instance of FinalBeamSearchDecoderOutput where the\n            predicted_ids are the result of calling _gather_tree.\n          final_state: The same input instance of BeamSearchDecoderState.\n        """"""\n        predicted_ids = gather_tree(\n            outputs.predicted_ids, outputs.parent_ids,\n            sequence_length=sequence_lengths)\n\n        outputs = FinalBeamSearchDecoderOutput(\n            beam_search_decoder_output=outputs, predicted_ids=predicted_ids)\n        return outputs, final_state\n\n    def _merge_batch_beams(self, t, s=None):\n        """"""Merges the tensor from a batch of beams into a batch by beams.\n        More exactly, t is a tensor of dimension [batch_size, beam_width, s]. We\n        reshape this into [batch_size*beam_width, s]\n        Args:\n          t: Tensor of dimension [batch_size, beam_width, s]\n          s: (Possibly known) depth shape.\n        Returns:\n          A reshaped version of t with dimension [batch_size * beam_width, s].\n        """"""\n        if isinstance(s, ops.Tensor):\n            s = tensor_shape.as_shape(tensor_util.constant_value(s))\n        else:\n            s = tensor_shape.TensorShape(s)\n        t_shape = tf.shape(t)\n        static_batch_size = tensor_util.constant_value(self._batch_size)\n        batch_size_beam_width = (\n            None if static_batch_size is None\n            else static_batch_size * self._beam_width)\n        reshaped_t = tf.reshape(\n            t, tf.concat(\n                ([self._batch_size * self._beam_width], t_shape[2:]), 0))\n        reshaped_t.set_shape(\n            (tensor_shape.TensorShape([batch_size_beam_width]).concatenate(s)))\n        return reshaped_t\n\n    def _split_batch_beams(self, t, s=None):\n        """"""Splits the tensor from a batch by beams into a batch of beams.\n        More exactly, t is a tensor of dimension [batch_size*beam_width, s]. We\n        reshape this into [batch_size, beam_width, s]\n        Args:\n          t: Tensor of dimension [batch_size*beam_width, s].\n          s: (Possibly known) depth shape.\n        Returns:\n          A reshaped version of t with dimension [batch_size, beam_width, s].\n        Raises:\n          ValueError: If, after reshaping, the new tensor is not shaped\n            `[batch_size, beam_width, s]` (assuming batch_size and beam_width\n            are known statically).\n        """"""\n        if isinstance(s, ops.Tensor):\n            s = tensor_shape.TensorShape(tensor_util.constant_value(s))\n        else:\n            s = tensor_shape.TensorShape(s)\n        t_shape = tf.shape(t)\n        reshaped_t = tf.reshape(\n            t, tf.concat(\n                ([self._batch_size, self._beam_width], t_shape[1:]), 0))\n        static_batch_size = tensor_util.constant_value(self._batch_size)\n        expected_reshaped_shape = tensor_shape.TensorShape(\n            [static_batch_size, self._beam_width]).concatenate(s)\n        if not reshaped_t.shape.is_compatible_with(expected_reshaped_shape):\n            raise ValueError(""Unexpected behavior when reshaping between beam width ""\n                             ""and batch size.  The reshaped tensor has shape: %s.  ""\n                             ""We expected it to have shape ""\n                             ""(batch_size, beam_width, depth) == %s.  Perhaps you ""\n                             ""forgot to create a zero_state with ""\n                             ""batch_size=encoder_batch_size * beam_width?""\n                             % (reshaped_t.shape, expected_reshaped_shape))\n        reshaped_t.set_shape(expected_reshaped_shape)\n        return reshaped_t\n\n    def _maybe_split_batch_beams(self, t, s):\n        """"""Maybe splits the tensor from a batch by beams into a batch of beams.\n        We do this so that we can use nest and not run into problems with shapes.\n        Args:\n          t: Tensor of dimension [batch_size*beam_width, s]\n          s: Tensor, Python int, or TensorShape.\n        Returns:\n          Either a reshaped version of t with dimension\n          [batch_size, beam_width, s] if t\'s first dimension is of size\n          batch_size*beam_width or t if not.\n        Raises:\n          TypeError: If t is an instance of TensorArray.\n          ValueError: If the rank of t is not statically known.\n        """"""\n        _check_maybe(t)\n        if t.shape.ndims >= 1:\n            return self._split_batch_beams(t, s)\n        else:\n            return t\n\n    def _maybe_merge_batch_beams(self, t, s):\n        """"""Splits the tensor from a batch by beams into a batch of beams.\n        More exactly, t is a tensor of dimension [batch_size*beam_width, s]. We\n        reshape this into [batch_size, beam_width, s]\n        Args:\n          t: Tensor of dimension [batch_size*beam_width, s]\n          s: Tensor, Python int, or TensorShape.\n        Returns:\n          A reshaped version of t with dimension [batch_size, beam_width, s].\n        Raises:\n          TypeError: If t is an instance of TensorArray.\n          ValueError:  If the rank of t is not statically known.\n        """"""\n        _check_maybe(t)\n        if t.shape.ndims >= 2:\n            return self._merge_batch_beams(t, s)\n        else:\n            return t\n\n    def step(self, time, inputs, state, name=None):\n        """"""Perform a decoding step.\n        Args:\n          time: scalar `int32` tensor.\n          inputs: A (structure of) input tensors.\n          state: A (structure of) state tensors and TensorArrays.\n          name: Name scope for any created operations.\n        Returns:\n          `(outputs, next_state, next_inputs, finished)`.\n        """"""\n        batch_size = self._batch_size\n        beam_width = self._beam_width\n        end_token = self._end_token\n        length_penalty_weight = self._length_penalty_weight\n\n        with tf.name_scope(name, ""BeamSearchDecoderStep"", (time, inputs, state)):\n            cell_state = state.cell_state\n            inputs = nest.map_structure(\n                lambda inp: self._merge_batch_beams(inp, s=inp.shape[2:]), inputs)\n            cell_state = nest.map_structure(\n                self._maybe_merge_batch_beams,\n                cell_state, self._cell.state_size)\n            cell_outputs, next_cell_state = self._cell(inputs, cell_state)\n            cell_outputs = nest.map_structure(\n                lambda out: self._split_batch_beams(out, out.shape[1:]), cell_outputs)\n            next_cell_state = nest.map_structure(\n                self._maybe_split_batch_beams,\n                next_cell_state, self._cell.state_size)\n\n            if self._output_layer is not None:\n                cell_outputs = self._output_layer(cell_outputs)\n\n            beam_search_output, beam_search_state = _beam_search_step(\n                time=time,\n                logits=cell_outputs,\n                next_cell_state=next_cell_state,\n                beam_state=state,\n                batch_size=batch_size,\n                beam_width=beam_width,\n                end_token=end_token,\n                length_penalty_weight=length_penalty_weight)\n\n            finished = beam_search_state.finished\n            sample_ids = beam_search_output.predicted_ids\n            next_inputs = tf.cond(\n                tf.reduce_all(finished), lambda: self._start_inputs,\n                lambda: self._embedding_fn(sample_ids))\n\n        return (beam_search_output, beam_search_state, next_inputs, finished)\n\n\ndef _beam_search_step(time, logits, next_cell_state, beam_state, batch_size,\n                      beam_width, end_token, length_penalty_weight):\n    """"""Performs a single step of Beam Search Decoding.\n    Args:\n      time: Beam search time step, should start at 0. At time 0 we assume\n        that all beams are equal and consider only the first beam for\n        continuations.\n      logits: Logits at the current time step. A tensor of shape\n        `[batch_size, beam_width, vocab_size]`\n      next_cell_state: The next state from the cell, e.g. an instance of\n        AttentionWrapperState if the cell is attentional.\n      beam_state: Current state of the beam search.\n        An instance of `BeamSearchDecoderState`.\n      batch_size: The batch size for this input.\n      beam_width: Python int.  The size of the beams.\n      end_token: The int32 end token.\n      length_penalty_weight: Float weight to penalize length. Disabled with 0.0.\n    Returns:\n      A new beam state.\n    """"""\n    static_batch_size = tensor_util.constant_value(batch_size)\n\n    # Calculate the current lengths of the predictions\n    prediction_lengths = beam_state.lengths\n    previously_finished = beam_state.finished\n\n    # Calculate the total log probs for the new hypotheses\n    # Final Shape: [batch_size, beam_width, vocab_size]\n    step_log_probs = nn_ops.log_softmax(logits)\n    step_log_probs = _mask_probs(\n        step_log_probs, end_token, previously_finished)\n    total_probs = tf.expand_dims(\n        beam_state.log_probs, axis=2) + step_log_probs\n\n    # Calculate the continuation lengths by adding to all continuing beams.\n    vocab_size = logits.shape[-1].value\n    lengths_to_add = tf.one_hot(\n        indices=tf.tile(\n            tf.reshape(end_token, [1, 1]), [batch_size, beam_width]),\n        depth=vocab_size,\n        on_value=0,\n        off_value=1)\n    add_mask = (1 - tf.to_int32(previously_finished))\n    lengths_to_add = tf.expand_dims(add_mask, 2) * lengths_to_add\n    new_prediction_lengths = (\n        lengths_to_add + tf.expand_dims(prediction_lengths, 2))\n\n    # Calculate the scores for each beam\n    scores = _get_scores(\n        log_probs=total_probs,\n        sequence_lengths=new_prediction_lengths,\n        length_penalty_weight=length_penalty_weight)\n\n    time = ops.convert_to_tensor(time, name=""time"")\n    # During the first time step we only consider the initial beam\n    scores_shape = tf.shape(scores)\n    scores_flat = tf.cond(\n        time > 0,\n        lambda: tf.reshape(scores, [batch_size, -1]),\n        lambda: scores[:, 0])\n    num_available_beam = tf.cond(\n        time > 0, lambda: tf.reduce_prod(scores_shape[1:]),\n        lambda: tf.reduce_prod(scores_shape[2:]))\n\n    # Pick the next beams according to the specified successors function\n    next_beam_size = tf.minimum(\n        ops.convert_to_tensor(\n            beam_width, dtype=dtypes.int32, name=""beam_width""),\n        num_available_beam)\n    next_beam_scores, word_indices = nn_ops.top_k(\n        scores_flat, k=next_beam_size)\n    next_beam_scores.set_shape([static_batch_size, beam_width])\n    word_indices.set_shape([static_batch_size, beam_width])\n\n    # Pick out the probs, beam_ids, and states according to the chosen\n    # predictions\n    next_beam_probs = _tensor_gather_helper(\n        gather_indices=word_indices,\n        gather_from=total_probs,\n        batch_size=batch_size,\n        range_size=beam_width * vocab_size,\n        gather_shape=[-1])\n    next_word_ids = tf.to_int32(word_indices % vocab_size)\n    next_beam_ids = tf.to_int32(word_indices / vocab_size)\n\n    # Append new ids to current predictions\n    previously_finished = _tensor_gather_helper(\n        gather_indices=next_beam_ids,\n        gather_from=previously_finished,\n        batch_size=batch_size,\n        range_size=beam_width,\n        gather_shape=[-1])\n    next_finished = tf.logical_or(previously_finished,\n                                  tf.equal(next_word_ids, end_token))\n\n    # Calculate the length of the next predictions.\n    # 1. Finished beams remain unchanged\n    # 2. Beams that are now finished (EOS predicted) remain unchanged\n    # 3. Beams that are not yet finished have their length increased by 1\n    lengths_to_add = tf.to_int32(\n        tf.not_equal(next_word_ids, end_token))\n    lengths_to_add = (1 - tf.to_int32(next_finished)) * lengths_to_add\n    next_prediction_len = _tensor_gather_helper(\n        gather_indices=next_beam_ids,\n        gather_from=beam_state.lengths,\n        batch_size=batch_size,\n        range_size=beam_width,\n        gather_shape=[-1])\n    next_prediction_len += lengths_to_add\n\n    # Pick out the cell_states according to the next_beam_ids. We use a\n    # different gather_shape here because the cell_state tensors, i.e.\n    # the tensors that would be gathered from, all have dimension\n    # greater than two and we need to preserve those dimensions.\n    next_cell_state = nest.map_structure(\n        lambda gather_from: _maybe_tensor_gather_helper(\n            gather_indices=next_beam_ids,\n            gather_from=gather_from,\n            batch_size=batch_size,\n            range_size=beam_width,\n            gather_shape=[batch_size * beam_width, -1]),\n        next_cell_state)\n\n    next_state = BeamSearchDecoderState(\n        cell_state=next_cell_state,\n        log_probs=next_beam_probs,\n        lengths=next_prediction_len,\n        finished=next_finished)\n\n    output = BeamSearchDecoderOutput(\n        scores=next_beam_scores,\n        predicted_ids=next_word_ids,\n        parent_ids=next_beam_ids)\n\n    return output, next_state\n\n\ndef _mask_probs(probs, eos_token, finished):\n    vocab_size = tf.shape(probs)[2]\n    finished_mask = tf.expand_dims(\n        tf.to_float(1. - tf.to_float(finished)), axis=2)\n\n\ndef _maybe_tensor_gather_helper(gather_indices, gather_from, batch_size,\n                                range_size, gather_shape):\n    """"""Maybe applies _tensor_gather_helper.\n    This applies _tensor_gather_helper when the gather_from dims is at least as\n    big as the length of gather_shape. This is used in conjunction with nest so\n    that we don\'t apply _tensor_gather_helper to inapplicable values like scalars.\n    Args:\n      gather_indices: The tensor indices that we use to gather.\n      gather_from: The tensor that we are gathering from.\n      batch_size: The batch size.\n      range_size: The number of values in each range. Likely equal to beam_width.\n      gather_shape: What we should reshape gather_from to in order to preserve the\n        correct values. An example is when gather_from is the attention from an\n        AttentionWrapperState with shape [batch_size, beam_width, attention_size].\n        There, we want to preserve the attention_size elements, so gather_shape is\n        [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n        attention_size as desired.\n    Returns:\n      output: Gathered tensor of shape tf.shape(gather_from)[:1+len(gather_shape)]\n        or the original tensor if its dimensions are too small.\n    """"""\n    _check_maybe(gather_from)\n    if gather_from.shape.ndims >= len(gather_shape):\n        return _tensor_gather_helper(\n            gather_indices=gather_indices,\n            gather_from=gather_from,\n            batch_size=batch_size,\n            range_size=range_size,\n            gather_shape=gather_shape)\n    else:\n        return gather_from\n\n\ndef _tensor_gather_helper(gather_indices, gather_from, batch_size,\n                          range_size, gather_shape):\n    """"""Helper for gathering the right indices from the tensor.\n    This works by reshaping gather_from to gather_shape (e.g. [-1]) and then\n    gathering from that according to the gather_indices, which are offset by\n    the right amounts in order to preserve the batch order.\n    Args:\n      gather_indices: The tensor indices that we use to gather.\n      gather_from: The tensor that we are gathering from.\n      batch_size: The input batch size.\n      range_size: The number of values in each range. Likely equal to beam_width.\n      gather_shape: What we should reshape gather_from to in order to preserve the\n        correct values. An example is when gather_from is the attention from an\n        AttentionWrapperState with shape [batch_size, beam_width, attention_size].\n        There, we want to preserve the attention_size elements, so gather_shape is\n        [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n        attention_size as desired.\n    Returns:\n      output: Gathered tensor of shape tf.shape(gather_from)[:1+len(gather_shape)]\n    """"""\n    range_ = tf.expand_dims(tf.range(batch_size) * range_size, 1)\n    gather_indices = tf.reshape(gather_indices + range_, [-1])\n    output = tf.gather(\n        tf.reshape(gather_from, gather_shape), gather_indices)\n    final_shape = tf.shape(gather_from)[:1 + len(gather_shape)]\n    static_batch_size = tensor_util.constant_value(batch_size)\n    final_static_shape = (tensor_shape.TensorShape([static_batch_size])\n                          .concatenate(\n                              gather_from.shape[1:1 + len(gather_shape)]))\n    output = tf.reshape(output, final_shape)\n    output.set_shape(final_static_shape)\n    return output\n'"
models/attention/decoders/decoder_util.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport collections\n\n\ndef _flatten_dict(dict_, parent_key="""", sep="".""):\n    """"""Flattens a nested dictionary. Namedtuples within\n    the dictionary are converted to dicts.\n    Args:\n        dict_: The dictionary to flatten.\n        parent_key: A prefix to prepend to each key.\n        sep: Separator between parent and child keys, a string. For example\n            { ""a"": { ""b"": 3 } } will become { ""a.b"": 3 } if the separator\n            is ""."".\n    Returns:\n        A new flattened dictionary.\n    """"""\n    items = []\n    for key, value in dict_.items():\n        new_key = parent_key + sep + key if parent_key else key\n        if isinstance(value, collections.MutableMapping):\n            items.extend(_flatten_dict(value, new_key, sep=sep).items())\n        elif isinstance(value, tuple) and hasattr(value, ""_asdict""):\n            dict_items = collections.OrderedDict(zip(value._fields, value))\n            items.extend(_flatten_dict(\n                dict_items, new_key, sep=sep).items())\n        else:\n            items.append((new_key, value))\n    return dict(items)\n'"
models/attention/decoders/dynamic_decoder.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Seq2seq layer operations for use in neural networks.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.util import nest\n\nimport tensorflow as tf\n\n\ndef _transpose_batch_time(x):\n    """"""Transpose the batch and time dimensions of a Tensor.\n    Retains as much of the static shape information as possible.\n    Args:\n        x: A tensor of rank 2 or higher.\n    Returns:\n        x transposed along the first two dimensions.\n    Raises:\n        ValueError: if `x` is rank 1 or lower.\n    """"""\n    x_static_shape = x.get_shape()\n    if x_static_shape.ndims is not None and x_static_shape.ndims < 2:\n        raise ValueError(\n            ""Expected input tensor %s to have rank at least 2, but saw shape: %s"" %\n            (x, x_static_shape))\n    x_rank = array_ops.rank(x)\n    x_t = array_ops.transpose(\n        x, array_ops.concat(\n            ([1, 0], math_ops.range(2, x_rank)), axis=0))\n    x_t.set_shape(\n        tensor_shape.TensorShape([\n            x_static_shape[1].value, x_static_shape[0].value\n        ]).concatenate(x_static_shape[2:]))\n    return x_t\n\n\ndef _create_zero_outputs(size, dtype, batch_size):\n    """"""Create a zero outputs Tensor structure.""""""\n    def _t(s):\n        return (s if isinstance(s, ops.Tensor) else constant_op.constant(\n            tensor_shape.TensorShape(s).as_list(),\n            dtype=dtypes.int32,\n            name=""zero_suffix_shape""))\n\n    def _create(s, d):\n        return array_ops.zeros(\n            array_ops.concat(\n                ([batch_size], _t(s)), axis=0), dtype=d)\n\n    return nest.map_structure(_create, size, dtype)\n\n\ndef dynamic_decode(decoder,\n                   output_time_major=False,\n                   impute_finished=False,\n                   maximum_iterations=None,\n                   parallel_iterations=32,\n                   swap_memory=False,\n                   scope=None):\n    """"""Perform dynamic decoding with `decoder`.\n    Args:\n        decoder: A `Decoder` instance.\n        output_time_major: Python boolean.  Default: `False` (batch major).  If\n            `True`, outputs are returned as time major tensors (this mode is faster).\n            Otherwise, outputs are returned as batch major tensors (this adds extra\n            time to the computation).\n        impute_finished: Python boolean.  If `True`, then states for batch\n            entries which are marked as finished get copied through and the\n            corresponding outputs get zeroed out.  This causes some slowdown at\n            each time step, but ensures that the final state and outputs have\n            the correct values and that backprop ignores time steps that were\n            marked as finished.\n        maximum_iterations: `int32` scalar, maximum allowed number of decoding\n            steps.  Default is `None` (decode until the decoder is fully done).\n        parallel_iterations: Argument passed to `tf.while_loop`.\n        swap_memory: Argument passed to `tf.while_loop`.\n        scope: Optional variable scope to use.\n    Returns:\n        `(final_outputs, final_state)`.\n    Raises:\n        TypeError: if `decoder` is not an instance of `Decoder`.\n        ValueError: if maximum_iterations is provided but is not a scalar.\n    """"""\n    if not isinstance(decoder, tf.contrib.seq2seq.Decoder):\n        raise TypeError(""Expected decoder to be type Decoder, but saw: %s"" %\n                        type(decoder))\n\n    with variable_scope.variable_scope(scope or ""decoder"") as varscope:\n        # Properly cache variable values inside the while_loop\n        if varscope.caching_device is None:\n            varscope.set_caching_device(lambda op: op.device)\n\n        if maximum_iterations is not None:\n            maximum_iterations = ops.convert_to_tensor(\n                maximum_iterations, dtype=dtypes.int32, name=""maximum_iterations"")\n            if maximum_iterations.get_shape().ndims != 0:\n                raise ValueError(""maximum_iterations must be a scalar"")\n\n        initial_finished, initial_inputs, initial_state = decoder.initialize()\n\n        zero_outputs = _create_zero_outputs(decoder.output_size,\n                                            decoder.output_dtype,\n                                            decoder.batch_size)\n\n        if maximum_iterations is not None:\n            initial_finished = math_ops.logical_or(\n                initial_finished, 0 >= maximum_iterations)\n        initial_time = constant_op.constant(0, dtype=dtypes.int32)\n\n        def _shape(batch_size, from_shape):\n            if not isinstance(from_shape, tensor_shape.TensorShape):\n                return tensor_shape.TensorShape(None)\n            else:\n                batch_size = tensor_util.constant_value(\n                    ops.convert_to_tensor(\n                        batch_size, name=""batch_size""))\n                return tensor_shape.TensorShape([batch_size]).concatenate(from_shape)\n\n        def _create_ta(s, d):\n            return tensor_array_ops.TensorArray(\n                dtype=d,\n                size=0,\n                dynamic_size=True,\n                element_shape=_shape(decoder.batch_size, s))\n\n        initial_outputs_ta = nest.map_structure(_create_ta, decoder.output_size,\n                                                decoder.output_dtype)\n\n        def condition(unused_time, unused_outputs_ta, unused_state, unused_inputs,\n                      finished):\n            return math_ops.logical_not(math_ops.reduce_all(finished))\n\n        def body(time, outputs_ta, state, inputs, finished):\n            """"""Internal while_loop body.\n            Args:\n                time: scalar int32 tensor.\n                outputs_ta: structure of TensorArray.\n                state: (structure of) state tensors and TensorArrays.\n                inputs: (structure of) input tensors.\n                finished: 1-D bool tensor.\n            Returns:\n                `(time + 1, outputs_ta, next_state, next_inputs, next_finished)`.\n            """"""\n            (next_outputs, decoder_state, next_inputs,\n             decoder_finished) = decoder.step(time, inputs, state)\n            next_finished = math_ops.logical_or(decoder_finished, finished)\n            if maximum_iterations is not None:\n                next_finished = math_ops.logical_or(\n                    next_finished, time + 1 >= maximum_iterations)\n\n            nest.assert_same_structure(state, decoder_state)\n            nest.assert_same_structure(outputs_ta, next_outputs)\n            nest.assert_same_structure(inputs, next_inputs)\n\n            # Zero out output values past finish\n            if impute_finished:\n                emit = nest.map_structure(\n                    lambda out, zero: array_ops.where(finished, zero, out),\n                    next_outputs,\n                    zero_outputs)\n            else:\n                emit = next_outputs\n\n            # Copy through states past finish\n            def _maybe_copy_state(new, cur):\n                # TensorArrays and scalar states get passed through.\n                if isinstance(cur, tensor_array_ops.TensorArray):\n                    pass_through = True\n                else:\n                    new.set_shape(cur.shape)\n                    pass_through = (new.shape.ndims == 0)\n                return new if pass_through else array_ops.where(finished, cur, new)\n\n            if impute_finished:\n                next_state = nest.map_structure(\n                    _maybe_copy_state, decoder_state, state)\n            else:\n                next_state = decoder_state\n\n            outputs_ta = nest.map_structure(lambda ta, out: ta.write(time, out),\n                                            outputs_ta, emit)\n            return (time + 1, outputs_ta, next_state, next_inputs, next_finished)\n\n        res = control_flow_ops.while_loop(\n            condition,\n            body,\n            loop_vars=[\n                initial_time, initial_outputs_ta, initial_state, initial_inputs,\n                initial_finished\n            ],\n            parallel_iterations=parallel_iterations,\n            swap_memory=swap_memory)\n\n        final_outputs_ta = res[1]\n        final_state = res[2]\n\n        final_outputs = nest.map_structure(\n            lambda ta: ta.stack(), final_outputs_ta)\n        if not output_time_major:\n            final_outputs = nest.map_structure(\n                _transpose_batch_time, final_outputs)\n\n    return final_outputs, final_state\n'"
models/ctc/decoders/__init__.py,0,b''
models/ctc/decoders/beam_search_decoder.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Beam search (prefix search) decoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom collections import defaultdict\n\nNEG_INF = -float(""inf"")\nLOG_0 = NEG_INF\nLOG_1 = 0\n\n\ndef _make_new_beam():\n    def fn(): return (NEG_INF, NEG_INF)\n    return defaultdict(fn)\n\n\ndef _logsumexp(*args):\n    """"""\n    Stable log sum exp.\n    """"""\n    if all(a == NEG_INF for a in args):\n        return NEG_INF\n    a_max = np.max(args)\n    lsp = np.log(np.sum(np.exp(a - a_max)\n                        for a in args))\n    return a_max + lsp\n\n\n# def _logsumexp_chainer(a, xp, axis=None):\n#     vmax = xp.amax(a, axis=axis, keepdims=True)\n#     vmax += xp.log(xp.sum(xp.exp(a - vmax),\n#                           axis=axis, keepdims=True, dtype=a.dtype))\n#     return xp.squeeze(vmax, axis=axis)\n\n\nclass BeamSearchDecoder(object):\n    """"""Beam search decoder.\n    Arga:\n        space_index (int): the index of the space label\n        blank_index (int): the index of the blank label\n    """"""\n\n    def __init__(self, space_index, blank_index):\n        self._space = space_index\n        self._blank = blank_index\n\n    def __call__(self, probs, seq_len, beam_width=1, alpha=0., beta=0.):\n        """"""Performs inference for the given output probabilities.\n        Args:\n            probs (np.ndarray): The output probabilities (e.g. post-softmax)\n                for each time step.\n                A tensor of size `[B, T, num_classes]`\n            seq_len (np.ndarray): A tensor of size `[B]`\n            beam_width (int): the size of beam\n            alpha (float): language model weight\n            beta (float): insertion bonus\n        Returns:\n            results (np.ndarray): Best path hypothesis (the output label sequence)\n            scores (np.ndarray): The corresponding negative\n            log-likelihood estimated by the decoder\n        """"""\n        # Convert to log scale\n        log_probs = np.log(probs)\n\n        batch_size, max_time, num_classes = log_probs.shape\n        results = [] * batch_size\n        scores = [] * batch_size\n\n        ##############################\n        # Loop pver batch\n        ##############################\n        for i_batch in range(batch_size):\n            # Elements in the beam are (prefix, (p_blank, p_no_blank))\n            # Initialize the beam with the empty sequence, a probability of\n            # 1 for ending in blank and zero for ending in non-blank\n            # (in log space).\n            beam = [(tuple(), (LOG_1, LOG_0))]\n            time = seq_len[i_batch]\n\n            ##############################\n            # Loop over time\n            ##############################\n            for t in range(time):\n\n                # A default dictionary to store the next step candidates.\n                next_beam = _make_new_beam()\n\n                ##############################\n                # Loop over vocab\n                ##############################\n                for c in range(num_classes):\n                    p_t = log_probs[i_batch, t, c]\n\n                    # The variables p_b and p_nb are respectively the\n                    # probabilities for the prefix given that it ends in a\n                    # blank and does not end in a blank at this time step.\n                    for prefix, (p_b, p_nb) in beam:  # Loop over beam\n\n                        # If we propose a blank the prefix doesn\'t change.\n                        # Only the probability of ending in blank gets updated.\n                        if c == self._blank:\n                            new_p_b, new_p_nb = next_beam[prefix]\n                            new_p_b = _logsumexp(\n                                new_p_b, p_b + p_t, p_nb + p_t)\n                            next_beam[prefix] = (new_p_b, new_p_nb)\n                            continue\n\n                        # Extend the prefix by the new character c and it to the\n                        # beam. Only the probability of not ending in blank gets\n                        # updated.\n                        prefix_end = prefix[-1] if prefix else None\n                        new_prefix = prefix + (c,)\n                        new_p_b, new_p_nb = next_beam[new_prefix]\n                        if c != prefix_end:\n                            new_p_nb = _logsumexp(\n                                new_p_nb, p_b + p_t, p_nb + p_t)\n                        else:\n                            # We don\'t include the previous probability of not ending\n                            # in blank (p_nb) if c is repeated at the end. The CTC\n                            # algorithm merges characters not separated by a\n                            # blank.\n                            new_p_nb = _logsumexp(new_p_nb, p_b + p_t)\n\n                        next_beam[new_prefix] = (new_p_b, new_p_nb)\n\n                        # TODO: add LM score here\n\n                        # If c is repeated at the end we also update the unchanged\n                        # prefix. This is the merging case.\n                        if c == prefix_end:\n                            new_p_b, new_p_nb = next_beam[prefix]\n                            new_p_nb = _logsumexp(new_p_nb, p_nb + p_t)\n                            next_beam[prefix] = (new_p_b, new_p_nb)\n\n                # Sort and trim the beam before moving on to the\n                # next time-step.\n                beam = sorted(next_beam.items(),\n                              key=lambda x: _logsumexp(*x[1]),\n                              reverse=True)\n                beam = beam[:beam_width]\n\n            best_hyp = beam[0]\n            results.append(list(best_hyp[0]))\n            scores.append(-_logsumexp(*best_hyp[1]))\n\n        return np.array(results), np.array(scores)\n'"
models/ctc/decoders/charlm_beam_search_decoder.py,0,b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'
models/ctc/decoders/greedy_decoder.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Greedy (best pass) decoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom itertools import groupby\n\n\nclass GreedyDecoder(object):\n\n    def __init__(self, blank_index):\n        self.blank = blank_index\n\n    def __call__(self, probs, seq_len):\n        """"""\n        Args:\n            probs (np.ndarray): A tensor of size `[B, T, num_classes]`\n            seq_len (np.ndarray): A tensor of size `[B]`\n        Returns:\n            results (np.ndarray): Best path hypothesis, A tensor of size `[B, max_len]`\n        """"""\n        # Convert to log scale\n        log_probs = np.log(probs)\n\n        batch_size = log_probs.shape[0]\n        results = [] * batch_size\n\n        # Pickup argmax class\n        for i_batch in range(batch_size):\n            indices = []\n            time = seq_len[i_batch]\n            for t in range(time):\n                arg_max = np.argmax(log_probs[i_batch][t], axis=0)\n                indices.append(arg_max)\n\n            # Step 1. Collapse repeated labels\n            collapsed_indices = [x[0] for x in groupby(indices)]\n\n            # Step 2. Remove all blank labels\n            best_hyp = [x for x in filter(\n                lambda x: x != self.blank, collapsed_indices)]\n\n            results.append(best_hyp)\n\n        return np.array(results)\n'"
models/encoders/core/__init__.py,0,b''
models/encoders/core/blstm.py,63,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Bidirectional LSTM encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass BLSTMEncoder(object):\n    """"""Bidirectional LSTM encoder.\n    Args:\n        num_units (int): the number of units in each layer\n        num_proj (int): the number of nodes in the projection layer\n        num_layers (int): the number of layers\n        lstm_impl (string, optional): a base implementation of LSTM.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n        use_peephole (bool): if True, use peephole\n        parameter_init (float): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_activation (float): the range of activation clipping (> 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 num_units,\n                 num_proj,\n                 num_layers,\n                 lstm_impl,\n                 use_peephole,\n                 parameter_init,\n                 clip_activation,\n                 time_major=True,\n                 name=\'lstm_encoder\'):\n\n        assert num_proj != 0\n\n        self.num_units = num_units\n        if lstm_impl != \'LSTMCell\':\n            self.num_proj = None\n        else:\n            self.num_proj = num_proj\n        # TODO: fix this\n        self.num_layers = num_layers\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n        self.parameter_init = parameter_init\n        self.clip_activation = clip_activation\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size\n                    `[T, B, num_units (num_proj)]`\n                otherwise, `[B, T, num_units (num_proj)]`\n            final_state: A final hidden state of the encoder\n        """"""\n        # inputs = tf.nn.dropout(inputs, keep_prob)\n\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        if self.lstm_impl == \'BasicLSTMCell\':\n            outputs, final_state = basiclstmcell(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMCell\':\n            outputs, final_state = lstmcell(\n                self.num_units, self.num_proj, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockCell\':\n            outputs, final_state = lstmblockcell(\n                self.num_units, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockFusedCell\':\n            outputs, final_state = lstmblockfusedcell(\n                self.num_units, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'CudnnLSTM\':\n            outputs, final_state = cudnnlstm(\n                self.num_units, self.num_layers, self.parameter_init,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        else:\n            raise IndexError(\n                \'lstm_impl is ""BasicLSTMCell"" or ""LSTMCell"" or \' +\n                \'""LSTMBlockCell"" or ""LSTMBlockFusedCell"" or \' +\n                \'""CudnnLSTM"".\')\n\n        return outputs, final_state\n\n\ndef basiclstmcell(num_units, num_layers,\n                  inputs, inputs_seq_len, keep_prob,\n                  initializer, time_major, num_layers_sub=None):\n    """"""\n    Args:\n        num_units (int): the number of units in each layer\n        num_layers (int): the number of layers\n        inputs (placeholder): A tensor of size`[B, T, input_dim]`\n        inputs_seq_len (placeholder): A tensor of size` [B]`\n        keep_prob (placeholder, float): A probability to keep nodes\n            in the hidden-hidden connection\n        initializer ():\n        time_major (bool): if True, time-major computation will be performed\n        num_layers_sub (int, optional): the number of layers of the sub task\n    """"""\n    if time_major:\n        # Convert form batch-major to time-major\n        inputs = tf.transpose(inputs, [1, 0, 2])\n\n    outputs = inputs\n    for i_layer in range(1, num_layers + 1, 1):\n        with tf.variable_scope(\'blstm_hidden\' + str(i_layer),\n                               initializer=initializer) as scope:\n\n            lstm_fw = tf.contrib.rnn.BasicLSTMCell(\n                num_units,\n                forget_bias=1.0,\n                state_is_tuple=True,\n                activation=tf.tanh)\n            lstm_bw = tf.contrib.rnn.BasicLSTMCell(\n                num_units,\n                forget_bias=1.0,\n                state_is_tuple=True,\n                activation=tf.tanh)\n\n            # Dropout for the hidden-hidden connections\n            lstm_fw = tf.contrib.rnn.DropoutWrapper(\n                lstm_fw, output_keep_prob=keep_prob)\n            lstm_bw = tf.contrib.rnn.DropoutWrapper(\n                lstm_bw, output_keep_prob=keep_prob)\n\n            (outputs_fw, outputs_bw), final_state = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=lstm_fw,\n                cell_bw=lstm_bw,\n                inputs=outputs,\n                sequence_length=inputs_seq_len,\n                dtype=tf.float32,\n                time_major=time_major,\n                scope=scope)\n            # NOTE: initial states are zero states by default\n\n            outputs = tf.concat(axis=2, values=[outputs_fw, outputs_bw])\n\n            if num_layers_sub is not None and i_layer == num_layers_sub:\n                outputs_sub = outputs\n                final_state_sub = final_state\n\n    if num_layers_sub is not None:\n        return outputs, final_state, outputs_sub, final_state_sub\n    else:\n        return outputs, final_state\n\n\ndef lstmcell(num_units, num_proj, num_layers, use_peephole, clip_activation,\n             inputs, inputs_seq_len, keep_prob,\n             initializer, time_major,\n             num_layers_sub=None):\n    """"""\n    Args:\n        num_units (int): the number of units in each layer\n        num_proj (int):\n        num_layers (int): the number of layers\n        use_peephole (bool):\n        clip_activation (float):\n        inputs (placeholder): A tensor of size`[B, T, input_dim]`\n        inputs_seq_len (placeholder): A tensor of size` [B]`\n        keep_prob (placeholder, float): A probability to keep nodes\n            in the hidden-hidden connection\n        initializer ():\n        time_major (bool): if True, time-major computation will be performed\n        num_layers_sub (int, optional): the number of layers of the sub task\n    """"""\n    if time_major:\n        # Convert form batch-major to time-major\n        inputs = tf.transpose(inputs, [1, 0, 2])\n\n    outputs = inputs\n    for i_layer in range(1, num_layers + 1, 1):\n        with tf.variable_scope(\'blstm_hidden\' + str(i_layer),\n                               initializer=initializer) as scope:\n\n            lstm_fw = tf.contrib.rnn.LSTMCell(\n                num_units,\n                use_peepholes=use_peephole,\n                cell_clip=clip_activation,\n                num_proj=num_proj,\n                forget_bias=1.0,\n                state_is_tuple=True)\n            lstm_bw = tf.contrib.rnn.LSTMCell(\n                num_units,\n                use_peepholes=use_peephole,\n                cell_clip=clip_activation,\n                num_proj=num_proj,\n                forget_bias=1.0,\n                state_is_tuple=True)\n\n            # Dropout for the hidden-hidden connections\n            lstm_fw = tf.contrib.rnn.DropoutWrapper(\n                lstm_fw, output_keep_prob=keep_prob)\n            lstm_bw = tf.contrib.rnn.DropoutWrapper(\n                lstm_bw, output_keep_prob=keep_prob)\n\n            (outputs_fw, outputs_bw), final_state = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=lstm_fw,\n                cell_bw=lstm_bw,\n                inputs=outputs,\n                sequence_length=inputs_seq_len,\n                dtype=tf.float32,\n                time_major=time_major,\n                scope=scope)\n            # NOTE: initial states are zero states by default\n\n            outputs = tf.concat(axis=2, values=[outputs_fw, outputs_bw])\n\n            if num_layers_sub is not None and i_layer == num_layers_sub:\n                outputs_sub = outputs\n                final_state_sub = final_state\n\n    if num_layers_sub is not None:\n        return outputs, final_state, outputs_sub, final_state_sub\n    else:\n        return outputs, final_state\n\n\ndef lstmblockcell(num_units, num_layers, use_peephole, clip_activation,\n                  inputs, inputs_seq_len, keep_prob,\n                  initializer, time_major,\n                  num_layers_sub=None):\n    """"""\n    Args:\n        num_units (int): the number of units in each layer\n        num_proj (int):\n        num_layers (int): the number of layers\n        use_peephole (bool):\n        clip_activation (float):\n        inputs (placeholder): A tensor of size`[B, T, input_dim]`\n        inputs_seq_len (placeholder): A tensor of size` [B]`\n        keep_prob (placeholder, float): A probability to keep nodes\n            in the hidden-hidden connection\n        initializer ():\n        time_major (bool): if True, time-major computation will be performed\n        num_layers_sub (int, optional): the number of layers of the sub task\n    """"""\n    if time_major:\n        # Convert form batch-major to time-major\n        inputs = tf.transpose(inputs, [1, 0, 2])\n\n    outputs = inputs\n    for i_layer in range(1, num_layers + 1, 1):\n        with tf.variable_scope(\'blstm_hidden\' + str(i_layer),\n                               initializer=initializer) as scope:\n\n            if tf.__version__ == \'1.3.0\':\n                lstm_fw = tf.contrib.rnn.LSTMBlockCell(\n                    num_units,\n                    forget_bias=1.0,\n                    clip_cell=clip_activation,\n                    use_peephole=use_peephole)\n                lstm_bw = tf.contrib.rnn.LSTMBlockCell(\n                    num_units,\n                    forget_bias=1.0,\n                    clip_cell=clip_activation,\n                    use_peephole=use_peephole)\n            else:\n                lstm_fw = tf.contrib.rnn.LSTMBlockCell(\n                    num_units,\n                    forget_bias=1.0,\n                    use_peephole=use_peephole)\n                lstm_bw = tf.contrib.rnn.LSTMBlockCell(\n                    num_units,\n                    forget_bias=1.0,\n                    use_peephole=use_peephole)\n\n            # Dropout for the hidden-hidden connections\n            lstm_fw = tf.contrib.rnn.DropoutWrapper(\n                lstm_fw, output_keep_prob=keep_prob)\n            lstm_bw = tf.contrib.rnn.DropoutWrapper(\n                lstm_bw, output_keep_prob=keep_prob)\n\n            (outputs_fw, outputs_bw), final_state = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=lstm_fw,\n                cell_bw=lstm_bw,\n                inputs=outputs,\n                sequence_length=inputs_seq_len,\n                dtype=tf.float32,\n                time_major=time_major,\n                scope=scope)\n            # NOTE: initial states are zero states by default\n\n            outputs = tf.concat(axis=2, values=[outputs_fw, outputs_bw])\n\n            if num_layers_sub is not None and i_layer == num_layers_sub:\n                outputs_sub = outputs\n                final_state_sub = final_state\n\n    if num_layers_sub is not None:\n        return outputs, final_state, outputs_sub, final_state_sub\n    else:\n        return outputs, final_state\n\n\ndef lstmblockfusedcell(num_units, num_layers, use_peephole, clip_activation,\n                       inputs, inputs_seq_len, keep_prob,\n                       initializer, time_major,\n                       num_layers_sub=None):\n    """"""\n    Args:\n        num_units (int): the number of units in each layer\n        num_proj (int):\n        num_layers (int): the number of layers\n        use_peephole (bool):\n        clip_activation (float):\n        inputs (placeholder): A tensor of size`[B, T, input_dim]`\n        inputs_seq_len (placeholder): A tensor of size` [B]`\n        keep_prob (placeholder, float): A probability to keep nodes\n            in the hidden-hidden connection\n        initializer ():\n        time_major (bool): if True, time-major computation will be performed\n        num_layers_sub (int, optional): the number of layers of the sub task\n    """"""\n    # TODO: add time-major\n\n    outputs = inputs\n    for i_layer in range(1, num_layers + 1, 1):\n        with tf.variable_scope(\'blstm_hidden\' + str(i_layer),\n                               initializer=initializer) as scope:\n\n            if tf.__version__ == \'1.3.0\':\n                lstm_fw = tf.contrib.rnn.LSTMBlockFusedCell(\n                    num_units,\n                    forget_bias=1.0,\n                    cell_clip=clip_activation,\n                    use_peephole=use_peephole)\n                lstm_bw = tf.contrib.rnn.LSTMBlockFusedCell(\n                    num_units,\n                    forget_bias=1.0,\n                    cell_clip=clip_activation,\n                    use_peephole=use_peephole)\n            else:\n                lstm_fw = tf.contrib.rnn.LSTMBlockFusedCell(\n                    num_units,\n                    forget_bias=1.0,\n                    use_peephole=use_peephole)\n                lstm_bw = tf.contrib.rnn.LSTMBlockFusedCell(\n                    num_units,\n                    forget_bias=1.0,\n                    use_peephole=use_peephole)\n\n            lstm_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_bw)\n\n            with tf.variable_scope(""lstm_fw"") as scope:\n                outputs_fw, final_state_fw = lstm_fw(\n                    outputs, dtype=tf.float32,\n                    sequence_length=inputs_seq_len, scope=scope)\n            with tf.variable_scope(""lstm_bw"") as scope:\n                outputs_bw, final_state_bw = lstm_bw(\n                    outputs, dtype=tf.float32,\n                    sequence_length=inputs_seq_len, scope=scope)\n            final_state = tf.contrib.rnn.LSTMStateTuple(\n                h=final_state_fw, c=final_state_bw)\n\n            # TODO: add dropout\n\n            outputs = tf.concat(axis=2, values=[outputs_fw, outputs_bw])\n\n            # if self.num_proj > 0:\n            #     outputs = tf.contrib.layers.fully_connected(\n            #         activation_fn=None, inputs=outputs,\n            #         num_outputs=self.num_proj, scope=""projection"")\n            # TODO: add projection layers\n\n    return outputs, final_state\n\n\ndef cudnnlstm(num_units, num_layers, parameter_init,\n              inputs, inputs_seq_len, keep_prob,\n              initializer, time_major,\n              num_layers_sub=None):\n    """"""\n    Args:\n        num_units (int): the number of units in each layer\n        num_layers (int): the number of layers\n        parameter_init (float):\n        inputs (placeholder): A tensor of size`[B, T, input_dim]`\n        inputs_seq_len (placeholder): A tensor of size` [B]`\n        keep_prob (placeholder, float): A probability to keep nodes\n            in the hidden-hidden connection\n        initializer ():\n        time_major (bool): if True, time-major computation will be performed\n        num_layers_sub (int, optional): the number of layers of the sub task\n    """"""\n    batch_size = tf.shape(inputs)[0]\n    input_size = tf.shape(inputs)[2]\n\n    # TODO: add time-major\n\n    stacked_lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\n        num_layers=num_layers,\n        num_units=num_units,\n        input_size=input_size,\n        input_mode=\'auto_select\',\n        direction=\'bidirectional\',\n        dropout=0,\n        seed=0)\n\n    params_size_t = stacked_lstm.params_size()\n    init_h = tf.zeros(\n        [num_layers * 2, batch_size, num_units],\n        dtype=tf.float32, name=""init_lstm_h"")\n    init_c = tf.zeros(\n        [num_layers * 2, batch_size, num_units],\n        dtype=tf.float32, name=""init_lstm_c"")\n    # cudnn_params = tf.Variable(tf.random_uniform(\n    #     [params_size_t], -self.parameter_init, self.parameter_init),\n    #     validate_shape=False, name=""lstm_params"", trainable=True)\n    lstm_params = tf.get_variable(\n        ""lstm_params"",\n        initializer=tf.random_uniform(\n            [params_size_t], -parameter_init, parameter_init),\n        validate_shape=False,\n        trainable=True)\n    # TODO is_training=is_training should be changed!\n\n    # outputs = tf.contrib.layers.fully_connected(\n    #     activation_fn=None, inputs=outputs,\n    #     num_outputs=nproj, scope=""projection"")\n    # TODO: add projection layers\n\n    outputs, output_h, output_c = stacked_lstm(\n        input_data=inputs,\n        input_h=init_h,\n        input_c=init_c,\n        params=lstm_params,\n        is_training=True)\n    # NOTE: outputs: `[T, B, num_units * num_direction]`\n\n    final_state = tf.contrib.rnn.LSTMStateTuple(h=output_h, c=output_c)\n    # TODO: add dropout\n\n    return outputs, final_state\n'"
models/encoders/core/cldnn_wang.py,28,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""CLDNN (CNN + bidirectional LSTM + DNN) encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\nfrom models.encoders.core.blstm import basiclstmcell, lstmcell, lstmblockcell, lstmblockfusedcell, cudnnlstm\n\n############################################################\n# Architecture: (feature map, kernel, stride)\n\n# CNN1: (32, 11*21, (3,2)) * 1 layer\n# ReLU\n# (Batch normalization)\n# max pooling\n# dropout\n\n# CNN2: (32, 11*11, (1,2)) * 1 layer\n# ReLU\n# (Batch normalization)\n# max pooling\n# dropout\n\n# CNN3:(96, 3*3, (1,1)) * 1 layer\n# ReLU\n# (Batch normalization)\n# max pooling\n# dropout\n\n# BLSTM: 896 * 2 layers\n\n# fc: 896 * 1 layer\n# dropout\n# fc: 74 * 1 layer\n\n# softmax\n############################################################\n\n\nclass CLDNNEncoder(object):\n    """"""CLDNN (CNN + bidirectional LSTM + DNN) encoder.\n        This implementation is based on\n         https://arxiv.org/abs/1702.07793.\n             Wang, Yisen, et al.\n             ""Residual convolutional CTC networks for automatic speech recognition.""\n             arXiv preprint arXiv:1702.07793 (2017).\n    Args:\n        input_size (int): the dimensions of input vectors\xef\xbc\x8e\n            This is expected to be num_channels * 3 (static + \xce\x94 + \xce\x94\xce\x94)\n        splice (int): frames to splice\n        num_stack (int): the number of frames to stack\n        num_units (int): the number of units in each layer\n        num_proj (int): the number of nodes in the projection layer\n        num_layers (int): the number of layers\n        lstm_impl (string, optional): a base implementation of LSTM.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n        use_peephole (bool): if True, use peephole\n        parameter_init (float): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_activation (float): the range of activation clipping (> 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 num_units,\n                 num_proj,\n                 num_layers,\n                 lstm_impl,\n                 use_peephole,\n                 parameter_init,\n                 clip_activation,\n                 time_major=False,\n                 name=\'cldnn_wang_encoder\'):\n\n        assert num_proj != 0\n        assert input_size % 3 == 0\n\n        self.num_channels = input_size // 3\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_units = num_units\n        if lstm_impl != \'LSTMCell\':\n            self.num_proj = None\n        else:\n            self.num_proj = num_proj\n        # TODO: fix this\n        self.num_layers = num_layers\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n        self.parameter_init = parameter_init\n        self.clip_activation = clip_activation\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, T, input_size (num_channels * (splice * num_stack) * 3)]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size\n                    `[T, B, num_units (num_proj)]`\n                otherwise, `[B, T, num_units (num_proj)]`\n            final_state: A final hidden state of the encoder\n        """"""\n        # inputs: 3D tensor `[B, T, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # For debug\n        # print(input_dim)\n        # print(self.num_channels)\n        # print(self.splice)\n        # print(self.num_stack)\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B * T, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size * max_time, self.num_channels, self.splice * self.num_stack, 3])\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        with tf.variable_scope(\'CNN1\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[11, 21, 3, 32],\n                                stride=[3, 2],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[1, 1],\n                              stride=[1, 1],\n                              name=\'max_pool\')\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        with tf.variable_scope(\'CNN2\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[11, 11, 32, 32],\n                                stride=[1, 2],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[1, 1],\n                              stride=[1, 1],\n                              name=\'max_pool\')\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        with tf.variable_scope(\'CNN3\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 32, 96],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[1, 1],\n                              stride=[1, 1],\n                              name=\'max_pool\')\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        # Reshape to 3D tensor `[B, T, new_h * new_w * C_out]`\n        inputs = tf.reshape(\n            inputs, shape=[batch_size, max_time, np.prod(inputs.shape.as_list()[-3:])])\n\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        if self.lstm_impl == \'BasicLSTMCell\':\n            outputs, final_state = basiclstmcell(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMCell\':\n            outputs, final_state = lstmcell(\n                self.num_units, self.num_proj, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockCell\':\n            outputs, final_state = lstmblockcell(\n                self.num_units, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockFusedCell\':\n            outputs, final_state = lstmblockfusedcell(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'CudnnLSTM\':\n            outputs, final_state = cudnnlstm(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n        else:\n            raise IndexError(\n                \'lstm_impl is ""BasicLSTMCell"" or ""LSTMCell"" or \' +\n                \'""LSTMBlockCell"" or ""LSTMBlockFusedCell"" or \' +\n                \'""CudnnLSTM"".\')\n\n        # Reshape to 2D tensor `[B * T (T * B), output_dim]`\n        output_dim = outputs.shape.as_list()[-1]\n        outputs = tf.reshape(\n            outputs, shape=[batch_size * max_time, output_dim])\n\n        with tf.variable_scope(\'fc1\') as scope:\n            outputs = tf.contrib.layers.fully_connected(\n                inputs=outputs,\n                num_outputs=896,\n                activation_fn=tf.nn.relu,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n            outputs = tf.nn.dropout(outputs, keep_prob)\n\n        with tf.variable_scope(\'fc2\') as scope:\n            outputs = tf.contrib.layers.fully_connected(\n                inputs=outputs,\n                num_outputs=74,\n                activation_fn=tf.nn.relu,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n\n        output_dim = outputs.shape.as_list()[-1]\n        if self.time_major:\n            # Reshape back to 3D tensor `[T, B, 74]`\n            outputs = tf.reshape(\n                outputs, shape=[max_time, batch_size, output_dim])\n        else:\n            # Reshape back to 3D tensor `[B, T, 74]`\n            outputs = tf.reshape(\n                outputs, shape=[batch_size, max_time, output_dim])\n\n        return outputs, final_state\n'"
models/encoders/core/cnn_util.py,25,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Utilities for CNN-like layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef max_pool(bottom, pooling_size, stride=[2, 2], name=\'max_pool\'):\n    """"""A max pooling layer.\n    Args:\n        bottom: A tensor of size `[B (* T), H, W, C_in]`\n        pooling_size (list): A list of `[pool_H, pool_w]`\n        stride (list, optional): A list of `[stride_H, stride_W]`\n        name (string, optional): A layer name\n    Returns:\n        outputs: A tensor of size `[B (* T), H, W, C_out]`\n    """"""\n    return tf.nn.max_pool(\n        bottom,\n        ksize=[1, pooling_size[0], pooling_size[1], 1],\n        strides=[1, stride[0], stride[1], 1],\n        padding=\'SAME\',\n        name=name)\n\n\ndef avg_pool(bottom, pooling_size, stride=[2, 2], name=\'avg_pool\'):\n    """"""An average pooling layer.\n    Args:\n        bottom: A tensor of size `[B (* T), H, W, C_in]`\n        pooling_size (list): A list of `[pool_H, pool_w]`\n        stride (list, optional): A list of `[stride_H, stride_W]`\n        name (string, optional): A layer name\n    Returns:\n        outputs: A tensor of size `[B (* T), H, W, C_out]`\n    """"""\n    return tf.nn.avg_pool(\n        bottom,\n        ksize=[1, pooling_size[0], pooling_size[1], 1],\n        strides=[1, stride[0], stride[1], 1],\n        padding=\'SAME\',\n        name=name)\n\n\ndef conv_layer(bottom, filter_size, stride=[1, 1], parameter_init=0.1,\n               activation=None, name=\'conv\'):\n    """"""A convolutional layer\n    Args:\n        bottom: A tensor of size `[B (* T), H, W, C_in]`\n        filter_size (list): A list of `[H, W, C_in, C_out]`\n        stride (list, optional): A list of `[stride_H, stride_W]`\n        parameter_init (float, optional):\n        activation (string, optional): relu\n        name (string, optional): A layer name\n    Returns:\n        outputs: A tensor of size `[B (* T), H, W, C_out]`\n    """"""\n    assert len(filter_size) == 4\n    assert len(stride) == 2\n\n    with tf.variable_scope(name):\n        W = tf.Variable(tf.truncated_normal(shape=filter_size,\n                                            stddev=parameter_init),\n                        name=\'weight\')\n        b = tf.Variable(tf.zeros(shape=filter_size[-1]), name=\'bias\')\n        conv_bottom = tf.nn.conv2d(bottom, W,\n                                   strides=[1, stride[0], stride[1], 1],\n                                   padding=\'SAME\')\n        outputs = tf.nn.bias_add(conv_bottom, b)\n\n        if activation is None:\n            return outputs\n        elif activation == \'relu\':\n            return tf.nn.relu(outputs)\n        elif activation == \'prelu\':\n            raise NotImplementedError\n        elif activation == \'maxout\':\n            raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n\ndef batch_normalization(tensor, is_training=True, epsilon=0.001, momentum=0.9,\n                        fused_batch_norm=False, name=None):\n    """"""Performs batch normalization on given 4-D tensor.\n    Args:\n        tensor:\n        epsilon:\n        momentum:\n        fused_batch_norm:\n        name:\n    Returns:\n\n    The features are assumed to be in NHWC format. Noe that you need to\n    run UPDATE_OPS in order for this function to perform correctly, e.g.:\n\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n      train_op = optimizer.minimize(loss)\n\n    Based on: https://arxiv.org/abs/1502.03167\n    """"""\n    with tf.variable_scope(name, default_name=""batch_norm""):\n        channels = tensor.shape.as_list()[-1]\n        axes = list(range(tensor.shape.ndims - 1))  # [0,1,2]\n\n        beta = tf.get_variable(\n            \'beta\', channels, initializer=tf.zeros_initializer())\n        gamma = tf.get_variable(\n            \'gamma\', channels, initializer=tf.ones_initializer())\n        # NOTE: these are trainable parameters\n\n        avg_mean = tf.get_variable(\n            ""avg_mean"", channels, initializer=tf.zeros_initializer(),\n            trainable=False)\n        avg_variance = tf.get_variable(\n            ""avg_variance"", channels, initializer=tf.ones_initializer(),\n            trainable=False)\n\n        if is_training:\n            if fused_batch_norm:\n                mean, variance = None, None\n            else:\n                mean, variance = tf.nn.moments(tensor, axes=axes)\n        else:\n            mean, variance = avg_mean, avg_variance\n\n        if fused_batch_norm:\n            tensor, mean, variance = tf.nn.fused_batch_norm(\n                tensor, scale=gamma, offset=beta, mean=mean, variance=variance,\n                epsilon=epsilon, is_training=is_training)\n        else:\n            tensor = tf.nn.batch_normalization(\n                tensor, mean, variance, beta, gamma, epsilon)\n\n        if is_training:\n            update_mean = tf.assign(\n                avg_mean, avg_mean * momentum + mean * (1.0 - momentum))\n            update_variance = tf.assign(\n                avg_variance, avg_variance * momentum + variance * (1.0 - momentum))\n\n            # Ops before gradient update\n            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_mean)\n            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_variance)\n\n    return tensor\n'"
models/encoders/core/cnn_zhang.py,16,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""CNN encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\n\n############################################################\n# Architecture: (feature map, kernel, stride)\n\n# CNN1: (128, 3*5, (1,1)) * 1 layer\n# ReLU\n# (Batch normalization)\n# max pooling\n# dropout\n\n# CNN2-5: (128, 3*5, (1,1)) * 4 layer\n# ReLU\n# (Batch normalization)\n# dropout\n\n# CNN6-10: (256, 3*5, (1,1)) * 5 layer\n# ReLU\n# (Batch normalization)\n# dropout\n\n# fc: 1024 * 3 layer\n# (dropout, except for the last layer)\n\n# softmax\n############################################################\n\n\nclass CNNEncoder(object):\n    """"""CNN encoder.\n       This implementation is based on\n           https://arxiv.org/abs/1701.02720.\n               Zhang, Ying, et al.\n               ""Towards end-to-end speech recognition with deep convolutional\n                neural networks.""\n               arXiv preprint arXiv:1701.02720 (2017).\n    Args:\n        input_size (int): the dimensions of input vectors\n        splice (int): frames to splice. Default is 1 frame.\n        num_stack (int): the number of frames to stack\n        parameter_init (float, optional): Range of uniform distribution to\n            initialize weight parameters\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 parameter_init,\n                 time_major,\n                 name=\'cnn_zhang_encoder\'):\n\n        self.num_channels = input_size // 3\n        self.splice = splice\n        self.num_stack = num_stack\n        self.parameter_init = parameter_init\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, T, input_size (num_channels * splice * num_stack * 3)]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size `[T, B, output_dim]`\n                otherwise, `[B, T, output_dim]`\n            final_state: None\n        """"""\n        # inputs: 3D tensor `[B, T, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # For debug\n        # print(input_dim)\n        # print(self.num_channels)\n        # print(self.splice)\n        # print(self.num_stack)\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B * T, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size * max_time, self.num_channels, self.splice * self.num_stack, 3])\n\n        # Choose the activation function\n        activation = \'relu\'\n        # activation = \'prelu\'\n        # activation = \'maxout\'\n        # TODO: add prelu and maxout layers\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        # 1-4th layers\n        for i_layer in range(1, 5, 1):\n            with tf.variable_scope(\'CNN%d\' % i_layer):\n                input_channels = inputs.shape.as_list()[-1]\n                inputs = conv_layer(inputs,\n                                    filter_size=[3, 5, input_channels, 128],\n                                    stride=[1, 1],\n                                    parameter_init=self.parameter_init,\n                                    activation=activation,\n                                    name=\'conv\')\n                # inputs = batch_normalization(inputs, is_training=is_training)\n                if i_layer == 1:\n                    inputs = max_pool(inputs,\n                                      pooling_size=[3, 1],\n                                      stride=[3, 1],\n                                      name=\'pool\')\n                inputs = tf.nn.dropout(inputs, keep_prob)\n\n        # 5-10th layers\n        for i_layer in range(5, 11, 1):\n            with tf.variable_scope(\'CNN%d\' % i_layer):\n                input_channels = inputs.shape.as_list()[-1]\n                inputs = conv_layer(inputs,\n                                    filter_size=[3, 5, input_channels, 256],\n                                    stride=[1, 1],\n                                    parameter_init=self.parameter_init,\n                                    activation=activation,\n                                    name=\'conv\')\n                # inputs = batch_normalization(inputs, is_training=is_training)\n                # NOTE: No poling\n                inputs = tf.nn.dropout(inputs, keep_prob)\n\n        # Reshape to 2D tensor `[B * T, new_h * new_w * C_out]`\n        outputs = tf.reshape(\n            inputs, shape=[batch_size * max_time, np.prod(inputs.shape.as_list()[-3:])])\n\n        # 11-14th layers\n        for i_layer in range(1, 4, 1):\n            with tf.variable_scope(\'fc%d\' % i_layer) as scope:\n                outputs = tf.contrib.layers.fully_connected(\n                    inputs=outputs,\n                    num_outputs=1024,\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=scope)\n                if i_layer != 3:\n                    outputs = tf.nn.dropout(outputs, keep_prob)\n\n        # Reshape back to 3D tensor `[B, T, 1024]`\n        logits = tf.reshape(\n            outputs, shape=[batch_size, max_time, 1024])\n\n        if self.time_major:\n            # Convert to time-major: `[T, B, 1024]\'\n            logits = tf.transpose(logits, [1, 0, 2])\n\n        return logits, None\n'"
models/encoders/core/gru.py,18,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""GRU encoders.""""""\n\nimport tensorflow as tf\n\n\nclass GRUEncoder(object):\n    """"""Unidirectional GRU encoder.\n    Args:\n        num_units (int): the number of units in each layer\n        num_layers (int): the number of layers\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 num_units,\n                 num_layers,\n                 parameter_init,\n                 time_major=False,\n                 name=\'gru_encoder\'):\n\n        self.num_units = num_units\n        self.num_layers = num_layers\n        self.parameter_init = parameter_init\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states, a tensor of size `[T, B, num_units]`\n            final_state: A final hidden state of the encoder\n        """"""\n        if self.time_major:\n            # Convert form batch-major to time-major\n            inputs = tf.transpose(inputs, [1, 0, 2])\n\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        # Hidden layers\n        gru_list = []\n        with tf.variable_scope(\'multi_gru\', initializer=initializer) as scope:\n            for i_layer in range(1, self.num_layers + 1, 1):\n\n                gru = tf.contrib.rnn.GRUCell(self.num_units)\n\n                # Dropout for the hidden-hidden connections\n                gru = tf.contrib.rnn.DropoutWrapper(\n                    gru, output_keep_prob=keep_prob)\n\n                gru_list.append(gru)\n\n            # Stack multiple cells\n            stacked_gru = tf.contrib.rnn.MultiRNNCell(\n                gru_list, state_is_tuple=True)\n\n            # Ignore 2nd return (the last state)\n            outputs, final_state = tf.nn.dynamic_rnn(\n                cell=stacked_gru,\n                inputs=inputs,\n                sequence_length=inputs_seq_len,\n                dtype=tf.float32,\n                time_major=self.time_major)\n            # NOTE: initial states are zero states by default\n\n        return outputs, final_state\n\n\nclass BGRUEncoder(object):\n    """"""Bidirectional GRU encoder.\n    Args:\n        num_units (int): the number of units in each layer\n        num_layers (int): the number of layers\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of the encoder\n    """"""\n\n    def __init__(self,\n                 num_units,\n                 num_layers,\n                 parameter_init,\n                 time_major=False,\n                 name=\'bgru_encoder\'):\n\n        self.num_units = num_units\n        self.num_layers = num_layers\n        self.parameter_init = parameter_init\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states, a tensor of size\xe3\x80\x80`[T, B, num_units]`\n            final_state: A final hidden state of the encoder\n        """"""\n        if self.time_major:\n            # Convert form batch-major to time-major\n            inputs = tf.transpose(inputs, [1, 0, 2])\n\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        # Hidden layers\n        outputs = inputs\n        for i_layer in range(1, self.num_layers + 1, 1):\n            with tf.variable_scope(\'bgru_hidden\' + str(i_layer),\n                                   initializer=initializer) as scope:\n\n                gru_fw = tf.contrib.rnn.GRUCell(self.num_units)\n                gru_bw = tf.contrib.rnn.GRUCell(self.num_units)\n\n                # Dropout for the hidden-hidden connections\n                gru_fw = tf.contrib.rnn.DropoutWrapper(\n                    gru_fw, output_keep_prob=keep_prob)\n                gru_bw = tf.contrib.rnn.DropoutWrapper(\n                    gru_bw, output_keep_prob=keep_prob)\n\n                # Ignore 2nd return (the last state)\n                (outputs_fw, outputs_bw), final_state = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=gru_fw,\n                    cell_bw=gru_bw,\n                    inputs=outputs,\n                    sequence_length=inputs_seq_len,\n                    dtype=tf.float32,\n                    time_major=self.time_major,\n                    scope=scope)\n                # NOTE: initial states are zero states by default\n\n                outputs = tf.concat(axis=2, values=[outputs_fw, outputs_bw])\n\n        return outputs, final_state\n'"
models/encoders/core/lstm.py,40,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Unidirectional LSTM encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass LSTMEncoder(object):\n    """"""Unidirectional LSTM encoder.\n    Args:\n        num_units (int): the number of units in each layer\n        num_proj (int): the number of nodes in the projection layer\n        num_layers (int): the number of layers\n        lstm_impl (string, optional): a base implementation of LSTM.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n        use_peephole (bool): if True, use peephole\n        parameter_init (float): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_activation (float): the range of activation clipping (> 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 num_units,\n                 num_proj,\n                 num_layers,\n                 lstm_impl,\n                 use_peephole,\n                 parameter_init,\n                 clip_activation,\n                 time_major=False,\n                 name=\'lstm_encoder\'):\n\n        assert num_proj != 0\n\n        self.num_units = num_units\n        if lstm_impl != \'LSTMCell\':\n            self.num_proj = None\n        else:\n            self.num_proj = num_proj\n        # TODO: fix this\n        self.num_layers = num_layers\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n        self.parameter_init = parameter_init\n        self.clip_activation = clip_activation\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size\n                    `[T, B, num_units (num_proj)]`\n                otherwise, `[B, T, num_units (num_proj)]`\n            final_state: A final hidden state of the encoder\n        """"""\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        if self.lstm_impl == \'BasicLSTMCell\':\n            outputs, final_state = basiclstmcell(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMCell\':\n            outputs, final_state = lstmcell(\n                self.num_units, self.num_proj, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockCell\':\n            outputs, final_state = lstmblockcell(\n                self.num_units, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockFusedCell\':\n            outputs, final_state = lstmblockfusedcell(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'CudnnLSTM\':\n            outputs, final_state = cudnnlstm(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n        else:\n            raise IndexError(\n                \'lstm_impl is ""BasicLSTMCell"" or ""LSTMCell"" or \' +\n                \'""LSTMBlockCell"" or ""LSTMBlockFusedCell"" or \' +\n                \'""CudnnLSTM"".\')\n\n        return outputs, final_state\n\n\ndef basiclstmcell(num_units, num_layers, inputs, inputs_seq_len,\n                  keep_prob, initializer, time_major, num_layers_sub=None):\n\n    if time_major:\n        # Convert form batch-major to time-major\n        inputs = tf.transpose(inputs, [1, 0, 2])\n\n    lstm_list = []\n    with tf.variable_scope(\'multi_lstm\', initializer=initializer) as scope:\n        for i_layer in range(1, num_layers + 1, 1):\n\n            lstm = tf.contrib.rnn.BasicLSTMCell(\n                num_units,\n                forget_bias=1.0,\n                state_is_tuple=True,\n                activation=tf.tanh)\n\n            # Dropout for the hidden-hidden connections\n            lstm = tf.contrib.rnn.DropoutWrapper(\n                lstm, output_keep_prob=keep_prob)\n\n            lstm_list.append(lstm)\n\n            if num_layers_sub is not None and i_layer == num_layers_sub:\n                lstm_list_sub = lstm_list\n\n        # Stack multiple cells\n        stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n            lstm_list, state_is_tuple=True)\n\n        # Ignore 2nd return (the last state)\n        outputs, final_state = tf.nn.dynamic_rnn(\n            cell=stacked_lstm,\n            inputs=inputs,\n            sequence_length=inputs_seq_len,\n            dtype=tf.float32,\n            time_major=time_major,\n            scope=scope)\n        # NOTE: initial states are zero states by default\n\n    if num_layers_sub is not None:\n        with tf.variable_scope(\'multi_lstm\', initializer=initializer, reuse=True) as scope:\n            # Stack multiple cells\n            stacked_lstm_sub = tf.contrib.rnn.MultiRNNCell(\n                lstm_list_sub, state_is_tuple=True)\n\n            # Ignore 2nd return (the last state)\n            outputs_sub, final_state_sub = tf.nn.dynamic_rnn(\n                cell=stacked_lstm_sub,\n                inputs=inputs,\n                sequence_length=inputs_seq_len,\n                dtype=tf.float32,\n                time_major=time_major,\n                scope=scope)\n        return outputs, final_state, outputs_sub, final_state_sub\n    else:\n        return outputs, final_state\n\n\ndef lstmcell(num_units, num_proj, num_layers, use_peephole, clip_activation,\n             inputs, inputs_seq_len, keep_prob, initializer, time_major,\n             num_layers_sub=None):\n\n    if time_major:\n        # Convert form batch-major to time-major\n        inputs = tf.transpose(inputs, [1, 0, 2])\n\n    lstm_list = []\n    with tf.variable_scope(\'multi_lstm\', initializer=initializer) as scope:\n        for i_layer in range(1, num_layers + 1, 1):\n\n            lstm = tf.contrib.rnn.LSTMCell(\n                num_units,\n                use_peepholes=use_peephole,\n                cell_clip=clip_activation,\n                num_proj=num_proj,\n                forget_bias=1.0,\n                state_is_tuple=True)\n\n            # Dropout for the hidden-hidden connections\n            lstm = tf.contrib.rnn.DropoutWrapper(\n                lstm, output_keep_prob=keep_prob)\n\n            lstm_list.append(lstm)\n\n            if num_layers_sub is not None and i_layer == num_layers_sub:\n                lstm_list_sub = lstm_list\n\n        # Stack multiple cells\n        stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n            lstm_list, state_is_tuple=True)\n\n        # Ignore 2nd return (the last state)\n        outputs, final_state = tf.nn.dynamic_rnn(\n            cell=stacked_lstm,\n            inputs=inputs,\n            sequence_length=inputs_seq_len,\n            dtype=tf.float32,\n            time_major=time_major,\n            scope=scope)\n        # NOTE: initial states are zero states by default\n\n    if num_layers_sub is not None:\n        with tf.variable_scope(\'multi_lstm\', initializer=initializer, reuse=True) as scope:\n            # Stack multiple cells\n            stacked_lstm_sub = tf.contrib.rnn.MultiRNNCell(\n                lstm_list_sub, state_is_tuple=True)\n\n            # Ignore 2nd return (the last state)\n            outputs_sub, final_state_sub = tf.nn.dynamic_rnn(\n                cell=stacked_lstm_sub,\n                inputs=inputs,\n                sequence_length=inputs_seq_len,\n                dtype=tf.float32,\n                time_major=time_major,\n                scope=scope)\n        return outputs, final_state, outputs_sub, final_state_sub\n    else:\n        return outputs, final_state\n\n\ndef lstmblockcell(num_units, num_layers, use_peephole, clip_activation, inputs,\n                  inputs_seq_len, keep_prob, initializer, time_major,\n                  num_layers_sub=None):\n\n    if time_major:\n        # Convert form batch-major to time-major\n        inputs = tf.transpose(inputs, [1, 0, 2])\n\n    lstm_list = []\n    with tf.variable_scope(\'multi_lstm\', initializer=initializer) as scope:\n        for i_layer in range(1, num_layers + 1, 1):\n\n            if tf.__version__ == \'1.3.0\':\n                lstm = tf.contrib.rnn.LSTMBlockCell(\n                    num_units,\n                    forget_bias=1.0,\n                    clip_cell=clip_activation,\n                    use_peephole=use_peephole)\n            else:\n                lstm = tf.contrib.rnn.LSTMBlockCell(\n                    num_units,\n                    forget_bias=1.0,\n                    use_peephole=use_peephole)\n\n            # Dropout for the hidden-hidden connections\n            lstm = tf.contrib.rnn.DropoutWrapper(\n                lstm, output_keep_prob=keep_prob)\n\n            lstm_list.append(lstm)\n\n            if num_layers_sub is not None and i_layer == num_layers_sub:\n                lstm_list_sub = lstm_list\n\n        # Stack multiple cells\n        stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n            lstm_list, state_is_tuple=True)\n\n        # Ignore 2nd return (the last state)\n        outputs, final_state = tf.nn.dynamic_rnn(\n            cell=stacked_lstm,\n            inputs=inputs,\n            sequence_length=inputs_seq_len,\n            dtype=tf.float32,\n            time_major=time_major,\n            scope=scope)\n        # NOTE: initial states are zero states by default\n\n    if num_layers_sub is not None:\n        with tf.variable_scope(\'multi_lstm\', initializer=initializer, reuse=True) as scope:\n            # Stack multiple cells\n            stacked_lstm_sub = tf.contrib.rnn.MultiRNNCell(\n                lstm_list_sub, state_is_tuple=True)\n\n            # Ignore 2nd return (the last state)\n            outputs_sub, final_state_sub = tf.nn.dynamic_rnn(\n                cell=stacked_lstm_sub,\n                inputs=inputs,\n                sequence_length=inputs_seq_len,\n                dtype=tf.float32,\n                time_major=time_major,\n                scope=scope)\n        return outputs, final_state, outputs_sub, final_state_sub\n    else:\n        return outputs, final_state\n\n\ndef lstmblockfusedcell(num_units, num_layers, use_peephole, clip_activation,\n                       inputs, inputs_seq_len, keep_prob, initializer,\n                       time_major, num_layers_sub=None):\n    raise NotImplementedError\n\n\ndef cudnnlstm(num_units, num_layers, parameter_init,\n              inputs, inputs_seq_len, keep_prob, initializer, time_major,\n              num_layers_sub=None):\n    raise NotImplementedError\n'"
models/encoders/core/multitask_blstm.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Multi-task bidirectional LSTM encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom models.encoders.core.blstm import basiclstmcell, lstmcell, lstmblockcell, lstmblockfusedcell, cudnnlstm\n\n\nclass MultitaskBLSTMEncoder(object):\n    """"""Multi-task bidirectional LSTM encoder.\n    Args:\n        num_units (int): the number of units in each layer\n        num_proj (int): the number of nodes in recurrent projection layer\n        num_layers_main (int): the number of layers of the main task\n        num_layers_sub (int): the number of layers of the sub task\n        lstm_impl (string, optional): a base implementation of LSTM.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n        use_peephole (bool): if True, use peephole\n        parameter_init (float): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_activation (float): the range of activation clipping (> 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 num_units,\n                 num_proj,\n                 num_layers_main,\n                 num_layers_sub,\n                 lstm_impl,\n                 use_peephole,\n                 parameter_init,\n                 clip_activation,\n                 time_major=False,\n                 name=\'multitask_blstm_encoder\'):\n\n        assert num_proj != 0\n\n        self.num_units = num_units\n        if lstm_impl != \'LSTMCell\':\n            self.num_proj = None\n        else:\n            self.num_proj = num_proj\n        # TODO: fix this\n        self.num_layers_main = num_layers_main\n        self.num_layers_sub = num_layers_sub\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n        self.parameter_init = parameter_init\n        self.clip_activation = clip_activation\n        self.time_major = time_major\n        self.name = name\n\n        if self.num_layers_sub < 1 or self.num_layers_main < self.num_layers_sub:\n            raise ValueError(\n                \'Set num_layers_sub between 1 to num_layers_main.\')\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: A tensor of size `[T, B, input_size]` in the main task\n            final_state: A final hidden state of the encoder in the main task\n            outputs_sub: A tensor of size `[T, B, input_size]` in the sub task\n            final_state_sub: A final hidden state of the encoder in the sub task\n        """"""\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        if self.lstm_impl == \'BasicLSTMCell\':\n            outputs, final_state, outputs_sub, final_state_sub = basiclstmcell(\n                self.num_units, self.num_layers_main,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        elif self.lstm_impl == \'LSTMCell\':\n            outputs, final_state, outputs_sub, final_state_sub = lstmcell(\n                self.num_units, self.num_proj, self.num_layers_main,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        elif self.lstm_impl == \'LSTMBlockCell\':\n            outputs, final_state, outputs_sub, final_state_sub = lstmblockcell(\n                self.num_units, self.num_layers_main,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        elif self.lstm_impl == \'LSTMBlockFusedCell\':\n            outputs, final_state, outputs_sub, final_state_sub = lstmblockfusedcell(\n                self.num_units, self.num_layers_main,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        elif self.lstm_impl == \'CudnnLSTM\':\n            outputs, final_state, outputs_sub, final_state_sub = cudnnlstm(\n                self.num_units, self.num_layers_main,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        else:\n            raise IndexError(\n                \'lstm_impl is ""BasicLSTMCell"" or ""LSTMCell"" or \' +\n                \'""LSTMBlockCell"" or ""LSTMBlockFusedCell"" or \' +\n                \'""CudnnLSTM"".\')\n\n        return outputs, final_state, outputs_sub, final_state_sub\n'"
models/encoders/core/multitask_lstm.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Multi-task unidirectional LSTM encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom models.encoders.core.lstm import basiclstmcell, lstmcell, lstmblockcell, lstmblockfusedcell, cudnnlstm\n\n\nclass MultitaskLSTMEncoder(object):\n    """"""Multi-task unidirectional LSTM encoder.\n    Args:\n        num_units (int): the number of units in each layer\n        num_proj (int): the number of nodes in recurrent projection layer\n        num_layers_main (int): the number of layers of the main task\n        num_layers_sub (int): the number of layers of the sub task\n        lstm_impl (string, optional): a base implementation of LSTM.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n        use_peephole (bool): if True, use peephole\n        parameter_init (float): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_activation (float): the range of activation clipping (> 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 num_units,\n                 num_proj,\n                 num_layers_main,\n                 num_layers_sub,\n                 lstm_impl,\n                 use_peephole,\n                 parameter_init,\n                 clip_activation,\n                 time_major=False,\n                 name=\'multitask_lstm_encoder\'):\n\n        assert num_proj != 0\n\n        self.num_units = num_units\n        if lstm_impl != \'LSTMCell\':\n            self.num_proj = None\n        else:\n            self.num_proj = num_proj\n        # TODO: fix this\n        self.num_layers_main = num_layers_main\n        self.num_layers_sub = num_layers_sub\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n        self.parameter_init = parameter_init\n        self.clip_activation = clip_activation\n        self.time_major = time_major\n        self.name = name\n\n        if self.num_layers_sub < 1 or self.num_layers_main < self.num_layers_sub:\n            raise ValueError(\n                \'Set num_layers_sub between 1 to num_layers_main.\')\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: A tensor of size `[T, B, input_size]` in the main task\n            final_state: A final hidden state of the encoder in the main task\n            outputs_sub: A tensor of size `[T, B, input_size]` in the sub task\n            final_state_sub: A final hidden state of the encoder in the sub task\n        """"""\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        if self.lstm_impl == \'BasicLSTMCell\':\n            outputs, final_state, outputs_sub, final_state_sub = basiclstmcell(\n                self.num_units, self.num_layers_main,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        elif self.lstm_impl == \'LSTMCell\':\n            outputs, final_state, outputs_sub, final_state_sub = lstmcell(\n                self.num_units, self.num_proj, self.num_layers_main,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        elif self.lstm_impl == \'LSTMBlockCell\':\n            outputs, final_state, outputs_sub, final_state_sub = lstmblockcell(\n                self.num_units, self.num_layers_main,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        elif self.lstm_impl == \'LSTMBlockFusedCell\':\n            outputs, final_state, outputs_sub, final_state_sub = lstmblockfusedcell(\n                self.num_units, self.num_layers_main,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        elif self.lstm_impl == \'CudnnLSTM\':\n            outputs, final_state, outputs_sub, final_state_sub = cudnnlstm(\n                self.num_units, self.num_layers_main,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major, self.num_layers_sub)\n\n        else:\n            raise IndexError(\n                \'lstm_impl is ""BasicLSTMCell"" or ""LSTMCell"" or \' +\n                \'""LSTMBlockCell"" or ""LSTMBlockFusedCell"" or \' +\n                \'""CudnnLSTM"".\')\n\n        return outputs, final_state, outputs_sub, final_state_sub\n'"
models/encoders/core/pyramidal_blstm.py,27,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Pyramidal bidirectional LSTM encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass PyramidBLSTMEncoder(object):\n    """"""Pyramidal bidirectional LSTM Encoder.\n    Args:\n        num_units (int): the number of units in each layer\n        num_layers (int): the number of layers\n        lstm_impl (string, optional): BasicLSTMCell or LSTMCell or\n            LSTMBlockCell or LSTMBlockFusedCell or CudnnLSTM.\n            Choose the background implementation of tensorflow.\n            Default is LSTMBlockCell.\n        use_peephole (bool, optional): if True, use peephole\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_activation (float, optional): the range of activation clipping (> 0)\n        # num_proj (int, optional): the number of nodes in the projection layer\n        concat (bool, optional):\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 num_units,\n                 num_layers,\n                 lstm_impl,\n                 use_peephole,\n                 parameter_init,\n                 clip_activation,\n                 num_proj,\n                 concat=False,\n                 name=\'pblstm_encoder\'):\n\n        assert num_proj != 0\n        assert num_units % 2 == 0, \'num_unit should be even number.\'\n\n        self.num_units = num_units\n        self.num_proj = None\n        self.num_layers = num_layers\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n        self.parameter_init = parameter_init\n        self.clip_activation = clip_activation\n        self.name = name\n\n    def _build(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct Pyramidal Bidirectional LSTM encoder.\n        Args:\n            inputs (placeholder): A tensor of size`[B, T, input_size]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states, a tensor of size\n                `[T, B, num_units (num_proj)]`\n            final_state: A final hidden state of the encoder\n        """"""\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        # Hidden layers\n        outputs = inputs\n        for i_layer in range(1, self.num_layers + 1, 1):\n            with tf.variable_scope(\'pblstm_hidden\' + str(i_layer),\n                                   initializer=initializer) as scope:\n\n                lstm_fw = tf.contrib.rnn.LSTMCell(\n                    self.num_units,\n                    use_peepholes=self.use_peephole,\n                    cell_clip=self.clip_activation,\n                    initializer=initializer,\n                    num_proj=None,\n                    forget_bias=1.0,\n                    state_is_tuple=True)\n                lstm_bw = tf.contrib.rnn.LSTMCell(\n                    self.num_units,\n                    use_peepholes=self.use_peephole,\n                    cell_clip=self.clip_activation,\n                    initializer=initializer,\n                    num_proj=self.num_proj,\n                    forget_bias=1.0,\n                    state_is_tuple=True)\n\n                # Dropout for the hidden-hidden connections\n                lstm_fw = tf.contrib.rnn.DropoutWrapper(\n                    lstm_fw, output_keep_prob=keep_prob)\n                lstm_bw = tf.contrib.rnn.DropoutWrapper(\n                    lstm_bw, output_keep_prob=keep_prob)\n\n                if i_layer > 0:\n                    # Convert to time-major: `[T, B, input_size]`\n                    outputs = tf.transpose(outputs, (1, 0, 2))\n                    max_time = tf.shape(outputs)[0]\n\n                    max_time_half = tf.floor(max_time / 2) + 1\n\n                    # Apply concat_fn to each tensor in outputs along\n                    # dimension 0 (times-axis)\n                    i_time = tf.constant(0)\n                    final_time, outputs, tensor_list = tf.while_loop(\n                        cond=lambda t, hidden, tensor_list: t < max_time,\n                        body=lambda t, hidden, tensor_list: self._concat_fn(\n                            t, hidden, tensor_list),\n                        loop_vars=[i_time, outputs, tf.Variable([])],\n                        shape_invariants=[i_time.get_shape(),\n                                          outputs.get_shape(),\n                                          tf.TensorShape([None])])\n\n                    outputs = tf.stack(tensor_list, axis=0)\n\n                    inputs_seq_len = tf.cast(tf.floor(\n                        tf.cast(inputs_seq_len, tf.float32) / 2),\n                        tf.int32)\n\n                    # Transpose to `[batch_size, time, input_size]`\n                    outputs = tf.transpose(outputs, (1, 0, 2))\n\n                (outputs_fw, outputs_bw), final_state = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=lstm_fw,\n                    cell_bw=lstm_bw,\n                    inputs=outputs,\n                    sequence_length=inputs_seq_len,\n                    dtype=tf.float32,\n                    time_major=False,\n                    scope=scope)\n                # NOTE: initial states are zero states by default\n\n                # Concatenate each direction\n                outputs = tf.concat(axis=2, values=[outputs_fw, outputs_bw])\n\n        return outputs, final_state\n\n    def _concat_fn(self, current_time, x, tensor_list):\n        """"""Concatenate each 2 time steps to reduce time resolution.\n        Args:\n            current_time: The current timestep\n            x: A tensor of size `[max_time, batch_size, feature_dim]`\n            result: A tensor of size `[t, batch_size, feature_dim * 2]`\n        Returns:\n            current_time: current_time + 2\n            x: A tensor of size `[max_time, batch_size, feature_dim]`\n            result: A tensor of size `[t + 1, batch_size, feature_dim * 2]`\n        """"""\n        print(tensor_list)\n        print(current_time)\n        print(\'-----\')\n\n        batch_size = tf.shape(x)[1]\n        feature_dim = x.get_shape().as_list()[2]\n\n        # Concat features in 2 timesteps\n        concat_x = tf.concat(\n            axis=0,\n            values=[tf.reshape(x[current_time],\n                               shape=[1, batch_size, feature_dim]),\n                    tf.reshape(x[current_time + 1],\n                               shape=[1, batch_size, feature_dim])])\n\n        # Reshape to `[1, batch_size, feature_dim * 2]`\n        concat_x = tf.reshape(concat_x,\n                              shape=[1, batch_size, feature_dim * 2])\n\n        tensor_list = tf.concat(axis=0, values=[tensor_list, [concat_x]])\n\n        # Skip 2 timesteps\n        current_time += 2\n\n        return current_time, x, tensor_list\n'"
models/encoders/core/rnn_util.py,6,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Utilities for RNN layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef sequence_length(inputs, time_major=True, dtype=tf.int32):\n    """"""Inspect the length of the sequence of input data.\n    Args:\n        inputs: A tensor of size `[B, T, input_size]`\n        time_major (bool, optional): set True if inputs is time-major\n        dtype (optional): default is tf.int32\n    Returns:\n        seq_len: A tensor of size `[B,]`\n    """"""\n    time_axis = 0 if time_major else 1\n    with tf.variable_scope(""seq_len""):\n        used = tf.sign(tf.reduce_max(tf.abs(inputs), axis=2))\n        seq_len = tf.reduce_sum(used, axis=time_axis)\n        seq_len = tf.cast(seq_len, dtype)\n    return seq_len\n'"
models/encoders/core/student_cnn_compact_ctc.py,14,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Student compact CNN encoder for CTC training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\n\n############################################################\n# Architecture: (feature map, kernel(f*t), stride(f,t))\n\n# CNN1: (64, 9*9, (1,1)) * 1 layers\n# Batch normalization\n# ReLU\n# Max pool (3,1)\n\n# CNN2: (128, 3*4, (1,1)) * 1 layers\n# Batch normalization\n# ReLU\n# Max pool (1,1)\n\n# fc: 768 (ReLU) * 2 layers\n############################################################\n\n\nclass StudentCNNCompactCTCEncoder(object):\n    """"""Student compact CNN encoder for CTC training.\n    Args:\n        input_size (int): the dimensions of input vectors.\n            This is expected to be num_channels * 3 (static + \xce\x94 + \xce\x94\xce\x94)\n        splice (int): frames to splice\n        num_stack (int): the number of frames to stack\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 parameter_init,\n                 time_major,\n                 name=\'cnn_student_compact_encoder\'):\n\n        assert input_size % 3 == 0\n\n        self.num_channels = (input_size // 3) // num_stack\n        self.splice = splice\n        self.num_stack = num_stack\n        self.parameter_init = parameter_init\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, T, input_size (num_channels * splice * num_stack * 3)]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size `[T, B, output_dim]`\n                otherwise, `[B, T, output_dim]`\n            final_state: None\n        """"""\n        # inputs: 3D tensor `[B, T, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # For debug\n        # print(input_dim)\n        # print(self.num_channels)\n        # print(self.splice)\n        # print(self.num_stack)\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B * T, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size * max_time, self.num_channels, self.splice * self.num_stack, 3])\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        with tf.variable_scope(\'CNN1\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[9, 9, 3, 64],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[3, 1],\n                              stride=[3, 1],\n                              name=\'max_pool\')\n            # TODO: try dropout\n\n        with tf.variable_scope(\'CNN2\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 4, 64, 128],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[1, 1],\n                              stride=[1, 1],\n                              name=\'max_pool\')\n            # TODO: try dropout\n\n        # Reshape to 2D tensor `[B * T, new_h * new_w * C_out]`\n        outputs = tf.reshape(\n            inputs, shape=[batch_size * max_time, np.prod(inputs.shape.as_list()[-3:])])\n\n        for i in range(1, 3, 1):\n            with tf.variable_scope(\'fc%d\' % (i)) as scope:\n                outputs = tf.contrib.layers.fully_connected(\n                    inputs=outputs,\n                    num_outputs=768,\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=scope)\n                outputs = tf.nn.dropout(outputs, keep_prob)\n\n        # Reshape back to 3D tensor `[B, T, 768]`\n        outputs = tf.reshape(\n            outputs, shape=[batch_size, max_time, 768])\n\n        if self.time_major:\n            # Convert to time-major: `[T, B, num_classes]\'\n            outputs = tf.transpose(outputs, [1, 0, 2])\n\n        return outputs, None\n'"
models/encoders/core/student_cnn_compact_xe.py,11,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Student CNN encoder for XE training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\n\n############################################################\n# Architecture: (feature map, kernel(f*t), stride(f,t))\n\n# CNN1: (128, 9*9, (1,1)) * 1 layers\n# Batch normalization\n# ReLU\n# Max pool (3,1)\n\n# CNN2: (256, 3*4, (1,1)) * 1 layers\n# Batch normalization\n# ReLU\n# Max pool (1,1)\n\n# fc: 2048 (ReLU) * 4 layers\n############################################################\n\n\nclass StudentCNNCompactXEEncoder(object):\n    """"""Student CNN encoder for XE training.\n    Args:\n        input_size (int): the dimensions of input vectors.\n            This is expected to be num_channels * 3 (static + \xce\x94 + \xce\x94\xce\x94)\n        splice (int): frames to splice\n        num_stack (int): the number of frames to stack\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 parameter_init,\n                 name=\'cnn_student_compact_xe_encoder\'):\n\n        assert input_size % 3 == 0\n\n        self.num_channels = (input_size // 3) // num_stack // splice\n        self.splice = splice\n        self.num_stack = num_stack\n        self.parameter_init = parameter_init\n        self.name = name\n\n    def __call__(self, inputs, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, input_size (num_channels * splice * num_stack * 3)]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size `[T, B, output_dim]`\n                otherwise, `[B, output_dim]`\n        """"""\n        # inputs: 2D tensor `[B, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # for debug\n        # print(input_dim)  # 1200\n        # print(self.num_channels)  # 40\n        # print(self.splice)  # 5\n        # print(self.num_stack)  # 2\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size, self.num_channels, self.splice * self.num_stack, 3])\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        with tf.variable_scope(\'CNN1\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[9, 9, 3, 64],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[3, 1],\n                              stride=[3, 1],\n                              name=\'max_pool\')\n            # TODO: add dropout\n\n        with tf.variable_scope(\'CNN2\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 4, 64, 128],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[1, 1],\n                              stride=[1, 1],\n                              name=\'max_pool\')\n            # TODO: add dropout\n\n        # Reshape to 2D tensor `[B, new_h * new_w * C_out]`\n        outputs = tf.reshape(\n            inputs, shape=[batch_size, np.prod(inputs.shape.as_list()[-3:])])\n\n        for i in range(1, 3, 1):\n            with tf.variable_scope(\'fc%d\' % (i)) as scope:\n                outputs = tf.contrib.layers.fully_connected(\n                    inputs=outputs,\n                    num_outputs=768,\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=scope)\n                outputs = tf.nn.dropout(outputs, keep_prob)\n\n        return outputs\n'"
models/encoders/core/student_cnn_ctc.py,14,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Student CNN encoder for CTC training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\n\n############################################################\n# Architecture: (feature map, kernel(f*t), stride(f,t))\n\n# CNN1: (128, 9*9, (1,1)) * 1 layers\n# Batch normalization\n# ReLU\n# Max pool (3,1)\n\n# CNN2: (256, 3*4, (1,1)) * 1 layers\n# Batch normalization\n# ReLU\n# Max pool (1,1)\n\n# fc: 2048 (ReLU) * 4 layers\n############################################################\n\n\nclass StudentCNNCTCEncoder(object):\n    """"""Student CNN encoder for CTC training.\n    Args:\n        input_size (int): the dimensions of input vectors.\n            This is expected to be num_channels * 3 (static + \xce\x94 + \xce\x94\xce\x94)\n        splice (int): frames to splice\n        num_stack (int): the number of frames to stack\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 parameter_init,\n                 time_major,\n                 name=\'cnn_student_encoder\'):\n\n        assert input_size % 3 == 0\n\n        self.num_channels = (input_size // 3) // num_stack\n        self.splice = splice\n        self.num_stack = num_stack\n        self.parameter_init = parameter_init\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, T, input_size (num_channels * splice * num_stack * 3)]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size `[T, B, output_dim]`\n                otherwise, `[B, T, output_dim]`\n            final_state: None\n        """"""\n        # inputs: 3D tensor `[B, T, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # For debug\n        # print(input_dim)\n        # print(self.num_channels)\n        # print(self.splice)\n        # print(self.num_stack)\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B * T, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size * max_time, self.num_channels, self.splice * self.num_stack, 3])\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        with tf.variable_scope(\'CNN1\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[9, 9, 3, 128],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[3, 1],\n                              stride=[3, 1],\n                              name=\'max_pool\')\n            # TODO: try dropout\n\n        with tf.variable_scope(\'CNN2\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 4, 128, 256],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[1, 1],\n                              stride=[1, 1],\n                              name=\'max_pool\')\n            # TODO: try dropout\n\n        # Reshape to 2D tensor `[B * T, new_h * new_w * C_out]`\n        outputs = tf.reshape(\n            inputs, shape=[batch_size * max_time, np.prod(inputs.shape.as_list()[-3:])])\n\n        for i in range(1, 5, 1):\n            with tf.variable_scope(\'fc%d\' % (i)) as scope:\n                outputs = tf.contrib.layers.fully_connected(\n                    inputs=outputs,\n                    num_outputs=2048,\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=scope)\n                outputs = tf.nn.dropout(outputs, keep_prob)\n\n        # Reshape back to 3D tensor `[B, T, 2048]`\n        outputs = tf.reshape(\n            outputs, shape=[batch_size, max_time, 2048])\n\n        if self.time_major:\n            # Convert to time-major: `[T, B, num_classes]\'\n            outputs = tf.transpose(outputs, [1, 0, 2])\n\n        return outputs, None\n'"
models/encoders/core/student_cnn_xe.py,10,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Student CNN encoder for XE training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\n\n############################################################\n# Architecture: (feature map, kernel(f*t), stride(f,t))\n\n# CNN1: (128, 9*9, (1,1)) * 1 layers\n# Batch normalization\n# ReLU\n# Max pool (3,1)\n\n# CNN2: (256, 3*4, (1,1)) * 1 layers\n# Batch normalization\n# ReLU\n# Max pool (1,1)\n\n# fc: 2048 (ReLU) * 4 layers\n############################################################\n\n\nclass StudentCNNXEEncoder(object):\n    """"""Student CNN encoder for XE training.\n    Args:\n        input_size (int): the dimensions of input vectors.\n            This is expected to be num_channels * 3 (static + \xce\x94 + \xce\x94\xce\x94)\n        splice (int): frames to splice\n        num_stack (int): the number of frames to stack\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 parameter_init,\n                 name=\'cnn_student_xe_encoder\'):\n\n        assert input_size % 3 == 0\n\n        self.num_channels = (input_size // 3) // num_stack // splice\n        self.splice = splice\n        self.num_stack = num_stack\n        self.parameter_init = parameter_init\n        self.name = name\n\n    def __call__(self, inputs, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, input_size (num_channels * splice * num_stack * 3)]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size `[T, B, output_dim]`\n                otherwise, `[B, output_dim]`\n        """"""\n        # inputs: 2D tensor `[B, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # for debug\n        # print(input_dim)  # 1200\n        # print(self.num_channels)  # 40\n        # print(self.splice)  # 5\n        # print(self.num_stack)  # 2\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size, self.num_channels, self.splice * self.num_stack, 3])\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        with tf.variable_scope(\'CNN1\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[9, 9, 3, 128],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[3, 1],\n                              stride=[3, 1],\n                              name=\'max_pool\')\n\n        with tf.variable_scope(\'CNN2\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 4, 128, 256],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\')\n            inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[1, 1],\n                              stride=[1, 1],\n                              name=\'max_pool\')\n\n        # Reshape to 2D tensor `[B, new_h * new_w * C_out]`\n        outputs = tf.reshape(\n            inputs, shape=[batch_size, np.prod(inputs.shape.as_list()[-3:])])\n\n        for i in range(1, 5, 1):\n            with tf.variable_scope(\'fc%d\' % (i)) as scope:\n                outputs = tf.contrib.layers.fully_connected(\n                    inputs=outputs,\n                    num_outputs=2048,\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=scope)\n\n        return outputs\n'"
models/encoders/core/vgg_blstm.py,21,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""VGG + bidirectional LSTM encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\nfrom models.encoders.core.blstm import basiclstmcell, lstmcell, lstmblockcell, lstmblockfusedcell, cudnnlstm\n\n\nclass VGGBLSTMEncoder(object):\n    """"""VGG + bidirectional LSTM encoder.\n    Args:\n        input_size (int): the dimensions of input vectors\xef\xbc\x8e\n            This is expected to be num_channels * 3 (static + \xce\x94 + \xce\x94\xce\x94)\n        splice (int): frames to splice\n        num_stack (int): the number of frames to stack\n        num_units (int): the number of units in each layer\n        num_proj (int): the number of nodes in the projection layer\n        num_layers (int): the number of layers\n        lstm_impl (string, optional): a base implementation of LSTM.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n        use_peephole (bool): if True, use peephole\n        parameter_init (float): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_activation (float): the range of activation clipping (> 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 num_units,\n                 num_proj,\n                 num_layers,\n                 lstm_impl,\n                 use_peephole,\n                 parameter_init,\n                 clip_activation,\n                 time_major=False,\n                 name=\'vgg_blstm_encoder\'):\n\n        assert num_proj != 0\n        assert input_size % 3 == 0\n\n        self.num_channels = input_size // 3\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_units = num_units\n        if lstm_impl != \'LSTMCell\':\n            self.num_proj = None\n        else:\n            self.num_proj = num_proj\n        # TODO: fix this\n        self.num_layers = num_layers\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n        self.parameter_init = parameter_init\n        self.clip_activation = clip_activation\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, T, input_size (num_channels * (splice * num_stack) * 3)]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size\n                    `[T, B, num_units (num_proj)]`\n                otherwise, `[B, T, num_units (num_proj)]`\n            final_state: A final hidden state of the encoder\n        """"""\n        # inputs: 3D tensor `[B, T, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # For debug\n        # print(input_dim)\n        # print(self.num_channels)\n        # print(self.splice)\n        # print(self.num_stack)\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B * T, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size * max_time, self.num_channels, self.splice * self.num_stack, 3])\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        with tf.variable_scope(\'VGG1\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 3, 64],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\',\n                                name=\'conv1\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 64, 64],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\',\n                                name=\'conv2\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[2, 2],\n                              stride=[2, 2],\n                              name=\'max_pool\')\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        with tf.variable_scope(\'VGG2\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 64, 128],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\',\n                                name=\'conv1\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 128, 128],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\',\n                                name=\'conv2\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[2, 2],\n                              stride=[2, 2],\n                              name=\'max_pool\')\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        # Reshape to 2D tensor `[B * T, new_h * new_w * C_out]`\n        inputs = tf.reshape(\n            inputs, shape=[batch_size * max_time, np.prod(inputs.shape.as_list()[-3:])])\n\n        # Insert linear layer to recude CNN\'s output demention\n        # from (new_h * new_w * C_out) to 256\n        with tf.variable_scope(\'bridge\') as scope:\n            inputs = tf.contrib.layers.fully_connected(\n                inputs=inputs,\n                num_outputs=256,\n                activation_fn=tf.nn.relu,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        # Reshape back to 3D tensor `[B, T, 256]`\n        inputs = tf.reshape(inputs, shape=[batch_size, max_time, 256])\n\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        if self.lstm_impl == \'BasicLSTMCell\':\n            outputs, final_state = basiclstmcell(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMCell\':\n            outputs, final_state = lstmcell(\n                self.num_units, self.num_proj, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockCell\':\n            outputs, final_state = lstmblockcell(\n                self.num_units, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockFusedCell\':\n            outputs, final_state = lstmblockfusedcell(\n                self.num_units, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'CudnnLSTM\':\n            outputs, final_state = cudnnlstm(\n                self.num_units, self.num_layers, self.parameter_init,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n        else:\n            raise IndexError(\n                \'lstm_impl is ""BasicLSTMCell"" or ""LSTMCell"" or \' +\n                \'""LSTMBlockCell"" or ""LSTMBlockFusedCell"" or \' +\n                \'""CudnnLSTM"".\')\n\n        return outputs, final_state\n'"
models/encoders/core/vgg_lstm.py,21,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""VGG + unidirectional LSTM encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\nfrom models.encoders.core.lstm import basiclstmcell, lstmcell, lstmblockcell, lstmblockfusedcell, cudnnlstm\n\n\nclass VGGLSTMEncoder(object):\n    """"""VGG + unidirectional LSTM encoder.\n    Args:\n        input_size (int): the dimensions of input vectors\xef\xbc\x8e\n            This is expected to be num_channels * 3 (static + \xce\x94 + \xce\x94\xce\x94)\n        splice (int): frames to splice\n        num_stack (int): the number of frames to stack\n        num_units (int): the number of units in each layer\n        num_proj (int): the number of nodes in the projection layer\n        num_layers (int): the number of layers\n        lstm_impl (string, optional): a base implementation of LSTM.\n                - BasicLSTMCell: tf.contrib.rnn.BasicLSTMCell (no peephole)\n                - LSTMCell: tf.contrib.rnn.LSTMCell\n                - LSTMBlockCell: tf.contrib.rnn.LSTMBlockCell\n                - LSTMBlockFusedCell: under implementation\n                - CudnnLSTM: under implementation\n            Choose the background implementation of tensorflow.\n        use_peephole (bool): if True, use peephole\n        parameter_init (float): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        clip_activation (float): the range of activation clipping (> 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 num_units,\n                 num_proj,\n                 num_layers,\n                 lstm_impl,\n                 use_peephole,\n                 parameter_init,\n                 clip_activation,\n                 time_major=False,\n                 name=\'vgg_lstm_encoder\'):\n\n        assert num_proj != 0\n        assert input_size % 3 == 0\n\n        self.num_channels = input_size // 3\n        self.splice = splice\n        self.num_stack = num_stack\n        self.num_units = num_units\n        if lstm_impl != \'LSTMCell\':\n            self.num_proj = None\n        else:\n            self.num_proj = num_proj\n        # TODO: fix this\n        self.num_layers = num_layers\n        self.lstm_impl = lstm_impl\n        self.use_peephole = use_peephole\n        self.parameter_init = parameter_init\n        self.clip_activation = clip_activation\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, T, input_size (num_channels * splice * num_stack * 3)]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size\n                    `[T, B, num_units (num_proj)]`\n                otherwise, `[B, T, num_units (num_proj)]`\n            final_state: A final hidden state of the encoder\n        """"""\n        # inputs: 3D tensor `[B, T, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # For debug\n        # print(input_dim)\n        # print(self.num_channels)\n        # print(self.splice)\n        # print(self.num_stack)\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B * T, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size * max_time, self.num_channels, self.splice * self.num_stack, 3])\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        with tf.variable_scope(\'VGG1\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 3, 64],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\',\n                                name=\'conv1\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 64, 64],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\',\n                                name=\'conv2\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[2, 2],\n                              stride=[2, 2],\n                              name=\'max_pool\')\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        with tf.variable_scope(\'VGG2\'):\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 64, 128],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\',\n                                name=\'conv1\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n            inputs = conv_layer(inputs,\n                                filter_size=[3, 3, 128, 128],\n                                stride=[1, 1],\n                                parameter_init=self.parameter_init,\n                                activation=\'relu\',\n                                name=\'conv2\')\n            # inputs = batch_normalization(inputs, is_training=is_training)\n            inputs = max_pool(inputs,\n                              pooling_size=[2, 2],\n                              stride=[2, 2],\n                              name=\'max_pool\')\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        # Reshape to 2D tensor `[B * T, new_h * new_w * C_out]`\n        inputs = tf.reshape(\n            inputs, shape=[batch_size * max_time, np.prod(inputs.shape.as_list()[-3:])])\n\n        # Insert linear layer to recude CNN\'s output demention\n        # from (new_h * new_w * C_out) to 256\n        with tf.variable_scope(\'bridge\') as scope:\n            inputs = tf.contrib.layers.fully_connected(\n                inputs=inputs,\n                num_outputs=256,\n                activation_fn=tf.nn.relu,\n                weights_initializer=tf.truncated_normal_initializer(\n                    stddev=self.parameter_init),\n                biases_initializer=tf.zeros_initializer(),\n                scope=scope)\n            inputs = tf.nn.dropout(inputs, keep_prob)\n\n        # Reshape back to 3D tensor `[B, T, 256]`\n        inputs = tf.reshape(inputs, shape=[batch_size, max_time, 256])\n\n        initializer = tf.random_uniform_initializer(\n            minval=-self.parameter_init, maxval=self.parameter_init)\n\n        if self.lstm_impl == \'BasicLSTMCell\':\n            outputs, final_state = basiclstmcell(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMCell\':\n            outputs, final_state = lstmcell(\n                self.num_units, self.num_proj, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockCell\':\n            outputs, final_state = lstmblockcell(\n                self.num_units, self.num_layers,\n                self.use_peephole, self.clip_activation,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'LSTMBlockFusedCell\':\n            outputs, final_state = lstmblockfusedcell(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n\n        elif self.lstm_impl == \'CudnnLSTM\':\n            outputs, final_state = cudnnlstm(\n                self.num_units, self.num_layers,\n                inputs, inputs_seq_len, keep_prob, initializer,\n                self.time_major)\n        else:\n            raise IndexError(\n                \'lstm_impl is ""BasicLSTMCell"" or ""LSTMCell"" or \' +\n                \'""LSTMBlockCell"" or ""LSTMBlockFusedCell"" or \' +\n                \'""CudnnLSTM"".\')\n\n        return outputs, final_state\n'"
models/encoders/core/vgg_wang.py,18,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""VGG encoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.encoders.core.cnn_util import conv_layer, max_pool, batch_normalization\n\n############################################################\n# Architecture: (feature map, kernel, stride)\n\n# VGG1: (96, 3*3, (1,1)) * 3 layers\n# Batch normalization\n# ReLU\n# Max pool\n# dropout\n\n# VGG2: (192, 3*3, (1,1)) * 4 layers\n# Batch normalization\n# ReLU\n# Max pool\n# dropout\n\n# VGG3: (384, 3*3, (1,1)) * 4 layers\n# Batch normalization\n# ReLU\n# Max pool\n# dropout\n\n# fc: 1024 * 2 layers\n# (dropout, first layer only)\n\n# softmax\n############################################################\n\n\nclass VGGEncoder(object):\n    """"""VGG encoder.\n       This implementation is based on\n        https://arxiv.org/abs/1702.07793.\n            Wang, Yisen, et al.\n            ""Residual convolutional CTC networks for automatic speech recognition.""\n            arXiv preprint arXiv:1702.07793 (2017).\n    Args:\n        input_size (int): the dimensions of input vectors.\n            This is expected to be num_channels * 3 (static + \xce\x94 + \xce\x94\xce\x94)\n        splice (int): frames to splice\n        num_stack (int): the number of frames to stack\n        parameter_init (float, optional): the range of uniform distribution to\n            initialize weight parameters (>= 0)\n        time_major (bool, optional): if True, time-major computation will be\n            performed\n        name (string, optional): the name of encoder\n    """"""\n\n    def __init__(self,\n                 input_size,\n                 splice,\n                 num_stack,\n                 parameter_init,\n                 time_major,\n                 name=\'vgg_wang_encoder\'):\n\n        assert input_size % 3 == 0\n\n        self.num_channels = input_size // 3\n        self.splice = splice\n        self.num_stack = num_stack\n        self.parameter_init = parameter_init\n        self.time_major = time_major\n        self.name = name\n\n    def __call__(self, inputs, inputs_seq_len, keep_prob, is_training):\n        """"""Construct model graph.\n        Args:\n            inputs (placeholder): A tensor of size\n                `[B, T, input_size (num_channels * (splice * num_stack) * 3)]`\n            inputs_seq_len (placeholder): A tensor of size` [B]`\n            keep_prob (placeholder, float): A probability to keep nodes\n                in the hidden-hidden connection\n            is_training (bool):\n        Returns:\n            outputs: Encoder states.\n                if time_major is True, a tensor of size `[T, B, output_dim]`\n                otherwise, `[B, T, output_dim]`\n            final_state: None\n        """"""\n        # inputs: 3D tensor `[B, T, input_dim]`\n        batch_size = tf.shape(inputs)[0]\n        max_time = tf.shape(inputs)[1]\n        input_dim = inputs.shape.as_list()[-1]\n        # NOTE: input_dim: num_channels * splice * num_stack * 3\n\n        # For debug\n        # print(input_dim)\n        # print(self.num_channels)\n        # print(self.splice)\n        # print(self.num_stack)\n\n        assert input_dim == self.num_channels * self.splice * self.num_stack * 3\n\n        # Reshape to 4D tensor `[B * T, num_channels, splice * num_stack, 3]`\n        inputs = tf.reshape(\n            inputs,\n            shape=[batch_size * max_time, self.num_channels, self.splice * self.num_stack, 3])\n\n        # NOTE: filter_size: `[H, W, C_in, C_out]`\n        with tf.variable_scope(\'VGG1\'):\n            for i_layer in range(1, 4, 1):\n                input_channels = inputs.shape.as_list()[-1]\n                inputs = conv_layer(inputs,\n                                    filter_size=[3, 3, input_channels, 96],\n                                    stride=[1, 1],\n                                    parameter_init=self.parameter_init,\n                                    activation=\'relu\',\n                                    name=\'conv1\')\n                inputs = batch_normalization(inputs, is_training=is_training)\n                if i_layer == 3:\n                    inputs = max_pool(inputs,\n                                      pooling_size=[2, 2],\n                                      stride=[2, 2],\n                                      name=\'max_pool\')\n                inputs = tf.nn.dropout(inputs, keep_prob)\n\n        with tf.variable_scope(\'VGG2\'):\n            for i_layer in range(1, 5, 1):\n                input_channels = inputs.shape.as_list()[-1]\n                inputs = conv_layer(inputs,\n                                    filter_size=[3, 3, input_channels, 192],\n                                    stride=[1, 1],\n                                    parameter_init=self.parameter_init,\n                                    activation=\'relu\',\n                                    name=\'conv%d\' % i_layer)\n                inputs = batch_normalization(inputs, is_training=is_training)\n                if i_layer == 4:\n                    inputs = max_pool(inputs,\n                                      pooling_size=[2, 2],\n                                      stride=[2, 2],\n                                      name=\'max_pool\')\n                inputs = tf.nn.dropout(inputs, keep_prob)\n\n        with tf.variable_scope(\'VGG3\'):\n            for i_layer in range(1, 5, 1):\n                input_channels = inputs.shape.as_list()[-1]\n                inputs = conv_layer(inputs,\n                                    filter_size=[3, 3, input_channels, 384],\n                                    parameter_init=self.parameter_init,\n                                    activation=\'relu\',\n                                    name=\'conv%d\' % i_layer)\n                inputs = batch_normalization(inputs, is_training=is_training)\n                if i_layer == 4:\n                    inputs = max_pool(inputs,\n                                      pooling_size=[2, 2],\n                                      stride=[2, 2],\n                                      name=\'max_pool\')\n                inputs = tf.nn.dropout(inputs, keep_prob)\n\n        # Reshape to 2D tensor `[B * T, new_h * new_w * C_out]`\n        outputs = tf.reshape(\n            inputs, shape=[batch_size * max_time, np.prod(inputs.shape.as_list()[-3:])])\n\n        for i_layer in range(1, 3, 1):\n            with tf.variable_scope(\'fc%d\' % i_layer) as scope:\n                outputs = tf.contrib.layers.fully_connected(\n                    inputs=outputs,\n                    num_outputs=1024,\n                    activation_fn=tf.nn.relu,\n                    weights_initializer=tf.truncated_normal_initializer(\n                        stddev=self.parameter_init),\n                    biases_initializer=tf.zeros_initializer(),\n                    scope=scope)\n                if i_layer == 1:\n                    outputs = tf.nn.dropout(outputs, keep_prob)\n\n        # Reshape back to 3D tensor `[B, T, 1024]`\n        output_dim = outputs.shape.as_list()[-1]\n        outputs = tf.reshape(\n            outputs, shape=[batch_size, max_time, output_dim])\n\n        if self.time_major:\n            # Convert to time-major: `[T, B, num_classes]\'\n            outputs = tf.transpose(outputs, [1, 0, 2])\n\n        return outputs, None\n'"
models/recurrent/layers/__init__.py,0,b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n'
models/recurrent/layers/basic_lstm.py,8,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Bisic Long-Short Term Memory (no peep-hole connections).\n   This code is taken directly from TensorFlow code (some functions are modified).\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import RNNCell, LSTMStateTuple\nfrom tensorflow.python.platform import tf_logging as logging\n\n\nclass BasicLSTMCell(RNNCell):\n    """"""Basic LSTM recurrent network cell.\n    The implementation is based on: http://arxiv.org/abs/1409.2329.\n    We add forget_bias (default: 1) to the biases of the forget gate in order to\n    reduce the scale of forgetting in the beginning of the training.\n    It does not allow cell clipping, a projection layer, and does not\n    use peep-hole connections: it is the basic baseline.\n    For advanced models, please use the full LSTMCell that follows.\n    """"""\n\n    def __init__(self, num_units, forget_bias=1.0, input_size=None,\n                 state_is_tuple=True, reuse=None):\n        """"""Initialize the basic LSTM cell.\n        Args:\n          num_units: int, The number of units in the LSTM cell.\n          forget_bias: float, The bias added to forget gates (see above).\n          input_size: Deprecated and unused.\n          state_is_tuple: If True, accepted and returned states are 2-tuples of\n            the `c_state` and `m_state`.  If False, they are concatenated\n            along the column axis.  The latter behavior will soon be deprecated.\n          reuse: (optional) Python boolean describing whether to reuse variables\n            in an existing scope.  If not `True`, and the existing scope already has\n            the given variables, an error is raised.\n        """"""\n        if not state_is_tuple:\n            logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                         ""deprecated.  Use state_is_tuple=True."", self)\n        if input_size is not None:\n            logging.warn(""%s: The input_size parameter is deprecated."", self)\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return (LSTMStateTuple(self._num_units, self._num_units)\n                if self._state_is_tuple else 2 * self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Long short-term memory cell (LSTM).""""""\n        with tf.variable_scope(self, scope or ""basic_lstm_cell"", reuse=self._reuse):\n            # Parameters of gates are concatenated into one multiply for\n            # efficiency.\n            if self._state_is_tuple:\n                c_prev, h_prev = state\n            else:\n                c_prev, h_prev = tf.split(\n                    value=state, num_or_size_splits=2, axis=1)\n            concat = tf.contrib.rnn._linear(\n                [inputs, h_prev], 4 * self._num_units, True)\n\n            # i = input_gate, g = new_input, f = forget_gate, o = output_gate\n            i, g, f, o = tf.split(value=concat, num_or_size_splits=4, axis=1)\n\n            c = (c_prev * tf.sigmoid(f + self._forget_bias) +\n                 tf.sigmoid(i) * tf.tanh(g))\n            h = tf.tanh(c) * tf.sigmoid(o)\n\n            if self._state_is_tuple:\n                new_state = LSTMStateTuple(c, h)\n            else:\n                new_state = tf.concat([c, h], 1)\n            return h, new_state\n'"
models/recurrent/layers/batch_normalization.py,16,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\ndef batch_norm(inputs, name_scope, is_training, epsilon=1e-3, decay=0.99):\n    with tf.variable_scope(name_scope):\n        size = inputs.get_shape().as_list()[1]\n\n        gamma = tf.get_variable(\n            'gamma', [size], initializer=tf.constant_initializer(0.1))\n        # beta = tf.get_variable('beta', [size], initializer=tf.constant_initializer(0))\n        beta = tf.get_variable('beta', [size])\n\n        pop_mean = tf.get_variable('pop_mean', [size],\n                                   initializer=tf.zeros_initializer(), trainable=False)\n        pop_var = tf.get_variable('pop_var', [size],\n                                  initializer=tf.ones_initializer(), trainable=False)\n        batch_mean, batch_var = tf.nn.moments(inputs, [0])\n\n        train_mean_op = tf.assign(\n            pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n        train_var_op = tf.assign(\n            pop_var, pop_var * decay + batch_var * (1 - decay))\n\n        def batch_statistics():\n            with tf.control_dependencies([train_mean_op, train_var_op]):\n                return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, gamma, epsilon)\n\n        def pop_statistics():\n            return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, gamma, epsilon)\n\n        # control flow\n        return tf.cond(is_training, batch_statistics, pop_statistics)\n"""
models/recurrent/layers/bn_basic_lstm.py,14,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Basic Long-Short Term Memory with Batch Normalization.""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import RNNCell, LSTMStateTuple\nfrom tensorflow.python.platform import tf_logging as logging\nfrom .batch_normalization import batch_norm\nfrom .initializer import orthogonal_initializer\n\n\nclass BatchNormBasicLSTMCell(RNNCell):\n    """"""Batch Normalized Basic LSTM recurrent network cell.\n    The implementation is based on: http://arxiv.org/abs/1409.2329.\n    We add forget_bias (default: 1) to the biases of the forget gate in order to\n    reduce the scale of forgetting in the beginning of the training.\n    It does not allow cell clipping, a projection layer, and does not\n    use peep-hole connections: it is the basic baseline.\n    For advanced models, please use the full LSTMCell that follows.\n    """"""\n\n    def __init__(self, num_units, is_training, forget_bias=1.0, input_size=None,\n                 state_is_tuple=True, reuse=None):\n        """"""Initialize the basic LSTM cell.\n        Args:\n          num_units: int, The number of units in the LSTM cell.\n          is_training: bool, set True when training.\n          forget_bias: float, The bias added to forget gates (see above).\n          input_size: Deprecated and unused.\n          state_is_tuple: If True, accepted and returned states are 2-tuples of\n            the `c_state` and `m_state`.  If False, they are concatenated\n            along the column axis.  The latter behavior will soon be deprecated.\n          reuse: (optional) Python boolean describing whether to reuse variables\n            in an existing scope.  If not `True`, and the existing scope already has\n            the given variables, an error is raised.\n        """"""\n        if not state_is_tuple:\n            logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                         ""deprecated.  Use state_is_tuple=True."", self)\n        if input_size is not None:\n            logging.warn(""%s: The input_size parameter is deprecated."", self)\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._reuse = reuse\n        self._is_training = is_training\n\n    @property\n    def state_size(self):\n        return (LSTMStateTuple(self._num_units, self._num_units)\n                if self._state_is_tuple else 2 * self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Long short-term memory cell (LSTM) with Recurrent Batch Normalization.""""""\n        input_size = inputs.get_shape().with_rank(2)[1]\n        if input_size.value is None:\n            raise ValueError(\n                ""Could not infer input size from inputs.get_shape()[-1]"")\n\n        with tf.variable_scope(scope or ""batch_norm_lstm_cell"", reuse=self._reuse):\n            # Parameters of gates are concatenated into one multiply for\n            # efficiency.\n            if self._state_is_tuple:\n                c_prev, h_prev = state\n            else:\n                c_prev, h_prev = tf.split(\n                    value=state, num_or_size_splits=2, axis=1)\n\n            W_xh = tf.get_variable(\'W_xh\', shape=[input_size, 4 * self._num_units],\n                                   initializer=orthogonal_initializer())\n            W_hh = tf.get_variable(\'W_hh\', shape=[self._num_units, 4 * self._num_units],\n                                   initializer=orthogonal_initializer())\n            bias = tf.get_variable(\'b\', [4 * self._num_units])\n\n            xh = tf.matmul(inputs, W_xh)\n            hh = tf.matmul(h_prev, W_hh)\n\n            bn_xh = batch_norm(xh, \'xh\', self._is_training)\n            bn_hh = batch_norm(hh, \'hh\', self._is_training)\n\n            # i = input_gate, g = new_input, f = forget_gate, o = output_gate\n            # lstm_matrix = tf.contrib.rnn._linear([inputs, h_prev], 4 * self._num_units, True)\n            lstm_matrix = tf.nn.bias_add(tf.add(bn_xh, bn_hh), bias)\n            i, g, f, o = tf.split(\n                value=lstm_matrix, num_or_size_splits=4, axis=1)\n\n            c = (c_prev * tf.sigmoid(f + self._forget_bias) +\n                 tf.sigmoid(i) * tf.tanh(g))\n\n            bn_c = batch_norm(c, \'bn_c\', self._is_training)\n\n            h = tf.tanh(bn_c) * tf.sigmoid(o)\n\n            if self._state_is_tuple:\n                new_state = LSTMStateTuple(c, h)\n            else:\n                new_state = tf.concat(values=[c, h], axis=1)\n            return h, new_state\n'"
models/recurrent/layers/bn_lstm.py,27,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Long-Short Term Memory with Batch Normalization.""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import RNNCell, LSTMStateTuple\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom .batch_normalization import batch_norm\n\n\nclass BatchNormLSTMCell(RNNCell):\n    """"""Batch Normalized Long short-term memory unit (LSTM) recurrent network cell.\n    The default non-peephole implementation is based on:\n      http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n    S. Hochreiter and J. Schmidhuber.\n    ""Long Short-Term Memory"". Neural Computation, 9(8):1735-1780, 1997.\n    The peephole implementation is based on:\n      https://research.google.com/pubs/archive/43905.pdf\n    Hasim Sak, Andrew Senior, and Francoise Beaufays.\n    ""Long short-term memory recurrent neural network architectures for\n     large scale acoustic modeling."" INTERSPEECH, 2014.\n    The class uses optional peep-hole connections, optional cell clipping, and\n    an optional projection layer.\n    """"""\n\n    def __init__(self, num_units, is_training, input_size=None,\n                 use_peepholes=False, cell_clip=None,\n                 initializer=None, num_proj=None, proj_clip=None,\n                 num_unit_shards=None, num_proj_shards=None,\n                 forget_bias=1.0, state_is_tuple=True,\n                 reuse=None):\n        """"""Initialize the parameters for an LSTM cell.\n        Args:\n          num_units: int, The number of units in the LSTM cell\n          is_training: bool, set True when training.\n          input_size: Deprecated and unused.\n          use_peepholes: bool, set True to enable diagonal/peephole connections.\n          cell_clip: (optional) A float value, if provided the cell state is clipped\n            by this value prior to the cell output activation.\n          initializer: (optional) The initializer to use for the weight and\n            projection matrices.\n          num_proj: (optional) int, The output dimensionality for the projection\n            matrices.  If None, no projection is performed.\n          proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n            provided, then the projected values are clipped elementwise to within\n            `[-proj_clip, proj_clip]`.\n          num_unit_shards: Deprecated, will be removed by Jan. 2017.\n            Use a variable_scope partitioner instead.\n          num_proj_shards: Deprecated, will be removed by Jan. 2017.\n            Use a variable_scope partitioner instead.\n          forget_bias: Biases of the forget gate are initialized by default to 1\n            in order to reduce the scale of forgetting at the beginning of\n            the training.\n          state_is_tuple: If True, accepted and returned states are 2-tuples of\n            the `c_state` and `m_state`.  If False, they are concatenated\n            along the column axis.  This latter behavior will soon be deprecated.\n          reuse: (optional) Python boolean describing whether to reuse variables\n            in an existing scope.  If not `True`, and the existing scope already has\n            the given variables, an error is raised.\n        """"""\n        if not state_is_tuple:\n            logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                         ""deprecated.  Use state_is_tuple=True."", self)\n        if input_size is not None:\n            logging.warn(""%s: The input_size parameter is deprecated."", self)\n        if num_unit_shards is not None or num_proj_shards is not None:\n            logging.warn(""%s: The num_unit_shards and proj_unit_shards parameters are ""\n                         ""deprecated and will be removed in Jan 2017.  ""\n                         ""Use a variable scope with a partitioner instead."", self)\n\n        self._num_units = num_units\n        self._use_peepholes = use_peepholes\n        self._cell_clip = cell_clip\n        self._initializer = initializer\n        self._num_proj = num_proj\n        self._proj_clip = proj_clip\n        self._num_unit_shards = num_unit_shards\n        self._num_proj_shards = num_proj_shards\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._reuse = reuse\n        self._is_training = is_training\n\n        if num_proj:\n            self._state_size = (LSTMStateTuple(num_units, num_proj)\n                                if state_is_tuple else num_units + num_proj)\n            self._output_size = num_proj\n        else:\n            self._state_size = (LSTMStateTuple(num_units, num_units)\n                                if state_is_tuple else 2 * num_units)\n            self._output_size = num_units\n\n    @property\n    def state_size(self):\n        return self._state_size\n\n    @property\n    def output_size(self):\n        return self._output_size\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run one step of LSTM.\n        Args:\n          inputs: input Tensor, 2D, batch x num_units.\n          state: if `state_is_tuple` is False, this must be a state Tensor,\n            `2-D, batch x state_size`.  If `state_is_tuple` is True, this must be a\n            tuple of state Tensors, both `2-D`, with column sizes `c_state` and\n            `m_state`.\n          scope: VariableScope for the created subgraph; defaults to ""lstm_cell"".\n        Returns:\n          A tuple containing:\n          - A `2-D, [batch x output_dim]`, Tensor representing the output of the\n            LSTM after reading `inputs` when previous state was `state`.\n            Here output_dim is:\n               num_proj if num_proj was set,\n               num_units otherwise.\n          - Tensor(s) representing the new state of LSTM after reading `inputs` when\n            the previous state was `state`.  Same type and shape(s) as `state`.\n        Raises:\n          ValueError: If input size cannot be inferred from inputs via\n            static shape inference.\n        """"""\n        num_proj = self._num_units if self._num_proj is None else self._num_proj\n\n        if self._state_is_tuple:\n            (c_prev, h_prev) = state\n        else:\n            c_prev = tf.slice(state, begin=[0, 0], size=[-1, self._num_units])\n            h_prev = tf.slice(\n                state, begin=[0, self._num_units], size=[-1, num_proj])\n\n        dtype = inputs.dtype\n        input_size = inputs.get_shape().with_rank(2)[1]\n        if input_size.value is None:\n            raise ValueError(\n                ""Could not infer input size from inputs.get_shape()[-1]"")\n\n        with tf.variable_scope(scope or ""lstm_cell"", initializer=self._initializer,\n                               reuse=self._reuse) as unit_scope:\n            if self._num_unit_shards is not None:\n                unit_scope.set_partitioner(\n                    partitioned_variables.fixed_size_partitioner(\n                        self._num_unit_shards))\n\n            W_xh = tf.get_variable(\'W_xh\', shape=[input_size, 4 * self._num_units],\n                                   initializer=self._initializer)\n            W_hh = tf.get_variable(\'W_hh\', shape=[num_proj, 4 * self._num_units],\n                                   initializer=self._initializer)\n            bias = tf.get_variable(\'b\', [4 * self._num_units])\n\n            xh = tf.matmul(inputs, W_xh)\n            hh = tf.matmul(h_prev, W_hh)\n\n            bn_xh = batch_norm(xh, \'xh\', self._is_training)\n            bn_hh = batch_norm(hh, \'hh\', self._is_training)\n\n            # i = input_gate, g = new_input, f = forget_gate, o = output_gate\n            # lstm_matrix = tf.contrib.rnn._linear([inputs, h_prev], 4 * self._num_units, bias=True)\n            lstm_matrix = tf.nn.bias_add(tf.add(bn_xh, bn_hh), bias)\n            i, g, f, o = tf.split(\n                value=lstm_matrix, num_or_size_splits=4, axis=1)\n\n            # Diagonal connections\n            if self._use_peepholes:\n                # tf.variable_scope\xe3\x81\xa8tf.get_variable\xe3\x81\xaf\xe3\x82\xbb\xe3\x83\x83\xe3\x83\x88\xe3\x81\xa7\xe4\xbd\xbf\xe3\x81\x86\n                with tf.variable_scope(unit_scope) as projection_scope:\n                    if self._num_unit_shards is not None:\n                        projection_scope.set_partitioner(None)\n                    p_f_diag = tf.get_variable(\n                        ""p_f_diag"", shape=[self._num_units], dtype=dtype)\n                    p_i_diag = tf.get_variable(\n                        ""p_i_diag"", shape=[self._num_units], dtype=dtype)\n                    p_o_diag = tf.get_variable(\n                        ""p_o_diag"", shape=[self._num_units], dtype=dtype)\n\n            if self._use_peepholes:\n                c = (tf.sigmoid(f + self._forget_bias + p_f_diag * c_prev) * c_prev +\n                     tf.sigmoid(i + p_i_diag * c_prev) * tf.tanh(g))\n            else:\n                c = (tf.sigmoid(f + self._forget_bias) * c_prev +\n                     tf.sigmoid(i) * tf.tanh(g))\n\n            if self._cell_clip is not None:\n                c = tf.clip_by_value(c, -self._cell_clip, self._cell_clip)\n\n            bn_c = batch_norm(c, \'bn_c\', self._is_training)\n\n            if self._use_peepholes:\n                # peephole\xe5\x81\xb4\xe3\x81\xab\xe3\x81\xaf\xe9\x81\xa9\xe7\x94\xa8\xe3\x81\x97\xe3\x81\xaa\xe3\x81\x84\n                h = tf.sigmoid(o + p_o_diag * c) * tf.tanh(bn_c)\n            else:\n                h = tf.sigmoid(o) * tf.tanh(bn_c)\n\n            if self._num_proj is not None:\n                with tf.variable_scope(""projection"") as proj_scope:\n                    if self._num_proj_shards is not None:\n                        proj_scope.set_partitioner(\n                            partitioned_variables.fixed_size_partitioner(\n                                self._num_proj_shards))\n                    h = tf.contrib.rnn._linear(h, self._num_proj, bias=False)\n\n                if self._proj_clip is not None:\n                    h = tf.clip_by_value(h, -self._proj_clip, self._proj_clip)\n\n        new_state = (LSTMStateTuple(c, h) if self._state_is_tuple else\n                     tf.concat(values=[c, h], axis=1))\n        return h, new_state\n'"
models/recurrent/layers/lstm.py,23,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Long-Short Term Memory.\n   This code is taken directly from TensorFlow code (some functions are modified).\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import RNNCell, LSTMStateTuple\nfrom tensorflow.python.platform import tf_logging as logging\n\n\nclass LSTMCell(RNNCell):\n    """"""Long short-term memory unit (LSTM) recurrent network cell.\n    The default non-peephole implementation is based on:\n      http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n    S. Hochreiter and J. Schmidhuber.\n    ""Long Short-Term Memory"". Neural Computation, 9(8):1735-1780, 1997.\n    The peephole implementation is based on:\n      https://research.google.com/pubs/archive/43905.pdf\n    Hasim Sak, Andrew Senior, and Francoise Beaufays.\n    ""Long short-term memory recurrent neural network architectures for\n     large scale acoustic modeling."" INTERSPEECH, 2014.\n    The class uses optional peep-hole connections, optional cell clipping, and\n    an optional projection layer.\n    """"""\n\n    def __init__(self, num_units, input_size=None,\n                 use_peepholes=False, cell_clip=None,\n                 initializer=None, num_proj=None, proj_clip=None,\n                 num_unit_shards=None, num_proj_shards=None,\n                 forget_bias=1.0, state_is_tuple=True,\n                 reuse=None):\n        """"""Initialize the parameters for an LSTM cell.\n        Args:\n          num_units: int, The number of units in the LSTM cell\n          input_size: Deprecated and unused.\n          use_peepholes: bool, set True to enable diagonal/peephole connections.\n          cell_clip: (optional) A float value, if provided the cell state is clipped\n            by this value prior to the cell output activation.\n          initializer: (optional) The initializer to use for the weight and\n            projection matrices.\n          num_proj: (optional) int, The output dimensionality for the projection\n            matrices.  If None, no projection is performed.\n          proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n            provided, then the projected values are clipped elementwise to within\n            `[-proj_clip, proj_clip]`.\n          num_unit_shards: Deprecated, will be removed by Jan. 2017.\n            Use a variable_scope partitioner instead.\n          num_proj_shards: Deprecated, will be removed by Jan. 2017.\n            Use a variable_scope partitioner instead.\n          forget_bias: Biases of the forget gate are initialized by default to 1\n            in order to reduce the scale of forgetting at the beginning of\n            the training.\n          state_is_tuple: If True, accepted and returned states are 2-tuples of\n            the `c_state` and `m_state`.  If False, they are concatenated\n            along the column axis.  This latter behavior will soon be deprecated.\n          reuse: (optional) Python boolean describing whether to reuse variables\n            in an existing scope.  If not `True`, and the existing scope already has\n            the given variables, an error is raised.\n        """"""\n        if not state_is_tuple:\n            logging.warn(""%s: Using a concatenated state is slower and will soon be ""\n                         ""deprecated.  Use state_is_tuple=True."", self)\n        if input_size is not None:\n            logging.warn(""%s: The input_size parameter is deprecated."", self)\n        if num_unit_shards is not None or num_proj_shards is not None:\n            logging.warn(""%s: The num_unit_shards and proj_unit_shards parameters are ""\n                         ""deprecated and will be removed in Jan 2017.  ""\n                         ""Use a variable scope with a partitioner instead."", self)\n\n        self._num_units = num_units\n        self._use_peepholes = use_peepholes\n        self._cell_clip = cell_clip\n        self._initializer = initializer\n        self._num_proj = num_proj\n        self._proj_clip = proj_clip\n        self._num_unit_shards = num_unit_shards\n        self._num_proj_shards = num_proj_shards\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._reuse = reuse\n\n        if num_proj:\n            self._state_size = (LSTMStateTuple(num_units, num_proj)\n                                if state_is_tuple else num_units + num_proj)\n            self._output_size = num_proj\n        else:\n            self._state_size = (LSTMStateTuple(num_units, num_units)\n                                if state_is_tuple else 2 * num_units)\n            self._output_size = num_units\n\n    @property\n    def state_size(self):\n        return self._state_size\n\n    @property\n    def output_size(self):\n        return self._output_size\n\n    def __call__(self, inputs, state, scope=None):\n        """"""Run one step of LSTM.\n        Args:\n          inputs: input Tensor, 2D, batch x num_units.\n          state: if `state_is_tuple` is False, this must be a state Tensor,\n            `2-D, batch x state_size`.  If `state_is_tuple` is True, this must be a\n            tuple of state Tensors, both `2-D`, with column sizes `c_state` and\n            `m_state`.\n          scope: VariableScope for the created subgraph; defaults to ""lstm_cell"".\n        Returns:\n          A tuple containing:\n          - A `2-D, [batch x output_dim]`, Tensor representing the output of the\n            LSTM after reading `inputs` when previous state was `state`.\n            Here output_dim is:\n               num_proj if num_proj was set,\n               num_units otherwise.\n          - Tensor(s) representing the new state of LSTM after reading `inputs` when\n            the previous state was `state`.  Same type and shape(s) as `state`.\n        Raises:\n          ValueError: If input size cannot be inferred from inputs via\n            static shape inference.\n        """"""\n        num_proj = self._num_units if self._num_proj is None else self._num_proj\n\n        if self._state_is_tuple:\n            (c_prev, h_prev) = state\n        else:\n            c_prev = tf.slice(state, begin=[0, 0], size=[-1, self._num_units])\n            h_prev = tf.slice(state, begin=[0, self._num_units], size=[-1, num_proj])\n\n        dtype = inputs.dtype\n        input_size = inputs.get_shape().with_rank(2)[1]\n        if input_size.value is None:\n            raise ValueError(""Could not infer input size from inputs.get_shape()[-1]"")\n\n        with tf.variable_scope(self, scope or ""lstm_cell"", initializer=self._initializer,\n                               reuse=self._reuse) as unit_scope:\n            if self._num_unit_shards is not None:\n                unit_scope.set_partitioner(\n                    tf.fixed_size_partitioner(self._num_unit_shards))\n\n            # i = input_gate, g = new_input, f = forget_gate, o = output_gate\n            lstm_matrix = tf.contrib.rnn._linear([inputs, h_prev], 4 * self._num_units, bias=True)\n            i, g, f, o = tf.split(value=lstm_matrix, num_or_size_splits=4, axis=1)\n\n            # Diagonal connections\n            if self._use_peepholes:\n                # tf.variable_scope\xe3\x81\xa8tf.get_variable\xe3\x81\xaf\xe3\x82\xbb\xe3\x83\x83\xe3\x83\x88\xe3\x81\xa7\xe4\xbd\xbf\xe3\x81\x86\n                with tf.variable_scope(unit_scope) as projection_scope:\n                    if self._num_unit_shards is not None:\n                        projection_scope.set_partitioner(None)\n                    w_f_diag = tf.get_variable(""w_f_diag"", shape=[self._num_units], dtype=dtype)\n                    w_i_diag = tf.get_variable(""w_i_diag"", shape=[self._num_units], dtype=dtype)\n                    w_o_diag = tf.get_variable(""w_o_diag"", shape=[self._num_units], dtype=dtype)\n\n            if self._use_peepholes:\n                c = (tf.sigmoid(f + self._forget_bias + w_f_diag * c_prev) * c_prev +\n                     tf.sigmoid(i + w_i_diag * c_prev) * tf.tanh(g))\n            else:\n                c = (tf.sigmoid(f + self._forget_bias) * c_prev +\n                     tf.sigmoid(i) * tf.tanh(g))\n\n            if self._cell_clip is not None:\n                c = tf.clip_by_value(c, -self._cell_clip, self._cell_clip)\n\n            if self._use_peepholes:\n                h = tf.sigmoid(o + w_o_diag * c) * tf.tanh(c)\n            else:\n                h = tf.sigmoid(o) * tf.tanh(c)\n\n            if self._num_proj is not None:\n                with tf.variable_scope(""projection"") as proj_scope:\n                    if self._num_proj_shards is not None:\n                        proj_scope.set_partitioner(\n                            tf.fixed_size_partitioner(self._num_proj_shards))\n                    h = tf.contrib.rnn._linear(h, self._num_proj, bias=False)\n\n                if self._proj_clip is not None:\n                    h = tf.clip_by_value(h, -self._proj_clip, self._proj_clip)\n\n        new_state = (LSTMStateTuple(c, h) if self._state_is_tuple else\n                     tf.concat([c, h], 1))\n        return h, new_state\n'"
models/recurrent/layers/qrnn.py,35,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Quasi Recurrent Neural Network.""""""\n\nimport tensorflow as tf\n\n\nclass QRNN(object):\n    """"""Quasi-Recurrent Neural Networks.\n        See details in https://arxiv.org/abs/1611.01576.\n    """"""\n\n    def __init__(self, in_size, size, conv_size=2):\n        """"""\n        Args:\n            in_size:\n            size:\n            conv_size:\n        """"""\n        self.kernel = None\n        self.batch_size = -1\n        self.conv_size = conv_size\n        self.c = None\n        self.h = None\n        self._x = None\n        if conv_size == 1:\n            self.kernel = QRNNLinear(in_size, size)\n        elif conv_size == 2:\n            self.kernel = QRNNWithPrevious(in_size, size)\n        else:\n            self.kernel = QRNNConvolution(in_size, size, conv_size)\n\n    def _step(self, f, z, o):\n        """"""\n        Args:\n            f:\n            z:\n            o:\n        Returns:\n            h:\n        """"""\n        with tf.variable_scope(""fo-Pool""):\n            # f,z,o is batch_size x size\n            f = tf.sigmoid(f)\n            z = tf.tanh(z)\n            o = tf.sigmoid(o)\n            self.c = tf.mul(f, self.c) + tf.mul(1 - f, z)\n            self.h = tf.mul(o, self.c)  # h is size vector\n\n        return self.h\n\n    def forward(self, x):\n        """"""\n        Args:\n            x:\n        Returns:\n            h:\n        """"""\n        def length(mx): return int(mx.get_shape()[0])\n\n        with tf.variable_scope(""QRNN/Forward""):\n            if self.c is None:\n                # init context cell\n                self.c = tf.zeros(\n                    [length(x), self.kernel.size], dtype=tf.float32)\n\n            if self.conv_size <= 2:\n                # x is batch_size x sentence_length x word_length\n                # -> now, transpose it to sentence_length x batch_size x word_length\n                _x = tf.transpose(x, [1, 0, 2])\n\n                for i in range(length(_x)):\n                    t = _x[i]  # t is batch_size x word_length matrix\n                    f, z, o = self.kernel.forward(t)\n                    self._step(f, z, o)\n            else:\n                c_f, c_z, c_o = self.kernel.conv(x)\n                for i in range(length(c_f)):\n                    f, z, o = c_f[i], c_z[i], c_o[i]\n                    self._step(f, z, o)\n\n        return self.h\n\n\nclass QRNNLinear():\n    """"""\n    """"""\n\n    def __init__(self, in_size, size):\n        """"""\n        Args:\n            in_size:\n            size:\n        """"""\n        self.in_size = in_size\n        self.size = size\n        self._weight_size = self.size * 3  # z, f, o\n        with tf.variable_scope(""QRNN/Variable/Linear""):\n            initializer = tf.random_normal_initializer()\n            self.W = tf.get_variable(\n                ""W"", [self.in_size, self._weight_size],\n                initializer=initializer)\n            self.b = tf.get_variable(\n                ""b"", [self._weight_size], initializer=initializer)\n\n    def forward(self, t):\n        # x is batch_size x word_length matrix\n        _weighted = tf.matmul(t, self.W)\n        _weighted = tf.add(_weighted, self.b)\n\n        # now, _weighted is batch_size x weight_size\n        # split to f, z, o. each matrix is batch_size x size\n        f, z, o = tf.split(1, 3, _weighted)\n        return f, z, o\n\n\nclass QRNNWithPrevious():\n\n    def __init__(self, in_size, size):\n        """"""\n        Args:\n            in_size:\n            size:\n        """"""\n        self.in_size = in_size\n        self.size = size\n        self._weight_size = self.size * 3  # z, f, o\n        self._previous = None\n        with tf.variable_scope(""QRNN/Variable/WithPrevious""):\n            initializer = tf.random_normal_initializer()\n            self.W = tf.get_variable(\n                ""W"", [self.in_size, self._weight_size],\n                initializer=initializer)\n            self.V = tf.get_variable(\n                ""V"", [self.in_size, self._weight_size],\n                initializer=initializer)\n            self.b = tf.get_variable(\n                ""b"", [self._weight_size], initializer=initializer)\n\n    def forward(self, t):\n        if self._previous is None:\n            self._previous = tf.get_variable(\n                ""previous"", [t.get_shape()[0], self.in_size],\n                initializer=tf.random_normal_initializer())\n\n        _current = tf.matmul(t, self.W)\n        _previous = tf.matmul(self._previous, self.V)\n        _previous = tf.add(_previous, self.b)\n        _weighted = tf.add(_current, _previous)\n\n        # split to f, z, o. each matrix is batch_size x size\n        f, z, o = tf.split(1, 3, _weighted)\n        self._previous = t\n        return f, z, o\n\n\nclass QRNNConvolution():\n\n    def __init__(self, in_size, size, conv_size):\n        """"""\n        Args:\n            in_size:\n            size:\n            conv_size:\n        """"""\n        self.in_size = in_size\n        self.size = size\n        self.conv_size = conv_size\n        self._weight_size = self.size * 3  # z, f, o\n\n        with tf.variable_scope(""QRNN/Variable/Convolution""):\n            initializer = tf.random_normal_initializer()\n            self.conv_filter = tf.get_variable(\n                ""conv_filter"", [conv_size, in_size, self._weight_size],\n                initializer=initializer)\n\n    def conv(self, x):\n        # !! x is batch_size x sentence_length x word_length(=channel) !!\n        _weighted = tf.nn.conv1d(\n            x, self.conv_filter, stride=1, padding=""SAME"", data_format=""NHWC"")\n\n        # _weighted is batch_size x conved_size x output_channel\n        # conved_size x  batch_size x output_channel\n        _w = tf.transpose(_weighted, [1, 0, 2])\n        # make 3(f, z, o) conved_size x  batch_size x size\n        _ws = tf.split(2, 3, _w)\n        return _ws\n'"
models/recurrent/tests/test_tf_qrnn_forward.py,12,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport unittest\n\nsys.path.append(os.pardir)\nfrom qrnn import QRNN\n\n\nclass TestQRNNForward(unittest.TestCase):\n\n    def test_qrnn_linear_forward(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_linear:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=1)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def test_qrnn_with_previous(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_with_previous:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=2)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def test_qrnn_convolution(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_conv:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=3)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def create_test_data(self, batch_size, sentence_length, word_size):\n        batch = []\n        for b in range(batch_size):\n            sentence = np.random.rand(sentence_length, word_size)\n            batch.append(sentence)\n        return np.array(batch)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
models/recurrent/tests/test_tf_qrnn_work.py,38,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport time\nimport functools\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.python.ops import rnn, rnn_cell\nimport unittest\n\nsys.path.append(os.pardir)\nfrom qrnn import QRNN\n\n\ndef measure_time(func):\n    @functools.wraps(func)\n    def _measure_time(*args, **kwargs):\n        start = time.time()\n        func(*args, **kwargs)\n        elapse = time.time() - start\n        print(""takes {} seconds."".format(elapse))\n    return _measure_time\n\n\nclass TestQRNNWork(unittest.TestCase):\n\n    @measure_time\n    def test_qrnn(self):\n        print(""QRNN Working check"")\n        with tf.Graph().as_default() as qrnn:\n            self.check_by_digits(qrnn, qrnn=5)\n\n    @measure_time\n    def test_baseline(self):\n        print(""Baseline(LSTM) Working check"")\n        with tf.Graph().as_default() as baseline:\n            self.check_by_digits(baseline, baseline=True)\n\n    @measure_time\n    def test_random(self):\n        print(""Random Working check"")\n        with tf.Graph().as_default() as random:\n            self.check_by_digits(random, random=True)\n\n    def check_by_digits(self, graph, qrnn=-1, baseline=False, random=False):\n        digits = load_digits()\n        horizon, vertical, n_class = (8, 8, 10)  # 8 x 8 image, 0~9 number(=10 class)\n        size = 128  # state vector size\n        batch_size = 128\n        images = digits.images / np.max(digits.images)  # simple normalization\n        target = np.array([[1 if t == i else 0 for i in range(n_class)]\n                           for t in digits.target])  # to 1 hot vector\n        learning_rate = 0.001\n        train_iter = 1000\n        summary_dir = os.path.join(os.path.dirname(__file__), ""./summary"")\n\n        with tf.name_scope(""placeholder""):\n            X = tf.placeholder(tf.float32, [batch_size, vertical, horizon])\n            y = tf.placeholder(tf.float32, [batch_size, n_class])\n\n        if qrnn > 0:\n            pred = self.qrnn_forward(X, size, n_class, batch_size, conv_size=qrnn)\n            summary_dir += ""/qrnn""\n        elif baseline:\n            pred = self.baseline_forward(X, size, n_class)\n            summary_dir += ""/lstm""\n        else:\n            pred = self.random_forward(X, size, n_class)\n            summary_dir += ""/random""\n\n        with tf.name_scope(""optimization""):\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n        with tf.name_scope(""evaluation""):\n            correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n        with tf.name_scope(""summary""):\n            tf.summary.scalar(""loss"", loss)\n            tf.summary.scalar(""accuracy"", accuracy)\n            merged = tf.summary.merge_all()\n        writer = tf.summary.FileWriter(summary_dir, graph)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            for i in range(train_iter):\n                indices = np.random.randint(len(digits.target) - batch_size, size=batch_size)\n                _X = images[indices]\n                _y = target[indices]\n                sess.run(optimizer, feed_dict={X: _X, y: _y})\n\n                if i % 100 == 0:\n                    _loss, _accuracy, _merged = sess.run(\n                        [loss, accuracy, merged], feed_dict={X: _X, y: _y})\n                    writer.add_summary(_merged, i)\n                    print(""Iter {}: loss={}, accuracy={}"".format(i, _loss, _accuracy))\n\n            with tf.name_scope(""test-evaluation""):\n                acc = sess.run(accuracy, feed_dict={\n                               X: images[-batch_size:], y: target[-batch_size:]})\n                print(""Testset Accuracy={}"".format(acc))\n\n    def baseline_forward(self, X, size, n_class):\n        shape = X.get_shape()\n        # batch_size x sentence_length x word_length -> batch_size x sentence_length x word_length\n        _X = tf.transpose(X, [1, 0, 2])\n        _X = tf.reshape(_X, [-1, int(shape[2])])  # (batch_size x sentence_length) x word_length\n        seq = tf.split(0, int(shape[1]), _X)  # sentence_length x (batch_size x word_length)\n\n        with tf.name_scope(""LSTM""):\n            lstm_cell = rnn_cell.BasicLSTMCell(size, forget_bias=1.0)\n            outputs, states = rnn.rnn(lstm_cell, seq, dtype=tf.float32)\n\n        with tf.name_scope(""LSTM-Classifier""):\n            W = tf.Variable(tf.random_normal([size, n_class]), name=""W"")\n            b = tf.Variable(tf.random_normal([n_class]), name=""b"")\n            output = tf.matmul(outputs[-1], W) + b\n\n        return output\n\n    def random_forward(self, X, size, n_class):\n        batch_size = int(X.get_shape()[0])\n\n        with tf.name_scope(""Random-Classifier""):\n            rand_vector = tf.random_normal([batch_size, size])  # batch_size x size random vector\n            W = tf.Variable(tf.random_normal([size, n_class]), name=""W"")\n            b = tf.Variable(tf.random_normal([n_class]), name=""b"")\n            output = tf.matmul(rand_vector, W) + b\n        return output\n\n    def qrnn_forward(self, X, size, n_class, batch_size, conv_size):\n        in_size = int(X.get_shape()[2])\n\n        qrnn = QRNN(in_size=in_size, size=size, conv_size=conv_size)\n        hidden = qrnn.forward(X)\n\n        with tf.name_scope(""QRNN-Classifier""):\n            W = tf.Variable(tf.random_normal([size, n_class]), name=""W"")\n            b = tf.Variable(tf.random_normal([n_class]), name=""b"")\n            output = tf.add(tf.matmul(hidden, W), b)\n\n        return output\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
utils/io/inputs/__init__.py,0,b''
utils/io/inputs/feature_extraction.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport scipy.io.wavfile\nfrom python_speech_features import mfcc, fbank\n\n\ndef wav2feature(wav_paths, feature_type=\'logfbank\', feature_dim=40,\n                energy=True, delta1=True, delta2=True):\n    """"""Read wav file & convert to MFCC or log mel filterbank features.\n    Args:\n        wav_paths (list): paths to a wav file\n        batch_size (int, optional): the batch size\n        feature_type (string, optional): logfbank or fbank or mfcc\n        feature_dim (int, optional): the demension of each feature\n        energy (bool, optional): if True, add energy\n        delta1 (bool, optional): if True, add delta features\n        delta2 (bool, optional): if True, add delta delta features\n    Returns:\n        inputs: A tensor of size `[B, T, input_size]`\n        inputs_seq_len: A tensor of size `[B]`\n    """"""\n    if feature_type not in [\'logmelfbank\', \'logfbank\', \'fbank\', \'mfcc\']:\n        raise ValueError(\n            \'feature_type is ""logmelfbank"" or ""logfbank"" or ""fbank"" or ""mfcc"".\')\n    if not isinstance(wav_paths, list):\n        raise ValueError(\'wav_paths must be a list.\')\n    if delta2 and not delta1:\n        delta1 = True\n\n    batch_size = len(wav_paths)\n    max_time = 0\n    for wav_path in wav_paths:\n        # Read wav file\n        fs, audio = scipy.io.wavfile.read(wav_path)\n        if len(audio) > max_time:\n            max_time = len(audio)\n    input_size = feature_dim\n    if energy:\n        input_size + 1\n    if delta2:\n        input_size *= 3\n    elif delta1:\n        input_size *= 2\n\n    inputs = None\n    inputs_seq_len = np.zeros((batch_size,), dtype=np.int32)\n    for i, wav_path in enumerate(wav_paths):\n        if feature_type == \'mfcc\':\n            feat = mfcc(audio, samplerate=fs, numcep=feature_dim)\n            if energy:\n                energy_feat = fbank(audio, samplerate=fs, nfilt=feature_dim)[1]\n                feat = np.c_[feat, energy_feat]\n        else:\n            fbank_feat, energy_feat = fbank(\n                audio, samplerate=fs, nfilt=feature_dim)\n            if feature_type == \'logfbank\':\n                fbank_feat = np.log(fbank_feat)\n            feat = fbank_feat\n            if energy:\n                # logenergy = np.log(energy_feat)\n                feat = np.c_[feat, energy_feat]\n\n        if delta2:\n            delta1_feat = _delta(feat, N=2)\n            delta2_feat = _delta(delta1_feat, N=2)\n            feat = np.c_[feat, delta1_feat, delta2_feat]\n        elif delta1:\n            delta1_feat = _delta(feat, N=2)\n            feat = np.c_[feat, delta1_feat]\n\n        # Normalize per wav\n        feat = (feat - np.mean(feat)) / np.std(feat)\n\n        if inputs is None:\n            max_time = feat.shape[0]\n            input_size = feat.shape[-1]\n            inputs = np.zeros((batch_size, max_time, input_size))\n\n        inputs[i] = feat\n        inputs_seq_len[i] = len(feat)\n\n    return inputs, inputs_seq_len\n\n\ndef _delta(feat, N):\n    """"""Compute delta features from a feature vector sequence.\n    Args:\n        feat: A numpy array of size (NUMFRAMES by number of features)\n            containing features. Each row holds 1 feature vector.\n        N: For each frame, calculate delta features based on preceding and\n            following N frames\n    Returns:\n        A numpy array of size (NUMFRAMES by number of features) containing\n            delta features. Each row holds 1 delta feature vector.\n    """"""\n    if N < 1:\n        raise ValueError(\'N must be an integer >= 1\')\n    NUMFRAMES = len(feat)\n    denominator = 2 * sum([i**2 for i in range(1, N + 1)])\n    delta_feat = np.empty_like(feat)\n    # padded version of feat\n    padded = np.pad(feat, ((N, N), (0, 0)), mode=\'edge\')\n    for t in range(NUMFRAMES):\n        # [t : t+2*N+1] == [(N+t)-N : (N+t)+N+1]\n        delta_feat[t] = np.dot(np.arange(-N, N + 1),\n                               padded[t: t + 2 * N + 1]) / denominator\n    return delta_feat\n'"
utils/io/inputs/frame_stacking.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport math\n\nfrom utils.progressbar import wrap_iterator\n\n\ndef stack_frame(input_list, num_stack, num_skip, progressbar=False):\n    """"""Stack & skip some frames. This implementation is based on\n       https://arxiv.org/abs/1507.06947.\n           Sak, Ha\xc5\x9fim, et al.\n           ""Fast and accurate recurrent neural network acoustic models for speech recognition.""\n           arXiv preprint arXiv:1507.06947 (2015).\n    Args:\n        input_list (list): list of input data\n        num_stack (int): the number of frames to stack\n        num_skip (int): the number of frames to skip\n        progressbar (bool, optional): if True, visualize progressbar\n    Returns:\n        input_list_new (list): list of frame-stacked inputs\n    """"""\n    if num_stack == 1 and num_stack == 1:\n        return input_list\n\n    if num_stack < num_skip:\n        raise ValueError(\'num_skip must be less than num_stack.\')\n\n    batch_size = len(input_list)\n\n    input_list_new = []\n    for i_batch in wrap_iterator(range(batch_size), progressbar):\n\n        frame_num, input_size = input_list[i_batch].shape\n        frame_num_new = math.ceil(frame_num / num_skip)\n\n        stacked_frames = np.zeros((frame_num_new, input_size * num_stack))\n        stack_count = 0  # counter\n        stack = []\n        for t, frame_t in enumerate(input_list[i_batch]):\n            #####################\n            # final frame\n            #####################\n            if t == len(input_list[i_batch]) - 1:\n                # Stack the final frame\n                stack.append(frame_t)\n\n                while stack_count != int(frame_num_new):\n                    # Concatenate stacked frames\n                    for i_stack in range(len(stack)):\n                        stacked_frames[stack_count][input_size *\n                                                    i_stack:input_size * (i_stack + 1)] = stack[i_stack]\n                    stack_count += 1\n\n                    # Delete some frames to skip\n                    for _ in range(num_skip):\n                        if len(stack) != 0:\n                            stack.pop(0)\n\n            ########################\n            # first & middle frames\n            ########################\n            elif len(stack) < num_stack:\n                # Stack some frames until stack is filled\n                stack.append(frame_t)\n\n                if len(stack) == num_stack:\n                    # Concatenate stacked frames\n                    for i_stack in range(num_stack):\n                        stacked_frames[stack_count][input_size *\n                                                    i_stack:input_size * (i_stack + 1)] = stack[i_stack]\n                    stack_count += 1\n\n                    # Delete some frames to skip\n                    for _ in range(num_skip):\n                        stack.pop(0)\n\n        input_list_new.append(stacked_frames)\n\n    return np.array(input_list_new)\n'"
utils/io/inputs/splicing.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Splice data.""""""\n\nimport numpy as np\n\n\ndef do_splice(inputs, splice=1, batch_size=1, num_stack=1):\n    """"""Splice input data. This is expected to be used for CNN-like models.\n    Args:\n        inputs (np.ndarray): list of size\n            `[B, T, input_size (num_channels * 3 * num_stack)]\'\n        splice (int): frames to splice. Default is 1 frame.\n            ex.) if splice == 11\n                [t-5, ..., t-1, t, t+1, ..., t+5] (total 11 frames)\n        batch_size (int): the size of mini-batch\n        num_stack (int, optional): the number of frames to stack\n    Returns:\n        data_spliced (np.ndarray): A tensor of size\n            `[B, T, num_channels * (splice * num_stack) * 3 (static + \xce\x94 + \xce\x94\xce\x94)]`\n    """"""\n    assert isinstance(inputs, np.ndarray), \'inputs should be np.ndarray.\'\n    assert len(inputs.shape) == 3, \'inputs must be 3 demension.\'\n    assert inputs.shape[-1] % 3 == 0\n\n    if splice == 1:\n        return inputs\n\n    batch_size, max_time, input_size = inputs.shape\n    num_channels = (input_size // 3) // num_stack\n    input_data_spliced = np.zeros(\n        (batch_size, max_time, num_channels * (splice * num_stack) * 3))\n\n    for i_batch in range(batch_size):\n        for i_time in range(max_time):\n            spliced_frames = np.zeros((splice * num_stack, num_channels, 3))\n            for i_splice in range(0, splice, 1):\n                #########################\n                # padding left frames\n                #########################\n                if i_time <= splice - 1 and i_splice < splice - i_time:\n                    # copy the first frame to left side\n                    copy_frame = inputs[i_batch][0]\n\n                #########################\n                # padding right frames\n                #########################\n                elif max_time - splice <= i_time and i_time + (i_splice - splice) > max_time - 1:\n                    # copy the last frame to right side\n                    copy_frame = inputs[i_batch][-1]\n\n                #########################\n                # middle of frames\n                #########################\n                else:\n                    copy_frame = inputs[i_batch][i_time + (i_splice - splice)]\n\n                # `[num_channels * 3 * num_stack]` -> `[num_channels, 3, num_stack]`\n                copy_frame = copy_frame.reshape((num_channels, 3, num_stack))\n\n                # `[num_channels, 3, num_stack]` -> `[num_stack, num_channels, 3]`\n                copy_frame = np.transpose(copy_frame, (2, 0, 1))\n\n                spliced_frames[i_splice: i_splice + num_stack] = copy_frame\n\n            # `[splice * num_stack, num_channels, 3] -> `[num_channels, splice * num_stack, 3]`\n            spliced_frames = np.transpose(spliced_frames, (1, 0, 2))\n\n            input_data_spliced[i_batch][i_time] = spliced_frames.reshape(\n                (num_channels * (splice * num_stack) * 3))\n\n    return input_data_spliced\n\n\ndef test():\n    sequence = np.zeros((3, 100, 5))\n    for i_batch in range(sequence.shape[0]):\n        for i_frame in range(sequence.shape[1]):\n            sequence[i_batch][i_frame][0] = i_frame\n    sequence_spliced = do_splice(sequence, splice=11)\n    assert sequence_spliced.shape == (3, 100, 5 * 11)\n\n    # for i in range(sequence_spliced.shape[1]):\n    #     print(sequence_spliced[0][i])\n\n\nif __name__ == \'__main__\':\n    test()\n'"
utils/io/labels/__init__.py,0,b''
utils/io/labels/character.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\nclass Char2idx(object):\n    """"""Convert from character to index.\n    Args:\n        map_file_path (string): path to the mapping file\n        double_letter (bool, optional): if True, group repeated letters\n    """"""\n\n    def __init__(self, map_file_path, double_letter=False):\n\n        self.double_letter = double_letter\n\n        # Read the mapping file\n        self.map_dict = {}\n        with open(map_file_path, \'r\') as f:\n            for line in f:\n                line = line.strip().split()\n                self.map_dict[line[0]] = int(line[1])\n\n    def __call__(self, str_char):\n        """"""\n        Args:\n            str_char (string): string of characters\n        Returns:\n            char_list (list): character indices\n        """"""\n        char_list = list(str_char)\n\n        # Convert from character to index\n        if self.double_letter:\n            skip_flag = False\n            for i in range(len(char_list) - 1):\n                if skip_flag:\n                    char_list[i] = \'\'\n                    skip_flag = False\n                    continue\n\n                if char_list[i] + char_list[i + 1] in self.map_dict.keys():\n                    char_list[i] = self.map_dict[char_list[i] +\n                                                 char_list[i + 1]]\n                    skip_flag = True\n                else:\n                    char_list[i] = self.map_dict[char_list[i]]\n\n            # Final character\n            if skip_flag:\n                char_list[-1] = \'\'\n            else:\n                char_list[-1] = self.map_dict[char_list[-1]]\n\n            # Remove skipped characters\n            while \'\' in char_list:\n                char_list.remove(\'\')\n        else:\n            for i in range(len(char_list)):\n                char_list[i] = self.map_dict[char_list[i]]\n\n        return char_list\n\n\nclass Idx2char(object):\n    """"""Convert from index to character.\n    Args:\n        map_file_path (string): path to the mapping file\n        capital_divide (bool, optional): set True when using capital-divided\n            character sequences\n        space_mark (string): the space mark to divide a sequence into words\n    """"""\n\n    def __init__(self, map_file_path, capital_divide=False, space_mark=\' \'):\n        self.capital_divide = capital_divide\n        self.space_mark = space_mark\n\n        # Read the mapping file\n        self.map_dict = {}\n        with open(map_file_path, \'r\') as f:\n            for line in f:\n                line = line.strip().split()\n                self.map_dict[int(line[1])] = line[0]\n\n    def __call__(self, index_list, padded_value=-1):\n        """"""\n        Args:\n            index_list (np.ndarray): list of character indices.\n                Batch size 1 is expected.\n            padded_value (int): the value used for padding\n        Returns:\n            str_char (string): a sequence of characters\n        """"""\n        # Remove padded values\n        assert type(\n            index_list) == np.ndarray, \'index_list should be np.ndarray.\'\n        index_list = np.delete(index_list, np.where(\n            index_list == padded_value), axis=0)\n\n        # Convert from indices to the corresponding characters\n        char_list = list(map(lambda x: self.map_dict[x], index_list))\n\n        if self.capital_divide:\n            char_list_tmp = []\n            for i in range(len(char_list)):\n                if i != 0 and \'A\' <= char_list[i] <= \'Z\':\n                    char_list_tmp += [self.space_mark, char_list[i].lower()]\n                else:\n                    char_list_tmp += [char_list[i].lower()]\n            str_char = \'\'.join(char_list_tmp)\n        else:\n            str_char = \'\'.join(char_list)\n\n        return str_char\n        # TODO: change to batch version\n'"
utils/io/labels/phone.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\nclass Phone2idx(object):\n    """"""Convert from phone to index.\n    Args:\n        map_file_path (string): path to the mapping file\n    """"""\n\n    def __init__(self, map_file_path):\n        # Read the mapping file\n        self.map_dict = {}\n        with open(map_file_path, \'r\') as f:\n            for line in f:\n                line = line.strip().split(\'  \')\n                self.map_dict[str(line[0])] = int(line[1])\n\n    def __call__(self, phone_list):\n        """"""\n        Args:\n            phone_list (list): list of phones (string)\n        Returns:\n            phone_list (list): phone indices\n        """"""\n        # Convert from phone to index\n        for i in range(len(phone_list)):\n            phone_list[i] = self.map_dict[phone_list[i]]\n        return np.array(phone_list)\n\n\nclass Idx2phone(object):\n    """"""Convert from index to phone.\n    Args:\n        map_file_path (string): path to the mapping file\n    """"""\n\n    def __init__(self, map_file_path):\n        # Read the mapping file\n        self.map_dict = {}\n        with open(map_file_path, \'r\') as f:\n            for line in f:\n                line = line.strip().split()\n                self.map_dict[int(line[1])] = line[0]\n\n    def __call__(self, index_list, padded_value=-1):\n        """"""\n        Args:\n            index_list (list): phone indices\n            padded_value (int): the value used for padding\n        Returns:\n            str_phone (string): a sequence of phones\n        """"""\n        # Remove padded values\n        assert type(index_list) == np.ndarray, \'index_list should be np.ndarray.\'\n        index_list = np.delete(index_list, np.where(index_list == -1), axis=0)\n\n        # Convert from indices to the corresponding phones\n        phone_list = list(map(lambda x: self.map_dict[x], index_list))\n        str_phone = \' \'.join(phone_list)\n\n        return str_phone\n'"
utils/io/labels/sparsetensor.py,1,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef list2sparsetensor(labels, padded_value):\n    """"""Convert labels from list to sparse tensor.\n    Args:\n        labels (list): list of labels, size of `[B, max_label_len]`\n        padded_value (int): the value used for padding\n    Returns:\n        labels_st: A SparseTensor of labels,\n            list of (indices, values, dense_shape)\n    """"""\n    if padded_value is None:\n        dtype_values = np.uint8\n    else:\n        dtype_values = np.int32\n\n    indices, values = [], []\n    for i_utt, each_label in enumerate(labels):\n        for i_l, l in enumerate(each_label):\n            # NOTE: -1 or None means empty\n            if l == padded_value:\n                break\n            indices.append([i_utt, i_l])\n            values.append(l)\n    dense_shape = [len(labels), np.asarray(indices).max(0)[1] + 1]\n    labels_st = [np.array(indices, dtype=np.int64),\n                 np.array(values, dtype=dtype_values),\n                 np.array(dense_shape, dtype=np.int64)]\n\n    return labels_st\n\n\ndef sparsetensor2list(labels_st, batch_size):\n    """"""Convert labels from sparse tensor to list.\n    Args:\n        labels_st: A SparseTensor of labels\n        batch_size (int): the size of mini-batch\n    Returns:\n        labels (list): list of np.ndarray, size of `[B]`. Each element is a\n            sequence of target labels of an input.\n    """"""\n    if isinstance(labels_st, tf.SparseTensorValue):\n        # Output of TensorFlow\n        indices = labels_st.indices\n        values = labels_st.values\n    else:\n        # labels_st is expected to be a list [indices, values, shape]\n        indices = labels_st[0]\n        values = labels_st[1]\n\n    if batch_size == 1:\n        return values.reshape((1, -1))\n\n    labels = []\n    batch_boundary = np.where(indices[:, 1] == 0)[0]\n\n    # TODO: Some errors occurred when ctc models do not output any labels\n    # print(batch_boundary)\n    # if len(batch_boundary) != batch_size:\n    #     batch_boundary = np.array(batch_boundary.tolist() + [max(batch_boundary) + 1])\n    # print(indices)\n\n    for i in range(batch_size - 1):\n        label_each_utt = values[batch_boundary[i]:batch_boundary[i + 1]]\n        labels.append(label_each_utt)\n    # Last label\n    labels.append(values[batch_boundary[-1]:])\n\n    return labels\n'"
utils/io/labels/word.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\nclass Idx2word(object):\n    """"""Convert from index to word.\n    Args:\n        map_file_path (string): path to the mapping file\n    """"""\n\n    def __init__(self, map_file_path):\n        # Read the mapping file\n        self.map_dict = {}\n        with open(map_file_path, \'r\') as f:\n            for line in f:\n                line = line.strip().split()\n                self.map_dict[int(line[1])] = line[0]\n\n    def __call__(self, index_list, padded_value=-1):\n        """"""\n        Args:\n            index_list (np.ndarray): list of word indices.\n                Batch size 1 is expected.\n            padded_value (int): the value used for padding\n        Returns:\n            word_list (list): list of words\n        """"""\n        # Remove padded values\n        assert type(index_list) == np.ndarray, \'index_list should be np.ndarray.\'\n        index_list = np.delete(index_list, np.where(index_list == -1), axis=0)\n\n        # Convert from indices to the corresponding words\n        word_list = list(map(lambda x: self.map_dict[x], index_list))\n\n        return word_list\n'"
examples/csj/data/test/test_load_dataset_attention.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport sys\nimport unittest\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.csj.data.load_dataset_attention import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetAttention(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(label_type='kanji', data_type='train')\n        self.check(label_type='kanji', data_type='dev')\n        self.check(label_type='kanji', data_type='eval1')\n        self.check(label_type='kanji', data_type='eval2')\n        self.check(label_type='kanji', data_type='eval3')\n\n        # label_type\n        self.check(label_type='kanji_divide')\n        self.check(label_type='kana')\n        self.check(label_type='kana_divide')\n\n        # sort\n        self.check(label_type='kanji', sort_utt=True)\n        self.check(label_type='kanji', sort_utt=True,\n                   sort_stop_epoch=True)\n\n        # frame stacking\n        self.check(label_type='kanji', frame_stacking=True)\n\n        # splicing\n        self.check(label_type='kanji', splice=11)\n\n        # multi-GPU\n        self.check(label_type='kanji', num_gpu=8)\n\n    @measure_time\n    def check(self, label_type, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1, num_gpu=1):\n\n        print('========================================')\n        print('  label_type: %s' % label_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('  num_gpu: %d' % num_gpu)\n        print('========================================')\n\n        if 'kana' in label_type:\n            map_file_path = '../../metrics/mapping_files/' + label_type + '.txt'\n        elif 'kanji' in label_type:\n            map_file_path = '../../metrics/mapping_files/' + \\\n                label_type + '_train_subset.txt'\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, train_data_size='train_subset',\n            label_type=label_type, map_file_path=map_file_path,\n            batch_size=64,  max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True, num_gpu=num_gpu)\n\n        print('=> Loading mini-batch...')\n\n        idx2char = Idx2char(map_file_path)\n        # idx2word = Idx2word(map_file_path)\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, labels_seq_len, input_names = data\n\n            if data_type == 'train':\n                for i, l in zip(inputs[0], labels[0]):\n                    if len(i) < len(l):\n                        raise ValueError(\n                            'input length must be longer than label length.')\n\n            if num_gpu > 1:\n                for inputs_gpu in inputs:\n                    print(inputs_gpu.shape)\n\n            if 'eval' in data_type:\n                str_true = labels[0][0][0]\n            else:\n                # if 'word' in label_type:\n                #     str_true = '_'.join(idx2word(labels[0][0]))\n                # else:\n                str_true = idx2char(labels[0][0][0: labels_seq_len[0][0]])\n\n            print('----- %s (epoch: %.3f) -----' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0].shape)\n            print(str_true)\n\n            if dataset.epoch_detail >= 0.1:\n                break\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/csj/data/test/test_load_dataset_ctc.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.csj.data.load_dataset_ctc import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetCTC(unittest.TestCase):\n\n    def test(self):\n\n        self.length_check = False\n\n        # data_type\n        # self.check(label_type='kanji', data_type='train')\n        self.check(label_type='kanji', data_type='dev')\n        self.check(label_type='kanji', data_type='eval1')\n        self.check(label_type='kanji', data_type='eval2')\n        self.check(label_type='kanji', data_type='eval3')\n\n        # label_type\n        self.check(label_type='kanji')\n        self.check(label_type='kanji_divide')\n        self.check(label_type='kana')\n        self.check(label_type='kana_divide')\n\n        # sort\n        self.check(label_type='kanji', sort_utt=True)\n        self.check(label_type='kanji', sort_utt=True,\n                   sort_stop_epoch=True)\n        self.check(label_type='kanji', shuffle=True)\n\n        # frame stacking\n        self.check(label_type='kanji', frame_stacking=True)\n\n        # splicing\n        self.check(label_type='kanji', splice=11)\n\n        # multi-GPU\n        self.check(label_type='kanji', num_gpu=8)\n\n    @measure_time\n    def check(self, label_type, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1, num_gpu=1):\n\n        print('========================================')\n        print('  label_type: %s' % label_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('  num_gpu: %d' % num_gpu)\n        print('========================================')\n\n        if 'kana' in label_type:\n            map_file_path = '../../metrics/mapping_files/' + label_type + '.txt'\n        elif 'kanji' in label_type:\n            map_file_path = '../../metrics/mapping_files/' + \\\n                label_type + '_train_subset.txt'\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, train_data_size='train_subset',\n            label_type=label_type,\n            batch_size=64, max_epoch=1, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True, num_gpu=num_gpu)\n\n        print('=> Loading mini-batch...')\n        idx2char = Idx2char(map_file_path)\n        # idx2word = Idx2word(map_file_path)\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, input_names = data\n\n            if data_type == 'train':\n                for i, l in zip(inputs[0], labels[0]):\n                    if len(i) < len(l):\n                        raise ValueError(\n                            'input length must be longer than label length.')\n\n            if num_gpu > 1:\n                for inputs_gpu in inputs:\n                    print(inputs_gpu.shape)\n\n            if 'eval' in data_type:\n                str_true = labels[0][0][0]\n            else:\n                # if 'word' in label_type:\n                #     str_true = '_'.join(idx2word(labels[0][0]))\n                # else:\n                str_true = idx2char(labels[0][0])\n\n            print('----- %s (epoch: %.3f) -----' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0].shape)\n            print(str_true)\n\n            # if dataset.epoch_detail >= 0.1:\n            #     break\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/csj/data/test/test_load_dataset_multitask_ctc.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport sys\nimport unittest\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.csj.data.load_dataset_multitask_ctc import Dataset\nfrom utils.io.labels.character import idx2char\nfrom utils.io.labels.phone import idx2phone\nfrom utils.measure_time_func import measure_time\n\n\nclass TestReadDatasetCTC(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           data_type='train')\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           data_type='dev')\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           data_type='eval1')\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           data_type='eval2')\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           data_type='eval3')\n\n        # label_type\n        self.check_loading(label_type_main='kanji', label_type_sub='phone')\n        self.check_loading(label_type_main='kanji_wakachi',\n                           label_type_sub='phone')\n        self.check_loading(label_type_main='kana', label_type_sub='phone')\n        self.check_loading(label_type_main='kana_wakachi',\n                           label_type_sub='phone')\n\n        # sort\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           sort_utt=True)\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           sort_utt=True, sort_stop_epoch=True)\n\n        # frame stacking\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           frame_stacking=True)\n\n        # splicing\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           splice=11)\n\n        # multi-GPU\n        self.check_loading(label_type_main='kanji', label_type_sub='kana',\n                           num_gpu=8)\n\n    @measure_time\n    def check_loading(self, label_type_main, label_type_sub, data_type='dev',\n                      sort_utt=False, sort_stop_epoch=None,\n                      frame_stacking=False, splice=1, num_gpu=1):\n\n        print('========================================')\n        print('  label_type_main: %s' % label_type_main)\n        print('  label_type_sub: %s' % label_type_sub)\n        print('  data_type: %s' % data_type)\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('  num_gpu: %d' % num_gpu)\n        print('========================================')\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, train_data_size='train_fullset',\n            label_type_main=label_type_main,\n            label_type_sub=label_type_sub,\n            batch_size=64, max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True, num_gpu=num_gpu)\n\n        print('=> Loading mini-batch...')\n        if label_type_main == 'kanji':\n            map_file_path_main = '../../metrics/mapping_files/ctc/kanji.txt'\n            map_fn_main = idx2char\n        elif label_type_main == 'kana':\n            map_file_path_main = '../../metrics/mapping_files/ctc/kana.txt'\n            map_fn_main = idx2char\n\n        if label_type_sub == 'kana':\n            map_file_path_sub = '../../metrics/mapping_files/ctc/kana.txt'\n            map_fn_sub = idx2char\n        elif label_type_sub == 'phone':\n            map_file_path_sub = '../../metrics/mapping_files/ctc/phone.txt'\n            map_fn_sub = idx2phone\n\n        for data, is_new_epoch in dataset:\n            inputs, labels_main, labels_sub, inputs_seq_len, input_names = data\n\n            if num_gpu > 1:\n                for inputs_gpu in inputs:\n                    print(inputs_gpu.shape)\n                inputs = inputs[0]\n                labels_main = labels_main[0]\n                labels_sub = labels_sub[0]\n\n            str_true_main = map_fn_main(labels_main[0], map_file_path_main)\n            str_true_main = re.sub(r'_', ' ', str_true_main)\n            str_true_sub = map_fn_sub(labels_sub[0], map_file_path_sub)\n            print('----- %s (epoch: %.3f) -----' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(str_true_main)\n            print(str_true_sub)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/erato/data/test/test_load_dataset_attention.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\nimport numpy as np\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.erato.data.load_dataset_attention import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetAttention(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(ss_type='insert_both', data_type='train')\n        self.check(ss_type='insert_both', data_type='dev')\n        self.check(ss_type='insert_both', data_type='test')\n\n        # ss_type\n        self.check(ss_type='insert_left', data_type='dev')\n        self.check(ss_type='insert_right', data_type='dev')\n        self.check(ss_type='remove', data_type='dev')\n\n        # sort\n        self.check(ss_type='insert_both', sort_utt=True)\n        self.check(ss_type='insert_both', sort_utt=True,\n                   sort_stop_epoch=2)\n        self.check(ss_type='insert_both', shuffle=True)\n\n        # frame stacking\n        self.check(ss_type='insert_both', frame_stacking=True)\n\n        # splicing\n        self.check(ss_type='insert_both', splice=11)\n\n    @measure_time\n    def check(self, ss_type, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1):\n\n        print('========================================')\n        print('  label_type: %s' % 'kana')\n        print('  ss_type: %s' % ss_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('========================================')\n\n        map_file_path = '../../metrics/mapping_files/kana_' + ss_type + '.txt'\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, label_type='kana', ss_type=ss_type,\n            batch_size=64, map_file_path=map_file_path,\n            max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True)\n\n        print('=> Loading mini-batch...')\n        map_fn = Idx2char(map_file_path=map_file_path)\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, labels_seq_len, input_names = data\n\n            if data_type == 'train':\n                for i_batch, l_batch in zip(inputs[0], labels[0]):\n                    if len(np.where(l_batch == dataset.padded_value)[0]) > 0:\n                        if i_batch.shape[0] < np.where(l_batch == dataset.padded_value)[0][0]:\n                            raise ValueError(\n                                'input length must be longer than label length.')\n                    else:\n                        if i_batch.shape[0] < len(l_batch):\n                            raise ValueError(\n                                'input length must be longer than label length.')\n\n            if data_type != 'test':\n                str_true = map_fn(labels[0][0][:labels_seq_len[0][0]])\n            else:\n                str_true = labels[0][0][0]\n\n            print('----- %s ----- (epoch: %.3f)' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0][0].shape)\n            print(str_true)\n\n            if dataset.epoch_detail >= 0.2:\n                break\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/erato/data/test/test_load_dataset_ctc.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\nimport numpy as np\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.erato.data.load_dataset_ctc import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetCTC(unittest.TestCase):\n\n    def test(self):\n\n        # self.count_labels()\n\n        # data_type\n        self.check(ss_type='insert_both', data_type='train')\n        self.check(ss_type='insert_both', data_type='dev')\n        self.check(ss_type='insert_both', data_type='test')\n\n        # ss_type\n        self.check(ss_type='insert_left', data_type='dev')\n        self.check(ss_type='insert_right', data_type='dev')\n        self.check(ss_type='remove', data_type='dev')\n\n        # sort\n        self.check(ss_type='insert_both', sort_utt=True)\n        self.check(ss_type='insert_both', sort_utt=True,\n                   sort_stop_epoch=2)\n        self.check(ss_type='insert_both', shuffle=True)\n\n        # frame stacking\n        self.check(ss_type='insert_both', frame_stacking=True)\n\n        # splicing\n        self.check(ss_type='insert_both', splice=11)\n\n    def count_labels(self):\n\n        train_data = Dataset(\n            data_type='train', label_type='kana', ss_type='insert_left',\n            batch_size=1,\n            sort_utt=True, sorta_grad=False, progressbar=True)\n        dev_data = Dataset(\n            data_type='dev', label_type='kana', ss_type='insert_left',\n            batch_size=1,\n            sort_utt=True, sorta_grad=False, progressbar=True)\n        test_data = Dataset(\n            data_type='test', label_type='kana', ss_type='insert_left',\n            batch_size=1,\n            sort_utt=True, sorta_grad=False, progressbar=True,\n        )\n\n        for dataset in [train_data, dev_data, test_data]:\n            input_frame_count = 0\n            laughter_count = 0\n            filler_count = 0\n            backchannel_count = 0\n            disfluency_count = 0\n\n            laughter_index = 147\n            filler_index = 148\n            backchannel_index = 149\n            disfluency_index = 150\n\n            for data, next_epoch_flag in dataset():\n                inputs, labels, _, _ = data\n\n                input_frame_count += len(inputs[0])\n                laughter_count += len(np.where(labels[0]\n                                               == laughter_index)[0])\n                filler_count += len(np.where(labels[0] == filler_index)[0])\n                backchannel_count += len(\n                    np.where(labels[0] == backchannel_index)[0])\n                disfluency_count += len(\n                    np.where(labels[0] == disfluency_index)[0])\n\n            print('Data size: %f hours' % (input_frame_count / (100 * 3600)))\n            print('The number of social signals')\n            print('  Laughter: %d' % laughter_count)\n            print('  Filler: %d' % filler_count)\n            print('  Backchannel: %d' % backchannel_count)\n            print('  Disfluency: %d' % disfluency_count)\n\n    @measure_time\n    def check(self, ss_type, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1):\n\n        print('========================================')\n        print('  label_type: %s' % 'kana')\n        print('  ss_type: %s' % ss_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('========================================')\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, label_type='kana', ss_type=ss_type,\n            batch_size=64, max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True)\n\n        print('=> Loading mini-batch...')\n        map_fn = Idx2char(\n            map_file_path='../../metrics/mapping_files/kana_' + ss_type + '.txt')\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, input_names = data\n\n            if data_type == 'train':\n                for i_batch, l_batch in zip(inputs[0], labels[0]):\n                    if len(np.where(l_batch == dataset.padded_value)[0]) > 0:\n                        if i_batch.shape[0] < np.where(l_batch == dataset.padded_value)[0][0]:\n                            raise ValueError(\n                                'input length must be longer than label length.')\n                    else:\n                        if i_batch.shape[0] < len(l_batch):\n                            raise ValueError(\n                                'input length must be longer than label length.')\n\n            if data_type != 'test':\n                str_true = map_fn(labels[0][0])\n            else:\n                str_true = labels[0][0][0]\n\n            print('----- %s ----- (epoch: %.3f)' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0][0].shape)\n            print(str_true)\n\n            if dataset.epoch_detail >= 0.2:\n                break\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/librispeech/data/test/test_load_dataset_ctc.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport sys\nimport unittest\nimport numpy as np\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.librispeech.data.load_dataset_ctc import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.word import Idx2word\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetCTC(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(label_type='character', data_type='train')\n        self.check(label_type='character', data_type='dev_clean')\n        self.check(label_type='character', data_type='dev_other')\n        self.check(label_type='character', data_type='test_clean')\n        self.check(label_type='character', data_type='test_other')\n        self.check(label_type='character_capital_divide', data_type='train')\n        self.check(label_type='character_capital_divide',\n                   data_type='dev_clean')\n        self.check(label_type='character_capital_divide',\n                   data_type='dev_other')\n        self.check(label_type='character_capital_divide',\n                   data_type='test_clean')\n        self.check(label_type='character_capital_divide',\n                   data_type='test_other')\n        self.check(label_type='word_freq10', data_type='train')\n        self.check(label_type='word_freq10', data_type='dev_clean')\n        self.check(label_type='word_freq10', data_type='dev_other')\n        self.check(label_type='word_freq10', data_type='test_clean')\n        self.check(label_type='word_freq10', data_type='test_other')\n\n        # sort\n        self.check(label_type='character', sort_utt=True)\n        self.check(label_type='character', sort_utt=True,\n                   sort_stop_epoch=1)\n        self.check(label_type='character', shuffle=True)\n\n        # frame stacking\n        self.check(label_type='character', frame_stacking=True)\n\n        # splicing\n        self.check(label_type='character', splice=11)\n\n        # multi-GPU\n        self.check(label_type='character', num_gpu=8)\n\n    @measure_time\n    def check(self, label_type, data_type='dev_clean',\n              shuffle=False,  sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1, num_gpu=1):\n\n        print('========================================')\n        print('  label_type: %s' % label_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('  num_gpu: %d' % num_gpu)\n        print('========================================')\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, train_data_size='train100h',\n            label_type=label_type,\n            batch_size=64, max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle, sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True, num_gpu=num_gpu)\n\n        print('=> Loading mini-batch...')\n        if label_type == 'character':\n            map_file_path = '../../metrics/mapping_files/character.txt'\n        else:\n            map_file_path = '../../metrics/mapping_files/' + label_type + '_' + \\\n                dataset.train_data_size + '.txt'\n\n        idx2char = Idx2char(map_file_path)\n        idx2word = Idx2word(map_file_path)\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, input_names = data\n\n            if data_type == 'train':\n                for i, l in zip(inputs[0], labels[0]):\n                    if len(i) < len(l):\n                        raise ValueError(\n                            'input length must be longer than label length.')\n\n            if num_gpu > 1:\n                for inputs_gpu in inputs:\n                    print(inputs_gpu.shape)\n\n            if 'test' in data_type:\n                str_true = labels[0][0][0]\n            else:\n                if 'word' in label_type:\n                    str_true = '_'.join(idx2word(labels[0][0]))\n                else:\n                    str_true = idx2char(labels[0][0])\n\n            print('----- %s (epoch: %.3f) -----' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0].shape)\n            print(str_true)\n\n            if dataset.epoch_detail >= 0.1:\n                break\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/librispeech/data/test/test_load_dataset_multitask_ctc.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport sys\nimport unittest\nimport numpy as np\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.librispeech.data.load_dataset_multitask_ctc import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.word import Idx2word\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetMultitaskCTC(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(label_type_sub='character', data_type='train')\n        self.check(label_type_sub='character', data_type='dev_clean')\n        self.check(label_type_sub='character', data_type='dev_other')\n        self.check(label_type_sub='character', data_type='test_clean')\n        self.check(label_type_sub='character', data_type='test_other')\n        self.check(label_type_sub='character_capital_divide',\n                   data_type='train')\n        self.check(label_type_sub='character_capital_divide',\n                   data_type='dev_clean')\n        self.check(label_type_sub='character_capital_divide',\n                   data_type='dev_other')\n        self.check(label_type_sub='character_capital_divide',\n                   data_type='test_clean')\n        self.check(label_type_sub='character_capital_divide',\n                   data_type='test_other')\n\n        # sort\n        self.check(label_type_sub='character', sort_utt=True)\n        self.check(label_type_sub='character', sort_utt=True,\n                   sort_stop_epoch=2)\n        self.check(label_type_sub='character', shuffle=True)\n\n        # frame stacking\n        self.check(label_type_sub='character', frame_stacking=True)\n\n        # splicing\n        self.check(label_type_sub='character', splice=11)\n\n        # multi-GPU\n        self.check(label_type_sub='character', num_gpu=8)\n\n    @measure_time\n    def check(self, label_type_sub, data_type='dev_clean',\n              shuffle=False,  sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1, num_gpu=1):\n\n        print('========================================')\n        print('  label_type_main: %s' % 'word_freq10')\n        print('  label_type_sub: %s' % label_type_sub)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('  num_gpu: %d' % num_gpu)\n        print('========================================')\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, train_data_size='train100h',\n            label_type_main='word_freq10', label_type_sub=label_type_sub,\n            batch_size=64, max_epoch=1, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle, sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True, num_gpu=num_gpu)\n\n        print('=> Loading mini-batch...')\n        if label_type_sub == 'character':\n            map_file_path_char = '../../metrics/mapping_files/character.txt'\n        elif label_type_sub == 'character_capital_divide':\n            map_file_path_char = '../../metrics/mapping_files/character_capital.txt'\n        map_file_path_word = '../../metrics/mapping_files/word_freq10_' + \\\n            dataset.train_data_size + '.txt'\n\n        idx2char = Idx2char(map_file_path=map_file_path_char)\n        idx2word = Idx2word(map_file_path=map_file_path_word)\n\n        for data, is_new_epoch in dataset:\n            inputs, labels_word, labels_char, inputs_seq_len, input_names = data\n\n            if data_type == 'train':\n                for i, l in zip(inputs[0], labels_char[0]):\n                    if len(i) < len(l):\n                        raise ValueError(\n                            'input length must be longer than label length.')\n\n            if num_gpu > 1:\n                for inputs_gpu in inputs:\n                    print(inputs_gpu.shape)\n\n            if 'test' in data_type:\n                str_true_word = labels_word[0][0][0]\n                str_true_char = labels_char[0][0][0]\n            else:\n                word_list = idx2word(labels_word[0][0])\n                str_true_word = '_'.join(word_list)\n                str_true_char = idx2char(labels_char[0][0])\n\n            print('----- %s (epoch: %.3f) -----' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0].shape)\n            print(str_true_word)\n            print(str_true_char)\n\n            if dataset.epoch_detail >= 0.1:\n                break\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/librispeech/data/test/test_load_dataset_xe.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.librispeech.data.load_dataset_xe import Dataset\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetCTC(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(data_type='train')\n        self.check(data_type='dev_clean')\n        self.check(data_type='dev_other')\n\n    @measure_time\n    def check(self, data_type='dev_clean'):\n\n        print('========================================')\n        print('  data_type: %s' % data_type)\n        print('========================================')\n\n        dataset = Dataset(\n            model_path='/speech7/takashi01_nb/inaguma/models/tensorflow/librispeech/ctc/character/train100h/blstm_ctc_320_5_rmsprop_lr1e-3_drop0.2_stack2_temp1_7/temp1',\n            data_type=data_type,\n            batch_size=512, max_epoch=4,\n            num_gpu=1)\n\n        print('=> Loading mini-batch...')\n        for data, is_new_epoch in dataset:\n            inputs, labels = data\n\n            print('----- (epoch_detail: %.3f) -----' %\n                  (dataset.epoch_detail))\n            print(inputs[0].shape)\n            print(labels[0].shape)\n\n            # if dataset.epoch_detail >= 0.1:\n            #     break\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/librispeech/visualization/core/__init__.py,0,b''
examples/svc/data/test/test_load_dataset_attention.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.svc.data.load_dataset_attention import Dataset\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetAttention(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(label_type='phone3', data_type='train')\n        self.check(label_type='phone3', data_type='dev')\n        self.check(label_type='phone3', data_type='test')\n\n        self.check(label_type='phone4', data_type='train')\n        self.check(label_type='phone4', data_type='dev')\n        self.check(label_type='phone4', data_type='test')\n\n        self.check(label_type='phone43', data_type='train')\n        self.check(label_type='phone43', data_type='dev')\n        self.check(label_type='phone43', data_type='test')\n\n    @measure_time\n    def check(self, label_type, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1):\n\n        print('========================================')\n        print('  label_type: %s' % label_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('========================================')\n\n        map_file_path = '../../metrics/mapping_files/' + label_type + '.txt'\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, label_type=label_type,\n            batch_size=64,  map_file_path=map_file_path,\n            max_epoch=1, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True)\n\n        print('=> Loading mini-batch...')\n        map_fn = Idx2phone(map_file_path=map_file_path)\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, labels_seq_len, input_names = data\n\n            str_true = map_fn(labels[0][0][0: labels_seq_len[0][0]])\n\n            print('----- %s ----- (epoch: %.3f)' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0][0].shape)\n            print(str_true)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/svc/data/test/test_load_dataset_ctc.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\nimport numpy as np\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.svc.data.load_dataset_ctc import Dataset\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetCTC(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(label_type='phone3', data_type='train')\n        self.check(label_type='phone3', data_type='dev')\n        self.check(label_type='phone3', data_type='test')\n\n        self.check(label_type='phone4', data_type='train')\n        self.check(label_type='phone4', data_type='dev')\n        self.check(label_type='phone4', data_type='test')\n\n        self.check(label_type='phone43', data_type='train')\n        self.check(label_type='phone43', data_type='dev')\n        self.check(label_type='phone43', data_type='test')\n\n    @measure_time\n    def check(self, label_type, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1):\n\n        print('========================================')\n        print('  label_type: %s' % label_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('========================================')\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, label_type=label_type,\n            batch_size=64, max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True)\n\n        print('=> Loading mini-batch...')\n        map_fn = Idx2phone(\n            map_file_path='../../metrics/mapping_files/' + label_type + '.txt')\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, input_names = data\n\n            if data_type == 'train':\n                for i_batch, l_batch in zip(inputs[0], labels[0]):\n                    if len(np.where(l_batch == dataset.padded_value)[0]) > 0:\n                        if i_batch.shape[0] < np.where(l_batch == dataset.padded_value)[0][0]:\n                            raise ValueError(\n                                'input length must be longer than label length.')\n                    else:\n                        if i_batch.shape[0] < len(l_batch):\n                            raise ValueError(\n                                'input length must be longer than label length.')\n\n            str_true = map_fn(labels[0][0])\n\n            print('----- %s ----- (epoch: %.3f)' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0][0].shape)\n            print(str_true)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/svc/evaluation/restore/__init__.py,0,b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n'
examples/svc/evaluation/restore/restore_ctc.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Evaluate trained CTC network (SVC corpus).""""""\n\nimport os\nimport sys\nimport time\nimport tensorflow as tf\n\nsys.path.append(os.path.pardir)\nsys.path.append(os.path.abspath(\'../../../../\'))\nfrom feature_extraction.read_dataset_ctc import DataSet\nfrom models.tf_model.ctc.load_model import load\nfrom evaluation.eval_ctc import do_eval_ler, do_eval_fmeasure, do_eval_fmeasure_time\n\n\ndef do_restore(network, label_type, feature, epoch=None, is_progressbar=True):\n    """"""Restore model.\n    Args:\n        network: model to restore\n        label_type: original or phone1 or phone2 or phone41\n        feature: fbank or is13\n        epoch: the number of epoch to restore\n        is_progressbar: if True, visualize progressbar\n    """"""\n    # load dataset\n    print(\'Loading dataset...\')\n    test_data = DataSet(data_type=\'test\', label_type=label_type,\n                        feature=feature, is_progressbar=is_progressbar)\n\n    # reset previous network\n    tf.reset_default_graph()\n\n    # add to the graph each operation\n    network.define()\n    # decode_op = network.greedy_decoder()\n    decode_op = network.beam_search_decoder(beam_width=20)\n    posteriors_op = network.posteriors(decode_op)\n    ler_op = network.ler(decode_op)\n\n    # create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(network.model_dir)\n\n        # if check point exists\n        if ckpt:\n            # use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch is not None:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        start_time_eval = time.time()\n\n        print(\'\xe2\x96\xa0Test Data Evaluation:\xe2\x96\xa0\')\n        # do_eval_ler(session=sess, ler_op=ler_op, network=network,\n        #             dataset=test_data, is_progressbar=is_progressbar)\n        print(\'Fmeasure (time)\')\n        do_eval_fmeasure_time(session=sess, decode_op=decode_op, posteriors_op=posteriors_op,\n                              network=network, dataset=test_data, is_progressbar=is_progressbar)\n\n        print(\'F-measure (sequence)\')\n        do_eval_fmeasure(session=sess, decode_op=decode_op, network=network,\n                         dataset=test_data, is_progressbar=is_progressbar)\n\n        # sys.stdout.flush()\n        duration_eval = time.time() - start_time_eval\n        print(\'Evaluation time: %.3f min\' % (duration_eval / 60))\n\n\ndef main():\n\n    label_type = \'phone1\'  # phone1 or phone2 or phone41\n    feature = \'is13\'  # fbank or is13\n    model = \'blstm\'\n    layer_num = 5\n    cell = 256\n    optimizer = \'rmsprop\'\n    learning_rate = 1e-3\n    epoch = 75\n    drop_in = 0.8\n    drop_h = 0.5\n    save_result = True\n\n    if feature == \'fbank\':\n        input_size = 123\n    elif feature == \'is13\':\n        input_size = 141\n\n    if label_type in [\'original\', \'phone1\']:\n        output_size = 3\n    elif label_type == \'phone2\':\n        output_size = 4\n    elif label_type == \'phone41\':\n        output_size = 43\n\n    CTCModel = load(model_type=model)\n    network = CTCModel(batch_size=32, input_size=input_size,\n                       num_cell=cell, num_layers=layer_num,\n                       output_size=output_size,\n                       clip_grad=5.0, clip_activation=50,  # \xe6\x8e\xa8\xe8\xab\x96\xe6\x99\x82\xe3\x81\xaf\xe3\x81\xa9\xe3\x81\x86\xe3\x81\x99\xe3\x82\x8b\xef\xbc\x9f\n                       dropout_ratio_input=drop_in,\n                       dropout_ratio_hidden=drop_h)\n    network.model_name = model.upper() + \'_CTC\'\n    network.model_name += \'_\' + str(cell) + \'_\' + str(layer_num)\n    network.model_name += \'_\' + optimizer + \'_lr\' + str(learning_rate)\n\n    # set restore path\n    network.model_dir = \'/n/sd8/inaguma/result/svc/ctc/\'\n    network.model_dir = os.path.join(network.model_dir, label_type)\n    network.model_dir = os.path.join(network.model_dir, feature)\n    network.model_dir = os.path.join(network.model_dir, network.model_name)\n\n    if save_result:\n        sys.stdout = open(os.path.join(network.model_dir, \'test_eval.txt\'), \'w\')\n        print(network.model_dir)\n        do_restore(network, label_type=label_type, feature=feature,\n                   epoch=epoch, is_progressbar=False)\n        sys.stdout = sys.__stdout__\n    else:\n        print(network.model_dir)\n        do_restore(network, label_type=label_type, feature=feature,\n                   epoch=epoch, is_progressbar=True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/svc/evaluation/restore/restore_ff.py,3,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Restore trained feed-forward network (SVC corpus).""""""\n\nimport os\nimport sys\nimport tensorflow as tf\n\nsys.path.append(os.path.pardir)\nsys.path.append(os.path.abspath(\'../../../../\'))\nfrom feature_extraction.read_dataset_ff import DataSet\nfrom models.tf_model.feed_forward.dnn import dnnModel\nfrom models.tf_model.feed_forward.cnn2_fc2 import cnnModel as cnn2\nfrom evaluation.eval_framewise import do_eval_uaauc\n\n\ndef do_restore(network, label_type, feature, epoch=None):\n    """"""Restore model.\n    Args:\n        network: model to restore\n        label_type: original or phone2 or phone41\n        epoch: the number of epoch to restore\n        feature: fbank or is13\n    """"""\n    # load dataset\n    print(\'Loading dataset...\')\n    test_data = DataSet(data_type=\'test\', feature=feature,\n                        label_type=label_type, is_progressbar=True)\n\n    # add to the graph each operation\n    network.define()\n    posteriors_op = network.posterior()\n\n    # create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(network.model_dir)\n\n        # if check point exists\n        if ckpt:\n            # use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch is not None:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        print(\'Test Data Evaluation:\')\n        do_eval_uaauc(session=sess, posteriors_op=posteriors_op,\n                      network=network, dataset=test_data, is_training=False)\n\n\ndef main():\n\n    label_type = \'original\'  # original or phone2 or phone41\n    feature = \'fbank\'  # fbank or is13\n    model = \'dnn\'\n    node_list = [400] * 4\n    optimizer = \'adam\'\n    learning_rate = 1e-3\n    epoch = 1\n    dropout = True\n    batch_norm = True\n\n    if feature == \'fbank\':\n        input_size = 123\n    elif feature == \'is13\':\n        input_size = 141\n\n    if label_type == \'original\':\n        output_size = 3\n    elif label_type == \'phone2\':\n        output_size = 4\n    elif label_type == \'phone41\':\n        output_size = 43\n\n    if model == \'dnn\':\n        network = dnnModel(batch_size=1, input_size=input_size, splice=5,\n                           hidden_size_list=node_list, output_size=output_size,\n                           is_dropout=dropout, dropout_ratio_list=[0.5] * len(node_list),\n                           is_batch_norm=batch_norm)\n        network.model_name = \'DNN\'\n    elif model == \'cnn2\':\n        pass\n        # network = cnn2(batch_size=1, input_size=141, splice=SPLICE, output_size=3,\n        #                weight_decay_lambda=0.0)\n    else:\n        raise ValueError(\'Error: model is ""dnn"" or ""cnn2"".\')\n\n    network.model_name += \'_\' + str(node_list[0]) + \'_\'\n    network.model_name += str(len(node_list)) + \'_\' + optimizer\n    network.model_name += \'_lr\' + str(learning_rate)\n\n    network.model_dir = \'/n/sd8/inaguma/result/svc/framewise/\'\n    network.model_dir = os.path.join(network.model_dir, label_type)\n    network.model_dir = os.path.join(network.model_dir, feature)\n    network.model_dir = os.path.join(network.model_dir, network.model_name)\n\n    print(network.model_dir)\n    do_restore(network, label_type=label_type, feature=feature, epoch=epoch)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/svc/evaluation/restore/visualize_ctc.py,4,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Visualize trained CTC network (SVC corpus).""""""\n\nimport os\nimport sys\nimport tensorflow as tf\n\nsys.path.append(os.path.pardir)\nsys.path.append(os.path.abspath(\'../../../../\'))\nfrom feature_extraction.read_dataset_ctc import DataSet\nfrom models.tf_model.ctc.load_model import load\nfrom evaluation.eval_ctc import decode_test, posterior_test\n\n\ndef do_restore(network, label_type, feature, epoch=None):\n    """"""Restore model.\n    Args:\n        network: model to restore\n        label_type: original or phone1 or phone2 or phone41\n        epoch: the number of epoch to restore\n        feature: fbank or is13\n    """"""\n    # load dataset\n    print(\'Loading dataset...\')\n    test_data = DataSet(data_type=\'test\', feature=feature,\n                        label_type=label_type, is_progressbar=True)\n\n    # reset previous network\n    tf.reset_default_graph()\n\n    # add to the graph each operation\n    network.define()\n    # decode_op = network.greedy_decoder()\n    decode_op = network.beam_search_decoder(beam_width=20)\n    posteriors_op = network.posteriors(decode_op)\n\n    # create a saver for writing training checkpoints\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        ckpt = tf.train.get_checkpoint_state(network.model_dir)\n\n        # if check point exists\n        if ckpt:\n            # use last saved model\n            model_path = ckpt.model_checkpoint_path\n            if epoch is not None:\n                model_path = model_path.split(\'/\')[:-1]\n                model_path = \'/\'.join(model_path) + \'/model.ckpt-\' + str(epoch)\n            saver.restore(sess, model_path)\n            print(""Model restored: "" + model_path)\n        else:\n            raise ValueError(\'There are not any checkpoints.\')\n\n        # visualize\n        # decode_test(session=sess, decode_op=decode_op,\n        #             network=network, dataset=test_data, label_type=label_type)\n        posterior_test(session=sess, posteriors_op=posteriors_op,\n                       network=network, dataset=test_data, label_type=label_type)\n\n\ndef main():\n\n    label_type = \'phone1\'  # phone1 or phone2 or phone41\n    feature = \'fbank\'  # fbank or is13\n    model = \'blstm\'\n    layer_num = 5\n    cell = 256\n    optimizer = \'rmsprop\'\n    learning_rate = 1e-3\n    epoch = 92\n    input_drop = 0.8\n    hidden_drop = 0.5\n\n    if feature == \'fbank\':\n        input_size = 123\n    elif feature == \'is13\':\n        input_size = 141\n\n    if label_type in [\'original\', \'phone1\']:\n        output_size = 3\n    elif label_type == \'phone2\':\n        output_size = 4\n    elif label_type == \'phone41\':\n        output_size = 43\n\n    # load model\n    CTCModel = load(model_type=model)\n    network = CTCModel(batch_size=32, input_size=input_size,\n                       num_cell=cell, num_layers=layer_num,\n                       output_size=output_size,\n                       clip_grad=5.0, clip_activation=50,  # \xe6\x8e\xa8\xe8\xab\x96\xe6\x99\x82\xe3\x81\xaf\xe3\x81\xa9\xe3\x81\x86\xe3\x81\x99\xe3\x82\x8b\xef\xbc\x9f\n                       dropout_ratio_input=input_drop,\n                       dropout_ratio_hidden=hidden_drop)\n    network.model_name = model.upper() + \'_CTC\'\n    network.model_name += \'_\' + str(cell) + \'_\' + str(layer_num)\n    network.model_name += \'_\' + optimizer + \'_lr\' + str(learning_rate)\n\n    # set restore path\n    network.model_dir = \'/n/sd8/inaguma/result/svc/ctc/\'\n    network.model_dir = os.path.join(network.model_dir, label_type)\n    network.model_dir = os.path.join(network.model_dir, feature)\n    network.model_dir = os.path.join(network.model_dir, network.model_name)\n\n    print(network.model_dir)\n    do_restore(network, label_type=label_type, feature=feature, epoch=epoch)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/timit/data/test/__init__.py,0,b''
examples/timit/data/test/test_load_dataset_attention.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.timit.data.load_dataset_attention import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetAttention(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(label_type='phone61', data_type='train')\n        self.check(label_type='phone61', data_type='dev')\n        self.check(label_type='phone61', data_type='test')\n\n        # label_type\n        self.check(label_type='phone61')\n        self.check(label_type='character')\n        self.check(label_type='character_capital_divide')\n\n        # sort\n        self.check(label_type='phone61', sort_utt=True)\n        self.check(label_type='phone61', sort_utt=True,\n                   sort_stop_epoch=2)\n        self.check(label_type='phone61', shuffle=True)\n\n        # frame stacking\n        self.check(label_type='phone61', frame_stacking=True)\n\n        # splicing\n        self.check(label_type='phone61', splice=11)\n\n    @measure_time\n    def check(self, label_type, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1):\n\n        print('========================================')\n        print('  label_type: %s' % label_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('========================================')\n\n        map_file_path = '../../metrics/mapping_files/' + label_type + '.txt'\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, label_type=label_type,\n            batch_size=64,  map_file_path=map_file_path,\n            max_epoch=1, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True)\n\n        print('=> Loading mini-batch...')\n        if label_type in ['character', 'character_capital_divide']:\n            map_fn = Idx2char(map_file_path=map_file_path)\n        else:\n            map_fn = Idx2phone(map_file_path=map_file_path)\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, labels_seq_len, input_names = data\n\n            if data_type != 'test':\n                str_true = map_fn(labels[0][0][:labels_seq_len[0][0]])\n            else:\n                str_true = labels[0][0][0]\n\n            print('----- %s ----- (epoch: %.3f)' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0][0].shape)\n            print(str_true)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/timit/data/test/test_load_dataset_ctc.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\nimport numpy as np\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.timit.data.load_dataset_ctc import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetCTC(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(label_type='phone61', data_type='train')\n        self.check(label_type='phone61', data_type='dev')\n        self.check(label_type='phone61', data_type='test')\n\n        # label_type\n        self.check(label_type='phone61')\n        self.check(label_type='character')\n        self.check(label_type='character_capital_divide')\n\n        # sort\n        self.check(label_type='phone61', sort_utt=True)\n        self.check(label_type='phone61', sort_utt=True,\n                   sort_stop_epoch=2)\n        self.check(label_type='phone61', shuffle=True)\n\n        # frame stacking\n        self.check(label_type='phone61', frame_stacking=True)\n\n        # splicing\n        self.check(label_type='phone61', splice=11)\n\n    @measure_time\n    def check(self, label_type, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1):\n\n        print('========================================')\n        print('  label_type: %s' % label_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('========================================')\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, label_type=label_type,\n            batch_size=64, max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True)\n\n        print('=> Loading mini-batch...')\n        if label_type in ['character', 'character_capital_divide']:\n            map_fn = Idx2char(\n                map_file_path='../../metrics/mapping_files/' + label_type + '.txt')\n        else:\n            map_fn = Idx2phone(\n                map_file_path='../../metrics/mapping_files/' + label_type + '.txt')\n\n        for data, is_new_epoch in dataset:\n            inputs, labels, inputs_seq_len, input_names = data\n\n            if data_type == 'train':\n                for i_batch, l_batch in zip(inputs[0], labels[0]):\n                    if len(np.where(l_batch == dataset.padded_value)[0]) > 0:\n                        if i_batch.shape[0] < np.where(l_batch == dataset.padded_value)[0][0]:\n                            raise ValueError(\n                                'input length must be longer than label length.')\n                    else:\n                        if i_batch.shape[0] < len(l_batch):\n                            raise ValueError(\n                                'input length must be longer than label length.')\n\n            if data_type != 'test':\n                str_true = map_fn(labels[0][0])\n            else:\n                str_true = labels[0][0][0]\n\n            print('----- %s ----- (epoch: %.3f)' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(inputs[0][0].shape)\n            print(str_true)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/timit/data/test/test_load_dataset_joint_ctc_attention.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.timit.data.load_dataset_joint_ctc_attention import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetJointCTCAttention(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check_loading(label_type='phone61', data_type='train')\n        self.check_loading(label_type='phone61', data_type='dev')\n        self.check_loading(label_type='phone61', data_type='test')\n\n        # label_type\n        self.check_loading(label_type='phone61')\n        self.check_loading(label_type='character')\n        self.check_loading(label_type='character_capital_divide')\n\n        # sort\n        self.check_loading(label_type='phone61', sort_utt=True)\n        self.check_loading(label_type='phone61', sort_utt=True,\n                           sort_stop_epoch=2)\n        self.check_loading(label_type='phone61', shuffle=True)\n\n        # frame stacking\n        self.check_loading(label_type='phone61', frame_stacking=True)\n\n        # splicing\n        self.check_loading(label_type='phone61', splice=11)\n\n    @measure_time\n    def check_loading(self, label_type, data_type='dev',\n                      shuffle=False, sort_utt=False, sort_stop_epoch=None,\n                      frame_stacking=False, splice=1):\n\n        print('========================================')\n        print('  label_type: %s' % label_type)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(sort_utt))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('========================================')\n\n        map_file_path = '../../metrics/mapping_files/' + label_type + '.txt'\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type, label_type=label_type,\n            batch_size=64, map_file_path=map_file_path,\n            max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True)\n\n        print('=> Loading mini-batch...')\n        if label_type in ['character', 'character_capital_divide']:\n            map_fn = Idx2char(map_file_path=map_file_path)\n        else:\n            map_fn = Idx2phone(map_file_path=map_file_path)\n\n        for data, is_new_epoch in dataset:\n            inputs, att_labels, ctc_labels, inputs_seq_len, att_labels_seq_len, input_names = data\n\n            if data_type != 'test':\n                att_str_true = map_fn(\n                    att_labels[0][0][0: att_labels_seq_len[0][0]])\n                ctc_str_true = map_fn(ctc_labels[0][0])\n            else:\n                att_str_true = att_labels[0][0][0]\n                ctc_str_true = ctc_labels[0][0][0]\n\n            print('----- %s ----- (epoch: %.3f)' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(att_str_true)\n            print(ctc_str_true)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/timit/data/test/test_load_dataset_multitask_ctc.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport unittest\n\nsys.path.append(os.path.abspath('../../../../'))\nfrom experiments.timit.data.load_dataset_multitask_ctc import Dataset\nfrom utils.io.labels.character import Idx2char\nfrom utils.io.labels.phone import Idx2phone\nfrom utils.measure_time_func import measure_time\n\n\nclass TestLoadDatasetMultitaskCTC(unittest.TestCase):\n\n    def test(self):\n\n        # data_type\n        self.check(label_type_main='character', data_type='train')\n        self.check(label_type_main='character', data_type='dev')\n        self.check(label_type_main='character', data_type='test')\n\n        # label_type\n        self.check(label_type_main='character')\n        self.check(label_type_main='character_capital_divide')\n\n        # sort\n        self.check(label_type_main='character', sort_utt=True)\n        self.check(label_type_main='character', sort_utt=True,\n                   sort_stop_epoch=2)\n        self.check(label_type_main='character', shuffle=True)\n\n        # frame stacking\n        self.check(label_type_main='character', frame_stacking=True)\n\n        # splicing\n        self.check(label_type_main='character', splice=11)\n\n    @measure_time\n    def check(self, label_type_main, data_type='dev',\n              shuffle=False, sort_utt=False, sort_stop_epoch=None,\n              frame_stacking=False, splice=1):\n\n        print('========================================')\n        print('  label_type_main: %s' % label_type_main)\n        print('  data_type: %s' % data_type)\n        print('  shuffle: %s' % str(shuffle))\n        print('  sort_utt: %s' % str(sort_utt))\n        print('  sort_stop_epoch: %s' % str(sort_stop_epoch))\n        print('  frame_stacking: %s' % str(frame_stacking))\n        print('  splice: %d' % splice)\n        print('========================================')\n\n        num_stack = 3 if frame_stacking else 1\n        num_skip = 3 if frame_stacking else 1\n        dataset = Dataset(\n            data_type=data_type,\n            label_type_main=label_type_main, label_type_sub='phone61',\n            batch_size=64, max_epoch=2, splice=splice,\n            num_stack=num_stack, num_skip=num_skip,\n            shuffle=shuffle,\n            sort_utt=sort_utt, sort_stop_epoch=sort_stop_epoch,\n            progressbar=True)\n\n        print('=> Loading mini-batch...')\n        idx2char = Idx2char(\n            map_file_path='../../metrics/mapping_files/' + label_type_main + '.txt')\n        idx2phone = Idx2phone(\n            map_file_path='../../metrics/mapping_files/phone61.txt')\n\n        for data, is_new_epoch in dataset:\n            inputs, labels_char, labels_phone, inputs_seq_len, input_names = data\n\n            if data_type != 'test':\n                str_true_char = idx2char(labels_char[0][0])\n                str_true_phone = idx2phone(labels_phone[0][0])\n            else:\n                str_true_char = labels_char[0][0][0]\n                str_true_phone = labels_phone[0][0][0]\n\n            print('----- %s ----- (epoch: %.3f)' %\n                  (input_names[0][0], dataset.epoch_detail))\n            print(str_true_char)\n            print(str_true_phone)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
examples/timit/visualization/core/__init__.py,0,b''
models/attention/decoders/beam_search/beam_search_decoder.py,40,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""A decoder that uses beam search. Can only be used for inference, not\ntraining.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom models.attention.decoders.dynamic_decoder import dynamic_decode\nfrom models.attention.decoders.attention_decoder import RNNDecoder\nfrom models.attention.decoders.beam_search.util import gather_tree\nfrom models.attention.decoders.beam_search.util import mask_probs\nfrom models.attention.decoders.beam_search.util import normalize_score\nfrom models.attention.decoders.beam_search.namedtuple import FinalBeamDecoderOutput\nfrom models.attention.decoders.beam_search.namedtuple import BeamSearchDecoderOutput\nfrom models.attention.decoders.beam_search.namedtuple import BeamSearchDecoderState\nfrom models.attention.decoders.beam_search.namedtuple import BeamSearchStepOutput\n\n\nclass BeamSearchDecoder(RNNDecoder):\n    """"""The BeamSearchDecoder wraps another decoder to perform beam search\n    instead of greedy selection. This decoder must be used with batch size of\n    1, which will result in an effective batch size of `beam_width`.\n    Args:\n        decoder: An instance of `RNNDecoder` to be used with beam search\n        beam_width: int, the number of beams to use\n        vocab_size: int, the number of classses\n        eos_index: int, the id of the EOS token, used to mark beams as ""done""\n        length_penalty_weight: A float value, weight for the length penalty\n            factor. 0.0\xe3\x80\x80disables the penalty.\n        choose_successors_fn: A function used to choose beam successors based\n            on their scores. Maps from\n            `(scores, config)` => `(chosen scores, chosen_ids)`\n    """"""\n\n    def __init__(self, decoder, beam_width, vocab_size, eos_index,\n                 length_penalty_weight, choose_successors_fn,\n                 name=\'beam_search_decoder\'):\n        super(BeamSearchDecoder, self).__init__(\n            decoder.cell,\n            decoder.parameter_init,\n            decoder.max_decode_length,\n            decoder.num_classes,\n            decoder.attention_encoder_states,\n            decoder.attention_values,\n            decoder.attention_values_length,\n            decoder.attention_layer,\n            decoder.time_major,\n            name)\n\n        self.decoder = decoder\n        self.beam_width = beam_width\n        self.vocab_size = vocab_size\n        self.eos_index = eos_index\n        self.length_penalty_weight = length_penalty_weight\n        self.choose_successors_fn = choose_successors_fn\n\n    def __call__(self, *args, **kwargs):\n        with tf.variable_scope(self.decoder.name, reuse=True):\n            return self._build(*args, **kwargs)\n\n    @property\n    def output_size(self):\n        return BeamSearchDecoderOutput(\n            logits=self.decoder.num_classes,\n            predicted_ids=tf.TensorShape([]),\n            log_probs=tf.TensorShape([]),\n            scores=tf.TensorShape([]),\n            beam_parent_ids=tf.TensorShape([]),\n            original_outputs=self.decoder.output_size)\n\n    @property\n    def output_dtype(self):\n        return BeamSearchDecoderOutput(\n            logits=tf.float32,\n            predicted_ids=tf.int32,\n            log_probs=tf.float32,\n            scores=tf.float32,\n            beam_parent_ids=tf.int32,\n            original_outputs=self.decoder.output_dtype)\n\n    @property\n    def batch_size(self):\n        # return tf.shape(nest.flatten([self.initial_state])[0])[0]\n        return self.beam_width\n\n    def _build(self, initial_state, helper, mode):\n        """"""\n        Args:\n            initial_state: A tensor or tuple of tensors used as the initial\n                cell state. Set to the final state of the encoder by default.\n            helper: An instance of `tf.contrib.seq2seq.Helper` to assist\n                decoding\n        Returns:\n            An instance of `RNNDecoder`\n        """"""\n        print(\'===== beam search decoder build =====\')\n\n        # Tile initial state\n        initial_state = nest.map_structure(\n            lambda x: tf.tile(x, [self.batch_size, 1]), initial_state)\n        self.decoder._setup(initial_state, helper)\n\n        self.reuse = True\n        maximum_iterations = self.max_decode_length\n\n        outputs, final_state = dynamic_decode(\n            decoder=self,\n            output_time_major=self.time_major,\n            impute_finished=True,\n            maximum_iterations=maximum_iterations,\n            scope=\'dynamic_decoder\')\n\n        # tf.contrib.seq2seq.dynamic_decode\n        # return self.finalize(outputs, final_state, final_seq_len)\n\n        # ./dynamic_decoder.py\n        return self.finalize(outputs, final_state, None)\n\n    def finalize(self, outputs, final_state, final_seq_len):\n        """"""\n        Args:\n            outputs: An instance of `BeamSearchDecoderOutput`\n            final_state:\n            final_seq_len:\n        Returns:\n            outputs:\n            final_state:\n        """"""\n        print(\'===== finalize (beam search) =====\')\n        # Gather according to beam search result\n        predicted_ids = gather_tree(outputs.predicted_ids,\n                                    outputs.beam_parent_ids)\n\n        # We\'re using a batch size of 1, so we add an extra dimension to\n        # convert tensors to [1, beam_width, ...] shape. This way Tensorflow\n        # doesn\'t confuse batch_size with beam_width\n        outputs = nest.map_structure(\n            lambda x: tf.expand_dims(x, axis=1), outputs)\n\n        outputs = FinalBeamDecoderOutput(\n            predicted_ids=tf.expand_dims(predicted_ids, axis=1),\n            beam_search_output=outputs)\n\n        return outputs, final_state\n\n    def initialize(self, name=None):\n        """"""Initialize the decoder.\n        Args:\n            name: Name scope for any created operations\n        Returns:\n            finished:\n            first_inputs:\n            initial_state:\n        """"""\n        print(\'===== initialize (beam search) =====\')\n        # Create inputs for the first time step\n        finished, first_inputs, initial_state = self.decoder.initialize()\n\n        # Create initial beam state\n        beam_state = BeamSearchDecoderState(\n            log_probs=tf.zeros([self.beam_width]),\n            finished=tf.zeros([self.beam_width], dtype=tf.bool),\n            lengths=tf.zeros([self.beam_width], dtype=tf.int32))\n\n        return finished, first_inputs, (initial_state, beam_state)\n\n    def step(self, time, inputs, state, name=None):\n        """"""Perform a decoding step.\n        Args:\n            time: scalar `int32` tensor\n            inputs: A input tensors\n            state: A state tensors and TensorArrays\n            name: Name scope for any created operations\n        Returns:\n            outputs: An instance of `BeamSearchDecoderOutput`\n            next_state: A state tensors and TensorArrays\n            next_inputs: The tensor that should be used as input for the\n                next step\n            finished: A boolean tensor telling whether the sequence is\n                complete, for each sequence in the batch\n        """"""\n        print(\'===== step (beam search) =====\')\n        decoder_state, beam_state = state\n\n        # Call the original decoder\n        decoder_output, decoder_state, _, _ = self.decoder.step(time, inputs,\n                                                                decoder_state)\n\n        # Perform a step of beam search\n        beam_search_output, beam_state = beam_search_step(\n            time=time,\n            logits=decoder_output.logits,\n            beam_state=beam_state,\n            beam_width=self.beam_width,\n            vocab_size=self.vocab_size,\n            eos_index=self.eos_index,\n            length_penalty_weight=self.length_penalty_weight,\n            choose_successors_fn=self.choose_successors_fn)\n\n        # Shuffle everything according to beam search result\n        decoder_state = nest.map_structure(\n            lambda x: tf.gather(x, beam_search_output.beam_parent_ids),\n            decoder_state)\n        decoder_output = nest.map_structure(\n            lambda x: tf.gather(x, beam_search_output.beam_parent_ids),\n            decoder_output)\n\n        next_state = (decoder_state, beam_state)\n\n        outputs = BeamSearchDecoderOutput(\n            logits=tf.zeros([self.beam_width, self.vocab_size]),\n            predicted_ids=beam_search_output.predicted_ids,\n            log_probs=beam_state.log_probs,\n            scores=beam_search_output.scores,\n            beam_parent_ids=beam_search_output.beam_parent_ids,\n            original_outputs=decoder_output)\n\n        finished, next_inputs, next_state = self.decoder.helper.next_inputs(\n            time=time,\n            outputs=decoder_output,\n            state=next_state,\n            sample_ids=beam_search_output.predicted_ids)\n        next_inputs.set_shape([self.batch_size, None])\n\n        return outputs, next_state, next_inputs, finished\n\n\ndef beam_search_step(time, logits, beam_state, beam_width, vocab_size,\n                     eos_index, length_penalty_weight, choose_successors_fn):\n    """"""Performs a single step of Beam Search Decoding.\n    Args:\n        time: Beam search time step, should start at 0. At time 0 we assume\n            that all beams are equal and consider only the first beam for\n            continuations\n        logits: Logits at the current time step. A tensor of shape\n            `[beam_width, vocab_size]`\n        beam_state: Current state of the beam search. An instance of\n            `BeamState??`\n        beam_width: int, the number of beams to use\n        vocab_size: int, the number of classses\n        eos_index: int, the id of the EOS token, used to mark beams as ""done""\n        length_penalty_weight: A float value, weight for the length penalty\n            factor. 0.0 disables the penalty.\n        choose_successors_fn: A function used to choose beam successors based\n            on their scores. Maps from\n            `(scores, config)` => `(chosen scores, chosen_ids)`\n    Returns:\n        output: An instance of `BeamSearchStepOutput`\n        next_state: An instance of `BeamSearchDecoderState`\n        A new beam state\n    """"""\n    # Calculate the current lengths of the predictions\n    prediction_lengths = beam_state.lengths\n    previously_finished = beam_state.finished\n\n    # Calculate the total log probs for the new hypotheses\n    # Final Shape: `[beam_width, vocab_size]`\n    probs = tf.nn.log_softmax(logits)\n    probs = mask_probs(probs, eos_index, previously_finished)\n    total_probs = tf.expand_dims(beam_state.log_probs, axis=1) + probs\n\n    # Calculate the continuation lengths\n    # We add 1 to all continuations that are not EOS and were not\n    # finished previously\n    lengths_to_add = tf.one_hot([eos_index] * beam_width,\n                                vocab_size,\n                                on_value=0,\n                                off_value=1)\n    add_mask = (1 - tf.to_int32(previously_finished))\n    lengths_to_add = tf.expand_dims(add_mask, axis=1) * lengths_to_add\n    new_prediction_lengths = tf.expand_dims(\n        prediction_lengths, axis=1) + lengths_to_add\n\n    # Calculate the scores for each beam\n    scores = normalize_score(\n        log_probs=total_probs,\n        sequence_lengths=new_prediction_lengths,\n        length_penalty_weight=length_penalty_weight)\n    scores_flat = tf.reshape(scores, shape=[-1])\n\n    # During the first time step, we only consider the initial beam\n    scores_flat = tf.cond(tf.convert_to_tensor(time) > 0,\n                          true_fn=lambda: scores_flat,  # otherwise\n                          false_fn=lambda: scores[0])   # first time step\n\n    # Pick the next beams according to the specified successors function\n    next_beam_scores, word_indices = choose_successors_fn(\n        scores_flat,\n        beam_width)\n    next_beam_scores.set_shape([beam_width])\n    word_indices.set_shape([beam_width])\n\n    # Pick out the probs, beam_ids, and states according to the chosen\n    # predictions\n    total_probs_flat = tf.reshape(total_probs, shape=[-1],\n                                  name=""total_probs_flat"")\n    next_beam_probs = tf.gather(total_probs_flat, word_indices)\n    next_beam_probs.set_shape([beam_width])\n    next_word_ids = tf.mod(word_indices, vocab_size)\n    next_beam_ids = tf.div(word_indices, vocab_size)\n\n    # Append new ids to current predictions\n    next_finished = tf.logical_or(\n        tf.gather(beam_state.finished, next_beam_ids),\n        tf.equal(next_word_ids, eos_index))\n\n    # Calculate the length of the next predictions.\n    # 1. Finished beams remain unchanged\n    # 2. Beams that are now finished (EOS predicted) remain unchanged\n    # 3. Beams that are not yet finished have their length increased by 1\n    lengths_to_add = tf.to_int32(tf.not_equal(next_word_ids, eos_index))\n    lengths_to_add = (1 - tf.to_int32(next_finished)) * lengths_to_add\n    next_prediction_len = tf.gather(beam_state.lengths, next_beam_ids)\n    next_prediction_len += lengths_to_add\n\n    next_state = BeamSearchDecoderState(\n        log_probs=next_beam_probs,\n        lengths=next_prediction_len,\n        finished=next_finished)\n\n    output = BeamSearchStepOutput(\n        scores=next_beam_scores,\n        predicted_ids=next_word_ids,\n        beam_parent_ids=next_beam_ids)\n\n    return output, next_state\n'"
models/attention/decoders/beam_search/namedtuple.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Classes for namedtuple.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\n\n\nclass FinalBeamDecoderOutput(namedtuple(\n        ""FinalBeamDecoderOutput"",\n        [\n            ""predicted_ids"",\n            ""beam_search_output""\n        ])):\n    """"""Final outputs returned by the beam search after all decoding is finished.\n    Args:\n        predicted_ids: The final prediction. A tensor of shape\n            `[time, 1, beam_width]???`.\n        beam_search_output: An instance of `BeamSearchDecoderOutput` that\n            describes the state of the beam search.\n    """"""\n    pass\n\n\nclass BeamSearchDecoderOutput(namedtuple(\n        ""BeamSearchDecoderOutput"",\n        [\n            ""logits"",\n            ""predicted_ids"",\n            ""log_probs"",\n            ""scores"",\n            ""beam_parent_ids"",\n            ""original_outputs""\n        ])):\n    """"""Structure for the output of a beam search decoder. This class is used\n    to define the output at each step as well as the final output of the\n    decoder. If used as the final output, a time dimension `time` is inserted\n    after the beam_size dimension.\n    Args:\n        logits: Logits at the current time step of shape\n            `[beam_width, num_classes]`\n        predicted_ids: Chosen softmax predictions at the current time step.\n            An int32 tensor of shape `[beam_width]`.\n        log_probs: Total log probabilities of all beams at the current time\n            step. A float32 tensor of shaep `[beam_width]`.\n        scores: Total scores of all beams at the current time step. This\n            differs from log probabilities in that the score may add additional\n            processing such as length normalization. A float32 tensor of shape\n            `[beam_width]`.\n        beam_parent_ids: The indices of the beams that are being continued.\n            An int32 tensor of shape `[beam_width]`.\n        original_outputs:\n    """"""\n    pass\n\n\nclass BeamSearchDecoderState(namedtuple(\n        ""BeamSearchDecoderState"",\n        [\n            ""log_probs"",\n            ""finished"",\n            ""lengths""\n        ])):\n    """"""State for a single step of beam search.\n    Args:\n        log_probs: The current log probabilities of all beams\n        finished: A boolean vector that specifies which beams are finished\n        lengths: Lengths of all beams\n    """"""\n    pass\n\n\nclass BeamSearchStepOutput(namedtuple(\n        ""BeamSearchStepOutput"",\n        [\n            ""scores"",\n            ""predicted_ids"",\n            ""beam_parent_ids""\n        ])):\n    """"""Outputs for a single step of beam search.\n    Args:\n        scores: Score for each beam, a float32 vector\n        predicted_ids: predictions for this step, an int32 vector\n        beam_parent_ids: an int32 vector containing the beam indices of the\n            continued beams from the previous step\n    """"""\n    pass\n'"
models/attention/decoders/beam_search/util.py,11,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Utilitiy functions for beam search decoder.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef gather_tree_py(values, parents):\n    """"""Gathers path through a tree backwards from the leave nodes. Used\n    to reconstruct beams given their parents.""""""\n    beam_length = values.shape[0]\n    num_beams = values.shape[1]\n    res = np.zeros_like(values)\n    res[-1, :] = values[-1, :]\n    for beam_id in range(num_beams):\n        parent = parents[-1][beam_id]\n        for level in reversed(range(beam_length - 1)):\n            res[level, beam_id] = values[level][parent]\n            parent = parents[level][parent]\n    return np.array(res).astype(values.dtype)\n\n\ndef gather_tree(values, parents):\n    """"""Tensor version of gather_tree_py.""""""\n    res = tf.py_func(\n        func=gather_tree_py, inp=[values, parents], Tout=values.dtype)\n    res.set_shape(values.get_shape().as_list())\n    return res\n\n\ndef mask_probs(probs, eos_token, finished):\n    """"""Masks log probabilities such that finished beams allocate all\n    probability mass to the EOS token and unfinished beams remain unchanged.\n    Args:\n        probs: Log probabiltiies of shape `[beam_width, vocab_size]`\n        eos_token: An int32 id corresponding to the EOS token to allocate\n            probability to\n        finished: A boolean tensor of shape `[beam_width]` that specifies which\n            elements in the beam are finished already\n    Returns:\n        A tensor of shape `[beam_width, vocab_size]`, where unfinished beams\n        stay unchanged and finished beams are replaced with a tensor that has\n        all probability on the EOS token.\n    """"""\n    vocab_size = tf.shape(probs)[1]\n    finished_mask = tf.expand_dims(\n        tf.to_float(1. - tf.to_float(finished)), axis=1)\n\n    # These examples are not finished and we leave them\n    non_finished_examples = finished_mask * probs\n\n    # All finished examples are replaced with a vector that has all\n    # probability on the EOS token\n    finished_row = tf.one_hot(\n        eos_token,\n        vocab_size,\n        dtype=tf.float32,\n        on_value=0.,\n        off_value=tf.float32.min)\n    finished_examples = (1. - finished_mask) * finished_row\n\n    return finished_examples + non_finished_examples\n\n\ndef normalize_score(log_probs, sequence_lengths, length_penalty_weight):\n    """"""Normalizes scores for beam search hypotheses by the length.\n    Args:\n        log_probs: The log probabilities with shape\n            `[beam_width, vocab_size]`.\n        sequence_lengths: The sequence length of all hypotheses, a tensor\n            of shape `[beam_size, vocab_size]`.\n        length_penalty_weight: A float value, a scalar that weights the length\n            penalty. Disabled with 0.0.\n    Returns:\n        score: The scores normalized by the length_penalty\n    """"""\n    # Calculate the length penality\n    length_penality = tf.div(\n        (5. + tf.to_float(sequence_lengths))**length_penalty_weight,\n        (5. + 1.)**length_penalty_weight)\n    # NOTE: See details in https://arxiv.org/abs/1609.08144.\n\n    # Normalize log probabiltiies by the length penality\n    if length_penalty_weight is None or length_penalty_weight == 1:\n        score = log_probs\n    else:\n        score = log_probs / length_penality\n\n    return score\n\n\ndef choose_top_k(scores_flat, beam_width):\n    """"""Chooses the top-k beams as successors.\n    Args:\n        scores_flat:\n        beam_width: int,\n    Returns:\n        next_beam_scores:\n        pred_indices:\n    """"""\n    next_beam_scores, pred_indices = tf.nn.top_k(\n        scores_flat, k=beam_width)\n    return next_beam_scores, pred_indices\n\n\ndef nest_map(inputs, map_fn, name=None):\n    """"""Applies a function to (possibly nested) tuple of tensors.\n    """"""\n    if nest.is_sequence(inputs):\n        inputs_flat = nest.flatten(inputs)\n        y_flat = [map_fn(_) for _ in inputs_flat]\n        outputs = nest.pack_sequence_as(inputs, y_flat)\n    else:\n        outputs = map_fn(inputs)\n    if name:\n        outputs = tf.identity(outputs, name=name)\n    return outputs\n'"
examples/csj/visualization/core/plot/ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""""Utilities for plotting of the CTC model (CSJ corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom experiments.utils.directory import mkdir_join\n\nplt.style.use(\'ggplot\')\nsns.set_style(""white"")\n\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\n\ndef posterior_test(session, posteriors_op, network, dataset, label_type,\n                   save_path=None, show=False):\n    """"""Visualize label posteriors of CTC model.\n    Args:\n        session: session of training model\n        posteriois_op: operation for computing posteriors\n        network: network to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type: string, kanji or kana or phone\n        save_path: path to save ctc outputs\n        show: if True, show each figure\n    """"""\n    # Batch size is expected to be 1\n    iteration = dataset.data_num\n\n    # Make data generator\n    mini_batch = dataset.next_batch(batch_size=1)\n\n    save_path = mkdir_join(save_path, \'ctc_output\')\n\n    for step in range(iteration):\n        # Create feed dictionary for next mini batch\n        inputs, _, inputs_seq_len, input_names = mini_batch.__next__()\n\n        feed_dict = {\n            network.inputs: inputs,\n            network.inputs_seq_len: inputs_seq_len,\n            network.keep_prob_input: 1.0,\n            network.keep_prob_hidden: 1.0,\n            network.keep_prob_output: 1.0\n        }\n\n        # Visualize\n        max_frame_num = inputs.shape[1]\n        posteriors = session.run(posteriors_op, feed_dict=feed_dict)\n\n        i_batch = 0  # index in mini-batch\n        posteriors_index = np.array(\n            [i_batch * max_frame_num + i for i in range(max_frame_num)])\n\n        plot_probs_ctc(\n            probs=posteriors[posteriors_index][:int(inputs_seq_len[0]), :],\n            wav_index=input_names[0],\n            label_type=label_type,\n            save_path=save_path,\n            show=show)\n\n\ndef plot_probs_ctc(probs, wav_index, label_type, save_path, show):\n    """"""Plot posteriors of phones.\n    Args:\n        probs:\n        wav_index: string\n        label_type: string, kanji or kana or phone\n        save_path: path to save ctc outpus\n        show: if True, show each figure\n    """"""\n    duration = probs.shape[0]\n    times_probs = np.arange(len(probs))\n    plt.clf()\n    plt.figure(figsize=(10, 4))\n\n    # Blank class is set to the last class in TensorFlow\n    if label_type == \'kanji\':\n        blank_index = 3386\n    elif label_type == \'kana\':\n        blank_index = 147\n    elif label_type == \'phone\':\n        blank_index = 38\n\n    # NOTE:\n    # 0: silence(_)\n    # 1: noise(NZ)\n\n    # Plot\n    plt.plot(times_probs, probs[:, 0],\n             label=\'silence\', color=\'black\', linewidth=2)\n    for i in range(1, blank_index, 1):\n        plt.plot(times_probs, probs[:, i], linewidth=2)\n    plt.plot(times_probs, probs[:, blank_index], \':\',\n             label=\'blank\', color=\'grey\', linewidth=2)\n    plt.xlabel(\'Time[sec]\', fontsize=12)\n    plt.ylabel(label_type, fontsize=12)\n    plt.xlim([0, duration])\n    plt.ylim([0.05, 1.05])\n    plt.xticks(list(range(0, int(len(probs) / 100) + 1, 1)))\n    plt.yticks(list(range(0, 2, 1)))\n    plt.legend(loc=""upper right"", fontsize=12)\n\n    if show:\n        plt.show()\n\n    # Save as a png file\n    if save_path is not None:\n        save_path = join(save_path, wav_index + \'.png\')\n        plt.savefig(save_path, dvi=500)\n'"
examples/librispeech/visualization/core/plot/__init__.py,0,b''
examples/librispeech/visualization/core/plot/ctc.py,0,"b'#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""""Utilities for plotting of the CTC model (Librispeech corpus).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os.path import join\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nplt.style.use(\'ggplot\')\nimport seaborn as sns\nsns.set_style(""white"")\n\nfrom utils.directory import mkdir_join\n\n\nblue = \'#4682B4\'\norange = \'#D2691E\'\ngreen = \'#006400\'\n\n\ndef posterior_test_multitask(session, posteriors_op_main, posteriors_op_sub,\n                             model, dataset, label_type_main, label_type_sub,\n                             save_path=None, show=False):\n    """"""Visualize label posteriors of the multi-task CTC model.\n    Args:\n        session: session of training model\n        posteriois_op_main: operation for computing posteriors in the main task\n        posteriois_op_sub: operation for computing posteriors in the sub task\n        model: model to evaluate\n        dataset: An instance of a `Dataset` class\n        label_type_sub (string): phone39 or phone48 or phone61\n        save_path (string): path to save ctc outpus\n        show (bool, optional): if True, show each figure\n    """"""\n    save_path = mkdir_join(save_path, \'ctc_output\')\n\n    # NOTE: Batch size is expected to be 1\n    while True:\n\n        # Create feed dictionary for next mini batch\n        data, is_new_epoch = dataset.next(batch_size=1)\n        inputs, _, _, inputs_seq_len, input_names = data\n\n        feed_dict = {\n            model.inputs_pl_list[0]: inputs[0],\n            model.inputs_seq_len_pl_list[0]: inputs_seq_len[0],\n            model.keep_prob_input_pl_list[0]: 1.0,\n            model.keep_prob_hidden_pl_list[0]: 1.0,\n            model.keep_prob_output_pl_list[0]: 1.0\n        }\n\n        # Visualize\n        max_frame_num = inputs[0].shape[1]\n        posteriors_char = session.run(posteriors_op_main, feed_dict=feed_dict)\n        posteriors_phone = session.run(posteriors_op_sub, feed_dict=feed_dict)\n\n        i_batch = 0  # index in mini-batch\n        posteriors_index = np.array(\n            [i_batch * max_frame_num + i for i in range(max_frame_num)])\n        posteriors_char = posteriors_char[posteriors_index][:int(\n            inputs_seq_len[0][0]), :]\n        posteriors_phone = posteriors_phone[posteriors_index][:int(\n            inputs_seq_len[0][0]), :]\n\n        plt.clf()\n        plt.figure(figsize=(10, 4))\n        frame_num = posteriors_char.shape[0]\n        times_probs = np.arange(len(posteriors_char))\n\n        # NOTE: Blank class is set to the last class in TensorFlow\n        # Plot characters\n        plt.subplot(211)\n        for i in range(0, posteriors_char.shape[1] - 1, 1):\n            plt.plot(times_probs, posteriors_char[:, i])\n        plt.plot(\n            times_probs, posteriors_char[:, -1], \':\', label=\'blank\', color=\'grey\')\n        plt.xlabel(\'Time [sec]\', fontsize=12)\n        plt.ylabel(\'Characters\', fontsize=12)\n        plt.xlim([0, frame_num])\n        plt.ylim([0.05, 1.05])\n        plt.xticks(list(range(0, int(len(posteriors_char) / 100) + 1, 1)))\n        plt.yticks(list(range(0, 2, 1)))\n        plt.legend(loc=""upper right"", fontsize=12)\n\n        # Plot phones\n        plt.subplot(212)\n        for i in range(0, posteriors_phone.shape[1] - 1, 1):\n            plt.plot(times_probs, posteriors_phone[:, i])\n        plt.plot(\n            times_probs, posteriors_phone[:, -1], \':\', label=\'blank\', color=\'grey\')\n        plt.xlabel(\'Time [sec]\', fontsize=12)\n        plt.ylabel(\'Phones\', fontsize=12)\n        plt.xlim([0, frame_num])\n        plt.ylim([0.05, 1.05])\n        plt.xticks(list(range(0, int(len(posteriors_phone) / 100) + 1, 1)))\n        plt.yticks(list(range(0, 2, 1)))\n        plt.legend(loc=""upper right"", fontsize=12)\n\n        if show:\n            plt.show()\n\n        # Save as a png file\n        if save_path is not None:\n            plt.savefig(join(save_path, input_names[0][0] + \'.png\'), dvi=500)\n\n        if is_new_epoch:\n            break\n'"
