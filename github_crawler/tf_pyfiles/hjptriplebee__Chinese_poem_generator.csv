file_path,api_count,code
config.py,0,"b'# coding: UTF-8\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n   file name: config.py\n   create time: 2017\xe5\xb9\xb406\xe6\x9c\x8825\xe6\x97\xa5 \xe6\x98\x9f\xe6\x9c\x9f\xe6\x97\xa5 10\xe6\x97\xb656\xe5\x88\x8655\xe7\xa7\x92\n   author: Jipeng Huang\n   e-mail: huangjipengnju@gmail.com\n   github: https://github.com/hjptriplebee\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport os\nimport random\nimport time\nimport collections\n\nbatchSize = 64\n\nlearningRateBase = 0.001\nlearningRateDecayStep = 1000\nlearningRateDecayRate = 0.95\n\nepochNum = 10                    # train epoch\ngenerateNum = 5                   # number of generated poems per time\n\ntype = ""poetrySong""                   # dataset to use, shijing, songci, etc\ntrainPoems = ""./dataset/"" + type + ""/"" + type + "".txt"" # training file location\ncheckpointsPath = ""./checkpoints/"" + type # checkpoints location\n\nsaveStep = 1000                   # save model every savestep\n\n\n\n# evaluate\ntrainRatio = 0.8                    # train percentage\nevaluateCheckpointsPath = ""./checkpoints/evaluate""'"
data.py,0,"b'# coding: UTF-8\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n   file name: data.py\n   create time: 2017\xe5\xb9\xb406\xe6\x9c\x8823\xe6\x97\xa5 \xe6\x98\x9f\xe6\x9c\x9f\xe4\xba\x94 17\xe6\x97\xb617\xe5\x88\x8636\xe7\xa7\x92\n   author: Jipeng Huang\n   e-mail: huangjipengnju@gmail.com\n   github: https://github.com/hjptriplebee\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\nfrom config import *\n\nclass POEMS:\n    ""poem class""\n    def __init__(self, filename, isEvaluate=False):\n        """"""pretreatment""""""\n        poems = []\n        file = open(filename, ""r"", encoding=\'utf-8\')\n        for line in file:  #every line is a poem\n            title, author, poem = line.strip().split(""::"")  #get title and poem\n            poem = poem.replace(\' \',\'\')\n            if len(poem) < 10 or len(poem) > 512:  #filter poem\n                continue\n            if \'_\' in poem or \'\xe3\x80\x8a\' in poem or \'[\' in poem or \'(\' in poem or \'\xef\xbc\x88\' in poem:\n                continue\n            poem = \'[\' + poem + \']\' #add start and end signs\n            poems.append(poem)\n            #print(title, author, poem)\n\n        #counting words\n        wordFreq = collections.Counter()\n        for poem in poems:\n            wordFreq.update(poem)\n        # print(wordFreq)\n\n        # erase words which are not common\n        #--------------------bug-------------------------\n        # word num less than original num, which causes nan value in loss function\n        # erase = []\n        # for key in wordFreq:\n        #     if wordFreq[key] < 2:\n        #         erase.append(key)\n        # for key in erase:\n        #     del wordFreq[key]\n\n        wordFreq["" ""] = -1\n        wordPairs = sorted(wordFreq.items(), key = lambda x: -x[1])\n        self.words, freq = zip(*wordPairs)\n        self.wordNum = len(self.words)\n\n        self.wordToID = dict(zip(self.words, range(self.wordNum))) #word to ID\n        poemsVector = [([self.wordToID[word] for word in poem]) for poem in poems] # poem to vector\n        if isEvaluate: #evaluating need divide dataset into test set and train set\n            self.trainVector = poemsVector[:int(len(poemsVector) * trainRatio)]\n            self.testVector = poemsVector[int(len(poemsVector) * trainRatio):]\n        else:\n            self.trainVector = poemsVector\n            self.testVector = []\n        print(""\xe8\xae\xad\xe7\xbb\x83\xe6\xa0\xb7\xe6\x9c\xac\xe6\x80\xbb\xe6\x95\xb0\xef\xbc\x9a %d"" % len(self.trainVector))\n        print(""\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa0\xb7\xe6\x9c\xac\xe6\x80\xbb\xe6\x95\xb0\xef\xbc\x9a %d"" % len(self.testVector))\n\n\n    def generateBatch(self, isTrain=True):\n        #padding length to batchMaxLength\n        if isTrain:\n            poemsVector = self.trainVector\n        else:\n            poemsVector = self.testVector\n\n        random.shuffle(poemsVector)\n        batchNum = (len(poemsVector) - 1) // batchSize\n        X = []\n        Y = []\n        #create batch\n        for i in range(batchNum):\n            batch = poemsVector[i * batchSize: (i + 1) * batchSize]\n            maxLength = max([len(vector) for vector in batch])\n            temp = np.full((batchSize, maxLength), self.wordToID["" ""], np.int32) # padding space\n            for j in range(batchSize):\n                temp[j, :len(batch[j])] = batch[j]\n            X.append(temp)\n            temp2 = np.copy(temp) #copy!!!!!!\n            temp2[:, :-1] = temp[:, 1:]\n            Y.append(temp2)\n        return X, Y\n\n'"
evaluate.py,15,"b'# coding: UTF-8\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n   file name: model.py\n   create time: 2018\xe5\xb9\xb406\xe6\x9c\x8811\xe6\x97\xa5 \xe6\x98\x9f\xe6\x9c\x9f\xe4\xb8\x80 13\xe6\x97\xb653\xe5\x88\x8642\xe7\xa7\x92\n   author: Jipeng Huang\n   e-mail: huangjipengnju@gmail.com\n   github: https://github.com/hjptriplebee\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n#evalute model, just for test\nimport data\nfrom model import *\n\nclass EVALUATE_MODEL(MODEL):\n    """"""evaluate model class""""""\n    def evaluate(self, reload=True):\n        """"""evaluate model""""""\n        print(""training..."")\n        gtX = tf.placeholder(tf.int32, shape=[batchSize, None])  # input\n        gtY = tf.placeholder(tf.int32, shape=[batchSize, None])  # output\n\n        logits, probs, a, b, c = self.buildModel(self.trainData.wordNum, gtX)\n\n        targets = tf.reshape(gtY, [-1])\n\n        # loss\n        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets],\n                                                                  [tf.ones_like(targets, dtype=tf.float32)])\n        globalStep = tf.Variable(0, trainable=False)\n        addGlobalStep = globalStep.assign_add(1)\n\n        cost = tf.reduce_mean(loss)\n        trainableVariables = tf.trainable_variables()\n        grads, a = tf.clip_by_global_norm(tf.gradients(cost, trainableVariables), 5) # prevent loss divergence caused by gradient explosion\n        learningRate = tf.train.exponential_decay(learningRateBase, global_step=globalStep,\n                                                  decay_steps=learningRateDecayStep, decay_rate=learningRateDecayRate)\n        optimizer = tf.train.AdamOptimizer(learningRate)\n        trainOP = optimizer.apply_gradients(zip(grads, trainableVariables))\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n\n            if not os.path.exists(evaluateCheckpointsPath):\n                os.mkdir(evaluateCheckpointsPath)\n\n            if reload:\n                checkPoint = tf.train.get_checkpoint_state(evaluateCheckpointsPath)\n                # if have checkPoint, restore checkPoint\n                if checkPoint and checkPoint.model_checkpoint_path:\n                    saver.restore(sess, checkPoint.model_checkpoint_path)\n                    print(""restored %s"" % checkPoint.model_checkpoint_path)\n                else:\n                    print(""no checkpoint found!"")\n\n            for epoch in range(epochNum):\n                X, Y = self.trainData.generateBatch()\n                epochSteps = len(X)  # equal to batch\n                for step, (x, y) in enumerate(zip(X, Y)):\n                    a, loss, gStep = sess.run([trainOP, cost, addGlobalStep], feed_dict={gtX: x, gtY: y})\n                    print(""epoch: %d, steps: %d/%d, loss: %3f"" % (epoch + 1, step + 1, epochSteps, loss))\n                    if gStep % saveStep == saveStep - 1:  # prevent save at the beginning\n                        print(""save model"")\n                        saver.save(sess, os.path.join(evaluateCheckpointsPath, type), global_step=gStep)\n\n                X, Y = self.trainData.generateBatch(isTrain=False)\n                print(""evaluating testing error..."")\n                wrongNum = 0\n                totalNum = 0\n                testBatchNum = len(X)\n                for step, (x, y) in enumerate(zip(X, Y)):\n                    print(""test batch %d/%d"" % (step + 1, testBatchNum))\n                    testProbs, testTargets = sess.run([probs, targets], feed_dict={gtX: x, gtY: y})\n                    wrongNum += len(np.nonzero(np.argmax(testProbs, axis=1) - testTargets)[0])\n                    totalNum += len(testTargets)\n                print(""accuracy: %.2f"" % ((totalNum - wrongNum) / totalNum))\n\n\n\nif __name__ == ""__main__"":\n    trainData = data.POEMS(trainPoems, isEvaluate=True)\n    MCPangHu = EVALUATE_MODEL(trainData)\n    MCPangHu.evaluate()\n'"
main.py,0,"b'# coding: UTF-8\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n   file name: main.py\n   create time: 2017\xe5\xb9\xb406\xe6\x9c\x8823\xe6\x97\xa5 \xe6\x98\x9f\xe6\x9c\x9f\xe4\xba\x94 16\xe6\x97\xb641\xe5\x88\x8654\xe7\xa7\x92\n   author: Jipeng Huang\n   e-mail: huangjipengnju@gmail.com\n   github: https://github.com/hjptriplebee\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\nfrom config import *\nimport data\nimport model\n\ndef defineArgs():\n    """"""define args""""""\n    parser = argparse.ArgumentParser(description = ""Chinese_poem_generator."")\n    parser.add_argument(""-m"", ""--mode"", help = ""select mode by \'train\' or test or head"",\n                        choices = [""train"", ""test"", ""head""], default = ""test"")\n    return parser.parse_args()\n\nif __name__ == ""__main__"":\n    args = defineArgs()\n    trainData = data.POEMS(trainPoems)\n    MCPangHu = model.MODEL(trainData)\n    if args.mode == ""train"":\n        MCPangHu.train()\n    else:\n        if args.mode == ""test"":\n            poems = MCPangHu.test()\n        else:\n            characters = input(""please input chinese character:"")\n            poems = MCPangHu.testHead(characters)'"
model.py,40,"b'# coding: UTF-8\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n   file name: model.py\n   create time: 2017\xe5\xb9\xb406\xe6\x9c\x8825\xe6\x97\xa5 \xe6\x98\x9f\xe6\x9c\x9f\xe6\x97\xa5 10\xe6\x97\xb647\xe5\x88\x8648\xe7\xa7\x92\n   author: Jipeng Huang\n   e-mail: huangjipengnju@gmail.com\n   github: https://github.com/hjptriplebee\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\nfrom config import *\n\nclass MODEL:\n    """"""model class""""""\n    def __init__(self, trainData):\n        self.trainData = trainData\n\n    def buildModel(self, wordNum, gtX, hidden_units = 128, layers = 2):\n        """"""build rnn""""""\n        with tf.variable_scope(""embedding""): #embedding\n            embedding = tf.get_variable(""embedding"", [wordNum, hidden_units], dtype = tf.float32)\n            inputbatch = tf.nn.embedding_lookup(embedding, gtX)\n\n        basicCell = tf.contrib.rnn.BasicLSTMCell(hidden_units, state_is_tuple = True)\n        stackCell = tf.contrib.rnn.MultiRNNCell([basicCell] * layers)\n        initState = stackCell.zero_state(np.shape(gtX)[0], tf.float32)\n        outputs, finalState = tf.nn.dynamic_rnn(stackCell, inputbatch, initial_state = initState)\n        outputs = tf.reshape(outputs, [-1, hidden_units])\n\n        with tf.variable_scope(""softmax""):\n            w = tf.get_variable(""w"", [hidden_units, wordNum])\n            b = tf.get_variable(""b"", [wordNum])\n            logits = tf.matmul(outputs, w) + b\n\n        probs = tf.nn.softmax(logits)\n        return logits, probs, stackCell, initState, finalState\n\n    def train(self, reload=True):\n        """"""train model""""""\n        print(""training..."")\n        gtX = tf.placeholder(tf.int32, shape=[batchSize, None])  # input\n        gtY = tf.placeholder(tf.int32, shape=[batchSize, None])  # output\n\n        logits, probs, a, b, c = self.buildModel(self.trainData.wordNum, gtX)\n\n        targets = tf.reshape(gtY, [-1])\n\n        #loss\n        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets],\n                                                                  [tf.ones_like(targets, dtype=tf.float32)])\n        globalStep = tf.Variable(0, trainable=False)\n        addGlobalStep = globalStep.assign_add(1)\n\n        cost = tf.reduce_mean(loss)\n        trainableVariables = tf.trainable_variables()\n        grads, a = tf.clip_by_global_norm(tf.gradients(cost, trainableVariables), 5) # prevent loss divergence caused by gradient explosion\n        learningRate = tf.train.exponential_decay(learningRateBase, global_step=globalStep,\n                                                  decay_steps=learningRateDecayStep, decay_rate=learningRateDecayRate)\n        optimizer = tf.train.AdamOptimizer(learningRate)\n        trainOP = optimizer.apply_gradients(zip(grads, trainableVariables))\n\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n\n            if not os.path.exists(checkpointsPath):\n                os.mkdir(checkpointsPath)\n\n            if reload:\n                checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n                # if have checkPoint, restore checkPoint\n                if checkPoint and checkPoint.model_checkpoint_path:\n                    saver.restore(sess, checkPoint.model_checkpoint_path)\n                    print(""restored %s"" % checkPoint.model_checkpoint_path)\n                else:\n                    print(""no checkpoint found!"")\n\n            for epoch in range(epochNum):\n                X, Y = self.trainData.generateBatch()\n                epochSteps = len(X) # equal to batch\n                for step, (x, y) in enumerate(zip(X, Y)):\n                    a, loss, gStep = sess.run([trainOP, cost, addGlobalStep], feed_dict = {gtX:x, gtY:y})\n                    print(""epoch: %d, steps: %d/%d, loss: %3f"" % (epoch + 1, step + 1, epochSteps, loss))\n                    if gStep % saveStep == saveStep - 1: # prevent save at the beginning\n                        print(""save model"")\n                        saver.save(sess, os.path.join(checkpointsPath, type), global_step=gStep)\n\n    def probsToWord(self, weights, words):\n        """"""probs to word""""""\n        prefixSum = np.cumsum(weights) #prefix sum\n        ratio = np.random.rand(1)\n        index = np.searchsorted(prefixSum, ratio * prefixSum[-1]) # large margin has high possibility to be sampled\n        return words[index[0]]\n\n    def test(self):\n        """"""write regular poem""""""\n        print(""genrating..."")\n        gtX = tf.placeholder(tf.int32, shape=[1, None])  # input\n        logits, probs, stackCell, initState, finalState = self.buildModel(self.trainData.wordNum, gtX)\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n            checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n            # if have checkPoint, restore checkPoint\n            if checkPoint and checkPoint.model_checkpoint_path:\n                saver.restore(sess, checkPoint.model_checkpoint_path)\n                print(""restored %s"" % checkPoint.model_checkpoint_path)\n            else:\n                print(""no checkpoint found!"")\n                exit(1)\n\n            poems = []\n            for i in range(generateNum):\n                state = sess.run(stackCell.zero_state(1, tf.float32))\n                x = np.array([[self.trainData.wordToID[\'[\']]]) # init start sign\n                probs1, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n                word = self.probsToWord(probs1, self.trainData.words)\n                poem = \'\'\n                sentenceNum = 0\n                while word not in [\' \', \']\']:\n                    poem += word\n                    if word in [\'\xe3\x80\x82\', \'\xef\xbc\x9f\', \'\xef\xbc\x81\', \'\xef\xbc\x8c\']:\n                        sentenceNum += 1\n                        if sentenceNum % 2 == 0:\n                            poem += \'\\n\'\n                    x = np.array([[self.trainData.wordToID[word]]])\n                    #print(word)\n                    probs2, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n                    word = self.probsToWord(probs2, self.trainData.words)\n                print(poem)\n                poems.append(poem)\n            return poems\n\n    def testHead(self, characters):\n        """"""write head poem""""""\n        print(""genrating..."")\n        gtX = tf.placeholder(tf.int32, shape=[1, None])  # input\n        logits, probs, stackCell, initState, finalState = self.buildModel(self.trainData.wordNum, gtX)\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n            checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n            # if have checkPoint, restore checkPoint\n            if checkPoint and checkPoint.model_checkpoint_path:\n                saver.restore(sess, checkPoint.model_checkpoint_path)\n                print(""restored %s"" % checkPoint.model_checkpoint_path)\n            else:\n                print(""no checkpoint found!"")\n                exit(1)\n            flag = 1\n            endSign = {-1: ""\xef\xbc\x8c"", 1: ""\xe3\x80\x82""}\n            poem = \'\'\n            state = sess.run(stackCell.zero_state(1, tf.float32))\n            x = np.array([[self.trainData.wordToID[\'[\']]])\n            probs1, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n            for word in characters:\n                if self.trainData.wordToID.get(word) == None:\n                    print(""\xe8\x83\x96\xe8\x99\x8e\xe4\xb8\x8d\xe8\xae\xa4\xe8\xaf\x86\xe8\xbf\x99\xe4\xb8\xaa\xe5\xad\x97\xef\xbc\x8c\xe4\xbd\xa0\xe7\x9c\x9f\xe6\x98\xaf\xe6\x96\x87\xe5\x8c\x96\xe4\xba\xba\xef\xbc\x81"")\n                    exit(0)\n                flag = -flag\n                while word not in [\']\', \'\xef\xbc\x8c\', \'\xe3\x80\x82\', \' \', \'\xef\xbc\x9f\', \'\xef\xbc\x81\']:\n                    poem += word\n                    x = np.array([[self.trainData.wordToID[word]]])\n                    probs2, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n                    word = self.probsToWord(probs2, self.trainData.words)\n\n                poem += endSign[flag]\n                # keep the context, state must be updated\n                if endSign[flag] == \'\xe3\x80\x82\':\n                    probs2, state = sess.run([probs, finalState],\n                                             feed_dict={gtX: np.array([[self.trainData.wordToID[""\xe3\x80\x82""]]]), initState: state})\n                    poem += \'\\n\'\n                else:\n                    probs2, state = sess.run([probs, finalState],\n                                             feed_dict={gtX: np.array([[self.trainData.wordToID[""\xef\xbc\x8c""]]]), initState: state})\n\n            print(characters)\n            print(poem)\n            return poem\n'"
dataset/jsonToTxt.py,0,"b'# coding: UTF-8\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n   file name: main.py\n   create time: 2017\xe5\xb9\xb405\xe6\x9c\x8828\xe6\x97\xa5 \xe6\x98\x9f\xe6\x9c\x9f\xe4\xb8\x80 11\xe6\x97\xb608\xe5\x88\x8654\xe7\xa7\x92\n   author: Jipeng Huang\n   e-mail: huangjipengnju@gmail.com\n   github: https://github.com/hjptriplebee\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n# Function: json dataset to txt\n# change parameters to transfer new json file\n# @typeName: name of dir\n# @titleField: title in json\n# @contentField: content in json\n# @author: author in json\n# generate a typeName.txt file in typeName dir.\n\nimport json\nimport os\nimport re\n\ntypeName = ""poetrySong""\n\ntitleField = ""title""\ncontentField = ""paragraphs""\nauthorField = ""author""\n\njsonPath = ""./"" + typeName\nsaveFile = open(os.path.join(jsonPath, typeName + "".txt""), ""w"")\n\nfor file in os.listdir(jsonPath):\n    if os.path.isfile(os.path.join(jsonPath,file)) and re.match(\'(.*)(\\.)(json)\', file) != None:\n        print(""processing file: %s"" % file)\n        poems = json.load(open(os.path.join(jsonPath,file), ""r""))\n        for singlePoem in poems:\n            content = """".join(singlePoem[contentField])\n            title = singlePoem[titleField]\n            author = singlePoem[authorField]\n            saveFile.write(title + ""::"" + author + ""::"" + content + ""\\n"")\n\nsaveFile.close()\n'"
dataset/poetry/divideTangSong.py,0,"b'# coding: UTF-8\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n   file name: divideTangSong.py\n   create time: 2018\xe5\xb9\xb406\xe6\x9c\x8804\xe6\x97\xa5 \xe6\x98\x9f\xe6\x9c\x9f\xe4\xb8\x80 19\xe6\x97\xb629\xe5\x88\x8655\xe7\xa7\x92\n   author: Jipeng Huang\n   e-mail: huangjipengnju@gmail.com\n   github: https://github.com/hjptriplebee\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n# divide tang and song poem\n\nimport os\nimport re\nimport shutil\n\nif not os.path.exists(""./tang""):\n    os.mkdir(""tang"")\nif not os.path.exists(""./song""):\n    os.mkdir(""song"")\n\nfor file in os.listdir("".""):\n    if os.path.isfile(os.path.join(""./"", file)) and re.match(\'(.*)(\\.)(json)\', file) != None:\n        shutil.copyfile(os.path.join(""./"", file), os.path.join(""./"", file.split(""."")[1], file))\n'"
