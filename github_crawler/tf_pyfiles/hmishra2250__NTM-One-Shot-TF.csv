file_path,api_count,code
Omniglot.py,30,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'3\'  #No logging TF\n\nimport tensorflow as tf\nimport numpy as np\nimport time\n\nfrom MANN.Model import memory_augmented_neural_network\nfrom MANN.Utils.Generator import OmniglotGenerator\nfrom MANN.Utils.Metrics import accuracy_instance\nfrom MANN.Utils.tf_utils import update_tensor\n\ndef omniglot():\n\n    sess = tf.InteractiveSession()\n\n    input_ph = tf.placeholder(dtype=tf.float32, shape=(16,50,400))   #(batch_size, time, input_dim)\n    target_ph = tf.placeholder(dtype=tf.int32, shape=(16,50))     #(batch_size, time)(label_indices)\n\n    ##Global variables for Omniglot Problem\n    nb_reads = 4\n    controller_size = 200\n    memory_shape = (128,40)\n    nb_class = 5\n    input_size = 20*20\n    batch_size = 16\n    nb_samples_per_class = 10\n\n    #Load Data\n    generator = OmniglotGenerator(data_folder=\'./data/omniglot\', batch_size=batch_size, nb_samples=nb_class, nb_samples_per_class=nb_samples_per_class, max_rotation=0., max_shift=0., max_iter=None)\n    output_var, output_var_flatten, params = memory_augmented_neural_network(input_ph, target_ph, batch_size=batch_size, nb_class=nb_class, memory_shape=memory_shape, controller_size=controller_size, input_size=input_size, nb_reads=nb_reads)\n\n    print \'Compiling the Model\'\n    \n\n    with tf.variable_scope(""Weights"", reuse=True):\n        W_key = tf.get_variable(\'W_key\', shape=(nb_reads, controller_size, memory_shape[1]))\n        b_key = tf.get_variable(\'b_key\', shape=(nb_reads, memory_shape[1]))\n        W_add = tf.get_variable(\'W_add\', shape=(nb_reads, controller_size, memory_shape[1]))\n        b_add = tf.get_variable(\'b_add\', shape=(nb_reads, memory_shape[1]))\n        W_sigma = tf.get_variable(\'W_sigma\', shape=(nb_reads, controller_size, 1))\n        b_sigma = tf.get_variable(\'b_sigma\', shape=(nb_reads, 1))\n        W_xh = tf.get_variable(\'W_xh\', shape=(input_size + nb_class, 4 * controller_size))\n        b_h = tf.get_variable(\'b_xh\', shape=(4 * controller_size))\n        W_o = tf.get_variable(\'W_o\', shape=(controller_size + nb_reads * memory_shape[1], nb_class))\n        b_o = tf.get_variable(\'b_o\', shape=(nb_class))\n        W_rh = tf.get_variable(\'W_rh\', shape=(nb_reads * memory_shape[1], 4 * controller_size))\n        W_hh = tf.get_variable(\'W_hh\', shape=(controller_size, 4 * controller_size))\n        gamma = tf.get_variable(\'gamma\', shape=[1], initializer=tf.constant_initializer(0.95))\n\n    params = [W_key, b_key, W_add, b_add, W_sigma, b_sigma, W_xh, W_rh, W_hh, b_h, W_o, b_o]\n    \n    #output_var = tf.cast(output_var, tf.int32)\n    target_ph_oh = tf.one_hot(target_ph, depth=generator.nb_samples)\n    print \'Output, Target shapes: \',output_var.get_shape().as_list(), target_ph_oh.get_shape().as_list()\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_var, labels=target_ph_oh), name=""cost"")\n    opt = tf.train.AdamOptimizer(learning_rate=1e-3)\n    train_step = opt.minimize(cost, var_list=params)\n\n    #train_step = tf.train.AdamOptimizer(1e-3).minimize(cost)\n    accuracies = accuracy_instance(tf.argmax(output_var, axis=2), target_ph, batch_size=generator.batch_size)\n    sum_out = tf.reduce_sum(tf.reshape(tf.one_hot(tf.argmax(output_var, axis=2), depth=generator.nb_samples), (-1, generator.nb_samples)), axis=0)\n\n    print \'Done\'\n\n    tf.summary.scalar(\'cost\', cost)\n    for i in range(generator.nb_samples_per_class):\n    \ttf.summary.scalar(\'accuracy-\'+str(i), accuracies[i])\n    \n    merged = tf.summary.merge_all()\n    #writer = tf.summary.FileWriter(\'/tmp/tensorflow\', graph=tf.get_default_graph())\n    train_writer = tf.summary.FileWriter(\'/tmp/tensorflow/\', sess.graph)\n\n    t0 = time.time()\n    all_scores, scores, accs = [],[],np.zeros(generator.nb_samples_per_class)\n\n\n    sess.run(tf.global_variables_initializer())\n\n    print \'Training the model\'\n\n\n\n    try:\n        for i, (batch_input, batch_output) in generator:\n            feed_dict = {\n                input_ph: batch_input,\n                target_ph: batch_output\n            }\n            #print batch_input.shape, batch_output.shape\n            train_step.run(feed_dict)\n            score = cost.eval(feed_dict)\n            acc = accuracies.eval(feed_dict)\n            temp = sum_out.eval(feed_dict)\n            summary = merged.eval(feed_dict)\n            train_writer.add_summary(summary, i)\n            print i, \' \',temp\n            all_scores.append(score)\n            scores.append(score)\n            accs += acc\n            if i>0 and not (i%100):\n                print(accs / 100.0)\n                print(\'Episode %05d: %.6f\' % (i, np.mean(score)))\n                scores, accs = [], np.zeros(generator.nb_samples_per_class)\n\n\n    except KeyboardInterrupt:\n        print time.time() - t0\n        pass\n\nif __name__ == \'__main__\':\n    omniglot()\n\n\n'"
TestUpd.py,22,"b'import tensorflow as tf\nimport numpy as np\nimport time\n\n\ndef omniglot():\n\n    sess = tf.InteractiveSession()\n\n    """"""    def wrapper(v):\n        return tf.Print(v, [v], message=""Printing v"")\n\n    v = tf.Variable(initial_value=np.arange(0, 36).reshape((6, 6)), dtype=tf.float32, name=\'Matrix\')\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n\n    temp = tf.Variable(initial_value=np.arange(0, 36).reshape((6, 6)), dtype=tf.float32, name=\'temp\')\n    temp = wrapper(v)\n    #with tf.control_dependencies([temp]):\n    temp.eval()\n    print \'Hello\'""""""\n\n    def update_tensor(V, dim2, val):  # Update tensor V, with index(:,dim2[:]) by val[:]\n        val = tf.cast(val, V.dtype)\n        def body(_, (v, d2, chg)):\n            d2_int = tf.cast(d2, tf.int32)\n            return tf.slice(tf.concat_v2([v[:d2_int],[chg] ,v[d2_int+1:]], axis=0), [0], [v.get_shape().as_list()[0]])\n        Z = tf.scan(body, elems=(V, dim2, val), initializer=tf.constant(1, shape=V.get_shape().as_list()[1:], dtype=tf.float32), name=""Scan_Update"")\n        return Z\n\n\n    print \'Compiling the Model\'\n\n    tt1 = tf.Variable(initial_value=np.arange(0, 36).reshape((6, 6)), dtype=tf.float32, name=\'Matrix\')\n    ix = tf.Variable(initial_value=np.arange(0, 6), name=\'Indices\')\n    val = tf.Variable(initial_value=np.arange(100, 106), name=\'Values\', dtype=tf.float32)\n\n    tt = tf.concat_v2([tt1[:3], tf.reshape(tf.range(0,6,dtype=tf.float32),shape=(1,6)), tt1[3:]], axis=0)\n    print tt1[:3].get_shape().as_list()\n\n    """"""op = tt1[4].assign(val)\n    sess.run(tf.global_variables_initializer())\n    sess.run(op)\n    print tt1.eval()""""""\n\n    op = tt1.assign(update_tensor(tt1, ix, val))\n    val = tf.Print(val, [val], ""This works fine"")\n\n\n    sess.run(tf.global_variables_initializer())\n    #sess.run(tf.local_variables_initializer())\n    print \'Training the model\'\n\n    print tt.eval()\n    writer = tf.summary.FileWriter(\'/tmp/tensorflow\', graph=tf.get_default_graph())\n    #tf.scalar_summary(\'cost\', cost)\n\n    print \'tt1: \',tt1.eval()\n    print \'ix: \',ix.eval()\n    print \'val: \',val.eval()\n\n    sess.run(op)\n    print \'After run\\n\', tt1.eval()\n    #with tf.control_dependencies([op]):\n    #    print \'********************\',\'\\n\',tt1.eval(),\'\\n\', op.eval()\n\n\n\nif __name__ == \'__main__\':\n    omniglot()\n\n\n'"
testing.py,4,"b""import os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n\n\nfrom MANN.Utils.Metrics import accuracy_instance\nimport tensorflow as tf\nimport numpy as np\nimport copy\n\nx = [0,0,0,0,0]*10\ny = [0,1,2,3,4]*10\nnp.random.shuffle(y)\nx = np.append([x],[x],axis=0)\ny = np.append([y], [y], axis=0)\n\np = tf.constant(x)\nt = tf.constant(y)\n\nsess = tf.InteractiveSession()\n\nzz = accuracy_instance(p, t, batch_size=2)\n\nsess.run(zz)\n\nprint p[0].eval()\nprint t[0].eval()\n\nprint zz.eval()\n\nprint tf.equal(p,t).eval()"""
Examples/Omniglot.py,30,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'3\'  #No logging TF\n\nimport tensorflow as tf\nimport numpy as np\nimport time\n\nfrom MANN.Model import memory_augmented_neural_network\nfrom MANN.Utils.Generator import OmniglotGenerator\nfrom MANN.Utils.Metrics import accuracy_instance\nfrom MANN.Utils.tf_utils import update_tensor\n\ndef omniglot():\n\n    sess = tf.InteractiveSession()\n\n    input_ph = tf.placeholder(dtype=tf.float32, shape=(16,50,400))   #(batch_size, time, input_dim)\n    target_ph = tf.placeholder(dtype=tf.int32, shape=(16,50))     #(batch_size, time)(label_indices)\n\n    ##Global variables for Omniglot Problem\n    nb_reads = 4\n    controller_size = 200\n    memory_shape = (128,40)\n    nb_class = 5\n    input_size = 20*20\n    batch_size = 16\n    nb_samples_per_class = 10\n\n    #Load Data\n    generator = OmniglotGenerator(data_folder=\'./data/omniglot\', batch_size=batch_size, nb_samples=nb_class, nb_samples_per_class=nb_samples_per_class, max_rotation=0., max_shift=0., max_iter=None)\n    output_var, output_var_flatten, params = memory_augmented_neural_network(input_ph, target_ph, batch_size=batch_size, nb_class=nb_class, memory_shape=memory_shape, controller_size=controller_size, input_size=input_size, nb_reads=nb_reads)\n\n    print \'Compiling the Model\'\n    \n\n    with tf.variable_scope(""Weights"", reuse=True):\n        W_key = tf.get_variable(\'W_key\', shape=(nb_reads, controller_size, memory_shape[1]))\n        b_key = tf.get_variable(\'b_key\', shape=(nb_reads, memory_shape[1]))\n        W_add = tf.get_variable(\'W_add\', shape=(nb_reads, controller_size, memory_shape[1]))\n        b_add = tf.get_variable(\'b_add\', shape=(nb_reads, memory_shape[1]))\n        W_sigma = tf.get_variable(\'W_sigma\', shape=(nb_reads, controller_size, 1))\n        b_sigma = tf.get_variable(\'b_sigma\', shape=(nb_reads, 1))\n        W_xh = tf.get_variable(\'W_xh\', shape=(input_size + nb_class, 4 * controller_size))\n        b_h = tf.get_variable(\'b_xh\', shape=(4 * controller_size))\n        W_o = tf.get_variable(\'W_o\', shape=(controller_size + nb_reads * memory_shape[1], nb_class))\n        b_o = tf.get_variable(\'b_o\', shape=(nb_class))\n        W_rh = tf.get_variable(\'W_rh\', shape=(nb_reads * memory_shape[1], 4 * controller_size))\n        W_hh = tf.get_variable(\'W_hh\', shape=(controller_size, 4 * controller_size))\n        gamma = tf.get_variable(\'gamma\', shape=[1], initializer=tf.constant_initializer(0.95))\n\n    params = [W_key, b_key, W_add, b_add, W_sigma, b_sigma, W_xh, W_rh, W_hh, b_h, W_o, b_o]\n    \n    #output_var = tf.cast(output_var, tf.int32)\n    target_ph_oh = tf.one_hot(target_ph, depth=generator.nb_samples)\n    print \'Output, Target shapes: \',output_var.get_shape().as_list(), target_ph_oh.get_shape().as_list()\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_var, target_ph_oh), name=""cost"")\n    opt = tf.train.AdamOptimizer(learning_rate=1e-3)\n    train_step = opt.minimize(cost, var_list=params)\n\n    #train_step = tf.train.AdamOptimizer(1e-3).minimize(cost)\n    accuracies = accuracy_instance(tf.argmax(output_var, axis=2), target_ph, batch_size=generator.batch_size)\n    sum_out = tf.reduce_sum(tf.reshape(tf.one_hot(tf.argmax(output_var, axis=2), depth=generator.nb_samples), (-1, generator.nb_samples)), axis=0)\n\n    print \'Done\'\n\n    tf.summary.scalar(\'cost\', cost)\n    for i in range(generator.nb_samples_per_class):\n    \ttf.summary.scalar(\'accuracy-\'+str(i), accuracies[i])\n    \n    merged = tf.summary.merge_all()\n    #writer = tf.summary.FileWriter(\'/tmp/tensorflow\', graph=tf.get_default_graph())\n    train_writer = tf.summary.FileWriter(\'/tmp/tensorflow/\', sess.graph)\n\n    t0 = time.time()\n    all_scores, scores, accs = [],[],np.zeros(generator.nb_samples_per_class)\n\n\n    sess.run(tf.global_variables_initializer())\n\n    print \'Training the model\'\n\n\n\n    try:\n        for i, (batch_input, batch_output) in generator:\n            feed_dict = {\n                input_ph: batch_input,\n                target_ph: batch_output\n            }\n            #print batch_input.shape, batch_output.shape\n            train_step.run(feed_dict)\n            score = cost.eval(feed_dict)\n            acc = accuracies.eval(feed_dict)\n            temp = sum_out.eval(feed_dict)\n            summary = merged.eval(feed_dict)\n            train_writer.add_summary(summary, i)\n            print i, \' \',temp\n            all_scores.append(score)\n            scores.append(score)\n            accs += acc\n            if i>0 and not (i%100):\n                print(accs / 100.0)\n                print(\'Episode %05d: %.6f\' % (i, np.mean(score)))\n                scores, accs = [], np.zeros(generator.nb_samples_per_class)\n\n\n    except KeyboardInterrupt:\n        print time.time() - t0\n        pass\n\nif __name__ == \'__main__\':\n    omniglot()\n\n\n'"
MANN/Model.py,68,"b'import tensorflow as tf\nimport numpy as np\n\nfrom .Utils.init import weight_and_bias_init, shared_glorot_uniform, shared_one_hot\nfrom .Utils.similarities import cosine_similarity\nfrom .Utils.tf_utils import shared_float32\nfrom .Utils.tf_utils import update_tensor\n\n\ndef memory_augmented_neural_network(input_var, target_var, \\\n                                    batch_size=16, nb_class=5, memory_shape=(128, 40), \\\n                                    controller_size=200, input_size=20 * 20, nb_reads=4):\n    ## input_var has dimensions (batch_size, time, \tinput_dim)\n    ## target_var has dimensions (batch_size, time) (label indices)\n\n    M_0 = shared_float32(1e-6 * np.ones((batch_size,) + memory_shape), name=\'memory\')\n    c_0 = shared_float32(np.zeros((batch_size, controller_size)), name=\'memory_cell_state\')\n    h_0 = shared_float32(np.zeros((batch_size, controller_size)), name=\'hidden_state\')\n    r_0 = shared_float32(np.zeros((batch_size, nb_reads * memory_shape[1])), name=\'read_vector\')\n    wr_0 = shared_one_hot((batch_size, nb_reads, memory_shape[0]), name=\'wr\')\n    wu_0 = shared_one_hot((batch_size, memory_shape[0]), name=\'wu\')\n    \n    def shape_high(shape):\n    \tshape = np.array(shape)\n    \tif isinstance(shape, int):\n            high = np.sqrt(6. / shape)\n    \telse:\n            high = np.sqrt(6. / (np.sum(shape[:2]) * np.prod(shape[2:])))\n        return (list(shape),high)\n\n    with tf.variable_scope(""Weights""):\n    \tshape, high = shape_high((nb_reads, controller_size, memory_shape[1]))\n        W_key = tf.get_variable(\'W_key\', shape=shape,initializer=tf.random_uniform_initializer(-1*high, high))\n        b_key = tf.get_variable(\'b_key\', shape=(nb_reads, memory_shape[1]),initializer=tf.constant_initializer(0))\n        shape, high = shape_high((nb_reads, controller_size, memory_shape[1]))\n        W_add = tf.get_variable(\'W_add\', shape=shape,initializer=tf.random_uniform_initializer(-1*high, high))\n        b_add = tf.get_variable(\'b_add\', shape=(nb_reads, memory_shape[1]),initializer=tf.constant_initializer(0))\n        shape, high = shape_high((nb_reads, controller_size, 1))\n        W_sigma = tf.get_variable(\'W_sigma\', shape=shape,initializer=tf.random_uniform_initializer(-1*high, high))\n        b_sigma = tf.get_variable(\'b_sigma\', shape=(nb_reads, 1),initializer=tf.constant_initializer(0))\n        shape, high = shape_high((input_size + nb_class, 4*controller_size))\n        W_xh = tf.get_variable(\'W_xh\', shape=shape,initializer=tf.random_uniform_initializer(-1*high, high))\n        b_h = tf.get_variable(\'b_xh\', shape=(4*controller_size),initializer=tf.constant_initializer(0))\n        shape, high = shape_high((controller_size + nb_reads * memory_shape[1], nb_class))\n        W_o = tf.get_variable(\'W_o\', shape=shape,initializer=tf.random_uniform_initializer(-1*high, high))\n        b_o = tf.get_variable(\'b_o\', shape=(nb_class),initializer=tf.constant_initializer(0))\n        shape, high = shape_high((nb_reads * memory_shape[1], 4 * controller_size))\n        W_rh = tf.get_variable(\'W_rh\', shape=shape,initializer=tf.random_uniform_initializer(-1*high, high))\n        shape, high = shape_high((controller_size, 4*controller_size))\n        W_hh = tf.get_variable(\'W_hh\', shape=shape,initializer=tf.random_uniform_initializer(-1*high, high))\n        gamma = tf.get_variable(\'gamma\', shape=[1], initializer=tf.constant_initializer(0.95))\n\n    def slice_equally(x, size, nb_slice):\n        # type: (object, object, object) -> object\n        return [x[:,n*size:(n+1)*size] for n in range(nb_slice)]\n\n\n    def step((M_tm1, c_tm1, h_tm1, r_tm1, wr_tm1, wu_tm1),(x_t)):\n\n        with tf.variable_scope(""Weights"", reuse=True):\n            W_key = tf.get_variable(\'W_key\', shape=(nb_reads, controller_size, memory_shape[1]))\n            b_key = tf.get_variable(\'b_key\', shape=(nb_reads, memory_shape[1]))\n            W_add = tf.get_variable(\'W_add\', shape=(nb_reads, controller_size, memory_shape[1]))\n            b_add = tf.get_variable(\'b_add\', shape=(nb_reads, memory_shape[1]))\n            W_sigma = tf.get_variable(\'W_sigma\', shape=(nb_reads, controller_size, 1))\n            b_sigma = tf.get_variable(\'b_sigma\', shape=(nb_reads, 1))\n            W_xh = tf.get_variable(\'W_xh\', shape=(input_size + nb_class, 4 * controller_size))\n            b_h = tf.get_variable(\'b_xh\', shape=(4 * controller_size))\n            W_o = tf.get_variable(\'W_o\', shape=(controller_size + nb_reads * memory_shape[1], nb_class))\n            b_o = tf.get_variable(\'b_o\', shape=(nb_class))\n            W_rh = tf.get_variable(\'W_rh\', shape=(nb_reads * memory_shape[1], 4 * controller_size))\n            W_hh = tf.get_variable(\'W_hh\', shape=(controller_size, 4 * controller_size))\n            gamma = tf.get_variable(\'gamma\', shape=[1], initializer=tf.constant_initializer(0.95))\n\n\n        #pt = M_tm1[0:2]\n        #pt = tf.Print(pt, [pt], message=\'Prinitng W_key: \')\n        #x_t = tf.transpose(X_t, perm=[1, 0, 2])[ix]\n        #with tf.control_dependencies([pt]):\n        preactivations = tf.matmul(x_t,W_xh) + tf.matmul(r_tm1,W_rh) + tf.matmul(h_tm1,W_hh) + b_h\n        gf_, gi_, go_, u_ = slice_equally(preactivations, controller_size, 4)\n        gf = tf.sigmoid(gf_)\n        gi = tf.sigmoid(gi_)\n        go = tf.sigmoid(go_)\n        u = tf.sigmoid(u_)\n\n        c_t = gf*c_tm1 + gi*u\n        h_t = go * tf.tanh(c_t)  #(batch_size, controller_size)\n\n        h_t_W_key = tf.matmul(h_t, tf.reshape(W_key, shape=(controller_size,-1)))\n        k_t = tf.tanh(tf.reshape(h_t_W_key, shape=(batch_size, nb_reads, memory_shape[1])) + b_key)  #(batch_size, nb_reads, memory_shape[1])\n        h_t_W_add = tf.matmul(h_t, tf.reshape(W_add, shape=(controller_size, -1)))\n        a_t = tf.tanh(tf.reshape(h_t_W_add, shape=(batch_size, nb_reads, memory_shape[1]))  + b_add)\n        h_t_W_sigma = tf.matmul(h_t, tf.reshape(W_sigma, shape=(controller_size, -1)))\n        sigma_t = tf.sigmoid(tf.reshape(h_t_W_sigma, shape=(batch_size, nb_reads,1)) + b_sigma)  #(batch_size, nb_reads, 1)\n\n        _,temp_indices = tf.nn.top_k(wu_tm1, memory_shape[0])\n        wlu_tm1 = tf.slice(temp_indices, [0,0], [batch_size,nb_reads])    #(batch_size, nb_reads)\n\n        sigma_t_wr_tm_1 = tf.tile(sigma_t, tf.stack([1, 1, wr_tm1.get_shape().as_list()[2]]))\n        ww_t = tf.reshape(sigma_t*wr_tm1, (batch_size*nb_reads, memory_shape[0]))    #(batch_size*nb_reads, memory_shape[0])\n        #with tf.variable_scope(""ww_t""):\n        ww_t = update_tensor(ww_t, tf.reshape(wlu_tm1,[-1]),1.0 - tf.reshape(sigma_t,shape=[-1]))   #Update tensor done using index slicing\n        ww_t = tf.reshape(ww_t,(batch_size, nb_reads, memory_shape[0]))\n\n        with tf.variable_scope(""M_t""):\n            print \'wlu_tm1 : \', wlu_tm1.get_shape().as_list()\n            M_t = update_tensor(M_tm1, wlu_tm1[:,0], tf.constant(0., shape=[batch_size, memory_shape[1]]))      #Update tensor done using sparse to dense\n        M_t = tf.add(M_t, tf.matmul(tf.transpose(ww_t, perm=[0,2,1]   ), a_t))   #(batch_size, memory_size[0], memory_size[1])\n        K_t = cosine_similarity(k_t, M_t)\n\n        wr_t = tf.nn.softmax(tf.reshape(K_t, (batch_size*nb_reads, memory_shape[0])))\n        wr_t = tf.reshape(wr_t, (batch_size, nb_reads, memory_shape[0]))    #(batch_size, nb_reads, memory_size[0])\n\n        wu_t = gamma * wu_tm1 + tf.reduce_sum(wr_t, axis=1) + tf.reduce_sum(ww_t, axis=1) #(batch_size, memory_size[0])\n\n        r_t = tf.reshape(tf.matmul(wr_t, M_t),[batch_size,-1])\n\n        return [M_t, c_t, h_t, r_t, wr_t, wu_t]\n\n    #Model Part:\n    sequence_length_var = target_var.get_shape().as_list()[1]   #length of the input\n    output_shape_var = (batch_size*sequence_length_var, nb_class)     #(batch_size*sequence_length_vat,nb_class)\n\n    # Input concat with time offset\n    one_hot_target_flattened = tf.one_hot(tf.reshape(target_var,[-1]), depth=nb_class)\n    one_hot_target = tf.reshape(one_hot_target_flattened, (batch_size, sequence_length_var, nb_class))    #(batch_size, sequence_var_length, nb_class)\n    offset_target_var = tf.concat([tf.zeros_like(tf.expand_dims(one_hot_target[:,0],1)),one_hot_target[:,:-1]],axis=1)   #(batch_size, sequence_var_length, nb_class)\n    l_input_var = tf.concat([input_var,offset_target_var],axis=2)    #(batch_size, sequence_var_length, input_size+nb_class)\n\n    #ix = tf.variable(0,dtype=tf.int32)\n    #cond = lambda M_0, c_0, h_0, r_0, wr_0, wu_0, ix: ix < sequence_length_var\n    l_ntm_var = tf.scan(step, elems=tf.transpose(l_input_var, perm=[1,0,2]),initializer=[M_0, c_0, h_0, r_0, wr_0, wu_0], name=""Scan_MANN_Last"")   #Set of all above parameters, as list\n    l_ntm_output_var = tf.transpose(tf.concat(l_ntm_var[2:4], axis=2), perm=[1, 0, 2])     #h_t & r_t, size=(batch_size, sequence_var_length, controller_size+nb_reads*memory_size[1])\n\n    l_input_var_W_o = tf.matmul(tf.reshape(l_ntm_output_var, shape=(batch_size*sequence_length_var,-1)), W_o)\n    output_var_preactivation = tf.add(tf.reshape(l_input_var_W_o, (batch_size, sequence_length_var,nb_class)), b_o)\n    output_var_flatten = tf.nn.softmax(tf.reshape(output_var_preactivation, output_shape_var))\n    output_var = tf.reshape(output_var_flatten, output_var_preactivation.get_shape().as_list())\n\n    #Parameters\n    params = [W_key, b_key, W_add, b_add, W_sigma, b_sigma, W_xh, W_rh, W_hh, b_h, W_o, b_o]\n\n    return output_var, output_var_flatten, params\n'"
MANN/Test_Model.py,0,b'import tensorflow as tf\nimport numpy as np\nimport pytest\n\nfrom .Model import memory_augmented_neural_network\n\n'
MANN/__init__.py,0,b''
MANN/Utils/Generator.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport pytest\n\nimport os\nimport random\nfrom Images import get_shuffled_images, time_offset_label, load_transform\n\nclass OmniglotGenerator(object):\n    """"""Docstring for OmniglotGenerator""""""\n    def __init__(self, data_folder, batch_size=1, nb_samples=5, nb_samples_per_class=10, max_rotation=-np.pi/6, max_shift=10, img_size=(20,20), max_iter=None):\n        super(OmniglotGenerator, self).__init__()\n        self.data_folder = data_folder\n        self.batch_size = batch_size\n        self.nb_samples = nb_samples\n        self.nb_samples_per_class = nb_samples_per_class\n        self.max_rotation = max_rotation\n        self.max_shift = max_shift\n        self.img_size = img_size\n        self.max_iter = max_iter\n        self.num_iter = 0\n        self.character_folders = [os.path.join(self.data_folder, family, character) \\\n                                  for family in os.listdir(self.data_folder) \\\n                                  if os.path.isdir(os.path.join(self.data_folder, family)) \\\n                                  for character in os.listdir(os.path.join(self.data_folder, family))]\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.next()\n\n    def next(self):\n        if (self.max_iter is None) or (self.num_iter < self.max_iter):\n            self.num_iter += 1\n            return (self.num_iter - 1), self.sample(self.nb_samples)\n        else:\n            raise StopIteration\n\n    def sample(self, nb_samples):\n        sampled_character_folders = random.sample(self.character_folders, nb_samples)\n        random.shuffle(sampled_character_folders)\n\n        example_inputs = np.zeros((self.batch_size, nb_samples * self.nb_samples_per_class, np.prod(self.img_size)), dtype=np.float32)\n        example_outputs = np.zeros((self.batch_size, nb_samples * self.nb_samples_per_class), dtype=np.float32)     #notice hardcoded np.float32 here and above, change it to something else in tf\n\n        for i in range(self.batch_size):\n            labels_and_images = get_shuffled_images(sampled_character_folders, range(nb_samples), nb_samples=self.nb_samples_per_class)\n            sequence_length = len(labels_and_images)\n            labels, image_files = zip(*labels_and_images)\n\n            angles = np.random.uniform(-self.max_rotation, self.max_rotation, size=sequence_length)\n            shifts = np.random.uniform(-self.max_shift, self.max_shift, size=sequence_length)\n\n            example_inputs[i] = np.asarray([load_transform(filename, angle=angle, s=shift, size=self.img_size).flatten() \\\n                                            for (filename, angle, shift) in zip(image_files, angles, shifts)], dtype=np.float32)\n            example_outputs[i] = np.asarray(labels, dtype=np.int32)\n\n        return example_inputs, example_outputs'"
MANN/Utils/Images.py,1,"b'import tensorflow as tf\nimport numpy as np\nimport scipy.misc\nimport matplotlib.pyplot as plt\nimport os\nimport random\n\nfrom scipy.ndimage import rotate,shift\nfrom scipy.misc import imread,imresize\n\n\ndef get_shuffled_images(paths, labels, nb_samples=None):\n    if nb_samples is not None:\n        sampler = lambda x: random.sample(x, nb_samples)\n    else:\n        sampler = lambda x:x\n\n    images = [(i, os.path.join(path, image)) for i,path in zip(labels,paths) for image in sampler(os.listdir(path)) ]\n    random.shuffle(images)\n    return images\n\ndef time_offset_label(labels_and_images):\n    labels, images = zip(*labels_and_images)\n    time_offset_labels = (None,) + labels[:-1]\n    return zip(images, time_offset_labels)\n\ndef load_transform(image_path, angle=0., s=(0,0), size=(20,20)):\n    #Load the image\n    original = imread(image_path, flatten=True)\n    #Rotate the image\n    rotated = np.maximum(np.minimum(rotate(original, angle=angle, cval=1.), 1.), 0.)\n    #Shift the image\n    shifted = shift(rotated, shift=s)\n    #Resize the image\n    resized = np.asarray(imresize(rotated, size=size), dtype=np.float32) / 255 #Note here we coded manually as np.float32, it should be tf.float32\n    #Invert the image\n    inverted = 1. - resized\n    max_value = np.max(inverted)\n    if max_value > 0:\n        inverted /= max_value\n    return inverted'"
MANN/Utils/Metrics.py,22,"b'import tensorflow as tf\nimport numpy as np\nfrom .tf_utils import update_tensor\n\n\n# prediction is the argmax\n\ndef accuracy_instance(predictions, targets, n=[1, 2, 3, 4, 5, 10], nb_classes=5, nb_samples_per_class=10, batch_size=1):\n    targets = tf.cast(targets, predictions.dtype)\n\n    accuracy = tf.constant(value=0, shape=(batch_size, nb_samples_per_class), dtype=tf.float32)\n    indices = tf.constant(value=0, shape=(batch_size, nb_classes+1), dtype=tf.float32)\n\n    def step_((accuracy, indices), (p, t)):\n        """"""with tf.variable_scope(""Metric_step_var"", reuse=True):\n            accuracy = tf.get_variable(name=""accuracy"", shape=(batch_size, nb_samples_per_class),\n                                       initializer=tf.constant_initializer(0), dtype=tf.float32)\n            indices = tf.get_variable(name=""indices"", shape=(batch_size, nb_classes + 1),\n                                      initializer=tf.constant_initializer(0), dtype=tf.float32)""""""\n\n        p = tf.cast(p, tf.int32)\n        t = tf.cast(t, tf.int32)\n        ##Accuracy Update\n        batch_range = tf.cast(tf.range(0, batch_size), dtype=tf.int32)\n        gather = tf.cast(tf.gather_nd(indices,tf.stack([tf.range(0,p.get_shape().as_list()[0]), t], axis=1)), tf.int32)\n        index = tf.cast(tf.stack([batch_range, gather], axis=1), dtype=tf.int64)\n        val = tf.cast(tf.equal(p, t), tf.float32)\n        delta = tf.SparseTensor(indices=index, values=val, dense_shape=tf.cast(accuracy.get_shape().as_list(), tf.int64))\n        accuracy = accuracy + tf.sparse_tensor_to_dense(delta)\n        ##Index Update\n        index = tf.cast(tf.stack([batch_range, t], axis=1), dtype=tf.int64)\n        val = tf.constant(1.0, shape=[batch_size])\n        delta = tf.SparseTensor(indices=index, values=val, dense_shape=tf.cast(indices.get_shape().as_list(), dtype=tf.int64))\n        indices = indices + tf.sparse_tensor_to_dense(delta)\n        return [accuracy, indices]\n\n    accuracy, indices = tf.scan(step_, elems=(tf.transpose(predictions, perm=[1, 0]), tf.transpose(targets, perm=[1, 0])),initializer=[accuracy, indices], name=""Scan_Metric_Last"")\n\n    accuracy = accuracy[-1]\n\n    accuracy = tf.reduce_mean(accuracy / nb_classes , axis=0)\n\n    return accuracy\n'"
MANN/Utils/__init__.py,0,b''
MANN/Utils/init.py,7,"b""import tensorflow\nimport tensorflow as tf\nimport numpy as np\nimport sys\n\ndef shared_glorot_uniform(shape, dtype=tf.float32, name='', n=None):\n\tif isinstance(shape,int):\n\t\thigh = np.sqrt(6. / shape)\n\t\tshape = [shape]\n\telse:\n\t\thigh = np.sqrt(6. / (np.sum(shape[:2]) * np.prod(shape[2:])))\n\tshape = shape if n is None else [\tn] + list(shape)\n\treturn tf.Variable(tf.random_uniform(shape, minval=-high, maxval=high, dtype=dtype, name=name))\n\t\ndef shared_zeros(shape, dtype=tf.float32, name='', n=None):\n\tshape = shape if n is None else (n,) + tuple(shape)\n\treturn tf.Variable(tf.zeros(shape, dtype=dtype), name=name)\n\t\ndef shared_one_hot(shape, dtype=tf.float32, name='', n=None):\n\tshape = (shape,) if isinstance(shape,int) else shape\n\tshape = shape if n is None else (n,) + shape\n\tinitial_vector = np.zeros(shape, dtype=np.float32)\n\tinitial_vector[...,0] = 1\n\treturn tf.Variable(tf.cast(initial_vector, tf.float32), name=name)\n\t\ndef weight_and_bias_init(shape, dtype=tf.float32, name='', n=None):\n\treturn (shared_glorot_uniform(shape, dtype=dtype, name='W_' + name, n=n), \\\n\t\tshared_zeros((shape[1],), dtype=dtype, name='b_' + name, n=n))\n"""
MANN/Utils/similarities.py,2,"b'import tensorflow as tf\n\ndef cosine_similarity(x, y, eps=1e-6):\n\tz = tf.matmul(x, tf.transpose(y, perm=[0,2,1]))\n\tz /= tf.sqrt(tf.multiply(tf.expand_dims(tf.reduce_sum(tf.multiply(x,x), 2), 2),tf.expand_dims(tf.reduce_sum(tf.multiply(y,y), 2), 1)) + eps)\n\t\n\treturn z\n'"
MANN/Utils/tf_utils.py,8,"b'import tensorflow as tf\nimport numpy as np\n\n\ndef shared_float32(x, name=\'\'):\n    return tf.Variable(tf.cast(np.asarray(x, dtype=np.float32), tf.float32), name=name)\n\n\ndef update_tensor(V, dim2, val):  # Update tensor V, with index(:,dim2[:]) by val[:]\n    print \'Shapes Recieved in Update: V, dim, val are ==> \',V.get_shape().as_list(), dim2.get_shape().as_list(), val.get_shape().as_list()\n    val = tf.cast(val, V.dtype)\n\n    def body(_, (v, d2, chg)):\n        print \'Shapes Recieved in Body of Update: v, d2, chg are ==> \', v.get_shape().as_list(), d2.get_shape().as_list(), chg.get_shape().as_list()\n        d2_int = tf.cast(d2, tf.int32)\n        if len(chg.get_shape().as_list()) == 0:\n            chg = [chg]\n        else:\n            chg = tf.reshape(chg, shape=[1]+chg.get_shape().as_list())\n        oob = lambda : tf.slice(tf.concat([v[:d2_int], chg], axis=0), tf.range(0,len(v.get_shape().as_list())), v.get_shape().as_list())\n        inb = lambda : tf.slice(tf.concat([v[:d2_int], chg, v[d2_int + 1:]], axis=0), tf.constant(0,shape=[len(v.get_shape().as_list())]), v.get_shape().as_list())\n        return tf.cond(tf.less(d2_int + 1, v.get_shape().as_list()[0]), inb, oob)\n\n    Z = tf.scan(body, elems=(V, dim2, val), initializer=tf.constant(1, shape=V.get_shape().as_list()[1:], dtype=tf.float32), name=""Scan_Update"")\n    return Z\n'"
