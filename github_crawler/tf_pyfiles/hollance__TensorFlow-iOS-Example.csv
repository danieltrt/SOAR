file_path,api_count,code
Scripts/export_weights.py,5,"b'# This script exports the learned parameters so that we can use them from Metal.\n\n# Note: Dor this simple demo project the weight matrix is only 20 values and the bias\n# is a single number. With such a simple model you might as well stick the parameters\n# inside a static array in the iOS app source code. In practice, however, most models \n# will have millions of parameters.\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\n\ncheckpoint_dir = ""/tmp/voice/""\n\nwith tf.Session() as sess:\n    # Load the graph.\n    graph_file = os.path.join(checkpoint_dir, ""graph.pb"")\n    with tf.gfile.FastGFile(graph_file, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        tf.import_graph_def(graph_def, name="""")\n\n    # Get the model\'s variables.\n    W = sess.graph.get_tensor_by_name(""model/W:0"")\n    b = sess.graph.get_tensor_by_name(""model/b:0"")\n\n    # Load the saved variables from the checkpoint back into the session.\n    checkpoint_file = os.path.join(checkpoint_dir, ""model"")\n    saver = tf.train.Saver([W, b])\n    saver.restore(sess, checkpoint_file)\n\n    # Just for debugging, print out the learned parameters.\n    print(""W:"", W.eval())\n    print(""b:"", b.eval())\n    \n    # Export the contents of W and b as binary files.\n    W.eval().tofile(""W.bin"")\n    b.eval().tofile(""b.bin"")\n'"
Scripts/split_data.py,0,"b'# This script loads the original dataset and splits it into a training set and test set. \n\nimport numpy as np\nimport pandas as pd\n\n# Read the CSV file.\ndf = pd.read_csv(""voice.csv"", header=0)\n\n# Extract the labels into a numpy array. The original labels are text but we convert\n# this to numbers: 1 = male, 0 = female.\nlabels = (df[""label""] == ""male"").values * 1\n\n# labels is a row vector but TensorFlow expects a column vector, so reshape it.\nlabels = labels.reshape(-1, 1)\n\n# Remove the column with the labels.\ndel df[""label""]\n\n# OPTIONAL: Do additional preprocessing, such as scaling the features.\n# for column in df.columns:\n#     mean = df[column].mean()\n#     std = df[column].std()\n#     df[column] = (df[column] - mean) / std\n\n# Convert the training data to a numpy array.\ndata = df.values\nprint(""Full dataset size:"", data.shape)\n\n# Split into a random training set and a test set.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=123456)\n\nprint(""Training set size:"", X_train.shape)\nprint(""Test set size:"", X_test.shape)\n\n# Save the matrices using numpy\'s native format.\nnp.save(""X_train.npy"", X_train)\nnp.save(""X_test.npy"", X_test)\nnp.save(""y_train.npy"", y_train)\nnp.save(""y_test.npy"", y_test)\n'"
Scripts/test.py,5,"b'# This script tests how well the trained model performs on the portion of the \n# data that was not used for training.\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\n\ncheckpoint_dir = ""/tmp/voice/""\n\n# Load the test data.\nX_test = np.load(""X_test.npy"")\ny_test = np.load(""y_test.npy"")\n\nprint(""Test set size:"", X_test.shape)\n\nwith tf.Session() as sess:\n    # Load the graph.\n    graph_file = os.path.join(checkpoint_dir, ""graph.pb"")\n    with tf.gfile.FastGFile(graph_file, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        tf.import_graph_def(graph_def, name="""")\n\n    # Uncomment the next line in case you\'re curious what the graph looks like.\n    #print(graph_def.ListFields())\n\n    # Get the model\'s variables.\n    W = sess.graph.get_tensor_by_name(""model/W:0"")\n    b = sess.graph.get_tensor_by_name(""model/b:0"")\n\n    # Load the saved variables from the checkpoint back into the session.\n    checkpoint_file = os.path.join(checkpoint_dir, ""model"")\n    saver = tf.train.Saver([W, b])\n    saver.restore(sess, checkpoint_file)\n\n    # Get the placeholders and the accuracy operation, so that we can compute\n    # the accuracy (% correct) of the test set.\n    x = sess.graph.get_tensor_by_name(""inputs/x-input:0"")\n    y = sess.graph.get_tensor_by_name(""inputs/y-input:0"")\n    accuracy = sess.graph.get_tensor_by_name(""score/accuracy:0"")\n    print(""Test set accuracy:"", sess.run(accuracy, feed_dict={x: X_test, y: y_test}))\n\n    # Also show some other reports.\n    inference = sess.graph.get_tensor_by_name(""inference/inference:0"")\n    predictions = sess.run(inference, feed_dict={x: X_test})\n    print(""\\nClassification report:"")\n    print(metrics.classification_report(y_test.ravel(), predictions))\n    print(""Confusion matrix:"")\n    print(metrics.confusion_matrix(y_test.ravel(), predictions))\n'"
Scripts/train.py,25,"b'# This script is used to train the model. It repeats indefinitely and saves the\n# model every so often to a checkpoint. \n#\n# Press Ctrl+C when you feel that training has gone on long enough (since this is \n# only a simple model it takes less than a minute to train, but a training a deep l\n# earning model could take days).\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\ncheckpoint_dir = ""/tmp/voice/""\nprint_every = 1000\nsave_every = 10000\nnum_inputs = 20\nnum_classes = 1\n\n# Load the training data.\nX_train = np.load(""X_train.npy"")\ny_train = np.load(""y_train.npy"")\n\nprint(""Training set size:"", X_train.shape)\n\n# Below we\'ll define the computational graph using TensorFlow. The different parts \n# of the model are grouped into different ""scopes"", making it easier to understand\n# what each part is doing.\n\n# Hyperparameters let you configure the model and how it is trained. They\'re\n# called ""hyper"" parameters because unlike the regular parameters they are not\n# learned by the model -- you have to set them to appropriate values yourself.\n#\n# The learning_rate tells the optimizer how big of a steps it should take.\n# Regularization is used to prevent overfitting on the training set.\nwith tf.name_scope(""hyperparameters""):\n    regularization = tf.placeholder(tf.float32, name=""regularization"")\n    learning_rate = tf.placeholder(tf.float32, name=""learning-rate"")\n\n# This is where we feed the training data (and later the test data) into the model. \n# In this dataset there are 20 features, so x is a matrix with 20 columns. Its number \n# of rows is None because it depends on how many examples at a time we put into this \n# matrix. This is a binary classifier so for every training example, y gives a single \n# output: 1 = male, 0 = female.\nwith tf.name_scope(""inputs""):\n    x = tf.placeholder(tf.float32, [None, num_inputs], name=""x-input"")\n    y = tf.placeholder(tf.float32, [None, num_classes], name=""y-input"")\n    \n# The parameters that we\'ll learn consist of W, a weight matrix, and b, a vector\n# of bias values. (Actually, b is just a single value since the classifier has only\n# one output. For a classifier that can recognize multiple classes, b would have as\n# many elements as there are classes.)\nwith tf.name_scope(""model""):\n    W = tf.Variable(tf.zeros([num_inputs, num_classes]), name=""W"")\n    b = tf.Variable(tf.zeros([num_classes]), name=""b"")\n\n    # The output is the probability the speaker is male. If this is greater than\n    # 0.5, we consider the speaker to be male, otherwise female.\n    y_pred = tf.sigmoid(tf.matmul(x, W) + b, name=""y_pred"")\n\n# This is a logistic classifier, so the loss function is the logistic loss.\nwith tf.name_scope(""loss-function""):\n    loss = tf.losses.log_loss(labels=y, predictions=y_pred)\n    \n    # Add L2 regularization to the loss.\n    loss += regularization * tf.nn.l2_loss(W)\n\n# Use the ADAM optimizer to minimize the loss.\nwith tf.name_scope(""train""):\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    train_op = optimizer.minimize(loss)\n\n# For doing inference on new data for which we don\'t have labels.\nwith tf.name_scope(""inference""):\n    inference = tf.to_float(y_pred > 0.5, name=""inference"")\n\n# The accuracy operation computes the % correct on a dataset with known labels. \nwith tf.name_scope(""score""):\n    correct_prediction = tf.equal(inference, y)\n    accuracy = tf.reduce_mean(tf.to_float(correct_prediction), name=""accuracy"")\n\ninit = tf.global_variables_initializer()\n\n# For writing training checkpoints and reading them back in.\nsaver = tf.train.Saver()\ntf.gfile.MakeDirs(checkpoint_dir)\n\nwith tf.Session() as sess:\n    # Write the graph definition to a file. We\'ll load this in the test.py script.\n    tf.train.write_graph(sess.graph_def, checkpoint_dir, ""graph.pb"", False)\n\n    # Reset W and b to zero.\n    sess.run(init)\n\n    # Sanity check: the initial loss should be 0.693146, which is -ln(0.5).\n    loss_value = sess.run(loss, feed_dict={x: X_train, y: y_train, regularization: 0})\n    print(""Initial loss:"", loss_value)\n\n    # Loop forever:\n    step = 0\n    while True:\n        # We randomly shuffle the examples every time we train.\n        perm = np.arange(len(X_train))\n        np.random.shuffle(perm)\n        X_train = X_train[perm]\n        y_train = y_train[perm]\n\n        # Run the optimizer over the entire training set at once. For larger datasets\n        # you would train in batches of 100-1000 examples instead of the entire thing.\n        feed = {x: X_train, y: y_train, learning_rate: 1e-2, regularization: 1e-5}\n        sess.run(train_op, feed_dict=feed)\n\n        # Print the loss once every so many steps. Because of the regularization, \n        # at some point the loss won\'t become smaller anymore. At that point, it\'s\n        # safe to press Ctrl+C to stop the training.\n        if step % print_every == 0:\n            train_accuracy, loss_value = sess.run([accuracy, loss], feed_dict=feed)\n            print(""step: %4d, loss: %.4f, training accuracy: %.4f"" % \\\n                    (step, loss_value, train_accuracy))\n\n        step += 1\n\n        # Save the model. You should only press Ctrl+C after you see this message.\n        if step % save_every == 0:\n            checkpoint_file = os.path.join(checkpoint_dir, ""model"")\n            saver.save(sess, checkpoint_file)            \n            print(""*** SAVED MODEL ***"")\n'"
