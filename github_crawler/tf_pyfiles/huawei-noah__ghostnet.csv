file_path,api_count,code
hubconf.py,0,"b'dependencies = [\'torch\']\nimport torch\nfrom pytorch.ghostnet import ghostnet\n\n\nstate_dict_url = \'https://github.com/huawei-noah/ghostnet/raw/master/pytorch/models/state_dict_93.98.pth\'\n\n\ndef ghostnet_1x(pretrained=False, **kwargs):\n\t  """""" # This docstring shows up in hub.help()\n    GhostNet 1.0x model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    """"""\n\t  model = ghostnet(num_classes=1000, width=1.0, dropout=0.2)\n\t  if pretrained:\n\t  \t  state_dict = torch.hub.load_state_dict_from_url(state_dict_url, progress=True)\n\t  \t  model.load_state_dict(state_dict)\n\t  return model\n'"
pytorch/ghostnet.py,0,"b'# 2020.06.09-Changed for building GhostNet\n#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n""""""\nCreates a GhostNet Model as defined in:\nGhostNet: More Features from Cheap Operations By Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu.\nhttps://arxiv.org/abs/1911.11907\nModified from https://github.com/d-li14/mobilenetv3.pytorch and https://github.com/rwightman/pytorch-image-models\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\n__all__ = [\'ghost_net\']\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    """"""\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    """"""\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef hard_sigmoid(x, inplace: bool = False):\n    if inplace:\n        return x.add_(3.).clamp_(0., 6.).div_(6.)\n    else:\n        return F.relu6(x + 3.) / 6.\n\n\nclass SqueezeExcite(nn.Module):\n    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n        super(SqueezeExcite, self).__init__()\n        self.gate_fn = gate_fn\n        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n        self.act1 = act_layer(inplace=True)\n        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\n\n    def forward(self, x):\n        x_se = self.avg_pool(x)\n        x_se = self.conv_reduce(x_se)\n        x_se = self.act1(x_se)\n        x_se = self.conv_expand(x_se)\n        x = x * self.gate_fn(x_se)\n        return x    \n\n    \nclass ConvBnAct(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size,\n                 stride=1, act_layer=nn.ReLU):\n        super(ConvBnAct, self).__init__()\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_chs)\n        self.act1 = act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        return x\n\n\nclass GhostModule(nn.Module):\n    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n        super(GhostModule, self).__init__()\n        self.oup = oup\n        init_channels = math.ceil(oup / ratio)\n        new_channels = init_channels*(ratio-1)\n\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n            nn.BatchNorm2d(init_channels),\n            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n        )\n\n        self.cheap_operation = nn.Sequential(\n            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n            nn.BatchNorm2d(new_channels),\n            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n        )\n\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_operation(x1)\n        out = torch.cat([x1,x2], dim=1)\n        return out[:,:self.oup,:,:]\n\n\nclass GhostBottleneck(nn.Module):\n    """""" Ghost bottleneck w/ optional SE""""""\n\n    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,\n                 stride=1, act_layer=nn.ReLU, se_ratio=0.):\n        super(GhostBottleneck, self).__init__()\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.stride = stride\n\n        # Point-wise expansion\n        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            self.conv_dw = nn.Conv2d(mid_chs, mid_chs, dw_kernel_size, stride=stride,\n                             padding=(dw_kernel_size-1)//2,\n                             groups=mid_chs, bias=False)\n            self.bn_dw = nn.BatchNorm2d(mid_chs)\n\n        # Squeeze-and-excitation\n        if has_se:\n            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio)\n        else:\n            self.se = None\n\n        # Point-wise linear projection\n        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)\n        \n        # shortcut\n        if (in_chs == out_chs and self.stride == 1):\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_chs, in_chs, dw_kernel_size, stride=stride,\n                       padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),\n                nn.BatchNorm2d(in_chs),\n                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_chs),\n            )\n\n\n    def forward(self, x):\n        residual = x\n\n        # 1st ghost bottleneck\n        x = self.ghost1(x)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            x = self.conv_dw(x)\n            x = self.bn_dw(x)\n\n        # Squeeze-and-excitation\n        if self.se is not None:\n            x = self.se(x)\n\n        # 2nd ghost bottleneck\n        x = self.ghost2(x)\n        \n        x += self.shortcut(residual)\n        return x\n\n\nclass GhostNet(nn.Module):\n    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):\n        super(GhostNet, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = cfgs\n        self.dropout = dropout\n\n        # building first layer\n        output_channel = _make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(output_channel)\n        self.act1 = nn.ReLU(inplace=True)\n        input_channel = output_channel\n\n        # building inverted residual blocks\n        stages = []\n        block = GhostBottleneck\n        for cfg in self.cfgs:\n            layers = []\n            for k, exp_size, c, se_ratio, s in cfg:\n                output_channel = _make_divisible(c * width, 4)\n                hidden_channel = _make_divisible(exp_size * width, 4)\n                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\n                              se_ratio=se_ratio))\n                input_channel = output_channel\n            stages.append(nn.Sequential(*layers))\n\n        output_channel = _make_divisible(exp_size * width, 4)\n        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n        input_channel = output_channel\n        \n        self.blocks = nn.Sequential(*stages)        \n\n        # building last several layers\n        output_channel = 1280\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)\n        self.act2 = nn.ReLU(inplace=True)\n        self.classifier = nn.Linear(output_channel, num_classes)\n\n    def forward(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        x = x.view(x.size(0), -1)\n        if self.dropout > 0.:\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef ghostnet(**kwargs):\n    """"""\n    Constructs a GhostNet model\n    """"""\n    cfgs = [\n        # k, t, c, SE, s \n        # stage1\n        [[3,  16,  16, 0, 1]],\n        # stage2\n        [[3,  48,  24, 0, 2]],\n        [[3,  72,  24, 0, 1]],\n        # stage3\n        [[5,  72,  40, 0.25, 2]],\n        [[5, 120,  40, 0.25, 1]],\n        # stage4\n        [[3, 240,  80, 0, 2]],\n        [[3, 200,  80, 0, 1],\n         [3, 184,  80, 0, 1],\n         [3, 184,  80, 0, 1],\n         [3, 480, 112, 0.25, 1],\n         [3, 672, 112, 0.25, 1]\n        ],\n        # stage5\n        [[5, 672, 160, 0.25, 2]],\n        [[5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1],\n         [5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1]\n        ]\n    ]\n    return GhostNet(cfgs, **kwargs)\n\n\nif __name__==\'__main__\':\n    model = ghostnet()\n    model.eval()\n    print(model)\n    input = torch.randn(32,3,320,256)\n    y = model(input)\n    print(y.size())'"
pytorch/validate.py,0,"b'# 2020.06.09-Changed for main script for testing GhostNet on ImageNet\n#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n""""""PyTorch Inference Script\n\nAn example inference script that outputs top-k class ids for images in a folder into a csv.\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport os\nimport time\nimport argparse\nimport logging\nimport numpy as np\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom ghostnet import ghostnet\n\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Inference\')\nparser.add_argument(\'--data\', metavar=\'DIR\', default=\'/cache/data/imagenet/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--output_dir\', metavar=\'DIR\', default=\'/cache/models/\',\n                    help=\'path to output files\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 2)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--num-classes\', type=int, default=1000,\n                    help=\'Number classes in dataset\')\nparser.add_argument(\'--width\', type=float, default=1.0, \n                    help=\'Width ratio (default: 1.0)\')\nparser.add_argument(\'--dropout\', type=float, default=0.2, metavar=\'PCT\',\n                    help=\'Dropout rate (default: 0.2)\')\nparser.add_argument(\'--num-gpu\', type=int, default=1,\n                    help=\'Number of GPUS to use\')\n\n\ndef main():\n    args = parser.parse_args()\n\n    model = ghostnet(num_classes=args.num_classes, width=args.width, dropout=args.dropout)\n    model.load_state_dict(torch.load(\'./models/state_dict_93.98.pth\'))\n\n    if args.num_gpu > 1:\n        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu))).cuda()\n    elif args.num_gpu < 1:\n        model = model\n    else:\n        model = model.cuda()\n    print(\'GhostNet created.\')\n    \n    valdir = os.path.join(args.data, \'val\')\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                          std=[0.229, 0.224, 0.225])\n    loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    model.eval()\n    \n    validate_loss_fn = nn.CrossEntropyLoss().cuda()\n    eval_metrics = validate(model, loader, validate_loss_fn, args)\n    print(eval_metrics)\n\n\ndef validate(model, loader, loss_fn, args, log_suffix=\'\'):\n    batch_time_m = AverageMeter()\n    losses_m = AverageMeter()\n    top1_m = AverageMeter()\n    top5_m = AverageMeter()\n\n    model.eval()\n\n    end = time.time()\n    last_idx = len(loader) - 1\n    with torch.no_grad():\n        for batch_idx, (input, target) in enumerate(loader):\n            last_batch = batch_idx == last_idx\n            input = input.cuda()\n            target = target.cuda()\n\n            output = model(input)\n            if isinstance(output, (tuple, list)):\n                output = output[0]\n\n            loss = loss_fn(output, target)\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n            reduced_loss = loss.data\n\n            torch.cuda.synchronize()\n\n            losses_m.update(reduced_loss.item(), input.size(0))\n            top1_m.update(acc1.item(), output.size(0))\n            top5_m.update(acc5.item(), output.size(0))\n\n            batch_time_m.update(time.time() - end)\n            end = time.time()\n            if (last_batch or batch_idx % 10 == 0):\n                log_name = \'Test\' + log_suffix\n                logging.info(\n                    \'{0}: [{1:>4d}/{2}]  \'\n                    \'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  \'\n                    \'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  \'\n                    \'Acc@1: {top1.val:>7.4f} ({top1.avg:>7.4f})  \'\n                    \'Acc@5: {top5.val:>7.4f} ({top5.avg:>7.4f})\'.format(\n                        log_name, batch_idx, last_idx, batch_time=batch_time_m,\n                        loss=losses_m, top1=top1_m, top5=top5_m))\n\n    metrics = OrderedDict([(\'loss\', losses_m.avg), (\'top1\', top1_m.avg), (\'top5\', top5_m.avg)])\n\n    return metrics\n\n\nclass AverageMeter:\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the accuracy over the k top predictions for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    return [correct[:k].view(-1).float().sum(0) * 100. / batch_size for k in topk]\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tensorflow/ghostnet.py,14,"b'# 2020.02.26-Changed for building GhostNet\r\n#            Huawei Technologies Co., Ltd. <foss@huawei.com>\r\n# modified from the code: https://github.com/balancap/tf-imagenet/blob/master/models/mobilenet/mobilenet_v2.py\r\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n# http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# =============================================================================\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom collections import namedtuple\r\nimport functools\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorpack.models import (\r\n    MaxPooling, GlobalAvgPooling, BatchNorm, Dropout, BNReLU, FullyConnected)\r\nfrom tensorpack.tfutils import argscope\r\nfrom tensorpack.models.common import layer_register\r\nfrom tensorpack.utils.argtools import shape2d\r\n\r\nfrom imagenet_utils import ImageNetModel\r\nimport utils\r\nfrom myconv2d import MyConv2D as Conv2D\r\nfrom myconv2d import BNNoReLU, SELayer\r\nfrom myconv2d import GhostModule as MyConv\r\n\r\nkernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)\r\n\r\nslim = tf.contrib.slim\r\n\r\n# =========================================================================== #\r\n# GhostNet model.\r\n# =========================================================================== #\r\nclass GhostNet(ImageNetModel):\r\n    """"""GhostNet model.\r\n    """"""\r\n    def __init__(self, num_classes=1000, dw_code=None, ratio_code=None, se=1, data_format=\'NHWC\', \r\n                 width=1.0, depth=1.0, lr=0.2, weight_decay = 0.00004, dropout_keep_prob=0.8,\r\n                 label_smoothing=0.0):\r\n        self.scope = \'MobileNetV2\'\r\n        self.num_classes = num_classes\r\n        self.dw_code = dw_code\r\n        self.ratio_code = ratio_code\r\n        self.se = se\r\n        self.depth = depth\r\n        self.depth_multiplier = width\r\n        self.data_format = data_format\r\n        self.lr = lr\r\n        self.weight_decay = weight_decay\r\n        self.dropout_keep_prob = dropout_keep_prob\r\n        self.label_smoothing = label_smoothing\r\n        self.image_shape = 224\r\n\r\n    def get_logits(self, inputs):\r\n        sc = ghostnet_arg_scope(\r\n            data_format=self.data_format,\r\n            weight_decay=self.weight_decay,\r\n            use_batch_norm=True,\r\n            batch_norm_decay=0.9997,\r\n            batch_norm_epsilon=0.001,\r\n            regularize_depthwise=False)\r\n        with slim.arg_scope(sc):\r\n            with argscope(Conv2D, \r\n                  kernel_initializer=kernel_initializer):\r\n                with argscope([Conv2D, BatchNorm], data_format=self.data_format):\r\n                    logits, end_points = ghost_net(\r\n                        inputs,\r\n                        dw_code=self.dw_code,\r\n                        ratio_code=self.ratio_code,\r\n                        se=self.se,\r\n                        num_classes=self.num_classes,\r\n                        dropout_keep_prob=self.dropout_keep_prob,\r\n                        min_depth=8,\r\n                        depth_multiplier=self.depth_multiplier,\r\n                        depth=self.depth,\r\n                        conv_defs=None,\r\n                        prediction_fn=tf.contrib.layers.softmax,\r\n                        spatial_squeeze=True,\r\n                        reuse=None,\r\n                        scope=self.scope,\r\n                        global_pool=False)\r\n                    return logits\r\n\r\n\r\n# =========================================================================== #\r\n# Functional definition.\r\n# =========================================================================== #\r\n# Conv and Bottleneck namedtuple define layers of the GhostNet architecture\r\n# Conv defines 3x3 convolution layers\r\n# stride is the stride of the convolution\r\n# depth is the number of channels or filters in a layer\r\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\', \'factor\', \'se\'])\r\nBottleneck = namedtuple(\'Bottleneck\', [\'kernel\', \'stride\', \'depth\', \'factor\', \'se\'])\r\n\r\n# _CONV_DEFS specifies the GhostNet body\r\n_CONV_DEFS_0 = [\r\n    Conv(kernel=[3, 3], stride=2, depth=16, factor=1, se=0),\r\n    Bottleneck(kernel=[3, 3], stride=1, depth=16, factor=1, se=0),\r\n\r\n    Bottleneck(kernel=[3, 3], stride=2, depth=24, factor=48/16, se=0),\r\n    Bottleneck(kernel=[3, 3], stride=1, depth=24, factor=72/24, se=0),\r\n\r\n    Bottleneck(kernel=[5, 5], stride=2, depth=40, factor=72/24, se=1),\r\n    Bottleneck(kernel=[5, 5], stride=1, depth=40, factor=120/40, se=1),\r\n\r\n    Bottleneck(kernel=[3, 3], stride=2, depth=80, factor=240/40, se=0),\r\n    Bottleneck(kernel=[3, 3], stride=1, depth=80, factor=200/80, se=0),\r\n    Bottleneck(kernel=[3, 3], stride=1, depth=80, factor=184/80, se=0),\r\n    Bottleneck(kernel=[3, 3], stride=1, depth=80, factor=184/80, se=0),\r\n\r\n    Bottleneck(kernel=[3, 3], stride=1, depth=112, factor=480/80, se=1),\r\n    Bottleneck(kernel=[3, 3], stride=1, depth=112, factor=672/112, se=1),\r\n    Bottleneck(kernel=[5, 5], stride=2, depth=160, factor=672/112, se=1),\r\n\r\n    Bottleneck(kernel=[5, 5], stride=1, depth=160, factor=960/160, se=0),\r\n    Bottleneck(kernel=[5, 5], stride=1, depth=160, factor=960/160, se=1),\r\n    Bottleneck(kernel=[5, 5], stride=1, depth=160, factor=960/160, se=0),\r\n    Bottleneck(kernel=[5, 5], stride=1, depth=160, factor=960/160, se=1),\r\n\r\n    Conv(kernel=[1, 1], stride=1, depth=960, factor=1, se=0),\r\n    Conv(kernel=[1, 1], stride=1, depth=1280, factor=1, se=0)\r\n]\r\n\r\n@layer_register(log_shape=True)\r\ndef DepthConv(x, kernel_shape, padding=\'SAME\', stride=1, data_format=\'NHWC\',\r\n              W_init=None, activation=tf.identity):\r\n    in_shape = x.get_shape().as_list()\r\n    if data_format==\'NHWC\':\r\n        in_channel = in_shape[3]\r\n        stride_shape = [1, stride, stride, 1]\r\n    elif data_format==\'NCHW\':\r\n        in_channel = in_shape[1]\r\n        stride_shape = [1, 1, stride, stride]\r\n    out_channel = in_channel\r\n    channel_mult = out_channel // in_channel\r\n\r\n    if W_init is None:\r\n        W_init = kernel_initializer\r\n    kernel_shape = shape2d(kernel_shape) #[kernel_shape, kernel_shape]\r\n    filter_shape = kernel_shape + [in_channel, channel_mult]\r\n\r\n    W = tf.get_variable(\'W\', filter_shape, initializer=W_init)\r\n    conv = tf.nn.depthwise_conv2d(x, W, stride_shape, padding=padding, data_format=data_format)\r\n    return activation(conv, name=\'output\')\r\n\r\n    \r\ndef ghostnet_base(inputs,\r\n                  final_endpoint=None,\r\n                  min_depth=8,\r\n                  depth_multiplier=1.0,\r\n                  depth=1.0,\r\n                  conv_defs=None,\r\n                  output_stride=None,\r\n                  dw_code=None,\r\n                  ratio_code=None,\r\n                  se=1,\r\n                  scope=None):\r\n    def depth(d):\r\n        d = max(int(d * depth_multiplier), min_depth)\r\n        d = round(d / 4) * 4\r\n        return d\r\n    \r\n    end_points = {}\r\n\r\n    # Used to find thinned depths for each layer.\r\n    if depth_multiplier <= 0:\r\n        raise ValueError(\'depth_multiplier is not greater than zero.\')\r\n\r\n    if conv_defs is None:\r\n        conv_defs = _CONV_DEFS_0\r\n        \r\n    if dw_code is None or len(dw_code) < len(conv_defs):\r\n        dw_code = [3] * len(conv_defs)\r\n    print(\'dw_code\', dw_code)\r\n        \r\n    if ratio_code is None or len(ratio_code) < len(conv_defs):\r\n        ratio_code = [2] * len(conv_defs)\r\n    print(\'ratio_code\', ratio_code)\r\n    \r\n    se_code =  [x.se for x in conv_defs]\r\n    print(\'se_code\', se_code)\r\n    \r\n    if final_endpoint is None:\r\n        final_endpoint = \'Conv2d_%d\'%(len(conv_defs)-1)\r\n\r\n    if output_stride is not None and output_stride not in [8, 16, 32]:\r\n        raise ValueError(\'Only allowed output_stride values are 8, 16, 32.\')\r\n        \r\n    with tf.variable_scope(scope, \'MobilenetV2\', [inputs]):\r\n        with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\r\n            # The current_stride variable keeps track of the output stride of the\r\n            # activations, i.e., the running product of convolution strides up to the\r\n            # current network layer. This allows us to invoke atrous convolution\r\n            # whenever applying the next convolution would result in the activations\r\n            # having output stride larger than the target output_stride.\r\n            current_stride = 1\r\n\r\n            # The atrous convolution rate parameter.\r\n            rate = 1\r\n            net = inputs\r\n            in_depth = 3\r\n            gi = 0\r\n            for i, conv_def in enumerate(conv_defs):\r\n                print(\'---\')\r\n                end_point_base = \'Conv2d_%d\' % i\r\n                if output_stride is not None and current_stride == output_stride:\r\n                    # If we have reached the target output_stride, then we need to employ\r\n                    # atrous convolution with stride=1 and multiply the atrous rate by the\r\n                    # current unit\'s stride for use in subsequent layers.\r\n                    layer_stride = 1\r\n                    layer_rate = rate\r\n                    rate *= conv_def.stride\r\n                else:\r\n                    layer_stride = conv_def.stride\r\n                    layer_rate = 1\r\n                    current_stride *= conv_def.stride\r\n                    \r\n                # change last bottleneck\r\n                if i+2 == len(conv_defs):\r\n                    end_point = end_point_base\r\n                    net = Conv2D(end_point, net, depth(conv_def.depth), [1, 1], stride=1, \r\n                                 data_format=\'NHWC\', activation=BNReLU, use_bias=False)\r\n                    \r\n                    ksize = utils.ksize_for_squeezing(net, 1024)\r\n                    net = slim.avg_pool2d(net, ksize, padding=\'VALID\',\r\n                                          scope=\'AvgPool_7\')\r\n                    end_points[end_point] = net\r\n                    \r\n                # Normal conv2d.\r\n                elif i+1 == len(conv_defs):\r\n                    end_point = end_point_base\r\n                    net = Conv2D(end_point, net, 1280, conv_def.kernel, stride=conv_def.stride, \r\n                                 data_format=\'NHWC\', activation=BNReLU, use_bias=False)\r\n                    end_points[end_point] = net\r\n                    \r\n                elif isinstance(conv_def, Conv):\r\n                    end_point = end_point_base\r\n                    net = Conv2D(end_point, net, depth(conv_def.depth), conv_def.kernel, stride=conv_def.stride, \r\n                                 data_format=\'NHWC\', activation=BNReLU, use_bias=False)\r\n                    end_points[end_point] = net\r\n\r\n                # Bottleneck block.\r\n                elif isinstance(conv_def, Bottleneck):\r\n                    # Stride > 1 or different depth: no residual part.\r\n                    if layer_stride == 1 and in_depth == conv_def.depth:\r\n                        res = net\r\n                    else:\r\n                        end_point = end_point_base + \'_shortcut_dw\'\r\n                        res = DepthConv(end_point, net, conv_def.kernel, stride=layer_stride, \r\n                                        data_format=\'NHWC\', activation=BNNoReLU)\r\n                        end_point = end_point_base + \'_shortcut_1x1\'\r\n                        res = Conv2D(end_point, res, depth(conv_def.depth), [1, 1], strides=1, data_format=\'NHWC\',\r\n                                     activation=BNNoReLU, use_bias=False)\r\n                    \r\n                    # Increase depth with 1x1 conv.\r\n                    end_point = end_point_base + \'_up_pointwise\'\r\n                    net = MyConv(end_point, net, depth(in_depth * conv_def.factor), [1, 1], dw_code[gi], ratio_code[gi], \r\n                                 strides=1, data_format=\'NHWC\', activation=BNReLU, use_bias=False)\r\n                    end_points[end_point] = net\r\n                    \r\n                    # Depthwise conv2d.\r\n                    if layer_stride > 1:\r\n                        end_point = end_point_base + \'_depthwise\'\r\n                        net = DepthConv(end_point, net, conv_def.kernel, stride=layer_stride, \r\n                                        data_format=\'NHWC\', activation=BNNoReLU)\r\n                        end_points[end_point] = net\r\n                    # SE\r\n                    if se_code[i] > 0 and se > 0:\r\n                        end_point = end_point_base + \'_se\'\r\n                        net = SELayer(end_point, net, depth(in_depth * conv_def.factor), 4)\r\n                        end_points[end_point] = net\r\n                        \r\n                    # Downscale 1x1 conv.\r\n                    end_point = end_point_base + \'_down_pointwise\'\r\n                    net = MyConv(end_point, net, depth(conv_def.depth), [1, 1], dw_code[gi], ratio_code[gi], strides=1, \r\n                                 data_format=\'NHWC\', activation=BNNoReLU if res is None else BNNoReLU, use_bias=False)\r\n                    gi += 1\r\n                        \r\n                    # Residual connection?\r\n                    end_point = end_point_base + \'_residual\'\r\n                    net = tf.add(res, net, name=end_point) if res is not None else net\r\n                    end_points[end_point] = net\r\n\r\n                # Unknown...\r\n                else:\r\n                    raise ValueError(\'Unknown convolution type %s for layer %d\'\r\n                                     % (conv_def.ltype, i))\r\n                in_depth = conv_def.depth\r\n                # Final end point?\r\n                if final_endpoint in end_points:\r\n                    return end_points[final_endpoint], end_points\r\n\r\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\r\n\r\n\r\ndef ghost_net(inputs,\r\n                 num_classes=1000,\r\n                 dropout_keep_prob=0.999,\r\n                 is_training=True,\r\n                 min_depth=8,\r\n                 depth_multiplier=1.0,\r\n                 depth=1.0,\r\n                 conv_defs=None,\r\n                 prediction_fn=tf.contrib.layers.softmax,\r\n                 spatial_squeeze=True,\r\n                 reuse=None,\r\n                 scope=\'MobilenetV2\',\r\n                 global_pool=False,\r\n                 dw_code=None,\r\n                 ratio_code=None,\r\n                 se=1,\r\n                ):\r\n    input_shape = inputs.get_shape().as_list()\r\n    if len(input_shape) != 4:\r\n        raise ValueError(\'Invalid input tensor rank, expected 4, was: %d\' %\r\n                         len(input_shape))\r\n\r\n    with tf.variable_scope(scope, \'MobilenetV2\', [inputs], reuse=reuse) as scope:\r\n        with slim.arg_scope([slim.batch_norm, slim.dropout],\r\n                            is_training=is_training):\r\n            net, end_points = ghostnet_base(inputs, scope=scope, dw_code=dw_code, ratio_code=ratio_code,\r\n                                                se=se, min_depth=min_depth, depth=depth,\r\n                                                depth_multiplier=depth_multiplier,\r\n                                                conv_defs=conv_defs)\r\n            with tf.variable_scope(\'Logits\'):\r\n                if not num_classes:\r\n                    return net, end_points\r\n                # 1 x 1 x 1280\r\n                net = Dropout(\'Dropout_1b\', net, keep_prob=dropout_keep_prob)\r\n                logits = Conv2D(\'Conv2d_1c_1x1\', net, num_classes, 1, strides=1, \r\n                                 data_format=\'NHWC\', activation=None)\r\n                if spatial_squeeze:\r\n                    logits = utils.spatial_squeeze(logits, scope=\'SpatialSqueeze\')\r\n            end_points[\'Logits\'] = logits\r\n            if prediction_fn:\r\n                end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\r\n    return logits, end_points\r\n\r\n\r\ndef ghostnet_arg_scope(is_training=True,\r\n                           data_format=\'NHWC\',\r\n                           weight_decay=0.00004,\r\n                           use_batch_norm=True,\r\n                           batch_norm_decay=0.99,\r\n                           batch_norm_epsilon=0.001,\r\n                           regularize_depthwise=False):\r\n    batch_norm_params = {\r\n        \'decay\': batch_norm_decay,\r\n        \'epsilon\': batch_norm_epsilon,\r\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\r\n        \'fused\': True,\r\n        \'scale\': True,\r\n        \'data_format\': data_format,\r\n        \'is_training\': is_training,\r\n    }\r\n    if use_batch_norm:\r\n        normalizer_fn = slim.batch_norm\r\n        normalizer_params = batch_norm_params\r\n    else:\r\n        normalizer_fn = None\r\n        normalizer_params = {}\r\n\r\n    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\r\n    weights_initializer = kernel_initializer\r\n    if regularize_depthwise:\r\n        depthwise_regularizer = weights_regularizer\r\n    else:\r\n        depthwise_regularizer = None\r\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\r\n                        weights_initializer=weights_initializer,\r\n                        activation_fn=tf.nn.relu,\r\n                        normalizer_fn=normalizer_fn,\r\n                        normalizer_params=normalizer_params):\r\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\r\n            with slim.arg_scope([slim.conv2d], weights_regularizer=weights_regularizer):\r\n                with slim.arg_scope([slim.separable_conv2d],\r\n                                    weights_regularizer=depthwise_regularizer):\r\n                    # Data format scope...\r\n                    data_sc = utils.data_format_scope(data_format)\r\n                    with slim.arg_scope(data_sc) as sc:\r\n                        return sc\r\n'"
tensorflow/imagenet_utils.py,32,"b'# 2020.02.26-Changed for utils for testing GhostNet on ImageNet\r\n#            Huawei Technologies Co., Ltd. <foss@huawei.com>\r\n# modified from https://github.com/tensorpack/tensorpack/blob/master/examples/ImageNetModels/imagenet_utils.py\r\nimport cv2\r\nimport numpy as np\r\nimport multiprocessing\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\nfrom abc import abstractmethod\r\n\r\nfrom tensorpack import imgaug, dataset, ModelDesc\r\nfrom tensorpack.dataflow import (\r\n    AugmentImageComponent, PrefetchDataZMQ, MapData,\r\n    BatchData, MultiThreadMapData)\r\nfrom tensorpack.predict import PredictConfig, SimpleDatasetPredictor\r\nfrom tensorpack.utils.stats import RatioCounter\r\nfrom tensorpack.models import regularize_cost\r\nfrom tensorpack.tfutils.summary import add_moving_summary\r\nfrom tensorpack.utils import logger\r\n\r\n\r\nclass GoogleNetResize(imgaug.ImageAugmentor):\r\n    """"""\r\n    crop 8%~100% of the original image\r\n    See `Going Deeper with Convolutions` by Google.\r\n    """"""\r\n    def __init__(self, crop_area_fraction=0.08,\r\n                 aspect_ratio_low=0.75, aspect_ratio_high=1.333,\r\n                 target_shape=224):\r\n        self._init(locals())\r\n\r\n    def _augment(self, img, _):\r\n        h, w = img.shape[:2]\r\n        area = h * w\r\n        for _ in range(10):\r\n            targetArea = self.rng.uniform(self.crop_area_fraction, 1.0) * area\r\n            aspectR = self.rng.uniform(self.aspect_ratio_low, self.aspect_ratio_high)\r\n            ww = int(np.sqrt(targetArea * aspectR) + 0.5)\r\n            hh = int(np.sqrt(targetArea / aspectR) + 0.5)\r\n            if self.rng.uniform() < 0.5:\r\n                ww, hh = hh, ww\r\n            if hh <= h and ww <= w:\r\n                x1 = 0 if w == ww else self.rng.randint(0, w - ww)\r\n                y1 = 0 if h == hh else self.rng.randint(0, h - hh)\r\n                out = img[y1:y1 + hh, x1:x1 + ww]\r\n                out = cv2.resize(out, (self.target_shape, self.target_shape), interpolation=cv2.INTER_CUBIC)\r\n                return out\r\n        out = imgaug.ResizeShortestEdge(self.target_shape, interp=cv2.INTER_CUBIC).augment(img)\r\n        out = imgaug.CenterCrop(self.target_shape).augment(out)\r\n        return out\r\n\r\ndef fbresnet_augmentor(isTrain):\r\n    """"""\r\n    Augmentor used in fb.resnet.torch, for BGR images in range [0,255].\r\n    """"""\r\n    if isTrain:\r\n        augmentors = [\r\n            GoogleNetResize(),\r\n            # It\'s OK to remove the following augs if your CPU is not fast enough.\r\n            # Removing brightness/contrast/saturation does not have a significant effect on accuracy.\r\n            # Removing lighting leads to a tiny drop in accuracy.\r\n            imgaug.RandomOrderAug(\r\n                [imgaug.BrightnessScale((0.6, 1.4), clip=False),\r\n                 imgaug.Contrast((0.6, 1.4), clip=False),\r\n                 imgaug.Saturation(0.4, rgb=False),\r\n                 # rgb-bgr conversion for the constants copied from fb.resnet.torch\r\n                 imgaug.Lighting(0.1,\r\n                                 eigval=np.asarray(\r\n                                     [0.2175, 0.0188, 0.0045][::-1]) * 255.0,\r\n                                 eigvec=np.array(\r\n                                     [[-0.5675, 0.7192, 0.4009],\r\n                                      [-0.5808, -0.0045, -0.8140],\r\n                                      [-0.5836, -0.6948, 0.4203]],\r\n                                     dtype=\'float32\')[::-1, ::-1]\r\n                                 )]),\r\n            imgaug.Flip(horiz=True),\r\n        ]\r\n    else:\r\n        augmentors = [\r\n            imgaug.ResizeShortestEdge(256, cv2.INTER_CUBIC),\r\n            imgaug.CenterCrop((224, 224)),\r\n        ]\r\n    return augmentors\r\n\r\n\r\ndef get_imagenet_dataflow(\r\n        datadir, name, batch_size,\r\n        augmentors, meta_dir=None, parallel=None):\r\n    """"""\r\n    See explanations in the tutorial:\r\n    http://tensorpack.readthedocs.io/en/latest/tutorial/efficient-dataflow.html\r\n    """"""\r\n    assert name in [\'train\', \'val\', \'test\']\r\n    assert datadir is not None\r\n    assert isinstance(augmentors, list)\r\n    isTrain = name == \'train\'\r\n    \r\n    #parallel = 1\r\n    \r\n    if parallel is None:\r\n        parallel = min(40, multiprocessing.cpu_count() // 2)  # assuming hyperthreading\r\n    if isTrain:\r\n        ds = dataset.ILSVRC12(datadir, name, meta_dir=meta_dir, shuffle=True)\r\n        ds = AugmentImageComponent(ds, augmentors, copy=False)\r\n        if parallel < 16:\r\n            logger.warn(""DataFlow may become the bottleneck when too few processes are used."")\r\n        ds = PrefetchDataZMQ(ds, parallel)\r\n        ds = BatchData(ds, batch_size, remainder=False)\r\n    else:\r\n        ds = dataset.ILSVRC12Files(datadir, name, meta_dir= meta_dir, shuffle=False)\r\n        aug = imgaug.AugmentorList(augmentors)\r\n\r\n        def mapf(dp):\r\n            fname, cls = dp\r\n            im = cv2.imread(fname, cv2.IMREAD_COLOR)\r\n            im = aug.augment(im)\r\n            return im, cls\r\n        ds = MultiThreadMapData(ds, parallel, mapf, buffer_size=2000, strict=True)\r\n        ds = BatchData(ds, batch_size, remainder=True)\r\n        ds = PrefetchDataZMQ(ds, 1)\r\n    return ds\r\n\r\n\r\ndef eval_on_ILSVRC12(model, sessinit, dataflow):\r\n    pred_config = PredictConfig(\r\n        model=model,\r\n        session_init=sessinit,\r\n        input_names=[\'input\', \'label\'],\r\n        output_names=[\'wrong-top1\', \'wrong-top5\']\r\n    )\r\n    pred = SimpleDatasetPredictor(pred_config, dataflow)\r\n    acc1, acc5 = RatioCounter(), RatioCounter()\r\n    for top1, top5 in pred.get_result():\r\n        batch_size = top1.shape[0]\r\n        acc1.feed(top1.sum(), batch_size)\r\n        acc5.feed(top5.sum(), batch_size)\r\n    print(""Top1 Error: {}"".format(acc1.ratio))\r\n    print(""Top5 Error: {}"".format(acc5.ratio))\r\n\r\n\r\nclass ImageNetModel(ModelDesc):\r\n    image_shape = 224\r\n    lr = 0.1\r\n\r\n    """"""\r\n    uint8 instead of float32 is used as input type to reduce copy overhead.\r\n    It might hurt the performance a liiiitle bit.\r\n    The pretrained models were trained with float32.\r\n    """"""\r\n    image_dtype = tf.float32\r\n\r\n    """"""\r\n    Either \'NCHW\' or \'NHWC\'\r\n    """"""\r\n    data_format = \'NCHW\'\r\n\r\n    """"""\r\n    Whether the image is BGR or RGB. If using DataFlow, then it should be BGR.\r\n    """"""\r\n    image_bgr = True\r\n\r\n    weight_decay = 4e-5\r\n    label_smoothing = 0.0\r\n\r\n    """"""\r\n    To apply on normalization parameters, use \'.*/gamma|.*/beta\'\r\n    to apply on depthwise, add \'.*/DW\'\r\n    """"""\r\n    weight_decay_pattern = \'.*/W|.*/M|.*/WB|.*/weights\'\r\n\r\n    """"""\r\n    Scale the loss, for whatever reasons (e.g., gradient averaging, fp16 training, etc)\r\n    """"""\r\n    loss_scale = 1.\r\n\r\n    def inputs(self):        \r\n        labels = tf.placeholder(tf.int32, [None], \'label\')\r\n        return [tf.placeholder(self.image_dtype, [None, self.image_shape, self.image_shape, 3], \'input\'),\r\n               labels]\r\n\r\n    def build_graph(self, image, label):\r\n        image = ImageNetModel.image_preprocess(image, bgr=self.image_bgr)\r\n        assert self.data_format in [\'NCHW\', \'NHWC\']\r\n        if self.data_format == \'NCHW\':\r\n            image = tf.transpose(image, [0, 3, 1, 2])\r\n\r\n        logits = self.get_logits(image)\r\n        print(\'self.label_smoothing\', self.label_smoothing)\r\n        loss = ImageNetModel.compute_loss_and_error(logits, label, self.label_smoothing)\r\n\r\n        if self.weight_decay > 0:\r\n            wd_loss = regularize_cost(self.weight_decay_pattern,\r\n                                      tf.contrib.layers.l2_regularizer(self.weight_decay),\r\n                                      name=\'l2_regularize_loss\')\r\n            add_moving_summary(loss, wd_loss)\r\n            total_cost = tf.add_n([loss, wd_loss], name=\'cost\')\r\n        else:\r\n            total_cost = tf.identity(loss, name=\'cost\')\r\n            add_moving_summary(total_cost)\r\n\r\n        if self.loss_scale != 1.:\r\n            logger.info(""Scaling the total loss by {} ..."".format(self.loss_scale))\r\n            return total_cost * self.loss_scale\r\n        else:\r\n            return total_cost\r\n\r\n    @abstractmethod\r\n    def get_logits(self, image):\r\n        """"""\r\n        Args:\r\n            image: 4D tensor of ``self.input_shape`` in ``self.data_format``\r\n\r\n        Returns:\r\n            Nx#class logits\r\n        """"""\r\n\r\n    def optimizer(self):\r\n        lr = tf.get_variable(\'learning_rate\', initializer=self.lr, trainable=False)\r\n        tf.summary.scalar(\'learning_rate-summary\', lr)\r\n        return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\r\n        #return tf.train.RMSPropOptimizer(lr, momentum=0.9) #tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\r\n\r\n    @staticmethod\r\n    def image_preprocess(image, bgr=True):\r\n        with tf.name_scope(\'image_preprocess\'):\r\n            if image.dtype.base_dtype != tf.float32:\r\n                image = tf.cast(image, tf.float32)\r\n            image = image * (1.0 / 255)\r\n\r\n            mean = [0.485, 0.456, 0.406]    # rgb\r\n            std = [0.229, 0.224, 0.225]\r\n            if bgr:\r\n                mean = mean[::-1]\r\n                std = std[::-1]\r\n            image_mean = tf.constant(mean, dtype=tf.float32)\r\n            image_std = tf.constant(std, dtype=tf.float32)\r\n            image = (image - image_mean) / image_std\r\n            return image\r\n\r\n    @staticmethod\r\n    def compute_loss_and_error(logits, label, label_smoothing):\r\n        loss = sparse_softmax_cross_entropy(\r\n                logits=logits, labels=label,\r\n                label_smoothing = label_smoothing,\r\n                weights=1.0)\r\n        loss = tf.reduce_mean(loss, name=\'xentropy-loss\')\r\n\r\n        def prediction_incorrect(logits, label, topk=1, name=\'incorrect_vector\'):\r\n            with tf.name_scope(\'prediction_incorrect\'):\r\n                x = tf.logical_not(tf.nn.in_top_k(logits, label, topk))\r\n            return tf.cast(x, tf.float32, name=name)\r\n        \r\n        if label.shape.ndims > 1:\r\n            label = tf.cast(tf.argmax(label, axis=1), tf.int32)\r\n        wrong = prediction_incorrect(logits, label, 1, name=\'wrong-top1\')\r\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top1\'))\r\n\r\n        wrong = prediction_incorrect(logits, label, 5, name=\'wrong-top5\')\r\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train-error-top5\'))\r\n        return loss\r\n\r\ndef sparse_softmax_cross_entropy(\r\n        labels,\r\n        logits,\r\n        weights=1.0,\r\n        label_smoothing=0.0,\r\n        scope=None,\r\n        loss_collection=ops.GraphKeys.LOSSES,\r\n        reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS):\r\n    """"""Cross-entropy loss using `tf.nn.sparse_softmax_cross_entropy_with_logits`.\r\n    `weights` acts as a coefficient for the loss. If a scalar is provided,\r\n    then the loss is simply scaled by the given value. If `weights` is a\r\n    tensor of shape [`batch_size`], then the loss weights apply to each\r\n    corresponding sample.\r\n    Args:\r\n        labels: `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of\r\n        `labels` and result) and dtype `int32` or `int64`. Each entry in `labels`\r\n        must be an index in `[0, num_classes)`. Other values will raise an\r\n        exception when this op is run on CPU, and return `NaN` for corresponding\r\n        loss and gradient rows on GPU.\r\n        logits: Unscaled log probabilities of shape\r\n        `[d_0, d_1, ..., d_{r-1}, num_classes]` and dtype `float32` or `float64`.\r\n        weights: Coefficients for the loss. This must be scalar or broadcastable to\r\n        `labels` (i.e. same rank and each dimension is either 1 or the same).\r\n        scope: the scope for the operations performed in computing the loss.\r\n        loss_collection: collection to which the loss will be added.\r\n        reduction: Type of reduction to apply to loss.\r\n    Returns:\r\n        Weighted loss `Tensor` of the same type as `logits`. If `reduction` is\r\n        `NONE`, this has the same shape as `labels`; otherwise, it is scalar.\r\n    Raises:\r\n        ValueError: If the shapes of `logits`, `labels`, and `weights` are\r\n        incompatible, or if any of them are None.\r\n    """"""\r\n    if labels is None:\r\n        raise ValueError(""labels must not be None."")\r\n    if logits is None:\r\n        raise ValueError(""logits must not be None."")\r\n    with tf.name_scope(scope, ""sparse_softmax_cross_entropy_loss"",\r\n                       (logits, labels, weights)) as scope:\r\n\r\n        if labels.shape.ndims == 1:\r\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n                logits=logits, labels=labels, name=""xentropy"")\r\n        else:\r\n            loss = tf.nn.softmax_cross_entropy_with_logits(\r\n                labels=labels, logits=logits, name=""xentropy"")            \r\n\r\n        loss = tf.losses.compute_weighted_loss(\r\n            loss, weights, scope, loss_collection, reduction=reduction)\r\n\r\n        # Label smoothing.\r\n        smooth_loss = 0.\r\n        if label_smoothing > 0:\r\n            # Label smoothing loss: sum of logits * weight.\r\n            loss = tf.scalar_mul(1. - label_smoothing, loss)\r\n            aux_log_softmax = -tf.nn.log_softmax(logits)\r\n\r\n            smooth_loss = tf.losses.compute_weighted_loss(\r\n                aux_log_softmax, label_smoothing * weights,\r\n                \'label_smoothing\', loss_collection, reduction=reduction)\r\n\r\n        return loss + smooth_loss\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    import argparse\r\n    from tensorpack.dataflow import TestDataSpeed\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'--data\', required=True)\r\n    parser.add_argument(\'--batch\', type=int, default=32)\r\n    parser.add_argument(\'--aug\', choices=[\'train\', \'val\'], default=\'val\')\r\n    args = parser.parse_args()\r\n\r\n    if args.aug == \'val\':\r\n        augs = fbresnet_augmentor(False)\r\n    elif args.aug == \'train\':\r\n        augs = fbresnet_augmentor(True)\r\n    df = get_imagenet_dataflow(\r\n        args.data, \'train\', args.batch, augs)\r\n    # For val augmentor, Should get >100 it/s (i.e. 3k im/s) here on a decent E5 server.\r\n    TestDataSpeed(df).start()\r\n'"
tensorflow/main.py,5,"b'# 2020.02.26-Changed for main script for testing GhostNet on ImageNet\r\n#            Huawei Technologies Co., Ltd. <foss@huawei.com>\r\n# modified from https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/imagenet-resnet.py\r\nimport argparse\r\nimport numpy as np\r\nimport math\r\nimport os\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\r\nimport cv2\r\nfrom time import time\r\nimport zipfile\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorpack import *\r\nfrom tensorpack.dataflow import imgaug\r\nfrom tensorpack.tfutils import argscope, get_model_loader, model_utils\r\nfrom tensorpack.tfutils.scope_utils import under_name_scope\r\nfrom tensorpack.utils.gpu import get_num_gpu\r\nfrom tensorpack.utils import logger\r\n\r\nfrom imagenet_utils import (\r\n    get_imagenet_dataflow,\r\n    ImageNetModel, GoogleNetResize, eval_on_ILSVRC12)\r\n\r\n\r\ndef get_data(name, batch):\r\n    isTrain = name == \'train\'\r\n    image_shape = 224\r\n\r\n    if isTrain:\r\n        augmentors = [\r\n            # use lighter augs if model is too small\r\n            GoogleNetResize(crop_area_fraction=0.49 if args.width_ratio < 1 else 0.08,\r\n                           target_shape=image_shape),\r\n            imgaug.RandomOrderAug(\r\n                [imgaug.BrightnessScale((0.6, 1.4), clip=False),\r\n                 imgaug.Contrast((0.6, 1.4), clip=False),\r\n                 imgaug.Saturation(0.4, rgb=False),\r\n                ]),\r\n            imgaug.Flip(horiz=True),\r\n        ]\r\n    else:\r\n        augmentors = [\r\n            imgaug.ResizeShortestEdge(int(image_shape*256/224), cv2.INTER_CUBIC),\r\n            imgaug.CenterCrop((image_shape, image_shape)),\r\n        ]\r\n    return get_imagenet_dataflow(args.data_dir, name, batch, augmentors, \r\n                       meta_dir = args.meta_dir)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'--eval\', action=\'store_true\')\r\n    parser.add_argument(\'--data_dir\', help=\'dataset dir.\', type=str, default=\'/cache/data/imagenet/\')\r\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\')\r\n    parser.add_argument(\'--batch\', type=int, default=1024, help=\'total batch size\')\r\n    parser.add_argument(\'--lr\', type=float, default=0.1, help=\'base learning rate\')\r\n    parser.add_argument(\'--epochs\', type=int, default=400, help=\'total epochs\')\r\n    parser.add_argument(\'--load\', help=\'path to load a model from\', default=\'./ghostnet_chechpoint\')\r\n    parser.add_argument(\'--flops\', type=int, help=\'print flops and exit\', default=0)\r\n    parser.add_argument(\'--weight_decay\', type=float, help=\'weight_decay\', default=0.00004)\r\n    parser.add_argument(\'--label_smoothing\', type=float, help=\'label_smoothing\', default=0.1)\r\n    parser.add_argument(\'--data-format\', help=\'image data format\',\r\n                        default=\'NHWC\', choices=[\'NCHW\', \'NHWC\'])\r\n    # param parser\r\n    parser.add_argument(\'--width_ratio\', help=\'width_ratio\', type=float, default=1)\r\n    parser.add_argument(\'--dropout_keep_prob\', help=\'dropout_keep_prob\', type=float, default=0.8)\r\n    parser.add_argument(\'--se\', help=\'se\', type=int, default=3)\r\n    parser.add_argument(\'--dw_code_str\', help=\'dw_code_str\', type=str, default=\'\')\r\n    parser.add_argument(\'--ratio_code_str\', help=\'ratio_code_str\', type=str, default=\'\')\r\n    args, unparsed = parser.parse_known_args()\r\n    args.meta_dir = os.path.join(args.data_dir, \'caffe_ilsvrc12\')\r\n    print(args)\r\n\r\n    if args.gpu:\r\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\r\n\r\n    if args.batch != 1024:\r\n        logger.warn(""Total batch size != 1024, you need to change other hyperparameters to get the same results."")\r\n    TOTAL_BATCH_SIZE = args.batch\r\n    \r\n    if len(args.dw_code_str) == 0:\r\n        dw_code = None\r\n    else:\r\n        dw_code = [int(s) for s in args.dw_code_str.split(\',\')]\r\n    print(\'dw_code\', dw_code)\r\n    \r\n    if len(args.ratio_code_str) == 0:\r\n        ratio_code = None\r\n    else:\r\n        ratio_code = [int(s) for s in args.ratio_code_str.split(\',\')]\r\n    print(\'ratio_code\', ratio_code)\r\n\r\n    # create GhostNet\r\n    from ghostnet import GhostNet\r\n    model = GhostNet(width=args.width_ratio, se=args.se, \r\n                      weight_decay=args.weight_decay,\r\n                      dw_code=dw_code, ratio_code=ratio_code,\r\n                      label_smoothing=args.label_smoothing)\r\n    model.data_format = args.data_format\r\n    print(\'model created\')\r\n\r\n    # start evaluation\r\n    if args.eval:\r\n        batch = 256    # something that can run on your gpu\r\n        ds = get_data(\'val\', batch)\r\n        start = time()\r\n        eval_on_ILSVRC12(model, get_model_loader(args.load), ds)\r\n        stop = time()\r\n        print(\'Evaluation used time: %.2fs.\' % (stop-start))\r\n    elif args.flops > 0:\r\n        # manually build the graph with batch=1\r\n        image_shape = 224\r\n        input_desc = [\r\n            InputDesc(tf.float32, [1, image_shape, image_shape, 3], \'input\'),\r\n            InputDesc(tf.int32, [1], \'label\')\r\n        ]\r\n        input = PlaceholderInput()\r\n        input.setup(input_desc)\r\n        with TowerContext(\'\', is_training=False):\r\n            model.build_graph(*input.get_input_tensors())\r\n        model_utils.describe_trainable_vars()\r\n\r\n        tf.profiler.profile(\r\n            tf.get_default_graph(),\r\n            cmd=\'op\',\r\n            options=tf.profiler.ProfileOptionBuilder.float_operation())\r\n        logger.info(""Note that TensorFlow counts flops in a different way from the paper."")\r\n        logger.info(""TensorFlow counts multiply+add as two flops, however the paper counts them ""\r\n                    ""as 1 flop because it can be executed in one instruction."")\r\n    else:\r\n        print(\'nothing done\')\r\n        '"
tensorflow/myconv2d.py,16,"b'\'\'\'\r\nCopyright (C) 2020. Huawei Technologies Co., Ltd. All rights reserved.\r\nThis program is free software; you can redistribute it and/or modify\r\nit under the terms of Apache License, Version 2.0 License.\r\nThis program is distributed in the hope that it will be useful,\r\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\r\nApache License, Version 2.0 License for more details.\r\n\'\'\'\r\n# implementation of Ghost module\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\r\nfrom tensorpack.models.common import layer_register, VariableHolder\r\nfrom tensorpack.tfutils.common import get_tf_version_tuple\r\nfrom tensorpack.tfutils.tower import get_current_tower_context\r\nfrom tensorpack.utils.argtools import shape2d, shape4d, get_data_format\r\nfrom tensorpack.models.tflayer import rename_get_variable, convert_to_tflayer_args\r\nfrom tensorpack.models import BatchNorm, BNReLU, Conv2D\r\nimport numpy as np\r\nimport math\r\nimport utils\r\n__all__ = [\'GhostModule\', \'SELayer\', \'MyConv2D\', \'BNNoReLU\']\r\n\r\nkernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0)\r\n\r\n@layer_register(log_shape=True)\r\ndef MyDepthConv(x, kernel_shape, channel_mult=1, padding=\'SAME\', stride=1, rate=1, data_format=\'NHWC\',\r\n              W_init=None, activation=tf.identity):\r\n    in_shape = x.get_shape().as_list()\r\n    if data_format==\'NHWC\':\r\n        in_channel = in_shape[3]\r\n        stride_shape = [1, stride, stride, 1]\r\n    elif data_format==\'NCHW\':\r\n        in_channel = in_shape[1]\r\n        stride_shape = [1, 1, stride, stride]\r\n    out_channel = in_channel * channel_mult\r\n\r\n    if W_init is None:\r\n        W_init = kernel_initializer\r\n    kernel_shape = shape2d(kernel_shape) #[kernel_shape, kernel_shape]\r\n    filter_shape = kernel_shape + [in_channel, channel_mult]\r\n\r\n    W = tf.get_variable(\'DW\', filter_shape, initializer=W_init)\r\n    conv = tf.nn.depthwise_conv2d(x, W, stride_shape, padding=padding, rate=[rate,rate], data_format=data_format)\r\n    if activation is None:\r\n        return conv\r\n    else:\r\n        return activation(conv, name=\'output\')\r\n\r\n    \r\ndef GhostModule(name, x, filters, kernel_size, dw_size, ratio, padding=\'SAME\', strides=1, data_format=\'NHWC\', use_bias=False,\r\n                activation=tf.identity):\r\n    with tf.variable_scope(name):\r\n        init_channels = math.ceil(filters / ratio)\r\n        x = Conv2D(\'conv1\', x, init_channels, kernel_size, strides=strides, activation=activation, data_format=data_format,\r\n                   kernel_initializer=kernel_initializer, use_bias=use_bias)\r\n        if ratio == 1:\r\n            return x #activation(x, name=\'output\')\r\n        dw1 = MyDepthConv(\'dw1\', x, [dw_size,dw_size], channel_mult=ratio-1, stride=1, data_format=data_format, activation=activation)\r\n        dw1 = dw1[:,:,:,:filters-init_channels] if data_format==\'NHWC\' else dw1[:,:filters-init_channels,:,:]\r\n        x = tf.concat([x, dw1], 3 if data_format==\'NHWC\' else 1)\r\n        return x\r\n\r\n\r\n@layer_register(log_shape=True)\r\ndef SELayer(x, out_dim, ratio):\r\n    squeeze = utils.spatial_mean(x, keep_dims=True, scope=\'global_pool\')\r\n    excitation = Conv2D(\'fc1\', squeeze, int(out_dim / ratio), 1, strides=1, kernel_initializer=kernel_initializer, \r\n                             data_format=\'NHWC\', activation=None)\r\n    excitation = tf.nn.relu(excitation, name=\'relu\')\r\n    excitation = Conv2D(\'fc2\', excitation, out_dim, 1, strides=1, kernel_initializer=kernel_initializer, \r\n                             data_format=\'NHWC\', activation=None)\r\n    excitation = tf.clip_by_value(excitation, 0, 1, name=\'hsigmoid\')\r\n    scale = x * excitation\r\n    return scale\r\n\r\n\r\n@layer_register(log_shape=True)\r\n@convert_to_tflayer_args(\r\n    args_names=[\'filters\', \'kernel_size\'],\r\n    name_mapping={\r\n        \'out_channel\': \'filters\',\r\n        \'kernel_shape\': \'kernel_size\',\r\n        \'stride\': \'strides\',\r\n    })\r\ndef MyConv2D(\r\n        inputs,\r\n        filters,\r\n        kernel_size,\r\n        strides=(1, 1),\r\n        padding=\'same\',\r\n        data_format=\'channels_last\',\r\n        dilation_rate=(1, 1),\r\n        activation=None,\r\n        use_bias=True,\r\n        kernel_initializer=kernel_initializer,#tf.contrib.layers.variance_scaling_initializer(2.0),\r\n        bias_initializer=tf.zeros_initializer(),\r\n        kernel_regularizer=None,\r\n        bias_regularizer=None,\r\n        activity_regularizer=None,\r\n        ):\r\n    """"""\r\n    A wrapper around `tf.layers.Conv2D`.\r\n    Some differences to maintain backward-compatibility:\r\n\r\n    1. Default kernel initializer is variance_scaling_initializer(2.0).\r\n    2. Default padding is \'same\'.\r\n\r\n    Variable Names:\r\n\r\n    * ``W``: weights\r\n    * ``b``: bias\r\n    """"""\r\n    with rename_get_variable({\'kernel\': \'W\', \'bias\': \'b\'}):\r\n        layer = tf.layers.Conv2D(\r\n            filters,\r\n            kernel_size,\r\n            strides=strides,\r\n            padding=padding,\r\n            data_format=data_format,\r\n            dilation_rate=dilation_rate,\r\n            activation=activation,\r\n            use_bias=use_bias,\r\n            kernel_initializer=kernel_initializer,\r\n            bias_initializer=bias_initializer,\r\n            kernel_regularizer=kernel_regularizer,\r\n            bias_regularizer=bias_regularizer,\r\n            activity_regularizer=activity_regularizer,\r\n            _reuse=tf.get_variable_scope().reuse)\r\n        ret = layer.apply(inputs, scope=tf.get_variable_scope())\r\n        ret = tf.identity(ret, name=\'output\')\r\n\r\n    ret.variables = VariableHolder(W=layer.kernel)\r\n    if use_bias:\r\n        ret.variables.b = layer.bias\r\n    return ret\r\n\r\n\r\n@layer_register(use_scope=None)\r\ndef BNNoReLU(x, name=None):\r\n    """"""\r\n    A shorthand of BatchNormalization.\r\n    """"""\r\n    if name is None:\r\n        x = BatchNorm(\'bn\', x)\r\n    else:\r\n        x = BatchNorm(name, x)\r\n    return x\r\n'"
tensorflow/utils.py,33,"b'# ==============================================================================\r\n# Copyright 2018 Paul Balanca. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n""""""Misc. collection of useful layers, mostly very simple!\r\n""""""\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\r\nfrom tensorpack.callbacks.param import HyperParamSetter\r\n\r\nslim = tf.contrib.slim\r\n\r\n# =========================================================================== #\r\n# Tools...\r\n# =========================================================================== #\r\n\r\ndef _get_dimension(shape, dim, min_rank=1):\r\n    """"""Returns the `dim` dimension of `shape`, while checking it has `min_rank`.\r\n    Args:\r\n        shape: A `TensorShape`.\r\n        dim: Integer, which dimension to return.\r\n        min_rank: Integer, minimum rank of shape.\r\n    Returns:\r\n        The value of the `dim` dimension.\r\n    Raises:\r\n        ValueError: if inputs don\'t have at least min_rank dimensions, or if the\r\n            first dimension value is not defined.\r\n    """"""\r\n    dims = shape.dims\r\n    if dims is None:\r\n        raise ValueError(\'dims of shape must be known but is None\')\r\n    if len(dims) < min_rank:\r\n        raise ValueError(\'rank of shape must be at least %d not: %d\' % (min_rank,\r\n                                                                        len(dims)))\r\n    value = dims[dim].value\r\n    if value is None:\r\n        raise ValueError(\r\n            \'dimension %d of shape must be known but is None: %s\' % (dim, shape))\r\n    return value\r\n\r\n\r\n# =========================================================================== #\r\n# Extension of TensorFlow common layers.\r\n# =========================================================================== #\r\n@add_arg_scope\r\ndef channel_dimension(shape, data_format=\'NHWC\', min_rank=1):\r\n    """"""Returns the channel dimension of shape, while checking it has min_rank.\r\n    Args:\r\n        shape: A `TensorShape`.\r\n        data_format: `NCHW` or `NHWC`.\r\n        min_rank: Integer, minimum rank of shape.\r\n    Returns:\r\n         value of the first dimension.\r\n    Raises:\r\n        ValueError: if inputs don\'t have at least min_rank dimensions, or if the\r\n            first dimension value is not defined.\r\n    """"""\r\n    return _get_dimension(shape, 1 if data_format == \'NCHW\' else -1,\r\n                          min_rank=min_rank)\r\n\r\n@add_arg_scope\r\ndef channel_to_last(inputs, data_format=\'NHWC\', scope=None):\r\n    """"""Move the channel axis to the last dimension. Allows to\r\n    provide a consistent NHWC output format whatever the input data format.\r\n\r\n    Args:\r\n      inputs: Input Tensor;\r\n      data_format: NHWC or NCHW.\r\n    """"""\r\n    with tf.name_scope(scope, \'channel_to_last\', [inputs]):\r\n        if data_format == \'NHWC\':\r\n            net = inputs\r\n        elif data_format == \'NCHW\':\r\n            net = tf.transpose(inputs, perm=(0, 2, 3, 1))\r\n        return net\r\n\r\n@add_arg_scope\r\ndef to_nhwc(inputs, data_format=\'NHWC\', scope=None):\r\n    """"""Move the channel axis to the last dimension. Allows to\r\n    provide a consistent NHWC output format whatever the input data format.\r\n\r\n    Args:\r\n      inputs: Input Tensor;\r\n      data_format: NHWC or NCHW.\r\n    """"""\r\n    with tf.name_scope(scope, \'to_nhwc\', [inputs]):\r\n        if data_format == \'NHWC\':\r\n            net = inputs\r\n        elif data_format == \'NCHW\':\r\n            net = tf.transpose(inputs, perm=(0, 2, 3, 1))\r\n        return net\r\n\r\n@add_arg_scope\r\ndef to_nchw(inputs, data_format=\'NHWC\', scope=None):\r\n    """"""Move the channel axis to the last dimension. Allows to\r\n    provide a consistent NHWC output format whatever the input data format.\r\n\r\n    Args:\r\n      inputs: Input Tensor;\r\n      data_format: NHWC or NCHW.\r\n    """"""\r\n    with tf.name_scope(scope, \'to_nchw\', [inputs]):\r\n        if data_format == \'NHWC\':\r\n            net = tf.transpose(inputs, perm=(0, 3, 1, 2))\r\n        elif data_format == \'NCHW\':\r\n            net = inputs\r\n        return net\r\n\r\n@add_arg_scope\r\ndef channel_to_hw(inputs, factors=[1, 1], data_format=\'NHWC\', scope=None):\r\n    """"""Move the channel axis to the last dimension. Allows to\r\n    provide a consistent NHWC output format whatever the input data format.\r\n\r\n    Args:\r\n      inputs: Input Tensor;\r\n      data_format: NHWC or NCHW.\r\n    """"""\r\n    with tf.name_scope(scope, \'channel_to_hw\', [inputs]):\r\n        net = inputs\r\n        if factors[0] == 1 and factors[1] == 1:\r\n            return net\r\n\r\n        if data_format == \'NCHW\':\r\n            net = tf.transpose(net, perm=(0, 2, 3, 1))\r\n        # Inputs in NHWC format.\r\n        shape = net.get_shape().as_list()\r\n        shape[1] = int(shape[1] / factors[0])\r\n        shape[2] = int(shape[2] / factors[1])\r\n        shape[3] = -1\r\n        net = tf.reshape(net, shape)\r\n        # Original format.\r\n        if data_format == \'NCHW\':\r\n            net = tf.transpose(net, perm=(0, 3, 1, 2))\r\n        return net\r\n\r\n@add_arg_scope\r\ndef concat_channels(l_inputs, data_format=\'NHWC\', scope=None):\r\n    """"""Concat a list of tensors on the channel axis.\r\n\r\n    Args:\r\n      inputs: List Tensors;\r\n      data_format: NHWC or NCHW.\r\n    """"""\r\n    with tf.name_scope(scope, \'concat_channels\', l_inputs):\r\n        if data_format == \'NHWC\':\r\n            net = tf.concat(l_inputs, axis=3)\r\n        elif data_format == \'NCHW\':\r\n            net = tf.concat(l_inputs, axis=1)\r\n        return net\r\n\r\n@add_arg_scope\r\ndef split_channels(inputs, nsplits, data_format=\'NHWC\', scope=None):\r\n    """"""Split a tensor on the channel axis.\r\n\r\n    Args:\r\n      inputs: List Tensors;\r\n      data_format: NHWC or NCHW.\r\n    """"""\r\n    with tf.name_scope(scope, \'split_channels\', [inputs]):\r\n        if data_format == \'NHWC\':\r\n            nets = tf.split(inputs, nsplits, axis=3)\r\n        elif data_format == \'NCHW\':\r\n            nets = tf.split(inputs, nsplits, axis=1)\r\n        return nets\r\n\r\n@add_arg_scope\r\ndef pad2d(inputs,\r\n          pad=(0, 0),\r\n          mode=\'CONSTANT\',\r\n          data_format=\'NHWC\',\r\n          scope=None):\r\n    """"""2D Padding layer, adding a symmetric padding to H and W dimensions.\r\n\r\n    Aims to mimic padding in Caffe and MXNet, helping the port of models to\r\n    TensorFlow. Tries to follow the naming convention of `tf.contrib.layers`.\r\n\r\n    Args:\r\n      inputs: 4D input Tensor;\r\n      pad: 2-Tuple with padding values for H and W dimensions;\r\n      mode: Padding mode. C.f. `tf.pad`\r\n      data_format:  NHWC or NCHW data format.\r\n    """"""\r\n    with tf.name_scope(scope, \'pad2d\', [inputs]):\r\n        # Padding shape.\r\n        if data_format == \'NHWC\':\r\n            paddings = [[0, 0], [pad[0], pad[0]], [pad[1], pad[1]], [0, 0]]\r\n        elif data_format == \'NCHW\':\r\n            paddings = [[0, 0], [0, 0], [pad[0], pad[0]], [pad[1], pad[1]]]\r\n        net = tf.pad(inputs, paddings, mode=mode)\r\n        return net\r\n\r\n@add_arg_scope\r\ndef pad_logits(logits, pad=(0, 0)):\r\n    """"""Pad logits Tensor, to deal with different number of classes.\r\n    """"""\r\n    shape = logits.get_shape().as_list()\r\n    dtype = logits.dtype\r\n    l = [logits]\r\n    if pad[0] > 0:\r\n        a = tf.constant(dtype.min, dtype, (shape[0], pad[0]))\r\n        l = [a] + l\r\n    if pad[1] > 0:\r\n        a = tf.constant(dtype.min, dtype, (shape[0], pad[1]))\r\n        l = l + [a]\r\n    output = tf.concat(l, axis=1)\r\n    return output\r\n\r\n@add_arg_scope\r\ndef spatial_mean(inputs, scaling=None, keep_dims=False,\r\n                 data_format=\'NHWC\', scope=None):\r\n    """"""Average tensor along spatial dimensions.\r\n\r\n    Args:\r\n      inputs: Input tensor;\r\n      keep_dims: Keep spatial dimensions?\r\n      data_format: NHWC or NCHW.\r\n    """"""\r\n    with tf.name_scope(scope, \'spatial_mean\', [inputs]):\r\n        axes = [1, 2] if data_format == \'NHWC\' else [2, 3]\r\n        net = tf.reduce_mean(inputs, axes, keep_dims=keep_dims)\r\n        return net\r\n\r\n@add_arg_scope\r\ndef spatial_squeeze(inputs, data_format=\'NHWC\', scope=None):\r\n    """"""Squeeze spatial dimensions, if possible.\r\n\r\n    Args:\r\n      inputs: Input tensor;\r\n      data_format: NHWC or NCHW.\r\n    """"""\r\n    with tf.name_scope(scope, \'spatial_squeeze\', [inputs]):\r\n        axes = [1, 2] if data_format == \'NHWC\' else [2, 3]\r\n        net = tf.squeeze(inputs, axes)\r\n        return net\r\n\r\n@add_arg_scope\r\ndef ksize_for_squeezing(inputs, default_ksize=1024, data_format=\'NHWC\'):\r\n    """"""Get the correct kernel size for squeezing an input tensor.\r\n    """"""\r\n    shape = inputs.get_shape().as_list()\r\n    kshape = shape[1:3] if data_format == \'NHWC\' else shape[2:]\r\n    if kshape[0] is None or kshape[1] is None:\r\n        kernel_size_out = [default_ksize, default_ksize]\r\n    else:\r\n        kernel_size_out = [min(kshape[0], default_ksize),\r\n                           min(kshape[1], default_ksize)]\r\n    return kernel_size_out\r\n\r\n@add_arg_scope\r\ndef batch_norm(inputs,\r\n               activation_fn=None,\r\n               normalizer_fn=None,\r\n               normalizer_params=None):\r\n    """"""Batch normalization layer compatible with the classic conv. API.\r\n    Simpler to use with arg. scopes.\r\n    """"""\r\n    outputs = inputs\r\n    # BN...\r\n    if normalizer_fn is not None:\r\n        normalizer_params = normalizer_params or {}\r\n        outputs = normalizer_fn(outputs, **normalizer_params)\r\n    if activation_fn is not None:\r\n        outputs = activation_fn(outputs)\r\n    return outputs\r\n\r\n@add_arg_scope\r\ndef drop_path(inputs, keep_prob, is_training=True, scope=None):\r\n    """"""Drops out a whole example hiddenstate with the specified probability.\r\n    """"""\r\n    with tf.name_scope(scope, \'drop_path\', [inputs]):\r\n        net = inputs\r\n        if is_training:\r\n            batch_size = tf.shape(net)[0]\r\n            noise_shape = [batch_size, 1, 1, 1]\r\n            random_tensor = keep_prob\r\n            random_tensor += tf.random_uniform(noise_shape, dtype=tf.float32)\r\n            binary_tensor = tf.floor(random_tensor)\r\n            net = tf.div(net, keep_prob) * binary_tensor\r\n        return net\r\n\r\n# =========================================================================== #\r\n# Useful methods\r\n# =========================================================================== #\r\ndef data_format_scope(data_format):\r\n    """"""Create the default scope for a given data format.\r\n    Tries to combine all existing layers in one place!\r\n    """"""\r\n    with slim.arg_scope([slim.conv2d,\r\n                         slim.separable_conv2d,\r\n                         slim.max_pool2d,\r\n                         slim.avg_pool2d,\r\n                         slim.batch_norm,\r\n                         concat_channels,\r\n                         split_channels,\r\n                         channel_to_last,\r\n                         to_nchw,\r\n                         to_nhwc,\r\n                         channel_to_hw,\r\n                         spatial_squeeze,\r\n                         spatial_mean,\r\n                         ksize_for_squeezing,\r\n                         channel_dimension],\r\n                        data_format=data_format) as sc:\r\n        return sc\r\n\r\nclass HyperParamSetterWithCosine(HyperParamSetter):\r\n    """""" Set the parameter by a function of epoch num. """"""\r\n    def __init__(self, param, base_lr, start_step, n_step, step_based=True):\r\n        """"""\r\n        Cosine learning rate\r\n        """"""\r\n        super(HyperParamSetterWithCosine, self).__init__(param)\r\n        self._base_lr = base_lr\r\n        self._start_step = start_step\r\n        self._n_step = n_step\r\n        self._step = step_based\r\n\r\n    def _get_value_to_set(self):\r\n        refnum = self.global_step if self._step else self.epoch_num\r\n        if self._start_step > refnum:\r\n            return None\r\n        return 0.5*self._base_lr*(1+np.cos(np.pi*(refnum-self._start_step)/self._n_step))\r\n\r\n    def _trigger_epoch(self):\r\n        if not self._step:\r\n            self.trigger()\r\n\r\n    def _trigger_step(self):\r\n        if self._step:\r\n            self.trigger()'"
