file_path,api_count,code
setup.py,0,"b'# Lint as: python3\n""""""HuggingFace/NLP is an open library of NLP datasets.\n\nSimple check list for release from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py\n\nTo create the package for pypi.\n\n1. Change the version in __init__.py, setup.py as well as docs/source/conf.py.\n\n2. Commit these changes with the message: ""Release: VERSION""\n\n3. Add a tag in git to mark the release: ""git tag VERSION -m\'Adds tag VERSION for pypi\' ""\n   Push the tag to git: git push --tags origin master\n\n4. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n\n   For the wheel, run: ""python setup.py bdist_wheel"" in the top level directory.\n   (this will build a wheel for the python version you use to build it).\n\n   For the sources, run: ""python setup.py sdist""\n   You should now have a /dist directory with both .whl and .tar.gz source versions.\n\n5. Check that everything looks correct by uploading the package to the pypi test server:\n\n   twine upload dist/* -r pypitest\n   (pypi suggest using twine as other methods upload files via plaintext.)\n   You may have to specify the repository url, use the following command then:\n   twine upload dist/* -r pypitest --repository-url=https://test.pypi.org/legacy/\n\n   Check that you can install it in a virtualenv by running:\n   pip install -i https://testpypi.python.org/pypi nlp\n\n6. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n\n7. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.\n\n8. Update the documentation commit in .circleci/deploy.sh for the accurate documentation to be displayed\n\n9. Update README.md to redirect to correct documentation.\n""""""\n\nimport datetime\nimport itertools\nimport os\nimport sys\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nDOCLINES = __doc__.split(\'\\n\')\n\nREQUIRED_PKGS = [\n    \'numpy\',\n    # Backend and serialization\n    \'pyarrow>=0.16.0\',\n    # For smart caching dataset processing\n    \'dill\',\n    # for downloading datasets over HTTPS\n    \'requests>=2.19.0\',\n    # progress bars in download and scripts\n    ""tqdm >= 4.27"",\n    # dataclasses for Python versions that don\'t have it\n    ""dataclasses;python_version<\'3.7\'"",\n    # filesystem locks e.g. to prevent parallel downloads\n    ""filelock"",\n]\n\nTESTS_REQUIRE = [\n    \'apache-beam\',\n    \'absl-py\',\n    \'bs4\',\n    \'langdetect\',\n    \'mwparserfromhell\',\n    \'nltk\',\n    \'pytest\',\n    \'pytest-xdist\',\n    \'tensorflow\',\n    \'tldextract\',\n    \'zstandard\'\n]\n\n\nQUALITY_REQUIRE = [\n    ""black"",\n    ""isort @ git+git://github.com/timothycrosley/isort.git@e63ae06ec7d70b06df9e528357650281a3d3ec22#egg=isort"",\n    ""flake8==3.7.9"",\n]\n\n\nEXTRAS_REQUIRE = {\n    \'apache-beam\': [\'apache-beam\'],\n    \'tensorflow\': [\'tensorflow>=2.2.0\'],\n    \'tensorflow_gpu\': [\'tensorflow-gpu>=2.2.0\'],\n    \'torch\': [\'torch\'],\n    \'tests\': TESTS_REQUIRE,\n    \'quality\': QUALITY_REQUIRE,\n}\n\nsetup(\n    name=\'nlp\',\n    version=""0.2.0"",\n    description=DOCLINES[0],\n    long_description=\'\\n\'.join(DOCLINES[2:]),\n    author=\'HuggingFace Inc.\',\n    author_email=\'thomas@huggingface.co\',\n    url=\'https://github.com/huggingface/nlp\',\n    download_url=\'https://github.com/huggingface/nlp/tags\',\n    license=\'Apache 2.0\',\n    package_dir={"""": ""src""},\n    packages=find_packages(""src""),\n    package_data={\n        \'nlp\': [\n            \'scripts/templates/*\',\n        ],\n    },\n    scripts=[""nlp-cli""],\n    install_requires=REQUIRED_PKGS,\n    extras_require=EXTRAS_REQUIRE,\n    classifiers=[\n        ""Development Status :: 5 - Production/Stable"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Education"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: OS Independent"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n    keywords=\'nlp machine learning datasets metrics\',\n)\n'"
tests/__init__.py,0,b''
tests/test_arrow_dataset.py,0,"b'import os\nimport tempfile\nfrom unittest import TestCase\n\nimport numpy as np\nimport pyarrow as pa\n\nfrom nlp.arrow_reader import BaseReader\nfrom nlp.info import DatasetInfo\nfrom nlp.splits import SplitDict, SplitInfo\n\n\nclass ReaderTester(BaseReader):\n    """"""\n    Build a Dataset object out of Instruction instance(s).\n    This reader is made for testing. It mocks file reads.\n    """"""\n\n    def _get_dataset_from_filename(self, filename_skip_take):\n        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n        filename, skip, take = (\n            filename_skip_take[""filename""],\n            filename_skip_take[""skip""] if ""skip"" in filename_skip_take else None,\n            filename_skip_take[""take""] if ""take"" in filename_skip_take else None,\n        )\n        pa_table = pa.Table.from_pydict({""filename"": [filename + ""_"" + str(x) for x in np.arange(100).tolist()]})\n        if skip is not None and take is not None:\n            pa_table = pa_table.slice(skip, take)\n        return pa_table\n\n\nclass BaseDatasetTest(TestCase):\n    def _create_dummy_dataset(self):\n        name = ""my_name""\n        train_info = SplitInfo(name=""train"", num_examples=30)\n        test_info = SplitInfo(name=""test"", num_examples=30)\n        split_infos = [train_info, test_info]\n        split_dict = SplitDict()\n        split_dict.add(train_info)\n        split_dict.add(test_info)\n        info = DatasetInfo(splits=split_dict)\n        reader = ReaderTester("""", info)\n        dset = reader.read(name, ""train"", split_infos)\n        return dset\n\n    def test_map(self):\n        dset = self._create_dummy_dataset()\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, ""test.arrow"")\n            dset_test = dset.map(\n                lambda x: {""name"": x[""filename""][:-2], ""id"": int(x[""filename""][-1])}, cache_file_name=tmp_file\n            )\n            self.assertEqual(len(dset_test), 30)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, ""test.arrow"")\n            dset_test = dset.map(\n                lambda x: {""name"": x[""filename""][:-2], ""id"": int(x[""filename""][-1])}, cache_file_name=tmp_file\n            )\n            dset_test_with_indices = dset.map(\n                lambda x, i: {""name"": x[""filename""][:-2], ""id"": i}, with_indices=True, cache_file_name=tmp_file\n            )\n            self.assertEqual(len(dset_test_with_indices), 30)\n\n    def test_map_batched(self):\n        dset = self._create_dummy_dataset()\n\n        def map_batched(example):\n            return {""filename_new"": [x + ""_extension"" for x in example[""filename""]]}\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, ""test.arrow"")\n            dset_test_batched = dset.map(map_batched, batched=True, cache_file_name=tmp_file)\n            self.assertEqual(len(dset_test_batched), 30)\n\n        def map_batched_with_indices(example, idx):\n            return {""filename_new"": [x + ""_extension_"" + str(idx) for x in example[""filename""]]}\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, ""test.arrow"")\n            dset_test_with_indices_batched = dset.map(\n                map_batched_with_indices, batched=True, with_indices=True, cache_file_name=tmp_file\n            )\n            self.assertEqual(len(dset_test_with_indices_batched), 30)\n\n    def test_remove_colums(self):\n        dset = self._create_dummy_dataset()\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, ""test.arrow"")\n            dset = dset.map(\n                lambda x, i: {""name"": x[""filename""][:-2], ""id"": i}, with_indices=True, cache_file_name=tmp_file\n            )\n            self.assertTrue(""id"" in dset[0])\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, ""test.arrow"")\n            dset = dset.map(lambda x: x, remove_columns=[""id""], cache_file_name=tmp_file)\n            self.assertTrue(""id"" not in dset[0])\n\n    def test_filter(self):\n        dset = self._create_dummy_dataset()\n        # keep only first five examples\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, ""test.arrow"")\n            dset_filter_first_five = dset.filter(lambda x, i: i < 5, with_indices=True, cache_file_name=tmp_file)\n            self.assertEqual(len(dset_filter_first_five), 5)\n\n        # filter filenames with even id at the end\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, ""test.arrow"")\n            dset_filter_even_num = dset.filter(lambda x: (int(x[""filename""][-1]) % 2 == 0), cache_file_name=tmp_file)\n            self.assertEqual(len(dset_filter_even_num), 15)\n'"
tests/test_arrow_reader.py,0,"b'from unittest import TestCase\n\nimport pyarrow as pa\n\nfrom nlp.arrow_reader import BaseReader\nfrom nlp.info import DatasetInfo\nfrom nlp.splits import SplitDict, SplitInfo\n\n\nclass ReaderTest(BaseReader):\n    """"""\n    Build a Dataset object out of Instruction instance(s).\n    This reader is made for testing. It mocks file reads.\n    """"""\n\n    def _get_dataset_from_filename(self, filename_skip_take):\n        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n        filename, skip, take = (\n            filename_skip_take[""filename""],\n            filename_skip_take[""skip""] if ""skip"" in filename_skip_take else None,\n            filename_skip_take[""take""] if ""take"" in filename_skip_take else None,\n        )\n        pa_table = pa.Table.from_pydict({""filename"": [filename] * 100})\n        if skip is not None and take is not None:\n            pa_table = pa_table.slice(skip, take)\n        return pa_table\n\n\nclass BaseReaderTest(TestCase):\n    def test_read(self):\n        name = ""my_name""\n        train_info = SplitInfo(name=""train"", num_examples=100)\n        test_info = SplitInfo(name=""test"", num_examples=100)\n        split_infos = [train_info, test_info]\n        split_dict = SplitDict()\n        split_dict.add(train_info)\n        split_dict.add(test_info)\n        info = DatasetInfo(splits=split_dict)\n        reader = ReaderTest("""", info)\n\n        instructions = ""test[:33%]""\n        dset = reader.read(name, instructions, split_infos)\n        self.assertEqual(dset[""filename""][0], f""{name}-test"")\n        self.assertEqual(dset.num_rows, 33)\n        self.assertEqual(dset.num_columns, 1)\n\n        instructions = [""train"", ""test[:33%]""]\n        train_dset, test_dset = reader.read(name, instructions, split_infos)\n        self.assertEqual(train_dset[""filename""][0], f""{name}-train"")\n        self.assertEqual(train_dset.num_rows, 100)\n        self.assertEqual(train_dset.num_columns, 1)\n        self.assertEqual(test_dset[""filename""][0], f""{name}-test"")\n        self.assertEqual(test_dset.num_rows, 33)\n        self.assertEqual(test_dset.num_columns, 1)\n\n    def test_read_files(self):\n        train_info = SplitInfo(name=""train"", num_examples=100)\n        test_info = SplitInfo(name=""test"", num_examples=100)\n        split_dict = SplitDict()\n        split_dict.add(train_info)\n        split_dict.add(test_info)\n        info = DatasetInfo(splits=split_dict)\n        reader = ReaderTest("""", info)\n\n        files = [{""filename"": ""train""}, {""filename"": ""test"", ""skip"": 10, ""take"": 10}]\n        dset = reader.read_files(files)\n        self.assertEqual(dset.num_rows, 110)\n        self.assertEqual(dset.num_columns, 1)\n        self.assertEqual(dset._data_files, files)\n'"
tests/test_beam.py,0,"b'import os\nimport tempfile\nfrom unittest import TestCase\n\nimport apache_beam as beam\n\nimport nlp\n\n\nclass DummyBeamDataset(nlp.BeamBasedBuilder):\n    """"""Dummy beam dataset.""""""\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            features=nlp.Features({""content"": nlp.Value(""string"")}),\n            # No default supervised_keys.\n            supervised_keys=None,\n        )\n\n    def _split_generators(self, dl_manager, pipeline):\n        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""examples"": get_test_examples()})]\n\n    def _build_pcollection(self, pipeline, examples):\n        return pipeline | ""Load Examples"" >> beam.Create(examples)\n\n\ndef get_test_examples():\n    return [(i, {""content"": content}) for i, content in enumerate([""foo"", ""bar"", ""foobar""])]\n\n\nclass BeamBuilderTest(TestCase):\n    def test_download_and_prepare(self):\n        expected_num_examples = len(get_test_examples())\n        with tempfile.TemporaryDirectory() as tmp_cache_dir:\n            builder = DummyBeamDataset(cache_dir=tmp_cache_dir, beam_runner=""DirectRunner"")\n            builder.download_and_prepare()\n            dset = builder.as_dataset()\n            self.assertEqual(dset[""train""].num_rows, expected_num_examples)\n            self.assertEqual(dset[""train""].info.splits[""train""].num_examples, expected_num_examples)\n            self.assertTrue(\n                os.path.exists(\n                    os.path.join(tmp_cache_dir, ""dummy_beam_dataset"", ""default"", ""0.0.0"", ""dataset_info.json"")\n                )\n            )\n\n    def test_no_beam_options(self):\n        with tempfile.TemporaryDirectory() as tmp_cache_dir:\n            builder = DummyBeamDataset(cache_dir=tmp_cache_dir)\n            self.assertRaises(nlp.builder.MissingBeamOptions, builder.download_and_prepare)\n'"
tests/test_dataset_common.py,0,"b'# coding=utf-8\n# Copyright 2020 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport glob\nimport logging\nimport os\nimport tempfile\n\nimport requests\nfrom absl.testing import parameterized\n\nfrom nlp import (\n    BeamBasedBuilder,\n    BuilderConfig,\n    DatasetBuilder,\n    DownloadConfig,\n    GenerateMode,\n    MockDownloadManager,\n    hf_api,\n    hf_bucket_url,\n    import_main_class,\n    load_dataset,\n    prepare_module,\n)\n\nfrom .utils import aws, local, slow\n\n\nlogging.basicConfig(level=logging.INFO)\n\n\nclass DatasetTester(object):\n    def __init__(self, parent):\n        self.parent = parent\n\n    def load_builder_class(self, dataset_name, is_local=False):\n        # Download/copy dataset script\n        if is_local is True:\n            module_path = prepare_module(""./datasets/"" + dataset_name)\n        else:\n            module_path = prepare_module(dataset_name, download_config=DownloadConfig(force_download=True))\n        # Get dataset builder class\n        builder_cls = import_main_class(module_path)\n        # Instantiate dataset builder\n        return builder_cls\n\n    def load_all_configs(self, dataset_name, is_local=False):\n        # get builder class\n        builder_cls = self.load_builder_class(dataset_name, is_local=is_local)\n        builder = builder_cls\n\n        if len(builder.BUILDER_CONFIGS) == 0:\n            return [None]\n        return builder.BUILDER_CONFIGS\n\n    def check_load_dataset(self, dataset_name, configs, is_local=False):\n        # test only first config to speed up testing\n        for config in configs:\n            with tempfile.TemporaryDirectory() as processed_temp_dir, tempfile.TemporaryDirectory() as raw_temp_dir:\n\n                # create config and dataset\n                dataset_builder_cls = self.load_builder_class(dataset_name, is_local=is_local)\n                name = config.name if config is not None else None\n                dataset_builder = dataset_builder_cls(name=name, cache_dir=processed_temp_dir)\n\n                # TODO: skip Beam datasets for now\n                if isinstance(dataset_builder, BeamBasedBuilder):\n                    logging.info(""Skip tests for Beam datasets for now"")\n                    return\n\n                if config is not None:\n                    version = config.version\n                else:\n                    version = dataset_builder.VERSION\n\n                # create mock data loader manager that has a special download_and_extract() method to download dummy data instead of real data\n                mock_dl_manager = MockDownloadManager(\n                    dataset_name=dataset_name,\n                    config=config,\n                    version=version,\n                    cache_dir=raw_temp_dir,\n                    is_local=is_local,\n                )\n\n                if dataset_builder.__class__.__name__ == ""Csv"":\n                    # need slight adoption for csv dataset\n                    mock_dl_manager.download_dummy_data()\n                    path_to_dummy_data = mock_dl_manager.dummy_file\n                    dataset_builder.config.data_files = {\n                        ""train"": os.path.join(path_to_dummy_data, ""train.csv""),\n                        ""test"": os.path.join(path_to_dummy_data, ""test.csv""),\n                        ""dev"": os.path.join(path_to_dummy_data, ""dev.csv""),\n                    }\n                elif dataset_builder.__class__.__name__ == ""Json"":\n                    # need slight adoption for json dataset\n                    mock_dl_manager.download_dummy_data()\n                    path_to_dummy_data = mock_dl_manager.dummy_file\n                    dataset_builder.config.data_files = {\n                        ""train"": os.path.join(path_to_dummy_data, ""train.json""),\n                        ""test"": os.path.join(path_to_dummy_data, ""test.json""),\n                        ""dev"": os.path.join(path_to_dummy_data, ""dev.json""),\n                    }\n\n                # generate examples from dummy data\n                dataset_builder.download_and_prepare(\n                    dl_manager=mock_dl_manager, download_mode=GenerateMode.FORCE_REDOWNLOAD, ignore_verifications=True\n                )\n\n                # get dataset\n                dataset = dataset_builder.as_dataset()\n\n                # check that dataset is not empty\n                for split in dataset_builder.info.splits.keys():\n                    # check that loaded datset is not empty\n                    self.parent.assertTrue(len(dataset[split]) > 0)\n\n\ndef get_local_dataset_names():\n    datasets = [dataset_dir.split(""/"")[-2] for dataset_dir in glob.glob(""./datasets/*/"")]\n    return [{""testcase_name"": x, ""dataset_name"": x} for x in datasets]\n\n\n@parameterized.named_parameters(get_local_dataset_names())\n@local\nclass LocalDatasetTest(parameterized.TestCase):\n    dataset_name = None\n\n    def setUp(self):\n        self.dataset_tester = DatasetTester(self)\n\n    def test_load_dataset(self, dataset_name):\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\n        self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True)\n\n    def test_builder_class(self, dataset_name):\n        builder_cls = self.dataset_tester.load_builder_class(dataset_name, is_local=True)\n        name = builder_cls.BUILDER_CONFIGS[0].name if builder_cls.BUILDER_CONFIGS else None\n        with tempfile.TemporaryDirectory() as tmp_cache_dir:\n            builder = builder_cls(name=name, cache_dir=tmp_cache_dir)\n            self.assertTrue(isinstance(builder, DatasetBuilder))\n\n    def test_builder_configs(self, dataset_name):\n        builder_configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)\n        self.assertTrue(len(builder_configs) > 0)\n\n        if builder_configs[0] is not None:\n            all(self.assertTrue(isinstance(config, BuilderConfig)) for config in builder_configs)\n\n    @slow\n    def test_load_dataset_all_configs(self, dataset_name):\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)\n        self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True)\n\n    @slow\n    def test_load_real_dataset(self, dataset_name):\n        with tempfile.TemporaryDirectory() as temp_data_dir:\n            download_config = DownloadConfig()\n            download_config.download_mode = GenerateMode.FORCE_REDOWNLOAD\n\n            dataset = load_dataset(\n                ""./datasets/"" + dataset_name, data_dir=temp_data_dir, download_config=download_config\n            )\n            for split in dataset.keys():\n                self.assertTrue(len(dataset[split]) > 0)\n\n\ndef get_aws_dataset_names():\n    api = hf_api.HfApi()\n    # fetch all dataset names\n    datasets = [x.id for x in api.dataset_list()]\n    return [{""testcase_name"": x, ""dataset_name"": x} for x in datasets]\n\n\n@parameterized.named_parameters(get_aws_dataset_names())\n@aws\nclass AWSDatasetTest(parameterized.TestCase):\n    dataset_name = None\n\n    def setUp(self):\n        self.dataset_tester = DatasetTester(self)\n\n    def test_dataset_has_valid_etag(self, dataset_name):\n        py_script_path = list(filter(lambda x: x, dataset_name.split(""/"")))[-1] + "".py""\n        dataset_url = hf_bucket_url(dataset_name, filename=py_script_path, dataset=True)\n        etag = None\n        try:\n            response = requests.head(dataset_url, allow_redirects=True, proxies=None, timeout=10)\n\n            if response.status_code == 200:\n                etag = response.headers.get(""Etag"")\n        except (EnvironmentError, requests.exceptions.Timeout):\n            pass\n\n        self.assertIsNotNone(etag)\n\n    def test_builder_class(self, dataset_name):\n        builder_cls = self.dataset_tester.load_builder_class(dataset_name)\n        name = builder_cls.BUILDER_CONFIGS[0].name if builder_cls.BUILDER_CONFIGS else None\n        with tempfile.TemporaryDirectory() as tmp_cache_dir:\n            builder = builder_cls(name=name, cache_dir=tmp_cache_dir)\n            self.assertTrue(isinstance(builder, DatasetBuilder))\n\n    def test_builder_configs(self, dataset_name):\n        builder_configs = self.dataset_tester.load_all_configs(dataset_name)\n        self.assertTrue(len(builder_configs) > 0)\n\n        if builder_configs[0] is not None:\n            all(self.assertTrue(isinstance(config, BuilderConfig)) for config in builder_configs)\n\n    def test_load_dataset(self, dataset_name):\n        configs = self.dataset_tester.load_all_configs(dataset_name)[:1]\n        self.dataset_tester.check_load_dataset(dataset_name, configs)\n\n    @slow\n    def test_load_dataset_all_configs(self, dataset_name):\n        configs = self.dataset_tester.load_all_configs(dataset_name)\n        self.dataset_tester.check_load_dataset(dataset_name, configs)\n\n    @slow\n    def test_load_real_dataset(self, dataset_name):\n        with tempfile.TemporaryDirectory() as temp_data_dir:\n            download_config = DownloadConfig()\n            download_config.download_mode = GenerateMode.FORCE_REDOWNLOAD\n\n            dataset = load_dataset(dataset_name, data_dir=temp_data_dir, download_config=download_config)\n            for split in dataset.keys():\n                self.assertTrue(len(dataset[split]) > 0)\n'"
tests/utils.py,0,"b'import os\nimport unittest\nfrom distutils.util import strtobool\n\n\ndef parse_flag_from_env(key, default=False):\n    try:\n        value = os.environ[key]\n    except KeyError:\n        # KEY isn\'t set, default to `default`.\n        _value = default\n    else:\n        # KEY is set, convert it to True or False.\n        try:\n            _value = strtobool(value)\n        except ValueError:\n            # More values are supported, but let\'s keep the message simple.\n            raise ValueError(""If set, {} must be yes or no."".format(key))\n    return _value\n\n\n_run_slow_tests = parse_flag_from_env(""RUN_SLOW"", default=False)\n_run_aws_tests = parse_flag_from_env(""RUN_AWS"", default=True)\n_run_local_tests = parse_flag_from_env(""RUN_LOCAL"", default=True)\n\n\ndef slow(test_case):\n    """"""\n    Decorator marking a test as slow.\n\n    Slow tests are skipped by default. Set the RUN_SLOW environment variable\n    to a truthy value to run them.\n\n    """"""\n    if not _run_slow_tests or _run_slow_tests == 0:\n        test_case = unittest.skip(""test is slow"")(test_case)\n    return test_case\n\n\ndef local(test_case):\n    """"""\n    Decorator marking a test as local\n\n    Local tests are run by default. Set the RUN_LOCAL environment variable\n    to a falsy value to not run them.\n    """"""\n    if not _run_local_tests or _run_local_tests == 0:\n        test_case = unittest.skip(""test is local"")(test_case)\n    return test_case\n\n\ndef aws(test_case):\n    """"""\n    Decorator marking a test as one that relies on AWS.\n\n    AWS tests are skipped by default. Set the RUN_AWS environment variable\n    to a falsy value to not run them.\n    """"""\n    if not _run_aws_tests or _run_aws_tests == 0:\n        test_case = unittest.skip(""test requires aws"")(test_case)\n    return test_case\n'"
datasets/aeslc/aeslc.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Annotated Enron Subject Line Corpus Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport glob\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@misc{zhang2019email,\n    title={This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation},\n    author={Rui Zhang and Joel Tetreault},\n    year={2019},\n    eprint={1906.03497},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n""""""\n\n_DESCRIPTION = """"""\nA collection of email messages of employees in the Enron Corporation.\n\nThere are two features:\n  - email_body: email body text.\n  - subject_line: email subject text.\n""""""\n\n_URL = ""https://github.com/ryanzhumich/AESLC/archive/master.zip""\n\n_DOCUMENT = ""email_body""\n_SUMMARY = ""subject_line""\n\n\nclass Aeslc(nlp.GeneratorBasedBuilder):\n    """"""Annotated Enron Subject Line Corpus Dataset.""""""\n\n    VERSION = nlp.Version(""1.0.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({_DOCUMENT: nlp.Value(""string""), _SUMMARY: nlp.Value(""string"")}),\n            supervised_keys=(_DOCUMENT, _SUMMARY),\n            homepage=""https://github.com/ryanzhumich/AESLC"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_path = dl_manager.download_and_extract(_URL)\n        input_path = os.path.join(dl_path, ""AESLC-master"", ""enron_subject_line"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""pattern"": os.path.join(input_path, ""train"", ""*.subject"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""pattern"": os.path.join(input_path, ""dev"", ""*.subject"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST, gen_kwargs={""pattern"": os.path.join(input_path, ""test"", ""*.subject"")},\n            ),\n        ]\n\n    def _generate_examples(self, pattern=None):\n        """"""Yields examples.""""""\n        for filename in sorted(glob.glob(pattern)):\n            email_body, subject_line = _parse_email_file(filename)\n            key = os.path.basename(filename).rstrip("".subject"")\n            yield key, {_DOCUMENT: email_body, _SUMMARY: subject_line}\n\n\ndef _parse_email_file(filename):\n    """"""Parse email file text for email body and subject.""""""\n    with open(filename) as f:\n        email_body = """"\n        for line in f:\n            if line == ""\\n"":\n                break\n            email_body += line\n        line = next(f)\n        subject = """"\n        for line in f:\n            if line == ""\\n"":\n                break\n            subject += line\n    return email_body, subject\n'"
datasets/ai2_arc/ai2_arc.py,0,"b'""""""TODO(arc): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(ai2_arc): BibTeX citation\n_CITATION = """"""\\\n@article{allenai:arc,\n      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and\n                    Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},\n      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},\n      journal   = {arXiv:1803.05457v1},\n      year      = {2018},\n}\n""""""\n\n# TODO(ai2_arc):\n_DESCRIPTION = """"""\\\nA new dataset of 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in\n advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, where the former contains\n only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. We are also\n including a corpus of over 14 million science sentences relevant to the task, and an implementation of three neural baseline models for this dataset. We pose ARC as a challenge to the community.\n""""""\n\n_URL = ""https://s3-us-west-2.amazonaws.com/ai2-website/data/ARC-V1-Feb2018.zip""\n\n\nclass Ai2ArcConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Ai2ARC.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for Ai2Arc.\n\n        Args:\n          **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(Ai2ArcConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass Ai2Arc(nlp.GeneratorBasedBuilder):\n    """"""TODO(arc): Short description of my dataset.""""""\n\n    # TODO(arc): Set up version.\n    VERSION = nlp.Version(""1.0.0"")\n    BUILDER_CONFIGS = [\n        Ai2ArcConfig(\n            name=""ARC-Challenge"",\n            description=""""""\\\n          Challenge Set of 2590 \xe2\x80\x9chard\xe2\x80\x9d questions (those that both a retrieval and a co-occurrence method fail to answer correctly)\n          """""",\n        ),\n        Ai2ArcConfig(\n            name=""ARC-Easy"",\n            description=""""""\\\n          Easy Set of 5197 questions\n          """""",\n        ),\n    ]\n\n    def _info(self):\n        # TODO(ai2_arc): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""choices"": nlp.features.Sequence({""text"": nlp.Value(""string""), ""label"": nlp.Value(""string"")}),\n                    ""answerKey"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://allenai.org/data/arc"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(ai2_arc): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""ARC-V1-Feb2018-2"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, self.config.name, self.config.name + ""-Train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, self.config.name, self.config.name + ""-Test.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, self.config.name, self.config.name + ""-Dev.jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(ai2_arc): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for row in f:\n                data = json.loads(row)\n                answerkey = data[""answerKey""]\n                id_ = data[""id""]\n                question = data[""question""][""stem""]\n                choices = data[""question""][""choices""]\n                text_choices = [choice[""text""] for choice in choices]\n                label_choices = [choice[""label""] for choice in choices]\n                yield id_, {\n                    ""id"": id_,\n                    ""answerKey"": answerkey,\n                    ""question"": question,\n                    ""choices"": {""text"": text_choices, ""label"": label_choices},\n                }\n'"
datasets/arcd/arcd.py,0,"b'""""""ARCD: Arabic Reading Comprehension Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport logging\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@inproceedings{mozannar-etal-2019-neural,\n    title = ""Neural {A}rabic Question Answering"",\n    author = ""Mozannar, Hussein  and\n      Maamary, Elie  and\n      El Hajal, Karl  and\n      Hajj, Hazem"",\n    booktitle = ""Proceedings of the Fourth Arabic Natural Language Processing Workshop"",\n    month = aug,\n    year = ""2019"",\n    address = ""Florence, Italy"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/W19-4612"",\n    doi = ""10.18653/v1/W19-4612"",\n    pages = ""108--118"",\n    abstract = ""This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score."",\n}\n""""""\n\n_DESCRIPTION = """"""\\\n Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions\\\n      posed by crowdworkers on Wikipedia articles.\n""""""\n\n\nclass ArcdConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for ARCD.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for ARCD.\n\n    Args:\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(ArcdConfig, self).__init__(**kwargs)\n\n\nclass Arcd(nlp.GeneratorBasedBuilder):\n    """"""ARCD: Arabic Reading Comprehension Dataset.""""""\n\n    _URL = ""https://raw.githubusercontent.com/husseinmozannar/SOQAL/master/data/""\n    _DEV_FILE = ""arcd-test.json""\n    _TRAINING_FILE = ""arcd-train.json""\n\n    BUILDER_CONFIGS = [\n        ArcdConfig(\n            name=""plain_text"",\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            description=""Plain text"",\n        )\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""title"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32"")}\n                    ),\n                }\n            ),\n            # No default supervised_keys (as we have to pass both question\n            # and context as input).\n            supervised_keys=None,\n            homepage=""https://github.com/husseinmozannar/SOQAL/tree/master/data"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        urls_to_download = {\n            ""train"": os.path.join(self._URL, self._TRAINING_FILE),\n            ""dev"": os.path.join(self._URL, self._DEV_FILE),\n        }\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""This function returns the examples in the raw (text) form.""""""\n        logging.info(""generating examples from = %s"", filepath)\n        with open(filepath) as f:\n            arcd = json.load(f)\n            for article in arcd[""data""]:\n                title = article.get(""title"", """").strip()\n                for paragraph in article[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        # Features currently used are ""context"", ""question"", and ""answers"".\n                        # Others are extracted here for the ease of future expansions.\n                        yield id_, {\n                            ""title"": title,\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers},\n                        }\n'"
datasets/art/art.py,0,"b'""""""TODO(art): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(art): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{anli,\n  author = ""Chandra, Bhagavatula\n    and Ronan, Le Bras\n    and Chaitanya, Malaviya\n    and Keisuke, Sakaguchi\n    and Ari, Holtzman\n    and Hannah, Rashkin\n    and Doug, Downey\n    and Scott, Wen-tau Yih\n    and Yejin, Choi"",\n  title = ""Abductive Commonsense Reasoning"",\n  year = ""2020"",\n}""""""\n\n# TODO(art):\n_DESCRIPTION = """"""\\\nthe Abductive Natural Language Inference Dataset from AI2\n""""""\n_DATA_URL = ""https://storage.googleapis.com/ai2-mosaic/public/alphanli/alphanli-train-dev.zip""\n\n\nclass ArtConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Art.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for Art.\n        Args:\n          **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(ArtConfig, self).__init__(\n            version=nlp.Version(""0.1.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass Art(nlp.GeneratorBasedBuilder):\n    """"""TODO(art): Short description of my dataset.""""""\n\n    # TODO(art): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        ArtConfig(\n            name=""anli"",\n            description=""""""\\\n          the Abductive Natural Language Inference Dataset from AI2.\n          """""",\n        ),\n    ]\n\n    def _info(self):\n        # TODO(art): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""observation_1"": nlp.Value(""string""),\n                    ""observation_2"": nlp.Value(""string""),\n                    ""hypothesis_1"": nlp.Value(""string""),\n                    ""hypothesis_2"": nlp.Value(""string""),\n                    ""label"": nlp.features.ClassLabel(num_classes=3)\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://leaderboard.allenai.org/anli/submissions/get-started"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(art): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_DATA_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={\n                    ""filepath"": os.path.join(dl_dir, ""dev.jsonl""),\n                    ""labelpath"": os.path.join(dl_dir, ""dev-labels.lst""),\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={\n                    ""filepath"": os.path.join(dl_dir, ""train.jsonl""),\n                    ""labelpath"": os.path.join(dl_dir, ""train-labels.lst""),\n                },\n            ),\n        ]\n\n    def _generate_examples(self, filepath, labelpath):\n        """"""Yields examples.""""""\n        # TODO(art): Yields (key, example) tuples from the dataset\n        data = []\n        for line in open(filepath):\n            data.append(json.loads(line))\n        labels = []\n        with open(labelpath) as f:\n            for word in f:\n                labels.append(word)\n        for idx, row in enumerate(data):\n            yield idx, {\n                ""observation_1"": row[""obs1""],\n                ""observation_2"": row[""obs2""],\n                ""hypothesis_1"": row[""hyp1""],\n                ""hypothesis_2"": row[""hyp2""],\n                ""label"": labels[idx],\n            }\n'"
datasets/billsum/billsum.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""BillSum Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@misc{kornilova2019billsum,\n    title={BillSum: A Corpus for Automatic Summarization of US Legislation},\n    author={Anastassia Kornilova and Vlad Eidelman},\n    year={2019},\n    eprint={1910.00523},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n""""""\n\n_DESCRIPTION = """"""\nBillSum, summarization of US Congressional and California state bills.\n\nThere are several features:\n  - text: bill text.\n  - summary: summary of the bills.\n  - title: title of the bills.\nfeatures for us bills. ca bills does not have.\n  - text_len: number of chars in text.\n  - sum_len: number of chars in summary.\n""""""\n\n_URL = ""https://drive.google.com/uc?export=download&id=1g89WgFHMRbr4QrvA0ngh26PY081Nv3lx""\n\n_DOCUMENT = ""text""\n_SUMMARY = ""summary""\n\n\nclass Billsum(nlp.GeneratorBasedBuilder):\n    """"""BillSum Dataset.""""""\n\n    # 2.0.0 data source updated to filter near duplicates.\n    # 3.0.0  none of the test examples are \'near duplicates\' of an example in the\n    #   train set AND they dont have the same title, regardless of similarity.\n    VERSION = nlp.Version(""3.0.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {_DOCUMENT: nlp.Value(""string""), _SUMMARY: nlp.Value(""string""), ""title"": nlp.Value(""string""),}\n            ),\n            supervised_keys=(_DOCUMENT, _SUMMARY),\n            homepage=""https://github.com/FiscalNote/BillSum"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_path = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={""path"": os.path.join(dl_path, ""us_train_data_final_OFFICIAL.jsonl""), ""key"": ""bill_id""},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={""path"": os.path.join(dl_path, ""us_test_data_final_OFFICIAL.jsonl""), ""key"": ""bill_id""},\n            ),\n            nlp.SplitGenerator(\n                name=""ca_test"",\n                gen_kwargs={""path"": os.path.join(dl_path, ""ca_test_data_final_OFFICIAL.jsonl""), ""key"": ""external_id""},\n            ),\n        ]\n\n    def _generate_examples(self, path=None, key=None):\n        """"""Yields examples.""""""\n        with open(path) as f:\n            for line in f:\n                # in us bills, json has fields:\n                #   text, summary, title, bill_id, text_len, sum_len\n                # in ca bills, json has fields:\n                #   text, summary, title, external_id\n                d = json.loads(line)\n                yield d[key], {k: d[k] for k in [_DOCUMENT, _SUMMARY, ""title""]}\n'"
datasets/blended_skill_talk/blended_skill_talk.py,0,"b'""""""TODO(blended_skill_talk): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(blended_skill_talk): BibTeX citation\n_CITATION = """"""\\\n@misc{smith2020evaluating,\n    title={Can You Put it All Together: Evaluating Conversational Agents\' Ability to Blend Skills},\n    author={Eric Michael Smith and Mary Williamson and Kurt Shuster and Jason Weston and Y-Lan Boureau},\n    year={2020},\n    eprint={2004.08449},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n""""""\n\n# TODO(blended_skill_talk):\n_DESCRIPTION = """"""\\\nA dataset of 7k conversations explicitly designed to exhibit multiple conversation modes: displaying personality, having empathy, and demonstrating knowledge.\n""""""\n_URL = ""http://parl.ai/downloads/blended_skill_talk/blended_skill_talk.tar.gz""\n\n_TASK = [""convai2"", ""empathetic_dialogues"", ""wizard_of_wikipedia""]\n\n\nclass BlendedSkillTalk(nlp.GeneratorBasedBuilder):\n    """"""TODO(blended_skill_talk): Short description of my dataset.""""""\n\n    # TODO(blended_skill_talk): Set up version.\n    VERSION = nlp.Version(""1.0.0"")\n\n    def _info(self):\n        # TODO(blended_skill_talk): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""personas"": nlp.features.Sequence({""persona"": nlp.Value(""string""),}),\n                    ""additional_context"": nlp.Value(""string""),\n                    ""previous_utterance"": nlp.features.Sequence({""previous_utterance"": nlp.Value(""string""),}),\n                    ""context"": nlp.Value(""string""),\n                    ""free_messages"": nlp.features.Sequence({""free_message"": nlp.Value(""string""),}),\n                    ""guided_messgaes"": nlp.features.Sequence({""guided_messgae"": nlp.Value(""string""),}),\n                    ""suggestions"": nlp.features.Sequence({task: nlp.Value(""string"") for task in _TASK})\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://parl.ai/projects/bst/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(blended_skill_talk): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        data_dir = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""train.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""valid.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""test.json"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(blended_skill_talk): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for id_, row in enumerate(data):\n                personas = [row[""personas""][1][0], row[""personas""][1][1]]\n                dialogs = [dialog[1] for dialog in row[""dialog""]]\n                free_messages = []\n                guided_messages = []\n\n                for i in range(len(dialogs) // 2):\n                    free_messages.append(dialogs[2 * i])\n                    guided_messages.append(dialogs[2 * i + 1])\n                context = row[""context_dataset""]\n                add_context = row[""additional_context""] if context == ""wizard_of_wikipedia"" else """"\n                previous_utterance = [row[""free_turker_utterance""], row[""guided_turker_utterance""]]\n                suggestions = row[""suggestions""]\n                convai_suggestions = []\n                empathetic_suggestions = []\n                wow_suggestions = []\n                for i in range(len(suggestions) // 2):\n                    convai_suggestions.append(suggestions[2 * i + 1][""convai2""])\n                    empathetic_suggestions.append(suggestions[2 * i + 1][""empathetic_dialogues""])\n                    wow_suggestions.append(suggestions[2 * i + 1][""wizard_of_wikipedia""])\n                yield id_, {\n                    ""personas"": {""persona"": personas,},\n                    ""additional_context"": add_context,\n                    ""previous_utterance"": {""previous_utterance"": previous_utterance,},\n                    ""context"": context,\n                    ""free_messages"": {""free_message"": free_messages,},\n                    ""guided_messgaes"": {""guided_messgae"": guided_messages,},\n                    ""suggestions"": {\n                        ""convai2"": convai_suggestions,\n                        ""empathetic_dialogues"": empathetic_suggestions,\n                        ""wizard_of_wikipedia"": wow_suggestions,\n                    },\n                }\n'"
datasets/blimp/blimp.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""BLiMP dataset with minimal pairs of grammatical phenomena in English.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@article{warstadt2019blimp,\n  title={BLiMP: A Benchmark of Linguistic Minimal Pairs for English},\n  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei, and Wang, Sheng-Fu and Bowman, Samuel R},\n  journal={arXiv preprint arXiv:1912.00582},\n  year={2019}\n}\n""""""\n\n_DESCRIPTION = """"""\nBLiMP is a challenge set for evaluating what language models (LMs) know about\nmajor grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each\ncontaining 1000 minimal pairs isolating specific contrasts in syntax,\nmorphology, or semantics. The data is automatically generated according to\nexpert-crafted grammars.\n""""""\n\n_PROJECT_URL = ""https://github.com/alexwarstadt/blimp/tree/master/""\n_DOWNLOAD_URL = ""https://raw.githubusercontent.com/alexwarstadt/blimp/master/""\n\n\nclass BlimpConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Blimp.""""""\n\n    def __init__(self, paradigm_uid, **kwargs):\n        """"""BuilderConfig for Blimp.\n\n    Args:\n      paradigm_uid: string, UID of the linguistic paradigm\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = paradigm_uid\n\n        description = _DESCRIPTION\n        description += (""This configuration includes the paradigm {}."").format(name)\n\n        super(BlimpConfig, self).__init__(name=name, description=description, version=nlp.Version(""0.1.0""), **kwargs)\n\n\nclass Blimp(nlp.GeneratorBasedBuilder):\n    """"""Minimal grammatical and ungrammatical pairs of 67 linguistic paradigms.""""""\n\n    all_paradigms = [\n        ""adjunct_island"",\n        ""anaphor_gender_agreement"",\n        ""anaphor_number_agreement"",\n        ""animate_subject_passive"",\n        ""animate_subject_trans"",\n        ""causative"",\n        ""complex_NP_island"",\n        ""coordinate_structure_constraint_complex_left_branch"",\n        ""coordinate_structure_constraint_object_extraction"",\n        ""determiner_noun_agreement_1"",\n        ""determiner_noun_agreement_2"",\n        ""determiner_noun_agreement_irregular_1"",\n        ""determiner_noun_agreement_irregular_2"",\n        ""determiner_noun_agreement_with_adj_2"",\n        ""determiner_noun_agreement_with_adj_irregular_1"",\n        ""determiner_noun_agreement_with_adj_irregular_2"",\n        ""determiner_noun_agreement_with_adjective_1"",\n        ""distractor_agreement_relational_noun"",\n        ""distractor_agreement_relative_clause"",\n        ""drop_argument"",\n        ""ellipsis_n_bar_1"",\n        ""ellipsis_n_bar_2"",\n        ""existential_there_object_raising"",\n        ""existential_there_quantifiers_1"",\n        ""existential_there_quantifiers_2"",\n        ""existential_there_subject_raising"",\n        ""expletive_it_object_raising"",\n        ""inchoative"",\n        ""intransitive"",\n        ""irregular_past_participle_adjectives"",\n        ""irregular_past_participle_verbs"",\n        ""irregular_plural_subject_verb_agreement_1"",\n        ""irregular_plural_subject_verb_agreement_2"",\n        ""left_branch_island_echo_question"",\n        ""left_branch_island_simple_question"",\n        ""matrix_question_npi_licensor_present"",\n        ""npi_present_1"",\n        ""npi_present_2"",\n        ""only_npi_licensor_present"",\n        ""only_npi_scope"",\n        ""passive_1"",\n        ""passive_2"",\n        ""principle_A_c_command"",\n        ""principle_A_case_1"",\n        ""principle_A_case_2"",\n        ""principle_A_domain_1"",\n        ""principle_A_domain_2"",\n        ""principle_A_domain_3"",\n        ""principle_A_reconstruction"",\n        ""regular_plural_subject_verb_agreement_1"",\n        ""regular_plural_subject_verb_agreement_2"",\n        ""sentential_negation_npi_licensor_present"",\n        ""sentential_negation_npi_scope"",\n        ""sentential_subject_island"",\n        ""superlative_quantifiers_1"",\n        ""superlative_quantifiers_2"",\n        ""tough_vs_raising_1"",\n        ""tough_vs_raising_2"",\n        ""transitive"",\n        ""wh_island"",\n        ""wh_questions_object_gap"",\n        ""wh_questions_subject_gap"",\n        ""wh_questions_subject_gap_long_distance"",\n        ""wh_vs_that_no_gap"",\n        ""wh_vs_that_no_gap_long_distance"",\n        ""wh_vs_that_with_gap"",\n        ""wh_vs_that_with_gap_long_distance"",\n    ]\n\n    BUILDER_CONFIGS = [BlimpConfig(paradigm_uid=paradigm) for paradigm in all_paradigms]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""sentence_good"": nlp.Value(""string""),\n                    ""sentence_bad"": nlp.Value(""string""),\n                    ""field"": nlp.Value(""string""),\n                    ""linguistics_term"": nlp.Value(""string""),\n                    ""UID"": nlp.Value(""string""),\n                    ""simple_LM_method"": nlp.Value(""bool""),\n                    ""one_prefix_method"": nlp.Value(""bool""),\n                    ""two_prefix_method"": nlp.Value(""bool""),\n                    ""lexically_identical"": nlp.Value(""bool""),\n                    ""pair_id"": nlp.Value(""int32""),\n                }\n            ),\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=_PROJECT_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        cfg = self.config\n        download_urls = {cfg.name: os.path.join(_DOWNLOAD_URL, ""data"", cfg.name + "".jsonl"")}\n\n        downloaded_files = dl_manager.download_and_extract(download_urls)\n\n        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[cfg.name]})]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        with open(filepath, ""rb"") as f:\n            for line in f:\n                line_dict = json.loads(line)\n                id_ = line_dict[""UID""] + ""_"" + line_dict[""pairID""]\n                feats = {\n                    ""sentence_good"": line_dict[""sentence_good""],\n                    ""sentence_bad"": line_dict[""sentence_bad""],\n                    ""field"": line_dict[""field""],\n                    ""linguistics_term"": line_dict[""linguistics_term""],\n                    ""UID"": line_dict[""UID""],\n                    ""simple_LM_method"": line_dict[""simple_LM_method""],\n                    ""one_prefix_method"": line_dict[""one_prefix_method""],\n                    ""two_prefix_method"": line_dict[""two_prefix_method""],\n                    ""lexically_identical"": line_dict[""lexically_identical""],\n                    ""pair_id"": int(line_dict[""pairID""]),\n                }\n                yield id_, feats\n'"
datasets/blog_authorship_corpus/blog_authorship_corpus.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport glob\nimport logging\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@inproceedings{schler2006effects,\n    title={Effects of age and gender on blogging.},\n    author={Schler, Jonathan and Koppel, Moshe and Argamon, Shlomo and Pennebaker, James W},\n    booktitle={AAAI spring symposium: Computational approaches to analyzing weblogs},\n    volume={6},\n    pages={199--205},\n    year={2006}\n}\n""""""\n\n_DESCRIPTION = """"""\\\nThe Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\n\nEach blog is presented as a separate file, the name of which indicates a blogger id# and the blogger\xe2\x80\x99s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n\nAll bloggers included in the corpus fall into one of three age groups:\n\n\xc2\xb7          8240 ""10s"" blogs (ages 13-17),\n\n\xc2\xb7          8086 ""20s"" blogs(ages 23-27)\n\n\xc2\xb7          2994 ""30s"" blogs (ages 33-47).\n\nFor each age group there are an equal number of male and female bloggers.\n\nEach blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.\n\nThe corpus may be freely used for non-commercial research purposes\n""""""\n_URL = ""https://u.cs.biu.ac.il/~koppel/BlogCorpus.htm""\n_DATA_URL = ""http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip""\n\n\nclass BlogAuthorshipCorpusConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for BlogAuthorship.""""""\n\n    def __init__(self, data_url, **kwargs):\n        """"""BuilderConfig for BlogAuthorship\n\n        Args:\n          data_url: `string`, url to the dataset (word or raw level)\n          **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(BlogAuthorshipCorpusConfig, self).__init__(version=nlp.Version(""1.0.0"",), **kwargs)\n        self.data_url = data_url\n\n\nclass BlogAuthorshipCorpus(nlp.GeneratorBasedBuilder):\n    """"""TODO(BlogAuthorship): Short description of my dataset.""""""\n\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        BlogAuthorshipCorpusConfig(\n            name=""blog-authorship-corpus"",\n            data_url=_DATA_URL,\n            description=""word level dataset. No processing is needed other than replacing newlines with <eos> tokens."",\n        ),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""text"": nlp.Value(""string""),\n                    ""date"": nlp.Value(""string""),\n                    ""gender"": nlp.Value(""string""),\n                    ""age"": nlp.Value(""int32""),\n                    ""horoscope"": nlp.Value(""string""),\n                    ""job"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        if self.config.name == ""blog-authorship-corpus"":\n            data = dl_manager.download_and_extract(self.config.data_url)\n            data_dir = os.path.join(data, ""blogs"")\n            files = sorted(glob.glob(os.path.join(data_dir, ""*.xml"")))\n            train_files = []\n            validation_files = []\n\n            for i, file_path in enumerate(files):\n                # 95% / 5% (train / val) split\n                if i % 20 == 0:\n                    validation_files.append(file_path)\n                else:\n                    train_files.append(file_path)\n\n            return [\n                nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""files"": train_files, ""split"": ""train""},),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION, gen_kwargs={""files"": validation_files, ""split"": ""validation""},\n                ),\n            ]\n        else:\n            raise ValueError(""{} does not exist"".format(self.config.name))\n\n    def _generate_examples(self, files, split):\n        def parse_date(line):\n            # parse line to date\n            return line.strip().split(""<date>"")[-1].split(""</date>"")[0]\n\n        for file_path in files:\n            counter = 0\n            file_name = os.path.basename(file_path)\n            logging.info(""generating examples from = %s"", file_path)\n            file_id, gender, age, job, horoscope = tuple(file_name.split(""."")[:-1])\n\n            # Note: import xml.etree.ElementTree as etree does not work. File cannot be parsed\n            # use open instead\n            with open(file_path) as f:\n                # some files are corrupted, so have to work with python`s try here\n                try:\n                    date = """"\n                    for line in f:\n                        line = line.strip()\n                        if ""<date>"" in line:\n                            date = parse_date(line)\n                        elif line != """" and not line.startswith(""<""):\n                            # need sub_id to be certain that no tf_records is identical\n                            sub_id = counter\n                            counter += 1\n                            if date == """":\n                                logging.warning(""Date missing for {} in {}"".format(line, file_name))\n                            assert date is not None, ""Date is missing before {}"".format(line)\n                            blog = {\n                                ""text"": line,\n                                ""date"": date,\n                                ""gender"": gender,\n                                ""age"": int(age),\n                                ""job"": job,\n                                ""horoscope"": horoscope,\n                            }\n                            yield ""{}_{}_{}"".format(file_id, sub_id, date), blog\n                        else:\n                            continue\n                except UnicodeDecodeError as e:\n                    logging.warning(""{} cannot be loaded. Error message: {}"".format(file_path, e))\n'"
datasets/boolq/boolq.py,1,"b'""""""TODO(boolq): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport tensorflow as tf\n\nimport nlp\n\n\n# TODO(boolq): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{clark2019boolq,\n  title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},\n  author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei, and Kwiatkowski, Tom and Collins, Michael, and Toutanova, Kristina},\n  booktitle = {NAACL},\n  year =      {2019},\n}\n""""""\n\n# TODO(boolq):\n_DESCRIPTION = """"""\\\nBoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally \noccurring ---they are generated in unprompted and unconstrained settings. \nEach example is a triplet of (question, passage, answer), with the title of the page as optional additional context. \nThe text-pair classification setup is similar to existing natural language inference tasks.\n""""""\n\n_URL = ""gs://boolq""\n_TRAIN_FILE_NAME = ""train.jsonl""\n_DEV_FILE_NAME = ""dev.jsonl""\n\n\nclass Boolq(nlp.GeneratorBasedBuilder):\n    """"""TODO(boolq): Short description of my dataset.""""""\n\n    # TODO(boolq): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(boolq): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""question"": nlp.Value(""string""),\n                    ""answer"": nlp.Value(""bool""),\n                    ""passage"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/google-research-datasets/boolean-questions"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(boolq): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {\n            ""train"": os.path.join(_URL, _TRAIN_FILE_NAME),\n            ""dev"": os.path.join(_URL, _DEV_FILE_NAME),\n        }\n        downloaded_files = dl_manager.download_custom(urls_to_download, tf.io.gfile.copy)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]},),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(boolq): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n                question = data[""question""]\n                answer = data[""answer""]\n                passage = data[""passage""]\n                yield id_, {""question"": question, ""answer"": answer, ""passage"": passage}\n'"
datasets/break_data/break_data.py,0,"b'""""""TODO(break_data): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport json\nimport os\nimport textwrap\n\nimport six\n\nimport nlp\n\n\n# TODO(break): BibTeX citation\n_CITATION = """"""\\\n@article{Wolfson2020Break,\n  title={Break It Down: A Question Understanding Benchmark},\n  author={Wolfson, Tomer and Geva, Mor and Gupta, Ankit and Gardner, Matt and Goldberg, Yoav and Deutch, Daniel and Berant, Jonathan},\n  journal={Transactions of the Association for Computational Linguistics},\n  year={2020},\n}\n""""""\n\n# TODO(break):\n_DESCRIPTION = """"""\\\nBreak is a human annotated dataset of natural language questions and their Question Decomposition Meaning Representations\n(QDMRs). Break consists of 83,978 examples sampled from 10 question answering datasets over text, images and databases. \nThis repository contains the Break dataset along with information on the exact data format.\n""""""\n_URL = ""https://github.com/allenai/Break/raw/master/break_dataset/Break-dataset.zip""\n\n\nclass BreakDataConfig(nlp.BuilderConfig):\n\n    """"""BuilderConfig for Break""""""\n\n    def __init__(self, text_features, lexicon_tokens, **kwargs):\n        """"""\n\n        Args:\n            text_features: `dict[string, string]`, map from the name of the feature\n        dict for each text field to the name of the column in the tsv file\n            lexicon_tokens: to define if we want to load the lexicon_tokens files or not\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(BreakDataConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n        self.text_features = text_features\n        self.lexicon_tokens = lexicon_tokens\n\n\nclass BreakData(nlp.GeneratorBasedBuilder):\n    """"""TODO(break_data): Short description of my dataset.""""""\n\n    # TODO(break_data): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        BreakDataConfig(\n            name=""QDMR-high-level"",\n            description=textwrap.dedent(\n                """"""\n             Contains questions annotated with the high-level variant of QDMR. These decomposition are exclusive to Reading \n             Comprehension tasks (Section 2). lexicon_tokens files are also provided.""""""\n            ),\n            text_features={\n                ""question_id"": ""question_id"",\n                ""question_text"": ""question_text"",\n                ""decomposition"": ""decomposition"",\n                ""operators"": ""operators"",\n                ""split"": ""split"",\n            },\n            lexicon_tokens=False,\n        ),\n        BreakDataConfig(\n            name=""QDMR-high-level-lexicon"",\n            description=textwrap.dedent(\n                """"""\n               Contains questions annotated with the high-level variant of QDMR. These decomposition are exclusive to Reading \n               Comprehension tasks (Section 2). lexicon_tokens files are also provided.""""""\n            ),\n            text_features={""source"": ""source"", ""allowed_tokens"": ""allowed_tokens"",},\n            lexicon_tokens=True,\n        ),\n        BreakDataConfig(\n            name=""QDMR"",\n            description=textwrap.dedent(\n                """"""\n               Contains questions over text, images and databases annotated with their Question Decomposition Meaning \n               Representation. In addition to the train, dev and (hidden) test sets we provide lexicon_tokens files. For \n               each question, the lexicon file contains the set of valid tokens that could potentially appear in its \n               decomposition """"""\n            ),\n            text_features={\n                ""question_id"": ""question_id"",\n                ""question_text"": ""question_text"",\n                ""decomposition"": ""decomposition"",\n                ""operators"": ""operators"",\n                ""split"": ""split"",\n            },\n            lexicon_tokens=False,\n        ),\n        BreakDataConfig(\n            name=""QDMR-lexicon"",\n            description=textwrap.dedent(\n                """"""\n                 Contains questions over text, images and databases annotated with their Question Decomposition Meaning \n               Representation. In addition to the train, dev and (hidden) test sets we provide lexicon_tokens files. For \n               each question, the lexicon file contains the set of valid tokens that could potentially appear in its \n               decomposition """"""\n            ),\n            text_features={""source"": ""source"", ""allowed_tokens"": ""allowed_tokens"",},\n            lexicon_tokens=True,\n        ),\n        BreakDataConfig(\n            name=""logical-forms"",\n            description=textwrap.dedent(\n                """"""\n               Contains questions and QDMRs annotated with full logical-forms of QDMR operators + arguments. Full logical-forms \n               were inferred by the annotation-consistency algorithm described in """"""\n            ),\n            lexicon_tokens=False,\n            text_features={\n                ""question_id"": ""question_id"",\n                ""question_text"": ""question_text"",\n                ""decomposition"": ""decomposition"",\n                ""operators"": ""operators"",\n                ""split"": ""split"",\n                ""program"": ""program"",\n            },\n        ),\n    ]\n\n    def _info(self):\n        # TODO(break_data): Specifies the nlp.DatasetInfo object\n        features = {text_feature: nlp.Value(""string"") for text_feature in six.iterkeys(self.config.text_features)}\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                features\n                # These are the features of your dataset like images, labels ...\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/allenai/Break"",\n            citation=_CITATION,\n        )\n        # if\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(break_data): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""Break-dataset"")\n        qdmr_high_level = os.path.join(data_dir, ""QDMR-high-level"")\n        qdmr = os.path.join(data_dir, ""QDMR"")\n        logical = os.path.join(data_dir, ""logical-forms"")\n        if self.config.name == ""QDMR"" or self.config.name == ""QDMR-lexicon"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(qdmr, ""train.csv"")\n                        if not self.config.lexicon_tokens\n                        else os.path.join(qdmr, ""train_lexicon_tokens.json"")\n                    },\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(qdmr, ""dev.csv"")\n                        if not self.config.lexicon_tokens\n                        else os.path.join(qdmr, ""dev_lexicon_tokens.json"")\n                    },\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(qdmr, ""test.csv"")\n                        if not self.config.lexicon_tokens\n                        else os.path.join(qdmr, ""test_lexicon_tokens.json"")\n                    },\n                ),\n            ]\n        elif self.config.name == ""QDMR-high-level"" or self.config.name == ""QDMR-high-level-lexicon"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(qdmr_high_level, ""train.csv"")\n                        if not self.config.lexicon_tokens\n                        else os.path.join(qdmr_high_level, ""train_lexicon_tokens.json"")\n                    },\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(qdmr_high_level, ""dev.csv"")\n                        if not self.config.lexicon_tokens\n                        else os.path.join(qdmr_high_level, ""dev_lexicon_tokens.json"")\n                    },\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(qdmr_high_level, ""test.csv"")\n                        if not self.config.lexicon_tokens\n                        else os.path.join(qdmr_high_level, ""test_lexicon_tokens.json"")\n                    },\n                ),\n            ]\n        elif self.config.name == ""logical-forms"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(logical, ""train.csv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(logical, ""dev.csv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(logical, ""test.csv"")},\n                ),\n            ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(break_data): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            if (\n                self.config.name == ""QDMR-high-level""\n                or self.config.name == ""QDMR""\n                or self.config.name == ""logical-forms""\n            ):\n                data = csv.DictReader(f)\n                for id_, row in enumerate(data):\n                    yield id_, row\n            elif self.config.name == ""QDMR-high-level-lexicon"" or self.config.name == ""QDMR-lexicon"":\n                for id_, row in enumerate(f):\n                    data = json.loads(row)\n                    yield id_, data\n'"
datasets/c4/c4.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""C4 dataset based on Common Crawl.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport logging\nimport os\n\nimport apache_beam as beam\n\nimport nlp\n\nfrom .c4_utils import (\n    dedupe_urls,\n    filter_by_webtextlike,\n    get_clean_page_fn,\n    get_counter_inc_fn,\n    get_hashed_url_filter_fn,\n    is_language,\n    is_realnews_domain,\n    is_valid_length,\n    normalize_url,\n    remove_duplicate_text,\n    split_wet_file,\n)\n\n\n_DESCRIPTION = """"""\\\nA colossal, cleaned version of Common Crawl\'s web crawl corpus.\n\nBased on Common Crawl dataset: ""https://commoncrawl.org""\n\nDue to the overhead of cleaning the dataset, it is recommend you prepare it with\na distributed service like Cloud Dataflow. More info at\nhttps://www.tensorflow.org/datasets/beam_datasets.\n""""""\n_CITATION = """"""\n@article{2019t5,\n    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n    journal = {arXiv e-prints},\n    year = {2019},\n    archivePrefix = {arXiv},\n    eprint = {1910.10683},\n}\n""""""\n_VERSION = nlp.Version(""2.3.0"", ""Deduplicate lines within a page."")\n\n_DOWNLOAD_HOST = ""https://commoncrawl.s3.amazonaws.com""\n_WET_PATH_URL = ""https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-{cc_version}/wet.paths.gz""\n_REALNEWS_DOMAINS_URL = ""https://raw.githubusercontent.com/rowanz/grover/38f7184bd87237ae2d3bc330b99f1e2e246f6d51/realnews/domain_to_allowed_subdomains.json""\n_BADWORDS_URL = ""https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/25e679f03d96baa721cde20db9944649e8d0a844/{lang}""\n_CHECKSUMS_URL = ""https://storage.googleapis.com/tfds-data/manual_checksums/c4.txt""\n_OPENWEBTEXT_URLS_ZIP = ""OpenWebText.zip""\n_OPENWEBTEXT_URLS_URL = ""https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ""\n_OPENWEBTEXT_URLS_FILE_PATTERN = ""OpenWebText/Version 1/URLs/*.txt""\n\n_DEFAULT_CC_VERSIONS = (""2019-18"",)  # April 2019\n_DEFAULT_WEBTEXTLIKE_CC_VERSIONS = (  # August 2018 - July 2019\n    ""2018-34"",\n    ""2018-39"",\n    ""2018-43"",\n    ""2018-47"",\n    ""2018-51"",\n    ""2019-04"",\n    ""2019-09"",\n    ""2019-13"",\n    ""2019-18"",\n    ""2019-22"",\n    ""2019-26"",\n    ""2019-30"",\n)\n\n\nclass C4Config(nlp.BuilderConfig):\n    """"""BuilderConfig for C4 dataset.""""""\n\n    def __init__(self, language, cc_versions=None, clean=True, realnewslike=False, webtextlike=False, **kwargs):\n        """"""BuilderConfig for C4.\n\n        Args:\n            language: string, the language code, or ""all"" to disable language\n                filtering.\n            cc_versions: tuple(string), a collection of versions of Common Crawl to\n                use as the raw source text. Set to None to use defaults.\n            clean: bool, whether to clean the dataset for badwords, duplications, etc.\n            realnewslike: bool, whether to limit to news domains as compiled by\n                RealNews.\n            webtextlike: bool, whether to limit to WebText-like URLs.\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        name_parts = [language]\n        if cc_versions:\n            name_parts.append(""_"".join(cc_versions))\n        if not clean:\n            name_parts.append(""noclean"")\n        if realnewslike:\n            name_parts.append(""realnewslike"")\n        if webtextlike:\n            name_parts.append(""webtextlike"")\n        name = ""."".join(name_parts)\n        super(C4Config, self).__init__(name=name, version=_VERSION, **kwargs)\n        self.lang = language\n        self.cc_versions = cc_versions or (_DEFAULT_WEBTEXTLIKE_CC_VERSIONS if webtextlike else _DEFAULT_CC_VERSIONS)\n        self.clean = clean\n        self.realnewslike = realnewslike\n        self.webtextlike = webtextlike\n\n\nclass C4(nlp.BeamBasedBuilder):\n    """"""C4 dataset based on Common Crawl.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n    For the WebText-like config, you must manually download \'OpenWebText.zip\'\n    (from https://mega.nz/#F!EZZD0YwJ!9_PlEQzdMVLaNdKv_ICNVQ) and the Common Crawl\n    WET files from August 2018 to July 2019\n    (https://commoncrawl.org/the-data/get-started/) and place them in the\n    `manual_dir`.\n\n    """"""\n\n    BUILDER_CONFIGS = [\n        C4Config(language=""en"", description=""English C4 dataset.""),\n        C4Config(\n            language=""en"",\n            clean=False,\n            description=""Disables all cleaning (deduplication, removal based on bad words, "" ""etc.)"",\n        ),\n        C4Config(\n            language=""en"",\n            realnewslike=True,\n            description=""Filters from the default config to only include content from the ""\n            ""domains used in the \'RealNews\' dataset (Zellers et al., 2019)."",\n        ),\n        C4Config(\n            language=""en"",\n            webtextlike=True,\n            description=""Filters from the default config to only include content from the ""\n            ""URLs in OpenWebText (https://github.com/jcpeterson/openwebtext)."",\n        ),\n    ]\n\n    def _info(self):\n        features = {\n            ""text"": nlp.Value(""string""),\n            ""url"": nlp.Value(""string""),\n            ""content-type"": nlp.Value(""string""),\n            ""content-length"": nlp.Value(""string""),\n            ""timestamp"": nlp.Value(""string""),\n        }\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(features),\n            citation=_CITATION,\n            homepage=""https://github.com/google-research/text-to-text-transfer-transformer#datasets"",\n        )\n\n    def _split_generators(self, dl_manager, pipeline):\n\n        # We will automatically down the default CC version(s), but others need to\n        # be manually downloaded.\n        cc_versions = set(self.config.cc_versions)\n        auto_cc_versions = cc_versions & set(_DEFAULT_CC_VERSIONS)\n        manual_cc_versions = cc_versions - set(_DEFAULT_CC_VERSIONS)\n\n        files_to_download = {}\n        files_to_download[""wet_path_urls""] = [\n            _WET_PATH_URL.format(cc_version=cc_version) for cc_version in auto_cc_versions\n        ]\n        if self.config.clean:\n            files_to_download[""badwords""] = _BADWORDS_URL.format(lang=self.config.lang)\n        if self.config.realnewslike:\n            files_to_download[""realnews_domains""] = _REALNEWS_DOMAINS_URL\n        file_paths = dl_manager.download_and_extract(files_to_download)\n\n        if self.config.webtextlike:\n            owt_path = os.path.join(dl_manager.manual_dir, _OPENWEBTEXT_URLS_ZIP)\n            if not os.path.exists(owt_path):\n                raise FileNotFoundError(\n                    ""{} does not exist. Make sure you insert a manual dir via `nlp.load(\'c4\', data_dir=...)` that includes a file name {}. Manual download instructions: {})"".format(\n                        owt_path, _OPENWEBTEXT_URLS_ZIP, self.MANUAL_DOWNLOAD_INSTRUCTIONS\n                    )\n                )\n            file_paths[""openwebtext_urls_zip""] = dl_manager.extract(owt_path)\n\n        wet_urls = []\n        for wet_path_url in file_paths[""wet_path_urls""]:\n            with open(wet_path_url, ""r"") as f:\n                wet_urls.extend([""%s/%s"" % (_DOWNLOAD_HOST, l.strip()) for l in f])\n        file_paths[""wet_urls""] = wet_urls\n        file_paths[""wet_files""] = []\n\n        for cc_version in manual_cc_versions:\n            cc_dir = os.path.join(dl_manager.manual_dir, cc_version)\n            wet_files = beam.io.filesystems.FileSystems.match(os.path.join(cc_dir, ""*.warc.wet.gz""))\n            if not os.path.exists(cc_dir):\n                raise FileNotFoundError(\n                    ""{} does not exist. Make sure you insert a manual dir via `nlp.load(\'c4\', data_dir=...)` that includes the files {}. Manual download instructions: {})"".format(\n                        cc_dir, ""*.warc.wet.gz"", self.MANUAL_DOWNLOAD_INSTRUCTIONS\n                    )\n                )\n            logging.info(""Adding %d WET files for manually downloaded version %s."", len(wet_files), cc_version)\n            file_paths[""wet_files""].extend(wet_files)\n\n        page_content_pcollection = self._get_page_content(pipeline, file_paths, dl_manager)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs=dict(\n                    split=""train"",\n                    page_content=page_content_pcollection,\n                    hashed_url_predicate=lambda x: x % 1000 != 0,  # 99.9%\n                ),\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs=dict(\n                    split=""validation"",\n                    page_content=page_content_pcollection,\n                    hashed_url_predicate=lambda x: x % 1000 == 0,  # 0.01%\n                ),\n            ),\n        ]\n\n    def _get_page_content(self, pipeline, file_paths, dl_manager):\n        """"""Build PCollection of un-split page content.""""""\n\n        wet_file_paths = pipeline | ""create_wet_files"" >> beam.Create(file_paths[""wet_files""])\n        if ""wet_urls"" in file_paths:\n\n            def download_url(url, downloader, pipeline):\n                path = downloader.download(url)\n                if not pipeline.is_local():\n                    path = downloader.ship_files_with_pipeline(path, pipeline)\n                return path\n\n            dl_wet_file_paths = (\n                pipeline\n                | ""create_wet_urls"" >> beam.Create(file_paths[""wet_urls""])\n                | beam.Map(download_url, downloader=dl_manager, pipeline=pipeline)\n            )\n            wet_file_paths = (wet_file_paths, dl_wet_file_paths) | beam.Flatten()\n\n        # Parse WET files and filter by length.\n        # Output: url, text\n        page_content = wet_file_paths | beam.FlatMap(split_wet_file) | beam.Filter(is_valid_length)\n\n        # Optionally filter for RealNews domains.\n        # Output: url, text\n        if self.config.realnewslike:\n            with open(file_paths[""realnews_domains""], ""r"") as f:\n                realnews_domains = json.load(f)\n            page_content = page_content | beam.Filter(is_realnews_domain, realnews_domains)\n\n        # Normalize and deduplicate by URL.\n        # Output: url, text\n        page_content = (\n            page_content\n            | ""normalize_url"" >> beam.Map(normalize_url)\n            | ""group_url"" >> beam.GroupByKey()\n            | beam.Map(dedupe_urls)\n        )\n\n        # Optionally filter for WebText-like URLs.\n        # Output: url, text\n        if self.config.webtextlike:\n            webtextlike_urls = (\n                pipeline\n                | ""read_webtextlike_urls""\n                >> beam.io.ReadFromText(\n                    os.path.join(file_paths[""openwebtext_urls_zip""], _OPENWEBTEXT_URLS_FILE_PATTERN)\n                )\n                | ""add_dummy_page"" >> beam.Map(lambda x: (x, """"))\n                | ""normal_webtext_url"" >> beam.Map(normalize_url)\n            )\n            page_content = (\n                {""text"": page_content, ""webtextlike_urls"": webtextlike_urls}\n                | ""group_webtextlike_urls"" >> beam.CoGroupByKey()\n                | beam.FlatMap(filter_by_webtextlike)\n            )\n\n        # Optionally clean pages of badwords, boilerpolate text, and duplicate\n        # spans of sentences.\n        # Output: url, text\n        if self.config.clean:\n            with open(file_paths[""badwords""], ""r"") as f:\n                badwords = [l.strip() for l in f]\n            page_content = page_content | ""clean_pages"" >> beam.FlatMap(get_clean_page_fn(badwords))\n            page_content = remove_duplicate_text(page_content)\n\n        # Optionally filter out non-`language` pages. We do this after cleaning\n        # since it may change the predominate language.\n        if self.config.lang != ""all"":\n            page_content |= beam.Filter(is_language, language=self.config.lang)\n\n        return page_content\n\n    def _build_pcollection(self, unused_pipeline, split, page_content, hashed_url_predicate):\n        def _emit_examples(el):\n            get_counter_inc_fn(split)(""examples"")\n            _, features = el\n            return (\n                features[""url""],\n                {\n                    ""url"": features[""url""],\n                    ""text"": features[""text""],\n                    ""content-type"": features[""content-type""],\n                    ""content-length"": features[""content-length""],\n                    ""timestamp"": features[""timestamp""],\n                },\n            )\n\n        return page_content | beam.Filter(get_hashed_url_filter_fn(hashed_url_predicate)) | beam.Map(_emit_examples)\n'"
datasets/c4/c4_utils.py,5,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Utilities for generating the C4 dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport functools\nimport gzip\nimport hashlib\nimport io\nimport re\nimport threading\n\nimport apache_beam as beam\nimport nltk\nimport tensorflow.compat.v2 as tf\nfrom absl import logging\n\nimport langdetect\nimport tldextract\n\n\n# WET file constants\n_PAGE_DELIMITER = ""WARC/1.0""\n_URL_KEY = ""WARC-Target-URI:""\n_URL_DATE = ""WARC-Date:""\n_CONTENT_TYPE = ""Content-Type:""\n_CONTENT_LEN = ""Content-Length:""\n_METADATA_PREFIXES = (""WARC"", ""CONTENT-"", ""Content-"")\n\n# Filters\n_MIN_WORDS_PER_LINE = 5\n_MIN_NUM_SENTENCES = 3\n_MAX_WORD_LENGTH = 1000\n_END_MARKS = (""."", ""?"", ""!"", \'""\')\n_ELLIPSIS = ""...""\n_POLICY_SUBSTRINGS = [\n    ""terms of use"",\n    ""privacy policy"",\n    ""cookie policy"",\n    ""uses cookies"",\n    ""use of cookies"",\n    ""use cookies"",\n]\n\n# Memoized sentence tokenizer.\n_SENTENCE_TOKENIZER = None\n\n\ndef get_counter_inc_fn(namespace):\n    def counter_inc_fn(counter, amt=1):\n        beam.metrics.Metrics.counter(namespace, counter).inc(amt)\n\n    return counter_inc_fn\n\n\ndef get_hashed_url_filter_fn(predicate_fn):\n    def filter_fn(el):\n        url, _ = el\n        val = int(hashlib.md5(tf.compat.as_text(url).encode(""utf-8"")).hexdigest(), 16)\n        return predicate_fn(val)\n\n    return filter_fn\n\n\ndef _load_sentence_tokenizer():\n    """"""Returns a sentence tokenization function.""""""\n    # Lock to avoid a race-condition in the creation of the download directory.\n    with threading.Lock():\n        nltk.download(""punkt"")\n        return nltk.data.load(""nltk:tokenizers/punkt/english.pickle"")\n\n\ndef _get_sentences(text):\n    global _SENTENCE_TOKENIZER\n    if not _SENTENCE_TOKENIZER:\n        _SENTENCE_TOKENIZER = _load_sentence_tokenizer()\n    return list(_SENTENCE_TOKENIZER.tokenize(tf.compat.as_text(text)))\n\n\ndef _get_sentences_by_line(text, lower=False):\n    sentences = []\n    for line in text.splitlines():\n        sentences.append([s.lower() if lower else s for s in _get_sentences(line)])\n    return sentences\n\n\ndef is_language(page, language, min_probability=0.99):\n    """"""Returns True iff text is in `language` with at least `min_probability`.""""""\n    unused_url, features = page\n    text = features[""text""]\n\n    counter_inc_fn = get_counter_inc_fn(""detected-lang"")\n\n    # Make langdetect predictions deterministic.\n    langdetect.DetectorFactory.seed = 0\n    try:\n        predictions = langdetect.detect_langs(text)\n    except langdetect.lang_detect_exception.LangDetectException:\n        counter_inc_fn(""langdetect-exception"")\n        return False\n    if not predictions:\n        counter_inc_fn(""page-filtered-nolangpredictions"")\n        return False\n    best_prediction = predictions[0]\n    if best_prediction.prob < min_probability:\n        counter_inc_fn(""page-filtered-lowlangdetectconf"")\n        return False\n    if best_prediction.lang != language:\n        counter_inc_fn(""page-filtered-ignoredlang"")\n        counter_inc_fn(""page-filtered-ignoredlang-%s"" % (best_prediction.lang))\n        return False\n    counter_inc_fn(""page-emited-%s"" % best_prediction.lang)\n    return True\n\n\ndef get_clean_page_fn(badwords=None):\n    """"""Returns `clean_page` with pre-compiled badword and citation regexes.""""""\n    # Used to filter citation from Wikipedia pages (among others).\n    citation_regex = re.compile(r""\\[\\d*\\]|\\[edit\\]|\\[citation needed\\]"")\n    if badwords:\n        badwords_regex = re.compile(""[^a-z]({})[^a-z]"".format(""|"".join(badwords or [])))\n    else:\n        badwords_regex = None\n    return functools.partial(clean_page, citation_regex=citation_regex, badwords_regex=badwords_regex)\n\n\ndef clean_page(\n    url_and_features,\n    citation_regex,\n    badwords_regex=None,\n    counter_inc_fn=None,\n    min_words_per_line=_MIN_WORDS_PER_LINE,\n    min_num_sentences=_MIN_NUM_SENTENCES,\n    max_word_length=_MAX_WORD_LENGTH,\n):\n    """"""Cleans a CommonCrawl page, yielding nothing if it should be skipped.\n\n    Cleaning removes lines with no end marks or with too few words. After line\n    filtering, pages are filtered out if they have too few sentences based on a\n    simple count of end marks.\n\n    Args:\n        url_and_features: tuple(string, dict), the url and features of the page.\n        citation_regex: Regex to use for finding Wikipedia-like citations to filter.\n        badwords_regex: Regex to use for finding badwords. Default None, which means\n            don\'t apply badwords filtering.\n        counter_inc_fn: function, a function taking the name of a counter to be\n            incremented and the (optional) amount. Defaults to a beam Metric counter.\n        min_words_per_line: int, the minimum number of words a line needs to not be\n            removed.\n        min_num_sentences: int, the minimum number of sentences a page needs to not\n            be skipped.\n        max_word_length: int, the maximum number of characters allowed in a word.\n            Lines containing a word with too many characters are removed.\n    Yields:\n        The url and cleaned text for the page.\n    """"""\n    url, features = url_and_features\n    text = features[""text""]\n\n    if not counter_inc_fn:\n        counter_inc_fn = get_counter_inc_fn(""clean-page"")\n\n    lines = text.splitlines()\n    valid_lines = []\n    num_sentences = 0\n\n    def line_has_too_long_word(line):\n        for word in line.split():\n            if len(word) > max_word_length:\n                return True\n        return False\n\n    for line in lines:\n        line = line.strip()\n        if line_has_too_long_word(line):\n            counter_inc_fn(""lines-with-too-long-word"")\n            continue\n        line = citation_regex.sub("""", line)\n        if not line.endswith(_END_MARKS) or line.endswith(_ELLIPSIS):\n            counter_inc_fn(""lines-no-endmark"")\n            continue\n        if len(line.split()) < min_words_per_line:\n            counter_inc_fn(""lines-too-short"")\n            continue\n        line_lower = line.lower()\n        # Remove documents which contain lorem ipsum\n        if ""lorem ipsum"" in line_lower:\n            counter_inc_fn(""filtered-page-loremipsum"")\n            return\n        # Remove ""javascript must be enabled"" notices\n        if ""javascript"" in line_lower:\n            counter_inc_fn(""lines-javascript"")\n            continue\n        # Remove docs which probably contain javascript code\n        if ""{"" in line:\n            counter_inc_fn(""filtered-page-squigglybracket"")\n            return\n        # Remove policy lines\n        if any(p in line_lower for p in _POLICY_SUBSTRINGS):\n            counter_inc_fn(""lines-policy"")\n            continue\n        # If any badword appears on its own in the line, skip this doc\n        if badwords_regex:\n            badwords_found = badwords_regex.search(line_lower)\n            if badwords_found is not None:\n                counter_inc_fn(""filtered-page-badword"")\n                return\n        num_sentences += len(_get_sentences(line))\n        valid_lines.append(line)\n        counter_inc_fn(""lines-valid"")\n\n    if num_sentences < min_num_sentences:\n        counter_inc_fn(""filtered-page-toofewsentences"")\n        return\n    counter_inc_fn(""emitted-clean-pages"")\n    features[""text""] = ""\\n"".join(valid_lines).strip()\n    yield url, features\n\n\ndef _hash_line(line):\n    m = hashlib.md5()\n    m.update(tf.compat.as_text(line).encode(""utf-8"").strip().lower())\n    return m.hexdigest()\n\n\ndef _emit_url_to_lines(page):\n    """"""Emits url to all (lower-cased, hashed) lines.""""""\n    url, features = page\n    text = features[""text""]\n    for line in text.split(""\\n""):\n        yield _hash_line(line), url\n\n\ndef _emit_line_to_urls(el, counter_inc_fn):\n    """"""Emits (hashed) line to all but one url.""""""\n    line, urls = el\n    # Materialize urls as a list.\n    urls = list(urls)\n    # Hash urls and sort to have a consistent, but unbiased, selection when the\n    # same urls exist for multiple lines.\n    skip_url = min(urls, key=lambda x: hashlib.md5(tf.compat.as_text(x).encode(""utf-8"")).hexdigest())\n    for url in urls:\n        if url != skip_url:\n            yield url, line\n    counter_inc_fn(""emitted-line-duplicate"", amt=len(urls) - 1)\n\n\ndef _remove_lines_from_text(el, counter_inc_fn, min_num_sentences=_MIN_NUM_SENTENCES):\n    """"""Removes matching lines from the page.\n\n    Process the result of a join containing a single value for \'features\' and zero\n    or more values for \'lines\'. Each value in \'lines\' is a lower-cased, hashed\n    line.\n\n    If a line has fewer sentences than `max_window_size`, the full line is\n    compared for a match.\n\n    Args:\n        el: `(string, {\'features\': features_dict, \'lines\': [string]})`,\n            element containing the result of a join on key with both the page text\n            and lower-cased, hashed lines to remove.\n        counter_inc_fn: function, a function taking the name of a counter to be\n            incremented and the (optional) amount.\n        min_num_sentences: int, the minimum number of sentences a page needs to not\n            be skipped.\n\n    Yields:\n        url: The URL of the page.\n        features: The page features with lines removed from text.\n    """"""\n    url, join_values = el\n    features = join_values[""features""]\n\n    assert len(features) == 1, ""Invalid page count (%d) for %s"" % (len(features), url)\n    features = features[0]\n    text = features[""text""]\n    lines_to_remove = set(join_values[""lines""])\n    new_lines = []\n    hashed_lines = set()\n    for line in text.split(""\\n""):\n        hashed_line = _hash_line(line)\n        if hashed_line in lines_to_remove:\n            counter_inc_fn(""filtered-lines-duplicate"")\n        elif hashed_line not in hashed_lines:\n            new_lines.append(line)\n            hashed_lines.add(hashed_line)\n    new_text = ""\\n"".join(new_lines)\n    if len(_get_sentences(new_text)) < min_num_sentences:\n        counter_inc_fn(""filtered-doc-toofewsentences"")\n        return\n    new_features = features.copy()\n    new_features[""text""] = new_text\n    yield (url, new_features)\n\n\ndef remove_duplicate_text(pages):\n    """"""Utility to remove duplicate lines across text documents.""""""\n    # Output: url, lines\n    counter_inc_fn = get_counter_inc_fn(""dedupe-lines"")\n    lines_to_remove = (\n        pages\n        | beam.FlatMap(_emit_url_to_lines)\n        | ""group_sentences"" >> beam.GroupByKey()\n        | beam.FlatMap(_emit_line_to_urls, counter_inc_fn=counter_inc_fn)\n    )\n\n    # Output: url, text\n    final_docs = (\n        {""features"": pages, ""lines"": lines_to_remove}\n        | ""group_features_and_lines_by_url"" >> beam.CoGroupByKey()\n        | beam.FlatMap(_remove_lines_from_text, counter_inc_fn=counter_inc_fn)\n    )\n\n    return final_docs\n\n\ndef split_wet_file(wet_file_path, counter_inc_fn=None):\n    """"""Split a WET file into separate pages.""""""\n    logging.info(""Splitting file: %s"", wet_file_path)\n    if not counter_inc_fn:\n        counter_inc_fn = get_counter_inc_fn(""split-wet-file"")\n    counter_inc_fn(""wet-file"")\n\n    with beam.io.filesystems.FileSystems.open(wet_file_path) as f, gzip.GzipFile(fileobj=f) as g:\n        url = None\n        content = None\n        content_len = None\n        content_type = None\n        timestamp = None\n\n        def _maybe_get_page():\n            """"""Generate a (url, {features}) page.""""""\n            if not url and url is not None:\n                counter_inc_fn(""page-filtered-nourl"")\n            if not content and content is not None:\n                counter_inc_fn(""page-filtered-nocontent"")\n            if not content_type and content_type is not None:\n                counter_inc_fn(""page-nocontenttype"")\n            if not content_len and content_len is not None:\n                counter_inc_fn(""page-nocontentlen"")\n            if not timestamp and timestamp is not None:\n                counter_inc_fn(""page-notimestamp"")\n            if content and url:\n                counter_inc_fn(""page-emitted"")\n                return (\n                    url,\n                    {\n                        ""text"": ""\\n"".join(content),\n                        ""content-type"": content_type,\n                        ""content-length"": content_len,\n                        ""timestamp"": timestamp,\n                        ""url"": url,\n                    },\n                )\n            return None\n\n        for line in io.TextIOWrapper(g, encoding=""utf-8""):\n            line = line.strip()\n            if not line:\n                continue\n            if line == _PAGE_DELIMITER:\n                page = _maybe_get_page()\n                if page:\n                    yield page\n                url = """"\n                content = []\n                content_len = """"\n                content_type = """"\n                timestamp = """"\n\n            if line.startswith(_URL_KEY):\n                url = line[len(_URL_KEY) :].strip()\n\n            if line.startswith(_URL_DATE):\n                timestamp = line[len(_URL_DATE) :].strip()\n\n            if line.startswith(_CONTENT_TYPE):\n                content_type = line[len(_CONTENT_TYPE) :].strip()\n\n            if line.startswith(_CONTENT_LEN):\n                content_len = line[len(_CONTENT_LEN) :].strip()\n\n            if line.startswith(_METADATA_PREFIXES):\n                continue\n\n            content.append(line)\n\n        page = _maybe_get_page()\n        if page:\n            yield page\n\n\ndef dedupe_urls(el):\n    """"""Returns the first value for a given URL.""""""\n    counter_inc_fn = get_counter_inc_fn(""dedupe-urls"")\n    url, vals = el\n    cnt = 0\n    v = None\n    for v in vals:\n        cnt += 1\n    counter_inc_fn(""filtered-url-duplicate"", cnt - 1)\n    counter_inc_fn(""unique-url"")\n    return url, v\n\n\ndef is_valid_length(el, max_length=1.9e5):\n    """"""Returns False iff page\'s text is too long.""""""\n    counter_inc_fn = get_counter_inc_fn(""is-valid-length"")\n    _, page = el\n    if len(page[""text""]) > max_length:\n        counter_inc_fn(""filtered-page-contenttoolong"")\n        return False\n    counter_inc_fn(""valid-length"")\n    return True\n\n\ndef is_realnews_domain(el, realnews_domains):\n    """"""Returns False iff page\'s (sub)domain is not allowed.""""""\n    counter_inc_fn = get_counter_inc_fn(""is-realnews-domain"")\n    url, _ = el\n    ext = tldextract.extract(url)\n    main_domain = ext.domain + ""."" + ext.suffix\n    if main_domain not in realnews_domains:\n        counter_inc_fn(""filtered-url-invaliddomain"")\n        return False\n    allowed_subdomains = realnews_domains[main_domain]\n    if isinstance(allowed_subdomains, list) and ext.subdomain not in allowed_subdomains:\n        counter_inc_fn(""filtered-url-invalidsubdomain"")\n        return False\n    counter_inc_fn(""realnews-domain"")\n    return True\n\n\ndef filter_by_webtextlike(el):\n    """"""Yields only pages with a matching WebText-like URL.""""""\n    counter_inc_fn = get_counter_inc_fn(""filter-by-webtextlike"")\n    url, join_values = el\n    text = join_values[""text""]\n    webtextlike = join_values[""webtextlike_urls""]\n    if not webtextlike:\n        counter_inc_fn(""filtered-url-notwebtextlike"")\n        return\n    if not text:\n        counter_inc_fn(""missing-webtextlike"")\n        return\n    assert len(text) == 1\n    counter_inc_fn(""found-webtextlike"")\n    yield url, text[0]\n\n\ndef normalize_url(el):\n    url, val = el\n    url = tf.compat.as_text(url)\n    url = re.sub(r""https?:\\/\\/(www\\.)?"", """", url)\n    url = re.sub(r""\\?(utm_|ref|feed).*"", """", url)\n    url = url.rstrip(""/"")\n    return url, val\n'"
datasets/cfq/cfq.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""CFQ (Compositional Freebase Question) dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport logging\nimport os\nimport re\n\nimport nlp\n\n\n_CITATION = """"""\n@inproceedings{Keysers2020,\n  title={Measuring Compositional Generalization: A Comprehensive Method on\n         Realistic Data},\n  author={Daniel Keysers and Nathanael Sch\\""{a}rli and Nathan Scales and\n          Hylke Buisman and Daniel Furrer and Sergii Kashubin and\n          Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and\n          Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and\n          Olivier Bousquet},\n  booktitle={ICLR},\n  year={2020},\n  url={https://arxiv.org/abs/1912.09713.pdf},\n}\n""""""\n\n_DESCRIPTION = """"""\nThe CFQ dataset (and it\'s splits) for measuring compositional generalization.\n\nSee https://arxiv.org/abs/1912.09713.pdf for background.\n\nExample usage:\ndata = nlp.load_dataset(\'cfq/mcd1\')\n""""""\n\n_DATA_URL = ""https://storage.googleapis.com/cfq_dataset/cfq.tar.gz""\n\n\nclass CfqConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for CFQ splits.""""""\n\n    def __init__(self, name, directory=""splits"", **kwargs):\n        """"""BuilderConfig for CFQ.\n\n    Args:\n      name: Unique name of the split.\n      directory: Which subdirectory to read the split from.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        # Version history:\n        super(CfqConfig, self).__init__(name=name, version=nlp.Version(""1.0.1""), description=_DESCRIPTION, **kwargs)\n        self.split_file = os.path.join(directory, name + "".json"")\n\n\n_QUESTION = ""question""\n_QUERY = ""query""\n_QUESTION_FIELD = ""questionPatternModEntities""\n_QUERY_FIELD = ""sparqlPatternModEntities""\n\n\nclass Cfq(nlp.GeneratorBasedBuilder):\n    """"""CFQ task / splits.""""""\n\n    BUILDER_CONFIGS = [\n        CfqConfig(name=""mcd1""),\n        CfqConfig(name=""mcd2""),\n        CfqConfig(name=""mcd3""),\n        CfqConfig(name=""question_complexity_split""),\n        CfqConfig(name=""question_pattern_split""),\n        CfqConfig(name=""query_complexity_split""),\n        CfqConfig(name=""query_pattern_split""),\n        CfqConfig(name=""random_split""),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({_QUESTION: nlp.Value(""string""), _QUERY: nlp.Value(""string""),}),\n            supervised_keys=(_QUESTION, _QUERY),\n            homepage=""https://github.com/google-research/google-research/tree/master/cfq"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        data_dir = dl_manager.download_and_extract(_DATA_URL)\n        data_dir = os.path.join(data_dir, ""cfq"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={\n                    ""base_directory"": data_dir,\n                    ""splits_file"": self.config.split_file,\n                    ""split_id"": ""trainIdxs"",\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={""base_directory"": data_dir, ""splits_file"": self.config.split_file, ""split_id"": ""testIdxs""},\n            ),\n        ]\n\n    def _scrub_json(self, content):\n        """"""Reduce JSON by filtering out only the fields of interest.""""""\n        # Loading of json data with the standard Python library is very inefficient:\n        # For the 4GB dataset file it requires more than 40GB of RAM and takes 3min.\n        # There are more efficient libraries but in order to avoid additional\n        # dependencies we use a simple (perhaps somewhat brittle) regexp to reduce\n        # the content to only what is needed. This takes 1min to execute but\n        # afterwards loading requires only 500MB or RAM and is done in 2s.\n        regex = re.compile(r\'(""%s"":\\s*""[^""]*"").*?(""%s"":\\s*""[^""]*"")\' % (_QUESTION_FIELD, _QUERY_FIELD), re.DOTALL)\n        return ""["" + "","".join([""{"" + m.group(1) + "","" + m.group(2) + ""}"" for m in regex.finditer(content)]) + ""]""\n\n    def _generate_examples(self, base_directory, splits_file, split_id):\n        """"""Yields examples.""""""\n        samples_path = os.path.join(base_directory, ""dataset.json"")\n        splits_path = os.path.join(base_directory, splits_file)\n        with open(samples_path) as samples_file:\n            with open(splits_path) as splits_file:\n                logging.info(""Reading json from %s into memory..."", samples_path)\n                samples = json.loads(self._scrub_json(samples_file.read()))\n                logging.info(""%d samples loaded"", len(samples))\n                logging.info(""Loaded json data from %s."", samples_path)\n                splits = json.load(splits_file)\n                for idx in splits[split_id]:\n                    sample = samples[idx]\n                    yield idx, {_QUESTION: sample[_QUESTION_FIELD], _QUERY: sample[_QUERY_FIELD]}\n'"
datasets/civil_comments/civil_comments.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""CivilComments from Jigsaw Unintended Bias Kaggle Competition.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@article{DBLP:journals/corr/abs-1903-04561,\n  author    = {Daniel Borkan and\n               Lucas Dixon and\n               Jeffrey Sorensen and\n               Nithum Thain and\n               Lucy Vasserman},\n  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text\n               Classification},\n  journal   = {CoRR},\n  volume    = {abs/1903.04561},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1903.04561},\n  archivePrefix = {arXiv},\n  eprint    = {1903.04561},\n  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n""""""\n\n_DESCRIPTION = """"""\nThe comments in this dataset come from an archive of the Civil Comments\nplatform, a commenting plugin for independent news sites. These public comments\nwere created from 2015 - 2017 and appeared on approximately 50 English-language\nnews sites across the world. When Civil Comments shut down in 2017, they chose\nto make the public comments available in a lasting open archive to enable future\nresearch. The original data, published on figshare, includes the public comment\ntext, some associated metadata such as article IDs, timestamps and\ncommenter-generated ""civility"" labels, but does not include user ids. Jigsaw\nextended this dataset by adding additional labels for toxicity and identity\nmentions. This data set is an exact replica of the data released for the\nJigsaw Unintended Bias in Toxicity Classification Kaggle challenge.  This\ndataset is released under CC0, as is the underlying comment text.\n""""""\n\n_DOWNLOAD_URL = ""https://storage.googleapis.com/jigsaw-unintended-bias-in-toxicity-classification/civil_comments.zip""\n\n\nclass CivilComments(nlp.GeneratorBasedBuilder):\n    """"""Classification and tagging of 2M comments on news sites.\n\n  This version of the CivilComments Dataset provides access to the primary\n  seven labels that were annotated by crowd workers, the toxicity and other\n  tags are a value between 0 and 1 indicating the fraction of annotators that\n  assigned these attributes to the comment text.\n\n  The other tags, which are only available for a fraction of the input examples\n  are currently ignored, as are all of the attributes that were part of the\n  original civil comments release. See the Kaggle documentation for more\n  details about the available features.\n  """"""\n\n    VERSION = nlp.Version(""0.9.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""text"": nlp.Value(""string""),\n                    ""toxicity"": nlp.Value(""float32""),\n                    ""severe_toxicity"": nlp.Value(""float32""),\n                    ""obscene"": nlp.Value(""float32""),\n                    ""threat"": nlp.Value(""float32""),\n                    ""insult"": nlp.Value(""float32""),\n                    ""identity_attack"": nlp.Value(""float32""),\n                    ""sexual_explicit"": nlp.Value(""float32""),\n                }\n            ),\n            # The supervised_keys version is very impoverished.\n            supervised_keys=(""text"", ""toxicity""),\n            homepage=""https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_path = dl_manager.download_and_extract(_DOWNLOAD_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={""filename"": os.path.join(dl_path, ""train.csv""), ""toxicity_label"": ""target""},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={\n                    ""filename"": os.path.join(dl_path, ""test_public_expanded.csv""),\n                    ""toxicity_label"": ""toxicity"",\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={\n                    ""filename"": os.path.join(dl_path, ""test_private_expanded.csv""),\n                    ""toxicity_label"": ""toxicity"",\n                },\n            ),\n        ]\n\n    def _generate_examples(self, filename, toxicity_label):\n        """"""Yields examples.\n\n    Each example contains a text input and then seven annotation labels.\n\n    Args:\n      filename: the path of the file to be read for this split.\n      toxicity_label: indicates \'target\' or \'toxicity\' to capture the variation\n        in the released labels for this dataset.\n\n    Yields:\n      A dictionary of features, all floating point except the input text.\n    """"""\n        with open(filename) as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                example = {}\n                example[""text""] = row[""comment_text""]\n                example[""toxicity""] = float(row[toxicity_label])\n                for label in [""severe_toxicity"", ""obscene"", ""threat"", ""insult"", ""identity_attack"", ""sexual_explicit""]:\n                    example[label] = float(row[label])\n                yield row[""id""], example\n'"
datasets/cmrc2018/cmrc2018.py,0,"b'""""""TODO(cmrc2018): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(cmrc2018): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{cui-emnlp2019-cmrc2018,\n    title = ""A Span-Extraction Dataset for {C}hinese Machine Reading Comprehension"",\n    author = ""Cui, Yiming  and\n      Liu, Ting  and\n      Che, Wanxiang  and\n      Xiao, Li  and\n      Chen, Zhipeng  and\n      Ma, Wentao  and\n      Wang, Shijin  and\n      Hu, Guoping"",\n    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"",\n    month = nov,\n    year = ""2019"",\n    address = ""Hong Kong, China"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/D19-1600"",\n    doi = ""10.18653/v1/D19-1600"",\n    pages = ""5886--5891"",\n}\n""""""\n\n# TODO(cmrc2018):\n_DESCRIPTION = """"""\\\nA Span-Extraction dataset for Chinese machine reading comprehension to add language\ndiversities in this area. The dataset is composed by near 20,000 real questions annotated\non Wikipedia paragraphs by human experts. We also annotated a challenge set which\ncontains the questions that need comprehensive understanding and multi-sentence\ninference throughout the context.\n""""""\n_URL = ""https://github.com/ymcui/cmrc2018""\n_TRAIN_FILE = ""https://worksheets.codalab.org/rest/bundles/0x15022f0c4d3944a599ab27256686b9ac/contents/blob/""\n_DEV_FILE = ""https://worksheets.codalab.org/rest/bundles/0x72252619f67b4346a85e122049c3eabd/contents/blob/""\n_TEST_FILE = ""https://worksheets.codalab.org/rest/bundles/0x182c2e71fac94fc2a45cc1a3376879f7/contents/blob/""\n\n\nclass Cmrc2018(nlp.GeneratorBasedBuilder):\n    """"""TODO(cmrc2018): Short description of my dataset.""""""\n\n    # TODO(cmrc2018): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(cmrc2018): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(cmrc2018): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {""train"": _TRAIN_FILE, ""dev"": _DEV_FILE, ""test"": _TEST_FILE}\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]}),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": downloaded_files[""test""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(cmrc2018): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for example in data[""data""]:\n                for paragraph in example[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        yield id_, {\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                        }\n'"
datasets/cnn_dailymail/cnn_dailymail.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""CNN/DailyMail Summarization dataset, non-anonymized version.""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport hashlib\nimport logging\nimport os\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nCNN/DailyMail non-anonymized summarization dataset.\n\nThere are two features:\n  - article: text of news article, used as the document to be summarized\n  - highlights: joined text of highlights with <s> and </s> around each\n    highlight, which is the target summary\n""""""\n\n# The second citation introduces the source data, while the first\n# introduces the specific form (non-anonymized) we use here.\n_CITATION = """"""\\\n@article{DBLP:journals/corr/SeeLM17,\n  author    = {Abigail See and\n               Peter J. Liu and\n               Christopher D. Manning},\n  title     = {Get To The Point: Summarization with Pointer-Generator Networks},\n  journal   = {CoRR},\n  volume    = {abs/1704.04368},\n  year      = {2017},\n  url       = {http://arxiv.org/abs/1704.04368},\n  archivePrefix = {arXiv},\n  eprint    = {1704.04368},\n  timestamp = {Mon, 13 Aug 2018 16:46:08 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/SeeLM17},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@inproceedings{hermann2015teaching,\n  title={Teaching machines to read and comprehend},\n  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},\n  booktitle={Advances in neural information processing systems},\n  pages={1693--1701},\n  year={2015}\n}\n""""""\n\n_DL_URLS = {\n    # pylint: disable=line-too-long\n    ""cnn_stories"": ""https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ"",\n    ""dm_stories"": ""https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs"",\n    ""test_urls"": ""https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_test.txt"",\n    ""train_urls"": ""https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt"",\n    ""val_urls"": ""https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_val.txt"",\n    # pylint: enable=line-too-long\n}\n\n_HIGHLIGHTS = ""highlights""\n_ARTICLE = ""article""\n\n_SUPPORTED_VERSIONS = [\n    # Using cased version.\n    nlp.Version(""3.0.0"", ""Using cased version.""),\n    # Same data as 0.0.2\n    nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n    # Having the model predict newline separators makes it easier to evaluate\n    # using summary-level ROUGE.\n    nlp.Version(""2.0.0"", ""Separate target sentences with newline.""),\n]\n\n\n_DEFAULT_VERSION = nlp.Version(""3.0.0"", ""Using cased version."")\n\n\nclass CnnDailymailConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for CnnDailymail.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for CnnDailymail.\n\n    Args:\n\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(CnnDailymailConfig, self).__init__(**kwargs)\n\n\ndef _get_url_hashes(path):\n    """"""Get hashes of urls in file.""""""\n    urls = _read_text_file(path)\n\n    def url_hash(u):\n        h = hashlib.sha1()\n        try:\n            u = u.encode(""utf-8"")\n        except UnicodeDecodeError:\n            logging.error(""Cannot hash url: %s"", u)\n        h.update(u)\n        return h.hexdigest()\n\n    return {url_hash(u): True for u in urls}\n\n\ndef _find_files(dl_paths, publisher, url_dict):\n    """"""Find files corresponding to urls.""""""\n    if publisher == ""cnn"":\n        top_dir = os.path.join(dl_paths[""cnn_stories""], ""cnn"", ""stories"")\n    elif publisher == ""dm"":\n        top_dir = os.path.join(dl_paths[""dm_stories""], ""dailymail"", ""stories"")\n    else:\n        logging.fatal(""Unsupported publisher: %s"", publisher)\n    files = sorted(os.listdir(top_dir))\n\n    ret_files = []\n    for p in files:\n        basename = os.path.basename(p)\n        if basename[0 : basename.find("".story"")] in url_dict:\n            ret_files.append(os.path.join(top_dir, p))\n    return ret_files\n\n\ndef _subset_filenames(dl_paths, split):\n    """"""Get filenames for a particular split.""""""\n    assert isinstance(dl_paths, dict), dl_paths\n    # Get filenames for a split.\n    if split == nlp.Split.TRAIN:\n        urls = _get_url_hashes(dl_paths[""train_urls""])\n    elif split == nlp.Split.VALIDATION:\n        urls = _get_url_hashes(dl_paths[""val_urls""])\n    elif split == nlp.Split.TEST:\n        urls = _get_url_hashes(dl_paths[""test_urls""])\n    else:\n        logging.fatal(""Unsupported split: %s"", split)\n    cnn = _find_files(dl_paths, ""cnn"", urls)\n    dm = _find_files(dl_paths, ""dm"", urls)\n    return cnn + dm\n\n\nDM_SINGLE_CLOSE_QUOTE = ""\\u2019""  # unicode\nDM_DOUBLE_CLOSE_QUOTE = ""\\u201d""\n# acceptable ways to end a sentence\nEND_TOKENS = [""."", ""!"", ""?"", ""..."", ""\'"", ""`"", \'""\', DM_SINGLE_CLOSE_QUOTE, DM_DOUBLE_CLOSE_QUOTE, "")""]\n\n\ndef _read_text_file(text_file):\n    lines = []\n    with open(text_file, ""r"") as f:\n        for line in f:\n            lines.append(line.strip())\n    return lines\n\n\ndef _get_art_abs(story_file, tfds_version):\n    """"""Get abstract (highlights) and article from a story file path.""""""\n    # Based on https://github.com/abisee/cnn-dailymail/blob/master/\n    #     make_datafiles.py\n\n    lines = _read_text_file(story_file)\n\n    # The github code lowercase the text and we removed it in 3.0.0.\n\n    # Put periods on the ends of lines that are missing them\n    # (this is a problem in the dataset because many image captions don\'t end in\n    # periods; consequently they end up in the body of the article as run-on\n    # sentences)\n    def fix_missing_period(line):\n        """"""Adds a period to a line that is missing a period.""""""\n        if ""@highlight"" in line:\n            return line\n        if not line:\n            return line\n        if line[-1] in END_TOKENS:\n            return line\n        return line + "" .""\n\n    lines = [fix_missing_period(line) for line in lines]\n\n    # Separate out article and abstract sentences\n    article_lines = []\n    highlights = []\n    next_is_highlight = False\n    for line in lines:\n        if not line:\n            continue  # empty line\n        elif line.startswith(""@highlight""):\n            next_is_highlight = True\n        elif next_is_highlight:\n            highlights.append(line)\n        else:\n            article_lines.append(line)\n\n    # Make article into a single string\n    article = "" "".join(article_lines)\n\n    if tfds_version >= ""2.0.0"":\n        abstract = ""\\n"".join(highlights)\n    else:\n        abstract = "" "".join(highlights)\n\n    return article, abstract\n\n\nclass CnnDailymail(nlp.GeneratorBasedBuilder):\n    """"""CNN/DailyMail non-anonymized summarization dataset.""""""\n\n    BUILDER_CONFIGS = [\n        CnnDailymailConfig(name=str(version), description=""Plain text"", version=version)\n        for version in _SUPPORTED_VERSIONS\n    ]\n\n    def _info(self):\n        # Should return a nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({_ARTICLE: nlp.Value(""string""), _HIGHLIGHTS: nlp.Value(""string"")}),\n            supervised_keys=None,\n            homepage=""https://github.com/abisee/cnn-dailymail"",\n            citation=_CITATION,\n        )\n\n    def _vocab_text_gen(self, paths):\n        for _, ex in self._generate_examples(paths):\n            yield "" "".join([ex[_ARTICLE], ex[_HIGHLIGHTS]])\n\n    def _split_generators(self, dl_manager):\n        dl_paths = dl_manager.download_and_extract(_DL_URLS)\n        train_files = _subset_filenames(dl_paths, nlp.Split.TRAIN)\n        # Generate shared vocabulary\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""files"": train_files}),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""files"": _subset_filenames(dl_paths, nlp.Split.VALIDATION)}\n            ),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""files"": _subset_filenames(dl_paths, nlp.Split.TEST)}),\n        ]\n\n    def _generate_examples(self, files):\n        for p in files:\n            article, highlights = _get_art_abs(p, self.config.version)\n            if not article or not highlights:\n                continue\n            fname = os.path.basename(p)\n            yield fname, {_ARTICLE: article, _HIGHLIGHTS: highlights}\n'"
datasets/coarse_discourse/coarse_discourse.py,0,"b'""""""TODO(coarse_discourse): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(coarse_discourse): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{coarsediscourse, title={Characterizing Online Discussion Using Coarse Discourse Sequences}, author={Zhang, Amy X. and Culbertson, Bryan and Paritosh, Praveen}, booktitle={Proceedings of the 11th International AAAI Conference on Weblogs and Social Media}, series={ICWSM \'17}, year={2017}, location = {Montreal, Canada} }\n""""""\n\n# TODO(coarse_discourse):\n_DESCRIPTION = """"""\\\ndataset contains discourse annotation and relation on threads from reddit during 2016\n""""""\n_URL = ""https://github.com/google-research-datasets/coarse-discourse/archive/master.zip""\n\n\nclass CoarseDiscourse(nlp.GeneratorBasedBuilder):\n    """"""TODO(coarse_discourse): Short description of my dataset.""""""\n\n    # TODO(coarse_discourse): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(coarse_discourse): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""title"": nlp.Value(""string""),\n                    ""is_self_post"": nlp.Value(""bool""),\n                    ""subreddit"": nlp.Value(""string""),\n                    ""url"": nlp.Value(""string""),\n                    ""majority_link"": nlp.Value(""string""),\n                    ""is_first_post"": nlp.Value(""bool""),\n                    ""majority_type"": nlp.Value(""string""),\n                    ""id_post"": nlp.Value(""string""),\n                    ""post_depth"": nlp.Value(""int32""),\n                    ""in_reply_to"": nlp.Value(""string""),\n                    ""annotations"": nlp.features.Sequence(\n                        {\n                            ""annotator"": nlp.Value(""string""),\n                            ""link_to_post"": nlp.Value(""string""),\n                            ""main_type"": nlp.Value(""string""),\n                        }\n                    ),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/google-research-datasets/coarse-discourse"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(coarse_discourse): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    ""filepath"": os.path.join(dl_dir, ""coarse-discourse-master"", ""coarse_discourse_dataset.json"")\n                },\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(coarse_discourse): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n                url = data.get(""url"", """")\n                is_self_post = data.get(""is_self_post"", """")\n                subreddit = data.get(""subreddit"", """")\n                title = data.get(""title"", """")\n                posts = data.get(""posts"", """")\n                for id1, post in enumerate(posts):\n                    maj_link = post.get(""majority_link"", """")\n                    maj_type = post.get(""majority_type"", """")\n                    id_post = post.get(""id"", """")\n                    is_first_post = post.get(""is_firs_post"", """")\n                    post_depth = post.get(""post_depth"", -1)\n                    in_reply_to = post.get(""in_reply_to"", """")\n                    annotations = post[""annotations""]\n                    annotators = [annotation.get(""annotator"", """") for annotation in annotations]\n                    main_types = [annotation.get(""main_type"", """") for annotation in annotations]\n                    link_posts = [annotation.get(""linkk_to_post"", """") for annotation in annotations]\n\n                    yield str(id_) + ""_"" + str(id1), {\n                        ""title"": title,\n                        ""is_self_post"": is_self_post,\n                        ""subreddit"": subreddit,\n                        ""url"": url,\n                        ""majority_link"": maj_link,\n                        ""is_first_post"": is_first_post,\n                        ""majority_type"": maj_type,\n                        ""id_post"": id_post,\n                        ""post_depth"": post_depth,\n                        ""in_reply_to"": in_reply_to,\n                        ""annotations"": {""annotator"": annotators, ""link_to_post"": link_posts, ""main_type"": main_types},\n                    }\n'"
datasets/com_qa/com_qa.py,0,"b'""""""TODO(com_qa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(com_qa): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{abujabal-etal-2019-comqa,\n    title = ""{C}om{QA}: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters"",\n    author = ""Abujabal, Abdalghani  and\n      Saha Roy, Rishiraj  and\n      Yahya, Mohamed  and\n      Weikum, Gerhard"",\n    booktitle = ""Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"",\n    month = jun,\n    year = ""2019"",\n    address = ""Minneapolis, Minnesota"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/N19-1027"",\n    doi = ""10.18653/v1/N19-1027"",\n    pages = ""307--317"",\n    }\n""""""\n\n# TODO(com_qa):\n_DESCRIPTION = """"""\\\nComQA is a dataset of 11,214 questions, which were collected from WikiAnswers, a community question answering website. \nBy collecting questions from such a site we ensure that the information needs are ones of interest to actual users. \nMoreover, questions posed there are often cannot be answered by commercial search engines or QA technology, making them \nmore interesting for driving future research compared to those collected from an engine\'s query log. The dataset contains \nquestions with various challenging phenomena such as the need for temporal reasoning, comparison (e.g., comparatives, \nsuperlatives, ordinals), compositionality (multiple, possibly nested, subquestions with multiple entities), and \nunanswerable questions (e.g., Who was the first human being on Mars?). Through a large crowdsourcing effort, questions \nin ComQA are grouped into 4,834 paraphrase clusters that express the same information need. Each cluster is annotated \nwith its answer(s). ComQA answers come in the form of Wikipedia entities wherever possible. Wherever the answers are \ntemporal or measurable quantities, TIMEX3 and the International System of Units (SI) are used for normalization.\n""""""\n_URL = ""https://qa.mpi-inf.mpg.de/comqa""\n_TRAIN_FILE = ""comqa_train.json""\n_DEV_FILE = ""comqa_dev.json""\n_TEST_FILE = ""comqa_test.json""\n\n\nclass ComQa(nlp.GeneratorBasedBuilder):\n    """"""TODO(com_qa): Short description of my dataset.""""""\n\n    # TODO(com_qa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(com_qa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""cluster_id"": nlp.Value(""string""),\n                    ""questions"": nlp.features.Sequence({""question"": nlp.Value(""string"")}),\n                    ""answers"": nlp.features.Sequence({""answer"": nlp.Value(""string"")}),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""http://qa.mpi-inf.mpg.de/comqa/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(com_qa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {\n            ""train"": os.path.join(_URL, _TRAIN_FILE),\n            ""dev"": os.path.join(_URL, _DEV_FILE),\n            ""test"": os.path.join(_URL, _TEST_FILE),\n        }\n        dl_dir = dl_manager.download_and_extract(urls_to_download)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""train""], ""split"": ""train""},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""test""], ""split"": ""test""},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""dev""], ""split"": ""dev""},\n            ),\n        ]\n\n    def _generate_examples(self, filepath, split):\n        """"""Yields examples.""""""\n        # TODO(com_qa): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for id_, example in enumerate(data):\n                questions = []\n                if split == ""test"":\n                    cluster_id = str(example[""id""])\n                    questions.append(example[""question""])\n                else:\n                    cluster_id = example[""cluster_id""]\n                    questions = example[""questions""]\n                answers = example[""answers""]\n                yield id_, {\n                    ""cluster_id"": cluster_id,\n                    ""questions"": {""question"": questions},\n                    ""answers"": {""answer"": answers},\n                }\n'"
datasets/commonsense_qa/commonsense_qa.py,0,"b'""""""TODO(commonsense_qa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(commonsense_qa): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{commonsense_QA,\ntitle={COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge},\nauthor={Alon, Talmor and Jonathan, Herzig and Nicholas, Lourie and Jonathan ,Berant},\njournal={arXiv preprint arXiv:1811.00937v2},\nyear={2019}\n\n""""""\n\n# TODO(commonsense_qa):\n_DESCRIPTION = """"""\\\nCommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge\n to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.\n The dataset is provided in two major training/validation/testing set splits: ""Random split"" which is the main evaluation\n  split, and ""Question token split"", see paper for details.\n""""""\n_URL = ""https://s3.amazonaws.com/commensenseqa""\n_TRAINING_FILE = ""train_rand_split.jsonl""\n_DEV_FILE = ""dev_rand_split.jsonl""\n_TEST_FILE = ""test_rand_split_no_answers.jsonl""\n\n\nclass CommonsenseQa(nlp.GeneratorBasedBuilder):\n    """"""TODO(commonsense_qa): Short description of my dataset.""""""\n\n    # TODO(commonsense_qa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # These are the features of your dataset like images, labels ...\n        features = nlp.Features(\n            {\n                ""answerKey"": nlp.Value(""string""),\n                ""question"": nlp.Value(""string""),\n                ""choices"": nlp.features.Sequence({""label"": nlp.Value(""string""), ""text"": nlp.Value(""string""),}),\n            }\n        )\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=features,\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://www.tau-nlp.org/commonsenseqa"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n\n        download_urls = {\n            ""train"": os.path.join(_URL, _TRAINING_FILE),\n            ""test"": os.path.join(_URL, _TEST_FILE),\n            ""dev"": os.path.join(_URL, _DEV_FILE),\n        }\n\n        downloaded_files = dl_manager.download_and_extract(download_urls)\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""], ""split"": ""train""}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""], ""split"": ""dev"",}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST, gen_kwargs={""filepath"": downloaded_files[""test""], ""split"": ""test"",}\n            ),\n        ]\n\n    def _generate_examples(self, filepath, split):\n        """"""Yields examples.""""""\n        # TODO(commonsense_qa): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n                question = data[""question""]\n                choices = question[""choices""]\n                labels = [label[""label""] for label in choices]\n                texts = [text[""text""] for text in choices]\n                stem = question[""stem""]\n                if split == ""test"":\n                    answerkey = """"\n                else:\n                    answerkey = data[""answerKey""]\n\n                yield id_, {\n                    ""answerKey"": answerkey,\n                    ""question"": stem,\n                    ""choices"": {""label"": labels, ""text"": texts},\n                }\n'"
datasets/coqa/coqa.py,0,"b'""""""TODO(coqa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(coqa): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{SivaAndAl:Coca,\n       author = {Siva, Reddy and Danqi, Chen and  Christopher D., Manning},\n        title = ""{WikiQA: A Challenge Dataset for Open-Domain Question Answering}"",\n      journal = { arXiv},\n         year = 2018,\n\n}\n""""""\n\n# TODO(coqa):\n_DESCRIPTION = """"""\\\nCoQA: A Conversational Question Answering Challenge\n""""""\n\n_TRAIN_DATA_URL = ""https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json""\n_DEV_DATA_URL = ""https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json""\n\n#\n# class CoqaConfig(nlp.BuilderConfig):\n#   """"""BuilderConfig for Coqa.""""""\n#\n#   def __init__(self,\n#                story,\n#                source,\n#                questions,\n#                answers,\n#                citation,\n#                description,\n#                additional_answers=None,\n#                **kwargs):\n#     """"""BuilderConfig for Coca.\n#\n#     Args:\n#       story: `text`,  context\n#       source: `text`, source of the story\n#       questions `Sequence` set of questions\n#       answers: `Sequence` set of answers to the questions\n#       data_url: `string`, url to download the  file from\n#       citation: `string`, citation for the data set\n#      additional_answers: `Sequence`, in the dev set questions have also set of additional answers\n#       **kwargs: keyword arguments forwarded to super.\n#     """"""\n#     super(CoqaConfig, self).__init__(\n#         version=nlp.Version(\n#             ""1.0.0"",\n#             ""New split API (https://tensorflow.org/datasets/splits)""),\n#         **kwargs)\n#     self.story = story\n#     self.source = source\n#     self.questions = questions\n#     self.answers = answers\n#     self.additional_answers = additional_answers\n#     self.citation = citation\n#     self.description = description\n\n\nclass Coqa(nlp.GeneratorBasedBuilder):\n    """"""TODO(coqa): Short description of my dataset.""""""\n\n    # TODO(coqa): Set up version.\n    VERSION = nlp.Version(""1.0.0"")\n    # BUILDER_CONFIGS = CoqaConfig(\n    #     story= \'story\',\n    #     source=\'source\',\n    #     questions=\'questions\',\n    #     answers=\'answers\',\n    #     additional_answers=\'additional_answers\',\n    #     description= _DESCRIPTION,\n    #     citation= _CITATION\n    #\n    # )\n    def _info(self):\n        # TODO(coqa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""source"": nlp.Value(""string""),\n                    ""story"": nlp.Value(""string""),\n                    ""questions"": nlp.features.Sequence({""input_text"": nlp.Value(""string""),}),\n                    ""answers"": nlp.features.Sequence(\n                        {\n                            ""input_text"": nlp.Value(""string""),\n                            ""answer_start"": nlp.Value(""int32""),\n                            ""answer_end"": nlp.Value(""int32""),\n                        }\n                    ),\n                    # ##the foloowing feature allows to take into account additional answers in the validation set\n                    # \'additional_answers\': nlp.features.Sequence({\n                    #         ""input_texts"": nlp.Value(\'int32\'),\n                    #         ""answers_start"": nlp.Value(\'int32\'),\n                    #         ""answers_end"": nlp.Value(\'int32\')\n                    #     }),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://stanfordnlp.github.io/coqa/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(coqa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {""train"": _TRAIN_DATA_URL, ""dev"": _DEV_DATA_URL}\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""], ""split"": ""train""}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""], ""split"": ""validation""}\n            ),\n        ]\n\n    def _generate_examples(self, filepath, split):\n        """"""Yields examples.""""""\n        # TODO(coqa): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for row in data[""data""]:\n                questions = [question[""input_text""] for question in row[""questions""]]\n                story = row[""story""]\n                source = row[""source""]\n                answers_start = [answer[""span_start""] for answer in row[""answers""]]\n                answers_end = [answer[""span_end""] for answer in row[""answers""]]\n                answers = [answer[""input_text""] for answer in row[""answers""]]\n                # add_answers = row[\'additional_answers\']\n                # add_input_tests = []\n                # add_start_answers = []\n                # add_end_answers = []\n                # for key in add_answers:\n                #     add_answers_key = add_answers[key]\n                #     add_input_tests.append([add_answer[\'input_text\'] for add_answer in add_answers_key])\n                #     add_start_answers.append([add_answer[\'span_start\'] for add_answer in add_answers_key])\n                #     add_end_answers.append([add_answer[\'span_end\'] for add_answer in add_answers_key])\n                yield row[""id""], {\n                    ""source"": source,\n                    ""story"": story,\n                    ""questions"": {""input_text"": questions,},\n                    ""answers"": {""input_text"": answers, ""answer_start"": answers_start, ""answer_end"": answers_end}\n                    # \'additional_answers\': {\n                    #     ""input_texts"": add_input_tests ,\n                    #     ""answers_start"": add_start_answers,\n                    #     ""answers_end"": add_end_answers,\n                    # }\n                }\n'"
datasets/cornell_movie_dialog/cornell_movie_dialog.py,0,"b'""""""TODO(cornell_movie_dialog): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport ast\nimport csv\nimport os\n\nimport nlp\n\n\n# TODO(cornell_movie_dialog): BibTeX citation\n_CITATION = """"""\\\n  @InProceedings{Danescu-Niculescu-Mizil+Lee:11a,\n\n  author={Cristian Danescu-Niculescu-Mizil and Lillian Lee},\n\n  title={Chameleons in imagined conversations: \n  A new approach to understanding coordination of linguistic style in dialogs.},\n\n  booktitle={Proceedings of the \n\n        Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011},\n\n  year={2011}\n\n}\n""""""\n\n# TODO(cornell_movie_dialog):\n_DESCRIPTION = """"""\\\n     \nThis corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:\n- 220,579 conversational exchanges between 10,292 pairs of movie characters\n- involves 9,035 characters from 617 movies\n- in total 304,713 utterances\n- movie metadata included:\n    - genres\n    - release year\n    - IMDB rating\n    - number of IMDB votes\n    - IMDB rating\n- character metadata included:\n    - gender (for 3,774 characters)\n    - position on movie credits (3,321 characters)\n""""""\n\n_URL = ""https://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip""\n\n\nclass CornellMovieDialog(nlp.GeneratorBasedBuilder):\n    """"""TODO(cornell_movie_dialog): Short description of my dataset.""""""\n\n    # TODO(cornell_movie_dialog): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(cornell_movie_dialog): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""movieID"": nlp.Value(""string""),\n                    ""movieTitle"": nlp.Value(""string""),\n                    ""movieYear"": nlp.Value(""string""),\n                    ""movieIMDBRating"": nlp.Value(""string""),\n                    ""movieNoIMDBVotes"": nlp.Value(""string""),\n                    ""movieGenres"": nlp.features.Sequence({""genre"": nlp.Value(""string"")}),\n                    ""characterID1"": nlp.Value(""string""),\n                    ""characterID2"": nlp.Value(""string""),\n                    ""characterName1"": nlp.Value(""string""),\n                    ""characterName2"": nlp.Value(""string""),\n                    ""utterance"": nlp.features.Sequence({""text"": nlp.Value(""string""), ""LineID"": nlp.Value(""string"")})\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(cornell_movie_dialog): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepaths"": os.path.join(dl_dir, ""cornell movie-dialogs corpus"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepaths):\n        """"""Yields examples.""""""\n        # TODO(cornell_movie_dialog): Yields (key, example) tuples from the dataset\n        movie_char_file = os.path.join(filepaths, ""movie_characters_metadata.txt"")\n        movie_conv_file = os.path.join(filepaths, ""movie_conversations.txt"")\n        movie_lines_file = os.path.join(filepaths, ""movie_lines.txt"")\n        movie_titles_file = os.path.join(filepaths, ""movie_titles_metadata.txt"")\n\n        with open(movie_char_file, ""rb"") as f:\n            movie_char_data = [x.decode(""latin"").split(""+++$+++"") for x in f.readlines()]\n\n        with open(movie_conv_file, ""rb"") as f:\n            movie_conv_data = [x.decode(""latin"").split(""+++$+++"") for x in f.readlines()]\n\n        with open(movie_lines_file, ""rb"") as f:\n            movie_lines_data = [x.decode(""latin"").split(""+++$+++"") for x in f.readlines()]\n\n        with open(movie_titles_file, ""rb"") as f:\n            movie_titles_data = [x.decode(""latin"").split(""+++$+++"") for x in f.readlines()]\n        ## looping over movie conversation file\n        for id_, conv in enumerate(movie_conv_data):\n            char_id_1 = conv[0]\n            char_id_2 = conv[1]\n            movie_id = conv[2]\n            line_ids = conv[-1].replace(""\\n"", """")\n            line_ids = ast.literal_eval(line_ids.strip())\n            lines_texts = []\n            ## searching text corresponding to each lineID in line_ids in movie lines file\n            for line_id in line_ids:\n                i = 0\n                while i < len(movie_lines_data) and movie_lines_data[i][0].strip() != line_id:\n                    i += 1\n                lines_texts.append(movie_lines_data[i][0])  # if i < len(movie_lines_data) else \'\')\n            ## look for char names in movie character file\n            j = 0\n            while j < len(movie_char_data) and movie_char_data[j][0].strip() != char_id_1.strip():\n                j += 1\n            char_name_1 = movie_char_data[j][1]  # if j < len(movie_char_data) else \'\'\n            movie_title = movie_char_data[j][3]  # if j < len(movie_char_data) else \'\'\n\n            k = 0\n            while k < len(movie_char_data) and movie_char_data[k][0].strip() != char_id_2.strip():\n                k += 1\n            char_name_2 = movie_char_data[k][1]\n\n            ##look for movie year, IMDBRating, genre, no_imdb_voting in movie tiles file\n            l = 0\n            while l < len(movie_titles_data) and movie_titles_data[l][0].strip() != movie_id.strip():\n                l += 1\n            movie_year = movie_titles_data[l][2]\n            imdb_rating = movie_titles_data[l][3]\n            no_imdb_vote = movie_titles_data[l][4]\n            genre = movie_titles_data[l][5].replace(""\\n"", """").strip()\n            movie_genres = ast.literal_eval(genre)\n\n            yield id_, {\n                ""movieID"": movie_id,\n                ""movieTitle"": movie_title,\n                ""movieYear"": movie_year,\n                ""movieIMDBRating"": imdb_rating,\n                ""movieNoIMDBVotes"": no_imdb_vote,\n                ""movieGenres"": {""genre"": movie_genres},\n                ""characterID1"": char_id_1,\n                ""characterID2"": char_id_2,\n                ""characterName1"": char_name_1,\n                ""characterName2"": char_name_2,\n                ""utterance"": {""text"": lines_texts, ""LineID"": line_ids},\n            }\n'"
datasets/cos_e/cos_e.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Commonsense Explanations (CoS-E) Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@inproceedings{rajani2019explain,\n     title = ""Explain Yourself! Leveraging Language models for Commonsense Reasoning"",\n    author = ""Rajani, Nazneen Fatema  and\n      McCann, Bryan  and\n      Xiong, Caiming  and\n      Socher, Richard"",\n      year=""2019"",\n    booktitle = ""Proceedings of the 2019 Conference of the Association for Computational Linguistics (ACL2019)"",\n    url =""https://arxiv.org/abs/1906.02361""\n}\n""""""\n\n_DESCRIPTION = """"""\nCommon Sense Explanations (CoS-E) allows for training language models to\nautomatically generate explanations that can be used during training and\ninference in a novel Commonsense Auto-Generated Explanation (CAGE) framework.\n""""""\n\n_COS_E_URL = ""https://raw.githubusercontent.com/salesforce/cos-e/master/data/""\n\n# COS E has explanations for the CQA dataset, which is joined by ID.\n_CQA_URL_TRAIN = ""https://s3.amazonaws.com/commensenseqa/train_rand_split.jsonl""\n_CQA_URL_DEV = ""https://s3.amazonaws.com/commensenseqa/dev_rand_split.jsonl""\n_CQA_URL_TEST = ""https://s3.amazonaws.com/commensenseqa/test_rand_split_no_answers.jsonl""\n\n\ndef _download_and_index_cqa(dl_manager):\n    """"""Downloads CQA and returns it, indexed by id, for joining with Cos-E.""""""\n\n    downloaded_files = dl_manager.download_and_extract(\n        {""cqa_train"": _CQA_URL_TRAIN, ""cqa_dev"": _CQA_URL_DEV, ""cqa_test"": _CQA_URL_TEST}\n    )\n\n    # NB: ""cqa_test"" is included in the files, but not in any of the CoS-E splits.\n    cqa_splits = [""cqa_train"", ""cqa_dev""]\n    cqa_complete = []\n    for split in cqa_splits:\n        with open(downloaded_files[split]) as f:\n            for _, line in enumerate(f):\n                d = json.loads(line)\n                cqa_complete.append(d)\n\n        # Index the CQA dataset by id for joining with Cos-E.\n        cqa_indexed = {}\n    for d in cqa_complete:\n        cqa_indexed[d[""id""]] = d\n    return cqa_indexed\n\n\ndef _get_choices_and_answer(cqa):\n    """"""Returns choices and the answer from a cqa example.""""""\n    choices = []\n    answer_key = cqa[""answerKey""]\n    answer = None\n    for choice in cqa[""question""][""choices""]:\n        choices.append(choice[""text""])\n        if answer_key == choice[""label""]:\n            answer = choice[""text""]\n    return choices, answer\n\n\nclass CosE(nlp.GeneratorBasedBuilder):\n    """"""CoS-E: Common Sense Explanations corpus.""""""\n\n    VERSION = nlp.Version(""0.0.1"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""choices"": nlp.features.Sequence(nlp.Value(""string"")),\n                    ""answer"": nlp.Value(""string""),\n                    ""abstractive_explanation"": nlp.Value(""string""),\n                    ""extractive_explanation"": nlp.Value(""string""),\n                }\n            ),\n            supervised_keys=None,\n            homepage=""https://github.com/salesforce/cos-e"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n\n        # NB: The CQA Dataset should be read only once, and only by callers who\n        # want to _create_ the Cos-E dataset from scratch.\n        cqa_indexed = _download_and_index_cqa(dl_manager)\n\n        files = dl_manager.download_and_extract(\n            {\n                ""dev"": [os.path.join(_COS_E_URL, ""v1.11/dev/cose_dev_v1.11_processed.jsonl"")],\n                ""train"": [os.path.join(_COS_E_URL, ""v1.11/train/cose_train_v1.11_processed.jsonl"")],\n            }\n        )\n\n        # We use the CoS-E/CQA dev set as our validation set.\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""files"": files[""dev""], ""cqa_indexed"": cqa_indexed},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""files"": files[""train""], ""cqa_indexed"": cqa_indexed},\n            ),\n        ]\n\n    def _generate_examples(self, files, **kwargs):\n        """"""Yields examples.""""""\n        cqa_indexed = kwargs[""cqa_indexed""]\n        for filepath in files:\n            with open(filepath) as f:\n                for line in f:\n                    cos = json.loads(line)\n                    cqa = cqa_indexed[cos[""id""]]\n                    choices, answer = _get_choices_and_answer(cqa)\n                    yield cos[""id""], {\n                        ""id"": cos[""id""],\n                        ""question"": cqa[""question""][""stem""],\n                        ""choices"": choices,\n                        ""answer"": answer,\n                        ""abstractive_explanation"": cos[""explanation""][""open-ended""],\n                        ""extractive_explanation"": cos[""explanation""][""selected""],\n                    }\n'"
datasets/cosmos_qa/cosmos_qa.py,0,"b'""""""TODO(cosmos_qa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(cosmos_qa): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{cosmos,\n    title={COSMOS QA: Machine Reading Comprehension\n    with Contextual Commonsense Reasoning},\n    author={Lifu Huang and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},\n    booktitle ={arXiv:1909.00277v2},\n    year={2019}\n}\n""""""\n\n# TODO(cosmos_qa):\n_DESCRIPTION = """"""\\\nCosmos QA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people\'s everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context\n""""""\n_URL = ""https://github.com/wilburOne/cosmosqa/raw/master/data/""\n_TEST_FILE = ""test.jsonl""\n_TRAIN_FILE = ""train.csv""\n_DEV_FILE = ""valid.csv""\n\n\nclass CosmosQa(nlp.GeneratorBasedBuilder):\n    """"""TODO(cosmos_qa): Short description of my dataset.""""""\n\n    # TODO(cosmos_qa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(cosmos_qa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answer0"": nlp.Value(""string""),\n                    ""answer1"": nlp.Value(""string""),\n                    ""answer2"": nlp.Value(""string""),\n                    ""answer3"": nlp.Value(""string""),\n                    ""label"": nlp.Value(""int32"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://wilburone.github.io/cosmos/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(cosmos_qa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {\n            ""train"": os.path.join(_URL, _TRAIN_FILE),\n            ""test"": os.path.join(_URL, _TEST_FILE),\n            ""dev"": os.path.join(_URL, _DEV_FILE),\n        }\n        dl_dir = dl_manager.download_and_extract(urls_to_download)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""train""], ""split"": ""train""},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""test""], ""split"": ""test""},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""dev""], ""split"": ""dev""},\n            ),\n        ]\n\n    def _generate_examples(self, filepath, split):\n        """"""Yields examples.""""""\n        # TODO(cosmos_qa): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            if split == ""test"":\n                for id_, row in enumerate(f):\n                    data = json.loads(row)\n                    yield id_, {\n                        ""id"": data[""id""],\n                        ""context"": data[""context""],\n                        ""question"": data[""question""],\n                        ""answer0"": data[""answer0""],\n                        ""answer1"": data[""answer1""],\n                        ""answer2"": data[""answer2""],\n                        ""answer3"": data[""answer3""],\n                        ""label"": int(data.get(""label"", -1)),\n                    }\n            else:\n                data = csv.DictReader(f)\n                for id_, row in enumerate(data):\n                    yield id_, {\n                        ""id"": row[""id""],\n                        ""context"": row[""context""],\n                        ""question"": row[""question""],\n                        ""answer0"": row[""answer0""],\n                        ""answer1"": row[""answer1""],\n                        ""answer2"": row[""answer2""],\n                        ""answer3"": row[""answer3""],\n                        ""label"": int(row.get(""label"", -1)),\n                    }\n'"
datasets/crime_and_punish/crime_and_punish.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\n\n""""""\n_URL = ""https://www.gutenberg.org/files/2554/2554-h/2554-h.htm""\n_DATA_URL = ""https://raw.githubusercontent.com/patrickvonplaten/datasets/master/crime_and_punishment.txt""\n\n\nclass CrimeAndPunishConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Crime and Punish.""""""\n\n    def __init__(self, data_url, **kwargs):\n        """"""BuilderConfig for BlogAuthorship\n\n        Args:\n          data_url: `string`, url to the dataset (word or raw level)\n          **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(CrimeAndPunishConfig, self).__init__(version=nlp.Version(""1.0.0"",), **kwargs)\n        self.data_url = data_url\n\n\nclass CrimeAndPunish(nlp.GeneratorBasedBuilder):\n\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        CrimeAndPunishConfig(\n            name=""crime-and-punish"",\n            data_url=_DATA_URL,\n            description=""word level dataset. No processing is needed other than replacing newlines with <eos> tokens."",\n        ),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features({""line"": nlp.Value(""string""),}),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            homepage=_URL,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n\n        if self.config.name == ""crime-and-punish"":\n            data = dl_manager.download_and_extract(self.config.data_url)\n\n            return [\n                nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""data_file"": data, ""split"": ""train""},),\n            ]\n        else:\n            raise ValueError(""{} does not exist"".format(self.config.name))\n\n    def _generate_examples(self, data_file, split):\n\n        with open(data_file, ""rb"") as f:\n            id_counter = 0\n            add_text = False\n            crime_and_punishment_occ_counter = 0\n\n            for line in f:\n                line = line.decode(""UTF-8"")\n                if ""CRIME AND PUNISHMENT"" in line:\n                    crime_and_punishment_occ_counter += 1\n                    add_text = crime_and_punishment_occ_counter == 3\n                if ""End of Project"" in line:\n                    add_text = False\n\n                if add_text is True:\n                    result = {""line"": line}\n                    id_counter += 1\n                    yield id_counter, result\n'"
datasets/csv/csv.py,0,"b'# coding=utf-8\n\nfrom dataclasses import dataclass\n\nimport pyarrow.csv as pac\n\nimport nlp\n\n\n@dataclass\nclass CsvConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for CSV.""""""\n\n    skip_rows: int = 0\n    header_as_column_names: bool = True\n    delimiter: str = "",""\n    quote_char: str = \'""\'\n    read_options: pac.ReadOptions = None\n    parse_options: pac.ParseOptions = None\n    convert_options: pac.ConvertOptions = None\n\n    @property\n    def pa_read_options(self):\n        read_options = self.read_options or pac.ReadOptions()\n        read_options.skip_rows = self.skip_rows\n        read_options.autogenerate_column_names = not self.header_as_column_names\n        return read_options\n\n    @property\n    def pa_parse_options(self):\n        parse_options = self.parse_options or pac.ParseOptions()\n        parse_options.delimiter = self.delimiter\n        parse_options.quote_char = self.quote_char\n        return parse_options\n\n    @property\n    def pa_convert_options(self):\n        convert_options = self.convert_options or pac.ConvertOptions()\n        return convert_options\n\n\nclass Csv(nlp.ArrowBasedBuilder):\n    BUILDER_CONFIG_CLASS = CsvConfig\n\n    def _info(self):\n        return nlp.DatasetInfo()\n\n    def _split_generators(self, dl_manager):\n        """""" We handle string, list and dicts in datafiles\n        """"""\n        if isinstance(self.config.data_files, (str, list, tuple)):\n            files = self.config.data_files\n            if isinstance(files, str):\n                files = [files]\n            return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""files"": files})]\n        splits = []\n        for split_name in [nlp.Split.TRAIN, nlp.Split.VALIDATION, nlp.Split.TEST]:\n            if split_name in self.config.data_files:\n                files = self.config.data_files[split_name]\n                if isinstance(files, str):\n                    files = [files]\n                splits.append(nlp.SplitGenerator(name=split_name, gen_kwargs={""files"": files}))\n        return splits\n\n    def _generate_tables(self, files):\n        for i, file in enumerate(files):\n            pa_table = pac.read_csv(\n                file,\n                read_options=self.config.pa_read_options,\n                parse_options=self.config.pa_parse_options,\n                convert_options=self.config.convert_options,\n            )\n            yield i, pa_table\n'"
datasets/definite_pronoun_resolution/definite_pronoun_resolution.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The Definite Pronoun Resolution Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport nlp\n\n\n_CITATION = """"""\\\n@inproceedings{rahman2012resolving,\n  title={Resolving complex cases of definite pronouns: the winograd schema challenge},\n  author={Rahman, Altaf and Ng, Vincent},\n  booktitle={Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},\n  pages={777--789},\n  year={2012},\n  organization={Association for Computational Linguistics}\n}""""""\n\n_DESCRIPTION = """"""\\\nComposed by 30 students from one of the author\'s undergraduate classes. These\nsentence pairs cover topics ranging from real events (e.g., Iran\'s plan to\nattack the Saudi ambassador to the U.S.) to events/characters in movies (e.g.,\nBatman) and purely imaginary situations, largely reflecting the pop culture as\nperceived by the American kids born in the early 90s. Each annotated example\nspans four lines: the first line contains the sentence, the second line contains\nthe target pronoun, the third line contains the two candidate antecedents, and\nthe fourth line contains the correct antecedent. If the target pronoun appears\nmore than once in the sentence, its first occurrence is the one to be resolved.\n""""""\n\n_DATA_URL_PATTERN = ""http://www.hlt.utdallas.edu/~vince/data/emnlp12/{}.c.txt""\n\n\nclass DefinitePronounResolution(nlp.GeneratorBasedBuilder):\n    """"""The Definite Pronoun Resolution Dataset.""""""\n\n    BUILDER_CONFIGS = [\n        nlp.BuilderConfig(\n            name=""plain_text"",\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            description=""Plain text import of the Definite Pronoun Resolution Dataset."",  # pylint: disable=line-too-long\n        )\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""sentence"": nlp.Value(""string""),\n                    ""pronoun"": nlp.Value(""string""),\n                    ""candidates"": nlp.features.Sequence(nlp.Value(""string""), length=2),\n                    ""label"": nlp.features.ClassLabel(num_classes=2),\n                }\n            ),\n            supervised_keys=(""sentence"", ""label""),\n            homepage=""http://www.hlt.utdallas.edu/~vince/data/emnlp12/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        files = dl_manager.download_and_extract(\n            {""train"": _DATA_URL_PATTERN.format(""train""), ""test"": _DATA_URL_PATTERN.format(""test""),}\n        )\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": files[""test""]}),\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": files[""train""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        with open(filepath) as f:\n            line_num = -1\n            while True:\n                line_num += 1\n                sentence = f.readline().strip()\n                pronoun = f.readline().strip()\n                candidates = [c.strip() for c in f.readline().strip().split("","")]\n                correct = f.readline().strip()\n                f.readline()\n                if not sentence:\n                    break\n                yield line_num, {\n                    ""sentence"": sentence,\n                    ""pronoun"": pronoun,\n                    ""candidates"": candidates,\n                    ""label"": candidates.index(correct),\n                }\n'"
datasets/discofuse/discofuse.py,0,"b'""""""TODO(discofuse): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport tensorflow as tf\n\nimport nlp\n\n\n# TODO(discofuse): BibTeX citation\n\n_URL_ = ""https://storage.googleapis.com/discofuse_dataset_v1/""\n_CITATION = """"""\\\n@InProceedings{GevaEtAl2019,\n  title = {{DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion}},\n  author = {Geva, Mor and Malmi, Eric and Szpektor, Idan and Berant, Jonathan},\n  booktitle = {Proceedings of the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics},\n  note = {arXiv preprint arXiv:1902.10526},\n  year = {2019}\n}\n\n""""""\n\n# TODO(discofuse):\n_DESCRIPTION = """"""\\\n DISCOFUSE is a large scale dataset for discourse-based sentence fusion. \n""""""\n\n\nclass DiscofuseConfig(nlp.BuilderConfig):\n\n    """""" BuilderConfig for Discofuse""""""\n\n    def __init__(self, data_url, balanced=False, **kwargs):\n        """"""\n\n        Args:\n            balanced: to specify if we want to load the balanced file or the full file\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(DiscofuseConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n        self.balanced = balanced\n        self.data_url = data_url\n\n\nclass Discofuse(nlp.GeneratorBasedBuilder):\n    """"""TODO(discofuse): Short description of my dataset.""""""\n\n    # TODO(discofuse): Set up version.\n    VERSION = nlp.Version(""1.0.0"")\n    BUILDER_CONFIGS = [\n        DiscofuseConfig(\n            name=""discofuse-sport"", description=""sentence fusion"", data_url=_URL_ + ""discofuse_v1_sports.tar.gz""\n        ),\n        DiscofuseConfig(\n            name=""discofuse-wikipedia"", description=""sentence fusion"", data_url=_URL_ + ""discofuse_v1_wikipedia.tar.gz""\n        ),\n    ]\n\n    def _info(self):\n        # TODO(discofuse): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""connective_string"": nlp.Value(""string""),\n                    ""discourse_type"": nlp.Value(""string""),\n                    ""coherent_second_sentence"": nlp.Value(""string""),\n                    ""has_coref_type_pronoun"": nlp.Value(""float32""),\n                    ""incoherent_first_sentence"": nlp.Value(""string""),\n                    ""incoherent_second_sentence"": nlp.Value(""string""),\n                    ""has_coref_type_nominal"": nlp.Value(""float32""),\n                    ""coherent_first_sentence"": nlp.Value(""string""),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/google-research-datasets/discofuse"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(discofuse): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        if self.config.name == ""discofuse-sport"":\n            dl_dir = dl_manager.download_and_extract(self.config.data_url)\n            data_dir = os.path.join(dl_dir, ""discofuse_v1/sports"")\n            if self.config.balanced:\n                return [\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TRAIN,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={""filepath"": os.path.join(data_dir, ""train_balanced.tsv"")},\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TEST,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={""filepath"": os.path.join(data_dir, ""test_balanced.tsv"")},\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.VALIDATION,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={""filepath"": os.path.join(data_dir, ""dev_balanced.tsv"")},\n                    ),\n                ]\n            else:\n                return [\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TRAIN,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={""filepath"": os.path.join(data_dir, ""train.tsv"")},\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TEST,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={""filepath"": os.path.join(data_dir, ""test.tsv"")},\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.VALIDATION,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={""filepath"": os.path.join(data_dir, ""dev.tsv"")},\n                    ),\n                ]\n        else:\n            if self.config.name == ""discofuse-wikipedia"":\n                dl_dir = dl_manager.download_and_extract(self.config.data_url)\n                data_dir = os.path.join(dl_dir, ""discofuse_v1/wikipedia"")\n                if self.config.balanced:\n                    return [\n                        nlp.SplitGenerator(\n                            name=nlp.Split.TRAIN,\n                            # These kwargs will be passed to _generate_examples\n                            gen_kwargs={""filepath"": os.path.join(data_dir, ""train_balanced.tsv"")},\n                        ),\n                        nlp.SplitGenerator(\n                            name=nlp.Split.TEST,\n                            # These kwargs will be passed to _generate_examples\n                            gen_kwargs={""filepath"": os.path.join(data_dir, ""test_balanced.tsv"")},\n                        ),\n                        nlp.SplitGenerator(\n                            name=nlp.Split.VALIDATION,\n                            # These kwargs will be passed to _generate_examples\n                            gen_kwargs={""filepath"": os.path.join(data_dir, ""dev_balanced.tsv"")},\n                        ),\n                    ]\n                else:\n                    return [\n                        nlp.SplitGenerator(\n                            name=nlp.Split.TRAIN,\n                            # These kwargs will be passed to _generate_examples\n                            gen_kwargs={""filepath"": os.path.join(data_dir, ""train.tsv"")},\n                        ),\n                        nlp.SplitGenerator(\n                            name=nlp.Split.TEST,\n                            # These kwargs will be passed to _generate_examples\n                            gen_kwargs={""filepath"": os.path.join(data_dir, ""test.tsv"")},\n                        ),\n                        nlp.SplitGenerator(\n                            name=nlp.Split.VALIDATION,\n                            # These kwargs will be passed to _generate_examples\n                            gen_kwargs={""filepath"": os.path.join(data_dir, ""dev.tsv"")},\n                        ),\n                    ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(discofuse): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = csv.DictReader(f, delimiter=""\\t"")\n            for id_, row in enumerate(data):\n                co_first_sent = row[""coherent_first_sentence""]\n                co_second_sent = row[""coherent_second_sentence""]\n                connect_str = row[""connective_string""]\n                discourse_type = row[""discourse_type""]\n                has_coref_pronoun = row[""has_coref_type_pronoun""]\n                has_coref_nominal = row[""has_coref_type_nominal""]\n                inco_first_sent = row[""incoherent_first_sentence""]\n                inco_second_sent = row[""incoherent_second_sentence""]\n                yield id_, {\n                    ""connective_string"": connect_str,\n                    ""discourse_type"": discourse_type,\n                    ""coherent_second_sentence"": co_second_sent,\n                    ""has_coref_type_pronoun"": has_coref_pronoun,\n                    ""incoherent_first_sentence"": inco_first_sent,\n                    ""incoherent_second_sentence"": inco_second_sent,\n                    ""has_coref_type_nominal"": has_coref_nominal,\n                    ""coherent_first_sentence"": co_first_sent,\n                }\n'"
datasets/drop/drop.py,0,"b'""""""TODO(drop): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(drop): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{Dua2019DROP,\n  author={Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},\n  title={  {DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},\n  booktitle={Proc. of NAACL},\n  year={2019}\n}\n""""""\n\n# TODO(drop):\n_DESCRIPTION = """"""\\\nDROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.\n. DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a \nquestion, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or\n sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was \n necessary for prior datasets.\n""""""\n_URl = ""https://s3-us-west-2.amazonaws.com/allennlp/datasets/drop/drop_dataset.zip""\n\n\nclass Drop(nlp.GeneratorBasedBuilder):\n    """"""TODO(drop): Short description of my dataset.""""""\n\n    # TODO(drop): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(drop): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""passage"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers_spans"": nlp.features.Sequence({""spans"": nlp.Value(""string"")})\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://allennlp.org/drop"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(drop): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URl)\n        data_dir = os.path.join(dl_dir, ""drop_dataset"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""drop_dataset_train.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""drop_dataset_dev.json"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(drop): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            # print(data.keys())\n            for i, key in enumerate(data):\n                example = data[key]\n                # print(example[\'passage\'])\n                qa_pairs = example[""qa_pairs""]\n                for j, qa in enumerate(qa_pairs):\n                    question = qa[""question""]\n                    answers = qa[""answer""][""spans""]\n                    yield str(i) + ""_"" + str(j), {\n                        ""passage"": example[""passage""],\n                        ""question"": question,\n                        ""answers_spans"": {""spans"": answers},\n                    }\n'"
datasets/empathetic_dialogues/empathetic_dialogues.py,0,"b'""""""TODO(empathetic_dialogues): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@inproceedings{rashkin2019towards,\n  title = {Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset},\n  author = {Hannah Rashkin and Eric Michael Smith and Margaret Li and Y-Lan Boureau},\n  booktitle = {ACL},\n  year = {2019},\n}\n""""""\n\n_DESCRIPTION = """"""\\\nPyTorch original implementation of Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset\n""""""\n_URL = ""https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz""\n\n\nclass EmpatheticDialogues(nlp.GeneratorBasedBuilder):\n    """"""TODO(empathetic_dialogues): Short description of my dataset.""""""\n\n    # TODO(empathetic_dialogues): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(empathetic_dialogues): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""conv_id"": nlp.Value(""string""),\n                    ""utterance_idx"": nlp.Value(""int32""),\n                    ""context"": nlp.Value(""string""),\n                    ""prompt"": nlp.Value(""string""),\n                    ""speaker_idx"": nlp.Value(""int32""),\n                    ""utterance"": nlp.Value(""string""),\n                    ""selfeval"": nlp.Value(""string""),\n                    ""tags"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/facebookresearch/EmpatheticDialogues"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(empathetic_dialogues): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""empatheticdialogues"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""train.csv"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""valid.csv"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""test.csv"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(empathetic_dialogues): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = csv.DictReader(f)\n            for id_, row in enumerate(data):\n                utterance = row[""utterance""]\n                speaker_id = int(row[""speaker_idx""])\n                context = row[""context""]\n                conv_id = row[""conv_id""]\n                tags = row[""tags""] if row[""tags""] else """"\n                selfeval = row[""selfeval""] if row[""selfeval""] else """"\n                utterance_id = int(row[""utterance_idx""])\n                prompt = row[""prompt""]\n                yield id_, {\n                    ""utterance"": utterance,\n                    ""utterance_idx"": utterance_id,\n                    ""context"": context,\n                    ""speaker_idx"": speaker_id,\n                    ""conv_id"": conv_id,\n                    ""selfeval"": selfeval,\n                    ""prompt"": prompt,\n                    ""tags"": tags,\n                }\n'"
datasets/eraser_multi_rc/eraser_multi_rc.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Passage, query, answers and answer classification with explanations.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@unpublished{eraser2019,\n    title = {ERASER: A Benchmark to Evaluate Rationalized NLP Models},\n    author = {Jay DeYoung and Sarthak Jain and Nazneen Fatema Rajani and Eric Lehman and Caiming Xiong and Richard Socher and Byron C. Wallace}\n}\n@inproceedings{MultiRC2018,\n    author = {Daniel Khashabi and Snigdha Chaturvedi and Michael Roth and Shyam Upadhyay and Dan Roth},\n    title = {Looking Beyond the Surface:A Challenge Set for Reading Comprehension over Multiple Sentences},\n    booktitle = {NAACL},\n    year = {2018}\n}\n""""""\n\n_DESCRIPTION = """"""\nEraser Multi RC is a dataset for queries over multi-line passages, along with\nanswers and a rationalte. Each example in this dataset has the following 5 parts\n1. A Mutli-line Passage\n2. A Query about the passage\n3. An Answer to the query\n4. A Classification as to whether the answer is right or wrong\n5. An Explanation justifying the classification\n""""""\n\n_DOWNLOAD_URL = ""http://www.eraserbenchmark.com/zipped/multirc.tar.gz""\n\n\nclass EraserMultiRc(nlp.GeneratorBasedBuilder):\n    """"""Multi Sentence Reasoning with Explanations (Eraser Benchmark).""""""\n\n    VERSION = nlp.Version(""0.1.1"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""passage"": nlp.Value(""string""),\n                    ""query_and_answer"": nlp.Value(""string""),\n                    ""label"": nlp.features.ClassLabel(names=[""False"", ""True""]),\n                    ""evidences"": nlp.features.Sequence(nlp.Value(""string"")),\n                }\n            ),\n            supervised_keys=None,\n            homepage=""https://cogcomp.seas.upenn.edu/multirc/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n\n        dl_dir = dl_manager.download_and_extract(_DOWNLOAD_URL)\n        data_dir = os.path.join(dl_dir, ""multirc"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""data_dir"": data_dir, ""filepath"": os.path.join(data_dir, ""train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""data_dir"": data_dir, ""filepath"": os.path.join(data_dir, ""val.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""data_dir"": data_dir, ""filepath"": os.path.join(data_dir, ""test.jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, data_dir, filepath):\n        """"""Yields examples.""""""\n\n        multirc_dir = os.path.join(data_dir, ""docs"")\n        with open(filepath) as f:\n            for line in f:\n                row = json.loads(line)\n                evidences = []\n\n                for evidence in row[""evidences""][0]:\n                    docid = evidence[""docid""]\n                    evidences.append(evidence[""text""])\n\n                passage_file = os.path.join(multirc_dir, docid)\n                with open(passage_file) as f1:\n                    passage_text = f1.read()\n\n                yield row[""annotation_id""], {\n                    ""passage"": passage_text,\n                    ""query_and_answer"": row[""query""],\n                    ""label"": row[""classification""],\n                    ""evidences"": evidences,\n                }\n'"
datasets/esnli/esnli.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""e-SNLI: Natural Language Inference with Natural Language Explanations.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@incollection{NIPS2018_8163,\ntitle = {e-SNLI: Natural Language Inference with Natural Language Explanations},\nauthor = {Camburu, Oana-Maria and Rockt\\""{a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},\nbooktitle = {Advances in Neural Information Processing Systems 31},\neditor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},\npages = {9539--9549},\nyear = {2018},\npublisher = {Curran Associates, Inc.},\nurl = {http://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations.pdf}\n}\n""""""\n\n_DESCRIPTION = """"""\nThe e-SNLI dataset extends the Stanford Natural Language Inference Dataset to\ninclude human-annotated natural language explanations of the entailment\nrelations.\n""""""\n_URL = ""https://raw.githubusercontent.com/OanaMariaCamburu/e-SNLI/master/dataset/""\n\n\nclass Esnli(nlp.GeneratorBasedBuilder):\n    """"""e-SNLI: Natural Language Inference with Natural Language Explanations corpus.""""""\n\n    # Version History\n    # 0.0.2 Added explanation_2, explanation_3 fields which exist in the dev/test\n    # splits only.\n    # 0.0.1 Initial version\n    BUILDER_CONFIGS = [\n        nlp.BuilderConfig(name=""plain_text"", version=nlp.Version(""0.0.2""), description=""Plain text import of e-SNLI"",)\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""premise"": nlp.Value(""string""),\n                    ""hypothesis"": nlp.Value(""string""),\n                    ""label"": nlp.features.ClassLabel(names=[""entailment"", ""neutral"", ""contradiction""]),\n                    ""explanation_1"": nlp.Value(""string""),\n                    ""explanation_2"": nlp.Value(""string""),\n                    ""explanation_3"": nlp.Value(""string""),\n                }\n            ),\n            supervised_keys=None,\n            homepage=""https://github.com/OanaMariaCamburu/e-SNLI"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n\n        files = dl_manager.download_and_extract(\n            {\n                ""train"": [os.path.join(_URL, ""esnli_train_1.csv""), os.path.join(_URL, ""esnli_train_2.csv"")],\n                ""validation"": [os.path.join(_URL, ""esnli_dev.csv"")],\n                ""test"": [os.path.join(_URL, ""esnli_test.csv"")],\n            }\n        )\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""files"": files[""train""]},),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""files"": files[""validation""]},),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""files"": files[""test""]},),\n        ]\n\n    def _generate_examples(self, files):\n        """"""Yields examples.""""""\n        for filepath in files:\n            with open(filepath) as f:\n                reader = csv.DictReader(f)\n                for _, row in enumerate(reader):\n                    yield row[""pairID""], {\n                        ""premise"": row[""Sentence1""],\n                        ""hypothesis"": row[""Sentence2""],\n                        ""label"": row[""gold_label""],\n                        ""explanation_1"": row[""Explanation_1""],\n                        ""explanation_2"": row.get(""Explanation_2"", """"),\n                        ""explanation_3"": row.get(""Explanation_3"", """"),\n                    }\n'"
datasets/event2Mind/event2Mind.py,0,"b'""""""TODO(event2Mind): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n# TODO(event2Mind): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{event2Mind,\n    title={Event2Mind: Commonsense Inference on Events, Intents, and Reactions},\n    author={Hannah Rashkin and Maarten Sap and Emily Allaway and Noah A. Smith\xe2\x80\xa0 Yejin Choi},\n    year={2018}\n}\n""""""\n\n# TODO(event2Mind):\\\n\n_DESCRIPTION = """"""\\\nIn Event2Mind, we explore the task of understanding stereotypical intents and reactions to events. Through crowdsourcing, we create a large corpus with 25,000 events and free-form descriptions of their intents and reactions, both of the event\'s subject and (potentially implied) other participants.\n""""""\n_URL = ""https://uwnlp.github.io/event2mind/data/event2mind.zip""\n\n\nclass Event2mind(nlp.GeneratorBasedBuilder):\n    """"""TODO(event2Mind): Short description of my dataset.""""""\n\n    # TODO(event2Mind): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(event2Mind): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""Source"": nlp.Value(""string""),\n                    ""Event"": nlp.Value(""string""),\n                    ""Xintent"": nlp.Value(""string""),\n                    ""Xemotion"": nlp.Value(""string""),\n                    ""Otheremotion"": nlp.Value(""string""),\n                    ""Xsent"": nlp.Value(""string""),\n                    ""Osent"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://uwnlp.github.io/event2mind/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(event2Mind): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""train.csv"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""test.csv"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""dev.csv"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(event2Mind): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = csv.DictReader(f)\n            for id_, row in enumerate(data):\n                yield id_, {\n                    ""Source"": row[""Source""],\n                    ""Event"": row[""Event""],\n                    ""Xintent"": row[""Xintent""],\n                    ""Xemotion"": row[""Xemotion""],\n                    ""Otheremotion"": row[""Otheremotion""],\n                    ""Xsent"": row[""Xsent""],\n                    ""Osent"": row[""Osent""],\n                }\n'"
datasets/flores/flores.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Facebook Low Resource (FLoRes) machine translation benchmark dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport collections\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nEvaluation datasets for low-resource machine translation: Nepali-English and Sinhala-English.\n""""""\n\n_CITATION = """"""\\\n@misc{guzmn2019new,\n    title={Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English},\n    author={Francisco Guzman and Peng-Jen Chen and Myle Ott and Juan Pino and Guillaume Lample and Philipp Koehn and Vishrav Chaudhary and Marc\'Aurelio Ranzato},\n    year={2019},\n    eprint={1902.01382},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n""""""\n\n_DATA_URL = ""https://github.com/facebookresearch/flores/raw/master/data/wikipedia_en_ne_si_test_sets.tgz""\n\n# Tuple that describes a single pair of files with matching translations.\n# language_to_file is the map from language (2 letter string: example \'en\')\n# to the file path in the extracted directory.\nTranslateData = collections.namedtuple(""TranslateData"", [""url"", ""language_to_file""])\n\n\nclass FloresConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for FLoRes.""""""\n\n    def __init__(self, language_pair=(None, None), **kwargs):\n        """"""BuilderConfig for FLoRes.\n\n    Args:\n        for the `nlp.features.text.TextEncoder` used for the features feature.\n      language_pair: pair of languages that will be used for translation. Should\n        contain 2-letter coded strings. First will be used at source and second\n        as target in supervised mode. For example: (""se"", ""en"").\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s%s"" % (language_pair[0], language_pair[1])\n\n        description = (""Translation dataset from %s to %s"") % (language_pair[0], language_pair[1])\n        super(FloresConfig, self).__init__(\n            name=name,\n            description=description,\n            version=nlp.Version(""1.1.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            **kwargs,\n        )\n\n        # Validate language pair.\n        assert ""en"" in language_pair, (""Config language pair must contain `en`, got: %s"", language_pair)\n        source, target = language_pair\n        non_en = source if target == ""en"" else target\n        assert non_en in [""ne"", ""si""], (""Invalid non-en language in pair: %s"", non_en)\n\n        self.language_pair = language_pair\n\n\nclass Flores(nlp.GeneratorBasedBuilder):\n    """"""FLoRes machine translation dataset.""""""\n\n    BUILDER_CONFIGS = [\n        FloresConfig(language_pair=(""ne"", ""en""),),\n        FloresConfig(language_pair=(""si"", ""en""),),\n    ]\n\n    def _info(self):\n        source, target = self.config.language_pair\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            supervised_keys=(source, target),\n            homepage=""https://github.com/facebookresearch/flores/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        dl_dir = dl_manager.download_and_extract(_DATA_URL)\n\n        source, target = self.config.language_pair\n        non_en = source if target == ""en"" else target\n        path_tmpl = ""{dl_dir}/wikipedia_en_ne_si_test_sets/wikipedia.{split}.{non_en}-en."" ""{lang}""\n\n        files = {}\n        for split in (""dev"", ""devtest""):\n            files[split] = {\n                ""source_file"": path_tmpl.format(dl_dir=dl_dir, split=split, non_en=non_en, lang=source),\n                ""target_file"": path_tmpl.format(dl_dir=dl_dir, split=split, non_en=non_en, lang=target),\n            }\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs=files[""dev""]),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs=files[""devtest""]),\n        ]\n\n    def _generate_examples(self, source_file, target_file):\n        """"""This function returns the examples in the raw (text) form.""""""\n        with open(source_file) as f:\n            source_sentences = f.read().split(""\\n"")\n        with open(target_file) as f:\n            target_sentences = f.read().split(""\\n"")\n\n        assert len(target_sentences) == len(source_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(source_sentences),\n            len(target_sentences),\n            source_file,\n            target_file,\n        )\n\n        source, target = self.config.language_pair\n        for idx, (l1, l2) in enumerate(zip(source_sentences, target_sentences)):\n            result = {""translation"": {source: l1, target: l2}}\n            # Make sure that both translations are non-empty.\n            if all(result.values()):\n                yield idx, result\n'"
datasets/fquad/fquad.py,0,"b'""""""TODO(fquad): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(fquad): BibTeX citation\n_CITATION = """"""\\\n@ARTICLE{2020arXiv200206071\n       author = {{Martin}, d\'Hoffschmidt and {Maxime}, Vidal and\n         {Wacim}, Belblidia and {Tom}, Brendl\xc3\xa9},\n        title = ""{FQuAD: French Question Answering Dataset}"",\n      journal = {arXiv e-prints},\n     keywords = {Computer Science - Computation and Language},\n         year = ""2020"",\n        month = ""Feb"",\n          eid = {arXiv:2002.06071},\n        pages = {arXiv:2002.06071},\narchivePrefix = {arXiv},\n       eprint = {2002.06071},\n primaryClass = {cs.CL}\n}\n""""""\n\n# TODO(fquad):\n_DESCRIPTION = """"""\\\nFQuAD: French Question Answering Dataset\nWe introduce FQuAD, a native French Question Answering Dataset. FQuAD contains 25,000+ question and answer pairs.\nFinetuning CamemBERT on FQuAD yields a F1 score of 88% and an exact match of 77.9%.\n\n""""""\n_URL = ""https://storage.googleapis.com/illuin/fquad""\n_TRAIN_DATA = ""train.json.zip""\n_VALID_DATA = ""valid.json.zip""\n\n\nclass Fquad(nlp.GeneratorBasedBuilder):\n    """"""TODO(fquad): Short description of my dataset.""""""\n\n    # TODO(fquad): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(fquad): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""context"": nlp.Value(""string""),\n                    ""questions"": nlp.features.Sequence({""question"": nlp.Value(""string""),}),\n                    ""answers"": nlp.features.Sequence(\n                        {""texts"": nlp.Value(""string""), ""answers_starts"": nlp.Value(""int32"")}\n                    ),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://fquad.illuin.tech/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(fquad): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        download_urls = {""train"": os.path.join(_URL, _TRAIN_DATA), ""valid"": os.path.join(_URL, _VALID_DATA)}\n        dl_dir = dl_manager.download_and_extract(download_urls)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir[""train""], ""train.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir[""valid""], ""valid.json"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n\n        """"""Yields examples.""""""\n        # TODO(fquad): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for id1, examples in enumerate(data[""data""]):\n                for id2, example in enumerate(examples[""paragraphs""]):\n                    questions = [question[""question""] for question in example[""qas""]]\n                    answers = [answer[""answers""] for answer in example[""qas""]]\n                    texts = [answer[0][""text""] for answer in answers]\n                    answers_starts = [answer[0][""answer_start""] for answer in answers]\n\n                    yield str(id1) + ""_"" + str(id2), {\n                        ""context"": example[""context""],\n                        ""questions"": {""question"": questions,},\n                        ""answers"": {""texts"": texts, ""answers_starts"": answers_starts},\n                    }\n'"
datasets/gap/gap.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""GAP is a gender-balanced text data set.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\n\nimport nlp\n\n\n_CITATION = """"""\n@article{DBLP:journals/corr/abs-1810-05201,\n  author    = {Kellie Webster and\n               Marta Recasens and\n               Vera Axelrod and\n               Jason Baldridge},\n  title     = {Mind the {GAP:} {A} Balanced Corpus of Gendered Ambiguous Pronouns},\n  journal   = {CoRR},\n  volume    = {abs/1810.05201},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.05201},\n  archivePrefix = {arXiv},\n  eprint    = {1810.05201},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-05201},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n""""""\n\n_DESCRIPTION = """"""\nGAP is a gender-balanced dataset containing 8,908 coreference-labeled pairs of \n(ambiguous pronoun, antecedent name), sampled from Wikipedia and released by \nGoogle AI Language for the evaluation of coreference resolution in practical \napplications.\n""""""\n\n_TRAINURL = ""https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv""\n_VALIDATIONURL = ""https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv""\n_TESTURL = ""https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv""\n\n\nclass Gap(nlp.GeneratorBasedBuilder):\n    """"""GAP is a gender-balanced dataset.\n\n  It contains 8,908 coreference-labeled pairs\n  of (ambiguous pronoun, antecedent name), sampled from Wikipedia.\n  """"""\n\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""ID"": nlp.Value(""string""),\n                    ""Text"": nlp.Value(""string""),\n                    ""Pronoun"": nlp.Value(""string""),\n                    ""Pronoun-offset"": nlp.Value(""int32""),\n                    ""A"": nlp.Value(""string""),\n                    ""A-offset"": nlp.Value(""int32""),\n                    ""A-coref"": nlp.Value(""bool""),\n                    ""B"": nlp.Value(""string""),\n                    ""B-offset"": nlp.Value(""int32""),\n                    ""B-coref"": nlp.Value(""bool""),\n                    ""URL"": nlp.Value(""string""),\n                }\n            ),\n            supervised_keys=None,\n            homepage=""https://github.com/google-research-datasets/gap-coreference"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        directory = dl_manager.download_and_extract(\n            {""train"": _TRAINURL, ""validation"": _VALIDATIONURL, ""test"": _TESTURL}\n        )\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": directory[""train""]},),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": directory[""validation""]},),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": directory[""test""]},),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        with open(filepath) as tsvfile:\n            reader = csv.DictReader(tsvfile, dialect=""excel-tab"")\n            for i, row in enumerate(reader):\n                row[""A-coref""] = bool(row[""A-coref""])\n                row[""B-coref""] = bool(row[""B-coref""])\n                row[""A-offset""] = int(row[""A-offset""])\n                row[""B-offset""] = int(row[""B-offset""])\n                row[""Pronoun-offset""] = int(row[""Pronoun-offset""])\n                yield i, row\n'"
datasets/germeval_14/germeval_14.py,0,"b'# coding=utf-8\n# Copyright 2020 HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The GermEval 2014 NER Shared Task dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport glob\nimport logging\nimport os\nfrom pathlib import Path\n\nimport nlp\n\n\n_CITATION = """"""\\\n@inproceedings{benikova-etal-2014-nosta,\n    title = ""{N}o{S}ta-D Named Entity Annotation for {G}erman: Guidelines and Dataset"",\n    author = ""Benikova, Darina  and\n      Biemann, Chris  and\n      Reznicek, Marc"",\n    booktitle = ""Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}\'14)"",\n    month = may,\n    year = ""2014"",\n    address = ""Reykjavik, Iceland"",\n    publisher = ""European Language Resources Association (ELRA)"",\n    url = ""http://www.lrec-conf.org/proceedings/lrec2014/pdf/276_Paper.pdf"",\n    pages = ""2524--2531"",\n}\n""""""\n\n_DESCRIPTION = """"""\\\nThe GermEval 2014 NER Shared Task builds on a new dataset with German Named Entity annotation with the following properties:\\\n    - The data was sampled from German Wikipedia and News Corpora as a collection of citations.\\\n    - The dataset covers over 31,000 sentences corresponding to over 590,000 tokens.\\\n    - The NER annotation uses the NoSta-D guidelines, which extend the T\xc3\xbcbingen Treebank guidelines,\\\n      using four main NER categories with sub-structure, and annotating embeddings among NEs\\\n      such as [ORG FC Kickers [LOC Darmstadt]].\n""""""\n\n_URL = ""https://sites.google.com/site/germeval2014ner/data/""\n_TRAINING_FILE = ""NER-de-train.tsv""\n_DEV_FILE = ""NER-de-dev.tsv""\n_TEST_FILE = ""NER-de-test.tsv""\n\n\nclass GermEval14Config(nlp.BuilderConfig):\n    """"""BuilderConfig for GermEval 2014.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for GermEval 2014.\n\n    Args:\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(GermEval14Config, self).__init__(**kwargs)\n\n\nclass GermEval14(nlp.GeneratorBasedBuilder):\n    """"""GermEval 2014 NER Shared Task dataset.""""""\n\n    BUILDER_CONFIGS = [\n        GermEval14Config(\n            name=""germeval_14"", version=nlp.Version(""1.0.0""), description=""GermEval 2014 NER Shared Task dataset""\n        ),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""source"": nlp.Value(""string""),\n                    ""tokens"": nlp.Sequence(nlp.Value(""string"")),\n                    ""labels"": nlp.Sequence(nlp.Value(""string"")),\n                    ""nested-labels"": nlp.Sequence(nlp.Value(""string"")),\n                }\n            ),\n            supervised_keys=None,\n            homepage=""https://sites.google.com/site/germeval2014ner/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        urls_to_download = {\n            ""train"": f""{_URL}{_TRAINING_FILE}"",\n            ""dev"": f""{_URL}{_DEV_FILE}"",\n            ""test"": f""{_URL}{_TEST_FILE}"",\n        }\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]}),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": downloaded_files[""test""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        logging.info(""\xe2\x8f\xb3 Generating examples from = %s"", filepath)\n        with open(filepath) as f:\n            data = csv.reader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n            current_source = """"\n            current_tokens = []\n            current_labels = []\n            current_nested_labels = []\n            sentence_counter = 0\n            for row in data:\n                if row:\n                    if row[0] == ""#"":\n                        current_source = "" "".join(row[1:])\n                        continue\n                    id_, token, label, nested_label = row[:4]\n                    current_tokens.append(token)\n                    current_labels.append(label)\n                    current_nested_labels.append(nested_label)\n                else:\n                    # New sentence\n                    if not current_tokens:\n                        # Consecutive empty lines will cause empty sentences\n                        continue\n                    assert len(current_tokens) == len(current_labels), ""\xf0\x9f\x92\x94 between len of tokens & labels""\n                    assert len(current_labels) == len(current_nested_labels), ""\xf0\x9f\x92\x94 between len of labels & nested labels""\n                    assert current_source, ""\xf0\x9f\x92\xa5 Source for new sentence was not set""\n                    sentence = (\n                        sentence_counter,\n                        {\n                            ""id"": str(sentence_counter),\n                            ""tokens"": current_tokens,\n                            ""labels"": current_labels,\n                            ""nested-labels"": current_nested_labels,\n                            ""source"": current_source,\n                        },\n                    )\n                    sentence_counter += 1\n                    current_tokens = []\n                    current_labels = []\n                    current_nested_labels = []\n                    current_source = """"\n                    yield sentence\n            # Don\'t forget last sentence in dataset \xf0\x9f\xa7\x90\n            yield sentence_counter, {\n                ""id"": str(sentence_counter),\n                ""tokens"": current_tokens,\n                ""labels"": current_labels,\n                ""nested-labels"": current_nested_labels,\n                ""source"": current_source,\n            }\n'"
datasets/gigaword/gigaword.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Gigaword summarization dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@article{graff2003english,\n  title={English gigaword},\n  author={Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},\n  journal={Linguistic Data Consortium, Philadelphia},\n  volume={4},\n  number={1},\n  pages={34},\n  year={2003}\n}\n\n@article{Rush_2015,\n   title={A Neural Attention Model for Abstractive Sentence Summarization},\n   url={http://dx.doi.org/10.18653/v1/D15-1044},\n   DOI={10.18653/v1/d15-1044},\n   journal={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},\n   publisher={Association for Computational Linguistics},\n   author={Rush, Alexander M. and Chopra, Sumit and Weston, Jason},\n   year={2015}\n}\n""""""\n\n_DESCRIPTION = """"""\nHeadline-generation on a corpus of article pairs from Gigaword consisting of\naround 4 million articles. Use the \'org_data\' provided by\nhttps://github.com/microsoft/unilm/ which is identical to\nhttps://github.com/harvardnlp/sent-summary but with better format.\n\nThere are two features:\n  - document: article.\n  - summary: headline.\n\n""""""\n\n_URL = ""https://drive.google.com/uc?export=download&id=1USoQ8lJgN8kAWnUnRrupMGrPMLlDVqlV""\n\n_DOCUMENT = ""document""\n_SUMMARY = ""summary""\n\n\nclass Gigaword(nlp.GeneratorBasedBuilder):\n    """"""Gigaword summarization dataset.""""""\n\n    # 1.0.0 contains a bug that uses validation data as training data.\n    # 1.1.0 Update to the correct train, validation and test data.\n    # 1.2.0 Replace <unk> with <UNK> in train/val to be consistent with test.\n    VERSION = nlp.Version(""1.2.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({_DOCUMENT: nlp.Value(""string""), _SUMMARY: nlp.Value(""string"")}),\n            supervised_keys=(_DOCUMENT, _SUMMARY),\n            homepage=""https://github.com/harvardnlp/sent-summary"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_path = dl_manager.download_and_extract(_URL)\n        pattern = os.path.join(dl_path, ""org_data"", ""%s.%s.txt"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={\n                    ""src_path"": pattern % (""train"", ""src""),\n                    ""tgt_path"": pattern % (""train"", ""tgt""),\n                    ""replace_unk"": True,\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={\n                    ""src_path"": pattern % (""dev"", ""src""),\n                    ""tgt_path"": pattern % (""dev"", ""tgt""),\n                    ""replace_unk"": True,\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={\n                    ""src_path"": pattern % (""test"", ""src""),\n                    ""tgt_path"": pattern % (""test"", ""tgt""),\n                    ""replace_unk"": False,\n                },\n            ),\n        ]\n\n    def _generate_examples(self, src_path=None, tgt_path=None, replace_unk=None):\n        """"""Yields examples.""""""\n        with open(src_path) as f_d, open(tgt_path) as f_s:\n            for i, (doc_text, sum_text) in enumerate(zip(f_d, f_s)):\n                if replace_unk:\n                    yield i, {\n                        _DOCUMENT: doc_text.strip().replace(""<unk>"", ""UNK""),\n                        _SUMMARY: sum_text.strip().replace(""<unk>"", ""UNK""),\n                    }\n                else:\n                    yield i, {_DOCUMENT: doc_text.strip(), _SUMMARY: sum_text.strip()}\n'"
datasets/glue/glue.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The General Language Understanding Evaluation (GLUE) benchmark.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\nimport textwrap\n\nimport numpy as np\nimport six\n\nimport nlp\n\n\n_GLUE_CITATION = """"""\\\n@inproceedings{wang2019glue,\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n  note={In the Proceedings of ICLR.},\n  year={2019}\n}\n\nNote that each GLUE dataset has its own citation. Please see the source to see\nthe correct citation for each contained dataset.""""""\n\n_GLUE_DESCRIPTION = """"""\\\nGLUE, the General Language Understanding Evaluation benchmark\n(https://gluebenchmark.com/) is a collection of resources for training,\nevaluating, and analyzing natural language understanding systems.\n\n""""""\n\n_MRPC_DEV_IDS = ""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc""\n_MRPC_TRAIN = ""https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt""\n_MRPC_TEST = ""https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt""\n\n_MNLI_BASE_KWARGS = dict(\n    text_features={""premise"": ""sentence1"", ""hypothesis"": ""sentence2"",},\n    label_classes=[""entailment"", ""neutral"", ""contradiction""],\n    label_column=""gold_label"",\n    data_url=""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce"",\n    data_dir=""MNLI"",\n    citation=textwrap.dedent(\n        """"""\\\n      @InProceedings{N18-1101,\n        author = ""Williams, Adina\n                  and Nangia, Nikita\n                  and Bowman, Samuel"",\n        title = ""A Broad-Coverage Challenge Corpus for\n                 Sentence Understanding through Inference"",\n        booktitle = ""Proceedings of the 2018 Conference of\n                     the North American Chapter of the\n                     Association for Computational Linguistics:\n                     Human Language Technologies, Volume 1 (Long\n                     Papers)"",\n        year = ""2018"",\n        publisher = ""Association for Computational Linguistics"",\n        pages = ""1112--1122"",\n        location = ""New Orleans, Louisiana"",\n        url = ""http://aclweb.org/anthology/N18-1101""\n      }\n      @article{bowman2015large,\n        title={A large annotated corpus for learning natural language inference},\n        author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},\n        journal={arXiv preprint arXiv:1508.05326},\n        year={2015}\n      }""""""\n    ),\n    url=""http://www.nyu.edu/projects/bowman/multinli/"",\n)\n\n\nclass GlueConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for GLUE.""""""\n\n    def __init__(\n        self,\n        text_features,\n        label_column,\n        data_url,\n        data_dir,\n        citation,\n        url,\n        label_classes=None,\n        process_label=lambda x: x,\n        **kwargs,\n    ):\n        """"""BuilderConfig for GLUE.\n\n    Args:\n      text_features: `dict[string, string]`, map from the name of the feature\n        dict for each text field to the name of the column in the tsv file\n      label_column: `string`, name of the column in the tsv file corresponding\n        to the label\n      data_url: `string`, url to download the zip file from\n      data_dir: `string`, the path to the folder containing the tsv files in the\n        downloaded zip\n      citation: `string`, citation for the data set\n      url: `string`, url for information about the data set\n      label_classes: `list[string]`, the list of classes if the label is\n        categorical. If not provided, then the label will be of type\n        `nlp.Value(\'float32\')`.\n      process_label: `Function[string, any]`, function  taking in the raw value\n        of the label and processing it to the form required by the label feature\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(GlueConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n        self.text_features = text_features\n        self.label_column = label_column\n        self.label_classes = label_classes\n        self.data_url = data_url\n        self.data_dir = data_dir\n        self.citation = citation\n        self.url = url\n        self.process_label = process_label\n\n\nclass Glue(nlp.GeneratorBasedBuilder):\n    """"""The General Language Understanding Evaluation (GLUE) benchmark.""""""\n\n    BUILDER_CONFIGS = [\n        GlueConfig(\n            name=""cola"",\n            description=textwrap.dedent(\n                """"""\\\n            The Corpus of Linguistic Acceptability consists of English\n            acceptability judgments drawn from books and journal articles on\n            linguistic theory. Each example is a sequence of words annotated\n            with whether it is a grammatical English sentence.""""""\n            ),\n            text_features={""sentence"": ""sentence""},\n            label_classes=[""unacceptable"", ""acceptable""],\n            label_column=""is_acceptable"",\n            data_url=""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4"",\n            data_dir=""CoLA"",\n            citation=textwrap.dedent(\n                """"""\\\n            @article{warstadt2018neural,\n              title={Neural Network Acceptability Judgments},\n              author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},\n              journal={arXiv preprint arXiv:1805.12471},\n              year={2018}\n            }""""""\n            ),\n            url=""https://nyu-mll.github.io/CoLA/"",\n        ),\n        GlueConfig(\n            name=""sst2"",\n            description=textwrap.dedent(\n                """"""\\\n            The Stanford Sentiment Treebank consists of sentences from movie reviews and\n            human annotations of their sentiment. The task is to predict the sentiment of a\n            given sentence. We use the two-way (positive/negative) class split, and use only\n            sentence-level labels.""""""\n            ),\n            text_features={""sentence"": ""sentence""},\n            label_classes=[""negative"", ""positive""],\n            label_column=""label"",\n            data_url=""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8"",\n            data_dir=""SST-2"",\n            citation=textwrap.dedent(\n                """"""\\\n            @inproceedings{socher2013recursive,\n              title={Recursive deep models for semantic compositionality over a sentiment treebank},\n              author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},\n              booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},\n              pages={1631--1642},\n              year={2013}\n            }""""""\n            ),\n            url=""https://nlp.stanford.edu/sentiment/index.html"",\n        ),\n        GlueConfig(\n            name=""mrpc"",\n            description=textwrap.dedent(\n                """"""\\\n            The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of\n            sentence pairs automatically extracted from online news sources, with human annotations\n            for whether the sentences in the pair are semantically equivalent.""""""\n            ),  # pylint: disable=line-too-long\n            text_features={""sentence1"": """", ""sentence2"": """"},\n            label_classes=[""not_equivalent"", ""equivalent""],\n            label_column=""Quality"",\n            data_url="""",  # MRPC isn\'t hosted by GLUE.\n            data_dir=""MRPC"",\n            citation=textwrap.dedent(\n                """"""\\\n            @inproceedings{dolan2005automatically,\n              title={Automatically constructing a corpus of sentential paraphrases},\n              author={Dolan, William B and Brockett, Chris},\n              booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\n              year={2005}\n            }""""""\n            ),\n            url=""https://www.microsoft.com/en-us/download/details.aspx?id=52398"",\n        ),\n        GlueConfig(\n            name=""qqp"",\n            description=textwrap.dedent(\n                """"""\\\n            The Quora Question Pairs2 dataset is a collection of question pairs from the\n            community question-answering website Quora. The task is to determine whether a\n            pair of questions are semantically equivalent.""""""\n            ),\n            text_features={""question1"": ""question1"", ""question2"": ""question2"",},\n            label_classes=[""not_duplicate"", ""duplicate""],\n            label_column=""is_duplicate"",\n            data_url=""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5"",\n            data_dir=""QQP"",\n            citation=textwrap.dedent(\n                """"""\\\n          @online{WinNT,\n            author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornel},\n            title = {First Quora Dataset Release: Question Pairs},\n            year = 2017,\n            url = {https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs},\n            urldate = {2019-04-03}\n          }""""""\n            ),\n            url=""https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs"",\n        ),\n        GlueConfig(\n            name=""stsb"",\n            description=textwrap.dedent(\n                """"""\\\n            The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of\n            sentence pairs drawn from news headlines, video and image captions, and natural\n            language inference data. Each pair is human-annotated with a similarity score\n            from 1 to 5.""""""\n            ),\n            text_features={""sentence1"": ""sentence1"", ""sentence2"": ""sentence2"",},\n            label_column=""score"",\n            data_url=""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5"",\n            data_dir=""STS-B"",\n            citation=textwrap.dedent(\n                """"""\\\n            @article{cer2017semeval,\n              title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},\n              author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},\n              journal={arXiv preprint arXiv:1708.00055},\n              year={2017}\n            }""""""\n            ),\n            url=""http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark"",\n            process_label=np.float32,\n        ),\n        GlueConfig(\n            name=""mnli"",\n            description=textwrap.dedent(\n                """"""\\\n            The Multi-Genre Natural Language Inference Corpus is a crowdsourced\n            collection of sentence pairs with textual entailment annotations. Given a premise sentence\n            and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis\n            (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are\n            gathered from ten different sources, including transcribed speech, fiction, and government reports.\n            We use the standard test set, for which we obtained private labels from the authors, and evaluate\n            on both the matched (in-domain) and mismatched (cross-domain) section. We also use and recommend\n            the SNLI corpus as 550k examples of auxiliary training data.""""""\n            ),\n            **_MNLI_BASE_KWARGS,\n        ),\n        GlueConfig(\n            name=""mnli_mismatched"",\n            description=textwrap.dedent(\n                """"""\\\n          The mismatched validation and test splits from MNLI.\n          See the ""mnli"" BuilderConfig for additional information.""""""\n            ),\n            **_MNLI_BASE_KWARGS,\n        ),\n        GlueConfig(\n            name=""mnli_matched"",\n            description=textwrap.dedent(\n                """"""\\\n          The matched validation and test splits from MNLI.\n          See the ""mnli"" BuilderConfig for additional information.""""""\n            ),\n            **_MNLI_BASE_KWARGS,\n        ),\n        GlueConfig(\n            name=""qnli"",\n            description=textwrap.dedent(\n                """"""\\\n            The Stanford Question Answering Dataset is a question-answering\n            dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn\n            from Wikipedia) contains the answer to the corresponding question (written by an annotator). We\n            convert the task into sentence pair classification by forming a pair between each question and each\n            sentence in the corresponding context, and filtering out pairs with low lexical overlap between the\n            question and the context sentence. The task is to determine whether the context sentence contains\n            the answer to the question. This modified version of the original task removes the requirement that\n            the model select the exact answer, but also removes the simplifying assumptions that the answer\n            is always present in the input and that lexical overlap is a reliable cue.""""""\n            ),  # pylint: disable=line-too-long\n            text_features={""question"": ""question"", ""sentence"": ""sentence"",},\n            label_classes=[""entailment"", ""not_entailment""],\n            label_column=""label"",\n            data_url=""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601"",\n            data_dir=""QNLI"",\n            citation=textwrap.dedent(\n                """"""\\\n            @article{rajpurkar2016squad,\n              title={Squad: 100,000+ questions for machine comprehension of text},\n              author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},\n              journal={arXiv preprint arXiv:1606.05250},\n              year={2016}\n            }""""""\n            ),\n            url=""https://rajpurkar.github.io/SQuAD-explorer/"",\n        ),\n        GlueConfig(\n            name=""rte"",\n            description=textwrap.dedent(\n                """"""\\\n            The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual\n            entailment challenges. We combine the data from RTE1 (Dagan et al., 2006), RTE2 (Bar Haim\n            et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5 (Bentivogli et al., 2009).4 Examples are\n            constructed based on news and Wikipedia text. We convert all datasets to a two-class split, where\n            for three-class datasets we collapse neutral and contradiction into not entailment, for consistency.""""""\n            ),  # pylint: disable=line-too-long\n            text_features={""sentence1"": ""sentence1"", ""sentence2"": ""sentence2"",},\n            label_classes=[""entailment"", ""not_entailment""],\n            label_column=""label"",\n            data_url=""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb"",\n            data_dir=""RTE"",\n            citation=textwrap.dedent(\n                """"""\\\n            @inproceedings{dagan2005pascal,\n              title={The PASCAL recognising textual entailment challenge},\n              author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},\n              booktitle={Machine Learning Challenges Workshop},\n              pages={177--190},\n              year={2005},\n              organization={Springer}\n            }\n            @inproceedings{bar2006second,\n              title={The second pascal recognising textual entailment challenge},\n              author={Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},\n              booktitle={Proceedings of the second PASCAL challenges workshop on recognising textual entailment},\n              volume={6},\n              number={1},\n              pages={6--4},\n              year={2006},\n              organization={Venice}\n            }\n            @inproceedings{giampiccolo2007third,\n              title={The third pascal recognizing textual entailment challenge},\n              author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill},\n              booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},\n              pages={1--9},\n              year={2007},\n              organization={Association for Computational Linguistics}\n            }\n            @inproceedings{bentivogli2009fifth,\n              title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},\n              author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},\n              booktitle={TAC},\n              year={2009}\n            }""""""\n            ),\n            url=""https://aclweb.org/aclwiki/Recognizing_Textual_Entailment"",\n        ),\n        GlueConfig(\n            name=""wnli"",\n            description=textwrap.dedent(\n                """"""\\\n            The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task\n            in which a system must read a sentence with a pronoun and select the referent of that pronoun from\n            a list of choices. The examples are manually constructed to foil simple statistical methods: Each\n            one is contingent on contextual information provided by a single word or phrase in the sentence.\n            To convert the problem into sentence pair classification, we construct sentence pairs by replacing\n            the ambiguous pronoun with each possible referent. The task is to predict if the sentence with the\n            pronoun substituted is entailed by the original sentence. We use a small evaluation set consisting of\n            new examples derived from fiction books that was shared privately by the authors of the original\n            corpus. While the included training set is balanced between two classes, the test set is imbalanced\n            between them (65% not entailment). Also, due to a data quirk, the development set is adversarial:\n            hypotheses are sometimes shared between training and development examples, so if a model memorizes the\n            training examples, they will predict the wrong label on corresponding development set\n            example. As with QNLI, each example is evaluated separately, so there is not a systematic correspondence\n            between a model\'s score on this task and its score on the unconverted original task. We\n            call converted dataset WNLI (Winograd NLI).""""""\n            ),\n            text_features={""sentence1"": ""sentence1"", ""sentence2"": ""sentence2"",},\n            label_classes=[""not_entailment"", ""entailment""],\n            label_column=""label"",\n            data_url=""https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf"",\n            data_dir=""WNLI"",\n            citation=textwrap.dedent(\n                """"""\\\n            @inproceedings{levesque2012winograd,\n              title={The winograd schema challenge},\n              author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},\n              booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},\n              year={2012}\n            }""""""\n            ),\n            url=""https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html"",\n        ),\n        GlueConfig(\n            name=""ax"",\n            description=textwrap.dedent(\n                """"""\\\n            A manually-curated evaluation dataset for fine-grained analysis of\n            system performance on a broad range of linguistic phenomena. This\n            dataset evaluates sentence understanding through Natural Language\n            Inference (NLI) problems. Use a model trained on MulitNLI to produce\n            predictions for this dataset.""""""\n            ),\n            text_features={""premise"": ""sentence1"", ""hypothesis"": ""sentence2"",},\n            label_classes=[""entailment"", ""neutral"", ""contradiction""],\n            label_column="""",  # No label since we only have test set.\n            # We must use a URL shortener since the URL from GLUE is very long and\n            # causes issues in TFDS.\n            data_url=""https://bit.ly/2BOtOJ7"",\n            data_dir="""",  # We are downloading a tsv.\n            citation="""",  # The GLUE citation is sufficient.\n            url=""https://gluebenchmark.com/diagnostics"",\n        ),\n    ]\n\n    def _info(self):\n        features = {text_feature: nlp.Value(""string"") for text_feature in six.iterkeys(self.config.text_features)}\n        if self.config.label_classes:\n            features[""label""] = nlp.features.ClassLabel(names=self.config.label_classes)\n        else:\n            features[""label""] = nlp.Value(""float32"")\n        features[""idx""] = nlp.Value(""int32"")\n        return nlp.DatasetInfo(\n            description=_GLUE_DESCRIPTION,\n            features=nlp.Features(features),\n            homepage=self.config.url,\n            citation=self.config.citation + ""\\n"" + _GLUE_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        if self.config.name == ""ax"":\n            data_file = dl_manager.download(self.config.data_url)\n            return [nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""data_file"": data_file, ""split"": ""test"",})]\n\n        if self.config.name == ""mrpc"":\n            data_dir = None\n            mrpc_files = dl_manager.download({""dev_ids"": _MRPC_DEV_IDS, ""train"": _MRPC_TRAIN, ""test"": _MRPC_TEST,})\n        else:\n            dl_dir = dl_manager.download_and_extract(self.config.data_url)\n            data_dir = os.path.join(dl_dir, self.config.data_dir)\n            mrpc_files = None\n        train_split = nlp.SplitGenerator(\n            name=nlp.Split.TRAIN,\n            gen_kwargs={\n                ""data_file"": os.path.join(data_dir or """", ""train.tsv""),\n                ""split"": ""train"",\n                ""mrpc_files"": mrpc_files,\n            },\n        )\n        if self.config.name == ""mnli"":\n            return [\n                train_split,\n                _mnli_split_generator(""validation_matched"", data_dir, ""dev"", matched=True),\n                _mnli_split_generator(""validation_mismatched"", data_dir, ""dev"", matched=False),\n                _mnli_split_generator(""test_matched"", data_dir, ""test"", matched=True),\n                _mnli_split_generator(""test_mismatched"", data_dir, ""test"", matched=False),\n            ]\n        elif self.config.name == ""mnli_matched"":\n            return [\n                _mnli_split_generator(""validation"", data_dir, ""dev"", matched=True),\n                _mnli_split_generator(""test"", data_dir, ""test"", matched=True),\n            ]\n        elif self.config.name == ""mnli_mismatched"":\n            return [\n                _mnli_split_generator(""validation"", data_dir, ""dev"", matched=False),\n                _mnli_split_generator(""test"", data_dir, ""test"", matched=False),\n            ]\n        else:\n            return [\n                train_split,\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    gen_kwargs={\n                        ""data_file"": os.path.join(data_dir or """", ""dev.tsv""),\n                        ""split"": ""dev"",\n                        ""mrpc_files"": mrpc_files,\n                    },\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    gen_kwargs={\n                        ""data_file"": os.path.join(data_dir or """", ""test.tsv""),\n                        ""split"": ""test"",\n                        ""mrpc_files"": mrpc_files,\n                    },\n                ),\n            ]\n\n    def _generate_examples(self, data_file, split, mrpc_files=None):\n        if self.config.name == ""mrpc"":\n            # We have to prepare the MRPC dataset from the original sources ourselves.\n            examples = self._generate_example_mrpc_files(mrpc_files=mrpc_files, split=split)\n            for example in examples:\n                yield example[""idx""], example\n        else:\n            process_label = self.config.process_label\n            label_classes = self.config.label_classes\n\n            # The train and dev files for CoLA are the only tsv files without a\n            # header.\n            is_cola_non_test = self.config.name == ""cola"" and split != ""test""\n\n            with open(data_file, encoding=\'utf8\') as f:\n                reader = csv.DictReader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n                if is_cola_non_test:\n                    reader = csv.reader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n\n                for n, row in enumerate(reader):\n                    if is_cola_non_test:\n                        row = {\n                            ""sentence"": row[3],\n                            ""is_acceptable"": row[1],\n                        }\n\n                    example = {feat: row[col] for feat, col in six.iteritems(self.config.text_features)}\n                    example[""idx""] = n\n\n                    if self.config.label_column in row:\n                        label = row[self.config.label_column]\n                        # For some tasks, the label is represented as 0 and 1 in the tsv\n                        # files and needs to be cast to integer to work with the feature.\n                        if label_classes and label not in label_classes:\n                            label = int(label) if label else None\n                        example[""label""] = process_label(label)\n                    else:\n                        example[""label""] = process_label(-1)\n\n                    # Filter out corrupted rows.\n                    for value in six.itervalues(example):\n                        if value is None:\n                            break\n                    else:\n                        yield example[""idx""], example\n\n    def _generate_example_mrpc_files(self, mrpc_files, split):\n        if split == ""test"":\n            with open(mrpc_files[""test""]) as f:\n                reader = csv.DictReader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n                for n, row in enumerate(reader):\n                    yield {\n                        ""sentence1"": row[""#1 String""],\n                        ""sentence2"": row[""#2 String""],\n                        ""label"": -1,\n                        ""idx"": n,\n                    }\n        else:\n            with open(mrpc_files[""dev_ids""]) as f:\n                reader = csv.reader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n                dev_ids = [[row[0], row[1]] for row in reader]\n            with open(mrpc_files[""train""]) as f:\n                # The first 3 bytes are the utf-8 BOM \\xef\\xbb\\xbf, which messes with\n                # the Quality key.\n                f.seek(3)\n                reader = csv.DictReader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n                for n, row in enumerate(reader):\n                    is_row_in_dev = [row[""#1 ID""], row[""#2 ID""]] in dev_ids\n                    if is_row_in_dev == (split == ""dev""):\n                        yield {\n                            ""sentence1"": row[""#1 String""],\n                            ""sentence2"": row[""#2 String""],\n                            ""label"": int(row[""Quality""]),\n                            ""idx"": n,\n                        }\n\n\ndef _mnli_split_generator(name, data_dir, split, matched):\n    return nlp.SplitGenerator(\n        name=name,\n        gen_kwargs={\n            ""data_file"": os.path.join(data_dir, ""%s_%s.tsv"" % (split, ""matched"" if matched else ""mismatched"")),\n            ""split"": split,\n            ""mrpc_files"": None,\n        },\n    )\n'"
datasets/hansards/hansards.py,0,"b'""""""TODO(hansards): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport glob\nimport os\n\nimport nlp\n\n\n# TODO(hansards): BibTeX citation\n_CITATION = """"""\n""""""\n\n# TODO(hansards):\n_DESCRIPTION = """"""\nThis release contains 1.3 million pairs of aligned text chunks (sentences or smaller fragments)\nfrom the official records (Hansards) of the 36th Canadian Parliament.\n\nThe complete Hansards of the debates in the House and Senate of the 36th Canadian Parliament,\nas far as available, were aligned. The corpus was then split into 5 sets of sentence pairs:\ntraining (80% of the sentence pairs), two sets of sentence pairs for testing (5% each), and\ntwo sets of sentence pairs for final evaluation (5% each). The current release consists of the\ntraining and testing sets. The evaluation sets are reserved for future MT evaluation purposes\nand currently not available.\n\nCaveats\n1. This release contains only sentence pairs. Even though the order of the sentences is the same\nas in the original, there may be gaps resulting from many-to-one, many-to-many, or one-to-many\nalignments that were filtered out. Therefore, this release may not be suitable for\ndiscourse-related research.\n2. Neither the sentence splitting nor the alignments are perfect. In particular, watch out for\npairs that differ considerably in length. You may want to filter these out before you do\nany statistical training.\n\nThe alignment of the Hansards was performed as part of the ReWrite project under funding\nfrom the DARPA TIDES program.\n""""""\n\n_URL = ""https://www.isi.edu/natural-language/download/hansard/""\n_DATA_URL = ""http://www.isi.edu/natural-language/download/hansard/""\n_HOUSE_DEBATES_TRAIN_SET_FILE = ""hansard.36.r2001-1a.house.debates.training.tar""\n_HOUSE_DEBATES_TEST_SET_FILE = ""hansard.36.r2001-1a.house.debates.testing.tar""\n_SENATE_DEBATES_TRAIN_SET_FILE = ""hansard.36.r2001-1a.senate.debates.training.tar""\n_SENATE_DEBATES_TEST_SET_FILE = ""hansard.36.r2001-1a.senate.debates.testing.tar""\n\n\nclass HansardsConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Hansards.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for Hansards.\n        Args:\n          **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(HansardsConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass Hansards(nlp.GeneratorBasedBuilder):\n    """"""TODO(hansards): Short description of my dataset.""""""\n\n    # TODO(hansards): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        HansardsConfig(\n            name=""house"",\n            description=""""""\\\n          Alignment of debates in the House of the 36th Canadian Parliament: 1,070K sentence pairs.\n          """""",\n        ),\n        HansardsConfig(\n            name=""senate"",\n            description=""""""\\\n          Alignment of debates in the Senate of the 36th Canadian Parliament: 208K sentence pairs.\n          """""",\n        ),\n    ]\n\n    def _info(self):\n        # TODO(hansards): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""fr"": nlp.Value(""string""),\n                    ""en"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(hansards): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        name = self.config.name\n        if name == ""house"":\n            urls_to_download = {\n                ""train"": os.path.join(_DATA_URL, _HOUSE_DEBATES_TRAIN_SET_FILE),\n                ""test"": os.path.join(_DATA_URL, _HOUSE_DEBATES_TEST_SET_FILE),\n            }\n        elif name == ""senate"":\n            urls_to_download = {\n                ""train"": os.path.join(_DATA_URL, _SENATE_DEBATES_TRAIN_SET_FILE),\n                ""test"": os.path.join(_DATA_URL, _SENATE_DEBATES_TEST_SET_FILE),\n            }\n        else:\n            raise ValueError(""Wrong builder config name \'{}\', it has to be either \'house\' or \'senate\'."".format(name))\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n        if type(downloaded_files) == str:\n            downloaded_files = {k: downloaded_files for k in urls_to_download.keys()}\n        fr_files = {}\n        en_files = {}\n        for split_name in downloaded_files.keys():\n            archive_dir = ""hansard.36/Release-2001.1a/sentence-pairs/{}/debates/development/{}"".format(\n                name, split_name + ""ing""\n            )\n            data_dir = os.path.join(downloaded_files[split_name], archive_dir)\n            split_compress_files = list(sorted(glob.glob(os.path.join(data_dir, ""*.gz""))))\n            split_compress_files += list(sorted(glob.glob(os.path.join(data_dir, ""**/*.gz""))))\n            fr_split_compress_files = sorted([f for f in split_compress_files if f.endswith("".f.gz"")])\n            en_split_compress_files = sorted([f for f in split_compress_files if f.endswith("".e.gz"")])\n            fr_files[split_name] = dl_manager.extract(fr_split_compress_files)\n            en_files[split_name] = dl_manager.extract(en_split_compress_files)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""fr_files"": fr_files[""train""], ""en_files"": en_files[""train""]},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""fr_files"": fr_files[""test""], ""en_files"": en_files[""test""]},\n            ),\n        ]\n\n    def _generate_examples(self, fr_files, en_files):\n        """"""Yields examples.""""""\n        # TODO(hansards): Yields (key, example) tuples from the dataset\n        for fr_file, en_file in zip(fr_files, en_files):\n            with open(fr_file, ""rb"") as fr:\n                with open(en_file, ""rb"") as en:\n                    for j, (fr_line, en_line) in enumerate(zip(fr, en)):\n                        line_id = ""{}:{}"".format(fr_file, j)\n                        rec = {""fr"": fr_line.decode(""ISO-8859-1"").strip(), ""en"": en_line.decode(""ISO-8859-1"").strip()}\n                        yield line_id, rec\n'"
datasets/hellaswag/hellaswag.py,0,"b'""""""TODO(hellaswag): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(hellaswag): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{zellers2019hellaswag,\n    title={HellaSwag: Can a Machine Really Finish Your Sentence?},\n    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},\n    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},\n    year={2019}\n}\n""""""\n\n# TODO(hellaswag):\n_DESCRIPTION = """"""\n""""""\n_URL = ""https://github.com/rowanz/hellaswag/raw/master/data/""\n_TEST_FILE = ""hellaswag_test.jsonl""\n_TRAIN_FILE = ""hellaswag_train.jsonl""\n_DEV_FILE = ""hellaswag_val.jsonl""\n\n\nclass Hellaswag(nlp.GeneratorBasedBuilder):\n    """"""TODO(hellaswag): Short description of my dataset.""""""\n\n    # TODO(hellaswag): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(hellaswag): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""ind"": nlp.Value(""int32""),\n                    ""activity_label"": nlp.Value(""string""),\n                    ""ctx_a"": nlp.Value(""string""),\n                    ""ctx_b"": nlp.Value(""string""),\n                    ""ctx"": nlp.Value(""string""),\n                    ""endings"": nlp.features.Sequence({""ending"": nlp.Value(""string"")}),\n                    ""source_id"": nlp.Value(""string""),\n                    ""split"": nlp.Value(""string""),\n                    ""split_type"": nlp.Value(""string""),\n                    ""label"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://rowanzellers.com/hellaswag/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(hellaswag): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {\n            ""train"": os.path.join(_URL, _TRAIN_FILE),\n            ""test"": os.path.join(_URL, _TEST_FILE),\n            ""dev"": os.path.join(_URL, _DEV_FILE),\n        }\n        dl_dir = dl_manager.download_and_extract(urls_to_download)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""train""]},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""test""]},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""dev""]},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(hellaswag): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n                yield id_, {\n                    ""ind"": int(data[""ind""]),\n                    ""activity_label"": data[""activity_label""],\n                    ""ctx_a"": data.get(""ctx_a"", """"),\n                    ""ctx_b"": data.get(""ctx_b"", """"),\n                    ""ctx"": data[""ctx""],\n                    ""endings"": {""ending"": data.get(""endings"", [])},\n                    ""source_id"": data[""source_id""],\n                    ""split"": data[""split""],\n                    ""split_type"": data[""split_type""],\n                    ""label"": str(data.get(""label"", """")),\n                }\n'"
datasets/imdb/imdb.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""IMDB movie reviews dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nLarge Movie Review Dataset.\nThis is a dataset for binary sentiment classification containing substantially \\\nmore data than previous benchmark datasets. We provide a set of 25,000 highly \\\npolar movie reviews for training, and 25,000 for testing. There is additional \\\nunlabeled data for use as well.\\\n""""""\n\n_CITATION = """"""\\\n@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n  title     = {Learning Word Vectors for Sentiment Analysis},\n  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n  month     = {June},\n  year      = {2011},\n  address   = {Portland, Oregon, USA},\n  publisher = {Association for Computational Linguistics},\n  pages     = {142--150},\n  url       = {http://www.aclweb.org/anthology/P11-1015}\n}\n""""""\n\n_DOWNLOAD_URL = ""http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz""\n\n\nclass IMDBReviewsConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for IMDBReviews.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for IMDBReviews.\n\n    Args:\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(IMDBReviewsConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass Imdb(nlp.GeneratorBasedBuilder):\n    """"""IMDB movie reviews dataset.""""""\n\n    BUILDER_CONFIGS = [IMDBReviewsConfig(name=""plain_text"", description=""Plain text"",)]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {""text"": nlp.Value(""string""), ""label"": nlp.features.ClassLabel(names=[""neg"", ""pos""])}\n            ),\n            supervised_keys=None,\n            homepage=""http://ai.stanford.edu/~amaas/data/sentiment/"",\n            citation=_CITATION,\n        )\n\n    def _vocab_text_gen(self, archive):\n        for _, ex in self._generate_examples(archive, os.path.join(""aclImdb"", ""train"")):\n            yield ex[""text""]\n\n    def _split_generators(self, dl_manager):\n        arch_path = dl_manager.download_and_extract(_DOWNLOAD_URL)\n        data_dir = os.path.join(arch_path, ""aclImdb"")\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""directory"": os.path.join(data_dir, ""train"")}),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""directory"": os.path.join(data_dir, ""test"")}),\n            nlp.SplitGenerator(\n                name=nlp.Split(""unsupervised""),\n                gen_kwargs={""directory"": os.path.join(data_dir, ""train""), ""labeled"": False},\n            ),\n        ]\n\n    def _generate_examples(self, directory, labeled=True):\n        """"""Generate IMDB examples.""""""\n        # For labeled examples, extract the label from the path.\n        if labeled:\n            files = {\n                ""pos"": sorted(os.listdir(os.path.join(directory, ""pos""))),\n                ""neg"": sorted(os.listdir(os.path.join(directory, ""neg""))),\n            }\n            for key in files:\n                for id_, file in enumerate(files[key]):\n                    filepath = os.path.join(directory, key, file)\n                    with open(filepath) as f:\n                        yield key + ""_"" + str(id_), {""text"": f.read(), ""label"": key}\n        else:\n            unsup_files = sorted(os.listdir(os.path.join(directory, ""unsup"")))\n            for id_, file in enumerate(unsup_files):\n                filepath = os.path.join(directory, ""unsup"", file)\n                with open(filepath) as f:\n                    yield id_, {""text"": f.read(), ""label"": -1}\n'"
datasets/jeopardy/jeopardy.py,0,"b'""""""TODO(jeopardy): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(jeopardy): BibTeX citation\n_CITATION = """"""\n""""""\n\n# TODO(jeopardy):\n_DESCRIPTION = """"""\nDataset containing 216,930 Jeopardy questions, answers and other data.\n\nThe json file is an unordered list of questions where each question has\n\'category\' : the question category, e.g. ""HISTORY""\n\'value\' : integer $ value of the question as string, e.g. ""200""\nNote: This is ""None"" for Final Jeopardy! and Tiebreaker questions\n\'question\' : text of question\nNote: This sometimes contains hyperlinks and other things messy text such as when there\'s a picture or video question\n\'answer\' : text of answer\n\'round\' : one of ""Jeopardy!"",""Double Jeopardy!"",""Final Jeopardy!"" or ""Tiebreaker""\nNote: Tiebreaker questions do happen but they\'re very rare (like once every 20 years)\n\'show_number\' : int of show number, e.g \'4680\'\n\'air_date\' : string of the show air date in format YYYY-MM-DD\n""""""\n_URL = ""https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/""\n_DATA_URL = ""http://skeeto.s3.amazonaws.com/share/JEOPARDY_QUESTIONS1.json.gz""\n_DATA_FILE = ""JEOPARDY_QUESTIONS1.json""\n\n\nclass Jeopardy(nlp.GeneratorBasedBuilder):\n    """"""TODO(jeopardy): Short description of my dataset.""""""\n\n    # TODO(jeopardy): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(jeopardy): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""category"": nlp.Value(""string""),\n                    ""air_date"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""value"": nlp.Value(""int32""),\n                    ""answer"": nlp.Value(""string""),\n                    ""round"": nlp.Value(""string""),\n                    ""category"": nlp.Value(""string""),\n                    ""show_number"": nlp.Value(""int32""),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(jeopardy): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        filepath = dl_manager.download_and_extract(_DATA_URL)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": filepath}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(jeopardy): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for i, example in enumerate(data):\n                category = example[""category""]\n                air_date = example[""air_date""]\n                question = example[""question""]\n                if example[""value""] is None:\n                    value = -1  # for Final Jeopardy! and Tiebreaker questions\n                else:\n                    value = int(example[""value""][1:].replace("","", """"))\n                answer = example[""answer""]\n                round = example[""round""]\n                show_number = int(example[""show_number""])\n                yield i, {\n                    ""category"": category,\n                    ""air_date"": air_date,\n                    ""question"": question,\n                    ""value"": value,\n                    ""answer"": answer,\n                    ""round"": round,\n                    ""category"": category,\n                    ""show_number"": show_number,\n                }\n'"
datasets/json/json.py,0,"b'# coding=utf-8\n\nfrom dataclasses import dataclass\n\nimport pyarrow as pa\nimport pyarrow.json as paj\n\nimport nlp\n\n\n@dataclass\nclass JsonConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for JSON.""""""\n\n    read_options: paj.ReadOptions = paj.ReadOptions()\n    parse_options: paj.ParseOptions = paj.ParseOptions()\n\n    @property\n    def pa_read_options(self):\n        return self.read_options\n\n    @property\n    def pa_parse_options(self):\n        return self.parse_options\n\n\nclass Json(nlp.ArrowBasedBuilder):\n    BUILDER_CONFIG_CLASS = JsonConfig\n\n    def _info(self):\n        return nlp.DatasetInfo()\n\n    def _split_generators(self, dl_manager):\n        """""" We handle string, list and dicts in datafiles\n        """"""\n        if isinstance(self.config.data_files, (str, list, tuple)):\n            files = self.config.data_files\n            if isinstance(files, str):\n                files = [files]\n            return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""files"": files})]\n        splits = []\n        for split_name in [nlp.Split.TRAIN, nlp.Split.VALIDATION, nlp.Split.TEST]:\n            if split_name in self.config.data_files:\n                files = self.config.data_files[split_name]\n                if isinstance(files, str):\n                    files = [files]\n                splits.append(nlp.SplitGenerator(name=split_name, gen_kwargs={""files"": files}))\n        return splits\n\n    def _generate_tables(self, files):\n        for i, file in enumerate(files):\n            pa_table = paj.read_json(\n                file, read_options=self.config.pa_read_options, parse_options=self.config.pa_parse_options,\n            )\n            yield i, pa_table\n'"
datasets/kor_nli/kor_nli.py,0,"b'""""""TODO(kor_nli): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n# TODO(kor_nli): BibTeX citation\n_CITATION = """"""\\\n@article{ham2020kornli,\n  title={KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding},\n  author={Ham, Jiyeon and Choe, Yo Joong and Park, Kyubyong and Choi, Ilji and Soh, Hyungjoon},\n  journal={arXiv preprint arXiv:2004.03289},\n  year={2020}\n}\n""""""\n\n# TODO(kor_nli):\n_DESCRIPTION = """""" Korean Natural  Language Inference datasets\n""""""\n_URL = ""https://github.com/kakaobrain/KorNLUDatasets/archive/master.zip""\n\n\nclass KorNLIConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for KorNLI.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for KorNLI.\n\n    Args:\n\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        # Version 1.1.0 remove empty document and summary strings.\n        super(KorNLIConfig, self).__init__(version=nlp.Version(""1.0.0""), **kwargs)\n\n\nclass KorNli(nlp.GeneratorBasedBuilder):\n    """"""TODO(kor_nli): Short description of my dataset.""""""\n\n    # TODO(kor_nli): Set up version.\n    VERSION = nlp.Version(""1.0.0"")\n    BUILDER_CONFIGS = [\n        KorNLIConfig(name=""multi_nli"", description=""Korean multi NLI datasets""),\n        KorNLIConfig(name=""snli"", description=""Korean SNLI dataset""),\n        KorNLIConfig(name=""xnli"", description=""Korean XNLI dataset""),\n    ]\n\n    def _info(self):\n        # TODO(kor_nli): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""sentence1"": nlp.Value(""string""),\n                    ""sentence2"": nlp.Value(""string""),\n                    ""gold_label"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/kakaobrain/KorNLUDatasets"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(kor_nli): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        dl_dir = os.path.join(dl_dir, ""KorNLUDatasets-master"", ""KorNLI"")\n        if self.config.name == ""multi_nli"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dl_dir, ""multinli.train.ko.tsv"")},\n                ),\n            ]\n        elif self.config.name == ""snli"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dl_dir, ""snli_1.0_train.ko.tsv"")},\n                ),\n            ]\n        else:\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dl_dir, ""xnli.dev.ko.tsv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dl_dir, ""xnli.test.ko.tsv"")},\n                ),\n            ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(kor_nli): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = csv.DictReader(f, dialect=""excel-tab"")\n            for id_, row in enumerate(data):\n\n                if len(row) != 3:\n                    continue\n                yield id_, row\n'"
datasets/lc_quad/lc_quad.py,0,"b'""""""TODO(lc_quad): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(lc_quad): BibTeX citation\n_CITATION = """"""\n@inproceedings{dubey2017lc2,\ntitle={LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and DBpedia},\nauthor={Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},\nbooktitle={Proceedings of the 18th International Semantic Web Conference (ISWC)},\nyear={2019},\norganization={Springer}\n}\n""""""\n\n# TODO(lc_quad):\n_DESCRIPTION = """"""\\\nLC-QuAD 2.0 is a Large Question Answering dataset with 30,000 pairs of question and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version. Please see our paper for details about the dataset creation process and framework.\n""""""\n_URL = ""https://github.com/AskNowQA/LC-QuAD2.0/archive/master.zip""\n\n\nclass LcQuad(nlp.GeneratorBasedBuilder):\n    """"""TODO(lc_quad): Short description of my dataset.""""""\n\n    # TODO(lc_quad): Set up version.\n    VERSION = nlp.Version(""2.0.0"")\n\n    def _info(self):\n        # TODO(lc_quad): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""NNQT_question"": nlp.Value(""string""),\n                    ""uid"": nlp.Value(""int32""),\n                    ""subgraph"": nlp.Value(""string""),\n                    ""template_index"": nlp.Value(""int32""),\n                    ""question"": nlp.Value(""string""),\n                    ""sparql_wikidata"": nlp.Value(""string""),\n                    ""sparql_dbpedia18"": nlp.Value(""string""),\n                    ""template"": nlp.Value(""string""),\n                    # ""template_id"": nlp.Value(\'string\'),\n                    ""paraphrased_question"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""http://lc-quad.sda.tech/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(lc_quad): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        dl_dir = os.path.join(dl_dir, ""LC-QuAD2.0-master"", ""dataset"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""train.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""test.json"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(lc_quad): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for id_, row in enumerate(data):\n                is_list = False\n                for key in row:\n                    if key != ""answer"" and isinstance(row[key], list):\n                        is_list = True\n                if is_list:\n                    continue\n                yield id_, {\n                    ""NNQT_question"": row[""NNQT_question""],\n                    ""uid"": row[""uid""],\n                    ""subgraph"": row[""subgraph""],\n                    ""template_index"": row[""template_index""],\n                    ""question"": row[""question""],\n                    ""sparql_wikidata"": row[""sparql_wikidata""],\n                    ""sparql_dbpedia18"": row[""sparql_dbpedia18""],\n                    ""template"": row[""template""],\n                    # ""template_id"": str(row[\'template_id\']),\n                    ""paraphrased_question"": row[""paraphrased_question""],\n                }\n'"
datasets/librispeech_lm/librispeech_lm.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Librispeech language modeling dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport nlp\n\n\n_CITATION = """"""\\\n@inproceedings{panayotov2015librispeech,\n  title={Librispeech: an ASR corpus based on public domain audio books},\n  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},\n  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},\n  pages={5206--5210},\n  year={2015},\n  organization={IEEE}\n}\n""""""\n\n_DESCRIPTION = """"""\\\nLanguage modeling resources to be used in conjunction with the LibriSpeech ASR corpus.\n""""""\n\n_URL = ""http://www.openslr.org/11""\n\n_DL_URL = ""http://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz""\n\n\nclass LibrispeechLm(nlp.GeneratorBasedBuilder):\n    """"""Librispeech language modeling dataset.""""""\n\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""text"": nlp.Value(""string""),}),\n            supervised_keys=(""text"", ""text""),\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        archive_path = dl_manager.download_and_extract(_DL_URL)\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""archive_path"": archive_path}),\n        ]\n\n    def _generate_examples(self, archive_path):\n        """"""Yields examples.""""""\n        with open(archive_path, ""r"") as f:\n            for key, line in enumerate(f):\n                text = line.strip()\n                if text:  # Skip empty lines.\n                    yield key, {""text"": text}\n'"
datasets/lm1b/lm1b.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The Language Model 1 Billion dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport glob\nimport logging\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@article{DBLP:journals/corr/ChelbaMSGBK13,\n  author    = {Ciprian Chelba and\n               Tomas Mikolov and\n               Mike Schuster and\n               Qi Ge and\n               Thorsten Brants and\n               Phillipp Koehn},\n  title     = {One Billion Word Benchmark for Measuring Progress in Statistical Language\n               Modeling},\n  journal   = {CoRR},\n  volume    = {abs/1312.3005},\n  year      = {2013},\n  url       = {http://arxiv.org/abs/1312.3005},\n  archivePrefix = {arXiv},\n  eprint    = {1312.3005},\n  timestamp = {Mon, 13 Aug 2018 16:46:16 +0200},\n  biburl    = {https://dblp.org/rec/bib/journals/corr/ChelbaMSGBK13},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n""""""\n\n_DESCRIPTION = """"""\\\nA benchmark corpus to be used for measuring progress in statistical language \\\nmodeling. This has almost one billion words in the training data.\n""""""\n\n_DOWNLOAD_URL = ""http://www.statmt.org/lm-benchmark/"" ""1-billion-word-language-modeling-benchmark-r13output.tar.gz""\n_TOP_LEVEL_DIR = ""1-billion-word-language-modeling-benchmark-r13output""\n_TRAIN_FILE_FORMAT = os.path.join(_TOP_LEVEL_DIR, ""training-monolingual.tokenized.shuffled"", ""news.en-*"")\n_HELDOUT_FILE_FORMAT = os.path.join(_TOP_LEVEL_DIR, ""heldout-monolingual.tokenized.shuffled"", ""news.en.heldout-*"")\n\n\nclass Lm1bConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Lm1b.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for Lm1b.\n\n    Args:\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(Lm1bConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\ndef _train_data_filenames(tmp_dir):\n    return sorted(glob.glob(os.path.join(tmp_dir, _TRAIN_FILE_FORMAT)))\n\n\ndef _test_data_filenames(tmp_dir):\n    return sorted(glob.glob(os.path.join(tmp_dir, _HELDOUT_FILE_FORMAT)))\n\n\nclass Lm1b(nlp.GeneratorBasedBuilder):\n    """"""1 Billion Word Language Model Benchmark dataset.""""""\n\n    BUILDER_CONFIGS = [\n        Lm1bConfig(name=""plain_text"", description=""Plain text"",),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""text"": nlp.Value(""string"")}),\n            supervised_keys=(""text"", ""text""),\n            homepage=""http://www.statmt.org/lm-benchmark/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        lm1b_path = dl_manager.download_and_extract(_DOWNLOAD_URL)\n\n        train_files = _train_data_filenames(lm1b_path)\n        test_files = _test_data_filenames(lm1b_path)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""files"": train_files}),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""files"": test_files}),\n        ]\n\n    def _generate_examples(self, files):\n        for filepath in files:\n            logging.info(""generating examples from = %s"", filepath)\n            with open(filepath) as f:\n                for idx, line in enumerate(f):\n                    yield ""%s_%d"" % (os.path.basename(filepath), idx), {\n                        ""text"": line.strip(),\n                    }\n'"
datasets/math_dataset/math_dataset.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Mathematics database.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@article{2019arXiv,\n  author = {Saxton, Grefenstette, Hill, Kohli},\n  title = {Analysing Mathematical Reasoning Abilities of Neural Models},\n  year = {2019},\n  journal = {arXiv:1904.01557}\n}\n""""""\n\n_DESCRIPTION = """"""\nMathematics database.\n\nThis dataset code generates mathematical question and answer pairs,\nfrom a range of question types at roughly school-level difficulty.\nThis is designed to test the mathematical learning and algebraic\nreasoning skills of learning models.\n\nOriginal paper: Analysing Mathematical Reasoning Abilities of Neural Models\n(Saxton, Grefenstette, Hill, Kohli).\n\nExample usage:\ntrain_examples, val_examples = nlp.load_dataset(\n    \'math_dataset/arithmetic__mul\',\n    split=[\'train\', \'test\'],\n    as_supervised=True)\n""""""\n\n_DATA_URL = ""https://storage.googleapis.com/mathematics-dataset/mathematics_dataset-v1.0.tar.gz""\n\n_TRAIN_CATEGORY = [\n    ""train-easy"",\n    ""train-medium"",\n    ""train-hard"",\n]\n\n_INTERPOLATE_CATEGORY = [\n    ""interpolate"",\n]\n\n_MODULES = [\n    # extrapolate\n    ""measurement__conversion"",\n    # interpolate\n    ""algebra__linear_1d"",\n    ""algebra__linear_1d_composed"",\n    ""algebra__linear_2d"",\n    ""algebra__linear_2d_composed"",\n    ""algebra__polynomial_roots"",\n    ""algebra__polynomial_roots_composed"",\n    ""algebra__sequence_next_term"",\n    ""algebra__sequence_nth_term"",\n    ""arithmetic__add_or_sub"",\n    ""arithmetic__add_or_sub_in_base"",\n    ""arithmetic__add_sub_multiple"",\n    ""arithmetic__div"",\n    ""arithmetic__mixed"",\n    ""arithmetic__mul"",\n    ""arithmetic__mul_div_multiple"",\n    ""arithmetic__nearest_integer_root"",\n    ""arithmetic__simplify_surd"",\n    ""calculus__differentiate"",\n    ""calculus__differentiate_composed"",\n    ""comparison__closest"",\n    ""comparison__closest_composed"",\n    ""comparison__kth_biggest"",\n    ""comparison__kth_biggest_composed"",\n    ""comparison__pair"",\n    ""comparison__pair_composed"",\n    ""comparison__sort"",\n    ""comparison__sort_composed"",\n    ""measurement__conversion"",\n    ""measurement__time"",\n    ""numbers__base_conversion"",\n    ""numbers__div_remainder"",\n    ""numbers__div_remainder_composed"",\n    ""numbers__gcd"",\n    ""numbers__gcd_composed"",\n    ""numbers__is_factor"",\n    ""numbers__is_factor_composed"",\n    ""numbers__is_prime"",\n    ""numbers__is_prime_composed"",\n    ""numbers__lcm"",\n    ""numbers__lcm_composed"",\n    ""numbers__list_prime_factors"",\n    ""numbers__list_prime_factors_composed"",\n    ""numbers__place_value"",\n    ""numbers__place_value_composed"",\n    ""numbers__round_number"",\n    ""numbers__round_number_composed"",\n    ""polynomials__add"",\n    ""polynomials__coefficient_named"",\n    ""polynomials__collect"",\n    ""polynomials__compose"",\n    ""polynomials__evaluate"",\n    ""polynomials__evaluate_composed"",\n    ""polynomials__expand"",\n    ""polynomials__simplify_power"",\n    ""probability__swr_p_level_set"",\n    ""probability__swr_p_sequence"",\n    # train-easy train-medium train-hard\n    ""algebra__linear_1d"",\n    ""algebra__linear_1d_composed"",\n    ""algebra__linear_2d"",\n    ""algebra__linear_2d_composed"",\n    ""algebra__polynomial_roots"",\n    ""algebra__polynomial_roots_composed"",\n    ""algebra__sequence_next_term"",\n    ""algebra__sequence_nth_term"",\n    ""arithmetic__add_or_sub"",\n    ""arithmetic__add_or_sub_in_base"",\n    ""arithmetic__add_sub_multiple"",\n    ""arithmetic__div"",\n    ""arithmetic__mixed"",\n    ""arithmetic__mul"",\n    ""arithmetic__mul_div_multiple"",\n    ""arithmetic__nearest_integer_root"",\n    ""arithmetic__simplify_surd"",\n    ""calculus__differentiate"",\n    ""calculus__differentiate_composed"",\n    ""comparison__closest"",\n    ""comparison__closest_composed"",\n    ""comparison__kth_biggest"",\n    ""comparison__kth_biggest_composed"",\n    ""comparison__pair"",\n    ""comparison__pair_composed"",\n    ""comparison__sort"",\n    ""comparison__sort_composed"",\n    ""measurement__conversion"",\n    ""measurement__time"",\n    ""numbers__base_conversion"",\n    ""numbers__div_remainder"",\n    ""numbers__div_remainder_composed"",\n    ""numbers__gcd"",\n    ""numbers__gcd_composed"",\n    ""numbers__is_factor"",\n    ""numbers__is_factor_composed"",\n    ""numbers__is_prime"",\n    ""numbers__is_prime_composed"",\n    ""numbers__lcm"",\n    ""numbers__lcm_composed"",\n    ""numbers__list_prime_factors"",\n    ""numbers__list_prime_factors_composed"",\n    ""numbers__place_value"",\n    ""numbers__place_value_composed"",\n    ""numbers__round_number"",\n    ""numbers__round_number_composed"",\n    ""polynomials__add"",\n    ""polynomials__coefficient_named"",\n    ""polynomials__collect"",\n    ""polynomials__compose"",\n    ""polynomials__evaluate"",\n    ""polynomials__evaluate_composed"",\n    ""polynomials__expand"",\n    ""polynomials__simplify_power"",\n    ""probability__swr_p_level_set"",\n    ""probability__swr_p_sequence"",\n]\n\n_QUESTION = ""question""\n_ANSWER = ""answer""\n\n_DATASET_VERSION = ""mathematics_dataset-v1.0""\n\n\ndef _generate_builder_configs():\n    """"""Generate configs with different subsets of mathematics dataset.""""""\n    configs = []\n    for module in sorted(set(_MODULES)):\n        configs.append(nlp.BuilderConfig(name=module, version=nlp.Version(""1.0.0""), description=_DESCRIPTION,))\n\n    return configs\n\n\nclass MathDataset(nlp.GeneratorBasedBuilder):\n    """"""Math Dataset.""""""\n\n    BUILDER_CONFIGS = _generate_builder_configs()\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({_QUESTION: nlp.Value(""string""), _ANSWER: nlp.Value(""string""),}),\n            supervised_keys=(_QUESTION, _ANSWER),\n            homepage=""https://github.com/deepmind/mathematics_dataset"",\n            citation=_CITATION,\n        )\n\n    def _read_data_from_all_categories(self, directory, config, categories):\n        lines = []\n        for category in categories:\n            data_file = os.path.join(directory, _DATASET_VERSION, category, config)\n            if os.path.exists(data_file):\n                with open(data_file) as f:\n                    ls = f.read().split(""\\n"")\n\n                    for l in ls[::-1]:\n                        if not l:\n                            ls.remove(l)\n\n                    lines.extend(ls)\n\n        return lines\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n\n        directory = dl_manager.download_and_extract(_DATA_URL)\n        config = self.config.name + "".txt""\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={""directory"": directory, ""config"": config, ""categories"": _TRAIN_CATEGORY,},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={""directory"": directory, ""config"": config, ""categories"": _INTERPOLATE_CATEGORY,},\n            ),\n        ]\n\n    def _generate_examples(self, directory, config, categories):\n        """"""Yields examples based on directory, module file..""""""\n\n        lines = self._read_data_from_all_categories(directory, config, categories)\n        logging.info(""%s: %s contains total: %d"", categories, config, len(lines))\n        questions = lines[::2]\n        answers = lines[1::2]\n\n        assert len(answers) == len(questions), ""answers: %d do not match questions: %d"" % (\n            len(answers),\n            len(questions),\n        )\n\n        for idx, (q, a) in enumerate(zip(questions, answers)):\n            result = {_QUESTION: q, _ANSWER: a}\n            if all(result.values()):\n                yield idx, result\n'"
datasets/math_qa/math_qa.py,0,"b'""""""TODO(math_qa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(math_qa): BibTeX citation\n_CITATION = """"""\n""""""\n\n# TODO(math_qa):\n_DESCRIPTION = """"""\nOur dataset is gathered by using a new representation language to annotate over the AQuA-RAT dataset. AQuA-RAT has provided the questions, options, rationale, and the correct options.\n""""""\n_URL = ""https://math-qa.github.io/math-QA/data/MathQA.zip""\n\n\nclass MathQa(nlp.GeneratorBasedBuilder):\n    """"""TODO(math_qa): Short description of my dataset.""""""\n\n    # TODO(math_qa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(math_qa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""Problem"": nlp.Value(""string""),\n                    ""Rationale"": nlp.Value(""string""),\n                    ""options"": nlp.Value(""string""),\n                    ""correct"": nlp.Value(""string""),\n                    ""annotated_formula"": nlp.Value(""string""),\n                    ""linear_formula"": nlp.Value(""string""),\n                    ""category"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://math-qa.github.io/math-QA/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(math_qa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_path = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_path, ""train.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_path, ""test.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_path, ""dev.json"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(math_qa): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for id_, row in enumerate(data):\n                yield id_, row\n'"
datasets/mlqa/mlqa.py,0,"b'""""""TODO(mlqa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(mlqa): BibTeX citation\n_CITATION = """"""\\\n@article{lewis2019mlqa,\n  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  journal={arXiv preprint arXiv:1910.07475},\n  year={2019}\n}\n""""""\n\n# TODO(mlqa):\n_DESCRIPTION = """"""\\\n    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n""""""\n_URL = ""https://dl.fbaipublicfiles.com/MLQA/""\n_DEV_TEST_URL = ""MLQA_V1.zip""\n_TRANSLATE_TEST_URL = ""mlqa-translate-test.tar.gz""\n_TRANSLATE_TRAIN_URL = ""mlqa-translate-train.tar.gz""\n_LANG = [""ar"", ""de"", ""vi"", ""zh"", ""en"", ""es"", ""hi""]\n_TRANSLATE_LANG = [""ar"", ""de"", ""vi"", ""zh"", ""es"", ""hi""]\n\n\nclass MlqaConfig(nlp.BuilderConfig):\n    def __init__(self, data_url, **kwargs):\n        """"""BuilderConfig for MLQA\n\n        Args:\n          data_url: `string`, url to the dataset\n          **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(MlqaConfig, self).__init__(version=nlp.Version(""1.0.0"",), **kwargs)\n        self.data_url = data_url\n\n\nclass Mlqa(nlp.GeneratorBasedBuilder):\n    """"""TODO(mlqa): Short description of my dataset.""""""\n\n    # TODO(mlqa): Set up version.\n    VERSION = nlp.Version(""1.0.0"")\n    BUILDER_CONFIGS = (\n        [\n            MlqaConfig(\n                name=""mlqa-translate-train."" + lang,\n                data_url=_URL + _TRANSLATE_TRAIN_URL,\n                description=""Machine-translated data for Translate-train (SQuAD Train and Dev sets machine-translated into ""\n                ""Arabic, German, Hindi, Vietnamese, Simplified Chinese and Spanish)"",\n            )\n            for lang in _LANG\n            if lang != ""en""\n        ]\n        + [\n            MlqaConfig(\n                name=""mlqa-translate-test."" + lang,\n                data_url=_URL + _TRANSLATE_TEST_URL,\n                description=""Machine-translated data for Translate-Test (MLQA-test set machine-translated into English) "",\n            )\n            for lang in _LANG\n            if lang != ""en""\n        ]\n        + [\n            MlqaConfig(\n                name=""mlqa."" + lang1 + ""."" + lang2,\n                data_url=_URL + _DEV_TEST_URL,\n                description=""development and test splits"",\n            )\n            for lang1 in _LANG\n            for lang2 in _LANG\n        ]\n    )\n\n    def _info(self):\n        # TODO(mlqa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""context"": nlp.Value(""string""),\n                    ""questions"": nlp.features.Sequence({""question"": nlp.Value(""string"")}),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                    ""ids"": nlp.features.Sequence({""idx"": nlp.Value(""string"")})\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/facebookresearch/MLQA"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(mlqa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        if self.config.name.startswith(""mlqa-translate-train""):\n            dl_file = dl_manager.download_and_extract(self.config.data_url)\n            lang = self.config.name.split(""."")[-1]\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(\n                            os.path.join(dl_file, ""mlqa-translate-train""),\n                            ""{}_squad-translate-train-train-v1.1.json"".format(lang),\n                        ),\n                        ""lang"": lang,\n                    },\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(\n                            os.path.join(dl_file, ""mlqa-translate-train""),\n                            ""{}_squad-translate-train-dev-v1.1.json"".format(lang),\n                        ),\n                        ""lang"": lang,\n                    },\n                ),\n            ]\n\n        else:\n            if self.config.name.startswith(""mlqa.""):\n                dl_file = dl_manager.download_and_extract(self.config.data_url)\n                name = self.config.name.split(""."")\n                l1, l2 = name[1:]\n                return [\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TEST,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={\n                            ""filepath"": os.path.join(\n                                os.path.join(dl_file, ""MLQA_V1/test""),\n                                ""test-context-{}-question-{}.json"".format(l1, l2),\n                            ),\n                            ""lang"": (l1, l2),\n                        },\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.VALIDATION,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={\n                            ""filepath"": os.path.join(\n                                os.path.join(dl_file, ""MLQA_V1/dev""), ""dev-context-{}-question-{}.json"".format(l1, l2)\n                            ),\n                            ""lang"": (l1, l2),\n                        },\n                    ),\n                ]\n            else:\n                if self.config.name.startswith(""mlqa-translate-test""):\n                    dl_file = dl_manager.download_and_extract(self.config.data_url)\n                    lang = self.config.name.split(""."")[-1]\n                    return [\n                        nlp.SplitGenerator(\n                            name=nlp.Split.TEST,\n                            # These kwargs will be passed to _generate_examples\n                            gen_kwargs={\n                                ""filepath"": os.path.join(\n                                    os.path.join(dl_file, ""mlqa-translate-test""),\n                                    ""translate-test-context-{}-question-{}.json"".format(lang, lang),\n                                ),\n                                ""lang"": lang,\n                            },\n                        ),\n                    ]\n\n    def _generate_examples(self, filepath, lang):\n        """"""Yields examples.""""""\n        # TODO(mlqa): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n        for id1, examples in enumerate(data[""data""]):\n            for id2, example in enumerate(examples[""paragraphs""]):\n                context = example[""context""]\n                questions = [qa[""question""] for qa in example[""qas""]]\n                answers = [qa[""answers""] for qa in example[""qas""]]\n                ids = [qa[""id""] for qa in example[""qas""]]\n                answers_start = [answer[0][""answer_start""] for answer in answers]\n                answers_text = [answer[0][""text""] for answer in answers]\n                yield str(id1) + ""-"" + str(id2), {\n                    ""context"": context,\n                    ""questions"": {""question"": questions,},\n                    ""answers"": {""answer_start"": answers_start, ""text"": answers_text},\n                    ""ids"": {""idx"": ids},\n                }\n'"
datasets/movie_rationales/movie_rationales.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Movie reviews with human annotated rationales.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@unpublished{eraser2019,\n    title = {ERASER: A Benchmark to Evaluate Rationalized NLP Models},\n    author = {Jay DeYoung and Sarthak Jain and Nazneen Fatema Rajani and Eric Lehman and Caiming Xiong and Richard Socher and Byron C. Wallace}\n}\n@InProceedings{zaidan-eisner-piatko-2008:nips,\n  author    =  {Omar F. Zaidan  and  Jason Eisner  and  Christine Piatko},\n  title     =  {Machine Learning with Annotator Rationales to Reduce Annotation Cost},\n  booktitle =  {Proceedings of the NIPS*2008 Workshop on Cost Sensitive Learning},\n  month     =  {December},\n  year      =  {2008}\n}\n""""""\n\n_DESCRIPTION = """"""\nThe movie rationale dataset contains human annotated rationales for movie\nreviews.\n""""""\n\n_DOWNLOAD_URL = ""http://www.eraserbenchmark.com/zipped/movies.tar.gz""\n\n\nclass MovieRationales(nlp.GeneratorBasedBuilder):\n    """"""Movie reviews with human annotated rationales.""""""\n\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""review"": nlp.Value(""string""),\n                    ""label"": nlp.features.ClassLabel(names=[""NEG"", ""POS""]),\n                    ""evidences"": nlp.features.Sequence(nlp.Value(""string"")),\n                }\n            ),\n            supervised_keys=None,\n            homepage=""http://www.cs.jhu.edu/~ozaidan/rationales/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_dir = dl_manager.download_and_extract(_DOWNLOAD_URL)\n        data_dir = os.path.join(dl_dir, ""movies"")\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={""data_dir"": data_dir, ""filepath"": os.path.join(data_dir, ""train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={""data_dir"": data_dir, ""filepath"": os.path.join(data_dir, ""val.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={""data_dir"": data_dir, ""filepath"": os.path.join(data_dir, ""test.jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, data_dir, filepath):\n        """"""Yields examples.""""""\n        reviews_dir = os.path.join(data_dir, ""docs"")\n\n        with open(filepath) as f:\n            for line in f:\n                row = json.loads(line)\n                doc_id = row[""annotation_id""]\n                review_file = os.path.join(reviews_dir, doc_id)\n                with open(review_file) as f1:\n                    review_text = f1.read()\n\n                evidences = []\n                for evidence in row[""evidences""]:\n                    for e in evidence:\n                        evidences.append(e[""text""])\n\n                yield doc_id, {\n                    ""review"": review_text,\n                    ""label"": row[""classification""],\n                    ""evidences"": evidences,\n                }\n'"
datasets/multi_news/multi_news.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Multi-News dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@misc{alex2019multinews,\n    title={Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},\n    author={Alexander R. Fabbri and Irene Li and Tianwei She and Suyi Li and Dragomir R. Radev},\n    year={2019},\n    eprint={1906.01749},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n""""""\n\n_DESCRIPTION = """"""\nMulti-News, consists of news articles and human-written summaries\nof these articles from the site newser.com.\nEach summary is professionally written by editors and\nincludes links to the original articles cited.\n\nThere are two features:\n  - document: text of news articles seperated by special token ""|||||"".\n  - summary: news summary.\n""""""\n\n_URL = ""https://drive.google.com/uc?export=download&id=1vRY2wM6rlOZrf9exGTm5pXj5ExlVwJ0C""\n\n_DOCUMENT = ""document""\n_SUMMARY = ""summary""\n\n\nclass MultiNews(nlp.GeneratorBasedBuilder):\n    """"""Multi-News dataset.""""""\n\n    VERSION = nlp.Version(""1.0.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({_DOCUMENT: nlp.Value(""string""), _SUMMARY: nlp.Value(""string"")}),\n            supervised_keys=(_DOCUMENT, _SUMMARY),\n            homepage=""https://github.com/Alex-Fabbri/Multi-News"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        extract_path = os.path.join(dl_manager.download_and_extract(_URL), ""multi-news-original"")\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""path"": os.path.join(extract_path, ""train"")},),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""path"": os.path.join(extract_path, ""val"")},),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""path"": os.path.join(extract_path, ""test"")},),\n        ]\n\n    def _generate_examples(self, path=None):\n        """"""Yields examples.""""""\n        with open(os.path.join(path + "".src"")) as src_f, open(os.path.join(path + "".tgt"")) as tgt_f:\n            for i, (src_line, tgt_line) in enumerate(zip(src_f, tgt_f)):\n                yield i, {\n                    # In original file, each line has one example and natural newline\n                    # tokens ""\\n"" are being replaced with ""NEWLINE_CHAR"". Here restore\n                    # the natural newline token to avoid special vocab ""NEWLINE_CHAR"".\n                    _DOCUMENT: src_line.strip().replace(""NEWLINE_CHAR"", ""\\n""),\n                    # Remove the starting token ""- "" for every target sequence.\n                    _SUMMARY: tgt_line.strip().lstrip(""- ""),\n                }\n'"
datasets/multi_nli/multi_nli.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The Multi-Genre NLI Corpus.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@InProceedings{N18-1101,\n  author = ""Williams, Adina\n            and Nangia, Nikita\n            and Bowman, Samuel"",\n  title = ""A Broad-Coverage Challenge Corpus for\n           Sentence Understanding through Inference"",\n  booktitle = ""Proceedings of the 2018 Conference of\n               the North American Chapter of the\n               Association for Computational Linguistics:\n               Human Language Technologies, Volume 1 (Long\n               Papers)"",\n  year = ""2018"",\n  publisher = ""Association for Computational Linguistics"",\n  pages = ""1112--1122"",\n  location = ""New Orleans, Louisiana"",\n  url = ""http://aclweb.org/anthology/N18-1101""\n}\n""""""\n\n_DESCRIPTION = """"""\\\nThe Multi-Genre Natural Language Inference (MultiNLI) corpus is a\ncrowd-sourced collection of 433k sentence pairs annotated with textual\nentailment information. The corpus is modeled on the SNLI corpus, but differs in\nthat covers a range of genres of spoken and written text, and supports a\ndistinctive cross-genre generalization evaluation. The corpus served as the\nbasis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.\n""""""\n\n\nclass MultiNLIConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for MultiNLI.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for MultiNLI.\n\n    Args:\n.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(MultiNLIConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass MultiNli(nlp.GeneratorBasedBuilder):\n    """"""MultiNLI: The Stanford Question Answering Dataset. Version 1.1.""""""\n\n    BUILDER_CONFIGS = [\n        MultiNLIConfig(name=""plain_text"", description=""Plain text"",),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""premise"": nlp.Value(""string""),\n                    ""hypothesis"": nlp.Value(""string""),\n                    ""label"": nlp.features.ClassLabel(names=[""entailment"", ""neutral"", ""contradiction""]),\n                }\n            ),\n            # No default supervised_keys (as we have to pass both premise\n            # and hypothesis as input).\n            supervised_keys=None,\n            homepage=""https://www.nyu.edu/projects/bowman/multinli/"",\n            citation=_CITATION,\n        )\n\n    def _vocab_text_gen(self, filepath):\n        for _, ex in self._generate_examples(filepath):\n            yield "" "".join([ex[""premise""], ex[""hypothesis""]])\n\n    def _split_generators(self, dl_manager):\n\n        downloaded_dir = dl_manager.download_and_extract(\n            ""http://storage.googleapis.com/tfds-data/downloads/multi_nli/multinli_1.0.zip""\n        )\n        mnli_path = os.path.join(downloaded_dir, ""multinli_1.0"")\n        train_path = os.path.join(mnli_path, ""multinli_1.0_train.txt"")\n        matched_validation_path = os.path.join(mnli_path, ""multinli_1.0_dev_matched.txt"")\n        mismatched_validation_path = os.path.join(mnli_path, ""multinli_1.0_dev_mismatched.txt"")\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": train_path}),\n            nlp.SplitGenerator(name=""validation_matched"", gen_kwargs={""filepath"": matched_validation_path}),\n            nlp.SplitGenerator(name=""validation_mismatched"", gen_kwargs={""filepath"": mismatched_validation_path}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Generate mnli examples.\n\n    Args:\n      filepath: a string\n\n    Yields:\n      dictionaries containing ""premise"", ""hypothesis"" and ""label"" strings\n    """"""\n        for idx, line in enumerate(open(filepath, ""rb"")):\n            if idx == 0:\n                continue  # skip header\n            line = line.strip().decode(""utf-8"")\n            split_line = line.split(""\\t"")\n            # Examples not marked with a three out of five consensus are marked with\n            # ""-"" and should not be used in standard evaluations.\n            if split_line[0] == ""-"":\n                continue\n            # Works for both splits even though dev has some extra human labels.\n            yield idx, {""premise"": split_line[5], ""hypothesis"": split_line[6], ""label"": split_line[0]}\n'"
datasets/multi_nli_mismatch/multi_nli_mismatch.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The Multi-Genre NLI Corpus.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@InProceedings{N18-1101,\n  author = ""Williams, Adina\n            and Nangia, Nikita\n            and Bowman, Samuel"",\n  title = ""A Broad-Coverage Challenge Corpus for\n           Sentence Understanding through Inference"",\n  booktitle = ""Proceedings of the 2018 Conference of\n               the North American Chapter of the\n               Association for Computational Linguistics:\n               Human Language Technologies, Volume 1 (Long\n               Papers)"",\n  year = ""2018"",\n  publisher = ""Association for Computational Linguistics"",\n  pages = ""1112--1122"",\n  location = ""New Orleans, Louisiana"",\n  url = ""http://aclweb.org/anthology/N18-1101""\n}\n""""""\n\n_DESCRIPTION = """"""\\\nThe Multi-Genre Natural Language Inference (MultiNLI) corpus is a\ncrowd-sourced collection of 433k sentence pairs annotated with textual\nentailment information. The corpus is modeled on the SNLI corpus, but differs in\nthat covers a range of genres of spoken and written text, and supports a\ndistinctive cross-genre generalization evaluation. The corpus served as the\nbasis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.\n""""""\n\nROOT_URL = ""http://storage.googleapis.com/tfds-data/downloads/multi_nli/multinli_1.0.zip""\n\n\nclass MultiNLIMismatchConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for MultiNLI Mismatch.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for MultiNLI Mismatch.\n\n    Args:\n\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(MultiNLIMismatchConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass MultiNliMismatch(nlp.GeneratorBasedBuilder):\n    """"""MultiNLI: The Stanford Question Answering Dataset. Version 1.1.""""""\n\n    BUILDER_CONFIGS = [\n        MultiNLIMismatchConfig(name=""plain_text"", description=""Plain text"",),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {""premise"": nlp.Value(""string""), ""hypothesis"": nlp.Value(""string""), ""label"": nlp.Value(""string""),}\n            ),\n            # No default supervised_keys (as we have to pass both premise\n            # and hypothesis as input).\n            supervised_keys=None,\n            homepage=""https://www.nyu.edu/projects/bowman/multinli/"",\n            citation=_CITATION,\n        )\n\n    def _vocab_text_gen(self, filepath):\n        for _, ex in self._generate_examples(filepath):\n            yield "" "".join([ex[""premise""], ex[""hypothesis""], ex[""label""]])\n\n    def _split_generators(self, dl_manager):\n\n        downloaded_dir = dl_manager.download_and_extract(ROOT_URL)\n        mnli_path = os.path.join(downloaded_dir, ""multinli_1.0"")\n        train_path = os.path.join(mnli_path, ""multinli_1.0_train.txt"")\n\n        validation_path = os.path.join(mnli_path, ""multinli_1.0_dev_mismatched.txt"")\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": train_path}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": validation_path}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Generate mnli mismatch examples.\n\n    Args:\n      filepath: a string\n\n    Yields:\n      dictionaries containing ""premise"", ""hypothesis"" and ""label"" strings\n    """"""\n        for idx, line in enumerate(open(filepath, ""rb"")):\n            if idx == 0:\n                continue\n            line = line.strip().decode(""utf-8"")\n            split_line = line.split(""\\t"")\n            yield idx, {""premise"": split_line[5], ""hypothesis"": split_line[6], ""label"": split_line[0]}\n'"
datasets/natural_questions/natural_questions.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Natural Questions: A Benchmark for Question Answering Research.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport re\n\nimport apache_beam as beam\nimport six\n\nimport nlp\n\n\nif six.PY2:\n    import HTMLParser as html_parser  # pylint:disable=g-import-not-at-top\n\n    html_unescape = html_parser.HTMLParser().unescape\nelse:\n    import html  # pylint:disable=g-import-not-at-top\n\n    html_unescape = html.unescape\n\n_CITATION = """"""\n@article{47761,\ntitle\t= {Natural Questions: a Benchmark for Question Answering Research},\nauthor\t= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},\nyear\t= {2019},\njournal\t= {Transactions of the Association of Computational Linguistics}\n}\n""""""\n\n_DESCRIPTION = """"""\nThe NQ corpus contains questions from real users, and it requires QA systems to\nread and comprehend an entire Wikipedia article that may or may not contain the\nanswer to the question. The inclusion of real user questions, and the\nrequirement that solutions should read an entire page to find the answer, cause\nNQ to be a more realistic and challenging task than prior QA datasets.\n""""""\n\n_URL = ""https://ai.google.com/research/NaturalQuestions/dataset""\n\n_BASE_DOWNLOAD_URL = ""https://storage.googleapis.com/natural_questions/v1.0""\n_DOWNLOAD_URLS = {\n    ""train"": [""%s/train/nq-train-%02d.jsonl.gz"" % (_BASE_DOWNLOAD_URL, i) for i in range(50)],\n    ""validation"": [""%s/dev/nq-dev-%02d.jsonl.gz"" % (_BASE_DOWNLOAD_URL, i) for i in range(5)],\n}\n\n\nclass NaturalQuestions(nlp.BeamBasedBuilder):\n    """"""Natural Questions: A Benchmark for Question Answering Research.""""""\n\n    VERSION = nlp.Version(""0.0.2"")\n    SUPPORTED_VERSIONS = [nlp.Version(""0.0.1"")]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""document"": {\n                        ""title"": nlp.Value(""string""),\n                        ""url"": nlp.Value(""string""),\n                        ""html"": nlp.Value(""string""),\n                        ""tokens"": nlp.features.Sequence({""token"": nlp.Value(""string""), ""is_html"": nlp.Value(""bool"")}),\n                    },\n                    ""question"": {""text"": nlp.Value(""string""), ""tokens"": nlp.features.Sequence(nlp.Value(""string""))},\n                    ""annotations"": nlp.features.Sequence(\n                        {\n                            ""id"": nlp.Value(""string""),\n                            ""long_answer"": {\n                                ""start_token"": nlp.Value(""int64""),\n                                ""end_token"": nlp.Value(""int64""),\n                                ""start_byte"": nlp.Value(""int64""),\n                                ""end_byte"": nlp.Value(""int64""),\n                            },\n                            ""short_answers"": nlp.features.Sequence(\n                                {\n                                    ""start_token"": nlp.Value(""int64""),\n                                    ""end_token"": nlp.Value(""int64""),\n                                    ""start_byte"": nlp.Value(""int64""),\n                                    ""end_byte"": nlp.Value(""int64""),\n                                    ""text"": nlp.Value(""string""),\n                                }\n                            ),\n                            ""yes_no_answer"": nlp.features.ClassLabel(names=[""NO"", ""YES""]),  # Can also be -1 for NONE.\n                        }\n                    ),\n                }\n            ),\n            supervised_keys=None,\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager, pipeline):\n        """"""Returns SplitGenerators.""""""\n\n        files = dl_manager.download(_DOWNLOAD_URLS)\n        if not pipeline.is_local():\n            files = dl_manager.ship_files_with_pipeline(files, pipeline)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepaths"": files[""train""]},),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepaths"": files[""validation""]},),\n        ]\n\n    def _build_pcollection(self, pipeline, filepaths):\n        """"""Build PCollection of examples.""""""\n\n        def _parse_example(line):\n            """"""Parse a single json line and emit an example dict.""""""\n            ex_json = json.loads(line)\n            html_bytes = ex_json[""document_html""].encode(""utf-8"")\n\n            def _parse_short_answer(short_ans):\n                """"""""Extract text of short answer.""""""\n                ans_bytes = html_bytes[short_ans[""start_byte""] : short_ans[""end_byte""]]\n                # Remove non-breaking spaces.\n                ans_bytes = ans_bytes.replace(b""\\xc2\\xa0"", b"" "")\n                text = ans_bytes.decode(""utf-8"")\n                # Remove HTML markup.\n                text = re.sub(""<([^>]*)>"", """", html_unescape(text))\n                # Replace \\xa0 characters with spaces.\n                return {\n                    ""start_token"": short_ans[""start_token""],\n                    ""end_token"": short_ans[""end_token""],\n                    ""start_byte"": short_ans[""start_byte""],\n                    ""end_byte"": short_ans[""end_byte""],\n                    ""text"": text,\n                }\n\n            def _parse_annotation(an_json):\n                return {\n                    # Convert to str since some IDs cannot be represented by nlp.Value(\'int64\').\n                    ""id"": str(an_json[""annotation_id""]),\n                    ""long_answer"": {\n                        ""start_token"": an_json[""long_answer""][""start_token""],\n                        ""end_token"": an_json[""long_answer""][""end_token""],\n                        ""start_byte"": an_json[""long_answer""][""start_byte""],\n                        ""end_byte"": an_json[""long_answer""][""end_byte""],\n                    },\n                    ""short_answers"": [_parse_short_answer(ans) for ans in an_json[""short_answers""]],\n                    ""yes_no_answer"": (-1 if an_json[""yes_no_answer""] == ""NONE"" else an_json[""yes_no_answer""]),\n                }\n\n            beam.metrics.Metrics.counter(""nq"", ""examples"").inc()\n            # Convert to str since some IDs cannot be represented by nlp.Value(\'int64\').\n            id_ = str(ex_json[""example_id""])\n            return (\n                id_,\n                {\n                    ""id"": id_,\n                    ""document"": {\n                        ""title"": ex_json[""document_title""],\n                        ""url"": ex_json[""document_url""],\n                        ""html"": html_bytes,\n                        ""tokens"": [\n                            {""token"": t[""token""], ""is_html"": t[""html_token""]} for t in ex_json[""document_tokens""]\n                        ],\n                    },\n                    ""question"": {""text"": ex_json[""question_text""], ""tokens"": ex_json[""question_tokens""]},\n                    ""annotations"": [_parse_annotation(an_json) for an_json in ex_json[""annotations""]],\n                },\n            )\n\n        return pipeline | beam.Create(filepaths) | beam.io.ReadAllFromText() | beam.Map(_parse_example)\n'"
datasets/newsroom/newsroom.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""NEWSROOM Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@inproceedings{N18-1065,\n  author    = {Grusky, Max and Naaman, Mor and Artzi, Yoav},\n  title     = {NEWSROOM: A Dataset of 1.3 Million Summaries\n               with Diverse Extractive Strategies},\n  booktitle = {Proceedings of the 2018 Conference of the\n               North American Chapter of the Association for\n               Computational Linguistics: Human Language Technologies},\n  year      = {2018},\n}\n""""""\n\n_DESCRIPTION = """"""\nNEWSROOM is a large dataset for training and evaluating summarization systems.\nIt contains 1.3 million articles and summaries written by authors and\neditors in the newsrooms of 38 major publications.\n\nDataset features includes:\n  - text: Input news text.\n  - summary: Summary for the news.\nAnd additional features:\n  - title: news title.\n  - url: url of the news.\n  - date: date of the article.\n  - density: extractive density.\n  - coverage: extractive coverage.\n  - compression: compression ratio.\n  - density_bin: low, medium, high.\n  - coverage_bin: extractive, abstractive.\n  - compression_bin: low, medium, high.\n\nThis dataset can be downloaded upon requests. Unzip all the contents\n""train.jsonl, dev.josnl, test.jsonl"" to the tfds folder.\n\n""""""\n\n_DOCUMENT = ""text""\n_SUMMARY = ""summary""\n_ADDITIONAL_TEXT_FEATURES = [\n    ""title"",\n    ""url"",\n    ""date"",\n    ""density_bin"",\n    ""coverage_bin"",\n    ""compression_bin"",\n]\n_ADDITIONAL_FLOAT_FEATURES = [\n    ""density"",\n    ""coverage"",\n    ""compression"",\n]\n\n\nclass Newsroom(nlp.GeneratorBasedBuilder):\n    """"""NEWSROOM Dataset.""""""\n\n    VERSION = nlp.Version(""1.0.0"")\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  You should download the dataset from http://lil.nlp.cornell.edu/newsroom/\n  The webpage requires registration.\n  To unzip the .tar file run `tar -zxvf complete.tar`. To unzip the .gz files\n  run `gunzip train.json.gz` , ...\n  After downloading, please put the files under the following names\n  dev.jsonl, test.jsonl and train.jsonl in a dir of your choice,\n  which will be used as a manual_dir, e.g. `~/.manual_dirs/newsroom`\n  Newsroom can then be loaded via:\n  `nlp.load(""newsroom"", data_dir=""~/.manual_dirs/newsroom"")`.\n  """"""\n\n    def _info(self):\n        features = {k: nlp.Value(""string"") for k in [_DOCUMENT, _SUMMARY] + _ADDITIONAL_TEXT_FEATURES}\n        features.update({k: nlp.Value(""float32"") for k in _ADDITIONAL_FLOAT_FEATURES})\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(features),\n            supervised_keys=(_DOCUMENT, _SUMMARY),\n            homepage=""http://lil.nlp.cornell.edu/newsroom/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        data_dir = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))\n        if not os.path.exists(data_dir):\n            raise FileNotFoundError(\n                ""{} does not exist. Make sure you insert a manual dir via `nlp.load(\'newsroom\', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}"".format(\n                    data_dir, self.MANUAL_DOWNLOAD_INSTRUCTIONS\n                )\n            )\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""input_file"": os.path.join(data_dir, ""train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""input_file"": os.path.join(data_dir, ""dev.jsonl"")},\n            ),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""input_file"": os.path.join(data_dir, ""test.jsonl"")},),\n        ]\n\n    def _generate_examples(self, input_file=None):\n        """"""Yields examples.""""""\n        with open(input_file) as f:\n            for i, line in enumerate(f):\n                d = json.loads(line)\n                # fields are ""url"", ""archive"", ""title"", ""date"", ""text"",\n                #  ""compression_bin"", ""density_bin"", ""summary"", ""density"",\n                #  ""compression\', ""coverage"", ""coverage_bin"",\n                yield i, {\n                    k: d[k] for k in [_DOCUMENT, _SUMMARY] + _ADDITIONAL_TEXT_FEATURES + _ADDITIONAL_FLOAT_FEATURES\n                }\n'"
datasets/openbookqa/openbookqa.py,0,"b'""""""TODO(openBookQA): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\nimport textwrap\n\nimport nlp\n\n\n# TODO(openBookQA): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{OpenBookQA2018,\n title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},\n author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},\n booktitle={EMNLP},\n year={2018}\n}\n""""""\n\n# TODO(openBookQA):\n_DESCRIPTION = textwrap.dedent(\n    """"""\\\nOpenBookQA aims to promote research in advanced question-answering, probing a deeper understanding of both the topic \n(with salient facts summarized as an open book, also provided with the dataset) and the language it is expressed in. In \nparticular, it contains questions that require multi-step reasoning, use of additional common and commonsense knowledge, \nand rich text comprehension.\nOpenBookQA is a new kind of question-answering dataset modeled after open book exams for assessing human understanding of\na subject. \n""""""\n)\n_URL = ""https://s3-us-west-2.amazonaws.com/ai2-website/data/OpenBookQA-V1-Sep2018.zip""\n\n\nclass OpenbookqaConfig(nlp.BuilderConfig):\n    def __init__(self, data_dir, **kwargs):\n        """""" BuilderConfig for openBookQA dataset \n\n      Args:\n        data_dir: directory for the given dataset name\n        **kwargs: keyword arguments forwarded to super.\n\n      """"""\n\n        super(OpenbookqaConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n        self.data_dir = data_dir\n\n\nclass Openbookqa(nlp.GeneratorBasedBuilder):\n    """"""TODO(openBookQA): Short description of my dataset.""""""\n\n    # TODO(openBookQA): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        OpenbookqaConfig(\n            name=""main"",\n            description=textwrap.dedent(\n                """"""\n                                  It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), \n                                  which probe the understanding of a small \xe2\x80\x9cbook\xe2\x80\x9d of 1,326 core science facts and the application of these facts to novel \n                                  situations. For training, the dataset includes a mapping from each question to the core science fact it was designed to \n                                  probe. Answering OpenBookQA questions requires additional broad common knowledge, not contained in the book. The questions,\n                                  by design, are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. Strong neural\n                                  baselines achieve around 50% on OpenBookQA, leaving a large gap to the 92% accuracy of crowd-workers.\n                                """"""\n            ),\n            data_dir=""Main"",\n        ),\n        OpenbookqaConfig(\n            name=""additional"",\n            description=textwrap.dedent(\n                """"""\n                                  Additionally, we provide 5,167 crowd-sourced common knowledge facts, and an expanded version of the train/dev/test questions where \n                                  each question is associated with its originating core fact, a human accuracy score, a clarity score, and an anonymized crowd-worker \n                                  ID (in the \xe2\x80\x9cAdditional\xe2\x80\x9d folder).\n                                """"""\n            ),\n            data_dir=""Additional"",\n        ),\n    ]\n\n    def _info(self):\n        # TODO(openBookQA): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""id"": nlp.Value(""string""),\n                    ""question_stem"": nlp.Value(""string""),\n                    ""choices"": nlp.features.Sequence({""text"": nlp.Value(""string""), ""label"": nlp.Value(""string"")}),\n                    ""answerKey"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://allenai.org/data/open-book-qa"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(openBookQA): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""OpenBookQA-V1-Sep2018"", ""Data"")\n        data_dir = os.path.join(data_dir, self.config.data_dir)\n        train_file = (\n            os.path.join(data_dir, ""train.jsonl"")\n            if self.config.name == ""main""\n            else os.path.join(data_dir, ""train_complete.jsonl"")\n        )\n        test_file = (\n            os.path.join(data_dir, ""test.jsonl"")\n            if self.config.name == ""main""\n            else os.path.join(data_dir, ""test_complete.jsonl"")\n        )\n        dev_file = (\n            os.path.join(data_dir, ""dev.jsonl"")\n            if self.config.name == ""main""\n            else os.path.join(data_dir, ""dev_complete.jsonl"")\n        )\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": train_file},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": test_file},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dev_file},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(openBookQA): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for row in f:\n                data = json.loads(row)\n                yield data[""id""], {\n                    ""id"": data[""id""],\n                    ""question_stem"": data[""question""][""stem""],\n                    ""choices"": {\n                        ""text"": [choice[""text""] for choice in data[""question""][""choices""]],\n                        ""label"": [choice[""text""] for choice in data[""question""][""choices""]],\n                    },\n                    ""answerKey"": data[""answerKey""],\n                }\n'"
datasets/opinosis/opinosis.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Opinosis Opinion Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@inproceedings{ganesan2010opinosis,\n  title={Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions},\n  author={Ganesan, Kavita and Zhai, ChengXiang and Han, Jiawei},\n  booktitle={Proceedings of the 23rd International Conference on Computational Linguistics},\n  pages={340--348},\n  year={2010},\n  organization={Association for Computational Linguistics}\n}\n""""""\n\n_DESCRIPTION = """"""\nThe Opinosis Opinion Dataset consists of sentences extracted from reviews for 51 topics.\nTopics and opinions are obtained from Tripadvisor, Edmunds.com and Amazon.com.\n""""""\n\n_URL = ""https://github.com/kavgan/opinosis-summarization/raw/master/OpinosisDataset1.0_0.zip""\n\n_REVIEW_SENTS = ""review_sents""\n_SUMMARIES = ""summaries""\n\n\nclass Opinosis(nlp.GeneratorBasedBuilder):\n    """"""Opinosis Opinion Dataset.""""""\n\n    VERSION = nlp.Version(""1.0.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {_REVIEW_SENTS: nlp.Value(""string""), _SUMMARIES: nlp.features.Sequence(nlp.Value(""string""))}\n            ),\n            supervised_keys=(_REVIEW_SENTS, _SUMMARIES),\n            homepage=""http://kavita-ganesan.com/opinosis/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        extract_path = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""path"": extract_path},),\n        ]\n\n    def _generate_examples(self, path=None):\n        """"""Yields examples.""""""\n        topics_path = os.path.join(path, ""topics"")\n        filenames = sorted(os.listdir(topics_path))\n        for filename in filenames:\n            file_path = os.path.join(topics_path, filename)\n            topic_name = filename.split("".txt"")[0]\n            with open(file_path, ""rb"") as src_f:\n                input_data = src_f.read().decode(""latin-1"")\n            summaries_path = os.path.join(path, ""summaries-gold"", topic_name)\n            summary_lst = []\n            for summ_filename in sorted(os.listdir(summaries_path)):\n                file_path = os.path.join(summaries_path, summ_filename)\n                with open(file_path, ""rb"") as tgt_f:\n                    data = tgt_f.read().strip().decode(""latin-1"")\n                    summary_lst.append(data)\n            summary_data = summary_lst\n            yield filename, {_REVIEW_SENTS: input_data, _SUMMARIES: summary_data}\n'"
datasets/para_crawl/para_crawl.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""ParaCrawl (Bitextor) parallel open-source machine translation benchmark.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport collections\n\nimport nlp\n\n\n_DESCRIPTION = ""Web-Scale Parallel Corpora for Official European Languages.""\n\n_BENCHMARK_URL = ""https://paracrawl.eu/releases.html""\n\n_CITATION = """"""\\\n@misc {paracrawl,\n    title  = ""ParaCrawl"",\n    year   = ""2018"",\n    url    = ""http://paracrawl.eu/download.html.""\n}\n""""""\n\n_BASE_DATA_URL_FORMAT_STR = (\n    ""https://s3.amazonaws.com/web-language-models/"" ""paracrawl/release4/en-{target_lang}.bicleaner07."" ""txt.gz""\n)\n\n\ndef _target_languages():\n    """"""Create the sorted dictionary of language codes, and language names.\n\n  Returns:\n    The sorted dictionary as an instance of `collections.OrderedDict`.\n  """"""\n    langs = {\n        ""bg"": ""Bulgarian"",\n        ""cs"": ""Czech"",\n        ""da"": ""Danish"",\n        ""de"": ""German"",\n        ""el"": ""Greek"",\n        ""es"": ""Spanish"",\n        ""et"": ""Estonian"",\n        ""fi"": ""Finnish"",\n        ""fr"": ""French"",\n        ""ga"": ""Irish"",\n        ""hr"": ""Croatian"",\n        ""hu"": ""Hungarian"",\n        ""it"": ""Italian"",\n        ""lt"": ""Lithuanian"",\n        ""lv"": ""Latvian"",\n        ""mt"": ""Maltese"",\n        ""nl"": ""Dutch"",\n        ""pl"": ""Polish"",\n        ""pt"": ""Portuguese"",\n        ""ro"": ""Romanian"",\n        ""sk"": ""Slovak"",\n        ""sl"": ""Slovenian"",\n        ""sv"": ""Swedish"",\n    }\n    return collections.OrderedDict(sorted(langs.items()))\n\n\nclass ParaCrawlConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for ParaCrawl.""""""\n\n    def __init__(self, target_language=None, **kwargs):\n        """"""BuilderConfig for ParaCrawl.\n\n    Args:\n        for the `nlp.features.text.TextEncoder` used for the features feature.\n      target_language: Target language that will be used to translate to from\n        English which is always the source language. It has to contain 2-letter\n        coded strings. For example: ""se"", ""hu"".\n      **kwargs: Keyword arguments forwarded to super.\n    """"""\n        # Validate the target language.\n        if target_language not in _target_languages():\n            raise ValueError(""Invalid target language: %s "" % target_language)\n\n        # Initialize the base class.\n        name = ""en%s"" % (target_language)\n\n        description = (""Translation dataset from English to %s."") % (target_language)\n        super(ParaCrawlConfig, self).__init__(name=name, description=description, **kwargs)\n\n        # Store the attributes.\n\n        self.target_language = target_language\n        self.data_url = _BASE_DATA_URL_FORMAT_STR.format(target_lang=target_language)\n\n\nclass ParaCrawl(nlp.GeneratorBasedBuilder):\n    """"""ParaCrawl machine translation dataset.""""""\n\n    # Version history:\n    # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).\n    # 0.1.0: Initial version.\n    BUILDER_CONFIGS = [\n        # The version below does not refer to the version of the released\n        # database. It only indicates the version of the TFDS integration.\n        ParaCrawlConfig(  # pylint: disable=g-complex-comprehension\n            target_language=target_language, version=nlp.Version(""1.0.0""),\n        )\n        for target_language in _target_languages()\n    ]\n\n    def _info(self):\n        target_language = self.config.target_language\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=(""en"", target_language))}),\n            supervised_keys=(""en"", target_language),\n            homepage=_BENCHMARK_URL,\n            citation=_CITATION,\n        )\n\n    def _vocab_text_gen(self, files, language):\n        for _, ex in self._generate_examples(**files):\n            yield ex[language]\n\n    def _split_generators(self, dl_manager):\n        # Download the data file.\n        data_file = dl_manager.download_and_extract({""data_file"": self.config.data_url})\n\n        # Return the single split of the data.\n        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs=data_file)]\n\n    def _generate_examples(self, data_file):\n        """"""This function returns the examples in the raw (text) form.""""""\n        target_language = self.config.target_language\n\n        with open(data_file) as f:\n            for idx, line in enumerate(f):\n                line_parts = line.strip().split(""\\t"")\n                if len(line_parts) != 2:\n                    msg = (\n                        ""Wrong data format in line {}. The line \'{}\' does "" ""not have exactly one delimiter.""\n                    ).format(idx, line)\n                    raise ValueError(msg)\n                source, target = line_parts[0].strip(), line_parts[1].strip()\n                yield idx, {""translation"": {""en"": source, target_language: target}}\n'"
datasets/qa4mre/qa4mre.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""QA4MRE (CLEF 2011/2012/2013): a reading comprehension dataset.""""""\n\nfrom __future__ import division, print_function\n\nimport logging\nimport os\nimport xml.etree.ElementTree as ET\n\nimport nlp\n\n\n# pylint: disable=anomalous-backslash-in-string\n_CITATION = r""""""\n@InProceedings{10.1007/978-3-642-40802-1_29,\nauthor=""Pe{\\~{n}}as, Anselmo\nand Hovy, Eduard\nand Forner, Pamela\nand Rodrigo, {\\\'A}lvaro\nand Sutcliffe, Richard\nand Morante, Roser"",\neditor=""Forner, Pamela\nand M{\\""u}ller, Henning\nand Paredes, Roberto\nand Rosso, Paolo\nand Stein, Benno"",\ntitle=""QA4MRE 2011-2013: Overview of Question Answering for Machine Reading Evaluation"",\nbooktitle=""Information Access Evaluation. Multilinguality, Multimodality, and Visualization"",\nyear=""2013"",\npublisher=""Springer Berlin Heidelberg"",\naddress=""Berlin, Heidelberg"",\npages=""303--320"",\nabstract=""This paper describes the methodology for testing the performance of Machine Reading systems through Question Answering and Reading Comprehension Tests. This was the attempt of the QA4MRE challenge which was run as a Lab at CLEF 2011--2013. The traditional QA task was replaced by a new Machine Reading task, whose intention was to ask questions that required a deep knowledge of individual short texts and in which systems were required to choose one answer, by analysing the corresponding test document in conjunction with background text collections provided by the organization. Four different tasks have been organized during these years: Main Task, Processing Modality and Negation for Machine Reading, Machine Reading of Biomedical Texts about Alzheimer\'s disease, and Entrance Exams. This paper describes their motivation, their goals, their methodology for preparing the data sets, their background collections, their metrics used for the evaluation, and the lessons learned along these three years."",\nisbn=""978-3-642-40802-1""\n}\n""""""\n\n_DESCRIPTION = """"""\nQA4MRE dataset was created for the CLEF 2011/2012/2013 shared tasks to promote research in \nquestion answering and reading comprehension. The dataset contains a supporting \npassage and a set of questions corresponding to the passage. Multiple options \nfor answers are provided for each question, of which only one is correct. The \ntraining and test datasets are available for the main track.\nAdditional gold standard documents are available for two pilot studies: one on \nalzheimers data, and the other on entrance exams data.\n""""""\n\n_BASE_URL = ""http://nlp.uned.es/clef-qa/repository/js/scripts/downloadFile.php?file=/var/www/html/nlp/clef-qa/repository/resources/QA4MRE/""\n\nPATHS = {\n    ""2011"": {\n        ""_TRACKS"": (""main""),\n        ""_PATH_TMPL_MAIN_GS"": ""2011/Training_Data/Goldstandard/QA4MRE-2011-{}_GS.xml"",\n        ""_LANGUAGES_MAIN"": (""DE"", ""EN"", ""ES"", ""IT"", ""RO""),\n    },\n    ""2012"": {\n        ""_TRACKS"": (""main"", ""alzheimers""),\n        ""_PATH_TMPL_MAIN_GS"": ""2012/Main_Task/Training_Data/Goldstandard/Used_in_Evaluation/QA4MRE-2012-{}_GS.xml"",\n        ""_LANGUAGES_MAIN"": (""AR"", ""BG"", ""DE"", ""EN"", ""ES"", ""IT"", ""RO""),\n        ""_PATH_ALZHEIMER"": ""2012/Pilot_Tasks/Biomedical_About_Alzheimer/Training_Data/Goldstandard/QA4MRE-2012_BIOMEDICAL_GS.xml"",\n    },\n    ""2013"": {\n        ""_TRACKS"": (""main"", ""alzheimers"", ""entrance_exam""),\n        ""_PATH_TMPL_MAIN_GS"": ""2013/Main_Task/Training_Data/Goldstandard/QA4MRE-2013-{}_GS.xml"",\n        ""_LANGUAGES_MAIN"": (""AR"", ""BG"", ""EN"", ""ES"", ""RO""),\n        ""_PATH_ALZHEIMER"": ""2013/Biomedical_About_Alzheimer/Training_Data/Goldstandard/QA4MRE-2013_BIO_GS-RUN.xml"",\n        ""_PATH_ENTRANCE_EXAM"": ""2013/Entrance_Exams/Training_Data/Goldstandard/qa4mre-exam-test-withanswer.xml"",\n    },\n}\n\n\ndef _get_question(topic_id, topic_name, test_id, document_id, document_str, question):\n    """"""Gets instance ID and features for every question.\n\n  Args:\n    topic_id: string\n    topic_name: string\n    test_id: string\n    document_id: string\n    document_str: string\n    question: XML element for question\n\n  Returns:\n    id_: string. Unique ID for instance.\n    feats: dict of instance features\n  """"""\n\n    question_id = question.attrib[""q_id""]\n    for q_text in question.iter(""q_str""):\n        question_str = q_text.text\n    possible_answers = list()\n    for answer in question.iter(""answer""):\n        answer_id = answer.attrib[""a_id""]\n        answer_str = answer.text\n        possible_answers.append({""answer_id"": answer_id, ""answer_str"": answer_str})\n        if ""correct"" in answer.attrib:\n            correct_answer_id = answer_id\n            correct_answer_str = answer_str\n\n    id_ = ""_"".join([topic_id, topic_name, test_id, question_id])\n    logging.info(""ID: %s"", id_)\n\n    feats = {\n        ""topic_id"": topic_id,\n        ""topic_name"": topic_name,\n        ""test_id"": test_id,\n        ""document_id"": document_id,\n        ""document_str"": document_str,\n        ""question_id"": question_id,\n        ""question_str"": question_str,\n        ""answer_options"": possible_answers,\n        ""correct_answer_id"": correct_answer_id,\n        ""correct_answer_str"": correct_answer_str,\n    }\n\n    return id_, feats\n\n\nclass Qa4mreConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Qa4mre.""""""\n\n    def __init__(self, year, track=""main"", language=""EN"", **kwargs):\n        """"""BuilderConfig for Qa4Mre.\n\n    Args:\n      year: string, year of dataset\n      track: string, the task track from PATHS[year][\'_TRACKS\'].\n      language: string, Acronym for language in the main task.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        if track.lower() not in PATHS[year][""_TRACKS""]:\n            raise ValueError(""Incorrect track. Track should be one of the following: "", PATHS[year][""_TRACKS""])\n\n        if track.lower() != ""main"" and language.upper() != ""EN"":\n            logging.warn(""Only English documents available for pilot "" ""tracks. Setting English by default."")\n            language = ""EN""\n\n        if track.lower() == ""main"" and language.upper() not in PATHS[year][""_LANGUAGES_MAIN""]:\n            raise ValueError(\n                ""Incorrect language for the main track. Correct options: "", PATHS[year][""_LANGUAGES_MAIN""]\n            )\n\n        self.year = year\n        self.track = track.lower()\n        self.lang = language.upper()\n\n        name = self.year + ""."" + self.track + ""."" + self.lang\n\n        description = _DESCRIPTION\n        description += (""This configuration includes the {} track for {} language "" ""in {} year."").format(\n            self.track, self.lang, self.year\n        )\n\n        super(Qa4mreConfig, self).__init__(name=name, description=description, version=nlp.Version(""0.1.0""), **kwargs)\n\n\nclass Qa4mre(nlp.GeneratorBasedBuilder):\n    """"""QA4MRE dataset from CLEF shared tasks 2011, 2012, 2013.""""""\n\n    BUILDER_CONFIGS = [\n        Qa4mreConfig(year=""2011"", track=""main"", language=""DE""),  # 2011 Main track German (2011.main.DE)\n        Qa4mreConfig(year=""2011"", track=""main"", language=""EN""),  # 2011 Main track English (2011.main.EN)\n        Qa4mreConfig(year=""2011"", track=""main"", language=""ES""),  # 2011 Main track Spanish (2011.main.ES)\n        Qa4mreConfig(year=""2011"", track=""main"", language=""IT""),  # 2011 Main track Italian (2011.main.IT)\n        Qa4mreConfig(year=""2011"", track=""main"", language=""RO""),  # 2011 Main track Romanian (2011.main.RO)\n        Qa4mreConfig(year=""2012"", track=""main"", language=""AR""),  # 2012 Main track Arabic (2012.main.AR)\n        Qa4mreConfig(year=""2012"", track=""main"", language=""BG""),  # 2012 Main track Bulgarian (2012.main.BG)\n        Qa4mreConfig(year=""2012"", track=""main"", language=""DE""),  # 2012 Main track German (2012.main.DE)\n        Qa4mreConfig(year=""2012"", track=""main"", language=""EN""),  # 2012 Main track English (2012.main.EN)\n        Qa4mreConfig(year=""2012"", track=""main"", language=""ES""),  # 2012 Main track Spanish (2012.main.ES)\n        Qa4mreConfig(year=""2012"", track=""main"", language=""IT""),  # 2012 Main track Italian (2012.main.IT)\n        Qa4mreConfig(year=""2012"", track=""main"", language=""RO""),  # 2012 Main track Romanian (2012.main.RO)\n        Qa4mreConfig(year=""2012"", track=""alzheimers"", language=""EN""),  # (2012.alzheimers.EN)\n        Qa4mreConfig(year=""2013"", track=""main"", language=""AR""),  # 2013 Main track Arabic (2013.main.AR)\n        Qa4mreConfig(year=""2013"", track=""main"", language=""BG""),  # 2013 Main track Bulgarian (2013.main.BG)\n        Qa4mreConfig(year=""2013"", track=""main"", language=""EN""),  # 2013 Main track English (2013.main.EN)\n        Qa4mreConfig(year=""2013"", track=""main"", language=""ES""),  # 2013 Main track Spanish (2013.main.ES)\n        Qa4mreConfig(year=""2013"", track=""main"", language=""RO""),  # 2013 Main track Romanian (2013.main.RO)\n        Qa4mreConfig(year=""2013"", track=""alzheimers"", language=""EN""),  # (2013.alzheimers.EN)\n        Qa4mreConfig(year=""2013"", track=""entrance_exam"", language=""EN""),  # (2013.entrance_exam.EN)\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""topic_id"": nlp.Value(""string""),\n                    ""topic_name"": nlp.Value(""string""),\n                    ""test_id"": nlp.Value(""string""),\n                    ""document_id"": nlp.Value(""string""),\n                    ""document_str"": nlp.Value(""string""),\n                    ""question_id"": nlp.Value(""string""),\n                    ""question_str"": nlp.Value(""string""),\n                    ""answer_options"": nlp.features.Sequence(\n                        {""answer_id"": nlp.Value(""string""), ""answer_str"": nlp.Value(""string"")}\n                    ),\n                    ""correct_answer_id"": nlp.Value(""string""),\n                    ""correct_answer_str"": nlp.Value(""string""),\n                }\n            ),\n            # No default supervised keys because both passage and question are used\n            # to determine the correct answer.\n            supervised_keys=None,\n            homepage=""http://nlp.uned.es/clef-qa/repository/pastCampaigns.php"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        cfg = self.config\n        download_urls = dict()\n\n        if cfg.track == ""main"":\n            download_urls[""{}.main.{}"".format(cfg.year, cfg.lang)] = os.path.join(\n                _BASE_URL, PATHS[cfg.year][""_PATH_TMPL_MAIN_GS""].format(cfg.lang)\n            )\n\n        if cfg.year in [""2012"", ""2013""] and cfg.track == ""alzheimers"":\n            download_urls[""{}.alzheimers.EN"".format(cfg.year)] = os.path.join(\n                _BASE_URL, PATHS[cfg.year][""_PATH_ALZHEIMER""]\n            )\n\n        if cfg.year == ""2013"" and cfg.track == ""entrance_exam"":\n            download_urls[""2013.entrance_exam.EN""] = os.path.join(_BASE_URL, PATHS[cfg.year][""_PATH_ENTRANCE_EXAM""])\n\n        downloaded_files = dl_manager.download_and_extract(download_urls)\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={""filepath"": downloaded_files[""{}.{}.{}"".format(cfg.year, cfg.track, cfg.lang)]},\n            )\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        with open(filepath, ""rb"") as f:\n            tree = ET.parse(f)\n            root = tree.getroot()  # test-set\n            for topic in root:\n                topic_id = topic.attrib[""t_id""]\n                topic_name = topic.attrib[""t_name""]\n                for test in topic:\n                    test_id = test.attrib[""r_id""]\n                    for document in test.iter(""doc""):\n                        document_id = document.attrib[""d_id""]\n                        document_str = document.text\n                    for question in test.iter(""q""):\n                        yield _get_question(topic_id, topic_name, test_id, document_id, document_str, question)\n'"
datasets/qangaroo/qangaroo.py,0,"b'""""""TODO(qangaroo): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(qangaroo): BibTeX citation\n\n_CITATION = """"""\n""""""\n\n# TODO(quangaroo):\n_DESCRIPTION = """"""\\\n  We have created two new Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference.\n\nSeveral pieces of information often jointly imply another fact. In multi-hop inference, a new fact is derived by combining facts via a chain of multiple steps.\n\nOur aim is to build Reading Comprehension methods that perform multi-hop inference on text, where individual facts are spread out across different documents.\n\nThe two QAngaroo datasets provide a training and evaluation resource for such methods.\n""""""\n\n_MEDHOP_DESCRIPTION = """"""\\\n  With the same format as WikiHop, this dataset is based on research paper abstracts from PubMed, and the queries are about interactions between pairs of drugs. \n  The correct answer has to be inferred by combining information from a chain of reactions of drugs and proteins.\n  """"""\n_WIKIHOP_DESCRIPTION = """"""\\\n  With the same format as WikiHop, this dataset is based on research paper abstracts from PubMed, and the queries are about interactions between pairs of drugs.\n   The correct answer has to be inferred by combining information from a chain of reactions of drugs and proteins.\n  """"""\n\n_URL = ""https://drive.google.com/uc?export=download&id=1ytVZ4AhubFDOEL7o7XrIRIyhU8g9wvKA""\n\n\nclass QangarooConfig(nlp.BuilderConfig):\n    def __init__(self, data_dir, **kwargs):\n        """""" BuilderConfig for qangaroo dataset\n\n        Args:\n          data_dir: directory for the given dataset name\n          **kwargs: keyword arguments forwarded to super.\n\n        """"""\n\n        super(QangarooConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n        self.data_dir = data_dir\n\n\nclass Qangaroo(nlp.GeneratorBasedBuilder):\n    """"""TODO(qangaroo): Short description of my dataset.""""""\n\n    # TODO(qangaroo): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        QangarooConfig(name=""medhop"", description=_MEDHOP_DESCRIPTION, data_dir=""medhop""),\n        QangarooConfig(name=""masked_medhop"", description=_MEDHOP_DESCRIPTION, data_dir=""medhop""),\n        QangarooConfig(name=""wikihop"", description=_WIKIHOP_DESCRIPTION, data_dir=""wikihop""),\n        QangarooConfig(name=""masked_wikihop"", description=_WIKIHOP_DESCRIPTION, data_dir=""wikihop""),\n    ]\n\n    def _info(self):\n        # TODO(qangaroo): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""query"": nlp.Value(""string""),\n                    ""supports"": nlp.features.Sequence({""support"": nlp.Value(""string"")}),\n                    ""candidates"": nlp.features.Sequence({""candidate"": nlp.Value(""string"")}),\n                    ""answer"": nlp.Value(""string""),\n                    ""id"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""http://qangaroo.cs.ucl.ac.uk/index.html"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(qangaroo): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""qangaroo_v1.1"")\n        train_file = ""train.masked.json"" if ""masked"" in self.config.name else ""train.json""\n        dev_file = ""dev.masked.json"" if ""masked"" in self.config.name else ""dev.json""\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, self.config.data_dir, train_file)},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, self.config.data_dir, dev_file)},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(quangaroo): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for example in data:\n                id_ = example[""id""]\n                yield id_, {\n                    ""id"": example[""id""],\n                    ""query"": example[""query""],\n                    ""supports"": {""support"": example[""supports""]},\n                    ""candidates"": {""candidate"": example[""candidates""]},\n                    ""answer"": example[""answer""],\n                }\n'"
datasets/qanta/qanta.py,0,"b'""""""qanta dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nfrom typing import List, Tuple\n\nimport nlp\n\n\n_CITATION = """"""\n@article{Rodriguez2019QuizbowlTC,\n  title={Quizbowl: The Case for Incremental Question Answering},\n  author={Pedro Rodriguez and Shi Feng and Mohit Iyyer and He He and Jordan L. Boyd-Graber},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1904.04792}\n}\n""""""\n\n_DESCRIPTION = """"""\nThe Qanta dataset is a question answering dataset based on the academic trivia game Quizbowl.\n""""""\n\n\n_QANTA_URL = ""https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.mapped.2018.04.18.json""\n_TRICK_URL = ""https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/trick-tacl-datasets/qanta.tacl-trick.json""\n_VERSION = nlp.Version(""2018.04.18"")\n_FIRST = ""first""\n_FULL = ""full""\n_SENTENCES = ""sentences""\n_RUNS = ""runs""\n# Order matters, the first one is default\n_MODES = [_FULL, _FIRST, _SENTENCES, _RUNS]\n_DEFAULT_CHAR_SKIP = 25\n\n\nclass QantaConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Qanta.""""""\n\n    def __init__(self, mode: str, char_skip: int, **kwargs):\n        super(QantaConfig, self).__init__(version=_VERSION, **kwargs)\n        self.mode = mode\n        self.char_skip = char_skip\n\n\ndef create_char_runs(text: str, char_skip: int) -> List[Tuple[str, int]]:\n    """"""\n    Returns runs of the question based on skipping char_skip characters at a time. Also returns the indices used\n    q: name this first united states president.\n    runs with char_skip=10:\n    [\'name this \',\n        \'name this first unit\',\n        \'name this first united state p\',\n        \'name this first united state president.\']\n    :param char_skip: Number of characters to skip each time\n    """"""\n    char_indices = list(range(char_skip, len(text) + char_skip, char_skip))\n    return [(text[:idx], idx) for idx in char_indices]\n\n\ndef with_default(key, lookup, default):\n    if key in lookup:\n        value = lookup[key]\n        if value is None:\n            return default\n        else:\n            return value\n    else:\n        return default\n\n\ndef question_to_examples(question, mode: str, char_skip: int):\n    features = {\n        ""qanta_id"": question[""qanta_id""],\n        ""proto_id"": with_default(""proto_id"", question, """"),\n        ""qdb_id"": with_default(""qdb_id"", question, -1),\n        # We refer to the actual answer as page, but this\n        # may be misleading externally, so rename here to\n        # be clearer\n        ""page"": question[""page""],\n        ""answer"": question[""page""],\n        ""raw_answer"": question[""answer""],\n        ""dataset"": with_default(""dataset"", question, """"),\n        ""full_question"": question[""text""],\n        ""first_sentence"": question[""first_sentence""],\n        ""tokenizations"": question[""tokenizations""],\n        ""fold"": question[""fold""],\n        ""gameplay"": question[""gameplay""],\n        ""category"": with_default(""category"", question, """"),\n        ""subcategory"": with_default(""subcategory"", question, """"),\n        ""tournament"": question[""tournament""],\n        ""difficulty"": with_default(""difficulty"", question, """"),\n        ""year"": question[""year""],\n        ""char_idx"": -1,\n        ""sentence_idx"": -1,\n    }\n    if mode == _FULL:\n        yield {\n            ""text"": question[""text""],\n            ""id"": str(question[""qanta_id""]) + ""-full"",\n            **features,\n        }\n    elif mode == _FIRST:\n        yield {\n            ""text"": question[""first_sentence""],\n            ""id"": str(question[""qanta_id""]) + ""-first"",\n            **features,\n        }\n    elif mode == _RUNS:\n        text = question[""text""]\n        for text_run, char_idx in create_char_runs(text, char_skip):\n            yield {\n                ""text"": text_run,\n                ""char_idx"": char_idx,\n                ""id"": str(question[""qanta_id""]) + ""-char-"" + str(char_idx),\n                **features,\n            }\n    elif mode == _SENTENCES:\n        for sentence_idx, (start, end) in enumerate(question[""tokenizations""]):\n            sentence = question[""text""][start:end]\n            yield {\n                ""text"": sentence,\n                ""sentence_idx"": sentence_idx,\n                ""id"": str(question[""qanta_id""]) + ""-sentence-"" + str(sentence_idx),\n                **features,\n            }\n    else:\n        raise ValueError(f""Invalid mode: {mode}"")\n\n\n_FEATURES = {\n    # Generated ID based modes set, unique\n    ""id"": nlp.Value(""string""),\n    # Dataset defined IDs\n    ""qanta_id"": nlp.Value(""int32""),\n    ""proto_id"": nlp.Value(""string""),\n    ""qdb_id"": nlp.Value(""int32""),\n    ""dataset"": nlp.Value(""string""),\n    # Inputs\n    ""text"": nlp.Value(""string""),\n    ""full_question"": nlp.Value(""string""),\n    ""first_sentence"": nlp.Value(""string""),\n    ""char_idx"": nlp.Value(""int32""),\n    ""sentence_idx"": nlp.Value(""int32""),\n    # Character indices of sentences: List[Tuple[int, int]]\n    ""tokenizations"": nlp.features.Sequence(nlp.features.Sequence(nlp.Value(""int32""), length=2)),\n    # Labels: Number is equal to number of unique pages across all folds\n    ""answer"": nlp.Value(""string""),\n    ""page"": nlp.Value(""string""),\n    ""raw_answer"": nlp.Value(""string""),\n    # Meta Information\n    ""fold"": nlp.Value(""string""),\n    ""gameplay"": nlp.Value(""bool""),\n    ""category"": nlp.Value(""string""),\n    ""subcategory"": nlp.Value(""string""),\n    ""tournament"": nlp.Value(""string""),\n    ""difficulty"": nlp.Value(""string""),\n    ""year"": nlp.Value(""int32""),\n}\n\n\nclass Qanta(nlp.GeneratorBasedBuilder):\n    """"""The Qanta dataset is a question answering dataset based on the academic trivia game Quizbowl.\n  """"""\n\n    VERSION = _VERSION\n    BUILDER_CONFIGS = [\n        QantaConfig(\n            name=f""mode={mode},char_skip={_DEFAULT_CHAR_SKIP}"",\n            description=f""Question format: {mode}, char_skip: {_DEFAULT_CHAR_SKIP}"",\n            mode=mode,\n            char_skip=_DEFAULT_CHAR_SKIP,\n        )\n        for mode in _MODES\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(_FEATURES),\n            # Number of classes is a function of the dataset, ClassLabel doesn\'t support dynamic\n            # definition, so have to defer conversion to classes to later, so can\'t define\n            # supervied keys\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""http://www.qanta.org/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        qanta_path = dl_manager.download_and_extract(_QANTA_URL)\n        trick_path = dl_manager.download_and_extract(_TRICK_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split(""guesstrain""),\n                gen_kwargs={\n                    ""qanta_filepath"": qanta_path,\n                    ""trick_filepath"": trick_path,\n                    ""fold"": ""guesstrain"",\n                    ""mode"": self.config.mode,\n                    ""char_skip"": self.config.char_skip,\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split(""buzztrain""),\n                gen_kwargs={\n                    ""qanta_filepath"": qanta_path,\n                    ""trick_filepath"": trick_path,\n                    ""fold"": ""buzztrain"",\n                    ""mode"": self.config.mode,\n                    ""char_skip"": self.config.char_skip,\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split(""guessdev""),\n                gen_kwargs={\n                    ""qanta_filepath"": qanta_path,\n                    ""trick_filepath"": trick_path,\n                    ""fold"": ""guessdev"",\n                    ""mode"": self.config.mode,\n                    ""char_skip"": self.config.char_skip,\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split(""buzzdev""),\n                gen_kwargs={\n                    ""qanta_filepath"": qanta_path,\n                    ""trick_filepath"": trick_path,\n                    ""fold"": ""buzzdev"",\n                    ""mode"": self.config.mode,\n                    ""char_skip"": self.config.char_skip,\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split(""guesstest""),\n                gen_kwargs={\n                    ""qanta_filepath"": qanta_path,\n                    ""trick_filepath"": trick_path,\n                    ""fold"": ""guesstest"",\n                    ""mode"": self.config.mode,\n                    ""char_skip"": self.config.char_skip,\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split(""buzztest""),\n                gen_kwargs={\n                    ""qanta_filepath"": qanta_path,\n                    ""trick_filepath"": trick_path,\n                    ""fold"": ""buzztest"",\n                    ""mode"": self.config.mode,\n                    ""char_skip"": self.config.char_skip,\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split(""adversarial""),\n                gen_kwargs={\n                    ""qanta_filepath"": qanta_path,\n                    ""trick_filepath"": trick_path,\n                    ""fold"": ""adversarial"",\n                    ""mode"": self.config.mode,\n                    ""char_skip"": self.config.char_skip,\n                },\n            ),\n        ]\n\n    def _generate_examples(\n        self, qanta_filepath: str, trick_filepath: str, fold: str, mode: str, char_skip: int,\n    ):\n        """"""Yields examples.""""""\n        if mode not in _MODES:\n            raise ValueError(f""Invalid mode: {mode}"")\n\n        if fold == ""adversarial"":\n            path = trick_filepath\n        else:\n            path = qanta_filepath\n        with open(path) as f:\n            questions = json.load(f)[""questions""]\n            for q in questions:\n                if q[""page""] is not None and q[""fold""] == fold:\n                    for example in question_to_examples(q, mode, char_skip):\n                        yield example[""id""], example\n'"
datasets/qasc/qasc.py,0,"b'""""""TODO(qasc): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(qasc): BibTeX citation\n_CITATION = """"""\\\n@article{allenai:qasc,\n      author    = {Tushar Khot and Peter Clark and Michal Guerquin and Peter Jansen and Ashish Sabharwal},\n      title     = {QASC: A Dataset for Question Answering via Sentence Composition},\n      journal   = {arXiv:1910.11473v2},\n      year      = {2020},\n}\n""""""\n\n# TODO(qasc):\n_DESCRIPTION = """"""\nQASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice \nquestions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.\n""""""\n_URl = ""http://data.allenai.org/downloads/qasc/qasc_dataset.tar.gz""\n\n\nclass Qasc(nlp.GeneratorBasedBuilder):\n    """"""TODO(qasc): Short description of my dataset.""""""\n\n    # TODO(qasc): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(qasc): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""choices"": nlp.features.Sequence({""text"": nlp.Value(""string""), ""label"": nlp.Value(""string"")}),\n                    ""answerKey"": nlp.Value(""string""),\n                    ""fact1"": nlp.Value(""string""),\n                    ""fact2"": nlp.Value(""string""),\n                    ""combinedfact"": nlp.Value(""string""),\n                    ""formatted_question"": nlp.Value(""string""),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://allenai.org/data/qasc"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(qasc): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URl)\n        data_dir = os.path.join(dl_dir, ""QASC_Dataset"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""test.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""dev.jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(qasc): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for row in f:\n                data = json.loads(row)\n                answerkey = data.get(""answerKey"", """")\n                id_ = data[""id""]\n                question = data[""question""][""stem""]\n                choices = data[""question""][""choices""]\n                text_choices = [choice[""text""] for choice in choices]\n                label_choices = [choice[""label""] for choice in choices]\n                fact1 = data.get(""fact1"", """")\n                fact2 = data.get(""fact2"", """")\n                combined_fact = data.get(""combinedfact"", """")\n                formatted_question = data.get(""formatted_question"", """")\n                yield id_, {\n                    ""id"": id_,\n                    ""answerKey"": answerkey,\n                    ""question"": question,\n                    ""choices"": {""text"": text_choices, ""label"": label_choices},\n                    ""fact1"": fact1,\n                    ""fact2"": fact2,\n                    ""combinedfact"": combined_fact,\n                    ""formatted_question"": formatted_question,\n                }\n'"
datasets/quarel/quarel.py,0,"b'""""""TODO(quarel): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(quarel): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{quarel_v1,\n    title={QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships},\n    author={Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, Ashish Sabharwal},\n    year={2018},\n    journal={arXiv:1805.05377v1}\n}\n""""""\n\n# TODO(quarel):\n_DESCRIPTION = """"""\nQuaRel is a crowdsourced dataset of 2771 multiple-choice story questions, including their logical forms.\n""""""\n_URL = ""https://s3-us-west-2.amazonaws.com/ai2-website/data/quarel-dataset-v1-nov2018.zip""\n\n\nclass Quarel(nlp.GeneratorBasedBuilder):\n    """"""TODO(quarel): Short description of my dataset.""""""\n\n    # TODO(quarel): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(quarel): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""id"": nlp.Value(""string""),\n                    ""answer_index"": nlp.Value(""int32""),\n                    ""logical_forms"": nlp.features.Sequence({""logical_form"": nlp.Value(""string"")}),\n                    ""logical_form_pretty"": nlp.Value(""string""),\n                    ""world_literals"": nlp.features.Sequence(\n                        {""world1"": nlp.Value(""string""), ""world2"": nlp.Value(""string"")}\n                    ),\n                    ""question"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://allenai.org/data/quarel"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(quarel): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""quarel-dataset-v1"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""quarel-v1-train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""quarel-v1-test.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""quarel-v1-dev.jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(quarel): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n                yield id_, {\n                    ""id"": data[""id""],\n                    ""answer_index"": data[""answer_index""],\n                    ""logical_forms"": {""logical_form"": data[""logical_forms""],},\n                    ""world_literals"": {\n                        ""world1"": [data[""world_literals""][""world1""]],\n                        ""world2"": [data[""world_literals""][""world2""]],\n                    },\n                    ""logical_form_pretty"": data[""logical_form_pretty""],\n                    ""question"": data[""question""],\n                }\n'"
datasets/quartz/quartz.py,0,"b'""""""TODO(quartz): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(quartz): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{quartz,\n  author = {Oyvind Tafjord and Matt Gardner and Kevin Lin and Peter Clark},\n  title = {""QUARTZ: An Open-Domain Dataset of Qualitative Relationship\nQuestions""},\n \n  year = {""2019""},\n}\n""""""\n\n# TODO(quartz):\n_DESCRIPTION = """"""\\\nQuaRTz is a crowdsourced dataset of 3864 multiple-choice questions about open domain qualitative relationships. Each \nquestion is paired with one of 405 different background sentences (sometimes short paragraphs).\nThe QuaRTz dataset V1 contains 3864 questions about open domain qualitative relationships. Each question is paired with \none of 405 different background sentences (sometimes short paragraphs).\n\nThe dataset is split into train (2696), dev (384) and test (784). A background sentence will only appear in a single split.\n""""""\n\n_URL = ""https://s3-us-west-2.amazonaws.com/ai2-website/data/quartz-dataset-v1-aug2019.zip""\n\n\nclass Quartz(nlp.GeneratorBasedBuilder):\n    """"""TODO(quartz): Short description of my dataset.""""""\n\n    # TODO(quartz): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(quartz): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""id"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""choices"": nlp.features.Sequence({""text"": nlp.Value(""string""), ""label"": nlp.Value(""string"")}),\n                    ""answerKey"": nlp.Value(""string""),\n                    ""para"": nlp.Value(""string""),\n                    ""para_id"": nlp.Value(""string""),\n                    ""para_anno"": {\n                        ""effect_prop"": nlp.Value(""string""),\n                        ""cause_dir_str"": nlp.Value(""string""),\n                        ""effect_dir_str"": nlp.Value(""string""),\n                        ""cause_dir_sign"": nlp.Value(""string""),\n                        ""effect_dir_sign"": nlp.Value(""string""),\n                        ""cause_prop"": nlp.Value(""string""),\n                    },\n                    ""question_anno"": {\n                        ""more_effect_dir"": nlp.Value(""string""),\n                        ""less_effect_dir"": nlp.Value(""string""),\n                        ""less_cause_prop"": nlp.Value(""string""),\n                        ""more_effect_prop"": nlp.Value(""string""),\n                        ""less_effect_prop"": nlp.Value(""string""),\n                        ""less_cause_dir"": nlp.Value(""string""),\n                    },\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://allenai.org/data/quartz"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(quartz): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""quartz-dataset-v1-aug2019"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""test.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""dev.jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(quartz): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for row in f:\n                data = json.loads(row)\n                id_ = data[""id""]\n                question = data[""question""][""stem""]\n                answerKey = data[""answerKey""]\n                choices = data[""question""][""choices""]\n                choice_text = [choice[""text""] for choice in choices]\n                choice_label = [choice[""label""] for choice in choices]\n                para_id = data[""para_id""]\n                para = data[""para""]\n                para_ano = data[""para_anno""]\n                effect_prop = para_ano.get(""effect_prop"", """")\n                cause_dir_str = para_ano.get(""cause_dir_str"", """")\n                effect_dir_str = para_ano.get(""effect_dir_str"", """")\n                cause_dir_sign = para_ano.get(""cause_dir_sign"", """")\n                effect_dir_sign = para_ano.get(""effect_dir_sign"", """")\n                cause_prop = para_ano.get(""cause_prop"", """")\n                question_anno = data[""question_anno""]\n                more_effect_dir = """" if not question_anno else question_anno.get(""more_effect_dir"", """")\n                less_effect_dir = """" if not question_anno else question_anno.get(""less_effect_dir"", """")\n                less_cause_prop = """" if not question_anno else question_anno.get(""less_cause_prop"", """")\n                more_effect_prop = """" if not question_anno else question_anno.get(""more_effect_prop"", """")\n                less_effect_prop = """" if not question_anno else question_anno.get(""less_effect_prop"", """")\n                less_cause_dir = """" if not question_anno else question_anno.get(""less_effect_prop"", """")\n                yield id_, {\n                    ""id"": id_,\n                    ""question"": question,\n                    ""choices"": {""text"": choice_text, ""label"": choice_label},\n                    ""answerKey"": answerKey,\n                    ""para"": para,\n                    ""para_id"": para_id,\n                    ""para_anno"": {\n                        ""effect_prop"": effect_prop,\n                        ""cause_dir_str"": cause_dir_str,\n                        ""effect_dir_str"": effect_dir_str,\n                        ""cause_dir_sign"": cause_dir_sign,\n                        ""effect_dir_sign"": effect_dir_sign,\n                        ""cause_prop"": cause_prop,\n                    },\n                    ""question_anno"": {\n                        ""more_effect_dir"": more_effect_dir,\n                        ""less_effect_dir"": less_effect_dir,\n                        ""less_cause_prop"": less_cause_prop,\n                        ""more_effect_prop"": more_effect_prop,\n                        ""less_effect_prop"": less_effect_prop,\n                        ""less_cause_dir"": less_cause_dir,\n                    },\n                }\n'"
datasets/quoref/quoref.py,0,"b'""""""TODO(quoref): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(quoref): BibTeX citation\n_CITATION = """"""\\\n@article{allenai:quoref,\n      author    = {Pradeep Dasigi and Nelson F. Liu and Ana Marasovic and Noah A. Smith and  Matt Gardner},\n      title     = {Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning},\n      journal   = {arXiv:1908.05803v2 },\n      year      = {2019},\n}\n""""""\n\n# TODO(quoref):\n_DESCRIPTION = """"""\\\nQuoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems. In this \nspan-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard \ncoreferences before selecting the appropriate span(s) in the paragraphs for answering questions.\n""""""\n\n_URL = ""https://quoref-dataset.s3-us-west-2.amazonaws.com/train_and_dev/quoref-train-dev-v0.1.zip""\n\n\nclass Quoref(nlp.GeneratorBasedBuilder):\n    """"""TODO(quoref): Short description of my dataset.""""""\n\n    # TODO(quoref): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(quoref): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""title"": nlp.Value(""string""),\n                    ""url"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""answer_start"": nlp.Value(""int32""), ""text"": nlp.Value(""string""),}\n                    )\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://leaderboard.allenai.org/quoref/submissions/get-started"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(quoref): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""quoref-train-dev-v0.1"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""quoref-train-v0.1.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""quoref-dev-v0.1.json"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(quoref): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for article in data[""data""]:\n                title = article.get(""title"", """").strip()\n                url = article.get(""url"", """").strip()\n                for paragraph in article[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        # Features currently used are ""context"", ""question"", and ""answers"".\n                        # Others are extracted here for the ease of future expansions.\n                        yield id_, {\n                            ""title"": title,\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                            ""url"": url,\n                        }\n'"
datasets/race/race.py,0,"b'""""""TODO(race): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(race): BibTeX citation\n_CITATION = """"""\\\n@article{lai2017large,\n    title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},\n    author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},\n    journal={arXiv preprint arXiv:1704.04683},\n    year={2017}\n}\n""""""\n\n# TODO(race):\n_DESCRIPTION = """"""\\\nRace is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The\n dataset is collected from English examinations in China, which are designed for middle school and high school students.\nThe dataset can be served as the training and test sets for machine comprehension.\n\n""""""\n\n_URL = ""http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz""\n\n\nclass Race(nlp.GeneratorBasedBuilder):\n    """"""TODO(race): Short description of my dataset.""""""\n\n    # TODO(race): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(race): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""article"": nlp.Value(""string""),\n                    ""answer"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""options"": nlp.features.Sequence({""option"": nlp.Value(""string"")})\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""http://www.cs.cmu.edu/~glai1/data/race/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(race): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    ""files"": sorted(os.listdir(os.path.join(dl_dir, ""RACE/test/high""))),\n                    ""filespath"": os.path.join(dl_dir, ""RACE/test/high""),\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    ""files"": sorted(os.listdir(os.path.join(dl_dir, ""RACE/train/high""))),\n                    ""filespath"": os.path.join(dl_dir, ""RACE/train/high""),\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    ""files"": sorted(os.listdir(os.path.join(dl_dir, ""RACE/dev/high""))),\n                    ""filespath"": os.path.join(dl_dir, ""RACE/dev/high""),\n                },\n            ),\n        ]\n\n    def _generate_examples(self, files, filespath):\n        """"""Yields examples.""""""\n        # TODO(race): Yields (key, example) tuples from the dataset\n        for file in files:\n            filepath = os.path.join(filespath, file)\n            with open(filepath) as f:\n                data = json.load(f)\n                questions = data[""questions""]\n                answers = data[""answers""]\n                options = data[""options""]\n                for i in range(len(questions)):\n                    question = questions[i]\n                    answer = answers[i]\n                    option = options[i]\n                    yield i, {\n                        ""article"": data[""article""],\n                        ""question"": question,\n                        ""answer"": answer,\n                        ""options"": {""option"": option},\n                    }\n'"
datasets/reclor/reclor.py,0,"b'""""""TODO(reclor): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\nfrom pathlib import Path\n\nimport nlp\n\n\n# TODO(reclor): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{yu2020reclor,\n        author = {Yu, Weihao and Jiang, Zihang and Dong, Yanfei and Feng, Jiashi},\n        title = {ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning},\n        booktitle = {International Conference on Learning Representations (ICLR)},\n        month = {April},\n        year = {2020}\n    }\n\n""""""\n\n# TODO(reclor):\n_DESCRIPTION = """"""\\\nLogical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary \nlanguage as the definition from LSAC. ReClor is a dataset extracted from logical reasoning questions of standardized graduate \nadmission examinations. Empirical results show that the state-of-the-art models struggle on ReClor with poor performance \nindicating more research is needed to essentially enhance the logical reasoning ability of current models. We hope this \ndataset could help push Machine Reading Comprehension (MRC) towards more complicated reasonin\n""""""\n\n\nclass Reclor(nlp.GeneratorBasedBuilder):\n    """"""TODO(reclor): Short description of my dataset.""""""\n\n    # TODO(reclor): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  to use ReClor you need to download it manually. Please go to its homepage (http://whyu.me/reclor/) fill the google \n  form and you will recive a download link and a password to extract it.Please extract all files in one folder and use the path folder in nlp.load(\'reclor\', data_dir=\'path/to/folder/folder_name\')\n  """"""\n\n    def _info(self):\n        # TODO(reclor): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence({""answer"": nlp.Value(""string"")}),\n                    ""label"": nlp.Value(""string""),\n                    ""id_string"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""http://whyu.me/reclor/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(reclor): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        data_dir = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))\n\n        if not os.path.exists(data_dir):\n            raise FileNotFoundError(\n                ""{} does not exist. Make sure you insert a manual dir via `nlp.load(\'wikihow\', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}"".format(\n                    data_dir, self.MANUAL_DOWNLOAD_INSTRUCTIONS\n                )\n            )\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""train.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""test.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""val.json"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(reclor): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for id_, row in enumerate(data):\n                yield id_, {\n                    ""context"": row[""context""],\n                    ""question"": row[""question""],\n                    ""answers"": {""answer"": row[""answers""]},\n                    ""label"": str(row.get(""label"", """")),\n                    ""id_string"": row[""id_string""],\n                }\n'"
datasets/reddit/reddit.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Reddit dataset using tldr as summaries.""""""\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@inproceedings{volske-etal-2017-tl,\n    title = ""{TL};{DR}: Mining {R}eddit to Learn Automatic Summarization"",\n    author = {V{\\""o}lske, Michael  and\n      Potthast, Martin  and\n      Syed, Shahbaz  and\n      Stein, Benno},\n    booktitle = ""Proceedings of the Workshop on New Frontiers in Summarization"",\n    month = sep,\n    year = ""2017"",\n    address = ""Copenhagen, Denmark"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/W17-4508"",\n    doi = ""10.18653/v1/W17-4508"",\n    pages = ""59--63"",\n    abstract = ""Recent advances in automatic text summarization have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a new method for mining social media for author-provided summaries, taking advantage of the common practice of appending a {``}TL;DR{\'\'} to long posts. A case study using a large Reddit crawl yields the Webis-TLDR-17 dataset, complementing existing corpora primarily from the news genre. Our technique is likely applicable to other social media sites and general web crawls."",\n}\n""""""\n\n_DESCRIPTION = """"""\nThis corpus contains preprocessed posts from the Reddit dataset.\nThe dataset consists of 3,848,330 posts with an average length of 270 words for content,\nand 28 words for the summary.\n\nFeatures includes strings: author, body, normalizedBody, content, summary, subreddit, subreddit_id.\nContent is used as document and summary is used as summary.\n""""""\n\n_URL = ""https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip?download=1""\n\n_DOCUMENT = ""content""\n_SUMMARY = ""summary""\n_ADDITIONAL_FEATURES = [""author"", ""body"", ""normalizedBody"", ""subreddit"", ""subreddit_id"", ""id""]\n\n\nclass Reddit(nlp.GeneratorBasedBuilder):\n    """"""Reddit Dataset.""""""\n\n    VERSION = nlp.Version(""1.0.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({k: nlp.Value(""string"") for k in _ADDITIONAL_FEATURES + [_DOCUMENT, _SUMMARY]}),\n            supervised_keys=None,\n            homepage=""https://github.com/webis-de/webis-tldr-17-corpus"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_path = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""path"": os.path.join(dl_path, ""corpus-webis-tldr-17.json"")},\n            )\n        ]\n\n    def _generate_examples(self, path=None):\n        """"""Yields examples.""""""\n        with open(path, ""rb"") as f:\n            for i, line in enumerate(f):\n                # possible keys are:\n                #   author: string (nullable = true)\n                #   body: string (nullable = true)\n                #   normalizedBody: string (nullable = true)\n                #   content: string (nullable = true)\n                #   content_len: long (nullable = true)\n                #   summary: string (nullable = true)\n                #   summary_len: long (nullable = true)\n                #   id: string (nullable = true)\n                #   subreddit: string (nullable = true)\n                #   subreddit_id: string (nullable = true)\n                #   title: string (nullable = true)\n                d = json.loads(line)\n                if _SUMMARY in d and _DOCUMENT in d:\n                    yield i, {k: d.get(k, """") for k in _ADDITIONAL_FEATURES + [_DOCUMENT, _SUMMARY]}\n'"
datasets/reddit_tifu/reddit_tifu.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Reddit TIFU dataset using tifu or tldr from subreddit tifu.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\n\nimport nlp\n\n\n_CITATION = """"""\n@misc{kim2018abstractive,\n    title={Abstractive Summarization of Reddit Posts with Multi-level Memory Networks},\n    author={Byeongchang Kim and Hyunwoo Kim and Gunhee Kim},\n    year={2018},\n    eprint={1811.00783},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n""""""\n\n_DESCRIPTION = """"""\nReddit dataset, where TIFU denotes the name of subbreddit /r/tifu.\nAs defined in the publication, styel ""short"" uses title as summary and\n""long"" uses tldr as summary.\n\nFeatures includes:\n  - document: post text without tldr.\n  - tldr: tldr line.\n  - title: trimmed title without tldr.\n  - ups: upvotes.\n  - score: score.\n  - num_comments: number of comments.\n  - upvote_ratio: upvote ratio.\n""""""\n\n_URL = ""https://drive.google.com/uc?export=download&id=1ffWfITKFMJeqjT8loC8aiCLRNJpc_XnF""\n\n_DOCUMENT = ""documents""\n_TITLE = ""title""\n_TLDR = ""tldr""\n_ADDITIONAL_FEATURES = [""ups"", ""num_comments"", ""score"", ""upvote_ratio""]\n\n\nclass RedditTifuConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for RedditTifu.""""""\n\n    def __init__(self, summary_key=None, **kwargs):\n        """"""BuilderConfig for RedditTifu.\n\n    Args:\n      summary_key: key string of summary in downloaded json file.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        # Version 1.1.0 remove empty document and summary strings.\n        super(RedditTifuConfig, self).__init__(version=nlp.Version(""1.1.0""), **kwargs)\n        self.summary_key = summary_key\n\n\nclass RedditTifu(nlp.GeneratorBasedBuilder):\n    """"""Reddit TIFU Dataset.""""""\n\n    BUILDER_CONFIGS = [\n        RedditTifuConfig(name=""short"", summary_key=_TITLE, description=""Using title as summary."",),\n        RedditTifuConfig(name=""long"", summary_key=_TLDR, description=""Using TLDR as summary."",),\n    ]\n\n    def _info(self):\n        features = {\n            ""ups"": nlp.Value(""float32""),\n            ""num_comments"": nlp.Value(""float32""),\n            ""upvote_ratio"": nlp.Value(""float32""),\n            ""score"": nlp.Value(""float32""),\n        }\n        features.update({k: nlp.Value(""string"") for k in [_DOCUMENT, _TLDR, _TITLE]})\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(features),\n            supervised_keys=(_DOCUMENT, self.config.summary_key),\n            homepage=""https://github.com/ctr4si/MMN"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_path = dl_manager.download_and_extract(_URL)\n        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""path"": dl_path},)]\n\n    def _generate_examples(self, path=None):\n        """"""Yields examples.""""""\n        with open(path, ""rb"") as f:\n            for i, line in enumerate(f):\n                # keys are \'title_tokenized\',\'permalink\',\'title\',\'url\',\'num_comments\',\n                #   \'tldr\'(optional),\'created_utc\',\'trimmed_title_tokenized\',\'ups\',\n                #   \'selftext_html\',\'score\',\'upvote_ratio\',\'tldr_tokenized\'(optional),\n                #   \'selftext\',\'trimmed_title\',\'selftext_without_tldr_tokenized\',\n                #   \'id\',\'selftext_without_tldr\'\n                d = json.loads(line)\n                r = {\n                    _DOCUMENT: d[""selftext_without_tldr""].strip(),\n                    _TITLE: d[""trimmed_title""].strip(),\n                    _TLDR: (d[""tldr""] or """").strip(),\n                }\n                r.update({k: d[k] for k in _ADDITIONAL_FEATURES})\n                # skip if document or summary is empty\n                if r[_DOCUMENT] and r[self.config.summary_key]:\n                    yield i, r\n'"
datasets/scan/scan.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""SCAN tasks with various different splits.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@inproceedings{Lake2018GeneralizationWS,\n  title={Generalization without Systematicity: On the Compositional Skills of\n         Sequence-to-Sequence Recurrent Networks},\n  author={Brenden M. Lake and Marco Baroni},\n  booktitle={ICML},\n  year={2018},\n  url={https://arxiv.org/pdf/1711.00350.pdf},\n}\n""""""\n\n_DESCRIPTION = """"""SCAN tasks with various splits.\n\nSCAN is a set of simple language-driven navigation tasks for studying\ncompositional learning and zero-shot generalization.\n\nSee https://github.com/brendenlake/SCAN for a description of the splits.\n\nExample usage:\ndata = nlp.load_dataset(\'scan/length\')\n""""""\n\n_DATA_URL = ""https://github.com/brendenlake/SCAN/archive/master.zip""\n\n\nclass ScanConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for SCAN.""""""\n\n    def __init__(self, name, directory=None, **kwargs):\n        """"""BuilderConfig for SCAN.\n\n    Args:\n      name: Unique name of the split.\n      directory: Which subdirectory to read the split from.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        # Version history:\n        super(ScanConfig, self).__init__(name=name, version=nlp.Version(""1.0.0""), description=_DESCRIPTION, **kwargs)\n        if directory is None:\n            self.directory = name + ""_split""\n        else:\n            self.directory = directory\n\n\n_COMMANDS = ""commands""\n_ACTIONS = ""actions""\n\n\nclass Scan(nlp.GeneratorBasedBuilder):\n    """"""SCAN task / splits as proposed by Brenden M. Lake and Marco Baroni.""""""\n\n    BUILDER_CONFIGS = [\n        ScanConfig(name=""simple""),\n        ScanConfig(name=""addprim_jump"", directory=""add_prim_split""),\n        ScanConfig(name=""addprim_turn_left"", directory=""add_prim_split""),\n        ScanConfig(name=""filler_num0"", directory=""filler_split""),\n        ScanConfig(name=""filler_num1"", directory=""filler_split""),\n        ScanConfig(name=""filler_num2"", directory=""filler_split""),\n        ScanConfig(name=""filler_num3"", directory=""filler_split""),\n        ScanConfig(name=""length""),\n        ScanConfig(name=""template_around_right"", directory=""template_split""),\n        ScanConfig(name=""template_jump_around_right"", directory=""template_split""),\n        ScanConfig(name=""template_opposite_right"", directory=""template_split""),\n        ScanConfig(name=""template_right"", directory=""template_split""),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({_COMMANDS: nlp.Value(""string""), _ACTIONS: nlp.Value(""string""),}),\n            supervised_keys=None,\n            homepage=""https://github.com/brendenlake/SCAN"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        data_dir = dl_manager.download_and_extract(_DATA_URL)\n        data_dir = os.path.join(data_dir, ""SCAN-master"", self.config.directory)\n        split = self.config.name\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""filepath"": os.path.join(data_dir, ""tasks_train_"" + split + "".txt"")}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST, gen_kwargs={""filepath"": os.path.join(data_dir, ""tasks_test_"" + split + "".txt"")}\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        with open(filepath) as infile:\n            for i, line in enumerate(infile):\n                if not line.startswith(""IN: ""):\n                    continue\n                # Chop the prefix and split string between input and output\n                commands, actions = line[len(""IN: "") :].strip().split("" OUT: "", 1)\n                yield i, {_COMMANDS: commands, _ACTIONS: actions}\n'"
datasets/scicite/scicite.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""TODO(scicite): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@InProceedings{Cohan2019Structural,\n  author={Arman Cohan and Waleed Ammar and Madeleine Van Zuylen and Field Cady},\n  title={Structural Scaffolds for Citation Intent Classification in Scientific Publications},\n  booktitle=""NAACL"",\n  year=""2019""\n}\n""""""\n\n_DESCRIPTION = """"""\nThis is a dataset for classifying citation intents in academic papers.\nThe main citation intent label for each Json object is specified with the label\nkey while the citation context is specified in with a context key. Example:\n{\n \'string\': \'In chacma baboons, male-infant relationships can be linked to both\n    formation of friendships and paternity success [30,31].\'\n \'sectionName\': \'Introduction\',\n \'label\': \'background\',\n \'citingPaperId\': \'7a6b2d4b405439\',\n \'citedPaperId\': \'9d1abadc55b5e0\',\n ...\n }\nYou may obtain the full information about the paper using the provided paper ids\nwith the Semantic Scholar API (https://api.semanticscholar.org/).\nThe labels are:\nMethod, Background, Result\n""""""\n\n_SOURCE_NAMES = [""properNoun"", ""andPhrase"", ""acronym"", ""etAlPhrase"", ""explicit"", ""acronymParen"", ""nan""]\n\n\nclass Scicite(nlp.GeneratorBasedBuilder):\n    """"""This is a dataset for classifying citation intents in academic papers.""""""\n\n    VERSION = nlp.Version(""1.0.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""string"": nlp.Value(""string""),\n                    ""sectionName"": nlp.Value(""string""),\n                    ""label"": nlp.features.ClassLabel(names=[""method"", ""background"", ""result""]),\n                    ""citingPaperId"": nlp.Value(""string""),\n                    ""citedPaperId"": nlp.Value(""string""),\n                    ""excerpt_index"": nlp.Value(""int32""),\n                    ""isKeyCitation"": nlp.Value(""bool""),\n                    ""label2"": nlp.features.ClassLabel(\n                        names=[""supportive"", ""not_supportive"", ""cant_determine"", ""none""]\n                    ),\n                    ""citeEnd"": nlp.Value(""int64""),\n                    ""citeStart"": nlp.Value(""int64""),\n                    ""source"": nlp.features.ClassLabel(names=_SOURCE_NAMES),\n                    ""label_confidence"": nlp.Value(""float32""),\n                    ""label2_confidence"": nlp.Value(""float32""),\n                    ""id"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/allenai/scicite"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_paths = dl_manager.download_and_extract(\n            {""scicite"": ""https://s3-us-west-2.amazonaws.com/ai2-s2-research/scicite/scicite.tar.gz"",}\n        )\n        path = os.path.join(dl_paths[""scicite""], ""scicite"")\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""path"": os.path.join(path, ""train.jsonl"")},),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""path"": os.path.join(path, ""dev.jsonl"")},),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""path"": os.path.join(path, ""test.jsonl"")},),\n        ]\n\n    def _generate_examples(self, path=None):\n        """"""Yields examples.""""""\n        with open(path) as f:\n            unique_ids = {}\n            for line in f:\n                d = json.loads(line)\n                unique_id = str(d[""unique_id""])\n                if unique_id in unique_ids:\n                    continue\n                unique_ids[unique_id] = True\n                yield unique_id, {\n                    ""string"": d[""string""],\n                    ""label"": str(d[""label""]),\n                    ""sectionName"": str(d[""sectionName""]),\n                    ""citingPaperId"": str(d[""citingPaperId""]),\n                    ""citedPaperId"": str(d[""citedPaperId""]),\n                    ""excerpt_index"": int(d[""excerpt_index""]),\n                    ""isKeyCitation"": bool(d[""isKeyCitation""]),\n                    ""label2"": str(d.get(""label2"", ""none"")),\n                    ""citeEnd"": _safe_int(d[""citeEnd""]),\n                    ""citeStart"": _safe_int(d[""citeStart""]),\n                    ""source"": str(d[""source""]),\n                    ""label_confidence"": float(d.get(""label_confidence"", 0.0)),\n                    ""label2_confidence"": float(d.get(""label2_confidence"", 0.0)),\n                    ""id"": str(d[""id""]),\n                }\n\n\ndef _safe_int(a):\n    try:\n        # skip NaNs\n        return int(a)\n    except ValueError:\n        return -1\n'"
datasets/scientific_papers/scientific_papers.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Scientific Papers Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@article{Cohan_2018,\n   title={A Discourse-Aware Attention Model for Abstractive Summarization of\n            Long Documents},\n   url={http://dx.doi.org/10.18653/v1/n18-2097},\n   DOI={10.18653/v1/n18-2097},\n   journal={Proceedings of the 2018 Conference of the North American Chapter of\n          the Association for Computational Linguistics: Human Language\n          Technologies, Volume 2 (Short Papers)},\n   publisher={Association for Computational Linguistics},\n   author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},\n   year={2018}\n}\n""""""\n\n_DESCRIPTION = """"""\nScientific papers datasets contains two sets of long and structured documents.\nThe datasets are obtained from ArXiv and PubMed OpenAccess repositories.\n\nBoth ""arxiv"" and ""pubmed"" have two features:\n  - article: the body of the document, pagragraphs seperated by ""/n"".\n  - abstract: the abstract of the document, pagragraphs seperated by ""/n"".\n  - section_names: titles of sections, seperated by ""/n"".\n\n""""""\n\n_DOCUMENT = ""article""\n_SUMMARY = ""abstract""\n\n_URLS = {\n    ""arxiv"": ""https://drive.google.com/uc?id=1b3rmCSIoh6VhD4HKWjI4HOW-cSwcwbeC&export=download"",\n    ""pubmed"": ""https://drive.google.com/uc?id=1lvsqvsFi3W-pE1SqNZI0s8NR9rC1tsja&export=download"",\n}\n\n\nclass ScientificPapersConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Scientific Papers.""""""\n\n    def __init__(self, filename=None, **kwargs):\n        """"""BuilderConfig for Wikihow.\n\n    Args:\n      filename: filename of different configs for the dataset.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        # 1.1.0 remove sentence breaker <S> and </S> in summary.\n        super(ScientificPapersConfig, self).__init__(version=nlp.Version(""1.1.1""), **kwargs)\n        self.filename = filename\n\n\nclass ScientificPapers(nlp.GeneratorBasedBuilder):\n    """"""Scientific Papers.""""""\n\n    BUILDER_CONFIGS = [\n        ScientificPapersConfig(name=""pubmed"", description=""Documents from PubMed repository.""),\n        ScientificPapersConfig(name=""arxiv"", description=""Documents from ArXiv repository.""),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {_DOCUMENT: nlp.Value(""string""), _SUMMARY: nlp.Value(""string""), ""section_names"": nlp.Value(""string""),}\n            ),\n            supervised_keys=None,\n            homepage=""https://github.com/armancohan/long-summarization"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_paths = dl_manager.download_and_extract(_URLS)\n        path = os.path.join(dl_paths[self.config.name], self.config.name + ""-dataset"")\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""path"": os.path.join(path, ""train.txt"")},),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""path"": os.path.join(path, ""val.txt"")},),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""path"": os.path.join(path, ""test.txt"")},),\n        ]\n\n    def _generate_examples(self, path=None):\n        """"""Yields examples.""""""\n        with open(path) as f:\n            for line in f:\n                # Possible keys are:\n                # ""article_id"": str\n                # ""article_text"": list[str] article (list of paragraphs).\n                # ""abstract_text"": list[str], abstract (list of paragraphs).\n                # ""section_names"": list[str], list of section names.\n                # ""sections"": list[list[str]], list of sections (list of paragraphs)\n                d = json.loads(line)\n                summary = ""\\n"".join(d[""abstract_text""])\n                # In original paper, <S> and </S> are not used in vocab during training\n                # or during decoding.\n                # https://github.com/armancohan/long-summarization/blob/master/data.py#L27\n                summary = summary.replace(""<S>"", """").replace(""</S>"", """")\n                yield d[""article_id""], {\n                    _DOCUMENT: ""\\n"".join(d[""article_text""]),\n                    _SUMMARY: summary,\n                    ""section_names"": ""\\n"".join(d[""section_names""]),\n                }\n'"
datasets/scifact/scifact.py,0,"b'""""""TODO(scifact): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(scifact): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{scifact2020\n  title={ Fact or Fiction: Verifying Scientific Claims},\n  author={David,  Wadden and Kyle, Lo and Lucy Lu, Wang and Shanchuan, Lin and Madeleine van, Zuylen and Arman, Cohan and  Hannaneh, Hajishirzi},\n  booktitle={2011 AAAI Spring Symposium Series},\n  year={2020},\n}\n""""""\n\n# TODO(scifact):\n_DESCRIPTION = """"""\\\nSciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts, and annotated with labels and rationales\n""""""\n\n_URL = ""https://ai2-s2-scifact.s3-us-west-2.amazonaws.com/release/2020-05-01/data.tar.gz""\n\n\nclass ScifactConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Scifact""""""\n\n    def __init__(self, **kwargs):\n        """"""\n\n        Args:\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(ScifactConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass Scifact(nlp.GeneratorBasedBuilder):\n    """"""TODO(scifact): Short description of my dataset.""""""\n\n    # TODO(scifact): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        ScifactConfig(name=""corpus"", description="" The corpus of evidence documents""),\n        ScifactConfig(name=""claims"", description="" The claims are split into train, test, dev""),\n    ]\n\n    def _info(self):\n        # TODO(scifact): Specifies the nlp.DatasetInfo object\n        if self.config.name == ""corpus"":\n            features = {\n                ""doc_id"": nlp.Value(""int32""),  # The document\'s S2ORC ID.\n                ""title"": nlp.Value(""string""),  # The title.\n                ""abstract"": nlp.features.Sequence(\n                    {""sentence"": nlp.Value(""string"")}\n                ),  # The abstract, written as a list of sentences.\n                ""structured"": nlp.Value(""bool""),  # Indicator for whether this is a structured abstract.\n            }\n        else:\n            features = {\n                ""id"": nlp.Value(""int32""),  # An integer claim ID.\n                ""claim"": nlp.Value(""string""),  # The text of the claim.\n                ""evidence_doc_id"": nlp.Value(""string""),\n                ""evidence_label"": nlp.Value(""string""),  # Label for the rationale.\n                ""evidence_sentences"": nlp.features.Sequence({""sentence"": nlp.Value(""int32"")}),  # Rationale sentences.\n                ""cited_doc_ids"": nlp.features.Sequence(\n                    {""doc_id"": nlp.Value(""int32"")}\n                ),  # The claim\'s ""cited documents"".\n            }\n\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                features\n                # These are the features of your dataset like images, labels ...\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://scifact.apps.allenai.org/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(scifact): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n\n        if self.config.name == ""corpus"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dl_dir, ""data"", ""corpus.jsonl""), ""split"": ""train""},\n                ),\n            ]\n        else:\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dl_dir, ""data"", ""claims_train.jsonl""), ""split"": ""train""},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dl_dir, ""data"", ""claims_test.jsonl""), ""split"": ""test""},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dl_dir, ""data"", ""claims_dev.jsonl""), ""split"": ""dev""},\n                ),\n            ]\n\n    def _generate_examples(self, filepath, split):\n        """"""Yields examples.""""""\n        # TODO(scifact): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n                if self.config.name == ""corpus"":\n                    yield id_, {\n                        ""doc_id"": int(data[""doc_id""]),\n                        ""title"": data[""title""],\n                        ""abstract"": {""sentence"": data[""abstract""]},\n                        ""structured"": data[""structured""],\n                    }\n                else:\n                    if split == ""test"":\n                        yield id_, {\n                            ""id"": data[""id""],\n                            ""claim"": data[""claim""],\n                            ""evidence_doc_id"": """",\n                            ""evidence_label"": """",\n                            ""evidence_sentences"": {""sentence"": []},\n                            ""cited_doc_ids"": {""doc_id"": []},\n                        }\n                    else:\n                        evidences = data[""evidence""]\n                        if evidences:\n                            for id1, doc_id in enumerate(evidences):\n                                for id2, evidence in enumerate(evidences[doc_id]):\n                                    yield str(id_) + ""_"" + str(id1) + ""_"" + str(id2), {\n                                        ""id"": data[""id""],\n                                        ""claim"": data[""claim""],\n                                        ""evidence_doc_id"": doc_id,\n                                        ""evidence_label"": evidence[""label""],\n                                        ""evidence_sentences"": {""sentence"": evidence[""sentences""]},\n                                        ""cited_doc_ids"": {""doc_id"": data.get(""cited_doc_ids"", [])},\n                                    }\n                        else:\n                            yield id_, {\n                                ""id"": data[""id""],\n                                ""claim"": data[""claim""],\n                                ""evidence_doc_id"": """",\n                                ""evidence_label"": """",\n                                ""evidence_sentences"": {""sentence"": []},\n                                ""cited_doc_ids"": {""doc_id"": data.get(""cited_doc_ids"", [])},\n                            }\n'"
datasets/sciq/sciq.py,0,"b'""""""TODO(sciQ): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(sciQ): BibTeX citation\n_CITATION = """"""\\\n@inproceedings{SciQ,\n    title={Crowdsourcing Multiple Choice Science Questions},\n    author={Johannes Welbl, Nelson F. Liu, Matt Gardner},\n    year={2017},\n    journal={arXiv:1707.06209v1}\n}\n""""""\n\n# TODO(sciQ):\n_DESCRIPTION = """"""\\\nThe SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided.\n\n""""""\n_URL = ""https://s3-us-west-2.amazonaws.com/ai2-website/data/SciQ.zip""\n\n\nclass Sciq(nlp.GeneratorBasedBuilder):\n    """"""TODO(sciQ): Short description of my dataset.""""""\n\n    # TODO(sciQ): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(sciQ): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""question"": nlp.Value(""string""),\n                    ""distractor3"": nlp.Value(""string""),\n                    ""distractor1"": nlp.Value(""string""),\n                    ""distractor2"": nlp.Value(""string""),\n                    ""correct_answer"": nlp.Value(""string""),\n                    ""support"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://allenai.org/data/sciq"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(sciQ): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""SciQ dataset-2 3"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""train.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""valid.json"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""test.json"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(sciQ): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for id_, row in enumerate(data):\n                yield id_, row\n'"
datasets/scitail/scitail.py,0,"b'""""""TODO(sciTail): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport json\nimport os\nimport textwrap\n\nimport nlp\n\n\n# TODO(sciTail): BibTeX citation\n_CITATION = """"""\\\ninproceedings{scitail,\n     Author = {Tushar Khot and Ashish Sabharwal and Peter Clark},\n     Booktitle = {AAAI},\n     Title = {{SciTail}: A Textual Entailment Dataset from Science Question Answering},\n     Year = {2018}\n}\n""""""\n\n# TODO(sciTail):\n_DESCRIPTION = """"""\\\nThe SciTail dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question \nand the correct answer choice are converted into an assertive statement to form the hypothesis. We use information \nretrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We \ncrowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create \nthe SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples \nwith neutral label\n""""""\n\n_URL = ""http://data.allenai.org.s3.amazonaws.com/downloads/SciTailV1.1.zip""\n\n\nclass ScitailConfig(nlp.BuilderConfig):\n\n    """""" BuilderConfig for Xquad""""""\n\n    def __init__(self, **kwargs):\n        """"""\n\n        Args:\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(ScitailConfig, self).__init__(\n            version=nlp.Version(""1.1.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass Scitail(nlp.GeneratorBasedBuilder):\n    """"""TODO(sciTail): Short description of my dataset.""""""\n\n    # TODO(sciTail): Set up version.\n    VERSION = nlp.Version(""1.1.0"")\n    BUILDER_CONFIGS = [\n        ScitailConfig(\n            name=""snli_format"",\n            description=""JSONL format used by SNLI with a JSON object corresponding to each entailment example in each line."",\n        ),\n        ScitailConfig(\n            name=""tsv_format"", description=""Tab-separated format with three columns: premise hypothesis label""\n        ),\n        ScitailConfig(\n            name=""dgem_format"",\n            description=""Tab-separated format used by the DGEM model: premise hypothesis label hypothesis graph structure"",\n        ),\n        ScitailConfig(\n            name=""predictor_format"",\n            description=textwrap.dedent(\n                """"""\\\n          AllenNLP predictors work only with JSONL format. This folder contains the SciTail train/dev/test in JSONL format\n        so that it can be loaded into the predictors. Each line is a JSON object with the following keys:\n        gold_label : the example label from {entails, neutral}\n        sentence1: the premise\n        sentence2: the hypothesis\n        sentence2_structure: structure from the hypothesis """"""\n            ),\n        ),\n    ]\n\n    def _info(self):\n        # TODO(sciTail): Specifies the nlp.DatasetInfo object\n        if self.config.name == ""snli_format"":\n            return nlp.DatasetInfo(\n                # This is the description that will appear on the datasets page.\n                description=_DESCRIPTION,\n                # nlp.features.FeatureConnectors\n                features=nlp.Features(\n                    {\n                        ""sentence1_binary_parse"": nlp.Value(""string""),\n                        ""sentence1_parse"": nlp.Value(""string""),\n                        ""sentence1"": nlp.Value(""string""),\n                        ""sentence2_parse"": nlp.Value(""string""),\n                        ""sentence2"": nlp.Value(""string""),\n                        ""annotator_labels"": nlp.features.Sequence({""annotator_label"": nlp.Value(""string"")}),\n                        ""gold_label"": nlp.Value(""string"")\n                        # These are the features of your dataset like images, labels ...\n                    }\n                ),\n                # If there\'s a common (input, target) tuple from the features,\n                # specify them here. They\'ll be used if as_supervised=True in\n                # builder.as_dataset.\n                supervised_keys=None,\n                # Homepage of the dataset for documentation\n                homepage=""https://allenai.org/data/scitail"",\n                citation=_CITATION,\n            )\n        elif self.config.name == ""tsv_format"":\n            return nlp.DatasetInfo(\n                # This is the description that will appear on the datasets page.\n                description=_DESCRIPTION,\n                # nlp.features.FeatureConnectors\n                features=nlp.Features(\n                    {\n                        ""premise"": nlp.Value(""string""),\n                        ""hypothesis"": nlp.Value(""string""),\n                        ""label"": nlp.Value(""string"")\n                        # These are the features of your dataset like images, labels ...\n                    }\n                ),\n                # If there\'s a common (input, target) tuple from the features,\n                # specify them here. They\'ll be used if as_supervised=True in\n                # builder.as_dataset.\n                supervised_keys=None,\n                # Homepage of the dataset for documentation\n                homepage=""https://allenai.org/data/scitail"",\n                citation=_CITATION,\n            )\n        elif self.config.name == ""predictor_format"":\n            return nlp.DatasetInfo(\n                # This is the description that will appear on the datasets page.\n                description=_DESCRIPTION,\n                # nlp.features.FeatureConnectors\n                features=nlp.Features(\n                    {\n                        ""answer"": nlp.Value(""string""),\n                        ""sentence2_structure"": nlp.Value(""string""),\n                        ""sentence1"": nlp.Value(""string""),\n                        ""sentence2"": nlp.Value(""string""),\n                        ""gold_label"": nlp.Value(""string""),\n                        ""question"": nlp.Value(""string"")\n                        # These are the features of your dataset like images, labels ...\n                    }\n                ),\n                # If there\'s a common (input, target) tuple from the features,\n                # specify them here. They\'ll be used if as_supervised=True in\n                # builder.as_dataset.\n                supervised_keys=None,\n                # Homepage of the dataset for documentation\n                homepage=""https://allenai.org/data/scitail"",\n                citation=_CITATION,\n            )\n        elif self.config.name == ""dgem_format"":\n            return nlp.DatasetInfo(\n                # This is the description that will appear on the datasets page.\n                description=_DESCRIPTION,\n                # nlp.features.FeatureConnectors\n                features=nlp.Features(\n                    {\n                        ""premise"": nlp.Value(""string""),\n                        ""hypothesis"": nlp.Value(""string""),\n                        ""label"": nlp.Value(""string""),\n                        ""hypothesis_graph_structure"": nlp.Value(""string"")\n                        # These are the features of your dataset like images, labels ...\n                    }\n                ),\n                # If there\'s a common (input, target) tuple from the features,\n                # specify them here. They\'ll be used if as_supervised=True in\n                # builder.as_dataset.\n                supervised_keys=None,\n                # Homepage of the dataset for documentation\n                homepage=""https://allenai.org/data/scitail"",\n                citation=_CITATION,\n            )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(sciTail): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""SciTailV1.1"")\n        snli = os.path.join(data_dir, ""snli_format"")\n        dgem = os.path.join(data_dir, ""dgem_format"")\n        tsv = os.path.join(data_dir, ""tsv_format"")\n        predictor = os.path.join(data_dir, ""predictor_format"")\n        if self.config.name == ""snli_format"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(snli, ""scitail_1.0_train.txt"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(snli, ""scitail_1.0_test.txt"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(snli, ""scitail_1.0_dev.txt"")},\n                ),\n            ]\n        elif self.config.name == ""tsv_format"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(tsv, ""scitail_1.0_train.tsv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(tsv, ""scitail_1.0_test.tsv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(tsv, ""scitail_1.0_dev.tsv"")},\n                ),\n            ]\n        elif self.config.name == ""predictor_format"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(predictor, ""scitail_1.0_structure_train.jsonl"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(predictor, ""scitail_1.0_structure_test.jsonl"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(predictor, ""scitail_1.0_structure_dev.jsonl"")},\n                ),\n            ]\n        elif self.config.name == ""dgem_format"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dgem, ""scitail_1.0_structure_train.tsv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dgem, ""scitail_1.0_structure_test.tsv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(dgem, ""scitail_1.0_structure_dev.tsv"")},\n                ),\n            ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(sciTail): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            if self.config.name == ""snli_format"":\n                for id_, row in enumerate(f):\n                    data = json.loads(row)\n\n                    yield id_, {\n                        ""sentence1_binary_parse"": data[""sentence1_binary_parse""],\n                        ""sentence1_parse"": data[""sentence1_parse""],\n                        ""sentence1"": data[""sentence1""],\n                        ""sentence2_parse"": data[""sentence2_parse""],\n                        ""sentence2"": data[""sentence2""],\n                        ""annotator_labels"": {""annotator_label"": data[""annotator_labels""]},\n                        ""gold_label"": data[""gold_label""],\n                    }\n            elif self.config.name == ""tsv_format"":\n                data = csv.reader(f, delimiter=""\\t"")\n                for id_, row in enumerate(data):\n                    yield id_, {""premise"": row[0], ""hypothesis"": row[1], ""label"": row[2]}\n            elif self.config.name == ""dgem_format"":\n                data = csv.reader(f, delimiter=""\\t"")\n                for id_, row in enumerate(data):\n                    yield id_, {\n                        ""premise"": row[0],\n                        ""hypothesis"": row[1],\n                        ""label"": row[2],\n                        ""hypothesis_graph_structure"": row[3],\n                    }\n            elif self.config.name == ""predictor_format"":\n                for id_, row in enumerate(f):\n                    data = json.loads(row)\n                    yield id_, {\n                        ""answer"": data[""answer""],\n                        ""sentence2_structure"": data[""sentence2_structure""],\n                        ""sentence1"": data[""sentence1""],\n                        ""sentence2"": data[""sentence2""],\n                        ""gold_label"": data[""gold_label""],\n                        ""question"": data[""question""],\n                    }\n'"
datasets/sentiment140/sentiment140.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@article{go2009twitter,\n  title={Twitter sentiment classification using distant supervision},\n  author={Go, Alec and Bhayani, Richa and Huang, Lei},\n  journal={CS224N project report, Stanford},\n  volume={1},\n  number={12},\n  pages={2009},\n  year={2009}\n}\n""""""\n\n_DESCRIPTION = """"""\\\nSentiment140 consists of Twitter messages with emoticons, which are used as noisy labels for\nsentiment classification. For more detailed information please refer to the paper.\n""""""\n_URL = ""http://help.sentiment140.com/home""\n_DATA_URL = ""http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip""\n\n_TEST_FILE_NAME = ""testdata.manual.2009.06.14.csv""\n_TRAIN_FILE_NAME = ""training.1600000.processed.noemoticon.csv""\n\n\nclass Sentiment140Config(nlp.BuilderConfig):\n\n    """"""BuilderConfig for Break""""""\n\n    def __init__(self, data_url, **kwargs):\n        """"""BuilderConfig for BlogAuthorship\n\n        Args:\n          data_url: `string`, url to the dataset (word or raw level)\n          **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(Sentiment140Config, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n        self.data_url = data_url\n\n\nclass Sentiment140(nlp.GeneratorBasedBuilder):\n\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        Sentiment140Config(\n            name=""sentiment140"",\n            data_url=_DATA_URL,\n            description=""sentiment classification dataset. Twitter messages are classified as either \'positive\'=0, \'neutral\'=1 or \'negative\'=2."",\n        )\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""text"": nlp.Value(""string""),\n                    ""date"": nlp.Value(""string""),\n                    ""user"": nlp.Value(""string""),\n                    ""sentiment"": nlp.Value(""int32""),\n                    ""query"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        data_dir = dl_manager.download_and_extract(_DATA_URL)\n\n        test_csv_file = os.path.join(data_dir, _TEST_FILE_NAME)\n        train_csv_file = os.path.join(data_dir, _TRAIN_FILE_NAME)\n\n        if self.config.name == ""sentiment140"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""file_path"": train_csv_file},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""file_path"": test_csv_file},\n                ),\n            ]\n        else:\n            raise NotImplementedError(""{} does not exist"".format(self.config.name))\n\n    def _generate_examples(self, file_path):\n        """"""Yields examples.""""""\n\n        with open(file_path, encoding=""ISO-8859-1"") as f:\n            data = csv.reader(f, delimiter="","", quotechar=\'""\')\n            for row_id, row in enumerate(data):\n                sentiment, tweet_id, date, query, user_name, message = row\n                yield ""{}_{}"".format(row_id, tweet_id), {\n                    ""text"": message,\n                    ""date"": date,\n                    ""user"": user_name,\n                    ""sentiment"": int(sentiment),\n                    ""query"": query,\n                }\n'"
datasets/snli/snli.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The Stanford Natural Language Inference (SNLI) Corpus.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@inproceedings{snli:emnlp2015,\n\tAuthor = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.},\n\tBooktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n\tPublisher = {Association for Computational Linguistics},\n\tTitle = {A large annotated corpus for learning natural language inference},\n\tYear = {2015}\n}\n""""""\n\n_DESCRIPTION = """"""\\\nThe SNLI corpus (version 1.0) is a collection of 570k human-written English\nsentence pairs manually labeled for balanced classification with the labels\nentailment, contradiction, and neutral, supporting the task of natural language\ninference (NLI), also known as recognizing textual entailment (RTE).\n""""""\n\n_DATA_URL = ""https://nlp.stanford.edu/projects/snli/snli_1.0.zip""\n\n\nclass Snli(nlp.GeneratorBasedBuilder):\n    """"""The Stanford Natural Language Inference (SNLI) Corpus.""""""\n\n    BUILDER_CONFIGS = [\n        nlp.BuilderConfig(\n            name=""plain_text"",\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            description=""Plain text import of SNLI"",\n        )\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""premise"": nlp.Value(""string""),\n                    ""hypothesis"": nlp.Value(""string""),\n                    ""label"": nlp.features.ClassLabel(names=[""entailment"", ""neutral"", ""contradiction""]),\n                }\n            ),\n            # No default supervised_keys (as we have to pass both premise\n            # and hypothesis as input).\n            supervised_keys=None,\n            homepage=""https://nlp.stanford.edu/projects/snli/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        dl_dir = dl_manager.download_and_extract(_DATA_URL)\n        data_dir = os.path.join(dl_dir, ""snli_1.0"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST, gen_kwargs={""filepath"": os.path.join(data_dir, ""snli_1.0_test.txt"")}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": os.path.join(data_dir, ""snli_1.0_dev.txt"")}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""filepath"": os.path.join(data_dir, ""snli_1.0_train.txt"")}\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""This function returns the examples in the raw (text) form.""""""\n        print(filepath)\n        print(""=="" * 100)\n        with open(filepath) as f:\n            reader = csv.DictReader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n            for idx, row in enumerate(reader):\n                label = -1 if row[""gold_label""] == ""-"" else row[""gold_label""]\n                yield idx, {\n                    ""premise"": row[""sentence1""],\n                    ""hypothesis"": row[""sentence2""],\n                    ""label"": label,\n                }\n'"
datasets/social_i_qa/social_i_qa.py,0,"b'""""""TODO(social_i_qa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(social_i_qa): BibTeX citation\n_CITATION = """"""\n""""""\n\n# TODO(social_i_qa):\n_DESCRIPTION = """"""\\\nWe introduce Social IQa: Social Interaction QA, a new question-answering benchmark for testing social commonsense intelligence. Contrary to many prior benchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on reasoning about people\xe2\x80\x99s actions and their social implications. For example, given an action like ""Jesse saw a concert"" and a question like ""Why did Jesse do this?"", humans can easily infer that Jesse wanted ""to see their favorite performer"" or ""to enjoy the music"", and not ""to see what\'s happening inside"" or ""to see if it works"". The actions in Social IQa span a wide variety of social situations, and answer candidates contain both human-curated answers and adversarially-filtered machine-generated candidates. Social IQa contains over 37,000 QA pairs for evaluating models\xe2\x80\x99 abilities to reason about the social implications of everyday events and situations. (Less)\n""""""\n_URL = ""https://storage.googleapis.com/ai2-mosaic/public/socialiqa/socialiqa-train-dev.zip""\n\n\nclass SocialIQa(nlp.GeneratorBasedBuilder):\n    """"""TODO(social_i_qa): Short description of my dataset.""""""\n\n    # TODO(social_i_qa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(social_i_qa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answerA"": nlp.Value(""string""),\n                    ""answerB"": nlp.Value(""string""),\n                    ""answerC"": nlp.Value(""string""),\n                    ""label"": nlp.Value(""string""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://leaderboard.allenai.org/socialiqa/submissions/get-started"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(social_i_qa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        dl_dir = os.path.join(dl_dir, ""socialiqa-train-dev"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    ""filepath"": os.path.join(dl_dir, ""train.jsonl""),\n                    ""labelpath"": os.path.join(dl_dir, ""train-labels.lst""),\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    ""filepath"": os.path.join(dl_dir, ""dev.jsonl""),\n                    ""labelpath"": os.path.join(dl_dir, ""dev-labels.lst""),\n                },\n            ),\n        ]\n\n    def _generate_examples(self, filepath, labelpath):\n        """"""Yields examples.""""""\n        # TODO(social_i_qa): Yields (key, example) tuples from the dataset\n        with open(labelpath) as f:\n            labels = [label for label in f]\n        with open(filepath) as f1:\n            for id_, row in enumerate(f1):\n                data = json.loads(row)\n                label = labels[id_]\n                context = data[""context""]\n                answerA = data[""answerA""]\n                answerB = data[""answerB""]\n                answerC = data[""answerC""]\n                question = data[""question""]\n                yield id_, {\n                    ""context"": context,\n                    ""question"": question,\n                    ""answerA"": answerA,\n                    ""answerB"": answerB,\n                    ""answerC"": answerC,\n                    ""label"": label,\n                }\n'"
datasets/squad/squad.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""SQUAD: The Stanford Question Answering Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport logging\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = ""{SQuAD: 100,000+ Questions for Machine Comprehension of Text}"",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}\n""""""\n\n_DESCRIPTION = """"""\\\nStanford Question Answering Dataset (SQuAD) is a reading comprehension \\\ndataset, consisting of questions posed by crowdworkers on a set of Wikipedia \\\narticles, where the answer to every question is a segment of text, or span, \\\nfrom the corresponding reading passage, or the question might be unanswerable.\n""""""\n\n\nclass SquadConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for SQUAD.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for SQUAD.\n\n    Args:\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(SquadConfig, self).__init__(**kwargs)\n\n\nclass Squad(nlp.GeneratorBasedBuilder):\n    """"""SQUAD: The Stanford Question Answering Dataset. Version 1.1.""""""\n\n    _URL = ""https://rajpurkar.github.io/SQuAD-explorer/dataset/""\n    _DEV_FILE = ""dev-v1.1.json""\n    _TRAINING_FILE = ""train-v1.1.json""\n\n    BUILDER_CONFIGS = [\n        SquadConfig(\n            name=""plain_text"",\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            description=""Plain text"",\n        ),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""title"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                }\n            ),\n            # No default supervised_keys (as we have to pass both question\n            # and context as input).\n            supervised_keys=None,\n            homepage=""https://rajpurkar.github.io/SQuAD-explorer/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        urls_to_download = {\n            ""train"": os.path.join(self._URL, self._TRAINING_FILE),\n            ""dev"": os.path.join(self._URL, self._DEV_FILE),\n        }\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""This function returns the examples in the raw (text) form.""""""\n        logging.info(""generating examples from = %s"", filepath)\n        with open(filepath) as f:\n            squad = json.load(f)\n            for article in squad[""data""]:\n                title = article.get(""title"", """").strip()\n                for paragraph in article[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        # Features currently used are ""context"", ""question"", and ""answers"".\n                        # Others are extracted here for the ease of future expansions.\n                        yield id_, {\n                            ""title"": title,\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                        }\n'"
datasets/squad_es/squad_es.py,0,"b'""""""TODO(squad_es): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(squad_es): BibTeX citation\n_CITATION = """"""\\\n@article{2016arXiv160605250R,\n       author = {Casimiro Pio , Carrino and  Marta R. , Costa-jussa and  Jose A. R. , Fonollosa},\n        title = ""{Automatic Spanish Translation of the SQuAD Dataset for Multilingual\nQuestion Answering}"",\n      journal = {arXiv e-prints},\n         year = 2019,\n          eid = {arXiv:1912.05200v1},\n        pages = {arXiv:1912.05200v1},\narchivePrefix = {arXiv},\n       eprint = {1912.05200v2},\n}\n""""""\n\n# TODO(squad_es_v1):\n_DESCRIPTION = """"""\\\nautomatic translation of the Stanford Question Answering Dataset (SQuAD) v2 into Spanish\n""""""\n\n_URL = ""https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/""\n\n\nclass SquadEsConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for SQUADEsV2.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for SQUADEsV2.\n\n    Args:\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(SquadEsConfig, self).__init__(**kwargs)\n\n\nclass SquadEs(nlp.GeneratorBasedBuilder):\n    """"""TODO(squad_es): Short description of my dataset.""""""\n\n    # TODO(squad_es): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    BUILDER_CONFIGS = [\n        SquadEsConfig(\n            name=""v1.1.0"",\n            version=nlp.Version(""1.1.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            description=""Plain text Spanish squad version 1"",\n        ),\n        SquadEsConfig(\n            name=""v2.0.0"",\n            version=nlp.Version(""2.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            description=""Plain text Spanish squad version 2"",\n        ),\n    ]\n\n    def _info(self):\n        # TODO(squad_es): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""id"": nlp.Value(""string""),\n                    ""title"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/ccasimiro88/TranslateAlignRetrieve"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(squad_es): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n\n        # download and extract URLs\n        v1_urls = {\n            ""train"": os.path.join(_URL, ""SQuAD-es-v1.1/train-v1.1-es.json""),\n            ""dev"": os.path.join(_URL, ""SQuAD-es-v1.1/dev-v1.1-es.json""),\n        }\n        v2_urls = {\n            ""train"": os.path.join(_URL, ""SQuAD-es-v2.0/train-v2.0-es.json""),\n            ""dev"": os.path.join(_URL, ""SQuAD-es-v2.0/dev-v2.0-es.json""),\n        }\n        if self.config.name == ""v1.1.0"":\n            dl_dir = dl_manager.download_and_extract(v1_urls)\n        elif self.config.name == ""v2.0.0"":\n            dl_dir = dl_manager.download_and_extract(v2_urls)\n        else:\n            raise Exception(""version does not match any existing one"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""train""]},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""dev""]},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(squad_es): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for example in data[""data""]:\n                title = example.get(""title"", """").strip()\n                for paragraph in example[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        yield id_, {\n                            ""title"": title,\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                        }\n'"
datasets/squad_it/squad_it.py,0,"b'""""""TODO(squad_it): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(squad_it): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{10.1007/978-3-030-03840-3_29,\n\tauthor=""Croce, Danilo and Zelenanska, Alexandra and Basili, Roberto"",\n\teditor=""Ghidini, Chiara and Magnini, Bernardo and Passerini, Andrea and Traverso, Paolo"",\n\ttitle=""Neural Learning for Question Answering in Italian"",\n\tbooktitle=""AI*IA 2018 -- Advances in Artificial Intelligence"",\n\tyear=""2018"",\n\tpublisher=""Springer International Publishing"",\n\taddress=""Cham"",\n\tpages=""389--402"",\n\tisbn=""978-3-030-03840-3""\n}\n""""""\n\n# TODO(squad_it):\n_DESCRIPTION = """"""\\\nSQuAD-it is derived from the SQuAD dataset and it is obtained through semi-automatic translation of the SQuAD dataset \ninto Italian. It represents a large-scale dataset for open question answering processes on factoid questions in Italian.\n The dataset contains more than 60,000 question/answer pairs derived from the original English dataset. The dataset is \n split into training and test sets to support the replicability of the benchmarking of QA systems:\n""""""\n_URL = ""https://github.com/crux82/squad-it/raw/master""\n_TRAIN_FILE = ""SQuAD_it-train.json.gz""\n_TEST_FILE = ""SQuAD_it-test.json.gz""\n\n\nclass SquadIt(nlp.GeneratorBasedBuilder):\n    """"""TODO(squad_it): Short description of my dataset.""""""\n\n    # TODO(squad_it): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(squad_it): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/crux82/squad-it"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(squad_it): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {""train"": os.path.join(_URL, _TRAIN_FILE), ""test"": os.path.join(_URL, _TEST_FILE)}\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": downloaded_files[""test""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(squad_it): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for example in data[""data""]:\n                for paragraph in example[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        yield id_, {\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                        }\n'"
datasets/squad_v1_pt/squad_v1_pt.py,0,"b'""""""TODO(squad_v1_pt): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(squad_v1_pt): BibTeX citation\n_CITATION = """"""\\\n@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = ""{SQuAD: 100,000+ Questions for Machine Comprehension of Text}"",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}\n""""""\n\n# TODO(squad_v1_pt):\n_DESCRIPTION = """"""\\\nPortuguese translation of the SQuAD dataset. The translation was performed automatically using the Google Cloud API.\n""""""\n_URL = ""https://github.com/nunorc/squad-v1.1-pt/raw/master""\n_TRAIN_FILE = ""train-v1.1-pt.json""\n_DEV_FILE = ""dev-v1.1-pt.json""\n\n\nclass SquadV1Pt(nlp.GeneratorBasedBuilder):\n    """"""TODO(squad_v1_pt): Short description of my dataset.""""""\n\n    # TODO(squad_v1_pt): Set up version.\n    VERSION = nlp.Version(""1.1.0"")\n\n    def _info(self):\n        # TODO(squad_v1_pt): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""title"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/nunorc/squad-v1.1-pt"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(squad_v1_pt): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {""train"": os.path.join(_URL, _TRAIN_FILE), ""dev"": os.path.join(_URL, _DEV_FILE)}\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(squad_v1_pt): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = json.load(f)\n            for example in data[""data""]:\n                title = example.get(""title"", """").strip()\n                for paragraph in example[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        yield id_, {\n                            ""title"": title,\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                        }\n'"
datasets/squad_v2/squad_v2.py,0,"b'""""""TODO(squad_v2): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(squad_v2): BibTeX citation\n_CITATION = """"""\\\n@article{2016arXiv160605250R,\n       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                 Konstantin and {Liang}, Percy},\n        title = ""{SQuAD: 100,000+ Questions for Machine Comprehension of Text}"",\n      journal = {arXiv e-prints},\n         year = 2016,\n          eid = {arXiv:1606.05250},\n        pages = {arXiv:1606.05250},\narchivePrefix = {arXiv},\n       eprint = {1606.05250},\n}\n""""""\n\n_DESCRIPTION = """"""\\\ncombines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers\n to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but \n also determine when no answer is supported by the paragraph and abstain from answering.\n""""""\n\n_URL = ""https://rajpurkar.github.io/SQuAD-explorer/dataset/""\n_DEV_FILE = ""dev-v2.0.json""\n_TRAINING_FILE = ""train-v2.0.json""\n\n\nclass SquadV2Config(nlp.BuilderConfig):\n    """"""BuilderConfig for SQUAD.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for SQUADV2.\n\n    Args:\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(SquadV2Config, self).__init__(**kwargs)\n\n\nclass SquadV2(nlp.GeneratorBasedBuilder):\n    """"""TODO(squad_v2): Short description of my dataset.""""""\n\n    # TODO(squad_v2): Set up version.\n    BUILDER_CONFIGS = [\n        SquadV2Config(name=""squad_v2"", version=nlp.Version(""2.0.0""), description=""SQuAD plaint text version 2""),\n    ]\n\n    def _info(self):\n        # TODO(squad_v2): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""title"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://rajpurkar.github.io/SQuAD-explorer/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(squad_v2): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {""train"": os.path.join(_URL, _TRAINING_FILE), ""dev"": os.path.join(_URL, _DEV_FILE)}\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(squad_v2): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            squad = json.load(f)\n            for example in squad[""data""]:\n                title = example.get(""title"", """").strip()\n                for paragraph in example[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        # Features currently used are ""context"", ""question"", and ""answers"".\n                        # Others are extracted here for the ease of future expansions.\n                        yield id_, {\n                            ""title"": title,\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                        }\n'"
datasets/super_glue/super_glue.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The SuperGLUE benchmark.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport six\n\nimport nlp\n\n\n_SUPER_GLUE_CITATION = """"""\\\n@article{wang2019superglue,\n  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},\n  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},\n  journal={arXiv preprint arXiv:1905.00537},\n  year={2019}\n}\n\nNote that each SuperGLUE dataset has its own citation. Please see the source to\nget the correct citation for each contained dataset.\n""""""\n\n_GLUE_DESCRIPTION = """"""\\\nSuperGLUE (https://super.gluebenchmark.com/) is a new benchmark styled after\nGLUE with a new set of more difficult language understanding tasks, improved\nresources, and a new public leaderboard.\n\n""""""\n\n_BOOLQ_DESCRIPTION = """"""\\\nBoolQ (Boolean Questions, Clark et al., 2019a) is a QA task where each example consists of a short\npassage and a yes/no question about the passage. The questions are provided anonymously and\nunsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\nWikipedia article containing the answer. Following the original work, we evaluate with accuracy.""""""\n\n_CB_DESCRIPTION = """"""\\\nThe CommitmentBank (De Marneffe et al., 2019) is a corpus of short texts in which at least\none sentence contains an embedded clause. Each of these embedded clauses is annotated with the\ndegree to which we expect that the person who wrote the text is committed to the truth of the clause.\nThe resulting task framed as three-class textual entailment on examples that are drawn from the Wall\nStreet Journal, fiction from the British National Corpus, and Switchboard. Each example consists\nof a premise containing an embedded clause and the corresponding hypothesis is the extraction of\nthat clause. We use a subset of the data that had inter-annotator agreement above 0.85. The data is\nimbalanced (relatively fewer neutral examples), so we evaluate using accuracy and F1, where for\nmulti-class F1 we compute the unweighted average of the F1 per class.""""""\n\n_COPA_DESCRIPTION = """"""\\\nThe Choice Of Plausible Alternatives (COPA, Roemmele et al., 2011) dataset is a causal\nreasoning task in which a system is given a premise sentence and two possible alternatives. The\nsystem must choose the alternative which has the more plausible causal relationship with the premise.\nThe method used for the construction of the alternatives ensures that the task requires causal reasoning\nto solve. Examples either deal with alternative possible causes or alternative possible effects of the\npremise sentence, accompanied by a simple question disambiguating between the two instance\ntypes for the model. All examples are handcrafted and focus on topics from online blogs and a\nphotography-related encyclopedia. Following the recommendation of the authors, we evaluate using\naccuracy.""""""\n\n_RECORD_DESCRIPTION = """"""\\\n(Reading Comprehension with Commonsense Reasoning Dataset, Zhang et al., 2018) is a\nmultiple-choice QA task. Each example consists of a news article and a Cloze-style question about\nthe article in which one entity is masked out. The system must predict the masked out entity from a\ngiven list of possible entities in the provided passage, where the same entity may be expressed using\nmultiple different surface forms, all of which are considered correct. Articles are drawn from CNN\nand Daily Mail. Following the original work, we evaluate with max (over all mentions) token-level\nF1 and exact match (EM).""""""\n\n_RTE_DESCRIPTION = """"""\\\nThe Recognizing Textual Entailment (RTE) datasets come from a series of annual competitions\non textual entailment, the problem of predicting whether a given premise sentence entails a given\nhypothesis sentence (also known as natural language inference, NLI). RTE was previously included\nin GLUE, and we use the same data and format as before: We merge data from RTE1 (Dagan\net al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5 (Bentivogli\net al., 2009). All datasets are combined and converted to two-class classification: entailment and\nnot_entailment. Of all the GLUE tasks, RTE was among those that benefited from transfer learning\nthe most, jumping from near random-chance performance (~56%) at the time of GLUE\'s launch to\n85% accuracy (Liu et al., 2019c) at the time of writing. Given the eight point gap with respect to\nhuman performance, however, the task is not yet solved by machines, and we expect the remaining\ngap to be difficult to close.""""""\n\n_MULTIRC_DESCRIPTION = """"""\\\nThe Multi-Sentence Reading Comprehension dataset (MultiRC, Khashabi et al., 2018)\nis a true/false question-answering task. Each example consists of a context paragraph, a question\nabout that paragraph, and a list of possible answers to that question which must be labeled as true or\nfalse. Question-answering (QA) is a popular problem with many datasets. We use MultiRC because\nof a number of desirable properties: (i) each question can have multiple possible correct answers,\nso each question-answer pair must be evaluated independent of other pairs, (ii) the questions are\ndesigned such that answering each question requires drawing facts from multiple context sentences,\nand (iii) the question-answer pair format more closely matches the API of other SuperGLUE tasks\nthan span-based extractive QA does. The paragraphs are drawn from seven domains including news,\nfiction, and historical text.""""""\n\n_WIC_DESCRIPTION = """"""\\\nThe Word-in-Context (WiC, Pilehvar and Camacho-Collados, 2019) dataset supports a word\nsense disambiguation task cast as binary classification over sentence pairs. Given two sentences and a\npolysemous (sense-ambiguous) word that appears in both sentences, the task is to determine whether\nthe word is used with the same sense in both sentences. Sentences are drawn from WordNet (Miller,\n1995), VerbNet (Schuler, 2005), and Wiktionary. We follow the original work and evaluate using\naccuracy.""""""\n\n_WSC_DESCRIPTION = """"""\\\nThe Winograd Schema Challenge (WSC, Levesque et al., 2012) is a reading comprehension\ntask in which a system must read a sentence with a pronoun and select the referent of that pronoun\nfrom a list of choices. Given the difficulty of this task and the headroom still left, we have included\nWSC in SuperGLUE and recast the dataset into its coreference form. The task is cast as a binary\nclassification problem, as opposed to N-multiple choice, in order to isolate the model\'s ability to\nunderstand the coreference links within a sentence as opposed to various other strategies that may\ncome into play in multiple choice conditions. With that in mind, we create a split with 65% negative\nmajority class in the validation set, reflecting the distribution of the hidden test set, and 52% negative\nclass in the training set. The training and validation examples are drawn from the original Winograd\nSchema dataset (Levesque et al., 2012), as well as those distributed by the affiliated organization\nCommonsense Reasoning. The test examples are derived from fiction books and have been shared\nwith us by the authors of the original dataset. Previously, a version of WSC recast as NLI as included\nin GLUE, known as WNLI. No substantial progress was made on WNLI, with many submissions\nopting to submit only majority class predictions. WNLI was made especially difficult due to an\nadversarial train/dev split: Premise sentences that appeared in the training set sometimes appeared\nin the development set with a different hypothesis and a flipped label. If a system memorized the\ntraining set without meaningfully generalizing, which was easy due to the small size of the training\nset, it could perform far below chance on the development set. We remove this adversarial design\nin the SuperGLUE version of WSC by ensuring that no sentences are shared between the training,\nvalidation, and test sets.\n\nHowever, the validation and test sets come from different domains, with the validation set consisting\nof ambiguous examples such that changing one non-noun phrase word will change the coreference\ndependencies in the sentence. The test set consists only of more straightforward examples, with a\nhigh number of noun phrases (and thus more choices for the model), but low to no ambiguity.""""""\n\n_AXB_DESCRIPTION = """"""\\\nAn expert-constructed,\ndiagnostic dataset that automatically tests models for a broad range of linguistic, commonsense, and\nworld knowledge. Each example in this broad-coverage diagnostic is a sentence pair labeled with\na three-way entailment relation (entailment, neutral, or contradiction) and tagged with labels that\nindicate the phenomena that characterize the relationship between the two sentences. Submissions\nto the GLUE leaderboard are required to include predictions from the submission\'s MultiNLI\nclassifier on the diagnostic dataset, and analyses of the results were shown alongside the main\nleaderboard. Since this broad-coverage diagnostic task has proved difficult for top models, we retain\nit in SuperGLUE. However, since MultiNLI is not part of SuperGLUE, we collapse contradiction\nand neutral into a single not_entailment label, and request that submissions include predictions\non the resulting set from the model used for the RTE task.\n""""""\n\n_AXG_DESCRIPTION = """"""\\\nWinogender is designed to measure gender\nbias in coreference resolution systems. We use the Diverse Natural Language Inference Collection\n(DNC; Poliak et al., 2018) version that casts Winogender as a textual entailment task. Each example\nconsists of a premise sentence with a male or female pronoun and a hypothesis giving a possible\nantecedent of the pronoun. Examples occur in minimal pairs, where the only difference between\nan example and its pair is the gender of the pronoun in the premise. Performance on Winogender\nis measured with both accuracy and the gender parity score: the percentage of minimal pairs for\nwhich the predictions are the same. We note that a system can trivially obtain a perfect gender parity\nscore by guessing the same class for all examples, so a high gender parity score is meaningless unless\naccompanied by high accuracy. As a diagnostic test of gender bias, we view the schemas as having high\npositive predictive value and low negative predictive value; that is, they may demonstrate the presence\nof gender bias in a system, but not prove its absence.\n""""""\n\n_BOOLQ_CITATION = """"""\\\n@inproceedings{clark2019boolq,\n  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},\n  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei, and Kwiatkowski, Tom and Collins, Michael, and Toutanova, Kristina},\n  booktitle={NAACL},\n  year={2019}\n}""""""\n\n_CB_CITATION = """"""\\\n@article{de marneff_simons_tonhauser_2019,\n  title={The CommitmentBank: Investigating projection in naturally occurring discourse},\n  journal={proceedings of Sinn und Bedeutung 23},\n  author={De Marneff, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},\n  year={2019}\n}""""""\n\n_COPA_CITATION = """"""\\\n@inproceedings{roemmele2011choice,\n  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},\n  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},\n  booktitle={2011 AAAI Spring Symposium Series},\n  year={2011}\n}""""""\n\n_RECORD_CITATION = """"""\\\n@article{zhang2018record,\n  title={Record: Bridging the gap between human and machine commonsense reading comprehension},\n  author={Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},\n  journal={arXiv preprint arXiv:1810.12885},\n  year={2018}\n}""""""\n\n_RTE_CITATION = """"""\\\n@inproceedings{dagan2005pascal,\n  title={The PASCAL recognising textual entailment challenge},\n  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},\n  booktitle={Machine Learning Challenges Workshop},\n  pages={177--190},\n  year={2005},\n  organization={Springer}\n}\n@inproceedings{bar2006second,\n  title={The second pascal recognising textual entailment challenge},\n  author={Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},\n  booktitle={Proceedings of the second PASCAL challenges workshop on recognising textual entailment},\n  volume={6},\n  number={1},\n  pages={6--4},\n  year={2006},\n  organization={Venice}\n}\n@inproceedings{giampiccolo2007third,\n  title={The third pascal recognizing textual entailment challenge},\n  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill},\n  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},\n  pages={1--9},\n  year={2007},\n  organization={Association for Computational Linguistics}\n}\n@inproceedings{bentivogli2009fifth,\n  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},\n  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},\n  booktitle={TAC},\n  year={2009}\n}""""""\n\n_MULTIRC_CITATION = """"""\\\n@inproceedings{MultiRC2018,\n    author = {Daniel Khashabi and Snigdha Chaturvedi and Michael Roth and Shyam Upadhyay and Dan Roth},\n    title = {Looking Beyond the Surface:A Challenge Set for Reading Comprehension over Multiple Sentences},\n    booktitle = {Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL)},\n    year = {2018}\n}""""""\n\n_WIC_CITATION = """"""\\\n@article{DBLP:journals/corr/abs-1808-09121,\n  author={Mohammad Taher Pilehvar and os{\\\'{e}} Camacho{-}Collados},\n  title={WiC: 10, 000 Example Pairs for Evaluating Context-Sensitive Representations},\n  journal={CoRR},\n  volume={abs/1808.09121},\n  year={2018},\n  url={http://arxiv.org/abs/1808.09121},\n  archivePrefix={arXiv},\n  eprint={1808.09121},\n  timestamp={Mon, 03 Sep 2018 13:36:40 +0200},\n  biburl={https://dblp.org/rec/bib/journals/corr/abs-1808-09121},\n  bibsource={dblp computer science bibliography, https://dblp.org}\n}""""""\n\n_WSC_CITATION = """"""\\\n@inproceedings{levesque2012winograd,\n  title={The winograd schema challenge},\n  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},\n  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},\n  year={2012}\n}""""""\n\n_AXG_CITATION = """"""\\\n@inproceedings{rudinger-EtAl:2018:N18,\n  author    = {Rudinger, Rachel  and  Naradowsky, Jason  and  Leonard, Brian  and  {Van Durme}, Benjamin},\n  title     = {Gender Bias in Coreference Resolution},\n  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  month     = {June},\n  year      = {2018},\n  address   = {New Orleans, Louisiana},\n  publisher = {Association for Computational Linguistics}\n}\n""""""\n\n\nclass SuperGlueConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for SuperGLUE.""""""\n\n    def __init__(self, features, data_url, citation, url, label_classes=(""False"", ""True""), **kwargs):\n        """"""BuilderConfig for SuperGLUE.\n\n    Args:\n      features: `list[string]`, list of the features that will appear in the\n        feature dict. Should not include ""label"".\n      data_url: `string`, url to download the zip file from.\n      citation: `string`, citation for the data set.\n      url: `string`, url for information about the data set.\n      label_classes: `list[string]`, the list of classes for the label if the\n        label is present as a string. Non-string labels will be cast to either\n        \'False\' or \'True\'.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        # Version history:\n        # 1.0.2: Fixed non-nondeterminism in ReCoRD.\n        # 1.0.1: Change from the pre-release trial version of SuperGLUE (v1.9) to\n        #        the full release (v2.0).\n        # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).\n        # 0.0.2: Initial version.\n        super(SuperGlueConfig, self).__init__(version=nlp.Version(""1.0.2""), **kwargs)\n        self.features = features\n        self.label_classes = label_classes\n        self.data_url = data_url\n        self.citation = citation\n        self.url = url\n\n\nclass SuperGlue(nlp.GeneratorBasedBuilder):\n    """"""The SuperGLUE benchmark.""""""\n\n    BUILDER_CONFIGS = [\n        SuperGlueConfig(\n            name=""boolq"",\n            description=_BOOLQ_DESCRIPTION,\n            features=[""question"", ""passage""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip"",\n            citation=_BOOLQ_CITATION,\n            url=""https://github.com/google-research-datasets/boolean-questions"",\n        ),\n        SuperGlueConfig(\n            name=""cb"",\n            description=_CB_DESCRIPTION,\n            features=[""premise"", ""hypothesis""],\n            label_classes=[""entailment"", ""contradiction"", ""neutral""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/CB.zip"",\n            citation=_CB_CITATION,\n            url=""https://github.com/mcdm/CommitmentBank"",\n        ),\n        SuperGlueConfig(\n            name=""copa"",\n            description=_COPA_DESCRIPTION,\n            label_classes=[""choice1"", ""choice2""],\n            # Note that question will only be the X in the statement ""What\'s\n            # the X for this?"".\n            features=[""premise"", ""choice1"", ""choice2"", ""question""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/COPA.zip"",\n            citation=_COPA_CITATION,\n            url=""http://people.ict.usc.edu/~gordon/copa.html"",\n        ),\n        SuperGlueConfig(\n            name=""multirc"",\n            description=_MULTIRC_DESCRIPTION,\n            features=[""paragraph"", ""question"", ""answer""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/MultiRC.zip"",\n            citation=_MULTIRC_CITATION,\n            url=""https://cogcomp.org/multirc/"",\n        ),\n        SuperGlueConfig(\n            name=""record"",\n            description=_RECORD_DESCRIPTION,\n            # Note that entities and answers will be a sequences of strings. Query\n            # will contain @placeholder as a substring, which represents the word\n            # to be substituted in.\n            features=[""passage"", ""query"", ""entities"", ""answers""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/ReCoRD.zip"",\n            citation=_RECORD_CITATION,\n            url=""https://sheng-z.github.io/ReCoRD-explorer/"",\n        ),\n        SuperGlueConfig(\n            name=""rte"",\n            description=_RTE_DESCRIPTION,\n            features=[""premise"", ""hypothesis""],\n            label_classes=[""entailment"", ""not_entailment""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/RTE.zip"",\n            citation=_RTE_CITATION,\n            url=""https://aclweb.org/aclwiki/Recognizing_Textual_Entailment"",\n        ),\n        SuperGlueConfig(\n            name=""wic"",\n            description=_WIC_DESCRIPTION,\n            # Note that start1, start2, end1, and end2 will be integers stored as\n            # nlp.Value(\'int32\').\n            features=[""word"", ""sentence1"", ""sentence2"", ""start1"", ""start2"", ""end1"", ""end2""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/WiC.zip"",\n            citation=_WIC_CITATION,\n            url=""https://pilehvar.github.io/wic/"",\n        ),\n        SuperGlueConfig(\n            name=""wsc"",\n            description=_WSC_DESCRIPTION,\n            # Note that span1_index and span2_index will be integers stored as\n            # nlp.Value(\'int32\').\n            features=[""text"", ""span1_index"", ""span2_index"", ""span1_text"", ""span2_text""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/WSC.zip"",\n            citation=_WSC_CITATION,\n            url=""https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html"",\n        ),\n        SuperGlueConfig(\n            name=""wsc.fixed"",\n            description=(\n                _WSC_DESCRIPTION + ""\\n\\nThis version fixes issues where the spans are not actually ""\n                ""substrings of the text.""\n            ),\n            # Note that span1_index and span2_index will be integers stored as\n            # nlp.Value(\'int32\').\n            features=[""text"", ""span1_index"", ""span2_index"", ""span1_text"", ""span2_text""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/WSC.zip"",\n            citation=_WSC_CITATION,\n            url=""https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html"",\n        ),\n        SuperGlueConfig(\n            name=""axb"",\n            description=_AXB_DESCRIPTION,\n            features=[""sentence1"", ""sentence2""],\n            label_classes=[""entailment"", ""not_entailment""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-b.zip"",\n            citation="""",  # The GLUE citation is sufficient.\n            url=""https://gluebenchmark.com/diagnostics"",\n        ),\n        SuperGlueConfig(\n            name=""axg"",\n            description=_AXG_DESCRIPTION,\n            features=[""premise"", ""hypothesis""],\n            label_classes=[""entailment"", ""not_entailment""],\n            data_url=""https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-g.zip"",\n            citation=_AXG_CITATION,\n            url=""https://github.com/rudinger/winogender-schemas"",\n        ),\n    ]\n\n    def _info(self):\n        features = {feature: nlp.Value(""string"") for feature in self.config.features}\n        if self.config.name.startswith(""wsc""):\n            features[""span1_index""] = nlp.Value(""int32"")\n            features[""span2_index""] = nlp.Value(""int32"")\n        if self.config.name == ""wic"":\n            features[""start1""] = nlp.Value(""int32"")\n            features[""start2""] = nlp.Value(""int32"")\n            features[""end1""] = nlp.Value(""int32"")\n            features[""end2""] = nlp.Value(""int32"")\n        if self.config.name == ""multirc"":\n            features[""idx""] = dict(\n                {""paragraph"": nlp.Value(""int32""), ""question"": nlp.Value(""int32""), ""answer"": nlp.Value(""int32""),}\n            )\n        elif self.config.name == ""record"":\n            features[""idx""] = dict({""passage"": nlp.Value(""int32""), ""query"": nlp.Value(""int32""),})\n        else:\n            features[""idx""] = nlp.Value(""int32"")\n\n        if self.config.name == ""record"":\n            # Entities are the set of possible choices for the placeholder.\n            features[""entities""] = nlp.features.Sequence(nlp.Value(""string""))\n            # Answers are the subset of entities that are correct.\n            features[""answers""] = nlp.features.Sequence(nlp.Value(""string""))\n        else:\n            features[""label""] = nlp.features.ClassLabel(names=self.config.label_classes)\n\n        return nlp.DatasetInfo(\n            description=_GLUE_DESCRIPTION + self.config.description,\n            features=nlp.Features(features),\n            homepage=self.config.url,\n            citation=self.config.citation + ""\\n"" + _SUPER_GLUE_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        dl_dir = dl_manager.download_and_extract(self.config.data_url) or """"\n        task_name = _get_task_name_from_data_url(self.config.data_url)\n        dl_dir = os.path.join(dl_dir, task_name)\n        if self.config.name in [""axb"", ""axg""]:\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    gen_kwargs={\n                        ""data_file"": os.path.join(dl_dir, ""{}.jsonl"".format(task_name)),\n                        ""split"": nlp.Split.TEST,\n                    },\n                ),\n            ]\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={""data_file"": os.path.join(dl_dir, ""train.jsonl""), ""split"": nlp.Split.TRAIN,},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={""data_file"": os.path.join(dl_dir, ""val.jsonl""), ""split"": nlp.Split.VALIDATION,},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={""data_file"": os.path.join(dl_dir, ""test.jsonl""), ""split"": nlp.Split.TEST,},\n            ),\n        ]\n\n    def _generate_examples(self, data_file, split):\n        with open(data_file) as f:\n            for line in f:\n                row = json.loads(line)\n\n                if self.config.name == ""multirc"":\n                    paragraph = row[""passage""]\n                    for question in paragraph[""questions""]:\n                        for answer in question[""answers""]:\n                            label = answer.get(""label"")\n                            key = ""%s_%s_%s"" % (row[""idx""], question[""idx""], answer[""idx""])\n                            yield key, {\n                                ""paragraph"": paragraph[""text""],\n                                ""question"": question[""question""],\n                                ""answer"": answer[""text""],\n                                ""label"": -1 if label is None else _cast_label(bool(label)),\n                                ""idx"": {""paragraph"": row[""idx""], ""question"": question[""idx""], ""answer"": answer[""idx""]},\n                            }\n                elif self.config.name == ""record"":\n                    passage = row[""passage""]\n                    for qa in row[""qas""]:\n                        yield qa[""idx""], {\n                            ""passage"": passage[""text""],\n                            ""query"": qa[""query""],\n                            ""entities"": _get_record_entities(passage),\n                            ""answers"": _get_record_answers(qa),\n                            ""idx"": {""passage"": row[""idx""], ""query"": qa[""idx""]},\n                        }\n                else:\n                    if self.config.name.startswith(""wsc""):\n                        row.update(row[""target""])\n                    example = {feature: row[feature] for feature in self.config.features}\n                    if self.config.name == ""wsc.fixed"":\n                        example = _fix_wst(example)\n                    example[""idx""] = row[""idx""]\n\n                    if ""label"" in row:\n                        if self.config.name == ""copa"":\n                            example[""label""] = ""choice2"" if row[""label""] else ""choice1""\n                        else:\n                            example[""label""] = _cast_label(row[""label""])\n                    else:\n                        assert split == nlp.Split.TEST, row\n                        example[""label""] = -1\n                    yield example[""idx""], example\n\n\ndef _fix_wst(ex):\n    """"""Fixes most cases where spans are not actually substrings of text.""""""\n\n    def _fix_span_text(k):\n        """"""Fixes a single span.""""""\n        text = ex[k + ""_text""]\n        index = ex[k + ""_index""]\n\n        if text in ex[""text""]:\n            return\n\n        if text in (""Kamenev and Zinoviev"", ""Kamenev, Zinoviev, and Stalin""):\n            # There is no way to correct these examples since the subjects have\n            # intervening text.\n            return\n\n        if ""theyscold"" in text:\n            ex[""text""].replace(""theyscold"", ""they scold"")\n            ex[""span2_index""] = 10\n        # Make sure case of the first words match.\n        first_word = ex[""text""].split()[index]\n        if first_word[0].islower():\n            text = text[0].lower() + text[1:]\n        else:\n            text = text[0].upper() + text[1:]\n        # Remove punctuation in span.\n        text = text.rstrip(""."")\n        # Replace incorrect whitespace character in span.\n        text = text.replace(""\\n"", "" "")\n        ex[k + ""_text""] = text\n        assert ex[k + ""_text""] in ex[""text""], ex\n\n    _fix_span_text(""span1"")\n    _fix_span_text(""span2"")\n    return ex\n\n\ndef _cast_label(label):\n    """"""Converts the label into the appropriate string version.""""""\n    if isinstance(label, six.string_types):\n        return label\n    elif isinstance(label, bool):\n        return ""True"" if label else ""False""\n    elif isinstance(label, six.integer_types):\n        assert label in (0, 1)\n        return str(label)\n    else:\n        raise ValueError(""Invalid label format."")\n\n\ndef _get_record_entities(passage):\n    """"""Returns the unique set of entities.""""""\n    text = passage[""text""]\n    entities = set()\n    for entity in passage[""entities""]:\n        entities.add(text[entity[""start""] : entity[""end""] + 1])\n    return sorted(entities)\n\n\ndef _get_record_answers(qa):\n    """"""Returns the unique set of answers.""""""\n    if ""answers"" not in qa:\n        return []\n    answers = set()\n    for answer in qa[""answers""]:\n        answers.add(answer[""text""])\n    return sorted(answers)\n\n\ndef _get_task_name_from_data_url(data_url):\n    return data_url.split(""/"")[-1].split(""."")[0]\n'"
datasets/ted_hrlr/ted_hrlr.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""TED talk high/low-resource paired language data set from Qi, et al. 2018.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nData sets derived from TED talk transcripts for comparing similar language pairs\nwhere one is high resource and the other is low resource.\n""""""\n\n_CITATION = """"""\\\n@inproceedings{Ye2018WordEmbeddings,\n  author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n  title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n  booktitle = {HLT-NAACL},\n  year    = {2018},\n  }\n""""""\n\n_DATA_URL = ""http://www.phontron.com/data/qi18naacl-dataset.tar.gz""\n\n_VALID_LANGUAGE_PAIRS = (\n    (""az"", ""en""),\n    (""az_tr"", ""en""),\n    (""be"", ""en""),\n    (""be_ru"", ""en""),\n    (""es"", ""pt""),\n    (""fr"", ""pt""),\n    (""gl"", ""en""),\n    (""gl_pt"", ""en""),\n    (""he"", ""pt""),\n    (""it"", ""pt""),\n    (""pt"", ""en""),\n    (""ru"", ""en""),\n    (""ru"", ""pt""),\n    (""tr"", ""en""),\n)\n\n\nclass TedHrlrConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for TED talk data comparing high/low resource languages.""""""\n\n    def __init__(self, language_pair=(None, None), **kwargs):\n        """"""BuilderConfig for TED talk data comparing high/low resource languages.\n\n    The first language in `language_pair` should either be a 2-letter coded\n    string or two such strings joined by an underscore (e.g., ""az"" or ""az_tr"").\n    In cases where it contains two languages, the train data set will contain an\n    (unlabelled) mix of the two languages and the validation and test sets\n    will contain only the first language. This dataset will refer to the\n    source language by the 5-letter string with the underscore. The second\n    language in `language_pair` must be a 2-letter coded string.\n\n    For example, to get pairings between Russian and English, specify\n    `(""ru"", ""en"")` as `language_pair`. To get a mix of Belarusian and Russian in\n    the training set and purely Belarusian in the validation and test sets,\n    specify `(""be_ru"", ""en"")`.\n\n    Args:\n      language_pair: pair of languages that will be used for translation. The\n        first will be used as source and second as target in supervised mode.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s_to_%s"" % (language_pair[0].replace(""_"", """"), language_pair[1])\n\n        description = (""Translation dataset from %s to %s in plain text."") % (language_pair[0], language_pair[1])\n        super(TedHrlrConfig, self).__init__(name=name, description=description, **kwargs)\n\n        # Validate language pair.\n        assert language_pair in _VALID_LANGUAGE_PAIRS, (\n            ""Config language pair (%s, "" ""%s) not supported""\n        ) % language_pair\n\n        self.language_pair = language_pair\n\n\nclass TedHrlr(nlp.GeneratorBasedBuilder):\n    """"""TED talk data set for comparing high and low resource languages.""""""\n\n    BUILDER_CONFIGS = [\n        TedHrlrConfig(  # pylint: disable=g-complex-comprehension\n            language_pair=pair, version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n        )\n        for pair in _VALID_LANGUAGE_PAIRS\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            homepage=""https://github.com/neulab/word-embeddings-for-nmt"",\n            supervised_keys=self.config.language_pair,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        dl_dir = dl_manager.download_and_extract(_DATA_URL)\n        source, target = self.config.language_pair\n\n        data_dir = os.path.join(dl_dir, ""datasets"", ""%s_to_%s"" % (source, target))\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={\n                    ""source_file"": os.path.join(data_dir, ""{}.train"".format(source.replace(""_"", ""-""))),\n                    ""target_file"": os.path.join(data_dir, ""{}.train"".format(target)),\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={\n                    ""source_file"": os.path.join(data_dir, ""{}.dev"".format(source.split(""_"")[0])),\n                    ""target_file"": os.path.join(data_dir, ""{}.dev"".format(target)),\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={\n                    ""source_file"": os.path.join(data_dir, ""{}.test"".format(source.split(""_"")[0])),\n                    ""target_file"": os.path.join(data_dir, ""{}.test"".format(target)),\n                },\n            ),\n        ]\n\n    def _generate_examples(self, source_file, target_file):\n        """"""This function returns the examples in the raw (text) form.""""""\n        with open(source_file) as f:\n            source_sentences = f.read().split(""\\n"")\n        with open(target_file) as f:\n            target_sentences = f.read().split(""\\n"")\n\n        assert len(target_sentences) == len(source_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(source_sentences),\n            len(target_sentences),\n            source_file,\n            target_file,\n        )\n\n        source, target = self.config.language_pair\n        for idx, (l1, l2) in enumerate(zip(source_sentences, target_sentences)):\n            result = {""translation"": {source: l1, target: l2}}\n            # Make sure that both translations are non-empty.\n            if all(result.values()):\n                yield idx, result\n'"
datasets/ted_multi/ted_multi.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""TED talk multilingual data set.""""""\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport six\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nMassively multilingual (60 language) data set derived from TED Talk transcripts.\nEach record consists of parallel arrays of language and text. Missing and\nincomplete translations will be filtered out.\n""""""\n\n_CITATION = """"""\\\n@InProceedings{qi-EtAl:2018:N18-2,\n  author    = {Qi, Ye  and  Sachan, Devendra  and  Felix, Matthieu  and  Padmanabhan, Sarguna  and  Neubig, Graham},\n  title     = {When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?},\n  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},\n  month     = {June},\n  year      = {2018},\n  address   = {New Orleans, Louisiana},\n  publisher = {Association for Computational Linguistics},\n  pages     = {529--535},\n  abstract  = {The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases -- providing gains of up to 20 BLEU points in the most favorable setting.},\n  url       = {http://www.aclweb.org/anthology/N18-2084}\n}\n""""""\n\n_DATA_URL = ""http://phontron.com/data/ted_talks.tar.gz""\n\n_LANGUAGES = (\n    ""en"",\n    ""es"",\n    ""pt-br"",\n    ""fr"",\n    ""ru"",\n    ""he"",\n    ""ar"",\n    ""ko"",\n    ""zh-cn"",\n    ""it"",\n    ""ja"",\n    ""zh-tw"",\n    ""nl"",\n    ""ro"",\n    ""tr"",\n    ""de"",\n    ""vi"",\n    ""pl"",\n    ""pt"",\n    ""bg"",\n    ""el"",\n    ""fa"",\n    ""sr"",\n    ""hu"",\n    ""hr"",\n    ""uk"",\n    ""cs"",\n    ""id"",\n    ""th"",\n    ""sv"",\n    ""sk"",\n    ""sq"",\n    ""lt"",\n    ""da"",\n    ""calv"",\n    ""my"",\n    ""sl"",\n    ""mk"",\n    ""fr-ca"",\n    ""fi"",\n    ""hy"",\n    ""hi"",\n    ""nb"",\n    ""ka"",\n    ""mn"",\n    ""et"",\n    ""ku"",\n    ""gl"",\n    ""mr"",\n    ""zh"",\n    ""ur"",\n    ""eo"",\n    ""ms"",\n    ""az"",\n    ""ta"",\n    ""bn"",\n    ""kk"",\n    ""be"",\n    ""eu"",\n    ""bs"",\n)\n\n\nclass TedMultiTranslate(nlp.GeneratorBasedBuilder):\n    """"""TED talk multilingual data set.""""""\n\n    BUILDER_CONFIGS = [\n        nlp.BuilderConfig(\n            name=""plain_text"",\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            description=""Plain text import of multilingual TED talk translations"",\n        )\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""translations"": nlp.features.TranslationVariableLanguages(languages=_LANGUAGES),\n                    ""talk_name"": nlp.Value(""string""),\n                }\n            ),\n            homepage=""https://github.com/neulab/word-embeddings-for-nmt"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        dl_dir = dl_manager.download_and_extract(_DATA_URL)\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""data_file"": os.path.join(dl_dir, ""all_talks_train.tsv"")}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""data_file"": os.path.join(dl_dir, ""all_talks_dev.tsv"")}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST, gen_kwargs={""data_file"": os.path.join(dl_dir, ""all_talks_test.tsv"")}\n            ),\n        ]\n\n    def _generate_examples(self, data_file):\n        """"""This function returns the examples in the raw (text) form.""""""\n        with open(data_file) as f:\n            reader = csv.DictReader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n            for idx, row in enumerate(reader):\n                # Everything in the row except for \'talk_name\' will be a translation.\n                # Missing/incomplete translations will contain the string ""__NULL__"" or\n                # ""_ _ NULL _ _"".\n                yield idx, {\n                    ""translations"": {\n                        lang: text\n                        for lang, text in six.iteritems(row)\n                        if lang != ""talk_name"" and _is_translation_complete(text)\n                    },\n                    ""talk_name"": row[""talk_name""],\n                }\n\n\ndef _is_translation_complete(text):\n    return text and ""__NULL__"" not in text and ""_ _ NULL _ _"" not in text\n'"
datasets/tiny_shakespeare/tiny_shakespeare.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tiny Shakespeare dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\\\n@misc{\n  author={Karpathy, Andrej},\n  title={char-rnn},\n  year={2015},\n  howpublished={\\\\url{https://github.com/karpathy/char-rnn}}\n}""""""\n\n_DESCRIPTION = """"""\\\n40,000 lines of Shakespeare from a variety of Shakespeare\'s plays. \\\nFeatured in Andrej Karpathy\'s blog post \'The Unreasonable Effectiveness of \\\nRecurrent Neural Networks\': \\\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n\nTo use for e.g. character modelling:\n\n```\nd = nlp.load_dataset(name=\'tiny_shakespeare\')[\'train\']\nd = d.map(lambda x: nlp.Value(\'strings\').unicode_split(x[\'text\'], \'UTF-8\'))\n# train split includes vocabulary for other splits\nvocabulary = sorted(set(next(iter(d)).numpy()))\nd = d.map(lambda x: {\'cur_char\': x[:-1], \'next_char\': x[1:]})\nd = d.unbatch()\nseq_len = 100\nbatch_size = 2\nd = d.batch(seq_len)\nd = d.batch(batch_size)\n```\n""""""\n\n\nclass TinyShakespeare(nlp.GeneratorBasedBuilder):\n    """"""Tiny Shakespeare dataset builder.""""""\n\n    VERSION = nlp.Version(""1.0.0"")\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""text"": nlp.Value(""string"")}),\n            supervised_keys=None,\n            homepage=""https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        download_path = dl_manager.download_and_extract(\n            ""https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt""\n        )\n        if os.path.isdir(download_path):\n            # During testing the download manager mock gives us a directory\n            txt_path = os.path.join(download_path, ""input.txt"")\n        else:\n            txt_path = download_path\n        with open(txt_path, ""r"") as f:\n            text = f.read()\n\n        # 90/5/5 split\n        i = int(len(text) * 0.9)\n        train_text, text = text[:i], text[i:]\n        i = int(len(text) * 0.5)\n        validation_text, text = text[:i], text[i:]\n        test_text = text\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""split_key"": ""train"", ""split_text"": train_text},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""split_key"": ""validation"", ""split_text"": validation_text},\n            ),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""split_key"": ""test"", ""split_text"": test_text},),\n        ]\n\n    def _generate_examples(self, split_key, split_text):\n        """"""Yields examples.""""""\n        data_key = split_key  # Should uniquely identify the thing yielded\n        feature_dict = {""text"": split_text}\n        yield data_key, feature_dict\n'"
datasets/trivia_qa/trivia_qa.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""TriviaQA: A Reading Comprehension Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport glob\nimport json\nimport logging\nimport os\n\nimport six\n\nimport nlp\n\n\n_CITATION = """"""\n@article{2017arXivtriviaqa,\n       author = {{Joshi}, Mandar and {Choi}, Eunsol and {Weld},\n                 Daniel and {Zettlemoyer}, Luke},\n        title = ""{triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}"",\n      journal = {arXiv e-prints},\n         year = 2017,\n          eid = {arXiv:1705.03551},\n        pages = {arXiv:1705.03551},\narchivePrefix = {arXiv},\n       eprint = {1705.03551},\n}\n""""""\n_DOWNLOAD_URL_TMPL = ""http://nlp.cs.washington.edu/triviaqa/data/triviaqa-{}.tar.gz""\n_TRAIN_FILE_FORMAT = ""*-train.json""\n_VALIDATION_FILE_FORMAT = ""*-dev.json""\n_TEST_FILE_FORMAT = ""*test-without-answers.json""\n_WEB_EVIDENCE_DIR = ""evidence/web""\n_WIKI_EVIDENCE_DIR = ""evidence/wikipedia""\n\n_DESCRIPTION = """"""\\\nTriviaqQA is a reading comprehension dataset containing over 650K\nquestion-answer-evidence triples. TriviaqQA includes 95K question-answer\npairs authored by trivia enthusiasts and independently gathered evidence\ndocuments, six per question on average, that provide high quality distant\nsupervision for answering the questions.\n""""""\n\n_RC_DESCRIPTION = """"""\\\nQuestion-answer pairs where all documents for a given question contain the\nanswer string(s).\n""""""\n\n_UNFILTERED_DESCRIPTION = """"""\\\n110k question-answer pairs for open domain QA where not all documents for a\ngiven question contain the answer string(s). This makes the unfiltered dataset\nmore appropriate for IR-style QA.\n""""""\n\n_CONTEXT_ADDENDUM = ""Includes context from Wikipedia and search results.""\n\n\ndef _web_evidence_dir(tmp_dir):\n    return sorted(glob.glob(os.path.join(tmp_dir, _WEB_EVIDENCE_DIR)))\n\n\ndef _wiki_evidence_dir(tmp_dir):\n    return sorted(glob.glob(os.path.join(tmp_dir, _WIKI_EVIDENCE_DIR)))\n\n\nclass TriviaQaConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for TriviaQA.""""""\n\n    def __init__(self, unfiltered=False, exclude_context=False, **kwargs):\n        """"""BuilderConfig for TriviaQA.\n\n    Args:\n      unfiltered: bool, whether to use the unfiltered version of the dataset,\n        intended for open-domain QA.\n      exclude_context: bool, whether to exclude Wikipedia and search context for\n        reduced size.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""unfiltered"" if unfiltered else ""rc""\n        if exclude_context:\n            name += "".nocontext""\n        description = _UNFILTERED_DESCRIPTION if unfiltered else _RC_DESCRIPTION\n        if not exclude_context:\n            description += _CONTEXT_ADDENDUM\n        super(TriviaQaConfig, self).__init__(\n            name=name, description=description, version=nlp.Version(""1.1.0""), **kwargs\n        )\n        self.unfiltered = unfiltered\n        self.exclude_context = exclude_context\n\n\nclass TriviaQa(nlp.GeneratorBasedBuilder):\n    """"""TriviaQA is a reading comprehension dataset.\n\n  It containss over 650K question-answer-evidence triples.\n  """"""\n\n    BUILDER_CONFIGS = [\n        TriviaQaConfig(unfiltered=False, exclude_context=False),  # rc\n        TriviaQaConfig(unfiltered=False, exclude_context=True),  # rc.nocontext\n        TriviaQaConfig(unfiltered=True, exclude_context=False),  # unfiltered\n        TriviaQaConfig(unfiltered=True, exclude_context=True),\n        # unfilered.nocontext\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""question"": nlp.Value(""string""),\n                    ""question_id"": nlp.Value(""string""),\n                    ""question_source"": nlp.Value(""string""),\n                    ""entity_pages"": nlp.features.Sequence(\n                        {\n                            ""doc_source"": nlp.Value(""string""),\n                            ""filename"": nlp.Value(""string""),\n                            ""title"": nlp.Value(""string""),\n                            ""wiki_context"": nlp.Value(""string""),\n                        }\n                    ),\n                    ""search_results"": nlp.features.Sequence(\n                        {\n                            ""description"": nlp.Value(""string""),\n                            ""filename"": nlp.Value(""string""),\n                            ""rank"": nlp.Value(""int32""),\n                            ""title"": nlp.Value(""string""),\n                            ""url"": nlp.Value(""string""),\n                            ""search_context"": nlp.Value(""string""),\n                        }\n                    ),\n                    ""answer"": dict(\n                        {\n                            ""aliases"": nlp.features.Sequence(nlp.Value(""string"")),\n                            ""normalized_aliases"": nlp.features.Sequence(nlp.Value(""string"")),\n                            ""matched_wiki_entity_name"": nlp.Value(""string""),\n                            ""normalized_matched_wiki_entity_name"": nlp.Value(""string""),\n                            ""normalized_value"": nlp.Value(""string""),\n                            ""type"": nlp.Value(""string""),\n                            ""value"": nlp.Value(""string""),\n                        }\n                    ),\n                }\n            ),\n            supervised_keys=None,\n            homepage=""http://nlp.cs.washington.edu/triviaqa/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        cfg = self.config\n        download_urls = dict()\n        if not (cfg.unfiltered and cfg.exclude_context):\n            download_urls[""rc""] = _DOWNLOAD_URL_TMPL.format(""rc"")\n        if cfg.unfiltered:\n            download_urls[""unfiltered""] = _DOWNLOAD_URL_TMPL.format(""unfiltered"")\n        file_paths = dl_manager.download_and_extract(download_urls)\n\n        qa_dir = (\n            os.path.join(file_paths[""unfiltered""], ""triviaqa-unfiltered"")\n            if cfg.unfiltered\n            else os.path.join(file_paths[""rc""], ""qa"")\n        )\n        train_files = sorted(glob.glob(os.path.join(qa_dir, _TRAIN_FILE_FORMAT)))\n        valid_files = sorted(glob.glob(os.path.join(qa_dir, _VALIDATION_FILE_FORMAT)))\n        test_files = sorted(glob.glob(os.path.join(qa_dir, _TEST_FILE_FORMAT)))\n\n        if cfg.exclude_context:\n            web_evidence_dir = None\n            wiki_evidence_dir = None\n        else:\n            web_evidence_dir = os.path.join(file_paths[""rc""], _WEB_EVIDENCE_DIR)\n            wiki_evidence_dir = os.path.join(file_paths[""rc""], _WIKI_EVIDENCE_DIR)\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={""files"": train_files, ""web_dir"": web_evidence_dir, ""wiki_dir"": wiki_evidence_dir},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={""files"": valid_files, ""web_dir"": web_evidence_dir, ""wiki_dir"": wiki_evidence_dir},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={""files"": test_files, ""web_dir"": web_evidence_dir, ""wiki_dir"": wiki_evidence_dir},\n            ),\n        ]\n\n    def _generate_examples(self, files, web_dir, wiki_dir):\n        """"""This function returns the examples.""""""\n\n        def parse_example(article):\n            """"""Return a single example from an article JSON record.""""""\n\n            def _strip(collection):\n                return [item.strip() for item in collection]\n\n            if ""Answer"" in article:\n                answer = article[""Answer""]\n                answer_dict = {\n                    ""aliases"": _strip(answer[""Aliases""]),\n                    ""normalized_aliases"": _strip(answer[""NormalizedAliases""]),\n                    ""matched_wiki_entity_name"": answer.get(""MatchedWikiEntryName"", """").strip(),\n                    ""normalized_matched_wiki_entity_name"": answer.get(""NormalizedMatchedWikiEntryName"", """").strip(),\n                    ""normalized_value"": answer[""NormalizedValue""].strip(),\n                    ""type"": answer[""Type""].strip(),\n                    ""value"": answer[""Value""].strip(),\n                }\n            else:\n                answer_dict = {\n                    ""aliases"": [],\n                    ""normalized_aliases"": [],\n                    ""matched_wiki_entity_name"": ""<unk>"",\n                    ""normalized_matched_wiki_entity_name"": ""<unk>"",\n                    ""normalized_value"": ""<unk>"",\n                    ""type"": """",\n                    ""value"": ""<unk>"",\n                }\n\n            if self.config.exclude_context:\n                article[""SearchResults""] = []\n                article[""EntityPages""] = []\n\n            def _add_context(collection, context_field, file_dir):\n                """"""Adds context from file, or skips if file does not exist.""""""\n                new_items = []\n                for item in collection:\n                    if ""Filename"" not in item:\n                        logging.info(""Missing context \'Filename\', skipping."")\n                        continue\n\n                    new_item = item.copy()\n                    fname = item[""Filename""]\n                    try:\n                        with open(os.path.join(file_dir, fname)) as f:\n                            new_item[context_field] = f.read()\n                    except (IOError, nlp.Value(""errors"").NotFoundError):\n                        logging.info(""File does not exist, skipping: %s"", fname)\n                        continue\n                    new_items.append(new_item)\n                return new_items\n\n            def _strip_if_str(v):\n                return v.strip() if isinstance(v, six.string_types) else v\n\n            def _transpose_and_strip_dicts(dicts, field_names):\n                return {\n                    nlp.naming.camelcase_to_snakecase(k): [_strip_if_str(d[k]) for d in dicts] for k in field_names\n                }\n\n            search_results = _transpose_and_strip_dicts(\n                _add_context(article.get(""SearchResults"", []), ""SearchContext"", web_dir),\n                [""Description"", ""Filename"", ""Rank"", ""Title"", ""Url"", ""SearchContext""],\n            )\n\n            entity_pages = _transpose_and_strip_dicts(\n                _add_context(article.get(""EntityPages"", []), ""WikiContext"", wiki_dir),\n                [""DocSource"", ""Filename"", ""Title"", ""WikiContext""],\n            )\n\n            question = article[""Question""].strip()\n            question_id = article[""QuestionId""]\n            question_source = article[""QuestionSource""].strip()\n\n            return {\n                ""entity_pages"": entity_pages,\n                ""search_results"": search_results,\n                ""question"": question,\n                ""question_id"": question_id,\n                ""question_source"": question_source,\n                ""answer"": answer_dict,\n            }\n\n        for filepath in files:\n            logging.info(""generating examples from = %s"", filepath)\n            fname = os.path.basename(filepath)\n\n            with open(filepath) as f:\n                current_record = """"\n                for line in f:\n                    if line == ""        {\\n"":\n                        current_record = line\n                    elif line.startswith(""        }""):  # Handles final record as well.\n                        article = json.loads(current_record + ""}"")\n                        current_record = """"\n                        example = parse_example(article)\n                        yield ""%s_%s"" % (fname, example[""question_id""]), example\n                    else:\n                        current_record += line\n'"
datasets/tydiqa/tydiqa.py,0,"b'""""""TODO(tydiqa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\nimport textwrap\n\nimport nlp\n\n\n# TODO(tydiqa): BibTeX citation\n_CITATION = """"""\\\n@article{tydiqa,\ntitle   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},\nauthor  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}\nyear    = {2020},\njournal = {Transactions of the Association for Computational Linguistics}\n}\n""""""\n\n# TODO(tydiqa):\n_DESCRIPTION = """"""\\\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. \nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language \nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages \nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic \ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but \ndon\xe2\x80\x99t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).\n""""""\n_URL = ""https://storage.googleapis.com/tydiqa/""\n_PRIMARY_TASK_TRAIN = ""v1.0/tydiqa-v1.0-train.jsonl.gz""\n_PRIMARY_TASK_DEV = ""v1.0/tydiqa-v1.0-dev.jsonl.gz""\n_SECONDARY_TASK_TRAIN = ""v1.1/tydiqa-goldp-v1.1-train.json""\n_SECONDARY_TASK_DEV = ""v1.1/tydiqa-goldp-v1.1-dev.json""\n\n\nclass TydiqaConfig(nlp.BuilderConfig):\n\n    """""" BuilderConfig for Tydiqa""""""\n\n    def __init__(self, **kwargs):\n        """"""\n\n        Args:\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(TydiqaConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass Tydiqa(nlp.GeneratorBasedBuilder):\n    """"""TODO(tydiqa): Short description of my dataset.""""""\n\n    # TODO(tydiqa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        TydiqaConfig(\n            name=""primary_task"",\n            description=textwrap.dedent(\n                """"""\\\n          Passage selection task (SelectP): Given a list of the passages in the article, return either (a) the index of \n          the passage that answers the question or (b) NULL if no such passage exists.\n          Minimal answer span task (MinSpan): Given the full text of an article, return one of (a) the start and end \n          byte indices of the minimal span that completely answers the question; (b) YES or NO if the question requires \n          a yes/no answer and we can draw a conclusion from the passage; (c) NULL if it is not possible to produce a \n          minimal answer for this question.""""""\n            ),\n        ),\n        TydiqaConfig(\n            name=""secondary_task"",\n            description=textwrap.dedent(\n                """"""\\Gold passage task (GoldP): Given a passage that is guaranteed to contain the \n          answer, predict the single contiguous span of characters that answers the question. This is more similar to \n          existing reading comprehension datasets (as opposed to the information-seeking task outlined above). \n          This task is constructed with two goals in mind: (1) more directly comparing with prior work and (2) providing \n          a simplified way for researchers to use TyDi QA by providing compatibility with existing code for SQuAD 1.1, \n          XQuAD, and MLQA. Toward these goals, the gold passage task differs from the primary task in several ways:\n          only the gold answer passage is provided rather than the entire Wikipedia article;\n          unanswerable questions have been discarded, similar to MLQA and XQuAD;\n          we evaluate with the SQuAD 1.1 metrics like XQuAD; and\n         Thai and Japanese are removed since the lack of whitespace breaks some tools.\n          """"""\n            ),\n        ),\n    ]\n\n    def _info(self):\n        # TODO(tydiqa): Specifies the nlp.DatasetInfo object\n        if self.config.name == ""primary_task"":\n            return nlp.DatasetInfo(\n                # This is the description that will appear on the datasets page.\n                description=_DESCRIPTION,\n                # nlp.features.FeatureConnectors\n                features=nlp.Features(\n                    {\n                        ""passage_answer_candidates"": nlp.features.Sequence(\n                            {""plaintext_start_byte"": nlp.Value(""int32""), ""plaintext_end_byte"": nlp.Value(""int32"")}\n                        ),\n                        ""question_text"": nlp.Value(""string""),\n                        ""document_title"": nlp.Value(""string""),\n                        ""language"": nlp.Value(""string""),\n                        ""annotations"": nlp.features.Sequence(\n                            {\n                                #\'annotation_id\': nlp.Value(\'variant\'),\n                                ""passage_answer_candidate_index"": nlp.Value(""int32""),\n                                ""minimal_answers_start_byte"": nlp.Value(""int32""),\n                                ""minimal_answers_end_byte"": nlp.Value(""int32""),\n                                ""yes_no_answer"": nlp.Value(""string""),\n                            }\n                        ),\n                        ""document_plaintext"": nlp.Value(""string""),\n                        # \'example_id\': nlp.Value(\'variant\'),\n                        ""document_url"": nlp.Value(""string"")\n                        # These are the features of your dataset like images, labels ...\n                    }\n                ),\n                # If there\'s a common (input, target) tuple from the features,\n                # specify them here. They\'ll be used if as_supervised=True in\n                # builder.as_dataset.\n                supervised_keys=None,\n                # Homepage of the dataset for documentation\n                homepage=""https://github.com/google-research-datasets/tydiqa"",\n                citation=_CITATION,\n            )\n        elif self.config.name == ""secondary_task"":\n            return nlp.DatasetInfo(\n                description=_DESCRIPTION,\n                features=nlp.Features(\n                    {\n                        ""id"": nlp.Value(""string""),\n                        ""title"": nlp.Value(""string""),\n                        ""context"": nlp.Value(""string""),\n                        ""question"": nlp.Value(""string""),\n                        ""answers"": nlp.features.Sequence(\n                            {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                        ),\n                    }\n                ),\n                # No default supervised_keys (as we have to pass both question\n                # and context as input).\n                supervised_keys=None,\n                homepage=""https://github.com/google-research-datasets/tydiqa"",\n                citation=_CITATION,\n            )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(tydiqa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        primary_urls_to_download = {\n            ""train"": os.path.join(_URL, _PRIMARY_TASK_TRAIN),\n            ""dev"": os.path.join(_URL, _PRIMARY_TASK_DEV),\n        }\n        secondary_urls_to_download = {\n            ""train"": os.path.join(_URL, _SECONDARY_TASK_TRAIN),\n            ""dev"": os.path.join(_URL, _SECONDARY_TASK_DEV),\n        }\n        primary_downloaded = dl_manager.download_and_extract(primary_urls_to_download)\n        secondary_downloaded = dl_manager.download_and_extract(secondary_urls_to_download)\n        if self.config.name == ""primary_task"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": primary_downloaded[""train""]},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": primary_downloaded[""dev""]},\n                ),\n            ]\n        elif self.config.name == ""secondary_task"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": secondary_downloaded[""train""]},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": secondary_downloaded[""dev""]},\n                ),\n            ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(tydiqa): Yields (key, example) tuples from the dataset\n        if self.config.name == ""primary_task"":\n            with open(filepath) as f:\n                for id_, row in enumerate(f):\n                    data = json.loads(row)\n                    passages = data[""passage_answer_candidates""]\n                    end_byte = [passage[""plaintext_end_byte""] for passage in passages]\n                    start_byte = [passage[""plaintext_start_byte""] for passage in passages]\n                    title = data[""document_title""]\n                    lang = data[""language""]\n                    question = data[""question_text""]\n                    annotations = data[""annotations""]\n                    annot_ids = [annotation[""annotation_id""] for annotation in annotations]\n                    yes_no_answers = [annotation[""yes_no_answer""] for annotation in annotations]\n                    min_answers_end_byte = [\n                        annotation[""minimal_answer""][""plaintext_end_byte""] for annotation in annotations\n                    ]\n                    min_answers_start_byte = [\n                        annotation[""minimal_answer""][""plaintext_start_byte""] for annotation in annotations\n                    ]\n                    passage_cand_answers = [\n                        annotation[""passage_answer""][""candidate_index""] for annotation in annotations\n                    ]\n                    doc = data[""document_plaintext""]\n                    example_id = data[""example_id""]\n                    url = data[""document_url""]\n                    yield id_, {\n                        ""passage_answer_candidates"": {\n                            ""plaintext_start_byte"": start_byte,\n                            ""plaintext_end_byte"": end_byte,\n                        },\n                        ""question_text"": question,\n                        ""document_title"": title,\n                        ""language"": lang,\n                        ""annotations"": {\n                            #\'annotation_id\': annot_ids,\n                            ""passage_answer_candidate_index"": passage_cand_answers,\n                            ""minimal_answers_start_byte"": min_answers_start_byte,\n                            ""minimal_answers_end_byte"": min_answers_end_byte,\n                            ""yes_no_answer"": yes_no_answers,\n                        },\n                        ""document_plaintext"": doc,\n                        #\'example_id\': example_id,\n                        ""document_url"": url,\n                    }\n        elif self.config.name == ""secondary_task"":\n            with open(filepath) as f:\n                data = json.load(f)\n                for article in data[""data""]:\n                    title = article.get(""title"", """").strip()\n                    for paragraph in article[""paragraphs""]:\n                        context = paragraph[""context""].strip()\n                        for qa in paragraph[""qas""]:\n                            question = qa[""question""].strip()\n                            id_ = qa[""id""]\n\n                            answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                            answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                            # Features currently used are ""context"", ""question"", and ""answers"".\n                            # Others are extracted here for the ease of future expansions.\n                            yield id_, {\n                                ""title"": title,\n                                ""context"": context,\n                                ""question"": question,\n                                ""id"": id_,\n                                ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                            }\n'"
datasets/ubuntu_dialogs_corpus/ubuntu_dialogs_corpus.py,0,"b'""""""TODO(ubuntu_dialogs_corpus): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n# TODO(ubuntu_dialogs_corpus): BibTeX citation\n_CITATION = """"""\\\n@article{DBLP:journals/corr/LowePSP15,\n  author    = {Ryan Lowe and\n               Nissan Pow and\n               Iulian Serban and\n               Joelle Pineau},\n  title     = {The Ubuntu Dialogue Corpus: {A} Large Dataset for Research in Unstructured\n               Multi-Turn Dialogue Systems},\n  journal   = {CoRR},\n  volume    = {abs/1506.08909},\n  year      = {2015},\n  url       = {http://arxiv.org/abs/1506.08909},\n  archivePrefix = {arXiv},\n  eprint    = {1506.08909},\n  timestamp = {Mon, 13 Aug 2018 16:48:23 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/LowePSP15.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n""""""\n\n# TODO(ubuntu_dialogs_corpus):\n_DESCRIPTION = """"""\\\nUbuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter.\n""""""\n\n\nclass UbuntuDialogsCorpusConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for UbuntuDialogsCorpus.""""""\n\n    def __init__(self, features, **kwargs):\n        """"""BuilderConfig for UbuntuDialogsCorpus.\n\n    Args:\n\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n\n        super(UbuntuDialogsCorpusConfig, self).__init__(version=nlp.Version(""2.0.0""), **kwargs)\n        self.features = features\n\n\nclass UbuntuDialogsCorpus(nlp.GeneratorBasedBuilder):\n    """"""TODO(ubuntu_dialogs_corpus): Short description of my dataset.""""""\n\n    # TODO(ubuntu_dialogs_corpus): Set up version.\n    VERSION = nlp.Version(""2.0.0"")\n    BUILDER_CONFIGS = [\n        UbuntuDialogsCorpusConfig(\n            name=""train"", features=[""Context"", ""Utterance"", ""Label""], description=""training features""\n        ),\n        UbuntuDialogsCorpusConfig(\n            name=""dev_test"",\n            features=[""Context"", ""Ground Truth Utterance""] + [""Distractor_"" + str(i) for i in range(9)],\n            description=""test and dev features"",\n        ),\n    ]\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Please download the Ubuntu Dialog Corpus from https://github.com/rkadlec/ubuntu-ranking-dataset-creator. Run ./generate.sh -t -s -l to download the\n   data. Others arguments are left to their default values here. Please save train.csv, test.csv and valid.csv in the same path""""""\n\n    def _info(self):\n        # TODO(ubuntu_dialogs_corpus): Specifies the nlp.DatasetInfo object\n        features = {feature: nlp.Value(""string"") for feature in self.config.features}\n        if self.config.name == ""train"":\n            features[""Label""] = nlp.Value(""int32"")\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                # These are the features of your dataset like images, labels ...\n                features\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/rkadlec/ubuntu-ranking-dataset-creator"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(ubuntu_dialogs_corpus): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        manual_dir = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))\n\n        if self.config.name == ""train"":\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(manual_dir, ""train.csv"")},\n                ),\n            ]\n        else:\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(manual_dir, ""test.csv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(manual_dir, ""valid.csv"")},\n                ),\n            ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(ubuntu_dialogs_corpus): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = csv.DictReader(f)\n            for id_, row in enumerate(data):\n                yield id_, row\n'"
datasets/wiki40b/wiki40b.py,1,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wiki40B: A clean Wikipedia dataset for 40+ languages.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n""""""\n\n_DESCRIPTION = """"""\nClean-up text for 40+ Wikipedia languages editions of pages\ncorrespond to entities. The datasets have train/dev/test splits per language.\nThe dataset is cleaned up by page filtering to remove disambiguation pages,\nredirect pages, deleted pages, and non-entity pages. Each example contains the\nwikidata id of the entity, and the full Wikipedia article after page processing\nthat removes non-content sections and structured objects.\n""""""\n\n_LICENSE = """"""\nThis work is licensed under the Creative Commons Attribution-ShareAlike\n3.0 Unported License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-sa/3.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\n_URL = ""https://research.google/pubs/pub49029/""\n\n_DATA_DIRECTORY = ""gs://tfds-data/downloads/wiki40b/tfrecord_prod""\n\nWIKIPEDIA_LANGUAGES = [\n    ""en"",\n    ""ar"",\n    ""zh-cn"",\n    ""zh-tw"",\n    ""nl"",\n    ""fr"",\n    ""de"",\n    ""it"",\n    ""ja"",\n    ""ko"",\n    ""pl"",\n    ""pt"",\n    ""ru"",\n    ""es"",\n    ""th"",\n    ""tr"",\n    ""bg"",\n    ""ca"",\n    ""cs"",\n    ""da"",\n    ""el"",\n    ""et"",\n    ""fa"",\n    ""fi"",\n    ""he"",\n    ""hi"",\n    ""hr"",\n    ""hu"",\n    ""id"",\n    ""lt"",\n    ""lv"",\n    ""ms"",\n    ""no"",\n    ""ro"",\n    ""sk"",\n    ""sl"",\n    ""sr"",\n    ""sv"",\n    ""tl"",\n    ""uk"",\n    ""vi"",\n]\n\n\nclass Wiki40bConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Wiki40B.""""""\n\n    def __init__(self, language=None, **kwargs):\n        """"""BuilderConfig for Wiki40B.\n\n    Args:\n      language: string, the language code for the Wiki40B dataset to use.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(Wiki40bConfig, self).__init__(\n            name=str(language), description=""Wiki40B dataset for {0}."".format(language), **kwargs\n        )\n        self.language = language\n\n\n_VERSION = nlp.Version(""1.1.0"")\n\n\nclass Wiki40b(nlp.BeamBasedBuilder):\n    """"""Wiki40B: A Clean Wikipedia Dataset for Mutlilingual Language Modeling.""""""\n\n    BUILDER_CONFIGS = [\n        Wiki40bConfig(version=_VERSION, language=lang,)  # pylint:disable=g-complex-comprehension\n        for lang in WIKIPEDIA_LANGUAGES\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {""wikidata_id"": nlp.Value(""string""), ""text"": nlp.Value(""string""), ""version_id"": nlp.Value(""string"")}\n            ),\n            supervised_keys=None,\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n\n        lang = self.config.language\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                gen_kwargs={""filepaths"": os.path.join(_DATA_DIRECTORY, ""train"", ""{}_examples-*"".format(lang))},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={""filepaths"": os.path.join(_DATA_DIRECTORY, ""dev"", ""{}_examples-*"".format(lang))},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                gen_kwargs={""filepaths"": os.path.join(_DATA_DIRECTORY, ""test"", ""{}_examples-*"".format(lang))},\n            ),\n        ]\n\n    def _build_pcollection(self, pipeline, filepaths):\n        """"""Build PCollection of examples.""""""\n        import apache_beam as beam\n        import tensorflow as tf\n\n        logging.info(""generating examples from = %s"", filepaths)\n\n        def _extract_content(example):\n            """"""Extracts content from a TFExample.""""""\n            wikidata_id = example.features.feature[""wikidata_id""].bytes_list.value[0].decode(""utf-8"")\n            text = example.features.feature[""text""].bytes_list.value[0].decode(""utf-8"")\n            version_id = example.features.feature[""version_id""].bytes_list.value[0].decode(""utf-8"")\n\n            # wikidata_id could be duplicated with different texts.\n            yield wikidata_id + text, {\n                ""wikidata_id"": wikidata_id,\n                ""text"": text,\n                ""version_id"": version_id,\n            }\n\n        return (\n            pipeline\n            | beam.io.ReadFromTFRecord(filepaths, coder=beam.coders.ProtoCoder(tf.train.Example))\n            | beam.FlatMap(_extract_content)\n        )\n'"
datasets/wiki_qa/wiki_qa.py,0,"b'""""""TODO(wiki_qa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n# TODO(wiki_qa): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{YangYihMeek:EMNLP2015:WikiQA,\n       author = {{Yi}, Yang and {Wen-tau},  Yih and {Christopher} Meek},\n        title = ""{WikiQA: A Challenge Dataset for Open-Domain Question Answering}"",\n      journal = {Association for Computational Linguistics},\n         year = 2015,\n          doi = {10.18653/v1/D15-1237},\n        pages = {2013\xe2\x80\x932018},\n}\n""""""\n\n# TODO(wiki_qa):\n_DESCRIPTION = """"""\\\nWiki Question Answering corpus from Microsoft\n""""""\n\n_DATA_URL = ""https://download.microsoft.com/download/E/5/f/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip""  #\'https://www.microsoft.com/en-us/download/confirmation.aspx?id=52419\'\n\n\nclass WikiQa(nlp.GeneratorBasedBuilder):\n    """"""TODO(wiki_qa): Short description of my dataset.""""""\n\n    # TODO(wiki_qa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(wiki_qa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""question_id"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""document_title"": nlp.Value(""string""),\n                    ""answer"": nlp.Value(""string""),\n                    ""label"": nlp.features.ClassLabel(num_classes=2),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://www.microsoft.com/en-us/download/details.aspx?id=52419"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(wiki_qa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_DATA_URL)\n        dl_dir = os.path.join(dl_dir, ""WikiQACorpus"")\n        # dl_dir = os.path.join(dl_dir, \'\')\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": os.path.join(dl_dir, ""WikiQA-test.tsv"")}),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": os.path.join(dl_dir, ""WikiQA-dev.tsv"")}\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""WikiQA-train.tsv"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(wiki_qa): Yields (key, example) tuples from the dataset\n\n        with open(filepath) as f:\n            print(""=="" * 100, filepath)\n            reader = csv.DictReader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n            for idx, row in enumerate(reader):\n                yield idx, {\n                    ""question_id"": row[""QuestionID""],\n                    ""question"": row[""Question""],\n                    ""document_title"": row[""DocumentTitle""],\n                    ""answer"": row[""Sentence""],\n                    ""label"": row[""Label""],\n                }\n'"
datasets/wiki_split/wiki_split.py,0,"b'""""""TODO(wiki_split): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\n\nimport nlp\n\n\n# TODO(wiki_split): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{BothaEtAl2018,\n  title = {{Learning To Split and Rephrase From Wikipedia Edit History}},\n  author = {Botha, Jan A and Faruqui, Manaal and Alex, John and Baldridge, Jason and Das, Dipanjan},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  pages = {to appear},\n  note = {arXiv preprint arXiv:1808.09468},\n  year = {2018}\n}\n""""""\n\n# TODO(wiki_split):\n_DESCRIPTION = """"""\\\nOne million English sentences, each split into two sentences that together preserve the original meaning, extracted from Wikipedia \nGoogle\'s WikiSplit dataset was constructed automatically from the publicly available Wikipedia revision history. Although \nthe dataset contains some inherent noise, it can serve as valuable training data for models that split or merge sentences.\n""""""\n_URL = ""https://github.com/google-research-datasets/wiki-split/raw/master""\n_TRAIN_FILE = ""train.tsv.zip""\n_TEST_FILE = ""test.tsv""\n_DEV_FILE = ""validation.tsv""\n\n\nclass WikiSplit(nlp.GeneratorBasedBuilder):\n    """"""TODO(wiki_split): Short description of my dataset.""""""\n\n    # TODO(wiki_split): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(wiki_split): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""complex_sentence"": nlp.Value(""string""),\n                    ""simple_sentence_1"": nlp.Value(""string""),\n                    ""simple_sentence_2"": nlp.Value(""string""),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://dataset-homepage/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(wiki_split): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {\n            ""train"": os.path.join(_URL, _TRAIN_FILE),\n            ""test"": os.path.join(_URL, _TEST_FILE),\n            ""dev"": os.path.join(_URL, _DEV_FILE),\n        }\n        dl_dir = dl_manager.download_and_extract(urls_to_download)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir[""train""], ""train.tsv"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""test""]},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": dl_dir[""dev""]},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(wiki_split): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            data = csv.reader(f, delimiter=""\\t"")\n            # data = csv.reader(f, delimiter=\'\\t\')\n\n            for id_, row in enumerate(data):\n                yield id_, {\n                    ""complex_sentence"": row[0],\n                    ""simple_sentence_1"": row[1].split(""<::::>"")[0],\n                    ""simple_sentence_2"": row[1].split(""<::::>"")[1],\n                }\n'"
datasets/wikihow/wikihow.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WikiHow Datasets.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\nimport re\n\nimport nlp\n\n\n_CITATION = """"""\n@misc{koupaee2018wikihow,\n    title={WikiHow: A Large Scale Text Summarization Dataset},\n    author={Mahnaz Koupaee and William Yang Wang},\n    year={2018},\n    eprint={1810.09305},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n""""""\n\n_DESCRIPTION = """"""\nWikiHow is a new large-scale dataset using the online WikiHow\n(http://www.wikihow.com/) knowledge base.\n\nThere are two features:\n  - text: wikihow answers texts.\n  - headline: bold lines as summary.\n\nThere are two separate versions:\n  - all: consisting of the concatenation of all paragraphs as the articles and\n         the bold lines as the reference summaries.\n  - sep: consisting of each paragraph and its summary.\n\nDownload ""wikihowAll.csv"" and ""wikihowSep.csv"" from\nhttps://github.com/mahnazkoupaee/WikiHow-Dataset and place them in manual folder\nhttps://www.tensorflow.org/datasets/api_docs/python/tfds/download/DownloadConfig.\nTrain/validation/test splits are provided by the authors.\nPreprocessing is applied to remove short articles\n(abstract length < 0.75 article length) and clean up extra commas.\n""""""\n\n_DOCUMENT = ""text""\n_SUMMARY = ""headline""\n\n_URLS = {\n    ""train"": ""https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_train.txt"",\n    ""validation"": ""https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_val.txt"",\n    ""test"": ""https://raw.githubusercontent.com/mahnazkoupaee/WikiHow-Dataset/master/all_test.txt"",\n}\n\n\nclass WikihowConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Wikihow.""""""\n\n    def __init__(self, filename=None, **kwargs):\n        """"""BuilderConfig for Wikihow.\n\n    Args:\n      filename: filename of different configs for the dataset.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        # Version 1.1.0 remove empty document and summary strings.\n        # Version 1.2.0 add train validation test split, add cleaning & filtering.\n        super(WikihowConfig, self).__init__(version=nlp.Version(""1.2.0""), **kwargs)\n        self.filename = filename\n\n\nclass Wikihow(nlp.GeneratorBasedBuilder):\n    """"""WikiHow: A Large Scale Text Summarization Dataset.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  You need to manually download two wikihow files. An overview of which files to download can be seen at https://github.com/mahnazkoupaee/WikiHow-Dataset.\n  You need to download the following two files manually:\n    1) https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358 and save the file under <path/to/folder>/wikihowAll.csv\n    2) https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag and save the file under <path/to/folder>/wikihowSep.csv\n\n  The <path/to/folder> can e.g. be ""~/manual_wikihow_data"".\n\n  Wikihow can then be loaded using the following command `nlp.load(""wikihow"", data_file=""<path/to/folder>"")`.\n  """"""\n\n    BUILDER_CONFIGS = [\n        WikihowConfig(\n            name=""all"",\n            filename=""wikihowAll.csv"",\n            description=""Use the concatenation of all paragraphs as the articles""\n            "" and the bold lines as the reference summaries"",\n        ),\n        WikihowConfig(name=""sep"", filename=""wikihowSep.csv"", description=""use each paragraph and its summary.""),\n    ]\n\n    def _info(self):\n        feature_names = [_DOCUMENT, _SUMMARY, ""title""]\n        if self.config.name == ""sep"":\n            feature_names.extend([""overview"", ""sectionLabel""])\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({k: nlp.Value(""string"") for k in feature_names}),\n            supervised_keys=None,\n            homepage=""https://github.com/mahnazkoupaee/WikiHow-Dataset"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_path = dl_manager.download_and_extract(_URLS)\n        titles = {k: set() for k in dl_path}\n        for k, path in dl_path.items():\n            with open(path) as f:\n                for line in f:\n                    titles[k].add(line.strip())\n\n        path_to_manual_file = os.path.join(\n            os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), self.config.filename\n        )\n\n        if not os.path.exists(path_to_manual_file):\n            raise FileNotFoundError(\n                ""{} does not exist. Make sure you insert a manual dir via `nlp.load(\'wikihow\', data_dir=...)` that includes a file name {}. Manual download instructions: {})"".format(\n                    path_to_manual_file, self.config.filename, self.MANUAL_DOWNLOAD_INSTRUCTIONS\n                )\n            )\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""path"": path_to_manual_file, ""title_set"": titles[""train""],},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                gen_kwargs={""path"": path_to_manual_file, ""title_set"": titles[""validation""],},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST, gen_kwargs={""path"": path_to_manual_file, ""title_set"": titles[""test""],},\n            ),\n        ]\n\n    def _generate_examples(self, path=None, title_set=None):\n        """"""Yields examples.""""""\n        with open(path) as f:\n            reader = csv.reader(f)\n            headers = next(reader)\n            if self.config.name == ""all"" and headers != [""headline"", ""title"", ""text""]:\n                raise ValueError(""Mismatched header in WikiAll.txt"")\n            if self.config.name == ""sep"" and headers != [""overview"", ""headline"", ""text"", ""sectionLabel"", ""title""]:\n                raise ValueError(""Mismatched header in WikiSep.txt"")\n            key2id = {key: i for i, key in enumerate(headers)}\n            for i, line in enumerate(reader):\n                # skip empty line or insufficient line.\n                if len(line) == len(key2id):\n                    summary = line[key2id[_SUMMARY]].strip()\n                    document = line[key2id[_DOCUMENT]].strip()\n                    summary, document = _filter_and_clean(summary, document)\n                    if summary and document:\n                        if line[key2id[""title""]].strip().replace("" "", """") in title_set:\n                            d = {k: line[v].strip() for k, v in key2id.items() if k not in [_SUMMARY, _DOCUMENT]}\n                            d[_DOCUMENT] = document\n                            d[_SUMMARY] = summary\n                            yield i, d\n\n\n# This functions follow data processing acoording to original paper at\n# https://github.com/mahnazkoupaee/WikiHow-Dataset/blob/master/process.py\ndef _filter_and_clean(abstract, article):\n    """"""Remove short article and clean up commas in abstract and article.""""""\n    # a threshold is used to remove short articles with long summaries\n    # as well as articles with no summary\n    if len(abstract) < (0.75 * len(article)):\n        # remove extra commas in abstracts\n        abstract = abstract.replace("".,"", ""."")\n        # remove extra commas in articles\n        article = re.sub(r""[.]+[\\n]+[,]"", "".\\n"", article)\n        return abstract, article\n    else:\n        return """", """"\n'"
datasets/wikipedia/wikipedia.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Wikipedia dataset containing cleaned articles of all languages.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport codecs\nimport json\nimport logging\nimport re\nimport xml.etree.cElementTree as etree\n\nimport six\n\nimport nlp\n\n\nif six.PY3:\n    import bz2  # pylint:disable=g-import-not-at-top\nelse:\n    # py2\'s built-in bz2 package does not support reading from file objects.\n    import bz2file as bz2  # pylint:disable=g-import-not-at-top\n\n_CITATION = """"""\\\n@ONLINE {wikidump,\n    author = ""Wikimedia Foundation"",\n    title  = ""Wikimedia Downloads"",\n    url    = ""https://dumps.wikimedia.org""\n}\n""""""\n\n_DESCRIPTION = """"""\\\nWikipedia dataset containing cleaned articles of all languages.\nThe datasets are built from the Wikipedia dump\n(https://dumps.wikimedia.org/) with one split per language. Each example\ncontains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).\n""""""\n\n_LICENSE = (\n    ""This work is licensed under the Creative Commons Attribution-ShareAlike ""\n    ""3.0 Unported License. To view a copy of this license, visit ""\n    ""http://creativecommons.org/licenses/by-sa/3.0/ or send a letter to ""\n    ""Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.""\n)\n\n# Source: https://en.wikipedia.org/wiki/List_of_Wikipedias (accessed 3/1/2019)\n# Removed because no articles: hz.\nWIKIPEDIA_LANGUAGES = [\n    ""aa"",\n    ""ab"",\n    ""ace"",\n    ""ady"",\n    ""af"",\n    ""ak"",\n    ""als"",\n    ""am"",\n    ""an"",\n    ""ang"",\n    ""ar"",\n    ""arc"",\n    ""arz"",\n    ""as"",\n    ""ast"",\n    ""atj"",\n    ""av"",\n    ""ay"",\n    ""az"",\n    ""azb"",\n    ""ba"",\n    ""bar"",\n    ""bat-smg"",\n    ""bcl"",\n    ""be"",\n    ""be-x-old"",\n    ""bg"",\n    ""bh"",\n    ""bi"",\n    ""bjn"",\n    ""bm"",\n    ""bn"",\n    ""bo"",\n    ""bpy"",\n    ""br"",\n    ""bs"",\n    ""bug"",\n    ""bxr"",\n    ""ca"",\n    ""cbk-zam"",\n    ""cdo"",\n    ""ce"",\n    ""ceb"",\n    ""ch"",\n    ""cho"",\n    ""chr"",\n    ""chy"",\n    ""ckb"",\n    ""co"",\n    ""cr"",\n    ""crh"",\n    ""cs"",\n    ""csb"",\n    ""cu"",\n    ""cv"",\n    ""cy"",\n    ""da"",\n    ""de"",\n    ""din"",\n    ""diq"",\n    ""dsb"",\n    ""dty"",\n    ""dv"",\n    ""dz"",\n    ""ee"",\n    ""el"",\n    ""eml"",\n    ""en"",\n    ""eo"",\n    ""es"",\n    ""et"",\n    ""eu"",\n    ""ext"",\n    ""fa"",\n    ""ff"",\n    ""fi"",\n    ""fiu-vro"",\n    ""fj"",\n    ""fo"",\n    ""fr"",\n    ""frp"",\n    ""frr"",\n    ""fur"",\n    ""fy"",\n    ""ga"",\n    ""gag"",\n    ""gan"",\n    ""gd"",\n    ""gl"",\n    ""glk"",\n    ""gn"",\n    ""gom"",\n    ""gor"",\n    ""got"",\n    ""gu"",\n    ""gv"",\n    ""ha"",\n    ""hak"",\n    ""haw"",\n    ""he"",\n    ""hi"",\n    ""hif"",\n    ""ho"",\n    ""hr"",\n    ""hsb"",\n    ""ht"",\n    ""hu"",\n    ""hy"",\n    ""ia"",\n    ""id"",\n    ""ie"",\n    ""ig"",\n    ""ii"",\n    ""ik"",\n    ""ilo"",\n    ""inh"",\n    ""io"",\n    ""is"",\n    ""it"",\n    ""iu"",\n    ""ja"",\n    ""jam"",\n    ""jbo"",\n    ""jv"",\n    ""ka"",\n    ""kaa"",\n    ""kab"",\n    ""kbd"",\n    ""kbp"",\n    ""kg"",\n    ""ki"",\n    ""kj"",\n    ""kk"",\n    ""kl"",\n    ""km"",\n    ""kn"",\n    ""ko"",\n    ""koi"",\n    ""krc"",\n    ""ks"",\n    ""ksh"",\n    ""ku"",\n    ""kv"",\n    ""kw"",\n    ""ky"",\n    ""la"",\n    ""lad"",\n    ""lb"",\n    ""lbe"",\n    ""lez"",\n    ""lfn"",\n    ""lg"",\n    ""li"",\n    ""lij"",\n    ""lmo"",\n    ""ln"",\n    ""lo"",\n    ""lrc"",\n    ""lt"",\n    ""ltg"",\n    ""lv"",\n    ""mai"",\n    ""map-bms"",\n    ""mdf"",\n    ""mg"",\n    ""mh"",\n    ""mhr"",\n    ""mi"",\n    ""min"",\n    ""mk"",\n    ""ml"",\n    ""mn"",\n    ""mr"",\n    ""mrj"",\n    ""ms"",\n    ""mt"",\n    ""mus"",\n    ""mwl"",\n    ""my"",\n    ""myv"",\n    ""mzn"",\n    ""na"",\n    ""nah"",\n    ""nap"",\n    ""nds"",\n    ""nds-nl"",\n    ""ne"",\n    ""new"",\n    ""ng"",\n    ""nl"",\n    ""nn"",\n    ""no"",\n    ""nov"",\n    ""nrm"",\n    ""nso"",\n    ""nv"",\n    ""ny"",\n    ""oc"",\n    ""olo"",\n    ""om"",\n    ""or"",\n    ""os"",\n    ""pa"",\n    ""pag"",\n    ""pam"",\n    ""pap"",\n    ""pcd"",\n    ""pdc"",\n    ""pfl"",\n    ""pi"",\n    ""pih"",\n    ""pl"",\n    ""pms"",\n    ""pnb"",\n    ""pnt"",\n    ""ps"",\n    ""pt"",\n    ""qu"",\n    ""rm"",\n    ""rmy"",\n    ""rn"",\n    ""ro"",\n    ""roa-rup"",\n    ""roa-tara"",\n    ""ru"",\n    ""rue"",\n    ""rw"",\n    ""sa"",\n    ""sah"",\n    ""sat"",\n    ""sc"",\n    ""scn"",\n    ""sco"",\n    ""sd"",\n    ""se"",\n    ""sg"",\n    ""sh"",\n    ""si"",\n    ""simple"",\n    ""sk"",\n    ""sl"",\n    ""sm"",\n    ""sn"",\n    ""so"",\n    ""sq"",\n    ""sr"",\n    ""srn"",\n    ""ss"",\n    ""st"",\n    ""stq"",\n    ""su"",\n    ""sv"",\n    ""sw"",\n    ""szl"",\n    ""ta"",\n    ""tcy"",\n    ""te"",\n    ""tet"",\n    ""tg"",\n    ""th"",\n    ""ti"",\n    ""tk"",\n    ""tl"",\n    ""tn"",\n    ""to"",\n    ""tpi"",\n    ""tr"",\n    ""ts"",\n    ""tt"",\n    ""tum"",\n    ""tw"",\n    ""ty"",\n    ""tyv"",\n    ""udm"",\n    ""ug"",\n    ""uk"",\n    ""ur"",\n    ""uz"",\n    ""ve"",\n    ""vec"",\n    ""vep"",\n    ""vi"",\n    ""vls"",\n    ""vo"",\n    ""wa"",\n    ""war"",\n    ""wo"",\n    ""wuu"",\n    ""xal"",\n    ""xh"",\n    ""xmf"",\n    ""yi"",\n    ""yo"",\n    ""za"",\n    ""zea"",\n    ""zh"",\n    ""zh-classical"",\n    ""zh-min-nan"",\n    ""zh-yue"",\n    ""zu"",\n]\n\n_BASE_URL_TMPL = ""https://dumps.wikimedia.org/{lang}wiki/{date}/""\n_INFO_FILE = ""dumpstatus.json""\n\n\nclass WikipediaConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Wikipedia.""""""\n\n    def __init__(self, language=None, date=None, **kwargs):\n        """"""BuilderConfig for Wikipedia.\n\n    Args:\n      language: string, the language code for the Wikipedia dump to use.\n      date: string, date of the Wikipedia dump in YYYYMMDD format. A list of\n        available dates can be found at https://dumps.wikimedia.org/enwiki/.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(WikipediaConfig, self).__init__(\n            name=""{0}.{1}"".format(date, language),\n            description=""Wikipedia dataset for {0}, parsed from {1} dump."".format(language, date),\n            **kwargs,\n        )\n        self.date = date\n        self.language = language\n\n\n_VERSION = nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)"")\n\n\nclass Wikipedia(nlp.BeamBasedBuilder):\n    """"""Wikipedia dataset.""""""\n\n    # Use mirror (your.org) to avoid download caps.\n\n    BUILDER_CONFIGS = [\n        WikipediaConfig(version=_VERSION, language=lang, date=""20200501"",)  # pylint:disable=g-complex-comprehension\n        for lang in WIKIPEDIA_LANGUAGES\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""title"": nlp.Value(""string""), ""text"": nlp.Value(""string"")}),\n            # No default supervised_keys.\n            supervised_keys=None,\n            homepage=""https://dumps.wikimedia.org"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager, pipeline):\n        def _base_url(lang):\n            return _BASE_URL_TMPL.format(lang=lang.replace(""-"", ""_""), date=self.config.date)\n\n        lang = self.config.language\n\n        info_url = _base_url(lang) + _INFO_FILE\n        # Use dictionary since testing mock always returns the same result.\n        downloaded_files = dl_manager.download_and_extract({""info"": info_url})\n\n        xml_urls = []\n        total_bytes = 0\n        with open(downloaded_files[""info""]) as f:\n            dump_info = json.load(f)\n        multistream_dump_info = dump_info[""jobs""][""articlesmultistreamdump""]\n        assert multistream_dump_info[""status""] == ""done"", (\n            ""Specified dump (%s) multistream status is not \'done\': %s""\n            % (_base_url(lang), multistream_dump_info[""status""])\n        )\n\n        for fname, info in multistream_dump_info[""files""].items():\n            if "".xml"" not in fname:\n                continue\n            total_bytes += info[""size""]\n            xml_urls.append(_base_url(lang) + fname)\n\n            # Use dictionary since testing mock always returns the same result.\n        downloaded_files = dl_manager.download({""xml"": xml_urls})\n        if not pipeline.is_local():\n            downloaded_files = dl_manager.ship_files_with_pipeline(downloaded_files, pipeline)\n\n        return [\n            nlp.SplitGenerator(  # pylint:disable=g-complex-comprehension\n                name=nlp.Split.TRAIN, gen_kwargs={""filepaths"": downloaded_files[""xml""], ""language"": lang}\n            )\n        ]\n\n    def _build_pcollection(self, pipeline, filepaths, language):\n        """"""Build PCollection of examples in the raw (text) form.""""""\n        import apache_beam as beam\n        import mwparserfromhell\n\n        def _extract_content(filepath):\n            """"""Extracts article content from a single WikiMedia XML file.""""""\n            logging.info(""generating examples from = %s"", filepath)\n            with beam.io.filesystems.FileSystems.open(filepath) as f:\n                f = bz2.BZ2File(filename=f)\n                if six.PY3:\n                    # Workaround due to:\n                    # https://github.com/tensorflow/tensorflow/issues/33563\n                    utf_f = codecs.getreader(""utf-8"")(f)\n                else:\n                    utf_f = f\n\n                # To clear root, to free-up more memory than just `elem.clear()`.\n                context = etree.iterparse(utf_f, events=(""end"",))\n                context = iter(context)\n                unused_event, root = next(context)\n                for unused_event, elem in context:\n                    if not elem.tag.endswith(""page""):\n                        continue\n                    namespace = elem.tag[:-4]\n                    title = elem.find(""./{0}title"".format(namespace)).text\n                    ns = elem.find(""./{0}ns"".format(namespace)).text\n                    id_ = elem.find(""./{0}id"".format(namespace)).text\n\n                    # Filter pages that are not in the ""main"" namespace.\n                    if ns != ""0"":\n                        root.clear()\n                        continue\n\n                    raw_content = elem.find(""./{0}revision/{0}text"".format(namespace)).text\n                    root.clear()\n\n                    # Filter redirects.\n                    if raw_content is None or raw_content.lower().startswith(""#redirect""):\n                        beam.metrics.Metrics.counter(language, ""filtered-redirects"").inc()\n                        continue\n\n                    beam.metrics.Metrics.counter(language, ""extracted-examples"").inc()\n                    yield (id_, title, raw_content)\n\n        def _clean_content(inputs):\n            """"""Cleans raw wikicode to extract text.""""""\n            id_, title, raw_content = inputs\n            try:\n                text = _parse_and_clean_wikicode(raw_content, parser=mwparserfromhell)\n            except (mwparserfromhell.parser.ParserError) as e:\n                beam.metrics.Metrics.counter(language, ""parser-error"").inc()\n                logging.error(""mwparserfromhell ParseError: %s"", e)\n                return\n\n            if not text:\n                beam.metrics.Metrics.counter(language, ""empty-clean-examples"").inc()\n                return\n\n            beam.metrics.Metrics.counter(language, ""cleaned-examples"").inc()\n\n            yield id_, {""title"": title, ""text"": text}\n\n        return (\n            pipeline\n            | ""Initialize"" >> beam.Create(filepaths)\n            | ""Extract content"" >> beam.FlatMap(_extract_content)\n            | ""Distribute"" >> beam.transforms.Reshuffle()\n            | ""Clean content"" >> beam.FlatMap(_clean_content)\n        )\n\n\ndef _parse_and_clean_wikicode(raw_content, parser):\n    """"""Strips formatting and unwanted sections from raw page content.""""""\n    wikicode = parser.parse(raw_content)\n\n    # Filters for references, tables, and file/image links.\n    re_rm_wikilink = re.compile(""^(?:File|Image|Media):"", flags=re.IGNORECASE | re.UNICODE)\n\n    def rm_wikilink(obj):\n        return bool(re_rm_wikilink.match(six.text_type(obj.title)))\n\n    def rm_tag(obj):\n        return six.text_type(obj.tag) in {""ref"", ""table""}\n\n    def rm_template(obj):\n        return obj.name.lower() in {""reflist"", ""notelist"", ""notelist-ua"", ""notelist-lr"", ""notelist-ur"", ""notelist-lg""}\n\n    def try_remove_obj(obj, section):\n        try:\n            section.remove(obj)\n        except ValueError:\n            # For unknown reasons, objects are sometimes not found.\n            pass\n\n    section_text = []\n    # Filter individual sections to clean.\n    for section in wikicode.get_sections(flat=True, include_lead=True, include_headings=True):\n        for obj in section.ifilter_wikilinks(matches=rm_wikilink, recursive=True):\n            try_remove_obj(obj, section)\n        for obj in section.ifilter_templates(matches=rm_template, recursive=True):\n            try_remove_obj(obj, section)\n        for obj in section.ifilter_tags(matches=rm_tag, recursive=True):\n            try_remove_obj(obj, section)\n\n        section_text.append(section.strip_code().strip())\n    return ""\\n\\n"".join(section_text)\n'"
datasets/wikitext/wikitext.py,0,"b'""""""TODO(wikitext): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n# TODO(wikitext): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{wikitext,\n    author={Stephen, Merity and Caiming ,Xiong and James, Bradbury and Richard Socher}\n    year=2016\n}\n""""""\n\n# TODO(wikitext):\n_DESCRIPTION = """"""\\\n The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified \n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\n""""""\n_URL = ""https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/""\n_DATA_URL = ""https://s3.amazonaws.com/research.metamind.io/wikitext""\n\n\nclass WikitextConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for GLUE.""""""\n\n    def __init__(self, data_url, **kwargs):\n        """"""BuilderConfig for Wikitext\n\n    Args:\n      data_url: `string`, url to the dataset (word or raw level)\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(WikitextConfig, self).__init__(version=nlp.Version(""1.0.0"",), **kwargs)\n        self.data_url = data_url\n\n\nclass Wikitext(nlp.GeneratorBasedBuilder):\n    """"""TODO(wikitext_103): Short description of my dataset.""""""\n\n    # TODO(wikitext_103): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [\n        WikitextConfig(\n            name=""wikitext-103-raw-v1"",\n            data_url=_DATA_URL + ""/"" + ""wikitext-103-raw-v1.zip"",\n            description=""word level dataset. No processing is needed other than replacing newlines with <eos> tokens."",\n        ),\n        WikitextConfig(\n            name=""wikitext-2-raw-v1"",\n            data_url=_DATA_URL + ""/"" + ""wikitext-2-raw-v1.zip"",\n            description=""word level dataset. No processing is needed other than replacing newlines with <eos> tokens."",\n        ),\n        WikitextConfig(\n            name=""wikitext-103-v1"",\n            data_url=_DATA_URL + ""/"" + ""wikitext-103-v1.zip"",\n            description=""raw level dataset. The raw tokens before the addition of <unk> tokens. ""\n            ""They should only be used for character level work or for creating newly derived datasets."",\n        ),\n        WikitextConfig(\n            name=""wikitext-2-v1"",\n            data_url=_DATA_URL + ""/"" + ""wikitext-2-v1.zip"",\n            description=""raw level dataset. The raw tokens before the addition of <unk> tokens. ""\n            ""They should only be used for character level work or for creating newly derived datasets."",\n        ),\n    ]\n\n    def _info(self):\n        # TODO(wikitext): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""text"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=_URL,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(wikitext): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        if self.config.name == ""wikitext-103-v1"":\n            data_file = dl_manager.download_and_extract(self.config.data_url)\n            data_dir = os.path.join(data_file, ""wikitext-103"")\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.test.tokens""), ""split"": ""test""},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.train.tokens""), ""split"": ""train""},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.valid.tokens""), ""split"": ""valid""},\n                ),\n            ]\n        else:\n            if self.config.name == ""wikitext-103-raw-v1"":\n                data_file = dl_manager.download_and_extract(self.config.data_url)\n                data_dir = os.path.join(data_file, ""wikitext-103-raw"")\n                return [\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TEST,\n                        gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.test.raw""), ""split"": ""test""},\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TRAIN,\n                        gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.train.raw""), ""split"": ""train""},\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.VALIDATION,\n                        gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.valid.raw""), ""split"": ""valid""},\n                    ),\n                ]\n            else:\n                if self.config.name == ""wikitext-2-raw-v1"":\n                    data_file = dl_manager.download_and_extract(self.config.data_url)\n                    data_dir = os.path.join(data_file, ""wikitext-2-raw"")\n                    return [\n                        nlp.SplitGenerator(\n                            name=nlp.Split.TEST,\n                            gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.test.raw""), ""split"": ""test""},\n                        ),\n                        nlp.SplitGenerator(\n                            name=nlp.Split.TRAIN,\n                            gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.train.raw""), ""split"": ""train""},\n                        ),\n                        nlp.SplitGenerator(\n                            name=nlp.Split.VALIDATION,\n                            gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.valid.raw""), ""split"": ""valid""},\n                        ),\n                    ]\n                else:\n                    if self.config.name == ""wikitext-2-v1"":\n                        data_file = dl_manager.download_and_extract(self.config.data_url)\n                        data_dir = os.path.join(data_file, ""wikitext-2"")\n                        return [\n                            nlp.SplitGenerator(\n                                name=nlp.Split.TEST,\n                                gen_kwargs={""data_file"": os.path.join(data_dir, ""wiki.test.tokens""), ""split"": ""test""},\n                            ),\n                            nlp.SplitGenerator(\n                                name=nlp.Split.TRAIN,\n                                gen_kwargs={\n                                    ""data_file"": os.path.join(data_dir, ""wiki.train.tokens""),\n                                    ""split"": ""train"",\n                                },\n                            ),\n                            nlp.SplitGenerator(\n                                name=nlp.Split.VALIDATION,\n                                gen_kwargs={\n                                    ""data_file"": os.path.join(data_dir, ""wiki.valid.tokens""),\n                                    ""split"": ""valid"",\n                                },\n                            ),\n                        ]\n\n    def _generate_examples(self, data_file, split):\n\n        """"""Yields examples.""""""\n        # TODO(wikitext): Yields (key, example) tuples from the dataset\n        # print(data_file)\n        # print(\'==\'*100)\n        with open(data_file) as f:\n            for idx, row in enumerate(f):\n                if row.strip():\n                    yield idx, {""text"": row}\n                else:\n                    yield idx, {""text"": """"}\n'"
datasets/winogrande/winogrande.py,0,"b'""""""TODO(winogrande): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(winogrande): BibTeX citation\n_CITATION = """"""\\\n@InProceedings{ai2:winogrande,\ntitle = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},\nauthors={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi\n},\nyear={2019}\n}\n""""""\n\n# TODO(winogrande):\n_DESCRIPTION = """"""\\\nWinoGrande is a new collection of 44k problems, inspired by Winograd Schema Challenge (Levesque, Davis, and Morgenstern\n 2011), but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a \nfill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires \ncommonsense reasoning. \n""""""\n\n_URL = ""https://storage.googleapis.com/ai2-mosaic/public/winogrande/winogrande_1.1.zip""\n_SIZES = [""xs"", ""s"", ""m"", ""l"", ""xl""]\n\n\nclass WinograndeConfig(nlp.BuilderConfig):\n\n    """""" BuilderConfig for Discofuse""""""\n\n    def __init__(self, data_size, **kwargs):\n        """"""\n\n        Args:\n            data_size: the size of the training set we want to us (xs, s, m, l, xl)\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(WinograndeConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n        self.data_size = data_size\n\n\nclass Winogrande(nlp.GeneratorBasedBuilder):\n    """"""TODO(winogrande): Short description of my dataset.""""""\n\n    # TODO(winogrande): Set up version.\n    VERSION = nlp.Version(""1.1.0"")\n    BUILDER_CONFIGS = [\n        WinograndeConfig(name=""winogrande_"" + size, description=""AI2 dataset"", data_size=size) for size in _SIZES\n    ]\n\n    def _info(self):\n        # TODO(winogrande): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""sentence"": nlp.Value(""string""),\n                    ""option1"": nlp.Value(""string""),\n                    ""option2"": nlp.Value(""string""),\n                    ""answer"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://leaderboard.allenai.org/winogrande/submissions/get-started"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(winogrande): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        data_dir = os.path.join(dl_dir, ""winogrande_1.1"")\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    ""filepath"": os.path.join(data_dir, ""train_{}.jsonl"".format(self.config.data_size)),\n                    #\'labelpath\': os.path.join(data_dir, \'train_{}-labels.lst\'.format(self.config.data_size)),\n                    ""split"": ""train"",\n                },\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""test.jsonl""), ""split"": ""test""},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    ""filepath"": os.path.join(data_dir, ""dev.jsonl""),\n                    #\'labelpath\': os.path.join(data_dir, \'dev-labels.lst\'),\n                    ""split"": ""dev"",\n                },\n            ),\n        ]\n\n    def _generate_examples(self, filepath, split):\n        """"""Yields examples.""""""\n        # TODO(winogrande): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n                if split == ""test"":\n                    yield id_, {\n                        ""sentence"": data[""sentence""],\n                        ""option1"": data[""option1""],\n                        ""option2"": data[""option2""],\n                        ""answer"": """",\n                    }\n                else:\n                    yield id_, {\n                        ""sentence"": data[""sentence""],\n                        ""option1"": data[""option1""],\n                        ""option2"": data[""option2""],\n                        ""answer"": data[""answer""],\n                    }\n\n\n# def _generate_test_example(filepath, split, labelpath=None):\n#       with open(filepath) as f:\n#           for id_, row in enumerate(f):\n#               data = json.loads(row)\n#               yield id_,{\n#                   \'sentence\': data[\'sentence\'],\n#                   \'option1\': data[\'option1\'],\n#                   \'option2\': data[\'option2\'],\n#                   \'answer\': None\n#               }\n'"
datasets/wiqa/wiqa.py,0,"b'""""""TODO(wiqa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(wiqa): BibTeX citation\n_CITATION = """"""\\\n@article{wiqa,\n      author    = {Niket Tandon and Bhavana Dalvi Mishra and Keisuke Sakaguchi and Antoine Bosselut and Peter Clark}\n      title     = {WIQA: A dataset for ""What if..."" reasoning over procedural text},\n      journal   = {arXiv:1909.04739v1},\n      year      = {2019},\n}\n""""""\n\n# TODO(wiqa):\n_DESCRIPTION = """"""\\\nThe WIQA dataset V1 has 39705 questions containing a perturbation and a possible effect in the context of a paragraph. \nThe dataset is split into 29808 train questions, 6894 dev questions and 3003 test questions.\n""""""\n_URL = ""https://public-aristo-processes.s3-us-west-2.amazonaws.com/wiqa_dataset_no_explanation_v2/wiqa-dataset-v2-october-2019.zip""\nURl = ""s3://ai2-s2-research-public/open-corpus/2020-04-10/""\n\n\nclass Wiqa(nlp.GeneratorBasedBuilder):\n    """"""TODO(wiqa): Short description of my dataset.""""""\n\n    # TODO(wiqa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(wiqa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""question_stem"": nlp.Value(""string""),\n                    ""question_para_step"": nlp.features.Sequence({""steps"": nlp.Value(""string"")}),\n                    ""answer_label"": nlp.Value(""string""),\n                    ""answer_label_as_choice"": nlp.Value(""string""),\n                    ""choices"": nlp.features.Sequence({""text"": nlp.Value(""string""), ""label"": nlp.Value(""string"")}),\n                    ""metadata_question_id"": nlp.Value(""string""),\n                    ""metadata_graph_id"": nlp.Value(""string""),\n                    ""metadata_para_id"": nlp.Value(""string""),\n                    ""metadata_question_type"": nlp.Value(""string""),\n                    ""metadata_path_len"": nlp.Value(""int32""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://allenai.org/data/wiqa"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(wiqa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""test.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""dev.jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(wiqa): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n\n                yield id_, {\n                    ""question_stem"": data[""question""][""stem""],\n                    ""question_para_step"": {""steps"": data[""question""][""para_steps""]},\n                    ""answer_label"": data[""question""][""answer_label""],\n                    ""answer_label_as_choice"": data[""question""][""answer_label_as_choice""],\n                    ""choices"": {\n                        ""text"": [choice[""text""] for choice in data[""question""][""choices""]],\n                        ""label"": [choice[""label""] for choice in data[""question""][""choices""]],\n                    },\n                    ""metadata_question_id"": data[""metadata""][""ques_id""],\n                    ""metadata_graph_id"": data[""metadata""][""graph_id""],\n                    ""metadata_para_id"": data[""metadata""][""para_id""],\n                    ""metadata_question_type"": data[""metadata""][""question_type""],\n                    ""metadata_path_len"": data[""metadata""][""path_len""],\n                }\n'"
datasets/wmt14/wmt14.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT14: Translate dataset.""""""\n\nimport nlp\n\nfrom .wmt_utils import Wmt, WmtConfig\n\n\n_URL = ""http://www.statmt.org/wmt14/translation-task.html""\n_CITATION = """"""\n@InProceedings{bojar-EtAl:2014:W14-33,\n  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\\v{s}},\n  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},\n  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},\n  month     = {June},\n  year      = {2014},\n  address   = {Baltimore, Maryland, USA},\n  publisher = {Association for Computational Linguistics},\n  pages     = {12--58},\n  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}\n}\n""""""\n\n_LANGUAGE_PAIRS = [(lang, ""en"") for lang in [""cs"", ""de"", ""fr"", ""hi"", ""ru""]]\n\n\nclass Wmt14(Wmt):\n    """"""WMT 14 translation datasets for all {xx, ""en""} language pairs.""""""\n\n    # Version history:\n    # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).\n    BUILDER_CONFIGS = [\n        WmtConfig(  # pylint:disable=g-complex-comprehension\n            description=""WMT 2014 %s-%s translation task dataset."" % (l1, l2),\n            url=_URL,\n            citation=_CITATION,\n            language_pair=(l1, l2),\n            version=nlp.Version(""1.0.0""),\n        )\n        for l1, l2 in _LANGUAGE_PAIRS\n    ]\n\n    @property\n    def _subsets(self):\n        return {\n            nlp.Split.TRAIN: [\n                ""europarl_v7"",\n                ""commoncrawl"",\n                ""multiun"",\n                ""newscommentary_v9"",\n                ""gigafren"",\n                ""czeng_10"",\n                ""yandexcorpus"",\n                ""wikiheadlines_hi"",\n                ""wikiheadlines_ru"",\n                ""hindencorp_01"",\n            ],\n            nlp.Split.VALIDATION: [""newsdev2014"", ""newstest2013""],\n            nlp.Split.TEST: [""newstest2014""],\n        }\n'"
datasets/wmt14/wmt_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT: Translate dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport codecs\nimport functools\nimport glob\nimport gzip\nimport itertools\nimport logging\nimport os\nimport re\nimport xml.etree.cElementTree as ElementTree\nfrom abc import ABC, abstractmethod\n\nimport six\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nTranslate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `nlp.translate.wmt.WmtConfig`.\n\n```\nconfig = nlp.wmt.WmtConfig(\n    version=""0.0.1"",\n    language_pair=(""fr"", ""de""),\n    subsets={\n        nlp.Split.TRAIN: [""commoncrawl_frde""],\n        nlp.Split.VALIDATION: [""euelections_dev2019""],\n    },\n)\nbuilder = nlp.builder(""wmt_translate"", config=config)\n```\n\n""""""\n\n\nCWMT_SUBSET_NAMES = [""casia2015"", ""casict2011"", ""casict2015"", ""datum2015"", ""datum2017"", ""neu2017""]\n\n\nclass SubDataset(object):\n    """"""Class to keep track of information on a sub-dataset of WMT.""""""\n\n    def __init__(self, name, target, sources, url, path, manual_dl_files=None):\n        """"""Sub-dataset of WMT.\n\n    Args:\n      name: `string`, a unique dataset identifier.\n      target: `string`, the target language code.\n      sources: `set<string>`, the set of source language codes.\n      url: `string` or `(string, string)`, URL(s) or URL template(s) specifying\n        where to download the raw data from. If two strings are provided, the\n        first is used for the source language and the second for the target.\n        Template strings can either contain \'{src}\' placeholders that will be\n        filled in with the source language code, \'{0}\' and \'{1}\' placeholders\n        that will be filled in with the source and target language codes in\n        alphabetical order, or all 3.\n      path: `string` or `(string, string)`, path(s) or path template(s)\n        specifing the path to the raw data relative to the root of the\n        downloaded archive. If two strings are provided, the dataset is assumed\n        to be made up of parallel text files, the first being the source and the\n        second the target. If one string is provided, both languages are assumed\n        to be stored within the same file and the extension is used to determine\n        how to parse it. Template strings should be formatted the same as in\n        `url`.\n      manual_dl_files: `<list>(string)` (optional), the list of files that must\n        be manually downloaded to the data directory.\n    """"""\n        self._paths = (path,) if isinstance(path, six.string_types) else path\n        self._urls = (url,) if isinstance(url, six.string_types) else url\n        self._manual_dl_files = manual_dl_files if manual_dl_files else []\n        self.name = name\n        self.target = target\n        self.sources = set(sources)\n\n    def _inject_language(self, src, strings):\n        """"""Injects languages into (potentially) template strings.""""""\n        if src not in self.sources:\n            raise ValueError(""Invalid source for \'{0}\': {1}"".format(self.name, src))\n\n        def _format_string(s):\n            if ""{0}"" in s and ""{1}"" and ""{src}"" in s:\n                return s.format(*sorted([src, self.target]), src=src)\n            elif ""{0}"" in s and ""{1}"" in s:\n                return s.format(*sorted([src, self.target]))\n            elif ""{src}"" in s:\n                return s.format(src=src)\n            else:\n                return s\n\n        return [_format_string(s) for s in strings]\n\n    def get_url(self, src):\n        return self._inject_language(src, self._urls)\n\n    def get_manual_dl_files(self, src):\n        return self._inject_language(src, self._manual_dl_files)\n\n    def get_path(self, src):\n        return self._inject_language(src, self._paths)\n\n\n# Subsets used in the training sets for various years of WMT.\n_TRAIN_SUBSETS = [\n    # pylint:disable=line-too-long\n    SubDataset(\n        name=""commoncrawl"",\n        target=""en"",  # fr-de pair in commoncrawl_frde\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n        path=(""commoncrawl.{src}-en.{src}"", ""commoncrawl.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""commoncrawl_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""czeng_10"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng10"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""czeng_16pre"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng16pre"",\n        manual_dl_files=[""czeng16pre.deduped-ignoring-sections.txt.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""czeng_16"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        # This dataset differs from the above in the filtering that is applied\n        # during parsing.\n        name=""czeng_17"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""dcep_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/dcep.lv-en.v1.tgz"",\n        path=(""dcep.en-lv/dcep.lv"", ""dcep.en-lv/dcep.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n        path=(""training/europarl-v7.{src}-en.{src}"", ""training/europarl-v7.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""europarl_v8_18"",\n        target=""en"",\n        sources={""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training/europarl-v8.{src}-en.{src}"", ""training/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v8_16"",\n        target=""en"",\n        sources={""fi"", ""ro""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training-parallel-ep-v8/europarl-v8.{src}-en.{src}"", ""training-parallel-ep-v8/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""http://www.statmt.org/europarl/v9/training/europarl-v9.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""gigafren"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://www.statmt.org/wmt10/training-giga-fren.tar"",\n        path=(""giga-fren.release2.fixed.fr.gz"", ""giga-fren.release2.fixed.en.gz""),\n    ),\n    SubDataset(\n        name=""hindencorp_01"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://ufallab.ms.mff.cuni.cz/~bojar/hindencorp"",\n        manual_dl_files=[""hindencorp0.1.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""leta_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/leta.v1.tgz"",\n        path=(""LETA-lv-en/leta.lv"", ""LETA-lv-en/leta.en""),\n    ),\n    SubDataset(\n        name=""multiun"",\n        target=""en"",\n        sources={""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-un.tgz"",\n        path=(""un/undoc.2000.{src}-en.{src}"", ""un/undoc.2000.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz"",\n        path=(""training/news-commentary-v9.{src}-en.{src}"", ""training/news-commentary-v9.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v10"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz"",\n        path=(""news-commentary-v10.{src}-en.{src}"", ""news-commentary-v10.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v11"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz"",\n        path=(\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.{src}"",\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v12"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"",\n        path=(""training/news-commentary-v12.{src}-en.{src}"", ""training/news-commentary-v12.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v13"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"",\n        path=(\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.{src}"",\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v14"",\n        target=""en"",  # fr-de pair in newscommentary_v14_frde\n        sources={""cs"", ""de"", ""kk"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.{0}-{1}.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""newscommentary_v14_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-fr.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""onlinebooks_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/books.lv-en.v1.tgz"",\n        path=(""farewell/farewell.lv"", ""farewell/farewell.en""),\n    ),\n    SubDataset(\n        name=""paracrawl_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-{src}.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.{src}"",\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v1_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.ru"",\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v3"",\n        target=""en"",  # fr-de pair in paracrawl_v3_frde\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release3/en-{src}.bicleaner07.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""paracrawl_v3_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""rapid_2016"",\n        target=""en"",\n        sources={""de"", ""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/rapid2016.tgz"",\n        path=(""rapid2016.{0}-{1}.{src}"", ""rapid2016.{0}-{1}.en""),\n    ),\n    SubDataset(\n        name=""rapid_2016_ltfi"",\n        target=""en"",\n        sources={""fi"", ""lt""},\n        url=""https://tilde-model.s3-eu-west-1.amazonaws.com/rapid2016.en-{src}.tmx.zip"",\n        path=""rapid2016.en-{src}.tmx"",\n    ),\n    SubDataset(\n        name=""rapid_2019"",\n        target=""en"",\n        sources={""de""},\n        url=""https://s3-eu-west-1.amazonaws.com/tilde-model/rapid2019.de-en.zip"",\n        path=(""rapid2019.de-en.de"", ""rapid2019.de-en.en""),\n    ),\n    SubDataset(\n        name=""setimes_2"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-{src}.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""uncorpus_v1"",\n        target=""en"",\n        sources={""ru"", ""zh""},\n        url=""https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-{src}.tar.gz"",\n        path=(""en-{src}/UNv1.0.en-{src}.{src}"", ""en-{src}/UNv1.0.en-{src}.en""),\n    ),\n    SubDataset(\n        name=""wikiheadlines_fi"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/fi-en/titles.fi-en"",\n    ),\n    SubDataset(\n        name=""wikiheadlines_hi"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://www.statmt.org/wmt14/wiki-titles.tgz"",\n        path=""wiki/hi-en/wiki-titles.hi-en"",\n    ),\n    SubDataset(\n        # Verified that wmt14 and wmt15 files are identical.\n        name=""wikiheadlines_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/ru-en/wiki.ru-en"",\n    ),\n    SubDataset(\n        name=""wikititles_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""gu"", ""kk"", ""lt"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wikititles/v1/wikititles-v1.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""yandexcorpus"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://translate.yandex.ru/corpus?lang=en"",\n        manual_dl_files=[""1mcorpus.zip""],\n        path=(""corpus.en_ru.1m.ru"", ""corpus.en_ru.1m.en""),\n    ),\n    # pylint:enable=line-too-long\n] + [\n    SubDataset(  # pylint:disable=g-complex-comprehension\n        name=ss,\n        target=""en"",\n        sources={""zh""},\n        url=""ftp://cwmt-wmt:cwmt-wmt@nlp.nju.edu.cn/parallel/%s.zip"" % ss,\n        path=(""%s/*_c[hn].txt"" % ss, ""%s/*_en.txt"" % ss),\n    )\n    for ss in CWMT_SUBSET_NAMES\n]\n\n_DEV_SUBSETS = [\n    SubDataset(\n        name=""euelections_dev2019"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/euelections_dev2019.fr-de.src.fr"", ""dev/euelections_dev2019.fr-de.tgt.de""),\n    ),\n    SubDataset(\n        name=""newsdev2014"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2014.hi"", ""dev/newsdev2014.en""),\n    ),\n    SubDataset(\n        name=""newsdev2015"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2015-fien-src.{src}.sgm"", ""dev/newsdev2015-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2016"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2016-{src}en-src.{src}.sgm"", ""dev/newsdev2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2017"",\n        target=""en"",\n        sources={""lv"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2017-{src}en-src.{src}.sgm"", ""dev/newsdev2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2018"",\n        target=""en"",\n        sources={""et""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2018-{src}en-src.{src}.sgm"", ""dev/newsdev2018-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2019"",\n        target=""en"",\n        sources={""gu"", ""kk"", ""lt""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2019-{src}en-src.{src}.sgm"", ""dev/newsdev2019-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newssyscomb2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newssyscomb2009.{src}"", ""dev/newssyscomb2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2008"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hu""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/news-test2008.{src}"", ""dev/news-test2008.en""),\n    ),\n    SubDataset(\n        name=""newstest2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2009.{src}"", ""dev/newstest2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2010"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2010.{src}"", ""dev/newstest2010.en""),\n    ),\n    SubDataset(\n        name=""newstest2011"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2011.{src}"", ""dev/newstest2011.en""),\n    ),\n    SubDataset(\n        name=""newstest2012"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2012.{src}"", ""dev/newstest2012.en""),\n    ),\n    SubDataset(\n        name=""newstest2013"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2013.{src}"", ""dev/newstest2013.en""),\n    ),\n    SubDataset(\n        name=""newstest2014"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2014-{src}en-src.{src}.sgm"", ""dev/newstest2014-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2015"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2015-{src}en-src.{src}.sgm"", ""dev/newstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2016"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ro"", ""ru"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2016-{src}en-src.{src}.sgm"", ""dev/newstest2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2016"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2016-enfi-ref.{src}.sgm"", ""dev/newstestB2016-enfi-src.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2017"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lv"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2017-{src}en-src.{src}.sgm"", ""dev/newstest2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2017"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2017-fien-src.fi.sgm"", ""dev/newstestB2017-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2018"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2018-{src}en-src.{src}.sgm"", ""dev/newstest2018-{src}en-ref.en.sgm""),\n    ),\n]\n\nDATASET_MAP = {dataset.name: dataset for dataset in _TRAIN_SUBSETS + _DEV_SUBSETS}\n\n_CZENG17_FILTER = SubDataset(\n    name=""czeng17_filter"",\n    target=""en"",\n    sources={""cs""},\n    url=""http://ufal.mff.cuni.cz/czeng/download.php?f=convert_czeng16_to_17.pl.zip"",\n    path=""convert_czeng16_to_17.pl"",\n)\n\n\nclass WmtConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for WMT.""""""\n\n    def __init__(self, url=None, citation=None, description=None, language_pair=(None, None), subsets=None, **kwargs):\n        """"""BuilderConfig for WMT.\n\n    Args:\n      url: The reference URL for the dataset.\n      citation: The paper citation for the dataset.\n      description: The description of the dataset.\n      language_pair: pair of languages that will be used for translation. Should\n                 contain 2 letter coded strings. For example: (""en"", ""de"").\n        configuration for the `nlp.features.text.TextEncoder` used for the\n        `nlp.features.text.Translation` features.\n      subsets: Dict[split, list[str]]. List of the subset to use for each of the\n        split. Note that WMT subclasses overwrite this parameter.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s-%s"" % (language_pair[0], language_pair[1])\n        if ""name"" in kwargs:  # Add name suffix for custom configs\n            name += ""."" + kwargs.pop(""name"")\n\n        super(WmtConfig, self).__init__(name=name, description=description, **kwargs)\n\n        self.url = url or ""http://www.statmt.org""\n        self.citation = citation\n        self.language_pair = language_pair\n        self.subsets = subsets\n\n        # TODO(PVP): remove when manual dir works\n        # +++++++++++++++++++++\n        if language_pair[1] in [""cs"", ""hi"", ""ru""]:\n            assert NotImplementedError(\n                ""The dataset for {}-en is currently not fully supported."".format(language_pair[1])\n            )\n        # +++++++++++++++++++++\n\n\nclass Wmt(ABC, nlp.GeneratorBasedBuilder):\n    """"""WMT translation dataset.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Some of the wmt configs here, require a manual download.\n  Please look into wmt.py to see the exact path (and file name) that has to\n  be downloaded.\n  """"""\n\n    def __init__(self, *args, **kwargs):\n        if type(self) == Wmt and ""config"" not in kwargs:  # pylint: disable=unidiomatic-typecheck\n            raise ValueError(\n                ""The raw `wmt_translate` can only be instantiated with the config ""\n                ""kwargs. You may want to use one of the `wmtYY_translate` ""\n                ""implementation instead to get the WMT dataset for a specific year.""\n            )\n        super(Wmt, self).__init__(*args, **kwargs)\n\n    @property\n    @abstractmethod\n    def _subsets(self):\n        """"""Subsets that make up each split of the dataset.""""""\n        raise NotImplementedError(""This is a abstract method"")\n\n    @property\n    def subsets(self):\n        """"""Subsets that make up each split of the dataset for the language pair.""""""\n        source, target = self.config.language_pair\n        filtered_subsets = {}\n        for split, ss_names in self._subsets.items():\n            filtered_subsets[split] = []\n            for ss_name in ss_names:\n                dataset = DATASET_MAP[ss_name]\n                if dataset.target != target or source not in dataset.sources:\n                    logging.info(""Skipping sub-dataset that does not include language pair: %s"", ss_name)\n                else:\n                    filtered_subsets[split].append(ss_name)\n        logging.info(""Using sub-datasets: %s"", filtered_subsets)\n        return filtered_subsets\n\n    def _info(self):\n        src, target = self.config.language_pair\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            supervised_keys=(src, target),\n            homepage=self.config.url,\n            citation=self.config.citation,\n        )\n\n    def _vocab_text_gen(self, split_subsets, extraction_map, language):\n        for _, ex in self._generate_examples(split_subsets, extraction_map, with_translation=False):\n            yield ex[language]\n\n    def _split_generators(self, dl_manager):\n        source, _ = self.config.language_pair\n        manual_paths_dict = {}\n        urls_to_download = {}\n        for ss_name in itertools.chain.from_iterable(self.subsets.values()):\n            if ss_name == ""czeng_17"":\n                # CzEng1.7 is CzEng1.6 with some blocks filtered out. We must download\n                # the filtering script so we can parse out which blocks need to be\n                # removed.\n                urls_to_download[_CZENG17_FILTER.name] = _CZENG17_FILTER.get_url(source)\n\n            # get dataset\n            dataset = DATASET_MAP[ss_name]\n            if dataset.get_manual_dl_files(source):\n                # TODO(PVP): following two lines skip configs that are incomplete for now\n                # +++++++++++++++++++++\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n                # +++++++++++++++++++++\n\n                manual_dl_files = dataset.get_manual_dl_files(source)\n                manual_paths = [\n                    os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), fname)\n                    for fname in manual_dl_files\n                ]\n                assert all(\n                    os.path.exists(path) for path in manual_paths\n                ), ""For {0}, you must manually download the following file(s) from {1} and place them in {2}: {3}"".format(\n                    dataset.name, dataset.get_url(source), dl_manager.manual_dir, "", "".join(manual_dl_files)\n                )\n\n                # set manual path for correct subset\n                manual_paths_dict[ss_name] = manual_paths\n            else:\n                urls_to_download[ss_name] = dataset.get_url(source)\n\n        # Download and extract files from URLs.\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n        # Extract manually downloaded files.\n        manual_files = dl_manager.extract(manual_paths_dict)\n        extraction_map = dict(downloaded_files, **manual_files)\n\n        for language in self.config.language_pair:\n            self._vocab_text_gen(self.subsets[nlp.Split.TRAIN], extraction_map, language)\n\n        return [\n            nlp.SplitGenerator(  # pylint:disable=g-complex-comprehension\n                name=split, gen_kwargs={""split_subsets"": split_subsets, ""extraction_map"": extraction_map}\n            )\n            for split, split_subsets in self.subsets.items()\n        ]\n\n    def _generate_examples(self, split_subsets, extraction_map, with_translation=True):\n        """"""Returns the examples in the raw (text) form.""""""\n        source, _ = self.config.language_pair\n\n        def _get_local_paths(dataset, extract_dirs):\n            rel_paths = dataset.get_path(source)\n            if len(extract_dirs) == 1:\n                extract_dirs = extract_dirs * len(rel_paths)\n            return [\n                os.path.join(ex_dir, rel_path) if rel_path else ex_dir\n                for ex_dir, rel_path in zip(extract_dirs, rel_paths)\n            ]\n\n        for ss_name in split_subsets:\n            # TODO(PVP) remove following five lines when manual data works\n            # +++++++++++++++++++++\n            dataset = DATASET_MAP[ss_name]\n            source, _ = self.config.language_pair\n            if dataset.get_manual_dl_files(source):\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n            # +++++++++++++++++++++\n\n            logging.info(""Generating examples from: %s"", ss_name)\n            dataset = DATASET_MAP[ss_name]\n            extract_dirs = extraction_map[ss_name]\n            files = _get_local_paths(dataset, extract_dirs)\n\n            if ss_name.startswith(""czeng""):\n                if ss_name.endswith(""16pre""):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=(""en"", ""cs""))\n                elif ss_name.endswith(""17""):\n                    filter_path = _get_local_paths(_CZENG17_FILTER, extraction_map[_CZENG17_FILTER.name])[0]\n                    sub_generator = functools.partial(_parse_czeng, filter_path=filter_path)\n                else:\n                    sub_generator = _parse_czeng\n            elif ss_name == ""hindencorp_01"":\n                sub_generator = _parse_hindencorp\n            elif len(files) == 2:\n                if ss_name.endswith(""_frde""):\n                    sub_generator = _parse_frde_bitext\n                else:\n                    sub_generator = _parse_parallel_sentences\n            elif len(files) == 1:\n                fname = files[0]\n                # Note: Due to formatting used by `download_manager`, the file\n                # extension may not be at the end of the file path.\n                if "".tsv"" in fname:\n                    sub_generator = _parse_tsv\n                elif (\n                    ss_name.startswith(""newscommentary_v14"")\n                    or ss_name.startswith(""europarl_v9"")\n                    or ss_name.startswith(""wikititles_v1"")\n                ):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=self.config.language_pair)\n                elif ""tmx"" in fname or ss_name.startswith(""paracrawl_v3""):\n                    sub_generator = _parse_tmx\n                elif ss_name.startswith(""wikiheadlines""):\n                    sub_generator = _parse_wikiheadlines\n                else:\n                    raise ValueError(""Unsupported file format: %s"" % fname)\n            else:\n                raise ValueError(""Invalid number of files: %d"" % len(files))\n\n            for sub_key, ex in sub_generator(*files):\n                if not all(ex.values()):\n                    continue\n                # TODO(adarob): Add subset feature.\n                # ex[""subset""] = subset\n                key = ""{}/{}"".format(ss_name, sub_key)\n                if with_translation is True:\n                    ex = {""translation"": ex}\n                yield key, ex\n\n\ndef _parse_parallel_sentences(f1, f2):\n    """"""Returns examples from parallel SGML or text files, which may be gzipped.""""""\n\n    def _parse_text(path):\n        """"""Returns the sentences from a single text file, which may be gzipped.""""""\n        split_path = path.split(""."")\n\n        if split_path[-1] == ""gz"":\n            lang = split_path[-2]\n            with open(path, ""rb"") as f, gzip.GzipFile(fileobj=f) as g:\n                return g.read().decode(""utf-8"").split(""\\n""), lang\n\n        if split_path[-1] == ""txt"":\n            # CWMT\n            lang = split_path[-2].split(""_"")[-1]\n            lang = ""zh"" if lang in (""ch"", ""cn"") else lang\n        else:\n            lang = split_path[-1]\n        with open(path, ""rb"") as f:\n            return f.read().decode(""utf-8"").split(""\\n""), lang\n\n    def _parse_sgm(path):\n        """"""Returns sentences from a single SGML file.""""""\n        lang = path.split(""."")[-2]\n        sentences = []\n        # Note: We can\'t use the XML parser since some of the files are badly\n        # formatted.\n        seg_re = re.compile(r""<seg id=\\""\\d+\\"">(.*)</seg>"")\n        with open(path) as f:\n            for line in f:\n                seg_match = re.match(seg_re, line)\n                if seg_match:\n                    assert len(seg_match.groups()) == 1\n                    sentences.append(seg_match.groups()[0])\n        return sentences, lang\n\n    parse_file = _parse_sgm if f1.endswith("".sgm"") else _parse_text\n\n    # Some datasets (e.g., CWMT) contain multiple parallel files specified with\n    # a wildcard. We sort both sets to align them and parse them one by one.\n    f1_files = sorted(glob.glob(f1))\n    f2_files = sorted(glob.glob(f2))\n\n    assert f1_files and f2_files, ""No matching files found: %s, %s."" % (f1, f2)\n    assert len(f1_files) == len(f2_files), ""Number of files do not match: %d vs %d for %s vs %s."" % (\n        len(f1_files),\n        len(f2_files),\n        f1,\n        f2,\n    )\n\n    for f_id, (f1_i, f2_i) in enumerate(zip(sorted(f1_files), sorted(f2_files))):\n        l1_sentences, l1 = parse_file(f1_i)\n        l2_sentences, l2 = parse_file(f2_i)\n\n        assert len(l1_sentences) == len(l2_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(l1_sentences),\n            len(l2_sentences),\n            f1_i,\n            f2_i,\n        )\n\n        for line_id, (s1, s2) in enumerate(zip(l1_sentences, l2_sentences)):\n            key = ""{}/{}"".format(f_id, line_id)\n            yield key, {l1: s1, l2: s2}\n\n\ndef _parse_frde_bitext(fr_path, de_path):\n    with open(fr_path) as f:\n        fr_sentences = f.read().split(""\\n"")\n    with open(de_path) as f:\n        de_sentences = f.read().split(""\\n"")\n    assert len(fr_sentences) == len(de_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n        len(fr_sentences),\n        len(de_sentences),\n        fr_path,\n        de_path,\n    )\n    for line_id, (s1, s2) in enumerate(zip(fr_sentences, de_sentences)):\n        yield line_id, {""fr"": s1, ""de"": s2}\n\n\ndef _parse_tmx(path):\n    """"""Generates examples from TMX file.""""""\n\n    def _get_tuv_lang(tuv):\n        for k, v in tuv.items():\n            if k.endswith(""}lang""):\n                return v\n        raise AssertionError(""Language not found in `tuv` attributes."")\n\n    def _get_tuv_seg(tuv):\n        segs = tuv.findall(""seg"")\n        assert len(segs) == 1, ""Invalid number of segments: %d"" % len(segs)\n        return segs[0].text\n\n    with open(path, ""rb"") as f:\n        if six.PY3:\n            # Workaround due to: https://github.com/tensorflow/tensorflow/issues/33563\n            utf_f = codecs.getreader(""utf-8"")(f)\n        else:\n            utf_f = f\n        for line_id, (_, elem) in enumerate(ElementTree.iterparse(utf_f)):\n            if elem.tag == ""tu"":\n                yield line_id, {_get_tuv_lang(tuv): _get_tuv_seg(tuv) for tuv in elem.iterfind(""tuv"")}\n                elem.clear()\n\n\ndef _parse_tsv(path, language_pair=None):\n    """"""Generates examples from TSV file.""""""\n    if language_pair is None:\n        lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])\\.tsv"", path)\n        assert lang_match is not None, ""Invalid TSV filename: %s"" % path\n        l1, l2 = lang_match.groups()\n    else:\n        l1, l2 = language_pair\n    with open(path) as f:\n        for j, line in enumerate(f):\n            cols = line.split(""\\t"")\n            if len(cols) != 2:\n                logging.warning(""Skipping line %d in TSV (%s) with %d != 2 columns."", j, path, len(cols))\n                continue\n            s1, s2 = cols\n            yield j, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_wikiheadlines(path):\n    """"""Generates examples from Wikiheadlines dataset file.""""""\n    lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])$"", path)\n    assert lang_match is not None, ""Invalid Wikiheadlines filename: %s"" % path\n    l1, l2 = lang_match.groups()\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            s1, s2 = line.split(""|||"")\n            yield line_id, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_czeng(*paths, **kwargs):\n    """"""Generates examples from CzEng v1.6, with optional filtering for v1.7.""""""\n    filter_path = kwargs.get(""filter_path"", None)\n    if filter_path:\n        re_block = re.compile(r""^[^-]+-b(\\d+)-\\d\\d[tde]"")\n        with open(filter_path) as f:\n            bad_blocks = {blk for blk in re.search(r""qw{([\\s\\d]*)}"", f.read()).groups()[0].split()}\n        logging.info(""Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7."", len(bad_blocks))\n\n    for path in paths:\n        for gz_path in sorted(glob.glob(path)):\n            with open(gz_path, ""rb"") as g, gzip.GzipFile(fileobj=g) as f:\n                filename = os.path.basename(gz_path)\n                for line_id, line in enumerate(f):\n                    line = line.decode(""utf-8"")  # required for py3\n                    if not line.strip():\n                        continue\n                    id_, unused_score, cs, en = line.split(""\\t"")\n                    if filter_path:\n                        block_match = re.match(re_block, id_)\n                        if block_match and block_match.groups()[0] in bad_blocks:\n                            continue\n                    sub_key = ""{}/{}"".format(filename, line_id)\n                    yield sub_key, {\n                        ""cs"": cs.strip(),\n                        ""en"": en.strip(),\n                    }\n\n\ndef _parse_hindencorp(path):\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            split_line = line.split(""\\t"")\n            if len(split_line) != 5:\n                logging.warning(""Skipping invalid HindEnCorp line: %s"", line)\n                continue\n            yield line_id, {""translation"": {""en"": split_line[3].strip(), ""hi"": split_line[4].strip()}}\n'"
datasets/wmt15/wmt15.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT15: Translate dataset.""""""\n\nimport nlp\n\nfrom .wmt_utils import Wmt, WmtConfig\n\n\n_URL = ""http://www.statmt.org/wmt15/translation-task.html""\n_CITATION = """"""\n@InProceedings{bojar-EtAl:2015:WMT,\n  author    = {Bojar, Ond\\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Haddow, Barry  and  Huck, Matthias  and  Hokamp, Chris  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Post, Matt  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco},\n  title     = {Findings of the 2015 Workshop on Statistical Machine Translation},\n  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},\n  month     = {September},\n  year      = {2015},\n  address   = {Lisbon, Portugal},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1--46},\n  url       = {http://aclweb.org/anthology/W15-3001}\n}\n""""""\n\n_LANGUAGE_PAIRS = [(lang, ""en"") for lang in [""cs"", ""de"", ""fi"", ""fr"", ""ru""]]\n\n\nclass Wmt15(Wmt):\n    """"""WMT 15 translation datasets for all {xx, ""en""} language pairs.""""""\n\n    BUILDER_CONFIGS = [\n        WmtConfig(  # pylint:disable=g-complex-comprehension\n            description=""WMT 2015 %s-%s translation task dataset."" % (l1, l2),\n            url=_URL,\n            citation=_CITATION,\n            language_pair=(l1, l2),\n            version=nlp.Version(""1.0.0""),\n        )\n        for l1, l2 in _LANGUAGE_PAIRS\n    ]\n\n    @property\n    def _subsets(self):\n        return {\n            nlp.Split.TRAIN: [\n                ""europarl_v7"",\n                ""europarl_v8_16"",\n                ""commoncrawl"",\n                ""multiun"",\n                ""newscommentary_v10"",\n                ""gigafren"",\n                ""czeng_10"",\n                ""yandexcorpus"",\n                ""wikiheadlines_fi"",\n                ""wikiheadlines_ru"",\n            ],\n            nlp.Split.VALIDATION: [""newsdev2015"", ""newsdiscussdev2015"", ""newstest2014""],\n            nlp.Split.TEST: [""newstest2015"", ""newsdiscusstest2015"",],\n        }\n'"
datasets/wmt15/wmt_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT: Translate dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport codecs\nimport functools\nimport glob\nimport gzip\nimport itertools\nimport logging\nimport os\nimport re\nimport xml.etree.cElementTree as ElementTree\nfrom abc import ABC, abstractmethod\n\nimport six\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nTranslate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `nlp.translate.wmt.WmtConfig`.\n\n```\nconfig = nlp.wmt.WmtConfig(\n    version=""0.0.1"",\n    language_pair=(""fr"", ""de""),\n    subsets={\n        nlp.Split.TRAIN: [""commoncrawl_frde""],\n        nlp.Split.VALIDATION: [""euelections_dev2019""],\n    },\n)\nbuilder = nlp.builder(""wmt_translate"", config=config)\n```\n\n""""""\n\n\nCWMT_SUBSET_NAMES = [""casia2015"", ""casict2011"", ""casict2015"", ""datum2015"", ""datum2017"", ""neu2017""]\n\n\nclass SubDataset(object):\n    """"""Class to keep track of information on a sub-dataset of WMT.""""""\n\n    def __init__(self, name, target, sources, url, path, manual_dl_files=None):\n        """"""Sub-dataset of WMT.\n\n    Args:\n      name: `string`, a unique dataset identifier.\n      target: `string`, the target language code.\n      sources: `set<string>`, the set of source language codes.\n      url: `string` or `(string, string)`, URL(s) or URL template(s) specifying\n        where to download the raw data from. If two strings are provided, the\n        first is used for the source language and the second for the target.\n        Template strings can either contain \'{src}\' placeholders that will be\n        filled in with the source language code, \'{0}\' and \'{1}\' placeholders\n        that will be filled in with the source and target language codes in\n        alphabetical order, or all 3.\n      path: `string` or `(string, string)`, path(s) or path template(s)\n        specifing the path to the raw data relative to the root of the\n        downloaded archive. If two strings are provided, the dataset is assumed\n        to be made up of parallel text files, the first being the source and the\n        second the target. If one string is provided, both languages are assumed\n        to be stored within the same file and the extension is used to determine\n        how to parse it. Template strings should be formatted the same as in\n        `url`.\n      manual_dl_files: `<list>(string)` (optional), the list of files that must\n        be manually downloaded to the data directory.\n    """"""\n        self._paths = (path,) if isinstance(path, six.string_types) else path\n        self._urls = (url,) if isinstance(url, six.string_types) else url\n        self._manual_dl_files = manual_dl_files if manual_dl_files else []\n        self.name = name\n        self.target = target\n        self.sources = set(sources)\n\n    def _inject_language(self, src, strings):\n        """"""Injects languages into (potentially) template strings.""""""\n        if src not in self.sources:\n            raise ValueError(""Invalid source for \'{0}\': {1}"".format(self.name, src))\n\n        def _format_string(s):\n            if ""{0}"" in s and ""{1}"" and ""{src}"" in s:\n                return s.format(*sorted([src, self.target]), src=src)\n            elif ""{0}"" in s and ""{1}"" in s:\n                return s.format(*sorted([src, self.target]))\n            elif ""{src}"" in s:\n                return s.format(src=src)\n            else:\n                return s\n\n        return [_format_string(s) for s in strings]\n\n    def get_url(self, src):\n        return self._inject_language(src, self._urls)\n\n    def get_manual_dl_files(self, src):\n        return self._inject_language(src, self._manual_dl_files)\n\n    def get_path(self, src):\n        return self._inject_language(src, self._paths)\n\n\n# Subsets used in the training sets for various years of WMT.\n_TRAIN_SUBSETS = [\n    # pylint:disable=line-too-long\n    SubDataset(\n        name=""commoncrawl"",\n        target=""en"",  # fr-de pair in commoncrawl_frde\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n        path=(""commoncrawl.{src}-en.{src}"", ""commoncrawl.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""commoncrawl_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""czeng_10"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng10"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""czeng_16pre"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng16pre"",\n        manual_dl_files=[""czeng16pre.deduped-ignoring-sections.txt.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""czeng_16"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        # This dataset differs from the above in the filtering that is applied\n        # during parsing.\n        name=""czeng_17"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""dcep_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/dcep.lv-en.v1.tgz"",\n        path=(""dcep.en-lv/dcep.lv"", ""dcep.en-lv/dcep.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n        path=(""training/europarl-v7.{src}-en.{src}"", ""training/europarl-v7.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""europarl_v8_18"",\n        target=""en"",\n        sources={""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training/europarl-v8.{src}-en.{src}"", ""training/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v8_16"",\n        target=""en"",\n        sources={""fi"", ""ro""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training-parallel-ep-v8/europarl-v8.{src}-en.{src}"", ""training-parallel-ep-v8/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""http://www.statmt.org/europarl/v9/training/europarl-v9.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""gigafren"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://www.statmt.org/wmt10/training-giga-fren.tar"",\n        path=(""giga-fren.release2.fixed.fr.gz"", ""giga-fren.release2.fixed.en.gz""),\n    ),\n    SubDataset(\n        name=""hindencorp_01"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://ufallab.ms.mff.cuni.cz/~bojar/hindencorp"",\n        manual_dl_files=[""hindencorp0.1.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""leta_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/leta.v1.tgz"",\n        path=(""LETA-lv-en/leta.lv"", ""LETA-lv-en/leta.en""),\n    ),\n    SubDataset(\n        name=""multiun"",\n        target=""en"",\n        sources={""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-un.tgz"",\n        path=(""un/undoc.2000.{src}-en.{src}"", ""un/undoc.2000.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz"",\n        path=(""training/news-commentary-v9.{src}-en.{src}"", ""training/news-commentary-v9.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v10"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz"",\n        path=(""news-commentary-v10.{src}-en.{src}"", ""news-commentary-v10.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v11"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz"",\n        path=(\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.{src}"",\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v12"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"",\n        path=(""training/news-commentary-v12.{src}-en.{src}"", ""training/news-commentary-v12.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v13"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"",\n        path=(\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.{src}"",\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v14"",\n        target=""en"",  # fr-de pair in newscommentary_v14_frde\n        sources={""cs"", ""de"", ""kk"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.{0}-{1}.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""newscommentary_v14_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-fr.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""onlinebooks_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/books.lv-en.v1.tgz"",\n        path=(""farewell/farewell.lv"", ""farewell/farewell.en""),\n    ),\n    SubDataset(\n        name=""paracrawl_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-{src}.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.{src}"",\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v1_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.ru"",\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v3"",\n        target=""en"",  # fr-de pair in paracrawl_v3_frde\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release3/en-{src}.bicleaner07.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""paracrawl_v3_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""rapid_2016"",\n        target=""en"",\n        sources={""de"", ""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/rapid2016.tgz"",\n        path=(""rapid2016.{0}-{1}.{src}"", ""rapid2016.{0}-{1}.en""),\n    ),\n    SubDataset(\n        name=""rapid_2016_ltfi"",\n        target=""en"",\n        sources={""fi"", ""lt""},\n        url=""https://tilde-model.s3-eu-west-1.amazonaws.com/rapid2016.en-{src}.tmx.zip"",\n        path=""rapid2016.en-{src}.tmx"",\n    ),\n    SubDataset(\n        name=""rapid_2019"",\n        target=""en"",\n        sources={""de""},\n        url=""https://s3-eu-west-1.amazonaws.com/tilde-model/rapid2019.de-en.zip"",\n        path=(""rapid2019.de-en.de"", ""rapid2019.de-en.en""),\n    ),\n    SubDataset(\n        name=""setimes_2"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-{src}.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""uncorpus_v1"",\n        target=""en"",\n        sources={""ru"", ""zh""},\n        url=""https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-{src}.tar.gz"",\n        path=(""en-{src}/UNv1.0.en-{src}.{src}"", ""en-{src}/UNv1.0.en-{src}.en""),\n    ),\n    SubDataset(\n        name=""wikiheadlines_fi"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/fi-en/titles.fi-en"",\n    ),\n    SubDataset(\n        name=""wikiheadlines_hi"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://www.statmt.org/wmt14/wiki-titles.tgz"",\n        path=""wiki/hi-en/wiki-titles.hi-en"",\n    ),\n    SubDataset(\n        # Verified that wmt14 and wmt15 files are identical.\n        name=""wikiheadlines_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/ru-en/wiki.ru-en"",\n    ),\n    SubDataset(\n        name=""wikititles_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""gu"", ""kk"", ""lt"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wikititles/v1/wikititles-v1.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""yandexcorpus"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://translate.yandex.ru/corpus?lang=en"",\n        manual_dl_files=[""1mcorpus.zip""],\n        path=(""corpus.en_ru.1m.ru"", ""corpus.en_ru.1m.en""),\n    ),\n    # pylint:enable=line-too-long\n] + [\n    SubDataset(  # pylint:disable=g-complex-comprehension\n        name=ss,\n        target=""en"",\n        sources={""zh""},\n        url=""ftp://cwmt-wmt:cwmt-wmt@nlp.nju.edu.cn/parallel/%s.zip"" % ss,\n        path=(""%s/*_c[hn].txt"" % ss, ""%s/*_en.txt"" % ss),\n    )\n    for ss in CWMT_SUBSET_NAMES\n]\n\n_DEV_SUBSETS = [\n    SubDataset(\n        name=""euelections_dev2019"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/euelections_dev2019.fr-de.src.fr"", ""dev/euelections_dev2019.fr-de.tgt.de""),\n    ),\n    SubDataset(\n        name=""newsdev2014"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2014.hi"", ""dev/newsdev2014.en""),\n    ),\n    SubDataset(\n        name=""newsdev2015"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2015-fien-src.{src}.sgm"", ""dev/newsdev2015-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2016"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2016-{src}en-src.{src}.sgm"", ""dev/newsdev2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2017"",\n        target=""en"",\n        sources={""lv"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2017-{src}en-src.{src}.sgm"", ""dev/newsdev2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2018"",\n        target=""en"",\n        sources={""et""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2018-{src}en-src.{src}.sgm"", ""dev/newsdev2018-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2019"",\n        target=""en"",\n        sources={""gu"", ""kk"", ""lt""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2019-{src}en-src.{src}.sgm"", ""dev/newsdev2019-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newssyscomb2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newssyscomb2009.{src}"", ""dev/newssyscomb2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2008"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hu""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/news-test2008.{src}"", ""dev/news-test2008.en""),\n    ),\n    SubDataset(\n        name=""newstest2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2009.{src}"", ""dev/newstest2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2010"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2010.{src}"", ""dev/newstest2010.en""),\n    ),\n    SubDataset(\n        name=""newstest2011"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2011.{src}"", ""dev/newstest2011.en""),\n    ),\n    SubDataset(\n        name=""newstest2012"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2012.{src}"", ""dev/newstest2012.en""),\n    ),\n    SubDataset(\n        name=""newstest2013"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2013.{src}"", ""dev/newstest2013.en""),\n    ),\n    SubDataset(\n        name=""newstest2014"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2014-{src}en-src.{src}.sgm"", ""dev/newstest2014-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2015"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2015-{src}en-src.{src}.sgm"", ""dev/newstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2016"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ro"", ""ru"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2016-{src}en-src.{src}.sgm"", ""dev/newstest2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2016"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2016-enfi-ref.{src}.sgm"", ""dev/newstestB2016-enfi-src.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2017"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lv"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2017-{src}en-src.{src}.sgm"", ""dev/newstest2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2017"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2017-fien-src.fi.sgm"", ""dev/newstestB2017-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2018"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2018-{src}en-src.{src}.sgm"", ""dev/newstest2018-{src}en-ref.en.sgm""),\n    ),\n]\n\nDATASET_MAP = {dataset.name: dataset for dataset in _TRAIN_SUBSETS + _DEV_SUBSETS}\n\n_CZENG17_FILTER = SubDataset(\n    name=""czeng17_filter"",\n    target=""en"",\n    sources={""cs""},\n    url=""http://ufal.mff.cuni.cz/czeng/download.php?f=convert_czeng16_to_17.pl.zip"",\n    path=""convert_czeng16_to_17.pl"",\n)\n\n\nclass WmtConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for WMT.""""""\n\n    def __init__(self, url=None, citation=None, description=None, language_pair=(None, None), subsets=None, **kwargs):\n        """"""BuilderConfig for WMT.\n\n    Args:\n      url: The reference URL for the dataset.\n      citation: The paper citation for the dataset.\n      description: The description of the dataset.\n      language_pair: pair of languages that will be used for translation. Should\n                 contain 2 letter coded strings. For example: (""en"", ""de"").\n        configuration for the `nlp.features.text.TextEncoder` used for the\n        `nlp.features.text.Translation` features.\n      subsets: Dict[split, list[str]]. List of the subset to use for each of the\n        split. Note that WMT subclasses overwrite this parameter.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s-%s"" % (language_pair[0], language_pair[1])\n        if ""name"" in kwargs:  # Add name suffix for custom configs\n            name += ""."" + kwargs.pop(""name"")\n\n        super(WmtConfig, self).__init__(name=name, description=description, **kwargs)\n\n        self.url = url or ""http://www.statmt.org""\n        self.citation = citation\n        self.language_pair = language_pair\n        self.subsets = subsets\n\n        # TODO(PVP): remove when manual dir works\n        # +++++++++++++++++++++\n        if language_pair[1] in [""cs"", ""hi"", ""ru""]:\n            assert NotImplementedError(\n                ""The dataset for {}-en is currently not fully supported."".format(language_pair[1])\n            )\n        # +++++++++++++++++++++\n\n\nclass Wmt(ABC, nlp.GeneratorBasedBuilder):\n    """"""WMT translation dataset.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Some of the wmt configs here, require a manual download.\n  Please look into wmt.py to see the exact path (and file name) that has to\n  be downloaded.\n  """"""\n\n    def __init__(self, *args, **kwargs):\n        if type(self) == Wmt and ""config"" not in kwargs:  # pylint: disable=unidiomatic-typecheck\n            raise ValueError(\n                ""The raw `wmt_translate` can only be instantiated with the config ""\n                ""kwargs. You may want to use one of the `wmtYY_translate` ""\n                ""implementation instead to get the WMT dataset for a specific year.""\n            )\n        super(Wmt, self).__init__(*args, **kwargs)\n\n    @property\n    @abstractmethod\n    def _subsets(self):\n        """"""Subsets that make up each split of the dataset.""""""\n        raise NotImplementedError(""This is a abstract method"")\n\n    @property\n    def subsets(self):\n        """"""Subsets that make up each split of the dataset for the language pair.""""""\n        source, target = self.config.language_pair\n        filtered_subsets = {}\n        for split, ss_names in self._subsets.items():\n            filtered_subsets[split] = []\n            for ss_name in ss_names:\n                dataset = DATASET_MAP[ss_name]\n                if dataset.target != target or source not in dataset.sources:\n                    logging.info(""Skipping sub-dataset that does not include language pair: %s"", ss_name)\n                else:\n                    filtered_subsets[split].append(ss_name)\n        logging.info(""Using sub-datasets: %s"", filtered_subsets)\n        return filtered_subsets\n\n    def _info(self):\n        src, target = self.config.language_pair\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            supervised_keys=(src, target),\n            homepage=self.config.url,\n            citation=self.config.citation,\n        )\n\n    def _vocab_text_gen(self, split_subsets, extraction_map, language):\n        for _, ex in self._generate_examples(split_subsets, extraction_map, with_translation=False):\n            yield ex[language]\n\n    def _split_generators(self, dl_manager):\n        source, _ = self.config.language_pair\n        manual_paths_dict = {}\n        urls_to_download = {}\n        for ss_name in itertools.chain.from_iterable(self.subsets.values()):\n            if ss_name == ""czeng_17"":\n                # CzEng1.7 is CzEng1.6 with some blocks filtered out. We must download\n                # the filtering script so we can parse out which blocks need to be\n                # removed.\n                urls_to_download[_CZENG17_FILTER.name] = _CZENG17_FILTER.get_url(source)\n\n            # get dataset\n            dataset = DATASET_MAP[ss_name]\n            if dataset.get_manual_dl_files(source):\n                # TODO(PVP): following two lines skip configs that are incomplete for now\n                # +++++++++++++++++++++\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n                # +++++++++++++++++++++\n\n                manual_dl_files = dataset.get_manual_dl_files(source)\n                manual_paths = [\n                    os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), fname)\n                    for fname in manual_dl_files\n                ]\n                assert all(\n                    os.path.exists(path) for path in manual_paths\n                ), ""For {0}, you must manually download the following file(s) from {1} and place them in {2}: {3}"".format(\n                    dataset.name, dataset.get_url(source), dl_manager.manual_dir, "", "".join(manual_dl_files)\n                )\n\n                # set manual path for correct subset\n                manual_paths_dict[ss_name] = manual_paths\n            else:\n                urls_to_download[ss_name] = dataset.get_url(source)\n\n        # Download and extract files from URLs.\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n        # Extract manually downloaded files.\n        manual_files = dl_manager.extract(manual_paths_dict)\n        extraction_map = dict(downloaded_files, **manual_files)\n\n        for language in self.config.language_pair:\n            self._vocab_text_gen(self.subsets[nlp.Split.TRAIN], extraction_map, language)\n\n        return [\n            nlp.SplitGenerator(  # pylint:disable=g-complex-comprehension\n                name=split, gen_kwargs={""split_subsets"": split_subsets, ""extraction_map"": extraction_map}\n            )\n            for split, split_subsets in self.subsets.items()\n        ]\n\n    def _generate_examples(self, split_subsets, extraction_map, with_translation=True):\n        """"""Returns the examples in the raw (text) form.""""""\n        source, _ = self.config.language_pair\n\n        def _get_local_paths(dataset, extract_dirs):\n            rel_paths = dataset.get_path(source)\n            if len(extract_dirs) == 1:\n                extract_dirs = extract_dirs * len(rel_paths)\n            return [\n                os.path.join(ex_dir, rel_path) if rel_path else ex_dir\n                for ex_dir, rel_path in zip(extract_dirs, rel_paths)\n            ]\n\n        for ss_name in split_subsets:\n            # TODO(PVP) remove following five lines when manual data works\n            # +++++++++++++++++++++\n            dataset = DATASET_MAP[ss_name]\n            source, _ = self.config.language_pair\n            if dataset.get_manual_dl_files(source):\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n            # +++++++++++++++++++++\n\n            logging.info(""Generating examples from: %s"", ss_name)\n            dataset = DATASET_MAP[ss_name]\n            extract_dirs = extraction_map[ss_name]\n            files = _get_local_paths(dataset, extract_dirs)\n\n            if ss_name.startswith(""czeng""):\n                if ss_name.endswith(""16pre""):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=(""en"", ""cs""))\n                elif ss_name.endswith(""17""):\n                    filter_path = _get_local_paths(_CZENG17_FILTER, extraction_map[_CZENG17_FILTER.name])[0]\n                    sub_generator = functools.partial(_parse_czeng, filter_path=filter_path)\n                else:\n                    sub_generator = _parse_czeng\n            elif ss_name == ""hindencorp_01"":\n                sub_generator = _parse_hindencorp\n            elif len(files) == 2:\n                if ss_name.endswith(""_frde""):\n                    sub_generator = _parse_frde_bitext\n                else:\n                    sub_generator = _parse_parallel_sentences\n            elif len(files) == 1:\n                fname = files[0]\n                # Note: Due to formatting used by `download_manager`, the file\n                # extension may not be at the end of the file path.\n                if "".tsv"" in fname:\n                    sub_generator = _parse_tsv\n                elif (\n                    ss_name.startswith(""newscommentary_v14"")\n                    or ss_name.startswith(""europarl_v9"")\n                    or ss_name.startswith(""wikititles_v1"")\n                ):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=self.config.language_pair)\n                elif ""tmx"" in fname or ss_name.startswith(""paracrawl_v3""):\n                    sub_generator = _parse_tmx\n                elif ss_name.startswith(""wikiheadlines""):\n                    sub_generator = _parse_wikiheadlines\n                else:\n                    raise ValueError(""Unsupported file format: %s"" % fname)\n            else:\n                raise ValueError(""Invalid number of files: %d"" % len(files))\n\n            for sub_key, ex in sub_generator(*files):\n                if not all(ex.values()):\n                    continue\n                # TODO(adarob): Add subset feature.\n                # ex[""subset""] = subset\n                key = ""{}/{}"".format(ss_name, sub_key)\n                if with_translation is True:\n                    ex = {""translation"": ex}\n                yield key, ex\n\n\ndef _parse_parallel_sentences(f1, f2):\n    """"""Returns examples from parallel SGML or text files, which may be gzipped.""""""\n\n    def _parse_text(path):\n        """"""Returns the sentences from a single text file, which may be gzipped.""""""\n        split_path = path.split(""."")\n\n        if split_path[-1] == ""gz"":\n            lang = split_path[-2]\n            with open(path, ""rb"") as f, gzip.GzipFile(fileobj=f) as g:\n                return g.read().decode(""utf-8"").split(""\\n""), lang\n\n        if split_path[-1] == ""txt"":\n            # CWMT\n            lang = split_path[-2].split(""_"")[-1]\n            lang = ""zh"" if lang in (""ch"", ""cn"") else lang\n        else:\n            lang = split_path[-1]\n        with open(path, ""rb"") as f:\n            return f.read().decode(""utf-8"").split(""\\n""), lang\n\n    def _parse_sgm(path):\n        """"""Returns sentences from a single SGML file.""""""\n        lang = path.split(""."")[-2]\n        sentences = []\n        # Note: We can\'t use the XML parser since some of the files are badly\n        # formatted.\n        seg_re = re.compile(r""<seg id=\\""\\d+\\"">(.*)</seg>"")\n        with open(path) as f:\n            for line in f:\n                seg_match = re.match(seg_re, line)\n                if seg_match:\n                    assert len(seg_match.groups()) == 1\n                    sentences.append(seg_match.groups()[0])\n        return sentences, lang\n\n    parse_file = _parse_sgm if f1.endswith("".sgm"") else _parse_text\n\n    # Some datasets (e.g., CWMT) contain multiple parallel files specified with\n    # a wildcard. We sort both sets to align them and parse them one by one.\n    f1_files = sorted(glob.glob(f1))\n    f2_files = sorted(glob.glob(f2))\n\n    assert f1_files and f2_files, ""No matching files found: %s, %s."" % (f1, f2)\n    assert len(f1_files) == len(f2_files), ""Number of files do not match: %d vs %d for %s vs %s."" % (\n        len(f1_files),\n        len(f2_files),\n        f1,\n        f2,\n    )\n\n    for f_id, (f1_i, f2_i) in enumerate(zip(sorted(f1_files), sorted(f2_files))):\n        l1_sentences, l1 = parse_file(f1_i)\n        l2_sentences, l2 = parse_file(f2_i)\n\n        assert len(l1_sentences) == len(l2_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(l1_sentences),\n            len(l2_sentences),\n            f1_i,\n            f2_i,\n        )\n\n        for line_id, (s1, s2) in enumerate(zip(l1_sentences, l2_sentences)):\n            key = ""{}/{}"".format(f_id, line_id)\n            yield key, {l1: s1, l2: s2}\n\n\ndef _parse_frde_bitext(fr_path, de_path):\n    with open(fr_path) as f:\n        fr_sentences = f.read().split(""\\n"")\n    with open(de_path) as f:\n        de_sentences = f.read().split(""\\n"")\n    assert len(fr_sentences) == len(de_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n        len(fr_sentences),\n        len(de_sentences),\n        fr_path,\n        de_path,\n    )\n    for line_id, (s1, s2) in enumerate(zip(fr_sentences, de_sentences)):\n        yield line_id, {""fr"": s1, ""de"": s2}\n\n\ndef _parse_tmx(path):\n    """"""Generates examples from TMX file.""""""\n\n    def _get_tuv_lang(tuv):\n        for k, v in tuv.items():\n            if k.endswith(""}lang""):\n                return v\n        raise AssertionError(""Language not found in `tuv` attributes."")\n\n    def _get_tuv_seg(tuv):\n        segs = tuv.findall(""seg"")\n        assert len(segs) == 1, ""Invalid number of segments: %d"" % len(segs)\n        return segs[0].text\n\n    with open(path, ""rb"") as f:\n        if six.PY3:\n            # Workaround due to: https://github.com/tensorflow/tensorflow/issues/33563\n            utf_f = codecs.getreader(""utf-8"")(f)\n        else:\n            utf_f = f\n        for line_id, (_, elem) in enumerate(ElementTree.iterparse(utf_f)):\n            if elem.tag == ""tu"":\n                yield line_id, {_get_tuv_lang(tuv): _get_tuv_seg(tuv) for tuv in elem.iterfind(""tuv"")}\n                elem.clear()\n\n\ndef _parse_tsv(path, language_pair=None):\n    """"""Generates examples from TSV file.""""""\n    if language_pair is None:\n        lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])\\.tsv"", path)\n        assert lang_match is not None, ""Invalid TSV filename: %s"" % path\n        l1, l2 = lang_match.groups()\n    else:\n        l1, l2 = language_pair\n    with open(path) as f:\n        for j, line in enumerate(f):\n            cols = line.split(""\\t"")\n            if len(cols) != 2:\n                logging.warning(""Skipping line %d in TSV (%s) with %d != 2 columns."", j, path, len(cols))\n                continue\n            s1, s2 = cols\n            yield j, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_wikiheadlines(path):\n    """"""Generates examples from Wikiheadlines dataset file.""""""\n    lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])$"", path)\n    assert lang_match is not None, ""Invalid Wikiheadlines filename: %s"" % path\n    l1, l2 = lang_match.groups()\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            s1, s2 = line.split(""|||"")\n            yield line_id, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_czeng(*paths, **kwargs):\n    """"""Generates examples from CzEng v1.6, with optional filtering for v1.7.""""""\n    filter_path = kwargs.get(""filter_path"", None)\n    if filter_path:\n        re_block = re.compile(r""^[^-]+-b(\\d+)-\\d\\d[tde]"")\n        with open(filter_path) as f:\n            bad_blocks = {blk for blk in re.search(r""qw{([\\s\\d]*)}"", f.read()).groups()[0].split()}\n        logging.info(""Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7."", len(bad_blocks))\n\n    for path in paths:\n        for gz_path in sorted(glob.glob(path)):\n            with open(gz_path, ""rb"") as g, gzip.GzipFile(fileobj=g) as f:\n                filename = os.path.basename(gz_path)\n                for line_id, line in enumerate(f):\n                    line = line.decode(""utf-8"")  # required for py3\n                    if not line.strip():\n                        continue\n                    id_, unused_score, cs, en = line.split(""\\t"")\n                    if filter_path:\n                        block_match = re.match(re_block, id_)\n                        if block_match and block_match.groups()[0] in bad_blocks:\n                            continue\n                    sub_key = ""{}/{}"".format(filename, line_id)\n                    yield sub_key, {\n                        ""cs"": cs.strip(),\n                        ""en"": en.strip(),\n                    }\n\n\ndef _parse_hindencorp(path):\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            split_line = line.split(""\\t"")\n            if len(split_line) != 5:\n                logging.warning(""Skipping invalid HindEnCorp line: %s"", line)\n                continue\n            yield line_id, {""translation"": {""en"": split_line[3].strip(), ""hi"": split_line[4].strip()}}\n'"
datasets/wmt16/wmt16.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT16: Translate dataset.""""""\n\nimport nlp\n\nfrom .wmt_utils import Wmt, WmtConfig\n\n\n_URL = ""http://www.statmt.org/wmt16/translation-task.html""\n_CITATION = """"""\n@InProceedings{bojar-EtAl:2016:WMT1,\n  author    = {Bojar, Ond\\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and  Jimeno Yepes, Antonio  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Neveol, Aurelie  and  Neves, Mariana  and  Popel, Martin  and  Post, Matt  and  Rubino, Raphael  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco  and  Verspoor, Karin  and  Zampieri, Marcos},\n  title     = {Findings of the 2016 Conference on Machine Translation},\n  booktitle = {Proceedings of the First Conference on Machine Translation},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {131--198},\n  url       = {http://www.aclweb.org/anthology/W/W16/W16-2301}\n}\n""""""\n\n_LANGUAGE_PAIRS = [(lang, ""en"") for lang in [""cs"", ""de"", ""fi"", ""ro"", ""ru"", ""tr""]]\n\n\nclass Wmt16(Wmt):\n    """"""WMT 16 translation datasets for all {xx, ""en""} language pairs.""""""\n\n    BUILDER_CONFIGS = [\n        WmtConfig(  # pylint:disable=g-complex-comprehension\n            description=""WMT 2016 %s-%s translation task dataset."" % (l1, l2),\n            url=_URL,\n            citation=_CITATION,\n            language_pair=(l1, l2),\n            version=nlp.Version(""1.0.0""),\n        )\n        for l1, l2 in _LANGUAGE_PAIRS\n    ]\n\n    @property\n    def _subsets(self):\n        return {\n            nlp.Split.TRAIN: [\n                ""europarl_v7"",\n                ""europarl_v8_16"",\n                ""commoncrawl"",\n                ""newscommentary_v11"",\n                ""czeng_16pre"",\n                ""yandexcorpus"",\n                ""wikiheadlines_fi"",\n                ""wikiheadlines_ru"",\n                ""setimes_2"",\n            ],\n            nlp.Split.VALIDATION: [""newsdev2016"", ""newstest2015""],\n            nlp.Split.TEST: [""newstest2016"", ""newstestB2016""],\n        }\n'"
datasets/wmt16/wmt_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT: Translate dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport codecs\nimport functools\nimport glob\nimport gzip\nimport itertools\nimport logging\nimport os\nimport re\nimport xml.etree.cElementTree as ElementTree\nfrom abc import ABC, abstractmethod\n\nimport six\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nTranslate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `nlp.translate.wmt.WmtConfig`.\n\n```\nconfig = nlp.wmt.WmtConfig(\n    version=""0.0.1"",\n    language_pair=(""fr"", ""de""),\n    subsets={\n        nlp.Split.TRAIN: [""commoncrawl_frde""],\n        nlp.Split.VALIDATION: [""euelections_dev2019""],\n    },\n)\nbuilder = nlp.builder(""wmt_translate"", config=config)\n```\n\n""""""\n\n\nCWMT_SUBSET_NAMES = [""casia2015"", ""casict2011"", ""casict2015"", ""datum2015"", ""datum2017"", ""neu2017""]\n\n\nclass SubDataset(object):\n    """"""Class to keep track of information on a sub-dataset of WMT.""""""\n\n    def __init__(self, name, target, sources, url, path, manual_dl_files=None):\n        """"""Sub-dataset of WMT.\n\n    Args:\n      name: `string`, a unique dataset identifier.\n      target: `string`, the target language code.\n      sources: `set<string>`, the set of source language codes.\n      url: `string` or `(string, string)`, URL(s) or URL template(s) specifying\n        where to download the raw data from. If two strings are provided, the\n        first is used for the source language and the second for the target.\n        Template strings can either contain \'{src}\' placeholders that will be\n        filled in with the source language code, \'{0}\' and \'{1}\' placeholders\n        that will be filled in with the source and target language codes in\n        alphabetical order, or all 3.\n      path: `string` or `(string, string)`, path(s) or path template(s)\n        specifing the path to the raw data relative to the root of the\n        downloaded archive. If two strings are provided, the dataset is assumed\n        to be made up of parallel text files, the first being the source and the\n        second the target. If one string is provided, both languages are assumed\n        to be stored within the same file and the extension is used to determine\n        how to parse it. Template strings should be formatted the same as in\n        `url`.\n      manual_dl_files: `<list>(string)` (optional), the list of files that must\n        be manually downloaded to the data directory.\n    """"""\n        self._paths = (path,) if isinstance(path, six.string_types) else path\n        self._urls = (url,) if isinstance(url, six.string_types) else url\n        self._manual_dl_files = manual_dl_files if manual_dl_files else []\n        self.name = name\n        self.target = target\n        self.sources = set(sources)\n\n    def _inject_language(self, src, strings):\n        """"""Injects languages into (potentially) template strings.""""""\n        if src not in self.sources:\n            raise ValueError(""Invalid source for \'{0}\': {1}"".format(self.name, src))\n\n        def _format_string(s):\n            if ""{0}"" in s and ""{1}"" and ""{src}"" in s:\n                return s.format(*sorted([src, self.target]), src=src)\n            elif ""{0}"" in s and ""{1}"" in s:\n                return s.format(*sorted([src, self.target]))\n            elif ""{src}"" in s:\n                return s.format(src=src)\n            else:\n                return s\n\n        return [_format_string(s) for s in strings]\n\n    def get_url(self, src):\n        return self._inject_language(src, self._urls)\n\n    def get_manual_dl_files(self, src):\n        return self._inject_language(src, self._manual_dl_files)\n\n    def get_path(self, src):\n        return self._inject_language(src, self._paths)\n\n\n# Subsets used in the training sets for various years of WMT.\n_TRAIN_SUBSETS = [\n    # pylint:disable=line-too-long\n    SubDataset(\n        name=""commoncrawl"",\n        target=""en"",  # fr-de pair in commoncrawl_frde\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n        path=(""commoncrawl.{src}-en.{src}"", ""commoncrawl.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""commoncrawl_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""czeng_10"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng10"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""czeng_16pre"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng16pre"",\n        manual_dl_files=[""czeng16pre.deduped-ignoring-sections.txt.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""czeng_16"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        # This dataset differs from the above in the filtering that is applied\n        # during parsing.\n        name=""czeng_17"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""dcep_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/dcep.lv-en.v1.tgz"",\n        path=(""dcep.en-lv/dcep.lv"", ""dcep.en-lv/dcep.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n        path=(""training/europarl-v7.{src}-en.{src}"", ""training/europarl-v7.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""europarl_v8_18"",\n        target=""en"",\n        sources={""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training/europarl-v8.{src}-en.{src}"", ""training/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v8_16"",\n        target=""en"",\n        sources={""fi"", ""ro""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training-parallel-ep-v8/europarl-v8.{src}-en.{src}"", ""training-parallel-ep-v8/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""http://www.statmt.org/europarl/v9/training/europarl-v9.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""gigafren"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://www.statmt.org/wmt10/training-giga-fren.tar"",\n        path=(""giga-fren.release2.fixed.fr.gz"", ""giga-fren.release2.fixed.en.gz""),\n    ),\n    SubDataset(\n        name=""hindencorp_01"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://ufallab.ms.mff.cuni.cz/~bojar/hindencorp"",\n        manual_dl_files=[""hindencorp0.1.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""leta_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/leta.v1.tgz"",\n        path=(""LETA-lv-en/leta.lv"", ""LETA-lv-en/leta.en""),\n    ),\n    SubDataset(\n        name=""multiun"",\n        target=""en"",\n        sources={""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-un.tgz"",\n        path=(""un/undoc.2000.{src}-en.{src}"", ""un/undoc.2000.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz"",\n        path=(""training/news-commentary-v9.{src}-en.{src}"", ""training/news-commentary-v9.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v10"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz"",\n        path=(""news-commentary-v10.{src}-en.{src}"", ""news-commentary-v10.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v11"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz"",\n        path=(\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.{src}"",\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v12"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"",\n        path=(""training/news-commentary-v12.{src}-en.{src}"", ""training/news-commentary-v12.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v13"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"",\n        path=(\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.{src}"",\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v14"",\n        target=""en"",  # fr-de pair in newscommentary_v14_frde\n        sources={""cs"", ""de"", ""kk"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.{0}-{1}.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""newscommentary_v14_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-fr.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""onlinebooks_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/books.lv-en.v1.tgz"",\n        path=(""farewell/farewell.lv"", ""farewell/farewell.en""),\n    ),\n    SubDataset(\n        name=""paracrawl_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-{src}.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.{src}"",\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v1_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.ru"",\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v3"",\n        target=""en"",  # fr-de pair in paracrawl_v3_frde\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release3/en-{src}.bicleaner07.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""paracrawl_v3_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""rapid_2016"",\n        target=""en"",\n        sources={""de"", ""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/rapid2016.tgz"",\n        path=(""rapid2016.{0}-{1}.{src}"", ""rapid2016.{0}-{1}.en""),\n    ),\n    SubDataset(\n        name=""rapid_2016_ltfi"",\n        target=""en"",\n        sources={""fi"", ""lt""},\n        url=""https://tilde-model.s3-eu-west-1.amazonaws.com/rapid2016.en-{src}.tmx.zip"",\n        path=""rapid2016.en-{src}.tmx"",\n    ),\n    SubDataset(\n        name=""rapid_2019"",\n        target=""en"",\n        sources={""de""},\n        url=""https://s3-eu-west-1.amazonaws.com/tilde-model/rapid2019.de-en.zip"",\n        path=(""rapid2019.de-en.de"", ""rapid2019.de-en.en""),\n    ),\n    SubDataset(\n        name=""setimes_2"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-{src}.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""uncorpus_v1"",\n        target=""en"",\n        sources={""ru"", ""zh""},\n        url=""https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-{src}.tar.gz"",\n        path=(""en-{src}/UNv1.0.en-{src}.{src}"", ""en-{src}/UNv1.0.en-{src}.en""),\n    ),\n    SubDataset(\n        name=""wikiheadlines_fi"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/fi-en/titles.fi-en"",\n    ),\n    SubDataset(\n        name=""wikiheadlines_hi"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://www.statmt.org/wmt14/wiki-titles.tgz"",\n        path=""wiki/hi-en/wiki-titles.hi-en"",\n    ),\n    SubDataset(\n        # Verified that wmt14 and wmt15 files are identical.\n        name=""wikiheadlines_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/ru-en/wiki.ru-en"",\n    ),\n    SubDataset(\n        name=""wikititles_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""gu"", ""kk"", ""lt"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wikititles/v1/wikititles-v1.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""yandexcorpus"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://translate.yandex.ru/corpus?lang=en"",\n        manual_dl_files=[""1mcorpus.zip""],\n        path=(""corpus.en_ru.1m.ru"", ""corpus.en_ru.1m.en""),\n    ),\n    # pylint:enable=line-too-long\n] + [\n    SubDataset(  # pylint:disable=g-complex-comprehension\n        name=ss,\n        target=""en"",\n        sources={""zh""},\n        url=""ftp://cwmt-wmt:cwmt-wmt@nlp.nju.edu.cn/parallel/%s.zip"" % ss,\n        path=(""%s/*_c[hn].txt"" % ss, ""%s/*_en.txt"" % ss),\n    )\n    for ss in CWMT_SUBSET_NAMES\n]\n\n_DEV_SUBSETS = [\n    SubDataset(\n        name=""euelections_dev2019"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/euelections_dev2019.fr-de.src.fr"", ""dev/euelections_dev2019.fr-de.tgt.de""),\n    ),\n    SubDataset(\n        name=""newsdev2014"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2014.hi"", ""dev/newsdev2014.en""),\n    ),\n    SubDataset(\n        name=""newsdev2015"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2015-fien-src.{src}.sgm"", ""dev/newsdev2015-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2016"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2016-{src}en-src.{src}.sgm"", ""dev/newsdev2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2017"",\n        target=""en"",\n        sources={""lv"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2017-{src}en-src.{src}.sgm"", ""dev/newsdev2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2018"",\n        target=""en"",\n        sources={""et""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2018-{src}en-src.{src}.sgm"", ""dev/newsdev2018-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2019"",\n        target=""en"",\n        sources={""gu"", ""kk"", ""lt""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2019-{src}en-src.{src}.sgm"", ""dev/newsdev2019-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newssyscomb2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newssyscomb2009.{src}"", ""dev/newssyscomb2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2008"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hu""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/news-test2008.{src}"", ""dev/news-test2008.en""),\n    ),\n    SubDataset(\n        name=""newstest2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2009.{src}"", ""dev/newstest2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2010"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2010.{src}"", ""dev/newstest2010.en""),\n    ),\n    SubDataset(\n        name=""newstest2011"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2011.{src}"", ""dev/newstest2011.en""),\n    ),\n    SubDataset(\n        name=""newstest2012"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2012.{src}"", ""dev/newstest2012.en""),\n    ),\n    SubDataset(\n        name=""newstest2013"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2013.{src}"", ""dev/newstest2013.en""),\n    ),\n    SubDataset(\n        name=""newstest2014"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2014-{src}en-src.{src}.sgm"", ""dev/newstest2014-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2015"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2015-{src}en-src.{src}.sgm"", ""dev/newstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2016"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ro"", ""ru"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2016-{src}en-src.{src}.sgm"", ""dev/newstest2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2016"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2016-enfi-ref.{src}.sgm"", ""dev/newstestB2016-enfi-src.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2017"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lv"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2017-{src}en-src.{src}.sgm"", ""dev/newstest2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2017"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2017-fien-src.fi.sgm"", ""dev/newstestB2017-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2018"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2018-{src}en-src.{src}.sgm"", ""dev/newstest2018-{src}en-ref.en.sgm""),\n    ),\n]\n\nDATASET_MAP = {dataset.name: dataset for dataset in _TRAIN_SUBSETS + _DEV_SUBSETS}\n\n_CZENG17_FILTER = SubDataset(\n    name=""czeng17_filter"",\n    target=""en"",\n    sources={""cs""},\n    url=""http://ufal.mff.cuni.cz/czeng/download.php?f=convert_czeng16_to_17.pl.zip"",\n    path=""convert_czeng16_to_17.pl"",\n)\n\n\nclass WmtConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for WMT.""""""\n\n    def __init__(self, url=None, citation=None, description=None, language_pair=(None, None), subsets=None, **kwargs):\n        """"""BuilderConfig for WMT.\n\n    Args:\n      url: The reference URL for the dataset.\n      citation: The paper citation for the dataset.\n      description: The description of the dataset.\n      language_pair: pair of languages that will be used for translation. Should\n                 contain 2 letter coded strings. For example: (""en"", ""de"").\n        configuration for the `nlp.features.text.TextEncoder` used for the\n        `nlp.features.text.Translation` features.\n      subsets: Dict[split, list[str]]. List of the subset to use for each of the\n        split. Note that WMT subclasses overwrite this parameter.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s-%s"" % (language_pair[0], language_pair[1])\n        if ""name"" in kwargs:  # Add name suffix for custom configs\n            name += ""."" + kwargs.pop(""name"")\n\n        super(WmtConfig, self).__init__(name=name, description=description, **kwargs)\n\n        self.url = url or ""http://www.statmt.org""\n        self.citation = citation\n        self.language_pair = language_pair\n        self.subsets = subsets\n\n        # TODO(PVP): remove when manual dir works\n        # +++++++++++++++++++++\n        if language_pair[1] in [""cs"", ""hi"", ""ru""]:\n            assert NotImplementedError(\n                ""The dataset for {}-en is currently not fully supported."".format(language_pair[1])\n            )\n        # +++++++++++++++++++++\n\n\nclass Wmt(ABC, nlp.GeneratorBasedBuilder):\n    """"""WMT translation dataset.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Some of the wmt configs here, require a manual download.\n  Please look into wmt.py to see the exact path (and file name) that has to\n  be downloaded.\n  """"""\n\n    def __init__(self, *args, **kwargs):\n        if type(self) == Wmt and ""config"" not in kwargs:  # pylint: disable=unidiomatic-typecheck\n            raise ValueError(\n                ""The raw `wmt_translate` can only be instantiated with the config ""\n                ""kwargs. You may want to use one of the `wmtYY_translate` ""\n                ""implementation instead to get the WMT dataset for a specific year.""\n            )\n        super(Wmt, self).__init__(*args, **kwargs)\n\n    @property\n    @abstractmethod\n    def _subsets(self):\n        """"""Subsets that make up each split of the dataset.""""""\n        raise NotImplementedError(""This is a abstract method"")\n\n    @property\n    def subsets(self):\n        """"""Subsets that make up each split of the dataset for the language pair.""""""\n        source, target = self.config.language_pair\n        filtered_subsets = {}\n        for split, ss_names in self._subsets.items():\n            filtered_subsets[split] = []\n            for ss_name in ss_names:\n                dataset = DATASET_MAP[ss_name]\n                if dataset.target != target or source not in dataset.sources:\n                    logging.info(""Skipping sub-dataset that does not include language pair: %s"", ss_name)\n                else:\n                    filtered_subsets[split].append(ss_name)\n        logging.info(""Using sub-datasets: %s"", filtered_subsets)\n        return filtered_subsets\n\n    def _info(self):\n        src, target = self.config.language_pair\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            supervised_keys=(src, target),\n            homepage=self.config.url,\n            citation=self.config.citation,\n        )\n\n    def _vocab_text_gen(self, split_subsets, extraction_map, language):\n        for _, ex in self._generate_examples(split_subsets, extraction_map, with_translation=False):\n            yield ex[language]\n\n    def _split_generators(self, dl_manager):\n        source, _ = self.config.language_pair\n        manual_paths_dict = {}\n        urls_to_download = {}\n        for ss_name in itertools.chain.from_iterable(self.subsets.values()):\n            if ss_name == ""czeng_17"":\n                # CzEng1.7 is CzEng1.6 with some blocks filtered out. We must download\n                # the filtering script so we can parse out which blocks need to be\n                # removed.\n                urls_to_download[_CZENG17_FILTER.name] = _CZENG17_FILTER.get_url(source)\n\n            # get dataset\n            dataset = DATASET_MAP[ss_name]\n            if dataset.get_manual_dl_files(source):\n                # TODO(PVP): following two lines skip configs that are incomplete for now\n                # +++++++++++++++++++++\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n                # +++++++++++++++++++++\n\n                manual_dl_files = dataset.get_manual_dl_files(source)\n                manual_paths = [\n                    os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), fname)\n                    for fname in manual_dl_files\n                ]\n                assert all(\n                    os.path.exists(path) for path in manual_paths\n                ), ""For {0}, you must manually download the following file(s) from {1} and place them in {2}: {3}"".format(\n                    dataset.name, dataset.get_url(source), dl_manager.manual_dir, "", "".join(manual_dl_files)\n                )\n\n                # set manual path for correct subset\n                manual_paths_dict[ss_name] = manual_paths\n            else:\n                urls_to_download[ss_name] = dataset.get_url(source)\n\n        # Download and extract files from URLs.\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n        # Extract manually downloaded files.\n        manual_files = dl_manager.extract(manual_paths_dict)\n        extraction_map = dict(downloaded_files, **manual_files)\n\n        for language in self.config.language_pair:\n            self._vocab_text_gen(self.subsets[nlp.Split.TRAIN], extraction_map, language)\n\n        return [\n            nlp.SplitGenerator(  # pylint:disable=g-complex-comprehension\n                name=split, gen_kwargs={""split_subsets"": split_subsets, ""extraction_map"": extraction_map}\n            )\n            for split, split_subsets in self.subsets.items()\n        ]\n\n    def _generate_examples(self, split_subsets, extraction_map, with_translation=True):\n        """"""Returns the examples in the raw (text) form.""""""\n        source, _ = self.config.language_pair\n\n        def _get_local_paths(dataset, extract_dirs):\n            rel_paths = dataset.get_path(source)\n            if len(extract_dirs) == 1:\n                extract_dirs = extract_dirs * len(rel_paths)\n            return [\n                os.path.join(ex_dir, rel_path) if rel_path else ex_dir\n                for ex_dir, rel_path in zip(extract_dirs, rel_paths)\n            ]\n\n        for ss_name in split_subsets:\n            # TODO(PVP) remove following five lines when manual data works\n            # +++++++++++++++++++++\n            dataset = DATASET_MAP[ss_name]\n            source, _ = self.config.language_pair\n            if dataset.get_manual_dl_files(source):\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n            # +++++++++++++++++++++\n\n            logging.info(""Generating examples from: %s"", ss_name)\n            dataset = DATASET_MAP[ss_name]\n            extract_dirs = extraction_map[ss_name]\n            files = _get_local_paths(dataset, extract_dirs)\n\n            if ss_name.startswith(""czeng""):\n                if ss_name.endswith(""16pre""):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=(""en"", ""cs""))\n                elif ss_name.endswith(""17""):\n                    filter_path = _get_local_paths(_CZENG17_FILTER, extraction_map[_CZENG17_FILTER.name])[0]\n                    sub_generator = functools.partial(_parse_czeng, filter_path=filter_path)\n                else:\n                    sub_generator = _parse_czeng\n            elif ss_name == ""hindencorp_01"":\n                sub_generator = _parse_hindencorp\n            elif len(files) == 2:\n                if ss_name.endswith(""_frde""):\n                    sub_generator = _parse_frde_bitext\n                else:\n                    sub_generator = _parse_parallel_sentences\n            elif len(files) == 1:\n                fname = files[0]\n                # Note: Due to formatting used by `download_manager`, the file\n                # extension may not be at the end of the file path.\n                if "".tsv"" in fname:\n                    sub_generator = _parse_tsv\n                elif (\n                    ss_name.startswith(""newscommentary_v14"")\n                    or ss_name.startswith(""europarl_v9"")\n                    or ss_name.startswith(""wikititles_v1"")\n                ):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=self.config.language_pair)\n                elif ""tmx"" in fname or ss_name.startswith(""paracrawl_v3""):\n                    sub_generator = _parse_tmx\n                elif ss_name.startswith(""wikiheadlines""):\n                    sub_generator = _parse_wikiheadlines\n                else:\n                    raise ValueError(""Unsupported file format: %s"" % fname)\n            else:\n                raise ValueError(""Invalid number of files: %d"" % len(files))\n\n            for sub_key, ex in sub_generator(*files):\n                if not all(ex.values()):\n                    continue\n                # TODO(adarob): Add subset feature.\n                # ex[""subset""] = subset\n                key = ""{}/{}"".format(ss_name, sub_key)\n                if with_translation is True:\n                    ex = {""translation"": ex}\n                yield key, ex\n\n\ndef _parse_parallel_sentences(f1, f2):\n    """"""Returns examples from parallel SGML or text files, which may be gzipped.""""""\n\n    def _parse_text(path):\n        """"""Returns the sentences from a single text file, which may be gzipped.""""""\n        split_path = path.split(""."")\n\n        if split_path[-1] == ""gz"":\n            lang = split_path[-2]\n            with open(path, ""rb"") as f, gzip.GzipFile(fileobj=f) as g:\n                return g.read().decode(""utf-8"").split(""\\n""), lang\n\n        if split_path[-1] == ""txt"":\n            # CWMT\n            lang = split_path[-2].split(""_"")[-1]\n            lang = ""zh"" if lang in (""ch"", ""cn"") else lang\n        else:\n            lang = split_path[-1]\n        with open(path, ""rb"") as f:\n            return f.read().decode(""utf-8"").split(""\\n""), lang\n\n    def _parse_sgm(path):\n        """"""Returns sentences from a single SGML file.""""""\n        lang = path.split(""."")[-2]\n        sentences = []\n        # Note: We can\'t use the XML parser since some of the files are badly\n        # formatted.\n        seg_re = re.compile(r""<seg id=\\""\\d+\\"">(.*)</seg>"")\n        with open(path) as f:\n            for line in f:\n                seg_match = re.match(seg_re, line)\n                if seg_match:\n                    assert len(seg_match.groups()) == 1\n                    sentences.append(seg_match.groups()[0])\n        return sentences, lang\n\n    parse_file = _parse_sgm if f1.endswith("".sgm"") else _parse_text\n\n    # Some datasets (e.g., CWMT) contain multiple parallel files specified with\n    # a wildcard. We sort both sets to align them and parse them one by one.\n    f1_files = sorted(glob.glob(f1))\n    f2_files = sorted(glob.glob(f2))\n\n    assert f1_files and f2_files, ""No matching files found: %s, %s."" % (f1, f2)\n    assert len(f1_files) == len(f2_files), ""Number of files do not match: %d vs %d for %s vs %s."" % (\n        len(f1_files),\n        len(f2_files),\n        f1,\n        f2,\n    )\n\n    for f_id, (f1_i, f2_i) in enumerate(zip(sorted(f1_files), sorted(f2_files))):\n        l1_sentences, l1 = parse_file(f1_i)\n        l2_sentences, l2 = parse_file(f2_i)\n\n        assert len(l1_sentences) == len(l2_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(l1_sentences),\n            len(l2_sentences),\n            f1_i,\n            f2_i,\n        )\n\n        for line_id, (s1, s2) in enumerate(zip(l1_sentences, l2_sentences)):\n            key = ""{}/{}"".format(f_id, line_id)\n            yield key, {l1: s1, l2: s2}\n\n\ndef _parse_frde_bitext(fr_path, de_path):\n    with open(fr_path) as f:\n        fr_sentences = f.read().split(""\\n"")\n    with open(de_path) as f:\n        de_sentences = f.read().split(""\\n"")\n    assert len(fr_sentences) == len(de_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n        len(fr_sentences),\n        len(de_sentences),\n        fr_path,\n        de_path,\n    )\n    for line_id, (s1, s2) in enumerate(zip(fr_sentences, de_sentences)):\n        yield line_id, {""fr"": s1, ""de"": s2}\n\n\ndef _parse_tmx(path):\n    """"""Generates examples from TMX file.""""""\n\n    def _get_tuv_lang(tuv):\n        for k, v in tuv.items():\n            if k.endswith(""}lang""):\n                return v\n        raise AssertionError(""Language not found in `tuv` attributes."")\n\n    def _get_tuv_seg(tuv):\n        segs = tuv.findall(""seg"")\n        assert len(segs) == 1, ""Invalid number of segments: %d"" % len(segs)\n        return segs[0].text\n\n    with open(path, ""rb"") as f:\n        if six.PY3:\n            # Workaround due to: https://github.com/tensorflow/tensorflow/issues/33563\n            utf_f = codecs.getreader(""utf-8"")(f)\n        else:\n            utf_f = f\n        for line_id, (_, elem) in enumerate(ElementTree.iterparse(utf_f)):\n            if elem.tag == ""tu"":\n                yield line_id, {_get_tuv_lang(tuv): _get_tuv_seg(tuv) for tuv in elem.iterfind(""tuv"")}\n                elem.clear()\n\n\ndef _parse_tsv(path, language_pair=None):\n    """"""Generates examples from TSV file.""""""\n    if language_pair is None:\n        lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])\\.tsv"", path)\n        assert lang_match is not None, ""Invalid TSV filename: %s"" % path\n        l1, l2 = lang_match.groups()\n    else:\n        l1, l2 = language_pair\n    with open(path) as f:\n        for j, line in enumerate(f):\n            cols = line.split(""\\t"")\n            if len(cols) != 2:\n                logging.warning(""Skipping line %d in TSV (%s) with %d != 2 columns."", j, path, len(cols))\n                continue\n            s1, s2 = cols\n            yield j, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_wikiheadlines(path):\n    """"""Generates examples from Wikiheadlines dataset file.""""""\n    lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])$"", path)\n    assert lang_match is not None, ""Invalid Wikiheadlines filename: %s"" % path\n    l1, l2 = lang_match.groups()\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            s1, s2 = line.split(""|||"")\n            yield line_id, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_czeng(*paths, **kwargs):\n    """"""Generates examples from CzEng v1.6, with optional filtering for v1.7.""""""\n    filter_path = kwargs.get(""filter_path"", None)\n    if filter_path:\n        re_block = re.compile(r""^[^-]+-b(\\d+)-\\d\\d[tde]"")\n        with open(filter_path) as f:\n            bad_blocks = {blk for blk in re.search(r""qw{([\\s\\d]*)}"", f.read()).groups()[0].split()}\n        logging.info(""Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7."", len(bad_blocks))\n\n    for path in paths:\n        for gz_path in sorted(glob.glob(path)):\n            with open(gz_path, ""rb"") as g, gzip.GzipFile(fileobj=g) as f:\n                filename = os.path.basename(gz_path)\n                for line_id, line in enumerate(f):\n                    line = line.decode(""utf-8"")  # required for py3\n                    if not line.strip():\n                        continue\n                    id_, unused_score, cs, en = line.split(""\\t"")\n                    if filter_path:\n                        block_match = re.match(re_block, id_)\n                        if block_match and block_match.groups()[0] in bad_blocks:\n                            continue\n                    sub_key = ""{}/{}"".format(filename, line_id)\n                    yield sub_key, {\n                        ""cs"": cs.strip(),\n                        ""en"": en.strip(),\n                    }\n\n\ndef _parse_hindencorp(path):\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            split_line = line.split(""\\t"")\n            if len(split_line) != 5:\n                logging.warning(""Skipping invalid HindEnCorp line: %s"", line)\n                continue\n            yield line_id, {""translation"": {""en"": split_line[3].strip(), ""hi"": split_line[4].strip()}}\n'"
datasets/wmt17/wmt17.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT17: Translate dataset.""""""\n\nimport nlp\n\nfrom .wmt_utils import CWMT_SUBSET_NAMES, Wmt, WmtConfig\n\n\n_URL = ""http://www.statmt.org/wmt17/translation-task.html""\n_CITATION = """"""\n@InProceedings{bojar-EtAl:2017:WMT1,\n  author    = {Bojar, Ond\\v{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huang, Shujian  and  Huck, Matthias  and  Koehn, Philipp  and  Liu, Qun  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Post, Matt  and  Rubino, Raphael  and  Specia, Lucia  and  Turchi, Marco},\n  title     = {Findings of the 2017 Conference on Machine Translation (WMT17)},\n  booktitle = {Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers},\n  month     = {September},\n  year      = {2017},\n  address   = {Copenhagen, Denmark},\n  publisher = {Association for Computational Linguistics},\n  pages     = {169--214},\n  url       = {http://www.aclweb.org/anthology/W17-4717}\n}\n""""""\n\n_LANGUAGE_PAIRS = [(lang, ""en"") for lang in [""cs"", ""de"", ""fi"", ""lv"", ""ru"", ""tr"", ""zh""]]\n\n\nclass Wmt17(Wmt):\n    """"""WMT 17 translation datasets for all {xx, ""en""} language pairs.""""""\n\n    BUILDER_CONFIGS = [\n        WmtConfig(  # pylint:disable=g-complex-comprehension\n            description=""WMT 2017 %s-%s translation task dataset."" % (l1, l2),\n            url=_URL,\n            citation=_CITATION,\n            language_pair=(l1, l2),\n            version=nlp.Version(""1.0.0""),\n        )\n        for l1, l2 in _LANGUAGE_PAIRS\n    ]\n\n    @property\n    def _subsets(self):\n        return {\n            nlp.Split.TRAIN: [\n                ""europarl_v7"",\n                ""europarl_v8_16"",\n                ""commoncrawl"",\n                ""newscommentary_v12"",\n                ""czeng_16"",\n                ""yandexcorpus"",\n                ""wikiheadlines_fi"",\n                ""wikiheadlines_ru"",\n                ""setimes_2"",\n                ""uncorpus_v1"",\n                ""rapid_2016"",\n                ""leta_v1"",\n                ""dcep_v1"",\n                ""onlinebooks_v1"",\n            ]\n            + CWMT_SUBSET_NAMES,\n            nlp.Split.VALIDATION: [""newsdev2017"", ""newstest2016"", ""newstestB2016""],\n            nlp.Split.TEST: [""newstest2017"", ""newstestB2017""],\n        }\n'"
datasets/wmt17/wmt_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT: Translate dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport codecs\nimport functools\nimport glob\nimport gzip\nimport itertools\nimport logging\nimport os\nimport re\nimport xml.etree.cElementTree as ElementTree\nfrom abc import ABC, abstractmethod\n\nimport six\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nTranslate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `nlp.translate.wmt.WmtConfig`.\n\n```\nconfig = nlp.wmt.WmtConfig(\n    version=""0.0.1"",\n    language_pair=(""fr"", ""de""),\n    subsets={\n        nlp.Split.TRAIN: [""commoncrawl_frde""],\n        nlp.Split.VALIDATION: [""euelections_dev2019""],\n    },\n)\nbuilder = nlp.builder(""wmt_translate"", config=config)\n```\n\n""""""\n\n\nCWMT_SUBSET_NAMES = [""casia2015"", ""casict2011"", ""casict2015"", ""datum2015"", ""datum2017"", ""neu2017""]\n\n\nclass SubDataset(object):\n    """"""Class to keep track of information on a sub-dataset of WMT.""""""\n\n    def __init__(self, name, target, sources, url, path, manual_dl_files=None):\n        """"""Sub-dataset of WMT.\n\n    Args:\n      name: `string`, a unique dataset identifier.\n      target: `string`, the target language code.\n      sources: `set<string>`, the set of source language codes.\n      url: `string` or `(string, string)`, URL(s) or URL template(s) specifying\n        where to download the raw data from. If two strings are provided, the\n        first is used for the source language and the second for the target.\n        Template strings can either contain \'{src}\' placeholders that will be\n        filled in with the source language code, \'{0}\' and \'{1}\' placeholders\n        that will be filled in with the source and target language codes in\n        alphabetical order, or all 3.\n      path: `string` or `(string, string)`, path(s) or path template(s)\n        specifing the path to the raw data relative to the root of the\n        downloaded archive. If two strings are provided, the dataset is assumed\n        to be made up of parallel text files, the first being the source and the\n        second the target. If one string is provided, both languages are assumed\n        to be stored within the same file and the extension is used to determine\n        how to parse it. Template strings should be formatted the same as in\n        `url`.\n      manual_dl_files: `<list>(string)` (optional), the list of files that must\n        be manually downloaded to the data directory.\n    """"""\n        self._paths = (path,) if isinstance(path, six.string_types) else path\n        self._urls = (url,) if isinstance(url, six.string_types) else url\n        self._manual_dl_files = manual_dl_files if manual_dl_files else []\n        self.name = name\n        self.target = target\n        self.sources = set(sources)\n\n    def _inject_language(self, src, strings):\n        """"""Injects languages into (potentially) template strings.""""""\n        if src not in self.sources:\n            raise ValueError(""Invalid source for \'{0}\': {1}"".format(self.name, src))\n\n        def _format_string(s):\n            if ""{0}"" in s and ""{1}"" and ""{src}"" in s:\n                return s.format(*sorted([src, self.target]), src=src)\n            elif ""{0}"" in s and ""{1}"" in s:\n                return s.format(*sorted([src, self.target]))\n            elif ""{src}"" in s:\n                return s.format(src=src)\n            else:\n                return s\n\n        return [_format_string(s) for s in strings]\n\n    def get_url(self, src):\n        return self._inject_language(src, self._urls)\n\n    def get_manual_dl_files(self, src):\n        return self._inject_language(src, self._manual_dl_files)\n\n    def get_path(self, src):\n        return self._inject_language(src, self._paths)\n\n\n# Subsets used in the training sets for various years of WMT.\n_TRAIN_SUBSETS = [\n    # pylint:disable=line-too-long\n    SubDataset(\n        name=""commoncrawl"",\n        target=""en"",  # fr-de pair in commoncrawl_frde\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n        path=(""commoncrawl.{src}-en.{src}"", ""commoncrawl.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""commoncrawl_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""czeng_10"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng10"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""czeng_16pre"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng16pre"",\n        manual_dl_files=[""czeng16pre.deduped-ignoring-sections.txt.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""czeng_16"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        # This dataset differs from the above in the filtering that is applied\n        # during parsing.\n        name=""czeng_17"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""dcep_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/dcep.lv-en.v1.tgz"",\n        path=(""dcep.en-lv/dcep.lv"", ""dcep.en-lv/dcep.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n        path=(""training/europarl-v7.{src}-en.{src}"", ""training/europarl-v7.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""europarl_v8_18"",\n        target=""en"",\n        sources={""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training/europarl-v8.{src}-en.{src}"", ""training/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v8_16"",\n        target=""en"",\n        sources={""fi"", ""ro""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training-parallel-ep-v8/europarl-v8.{src}-en.{src}"", ""training-parallel-ep-v8/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""http://www.statmt.org/europarl/v9/training/europarl-v9.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""gigafren"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://www.statmt.org/wmt10/training-giga-fren.tar"",\n        path=(""giga-fren.release2.fixed.fr.gz"", ""giga-fren.release2.fixed.en.gz""),\n    ),\n    SubDataset(\n        name=""hindencorp_01"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://ufallab.ms.mff.cuni.cz/~bojar/hindencorp"",\n        manual_dl_files=[""hindencorp0.1.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""leta_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/leta.v1.tgz"",\n        path=(""LETA-lv-en/leta.lv"", ""LETA-lv-en/leta.en""),\n    ),\n    SubDataset(\n        name=""multiun"",\n        target=""en"",\n        sources={""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-un.tgz"",\n        path=(""un/undoc.2000.{src}-en.{src}"", ""un/undoc.2000.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz"",\n        path=(""training/news-commentary-v9.{src}-en.{src}"", ""training/news-commentary-v9.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v10"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz"",\n        path=(""news-commentary-v10.{src}-en.{src}"", ""news-commentary-v10.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v11"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz"",\n        path=(\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.{src}"",\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v12"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"",\n        path=(""training/news-commentary-v12.{src}-en.{src}"", ""training/news-commentary-v12.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v13"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"",\n        path=(\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.{src}"",\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v14"",\n        target=""en"",  # fr-de pair in newscommentary_v14_frde\n        sources={""cs"", ""de"", ""kk"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.{0}-{1}.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""newscommentary_v14_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-fr.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""onlinebooks_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/books.lv-en.v1.tgz"",\n        path=(""farewell/farewell.lv"", ""farewell/farewell.en""),\n    ),\n    SubDataset(\n        name=""paracrawl_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-{src}.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.{src}"",\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v1_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.ru"",\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v3"",\n        target=""en"",  # fr-de pair in paracrawl_v3_frde\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release3/en-{src}.bicleaner07.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""paracrawl_v3_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""rapid_2016"",\n        target=""en"",\n        sources={""de"", ""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/rapid2016.tgz"",\n        path=(""rapid2016.{0}-{1}.{src}"", ""rapid2016.{0}-{1}.en""),\n    ),\n    SubDataset(\n        name=""rapid_2016_ltfi"",\n        target=""en"",\n        sources={""fi"", ""lt""},\n        url=""https://tilde-model.s3-eu-west-1.amazonaws.com/rapid2016.en-{src}.tmx.zip"",\n        path=""rapid2016.en-{src}.tmx"",\n    ),\n    SubDataset(\n        name=""rapid_2019"",\n        target=""en"",\n        sources={""de""},\n        url=""https://s3-eu-west-1.amazonaws.com/tilde-model/rapid2019.de-en.zip"",\n        path=(""rapid2019.de-en.de"", ""rapid2019.de-en.en""),\n    ),\n    SubDataset(\n        name=""setimes_2"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-{src}.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""uncorpus_v1"",\n        target=""en"",\n        sources={""ru"", ""zh""},\n        url=""https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-{src}.tar.gz"",\n        path=(""en-{src}/UNv1.0.en-{src}.{src}"", ""en-{src}/UNv1.0.en-{src}.en""),\n    ),\n    SubDataset(\n        name=""wikiheadlines_fi"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/fi-en/titles.fi-en"",\n    ),\n    SubDataset(\n        name=""wikiheadlines_hi"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://www.statmt.org/wmt14/wiki-titles.tgz"",\n        path=""wiki/hi-en/wiki-titles.hi-en"",\n    ),\n    SubDataset(\n        # Verified that wmt14 and wmt15 files are identical.\n        name=""wikiheadlines_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/ru-en/wiki.ru-en"",\n    ),\n    SubDataset(\n        name=""wikititles_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""gu"", ""kk"", ""lt"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wikititles/v1/wikititles-v1.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""yandexcorpus"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://translate.yandex.ru/corpus?lang=en"",\n        manual_dl_files=[""1mcorpus.zip""],\n        path=(""corpus.en_ru.1m.ru"", ""corpus.en_ru.1m.en""),\n    ),\n    # pylint:enable=line-too-long\n] + [\n    SubDataset(  # pylint:disable=g-complex-comprehension\n        name=ss,\n        target=""en"",\n        sources={""zh""},\n        url=""ftp://cwmt-wmt:cwmt-wmt@nlp.nju.edu.cn/parallel/%s.zip"" % ss,\n        path=(""%s/*_c[hn].txt"" % ss, ""%s/*_en.txt"" % ss),\n    )\n    for ss in CWMT_SUBSET_NAMES\n]\n\n_DEV_SUBSETS = [\n    SubDataset(\n        name=""euelections_dev2019"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/euelections_dev2019.fr-de.src.fr"", ""dev/euelections_dev2019.fr-de.tgt.de""),\n    ),\n    SubDataset(\n        name=""newsdev2014"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2014.hi"", ""dev/newsdev2014.en""),\n    ),\n    SubDataset(\n        name=""newsdev2015"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2015-fien-src.{src}.sgm"", ""dev/newsdev2015-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2016"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2016-{src}en-src.{src}.sgm"", ""dev/newsdev2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2017"",\n        target=""en"",\n        sources={""lv"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2017-{src}en-src.{src}.sgm"", ""dev/newsdev2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2018"",\n        target=""en"",\n        sources={""et""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2018-{src}en-src.{src}.sgm"", ""dev/newsdev2018-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2019"",\n        target=""en"",\n        sources={""gu"", ""kk"", ""lt""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2019-{src}en-src.{src}.sgm"", ""dev/newsdev2019-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newssyscomb2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newssyscomb2009.{src}"", ""dev/newssyscomb2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2008"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hu""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/news-test2008.{src}"", ""dev/news-test2008.en""),\n    ),\n    SubDataset(\n        name=""newstest2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2009.{src}"", ""dev/newstest2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2010"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2010.{src}"", ""dev/newstest2010.en""),\n    ),\n    SubDataset(\n        name=""newstest2011"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2011.{src}"", ""dev/newstest2011.en""),\n    ),\n    SubDataset(\n        name=""newstest2012"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2012.{src}"", ""dev/newstest2012.en""),\n    ),\n    SubDataset(\n        name=""newstest2013"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2013.{src}"", ""dev/newstest2013.en""),\n    ),\n    SubDataset(\n        name=""newstest2014"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2014-{src}en-src.{src}.sgm"", ""dev/newstest2014-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2015"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2015-{src}en-src.{src}.sgm"", ""dev/newstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2016"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ro"", ""ru"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2016-{src}en-src.{src}.sgm"", ""dev/newstest2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2016"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2016-enfi-ref.{src}.sgm"", ""dev/newstestB2016-enfi-src.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2017"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lv"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2017-{src}en-src.{src}.sgm"", ""dev/newstest2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2017"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2017-fien-src.fi.sgm"", ""dev/newstestB2017-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2018"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2018-{src}en-src.{src}.sgm"", ""dev/newstest2018-{src}en-ref.en.sgm""),\n    ),\n]\n\nDATASET_MAP = {dataset.name: dataset for dataset in _TRAIN_SUBSETS + _DEV_SUBSETS}\n\n_CZENG17_FILTER = SubDataset(\n    name=""czeng17_filter"",\n    target=""en"",\n    sources={""cs""},\n    url=""http://ufal.mff.cuni.cz/czeng/download.php?f=convert_czeng16_to_17.pl.zip"",\n    path=""convert_czeng16_to_17.pl"",\n)\n\n\nclass WmtConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for WMT.""""""\n\n    def __init__(self, url=None, citation=None, description=None, language_pair=(None, None), subsets=None, **kwargs):\n        """"""BuilderConfig for WMT.\n\n    Args:\n      url: The reference URL for the dataset.\n      citation: The paper citation for the dataset.\n      description: The description of the dataset.\n      language_pair: pair of languages that will be used for translation. Should\n                 contain 2 letter coded strings. For example: (""en"", ""de"").\n        configuration for the `nlp.features.text.TextEncoder` used for the\n        `nlp.features.text.Translation` features.\n      subsets: Dict[split, list[str]]. List of the subset to use for each of the\n        split. Note that WMT subclasses overwrite this parameter.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s-%s"" % (language_pair[0], language_pair[1])\n        if ""name"" in kwargs:  # Add name suffix for custom configs\n            name += ""."" + kwargs.pop(""name"")\n\n        super(WmtConfig, self).__init__(name=name, description=description, **kwargs)\n\n        self.url = url or ""http://www.statmt.org""\n        self.citation = citation\n        self.language_pair = language_pair\n        self.subsets = subsets\n\n        # TODO(PVP): remove when manual dir works\n        # +++++++++++++++++++++\n        if language_pair[1] in [""cs"", ""hi"", ""ru""]:\n            assert NotImplementedError(\n                ""The dataset for {}-en is currently not fully supported."".format(language_pair[1])\n            )\n        # +++++++++++++++++++++\n\n\nclass Wmt(ABC, nlp.GeneratorBasedBuilder):\n    """"""WMT translation dataset.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Some of the wmt configs here, require a manual download.\n  Please look into wmt.py to see the exact path (and file name) that has to\n  be downloaded.\n  """"""\n\n    def __init__(self, *args, **kwargs):\n        if type(self) == Wmt and ""config"" not in kwargs:  # pylint: disable=unidiomatic-typecheck\n            raise ValueError(\n                ""The raw `wmt_translate` can only be instantiated with the config ""\n                ""kwargs. You may want to use one of the `wmtYY_translate` ""\n                ""implementation instead to get the WMT dataset for a specific year.""\n            )\n        super(Wmt, self).__init__(*args, **kwargs)\n\n    @property\n    @abstractmethod\n    def _subsets(self):\n        """"""Subsets that make up each split of the dataset.""""""\n        raise NotImplementedError(""This is a abstract method"")\n\n    @property\n    def subsets(self):\n        """"""Subsets that make up each split of the dataset for the language pair.""""""\n        source, target = self.config.language_pair\n        filtered_subsets = {}\n        for split, ss_names in self._subsets.items():\n            filtered_subsets[split] = []\n            for ss_name in ss_names:\n                dataset = DATASET_MAP[ss_name]\n                if dataset.target != target or source not in dataset.sources:\n                    logging.info(""Skipping sub-dataset that does not include language pair: %s"", ss_name)\n                else:\n                    filtered_subsets[split].append(ss_name)\n        logging.info(""Using sub-datasets: %s"", filtered_subsets)\n        return filtered_subsets\n\n    def _info(self):\n        src, target = self.config.language_pair\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            supervised_keys=(src, target),\n            homepage=self.config.url,\n            citation=self.config.citation,\n        )\n\n    def _vocab_text_gen(self, split_subsets, extraction_map, language):\n        for _, ex in self._generate_examples(split_subsets, extraction_map, with_translation=False):\n            yield ex[language]\n\n    def _split_generators(self, dl_manager):\n        source, _ = self.config.language_pair\n        manual_paths_dict = {}\n        urls_to_download = {}\n        for ss_name in itertools.chain.from_iterable(self.subsets.values()):\n            if ss_name == ""czeng_17"":\n                # CzEng1.7 is CzEng1.6 with some blocks filtered out. We must download\n                # the filtering script so we can parse out which blocks need to be\n                # removed.\n                urls_to_download[_CZENG17_FILTER.name] = _CZENG17_FILTER.get_url(source)\n\n            # get dataset\n            dataset = DATASET_MAP[ss_name]\n            if dataset.get_manual_dl_files(source):\n                # TODO(PVP): following two lines skip configs that are incomplete for now\n                # +++++++++++++++++++++\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n                # +++++++++++++++++++++\n\n                manual_dl_files = dataset.get_manual_dl_files(source)\n                manual_paths = [\n                    os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), fname)\n                    for fname in manual_dl_files\n                ]\n                assert all(\n                    os.path.exists(path) for path in manual_paths\n                ), ""For {0}, you must manually download the following file(s) from {1} and place them in {2}: {3}"".format(\n                    dataset.name, dataset.get_url(source), dl_manager.manual_dir, "", "".join(manual_dl_files)\n                )\n\n                # set manual path for correct subset\n                manual_paths_dict[ss_name] = manual_paths\n            else:\n                urls_to_download[ss_name] = dataset.get_url(source)\n\n        # Download and extract files from URLs.\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n        # Extract manually downloaded files.\n        manual_files = dl_manager.extract(manual_paths_dict)\n        extraction_map = dict(downloaded_files, **manual_files)\n\n        for language in self.config.language_pair:\n            self._vocab_text_gen(self.subsets[nlp.Split.TRAIN], extraction_map, language)\n\n        return [\n            nlp.SplitGenerator(  # pylint:disable=g-complex-comprehension\n                name=split, gen_kwargs={""split_subsets"": split_subsets, ""extraction_map"": extraction_map}\n            )\n            for split, split_subsets in self.subsets.items()\n        ]\n\n    def _generate_examples(self, split_subsets, extraction_map, with_translation=True):\n        """"""Returns the examples in the raw (text) form.""""""\n        source, _ = self.config.language_pair\n\n        def _get_local_paths(dataset, extract_dirs):\n            rel_paths = dataset.get_path(source)\n            if len(extract_dirs) == 1:\n                extract_dirs = extract_dirs * len(rel_paths)\n            return [\n                os.path.join(ex_dir, rel_path) if rel_path else ex_dir\n                for ex_dir, rel_path in zip(extract_dirs, rel_paths)\n            ]\n\n        for ss_name in split_subsets:\n            # TODO(PVP) remove following five lines when manual data works\n            # +++++++++++++++++++++\n            dataset = DATASET_MAP[ss_name]\n            source, _ = self.config.language_pair\n            if dataset.get_manual_dl_files(source):\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n            # +++++++++++++++++++++\n\n            logging.info(""Generating examples from: %s"", ss_name)\n            dataset = DATASET_MAP[ss_name]\n            extract_dirs = extraction_map[ss_name]\n            files = _get_local_paths(dataset, extract_dirs)\n\n            if ss_name.startswith(""czeng""):\n                if ss_name.endswith(""16pre""):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=(""en"", ""cs""))\n                elif ss_name.endswith(""17""):\n                    filter_path = _get_local_paths(_CZENG17_FILTER, extraction_map[_CZENG17_FILTER.name])[0]\n                    sub_generator = functools.partial(_parse_czeng, filter_path=filter_path)\n                else:\n                    sub_generator = _parse_czeng\n            elif ss_name == ""hindencorp_01"":\n                sub_generator = _parse_hindencorp\n            elif len(files) == 2:\n                if ss_name.endswith(""_frde""):\n                    sub_generator = _parse_frde_bitext\n                else:\n                    sub_generator = _parse_parallel_sentences\n            elif len(files) == 1:\n                fname = files[0]\n                # Note: Due to formatting used by `download_manager`, the file\n                # extension may not be at the end of the file path.\n                if "".tsv"" in fname:\n                    sub_generator = _parse_tsv\n                elif (\n                    ss_name.startswith(""newscommentary_v14"")\n                    or ss_name.startswith(""europarl_v9"")\n                    or ss_name.startswith(""wikititles_v1"")\n                ):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=self.config.language_pair)\n                elif ""tmx"" in fname or ss_name.startswith(""paracrawl_v3""):\n                    sub_generator = _parse_tmx\n                elif ss_name.startswith(""wikiheadlines""):\n                    sub_generator = _parse_wikiheadlines\n                else:\n                    raise ValueError(""Unsupported file format: %s"" % fname)\n            else:\n                raise ValueError(""Invalid number of files: %d"" % len(files))\n\n            for sub_key, ex in sub_generator(*files):\n                if not all(ex.values()):\n                    continue\n                # TODO(adarob): Add subset feature.\n                # ex[""subset""] = subset\n                key = ""{}/{}"".format(ss_name, sub_key)\n                if with_translation is True:\n                    ex = {""translation"": ex}\n                yield key, ex\n\n\ndef _parse_parallel_sentences(f1, f2):\n    """"""Returns examples from parallel SGML or text files, which may be gzipped.""""""\n\n    def _parse_text(path):\n        """"""Returns the sentences from a single text file, which may be gzipped.""""""\n        split_path = path.split(""."")\n\n        if split_path[-1] == ""gz"":\n            lang = split_path[-2]\n            with open(path, ""rb"") as f, gzip.GzipFile(fileobj=f) as g:\n                return g.read().decode(""utf-8"").split(""\\n""), lang\n\n        if split_path[-1] == ""txt"":\n            # CWMT\n            lang = split_path[-2].split(""_"")[-1]\n            lang = ""zh"" if lang in (""ch"", ""cn"") else lang\n        else:\n            lang = split_path[-1]\n        with open(path, ""rb"") as f:\n            return f.read().decode(""utf-8"").split(""\\n""), lang\n\n    def _parse_sgm(path):\n        """"""Returns sentences from a single SGML file.""""""\n        lang = path.split(""."")[-2]\n        sentences = []\n        # Note: We can\'t use the XML parser since some of the files are badly\n        # formatted.\n        seg_re = re.compile(r""<seg id=\\""\\d+\\"">(.*)</seg>"")\n        with open(path) as f:\n            for line in f:\n                seg_match = re.match(seg_re, line)\n                if seg_match:\n                    assert len(seg_match.groups()) == 1\n                    sentences.append(seg_match.groups()[0])\n        return sentences, lang\n\n    parse_file = _parse_sgm if f1.endswith("".sgm"") else _parse_text\n\n    # Some datasets (e.g., CWMT) contain multiple parallel files specified with\n    # a wildcard. We sort both sets to align them and parse them one by one.\n    f1_files = sorted(glob.glob(f1))\n    f2_files = sorted(glob.glob(f2))\n\n    assert f1_files and f2_files, ""No matching files found: %s, %s."" % (f1, f2)\n    assert len(f1_files) == len(f2_files), ""Number of files do not match: %d vs %d for %s vs %s."" % (\n        len(f1_files),\n        len(f2_files),\n        f1,\n        f2,\n    )\n\n    for f_id, (f1_i, f2_i) in enumerate(zip(sorted(f1_files), sorted(f2_files))):\n        l1_sentences, l1 = parse_file(f1_i)\n        l2_sentences, l2 = parse_file(f2_i)\n\n        assert len(l1_sentences) == len(l2_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(l1_sentences),\n            len(l2_sentences),\n            f1_i,\n            f2_i,\n        )\n\n        for line_id, (s1, s2) in enumerate(zip(l1_sentences, l2_sentences)):\n            key = ""{}/{}"".format(f_id, line_id)\n            yield key, {l1: s1, l2: s2}\n\n\ndef _parse_frde_bitext(fr_path, de_path):\n    with open(fr_path) as f:\n        fr_sentences = f.read().split(""\\n"")\n    with open(de_path) as f:\n        de_sentences = f.read().split(""\\n"")\n    assert len(fr_sentences) == len(de_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n        len(fr_sentences),\n        len(de_sentences),\n        fr_path,\n        de_path,\n    )\n    for line_id, (s1, s2) in enumerate(zip(fr_sentences, de_sentences)):\n        yield line_id, {""fr"": s1, ""de"": s2}\n\n\ndef _parse_tmx(path):\n    """"""Generates examples from TMX file.""""""\n\n    def _get_tuv_lang(tuv):\n        for k, v in tuv.items():\n            if k.endswith(""}lang""):\n                return v\n        raise AssertionError(""Language not found in `tuv` attributes."")\n\n    def _get_tuv_seg(tuv):\n        segs = tuv.findall(""seg"")\n        assert len(segs) == 1, ""Invalid number of segments: %d"" % len(segs)\n        return segs[0].text\n\n    with open(path, ""rb"") as f:\n        if six.PY3:\n            # Workaround due to: https://github.com/tensorflow/tensorflow/issues/33563\n            utf_f = codecs.getreader(""utf-8"")(f)\n        else:\n            utf_f = f\n        for line_id, (_, elem) in enumerate(ElementTree.iterparse(utf_f)):\n            if elem.tag == ""tu"":\n                yield line_id, {_get_tuv_lang(tuv): _get_tuv_seg(tuv) for tuv in elem.iterfind(""tuv"")}\n                elem.clear()\n\n\ndef _parse_tsv(path, language_pair=None):\n    """"""Generates examples from TSV file.""""""\n    if language_pair is None:\n        lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])\\.tsv"", path)\n        assert lang_match is not None, ""Invalid TSV filename: %s"" % path\n        l1, l2 = lang_match.groups()\n    else:\n        l1, l2 = language_pair\n    with open(path) as f:\n        for j, line in enumerate(f):\n            cols = line.split(""\\t"")\n            if len(cols) != 2:\n                logging.warning(""Skipping line %d in TSV (%s) with %d != 2 columns."", j, path, len(cols))\n                continue\n            s1, s2 = cols\n            yield j, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_wikiheadlines(path):\n    """"""Generates examples from Wikiheadlines dataset file.""""""\n    lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])$"", path)\n    assert lang_match is not None, ""Invalid Wikiheadlines filename: %s"" % path\n    l1, l2 = lang_match.groups()\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            s1, s2 = line.split(""|||"")\n            yield line_id, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_czeng(*paths, **kwargs):\n    """"""Generates examples from CzEng v1.6, with optional filtering for v1.7.""""""\n    filter_path = kwargs.get(""filter_path"", None)\n    if filter_path:\n        re_block = re.compile(r""^[^-]+-b(\\d+)-\\d\\d[tde]"")\n        with open(filter_path) as f:\n            bad_blocks = {blk for blk in re.search(r""qw{([\\s\\d]*)}"", f.read()).groups()[0].split()}\n        logging.info(""Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7."", len(bad_blocks))\n\n    for path in paths:\n        for gz_path in sorted(glob.glob(path)):\n            with open(gz_path, ""rb"") as g, gzip.GzipFile(fileobj=g) as f:\n                filename = os.path.basename(gz_path)\n                for line_id, line in enumerate(f):\n                    line = line.decode(""utf-8"")  # required for py3\n                    if not line.strip():\n                        continue\n                    id_, unused_score, cs, en = line.split(""\\t"")\n                    if filter_path:\n                        block_match = re.match(re_block, id_)\n                        if block_match and block_match.groups()[0] in bad_blocks:\n                            continue\n                    sub_key = ""{}/{}"".format(filename, line_id)\n                    yield sub_key, {\n                        ""cs"": cs.strip(),\n                        ""en"": en.strip(),\n                    }\n\n\ndef _parse_hindencorp(path):\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            split_line = line.split(""\\t"")\n            if len(split_line) != 5:\n                logging.warning(""Skipping invalid HindEnCorp line: %s"", line)\n                continue\n            yield line_id, {""translation"": {""en"": split_line[3].strip(), ""hi"": split_line[4].strip()}}\n'"
datasets/wmt18/wmt18.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT18: Translate dataset.""""""\n\nimport nlp\n\nfrom .wmt_utils import CWMT_SUBSET_NAMES, Wmt, WmtConfig\n\n\n_URL = ""http://www.statmt.org/wmt18/translation-task.html""\n_CITATION = """"""\\\n@InProceedings{bojar-EtAl:2018:WMT1,\n  author    = {Bojar, Ond\\v{r}ej  and  Federmann, Christian  and  Fishel, Mark\n    and Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and\n    Koehn, Philipp  and  Monz, Christof},\n  title     = {Findings of the 2018 Conference on Machine Translation (WMT18)},\n  booktitle = {Proceedings of the Third Conference on Machine Translation,\n    Volume 2: Shared Task Papers},\n  month     = {October},\n  year      = {2018},\n  address   = {Belgium, Brussels},\n  publisher = {Association for Computational Linguistics},\n  pages     = {272--307},\n  url       = {http://www.aclweb.org/anthology/W18-6401}\n}\n""""""\n\n_LANGUAGE_PAIRS = [(lang, ""en"") for lang in [""cs"", ""de"", ""et"", ""fi"", ""kk"", ""ru"", ""tr"", ""zh""]]\n\n\nclass Wmt18(Wmt):\n    """"""WMT 18 translation datasets for all {xx, ""en""} language pairs.""""""\n\n    # Version history:\n    # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).\n    BUILDER_CONFIGS = [\n        WmtConfig(  # pylint:disable=g-complex-comprehension\n            description=""WMT 2018 %s-%s translation task dataset."" % (l1, l2),\n            url=_URL,\n            citation=_CITATION,\n            language_pair=(l1, l2),\n            version=nlp.Version(""1.0.0""),\n        )\n        for l1, l2 in _LANGUAGE_PAIRS\n    ]\n\n    @property\n    def _subsets(self):\n        return {\n            nlp.Split.TRAIN: [\n                ""europarl_v7"",\n                ""europarl_v8_18"",\n                ""paracrawl_v1"",\n                ""commoncrawl"",\n                ""newscommentary_v13"",\n                ""czeng_17"",\n                ""yandexcorpus"",\n                ""wikiheadlines_fi"",\n                ""wikiheadlines_ru"",\n                ""setimes_2"",\n                ""uncorpus_v1"",\n                ""rapid_2016"",\n            ]\n            + CWMT_SUBSET_NAMES,\n            nlp.Split.VALIDATION: [""newsdev2018"", ""newstest2017"", ""newstestB2017""],\n            nlp.Split.TEST: [""newstest2018""],\n        }\n'"
datasets/wmt18/wmt_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT: Translate dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport codecs\nimport functools\nimport glob\nimport gzip\nimport itertools\nimport logging\nimport os\nimport re\nimport xml.etree.cElementTree as ElementTree\nfrom abc import ABC, abstractmethod\n\nimport six\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nTranslate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `nlp.translate.wmt.WmtConfig`.\n\n```\nconfig = nlp.wmt.WmtConfig(\n    version=""0.0.1"",\n    language_pair=(""fr"", ""de""),\n    subsets={\n        nlp.Split.TRAIN: [""commoncrawl_frde""],\n        nlp.Split.VALIDATION: [""euelections_dev2019""],\n    },\n)\nbuilder = nlp.builder(""wmt_translate"", config=config)\n```\n\n""""""\n\n\nCWMT_SUBSET_NAMES = [""casia2015"", ""casict2011"", ""casict2015"", ""datum2015"", ""datum2017"", ""neu2017""]\n\n\nclass SubDataset(object):\n    """"""Class to keep track of information on a sub-dataset of WMT.""""""\n\n    def __init__(self, name, target, sources, url, path, manual_dl_files=None):\n        """"""Sub-dataset of WMT.\n\n    Args:\n      name: `string`, a unique dataset identifier.\n      target: `string`, the target language code.\n      sources: `set<string>`, the set of source language codes.\n      url: `string` or `(string, string)`, URL(s) or URL template(s) specifying\n        where to download the raw data from. If two strings are provided, the\n        first is used for the source language and the second for the target.\n        Template strings can either contain \'{src}\' placeholders that will be\n        filled in with the source language code, \'{0}\' and \'{1}\' placeholders\n        that will be filled in with the source and target language codes in\n        alphabetical order, or all 3.\n      path: `string` or `(string, string)`, path(s) or path template(s)\n        specifing the path to the raw data relative to the root of the\n        downloaded archive. If two strings are provided, the dataset is assumed\n        to be made up of parallel text files, the first being the source and the\n        second the target. If one string is provided, both languages are assumed\n        to be stored within the same file and the extension is used to determine\n        how to parse it. Template strings should be formatted the same as in\n        `url`.\n      manual_dl_files: `<list>(string)` (optional), the list of files that must\n        be manually downloaded to the data directory.\n    """"""\n        self._paths = (path,) if isinstance(path, six.string_types) else path\n        self._urls = (url,) if isinstance(url, six.string_types) else url\n        self._manual_dl_files = manual_dl_files if manual_dl_files else []\n        self.name = name\n        self.target = target\n        self.sources = set(sources)\n\n    def _inject_language(self, src, strings):\n        """"""Injects languages into (potentially) template strings.""""""\n        if src not in self.sources:\n            raise ValueError(""Invalid source for \'{0}\': {1}"".format(self.name, src))\n\n        def _format_string(s):\n            if ""{0}"" in s and ""{1}"" and ""{src}"" in s:\n                return s.format(*sorted([src, self.target]), src=src)\n            elif ""{0}"" in s and ""{1}"" in s:\n                return s.format(*sorted([src, self.target]))\n            elif ""{src}"" in s:\n                return s.format(src=src)\n            else:\n                return s\n\n        return [_format_string(s) for s in strings]\n\n    def get_url(self, src):\n        return self._inject_language(src, self._urls)\n\n    def get_manual_dl_files(self, src):\n        return self._inject_language(src, self._manual_dl_files)\n\n    def get_path(self, src):\n        return self._inject_language(src, self._paths)\n\n\n# Subsets used in the training sets for various years of WMT.\n_TRAIN_SUBSETS = [\n    # pylint:disable=line-too-long\n    SubDataset(\n        name=""commoncrawl"",\n        target=""en"",  # fr-de pair in commoncrawl_frde\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n        path=(""commoncrawl.{src}-en.{src}"", ""commoncrawl.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""commoncrawl_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""czeng_10"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng10"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""czeng_16pre"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng16pre"",\n        manual_dl_files=[""czeng16pre.deduped-ignoring-sections.txt.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""czeng_16"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        # This dataset differs from the above in the filtering that is applied\n        # during parsing.\n        name=""czeng_17"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""dcep_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/dcep.lv-en.v1.tgz"",\n        path=(""dcep.en-lv/dcep.lv"", ""dcep.en-lv/dcep.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n        path=(""training/europarl-v7.{src}-en.{src}"", ""training/europarl-v7.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""europarl_v8_18"",\n        target=""en"",\n        sources={""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training/europarl-v8.{src}-en.{src}"", ""training/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v8_16"",\n        target=""en"",\n        sources={""fi"", ""ro""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training-parallel-ep-v8/europarl-v8.{src}-en.{src}"", ""training-parallel-ep-v8/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""http://www.statmt.org/europarl/v9/training/europarl-v9.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""gigafren"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://www.statmt.org/wmt10/training-giga-fren.tar"",\n        path=(""giga-fren.release2.fixed.fr.gz"", ""giga-fren.release2.fixed.en.gz""),\n    ),\n    SubDataset(\n        name=""hindencorp_01"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://ufallab.ms.mff.cuni.cz/~bojar/hindencorp"",\n        manual_dl_files=[""hindencorp0.1.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""leta_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/leta.v1.tgz"",\n        path=(""LETA-lv-en/leta.lv"", ""LETA-lv-en/leta.en""),\n    ),\n    SubDataset(\n        name=""multiun"",\n        target=""en"",\n        sources={""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-un.tgz"",\n        path=(""un/undoc.2000.{src}-en.{src}"", ""un/undoc.2000.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz"",\n        path=(""training/news-commentary-v9.{src}-en.{src}"", ""training/news-commentary-v9.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v10"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz"",\n        path=(""news-commentary-v10.{src}-en.{src}"", ""news-commentary-v10.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v11"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz"",\n        path=(\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.{src}"",\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v12"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"",\n        path=(""training/news-commentary-v12.{src}-en.{src}"", ""training/news-commentary-v12.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v13"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"",\n        path=(\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.{src}"",\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v14"",\n        target=""en"",  # fr-de pair in newscommentary_v14_frde\n        sources={""cs"", ""de"", ""kk"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.{0}-{1}.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""newscommentary_v14_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-fr.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""onlinebooks_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/books.lv-en.v1.tgz"",\n        path=(""farewell/farewell.lv"", ""farewell/farewell.en""),\n    ),\n    SubDataset(\n        name=""paracrawl_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-{src}.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.{src}"",\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v1_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.ru"",\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v3"",\n        target=""en"",  # fr-de pair in paracrawl_v3_frde\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release3/en-{src}.bicleaner07.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""paracrawl_v3_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""rapid_2016"",\n        target=""en"",\n        sources={""de"", ""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/rapid2016.tgz"",\n        path=(""rapid2016.{0}-{1}.{src}"", ""rapid2016.{0}-{1}.en""),\n    ),\n    SubDataset(\n        name=""rapid_2016_ltfi"",\n        target=""en"",\n        sources={""fi"", ""lt""},\n        url=""https://tilde-model.s3-eu-west-1.amazonaws.com/rapid2016.en-{src}.tmx.zip"",\n        path=""rapid2016.en-{src}.tmx"",\n    ),\n    SubDataset(\n        name=""rapid_2019"",\n        target=""en"",\n        sources={""de""},\n        url=""https://s3-eu-west-1.amazonaws.com/tilde-model/rapid2019.de-en.zip"",\n        path=(""rapid2019.de-en.de"", ""rapid2019.de-en.en""),\n    ),\n    SubDataset(\n        name=""setimes_2"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-{src}.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""uncorpus_v1"",\n        target=""en"",\n        sources={""ru"", ""zh""},\n        url=""https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-{src}.tar.gz"",\n        path=(""en-{src}/UNv1.0.en-{src}.{src}"", ""en-{src}/UNv1.0.en-{src}.en""),\n    ),\n    SubDataset(\n        name=""wikiheadlines_fi"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/fi-en/titles.fi-en"",\n    ),\n    SubDataset(\n        name=""wikiheadlines_hi"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://www.statmt.org/wmt14/wiki-titles.tgz"",\n        path=""wiki/hi-en/wiki-titles.hi-en"",\n    ),\n    SubDataset(\n        # Verified that wmt14 and wmt15 files are identical.\n        name=""wikiheadlines_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/ru-en/wiki.ru-en"",\n    ),\n    SubDataset(\n        name=""wikititles_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""gu"", ""kk"", ""lt"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wikititles/v1/wikititles-v1.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""yandexcorpus"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://translate.yandex.ru/corpus?lang=en"",\n        manual_dl_files=[""1mcorpus.zip""],\n        path=(""corpus.en_ru.1m.ru"", ""corpus.en_ru.1m.en""),\n    ),\n    # pylint:enable=line-too-long\n] + [\n    SubDataset(  # pylint:disable=g-complex-comprehension\n        name=ss,\n        target=""en"",\n        sources={""zh""},\n        url=""ftp://cwmt-wmt:cwmt-wmt@nlp.nju.edu.cn/parallel/%s.zip"" % ss,\n        path=(""%s/*_c[hn].txt"" % ss, ""%s/*_en.txt"" % ss),\n    )\n    for ss in CWMT_SUBSET_NAMES\n]\n\n_DEV_SUBSETS = [\n    SubDataset(\n        name=""euelections_dev2019"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/euelections_dev2019.fr-de.src.fr"", ""dev/euelections_dev2019.fr-de.tgt.de""),\n    ),\n    SubDataset(\n        name=""newsdev2014"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2014.hi"", ""dev/newsdev2014.en""),\n    ),\n    SubDataset(\n        name=""newsdev2015"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2015-fien-src.{src}.sgm"", ""dev/newsdev2015-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2016"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2016-{src}en-src.{src}.sgm"", ""dev/newsdev2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2017"",\n        target=""en"",\n        sources={""lv"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2017-{src}en-src.{src}.sgm"", ""dev/newsdev2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2018"",\n        target=""en"",\n        sources={""et""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2018-{src}en-src.{src}.sgm"", ""dev/newsdev2018-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2019"",\n        target=""en"",\n        sources={""gu"", ""kk"", ""lt""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2019-{src}en-src.{src}.sgm"", ""dev/newsdev2019-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newssyscomb2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newssyscomb2009.{src}"", ""dev/newssyscomb2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2008"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hu""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/news-test2008.{src}"", ""dev/news-test2008.en""),\n    ),\n    SubDataset(\n        name=""newstest2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2009.{src}"", ""dev/newstest2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2010"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2010.{src}"", ""dev/newstest2010.en""),\n    ),\n    SubDataset(\n        name=""newstest2011"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2011.{src}"", ""dev/newstest2011.en""),\n    ),\n    SubDataset(\n        name=""newstest2012"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2012.{src}"", ""dev/newstest2012.en""),\n    ),\n    SubDataset(\n        name=""newstest2013"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2013.{src}"", ""dev/newstest2013.en""),\n    ),\n    SubDataset(\n        name=""newstest2014"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2014-{src}en-src.{src}.sgm"", ""dev/newstest2014-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2015"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2015-{src}en-src.{src}.sgm"", ""dev/newstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2016"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ro"", ""ru"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2016-{src}en-src.{src}.sgm"", ""dev/newstest2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2016"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2016-enfi-ref.{src}.sgm"", ""dev/newstestB2016-enfi-src.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2017"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lv"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2017-{src}en-src.{src}.sgm"", ""dev/newstest2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2017"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2017-fien-src.fi.sgm"", ""dev/newstestB2017-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2018"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2018-{src}en-src.{src}.sgm"", ""dev/newstest2018-{src}en-ref.en.sgm""),\n    ),\n]\n\nDATASET_MAP = {dataset.name: dataset for dataset in _TRAIN_SUBSETS + _DEV_SUBSETS}\n\n_CZENG17_FILTER = SubDataset(\n    name=""czeng17_filter"",\n    target=""en"",\n    sources={""cs""},\n    url=""http://ufal.mff.cuni.cz/czeng/download.php?f=convert_czeng16_to_17.pl.zip"",\n    path=""convert_czeng16_to_17.pl"",\n)\n\n\nclass WmtConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for WMT.""""""\n\n    def __init__(self, url=None, citation=None, description=None, language_pair=(None, None), subsets=None, **kwargs):\n        """"""BuilderConfig for WMT.\n\n    Args:\n      url: The reference URL for the dataset.\n      citation: The paper citation for the dataset.\n      description: The description of the dataset.\n      language_pair: pair of languages that will be used for translation. Should\n                 contain 2 letter coded strings. For example: (""en"", ""de"").\n        configuration for the `nlp.features.text.TextEncoder` used for the\n        `nlp.features.text.Translation` features.\n      subsets: Dict[split, list[str]]. List of the subset to use for each of the\n        split. Note that WMT subclasses overwrite this parameter.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s-%s"" % (language_pair[0], language_pair[1])\n        if ""name"" in kwargs:  # Add name suffix for custom configs\n            name += ""."" + kwargs.pop(""name"")\n\n        super(WmtConfig, self).__init__(name=name, description=description, **kwargs)\n\n        self.url = url or ""http://www.statmt.org""\n        self.citation = citation\n        self.language_pair = language_pair\n        self.subsets = subsets\n\n        # TODO(PVP): remove when manual dir works\n        # +++++++++++++++++++++\n        if language_pair[1] in [""cs"", ""hi"", ""ru""]:\n            assert NotImplementedError(\n                ""The dataset for {}-en is currently not fully supported."".format(language_pair[1])\n            )\n        # +++++++++++++++++++++\n\n\nclass Wmt(ABC, nlp.GeneratorBasedBuilder):\n    """"""WMT translation dataset.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Some of the wmt configs here, require a manual download.\n  Please look into wmt.py to see the exact path (and file name) that has to\n  be downloaded.\n  """"""\n\n    def __init__(self, *args, **kwargs):\n        if type(self) == Wmt and ""config"" not in kwargs:  # pylint: disable=unidiomatic-typecheck\n            raise ValueError(\n                ""The raw `wmt_translate` can only be instantiated with the config ""\n                ""kwargs. You may want to use one of the `wmtYY_translate` ""\n                ""implementation instead to get the WMT dataset for a specific year.""\n            )\n        super(Wmt, self).__init__(*args, **kwargs)\n\n    @property\n    @abstractmethod\n    def _subsets(self):\n        """"""Subsets that make up each split of the dataset.""""""\n        raise NotImplementedError(""This is a abstract method"")\n\n    @property\n    def subsets(self):\n        """"""Subsets that make up each split of the dataset for the language pair.""""""\n        source, target = self.config.language_pair\n        filtered_subsets = {}\n        for split, ss_names in self._subsets.items():\n            filtered_subsets[split] = []\n            for ss_name in ss_names:\n                dataset = DATASET_MAP[ss_name]\n                if dataset.target != target or source not in dataset.sources:\n                    logging.info(""Skipping sub-dataset that does not include language pair: %s"", ss_name)\n                else:\n                    filtered_subsets[split].append(ss_name)\n        logging.info(""Using sub-datasets: %s"", filtered_subsets)\n        return filtered_subsets\n\n    def _info(self):\n        src, target = self.config.language_pair\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            supervised_keys=(src, target),\n            homepage=self.config.url,\n            citation=self.config.citation,\n        )\n\n    def _vocab_text_gen(self, split_subsets, extraction_map, language):\n        for _, ex in self._generate_examples(split_subsets, extraction_map, with_translation=False):\n            yield ex[language]\n\n    def _split_generators(self, dl_manager):\n        source, _ = self.config.language_pair\n        manual_paths_dict = {}\n        urls_to_download = {}\n        for ss_name in itertools.chain.from_iterable(self.subsets.values()):\n            if ss_name == ""czeng_17"":\n                # CzEng1.7 is CzEng1.6 with some blocks filtered out. We must download\n                # the filtering script so we can parse out which blocks need to be\n                # removed.\n                urls_to_download[_CZENG17_FILTER.name] = _CZENG17_FILTER.get_url(source)\n\n            # get dataset\n            dataset = DATASET_MAP[ss_name]\n            if dataset.get_manual_dl_files(source):\n                # TODO(PVP): following two lines skip configs that are incomplete for now\n                # +++++++++++++++++++++\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n                # +++++++++++++++++++++\n\n                manual_dl_files = dataset.get_manual_dl_files(source)\n                manual_paths = [\n                    os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), fname)\n                    for fname in manual_dl_files\n                ]\n                assert all(\n                    os.path.exists(path) for path in manual_paths\n                ), ""For {0}, you must manually download the following file(s) from {1} and place them in {2}: {3}"".format(\n                    dataset.name, dataset.get_url(source), dl_manager.manual_dir, "", "".join(manual_dl_files)\n                )\n\n                # set manual path for correct subset\n                manual_paths_dict[ss_name] = manual_paths\n            else:\n                urls_to_download[ss_name] = dataset.get_url(source)\n\n        # Download and extract files from URLs.\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n        # Extract manually downloaded files.\n        manual_files = dl_manager.extract(manual_paths_dict)\n        extraction_map = dict(downloaded_files, **manual_files)\n\n        for language in self.config.language_pair:\n            self._vocab_text_gen(self.subsets[nlp.Split.TRAIN], extraction_map, language)\n\n        return [\n            nlp.SplitGenerator(  # pylint:disable=g-complex-comprehension\n                name=split, gen_kwargs={""split_subsets"": split_subsets, ""extraction_map"": extraction_map}\n            )\n            for split, split_subsets in self.subsets.items()\n        ]\n\n    def _generate_examples(self, split_subsets, extraction_map, with_translation=True):\n        """"""Returns the examples in the raw (text) form.""""""\n        source, _ = self.config.language_pair\n\n        def _get_local_paths(dataset, extract_dirs):\n            rel_paths = dataset.get_path(source)\n            if len(extract_dirs) == 1:\n                extract_dirs = extract_dirs * len(rel_paths)\n            return [\n                os.path.join(ex_dir, rel_path) if rel_path else ex_dir\n                for ex_dir, rel_path in zip(extract_dirs, rel_paths)\n            ]\n\n        for ss_name in split_subsets:\n            # TODO(PVP) remove following five lines when manual data works\n            # +++++++++++++++++++++\n            dataset = DATASET_MAP[ss_name]\n            source, _ = self.config.language_pair\n            if dataset.get_manual_dl_files(source):\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n            # +++++++++++++++++++++\n\n            logging.info(""Generating examples from: %s"", ss_name)\n            dataset = DATASET_MAP[ss_name]\n            extract_dirs = extraction_map[ss_name]\n            files = _get_local_paths(dataset, extract_dirs)\n\n            if ss_name.startswith(""czeng""):\n                if ss_name.endswith(""16pre""):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=(""en"", ""cs""))\n                elif ss_name.endswith(""17""):\n                    filter_path = _get_local_paths(_CZENG17_FILTER, extraction_map[_CZENG17_FILTER.name])[0]\n                    sub_generator = functools.partial(_parse_czeng, filter_path=filter_path)\n                else:\n                    sub_generator = _parse_czeng\n            elif ss_name == ""hindencorp_01"":\n                sub_generator = _parse_hindencorp\n            elif len(files) == 2:\n                if ss_name.endswith(""_frde""):\n                    sub_generator = _parse_frde_bitext\n                else:\n                    sub_generator = _parse_parallel_sentences\n            elif len(files) == 1:\n                fname = files[0]\n                # Note: Due to formatting used by `download_manager`, the file\n                # extension may not be at the end of the file path.\n                if "".tsv"" in fname:\n                    sub_generator = _parse_tsv\n                elif (\n                    ss_name.startswith(""newscommentary_v14"")\n                    or ss_name.startswith(""europarl_v9"")\n                    or ss_name.startswith(""wikititles_v1"")\n                ):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=self.config.language_pair)\n                elif ""tmx"" in fname or ss_name.startswith(""paracrawl_v3""):\n                    sub_generator = _parse_tmx\n                elif ss_name.startswith(""wikiheadlines""):\n                    sub_generator = _parse_wikiheadlines\n                else:\n                    raise ValueError(""Unsupported file format: %s"" % fname)\n            else:\n                raise ValueError(""Invalid number of files: %d"" % len(files))\n\n            for sub_key, ex in sub_generator(*files):\n                if not all(ex.values()):\n                    continue\n                # TODO(adarob): Add subset feature.\n                # ex[""subset""] = subset\n                key = ""{}/{}"".format(ss_name, sub_key)\n                if with_translation is True:\n                    ex = {""translation"": ex}\n                yield key, ex\n\n\ndef _parse_parallel_sentences(f1, f2):\n    """"""Returns examples from parallel SGML or text files, which may be gzipped.""""""\n\n    def _parse_text(path):\n        """"""Returns the sentences from a single text file, which may be gzipped.""""""\n        split_path = path.split(""."")\n\n        if split_path[-1] == ""gz"":\n            lang = split_path[-2]\n            with open(path, ""rb"") as f, gzip.GzipFile(fileobj=f) as g:\n                return g.read().decode(""utf-8"").split(""\\n""), lang\n\n        if split_path[-1] == ""txt"":\n            # CWMT\n            lang = split_path[-2].split(""_"")[-1]\n            lang = ""zh"" if lang in (""ch"", ""cn"") else lang\n        else:\n            lang = split_path[-1]\n        with open(path, ""rb"") as f:\n            return f.read().decode(""utf-8"").split(""\\n""), lang\n\n    def _parse_sgm(path):\n        """"""Returns sentences from a single SGML file.""""""\n        lang = path.split(""."")[-2]\n        sentences = []\n        # Note: We can\'t use the XML parser since some of the files are badly\n        # formatted.\n        seg_re = re.compile(r""<seg id=\\""\\d+\\"">(.*)</seg>"")\n        with open(path) as f:\n            for line in f:\n                seg_match = re.match(seg_re, line)\n                if seg_match:\n                    assert len(seg_match.groups()) == 1\n                    sentences.append(seg_match.groups()[0])\n        return sentences, lang\n\n    parse_file = _parse_sgm if f1.endswith("".sgm"") else _parse_text\n\n    # Some datasets (e.g., CWMT) contain multiple parallel files specified with\n    # a wildcard. We sort both sets to align them and parse them one by one.\n    f1_files = sorted(glob.glob(f1))\n    f2_files = sorted(glob.glob(f2))\n\n    assert f1_files and f2_files, ""No matching files found: %s, %s."" % (f1, f2)\n    assert len(f1_files) == len(f2_files), ""Number of files do not match: %d vs %d for %s vs %s."" % (\n        len(f1_files),\n        len(f2_files),\n        f1,\n        f2,\n    )\n\n    for f_id, (f1_i, f2_i) in enumerate(zip(sorted(f1_files), sorted(f2_files))):\n        l1_sentences, l1 = parse_file(f1_i)\n        l2_sentences, l2 = parse_file(f2_i)\n\n        assert len(l1_sentences) == len(l2_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(l1_sentences),\n            len(l2_sentences),\n            f1_i,\n            f2_i,\n        )\n\n        for line_id, (s1, s2) in enumerate(zip(l1_sentences, l2_sentences)):\n            key = ""{}/{}"".format(f_id, line_id)\n            yield key, {l1: s1, l2: s2}\n\n\ndef _parse_frde_bitext(fr_path, de_path):\n    with open(fr_path) as f:\n        fr_sentences = f.read().split(""\\n"")\n    with open(de_path) as f:\n        de_sentences = f.read().split(""\\n"")\n    assert len(fr_sentences) == len(de_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n        len(fr_sentences),\n        len(de_sentences),\n        fr_path,\n        de_path,\n    )\n    for line_id, (s1, s2) in enumerate(zip(fr_sentences, de_sentences)):\n        yield line_id, {""fr"": s1, ""de"": s2}\n\n\ndef _parse_tmx(path):\n    """"""Generates examples from TMX file.""""""\n\n    def _get_tuv_lang(tuv):\n        for k, v in tuv.items():\n            if k.endswith(""}lang""):\n                return v\n        raise AssertionError(""Language not found in `tuv` attributes."")\n\n    def _get_tuv_seg(tuv):\n        segs = tuv.findall(""seg"")\n        assert len(segs) == 1, ""Invalid number of segments: %d"" % len(segs)\n        return segs[0].text\n\n    with open(path, ""rb"") as f:\n        if six.PY3:\n            # Workaround due to: https://github.com/tensorflow/tensorflow/issues/33563\n            utf_f = codecs.getreader(""utf-8"")(f)\n        else:\n            utf_f = f\n        for line_id, (_, elem) in enumerate(ElementTree.iterparse(utf_f)):\n            if elem.tag == ""tu"":\n                yield line_id, {_get_tuv_lang(tuv): _get_tuv_seg(tuv) for tuv in elem.iterfind(""tuv"")}\n                elem.clear()\n\n\ndef _parse_tsv(path, language_pair=None):\n    """"""Generates examples from TSV file.""""""\n    if language_pair is None:\n        lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])\\.tsv"", path)\n        assert lang_match is not None, ""Invalid TSV filename: %s"" % path\n        l1, l2 = lang_match.groups()\n    else:\n        l1, l2 = language_pair\n    with open(path) as f:\n        for j, line in enumerate(f):\n            cols = line.split(""\\t"")\n            if len(cols) != 2:\n                logging.warning(""Skipping line %d in TSV (%s) with %d != 2 columns."", j, path, len(cols))\n                continue\n            s1, s2 = cols\n            yield j, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_wikiheadlines(path):\n    """"""Generates examples from Wikiheadlines dataset file.""""""\n    lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])$"", path)\n    assert lang_match is not None, ""Invalid Wikiheadlines filename: %s"" % path\n    l1, l2 = lang_match.groups()\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            s1, s2 = line.split(""|||"")\n            yield line_id, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_czeng(*paths, **kwargs):\n    """"""Generates examples from CzEng v1.6, with optional filtering for v1.7.""""""\n    filter_path = kwargs.get(""filter_path"", None)\n    if filter_path:\n        re_block = re.compile(r""^[^-]+-b(\\d+)-\\d\\d[tde]"")\n        with open(filter_path) as f:\n            bad_blocks = {blk for blk in re.search(r""qw{([\\s\\d]*)}"", f.read()).groups()[0].split()}\n        logging.info(""Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7."", len(bad_blocks))\n\n    for path in paths:\n        for gz_path in sorted(glob.glob(path)):\n            with open(gz_path, ""rb"") as g, gzip.GzipFile(fileobj=g) as f:\n                filename = os.path.basename(gz_path)\n                for line_id, line in enumerate(f):\n                    line = line.decode(""utf-8"")  # required for py3\n                    if not line.strip():\n                        continue\n                    id_, unused_score, cs, en = line.split(""\\t"")\n                    if filter_path:\n                        block_match = re.match(re_block, id_)\n                        if block_match and block_match.groups()[0] in bad_blocks:\n                            continue\n                    sub_key = ""{}/{}"".format(filename, line_id)\n                    yield sub_key, {\n                        ""cs"": cs.strip(),\n                        ""en"": en.strip(),\n                    }\n\n\ndef _parse_hindencorp(path):\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            split_line = line.split(""\\t"")\n            if len(split_line) != 5:\n                logging.warning(""Skipping invalid HindEnCorp line: %s"", line)\n                continue\n            yield line_id, {""translation"": {""en"": split_line[3].strip(), ""hi"": split_line[4].strip()}}\n'"
datasets/wmt19/wmt19.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT19: Translate dataset.""""""\n\nimport nlp\n\nfrom .wmt_utils import CWMT_SUBSET_NAMES, Wmt, WmtConfig\n\n\n_URL = ""http://www.statmt.org/wmt19/translation-task.html""\n# TODO(adarob): Update with citation of overview paper once it is published.\n_CITATION = """"""\n@ONLINE {wmt19translate,\n    author = ""Wikimedia Foundation"",\n    title  = ""ACL 2019 Fourth Conference on Machine Translation (WMT19), Shared Task: Machine Translation of News"",\n    url    = ""http://www.statmt.org/wmt19/translation-task.html""\n}\n""""""\n\n_LANGUAGE_PAIRS = [(lang, ""en"") for lang in [""cs"", ""de"", ""fi"", ""gu"", ""kk"", ""lt"", ""ru"", ""zh""]] + [(""fr"", ""de"")]\n\n\nclass Wmt19(Wmt):\n    """"""WMT 19 translation datasets for {(xx, ""en"")} + (""fr"", ""de"") pairs.""""""\n\n    # Version history:\n    # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).\n    BUILDER_CONFIGS = [\n        WmtConfig(  # pylint:disable=g-complex-comprehension\n            description=""WMT 2019 %s-%s translation task dataset."" % (l1, l2),\n            url=_URL,\n            citation=_CITATION,\n            language_pair=(l1, l2),\n            version=nlp.Version(""1.0.0""),\n        )\n        for l1, l2 in _LANGUAGE_PAIRS\n    ]\n\n    @property\n    def _subsets(self):\n        return {\n            nlp.Split.TRAIN: [\n                ""europarl_v9"",\n                ""europarl_v7_frde"",\n                ""paracrawl_v3"",\n                ""paracrawl_v1_ru"",\n                ""paracrawl_v3_frde"",\n                ""commoncrawl"",\n                ""commoncrawl_frde"",\n                ""newscommentary_v14"",\n                ""newscommentary_v14_frde"",\n                ""czeng_17"",\n                ""yandexcorpus"",\n                ""wikititles_v1"",\n                ""uncorpus_v1"",\n                ""rapid_2016_ltfi"",\n                ""rapid_2019"",\n            ]\n            + CWMT_SUBSET_NAMES,\n            nlp.Split.VALIDATION: [""euelections_dev2019"", ""newsdev2019"", ""newstest2018""],\n        }\n'"
datasets/wmt19/wmt_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT: Translate dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport codecs\nimport functools\nimport glob\nimport gzip\nimport itertools\nimport logging\nimport os\nimport re\nimport xml.etree.cElementTree as ElementTree\nfrom abc import ABC, abstractmethod\n\nimport six\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nTranslate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `nlp.translate.wmt.WmtConfig`.\n\n```\nconfig = nlp.wmt.WmtConfig(\n    version=""0.0.1"",\n    language_pair=(""fr"", ""de""),\n    subsets={\n        nlp.Split.TRAIN: [""commoncrawl_frde""],\n        nlp.Split.VALIDATION: [""euelections_dev2019""],\n    },\n)\nbuilder = nlp.builder(""wmt_translate"", config=config)\n```\n\n""""""\n\n\nCWMT_SUBSET_NAMES = [""casia2015"", ""casict2011"", ""casict2015"", ""datum2015"", ""datum2017"", ""neu2017""]\n\n\nclass SubDataset(object):\n    """"""Class to keep track of information on a sub-dataset of WMT.""""""\n\n    def __init__(self, name, target, sources, url, path, manual_dl_files=None):\n        """"""Sub-dataset of WMT.\n\n    Args:\n      name: `string`, a unique dataset identifier.\n      target: `string`, the target language code.\n      sources: `set<string>`, the set of source language codes.\n      url: `string` or `(string, string)`, URL(s) or URL template(s) specifying\n        where to download the raw data from. If two strings are provided, the\n        first is used for the source language and the second for the target.\n        Template strings can either contain \'{src}\' placeholders that will be\n        filled in with the source language code, \'{0}\' and \'{1}\' placeholders\n        that will be filled in with the source and target language codes in\n        alphabetical order, or all 3.\n      path: `string` or `(string, string)`, path(s) or path template(s)\n        specifing the path to the raw data relative to the root of the\n        downloaded archive. If two strings are provided, the dataset is assumed\n        to be made up of parallel text files, the first being the source and the\n        second the target. If one string is provided, both languages are assumed\n        to be stored within the same file and the extension is used to determine\n        how to parse it. Template strings should be formatted the same as in\n        `url`.\n      manual_dl_files: `<list>(string)` (optional), the list of files that must\n        be manually downloaded to the data directory.\n    """"""\n        self._paths = (path,) if isinstance(path, six.string_types) else path\n        self._urls = (url,) if isinstance(url, six.string_types) else url\n        self._manual_dl_files = manual_dl_files if manual_dl_files else []\n        self.name = name\n        self.target = target\n        self.sources = set(sources)\n\n    def _inject_language(self, src, strings):\n        """"""Injects languages into (potentially) template strings.""""""\n        if src not in self.sources:\n            raise ValueError(""Invalid source for \'{0}\': {1}"".format(self.name, src))\n\n        def _format_string(s):\n            if ""{0}"" in s and ""{1}"" and ""{src}"" in s:\n                return s.format(*sorted([src, self.target]), src=src)\n            elif ""{0}"" in s and ""{1}"" in s:\n                return s.format(*sorted([src, self.target]))\n            elif ""{src}"" in s:\n                return s.format(src=src)\n            else:\n                return s\n\n        return [_format_string(s) for s in strings]\n\n    def get_url(self, src):\n        return self._inject_language(src, self._urls)\n\n    def get_manual_dl_files(self, src):\n        return self._inject_language(src, self._manual_dl_files)\n\n    def get_path(self, src):\n        return self._inject_language(src, self._paths)\n\n\n# Subsets used in the training sets for various years of WMT.\n_TRAIN_SUBSETS = [\n    # pylint:disable=line-too-long\n    SubDataset(\n        name=""commoncrawl"",\n        target=""en"",  # fr-de pair in commoncrawl_frde\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n        path=(""commoncrawl.{src}-en.{src}"", ""commoncrawl.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""commoncrawl_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""czeng_10"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng10"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""czeng_16pre"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng16pre"",\n        manual_dl_files=[""czeng16pre.deduped-ignoring-sections.txt.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""czeng_16"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        # This dataset differs from the above in the filtering that is applied\n        # during parsing.\n        name=""czeng_17"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""dcep_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/dcep.lv-en.v1.tgz"",\n        path=(""dcep.en-lv/dcep.lv"", ""dcep.en-lv/dcep.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n        path=(""training/europarl-v7.{src}-en.{src}"", ""training/europarl-v7.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""europarl_v8_18"",\n        target=""en"",\n        sources={""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training/europarl-v8.{src}-en.{src}"", ""training/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v8_16"",\n        target=""en"",\n        sources={""fi"", ""ro""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training-parallel-ep-v8/europarl-v8.{src}-en.{src}"", ""training-parallel-ep-v8/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""http://www.statmt.org/europarl/v9/training/europarl-v9.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""gigafren"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://www.statmt.org/wmt10/training-giga-fren.tar"",\n        path=(""giga-fren.release2.fixed.fr.gz"", ""giga-fren.release2.fixed.en.gz""),\n    ),\n    SubDataset(\n        name=""hindencorp_01"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://ufallab.ms.mff.cuni.cz/~bojar/hindencorp"",\n        manual_dl_files=[""hindencorp0.1.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""leta_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/leta.v1.tgz"",\n        path=(""LETA-lv-en/leta.lv"", ""LETA-lv-en/leta.en""),\n    ),\n    SubDataset(\n        name=""multiun"",\n        target=""en"",\n        sources={""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-un.tgz"",\n        path=(""un/undoc.2000.{src}-en.{src}"", ""un/undoc.2000.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz"",\n        path=(""training/news-commentary-v9.{src}-en.{src}"", ""training/news-commentary-v9.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v10"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz"",\n        path=(""news-commentary-v10.{src}-en.{src}"", ""news-commentary-v10.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v11"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz"",\n        path=(\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.{src}"",\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v12"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"",\n        path=(""training/news-commentary-v12.{src}-en.{src}"", ""training/news-commentary-v12.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v13"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"",\n        path=(\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.{src}"",\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v14"",\n        target=""en"",  # fr-de pair in newscommentary_v14_frde\n        sources={""cs"", ""de"", ""kk"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.{0}-{1}.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""newscommentary_v14_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-fr.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""onlinebooks_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/books.lv-en.v1.tgz"",\n        path=(""farewell/farewell.lv"", ""farewell/farewell.en""),\n    ),\n    SubDataset(\n        name=""paracrawl_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-{src}.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.{src}"",\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v1_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.ru"",\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v3"",\n        target=""en"",  # fr-de pair in paracrawl_v3_frde\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release3/en-{src}.bicleaner07.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""paracrawl_v3_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""rapid_2016"",\n        target=""en"",\n        sources={""de"", ""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/rapid2016.tgz"",\n        path=(""rapid2016.{0}-{1}.{src}"", ""rapid2016.{0}-{1}.en""),\n    ),\n    SubDataset(\n        name=""rapid_2016_ltfi"",\n        target=""en"",\n        sources={""fi"", ""lt""},\n        url=""https://tilde-model.s3-eu-west-1.amazonaws.com/rapid2016.en-{src}.tmx.zip"",\n        path=""rapid2016.en-{src}.tmx"",\n    ),\n    SubDataset(\n        name=""rapid_2019"",\n        target=""en"",\n        sources={""de""},\n        url=""https://s3-eu-west-1.amazonaws.com/tilde-model/rapid2019.de-en.zip"",\n        path=(""rapid2019.de-en.de"", ""rapid2019.de-en.en""),\n    ),\n    SubDataset(\n        name=""setimes_2"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-{src}.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""uncorpus_v1"",\n        target=""en"",\n        sources={""ru"", ""zh""},\n        url=""https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-{src}.tar.gz"",\n        path=(""en-{src}/UNv1.0.en-{src}.{src}"", ""en-{src}/UNv1.0.en-{src}.en""),\n    ),\n    SubDataset(\n        name=""wikiheadlines_fi"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/fi-en/titles.fi-en"",\n    ),\n    SubDataset(\n        name=""wikiheadlines_hi"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://www.statmt.org/wmt14/wiki-titles.tgz"",\n        path=""wiki/hi-en/wiki-titles.hi-en"",\n    ),\n    SubDataset(\n        # Verified that wmt14 and wmt15 files are identical.\n        name=""wikiheadlines_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/ru-en/wiki.ru-en"",\n    ),\n    SubDataset(\n        name=""wikititles_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""gu"", ""kk"", ""lt"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wikititles/v1/wikititles-v1.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""yandexcorpus"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://translate.yandex.ru/corpus?lang=en"",\n        manual_dl_files=[""1mcorpus.zip""],\n        path=(""corpus.en_ru.1m.ru"", ""corpus.en_ru.1m.en""),\n    ),\n    # pylint:enable=line-too-long\n] + [\n    SubDataset(  # pylint:disable=g-complex-comprehension\n        name=ss,\n        target=""en"",\n        sources={""zh""},\n        url=""ftp://cwmt-wmt:cwmt-wmt@nlp.nju.edu.cn/parallel/%s.zip"" % ss,\n        path=(""%s/*_c[hn].txt"" % ss, ""%s/*_en.txt"" % ss),\n    )\n    for ss in CWMT_SUBSET_NAMES\n]\n\n_DEV_SUBSETS = [\n    SubDataset(\n        name=""euelections_dev2019"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/euelections_dev2019.fr-de.src.fr"", ""dev/euelections_dev2019.fr-de.tgt.de""),\n    ),\n    SubDataset(\n        name=""newsdev2014"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2014.hi"", ""dev/newsdev2014.en""),\n    ),\n    SubDataset(\n        name=""newsdev2015"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2015-fien-src.{src}.sgm"", ""dev/newsdev2015-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2016"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2016-{src}en-src.{src}.sgm"", ""dev/newsdev2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2017"",\n        target=""en"",\n        sources={""lv"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2017-{src}en-src.{src}.sgm"", ""dev/newsdev2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2018"",\n        target=""en"",\n        sources={""et""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2018-{src}en-src.{src}.sgm"", ""dev/newsdev2018-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2019"",\n        target=""en"",\n        sources={""gu"", ""kk"", ""lt""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2019-{src}en-src.{src}.sgm"", ""dev/newsdev2019-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newssyscomb2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newssyscomb2009.{src}"", ""dev/newssyscomb2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2008"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hu""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/news-test2008.{src}"", ""dev/news-test2008.en""),\n    ),\n    SubDataset(\n        name=""newstest2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2009.{src}"", ""dev/newstest2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2010"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2010.{src}"", ""dev/newstest2010.en""),\n    ),\n    SubDataset(\n        name=""newstest2011"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2011.{src}"", ""dev/newstest2011.en""),\n    ),\n    SubDataset(\n        name=""newstest2012"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2012.{src}"", ""dev/newstest2012.en""),\n    ),\n    SubDataset(\n        name=""newstest2013"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2013.{src}"", ""dev/newstest2013.en""),\n    ),\n    SubDataset(\n        name=""newstest2014"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2014-{src}en-src.{src}.sgm"", ""dev/newstest2014-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2015"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2015-{src}en-src.{src}.sgm"", ""dev/newstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2016"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ro"", ""ru"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2016-{src}en-src.{src}.sgm"", ""dev/newstest2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2016"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2016-enfi-ref.{src}.sgm"", ""dev/newstestB2016-enfi-src.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2017"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lv"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2017-{src}en-src.{src}.sgm"", ""dev/newstest2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2017"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2017-fien-src.fi.sgm"", ""dev/newstestB2017-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2018"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2018-{src}en-src.{src}.sgm"", ""dev/newstest2018-{src}en-ref.en.sgm""),\n    ),\n]\n\nDATASET_MAP = {dataset.name: dataset for dataset in _TRAIN_SUBSETS + _DEV_SUBSETS}\n\n_CZENG17_FILTER = SubDataset(\n    name=""czeng17_filter"",\n    target=""en"",\n    sources={""cs""},\n    url=""http://ufal.mff.cuni.cz/czeng/download.php?f=convert_czeng16_to_17.pl.zip"",\n    path=""convert_czeng16_to_17.pl"",\n)\n\n\nclass WmtConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for WMT.""""""\n\n    def __init__(self, url=None, citation=None, description=None, language_pair=(None, None), subsets=None, **kwargs):\n        """"""BuilderConfig for WMT.\n\n    Args:\n      url: The reference URL for the dataset.\n      citation: The paper citation for the dataset.\n      description: The description of the dataset.\n      language_pair: pair of languages that will be used for translation. Should\n                 contain 2 letter coded strings. For example: (""en"", ""de"").\n        configuration for the `nlp.features.text.TextEncoder` used for the\n        `nlp.features.text.Translation` features.\n      subsets: Dict[split, list[str]]. List of the subset to use for each of the\n        split. Note that WMT subclasses overwrite this parameter.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s-%s"" % (language_pair[0], language_pair[1])\n        if ""name"" in kwargs:  # Add name suffix for custom configs\n            name += ""."" + kwargs.pop(""name"")\n\n        super(WmtConfig, self).__init__(name=name, description=description, **kwargs)\n\n        self.url = url or ""http://www.statmt.org""\n        self.citation = citation\n        self.language_pair = language_pair\n        self.subsets = subsets\n\n        # TODO(PVP): remove when manual dir works\n        # +++++++++++++++++++++\n        if language_pair[1] in [""cs"", ""hi"", ""ru""]:\n            assert NotImplementedError(\n                ""The dataset for {}-en is currently not fully supported."".format(language_pair[1])\n            )\n        # +++++++++++++++++++++\n\n\nclass Wmt(ABC, nlp.GeneratorBasedBuilder):\n    """"""WMT translation dataset.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Some of the wmt configs here, require a manual download.\n  Please look into wmt.py to see the exact path (and file name) that has to\n  be downloaded.\n  """"""\n\n    def __init__(self, *args, **kwargs):\n        if type(self) == Wmt and ""config"" not in kwargs:  # pylint: disable=unidiomatic-typecheck\n            raise ValueError(\n                ""The raw `wmt_translate` can only be instantiated with the config ""\n                ""kwargs. You may want to use one of the `wmtYY_translate` ""\n                ""implementation instead to get the WMT dataset for a specific year.""\n            )\n        super(Wmt, self).__init__(*args, **kwargs)\n\n    @property\n    @abstractmethod\n    def _subsets(self):\n        """"""Subsets that make up each split of the dataset.""""""\n        raise NotImplementedError(""This is a abstract method"")\n\n    @property\n    def subsets(self):\n        """"""Subsets that make up each split of the dataset for the language pair.""""""\n        source, target = self.config.language_pair\n        filtered_subsets = {}\n        for split, ss_names in self._subsets.items():\n            filtered_subsets[split] = []\n            for ss_name in ss_names:\n                dataset = DATASET_MAP[ss_name]\n                if dataset.target != target or source not in dataset.sources:\n                    logging.info(""Skipping sub-dataset that does not include language pair: %s"", ss_name)\n                else:\n                    filtered_subsets[split].append(ss_name)\n        logging.info(""Using sub-datasets: %s"", filtered_subsets)\n        return filtered_subsets\n\n    def _info(self):\n        src, target = self.config.language_pair\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            supervised_keys=(src, target),\n            homepage=self.config.url,\n            citation=self.config.citation,\n        )\n\n    def _vocab_text_gen(self, split_subsets, extraction_map, language):\n        for _, ex in self._generate_examples(split_subsets, extraction_map, with_translation=False):\n            yield ex[language]\n\n    def _split_generators(self, dl_manager):\n        source, _ = self.config.language_pair\n        manual_paths_dict = {}\n        urls_to_download = {}\n        for ss_name in itertools.chain.from_iterable(self.subsets.values()):\n            if ss_name == ""czeng_17"":\n                # CzEng1.7 is CzEng1.6 with some blocks filtered out. We must download\n                # the filtering script so we can parse out which blocks need to be\n                # removed.\n                urls_to_download[_CZENG17_FILTER.name] = _CZENG17_FILTER.get_url(source)\n\n            # get dataset\n            dataset = DATASET_MAP[ss_name]\n            if dataset.get_manual_dl_files(source):\n                # TODO(PVP): following two lines skip configs that are incomplete for now\n                # +++++++++++++++++++++\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n                # +++++++++++++++++++++\n\n                manual_dl_files = dataset.get_manual_dl_files(source)\n                manual_paths = [\n                    os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), fname)\n                    for fname in manual_dl_files\n                ]\n                assert all(\n                    os.path.exists(path) for path in manual_paths\n                ), ""For {0}, you must manually download the following file(s) from {1} and place them in {2}: {3}"".format(\n                    dataset.name, dataset.get_url(source), dl_manager.manual_dir, "", "".join(manual_dl_files)\n                )\n\n                # set manual path for correct subset\n                manual_paths_dict[ss_name] = manual_paths\n            else:\n                urls_to_download[ss_name] = dataset.get_url(source)\n\n        # Download and extract files from URLs.\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n        # Extract manually downloaded files.\n        manual_files = dl_manager.extract(manual_paths_dict)\n        extraction_map = dict(downloaded_files, **manual_files)\n\n        for language in self.config.language_pair:\n            self._vocab_text_gen(self.subsets[nlp.Split.TRAIN], extraction_map, language)\n\n        return [\n            nlp.SplitGenerator(  # pylint:disable=g-complex-comprehension\n                name=split, gen_kwargs={""split_subsets"": split_subsets, ""extraction_map"": extraction_map}\n            )\n            for split, split_subsets in self.subsets.items()\n        ]\n\n    def _generate_examples(self, split_subsets, extraction_map, with_translation=True):\n        """"""Returns the examples in the raw (text) form.""""""\n        source, _ = self.config.language_pair\n\n        def _get_local_paths(dataset, extract_dirs):\n            rel_paths = dataset.get_path(source)\n            if len(extract_dirs) == 1:\n                extract_dirs = extract_dirs * len(rel_paths)\n            return [\n                os.path.join(ex_dir, rel_path) if rel_path else ex_dir\n                for ex_dir, rel_path in zip(extract_dirs, rel_paths)\n            ]\n\n        for ss_name in split_subsets:\n            # TODO(PVP) remove following five lines when manual data works\n            # +++++++++++++++++++++\n            dataset = DATASET_MAP[ss_name]\n            source, _ = self.config.language_pair\n            if dataset.get_manual_dl_files(source):\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n            # +++++++++++++++++++++\n\n            logging.info(""Generating examples from: %s"", ss_name)\n            dataset = DATASET_MAP[ss_name]\n            extract_dirs = extraction_map[ss_name]\n            files = _get_local_paths(dataset, extract_dirs)\n\n            if ss_name.startswith(""czeng""):\n                if ss_name.endswith(""16pre""):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=(""en"", ""cs""))\n                elif ss_name.endswith(""17""):\n                    filter_path = _get_local_paths(_CZENG17_FILTER, extraction_map[_CZENG17_FILTER.name])[0]\n                    sub_generator = functools.partial(_parse_czeng, filter_path=filter_path)\n                else:\n                    sub_generator = _parse_czeng\n            elif ss_name == ""hindencorp_01"":\n                sub_generator = _parse_hindencorp\n            elif len(files) == 2:\n                if ss_name.endswith(""_frde""):\n                    sub_generator = _parse_frde_bitext\n                else:\n                    sub_generator = _parse_parallel_sentences\n            elif len(files) == 1:\n                fname = files[0]\n                # Note: Due to formatting used by `download_manager`, the file\n                # extension may not be at the end of the file path.\n                if "".tsv"" in fname:\n                    sub_generator = _parse_tsv\n                elif (\n                    ss_name.startswith(""newscommentary_v14"")\n                    or ss_name.startswith(""europarl_v9"")\n                    or ss_name.startswith(""wikititles_v1"")\n                ):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=self.config.language_pair)\n                elif ""tmx"" in fname or ss_name.startswith(""paracrawl_v3""):\n                    sub_generator = _parse_tmx\n                elif ss_name.startswith(""wikiheadlines""):\n                    sub_generator = _parse_wikiheadlines\n                else:\n                    raise ValueError(""Unsupported file format: %s"" % fname)\n            else:\n                raise ValueError(""Invalid number of files: %d"" % len(files))\n\n            for sub_key, ex in sub_generator(*files):\n                if not all(ex.values()):\n                    continue\n                # TODO(adarob): Add subset feature.\n                # ex[""subset""] = subset\n                key = ""{}/{}"".format(ss_name, sub_key)\n                if with_translation is True:\n                    ex = {""translation"": ex}\n                yield key, ex\n\n\ndef _parse_parallel_sentences(f1, f2):\n    """"""Returns examples from parallel SGML or text files, which may be gzipped.""""""\n\n    def _parse_text(path):\n        """"""Returns the sentences from a single text file, which may be gzipped.""""""\n        split_path = path.split(""."")\n\n        if split_path[-1] == ""gz"":\n            lang = split_path[-2]\n            with open(path, ""rb"") as f, gzip.GzipFile(fileobj=f) as g:\n                return g.read().decode(""utf-8"").split(""\\n""), lang\n\n        if split_path[-1] == ""txt"":\n            # CWMT\n            lang = split_path[-2].split(""_"")[-1]\n            lang = ""zh"" if lang in (""ch"", ""cn"") else lang\n        else:\n            lang = split_path[-1]\n        with open(path, ""rb"") as f:\n            return f.read().decode(""utf-8"").split(""\\n""), lang\n\n    def _parse_sgm(path):\n        """"""Returns sentences from a single SGML file.""""""\n        lang = path.split(""."")[-2]\n        sentences = []\n        # Note: We can\'t use the XML parser since some of the files are badly\n        # formatted.\n        seg_re = re.compile(r""<seg id=\\""\\d+\\"">(.*)</seg>"")\n        with open(path) as f:\n            for line in f:\n                seg_match = re.match(seg_re, line)\n                if seg_match:\n                    assert len(seg_match.groups()) == 1\n                    sentences.append(seg_match.groups()[0])\n        return sentences, lang\n\n    parse_file = _parse_sgm if f1.endswith("".sgm"") else _parse_text\n\n    # Some datasets (e.g., CWMT) contain multiple parallel files specified with\n    # a wildcard. We sort both sets to align them and parse them one by one.\n    f1_files = sorted(glob.glob(f1))\n    f2_files = sorted(glob.glob(f2))\n\n    assert f1_files and f2_files, ""No matching files found: %s, %s."" % (f1, f2)\n    assert len(f1_files) == len(f2_files), ""Number of files do not match: %d vs %d for %s vs %s."" % (\n        len(f1_files),\n        len(f2_files),\n        f1,\n        f2,\n    )\n\n    for f_id, (f1_i, f2_i) in enumerate(zip(sorted(f1_files), sorted(f2_files))):\n        l1_sentences, l1 = parse_file(f1_i)\n        l2_sentences, l2 = parse_file(f2_i)\n\n        assert len(l1_sentences) == len(l2_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(l1_sentences),\n            len(l2_sentences),\n            f1_i,\n            f2_i,\n        )\n\n        for line_id, (s1, s2) in enumerate(zip(l1_sentences, l2_sentences)):\n            key = ""{}/{}"".format(f_id, line_id)\n            yield key, {l1: s1, l2: s2}\n\n\ndef _parse_frde_bitext(fr_path, de_path):\n    with open(fr_path) as f:\n        fr_sentences = f.read().split(""\\n"")\n    with open(de_path) as f:\n        de_sentences = f.read().split(""\\n"")\n    assert len(fr_sentences) == len(de_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n        len(fr_sentences),\n        len(de_sentences),\n        fr_path,\n        de_path,\n    )\n    for line_id, (s1, s2) in enumerate(zip(fr_sentences, de_sentences)):\n        yield line_id, {""fr"": s1, ""de"": s2}\n\n\ndef _parse_tmx(path):\n    """"""Generates examples from TMX file.""""""\n\n    def _get_tuv_lang(tuv):\n        for k, v in tuv.items():\n            if k.endswith(""}lang""):\n                return v\n        raise AssertionError(""Language not found in `tuv` attributes."")\n\n    def _get_tuv_seg(tuv):\n        segs = tuv.findall(""seg"")\n        assert len(segs) == 1, ""Invalid number of segments: %d"" % len(segs)\n        return segs[0].text\n\n    with open(path, ""rb"") as f:\n        if six.PY3:\n            # Workaround due to: https://github.com/tensorflow/tensorflow/issues/33563\n            utf_f = codecs.getreader(""utf-8"")(f)\n        else:\n            utf_f = f\n        for line_id, (_, elem) in enumerate(ElementTree.iterparse(utf_f)):\n            if elem.tag == ""tu"":\n                yield line_id, {_get_tuv_lang(tuv): _get_tuv_seg(tuv) for tuv in elem.iterfind(""tuv"")}\n                elem.clear()\n\n\ndef _parse_tsv(path, language_pair=None):\n    """"""Generates examples from TSV file.""""""\n    if language_pair is None:\n        lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])\\.tsv"", path)\n        assert lang_match is not None, ""Invalid TSV filename: %s"" % path\n        l1, l2 = lang_match.groups()\n    else:\n        l1, l2 = language_pair\n    with open(path) as f:\n        for j, line in enumerate(f):\n            cols = line.split(""\\t"")\n            if len(cols) != 2:\n                logging.warning(""Skipping line %d in TSV (%s) with %d != 2 columns."", j, path, len(cols))\n                continue\n            s1, s2 = cols\n            yield j, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_wikiheadlines(path):\n    """"""Generates examples from Wikiheadlines dataset file.""""""\n    lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])$"", path)\n    assert lang_match is not None, ""Invalid Wikiheadlines filename: %s"" % path\n    l1, l2 = lang_match.groups()\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            s1, s2 = line.split(""|||"")\n            yield line_id, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_czeng(*paths, **kwargs):\n    """"""Generates examples from CzEng v1.6, with optional filtering for v1.7.""""""\n    filter_path = kwargs.get(""filter_path"", None)\n    if filter_path:\n        re_block = re.compile(r""^[^-]+-b(\\d+)-\\d\\d[tde]"")\n        with open(filter_path) as f:\n            bad_blocks = {blk for blk in re.search(r""qw{([\\s\\d]*)}"", f.read()).groups()[0].split()}\n        logging.info(""Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7."", len(bad_blocks))\n\n    for path in paths:\n        for gz_path in sorted(glob.glob(path)):\n            with open(gz_path, ""rb"") as g, gzip.GzipFile(fileobj=g) as f:\n                filename = os.path.basename(gz_path)\n                for line_id, line in enumerate(f):\n                    line = line.decode(""utf-8"")  # required for py3\n                    if not line.strip():\n                        continue\n                    id_, unused_score, cs, en = line.split(""\\t"")\n                    if filter_path:\n                        block_match = re.match(re_block, id_)\n                        if block_match and block_match.groups()[0] in bad_blocks:\n                            continue\n                    sub_key = ""{}/{}"".format(filename, line_id)\n                    yield sub_key, {\n                        ""cs"": cs.strip(),\n                        ""en"": en.strip(),\n                    }\n\n\ndef _parse_hindencorp(path):\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            split_line = line.split(""\\t"")\n            if len(split_line) != 5:\n                logging.warning(""Skipping invalid HindEnCorp line: %s"", line)\n                continue\n            yield line_id, {""translation"": {""en"": split_line[3].strip(), ""hi"": split_line[4].strip()}}\n'"
datasets/wmt_t2t/wmt_t2t.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The WMT EnDe Translate dataset used by the Tensor2Tensor library.""""""\n\nimport nlp\n\nfrom .wmt_utils import Wmt, WmtConfig\n\n\n_URL = ""https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/translate_ende.py""\n_CITATION = """"""\n@InProceedings{bojar-EtAl:2014:W14-33,\n  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\\v{s}},\n  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},\n  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},\n  month     = {June},\n  year      = {2014},\n  address   = {Baltimore, Maryland, USA},\n  publisher = {Association for Computational Linguistics},\n  pages     = {12--58},\n  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}\n}\n""""""\n\n\nclass WmtT2t(Wmt):\n    """"""The WMT EnDe Translate dataset used by the Tensor2Tensor library.""""""\n\n    BUILDER_CONFIGS = [\n        WmtConfig(  # pylint:disable=g-complex-comprehension\n            description=""WMT T2T EnDe translation task dataset."",\n            url=_URL,\n            citation=_CITATION,\n            language_pair=(""de"", ""en""),\n            version=nlp.Version(""1.0.0""),\n        )\n    ]\n\n    @property\n    def _subsets(self):\n        return {\n            nlp.Split.TRAIN: [""europarl_v7"", ""commoncrawl"", ""newscommentary_v13""],\n            nlp.Split.VALIDATION: [""newstest2013""],\n            nlp.Split.TEST: [""newstest2014""],\n        }\n'"
datasets/wmt_t2t/wmt_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""WMT: Translate dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport codecs\nimport functools\nimport glob\nimport gzip\nimport itertools\nimport logging\nimport os\nimport re\nimport xml.etree.cElementTree as ElementTree\nfrom abc import ABC, abstractmethod\n\nimport six\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nTranslate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `nlp.translate.wmt.WmtConfig`.\n\n```\nconfig = nlp.wmt.WmtConfig(\n    version=""0.0.1"",\n    language_pair=(""fr"", ""de""),\n    subsets={\n        nlp.Split.TRAIN: [""commoncrawl_frde""],\n        nlp.Split.VALIDATION: [""euelections_dev2019""],\n    },\n)\nbuilder = nlp.builder(""wmt_translate"", config=config)\n```\n\n""""""\n\n\nCWMT_SUBSET_NAMES = [""casia2015"", ""casict2011"", ""casict2015"", ""datum2015"", ""datum2017"", ""neu2017""]\n\n\nclass SubDataset(object):\n    """"""Class to keep track of information on a sub-dataset of WMT.""""""\n\n    def __init__(self, name, target, sources, url, path, manual_dl_files=None):\n        """"""Sub-dataset of WMT.\n\n    Args:\n      name: `string`, a unique dataset identifier.\n      target: `string`, the target language code.\n      sources: `set<string>`, the set of source language codes.\n      url: `string` or `(string, string)`, URL(s) or URL template(s) specifying\n        where to download the raw data from. If two strings are provided, the\n        first is used for the source language and the second for the target.\n        Template strings can either contain \'{src}\' placeholders that will be\n        filled in with the source language code, \'{0}\' and \'{1}\' placeholders\n        that will be filled in with the source and target language codes in\n        alphabetical order, or all 3.\n      path: `string` or `(string, string)`, path(s) or path template(s)\n        specifing the path to the raw data relative to the root of the\n        downloaded archive. If two strings are provided, the dataset is assumed\n        to be made up of parallel text files, the first being the source and the\n        second the target. If one string is provided, both languages are assumed\n        to be stored within the same file and the extension is used to determine\n        how to parse it. Template strings should be formatted the same as in\n        `url`.\n      manual_dl_files: `<list>(string)` (optional), the list of files that must\n        be manually downloaded to the data directory.\n    """"""\n        self._paths = (path,) if isinstance(path, six.string_types) else path\n        self._urls = (url,) if isinstance(url, six.string_types) else url\n        self._manual_dl_files = manual_dl_files if manual_dl_files else []\n        self.name = name\n        self.target = target\n        self.sources = set(sources)\n\n    def _inject_language(self, src, strings):\n        """"""Injects languages into (potentially) template strings.""""""\n        if src not in self.sources:\n            raise ValueError(""Invalid source for \'{0}\': {1}"".format(self.name, src))\n\n        def _format_string(s):\n            if ""{0}"" in s and ""{1}"" and ""{src}"" in s:\n                return s.format(*sorted([src, self.target]), src=src)\n            elif ""{0}"" in s and ""{1}"" in s:\n                return s.format(*sorted([src, self.target]))\n            elif ""{src}"" in s:\n                return s.format(src=src)\n            else:\n                return s\n\n        return [_format_string(s) for s in strings]\n\n    def get_url(self, src):\n        return self._inject_language(src, self._urls)\n\n    def get_manual_dl_files(self, src):\n        return self._inject_language(src, self._manual_dl_files)\n\n    def get_path(self, src):\n        return self._inject_language(src, self._paths)\n\n\n# Subsets used in the training sets for various years of WMT.\n_TRAIN_SUBSETS = [\n    # pylint:disable=line-too-long\n    SubDataset(\n        name=""commoncrawl"",\n        target=""en"",  # fr-de pair in commoncrawl_frde\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n        path=(""commoncrawl.{src}-en.{src}"", ""commoncrawl.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""commoncrawl_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/commoncrawl.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""czeng_10"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng10"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""czeng_16pre"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng/czeng16pre"",\n        manual_dl_files=[""czeng16pre.deduped-ignoring-sections.txt.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""czeng_16"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        # This dataset differs from the above in the filtering that is applied\n        # during parsing.\n        name=""czeng_17"",\n        target=""en"",\n        sources={""cs""},\n        url=""http://ufal.mff.cuni.cz/czeng"",\n        manual_dl_files=[""data-plaintext-format.%d.tar"" % i for i in range(10)],\n        # Each tar contains multiple files, which we process specially in\n        # _parse_czeng.\n        path=(""data.plaintext-format/??train.gz"",) * 10,\n    ),\n    SubDataset(\n        name=""dcep_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/dcep.lv-en.v1.tgz"",\n        path=(""dcep.en-lv/dcep.lv"", ""dcep.en-lv/dcep.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n        path=(""training/europarl-v7.{src}-en.{src}"", ""training/europarl-v7.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v7_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.fr.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/europarl-v7.de.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""europarl_v8_18"",\n        target=""en"",\n        sources={""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training/europarl-v8.{src}-en.{src}"", ""training/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v8_16"",\n        target=""en"",\n        sources={""fi"", ""ro""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-ep-v8.tgz"",\n        path=(""training-parallel-ep-v8/europarl-v8.{src}-en.{src}"", ""training-parallel-ep-v8/europarl-v8.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""europarl_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""http://www.statmt.org/europarl/v9/training/europarl-v9.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""gigafren"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://www.statmt.org/wmt10/training-giga-fren.tar"",\n        path=(""giga-fren.release2.fixed.fr.gz"", ""giga-fren.release2.fixed.en.gz""),\n    ),\n    SubDataset(\n        name=""hindencorp_01"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://ufallab.ms.mff.cuni.cz/~bojar/hindencorp"",\n        manual_dl_files=[""hindencorp0.1.gz""],\n        path="""",\n    ),\n    SubDataset(\n        name=""leta_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/leta.v1.tgz"",\n        path=(""LETA-lv-en/leta.lv"", ""LETA-lv-en/leta.en""),\n    ),\n    SubDataset(\n        name=""multiun"",\n        target=""en"",\n        sources={""es"", ""fr""},\n        url=""http://www.statmt.org/wmt13/training-parallel-un.tgz"",\n        path=(""un/undoc.2000.{src}-en.{src}"", ""un/undoc.2000.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v9"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt14/training-parallel-nc-v9.tgz"",\n        path=(""training/news-commentary-v9.{src}-en.{src}"", ""training/news-commentary-v9.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v10"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fr"", ""ru""},\n        url=""http://www.statmt.org/wmt15/training-parallel-nc-v10.tgz"",\n        path=(""news-commentary-v10.{src}-en.{src}"", ""news-commentary-v10.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v11"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru""},\n        url=""http://data.statmt.org/wmt16/translation-task/training-parallel-nc-v11.tgz"",\n        path=(\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.{src}"",\n            ""training-parallel-nc-v11/news-commentary-v11.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v12"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"",\n        path=(""training/news-commentary-v12.{src}-en.{src}"", ""training/news-commentary-v12.{src}-en.en""),\n    ),\n    SubDataset(\n        name=""newscommentary_v13"",\n        target=""en"",\n        sources={""cs"", ""de"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz"",\n        path=(\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.{src}"",\n            ""training-parallel-nc-v13/news-commentary-v13.{src}-en.en"",\n        ),\n    ),\n    SubDataset(\n        name=""newscommentary_v14"",\n        target=""en"",  # fr-de pair in newscommentary_v14_frde\n        sources={""cs"", ""de"", ""kk"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.{0}-{1}.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""newscommentary_v14_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/news-commentary/v14/training/news-commentary-v14.de-fr.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""onlinebooks_v1"",\n        target=""en"",\n        sources={""lv""},\n        url=""http://data.statmt.org/wmt17/translation-task/books.lv-en.v1.tgz"",\n        path=(""farewell/farewell.lv"", ""farewell/farewell.en""),\n    ),\n    SubDataset(\n        name=""paracrawl_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-{src}.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.{src}"",\n            ""paracrawl-release1.en-{src}.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v1_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz"",\n        path=(\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.ru"",\n            ""paracrawl-release1.en-ru.zipporah0-dedup-clean.en"",\n        ),\n    ),\n    SubDataset(\n        name=""paracrawl_v3"",\n        target=""en"",  # fr-de pair in paracrawl_v3_frde\n        sources={""cs"", ""de"", ""fi"", ""lt""},\n        url=""https://s3.amazonaws.com/web-language-models/paracrawl/release3/en-{src}.bicleaner07.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""paracrawl_v3_frde"",\n        target=""de"",\n        sources={""fr""},\n        url=(\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.de.gz"",\n            ""http://data.statmt.org/wmt19/translation-task/fr-de/bitexts/de-fr.bicleaner07.fr.gz"",\n        ),\n        path=("""", """"),\n    ),\n    SubDataset(\n        name=""rapid_2016"",\n        target=""en"",\n        sources={""de"", ""et"", ""fi""},\n        url=""http://data.statmt.org/wmt18/translation-task/rapid2016.tgz"",\n        path=(""rapid2016.{0}-{1}.{src}"", ""rapid2016.{0}-{1}.en""),\n    ),\n    SubDataset(\n        name=""rapid_2016_ltfi"",\n        target=""en"",\n        sources={""fi"", ""lt""},\n        url=""https://tilde-model.s3-eu-west-1.amazonaws.com/rapid2016.en-{src}.tmx.zip"",\n        path=""rapid2016.en-{src}.tmx"",\n    ),\n    SubDataset(\n        name=""rapid_2019"",\n        target=""en"",\n        sources={""de""},\n        url=""https://s3-eu-west-1.amazonaws.com/tilde-model/rapid2019.de-en.zip"",\n        path=(""rapid2019.de-en.de"", ""rapid2019.de-en.en""),\n    ),\n    SubDataset(\n        name=""setimes_2"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-{src}.tmx.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""uncorpus_v1"",\n        target=""en"",\n        sources={""ru"", ""zh""},\n        url=""https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-{src}.tar.gz"",\n        path=(""en-{src}/UNv1.0.en-{src}.{src}"", ""en-{src}/UNv1.0.en-{src}.en""),\n    ),\n    SubDataset(\n        name=""wikiheadlines_fi"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/fi-en/titles.fi-en"",\n    ),\n    SubDataset(\n        name=""wikiheadlines_hi"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://www.statmt.org/wmt14/wiki-titles.tgz"",\n        path=""wiki/hi-en/wiki-titles.hi-en"",\n    ),\n    SubDataset(\n        # Verified that wmt14 and wmt15 files are identical.\n        name=""wikiheadlines_ru"",\n        target=""en"",\n        sources={""ru""},\n        url=""http://www.statmt.org/wmt15/wiki-titles.tgz"",\n        path=""wiki/ru-en/wiki.ru-en"",\n    ),\n    SubDataset(\n        name=""wikititles_v1"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""gu"", ""kk"", ""lt"", ""ru"", ""zh""},\n        url=""http://data.statmt.org/wikititles/v1/wikititles-v1.{src}-en.tsv.gz"",\n        path="""",\n    ),\n    SubDataset(\n        name=""yandexcorpus"",\n        target=""en"",\n        sources={""ru""},\n        url=""https://translate.yandex.ru/corpus?lang=en"",\n        manual_dl_files=[""1mcorpus.zip""],\n        path=(""corpus.en_ru.1m.ru"", ""corpus.en_ru.1m.en""),\n    ),\n    # pylint:enable=line-too-long\n] + [\n    SubDataset(  # pylint:disable=g-complex-comprehension\n        name=ss,\n        target=""en"",\n        sources={""zh""},\n        url=""ftp://cwmt-wmt:cwmt-wmt@nlp.nju.edu.cn/parallel/%s.zip"" % ss,\n        path=(""%s/*_c[hn].txt"" % ss, ""%s/*_en.txt"" % ss),\n    )\n    for ss in CWMT_SUBSET_NAMES\n]\n\n_DEV_SUBSETS = [\n    SubDataset(\n        name=""euelections_dev2019"",\n        target=""de"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/euelections_dev2019.fr-de.src.fr"", ""dev/euelections_dev2019.fr-de.tgt.de""),\n    ),\n    SubDataset(\n        name=""newsdev2014"",\n        target=""en"",\n        sources={""hi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2014.hi"", ""dev/newsdev2014.en""),\n    ),\n    SubDataset(\n        name=""newsdev2015"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2015-fien-src.{src}.sgm"", ""dev/newsdev2015-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2016"",\n        target=""en"",\n        sources={""ro"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2016-{src}en-src.{src}.sgm"", ""dev/newsdev2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2017"",\n        target=""en"",\n        sources={""lv"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2017-{src}en-src.{src}.sgm"", ""dev/newsdev2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2018"",\n        target=""en"",\n        sources={""et""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2018-{src}en-src.{src}.sgm"", ""dev/newsdev2018-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdev2019"",\n        target=""en"",\n        sources={""gu"", ""kk"", ""lt""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdev2019-{src}en-src.{src}.sgm"", ""dev/newsdev2019-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscussdev2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscussdev2015-{src}en-src.{src}.sgm"", ""dev/newsdiscussdev2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newssyscomb2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newssyscomb2009.{src}"", ""dev/newssyscomb2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2008"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hu""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/news-test2008.{src}"", ""dev/news-test2008.en""),\n    ),\n    SubDataset(\n        name=""newstest2009"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2009.{src}"", ""dev/newstest2009.en""),\n    ),\n    SubDataset(\n        name=""newstest2010"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2010.{src}"", ""dev/newstest2010.en""),\n    ),\n    SubDataset(\n        name=""newstest2011"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2011.{src}"", ""dev/newstest2011.en""),\n    ),\n    SubDataset(\n        name=""newstest2012"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2012.{src}"", ""dev/newstest2012.en""),\n    ),\n    SubDataset(\n        name=""newstest2013"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2013.{src}"", ""dev/newstest2013.en""),\n    ),\n    SubDataset(\n        name=""newstest2014"",\n        target=""en"",\n        sources={""cs"", ""de"", ""es"", ""fr"", ""hi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2014-{src}en-src.{src}.sgm"", ""dev/newstest2014-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2015"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ru""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2015-{src}en-src.{src}.sgm"", ""dev/newstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newsdiscusstest2015"",\n        target=""en"",\n        sources={""fr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newsdiscusstest2015-{src}en-src.{src}.sgm"", ""dev/newsdiscusstest2015-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2016"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""ro"", ""ru"", ""tr""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2016-{src}en-src.{src}.sgm"", ""dev/newstest2016-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2016"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2016-enfi-ref.{src}.sgm"", ""dev/newstestB2016-enfi-src.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2017"",\n        target=""en"",\n        sources={""cs"", ""de"", ""fi"", ""lv"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2017-{src}en-src.{src}.sgm"", ""dev/newstest2017-{src}en-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstestB2017"",\n        target=""en"",\n        sources={""fi""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstestB2017-fien-src.fi.sgm"", ""dev/newstestB2017-fien-ref.en.sgm""),\n    ),\n    SubDataset(\n        name=""newstest2018"",\n        target=""en"",\n        sources={""cs"", ""de"", ""et"", ""fi"", ""ru"", ""tr"", ""zh""},\n        url=""http://data.statmt.org/wmt19/translation-task/dev.tgz"",\n        path=(""dev/newstest2018-{src}en-src.{src}.sgm"", ""dev/newstest2018-{src}en-ref.en.sgm""),\n    ),\n]\n\nDATASET_MAP = {dataset.name: dataset for dataset in _TRAIN_SUBSETS + _DEV_SUBSETS}\n\n_CZENG17_FILTER = SubDataset(\n    name=""czeng17_filter"",\n    target=""en"",\n    sources={""cs""},\n    url=""http://ufal.mff.cuni.cz/czeng/download.php?f=convert_czeng16_to_17.pl.zip"",\n    path=""convert_czeng16_to_17.pl"",\n)\n\n\nclass WmtConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for WMT.""""""\n\n    def __init__(self, url=None, citation=None, description=None, language_pair=(None, None), subsets=None, **kwargs):\n        """"""BuilderConfig for WMT.\n\n    Args:\n      url: The reference URL for the dataset.\n      citation: The paper citation for the dataset.\n      description: The description of the dataset.\n      language_pair: pair of languages that will be used for translation. Should\n                 contain 2 letter coded strings. For example: (""en"", ""de"").\n        configuration for the `nlp.features.text.TextEncoder` used for the\n        `nlp.features.text.Translation` features.\n      subsets: Dict[split, list[str]]. List of the subset to use for each of the\n        split. Note that WMT subclasses overwrite this parameter.\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        name = ""%s-%s"" % (language_pair[0], language_pair[1])\n        if ""name"" in kwargs:  # Add name suffix for custom configs\n            name += ""."" + kwargs.pop(""name"")\n\n        super(WmtConfig, self).__init__(name=name, description=description, **kwargs)\n\n        self.url = url or ""http://www.statmt.org""\n        self.citation = citation\n        self.language_pair = language_pair\n        self.subsets = subsets\n\n        # TODO(PVP): remove when manual dir works\n        # +++++++++++++++++++++\n        if language_pair[1] in [""cs"", ""hi"", ""ru""]:\n            assert NotImplementedError(\n                ""The dataset for {}-en is currently not fully supported."".format(language_pair[1])\n            )\n        # +++++++++++++++++++++\n\n\nclass Wmt(ABC, nlp.GeneratorBasedBuilder):\n    """"""WMT translation dataset.""""""\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Some of the wmt configs here, require a manual download.\n  Please look into wmt.py to see the exact path (and file name) that has to\n  be downloaded.\n  """"""\n\n    def __init__(self, *args, **kwargs):\n        if type(self) == Wmt and ""config"" not in kwargs:  # pylint: disable=unidiomatic-typecheck\n            raise ValueError(\n                ""The raw `wmt_translate` can only be instantiated with the config ""\n                ""kwargs. You may want to use one of the `wmtYY_translate` ""\n                ""implementation instead to get the WMT dataset for a specific year.""\n            )\n        super(Wmt, self).__init__(*args, **kwargs)\n\n    @property\n    @abstractmethod\n    def _subsets(self):\n        """"""Subsets that make up each split of the dataset.""""""\n        raise NotImplementedError(""This is a abstract method"")\n\n    @property\n    def subsets(self):\n        """"""Subsets that make up each split of the dataset for the language pair.""""""\n        source, target = self.config.language_pair\n        filtered_subsets = {}\n        for split, ss_names in self._subsets.items():\n            filtered_subsets[split] = []\n            for ss_name in ss_names:\n                dataset = DATASET_MAP[ss_name]\n                if dataset.target != target or source not in dataset.sources:\n                    logging.info(""Skipping sub-dataset that does not include language pair: %s"", ss_name)\n                else:\n                    filtered_subsets[split].append(ss_name)\n        logging.info(""Using sub-datasets: %s"", filtered_subsets)\n        return filtered_subsets\n\n    def _info(self):\n        src, target = self.config.language_pair\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""translation"": nlp.features.Translation(languages=self.config.language_pair)}),\n            supervised_keys=(src, target),\n            homepage=self.config.url,\n            citation=self.config.citation,\n        )\n\n    def _vocab_text_gen(self, split_subsets, extraction_map, language):\n        for _, ex in self._generate_examples(split_subsets, extraction_map, with_translation=False):\n            yield ex[language]\n\n    def _split_generators(self, dl_manager):\n        source, _ = self.config.language_pair\n        manual_paths_dict = {}\n        urls_to_download = {}\n        for ss_name in itertools.chain.from_iterable(self.subsets.values()):\n            if ss_name == ""czeng_17"":\n                # CzEng1.7 is CzEng1.6 with some blocks filtered out. We must download\n                # the filtering script so we can parse out which blocks need to be\n                # removed.\n                urls_to_download[_CZENG17_FILTER.name] = _CZENG17_FILTER.get_url(source)\n\n            # get dataset\n            dataset = DATASET_MAP[ss_name]\n            if dataset.get_manual_dl_files(source):\n                # TODO(PVP): following two lines skip configs that are incomplete for now\n                # +++++++++++++++++++++\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n                # +++++++++++++++++++++\n\n                manual_dl_files = dataset.get_manual_dl_files(source)\n                manual_paths = [\n                    os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), fname)\n                    for fname in manual_dl_files\n                ]\n                assert all(\n                    os.path.exists(path) for path in manual_paths\n                ), ""For {0}, you must manually download the following file(s) from {1} and place them in {2}: {3}"".format(\n                    dataset.name, dataset.get_url(source), dl_manager.manual_dir, "", "".join(manual_dl_files)\n                )\n\n                # set manual path for correct subset\n                manual_paths_dict[ss_name] = manual_paths\n            else:\n                urls_to_download[ss_name] = dataset.get_url(source)\n\n        # Download and extract files from URLs.\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n        # Extract manually downloaded files.\n        manual_files = dl_manager.extract(manual_paths_dict)\n        extraction_map = dict(downloaded_files, **manual_files)\n\n        for language in self.config.language_pair:\n            self._vocab_text_gen(self.subsets[nlp.Split.TRAIN], extraction_map, language)\n\n        return [\n            nlp.SplitGenerator(  # pylint:disable=g-complex-comprehension\n                name=split, gen_kwargs={""split_subsets"": split_subsets, ""extraction_map"": extraction_map}\n            )\n            for split, split_subsets in self.subsets.items()\n        ]\n\n    def _generate_examples(self, split_subsets, extraction_map, with_translation=True):\n        """"""Returns the examples in the raw (text) form.""""""\n        source, _ = self.config.language_pair\n\n        def _get_local_paths(dataset, extract_dirs):\n            rel_paths = dataset.get_path(source)\n            if len(extract_dirs) == 1:\n                extract_dirs = extract_dirs * len(rel_paths)\n            return [\n                os.path.join(ex_dir, rel_path) if rel_path else ex_dir\n                for ex_dir, rel_path in zip(extract_dirs, rel_paths)\n            ]\n\n        for ss_name in split_subsets:\n            # TODO(PVP) remove following five lines when manual data works\n            # +++++++++++++++++++++\n            dataset = DATASET_MAP[ss_name]\n            source, _ = self.config.language_pair\n            if dataset.get_manual_dl_files(source):\n                logging.info(""Skipping {} for now. Incomplete dataset for {}"".format(dataset.name, self.config.name))\n                continue\n            # +++++++++++++++++++++\n\n            logging.info(""Generating examples from: %s"", ss_name)\n            dataset = DATASET_MAP[ss_name]\n            extract_dirs = extraction_map[ss_name]\n            files = _get_local_paths(dataset, extract_dirs)\n\n            if ss_name.startswith(""czeng""):\n                if ss_name.endswith(""16pre""):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=(""en"", ""cs""))\n                elif ss_name.endswith(""17""):\n                    filter_path = _get_local_paths(_CZENG17_FILTER, extraction_map[_CZENG17_FILTER.name])[0]\n                    sub_generator = functools.partial(_parse_czeng, filter_path=filter_path)\n                else:\n                    sub_generator = _parse_czeng\n            elif ss_name == ""hindencorp_01"":\n                sub_generator = _parse_hindencorp\n            elif len(files) == 2:\n                if ss_name.endswith(""_frde""):\n                    sub_generator = _parse_frde_bitext\n                else:\n                    sub_generator = _parse_parallel_sentences\n            elif len(files) == 1:\n                fname = files[0]\n                # Note: Due to formatting used by `download_manager`, the file\n                # extension may not be at the end of the file path.\n                if "".tsv"" in fname:\n                    sub_generator = _parse_tsv\n                elif (\n                    ss_name.startswith(""newscommentary_v14"")\n                    or ss_name.startswith(""europarl_v9"")\n                    or ss_name.startswith(""wikititles_v1"")\n                ):\n                    sub_generator = functools.partial(_parse_tsv, language_pair=self.config.language_pair)\n                elif ""tmx"" in fname or ss_name.startswith(""paracrawl_v3""):\n                    sub_generator = _parse_tmx\n                elif ss_name.startswith(""wikiheadlines""):\n                    sub_generator = _parse_wikiheadlines\n                else:\n                    raise ValueError(""Unsupported file format: %s"" % fname)\n            else:\n                raise ValueError(""Invalid number of files: %d"" % len(files))\n\n            for sub_key, ex in sub_generator(*files):\n                if not all(ex.values()):\n                    continue\n                # TODO(adarob): Add subset feature.\n                # ex[""subset""] = subset\n                key = ""{}/{}"".format(ss_name, sub_key)\n                if with_translation is True:\n                    ex = {""translation"": ex}\n                yield key, ex\n\n\ndef _parse_parallel_sentences(f1, f2):\n    """"""Returns examples from parallel SGML or text files, which may be gzipped.""""""\n\n    def _parse_text(path):\n        """"""Returns the sentences from a single text file, which may be gzipped.""""""\n        split_path = path.split(""."")\n\n        if split_path[-1] == ""gz"":\n            lang = split_path[-2]\n            with open(path, ""rb"") as f, gzip.GzipFile(fileobj=f) as g:\n                return g.read().decode(""utf-8"").split(""\\n""), lang\n\n        if split_path[-1] == ""txt"":\n            # CWMT\n            lang = split_path[-2].split(""_"")[-1]\n            lang = ""zh"" if lang in (""ch"", ""cn"") else lang\n        else:\n            lang = split_path[-1]\n        with open(path, ""rb"") as f:\n            return f.read().decode(""utf-8"").split(""\\n""), lang\n\n    def _parse_sgm(path):\n        """"""Returns sentences from a single SGML file.""""""\n        lang = path.split(""."")[-2]\n        sentences = []\n        # Note: We can\'t use the XML parser since some of the files are badly\n        # formatted.\n        seg_re = re.compile(r""<seg id=\\""\\d+\\"">(.*)</seg>"")\n        with open(path) as f:\n            for line in f:\n                seg_match = re.match(seg_re, line)\n                if seg_match:\n                    assert len(seg_match.groups()) == 1\n                    sentences.append(seg_match.groups()[0])\n        return sentences, lang\n\n    parse_file = _parse_sgm if f1.endswith("".sgm"") else _parse_text\n\n    # Some datasets (e.g., CWMT) contain multiple parallel files specified with\n    # a wildcard. We sort both sets to align them and parse them one by one.\n    f1_files = sorted(glob.glob(f1))\n    f2_files = sorted(glob.glob(f2))\n\n    assert f1_files and f2_files, ""No matching files found: %s, %s."" % (f1, f2)\n    assert len(f1_files) == len(f2_files), ""Number of files do not match: %d vs %d for %s vs %s."" % (\n        len(f1_files),\n        len(f2_files),\n        f1,\n        f2,\n    )\n\n    for f_id, (f1_i, f2_i) in enumerate(zip(sorted(f1_files), sorted(f2_files))):\n        l1_sentences, l1 = parse_file(f1_i)\n        l2_sentences, l2 = parse_file(f2_i)\n\n        assert len(l1_sentences) == len(l2_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n            len(l1_sentences),\n            len(l2_sentences),\n            f1_i,\n            f2_i,\n        )\n\n        for line_id, (s1, s2) in enumerate(zip(l1_sentences, l2_sentences)):\n            key = ""{}/{}"".format(f_id, line_id)\n            yield key, {l1: s1, l2: s2}\n\n\ndef _parse_frde_bitext(fr_path, de_path):\n    with open(fr_path) as f:\n        fr_sentences = f.read().split(""\\n"")\n    with open(de_path) as f:\n        de_sentences = f.read().split(""\\n"")\n    assert len(fr_sentences) == len(de_sentences), ""Sizes do not match: %d vs %d for %s vs %s."" % (\n        len(fr_sentences),\n        len(de_sentences),\n        fr_path,\n        de_path,\n    )\n    for line_id, (s1, s2) in enumerate(zip(fr_sentences, de_sentences)):\n        yield line_id, {""fr"": s1, ""de"": s2}\n\n\ndef _parse_tmx(path):\n    """"""Generates examples from TMX file.""""""\n\n    def _get_tuv_lang(tuv):\n        for k, v in tuv.items():\n            if k.endswith(""}lang""):\n                return v\n        raise AssertionError(""Language not found in `tuv` attributes."")\n\n    def _get_tuv_seg(tuv):\n        segs = tuv.findall(""seg"")\n        assert len(segs) == 1, ""Invalid number of segments: %d"" % len(segs)\n        return segs[0].text\n\n    with open(path, ""rb"") as f:\n        if six.PY3:\n            # Workaround due to: https://github.com/tensorflow/tensorflow/issues/33563\n            utf_f = codecs.getreader(""utf-8"")(f)\n        else:\n            utf_f = f\n        for line_id, (_, elem) in enumerate(ElementTree.iterparse(utf_f)):\n            if elem.tag == ""tu"":\n                yield line_id, {_get_tuv_lang(tuv): _get_tuv_seg(tuv) for tuv in elem.iterfind(""tuv"")}\n                elem.clear()\n\n\ndef _parse_tsv(path, language_pair=None):\n    """"""Generates examples from TSV file.""""""\n    if language_pair is None:\n        lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])\\.tsv"", path)\n        assert lang_match is not None, ""Invalid TSV filename: %s"" % path\n        l1, l2 = lang_match.groups()\n    else:\n        l1, l2 = language_pair\n    with open(path) as f:\n        for j, line in enumerate(f):\n            cols = line.split(""\\t"")\n            if len(cols) != 2:\n                logging.warning(""Skipping line %d in TSV (%s) with %d != 2 columns."", j, path, len(cols))\n                continue\n            s1, s2 = cols\n            yield j, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_wikiheadlines(path):\n    """"""Generates examples from Wikiheadlines dataset file.""""""\n    lang_match = re.match(r"".*\\.([a-z][a-z])-([a-z][a-z])$"", path)\n    assert lang_match is not None, ""Invalid Wikiheadlines filename: %s"" % path\n    l1, l2 = lang_match.groups()\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            s1, s2 = line.split(""|||"")\n            yield line_id, {l1: s1.strip(), l2: s2.strip()}\n\n\ndef _parse_czeng(*paths, **kwargs):\n    """"""Generates examples from CzEng v1.6, with optional filtering for v1.7.""""""\n    filter_path = kwargs.get(""filter_path"", None)\n    if filter_path:\n        re_block = re.compile(r""^[^-]+-b(\\d+)-\\d\\d[tde]"")\n        with open(filter_path) as f:\n            bad_blocks = {blk for blk in re.search(r""qw{([\\s\\d]*)}"", f.read()).groups()[0].split()}\n        logging.info(""Loaded %d bad blocks to filter from CzEng v1.6 to make v1.7."", len(bad_blocks))\n\n    for path in paths:\n        for gz_path in sorted(glob.glob(path)):\n            with open(gz_path, ""rb"") as g, gzip.GzipFile(fileobj=g) as f:\n                filename = os.path.basename(gz_path)\n                for line_id, line in enumerate(f):\n                    line = line.decode(""utf-8"")  # required for py3\n                    if not line.strip():\n                        continue\n                    id_, unused_score, cs, en = line.split(""\\t"")\n                    if filter_path:\n                        block_match = re.match(re_block, id_)\n                        if block_match and block_match.groups()[0] in bad_blocks:\n                            continue\n                    sub_key = ""{}/{}"".format(filename, line_id)\n                    yield sub_key, {\n                        ""cs"": cs.strip(),\n                        ""en"": en.strip(),\n                    }\n\n\ndef _parse_hindencorp(path):\n    with open(path) as f:\n        for line_id, line in enumerate(f):\n            split_line = line.split(""\\t"")\n            if len(split_line) != 5:\n                logging.warning(""Skipping invalid HindEnCorp line: %s"", line)\n                continue\n            yield line_id, {""translation"": {""en"": split_line[3].strip(), ""hi"": split_line[4].strip()}}\n'"
datasets/wnut_17/wnut_17.py,0,"b'# coding=utf-8\n# Copyright 2020 HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""The WNUT 17 Emerging Entities Dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport glob\nimport logging\nimport os\nfrom pathlib import Path\n\nimport nlp\n\n\n_CITATION = """"""\\\n@inproceedings{derczynski-etal-2017-results,\n    title = ""Results of the {WNUT}2017 Shared Task on Novel and Emerging Entity Recognition"",\n    author = ""Derczynski, Leon  and\n      Nichols, Eric  and\n      van Erp, Marieke  and\n      Limsopatham, Nut"",\n    booktitle = ""Proceedings of the 3rd Workshop on Noisy User-generated Text"",\n    month = sep,\n    year = ""2017"",\n    address = ""Copenhagen, Denmark"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/W17-4418"",\n    doi = ""10.18653/v1/W17-4418"",\n    pages = ""140--147"",\n    abstract = ""This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions.\n                Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization),\n                but recall on them is a real problem in noisy text - even among annotators.\n                This drop tends to be due to novel entities and surface forms.\n                Take for example the tweet {``}so.. kktny in 30 mins?!{\'\'} {--} even human experts find the entity {`}kktny{\'}\n                hard to detect and resolve. The goal of this task is to provide a definition of emerging and of rare entities,\n                and based on that, also datasets for detecting these entities. The task as described in this paper evaluated the\n                ability of participating entries to detect and classify novel and emerging named entities in noisy text."",\n}\n""""""\n\n_DESCRIPTION = """"""\\\nWNUT 17: Emerging and Rare entity recognition\n\nThis shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions.\nNamed entities form the basis of many modern approaches to other tasks (like event clustering and summarisation),\nbut recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms.\nTake for example the tweet \xe2\x80\x9cso.. kktny in 30 mins?\xe2\x80\x9d - even human experts find entity kktny hard to detect and resolve.\nThis task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.\n\nThe goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities.\n""""""\n\n_URL = ""https://raw.githubusercontent.com/leondz/emerging_entities_17/master/""\n_TRAINING_FILE = ""wnut17train.conll""\n_DEV_FILE = ""emerging.dev.conll""\n_TEST_FILE = ""emerging.test.annotated""\n\n\nclass WNUT_17Config(nlp.BuilderConfig):\n    """"""The WNUT 17 Emerging Entities Dataset.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for WNUT 17.\n\n    Args:\n      **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(WNUT_17Config, self).__init__(**kwargs)\n\n\nclass WNUT_17(nlp.GeneratorBasedBuilder):\n    """"""The WNUT 17 Emerging Entities Dataset.""""""\n\n    BUILDER_CONFIGS = [\n        WNUT_17Config(\n            name=""wnut_17"", version=nlp.Version(""1.0.0""), description=""The WNUT 17 Emerging Entities Dataset""\n        ),\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""tokens"": nlp.Sequence(nlp.Value(""string"")),\n                    ""labels"": nlp.Sequence(nlp.Value(""string"")),\n                }\n            ),\n            supervised_keys=None,\n            homepage=""http://noisy-text.github.io/2017/emerging-rare-entities.html"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        urls_to_download = {\n            ""train"": f""{_URL}{_TRAINING_FILE}"",\n            ""dev"": f""{_URL}{_DEV_FILE}"",\n            ""test"": f""{_URL}{_TEST_FILE}"",\n        }\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n            nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]}),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": downloaded_files[""test""]}),\n        ]\n\n    def _generate_examples(self, filepath):\n        logging.info(""\xe2\x8f\xb3 Generating examples from = %s"", filepath)\n        with open(filepath) as f:\n            current_tokens = []\n            current_labels = []\n            sentence_counter = 0\n            for row in f:\n                row = row.rstrip()\n                if row:\n                    token, label = row.split(""\\t"")\n                    current_tokens.append(token)\n                    current_labels.append(label)\n                else:\n                    # New sentence\n                    if not current_tokens:\n                        # Consecutive empty lines will cause empty sentences\n                        continue\n                    assert len(current_tokens) == len(current_labels), ""\xf0\x9f\x92\x94 between len of tokens & labels""\n                    sentence = (\n                        sentence_counter,\n                        {""id"": str(sentence_counter), ""tokens"": current_tokens, ""labels"": current_labels,},\n                    )\n                    sentence_counter += 1\n                    current_tokens = []\n                    current_labels = []\n                    yield sentence\n            # Don\'t forget last sentence in dataset \xf0\x9f\xa7\x90\n            if current_tokens:\n                yield sentence_counter, {\n                    ""id"": str(sentence_counter),\n                    ""tokens"": current_tokens,\n                    ""labels"": current_labels,\n                }\n'"
datasets/x_stance/x_stance.py,0,"b'""""""TODO(x_stance): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(x_stance): BibTeX citation\n_CITATION = """"""\\\n@article{vamvas2020xstance,\n  title={X-Stance: A Multilingual Multi-Target Dataset for Stance Detection},\n  author={Vamvas, Jannis and Sennrich, Rico},\n  journal={arXiv preprint arXiv:2003.08385},\n  url = ""https://arxiv.org/abs/2003.08385"",\n  year={2020}\n}\n""""""\n\n# TODO(x_stance):\n_DESCRIPTION = """"""\\\nThe x-stance dataset contains more than 150 political questions, and 67k comments written by candidates on those questions.\n\nIt can be used to train and evaluate stance detection systems.\n\n""""""\n\n_URL = ""http://tiny.uzh.ch/12p""\n\n\nclass XStance(nlp.GeneratorBasedBuilder):\n    """"""TODO(x_stance): Short description of my dataset.""""""\n\n    # TODO(x_stance): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n\n    def _info(self):\n        # TODO(x_stance): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""question"": nlp.Value(""string""),\n                    ""id"": nlp.Value(""int32""),\n                    ""question_id"": nlp.Value(""int32""),\n                    ""language"": nlp.Value(""string""),\n                    ""comment"": nlp.Value(""string""),\n                    ""label"": nlp.Value(""string""),\n                    ""numerical_label"": nlp.Value(""int32""),\n                    ""author"": nlp.Value(""string""),\n                    ""topic"": nlp.Value(""string"")\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/ZurichNLP/xstance"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(x_stance): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""train.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""test.jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(dl_dir, ""valid.jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(x_stance): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for id_, row in enumerate(f):\n                data = json.loads(row)\n\n                yield id_, {\n                    ""id"": data[""id""],\n                    ""question_id"": data[""question_id""],\n                    ""question"": data[""question""],\n                    ""comment"": data[""comment""],\n                    ""label"": data[""label""],\n                    ""author"": data[""author""],\n                    ""numerical_label"": data[""numerical_label""],\n                    ""topic"": data[""topic""],\n                    ""language"": data[""language""],\n                }\n'"
datasets/xcopa/xcopa.py,0,"b'""""""TODO(xcopa): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport nlp\n\n\n# TODO(xcopa): BibTeX citation\n_CITATION = """"""\\\n  @article{ponti2020xcopa,\n  title={{XCOPA: A} Multilingual Dataset for Causal Commonsense Reasoning},\n  author={Edoardo M. Ponti, Goran Glava\\v{s}, Olga Majewska, Qianchu Liu, Ivan Vuli\\\'{c} and Anna Korhonen},\n  journal={arXiv preprint},\n  year={2020},\n  url={https://ducdauge.github.io/files/xcopa.pdf}\n}\n\n@inproceedings{roemmele2011choice,\n  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},\n  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},\n  booktitle={2011 AAAI Spring Symposium Series},\n  year={2011},\n  url={https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF},\n}\n""""""\n\n# TODO(xcopa):\n_DESCRIPTION = """"""\\\n  XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning\nThe Cross-lingual Choice of Plausible Alternatives dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across \nlanguages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around \nthe globe. The dataset is challenging as it requires both the command of world knowledge and the ability to generalise to new languages. All the details about the \ncreation of XCOPA and the implementation of the baselines are available in the paper.\\n\n""""""\n\n_LANG = [""et"", ""ht"", ""it"", ""id"", ""qu"", ""sw"", ""zh"", ""ta"", ""th"", ""tr"", ""vi""]\n\n_URL = ""https://github.com/cambridgeltl/xcopa/archive/master.zip""\n\n\nclass XcopaConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Break""""""\n\n    def __init__(self, **kwargs):\n        """"""\n\n        Args:\n            data_dir: directory for the given language dataset\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(XcopaConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass Xcopa(nlp.GeneratorBasedBuilder):\n    """"""TODO(xcopa): Short description of my dataset.""""""\n\n    # TODO(xcopa): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    BUILDER_CONFIGS = [XcopaConfig(name=lang, description=""Xcopa language {}"".format(lang),) for lang in _LANG]\n\n    def _info(self):\n        # TODO(xcopa): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION + self.config.description,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    # These are the features of your dataset like images, labels ...\n                    ""premise"": nlp.Value(""string""),\n                    ""choice1"": nlp.Value(""string""),\n                    ""choice2"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""label"": nlp.Value(""int32""),\n                    ""idx"": nlp.Value(""int32""),\n                    ""changed"": nlp.Value(""bool""),\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/cambridgeltl/xcopa"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(xcopa): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        dl_dir = dl_manager.download_and_extract(_URL)\n\n        data_dir = os.path.join(dl_dir, ""xcopa-master"", ""data"", self.config.name)\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""test."" + self.config.name + "".jsonl"")},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": os.path.join(data_dir, ""val."" + self.config.name + "".jsonl"")},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(xcopa): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            for row in f:\n                data = json.loads(row)\n                idx = data[""idx""]\n                yield idx, data\n'"
datasets/xnli/xnli.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""XNLI: The Cross-Lingual NLI Corpus.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport collections\nimport csv\nimport os\n\nimport six\n\nimport nlp\n\n\n_CITATION = """"""\\\n@InProceedings{conneau2018xnli,\n  author = ""Conneau, Alexis\n                 and Rinott, Ruty\n                 and Lample, Guillaume\n                 and Williams, Adina\n                 and Bowman, Samuel R.\n                 and Schwenk, Holger\n                 and Stoyanov, Veselin"",\n  title = ""XNLI: Evaluating Cross-lingual Sentence Representations"",\n  booktitle = ""Proceedings of the 2018 Conference on Empirical Methods\n               in Natural Language Processing"",\n  year = ""2018"",\n  publisher = ""Association for Computational Linguistics"",\n  location = ""Brussels, Belgium"",\n}""""""\n\n_DESCRIPTION = """"""\\\nXNLI is a subset of a few thousand examples from MNLI which has been translated\ninto a 14 different languages (some low-ish resource). As with MNLI, the goal is\nto predict textual entailment (does sentence A imply/contradict/neither sentence\nB) and is a classification task (given two sentences, predict one of three\nlabels).\n""""""\n\n_DATA_URL = ""https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip""\n\n_LANGUAGES = (""ar"", ""bg"", ""de"", ""el"", ""en"", ""es"", ""fr"", ""hi"", ""ru"", ""sw"", ""th"", ""tr"", ""ur"", ""vi"", ""zh"")\n\n\nclass Xnli(nlp.GeneratorBasedBuilder):\n    """"""XNLI: The Cross-Lingual NLI Corpus. Version 1.0.""""""\n\n    BUILDER_CONFIGS = [\n        nlp.BuilderConfig(\n            name=""plain_text"",\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""),\n            description=""Plain text import of XNLI"",\n        )\n    ]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features(\n                {\n                    ""premise"": nlp.features.Translation(languages=_LANGUAGES,),\n                    ""hypothesis"": nlp.features.TranslationVariableLanguages(languages=_LANGUAGES,),\n                    ""label"": nlp.features.ClassLabel(names=[""entailment"", ""neutral"", ""contradiction""]),\n                }\n            ),\n            # No default supervised_keys (as we have to pass both premise\n            # and hypothesis as input).\n            supervised_keys=None,\n            homepage=""https://www.nyu.edu/projects/bowman/xnli/"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        dl_dir = dl_manager.download_and_extract(_DATA_URL)\n        data_dir = os.path.join(dl_dir, ""XNLI-1.0"")\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": os.path.join(data_dir, ""xnli.test.tsv"")}),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": os.path.join(data_dir, ""xnli.dev.tsv"")}\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""This function returns the examples in the raw (text) form.""""""\n        rows_per_pair_id = collections.defaultdict(list)\n\n        with open(filepath) as f:\n            reader = csv.DictReader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n            for row in reader:\n                rows_per_pair_id[row[""pairID""]].append(row)\n\n        for rows in six.itervalues(rows_per_pair_id):\n            premise = {row[""language""]: row[""sentence1""] for row in rows}\n            hypothesis = {row[""language""]: row[""sentence2""] for row in rows}\n            yield rows[0][""pairID""], {\n                ""premise"": premise,\n                ""hypothesis"": hypothesis,\n                ""label"": rows[0][""gold_label""],\n            }\n'"
datasets/xquad/xquad.py,0,"b'""""""TODO(xquad): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\n\nimport nlp\n\n\n_CITATION = """"""\\\n@article{Artetxe:etal:2019,\n      author    = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},\n      title     = {On the cross-lingual transferability of monolingual representations},\n      journal   = {CoRR},\n      volume    = {abs/1910.11856},\n      year      = {2019},\n      archivePrefix = {arXiv},\n      eprint    = {1910.11856}\n}\n""""""\n\n_DESCRIPTION = """"""\\\nXQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering\nperformance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set\nof SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German,\nGreek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the dataset is entirely parallel\nacross 11 languages.\n""""""\n\n_URL = ""https://github.com/deepmind/xquad/raw/master/""\n_LANG = [""ar"", ""de"", ""zh"", ""vi"", ""en"", ""es"", ""hi"", ""el"", ""th"", ""tr"", ""ru""]\n\n\nclass XquadConfig(nlp.BuilderConfig):\n\n    """""" BuilderConfig for Xquad""""""\n\n    def __init__(self, lang, **kwargs):\n        """"""\n\n        Args:\n            lang: string, language for the input text\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(XquadConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n        self.lang = lang\n\n\nclass Xquad(nlp.GeneratorBasedBuilder):\n    """"""TODO(xquad): Short description of my dataset.""""""\n\n    # TODO(xquad): Set up version.\n    VERSION = nlp.Version(""1.0.0"")\n    BUILDER_CONFIGS = [\n        XquadConfig(name=""xquad.{}"".format(lang), description=_DESCRIPTION, lang=lang) for lang in _LANG\n    ]\n\n    def _info(self):\n        # TODO(xquad): Specifies the nlp.DatasetInfo object\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=_DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                {\n                    ""id"": nlp.Value(""string""),\n                    ""context"": nlp.Value(""string""),\n                    ""question"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                    # These are the features of your dataset like images, labels ...\n                }\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/deepmind/xquad"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(xquad): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n        urls_to_download = {lang: _URL + ""xquad.{}.json"".format(lang) for lang in _LANG}\n        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={""filepath"": downloaded_files[self.config.lang]},\n            ),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(xquad): Yields (key, example) tuples from the dataset\n        with open(filepath) as f:\n            xquad = json.load(f)\n            for article in xquad[""data""]:\n                for paragraph in article[""paragraphs""]:\n                    context = paragraph[""context""].strip()\n                    for qa in paragraph[""qas""]:\n                        question = qa[""question""].strip()\n                        id_ = qa[""id""]\n\n                        answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                        answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                        # Features currently used are ""context"", ""question"", and ""answers"".\n                        # Others are extracted here for the ease of future expansions.\n                        yield id_, {\n                            ""context"": context,\n                            ""question"": question,\n                            ""id"": id_,\n                            ""answers"": {""answer_start"": answer_starts, ""text"": answers,},\n                        }\n'"
datasets/xsum/xsum.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""XSum dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport logging\nimport os\n\nimport nlp\n\n\n_CITATION = """"""\n@article{Narayan2018DontGM,\n  title={Don\'t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},\n  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},\n  journal={ArXiv},\n  year={2018},\n  volume={abs/1808.08745}\n}\n""""""\n\n_DESCRIPTION = """"""\nExtreme Summarization (XSum) Dataset.\n\nThere are two features:\n  - document: Input news article.\n  - summary: One sentence summary of the article.\n\nThis data need to manaully downloaded and extracted as described in\nhttps://github.com/EdinburghNLP/XSum/blob/master/XSum-Dataset/README.md.\nThe folder \'xsum-extracts-from-downloads\' need to be compressed as\n\'xsum-extracts-from-downloads.tar.gz\' and put in manually downloaded folder.\n""""""\n\n_URL = (\n    ""https://raw.githubusercontent.com/EdinburghNLP/XSum/master/XSum-Dataset/XSum-TRAINING-DEV-TEST-SPLIT-90-5-5.json""\n)\n\n_DOCUMENT = ""document""\n_SUMMARY = ""summary""\n\n_REMOVE_LINES = set(\n    [\n        ""Share this with\\n"",\n        ""Email\\n"",\n        ""Facebook\\n"",\n        ""Messenger\\n"",\n        ""Twitter\\n"",\n        ""Pinterest\\n"",\n        ""WhatsApp\\n"",\n        ""Linkedin\\n"",\n        ""LinkedIn\\n"",\n        ""Copy this link\\n"",\n        ""These are external links and will open in a new window\\n"",\n    ]\n)\n\n\nclass Xsum(nlp.GeneratorBasedBuilder):\n    """"""Extreme Summarization (XSum) Dataset.""""""\n\n    # Version 1.1.0 removes web contents.\n    VERSION = nlp.Version(""1.1.0"")\n    SUPPORTED_VERSIONS = [nlp.Version(""1.0.0"", ""Dataset without cleaning."")]\n\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n  Detailed download instructions (which require running a custom script) are\n  here:\n  https://github.com/EdinburghNLP/XSum/blob/master/XSum-Dataset/README.md . Please make sure you run download-bbc-articles.py and parse-bbc-html-data.py scripts\n \n\n  """"""\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({_DOCUMENT: nlp.Value(""string""), _SUMMARY: nlp.Value(""string""),}),\n            supervised_keys=(_DOCUMENT, _SUMMARY),\n            homepage=""https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset"",\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        dl_path = dl_manager.download_and_extract(_URL)\n        with open(dl_path, ""r"") as json_file:\n            split_ids = json.load(json_file)\n        downloaded_path = os.path.join(dl_manager.manual_dir, ""xsum-extracts-from-downloads"")\n        if not os.path.exists(downloaded_path):\n            raise FileNotFoundError(\n                ""{} does not exist. Make sure you indicate the data_dir as  `nlp.load(\'xsum\', data_dir=...), which points to your downloded dataset\'. Manual download instructions: {})"".format(\n                    downloaded_path, self.MANUAL_DOWNLOAD_INSTRUCTIONS\n                )\n            )\n        return [\n            nlp.SplitGenerator(\n                name=nlp.Split.TRAIN, gen_kwargs={""split_ids"": split_ids[""train""], ""path"": downloaded_path,},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.VALIDATION, gen_kwargs={""split_ids"": split_ids[""validation""], ""path"": downloaded_path,},\n            ),\n            nlp.SplitGenerator(\n                name=nlp.Split.TEST, gen_kwargs={""split_ids"": split_ids[""test""], ""path"": downloaded_path,},\n            ),\n        ]\n\n    def _generate_examples(self, split_ids=None, path=None):\n        """"""Yields examples.""""""\n        missing = 0\n        total_num = len(split_ids)\n        for i in split_ids:\n            filename = os.path.join(path, i + "".data"")\n            print(filename)\n\n            if os.path.exists(filename):\n                with open(filename) as f:\n\n                    text = """".join([line for line in f.readlines() if line not in _REMOVE_LINES and line.strip()])\n\n                    # Each file follows below format:\n                    # [XSUM]URL[XSUM]\n                    # http://somelink\n                    #\n                    # [XSUM]INTRODUCTION[XSUM]\n                    # some intro\n                    #\n                    # [XSUM]RESTBODY[XSUM]\n                    # text line.\n                    # another text line.\n                    # ""another text line.""\n                    segs = text.split(""[XSUM]"")\n                    yield i, {_DOCUMENT: segs[6].strip(), _SUMMARY: segs[4].strip()}\n            else:\n                missing += 1\n                logging.info(""id %s missing."", i)\n        if missing:\n            logging.warning(""%d out of %d examples are missing."", missing, total_num)\n'"
datasets/xtreme/xtreme.py,0,"b'""""""TODO(xtreme): Add a description here.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport glob\nimport json\nimport os\nimport textwrap\n\nimport six\n\nimport nlp\n\n\n# TODO(xtreme): BibTeX citation\n_CITATION = """"""\\\n@article{hu2020xtreme,\n      author    = {Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},\n      title     = {XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization},\n      journal   = {CoRR},\n      volume    = {abs/2003.11080},\n      year      = {2020},\n      archivePrefix = {arXiv},\n      eprint    = {2003.11080}\n}\n""""""\n\n# TODO(xtrem):\n_DESCRIPTION = """"""\\\nThe Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark is a benchmark for the evaluation of\nthe cross-lingual generalization ability of pre-trained multilingual models. It covers 40 typologically diverse languages\n(spanning 12 language families) and includes nine tasks that collectively require reasoning about different levels of\nsyntax and semantics. The languages in XTREME are selected to maximize language diversity, coverage in existing tasks,\nand availability of training data. Among these are many under-studied languages, such as the Dravidian languages Tamil\n(spoken in southern India, Sri Lanka, and Singapore), Telugu and Malayalam (spoken mainly in southern India), and the\nNiger-Congo languages Swahili and Yoruba, spoken in Africa.\n""""""\n_MLQA_LANG = [""ar"", ""de"", ""vi"", ""zh"", ""en"", ""es"", ""hi""]\n_XQUAD_LANG = [""ar"", ""de"", ""vi"", ""zh"", ""en"", ""es"", ""hi"", ""el"", ""ru"", ""th"", ""tr""]\n_PAWSX_LANG = [""de"", ""en"", ""fr"", ""ja"", ""ko"", ""zh""]\n_BUCC_LANG = [""de"", ""fr"", ""zh"", ""ru""]\n_TATOEBA_LANG = [\n    ""afr"",\n    ""ara"",\n    ""ben"",\n    ""bul"",\n    ""deu"",\n    ""cmn"",\n    ""ell"",\n    ""est"",\n    ""eus"",\n    ""fin"",\n    ""fra"",\n    ""heb"",\n    ""hin"",\n    ""hun"",\n    ""ind"",\n    ""ita"",\n    ""jav"",\n    ""jpn"",\n    ""kat"",\n    ""kaz"",\n    ""kor"",\n    ""mal"",\n    ""mar"",\n    ""nld"",\n    ""pes"",\n    ""por"",\n    ""rus"",\n    ""spa"",\n    ""swh"",\n    ""tam"",\n    ""tgl"",\n    ""tha"",\n    ""tur"",\n    ""urd"",\n    ""vie"",\n]\n\n_UD_POS_LANG = [\n    ""Afrikaans"",\n    ""Arabic"",\n    ""Basque"",\n    ""Bulgarian"",\n    ""Dutch"",\n    ""English"",\n    ""Estonian"",\n    ""Finnish"",\n    ""French"",\n    ""German"",\n    ""Greek"",\n    ""Hebrew"",\n    ""Hindi"",\n    ""Hungarian"",\n    ""Indonesian"",\n    ""Italian"",\n    ""Japanese"",\n    ""Kazakh"",\n    ""Korean"",\n    ""Chinese"",\n    ""Marathi"",\n    ""Persian"",\n    ""Portuguese"",\n    ""Russian"",\n    ""Spanish"",\n    ""Tagalog"",\n    ""Tamil"",\n    ""Telugu"",\n    ""Thai"",\n    ""Turkish"",\n    ""Urdu"",\n    ""Vietnamese"",\n    ""Yoruba"",\n]\n_PAN_X_LANG = [\n    ""af"",\n    ""ar"",\n    ""bg"",\n    ""bn"",\n    ""de"",\n    ""el"",\n    ""en"",\n    ""es"",\n    ""et"",\n    ""eu"",\n    ""fa"",\n    ""fi"",\n    ""fr"",\n    ""he"",\n    ""hi"",\n    ""hu"",\n    ""id"",\n    ""it"",\n    ""ja"",\n    ""jv"",\n    ""ka"",\n    ""kk"",\n    ""ko"",\n    ""ml"",\n    ""mr"",\n    ""ms"",\n    ""my"",\n    ""nl"",\n    ""pt"",\n    ""ru"",\n    ""sw"",\n    ""ta"",\n    ""te"",\n    ""th"",\n    ""tl"",\n    ""tr"",\n    ""ur"",\n    ""vi"",\n    ""yo"",\n    ""zh"",\n]\n_PAN_X_FOLDER = ""AmazonPhotos.zip""\n_NAMES = [""XNLI"", ""tydiqa"", ""SQuAD""]\nfor lang in _PAN_X_LANG:\n    _NAMES.append(""PAN-X.{}"".format(lang))\nfor lang1 in _MLQA_LANG:\n    for lang2 in _MLQA_LANG:\n        _NAMES.append(""MLQA.{}.{}"".format(lang1, lang2))\nfor lang in _XQUAD_LANG:\n    _NAMES.append(""XQuAD.{}"".format(lang))\nfor lang in _BUCC_LANG:\n    _NAMES.append(""bucc18.{}"".format(lang))\nfor lang in _PAWSX_LANG:\n    _NAMES.append(""PAWS-X.{}"".format(lang))\nfor lang in _TATOEBA_LANG:\n    _NAMES.append(""tatoeba.{}"".format(lang))\nfor lang in _UD_POS_LANG:\n    _NAMES.append(""udpos.{}"".format(lang))\n\n_DESCRIPTIONS = {\n    ""tydiqa"": textwrap.dedent(\n        """"""Gold passage task (GoldP): Given a passage that is guaranteed to contain the\n             answer, predict the single contiguous span of characters that answers the question. This is more similar to\n             existing reading comprehension datasets (as opposed to the information-seeking task outlined above).\n             This task is constructed with two goals in mind: (1) more directly comparing with prior work and (2) providing\n             a simplified way for researchers to use TyDi QA by providing compatibility with existing code for SQuAD 1.1,\n             XQuAD, and MLQA. Toward these goals, the gold passage task differs from the primary task in several ways:\n             only the gold answer passage is provided rather than the entire Wikipedia article;\n             unanswerable questions have been discarded, similar to MLQA and XQuAD;\n             we evaluate with the SQuAD 1.1 metrics like XQuAD; and\n            Thai and Japanese are removed since the lack of whitespace breaks some tools.\n             """"""\n    ),\n    ""XNLI"": textwrap.dedent(\n        """"""\n          The Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\n          2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\n          14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\n          Hindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated with the\n          corresponding hypothesis in the 15 languages, summing up to more than 1.5M combinations. The corpus is made to\n          evaluate how to perform inference in any language (including low-resources ones like Swahili or Urdu) when only\n          English NLI data is available at training time. One solution is cross-lingual sentence encoding, for which XNLI\n          is an evaluation benchmark.""""""\n    ),\n    ""PAWS-X"": textwrap.dedent(\n        """"""\n          This dataset contains 23,659 human translated PAWS evaluation pairs and 296,406 machine translated training\n          pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. All\n          translated pairs are sourced from examples in PAWS-Wiki.""""""\n    ),\n    ""XQuAD"": textwrap.dedent(\n        """"""\\\n          XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question\n          answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from\n          the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into\n          ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently,\n          the dataset is entirely parallel across 11 languages.""""""\n    ),\n    ""MLQA"": textwrap.dedent(\n        """"""\\\n          MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between\n    4 different languages on average.""""""\n    ),\n    ""tatoeba"": textwrap.dedent(\n        """"""\\\n          his data is extracted from the Tatoeba corpus, dated Saturday 2018/11/17.\n\n          For each languages, we have selected 1000 English sentences and their translations, if available. Please check\n          this paper for a description of the languages, their families and scripts as well as baseline results.\n\n          Please note that the English sentences are not identical for all language pairs. This means that the results are\n          not directly comparable across languages. In particular, the sentences tend to have less variety for several\n          low-resource languages, e.g. ""Tom needed water"", ""Tom needs water"", ""Tom is getting water"", ...\n                    """"""\n    ),\n    ""bucc18"": textwrap.dedent(\n        """"""Building and Using Comparable Corpora\n          """"""\n    ),\n    ""udpos"": textwrap.dedent(\n        """"""\\\n    Universal Dependencies (UD) is a framework for consistent annotation of grammar (parts of speech, morphological\n    features, and syntactic dependencies) across different human languages. UD is an open community effort with over 200\n    contributors producing more than 100 treebanks in over 70 languages. If you\xe2\x80\x99re new to UD, you should start by reading\n    the first part of the Short Introduction and then browsing the annotation guidelines.\n    """"""\n    ),\n    ""SQuAD"": textwrap.dedent(\n        """"""\\\n    Stanford Question Answering Dataset (SQuAD) is a reading comprehension \\\n    dataset, consisting of questions posed by crowdworkers on a set of Wikipedia \\\n    articles, where the answer to every question is a segment of text, or span, \\\n    from the corresponding reading passage, or the question might be unanswerable.""""""\n    ),\n    ""PAN-X"": textwrap.dedent(\n        """"""\\\n    The WikiANN dataset (Pan et al. 2017) is a dataset with NER annotations for PER, ORG and LOC. It has been\n    constructed using the linked entities in Wikipedia pages for 282 different languages including Danish. The dataset\n    can be loaded with the DaNLP package:""""""\n    ),\n}\n_CITATIONS = {\n    ""tydiqa"": textwrap.dedent(\n        (\n            """"""\\\n            @article{tydiqa,\n              title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},\n              author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}\n              year    = {2020},\n              journal = {Transactions of the Association for Computational Linguistics}\n              }""""""\n        )\n    ),\n    ""XNLI"": textwrap.dedent(\n        """"""\\\n          @InProceedings{conneau2018xnli,\n          author = ""Conneau, Alexis\n                         and Rinott, Ruty\n                         and Lample, Guillaume\n                         and Williams, Adina\n                         and Bowman, Samuel R.\n                         and Schwenk, Holger\n                         and Stoyanov, Veselin"",\n          title = ""XNLI: Evaluating Cross-lingual Sentence Representations"",\n          booktitle = ""Proceedings of the 2018 Conference on Empirical Methods\n                       in Natural Language Processing"",\n          year = ""2018"",\n          publisher = ""Association for Computational Linguistics"",\n          location = ""Brussels, Belgium"",\n        }""""""\n    ),\n    ""XQuAD"": textwrap.dedent(\n        """"""\n          @article{Artetxe:etal:2019,\n              author    = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},\n              title     = {On the cross-lingual transferability of monolingual representations},\n              journal   = {CoRR},\n              volume    = {abs/1910.11856},\n              year      = {2019},\n              archivePrefix = {arXiv},\n              eprint    = {1910.11856}\n        }\n        """"""\n    ),\n    ""MLQA"": textwrap.dedent(\n        """"""\\\n          @article{lewis2019mlqa,\n          title={MLQA: Evaluating Cross-lingual Extractive Question Answering},\n          author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n          journal={arXiv preprint arXiv:1910.07475},\n          year={2019}""""""\n    ),\n    ""PAWS-X"": textwrap.dedent(\n        """"""\\\n          @InProceedings{pawsx2019emnlp,\n          title = {{PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification}},\n          author = {Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},\n          booktitle = {Proc. of EMNLP},\n          year = {2019}\n        }""""""\n    ),\n    ""tatoeba"": textwrap.dedent(\n        """"""\\\n                    @article{tatoeba,\n            title={Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond},\n            author={Mikel, Artetxe and Holger, Schwenk,},\n            journal={arXiv:1812.10464v2},\n            year={2018}\n          }""""""\n    ),\n    ""bucc18"": textwrap.dedent(""""""""""""),\n    ""udpos"": textwrap.dedent(""""""""""""),\n    ""SQuAD"": textwrap.dedent(\n        """"""\\\n        @article{2016arXiv160605250R,\n           author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n                     Konstantin and {Liang}, Percy},\n            title = ""{SQuAD: 100,000+ Questions for Machine Comprehension of Text}"",\n          journal = {arXiv e-prints},\n             year = 2016,\n              eid = {arXiv:1606.05250},\n            pages = {arXiv:1606.05250},\n            archivePrefix = {arXiv},\n           eprint = {1606.05250},\n}""""""\n    ),\n    ""PAN-X"": textwrap.dedent(\n        """"""\\\n                    @article{pan-x,\n            title={Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond},\n            author={Xiaoman, Pan and Boliang, Zhang and Jonathan, May and Joel, Nothman and Kevin, Knight and Heng, Ji},\n            volume={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers}\n            year={2017}\n          }""""""\n    ),\n}\n\n_TEXT_FEATURES = {\n    ""XNLI"": {""language"": ""language"", ""sentence1"": ""sentence1"", ""sentence2"": ""sentence2""},\n    ""tydiqa"": {""id"": ""id"", ""title"": ""title"", ""context"": ""context"", ""question"": ""question"", ""answers"": ""answers""},\n    ""XQuAD"": {""id"": ""id"", ""context"": ""context"", ""question"": ""question"", ""answers"": ""answers""},\n    ""MLQA"": {""id"": ""id"", ""title"": ""title"", ""context"": ""context"", ""question"": ""question"", ""answers"": ""answers""},\n    ""tatoeba"": {""source_sentence"": """", ""target_sentence"": """", ""source_lang"": """", ""target_lang"": """"},\n    ""bucc18"": {""source_sentence"": """", ""target_sentence"": """", ""source_lang"": """", ""target_lang"": """"},\n    ""PAWS-X"": {""sentence1"": ""sentence1"", ""sentence2"": ""sentence2""},\n    ""udpos"": {""word"": """", ""pos_tag"": """"},\n    ""SQuAD"": {""id"": ""id"", ""title"": ""title"", ""context"": ""context"", ""question"": ""question"", ""answers"": ""answers""},\n    ""PAN-X"": {""word"": """", ""ner_tag"": """", ""lang"": """"},\n}\n_DATA_URLS = {\n    ""tydiqa"": ""https://storage.googleapis.com/tydiqa/"",\n    ""XNLI"": ""https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip"",\n    ""XQuAD"": ""https://github.com/deepmind/xquad/raw/master/"",\n    ""MLQA"": ""https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip"",\n    ""PAWS-X"": ""https://storage.googleapis.com/paws/pawsx/x-final.tar.gz"",\n    ""bucc18"": ""https://comparable.limsi.fr/bucc2018/"",\n    ""tatoeba"": ""https://github.com/facebookresearch/LASER/raw/master/data/tatoeba/v1"",\n    ""udpos"": ""https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz"",\n    ""SQuAD"": ""https://rajpurkar.github.io/SQuAD-explorer/dataset/"",\n    ""PAN-X"": """",\n}\n\n_URLS = {\n    ""tydiqa"": ""https://github.com/google-research-datasets/tydiqa"",\n    ""XQuAD"": ""https://github.com/deepmind/xquad"",\n    ""XNLI"": ""https://www.nyu.edu/projects/bowman/xnli/"",\n    ""MLQA"": ""https://github.com/facebookresearch/MLQA"",\n    ""PAWS-X"": ""https://github.com/google-research-datasets/paws/tree/master/pawsx"",\n    ""bucc18"": ""https://comparable.limsi.fr/bucc2018/"",\n    ""tatoeba"": ""https://github.com/facebookresearch/LASER/blob/master/data/tatoeba/v1/README.md"",\n    ""udpos"": ""https://universaldependencies.org/"",\n    ""SQuAD"": ""https://rajpurkar.github.io/SQuAD-explorer/"",\n    ""PAN-X"": """",\n}\n\n\nclass XtremeConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for Break""""""\n\n    def __init__(self, data_url, citation, url, text_features, **kwargs):\n        """"""\n\n        Args:\n            text_features: `dict[string, string]`, map from the name of the feature\n        dict for each text field to the name of the column in the tsv file\n            label_column:\n            label_classes\n            **kwargs: keyword arguments forwarded to super.\n        """"""\n        super(XtremeConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n        self.text_features = text_features\n        self.data_url = data_url\n        self.citation = citation\n        self.url = url\n\n\nclass Xtreme(nlp.GeneratorBasedBuilder):\n    """"""TODO(xtreme): Short description of my dataset.""""""\n\n    # TODO(xtreme): Set up version.\n    VERSION = nlp.Version(""0.1.0"")\n    MANUAL_DOWNLOAD_INSTRUCTIONS = """"""\\\n     You need to manually download the AmazonPhotos.zip file on Amazon Cloud Drive\n     (https://www.amazon.com/clouddrive/share/d3KGCRCIYwhKJF0H3eWA26hjg2ZCRhjpEQtDL70FSBN) and save the file under <path/to/folder>AmazonPhotos.zip\n\n    """"""\n    BUILDER_CONFIGS = [\n        XtremeConfig(\n            name=name,\n            description=_DESCRIPTIONS[name.split(""."")[0]],\n            citation=_CITATIONS[name.split(""."")[0]],\n            text_features=_TEXT_FEATURES[name.split(""."")[0]],\n            data_url=_DATA_URLS[name.split(""."")[0]],\n            url=_URLS[name.split(""."")[0]],\n        )\n        for name in _NAMES\n    ]\n\n    def _info(self):\n        # TODO(xtreme): Specifies the nlp.DatasetInfo object\n        features = {text_feature: nlp.Value(""string"") for text_feature in six.iterkeys(self.config.text_features)}\n        if ""answers"" in features.keys():\n            features[""answers""] = nlp.features.Sequence(\n                {""answer_start"": nlp.Value(""int32""), ""text"": nlp.Value(""string"")}\n            )\n        if self.config.name.startswith(""PAWS-X""):\n            features[""label""] = nlp.Value(""string"")\n        if self.config.name == ""XNLI"":\n            features[""gold_label""] = nlp.Value(""string"")\n\n        return nlp.DatasetInfo(\n            # This is the description that will appear on the datasets page.\n            description=self.config.description + ""\\n"" + _DESCRIPTION,\n            # nlp.features.FeatureConnectors\n            features=nlp.Features(\n                features\n                # These are the features of your dataset like images, labels ...\n            ),\n            # If there\'s a common (input, target) tuple from the features,\n            # specify them here. They\'ll be used if as_supervised=True in\n            # builder.as_dataset.\n            supervised_keys=None,\n            # Homepage of the dataset for documentation\n            homepage=""https://github.com/google-research/xtreme"" + ""\\t"" + self.config.url,\n            citation=self.config.citation + ""\\n"" + _CITATION,\n        )\n\n    def _split_generators(self, dl_manager):\n        """"""Returns SplitGenerators.""""""\n        # TODO(xtreme): Downloads the data and defines the splits\n        # dl_manager is a nlp.download.DownloadManager that can be used to\n        # download and extract URLs\n\n        if self.config.name == ""tydiqa"":\n            train_url = ""v1.1/tydiqa-goldp-v1.1-train.json""\n            dev_url = ""v1.1/tydiqa-goldp-v1.1-dev.json""\n            urls_to_download = {\n                ""train"": os.path.join(self.config.data_url, train_url),\n                ""dev"": os.path.join(self.config.data_url, dev_url),\n            }\n            dl_dir = dl_manager.download_and_extract(urls_to_download)\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": dl_dir[""train""]},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": dl_dir[""dev""]},\n                ),\n            ]\n        if self.config.name == ""XNLI"":\n            dl_dir = dl_manager.download_and_extract(self.config.data_url)\n            data_dir = os.path.join(dl_dir, ""XNLI-1.0"")\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST, gen_kwargs={""filepath"": os.path.join(data_dir, ""xnli.test.tsv"")}\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": os.path.join(data_dir, ""xnli.dev.tsv"")}\n                ),\n            ]\n\n        if self.config.name.startswith(""MLQA""):\n            mlqa_downloaded_files = dl_manager.download_and_extract(self.config.data_url)\n            l1 = self.config.name.split(""."")[1]\n            l2 = self.config.name.split(""."")[2]\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(\n                            os.path.join(mlqa_downloaded_files, ""MLQA_V1/test""),\n                            ""test-context-{}-question-{}.json"".format(l1, l2),\n                        )\n                    },\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(\n                            os.path.join(mlqa_downloaded_files, ""MLQA_V1/dev""),\n                            ""dev-context-{}-question-{}.json"".format(l1, l2),\n                        )\n                    },\n                ),\n            ]\n\n        if self.config.name.startswith(""XQuAD""):\n            lang = self.config.name.split(""."")[1]\n            xquad_downloaded_file = dl_manager.download_and_extract(\n                os.path.join(self.config.data_url, ""xquad.{}.json"".format(lang))\n            )\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": xquad_downloaded_file},\n                ),\n            ]\n        if self.config.name.startswith(""PAWS-X""):\n            lang = self.config.name.split(""."")[1]\n            paws_x_dir = dl_manager.download_and_extract(self.config.data_url)\n            data_dir = os.path.join(paws_x_dir, ""x-final"", lang)\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(data_dir, ""dev_2k.tsv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(data_dir, ""test_2k.tsv"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(data_dir, ""translated_train.tsv"")\n                        if lang != ""en""\n                        else os.path.join(data_dir, ""train.tsv"")\n                    },\n                ),\n            ]\n        elif self.config.name.startswith(""tatoeba""):\n            lang = self.config.name.split(""."")[1]\n\n            tatoeba_source_data = dl_manager.download_and_extract(\n                os.path.join(self.config.data_url, ""tatoeba.{}-eng.{}"".format(lang, lang))\n            )\n            tatoeba_eng_data = dl_manager.download_and_extract(\n                os.path.join(self.config.data_url, ""tatoeba.{}-eng.eng"".format(lang))\n            )\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": (tatoeba_source_data, tatoeba_eng_data)},\n                ),\n            ]\n        if self.config.name.startswith(""bucc18""):\n            lang = self.config.name.split(""."")[1]\n            bucc18_dl_test_dir = dl_manager.download_and_extract(\n                os.path.join(self.config.data_url, ""bucc2018-{}-en.training-gold.tar.bz2"".format(lang))\n            )\n            bucc18_dl_dev_dir = dl_manager.download_and_extract(\n                os.path.join(self.config.data_url, ""bucc2018-{}-en.sample-gold.tar.bz2"".format(lang))\n            )\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(bucc18_dl_dev_dir, ""bucc2018"", lang + ""-en"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(bucc18_dl_test_dir, ""bucc2018"", lang + ""-en"")},\n                ),\n            ]\n        if self.config.name.startswith(""udpos""):\n            udpos_downloaded_files = dl_manager.download_and_extract(self.config.data_url)\n            data_dir = os.path.join(udpos_downloaded_files, ""ud-treebanks-v2.5"")\n\n            lang = self.config.name.split(""."")[1]\n            data_dir = os.path.join(data_dir, ""*_"" + lang + ""*"")\n            folders = sorted(glob.glob(data_dir))\n\n            if lang == ""Kazakh"":\n                return [\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TEST,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={\n                            ""filepath"": [\n                                os.path.join(folder, file)\n                                for folder in folders\n                                for file in sorted(os.listdir(folder))\n                                if ""test"" in file and file.endswith("".conllu"")\n                            ]\n                        },\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TRAIN,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={\n                            ""filepath"": [\n                                os.path.join(folder, file)\n                                for folder in folders\n                                for file in sorted(os.listdir(folder))\n                                if ""train"" in file and file.endswith("".conllu"")\n                            ]\n                        },\n                    ),\n                ]\n            elif lang == ""Tagalog"" or lang == ""Thai"" or lang == ""Yoruba"":\n                return [\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TEST,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={\n                            ""filepath"": [\n                                os.path.join(folder, file)\n                                for folder in folders\n                                for file in sorted(os.listdir(folder))\n                                if ""test"" in file and file.endswith("".conllu"")\n                            ]\n                        },\n                    )\n                ]\n            else:\n                return [\n                    nlp.SplitGenerator(\n                        name=nlp.Split.VALIDATION,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={\n                            ""filepath"": [\n                                os.path.join(folder, file)\n                                for folder in folders\n                                for file in sorted(os.listdir(folder))\n                                if ""NYUAD"" not in folder and ""dev"" in file and file.endswith("".conllu"")\n                            ]\n                            # we exclude Arabic NYUAD which deos not contains any word, only _\n                        },\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TEST,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={\n                            ""filepath"": [\n                                os.path.join(folder, file)\n                                for folder in folders\n                                for file in sorted(os.listdir(folder))\n                                if ""NYUAD"" not in folder and ""test"" in file and file.endswith("".conllu"")\n                            ]\n                        },\n                    ),\n                    nlp.SplitGenerator(\n                        name=nlp.Split.TRAIN,\n                        # These kwargs will be passed to _generate_examples\n                        gen_kwargs={\n                            ""filepath"": [\n                                os.path.join(folder, file)\n                                for folder in folders\n                                for file in sorted(os.listdir(folder))\n                                if ""NYUAD"" not in folder and ""train"" in file and file.endswith("".conllu"")\n                            ]\n                        },\n                    ),\n                ]\n\n        if self.config.name == ""SQuAD"":\n\n            urls_to_download = {\n                ""train"": os.path.join(self.config.data_url, ""train-v1.1.json""),\n                ""dev"": os.path.join(self.config.data_url, ""dev-v1.1.json""),\n            }\n            downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n            return [\n                nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": downloaded_files[""train""]}),\n                nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": downloaded_files[""dev""]}),\n            ]\n\n        if self.config.name.startswith(""PAN-X""):\n            path_to_manual_folder = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))\n            panx_path = os.path.join(path_to_manual_folder, _PAN_X_FOLDER)\n\n            if not os.path.exists(panx_path):\n                raise FileNotFoundError(\n                    ""{} does not exist. Make sure you insert a manual dir via `nlp.load(\'wikihow\', data_dir=...)` that includes {}. Manual download instructions: {}"".format(\n                        panx_path, _PAN_X_FOLDER, self.MANUAL_DOWNLOAD_INSTRUCTIONS\n                    )\n                )\n\n            panx_dl_dir = dl_manager.extract(panx_path)\n            lang = self.config.name.split(""."")[1]\n            lang_folder = dl_manager.extract(os.path.join(panx_dl_dir, lang + "".tar.gz""))\n            return [\n                nlp.SplitGenerator(\n                    name=nlp.Split.VALIDATION,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={\n                        ""filepath"": os.path.join(lang_folder, ""dev"")\n                        # we exclude Arabic NYUAD which deos not contains any word, only _\n                    },\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TEST,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(lang_folder, ""test"")},\n                ),\n                nlp.SplitGenerator(\n                    name=nlp.Split.TRAIN,\n                    # These kwargs will be passed to _generate_examples\n                    gen_kwargs={""filepath"": os.path.join(lang_folder, ""train"")},\n                ),\n            ]\n\n    def _generate_examples(self, filepath):\n        """"""Yields examples.""""""\n        # TODO(xtreme): Yields (key, example) tuples from the dataset\n\n        if self.config.name == ""tydiqa"" or self.config.name.startswith(""MLQA"") or self.config.name == ""SQuAD"":\n            with open(filepath) as f:\n                data = json.load(f)\n                for article in data[""data""]:\n                    title = article.get(""title"", """").strip()\n                    for paragraph in article[""paragraphs""]:\n                        context = paragraph[""context""].strip()\n                        for qa in paragraph[""qas""]:\n                            question = qa[""question""].strip()\n                            id_ = qa[""id""]\n\n                            answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                            answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                            # Features currently used are ""context"", ""question"", and ""answers"".\n                            # Others are extracted here for the ease of future expansions.\n                            yield id_, {\n                                ""title"": title,\n                                ""context"": context,\n                                ""question"": question,\n                                ""id"": id_,\n                                ""answers"": {""answer_start"": answer_starts, ""text"": answers},\n                            }\n        if self.config.name == ""XNLI"":\n            with open(filepath) as f:\n                data = csv.DictReader(f, delimiter=""\\t"")\n                for id_, row in enumerate(data):\n                    yield id_, {\n                        ""sentence1"": row[""sentence1""],\n                        ""sentence2"": row[""sentence2""],\n                        ""language"": row[""language""],\n                        ""gold_label"": row[""gold_label""],\n                    }\n        if self.config.name.startswith(""PAWS-X""):\n            with open(filepath) as f:\n                data = csv.reader(f, delimiter=""\\t"")\n                for id_, row in enumerate(data):\n                    if len(row) == 4:\n                        yield id_, {""sentence1"": row[1], ""sentence2"": row[2], ""label"": row[3]}\n        if self.config.name.startswith(""XQuAD""):\n            with open(filepath) as f:\n                xquad = json.load(f)\n                for article in xquad[""data""]:\n                    for paragraph in article[""paragraphs""]:\n                        context = paragraph[""context""].strip()\n                        for qa in paragraph[""qas""]:\n                            question = qa[""question""].strip()\n                            id_ = qa[""id""]\n\n                            answer_starts = [answer[""answer_start""] for answer in qa[""answers""]]\n                            answers = [answer[""text""].strip() for answer in qa[""answers""]]\n\n                            # Features currently used are ""context"", ""question"", and ""answers"".\n                            # Others are extracted here for the ease of future expansions.\n                            yield id_, {\n                                ""context"": context,\n                                ""question"": question,\n                                ""id"": id_,\n                                ""answers"": {""answer_start"": answer_starts, ""text"": answers},\n                            }\n        if self.config.name.startswith(""bucc18""):\n            files = sorted(os.listdir(filepath))\n            target_file = ""/""\n            source_file = ""/""\n            source_target_file = ""/""\n            for file in files:\n                if file.endswith(""en""):\n                    target_file = os.path.join(filepath, file)\n                elif file.endswith(""gold""):\n                    source_target_file = os.path.join(filepath, file)\n                else:\n                    source_file = os.path.join(filepath, file)\n            with open(target_file) as f:\n                data = csv.reader(f, delimiter=""\\t"")\n                target_sentences = [row for row in data]\n            with open(source_file) as f:\n                data = csv.reader(f, delimiter=""\\t"")\n                source_sentences = [row for row in data]\n            with open(source_target_file) as f:\n                data = csv.reader(f, delimiter=""\\t"")\n                source_target_ids = [row for row in data]\n            for id_, pair in enumerate(source_target_ids):\n                source_id = pair[0]\n                target_id = pair[1]\n                source_sent = """"\n                target_sent = """"\n                for i in range(len(source_sentences)):\n                    if source_sentences[i][0] == source_id:\n                        source_sent = source_sentences[i][1]\n                        source_id = source_sentences[i][0]\n                        break\n                for j in range(len(target_sentences)):\n                    if target_sentences[j][0] == target_id:\n                        target_sent = target_sentences[j][1]\n                        target_id = target_sentences[j][0]\n                        break\n                yield id_, {\n                    ""source_sentence"": source_sent,\n                    ""target_sentence"": target_sent,\n                    ""source_lang"": source_id,\n                    ""target_lang"": target_id,\n                }\n        if self.config.name.startswith(""tatoeba""):\n            source_file = filepath[0]\n            target_file = filepath[1]\n            source_sentences = []\n            target_sentences = []\n            with open(source_file) as f1:\n                for row in f1:\n                    source_sentences.append(row)\n            with open(target_file) as f2:\n                for row in f2:\n                    target_sentences.append(row)\n            for i in range(len(source_sentences)):\n                yield i, {\n                    ""source_sentence"": source_sentences[i],\n                    ""target_sentence"": target_sentences[i],\n                    ""source_lang"": source_file.split(""."")[-1],\n                    ""target_lang"": ""eng"",\n                }\n        if self.config.name.startswith(""udpos""):\n            for id_file, file in enumerate(filepath):\n                with open(file) as f:\n                    data = csv.reader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n                    for id_row, row in enumerate(data):\n                        if len(row) >= 10 and row[1] != ""_"":\n                            yield str(id_file) + ""_"" + str(id_row), {""word"": row[1], ""pos_tag"": row[3]}\n        if self.config.name.startswith(""PAN-X""):\n            with open(filepath) as f:\n                data = csv.reader(f, delimiter=""\\t"", quoting=csv.QUOTE_NONE)\n                for id_, row in enumerate(data):\n                    if row:\n                        lang, word = row[0].split("":"")[0], row[0].split("":"")[1]\n                        tag = row[1]\n                        yield id_, {""word"": word, ""ner_tag"": tag, ""lang"": lang}\n'"
datasets/yelp_polarity/yelp_polarity.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n# Copyright 2019 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Yelp Polarity Reviews dataset.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport nlp\n\n\n_DESCRIPTION = """"""\\\nLarge Yelp Review Dataset.\nThis is a dataset for binary sentiment classification. \\\nWe provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \\\n\nORIGIN\nThe Yelp reviews dataset consists of reviews from Yelp. It is extracted\nfrom the Yelp Dataset Challenge 2015 data. For more information, please\nrefer to http://www.yelp.com/dataset_challenge\n\nThe Yelp reviews polarity dataset is constructed by\nXiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\nIt is first used as a text classification benchmark in the following paper:\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\nfor Text Classification. Advances in Neural Information Processing Systems 28\n(NIPS 2015).\n\n\nDESCRIPTION\n\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2\nnegative, and 3 and 4 positive. For each polarity 280,000 training samples and\n19,000 testing samples are take randomly. In total there are 560,000 trainig\nsamples and 38,000 testing samples. Negative polarity is class 1,\nand positive class 2.\n\nThe files train.csv and test.csv contain all the training samples as\ncomma-sparated values. There are 2 columns in them, corresponding to class\nindex (1 and 2) and review text. The review texts are escaped using double\nquotes (""), and any internal double quote is escaped by 2 double quotes ("""").\nNew lines are escaped by a backslash followed with an ""n"" character,\nthat is ""\\n"".\n""""""\n\n_CITATION = """"""\\\n@article{zhangCharacterlevelConvolutionalNetworks2015,\n  archivePrefix = {arXiv},\n  eprinttype = {arxiv},\n  eprint = {1509.01626},\n  primaryClass = {cs},\n  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},\n  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},\n  journal = {arXiv:1509.01626 [cs]},\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  month = sep,\n  year = {2015},\n}\n\n""""""\n\n_DOWNLOAD_URL = ""https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz""\n\n\nclass YelpPolarityReviewsConfig(nlp.BuilderConfig):\n    """"""BuilderConfig for YelpPolarityReviews.""""""\n\n    def __init__(self, **kwargs):\n        """"""BuilderConfig for YelpPolarityReviews.\n\n    Args:\n\n        **kwargs: keyword arguments forwarded to super.\n    """"""\n        super(YelpPolarityReviewsConfig, self).__init__(\n            version=nlp.Version(""1.0.0"", ""New split API (https://tensorflow.org/datasets/splits)""), **kwargs\n        )\n\n\nclass YelpPolarity(nlp.GeneratorBasedBuilder):\n    """"""Yelp Polarity reviews dataset.""""""\n\n    BUILDER_CONFIGS = [YelpPolarityReviewsConfig(name=""plain_text"", description=""Plain text"",)]\n\n    def _info(self):\n        return nlp.DatasetInfo(\n            description=_DESCRIPTION,\n            features=nlp.Features({""text"": nlp.Value(""string""), ""label"": nlp.features.ClassLabel(names=[""1"", ""2""]),}),\n            supervised_keys=None,\n            homepage=""https://course.fast.ai/datasets"",\n            citation=_CITATION,\n        )\n\n    def _vocab_text_gen(self, train_file):\n        for _, ex in self._generate_examples(train_file):\n            yield ex[""text""]\n\n    def _split_generators(self, dl_manager):\n        arch_path = dl_manager.download_and_extract(_DOWNLOAD_URL)\n        train_file = os.path.join(arch_path, ""yelp_review_polarity_csv"", ""train.csv"")\n        test_file = os.path.join(arch_path, ""yelp_review_polarity_csv"", ""test.csv"")\n        return [\n            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": train_file}),\n            nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": test_file}),\n        ]\n\n    def _generate_examples(self, filepath):\n        """"""Generate Yelp examples.""""""\n        with open(filepath) as f:\n            for line_id, line in enumerate(f):\n                # The format of the line is:\n                # ""1"", ""The text of the review.""\n                yield line_id, {""text"": line[5:-2].strip(), ""label"": line[1]}\n'"
metrics/bertscore/bertscore.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" BERTScore metric. """"""\n\nimport nlp\nimport bert_score\n\n_CITATION = """"""\\\n@inproceedings{bert-score,\n  title={BERTScore: Evaluating Text Generation with BERT},\n  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=SkeHuCVFDr}\n}\n""""""\n\n_DESCRIPTION = """"""\\\nBERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity.\nIt has been shown to correlate with human judgment on sentence-level and system-level evaluation. \nMoreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different language generation tasks.\n\nSee the [README.md] file at https://github.com/Tiiiger/bert_score for more information.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nBERTScore Metrics with the hashcode from a source against one or more references.\n\nArgs:\n    `predictions` (list of str): prediction/candidate sentences\n    `refereces` (list of str or list of list of str): reference sentences\n    `lang` (str): language of the sentences; required (e.g. \'en\')\n    `model_type` (str): bert specification, default using the suggested\n    model for the target langauge; has to specify at least one of\n    `model_type` or `lang`\n    `num_layers` (int): the layer of representation to use.\n    default using the number of layer tuned on WMT16 correlation data\n    `verbose` (bool): turn on intermediate status update\n    `idf` (bool or dict): use idf weighting, can also be a precomputed idf_dict\n    `device` (str): on which the contextual embedding model will be allocated on.\n    If this argument is None, the model lives on cuda:0 if cuda is available.\n    `nthreads` (int): number of threads\n    `batch_size` (int): bert score processing batch size\n    at least one of `model_type` or `lang`. `lang` needs to be\n    specified when `rescale_with_baseline` is True.\n    `rescale_with_baseline` (bool): rescale bertscore with pre-computed baseline\nReturns:\n    \'precision\': Precision,\n    \'recall\': Recall,\n    \'f1\', F1 score,\n    \'hashcode\': Hashcode of the library,\n""""""\n\nclass BERTScore(nlp.Metric):\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            homepage=""https://github.com/Tiiiger/bert_score"",\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Value(\'string\', id=\'sequence\'),\n                \'references\': nlp.Sequence(nlp.Value(\'string\', id=\'sequence\'), id=\'references\'),\n            }),\n            codebase_urls=[""https://github.com/Tiiiger/bert_score""],\n            reference_urls=[""https://github.com/Tiiiger/bert_score"",\n                            ""https://arxiv.org/abs/1904.09675""]\n        )\n\n    def _compute(\n        self,\n        predictions,\n        references, \n        lang=None,\n        model_type=None,\n        num_layers=None,\n        verbose=False,\n        idf=False,\n        device=None,\n        batch_size=64,\n        nthreads=4,\n        all_layers=False,\n        rescale_with_baseline=False,\n    ):\n        if model_type is None:\n            assert lang is not None, ""either lang or model_type should be specified""\n            model_type = bert_score.utils.lang2model[lang.lower()]\n\n        if num_layers is None:\n            num_layers = bert_score.utils.model2layers[model_type]\n\n        hashcode = bert_score.utils.get_hash(model_type, num_layers, idf, rescale_with_baseline)\n        if not hasattr(self, \'cached_bertscorer\') or self.cached_bertscorer.hash != hashcode:\n            self.cached_bertscorer = bert_score.BERTScorer(\n                model_type=model_type,\n                num_layers=num_layers,\n                batch_size=batch_size,\n                nthreads=nthreads,\n                all_layers=all_layers,\n                idf=idf,\n                device=device,\n                lang=lang,\n                rescale_with_baseline=rescale_with_baseline,\n            )\n\n        (P, R, F) = self.cached_bertscorer.score(\n            cands=predictions, refs=references, verbose=verbose, batch_size=batch_size,\n        )\n        output_dict = {\n            \'precision\': P,\n            \'recall\': R,\n            \'f1\': F,\n            \'hashcode\': hashcode,\n        }\n        return output_dict\n'"
metrics/bleu/bleu.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" BLEU metric. """"""\n\nimport nlp\nfrom .nmt_bleu import compute_bleu  # From: https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\n\n_CITATION = """"""\\\n@INPROCEEDINGS{Papineni02bleu:a,\n    author = {Kishore Papineni and Salim Roukos and Todd Ward and Wei-jing Zhu},\n    title = {BLEU: a Method for Automatic Evaluation of Machine Translation},\n    booktitle = {},\n    year = {2002},\n    pages = {311--318}\n}\n@inproceedings{lin-och-2004-orange,\n    title = ""{ORANGE}: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation"",\n    author = ""Lin, Chin-Yew  and\n      Och, Franz Josef"",\n    booktitle = ""{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics"",\n    month = ""aug 23{--}aug 27"",\n    year = ""2004"",\n    address = ""Geneva, Switzerland"",\n    publisher = ""COLING"",\n    url = ""https://www.aclweb.org/anthology/C04-1072"",\n    pages = ""501--507"",\n}\n""""""\n\n_DESCRIPTION = """"""\\\nBLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another.\nQuality is considered to be the correspondence between a machine\'s output and that of a human: ""the closer a machine translation is to a professional human translation,\nthe better it is"" \xe2\x80\x93 this is the central idea behind BLEU. BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and\nremains one of the most popular automated and inexpensive metrics.\n\nScores are calculated for individual translated segments\xe2\x80\x94generally sentences\xe2\x80\x94by comparing them with a set of good quality reference translations.\nThose scores are then averaged over the whole corpus to reach an estimate of the translation\'s overall quality. Intelligibility or grammatical correctness\nare not taken into account[citation needed].\n\nBLEU\'s output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1\nrepresenting more similar texts. Few human translations will attain a score of 1, since this would indicate that the candidate is identical to one of the\nreference translations. For this reason, it is not necessary to attain a score of 1. Because there are more opportunities to match, adding additional\nreference translations will increase the BLEU score.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nComputes BLEU score of translated segments against one or more references.\nArgs:\n    predictions: list of translations to score.\n        Each translation should be tokenized into a list of tokens.\n    references: list of lists of references for each translation.\n        Each reference should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\nReturns:\n    \'bleu\': bleu score,\n    \'precisions\': geometric mean of n-gram precisions,\n    \'brevity_penalty\': brevity penalty,\n    \'length_ratio\': ratio of lengths,\n    \'translation_length\': translation_length,\n    \'reference_length\': reference_length\n""""""\n\nclass Bleu(nlp.Metric):\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Sequence(nlp.Value(\'string\', id=\'token\'), id=\'sequence\'),\n                \'references\': nlp.Sequence(nlp.Sequence(nlp.Value(\'string\', id=\'token\'), id=\'sequence\'), id=\'references\'),\n            }),\n            codebase_urls=[""https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py""],\n            reference_urls=[""https://en.wikipedia.org/wiki/BLEU"",\n                            ""https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213""]\n        )\n\n    def _compute(self, predictions, references, max_order=4, smooth=False):\n        score = compute_bleu(reference_corpus=references, translation_corpus=predictions, max_order=max_order, smooth=smooth)\n        (bleu, precisions, bp, ratio, translation_length, reference_length) = score\n        return {\'bleu\': bleu,\n                \'precisions\': precisions,\n                \'brevity_penalty\': bp,\n                \'length_ratio\': ratio,\n                \'translation_length\': translation_length,\n                \'reference_length\': reference_length}\n'"
metrics/coval/coval.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" ROUGE metric. """"""\n\nimport nlp\nimport numpy\nimport scipy\n\nfrom .coval_backend.conll import reader  # From: https://github.com/ns-moosavi/coval\nfrom .coval_backend.conll import util\nfrom .coval_backend.eval import evaluator\n\n_CITATION = """"""\\\n@InProceedings{moosavi2019minimum,\n    author = { Nafise Sadat Moosavi, Leo Born, Massimo Poesio and Michael Strube},\n    title = {Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection},\n    year = {2019},\n    booktitle = {Proceedings of the 57th Annual Meeting of\n\t\tthe Association for Computational Linguistics (Volume 1: Long Papers)},\n    publisher = {Association for Computational Linguistics},\n    address = {Florence, Italy},\n}\n\n@inproceedings{10.3115/1072399.1072405,\nauthor = {Vilain, Marc and Burger, John and Aberdeen, John and Connolly, Dennis and Hirschman, Lynette},\ntitle = {A Model-Theoretic Coreference Scoring Scheme},\nyear = {1995},\nisbn = {1558604022},\npublisher = {Association for Computational Linguistics},\naddress = {USA},\nurl = {https://doi.org/10.3115/1072399.1072405},\ndoi = {10.3115/1072399.1072405},\nbooktitle = {Proceedings of the 6th Conference on Message Understanding},\npages = {45\xe2\x80\x9352},\nnumpages = {8},\nlocation = {Columbia, Maryland},\nseries = {MUC6 \xe2\x80\x9995}\n}\n\n@INPROCEEDINGS{Bagga98algorithmsfor,\n    author = {Amit Bagga and Breck Baldwin},\n    title = {Algorithms for Scoring Coreference Chains},\n    booktitle = {In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference},\n    year = {1998},\n    pages = {563--566}\n}\n\n@INPROCEEDINGS{Luo05oncoreference,\n    author = {Xiaoqiang Luo},\n    title = {On coreference resolution performance metrics},\n    booktitle = {In Proc. of HLT/EMNLP},\n    year = {2005},\n    pages = {25--32},\n    publisher = {URL}\n}\n\n@inproceedings{moosavi-strube-2016-coreference,\n    title = ""Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric"",\n    author = ""Moosavi, Nafise Sadat  and\n      Strube, Michael"",\n    booktitle = ""Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",\n    month = aug,\n    year = ""2016"",\n    address = ""Berlin, Germany"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/P16-1060"",\n    doi = ""10.18653/v1/P16-1060"",\n    pages = ""632--642"",\n}\n\n""""""\n\n_DESCRIPTION = """"""\\\nCoVal is a coreference evaluation tool for the CoNLL and ARRAU datasets which\nimplements of the common evaluation metrics including MUC [Vilain et al, 1995],\nB-cubed [Bagga and Baldwin, 1998], CEAFe [Luo et al., 2005],\nLEA [Moosavi and Strube, 2016] and the averaged CoNLL score\n(the average of the F1 values of MUC, B-cubed and CEAFe)\n[Denis and Baldridge, 2009a; Pradhan et al., 2011].\n\nThis wrapper of CoVal currently only work with CoNLL line format:\nThe CoNLL format has one word per line with all the annotation for this word in column separated by spaces:\nColumn\tType\tDescription\n1\tDocument ID\tThis is a variation on the document filename\n2\tPart number\tSome files are divided into multiple parts numbered as 000, 001, 002, ... etc.\n3\tWord number\t\n4\tWord itself\tThis is the token as segmented/tokenized in the Treebank. Initially the *_skel file contain the placeholder [WORD] which gets replaced by the actual token from the Treebank which is part of the OntoNotes release.\n5\tPart-of-Speech\t\n6\tParse bit\tThis is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a *. The full parse can be created by substituting the asterix with the ""([pos] [word])"" string (or leaf) and concatenating the items in the rows of that column.\n7\tPredicate lemma\tThe predicate lemma is mentioned for the rows for which we have semantic role information. All other rows are marked with a ""-""\n8\tPredicate Frameset ID\tThis is the PropBank frameset ID of the predicate in Column 7.\n9\tWord sense\tThis is the word sense of the word in Column 3.\n10\tSpeaker/Author\tThis is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data.\n11\tNamed Entities\tThese columns identifies the spans representing various named entities.\n12:N\tPredicate Arguments\tThere is one column each of predicate argument structure information for the predicate mentioned in Column 7.\nN\tCoreference\tCoreference chain information encoded in a parenthesis structure.\nMore informations on the format can be found here (section ""*_conll File Format""): http://www.conll.cemantix.org/2012/data.html\n\nDetails on the evaluation on CoNLL can be found here: https://github.com/ns-moosavi/coval/blob/master/conll/README.md\n\nCoVal code was written by @ns-moosavi.\nSome parts are borrowed from https://github.com/clarkkev/deep-coref/blob/master/evaluation.py\nThe test suite is taken from https://github.com/conll/reference-coreference-scorers/\nMention evaluation and the test suite are added by @andreasvc.\nParsing CoNLL files is developed by Leo Born.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nCalculates coreference evaluation metrics.\nArgs:\n    predictions: list of predictions to score in the CoNLL format.\n        Each prediction is a word with its annotations as a string made of columns joined with spaces.\n        Only columns 4, 5, 6 and the last column are used (word, POS, Pars and coreference annotation)\n        See the details on the format in the description of the metric.\n    predictions: list of references for scoring in the CoNLL format.\n        Each reference is a word with its annotations as a string made of columns joined with spaces.\n        Only columns 4, 5, 6 and the last column are used (word, POS, Pars and coreference annotation)\n        See the details on the format in the description of the metric.\n    keep_singletons: After extracting all mentions of key or system files,\n        mentions whose corresponding coreference chain is of size one,\n        are considered as singletons. The default evaluation mode will include\n        singletons in evaluations if they are included in the key or the system files.\n        By setting \'keep_singletons=False\', all singletons in the key and system files\n        will be excluded from the evaluation.\n    NP_only: Most of the recent coreference resolvers only resolve NP mentions and\n        leave out the resolution of VPs. By setting the \'NP_only\' option, the scorer will only evaluate the resolution of NPs.\n    min_spans: By setting \'min_spans\', the scorer reports the results based on automatically detected minimum spans.\n        Minimum spans are determined using the MINA algorithm.\n\nReturns:\n    \'mentions\': mentions\n    \'muc\': MUC metric [Vilain et al, 1995]\n    \'bcub\': B-cubed [Bagga and Baldwin, 1998]\n    \'ceafe\': CEAFe [Luo et al., 2005]\n    \'lea\': LEA [Moosavi and Strube, 2016]\n    \'conll_score\': averaged CoNLL score (the average of the F1 values of MUC, B-cubed and CEAFe)\n""""""\n\ndef get_coref_infos(key_lines,\n        sys_lines,\n        NP_only=False,\n        remove_nested=False,\n        keep_singletons=True,\n        min_span=False,\n        doc=""dummy_doc""):\n\n    key_doc_lines = {doc: key_lines}\n    sys_doc_lines = {doc: sys_lines}\n\n    doc_coref_infos = {}\n\n    key_nested_coref_num = 0\n    sys_nested_coref_num = 0\n    key_removed_nested_clusters = 0\n    sys_removed_nested_clusters = 0\n    key_singletons_num = 0\n    sys_singletons_num = 0\n\n    key_clusters, singletons_num = reader.get_doc_mentions(\n            doc, key_doc_lines[doc], keep_singletons)\n    key_singletons_num += singletons_num\n\n    if NP_only or min_span:\n        key_clusters = reader.set_annotated_parse_trees(key_clusters,\n                key_doc_lines[doc],\n                NP_only, min_span)\n\n    sys_clusters, singletons_num = reader.get_doc_mentions(\n            doc, sys_doc_lines[doc], keep_singletons)\n    sys_singletons_num += singletons_num\n\n    if NP_only or min_span:\n        sys_clusters = reader.set_annotated_parse_trees(sys_clusters,\n                key_doc_lines[doc],\n                NP_only, min_span)\n\n    if remove_nested:\n        nested_mentions, removed_clusters = reader.remove_nested_coref_mentions(\n                key_clusters, keep_singletons)\n        key_nested_coref_num += nested_mentions\n        key_removed_nested_clusters += removed_clusters\n\n        nested_mentions, removed_clusters = reader.remove_nested_coref_mentions(\n                sys_clusters, keep_singletons)\n        sys_nested_coref_num += nested_mentions\n        sys_removed_nested_clusters += removed_clusters\n\n    sys_mention_key_cluster = reader.get_mention_assignments(\n            sys_clusters, key_clusters)\n    key_mention_sys_cluster = reader.get_mention_assignments(\n            key_clusters, sys_clusters)\n\n    doc_coref_infos[doc] = (key_clusters, sys_clusters,\n            key_mention_sys_cluster, sys_mention_key_cluster)\n\n    if remove_nested:\n        print(\'Number of removed nested coreferring mentions in the key \'\n                \'annotation: %s; and system annotation: %s\' % (\n                key_nested_coref_num, sys_nested_coref_num))\n        print(\'Number of resulting singleton clusters in the key \'\n                \'annotation: %s; and system annotation: %s\' % (\n                key_removed_nested_clusters, sys_removed_nested_clusters))\n\n    if not keep_singletons:\n        print(\'%d and %d singletons are removed from the key and system \'\n                \'files, respectively\' % (\n                key_singletons_num, sys_singletons_num))\n\n    return doc_coref_infos\n\n\ndef evaluate(key_lines,\n        sys_lines, metrics, NP_only, remove_nested,\n        keep_singletons, min_span):\n    doc_coref_infos = get_coref_infos(key_lines,\n        sys_lines, NP_only,\n            remove_nested, keep_singletons, min_span)\n\n    output_scores = {}\n    conll = 0\n    conll_subparts_num = 0\n\n    for name, metric in metrics:\n        recall, precision, f1 = evaluator.evaluate_documents(doc_coref_infos,\n                metric,\n                beta=1)\n        if name in [""muc"", ""bcub"", ""ceafe""]:\n            conll += f1\n            conll_subparts_num += 1\n        output_scores.update({f""{name}/recall"": recall,\n                             f""{name}/precision"": precision,\n                             f""{name}/f1"": f1})\n\n        print(name.ljust(10), \'Recall: %.2f\' % (recall * 100),\n                \' Precision: %.2f\' % (precision * 100),\n                \' F1: %.2f\' % (f1 * 100))\n\n    if conll_subparts_num == 3:\n        conll = (conll / 3) * 100\n        print(\'CoNLL score: %.2f\' % conll)\n        output_scores.update({f""conll_score"": conll})\n\n    return output_scores\n\n\ndef check_gold_parse_annotation(key_lines):\n    has_gold_parse = False\n    for line in key_lines:\n        if not line.startswith(""#""):\n            if len(line.split())> 6:\n                parse_col = line.split()[5]\n                if not parse_col == ""-"":\n                    has_gold_parse = True\n                    break\n                else:\n                    break\n    return has_gold_parse\n\n\nclass Coval(nlp.Metric):\n    def __init__(self, **kwargs):\n        raise NotImplementedError(""CoVal is currently under construction."")\n\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Value(\'string\', id=\'sequence\'),\n                \'references\': nlp.Value(\'string\', id=\'sequence\'),\n            }),\n            codebase_urls=[""https://github.com/ns-moosavi/coval""],\n            reference_urls=[""https://github.com/ns-moosavi/coval"",\n                            ""https://www.aclweb.org/anthology/P16-1060"",\n                            ""http://www.conll.cemantix.org/2012/data.html""]\n        )\n\n    def _compute(self, predictions, references, keep_singletons=True,\n                 NP_only=False, min_spans=False, remove_nested=False):\n        allmetrics = [(\'mentions\', evaluator.mentions), (\'muc\', evaluator.muc),\n                      (\'bcub\', evaluator.b_cubed), (\'ceafe\', evaluator.ceafe),\n                      (\'lea\', evaluator.lea)]\n\n        if min_spans:\n            has_gold_parse = util.check_gold_parse_annotation(references)\n            if not has_gold_parse:\n                raise NotImplementedError(""References should have gold parse annotation to use \'min_spans\'."")\n                # util.parse_key_file(key_file)\n                # key_file = key_file + "".parsed""\n\n        score = evaluate(references, predictions, allmetrics, NP_only, remove_nested,\n                keep_singletons, min_spans)\n\n        return score\n'"
metrics/gleu/gleu.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" GLEU metric. """"""\n\nimport random\nimport nlp\nimport scipy.stats\nimport numpy as np\n\nfrom .gec_gleu import GLEU  # From: https://github.com/cnap/gec-ranking/blob/master/scripts/gleu.py\n\n_CITATION = """"""\\\n@InProceedings{napoles-EtAl:2015:ACL-IJCNLP,\n  author    = {Napoles, Courtney  and  Sakaguchi, Keisuke  and  Post, Matt  and  Tetreault, Joel},\n  title     = {Ground Truth for Grammatical Error Correction Metrics},\n  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},\n  month     = {July},\n  year      = {2015},\n  address   = {Beijing, China},\n  publisher = {Association for Computational Linguistics},\n  pages     = {588--593},\n  url       = {http://www.aclweb.org/anthology/P15-2097}\n}\n@Article{napoles2016gleu,\n  author    = {Napoles, Courtney  and  Sakaguchi, Keisuke  and  Post, Matt  and  Tetreault, Joel},\n  title     = {{GLEU} Without Tuning},\n  journal   = {eprint arXiv:1605.02592 [cs.CL]},\n  year      = {2016},\n  url       = {http://arxiv.org/abs/1605.02592}\n}\n""""""\n\n_DESCRIPTION = """"""\\\nThe GLEU metric is a variant of BLEU proposed for evaluating grammatical error corrections\nusing n-gram overlap with a set of reference sentences, as opposed to precision/recall of specific\nannotated errors (Napoles et al., 2015). GLEU hews more closely to human judgments than the rankings produced by\nmetrics such as MaxMatch and I-measure. The present metric is the second version of GLEU (Napoles et al., 2016)\nmodified to address problems that arise when using an increasing number of reference sets.\nThe modified metric does not require tuning and is recommended to be used instead of the original version.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nComputes GLEU score.\nArgs:\n    predictions: list of translations to score.\n        Each translation should be tokenized into a list of tokens.\n    references: list of lists of references for each translation.\n        Each reference should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\nReturns:\n    \'bleu\': bleu score,\n    \'precisions\': geometric mean of n-gram precisions,\n    \'brevity_penalty\': brevity penalty,\n    \'length_ratio\': ratio of lengths,\n    \'translation_length\': translation_length,\n    \'reference_length\': reference_length\n""""""\n\ndef get_gleu_stats(scores) :\n    mean = np.mean(scores)\n    std = np.std(scores)\n    ci = scipy.stats.norm.interval(0.95,loc=mean,scale=std)\n    return {\'mean\': mean,\n            \'std\': std,\n            \'ci\': ci}\n\nclass Gleu(nlp.Metric):\n    def __init__(self, **kwargs):\n        raise NotImplementedError(""Gleu is currently under construction."")\n\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Sequence(nlp.Value(\'string\', id=\'token\'), id=\'sequence\'),\n                \'references\': nlp.Sequence(nlp.Sequence(nlp.Value(\'string\', id=\'token\'), id=\'sequence\'), id=\'references\'),\n            }),\n            codebase_urls=[""https://github.com/cnap/gec-ranking""],\n            reference_urls=[""https://github.com/cnap/gec-ranking""]\n        )\n\n    def _compute(self, predictions, references, source, num_iterations=500, debug=False):\n        raise NotImplementedError(""To finish"")\n        gleu_calculator = GLEU()\n\n        gleu_calculator.load_sources(source)\n        gleu_calculator.load_references(references)\n\n        # first generate a random list of indices, using a different seed\n        # for each iteration\n        indices = []\n        for j in range(num_iterations) :\n            random.seed(j*101)\n            indices.append([random.randint(0,len(references)-1)\n                            for i in range(len(predictions))])\n\n        if debug :\n            print(\'===== Sentence-level scores =====\')\n            print(\'SID Mean Stdev 95%CI GLEU\')\n\n        iter_stats = [ [0 for i in range(2*4+2)]\n                        for j in range(num_iterations) ]\n\n        for i,h in enumerate(predictions) :\n\n            gleu_calculator.load_hypothesis_sentence(h)\n            # we are going to store the score of this sentence for each ref\n            # so we don\'t have to recalculate them 500 times\n\n            stats_by_ref = [ None for r in range(len(references)) ]\n\n            for j in range(num_iterations) :\n                ref = indices[j][i]\n                this_stats = stats_by_ref[ref]\n\n                if this_stats is None :\n                    this_stats = [ s for s in gleu_calculator.gleu_stats(\n                        i,r_ind=ref) ]\n                    stats_by_ref[ref] = this_stats\n\n                iter_stats[j] = [ sum(scores)\n                                    for scores in zip(iter_stats[j], this_stats)]\n\n            if debug :\n                # sentence-level GLEU is the mean GLEU of the hypothesis\n                # compared to each reference\n                for r in range(len(references)) :\n                    if stats_by_ref[r] is None :\n                        stats_by_ref[r] = [s for s in gleu_calculator.gleu_stats(\n                            i,r_ind=r) ]\n\n                print(i)\n                print(\' \'.join(get_gleu_stats([gleu_calculator.gleu(stats,smooth=True)\n                                                for stats in stats_by_ref])))\n\n        if debug :\n            print(\'\\n==== Overall score =====\')\n            print(\'Mean Stdev 95%CI GLEU\')\n            print(\' \'.join(get_gleu_stats([gleu_calculator.gleu(stats)\n                                            for stats in iter_stats ])))\n        return get_gleu_stats([gleu_calculator.gleu(stats)\n                                    for stats in iter_stats ])[0]\n'"
metrics/glue/glue.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" GLUE benchmark metric. """"""\n\nimport nlp\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\n_CITATION = """"""\\\n@inproceedings{wang2019glue,\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n  note={In the Proceedings of ICLR.},\n  year={2019}\n}\nNote that each GLUE dataset has its own citation. Please see the source to see\nthe correct citation for each contained dataset.""""""\n\n_DESCRIPTION = """"""\\\nGLUE, the General Language Understanding Evaluation benchmark\n(https://gluebenchmark.com/) is a collection of resources for training,\nevaluating, and analyzing natural language understanding systems.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nComputes BLEU score of translated segments against one or more references.\nArgs:\n    predictions: list of translations to score.\n        Each translation should be tokenized into a list of tokens.\n    references: list of lists of references for each translation.\n        Each reference should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\nReturns:\n    \'bleu\': bleu score,\n    \'precisions\': geometric mean of n-gram precisions,\n    \'brevity_penalty\': brevity penalty,\n    \'length_ratio\': ratio of lengths,\n    \'translation_length\': translation_length,\n    \'reference_length\': reference_length\n""""""\n\ndef simple_accuracy(preds, labels):\n    return (preds == labels).mean()\n\ndef acc_and_f1(preds, labels):\n    acc = simple_accuracy(preds, labels)\n    f1 = f1_score(y_true=labels, y_pred=preds)\n    return {\n        ""accuracy"": acc,\n        ""f1"": f1,\n    }\n\ndef pearson_and_spearman(preds, labels):\n    pearson_corr = pearsonr(preds, labels)[0]\n    spearman_corr = spearmanr(preds, labels)[0]\n    return {\n        ""pearson"": pearson_corr,\n        ""spearmanr"": spearman_corr,\n    }\n\n\nclass Glue(nlp.Metric):\n    def _info(self):\n        if self.config_name not in [""sst2"", ""mnli"", ""mnli_mismatched"", ""mnli_matched"",\n                ""cola"", ""stsb"", ""mrpc"", ""qqp"", ""qnli"", ""rte"", ""wnli"", ""hans""]:\n            raise KeyError(\'You should supply a configuration name selected in \'\n                           \'[""sst2"", ""mnli"", ""mnli_mismatched"", ""mnli_matched"", \'\n                           \'""cola"", ""stsb"", ""mrpc"", ""qqp"", ""qnli"", ""rte"", ""wnli"", ""hans""]\')\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Value(\'int64\' if self.config_name != \'sts-b\' else \'float32\'),\n                \'references\': nlp.Value(\'int64\' if self.config_name != \'sts-b\' else \'float32\'),\n            }),\n            codebase_urls=[],\n            reference_urls=[],\n            format=\'numpy\'\n        )\n\n    def _compute(self, predictions, references):\n        if self.config_name == ""cola"":\n            return {""matthews_correlation"": matthews_corrcoef(references, predictions)}\n        elif self.config_name == ""stsb"":\n            return pearson_and_spearman(predictions, references)\n        elif self.config_name in [""mrpc"", ""qqp""]:\n            return acc_and_f1(predictions, references)\n        elif self.config_name in [""sst2"", ""mnli"", ""mnli_mismatched"", ""mnli_matched"", ""qnli"", ""rte"", ""wnli"", ""hans""]:\n            return {""accuracy"": simple_accuracy(predictions, references)}\n        else:\n            raise KeyError(\'You should supply a configuration name selected in \'\n                           \'[""sst2"", ""mnli"", ""mnli_mismatched"", ""mnli_matched"", \'\n                           \'""cola"", ""stsb"", ""mrpc"", ""qqp"", ""qnli"", ""rte"", ""wnli"", ""hans""]\')\n'"
metrics/rouge/rouge.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" ROUGE metric from Google Research github repo. """"""\n\nimport nlp\n\n# The dependencies in https://github.com/google-research/google-research/blob/master/rouge/requirements.txt\nimport absl  # Here to have a nice missing dependency error message early on\nimport nltk  # Here to have a nice missing dependency error message early on\nimport numpy  # Here to have a nice missing dependency error message early on\nimport six  # Here to have a nice missing dependency error message early on\n\nfrom rouge_score import rouge_scorer\nfrom rouge_score import scoring\n\n_CITATION = """"""\\\n@inproceedings{lin-2004-rouge,\n    title = ""{ROUGE}: A Package for Automatic Evaluation of Summaries"",\n    author = ""Lin, Chin-Yew"",\n    booktitle = ""Text Summarization Branches Out"",\n    month = jul,\n    year = ""2004"",\n    address = ""Barcelona, Spain"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/W04-1013"",\n    pages = ""74--81"",\n}\n""""""\n\n_DESCRIPTION = """"""\\\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\nevaluating automatic summarization and machine translation software in natural language processing.\nThe metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n\nThis metrics is a wrapper around Google Research reimplementation of ROUGE:\nhttps://github.com/google-research/google-research/tree/master/rouge\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each predictions\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\nReturns:\n    rouge1: rouge_1 f1,\n    rouge2: rouge_2 f1,\n    rougeL: rouge_l f1,\n    rougeLsum: rouge_l precision\n""""""\n\nclass Rouge(nlp.Metric):\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Value(\'string\', id=\'sequence\'),\n                \'references\': nlp.Value(\'string\', id=\'sequence\'),\n            }),\n            codebase_urls=[""https://github.com/google-research/google-research/tree/master/rouge""],\n            reference_urls=[""https://en.wikipedia.org/wiki/ROUGE_(metric)"",\n                            ""https://github.com/google-research/google-research/tree/master/rouge""]\n        )\n\n    def _compute(self, predictions, references, rouge_types=None, use_agregator=True, use_stemmer=False):\n        if rouge_types is None:\n            rouge_types = [\'rouge1\', \'rougeL\']\n\n        scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)\n        if use_agregator:\n            aggregator = scoring.BootstrapAggregator()\n        else:\n            scores = []\n\n        for ref, pred in zip(references, predictions):\n            score = scorer.score(ref, pred)\n            if use_agregator:\n                aggregator.add_scores(score)\n            else:\n                scores.append(score)\n\n        if use_agregator:\n            result = aggregator.aggregate()\n        else:\n            result = {}\n            for key in scores[0]:\n                result[key] = list(score[key] for score in scores)\n\n        return result\n'"
metrics/sacrebleu/sacrebleu.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" SACREBLEU metric. """"""\n\nimport nlp\nimport sacrebleu as scb\n\n_CITATION = """"""\\\n@inproceedings{post-2018-call,\n    title = ""A Call for Clarity in Reporting {BLEU} Scores"",\n    author = ""Post, Matt"",\n    booktitle = ""Proceedings of the Third Conference on Machine Translation: Research Papers"",\n    month = oct,\n    year = ""2018"",\n    address = ""Belgium, Brussels"",\n    publisher = ""Association for Computational Linguistics"",\n    url = ""https://www.aclweb.org/anthology/W18-6319"",\n    pages = ""186--191"",\n}\n""""""\n\n_DESCRIPTION = """"""\\\nSacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores.\nInspired by Rico Sennrich\'s `multi-bleu-detok.perl`, it produces the official WMT scores but works with plain text.\nIt also knows all the standard test sets and handles downloading, processing, and tokenization for you.\n\nSee the [README.md] file at https://github.com/mjpost/sacreBLEU for more information.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nProduces BLEU scores along with its sufficient statistics\nfrom a source against one or more references.\n\nArgs:\n    predictions: The system stream (a sequence of segments)\n    references: A list of one or more reference streams (each a sequence of segments)\n    smooth: The smoothing method to use\n    smooth_value: For \'floor\' smoothing, the floor to use\n    force: Ignore data that looks already tokenized\n    lowercase: Lowercase the data\n    tokenize: The tokenizer to use\nReturns:\n    \'score\': BLEU score,\n    \'counts\': Counts,\n    \'totals\': Totals,\n    \'precisions\': Precisions,\n    \'bp\': Brevity penalty,\n    \'sys_len\': predictions length,\n    \'ref_len\': reference length,\n""""""\n\nclass Sacrebleu(nlp.Metric):\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            homepage=""https://github.com/mjpost/sacreBLEU"",\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Value(\'string\', id=\'sequence\'),\n                \'references\': nlp.Sequence(nlp.Value(\'string\', id=\'sequence\'), id=\'references\'),\n            }),\n            codebase_urls=[""https://github.com/mjpost/sacreBLEU""],\n            reference_urls=[""https://github.com/mjpost/sacreBLEU"",\n                            ""https://en.wikipedia.org/wiki/BLEU"",\n                            ""https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213""]\n        )\n\n    def _compute(self, predictions, references, smooth_method=\'exp\',\n                smooth_value=None,\n                force=False,\n                lowercase=False,\n                tokenize=scb.sacrebleu.DEFAULT_TOKENIZER,\n                use_effective_order=False):\n        output = scb.corpus_bleu(\n            sys_stream=predictions,\n            ref_streams=references,\n            smooth_method=smooth_method,\n            smooth_value=smooth_value,\n            force=force,\n            lowercase=lowercase,\n            tokenize=tokenize,\n            use_effective_order=use_effective_order)\n        output_dict = {\n            \'score\': output.score,\n            \'counts\': output.counts,\n            \'totals\': output.totals,\n            \'precisions\': output.precisions,\n            \'bp\': output.bp,\n            \'sys_len\': output.sys_len,\n            \'ref_len\': output.ref_len,\n        }\n        return output_dict\n'"
metrics/seqeval/seqeval.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" seqeval metric. """"""\n\nfrom collections import defaultdict\n\nimport nlp\nfrom seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n_CITATION = """"""\\\n""""""\n\n_DESCRIPTION = """"""\\\nseqeval is a Python framework for sequence labeling evaluation.\nseqeval can evaluate the performance of chunking tasks such as named-entity recognition, part-of-speech tagging, semantic role labeling and so on.\n\nThis is well-tested by using the Perl script conlleval, which can be used for\nmeasuring the performance of a system that has processed the CoNLL-2000 shared task data.\n\nseqeval supports following formats:\nIOB1\nIOB2\nIOE1\nIOE2\nIOBES\n\nSee the [README.md] file at https://github.com/chakki-works/seqeval for more information.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nProduces labelling scores along with its sufficient statistics\nfrom a source against one or more references.\n\nArgs:\n    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n    references: List of List of reference labels (Ground truth (correct) target values)\n    suffix: True if the types are not in IOBs format False otherwise. default: False\nReturns:\n    Overall:\n        \'accuracy\': accuracy,\n        \'precision\': precision,\n        \'recall\': recall,\n        \'f1\': F1 score, also known as balanced F-score or F-measure,\n    Per type:\n        \'precision\': precision,\n        \'recall\': recall,\n        \'f1\': F1 score, also known as balanced F-score or F-measure,\n""""""\n\ndef end_of_chunk(prev_tag, tag, prev_type, type_):\n    """"""Checks if a chunk ended between the previous and current word.\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n    Returns:\n        chunk_end: boolean.\n    """"""\n    chunk_end = False\n\n    if (prev_tag in [""B"", ""I""] and tag in [""B"", ""S"", ""O""]) or prev_tag in [""E"", ""S""]:\n        chunk_end = True\n\n    if prev_tag not in [\'O\', \'.\'] and prev_type != type_:\n        chunk_end = True\n\n    return chunk_end\n\n\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    """"""Checks if a chunk started between the previous and current word.\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n    Returns:\n        chunk_start: boolean.\n    """"""\n    chunk_start = False\n\n    if (prev_tag in [""E"", ""S"", ""O""] and tag in [""E"", ""I""]) or tag in [""B"", ""S""]:\n        chunk_start = True\n\n    if tag not in [\'O\', \'.\'] and prev_type != type_:\n        chunk_start = True\n\n    return chunk_start\n\ndef get_entities(seq, suffix=False):\n    """"""Gets entities from sequence.\n    Args:\n        seq (list): sequence of labels.\n    Returns:\n        list: list of (chunk_type, chunk_start, chunk_end).\n    """"""\n    if any(isinstance(s, list) for s in seq):\n        seq = [item for sublist in seq for item in sublist + [\'O\']]\n\n    prev_tag = \'O\'\n    prev_type = \'\'\n    begin_offset = 0\n    chunks = []\n    for i, chunk in enumerate(seq + [\'O\']):\n        if suffix:\n            tag = chunk[-1]\n            type_ = chunk.split(\'-\')[0]\n        else:\n            tag = chunk[0]\n            type_ = chunk.split(\'-\')[-1]\n\n        if end_of_chunk(prev_tag, tag, prev_type, type_):\n            chunks.append((prev_type, begin_offset, i-1))\n        if start_of_chunk(prev_tag, tag, prev_type, type_):\n            begin_offset = i\n        prev_tag = tag\n        prev_type = type_\n\n    return chunks\n\nclass Seqeval(nlp.Metric):\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            homepage=""https://github.com/chakki-works/seqeval"",\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Sequence(nlp.Value(\'string\', id=\'label\'), id=\'sequence\'),\n                \'references\': nlp.Sequence(nlp.Value(\'string\', id=\'label\'), id=\'sequence\'),\n            }),\n            codebase_urls=[""https://github.com/chakki-works/seqeval""],\n            reference_urls=[""https://github.com/chakki-works/seqeval""]\n        )\n\n    def _compute(self, predictions, references, suffix=False):\n        true_entities = set(get_entities(references, suffix))\n        pred_entities = set(get_entities(predictions, suffix))\n        d1 = defaultdict(set)\n        d2 = defaultdict(set)\n        scores = {}\n\n        for e in true_entities:\n            d1[e[0]].add((e[1], e[2]))\n\n        for e in pred_entities:\n            d2[e[0]].add((e[1], e[2]))\n        \n        for type_name, true_entities in d1.items():\n            scores[type_name] = {}\n            pred_entities = d2[type_name]\n            nb_correct = len(true_entities & pred_entities)\n            nb_pred = len(pred_entities)\n            nb_true = len(true_entities)\n\n            p = nb_correct / nb_pred if nb_pred > 0 else 0\n            r = nb_correct / nb_true if nb_true > 0 else 0\n            f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n\n            scores[type_name][""precision""] = p\n            scores[type_name][""recall""] = r\n            scores[type_name][""f1""] = f1\n            scores[type_name][""number""] = nb_true\n\n        scores[""overall_precision""] = precision_score(y_true=references, y_pred=predictions, suffix=suffix)\n        scores[""overall_recall""] = recall_score(y_true=references, y_pred=predictions, suffix=suffix)\n        scores[""overall_f1""] = f1_score(y_true=references, y_pred=predictions, suffix=suffix)\n        scores[""overall_accuracy""] = accuracy_score(y_true=references, y_pred=predictions)\n\n        return scores\n'"
metrics/squad/evaluate.py,0,"b'"""""" Official evaluation script for v1.1 of the SQuAD dataset. """"""\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\n\ndef normalize_answer(s):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef evaluate(dataset, predictions):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        for paragraph in article[\'paragraphs\']:\n            for qa in paragraph[\'qas\']:\n                total += 1\n                if qa[\'id\'] not in predictions:\n                    message = \'Unanswered question \' + qa[\'id\'] + \\\n                              \' will receive score 0.\'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x[\'text\'], qa[\'answers\']))\n                prediction = predictions[qa[\'id\']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {\'exact_match\': exact_match, \'f1\': f1}\n\n\nif __name__ == \'__main__\':\n    expected_version = \'1.1\'\n    parser = argparse.ArgumentParser(\n        description=\'Evaluation for SQuAD \' + expected_version)\n    parser.add_argument(\'dataset_file\', help=\'Dataset file\')\n    parser.add_argument(\'prediction_file\', help=\'Prediction File\')\n    args = parser.parse_args()\n    with open(args.dataset_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if (dataset_json[\'version\'] != expected_version):\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    with open(args.prediction_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    print(json.dumps(evaluate(dataset, predictions)))\n'"
metrics/squad/squad.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" SQuAD metric. """"""\n\nimport nlp\nfrom .evaluate import evaluate\n\n_CITATION = """"""\\\n@inproceedings{Rajpurkar2016SQuAD10,\n  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},\n  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},\n  booktitle={EMNLP},\n  year={2016}\n}\n""""""\n\n_DESCRIPTION = """"""\nThis metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).\n\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by\ncrowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,\nfrom the corresponding reading passage, or the question might be unanswerable.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nComputes SQuAD scores (F1 and EM).\nArgs:\n    predictions: List of question-answers dictionaries with the following key-values:\n        - \'id\': id of the question-answer pair as given in the references (see below)\n        - \'prediction_text\': the text of the answer\n    references: List of question-answers dictionaries with the following key-values:\n        - \'id\': id of the question-answer pair (see above),\n        - \'answers\': a Dict {\'text\': list of possible texts for the answer, as a list of strings}\nReturns:\n    \'exact_match\': Exact match (the normalized answer exactly match the gold answer)\n    \'f1\': The F-score of predicted tokens versus the gold answer\n""""""\n\nclass Squad(nlp.Metric):\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': {\n                    ""id"": nlp.Value(""string""),\n                    ""prediction_text"": nlp.Value(""string"")\n                },\n                \'references\': {\n                    \'id\': nlp.Value(\'string\'),\n                    \'answers\': nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    )\n                },\n            }),\n            codebase_urls=[""https://rajpurkar.github.io/SQuAD-explorer/""],\n            reference_urls=[""https://rajpurkar.github.io/SQuAD-explorer/""]\n        )\n\n    def _compute(self, predictions, references):\n        pred_dict = {prediction[""id""]: prediction[""prediction_text""] for prediction in predictions}\n        dataset = [{\'paragraphs\': [{\'qas\': [\n            {""answers"": [{""text"": answer_text} for answer_text in ref[""answers""][""text""]], ""id"": ref[""id""]} for ref in references\n        ]}]}]\n        score = evaluate(dataset=dataset, predictions=pred_dict)\n        return score\n'"
metrics/squad_v2/evaluate.py,0,"b'""""""Official evaluation script for SQuAD version 2.0.\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID\'s to the model\'s predicted probability\nthat a question is unanswerable.\n""""""\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\n\nOPTS = None\n\ndef parse_args():\n  parser = argparse.ArgumentParser(\'Official evaluation script for SQuAD version 2.0.\')\n  parser.add_argument(\'data_file\', metavar=\'data.json\', help=\'Input data JSON file.\')\n  parser.add_argument(\'pred_file\', metavar=\'pred.json\', help=\'Model predictions.\')\n  parser.add_argument(\'--out-file\', \'-o\', metavar=\'eval.json\',\n                      help=\'Write accuracy metrics to file (default is stdout).\')\n  parser.add_argument(\'--na-prob-file\', \'-n\', metavar=\'na_prob.json\',\n                      help=\'Model estimates of probability of no answer.\')\n  parser.add_argument(\'--na-prob-thresh\', \'-t\', type=float, default=1.0,\n                      help=\'Predict """" if no-answer probability exceeds this (default = 1.0).\')\n  parser.add_argument(\'--out-image-dir\', \'-p\', metavar=\'out_images\', default=None,\n                      help=\'Save precision-recall curves to directory.\')\n  parser.add_argument(\'--verbose\', \'-v\', action=\'store_true\')\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n  return parser.parse_args()\n\ndef make_qid_to_has_ans(dataset):\n  qid_to_has_ans = {}\n  for article in dataset:\n    for p in article[\'paragraphs\']:\n      for qa in p[\'qas\']:\n        qid_to_has_ans[qa[\'id\']] = bool(qa[\'answers\'])\n  return qid_to_has_ans\n\ndef normalize_answer(s):\n  """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n  def remove_articles(text):\n    regex = re.compile(r\'\\b(a|an|the)\\b\', re.UNICODE)\n    return re.sub(regex, \' \', text)\n  def white_space_fix(text):\n    return \' \'.join(text.split())\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return \'\'.join(ch for ch in text if ch not in exclude)\n  def lower(text):\n    return text.lower()\n  return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n  if not s: return []\n  return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n  gold_toks = get_tokens(a_gold)\n  pred_toks = get_tokens(a_pred)\n  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n  num_same = sum(common.values())\n  if len(gold_toks) == 0 or len(pred_toks) == 0:\n    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n    return int(gold_toks == pred_toks)\n  if num_same == 0:\n    return 0\n  precision = 1.0 * num_same / len(pred_toks)\n  recall = 1.0 * num_same / len(gold_toks)\n  f1 = (2 * precision * recall) / (precision + recall)\n  return f1\n\ndef get_raw_scores(dataset, preds):\n  exact_scores = {}\n  f1_scores = {}\n  for article in dataset:\n    for p in article[\'paragraphs\']:\n      for qa in p[\'qas\']:\n        qid = qa[\'id\']\n        gold_answers = [a[\'text\'] for a in qa[\'answers\']\n                        if normalize_answer(a[\'text\'])]\n        if not gold_answers:\n          # For unanswerable questions, only correct answer is empty string\n          gold_answers = [\'\']\n        if qid not in preds:\n          print(\'Missing prediction for %s\' % qid)\n          continue\n        a_pred = preds[qid]\n        # Take max over all gold answers\n        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n  return exact_scores, f1_scores\n\ndef apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n  new_scores = {}\n  for qid, s in scores.items():\n    pred_na = na_probs[qid] > na_prob_thresh\n    if pred_na:\n      new_scores[qid] = float(not qid_to_has_ans[qid])\n    else:\n      new_scores[qid] = s\n  return new_scores\n\ndef make_eval_dict(exact_scores, f1_scores, qid_list=None):\n  if not qid_list:\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        (\'exact\', 100.0 * sum(exact_scores.values()) / total),\n        (\'f1\', 100.0 * sum(f1_scores.values()) / total),\n        (\'total\', total),\n    ])\n  else:\n    total = len(qid_list)\n    return collections.OrderedDict([\n        (\'exact\', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n        (\'f1\', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n        (\'total\', total),\n    ])\n\ndef merge_eval(main_eval, new_eval, prefix):\n  for k in new_eval:\n    main_eval[\'%s_%s\' % (prefix, k)] = new_eval[k]\n\ndef plot_pr_curve(precisions, recalls, out_image, title):\n  plt.step(recalls, precisions, color=\'b\', alpha=0.2, where=\'post\')\n  plt.fill_between(recalls, precisions, step=\'post\', alpha=0.2, color=\'b\')\n  plt.xlabel(\'Recall\')\n  plt.ylabel(\'Precision\')\n  plt.xlim([0.0, 1.05])\n  plt.ylim([0.0, 1.05])\n  plt.title(title)\n  plt.savefig(out_image)\n  plt.clf()\n\ndef make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n                               out_image=None, title=None):\n  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n  true_pos = 0.0\n  cur_p = 1.0\n  cur_r = 0.0\n  precisions = [1.0]\n  recalls = [0.0]\n  avg_prec = 0.0\n  for i, qid in enumerate(qid_list):\n    if qid_to_has_ans[qid]:\n      true_pos += scores[qid]\n    cur_p = true_pos / float(i+1)\n    cur_r = true_pos / float(num_true_pos)\n    if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i+1]]:\n      # i.e., if we can put a threshold after this point\n      avg_prec += cur_p * (cur_r - recalls[-1])\n      precisions.append(cur_p)\n      recalls.append(cur_r)\n  if out_image:\n    plot_pr_curve(precisions, recalls, out_image, title)\n  return {\'ap\': 100.0 * avg_prec}\n\ndef run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs, \n                                  qid_to_has_ans, out_image_dir):\n  if out_image_dir and not os.path.exists(out_image_dir):\n    os.makedirs(out_image_dir)\n  num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n  if num_true_pos == 0:\n    return\n  pr_exact = make_precision_recall_eval(\n      exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n      out_image=os.path.join(out_image_dir, \'pr_exact.png\'),\n      title=\'Precision-Recall curve for Exact Match score\')\n  pr_f1 = make_precision_recall_eval(\n      f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n      out_image=os.path.join(out_image_dir, \'pr_f1.png\'),\n      title=\'Precision-Recall curve for F1 score\')\n  oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n  pr_oracle = make_precision_recall_eval(\n      oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n      out_image=os.path.join(out_image_dir, \'pr_oracle.png\'),\n      title=\'Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)\')\n  merge_eval(main_eval, pr_exact, \'pr_exact\')\n  merge_eval(main_eval, pr_f1, \'pr_f1\')\n  merge_eval(main_eval, pr_oracle, \'pr_oracle\')\n\ndef histogram_na_prob(na_probs, qid_list, image_dir, name):\n  if not qid_list:\n    return\n  x = [na_probs[k] for k in qid_list]\n  weights = np.ones_like(x) / float(len(x))\n  plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n  plt.xlabel(\'Model probability of no-answer\')\n  plt.ylabel(\'Proportion of dataset\')\n  plt.title(\'Histogram of no-answer probability: %s\' % name)\n  plt.savefig(os.path.join(image_dir, \'na_prob_hist_%s.png\' % name))\n  plt.clf()\n\ndef find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n  cur_score = num_no_ans\n  best_score = cur_score\n  best_thresh = 0.0\n  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n  for i, qid in enumerate(qid_list):\n    if qid not in scores: continue\n    if qid_to_has_ans[qid]:\n      diff = scores[qid]\n    else:\n      if preds[qid]:\n        diff = -1\n      else:\n        diff = 0\n    cur_score += diff\n    if cur_score > best_score:\n      best_score = cur_score\n      best_thresh = na_probs[qid]\n  return 100.0 * best_score / len(scores), best_thresh\n\ndef find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n  best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n  best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n  main_eval[\'best_exact\'] = best_exact\n  main_eval[\'best_exact_thresh\'] = exact_thresh\n  main_eval[\'best_f1\'] = best_f1\n  main_eval[\'best_f1_thresh\'] = f1_thresh\n\ndef main():\n  with open(OPTS.data_file) as f:\n    dataset_json = json.load(f)\n    dataset = dataset_json[\'data\']\n  with open(OPTS.pred_file) as f:\n    preds = json.load(f)\n  if OPTS.na_prob_file:\n    with open(OPTS.na_prob_file) as f:\n      na_probs = json.load(f)\n  else:\n    na_probs = {k: 0.0 for k in preds}\n  qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n  has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n  no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n  exact_raw, f1_raw = get_raw_scores(dataset, preds)\n  exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n                                        OPTS.na_prob_thresh)\n  f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n                                     OPTS.na_prob_thresh)\n  out_eval = make_eval_dict(exact_thresh, f1_thresh)\n  if has_ans_qids:\n    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n    merge_eval(out_eval, has_ans_eval, \'HasAns\')\n  if no_ans_qids:\n    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n    merge_eval(out_eval, no_ans_eval, \'NoAns\')\n  if OPTS.na_prob_file:\n    find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n  if OPTS.na_prob_file and OPTS.out_image_dir:\n    run_precision_recall_analysis(out_eval, exact_raw, f1_raw, na_probs, \n                                  qid_to_has_ans, OPTS.out_image_dir)\n    histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, \'hasAns\')\n    histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, \'noAns\')\n  if OPTS.out_file:\n    with open(OPTS.out_file, \'w\') as f:\n      json.dump(out_eval, f)\n  else:\n    print(json.dumps(out_eval, indent=2))\n\nif __name__ == \'__main__\':\n  OPTS = parse_args()\n  if OPTS.out_image_dir:\n    import matplotlib\n    matplotlib.use(\'Agg\')\n    import matplotlib.pyplot as plt \n  main()\n'"
metrics/squad_v2/squad_v2.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" SQuAD v2 metric. """"""\n\nimport nlp\nfrom .evaluate import evaluate\n\n_CITATION = """"""\\\n@inproceedings{Rajpurkar2016SQuAD10,\n  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},\n  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},\n  booktitle={EMNLP},\n  year={2016}\n}\n""""""\n\n_DESCRIPTION = """"""\nThis metric wrap the official scoring script for version 2 of the Stanford Question\nAnswering Dataset (SQuAD).\n\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by\ncrowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,\nfrom the corresponding reading passage, or the question might be unanswerable.\n\nSQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions\nwritten adversarially by crowdworkers to look similar to answerable ones.\nTo do well on SQuAD2.0, systems must not only answer questions when possible, but also\ndetermine when no answer is supported by the paragraph and abstain from answering.\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nComputes SQuAD v2 scores (F1 and EM).\nArgs:\n    predictions: List of triple for question-answers to score with the following elements:\n        - the question-answer \'id\' field as given in the references (see below)\n        - the text of the answer\n        - the probability that the question has no answer\n    references: List of question-answers dictionaries with the following key-values:\n            - \'id\': id of the question-answer pair (see above),\n            - \'answers\': a list of Dict {\'text\': text of the answer as a string}\n    no_answer_threshold: float\n        Probability threshold to decide that a question has no answer.\nReturns:\n    \'exact\': Exact match (the normalized answer exactly match the gold answer)\n    \'f1\': The F-score of predicted tokens versus the gold answer\n    \'total\': Number of score considered\n    \'HasAns_exact\': Exact match (the normalized answer exactly match the gold answer)\n    \'HasAns_f1\': The F-score of predicted tokens versus the gold answer\n    \'HasAns_total\': Number of score considered\n    \'NoAns_exact\': Exact match (the normalized answer exactly match the gold answer)\n    \'NoAns_f1\': The F-score of predicted tokens versus the gold answer\n    \'NoAns_total\': Number of score considered\n    \'best_exact\': Best exact match (with varying threshold)\n    \'best_exact_thresh\': No-answer probability threshold associated to the best exact match\n    \'best_f1\': Best F1 (with varying threshold)\n    \'best_f1_thresh\': No-answer probability threshold associated to the best F1\n""""""\n\nclass SquadV2(nlp.Metric):\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': {\n                    ""id"": nlp.Value(""string""),\n                    ""prediction_text"": nlp.Value(""string""),\n                    ""no_answer_probability"": nlp.Value(""float32"")\n                },\n                \'references\': {\n                    ""id"": nlp.Value(""string""),\n                    ""answers"": nlp.features.Sequence(\n                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}\n                    ),\n                },\n            }),\n            codebase_urls=[""https://rajpurkar.github.io/SQuAD-explorer/""],\n            reference_urls=[""https://rajpurkar.github.io/SQuAD-explorer/""]\n        )\n\n    def _compute(self, predictions, references, no_answer_threshold=1.0):\n        predictions = dict((p[\'id\'], p[\'prediction_text\']) for p in predictions)\n        dataset = [{\'paragraphs\': [{\'qas\': references}]}]\n        no_answer_probabilities = dict((p[\'id\'], p[\'no_answer_probability\']) for p in predictions)\n\n        qid_to_has_ans = evaluate.make_qid_to_has_ans(dataset)  # maps qid to True/False\n        has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n        no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n\n        exact_raw, f1_raw = evaluate.get_raw_scores(dataset, predictions)\n        exact_thresh = evaluate.apply_no_ans_threshold(exact_raw,\n                                                       no_answer_probabilities, \n                                                       qid_to_has_ans,\n                                                       no_answer_threshold)\n        f1_thresh = evaluate.apply_no_ans_threshold(f1_raw,\n                                                    no_answer_probabilities,\n                                                    qid_to_has_ans,\n                                                    no_answer_threshold)\n        out_eval = evaluate.make_eval_dict(exact_thresh, f1_thresh)\n\n        if has_ans_qids:\n            has_ans_eval = evaluate.make_eval_dict(exact_thresh,\n                                                   f1_thresh,\n                                                   qid_list=has_ans_qids)\n            evaluate.merge_eval(out_eval, has_ans_eval, \'HasAns\')\n        if no_ans_qids:\n            no_ans_eval = evaluate.make_eval_dict(exact_thresh,\n                                                  f1_thresh,\n                                                  qid_list=no_ans_qids)\n            evaluate.merge_eval(out_eval, no_ans_eval, \'NoAns\')\n        return out_eval\n'"
metrics/xnli/xnli.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" XNLI benchmark metric. """"""\n\nimport nlp\n\n_CITATION = """"""\\\n@InProceedings{conneau2018xnli,\n  author = ""Conneau, Alexis\n                 and Rinott, Ruty\n                 and Lample, Guillaume\n                 and Williams, Adina\n                 and Bowman, Samuel R.\n                 and Schwenk, Holger\n                 and Stoyanov, Veselin"",\n  title = ""XNLI: Evaluating Cross-lingual Sentence Representations"",\n  booktitle = ""Proceedings of the 2018 Conference on Empirical Methods\n               in Natural Language Processing"",\n  year = ""2018"",\n  publisher = ""Association for Computational Linguistics"",\n  location = ""Brussels, Belgium"",\n}\n""""""\n\n_DESCRIPTION = """"""\\\nXNLI is a subset of a few thousand examples from MNLI which has been translated\ninto a 14 different languages (some low-ish resource). As with MNLI, the goal is\nto predict textual entailment (does sentence A imply/contradict/neither sentence\nB) and is a classification task (given two sentences, predict one of three\nlabels).\n""""""\n\n_KWARGS_DESCRIPTION = """"""\nComputes XNLI score which is just simple accuracy.\nArgs:\n    predictions: Predicted labels.\n    references: Ground truth labels.\nReturns:\n    \'accuracy\': accuracy\n""""""\n\ndef simple_accuracy(preds, labels):\n    return (preds == labels).mean()\n\nclass Xnli(nlp.Metric):\n    def _info(self):\n        return nlp.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=nlp.Features({\n                \'predictions\': nlp.Value(\'int64\' if self.config_name != \'sts-b\' else \'float32\'),\n                \'references\': nlp.Value(\'int64\' if self.config_name != \'sts-b\' else \'float32\'),\n            }),\n            codebase_urls=[],\n            reference_urls=[],\n            format=\'numpy\'\n        )\n\n    def _compute(self, predictions, references):\n        return {""accuracy"": simple_accuracy(predictions, references)}\n'"
src/nlp/__init__.py,0,"b'# flake8: noqa\n# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n# pylint: enable=line-too-long\n# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\n\n__version__ = ""0.2.0""\n\nfrom pyarrow import total_allocated_bytes\n\nfrom . import datasets\nfrom .arrow_dataset import Dataset\nfrom .arrow_reader import ReadInstruction\nfrom .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\nfrom .features import ClassLabel, Features, Sequence, Tensor, Translation, TranslationVariableLanguages, Value\nfrom .info import DatasetInfo, MetricInfo\nfrom .inspect import inspect_dataset, inspect_metric, list_datasets, list_metrics\nfrom .load import import_main_class, load_dataset, load_metric, prepare_module\nfrom .metric import Metric\nfrom .splits import NamedSplit, Split, SplitBase, SplitDict, SplitGenerator, SplitInfo, SubSplitInfo, percent\nfrom .utils import *\nfrom .utils.tqdm_utils import disable_progress_bar\n'"
src/nlp/arrow_dataset.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n"""""" Simple Dataset wrapping an Arrow Table.""""""\n\nimport hashlib\nimport logging\nimport os\nfrom collections import defaultdict\nfrom collections.abc import Mapping\nfrom typing import Any, Dict, List, Optional, Union\n\nimport numpy as np\nimport pyarrow as pa\nfrom tqdm import tqdm\n\nfrom nlp.utils.py_utils import dumps\n\nfrom .arrow_writer import ArrowWriter\nfrom .utils import convert_tuples_in_lists, map_nested\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Dataset(object):\n    """""" A Dataset backed by an Arrow table or Record Batch.\n    """"""\n\n    def __init__(\n        self,\n        arrow_table: Union[pa.Table, pa.RecordBatch],\n        data_files: Optional[List[dict]] = None,\n        info: Optional[Any] = None,\n    ):\n        self._info = info\n        self._data: pa.Table = arrow_table\n        self._data_files: List[dict] = data_files if data_files is not None else []\n        self._format_type = None\n        self._format_columns = None\n        self._output_all_columns = False\n\n    @classmethod\n    def from_file(cls, filename: str):\n        """""" Instantiate a Dataset backed by an Arrow table at filename """"""\n        mmap = pa.memory_map(filename)\n        f = pa.ipc.open_stream(mmap)\n        pa_table = f.read_all()\n        return cls(arrow_table=pa_table, data_files=[{""filename"": filename}])\n\n    @classmethod\n    def from_buffer(cls, buffer: pa.Buffer):\n        """""" Instantiate a Dataset backed by an Arrow buffer """"""\n        mmap = pa.BufferReader(buffer)\n        f = pa.ipc.open_stream(mmap)\n        pa_table = f.read_all()\n        return cls(pa_table)\n\n    @property\n    def info(self):\n        return self._info\n\n    @property\n    def data(self):\n        return self._data\n\n    @property\n    def cache_files(self):\n        return self._data_files\n\n    @property\n    def columns(self):\n        return self._data.columns\n\n    @property\n    def nbytes(self):\n        return self._data.nbytes\n\n    @property\n    def num_columns(self):\n        return self._data.num_columns\n\n    @property\n    def num_rows(self):\n        return self._data.num_rows\n\n    @property\n    def column_names(self):\n        return self._data.column_names\n\n    @property\n    def schema(self) -> pa.Schema:\n        return self._data.schema\n\n    @property\n    def shape(self):\n        return self._data.shape\n\n    def drop(self, columns: Union[str, List[str]]):\n        """""" Drop one or more columns.\n\n        Args:\n            columns: list of str\n        """"""\n        if isinstance(columns, str):\n            columns = [columns]\n        if any(col not in self._data.column_names for col in columns):\n            raise ValueError(\n                ""Columns {} not in the dataset. Current columns in the dataset: {}"".format(\n                    list(filter(lambda col: col not in self._data.column_names, columns)), self._data.column_names\n                )\n            )\n        self._data = self._data.drop(columns)\n\n    def unique(self, column: str):\n        """""" Return a list of the unque elements in a column.\n\n        Args:\n            columns: str\n        """"""\n        if column not in self._data.column_names:\n            raise ValueError(f""Column ({column}) not in table columns ({self._data.column_names})."")\n        return self._data.column(column).unique().to_pylist()\n\n    def dictionary_encode_column(self, column: str):\n        """""" Dictionary encode a column.\n            Dictionnary encode can reduce the size of a column with many repetitions (e.g. string labels columns)\n            by storing a dictionnary of the strings. This only affect the internal storage.\n\n        Args:\n            columns: str\n        """"""\n        if column not in self._data.column_names:\n            raise ValueError(f""Column ({column}) not in table columns ({self._data.column_names})."")\n        casted_schema: pa.Schema = self._data.schema\n        field_index = casted_schema.get_field_index(column)\n        field: pa.Field = casted_schema.field(field_index)\n        casted_field = pa.field(field.name, pa.dictionary(pa.int32(), field.type), nullable=False)\n        casted_schema.set(field_index, casted_field)\n        self._data = self._data.cast(casted_schema)\n\n    def flatten(self):\n        """""" Flatten the Table.\n            Each column with a struct type is flattened into one column per struct field.\n            Other columns are left unchanged.\n        """"""\n        self._data = self._data.flatten()\n\n    def __len__(self):\n        return self._data.num_rows\n\n    def __iter__(self):\n        format_type = self._format_type\n        format_columns = self._format_columns\n        output_all_columns = self._output_all_columns\n        for index in range(self._data.num_rows):\n            yield self._getitem(\n                index, format_type=format_type, format_columns=format_columns, output_all_columns=output_all_columns,\n            )\n\n    def __repr__(self):\n        schema_str = dict((a, str(b)) for a, b in zip(self._data.schema.names, self._data.schema.types))\n        return f""Dataset(schema: {schema_str}, num_rows: {self.num_rows})""\n\n    @property\n    def format(self):\n        return {\n            ""type"": ""python"" if self._format_type is None else self._format_type,\n            ""columns"": self.column_names if self._format_columns is None else self._format_columns,\n            ""output_all_columns"": self._output_all_columns,\n        }\n\n    def set_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False):\n        """""" Set __getitem__ return format (type and columns)\n\n            Args:\n                type (Optional ``str``): output type selected in [None, \'numpy\', \'torch\', \'tensorflow\', \'pandas\']\n                    None means __getitem__ returns python objects (default)\n                columns (Optional ``List[str]``): columns to format in the output\n                    None means __getitem__ returns all columns (default)\n                output_all_columns (``bool`` default to False): keep un-formated columns as well in the output (as python objects)\n        """"""\n        # Check return type\n        if type == ""torch"":\n            try:\n                import torch  # noqa: F401\n            except ImportError:\n                logger.error(""PyTorch needs to be installed to be able to return PyTorch tensors."")\n        elif type == ""tensorflow"":\n            try:\n                import tensorflow  # noqa: F401\n            except ImportError:\n                logger.error(""Tensorflow needs to be installed to be able to return Tensorflow tensors."")\n        else:\n            assert (\n                type is None or type == ""numpy"" or type == ""pandas""\n            ), ""Return type should be None or selected in [\'numpy\', \'torch\', \'tensorflow\', \'pandas\'].""\n\n        # Check filter column\n        if isinstance(columns, str):\n            columns = [columns]\n        if columns is not None and any(col not in self._data.column_names for col in columns):\n            raise ValueError(\n                ""Columns {} not in the dataset. Current columns in the dataset: {}"".format(\n                    list(filter(lambda col: col not in self._data.column_names, columns)), self._data.column_names\n                )\n            )\n\n        self._format_type = type\n        self._format_columns = columns\n        self._output_all_columns = output_all_columns\n        logger.info(\n            ""Set __getitem__(key) output type to %s for %s columns ""\n            "" (when key is int or slice) and %s output other (un-formated) columns."",\n            ""python objects"" if type is None else type,\n            ""no"" if columns is None else str(columns),\n            ""do"" if output_all_columns else ""don\'t"",\n        )\n\n    def reset_format(self):\n        """""" Reset __getitem__ return format to python objects and all columns.\n\n            Same as ``self.set_format()``\n        """"""\n        self.set_format()\n\n    def _convert_outputs(self, outputs, format_type=None, format_columns=None, output_all_columns=False):\n        if format_type is None:\n            if output_all_columns:\n                return outputs\n            if isinstance(outputs, dict) and format_columns is not None:\n                return {k: v for k, v in outputs.items() if k in format_columns}\n            return outputs\n\n        if format_type == ""numpy"":\n            import numpy\n\n            command = numpy.array\n        elif format_type == ""torch"":\n            import torch\n\n            command = torch.tensor\n        elif format_type == ""tensorflow"":\n            import tensorflow\n\n            command = tensorflow.ragged.constant\n        else:\n\n            def identity(x):\n                return x\n\n            command = identity\n\n        if isinstance(outputs, (list, tuple)):\n            return command(outputs)\n        else:\n            output_dict = {}\n            for k, v in outputs.items():\n                if format_columns is not None and k not in format_columns and not output_all_columns:\n                    continue\n                if format_columns is None or k in format_columns:\n                    v = command(v)\n                output_dict[k] = v\n        return output_dict\n\n    @staticmethod\n    def _unnest(py_dict):\n        return dict((key, array[0]) for key, array in py_dict.items())\n\n    @staticmethod\n    def _nest(py_dict):\n        return dict((key, [elem]) for key, elem in py_dict.items())\n\n    def _getitem(\n        self, key: Union[int, slice, str], format_type=None, format_columns=None, output_all_columns=False\n    ) -> Union[Dict, List]:\n        """""" Can be used to index columns (by string names) or rows (by integer index or slices)\n        """"""\n        if isinstance(key, int):\n            if key < 0:\n                key = self._data.num_rows + key\n            if key >= self._data.num_rows:\n                raise IndexError(f""Index ({key}) outside of table length ({self._data.num_rows})."")\n            if format_type is not None and format_type == ""pandas"":\n                outputs = self._data.slice(key, 1).to_pandas()\n            else:\n                outputs = self._unnest(self._data.slice(key, 1).to_pydict())\n        elif isinstance(key, slice):\n            key_indices = key.indices(self._data.num_rows)\n            if key_indices[2] != 1 or key_indices[1] < key_indices[0]:\n                raise ValueError(""Slicing can only take contiguous and ordered slices."")\n            if format_type is not None and format_type == ""pandas"":\n                outputs = self._data.slice(key_indices[0], key_indices[1] - key_indices[0]).to_pandas()\n            else:\n                outputs = self._data.slice(key_indices[0], key_indices[1] - key_indices[0]).to_pydict()\n        elif isinstance(key, str):\n            if key not in self._data.column_names:\n                raise ValueError(f""Column ({key}) not in table columns ({self._data.column_names})."")\n            if format_type is not None:\n                if format_columns is None or key in format_columns:\n                    if format_type == ""pandas"":\n                        outputs = self._data[key].to_pandas()\n                    elif format_type == ""numpy"":\n                        outputs = np.concatenate([arr.to_numpy() for arr in self._data[key].chunks])\n                    else:\n                        outputs = self._convert_outputs(self._data[key].to_pylist(), format_type=format_type)\n                else:\n                    outputs = self._data[key].to_pylist()\n            else:\n                outputs = self._data[key].to_pylist()\n        else:\n            raise ValueError(""Can only get row(s) (int or slice) or columns (string)."")\n\n        if (\n            (format_type is not None or format_columns is not None)\n            and not isinstance(key, str)\n            and format_type != ""pandas""\n        ):\n            outputs = self._convert_outputs(\n                outputs, format_type=format_type, format_columns=format_columns, output_all_columns=output_all_columns\n            )\n        return outputs\n\n    def __getitem__(self, key: Union[int, slice, str]) -> Union[Dict, List]:\n        """""" Can be used to index columns (by string names) or rows (by integer index)\n        """"""\n        return self._getitem(\n            key,\n            format_type=self._format_type,\n            format_columns=self._format_columns,\n            output_all_columns=self._output_all_columns,\n        )\n\n    def cleanup_cache_files(self):\n        """""" Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n            Be carefull when running this command that no other process is currently using other cache files.\n\n            Return:\n                Number of removed files\n        """"""\n        if not self._data_files or ""filename"" not in self._data_files[0]:\n            return None\n        current_cache_file = os.path.abspath(self._data_files[0][""filename""])\n        cache_directory = os.path.dirname(current_cache_file)\n        logger.info(f""Listing files in {cache_directory}"")\n        files: List[str] = os.listdir(cache_directory)\n        files_to_remove = []\n        for f_name in files:\n            full_name = os.path.abspath(os.path.join(cache_directory, f_name))\n            if f_name.startswith(""cache-"") and f_name.endswith("".arrow""):\n                if full_name == current_cache_file:\n                    logger.info(f""Keeping current cache file at {full_name}"")\n                    continue\n                files_to_remove.append(full_name)\n        for file_path in files_to_remove:\n            logger.info(f""Removing {file_path}"")\n            os.remove(file_path)\n        return len(files_to_remove)\n\n    def _get_cache_file_path(self, function, cache_kwargs):\n        """""" Find a unique name from the filenames, kwargs and the function """"""\n        if not self._data_files or ""filename"" not in self._data_files[0]:\n            return None\n        previous_files_string = ""-"".join(\n            ""-"".join(str(k) + ""-"" + str(v) for k, v in f.items()) for f in self._data_files\n        )\n        cache_kwargs_string = ""-"".join(str(k) + ""-"" + str(v) for k, v in cache_kwargs.items())\n        function_bytes = dumps(function)\n        output_hash = hashlib.md5(\n            previous_files_string.encode(""utf-8"") + cache_kwargs_string.encode(""utf-8"") + function_bytes\n        ).hexdigest()\n        cache_file_name = ""cache-"" + output_hash + "".arrow""\n        cache_directory = os.path.dirname(self._data_files[0][""filename""])\n        cache_file_path = os.path.join(cache_directory, cache_file_name)\n        return cache_file_path\n\n    def map(\n        self,\n        function,\n        with_indices: bool = False,\n        batched: bool = False,\n        batch_size: Optional[int] = 1000,\n        remove_columns: Optional[List[str]] = None,\n        keep_in_memory: bool = False,\n        load_from_cache_file: bool = True,\n        cache_file_name: Optional[str] = None,\n        writer_batch_size: Optional[int] = 1000,\n        arrow_schema: Optional[pa.Schema] = None,\n        disable_nullable: bool = True,\n    ):\n        """""" Apply a function to all the elements in the table (individually or in batches)\n            and update the table (if function does updated examples).\n\n            Args:\n                `function` (`callable`): with one of the following signature:\n                    - `function(example: Dict) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n                    - `function(example: Dict, indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n                    - `function(batch: Dict[List]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n                    - `function(batch: Dict[List], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n                `with_indices` (`bool`, default: `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n                `batched` (`bool`, default: `False`): Provide batch of examples to `function`\n                `batch_size` (`Optional[int]`, default: `1000`): Number of examples per batch provided to `function` if `batched=True`\n                    `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n                `remove_columns` (`Optional[List[str]]`, default: `None`): Remove a selection of columns while doing the mapping.\n                    Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n                    columns with names in `remove_columns`, these columns will be kept.\n                `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.\n                `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`\n                    can be identified, use it instead of recomputing.\n                `cache_file_name` (`Optional[str]`, default: `None`): Provide the name of a cache file to use to store the\n                    results of the computation instead of the automatically generated cache file name.\n                `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.\n                    Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n                `arrow_schema` (`Optional[pa.Schema]`, default: `None`): Use a specific Apache Arrow Schema to store the cache file\n                    instead of the automatically generated one.\n                `disable_nullable` (`bool`, default: `True`): Allow null values in the table.\n        """"""\n        # If the array is empty we do nothing\n        if len(self) == 0:\n            return self\n\n        # Select the columns (arrow columns) to process\n        if remove_columns is not None and any(col not in self._data.column_names for col in remove_columns):\n            raise ValueError(\n                ""Column to remove {} not in the dataset. Current columns in the dataset: {}"".format(\n                    list(filter(lambda col: col not in self._data.column_names, remove_columns)),\n                    self._data.column_names,\n                )\n            )\n\n        # If we do batch computation but no batch sze is provided, default to the full dataset\n        if batched and (batch_size is None or batch_size <= 0):\n            batch_size = self._data.num_rows\n\n        # Check if the function returns updated examples\n        def does_function_return_dict(inputs, indices):\n            """""" Does the function returns a dict. """"""\n            processed_inputs = function(inputs, indices) if with_indices else function(inputs)\n            does_return_dict = isinstance(processed_inputs, Mapping)\n\n            if does_return_dict is False and processed_inputs is not None:\n                raise TypeError(\n                    ""Provided `function` which is applied to all elements of table returns a variable of type {}. Make sure provided `function` returns a variable of type `dict` to update the dataset or `None` if you are only interested in side effects."".format(\n                        type(processed_inputs)\n                    )\n                )\n            elif isinstance(test_indices, list) and does_return_dict is True:\n                all_dict_values_are_lists = all(isinstance(value, list) for value in processed_inputs.values())\n                if all_dict_values_are_lists is False:\n                    raise TypeError(\n                        ""Provided `function` which is applied to all elements of table returns a `dict` of types {}. When using `batched=True`, make sure provided `function` returns a `dict` of types `list`."".format(\n                            [type(x) for x in processed_inputs.values()]\n                        )\n                    )\n\n            return does_return_dict\n\n        # We only update the data table (and use the cache) if the function returns a dict.\n        # Test it on the first element or a small batch (0, 1) for batched inputs\n        test_inputs = self[:2] if batched else self[0]\n        test_indices = [0, 1] if batched else 0\n        update_data = does_function_return_dict(test_inputs, test_indices)\n\n        def apply_function_on_filtered_inputs(inputs, indices):\n            """""" Utility to apply the function on a selection of columns. """"""\n            processed_inputs = function(inputs, indices) if with_indices else function(inputs)\n            if not update_data:\n                return None  # Nothing to update, let\'s move on\n            if remove_columns is not None:\n                for column in remove_columns:\n                    inputs.pop(column)\n            if self._format_type is not None:\n                inputs = self._getitem(\n                    key=(indices if isinstance(indices, int) else slice(indices[0], indices[-1])),\n                    format_type=None,\n                    format_columns=None,\n                )\n            inputs.update(processed_inputs)\n            return inputs\n\n        # Find the output schema if none is given\n        test_inputs = self[:2] if batched else self[0]\n        test_indices = [0, 1] if batched else 0\n        test_output = apply_function_on_filtered_inputs(test_inputs, test_indices)\n        if arrow_schema is None and update_data:\n            if not batched:\n                test_output = self._nest(test_output)\n            test_output = convert_tuples_in_lists(test_output)\n            arrow_schema = pa.Table.from_pydict(test_output).schema\n            if disable_nullable:\n                arrow_schema = pa.schema(pa.field(field.name, field.type, nullable=False) for field in arrow_schema)\n\n        # Check if we\'ve already cached this computation (indexed by a hash)\n        if self._data_files and update_data:\n            if cache_file_name is None:\n                # we create a unique hash from the function, current dataset file and the mapping args\n                cache_kwargs = {\n                    ""with_indices"": with_indices,\n                    ""batched"": batched,\n                    ""batch_size"": batch_size,\n                    ""remove_columns"": remove_columns,\n                    ""keep_in_memory"": keep_in_memory,\n                    ""load_from_cache_file"": load_from_cache_file,\n                    ""cache_file_name"": cache_file_name,\n                    ""writer_batch_size"": writer_batch_size,\n                    ""arrow_schema"": arrow_schema,\n                    ""disable_nullable"": disable_nullable,\n                }\n                cache_file_name = self._get_cache_file_path(function, cache_kwargs)\n            if os.path.exists(cache_file_name) and load_from_cache_file:\n                logger.info(""Loading cached processed dataset at %s"", cache_file_name)\n                return Dataset.from_file(cache_file_name)\n\n        # Prepare output buffer and batched writer in memory or on file if we update the table\n        if update_data:\n            if keep_in_memory or not self._data_files:\n                buf_writer = pa.BufferOutputStream()\n                writer = ArrowWriter(schema=arrow_schema, stream=buf_writer, writer_batch_size=writer_batch_size)\n            else:\n                buf_writer = None\n                logger.info(""Caching processed dataset at %s"", cache_file_name)\n                writer = ArrowWriter(schema=arrow_schema, path=cache_file_name, writer_batch_size=writer_batch_size)\n\n        # Loop over single examples or batches and write to buffer/file if examples are to be updated\n        if not batched:\n            for i, example in tqdm(enumerate(self)):\n                example = apply_function_on_filtered_inputs(example, i)\n                if update_data:\n                    writer.write(example)\n        else:\n            for i in tqdm(range(0, len(self), batch_size)):\n                batch = self[i : i + batch_size]\n                indices = list(range(*(slice(i, i + batch_size).indices(self._data.num_rows))))  # Something simpler?\n                batch = apply_function_on_filtered_inputs(batch, indices)\n                if update_data:\n                    writer.write_batch(batch)\n\n        if update_data:\n            writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\n\n            # Create new Dataset from buffer or file\n            if buf_writer is None:\n                return Dataset.from_file(cache_file_name)\n            else:\n                return Dataset.from_buffer(buf_writer.getvalue())\n        else:\n            return self\n\n    def filter(self, function, with_indices=False, **kwargs):\n        """""" Apply a filter function to all the elements in the table in batches\n            and update the table so that the dataset only includes examples according to the filter function.\n\n            Args:\n                `function` (`callable`): with one of the following signature:\n                    - `function(example: Dict) -> bool` if `with_indices=False`\n                    - `function(example: Dict, indices: int) -> bool` if `with_indices=True`\n                `with_indices` (`bool`, default: `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n                `batch_size` (`Optional[int]`, default: `1000`): Number of examples per batch provided to `function` if `batched=True`\n                    `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n                `remove_columns` (`Optional[List[str]]`, default: `None`): Remove a selection of columns while doing the mapping.\n                    Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n                    columns with names in `remove_columns`, these columns will be kept.\n                `keep_in_memory` (`bool`, default: `False`): Keep the dataset in memory instead of writing it to a cache file.\n                `load_from_cache_file` (`bool`, default: `True`): If a cache file storing the current computation from `function`\n                    can be identified, use it instead of recomputing.\n                `cache_file_name` (`Optional[str]`, default: `None`): Provide the name of a cache file to use to store the\n                    results of the computation instead of the automatically generated cache file name.\n                `writer_batch_size` (`int`, default: `1000`): Number of rows per write operation for the cache file writer.\n                    Higher value gives smaller cache files, lower value consume less temporary memory while running `.map()`.\n                `disable_nullable` (`bool`, default: `True`): Allow null values in the table.\n        """"""\n\n        # transforme the filter function into the map function\n        def map_function(batch, *args):\n            result = defaultdict(list)\n            num_examples = len(batch[next(iter(batch.keys()))])\n\n            # create single examples\n            for i in range(num_examples):\n                example = map_nested(lambda x: x[i], batch, dict_only=True)\n\n                # check if example should be fildered or not\n                if with_indices:\n                    keep_example = function(example, args[0][i])\n                else:\n                    keep_example = function(example)\n\n                assert isinstance(\n                    keep_example, bool\n                ), f""The filter function returns a variable of type {type(keep_example)}, but should return a variable of type `bool`.""\n                # if example shall be kept add to result\n                if keep_example:\n                    for key in batch.keys():\n                        result[key].append(example[key])\n\n            # if no example shall be kept, init with empty list\n            if bool(result) is False:\n                for key in batch.keys():\n                    result[key] = []\n\n            return result\n\n        # to avoid errors with the arrow_schema we define it here\n        test_inputs = self[:2]\n        if ""remove_columns"" in kwargs:\n            test_inputs = {key: test_inputs[key] for key in (test_inputs.keys() - kwargs[""remove_columns""])}\n        arrow_schema = pa.Table.from_pydict(test_inputs).schema\n\n        # return map function\n        return self.map(map_function, batched=True, with_indices=with_indices, arrow_schema=arrow_schema, **kwargs)\n'"
src/nlp/arrow_reader.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n"""""" Arrow ArrowReader.""""""\n\nimport copy\nimport logging\nimport math\nimport os\nimport re\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nimport pyarrow as pa\nimport pyarrow.parquet\n\nfrom .arrow_dataset import Dataset\nfrom .naming import filename_for_dataset_split\nfrom .utils import cached_path, py_utils\n\n\nlogger = logging.getLogger(__name__)\n\n_BUFFER_SIZE = 8 << 20  # 8 MiB per file.\nHF_GCP_BASE_URL = ""https://storage.googleapis.com/huggingface-nlp/cache/datasets""\n\n_SUB_SPEC_RE = re.compile(\n    r""""""\n^\n (?P<split>\\w+)\n (\\[\n    ((?P<from>-?\\d+)\n     (?P<from_pct>%)?)?\n    :\n    ((?P<to>-?\\d+)\n     (?P<to_pct>%)?)?\n \\])?\n$\n"""""",\n    re.X,\n)\n\n_ADDITION_SEP_RE = re.compile(r""\\s*\\+\\s*"")\n\n\nclass DatasetNotOnHfGcs(ConnectionError):\n    """"""When you can\'t get the dataset from the Hf google cloud storage""""""\n\n    pass\n\n\n@dataclass(frozen=True)\nclass FileInstructions:\n    """"""The file instructions associated with a split ReadInstruction.\n\n    Attributes:\n        num_examples: `int`, The total number of examples\n        file_instructions: List[dict(filename, skip, take)], the files information.\n            The filenames contains the relative path, not absolute.\n            skip/take indicates which example read in the file: `ds.slice(skip, take)`\n    """"""\n\n    num_examples: int\n    file_instructions: List[dict]\n\n\ndef make_file_instructions(name, split_infos, instruction, filetype_suffix=None):\n    """"""Returns instructions of the split dict.\n\n    Args:\n        name: Name of the dataset.\n        split_infos: `List[SplitInfo]`, Dataset splits information\n        instruction: `ReadInstruction` or `str`\n        filetype_suffix: `Optional[str]` suffix of dataset files, e.g. \'arrow\' or \'parquet\'\n\n    Returns:\n        file_intructions: FileInstructions instance\n    """"""\n    name2len = {info.name: info.num_examples for info in split_infos}\n    if not isinstance(instruction, ReadInstruction):\n        instruction = ReadInstruction.from_spec(instruction)\n    # Create the absolute instruction (per split)\n    absolute_instructions = instruction.to_absolute(name2len)\n\n    return _make_file_instructions_from_absolutes(\n        name=name, name2len=name2len, absolute_instructions=absolute_instructions, filetype_suffix=filetype_suffix\n    )\n\n\ndef _make_file_instructions_from_absolutes(name, name2len, absolute_instructions, filetype_suffix=None):\n    """"""Returns the files instructions from the absolute instructions list.""""""\n    # For each split, return the files instruction (skip/take)\n    file_instructions = []\n    num_examples = 0\n    for abs_instr in absolute_instructions:\n        length = name2len[abs_instr.splitname]\n        filename = filename_for_dataset_split(\n            dataset_name=name, split=abs_instr.splitname, filetype_suffix=filetype_suffix\n        )\n        from_ = 0 if abs_instr.from_ is None else abs_instr.from_\n        to = length if abs_instr.to is None else abs_instr.to\n        num_examples += to - from_\n        single_file_instructions = [{""filename"": filename, ""skip"": from_, ""take"": to - from_}]\n        file_instructions.extend(single_file_instructions)\n    return FileInstructions(num_examples=num_examples, file_instructions=file_instructions,)\n\n\nclass BaseReader:\n    """"""\n    Build a Dataset object out of Instruction instance(s).\n    """"""\n\n    def __init__(self, path, info):\n        """"""Initializes ArrowReader.\n\n        Args:\n            path (str): path where tfrecords are stored.\n            info (DatasetInfo): info about the dataset.\n        """"""\n        self._path = path\n        self._info = info\n        self._filetype_suffix = None\n\n    def _get_dataset_from_filename(self, filename_skip_take):\n        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n        raise NotImplementedError\n\n    def _read_files(self, files, info) -> Dataset:\n        """"""Returns Dataset for given file instructions.\n\n        Args:\n            files: List[dict(filename, skip, take)], the files information.\n                The filenames contain the absolute path, not relative.\n                skip/take indicates which example read in the file: `ds.slice(skip, take)`\n        """"""\n        pa_batches = []\n        for f_dict in files:\n            pa_table: pa.Table = self._get_dataset_from_filename(f_dict)\n            pa_batches.extend(pa_table.to_batches())\n        if pa_batches:\n            pa_table = pa.Table.from_batches(pa_batches)\n        ds = Dataset(arrow_table=pa_table, data_files=files, info=info)\n        return ds\n\n    def get_file_instructions(self, name, instruction, split_infos):\n        """"""Return list of dict {\'filename\': str, \'skip\': int, \'take\': int}""""""\n        file_instructions = make_file_instructions(\n            name, split_infos, instruction, filetype_suffix=self._filetype_suffix\n        )\n        files = file_instructions.file_instructions\n        return files\n\n    def read(\n        self, name, instructions, split_infos,\n    ):\n        """"""Returns Dataset instance(s).\n\n        Args:\n            name (str): name of the dataset.\n            instructions (ReadInstruction, List[], Dict[]): instruction(s) to read.\n                Instructions can be string and will then be passed to the Instruction\n                constructor as it.\n            split_infos (list of SplitInfo proto): the available splits for dataset.\n\n        Returns:\n             a single Dataset instance if instruction is a single\n             ReadInstruction instance. Otherwise a dict/list of Dataset\n             corresponding to given instructions param shape.\n        """"""\n\n        def _read_instruction_to_ds(instruction):\n            files = self.get_file_instructions(name, instruction, split_infos)\n            if not files:\n                msg = \'Instruction ""%s"" corresponds to no data!\' % instruction\n                raise AssertionError(msg)\n            return self.read_files(files=tuple(files),)\n\n        return py_utils.map_nested(_read_instruction_to_ds, instructions)\n\n    def read_files(\n        self, files,\n    ):\n        """"""Returns single Dataset instance for the set of file instructions.\n\n        Args:\n            files: List[dict(filename, skip, take)], the files information.\n                The filenames contains the relative path, not absolute.\n                skip/take indicates which example read in the file: `ds.skip().take()`\n\n        Returns:\n             a Dataset instance.\n        """"""\n        # Prepend path to filename\n        files = copy.deepcopy(files)\n        for f in files:\n            f.update(filename=os.path.join(self._path, f[""filename""]))\n        dataset = self._read_files(files=files, info=self._info,)\n        return dataset\n\n    def download_from_hf_gcs(self, cache_dir, relative_data_dir):\n        """"""\n        Download the dataset files from the Hf GCS\n\n        Args:\n            cache_dir: `str`, the local cache directory where to save the dataset\n            relative_data_dir: `str`, the relative directory of the remote files from\n                the `datasets` directory on GCS.\n\n        """"""\n        remote_cache_dir = os.path.join(HF_GCP_BASE_URL, relative_data_dir)\n        try:\n            remote_dataset_info = os.path.join(remote_cache_dir, ""dataset_info.json"")\n            downloaded_dataset_info = cached_path(remote_dataset_info)\n            os.rename(downloaded_dataset_info, os.path.join(cache_dir, ""dataset_info.json""))\n        except ConnectionError:\n            raise DatasetNotOnHfGcs()\n        for split in self._info.splits:\n            file_instructions = self.get_file_instructions(\n                name=self._info.builder_name, instruction=split, split_infos=self._info.splits.values(),\n            )\n            for file_instruction in file_instructions:\n                remote_prepared_filename = os.path.join(remote_cache_dir, file_instruction[""filename""])\n                downloaded_prepared_filename = cached_path(remote_prepared_filename)\n                os.rename(downloaded_prepared_filename, os.path.join(cache_dir, file_instruction[""filename""]))\n\n\nclass ArrowReader(BaseReader):\n    """"""\n    Build a Dataset object out of Instruction instance(s).\n    This Reader uses memory mapping on arrow files.\n    """"""\n\n    def __init__(self, path, info):\n        """"""Initializes ArrowReader.\n\n        Args:\n            path (str): path where tfrecords are stored.\n            info (DatasetInfo): info about the dataset.\n        """"""\n        super().__init__(path, info)\n        self._filetype_suffix = ""arrow""\n\n    def _get_dataset_from_filename(self, filename_skip_take):\n        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n        filename, skip, take = (\n            filename_skip_take[""filename""],\n            filename_skip_take[""skip""] if ""skip"" in filename_skip_take else None,\n            filename_skip_take[""take""] if ""take"" in filename_skip_take else None,\n        )\n        mmap = pa.memory_map(filename)\n        f = pa.ipc.open_stream(mmap)\n        pa_table = f.read_all()\n        if skip is not None and take is not None:\n            pa_table = pa_table.slice(skip, take)\n        return pa_table\n\n\nclass ParquetReader(BaseReader):\n    """"""\n    Build a Dataset object out of Instruction instance(s).\n    This Reader uses memory mapping on parquet files.\n    """"""\n\n    def __init__(self, path, info):\n        """"""Initializes ParquetReader.\n\n        Args:\n            path (str): path where tfrecords are stored.\n            info (DatasetInfo): info about the dataset.\n        """"""\n        super().__init__(path, info)\n        self._filetype_suffix = ""parquet""\n\n    def _get_dataset_from_filename(self, filename_skip_take):\n        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n        filename, skip, take = (\n            filename_skip_take[""filename""],\n            filename_skip_take[""skip""] if ""skip"" in filename_skip_take else None,\n            filename_skip_take[""take""] if ""take"" in filename_skip_take else None,\n        )\n        pa_table = pa.parquet.read_table(filename, memory_map=True)\n        if skip is not None and take is not None:\n            pa_table = pa_table.slice(skip, take)\n        return pa_table\n\n\n@dataclass(frozen=True)\nclass _AbsoluteInstruction:\n    """"""A machine friendly slice: defined absolute positive boundaries.""""""\n\n    splitname: str\n    from_: int  # uint (starting index).\n    to: int  # uint (ending index).\n\n\n@dataclass(frozen=True)\nclass _RelativeInstruction:\n    """"""Represents a single parsed slicing instruction, can use % and negatives.""""""\n\n    splitname: str\n    from_: Optional[int] = None  # int (starting index) or None if no lower boundary.\n    to: Optional[int] = None  # int (ending index) or None if no upper boundary.\n    unit: Optional[str] = None\n    rounding: Optional[str] = None\n\n    def __post_init__(self):\n        assert self.unit is None or self.unit in [""%"", ""abs""]\n        assert self.rounding is None or self.rounding in [""closest"", ""pct1_dropremainder""]\n        if self.unit == ""%"" and self.from_ is not None and abs(self.from_) > 100:\n            raise AssertionError(""Percent slice boundaries must be > -100 and < 100."")\n        if self.unit == ""%"" and self.to is not None and abs(self.to) > 100:\n            raise AssertionError(""Percent slice boundaries must be > -100 and < 100."")\n\n\ndef _str_to_relative_instruction(spec):\n    """"""Returns ReadInstruction for given string.""""""\n    res = _SUB_SPEC_RE.match(spec)\n    if not res:\n        raise AssertionError(""Unrecognized instruction format: %s"" % spec)\n    unit = ""%"" if res.group(""from_pct"") or res.group(""to_pct"") else ""abs""\n    return ReadInstruction(\n        split_name=res.group(""split""),\n        rounding=""closest"",\n        from_=int(res.group(""from"")) if res.group(""from"") else None,\n        to=int(res.group(""to"")) if res.group(""to"") else None,\n        unit=unit,\n    )\n\n\ndef _pct_to_abs_pct1(boundary, num_examples):\n    # Using math.trunc here, since -99.5% should give -99%, not -100%.\n    if num_examples < 100:\n        msg = (\n            \'Using ""pct1_dropremainder"" rounding on a split with less than 100 \'\n            ""elements is forbidden: it always results in an empty dataset.""\n        )\n        raise AssertionError(msg)\n    return boundary * math.trunc(num_examples / 100.0)\n\n\ndef _pct_to_abs_closest(boundary, num_examples):\n    return int(round(boundary * num_examples / 100.0))\n\n\ndef _rel_to_abs_instr(rel_instr, name2len):\n    """"""Returns _AbsoluteInstruction instance for given RelativeInstruction.\n\n    Args:\n        rel_instr: RelativeInstruction instance.\n        name2len: dict {split_name: num_examples}.\n    """"""\n    pct_to_abs = _pct_to_abs_closest if rel_instr.rounding == ""closest"" else _pct_to_abs_pct1\n    split = rel_instr.splitname\n    if split not in name2len:\n        raise ValueError(\'Unknown split ""{}"". Should be one of {}.\'.format(split, list(name2len)))\n    num_examples = name2len[split]\n    from_ = rel_instr.from_\n    to = rel_instr.to\n    if rel_instr.unit == ""%"":\n        from_ = 0 if from_ is None else pct_to_abs(from_, num_examples)\n        to = num_examples if to is None else pct_to_abs(to, num_examples)\n    else:\n        from_ = 0 if from_ is None else from_\n        to = num_examples if to is None else to\n    if abs(from_) > num_examples or abs(to) > num_examples:\n        msg = ""Requested slice [%s:%s] incompatible with %s examples."" % (from_ or """", to or """", num_examples)\n        raise AssertionError(msg)\n    if from_ < 0:\n        from_ = num_examples + from_\n    elif from_ == 0:\n        from_ = None\n    if to < 0:\n        to = num_examples + to\n    elif to == num_examples:\n        to = None\n    return _AbsoluteInstruction(split, from_, to)\n\n\nclass ReadInstruction(object):\n    """"""Reading instruction for a dataset.\n\n    Examples of usage:\n\n    ```\n    # The following lines are equivalent:\n    ds = nlp.load_dataset(\'mnist\', split=\'test[:33%]\')\n    ds = nlp.load_dataset(\'mnist\', split=ReadInstruction.from_spec(\'test[:33%]\'))\n    ds = nlp.load_dataset(\'mnist\', split=ReadInstruction(\'test\', to=33, unit=\'%\'))\n    ds = nlp.load_dataset(\'mnist\', split=ReadInstruction(\n            \'test\', from_=0, to=33, unit=\'%\'))\n\n    # The following lines are equivalent:\n    ds = nlp.load_dataset(\'mnist\', split=\'test[:33%]+train[1:-1]\')\n    ds = nlp.load_dataset(\'mnist\', split=ReadInstruction.from_spec(\n            \'test[:33%]+train[1:-1]\'))\n    ds = nlp.load_dataset(\'mnist\', split=(\n            ReadInstruction.(\'test\', to=33, unit=\'%\') +\n            ReadInstruction.(\'train\', from_=1, to=-1, unit=\'abs\')))\n\n    # 10-fold validation:\n    tests = nlp.load_dataset(\n            \'mnist\',\n            [ReadInstruction(\'train\', from_=k, to=k+10, unit=\'%\')\n             for k in range(0, 100, 10)])\n    trains = nlp.load_dataset(\n            \'mnist\',\n            [RI(\'train\', to=k, unit=\'%\') + RI(\'train\', from_=k+10, unit=\'%\')\n             for k in range(0, 100, 10)])\n    ```\n\n    """"""\n\n    def _init(self, relative_instructions):\n        # Private initializer.\n        self._relative_instructions = relative_instructions\n\n    @classmethod\n    def _read_instruction_from_relative_instructions(cls, relative_instructions):\n        """"""Returns ReadInstruction obj initialized with relative_instructions.""""""\n        # Use __new__ to bypass __init__ used by public API and not conveniant here.\n        result = cls.__new__(cls)\n        result._init(relative_instructions)  # pylint: disable=protected-access\n        return result\n\n    def __init__(self, split_name, rounding=""closest"", from_=None, to=None, unit=None):\n        """"""Initialize ReadInstruction.\n\n        Args:\n            split_name (str): name of the split to read. Eg: \'train\'.\n            rounding (str): The rounding behaviour to use when percent slicing is\n                used. Ignored when slicing with absolute indices.\n                Possible values:\n                 - \'closest\' (default): The specified percentages are rounded to the\n                     closest value. Use this if you want specified percents to be as\n                     much exact as possible.\n                 - \'pct1_dropremainder\': the specified percentages are treated as\n                     multiple of 1%. Use this option if you want consistency. Eg:\n                         len(5%) == 5 * len(1%).\n                     Using this option, one might not be able to use the full set of\n                     examples, if the number of those is not a multiple of 100.\n            from_ (int):\n            to (int): alternative way of specifying slicing boundaries. If any of\n                {from_, to, unit} argument is used, slicing cannot be specified as\n                string.\n            unit (str): optional, one of:\n                \'%\': to set the slicing unit as percents of the split size.\n                \'abs\': to set the slicing unit as absolute numbers.\n        """"""\n        # This constructor is not always called. See factory method\n        # `_read_instruction_from_relative_instructions`. Common init instructions\n        # MUST be placed in the _init method.\n        self._init([_RelativeInstruction(split_name, from_, to, unit, rounding)])\n\n    @classmethod\n    def from_spec(cls, spec):\n        """"""Creates a ReadInstruction instance out of a string spec.\n\n        Args:\n            spec (str): split(s) + optional slice(s) to read. A slice can be\n                        specified, using absolute numbers (int) or percentages (int). E.g.\n                            `test`: test split.\n                            `test + validation`: test split + validation split.\n                            `test[10:]`: test split, minus its first 10 records.\n                            `test[:10%]`: first 10% records of test split.\n                            `test[:-5%]+train[40%:60%]`: first 95% of test + middle 20% of\n                                                                                     train.\n\n        Returns:\n            ReadInstruction instance.\n        """"""\n        spec = str(spec)  # Need to convert to str in case of NamedSplit instance.\n        subs = _ADDITION_SEP_RE.split(spec)\n        if not subs:\n            raise AssertionError(""No instructions could be built out of %s"" % spec)\n        instruction = _str_to_relative_instruction(subs[0])\n        return sum([_str_to_relative_instruction(sub) for sub in subs[1:]], instruction)\n\n    def __add__(self, other):\n        """"""Returns a new ReadInstruction obj, result of appending other to self.""""""\n        if not isinstance(other, ReadInstruction):\n            msg = ""ReadInstruction can only be added to another ReadInstruction obj.""\n            raise AssertionError(msg)\n        other_ris = other._relative_instructions  # pylint: disable=protected-access\n        if self._relative_instructions[0].rounding != other_ris[0].rounding:\n            raise AssertionError(""It is forbidden to sum ReadInstruction instances "" ""with different rounding values."")\n        return self._read_instruction_from_relative_instructions(self._relative_instructions + other_ris)\n\n    def __str__(self):\n        return ""ReadInstruction(%s)"" % self._relative_instructions\n\n    def to_absolute(self, name2len):\n        """"""Translate instruction into a list of absolute instructions.\n\n        Those absolute instructions are then to be added together.\n\n        Args:\n            name2len: dict associating split names to number of examples.\n\n        Returns:\n            list of _AbsoluteInstruction instances (corresponds to the + in spec).\n        """"""\n        return [_rel_to_abs_instr(rel_instr, name2len) for rel_instr in self._relative_instructions]\n'"
src/nlp/arrow_writer.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""To write records into Parquet files.""""""\nimport errno\nimport logging\nimport os\nimport socket\nfrom typing import Any, Dict, List, Optional\n\nimport pyarrow as pa\n\nfrom .utils.file_utils import HF_DATASETS_CACHE, hash_url_to_filename\n\n\nlogger = logging.getLogger(__name__)\n\n# Batch size constants. For more info, see:\n# https://github.com/apache/arrow/blob/master/docs/source/cpp/arrays.rst#size-limitations-and-recommendations)\nDEFAULT_MAX_BATCH_SIZE = 10_000  # hopefully it doesn\'t write too much at once (max is 2GB)\n\n\nclass ArrowWriter(object):\n    """"""Shuffles and writes Examples to Arrow files.\n    """"""\n\n    def __init__(\n        self,\n        data_type: Optional[pa.DataType] = None,\n        schema: Optional[pa.Schema] = None,\n        path: Optional[str] = None,\n        stream: Optional[pa.NativeFile] = None,\n        writer_batch_size: Optional[int] = None,\n        disable_nullable: bool = True,\n    ):\n        if path is None and stream is None:\n            raise ValueError(""At least one of path and stream must be provided."")\n\n        if data_type is not None:\n            self._type: pa.DataType = data_type\n            self._schema: pa.Schema = pa.schema(field for field in self._type)\n        elif schema is not None:\n            self._schema: pa.Schema = schema\n            self._type: pa.DataType = pa.struct(field for field in self._schema)\n        else:\n            self._schema = None\n            self._type = None\n\n        if disable_nullable and self._schema is not None:\n            self._schema = pa.schema(pa.field(field.name, field.type, nullable=False) for field in self._type)\n            self._type = pa.struct(pa.field(field.name, field.type, nullable=False) for field in self._type)\n\n        self._path = path\n        if stream is None:\n            self.stream = pa.OSFile(self._path, ""wb"")\n        else:\n            self.stream = stream\n\n        self.writer_batch_size = writer_batch_size or DEFAULT_MAX_BATCH_SIZE\n\n        self._num_examples = 0\n        self._num_bytes = 0\n        self.current_rows = []\n\n        self._build_writer(schema=self._schema)\n\n    def _build_writer(self, pa_table=None, schema=None):\n        if schema is not None:\n            self._schema: pa.Schema = schema\n            self._type: pa.DataType = pa.struct(field for field in self._schema)\n            self.pa_writer = pa.RecordBatchStreamWriter(self.stream, schema)\n        elif pa_table is not None:\n            self._schema: pa.Schema = pa_table.schema\n            self._type: pa.DataType = pa.struct(field for field in self._schema)\n            self.pa_writer = pa.RecordBatchStreamWriter(self.stream, self._schema)\n        else:\n            self.pa_writer = None\n\n    @property\n    def schema(self):\n        return self._schema if self._schema is not None else []\n\n    def _write_array_on_file(self, pa_array):\n        """"""Write a PyArrow Array""""""\n        pa_batch = pa.RecordBatch.from_struct_array(pa_array)\n        self._num_bytes += pa_array.nbytes\n        self.pa_writer.write_batch(pa_batch)\n\n    def write_on_file(self):\n        """""" Write stored examples\n        """"""\n        if self.current_rows:\n            pa_array = pa.array(self.current_rows, type=self._type)\n            first_example = pa.array(self.current_rows[0:1], type=self._type)[0]\n            # Sanity check\n            if pa_array[0] != first_example:\n                # There was an Overflow in StructArray. Let\'s reduce the batch_size\n                while pa_array[0] != first_example:\n                    new_batch_size = self.writer_batch_size // 2\n                    pa_array = pa.array(self.current_rows[:new_batch_size], type=self._type)\n                logger.warning(\n                    ""Batch size is too big (>2GB). Reducing it from {} to {}"".format(\n                        self.writer_batch_size, new_batch_size\n                    )\n                )\n                self.writer_batch_size = new_batch_size\n                n_batches = len(self.current_rows) // new_batch_size\n                n_batches += int(len(self.current_rows) % new_batch_size != 0)\n                for i in range(n_batches):\n                    pa_array = pa.array(\n                        self.current_rows[i * new_batch_size : (i + 1) * new_batch_size], type=self._type,\n                    )\n                    self._write_array_on_file(pa_array)\n            else:\n                # All good\n                self._write_array_on_file(pa_array)\n            self.current_rows = []\n\n    def write(self, example: Dict[str, Any], writer_batch_size: Optional[int] = None):\n        """""" Add a given Example to the write-pool which is written to file.\n\n        Args:\n            example: the Example to add.\n        """"""\n        self.current_rows.append(example)\n        self._num_examples += 1\n        if writer_batch_size is None:\n            writer_batch_size = self.writer_batch_size\n        if self.pa_writer is None:\n            self._build_writer(pa_table=pa.Table.from_pydict(example))\n        if writer_batch_size is not None and len(self.current_rows) >= writer_batch_size:\n            self.write_on_file()\n\n    def write_batch(\n        self, batch_examples: Dict[str, List[Any]], writer_batch_size: Optional[int] = None,\n    ):\n        """""" Write a batch of Example to file.\n\n        Args:\n            example: the Example to add.\n        """"""\n        if self.pa_writer is None:\n            self._build_writer(pa_table=pa.Table.from_pydict(batch_examples))\n        pa_table: pa.Table = pa.Table.from_pydict(batch_examples, schema=self._schema)\n        if writer_batch_size is None:\n            writer_batch_size = self.writer_batch_size\n        batches: List[pa.RecordBatch] = pa_table.to_batches(max_chunksize=writer_batch_size)\n        self._num_bytes += sum(batch.nbytes for batch in batches)\n        self._num_examples += pa_table.num_rows\n        for batch in batches:\n            self.pa_writer.write_batch(batch)\n\n    def write_table(self, pa_table: pa.Table, writer_batch_size: Optional[int] = None):\n        """""" Write a batch of Example to file.\n\n        Args:\n            example: the Example to add.\n        """"""\n        if writer_batch_size is None:\n            writer_batch_size = self.writer_batch_size\n        if self.pa_writer is None:\n            self._build_writer(pa_table=pa_table)\n        batches: List[pa.RecordBatch] = pa_table.to_batches(max_chunksize=writer_batch_size)\n        self._num_bytes += sum(batch.nbytes for batch in batches)\n        self._num_examples += pa_table.num_rows\n        for batch in batches:\n            self.pa_writer.write_batch(batch)\n\n    def finalize(self, close_stream=True):\n        if self.pa_writer is not None:\n            self.write_on_file()\n            self.pa_writer.close()\n        if close_stream:\n            self.stream.close()\n        logger.info(\n            ""Done writing %s examples in %s bytes %s."",\n            self._num_examples,\n            self._num_bytes,\n            self._path if self._path else """",\n        )\n        return self._num_examples, self._num_bytes\n\n\nclass BeamWriter(object):\n    """"""\n    Shuffles and writes Examples to Arrow files.\n    The Arrow files are converted from Parquet files that are the output of Apache Beam pipelines.\n    """"""\n\n    def __init__(\n        self,\n        data_type: Optional[pa.DataType] = None,\n        schema: Optional[pa.Schema] = None,\n        path: Optional[str] = None,\n        namespace: Optional[str] = None,\n        cache_dir: Optional[str] = None,\n    ):\n        if data_type is None and schema is None:\n            raise ValueError(""At least one of data_type and schema must be provided."")\n        if path is None:\n            raise ValueError(""Path must be provided."")\n\n        if data_type is not None:\n            self._type: pa.DataType = data_type\n            self._schema: pa.Schema = pa.schema(field for field in self._type)\n        else:\n            self._schema: pa.Schema = schema\n            self._type: pa.DataType = pa.struct(field for field in self._schema)\n\n        self._path = path\n        self._parquet_path = os.path.splitext(path)[0] + "".parquet""\n        self._namespace = namespace or ""default""\n        self._num_examples = None\n        self._cache_dir = cache_dir or HF_DATASETS_CACHE\n\n    def write_from_pcollection(self, pcoll_examples):\n        """"""Add the final steps of the beam pipeline: write to parquet files.""""""\n        import apache_beam as beam\n        from .utils.beam_utils import WriteToParquet\n\n        def inc_num_examples(example):\n            beam.metrics.Metrics.counter(self._namespace, ""num_examples"").inc()\n\n        # count examples\n        _ = pcoll_examples | ""Count N. Examples"" >> beam.Map(inc_num_examples)\n\n        # save dataset\n        return (\n            pcoll_examples\n            | ""Get values"" >> beam.Values()\n            | ""Save to parquet""\n            >> WriteToParquet(self._parquet_path, self._schema, num_shards=1, shard_name_template="""")\n        )\n\n    def finalize(self, metrics_query_result: dict):\n        """"""\n        Run after the pipeline has finished.\n        It converts the resulting parquet files to arrow and it completes the info from the pipeline metrics.\n\n        Args:\n            metrics_query_result: `dict` obtained from pipeline_results.metrics().query(m_filter). Make sure\n                that the filter keeps only the metrics for the considered split, under the namespace `split_name`.\n        """"""\n        import apache_beam as beam\n        from .utils import beam_utils\n\n        # Convert to arrow\n        logger.info(""Converting parquet file {} to arrow {}"".format(self._parquet_path, self._path))\n        try:  # stream conversion\n            with beam.io.filesystems.FileSystems.open(self._parquet_path) as src:\n                with beam.io.filesystems.FileSystems.create(self._path) as dest:\n                    parquet_to_arrow(src, dest)\n        except socket.error as e:  # broken pipe can happen if the connection is unstable, do local conversion instead\n            if e.errno != errno.EPIPE:  # not a broken pipe\n                raise e\n            logger.warning(""Broken Pipe during stream conversion from parquet to arrow. Using local convert instead"")\n            local_convert_dir = os.path.join(self._cache_dir, ""beam_convert"")\n            os.makedirs(local_convert_dir, exist_ok=True)\n            local_parquet_path = os.path.join(local_convert_dir, hash_url_to_filename(self._parquet_path) + "".parquet"")\n            local_arrow_path = os.path.splitext(local_parquet_path)[0] + "".arrow""\n            beam_utils.download_remote_to_local(self._parquet_path, local_parquet_path)\n            parquet_to_arrow(local_parquet_path, local_arrow_path)\n            beam_utils.upload_local_to_remote(local_arrow_path, self._path)\n\n        # Save metrics\n        counters_dict = {metric.key.metric.name: metric.result for metric in metrics_query_result[""counters""]}\n        self._num_examples = counters_dict[""num_examples""]\n        output_file_metadata = beam.io.filesystems.FileSystems.match([self._path], limits=[1])[0].metadata_list[0]\n        self._num_bytes = output_file_metadata.size_in_bytes\n        return self._num_examples, self._num_bytes\n\n\ndef parquet_to_arrow(source, destination):\n    """"""Convert parquet file to arrow file. Inputs can be str paths or file-like objects""""""\n    pf = pa.parquet.ParquetFile(source)\n    stream = None if isinstance(destination, str) else destination\n    writer = ArrowWriter(path=destination, stream=stream)\n    for i in range(pf.num_row_groups):\n        row_group_table = pf.read_row_group(i)\n        writer.write_table(row_group_table)\n    return destination\n'"
src/nlp/builder.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""DatasetBuilder base class.""""""\n\nimport abc\nimport contextlib\nimport inspect\nimport logging\nimport os\nimport shutil\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\n\nimport pyarrow as pa\n\nfrom . import utils\nfrom .arrow_reader import ArrowReader, DatasetNotOnHfGcs\nfrom .arrow_writer import ArrowWriter, BeamWriter\nfrom .features import Features, Value\nfrom .info import DATASET_INFO_FILENAME, DATASET_INFOS_DICT_FILE_NAME, LICENSE_FILENAME, DatasetInfo, DatasetInfosDict\nfrom .naming import camelcase_to_snakecase, filename_prefix_for_split\nfrom .splits import Split, SplitDict\nfrom .utils.download_manager import DownloadManager, GenerateMode\nfrom .utils.file_utils import HF_DATASETS_CACHE, DownloadConfig, is_remote_url\nfrom .utils.info_utils import verify_checksums, verify_splits\n\n\nlogger = logging.getLogger(__name__)\n\nFORCE_REDOWNLOAD = GenerateMode.FORCE_REDOWNLOAD\nREUSE_CACHE_IF_EXISTS = GenerateMode.REUSE_CACHE_IF_EXISTS\nREUSE_DATASET_IF_EXISTS = GenerateMode.REUSE_DATASET_IF_EXISTS\n\n\nclass InvalidConfigName(ValueError):\n    pass\n\n\n@dataclass\nclass BuilderConfig:\n    """"""Base class for `DatasetBuilder` data configuration.\n\n    DatasetBuilder subclasses with data configuration options should subclass\n    `BuilderConfig` and add their own properties.\n    """"""\n\n    name: str = ""default""\n    version: Optional[Union[str, utils.Version]] = ""0.0.0""\n    data_dir: str = None\n    data_files: Union[Dict, List] = None\n    description: str = None\n\n    def __post_init__(self):\n        # The config name is used to name the cache directory.\n        invalid_windows_characters = r""<>:/\\|?*""\n        for invalid_char in invalid_windows_characters:\n            if invalid_char in self.name:\n                raise InvalidConfigName(\n                    (\n                        ""Bad characters from black list \'{}\' found in \'{}\'. ""\n                        ""They could create issues when creating a directory ""\n                        ""for this config on Windows filesystem.""\n                    ).format(invalid_windows_characters, self.name)\n                )\n\n\nclass DatasetBuilder:\n    """"""Abstract base class for all datasets.\n\n    `DatasetBuilder` has 3 key methods:\n\n        * `nlp.DatasetBuilder.info`: documents the dataset, including feature\n            names, types, and shapes, version, splits, citation, etc.\n        * `nlp.DatasetBuilder.download_and_prepare`: downloads the source data\n            and writes it to disk.\n        * `nlp.DatasetBuilder.as_dataset`: generate an `Dataset`.\n\n    **Configuration**: Some `DatasetBuilder`s expose multiple variants of the\n    dataset by defining a `nlp.BuilderConfig` subclass and accepting a\n    config object (or name) on construction. Configurable datasets expose a\n    pre-defined set of configurations in `nlp.DatasetBuilder.builder_configs`.\n    """"""\n\n    # Default version.\n    VERSION = utils.Version(""0.0.0"")\n\n    # Class for the builder config.\n    BUILDER_CONFIG_CLASS = BuilderConfig\n\n    # Named configurations that modify the data generated by download_and_prepare.\n    BUILDER_CONFIGS = []\n\n    # Must be set for datasets that use \'manual_dir\' functionality - the ones\n    # that require users to do additional steps to download the data\n    # (this is usually due to some external regulations / rules).\n    #\n    # This field should contain a string with user instructions, including\n    # the list of files that should be present. It will be\n    # displayed in the dataset documentation.\n    MANUAL_DOWNLOAD_INSTRUCTIONS = None\n\n    def __init__(\n        self, cache_dir=None, name=None, **config_kwargs,\n    ):\n        """"""Constructs a DatasetBuilder.\n\n        Callers must pass arguments as keyword arguments.\n\n        Args:\n            cache_dir: `str`, directory to read/write data. Defaults to ""~/nlp"".\n            name: `str` name, optional configuration for the dataset that affects the data generated on disk. Different\n                `builder_config`s will have their own subdirectories and versions.\n                If not provided, uses the first configuration in self.BUILDER_CONFIGS\n            config_kwargs: will override the defaults kwargs in config\n\n        """"""\n        # DatasetBuilder name\n        self.name = camelcase_to_snakecase(self.__class__.__name__)\n\n        # Prepare config: DatasetConfig contains name, version and description but can be extended by each dataset\n        config_kwargs = dict((key, value) for key, value in config_kwargs.items() if value is not None)\n        self.config = self._create_builder_config(name, **config_kwargs,)\n\n        # prepare info: DatasetInfo are a standardized dataclass across all datasets\n        # Prefill datasetinfo\n        info = self.get_exported_dataset_info()\n        info.update(self._info())\n        info.builder_name = self.name\n        info.config_name = self.config.name\n        info.version = self.config.version\n        self.info = info\n\n        # prepare data dirs\n        self._cache_dir_root = os.path.expanduser(cache_dir or HF_DATASETS_CACHE)\n        self._cache_dir = self._build_cache_dir()\n        if os.path.exists(self._cache_dir):  # check if data exist\n            if len(os.listdir(self._cache_dir)) > 0:\n                logger.info(""Overwrite dataset info from restored data version."")\n                self.info = DatasetInfo.from_directory(self._cache_dir)\n            else:  # dir exists but no data, remove the empty dir as data aren\'t available anymore\n                logger.warning(\n                    ""Old caching folder {} for dataset {} exists but not data were found. Removing it. "".format(\n                        self._cache_dir, self.name\n                    )\n                )\n                os.rmdir(self._cache_dir)\n\n    @property\n    def does_require_manual_download(self):\n        return hasattr(self, ""MANUAL_DOWNLOAD_INSTRUCTIONS"") and (self.MANUAL_DOWNLOAD_INSTRUCTIONS is not None)\n\n    @classmethod\n    def get_all_exported_dataset_infos(cls) -> dict:\n        """"""Empty dict if doesn\'t exist""""""\n        dset_infos_file_path = os.path.join(cls.get_imported_module_dir(), DATASET_INFOS_DICT_FILE_NAME)\n        if os.path.exists(dset_infos_file_path):\n            return DatasetInfosDict.from_directory(cls.get_imported_module_dir())\n        return {}\n\n    def get_exported_dataset_info(self) -> DatasetInfo:\n        """"""Empty DatasetInfo if doesn\'t exist""""""\n        return self.get_all_exported_dataset_infos().get(self.config.name, DatasetInfo())\n\n    def _create_builder_config(self, name=None, **config_kwargs):\n        """""" Create and validate BuilderConfig object.\n            Uses the first configuration in self.BUILDER_CONFIGS if name is None\n            config_kwargs override the defaults kwargs in config\n        """"""\n        builder_config = None\n        if name is None and self.BUILDER_CONFIGS and not config_kwargs:\n            if len(self.BUILDER_CONFIGS) > 1:\n                example_of_usage = ""load_dataset(\'{}\', \'{}\')"".format(self.name, self.BUILDER_CONFIGS[0].name)\n                raise ValueError(\n                    ""Config name is missing.""\n                    ""\\nPlease pick one among the available configs: %s"" % list(self.builder_configs.keys())\n                    + ""\\nExample of usage:\\n\\t`{}`"".format(example_of_usage)\n                )\n            builder_config = self.BUILDER_CONFIGS[0]\n            logger.info(""No config specified, defaulting to first: %s/%s"", self.name, builder_config.name)\n        if isinstance(name, str):\n            builder_config = self.builder_configs.get(name)\n            if builder_config is None and self.BUILDER_CONFIGS:\n                raise ValueError(\n                    ""BuilderConfig %s not found. Available: %s"" % (name, list(self.builder_configs.keys()))\n                )\n        if not builder_config:\n            if name is not None:\n                config_kwargs[""name""] = name\n            if ""version"" not in config_kwargs and hasattr(self, ""VERSION"") and self.VERSION:\n                config_kwargs[""version""] = self.VERSION\n            builder_config = self.BUILDER_CONFIG_CLASS(**config_kwargs)\n\n        for key, value in config_kwargs.items():\n            if value is not None:\n                setattr(builder_config, key, value)\n\n        name = builder_config.name\n        if not name:\n            raise ValueError(""BuilderConfig must have a name, got %s"" % name)\n        is_custom = name not in self.builder_configs\n        if is_custom:\n            logger.warning(""Using custom data configuration %s"", name)\n        else:\n            if builder_config is not self.builder_configs[name]:\n                raise ValueError(\n                    ""Cannot name a custom BuilderConfig the same as an available ""\n                    ""BuilderConfig. Change the name. Available BuilderConfigs: %s""\n                    % (list(self.builder_configs.keys()))\n                )\n            if not builder_config.version:\n                raise ValueError(""BuilderConfig %s must have a version"" % name)\n            # if not builder_config.description:\n            #     raise ValueError(""BuilderConfig %s must have a description"" % name)\n        return builder_config\n\n    @utils.classproperty\n    @classmethod\n    @utils.memoize()\n    def builder_configs(cls):\n        """"""Pre-defined list of configurations for this builder class.""""""\n        config_dict = {config.name: config for config in cls.BUILDER_CONFIGS}\n        if len(config_dict) != len(cls.BUILDER_CONFIGS):\n            names = [config.name for config in cls.BUILDER_CONFIGS]\n            raise ValueError(""Names in BUILDER_CONFIGS must not be duplicated. Got %s"" % names)\n        return config_dict\n\n    @property\n    def cache_dir(self):\n        return self._cache_dir\n\n    def _relative_data_dir(self, with_version=True):\n        """"""Relative path of this dataset in cache_dir.""""""\n        builder_data_dir = self.name\n        builder_config = self.config\n        if builder_config:\n            builder_data_dir = os.path.join(builder_data_dir, builder_config.name)\n        if not with_version:\n            return builder_data_dir\n\n        version = self.config.version\n        version_data_dir = os.path.join(builder_data_dir, str(version))\n        return version_data_dir\n\n    def _build_cache_dir(self):\n        """"""Return the data directory for the current version.""""""\n        builder_data_dir = os.path.join(self._cache_dir_root, self._relative_data_dir(with_version=False))\n        version_data_dir = os.path.join(self._cache_dir_root, self._relative_data_dir(with_version=True))\n\n        def _other_versions_on_disk():\n            """"""Returns previous versions on disk.""""""\n            if not os.path.exists(builder_data_dir):\n                return []\n\n            version_dirnames = []\n            for dir_name in os.listdir(builder_data_dir):\n                try:\n                    version_dirnames.append((utils.Version(dir_name), dir_name))\n                except ValueError:  # Invalid version (ex: incomplete data dir)\n                    pass\n            version_dirnames.sort(reverse=True)\n            return version_dirnames\n\n        # Check and warn if other versions exist on disk\n        version_dirs = _other_versions_on_disk()\n        if version_dirs:\n            other_version = version_dirs[0][0]\n            if other_version != self.config.version:\n                warn_msg = (\n                    ""Found a different version {other_version} of dataset {name} in ""\n                    ""cache_dir {cache_dir}. Using currently defined version ""\n                    ""{cur_version}."".format(\n                        other_version=str(other_version),\n                        name=self.name,\n                        cache_dir=self._cache_dir_root,\n                        cur_version=str(self.config.version),\n                    )\n                )\n                logger.warning(warn_msg)\n\n        return version_data_dir\n\n    @abc.abstractmethod\n    def _info(self) -> DatasetInfo:\n        """"""Construct the DatasetInfo object. See `DatasetInfo` for details.\n\n        Warning: This function is only called once and the result is cached for all\n        following .info() calls.\n\n        Returns:\n            info: (DatasetInfo) The dataset information\n        """"""\n        raise NotImplementedError\n\n    @classmethod\n    def get_imported_module_dir(cls):\n        """"""Return the path of the module of this class or subclass.""""""\n        return os.path.dirname(inspect.getfile(inspect.getmodule(cls)))\n\n    def download_and_prepare(\n        self,\n        download_config: Optional[DownloadConfig] = None,\n        download_mode: Optional[GenerateMode] = None,\n        ignore_verifications: bool = False,\n        save_infos: bool = False,\n        try_from_hf_gcs: bool = True,\n        dl_manager: Optional[DownloadManager] = None,\n        **download_and_prepare_kwargs,\n    ):\n        """"""Downloads and prepares dataset for reading.\n\n        Args:\n            download_config (Optional ``nlp.DownloadConfig``: specific download configuration parameters.\n            download_mode (Optional `nlp.GenerateMode`): select the download/generate mode - Default to REUSE_DATASET_IF_EXISTS\n            ignore_verifications (bool): Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...)\n            save_infos (bool): Save the dataset information (checksums/size/splits/...)\n            try_from_hf_gcs (bool): If True, it will try to download the already prepared dataset from the Hf google cloud storage\n            dl_manager (Optional ``nlp.DownloadManager``): specific Download Manger to use\n        """"""\n        download_mode = GenerateMode(download_mode or GenerateMode.REUSE_DATASET_IF_EXISTS)\n\n        data_exists = os.path.exists(self._cache_dir)\n        if data_exists and download_mode == REUSE_DATASET_IF_EXISTS:\n            logger.info(""Reusing dataset %s (%s)"", self.name, self._cache_dir)\n            return\n\n        # Currently it\'s not possible to overwrite the data because it would\n        # conflict with versioning: If the last version has already been generated,\n        # it will always be reloaded and cache_dir will be set at construction.\n        if data_exists and download_mode != REUSE_CACHE_IF_EXISTS:\n            raise ValueError(\n                ""Trying to overwrite an existing dataset {} at {}. A dataset with ""\n                ""the same version {} already exists. If the dataset has changed, ""\n                ""please update the version number."".format(self.name, self._cache_dir, self.config.version)\n            )\n\n        logger.info(""Generating dataset %s (%s)"", self.name, self._cache_dir)\n        if not is_remote_url(self._cache_dir):  # if cache dir is local, check for available space\n            os.makedirs(self._cache_dir_root, exist_ok=True)\n            if not utils.has_sufficient_disk_space(self.info.size_in_bytes or 0, directory=self._cache_dir_root):\n                raise IOError(\n                    ""Not enough disk space. Needed: {} (download: {}, generated: {})"".format(\n                        utils.size_str(self.info.size_in_bytes or 0),\n                        utils.size_str(self.info.download_size or 0),\n                        utils.size_str(self.info.dataset_size or 0),\n                    )\n                )\n\n        @contextlib.contextmanager\n        def incomplete_dir(dirname):\n            """"""Create temporary dir for dirname and rename on exit.""""""\n            if is_remote_url(dirname):\n                yield dirname\n            else:\n                tmp_dir = dirname + "".incomplete""\n                os.makedirs(tmp_dir)\n                try:\n                    yield tmp_dir\n                    if os.path.isdir(dirname):\n                        shutil.rmtree(dirname)\n                    os.rename(tmp_dir, dirname)\n                finally:\n                    if os.path.exists(tmp_dir):\n                        shutil.rmtree(tmp_dir)\n\n        # Try to download the already prepared dataset files\n        if try_from_hf_gcs:\n            try:\n                # Create a tmp dir and rename to self._cache_dir on successful exit.\n                with incomplete_dir(self._cache_dir) as tmp_data_dir:\n                    # Temporarily assign _cache_dir to tmp_data_dir to avoid having to forward\n                    # it to every sub function.\n                    with utils.temporary_assignment(self, ""_cache_dir"", tmp_data_dir):\n                        reader = ArrowReader(self._cache_dir, self.info)\n                        reader.download_from_hf_gcs(self._cache_dir, self._relative_data_dir(with_version=True))\n                        downloaded_info = DatasetInfo.from_directory(self._cache_dir)\n                        self.info.update(downloaded_info)\n                logger.info(""Dataset downloaded from Hf google storage."")\n                print(\n                    f""Dataset {self.name} downloaded and prepared to {self._cache_dir}. ""\n                    f""Subsequent calls will reuse this data.""\n                )\n                return\n            except DatasetNotOnHfGcs:\n                logger.info(""Dataset not on Hf google storage. Downloading and preparing it from source"")\n\n        # Print is intentional: we want this to always go to stdout so user has\n        # information needed to cancel download/preparation if needed.\n        # This comes right before the progress bar.\n        print(\n            f""Downloading and preparing dataset {self.info.builder_name}/{self.info.config_name} ""\n            f""(download: {utils.size_str(self.info.download_size)}, generated: {utils.size_str(self.info.dataset_size)}, ""\n            f""total: {utils.size_str(self.info.size_in_bytes)}) to {self._cache_dir}...""\n        )\n\n        if dl_manager is None:\n            if download_config is None:\n                download_config = DownloadConfig()\n                download_config.cache_dir = os.path.join(self._cache_dir_root, ""downloads"")\n                download_config.force_download = download_mode == FORCE_REDOWNLOAD\n\n            dl_manager = DownloadManager(\n                dataset_name=self.name, download_config=download_config, data_dir=self.config.data_dir\n            )\n\n        if self.does_require_manual_download:\n            assert (\n                dl_manager.manual_dir is not None\n            ), ""The dataset {} with config {} requires manual data. \\n Please follow the manual download instructions: {}. \\n Manual data can be loaded with `nlp.load({}, data_dir=\'<path/to/manual/data>\')"".format(\n                self.name, self.config.name, self.MANUAL_DOWNLOAD_INSTRUCTIONS, self.name\n            )\n\n        # Create a tmp dir and rename to self._cache_dir on successful exit.\n        with incomplete_dir(self._cache_dir) as tmp_data_dir:\n            # Temporarily assign _cache_dir to tmp_data_dir to avoid having to forward\n            # it to every sub function.\n            with utils.temporary_assignment(self, ""_cache_dir"", tmp_data_dir):\n                verify_infos = not save_infos and not ignore_verifications\n                self._download_and_prepare(\n                    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\n                )\n                # Sync info\n                self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\n                self.info.download_checksums = dl_manager.get_recorded_sizes_checksums()\n                self.info.size_in_bytes = self.info.dataset_size + self.info.download_size\n                # Save info\n                self._save_info()\n\n        # Save to datasetinfos\n        if save_infos:\n            DatasetInfosDict(**{self.config.name: self.info}).write_to_directory(self.get_imported_module_dir())\n\n        print(\n            f""Dataset {self.name} downloaded and prepared to {self._cache_dir}. ""\n            f""Subsequent calls will reuse this data.""\n        )\n\n    def _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs):\n        """"""Downloads and prepares dataset for reading.\n\n        This is the internal implementation to overwrite called when user calls\n        `download_and_prepare`. It should download all required data and generate\n        the pre-processed datasets files.\n\n        Args:\n            dl_manager: (DownloadManager) `DownloadManager` used to download and cache\n                data.\n            verify_infos: bool, if True, do not perform checksums and size tests.\n            prepare_split_kwargs: Additional options.\n        """"""\n        # Generating data for all splits\n        split_dict = SplitDict(dataset_name=self.name)\n        split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\n        split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\n        # Checksums verification\n        if verify_infos:\n            verify_checksums(self.info.download_checksums, dl_manager.get_recorded_sizes_checksums())\n        for split_generator in split_generators:\n            if str(split_generator.split_info.name).lower() == ""all"":\n                raise ValueError(\n                    ""`all` is a special split keyword corresponding to the ""\n                    ""union of all splits, so cannot be used as key in ""\n                    ""._split_generator().""\n                )\n\n            logger.info(""Generating split %s"", split_generator.split_info.name)\n            split_dict.add(split_generator.split_info)\n\n            try:\n                # Prepare split will record examples associated to the split\n                self._prepare_split(split_generator, **prepare_split_kwargs)\n            except OSError:\n                raise OSError(""Cannot find data file. "" + (self.MANUAL_DOWNLOAD_INSTRUCTIONS or """"))\n\n        if verify_infos:\n            verify_splits(self.info.splits, split_dict)\n        # Update the info object with the splits.\n        self.info.splits = split_dict\n        self.info.download_size = dl_manager.downloaded_size\n\n    def _save_info(self):\n        self.info.write_to_directory(self._cache_dir)\n\n    def _make_split_generators_kwargs(self, prepare_split_kwargs):\n        """"""Get kwargs for `self._split_generators()` from `prepare_split_kwargs`.""""""\n        del prepare_split_kwargs\n        return {}\n\n    def as_dataset(self, split: Optional[Split] = None):\n        """""" Return a Dataset for the specified split.\n        """"""\n        logger.info(""Constructing Dataset for split %s, from %s"", split, self._cache_dir)\n        if not os.path.exists(self._cache_dir):\n            raise AssertionError(\n                (\n                    ""Dataset %s: could not find data in %s. Please make sure to call ""\n                    ""builder.download_and_prepare(), or pass download=True to ""\n                    ""nlp.load_dataset() before trying to access the Dataset object.""\n                )\n                % (self.name, self._cache_dir_root)\n            )\n\n        # By default, return all splits\n        if split is None:\n            split = {s: s for s in self.info.splits}\n\n        # Create a dataset for each of the given splits\n        datasets = utils.map_nested(self._build_single_dataset, split, map_tuple=True)\n        return datasets\n\n    def _build_single_dataset(self, split):\n        """"""as_dataset for a single split.""""""\n        if isinstance(split, str):\n            split = Split(split)\n\n        # Build base dataset\n        ds = self._as_dataset(split=split,)\n        return ds\n\n    def _as_dataset(self, split: Split = Split.TRAIN):\n        """"""Constructs a `Dataset`.\n\n        This is the internal implementation to overwrite called when user calls\n        `as_dataset`. It should read the pre-processed datasets files and generate\n        the `Dataset` object.\n\n        Args:\n            split: `nlp.Split` which subset of the data to read.\n\n        Returns:\n            `Dataset`\n        """"""\n\n        ds = ArrowReader(self._cache_dir, self.info).read(\n            name=self.name, instructions=split, split_infos=self.info.splits.values(),\n        )\n        return ds\n\n    @abc.abstractmethod\n    def _split_generators(self, dl_manager):\n        """"""Specify feature dictionary generators and dataset splits.\n\n        This function returns a list of `SplitGenerator`s defining how to generate\n        data and what splits to use.\n\n        Example:\n\n            return[\n                    nlp.SplitGenerator(\n                            name=nlp.Split.TRAIN,\n                            gen_kwargs={\'file\': \'train_data.zip\'},\n                    ),\n                    nlp.SplitGenerator(\n                            name=nlp.Split.TEST,\n                            gen_kwargs={\'file\': \'test_data.zip\'},\n                    ),\n            ]\n\n        The above code will first call `_generate_examples(file=\'train_data.zip\')`\n        to write the train data, then `_generate_examples(file=\'test_data.zip\')` to\n        write the test data.\n\n        Datasets are typically split into different subsets to be used at various\n        stages of training and evaluation.\n\n        Note that for datasets without a `VALIDATION` split, you can use a\n        fraction of the `TRAIN` data for evaluation as you iterate on your model\n        so as not to overfit to the `TEST` data.\n\n        For downloads and extractions, use the given `download_manager`.\n        Note that the `DownloadManager` caches downloads, so it is fine to have each\n        generator attempt to download the source data.\n\n        A good practice is to download all data in this function, and then\n        distribute the relevant parts to each split with the `gen_kwargs` argument\n\n        Args:\n            dl_manager: (DownloadManager) Download manager to download the data\n\n        Returns:\n            `list<SplitGenerator>`.\n        """"""\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def _prepare_split(self, split_generator, **kwargs):\n        """"""Generate the examples and record them on disk.\n\n        Args:\n            split_generator: `SplitGenerator`, Split generator to process\n            **kwargs: Additional kwargs forwarded from _download_and_prepare (ex:\n                beam pipeline)\n        """"""\n        raise NotImplementedError()\n\n\nclass GeneratorBasedBuilder(DatasetBuilder):\n    """"""Base class for datasets with data generation based on dict generators.\n\n    `GeneratorBasedBuilder` is a convenience class that abstracts away much\n    of the data writing and reading of `DatasetBuilder`. It expects subclasses to\n    implement generators of feature dictionaries across the dataset splits\n    (`_split_generators`). See the method docstrings for details.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)\n        self._writer_batch_size = kwargs.get(""writer_batch_size"")\n\n    @abc.abstractmethod\n    def _generate_examples(self, **kwargs):\n        """"""Default function generating examples for each `SplitGenerator`.\n\n        This function preprocess the examples from the raw data to the preprocessed\n        dataset files.\n        This function is called once for each `SplitGenerator` defined in\n        `_split_generators`. The examples yielded here will be written on\n        disk.\n\n        Args:\n            **kwargs: `dict`, Arguments forwarded from the SplitGenerator.gen_kwargs\n\n        Yields:\n            key: `str` or `int`, a unique deterministic example identification key.\n                * Unique: An error will be raised if two examples are yield with the\n                    same key.\n                * Deterministic: When generating the dataset twice, the same example\n                    should have the same key.\n                Good keys can be the image id, or line number if examples are extracted\n                from a text file.\n                The key will be hashed and sorted to shuffle examples deterministically,\n                such as generating the dataset multiple times keep examples in the\n                same order.\n            example: `dict<str feature_name, feature_value>`, a feature dictionary\n                ready to be encoded and written to disk. The example will be\n                encoded with `self.info.features.encode_example({...})`.\n        """"""\n        raise NotImplementedError()\n\n    def _prepare_split(self, split_generator):\n        split_info = split_generator.split_info\n\n        fname = ""{}-{}.arrow"".format(self.name, split_generator.name)\n        fpath = os.path.join(self._cache_dir, fname)\n        examples_type = self.info.features.type\n        writer = ArrowWriter(data_type=examples_type, path=fpath, writer_batch_size=self._writer_batch_size)\n\n        generator = self._generate_examples(**split_generator.gen_kwargs)\n        for key, record in utils.tqdm(generator, unit="" examples"", total=split_info.num_examples, leave=False):\n            example = self.info.features.encode_example(record)\n            writer.write(example)\n        num_examples, num_bytes = writer.finalize()\n\n        assert num_examples == num_examples, f""Expected to write {split_info.num_examples} but wrote {num_examples}""\n        split_generator.split_info.num_examples = num_examples\n        split_generator.split_info.num_bytes = num_bytes\n\n\nclass ArrowBasedBuilder(DatasetBuilder):\n    """"""Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet).\n\n    """"""\n\n    @abc.abstractmethod\n    def _generate_examples(self, **kwargs):\n        """"""Default function generating examples for each `SplitGenerator`.\n\n        This function preprocess the examples from the raw data to the preprocessed\n        dataset files.\n        This function is called once for each `SplitGenerator` defined in\n        `_split_generators`. The examples yielded here will be written on\n        disk.\n\n        Args:\n            **kwargs: `dict`, Arguments forwarded from the SplitGenerator.gen_kwargs\n\n        Yields:\n            key: `str` or `int`, a unique deterministic example identification key.\n                * Unique: An error will be raised if two examples are yield with the\n                    same key.\n                * Deterministic: When generating the dataset twice, the same example\n                    should have the same key.\n                Good keys can be the image id, or line number if examples are extracted\n                from a text file.\n                The key will be hashed and sorted to shuffle examples deterministically,\n                such as generating the dataset multiple times keep examples in the\n                same order.\n            example: `dict<str feature_name, feature_value>`, a feature dictionary\n                ready to be encoded and written to disk. The example will be\n                encoded with `self.info.features.encode_example({...})`.\n        """"""\n        raise NotImplementedError()\n\n    def _prepare_split(self, split_generator):\n        fname = ""{}-{}.arrow"".format(self.name, split_generator.name)\n        fpath = os.path.join(self._cache_dir, fname)\n\n        writer = ArrowWriter(path=fpath)\n\n        generator = self._generate_tables(**split_generator.gen_kwargs)\n        for key, table in utils.tqdm(generator, unit="" tables"", leave=False):\n            writer.write_table(table)\n        num_examples, num_bytes = writer.finalize()\n\n        split_generator.split_info.num_examples = num_examples\n        split_generator.split_info.num_bytes = num_bytes\n        features = {}\n\n        def parse_schema(schema, schema_dict):\n            for field in schema:\n                if pa.types.is_struct(field.type):\n                    schema_dict[field.name] = {}\n                    parse_schema(field.type, schema_dict[field.name])\n                elif pa.types.is_list(field.type) and pa.types.is_struct(field.type.value_type):\n                    schema_dict[field.name] = {}\n                    parse_schema(field.type.value_type, schema_dict[field.name])\n                else:\n                    schema_dict[field.name] = Value(str(field.type))\n\n        parse_schema(writer.schema, features)\n        self.info.features = Features(features)\n\n\nclass MissingBeamOptions(ValueError):\n    pass\n\n\nclass BeamBasedBuilder(DatasetBuilder):\n    """"""Beam based Builder.""""""\n\n    def __init__(self, *args, **kwargs):\n        self._beam_runner = kwargs.pop(""beam_runner"", None)\n        self._beam_options = kwargs.pop(""beam_options"", None)\n        super(BeamBasedBuilder, self).__init__(*args, **kwargs)\n        self._beam_writers = {}  # {split: beam_writer} mapping.\n\n    def _make_split_generators_kwargs(self, prepare_split_kwargs):\n        # Pass `pipeline` into `_split_generators()` from `prepare_split_kwargs` if\n        # it\'s in the call signature of `_split_generators()`.\n        # This allows for global preprocessing in beam.\n        split_generators_kwargs = {}\n        split_generators_arg_names = inspect.signature(self._split_generators).parameters.keys()\n        if ""pipeline"" in split_generators_arg_names:\n            split_generators_kwargs[""pipeline""] = prepare_split_kwargs[""pipeline""]\n        return split_generators_kwargs\n\n    @abc.abstractmethod\n    def _build_pcollection(self, pipeline, **kwargs):\n        """"""Build the beam pipeline examples for each `SplitGenerator`.\n\n        This function extracts examples from the raw data with parallel transforms\n        in a Beam pipeline. It is called once for each `SplitGenerator` defined in\n        `_split_generators`. The examples from the PCollection will be\n        encoded and written to disk.\n\n        Warning: When running in a distributed setup, make sure that the data\n        which will be read (download_dir, manual_dir,...) and written (cache_dir)\n        can be accessed by the workers jobs. The data should be located in a\n        shared filesystem, like GCS.\n\n        Example:\n\n        ```\n        def _build_pcollection(pipeline, extracted_dir):\n            return (\n                    pipeline\n                    | beam.Create(gfile.io.listdir(extracted_dir))\n                    | beam.Map(_process_file)\n            )\n        ```\n\n        Args:\n            pipeline: `beam.Pipeline`, root Beam pipeline\n            **kwargs: Arguments forwarded from the SplitGenerator.gen_kwargs\n\n        Returns:\n            pcollection: `PCollection`, an Apache Beam PCollection containing the\n                example to send to `self.info.features.encode_example(...)`.\n        """"""\n        raise NotImplementedError()\n\n    def _download_and_prepare(self, dl_manager, verify_infos):\n        # Create the Beam pipeline and forward it to _prepare_split\n        import apache_beam as beam\n        import nlp.utils.beam_utils as beam_utils\n\n        beam_runner = self._beam_runner\n        beam_options = self._beam_options\n\n        if not beam_runner and not beam_options:\n            usage_example = f""load_dataset(\'{self.name}\', \'{self.config.name}\', beam_runner=\'DirectRunner\')""\n            raise MissingBeamOptions(\n                ""Trying to generate a dataset using Apache Beam, yet no Beam Runner ""\n                ""or PipelineOptions() has been provided in `load_dataset` or in the ""\n                ""builder arguments. For big datasets it has to run on large-scale data ""\n                ""processing tools like Dataflow, Spark, etc. More information about ""\n                ""Apache Beam runners at ""\n                ""https://beam.apache.org/documentation/runners/capability-matrix/""\n                ""\\nIf you really want to run it locally because you feel like the ""\n                ""Dataset is small enough, you can use the local beam runner called ""\n                ""`DirectRunner` (you may run out of memory). \\nExample of usage: ""\n                ""\\n\\t`{}`"".format(usage_example)\n            )\n\n        beam_options = beam_options or beam.options.pipeline_options.PipelineOptions()\n        # Beam type checking assumes transforms multiple outputs are of same type,\n        # which is not our case. Plus it doesn\'t handle correctly all types, so we\n        # are better without it.\n        beam_options.view_as(beam.options.pipeline_options.TypeOptions).pipeline_type_check = False\n        # Use a single pipeline for all splits\n        pipeline = beam_utils.BeamPipeline(runner=beam_runner, options=beam_options,)\n        super(BeamBasedBuilder, self)._download_and_prepare(\n            dl_manager, verify_infos=False, pipeline=pipeline,\n        )  # TODO handle verify_infos in beam datasets\n        # Run pipeline\n        pipeline_results = pipeline.run()\n        pipeline_results.wait_until_finish()\n        metrics = pipeline_results.metrics()\n        # Update `info.splits`.\n        split_dict = self.info.splits\n        for split_name, beam_writer in self._beam_writers.items():\n            m_filter = beam.metrics.MetricsFilter().with_namespace(namespace=split_name)\n            num_examples, num_bytes = beam_writer.finalize(metrics.query(m_filter))\n            split_info = split_dict[split_name]\n            split_info.num_examples = num_examples\n            split_info.num_bytes = num_bytes\n\n    def _save_info(self):\n        import apache_beam as beam\n\n        fs = beam.io.filesystems.FileSystems\n        with fs.create(os.path.join(self._cache_dir, DATASET_INFO_FILENAME)) as f:\n            self.info._dump_info(f)\n        with fs.create(os.path.join(self._cache_dir, LICENSE_FILENAME)) as f:\n            self.info._dump_license(f)\n\n    def _prepare_split(self, split_generator, pipeline):\n        import apache_beam as beam\n\n        split_name = split_generator.split_info.name\n        output_prefix = filename_prefix_for_split(self.name, split_name)\n        output_prefix = os.path.join(self._cache_dir, output_prefix)\n\n        # To write examples to disk:\n        fname = ""{}-{}.arrow"".format(self.name, split_name)\n        fpath = os.path.join(self._cache_dir, fname)\n        examples_type = self.info.features.type\n        beam_writer = BeamWriter(examples_type, path=fpath, namespace=split_name, cache_dir=self._cache_dir)\n        self._beam_writers[split_name] = beam_writer\n\n        encode_example = self.info.features.encode_example\n\n        # Note: We need to wrap the pipeline in a PTransform to avoid re-using the\n        # same label names for each split\n        @beam.ptransform_fn\n        def _build_pcollection(pipeline):\n            """"""PTransformation which build a single split.""""""\n            # Encode the PCollection\n            pcoll_examples = self._build_pcollection(pipeline, **split_generator.gen_kwargs)\n            pcoll_examples |= ""Encode"" >> beam.Map(lambda key_ex: (key_ex[0], encode_example(key_ex[1])))\n            return beam_writer.write_from_pcollection(pcoll_examples)\n\n        # Add the PCollection to the pipeline\n        _ = pipeline | split_name >> _build_pcollection()  # pylint: disable=no-value-for-parameter\n'"
src/nlp/features.py,2,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n"""""" This class handle features definition in datasets and some utilities to display table type.""""""\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, ClassVar, Dict, List, Optional, Tuple, Union\n\nimport pyarrow as pa\n\nfrom . import utils\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef string_to_arrow(type_str: str):\n    if type_str not in pa.__dict__:\n        if str(type_str + ""_"") not in pa.__dict__:\n            raise ValueError(\n                f""Neither {type_str} nor {type_str + \'_\'} seems to be a pyarrow data type. ""\n                f""Please make sure to use a correct data type, see: ""\n                f""https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions""\n            )\n        arrow_data_type_str = str(type_str + ""_"")\n    else:\n        arrow_data_type_str = type_str\n\n    return pa.__dict__[arrow_data_type_str]()\n\n\n@dataclass\nclass Value:\n    """""" Encapsulate an Arrow datatype for easy serialization.\n    """"""\n\n    dtype: str\n    id: Optional[str] = None\n    # Automatically constructed\n    pa_type: ClassVar[Any] = None\n    _type: str = ""Value""\n\n    def __post_init__(self):\n        self.pa_type = string_to_arrow(self.dtype)\n\n    def __call__(self):\n        return self.pa_type\n\n    def encode_example(self, value):\n        if pa.types.is_boolean(self.pa_type):\n            return bool(value)\n        elif pa.types.is_integer(self.pa_type):\n            return int(value)\n        elif pa.types.is_floating(self.pa_type):\n            return float(value)\n        else:\n            return value\n\n\n@dataclass\nclass Tensor:\n    """""" Construct a 0D or 1D Tensor feature.\n        If 0D, the Tensor is an dtype element, if 1D it will be a fixed length list or dtype elements.\n        Mostly here for compatiblity with tfds.\n    """"""\n\n    shape: Union[Tuple[int], List[int]]\n    dtype: str\n    id: Optional[str] = None\n    # Automatically constructed\n    pa_type: ClassVar[Any] = None\n    _type: str = ""Tensor""\n\n    def __post_init__(self):\n        assert len(self.shape) < 2, ""Tensor can only take 0 or 1 dimensional shapes .""\n        if len(self.shape) == 1:\n            self.pa_type = pa.list_(string_to_arrow(self.dtype), self.shape[0])\n        else:\n            self.pa_type = string_to_arrow(self.dtype)\n\n    def __call__(self):\n        return self.pa_type\n\n\n@dataclass\nclass ClassLabel:\n    """""" Handle integer class labels. Here for compatiblity with tfds.\n\n        There are 3 ways to define a ClassLabel, which correspond to the 3\n        arguments:\n\n         * `num_classes`: create 0 to (num_classes-1) labels\n         * `names`: a list of label strings\n         * `names_file`: a file containing the list of labels.\n\n        Note: On python2, the strings are encoded as utf-8.\n\n        Args:\n            num_classes: `int`, number of classes. All labels must be < num_classes.\n            names: `list<str>`, string names for the integer classes. The\n                order in which the names are provided is kept.\n            names_file: `str`, path to a file with names for the integer\n                classes, one per line.\n    """"""\n\n    num_classes: int = None\n    names: List[str] = None\n    names_file: str = None\n    id: Optional[str] = None\n    # Automatically constructed\n    _str2int: ClassVar[Dict[str, int]] = None\n    _int2str: ClassVar[Dict[int, int]] = None\n    _type: str = ""ClassLabel""\n\n    def __post_init__(self):\n        # The label is explicitly set as undefined (no label defined)\n        if not sum(bool(a) for a in (self.num_classes, self.names, self.names_file)):\n            return\n\n        # if sum(bool(a) for a in (self.num_classes, self.names, self.names_file)) != 1:\n        #     raise ValueError(""Only a single argument of ClassLabel() should be provided."")\n\n        if self.num_classes is None:\n            if self.names is None:\n                self.names = self._load_names_from_file(self.names_file)\n        else:\n            if self.names is None:\n                self.names = [str(i) for i in range(self.num_classes)]\n            elif len(self.names) != self.num_classes:\n                raise ValueError(\n                    ""ClassLabel number of names do not match the defined num_classes. ""\n                    ""Got {} names VS {} num_classes"".format(len(self.names), self.num_classes)\n                )\n\n        # Prepare mappings\n        self._int2str = [str(name) for name in self.names]\n        self._str2int = {name: i for i, name in enumerate(self._int2str)}\n        if len(self._int2str) != len(self._str2int):\n            raise ValueError(""Some label names are duplicated. Each label name should be unique."")\n\n        # If num_classes has been defined, ensure that num_classes and names match\n        num_classes = len(self._str2int)\n        if self.num_classes is None:\n            self.num_classes = num_classes\n        elif self.num_classes != num_classes:\n            raise ValueError(\n                ""ClassLabel number of names do not match the defined num_classes. ""\n                ""Got {} names VS {} num_classes"".format(num_classes, self.num_classes)\n            )\n\n    def __call__(self):\n        return pa.int64()\n\n    def str2int(self, str_value):\n        """"""Conversion class name string => integer.""""""\n        str_value = str(str_value)\n\n        if self._str2int:\n            # strip key if not in dict\n            if str_value not in self._str2int:\n                str_value = str_value.strip()\n            return self._str2int[str_value]\n\n        # No names provided, try to integerize\n        failed_parse = False\n        try:\n            int_value = int(str_value)\n        except ValueError:\n            failed_parse = True\n        if failed_parse or not 0 <= int_value < self._num_classes:\n            raise ValueError(""Invalid string class label %s"" % str_value)\n        return int_value\n\n    def int2str(self, int_value):\n        """"""Conversion integer => class name string.""""""\n        if self._int2str:\n            # Maybe should support batched np array/eager tensors, to allow things\n            # like\n            # out_ids = model(inputs)\n            # labels = cifar10.info.features[\'label\'].int2str(out_ids)\n            return self._int2str[int_value]\n\n        # No names provided, return str(int)\n        if not 0 <= int_value < self._num_classes:\n            raise ValueError(""Invalid integer class label %d"" % int_value)\n        return str(int_value)\n\n    def encode_example(self, example_data):\n        if self.num_classes is None:\n            raise ValueError(\n                ""Trying to use ClassLabel feature with undefined number of class. ""\n                ""Please set ClassLabel.names or num_classes.""\n            )\n\n        # If a string is given, convert to associated integer\n        if isinstance(example_data, str):\n            example_data = self.str2int(example_data)\n\n        # Allowing -1 to mean no label.\n        if not -1 <= example_data < self.num_classes:\n            raise ValueError(\n                ""Class label %d greater than configured num_classes %d"" % (example_data, self.num_classes)\n            )\n        return example_data\n\n    @staticmethod\n    def _load_names_from_file(names_filepath):\n        with open(names_filepath, ""r"") as f:\n            return [name.strip() for name in f.read().split(""\\n"") if name.strip()]  # Filter empty names\n\n\n@dataclass\nclass Translation:\n    """"""`FeatureConnector` for translations with fixed languages per example.\n        Here for compatiblity with tfds.\n\n    Input: The Translate feature accepts a dictionary for each example mapping\n        string language codes to string translations.\n\n    Output: A dictionary mapping string language codes to translations as `Text`\n        features.\n\n    Example:\n    At construction time:\n\n    ```\n    nlp.features.Translation(languages=[\'en\', \'fr\', \'de\'])\n    ```\n\n    During data generation:\n\n    ```\n    yield {\n            \'en\': \'the cat\',\n            \'fr\': \'le chat\',\n            \'de\': \'die katze\'\n    }\n    ```\n\n    Tensor returned by `.as_dataset()`:\n\n    ```\n    {\n            \'en\': \'the cat\',\n            \'fr\': \'le chat\',\n            \'de\': \'die katze\',\n    }\n    ```\n    """"""\n\n    languages: List[str]\n    id: Optional[str] = None\n    # Automatically constructed\n    _type: str = ""Translation""\n\n    def __call__(self):\n        return pa.struct({lang: pa.string() for lang in self.languages})\n\n\n@dataclass\nclass TranslationVariableLanguages:\n    """"""`FeatureConnector` for translations with variable languages per example.\n        Here for compatiblity with tfds.\n\n    Input: The TranslationVariableLanguages feature accepts a dictionary for each\n        example mapping string language codes to one or more string translations.\n        The languages present may vary from example to example.\n\n    Output:\n        language: variable-length 1D tf.Tensor of tf.string language codes, sorted\n            in ascending order.\n        translation: variable-length 1D tf.Tensor of tf.string plain text\n            translations, sorted to align with language codes.\n\n    Example (fixed language list):\n    At construction time:\n\n    ```\n    nlp.features.Translation(languages=[\'en\', \'fr\', \'de\'])\n    ```\n\n    During data generation:\n\n    ```\n    yield {\n            \'en\': \'the cat\',\n            \'fr\': [\'le chat\', \'la chatte,\']\n            \'de\': \'die katze\'\n    }\n    ```\n\n    Tensor returned :\n\n    ```\n    {\n            \'language\': [\'en\', \'de\', \'fr\', \'fr\'],\n            \'translation\': [\'the cat\', \'die katze\', \'la chatte\', \'le chat\'],\n    }\n    ```\n    """"""\n\n    languages: List = None\n    num_languages: int = None\n    id: Optional[str] = None\n    # Automatically constructed\n    _type: str = ""TranslationVariableLanguages""\n\n    def __post_init__(self):\n        self.languages = list(sorted(list(set(self.languages)))) if self.languages else None\n        self.num_languages = len(self.languages) if self.languages else None\n\n    def __call__(self):\n        return pa.struct({""language"": pa.list_(pa.string()), ""translation"": pa.list_(pa.string())})\n\n    def encode_example(self, translation_dict):\n        lang_set = set(self.languages)\n        if self.languages and set(translation_dict) - lang_set:\n            raise ValueError(\n                ""Some languages in example ({0}) are not in valid set ({1})."".format(\n                    "", "".join(sorted(set(translation_dict) - lang_set)), "", "".join(lang_set)\n                )\n            )\n\n        # Convert dictionary into tuples, splitting out cases where there are\n        # multiple translations for a single language.\n        translation_tuples = []\n        for lang, text in translation_dict.items():\n            if isinstance(text, str):\n                translation_tuples.append((lang, text))\n            else:\n                translation_tuples.extend([(lang, el) for el in text])\n\n        # Ensure translations are in ascending order by language code.\n        languages, translations = zip(*sorted(translation_tuples))\n\n        return {""language"": languages, ""translation"": translations}\n\n\n@dataclass\nclass Sequence:\n    """""" Construct a list of feature from a single type or a dict of types.\n        Mostly here for compatiblity with tfds.\n    """"""\n\n    feature: Any\n    length: int = -1\n    id: Optional[str] = None\n    # Automatically constructed\n    _type: str = ""Sequence""\n\n\nFeatureType = Union[dict, list, tuple, Value, Tensor, ClassLabel, Translation, TranslationVariableLanguages, Sequence]\n\n\ndef get_nested_type(schema: FeatureType) -> pa.DataType:\n    """""" Convert our Feature nested object in an Apache Arrow type """"""\n    # Nested structures: we allow dict, list/tuples, sequences\n    if isinstance(schema, dict):\n        return pa.struct({key: get_nested_type(value) for key, value in schema.items()})\n    elif isinstance(schema, (list, tuple)):\n        assert len(schema) == 1, ""We defining list feature, you should just provide one example of the inner type""\n        inner_type = get_nested_type(schema[0])\n        return pa.list_(inner_type)\n    elif isinstance(schema, Sequence):\n        inner_type = get_nested_type(schema.feature)\n        # We allow to reverse list of dict => dict of list for compatiblity with tfds\n        if isinstance(inner_type, pa.StructType):\n            return pa.struct(dict((f.name, pa.list_(f.type, schema.length)) for f in inner_type))\n        return pa.list_(inner_type, schema.length)\n\n    # Other objects are callable which returns their data type (ClassLabel, Tensor, Translation, Arrow datatype creation methods)\n    return schema()\n\n\ndef encode_nested_example(schema, obj):\n    """""" Encode a nested example.\n        This is used since some features (in particular ClassLabel) have some logic during encoding.\n    """"""\n    # Nested structures: we allow dict, list/tuples, sequences\n    if isinstance(schema, dict):\n        return dict(\n            (k, encode_nested_example(sub_schema, sub_obj)) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)\n        )\n    elif isinstance(schema, (list, tuple)):\n        sub_schema = schema[0]\n        return [encode_nested_example(sub_schema, o) for o in obj]\n    elif isinstance(schema, Sequence):\n        # We allow to reverse list of dict => dict of list for compatiblity with tfds\n        if isinstance(schema.feature, dict):\n            # dict of list to fill\n            list_dict = {}\n            if isinstance(obj, (list, tuple)):\n                # obj is a list of dict\n                for k, dict_tuples in utils.zip_dict(schema.feature, *obj):\n                    list_dict[k] = [encode_nested_example(dict_tuples[0], o) for o in dict_tuples[1:]]\n                return list_dict\n            else:\n                # obj is a single dict\n                for k, (sub_schema, sub_objs) in utils.zip_dict(schema.feature, obj):\n                    list_dict[k] = [encode_nested_example(sub_schema, o) for o in sub_objs]\n                return list_dict\n        # schema.feature is not a dict\n        return [encode_nested_example(schema.feature, o) for o in obj]\n\n    # Object with special encoding:\n    # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\n    elif isinstance(schema, (ClassLabel, TranslationVariableLanguages, Value)):\n        return schema.encode_example(obj)\n\n    # Other object should be directly convertible to a native Arrow type (like Translation and Translation)\n    return obj\n\n\ndef generate_from_dict(obj: Any):\n    """""" Regenerate the nested feature object from a serialized dict.\n        We use the \'_type\' fields to get the dataclass name to load.\n    """"""\n    # Nested structures: we allow dict, list/tuples, sequences\n    if isinstance(obj, list):\n        return [generate_from_dict(value) for value in obj]\n    # Otherwise we have a dict or a dataclass\n    if ""_type"" not in obj:\n        return {key: generate_from_dict(value) for key, value in obj.items()}\n    class_type = globals()[obj.pop(""_type"")]\n\n    if class_type == Sequence:\n        return Sequence(feature=generate_from_dict(obj[""feature""]), length=obj[""length""])\n    return class_type(**obj)\n\n\ndef generate_from_arrow(pa_type: pa.DataType):\n    if isinstance(pa_type, pa.StructType):\n        return {field.name: generate_from_arrow(field.type) for field in pa_type}\n    elif isinstance(pa_type, pa.FixedSizeListType):\n        return Sequence(feature=generate_from_arrow(pa_type.value_type), length=pa_type.list_size)\n    elif isinstance(pa_type, pa.ListType):\n        return [generate_from_arrow(pa_type.value_type)]\n    elif isinstance(pa_type, pa.DictionaryType):\n        raise NotImplementedError  # TODO(thom) this will need access to the dictionary as well (for labels). I.e. to the py_table\n    elif isinstance(pa_type, pa.DataType):\n        return Value(dtype=str(pa_type))\n    else:\n        return ValueError(f""Cannot convert {pa_type} to a Feature type."")\n\n\nclass Features(dict):\n    @property\n    def type(self):\n        return get_nested_type(self)\n\n    @classmethod\n    def from_pyarrow_type(cls, pa_type: pa.DataType):\n        obj = generate_from_arrow(pa_type)\n        return cls(**obj)\n\n    @classmethod\n    def from_dict(cls, dic):\n        obj = generate_from_dict(dic)\n        return cls(**obj)\n\n    def encode_example(self, example):\n        return encode_nested_example(self, example)\n'"
src/nlp/hf_api.py,0,"b'# coding=utf-8\n# Copyright 2019-present, the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport io\nimport os\nfrom os.path import expanduser\nfrom typing import Dict, List, Optional, Tuple\n\nimport requests\nfrom tqdm import tqdm\n\n\nENDPOINT = ""https://huggingface.co""\n\n\nclass S3Obj:\n    """"""\n    Data structure that represents a file belonging to the current user.\n    """"""\n\n    def __init__(self, filename: str, LastModified: str, ETag: str, Size: int, **kwargs):\n        self.filename = filename\n        self.LastModified = LastModified\n        self.ETag = ETag\n        self.Size = Size\n\n\nclass PresignedUrl:\n    def __init__(self, write: str, access: str, type: str, **kwargs):\n        self.write = write\n        self.access = access\n        self.type = type  # mime-type to send to S3.\n\n\nclass S3Object:\n    """"""\n    Data structure that represents a public file accessible on our S3.\n    """"""\n\n    def __init__(\n        self,\n        key: str,  # S3 object key\n        etag: str,\n        lastModified: str,\n        size: int,\n        rfilename: str,  # filename relative to config.json\n        **kwargs,\n    ):\n        self.key = key\n        self.etag = etag\n        self.lastModified = lastModified\n        self.size = size\n        self.rfilename = rfilename\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n\nclass ObjectInfo:\n    """"""\n    Info about a public dataset or Metric accessible from our S3.\n    """"""\n\n    def __init__(\n        self,\n        id: str,\n        key: str,\n        lastModified: Optional[str] = None,\n        description: Optional[str] = None,\n        citation: Optional[str] = None,\n        size: Optional[int] = None,\n        etag: Optional[str] = None,\n        siblings: List[Dict] = None,\n        author: str = None,\n        **kwargs,\n    ):\n        self.id = id  # id of dataset\n        self.key = key  # S3 object key of config.json\n        self.lastModified = lastModified\n        self.description = description\n        self.citation = citation\n        self.size = size\n        self.etag = etag\n        self.siblings = siblings  # list of files that constitute the dataset\n        self.author = author\n        self.siblings = [S3Object(**x) for x in self.siblings] if self.siblings else None\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n\nclass HfApi:\n    ALLOWED_FILE_TYPES = [""datasets"", ""metrics""]\n\n    def __init__(self, endpoint=None):\n        """"""Create Api using a specific endpoint and also the file types (\'datasets\' or \'metrics\')""""""\n        self.endpoint = endpoint if endpoint is not None else ENDPOINT\n\n    def login(self, username: str, password: str) -> str:\n        """"""\n        Call HF API to sign in a user and get a token if credentials are valid.\n\n        Outputs:\n            token if credentials are valid\n\n        Throws:\n            requests.exceptions.HTTPError if credentials are invalid\n        """"""\n        path = ""{}/api/login"".format(self.endpoint)\n        r = requests.post(path, json={""username"": username, ""password"": password})\n        r.raise_for_status()\n        d = r.json()\n        return d[""token""]\n\n    def whoami(self, token: str) -> Tuple[str, List[str]]:\n        """"""\n        Call HF API to know ""whoami""\n        """"""\n        path = ""{}/api/whoami"".format(self.endpoint)\n        r = requests.get(path, headers={""authorization"": ""Bearer {}"".format(token)})\n        r.raise_for_status()\n        d = r.json()\n        return d[""user""], d[""orgs""]\n\n    def logout(self, token: str) -> None:\n        """"""\n        Call HF API to log out.\n        """"""\n        path = ""{}/api/logout"".format(self.endpoint)\n        r = requests.post(path, headers={""authorization"": ""Bearer {}"".format(token)})\n        r.raise_for_status()\n\n    def presign(\n        self, token: str, filename: str, organization: Optional[str] = None, file_types: Optional[str] = None\n    ) -> PresignedUrl:\n        """"""\n        Call HF API to get a presigned url to upload `filename` to S3.\n        """"""\n        assert file_types in self.ALLOWED_FILE_TYPES, ""Please specify file types from {}"".format(\n            self.ALLOWED_FILE_TYPES\n        )\n        path = ""{}/api/{}/presign"".format(self.endpoint, file_types)\n        r = requests.post(\n            path,\n            headers={""authorization"": ""Bearer {}"".format(token)},\n            json={""filename"": filename, ""organization"": organization},\n        )\n        r.raise_for_status()\n        d = r.json()\n        return PresignedUrl(**d)\n\n    def presign_and_upload(\n        self,\n        token: str,\n        filename: str,\n        filepath: str,\n        organization: Optional[str] = None,\n        file_types: Optional[str] = None,\n    ) -> str:\n        """"""\n        Get a presigned url, then upload file to S3.\n\n        Outputs:\n            url: Read-only url for the stored file on S3.\n        """"""\n        urls = self.presign(token, filename=filename, organization=organization, file_types=file_types)\n        # streaming upload:\n        # https://2.python-requests.org/en/master/user/advanced/#streaming-uploads\n        #\n        # Even though we presign with the correct content-type,\n        # the client still has to specify it when uploading the file.\n        with open(filepath, ""rb"") as f:\n            pf = TqdmProgressFileReader(f)\n            data = f if pf.total_size > 0 else """"\n\n            r = requests.put(urls.write, data=data, headers={""content-type"": urls.type})\n            r.raise_for_status()\n            pf.close()\n        return urls.access\n\n    def list_objs(\n        self, token: str, organization: Optional[str] = None, file_types: Optional[str] = None\n    ) -> List[S3Obj]:\n        """"""\n        Call HF API to list all stored files for user (or one of their organizations).\n        """"""\n        assert file_types in self.ALLOWED_FILE_TYPES, ""Please specify file types from {}"".format(\n            self.ALLOWED_FILE_TYPES\n        )\n        path = ""{}/api/{}/listObjs"".format(self.endpoint, file_types)\n        params = {""organization"": organization} if organization is not None else None\n        r = requests.get(path, params=params, headers={""authorization"": ""Bearer {}"".format(token)})\n        r.raise_for_status()\n        d = r.json()\n        return [S3Obj(**x) for x in d]\n\n    def delete_obj(\n        self, token: str, filename: str, organization: Optional[str] = None, file_types: Optional[str] = None\n    ):\n        """"""\n        Call HF API to delete a file stored by user\n        """"""\n        assert file_types in self.ALLOWED_FILE_TYPES, ""Please specify file types from {}"".format(\n            self.ALLOWED_FILE_TYPES\n        )\n        path = ""{}/api/{}/deleteObj"".format(self.endpoint, file_types)\n        r = requests.delete(\n            path,\n            headers={""authorization"": ""Bearer {}"".format(token)},\n            json={""filename"": filename, ""organization"": organization},\n        )\n        r.raise_for_status()\n\n    def dataset_list(self) -> List[ObjectInfo]:\n        """"""\n        Get the public list of all the datasets on huggingface, including the community datasets\n        """"""\n        path = ""{}/api/datasets"".format(self.endpoint)\n        r = requests.get(path)\n        r.raise_for_status()\n        d = r.json()\n        return [ObjectInfo(**x) for x in d]\n\n    def metric_list(self) -> List[ObjectInfo]:\n        """"""\n        Get the public list of all the metrics on huggingface, including the community metrics\n        """"""\n        path = ""{}/api/metrics"".format(self.endpoint)\n        r = requests.get(path)\n        r.raise_for_status()\n        d = r.json()\n        return [ObjectInfo(**x) for x in d]\n\n\nclass TqdmProgressFileReader:\n    """"""\n    Wrap an io.BufferedReader `f` (such as the output of `open(\xe2\x80\xa6, ""rb"")`)\n    and override `f.read()` so as to display a tqdm progress bar.\n\n    see github.com/huggingface/transformers/pull/2078#discussion_r354739608\n    for implementation details.\n    """"""\n\n    def __init__(self, f: io.BufferedReader):\n        self.f = f\n        self.total_size = os.fstat(f.fileno()).st_size\n        self.pbar = tqdm(total=self.total_size, leave=False)\n        self.read = f.read\n        f.read = self._read\n\n    def _read(self, n=-1):\n        self.pbar.update(n)\n        return self.read(n)\n\n    def close(self):\n        self.pbar.close()\n\n\nclass HfFolder:\n    path_token = expanduser(""~/.huggingface/token"")\n\n    @classmethod\n    def save_token(cls, token):\n        """"""\n        Save token, creating folder as needed.\n        """"""\n        os.makedirs(os.path.dirname(cls.path_token), exist_ok=True)\n        with open(cls.path_token, ""w+"") as f:\n            f.write(token)\n\n    @classmethod\n    def get_token(cls):\n        """"""\n        Get token or None if not existent.\n        """"""\n        try:\n            with open(cls.path_token, ""r"") as f:\n                return f.read()\n        except FileNotFoundError:\n            pass\n\n    @classmethod\n    def delete_token(cls):\n        """"""\n        Delete token.\n        Do not fail if token does not exist.\n        """"""\n        try:\n            os.remove(cls.path_token)\n        except FileNotFoundError:\n            pass\n'"
src/nlp/info.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n"""""" DatasetInfo and MetricInfo record information we know about a dataset and a metric.\n\nThis includes things that we know about the dataset statically, i.e.:\n - description\n - canonical location\n - does it have validation and tests splits\n - size\n - etc.\n\nThis also includes the things that can and should be computed once we\'ve\nprocessed the dataset as well:\n - number of examples (in each split)\n - etc.\n""""""\n\nimport json\nimport logging\nimport os\nfrom dataclasses import asdict, dataclass, field\nfrom typing import List, Optional, Union\n\nfrom nlp.utils.version import Version\n\nfrom .features import Features, Value\nfrom .splits import SplitDict\n\n\nlogger = logging.getLogger(__name__)\n\n# Name of the file to output the DatasetInfo p rotobuf object.\nDATASET_INFO_FILENAME = ""dataset_info.json""\nDATASET_INFOS_DICT_FILE_NAME = ""dataset_infos.json""\nLICENSE_FILENAME = ""LICENSE""\nMETRIC_INFO_FILENAME = ""metric_info.json""\n\n\n@dataclass\nclass SupervisedKeysData:\n    input: str = """"\n    output: str = """"\n\n\n@dataclass\nclass DownloadChecksumsEntryData:\n    key: str = """"\n    value: str = """"\n\n\nclass MissingCachedSizesConfigError(Exception):\n    """"""The expected cached sizes of the download file are missing.""""""\n\n\nclass NonMatchingCachedSizesError(Exception):\n    """"""The prepared split doesn\'t have expected sizes.""""""\n\n\n@dataclass\nclass DatasetInfo:\n    """"""Information about a dataset.\n\n    `DatasetInfo` documents datasets, including its name, version, and features.\n    See the constructor arguments and properties for a full list.\n\n    Note: Not all fields are known on construction and may be updated later.\n    """"""\n\n    # Set in the dataset scripts\n    description: str = field(default_factory=str)\n    citation: str = field(default_factory=str)\n    homepage: str = field(default_factory=str)\n    license: str = field(default_factory=str)\n    features: Features = None\n    supervised_keys: Optional[SupervisedKeysData] = None\n\n    # Set later by the builder\n    builder_name: Optional[str] = None\n    config_name: Optional[str] = None\n    version: Optional[Union[str, Version]] = None\n    # Set later by `download_and_prepare`\n    splits: Optional[dict] = None\n    download_checksums: Optional[dict] = None\n    download_size: Optional[int] = None\n    dataset_size: Optional[int] = None\n    size_in_bytes: Optional[int] = None\n\n    def __post_init__(self):\n        # Convert back to the correct classes when we reload from dict\n        if self.features is not None and not isinstance(self.features, Features):\n            self.features = Features.from_dict(self.features)\n        if self.version is not None and not isinstance(self.version, Version):\n            if isinstance(self.version, str):\n                self.version = Version(self.version)\n            else:\n                self.version = Version.from_dict(self.version)\n        if self.splits is not None and not isinstance(self.splits, SplitDict):\n            self.splits = SplitDict.from_split_dict(self.splits)\n        if self.supervised_keys is not None and not isinstance(self.supervised_keys, SupervisedKeysData):\n            if isinstance(self.supervised_keys, (tuple, list)):\n                self.supervised_keys = SupervisedKeysData(*self.supervised_keys)\n            else:\n                self.supervised_keys = SupervisedKeysData(**self.supervised_keys)\n\n    def _license_path(self, dataset_info_dir):\n        return os.path.join(dataset_info_dir, LICENSE_FILENAME)\n\n    def write_to_directory(self, dataset_info_dir):\n        """""" Write `DatasetInfo` as JSON to `dataset_info_dir`.\n            Also save the license separately in LICENCE.\n        """"""\n        with open(os.path.join(dataset_info_dir, DATASET_INFO_FILENAME), ""wb"") as f:\n            self._dump_info(f)\n\n        with open(os.path.join(dataset_info_dir, LICENSE_FILENAME), ""wb"") as f:\n            self._dump_license(f)\n\n    def _dump_info(self, file):\n        """"""Dump info in `file` file-like object open in bytes mode (to support remote files)""""""\n        file.write(json.dumps(asdict(self)).encode(""utf-8""))\n\n    def _dump_license(self, file):\n        """"""Dump license in `file` file-like object open in bytes mode (to support remote files)""""""\n        file.write(self.license.encode(""utf-8""))\n\n    @classmethod\n    def from_directory(cls, dataset_info_dir):\n        """"""Create DatasetInfo from the JSON file in `dataset_info_dir`.\n\n        This function updates all the dynamically generated fields (num_examples,\n        hash, time of creation,...) of the DatasetInfo.\n\n        This will overwrite all previous metadata.\n\n        Args:\n            dataset_info_dir: `str` The directory containing the metadata file. This\n                should be the root directory of a specific dataset version.\n        """"""\n        logger.info(""Loading Dataset info from %s"", dataset_info_dir)\n        if not dataset_info_dir:\n            raise ValueError(""Calling DatasetInfo.from_directory() with undefined dataset_info_dir."")\n\n        with open(os.path.join(dataset_info_dir, DATASET_INFO_FILENAME), ""r"") as f:\n            dataset_info_dict = json.load(f)\n        return cls(**dataset_info_dict)\n\n    def update(self, other_dataset_info, ignore_none=True):\n        self_dict = self.__dict__\n        self_dict.update(\n            **{k: v for k, v in other_dataset_info.__dict__.items() if (v is not None or not ignore_none)}\n        )\n\n\nclass DatasetInfosDict(dict):\n    def write_to_directory(self, dataset_infos_dir, overwrite=False):\n        total_dataset_infos = {}\n        dataset_infos_path = os.path.join(dataset_infos_dir, DATASET_INFOS_DICT_FILE_NAME)\n        if os.path.exists(dataset_infos_path) and not overwrite:\n            logger.info(""Dataset Infos already exists in {}. Completing it with new infos."".format(dataset_infos_dir))\n            total_dataset_infos = self.from_directory(dataset_infos_dir)\n        else:\n            logger.info(""Writing new Dataset Infos in {}"".format(dataset_infos_dir))\n        total_dataset_infos.update(self)\n        with open(dataset_infos_path, ""w"") as f:\n            json.dump({config_name: asdict(dset_info) for config_name, dset_info in total_dataset_infos.items()}, f)\n\n    @classmethod\n    def from_directory(cls, dataset_infos_dir):\n        logger.info(""Loading Dataset Infos from {}"".format(dataset_infos_dir))\n        with open(os.path.join(dataset_infos_dir, DATASET_INFOS_DICT_FILE_NAME), ""r"") as f:\n            dataset_info_dict = {\n                config_name: DatasetInfo(**dataset_info_dict)\n                for config_name, dataset_info_dict in json.load(f).items()\n            }\n        return cls(**dataset_info_dict)\n\n\n@dataclass\nclass MetricInfo:\n    """"""Information about a metric.\n\n    `MetricInfo` documents a metric, including its name, version, and features.\n    See the constructor arguments and properties for a full list.\n\n    Note: Not all fields are known on construction and may be updated later.\n    """"""\n\n    # Set in the dataset scripts\n    description: str\n    citation: str\n    features: Features\n    inputs_description: str = field(default_factory=str)\n    homepage: str = field(default_factory=str)\n    licence: str = field(default_factory=str)\n    codebase_urls: List[str] = field(default_factory=list)\n    reference_urls: List[str] = field(default_factory=list)\n    streamable: bool = False\n    format: Optional[str] = None\n\n    # Set later by the builder\n    metric_name: Optional[str] = None\n    config_name: Optional[str] = None\n    version: Optional[str] = None\n\n    def __post_init__(self):\n        assert ""predictions"" in self.features, ""Need to have at least a \'predictions\' field in \'features\'.""\n        if self.format is not None:\n            for key, value in self.features.items():\n                if not isinstance(value, Value):\n                    raise ValueError(\n                        f""When using \'numpy\' format, all features should be a `nlp.Value` feature. ""\n                        f""Here {key} is an instance of {value.__class__.__name__}""\n                    )\n\n    def write_to_directory(self, metric_info_dir):\n        """""" Write `MetricInfo` as JSON to `metric_info_dir`.\n            Also save the license separately in LICENCE.\n        """"""\n        with open(os.path.join(metric_info_dir, DATASET_INFO_FILENAME), ""w"") as f:\n            json.dump(asdict(self), f)\n\n        with open(os.path.join(metric_info_dir, LICENSE_FILENAME), ""w"") as f:\n            f.write(self.license)\n\n    @classmethod\n    def from_directory(cls, metric_info_dir):\n        """"""Create MetricInfo from the JSON file in `metric_info_dir`.\n\n        Args:\n            metric_info_dir: `str` The directory containing the metadata file. This\n                should be the root directory of a specific dataset version.\n        """"""\n        logger.info(""Loading Metric info from %s"", metric_info_dir)\n        if not metric_info_dir:\n            raise ValueError(""Calling MetricInfo.from_directory() with undefined metric_info_dir."")\n\n        with open(os.path.join(metric_info_dir, METRIC_INFO_FILENAME), ""r"") as f:\n            dataset_info_dict = json.load(f)\n        return cls(**dataset_info_dict)\n'"
src/nlp/inspect.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n"""""" List and inspect datasets and metrics.""""""\n\nimport logging\nfrom typing import Optional\n\nfrom .hf_api import HfApi\nfrom .load import prepare_module\nfrom .utils import DownloadConfig\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef list_datasets():\n    """""" List all the datasets scripts available on HuggingFace AWS bucket """"""\n    api = HfApi()\n    return api.dataset_list()\n\n\ndef list_metrics():\n    """""" List all the metrics script available on HuggingFace AWS bucket """"""\n    api = HfApi()\n    return api.metric_list()\n\n\ndef inspect_dataset(path: str, local_path: str, download_config: Optional[DownloadConfig] = None, **download_kwargs):\n    r""""""\n        Allow inspection/modification of a dataset script by copying on local drive at local_path.\n\n        Args:\n            path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n                    e.g. ``\'./dataset/squad\'`` or ``\'./dataset/squad/squad.py\'``\n                - a datatset identifier on HuggingFace AWS bucket (list all available datasets and ids with ``nlp.list_datasets()``)\n                    e.g. ``\'squad\'``, ``\'glue\'`` or ``\'openai/webtext\'``\n            local_path (``str``): path to the local folder to copy the datset script to.\n            download_config (Optional ``nlp.DownloadConfig``: specific download configuration parameters.\n            **download_kwargs: optional attributes for DownloadConfig() which will override the attributes in download_config if supplied.\n    """"""\n    module_path = prepare_module(\n        path, download_config=download_config, dataset=True, force_local_path=local_path, **download_kwargs\n    )\n    print(\n        f""The processing script for dataset {path} can be inspected at {local_path}. ""\n        f""The main class is in {module_path}. ""\n        f""You can modify this processing script and use it with `nlp.load_dataset({local_path})`.""\n    )\n\n\ndef inspect_metric(path: str, local_path: str, download_config: Optional[DownloadConfig] = None, **download_kwargs):\n    r""""""\n        Allow inspection/modification of a metric script by copying it on local drive at local_path.\n\n        Args:\n            path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n                    e.g. ``\'./dataset/squad\'`` or ``\'./dataset/squad/squad.py\'``\n                - a datatset identifier on HuggingFace AWS bucket (list all available datasets and ids with ``nlp.list_datasets()``)\n                    e.g. ``\'squad\'``, ``\'glue\'`` or ``\'openai/webtext\'``\n            local_path (``str``): path to the local folder to copy the datset script to.\n            download_config (Optional ``nlp.DownloadConfig``: specific download configuration parameters.\n            **download_kwargs: optional attributes for DownloadConfig() which will override the attributes in download_config if supplied.\n    """"""\n    module_path = prepare_module(\n        path, download_config=download_config, dataset=False, force_local_path=local_path, **download_kwargs\n    )\n    print(\n        f""The processing scripts for metric {path} can be inspected at {local_path}. ""\n        f""The main class is in {module_path}. ""\n        f""You can modify this processing scripts and use it with `nlp.load_metric({local_path})`.""\n    )\n'"
src/nlp/load.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Access datasets.""""""\n\nimport importlib\nimport inspect\nimport json\nimport logging\nimport os\nimport re\nimport shutil\nfrom hashlib import sha256\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union\nfrom urllib.parse import urlparse\n\nfrom filelock import FileLock\n\nfrom .arrow_dataset import Dataset\nfrom .builder import DatasetBuilder\nfrom .info import DATASET_INFOS_DICT_FILE_NAME\nfrom .metric import Metric\nfrom .splits import Split\nfrom .utils.download_manager import GenerateMode\nfrom .utils.file_utils import DownloadConfig, cached_path, hf_bucket_url\n\n\nlogger = logging.getLogger(__name__)\n\nCURRENT_FILE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\nDATASETS_PATH = os.path.join(CURRENT_FILE_DIRECTORY, ""datasets"")\nDATASETS_MODULE = ""nlp.datasets""\nMETRICS_PATH = os.path.join(CURRENT_FILE_DIRECTORY, ""metrics"")\nMETRICS_MODULE = ""nlp.metrics""\n\n\ndef import_main_class(module_path, dataset=True):\n    """""" Import a module at module_path and return its main class:\n        - a DatasetBuilder if dataset is True\n        - a Metric if dataset is False\n    """"""\n    importlib.invalidate_caches()\n    module = importlib.import_module(module_path)\n\n    if dataset:\n        main_cls_type = DatasetBuilder\n    else:\n        main_cls_type = Metric\n\n    # Find the main class in our imported module\n    module_main_cls = None\n    for name, obj in module.__dict__.items():\n        if isinstance(obj, type) and issubclass(obj, main_cls_type):\n            if inspect.isabstract(obj):\n                continue\n            module_main_cls = obj\n            break\n\n    return module_main_cls\n\n\ndef files_to_hash(file_paths: List[str]):\n    """"""\n    Convert a list of scripts or text files provided in file_paths into a hashed filename in a repeatable way.\n    """"""\n    # List all python files in directories if directories are supplied as part of external imports\n    to_use_files = []\n    for file_path in file_paths:\n        if os.path.isdir(file_path):\n            to_use_files.extend(list(Path(file_path).rglob(""*.[pP][yY]"")))\n        else:\n            to_use_files.append(file_path)\n\n    # Get the code from all these files\n    lines = []\n    for file_path in to_use_files:\n        with open(file_path, mode=""r"") as f:\n            lines.extend(f.readlines())\n    filtered_lines = []\n    for line in lines:\n        line.replace(""\\n"", """")  # remove line breaks, white space and comments\n        line.replace("" "", """")\n        line.replace(""\\t"", """")\n        line = re.sub(r""#.*"", """", line)\n        if line:\n            filtered_lines.append(line)\n    file_str = ""\\n"".join(filtered_lines)\n\n    # Make a hash from all this code\n    file_bytes = file_str.encode(""utf-8"")\n    file_hash = sha256(file_bytes)\n    filename = file_hash.hexdigest()\n\n    return filename\n\n\ndef convert_github_url(url_path: str):\n    """""" Convert a link to a file on a github repo in a link to the raw github object.\n    """"""\n    parsed = urlparse(url_path)\n    sub_directory = None\n    if parsed.scheme in (""http"", ""https"", ""s3"") and parsed.netloc == ""github.com"":\n        if ""blob"" in url_path:\n            assert url_path.endswith(\n                "".py""\n            ), f""External import from github at {url_path} should point to a file ending with \'.py\'""\n            url_path = url_path.replace(""blob"", ""raw"")  # Point to the raw file\n        else:\n            # Parse github url to point to zip\n            github_path = parsed.path[1:]\n            repo_info, branch = github_path.split(""/tree/"") if ""/tree/"" in github_path else (github_path, ""master"")\n            repo_owner, repo_name = repo_info.split(""/"")\n            url_path = ""https://github.com/{}/{}/archive/{}.zip"".format(repo_owner, repo_name, branch)\n            sub_directory = f""{repo_name}-{branch}""\n    return url_path, sub_directory\n\n\ndef get_imports(file_path: str):\n    r""""""\n        Find whether we should import or clone additional files for a given processing script.\n        And list the import.\n\n        We allow:\n        - library dependencies,\n        - local dependencies and\n        - external dependencies whose url is specified with a comment starting from ""# From:\' followed by the raw url to a file, an archive or a github repository.\n            external dependencies will be downloaded (and extracted if needed in the dataset folder).\n            We also add an `__init__.py` to each sub-folder of a downloaded folder so the user can import from them in the script.\n\n        Note that only direct import in the dataset processing script will be handled\n        We don\'t recursively explore the additional import to download further files.\n\n        ```python\n        import tensorflow\n        import .c4_utils\n        import .clicr.dataset-code.build_json_dataset  # From: https://raw.githubusercontent.com/clips/clicr/master/dataset-code/build_json_dataset\n        ```\n    """"""\n    lines = []\n    with open(file_path, mode=""r"") as f:\n        lines.extend(f.readlines())\n\n    logger.info(""Checking %s for additional imports."", file_path)\n    imports = []\n    for line in lines:\n        match = re.match(r""^import\\s+(\\.?)([^\\s\\.]+)[^#\\r\\n]*(?:#\\s+From:\\s+)?([^\\r\\n]*)"", line, flags=re.MULTILINE)\n        if match is None:\n            match = re.match(\n                r""^from\\s+(\\.?)([^\\s\\.]+)(?:[^\\s]*)\\s+import\\s+[^#\\r\\n]*(?:#\\s+From:\\s+)?([^\\r\\n]*)"",\n                line,\n                flags=re.MULTILINE,\n            )\n            if match is None:\n                continue\n        if match.group(1):\n            # The import starts with a \'.\', we will download the relevant file\n            if any(imp[1] == match.group(2) for imp in imports):\n                # We already have this import\n                continue\n            if match.group(3):\n                # The import has a comment with \'From:\', we\'ll retreive it from the given url\n                url_path = match.group(3)\n                url_path, sub_directory = convert_github_url(url_path)\n                imports.append((""external"", match.group(2), url_path, sub_directory))\n            elif match.group(2):\n                # The import should be at the same place as the file\n                imports.append((""internal"", match.group(2), match.group(2), None))\n        else:\n            imports.append((""library"", match.group(2), match.group(2), None))\n\n    return imports\n\n\ndef prepare_module(\n    path: str,\n    download_config: Optional[DownloadConfig] = None,\n    dataset: bool = True,\n    force_local_path: Optional[str] = None,\n    **download_kwargs,\n) -> DatasetBuilder:\n    r""""""\n        Download/extract/cache a dataset (if dataset==True) or a metric (if dataset==False)\n\n        Dataset and metrics codes are cached inside the lib to allow easy import (avoid ugly sys.path tweaks)\n        and using cloudpickle (among other things).\n\n        Args:\n\n            path (str): path to the dataset or metric script, can be either:\n                - a path to a local directory containing the dataset processing python script\n                - an url to a S3 directory with a dataset processing python script\n            download_config (Optional ``nlp.DownloadConfig``: specific download configuration parameters.\n            dataset (bool): True if the script to load is a dataset, False if the script is a metric.\n            force_local_path (Optional str): Optional path to a local path to download and prepare the script to.\n                Used to inspect or modify the script folder.\n            **download_kwargs: optional attributes for DownloadConfig() which will override the attributes in download_config if supplied.\n\n        Return: ``str`` with\n\n            - the import path of the dataset/metric package if force_local_path is False: e.g. \'nlp.datasets.squad\'\n            - the local path to the dataset/metric file if force_local_path is True: e.g. \'/User/huggingface/nlp/datasets/squad/squad.py\'\n    """"""\n    if download_config is None:\n        download_config = DownloadConfig(**download_kwargs)\n    download_config.extract_compressed_file = True\n    download_config.force_extract = True\n\n    module_type = ""dataset"" if dataset else ""metric""\n    name = list(filter(lambda x: x, path.split(""/"")))[-1]\n    if not name.endswith("".py""):\n        name = name + "".py""\n\n    # Short name is name without the \'.py\' at the end (for the module)\n    short_name = name[:-3]\n\n    # We have three ways to find the processing file:\n    # - if os.path.join(path, name) is a file or a remote url\n    # - if path is a file or a remote url\n    # - otherwise we assume path/name is a path to our S3 bucket\n    combined_path = os.path.join(path, name)\n    if os.path.isfile(combined_path):\n        file_path = combined_path\n    elif os.path.isfile(path):\n        file_path = path\n    else:\n        file_path = hf_bucket_url(path, filename=name, dataset=dataset)\n\n    base_path = os.path.dirname(file_path)  # remove the filename\n    dataset_infos = os.path.join(base_path, DATASET_INFOS_DICT_FILE_NAME)\n\n    # Load the module in two steps:\n    # 1. get the processing file on the local filesystem if it\'s not there (download to cache dir)\n    # 2. copy from the local file system inside the library to import it\n    local_path = cached_path(file_path, download_config=download_config)\n\n    # Download the dataset infos file if available\n    try:\n        local_dataset_infos_path = cached_path(dataset_infos, download_config=download_config,)\n    except (FileNotFoundError, ConnectionError):\n        local_dataset_infos_path = None\n\n    # Download external imports if needed\n    imports = get_imports(local_path)\n    local_imports = []\n    library_imports = []\n    for import_type, import_name, import_path, sub_directory in imports:\n        if import_type == ""library"":\n            library_imports.append(import_name)  # Import from a library\n            continue\n\n        if import_name == short_name:\n            raise ValueError(\n                f""Error in {module_type} script at {file_path}, importing relative {import_name} module ""\n                f""but {import_name} is the name of the {module_type} script. ""\n                f""Please change relative import {import_name} to another name and add a \'# From: URL_OR_PATH\' ""\n                f""comment pointing to the original realtive import file path.""\n            )\n        if import_type == ""internal"":\n            url_or_filename = base_path + ""/"" + import_path + "".py""\n        elif import_type == ""external"":\n            url_or_filename = import_path\n        else:\n            raise ValueError(""Wrong import_type"")\n\n        local_import_path = cached_path(url_or_filename, download_config=download_config,)\n        if sub_directory is not None:\n            local_import_path = os.path.join(local_import_path, sub_directory)\n        local_imports.append((import_name, local_import_path))\n\n    # Check library imports\n    needs_to_be_installed = []\n    for library_import in library_imports:\n        try:\n            lib = importlib.import_module(library_import)  # noqa F841\n        except ImportError:\n            needs_to_be_installed.append(library_import)\n    if needs_to_be_installed:\n        raise ImportError(\n            f""To be able to use this {module_type}, you need to install the following dependencies {needs_to_be_installed} ""\n            f""using \'pip install {\' \'.join(needs_to_be_installed)}\' for instance\'""\n        )\n\n    # Define a directory with a unique name in our dataset or metric folder\n    # path is: ./datasets|metrics/dataset|metric_name/hash_from_code/script.py\n    # we use a hash to be able to have multiple versions of a dataset/metric processing file together\n    hash = files_to_hash([local_path] + [loc[1] for loc in local_imports])\n\n    if force_local_path is None:\n        main_folder_path = os.path.join(DATASETS_PATH if dataset else METRICS_PATH, short_name)\n        hash_folder_path = os.path.join(main_folder_path, hash)\n    else:\n        main_folder_path = force_local_path\n        hash_folder_path = force_local_path\n\n    local_file_path = os.path.join(hash_folder_path, name)\n    dataset_infos_path = os.path.join(hash_folder_path, DATASET_INFOS_DICT_FILE_NAME)\n\n    # Prevent parallel disk operations\n    lock_path = local_path + "".lock""\n    with FileLock(lock_path):\n        # Create main dataset/metrics folder if needed\n        if not os.path.exists(main_folder_path):\n            logger.info(f""Creating main folder for {module_type} {file_path} at {main_folder_path}"")\n            os.makedirs(main_folder_path, exist_ok=True)\n        else:\n            logger.info(f""Found main folder for {module_type} {file_path} at {main_folder_path}"")\n\n        # add an __init__ file to the main dataset folder if needed\n        init_file_path = os.path.join(main_folder_path, ""__init__.py"")\n        if not os.path.exists(init_file_path):\n            with open(init_file_path, ""w""):\n                pass\n\n        # Create hash dataset folder if needed\n        if not os.path.exists(hash_folder_path):\n            logger.info(f""Creating specific version folder for {module_type} {file_path} at {hash_folder_path}"")\n            os.makedirs(hash_folder_path)\n        else:\n            logger.info(f""Found specific version folder for {module_type} {file_path} at {hash_folder_path}"")\n\n        # add an __init__ file to the hash dataset folder if needed\n        init_file_path = os.path.join(hash_folder_path, ""__init__.py"")\n        if not os.path.exists(init_file_path):\n            with open(init_file_path, ""w""):\n                pass\n\n        # Copy dataset.py file in hash folder if needed\n        if not os.path.exists(local_file_path):\n            logger.info(""Copying script file from %s to %s"", file_path, local_file_path)\n            shutil.copyfile(local_path, local_file_path)\n        else:\n            logger.info(""Found script file from %s to %s"", file_path, local_file_path)\n\n        # Copy dataset infos file if needed\n        if not os.path.exists(dataset_infos_path):\n            if local_dataset_infos_path is not None:\n                logger.info(""Copying dataset infos file from %s to %s"", dataset_infos, dataset_infos_path)\n                shutil.copyfile(local_dataset_infos_path, dataset_infos_path)\n            else:\n                logger.info(""Couldn\'t find dataset infos file at %s"", dataset_infos)\n        else:\n            logger.info(""Found dataset infos file from %s to %s"", dataset_infos, dataset_infos_path)\n\n        # Record metadata associating original dataset path with local unique folder\n        meta_path = local_file_path.split("".py"")[0] + "".json""\n        if not os.path.exists(meta_path):\n            logger.info(f""Creating metadata file for {module_type} {file_path} at {meta_path}"")\n            meta = {""original file path"": file_path, ""local file path"": local_file_path}\n            # the filename is *.py in our case, so better rename to filenam.json instead of filename.py.json\n            with open(meta_path, ""w"") as meta_file:\n                json.dump(meta, meta_file)\n        else:\n            logger.info(f""Found metadata file for {module_type} {file_path} at {meta_path}"")\n\n        # Copy all the additional imports\n        for import_name, import_path in local_imports:\n            if os.path.isfile(import_path):\n                full_path_local_import = os.path.join(hash_folder_path, import_name + "".py"")\n                if not os.path.exists(full_path_local_import):\n                    logger.info(""Copying local import file from %s at %s"", import_path, full_path_local_import)\n                    shutil.copyfile(import_path, full_path_local_import)\n                else:\n                    logger.info(""Found local import file from %s at %s"", import_path, full_path_local_import)\n            elif os.path.isdir(import_path):\n                full_path_local_import = os.path.join(hash_folder_path, import_name)\n                if not os.path.exists(full_path_local_import):\n                    logger.info(""Copying local import directory from %s at %s"", import_path, full_path_local_import)\n                    shutil.copytree(import_path, full_path_local_import)\n                else:\n                    logger.info(""Found local import directory from %s at %s"", import_path, full_path_local_import)\n            else:\n                raise OSError(f""Error with local import at {import_path}"")\n\n    if force_local_path is None:\n        module_path = ""."".join([DATASETS_MODULE if dataset else METRICS_MODULE, short_name, hash, short_name])\n    else:\n        module_path = local_file_path\n\n    return module_path\n\n\ndef load_metric(\n    path: str,\n    name: Optional[str] = None,\n    process_id: int = 0,\n    num_process: int = 1,\n    data_dir: Optional[str] = None,\n    experiment_id: Optional[str] = None,\n    in_memory: bool = False,\n    download_config: Optional[DownloadConfig] = None,\n    **metric_init_kwargs,\n) -> Metric:\n    r""""""\n        Load a `nlp.Metric`.\n\n        Args:\n\n            path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n                    e.g. ``\'./dataset/squad\'`` or ``\'./dataset/squad/squad.py\'``\n                - a datatset identifier on HuggingFace AWS bucket (list all available datasets and ids with ``nlp.list_datasets()``)\n                    e.g. ``\'squad\'``, ``\'glue\'`` or ``\'openai/webtext\'``\n            name (Optional ``str``): defining the name of the dataset configuration\n            process_id (Optional ``int``): for distributed evaluation: id of the process\n            num_process (Optional ``int``): for distributed evaluation: total number of processes\n            data_dir (Optional str): path to store the temporary predictions and references (default to `~/.nlp/`)\n            experiment_id (Optional str): An optional unique id for the experiment.\n            in_memory (bool): Weither to store the temporary results in memory (default: False)\n            download_config (Optional ``nlp.DownloadConfig``: specific download configuration parameters.\n\n        Returns: `nlp.Metric`.\n    """"""\n    module_path = prepare_module(path, download_config=download_config, dataset=False)\n    metric_cls = import_main_class(module_path, dataset=False)\n    metric = metric_cls(\n        name=name,\n        process_id=process_id,\n        num_process=num_process,\n        data_dir=data_dir,\n        experiment_id=experiment_id,\n        in_memory=in_memory,\n        **metric_init_kwargs,\n    )\n    return metric\n\n\ndef load_dataset(\n    path: str,\n    name: Optional[str] = None,\n    version: Optional[str] = None,\n    data_dir: Optional[str] = None,\n    data_files: Union[Dict, List] = None,\n    split: Optional[Union[str, Split]] = None,\n    cache_dir: Optional[str] = None,\n    download_config: Optional[DownloadConfig] = None,\n    download_mode: Optional[GenerateMode] = None,\n    ignore_verifications: bool = False,\n    save_infos: bool = False,\n    **config_kwargs,\n) -> Dataset:\n    r""""""Load a dataset\n\n        This method does the following under the hood:\n\n            1. Download and import in the library the dataset loading script from ``path`` if it\'s not already cached inside the library.\n\n                Processing scripts are small python scripts that define the citation, info and format of the dataset,\n                contain the URL to the original data files and the code to load examples from the original data files.\n\n                You can find some of the scripts here: https://github.com/huggingface/nlp/datasets\n                and easily upload yours to share them using the CLI ``nlp-cli``.\n\n            2. Run the dataset loading script which will:\n\n                * Download the dataset file from the original URL (see the script) if it\'s not already downloaded and cached.\n                * Process and cache the dataset in typed Arrow tables for caching.\n\n                    Arrow table are arbitrarly long, typed tables which can store nested objects and be mapped to numpy/pandas/python standard types.\n                    They can be directly access from drive, loaded in RAM or even streamed over the web.\n\n            3. Return a dataset build from the requested splits in ``split`` (default: all).\n\n        Args:\n\n            path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n                    e.g. ``\'./dataset/squad\'`` or ``\'./dataset/squad/squad.py\'``\n                - a datatset identifier on HuggingFace AWS bucket (list all available datasets and ids with ``nlp.list_datasets()``)\n                    e.g. ``\'squad\'``, ``\'glue\'`` or ``\'openai/webtext\'``\n            name (Optional ``str``): defining the name of the dataset configuration\n            version (Optional ``str``): defining the version of the dataset configuration\n            data_files (Optional ``str``): defining the data_files of the dataset configuration\n            data_dir (Optional ``str``): defining the data_dir of the dataset configuration\n            split (`nlp.Split` or `str`): which split of the data to load.\n                If None, will return a `dict` with all splits (typically `nlp.Split.TRAIN` and `nlp.Split.TEST`).\n                If given, will return a single Dataset.\n                Splits can be combined and specified like in tensorflow-datasets.\n            cache_dir (Optional ``str``): directory to read/write data. Defaults to ""~/nlp"".\n            download_config (Optional ``nlp.DownloadConfig``: specific download configuration parameters.\n            download_mode (Optional `nlp.GenerateMode`): select the download/generate mode - Default to REUSE_DATASET_IF_EXISTS\n            ignore_verifications (bool): Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...)\n            save_infos (bool): Save the dataset information (checksums/size/splits/...)\n            **config_kwargs (Optional ``dict``): keyword arguments to be passed to the ``nlp.BuilderConfig`` and used in the ``nlp.DatasetBuilder``.\n\n        Returns: ``nlp.Dataset`` or ``Dict[nlp.Split, nlp.Dataset]``\n\n            if `split` is not None: the dataset requested,\n            if `split` is None, a `dict<key: nlp.Split, value: nlp.Dataset>` with each split.\n    """"""\n    # Download/copy dataset processing script\n    module_path = prepare_module(path, download_config=download_config, dataset=True)\n\n    # Get dataset builder class from the processing script\n    builder_cls = import_main_class(module_path, dataset=True)\n\n    # Instantiate the dataset builder\n    builder_instance = builder_cls(\n        cache_dir=cache_dir, name=name, version=version, data_dir=data_dir, data_files=data_files, **config_kwargs,\n    )\n\n    # Download and prepare data\n    builder_instance.download_and_prepare(\n        download_config=download_config,\n        download_mode=download_mode,\n        ignore_verifications=ignore_verifications,\n        save_infos=save_infos,\n    )\n\n    # Build dataset for splits\n    ds = builder_instance.as_dataset(split=split)\n\n    return ds\n'"
src/nlp/metric.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n"""""" Metrics base class.""""""\nimport logging\nimport os\nfrom typing import Any, Dict, Optional\n\nimport pyarrow as pa\nfrom filelock import FileLock, Timeout\n\nfrom .arrow_reader import ArrowReader\nfrom .arrow_writer import ArrowWriter\nfrom .info import MetricInfo\nfrom .naming import camelcase_to_snakecase\nfrom .utils import HF_METRICS_CACHE, Version\n\n\nlogger = logging.getLogger(__file__)\n\n\nclass Metric(object):\n    def __init__(\n        self,\n        name: str = None,\n        process_id: int = 0,\n        num_process: int = 1,\n        data_dir: Optional[str] = None,\n        experiment_id: Optional[str] = None,\n        in_memory: bool = False,\n        **kwargs,\n    ):\n        """""" A Metrics is the base class and common API for all metrics.\n            Args:\n                process_id (int): specify the id of the node in a distributed settings between 0 and num_nodes-1\n                    This can be used, to compute metrics on distributed setups\n                    (in particular non-additive metrics like F1).\n                data_dir (str): path to a directory in which temporary data will be stored.\n                    This should be a shared file-system for distributed setups.\n                experiment_id (str): Should be used if you perform several concurrent experiments using\n                    the same caching directory (will be indicated in the raise error)\n                in_memory (bool): keep all predictions and references in memory. Not possible in distributed settings.\n        """"""\n        # Safety checks\n        assert isinstance(process_id, int) and process_id >= 0, ""\'process_id\' should be a number greater than 0""\n        assert (\n            isinstance(num_process, int) and num_process > process_id\n        ), ""\'num_process\' should be a number greater than process_id""\n        assert (\n            process_id == 0 or not in_memory\n        ), ""Using \'in_memory\' is not possible in distributed setting (process_id > 0).""\n\n        # Metric name\n        self.name = camelcase_to_snakecase(self.__class__.__name__)\n        # Configuration name\n        self.config_name = name\n\n        self.process_id = process_id\n        self.num_process = num_process\n        self.in_memory = in_memory\n        self.experiment_id = experiment_id if experiment_id is not None else ""cache""\n        self._version = ""1.0.0""\n        self._data_dir_root = os.path.expanduser(data_dir or HF_METRICS_CACHE)\n        self.data_dir = self._build_data_dir()\n\n        # prepare info\n        info = self._info()\n        info.metric_name = self.name\n        info.config_name = self.config_name\n        info.version = self._version\n        self.info = info\n\n        # Update \'compute\' and \'add\' docstring\n        self.compute.__func__.__doc__ += self.info.inputs_description\n        self.add_batch.__func__.__doc__ += self.info.inputs_description\n        self.add.__func__.__doc__ += self.info.inputs_description\n\n        self.arrow_schema = pa.schema(field for field in self.info.features.type)\n        self.buf_writer = None\n        self.writer = None\n        self.writer_batch_size = None\n        self.data = None\n\n        # Check we can write on the cache file without competitors\n        self.cache_file_name = self._get_cache_path(self.process_id)\n        self.filelock = FileLock(self.cache_file_name + "".lock"")\n        try:\n            self.filelock.acquire(timeout=1)\n        except Timeout:\n            raise ValueError(\n                ""Cannot acquire lock, caching file might be used by another process, ""\n                ""you should setup a unique \'experiment_id\' for this run.""\n            )\n\n    def _relative_data_dir(self, with_version=True):\n        """"""Relative path of this dataset in data_dir.""""""\n        builder_data_dir = self.name\n        if not with_version:\n            return builder_data_dir\n\n        version = self._version\n        version_data_dir = os.path.join(builder_data_dir, str(version))\n        return version_data_dir\n\n    def _build_data_dir(self):\n        """""" Return the directory for the current version.\n        """"""\n        builder_data_dir = os.path.join(self._data_dir_root, self._relative_data_dir(with_version=False))\n        version_data_dir = os.path.join(self._data_dir_root, self._relative_data_dir(with_version=True))\n\n        def _other_versions_on_disk():\n            """"""Returns previous versions on disk.""""""\n            if not os.path.exists(builder_data_dir):\n                return []\n\n            version_dirnames = []\n            for dir_name in os.listdir(builder_data_dir):\n                try:\n                    version_dirnames.append((Version(dir_name), dir_name))\n                except ValueError:  # Invalid version (ex: incomplete data dir)\n                    pass\n            version_dirnames.sort(reverse=True)\n            return version_dirnames\n\n        # Check and warn if other versions exist on disk\n        version_dirs = _other_versions_on_disk()\n        if version_dirs:\n            other_version = version_dirs[0][0]\n            if other_version != self._version:\n                warn_msg = (\n                    ""Found a different version {other_version} of metric {name} in ""\n                    ""data_dir {data_dir}. Using currently defined version ""\n                    ""{cur_version}."".format(\n                        other_version=str(other_version),\n                        name=self.name,\n                        data_dir=self._data_dir_root,\n                        cur_version=str(self._version),\n                    )\n                )\n                logger.warning(warn_msg)\n\n        os.makedirs(version_data_dir, exist_ok=True)\n        return version_data_dir\n\n    def _get_cache_path(self, node_id):\n        return os.path.join(self.data_dir, f""{self.experiment_id}-{self.name}-{node_id}.arrow"")\n\n    def finalize(self, timeout=120):\n        """""" Close all the writing process and load/gather the data\n            from all the nodes if main node or all_process is True.\n        """"""\n        self.writer.finalize()\n        self.writer = None\n        self.buf_writer = None\n        self.filelock.release()\n\n        if self.process_id == 0:\n            # Let\'s acquire a lock on each node files to be sure they are finished writing\n            node_files = []\n            locks = []\n            for node_id in range(self.num_process):\n                node_file = self._get_cache_path(node_id)\n                filelock = FileLock(node_file + "".lock"")\n                filelock.acquire(timeout=timeout)\n                node_files.append({""filename"": node_file})\n                locks.append(filelock)\n\n            # Read the predictions and references\n            reader = ArrowReader(path=self.data_dir, info=None)\n            self.data = reader.read_files(node_files)\n\n            # Release all of our locks\n            for lock in locks:\n                lock.release()\n\n    def compute(self, predictions=None, references=None, timeout=120, **metrics_kwargs):\n        """""" Compute the metrics.\n        """"""\n        if predictions is not None:\n            self.add_batch(predictions=predictions, references=references)\n        self.finalize(timeout=timeout)\n\n        self.data.set_format(type=self.info.format)\n\n        predictions = self.data[""predictions""]\n        references = self.data[""references""]\n        output = self._compute(predictions=predictions, references=references, **metrics_kwargs)\n        return output\n\n    def add_batch(self, predictions=None, references=None, **kwargs):\n        """""" Add a batch of predictions and references for the metric\'s stack.\n        """"""\n        batch = {""predictions"": predictions, ""references"": references}\n        if self.writer is None:\n            self._init_writer()\n        self.writer.write_batch(batch)\n\n    def add(self, prediction=None, reference=None, **kwargs):\n        """""" Add one prediction and reference for the metric\'s stack.\n        """"""\n        example = {""predictions"": prediction, ""references"": reference}\n        if self.writer is None:\n            self._init_writer()\n        self.writer.write(example)\n\n    def _init_writer(self):\n        if self.in_memory:\n            self.buf_writer = pa.BufferOutputStream()\n            self.writer = ArrowWriter(\n                schema=self.arrow_schema, stream=self.buf_writer, writer_batch_size=self.writer_batch_size\n            )\n        else:\n            self.buf_writer = None\n            self.writer = ArrowWriter(\n                schema=self.arrow_schema, path=self.cache_file_name, writer_batch_size=self.writer_batch_size\n            )\n\n    def _info(self) -> MetricInfo:\n        """"""Construct the MetricInfo object. See `MetricInfo` for details.\n\n        Warning: This function is only called once and the result is cached for all\n        following .info() calls.\n\n        Returns:\n            info: (MetricInfo) The metrics information\n        """"""\n        raise NotImplementedError\n\n    def _compute(self, predictions=None, references=None, **kwargs) -> Dict[str, Any]:\n        """""" This method defines the common API for all the metrics in the library """"""\n        raise NotImplementedError\n'"
src/nlp/naming.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Utilities for file names.""""""\n\nimport os\nimport re\n\n\n_first_cap_re = re.compile(""(.)([A-Z][a-z0-9]+)"")\n_all_cap_re = re.compile(""([a-z0-9])([A-Z])"")\n\n\ndef camelcase_to_snakecase(name):\n    """"""Convert camel-case string to snake-case.""""""\n    s1 = _first_cap_re.sub(r""\\1_\\2"", name)\n    return _all_cap_re.sub(r""\\1_\\2"", s1).lower()\n\n\ndef snake_to_camelcase(name):\n    """"""Convert snake-case string to camel-case string.""""""\n    return """".join(n.capitalize() for n in name.split(""_""))\n\n\ndef filename_prefix_for_name(name):\n    if os.path.basename(name) != name:\n        raise ValueError(""Should be a dataset name, not a path: %s"" % name)\n    return camelcase_to_snakecase(name)\n\n\ndef filename_prefix_for_split(name, split):\n    if os.path.basename(name) != name:\n        raise ValueError(""Should be a dataset name, not a path: %s"" % name)\n    return ""%s-%s"" % (filename_prefix_for_name(name), split)\n\n\ndef filepattern_for_dataset_split(dataset_name, split, data_dir, filetype_suffix=None):\n    prefix = filename_prefix_for_split(dataset_name, split)\n    if filetype_suffix:\n        prefix += "".%s"" % filetype_suffix\n    filepath = os.path.join(data_dir, prefix)\n    return ""%s*"" % filepath\n\n\ndef filename_for_dataset_split(dataset_name, split, filetype_suffix=None):\n    prefix = filename_prefix_for_split(dataset_name, split)\n    if filetype_suffix:\n        prefix += "".%s"" % filetype_suffix\n    return prefix\n\n\ndef filepath_for_dataset_split(dataset_name, split, data_dir, filetype_suffix=None):\n    filename = filename_for_dataset_split(dataset_name=dataset_name, split=split, filetype_suffix=filetype_suffix,)\n    filepath = os.path.join(data_dir, filename)\n    return filepath\n'"
src/nlp/splits.py,1,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Splits related API.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport abc\nimport collections\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Union\n\nfrom .arrow_reader import FileInstructions, make_file_instructions\nfrom .utils.py_utils import NonMutableDict\n\n\n@dataclass\nclass SplitInfo:\n    name: str = """"\n    num_bytes: int = 0\n    num_examples: int = 0\n    dataset_name: str = None\n\n    @property\n    def file_instructions(self):\n        """"""Returns the list of dict(filename, take, skip).""""""\n        # `self.dataset_name` is assigned in `SplitDict.add()`.\n        instructions = make_file_instructions(name=self.dataset_name, split_infos=[self], instruction=str(self.name),)\n        return instructions.file_instructions\n\n\n@dataclass\nclass SubSplitInfo:\n    """"""Wrapper around a sub split info.\n    This class expose info on the subsplit:\n    ```\n    ds, info = nlp.load_dataset(..., split=\'train[75%:]\', with_info=True)\n    info.splits[\'train[75%:]\'].num_examples\n    ```\n    """"""\n\n    instructions: FileInstructions\n\n    @property\n    def num_examples(self):\n        """"""Returns the number of example in the subsplit.""""""\n        return self.instructions.num_examples\n\n    @property\n    def file_instructions(self):\n        """"""Returns the list of dict(filename, take, skip).""""""\n        return self.instructions.file_instructions\n\n\nclass SplitBase(metaclass=abc.ABCMeta):\n    # pylint: disable=line-too-long\n    """"""Abstract base class for Split compositionality.\n\n    See the\n    [guide on splits](https://github.com/huggingface/nlp/tree/master/docs/splits.md)\n    for more information.\n\n    There are three parts to the composition:\n        1) The splits are composed (defined, merged, split,...) together before\n             calling the `.as_dataset()` function. This is done with the `__add__`,\n             `__getitem__`, which return a tree of `SplitBase` (whose leaf\n             are the `NamedSplit` objects)\n\n        ```\n        split = nlp.Split.TRAIN + nlp.Split.TEST.subsplit(nlp.percent[:50])\n        ```\n\n        2) The `SplitBase` is forwarded to the `.as_dataset()` function\n             to be resolved into actual read instruction. This is done by the\n             `.get_read_instruction()` method which takes the real dataset splits\n             (name, number of shards,...) and parse the tree to return a\n             `SplitReadInstruction()` object\n\n        ```\n        read_instruction = split.get_read_instruction(self.info.splits)\n        ```\n\n        3) The `SplitReadInstruction` is then used in the `tf.data.Dataset` pipeline\n             to define which files to read and how to skip examples within file.\n\n    """"""\n    # pylint: enable=line-too-long\n\n    @abc.abstractmethod\n    def get_read_instruction(self, split_dict):\n        """"""Parse the descriptor tree and compile all read instructions together.\n\n        Args:\n            split_dict: `dict`, The `dict[split_name, SplitInfo]` of the dataset\n\n        Returns:\n            split_read_instruction: `SplitReadInstruction`\n        """"""\n        raise NotImplementedError(""Abstract method"")\n\n    def __eq__(self, other):\n        """"""Equality: nlp.Split.TRAIN == \'train\'.""""""\n        if isinstance(other, (NamedSplit, str)):\n            return False\n        raise NotImplementedError(""Equality is not implemented between merged/sub splits."")\n\n    def __ne__(self, other):\n        """"""InEquality: nlp.Split.TRAIN != \'test\'.""""""\n        return not self.__eq__(other)\n\n    def __add__(self, other):\n        """"""Merging: nlp.Split.TRAIN + nlp.Split.TEST.""""""\n        return _SplitMerged(self, other)\n\n    def subsplit(self, arg=None, k=None, percent=None, weighted=None):  # pylint: disable=redefined-outer-name\n        """"""Divides this split into subsplits.\n\n        There are 3 ways to define subsplits, which correspond to the 3\n        arguments `k` (get `k` even subsplits), `percent` (get a slice of the\n        dataset with `nlp.percent`), and `weighted` (get subsplits with proportions\n        specified by `weighted`).\n\n        Examples:\n\n        ```\n        # 50% train, 50% test\n        train, test = split.subsplit(k=2)\n        # 50% train, 25% test, 25% validation\n        train, test, validation = split.subsplit(weighted=[2, 1, 1])\n        # Extract last 20%\n        subsplit = split.subsplit(nlp.percent[-20:])\n        ```\n\n        Warning: k and weighted will be converted into percent which mean that\n        values below the percent will be rounded up or down. The final split may be\n        bigger to deal with remainders. For instance:\n\n        ```\n        train, test, valid = split.subsplit(k=3)  # 33%, 33%, 34%\n        s1, s2, s3, s4 = split.subsplit(weighted=[2, 2, 1, 1])  # 33%, 33%, 16%, 18%\n        ```\n\n        Args:\n            arg: If no kwargs are given, `arg` will be interpreted as one of\n                `k`, `percent`, or `weighted` depending on the type.\n                For example:\n                ```\n                split.subsplit(10)  # Equivalent to split.subsplit(k=10)\n                split.subsplit(nlp.percent[:-20])  # percent=nlp.percent[:-20]\n                split.subsplit([1, 1, 2])  # weighted=[1, 1, 2]\n                ```\n            k: `int` If set, subdivide the split into `k` equal parts.\n            percent: `nlp.percent slice`, return a single subsplit corresponding to\n                a slice of the original split. For example:\n                `split.subsplit(nlp.percent[-20:])  # Last 20% of the dataset`.\n            weighted: `list[int]`, return a list of subsplits whose proportions match\n                the normalized sum of the list. For example:\n                `split.subsplit(weighted=[1, 1, 2])  # 25%, 25%, 50%`.\n\n        Returns:\n            A subsplit or list of subsplits extracted from this split object.\n        """"""\n        # Note that the percent kwargs redefine the outer name nlp.percent. This\n        # is done for consistency (.subsplit(percent=nlp.percent[:40]))\n        if sum(bool(x) for x in (arg, k, percent, weighted)) != 1:\n            raise ValueError(""Only one argument of subsplit should be set."")\n\n        # Auto deduce k\n        if isinstance(arg, int):\n            k = arg\n        elif isinstance(arg, slice):\n            percent = arg\n        elif isinstance(arg, list):\n            weighted = arg\n\n        if not (k or percent or weighted):\n            raise ValueError(\n                ""Invalid split argument {}. Only list, slice and int supported. ""\n                ""One of k, weighted or percent should be set to a non empty value."".format(arg)\n            )\n\n        def assert_slices_coverage(slices):\n            # Ensure that the expended slices cover all percents.\n            assert sum((list(range(*s.indices(100))) for s in slices), []) == list(range(100))\n\n        if k:\n            if not 0 < k <= 100:\n                raise ValueError(""Subsplit k should be between 0 and 100, got {}"".format(k))\n            shift = 100 // k\n            slices = [slice(i * shift, (i + 1) * shift) for i in range(k)]\n            # Round up last element to ensure all elements are taken\n            slices[-1] = slice(slices[-1].start, 100)\n            # Internal check to ensure full coverage\n            assert_slices_coverage(slices)\n            return tuple(_SubSplit(self, s) for s in slices)\n        elif percent:\n            return _SubSplit(self, percent)\n        elif weighted:\n            # Normalize the weighted sum\n            total = sum(weighted)\n            weighted = [100 * x // total for x in weighted]\n            # Create the slice for each of the elements\n            start = 0\n            stop = 0\n            slices = []\n            for v in weighted:\n                stop += v\n                slices.append(slice(start, stop))\n                start = stop\n            # Round up last element to ensure all elements are taken\n            slices[-1] = slice(slices[-1].start, 100)\n            # Internal check to ensure full coverage\n            assert_slices_coverage(slices)\n            return tuple(_SubSplit(self, s) for s in slices)\n        else:\n            # Should not be possible\n            raise ValueError(""Could not determine the split"")\n\n\n# 2 requirements:\n# 1. nlp.percent be sliceable\n# 2. nlp.percent be documented\n#\n# Instances are not documented, so we want nlp.percent to be a class, but to\n# have it be sliceable, we need this metaclass.\nclass PercentSliceMeta(type):\n    def __getitem__(cls, slice_value):\n        if not isinstance(slice_value, slice):\n            raise ValueError(""nlp.percent should only be called with slice, not {}"".format(slice_value))\n        return slice_value\n\n\nclass PercentSlice(metaclass=PercentSliceMeta):\n    # pylint: disable=line-too-long\n    """"""Syntactic sugar for defining slice subsplits: `nlp.percent[75:-5]`.\n\n    See the\n    [guide on splits](https://github.com/huggingface/nlp/tree/master/docs/splits.md)\n    for more information.\n    """"""\n    # pylint: enable=line-too-long\n    pass\n\n\npercent = PercentSlice  # pylint: disable=invalid-name\n\n\nclass _SplitMerged(SplitBase):\n    """"""Represent two split descriptors merged together.""""""\n\n    def __init__(self, split1, split2):\n        self._split1 = split1\n        self._split2 = split2\n\n    def get_read_instruction(self, split_dict):\n        read_instruction1 = self._split1.get_read_instruction(split_dict)\n        read_instruction2 = self._split2.get_read_instruction(split_dict)\n        return read_instruction1 + read_instruction2\n\n    def __repr__(self):\n        return ""({!r} + {!r})"".format(self._split1, self._split2)\n\n\nclass _SubSplit(SplitBase):\n    """"""Represent a sub split of a split descriptor.""""""\n\n    def __init__(self, split, slice_value):\n        self._split = split\n        self._slice_value = slice_value\n\n    def get_read_instruction(self, split_dict):\n        return self._split.get_read_instruction(split_dict)[self._slice_value]\n\n    def __repr__(self):\n        slice_str = ""{start}:{stop}""\n        if self._slice_value.step is not None:\n            slice_str += "":{step}""\n        slice_str = slice_str.format(\n            start="""" if self._slice_value.start is None else self._slice_value.start,\n            stop="""" if self._slice_value.stop is None else self._slice_value.stop,\n            step=self._slice_value.step,\n        )\n        return ""{!r}(nlp.percent[{}])"".format(self._split, slice_str)\n\n\nclass NamedSplit(SplitBase):\n    """"""Descriptor corresponding to a named split (train, test, ...).\n\n    Each descriptor can be composed with other using addition or slice. Ex:\n\n    ```\n    split = nlp.Split.TRAIN.subsplit(nlp.percent[0:25]) + nlp.Split.TEST\n    ```\n\n    The resulting split will correspond to 25% of the train split merged with\n    100% of the test split.\n\n    Warning:\n        A split cannot be added twice, so the following will fail:\n\n    ```\n    split = (\n            nlp.Split.TRAIN.subsplit(nlp.percent[:25]) +\n            nlp.Split.TRAIN.subsplit(nlp.percent[75:])\n    )  # Error\n    split = nlp.Split.TEST + nlp.Split.ALL  # Error\n    ```\n\n    Warning:\n        The slices can be applied only one time. So the following are valid:\n\n    ```\n    split = (\n            nlp.Split.TRAIN.subsplit(nlp.percent[:25]) +\n            nlp.Split.TEST.subsplit(nlp.percent[:50])\n    )\n    split = (nlp.Split.TRAIN + nlp.Split.TEST).subsplit(nlp.percent[:50])\n    ```\n\n        But not:\n\n    ```\n    train = nlp.Split.TRAIN\n    test = nlp.Split.TEST\n    split = train.subsplit(nlp.percent[:25]).subsplit(nlp.percent[:25])\n    split = (train.subsplit(nlp.percent[:25]) + test).subsplit(nlp.percent[:50])\n    ```\n\n    """"""\n\n    def __init__(self, name):\n        self._name = name\n\n    def __str__(self):\n        return self._name\n\n    def __repr__(self):\n        return ""NamedSplit(\'{name}\')"".format(name=self._name)\n\n    def __eq__(self, other):\n        """"""Equality: nlp.Split.TRAIN == \'train\'.""""""\n        if isinstance(other, NamedSplit):\n            return self._name == other._name  # pylint: disable=protected-access\n        elif isinstance(other, SplitBase):\n            return False\n        elif isinstance(other, str):  # Other should be string\n            return self._name == other\n        else:\n            raise ValueError(""Equality not supported between split {} and {}"".format(self, other))\n\n    def __hash__(self):\n        return hash(self._name)\n\n    def get_read_instruction(self, split_dict):\n        return SplitReadInstruction(split_dict[self._name])\n\n\nclass NamedSplitAll(NamedSplit):\n    """"""Split corresponding to the union of all defined dataset splits.""""""\n\n    def __init__(self):\n        super(NamedSplitAll, self).__init__(""all"")\n\n    def __repr__(self):\n        return f""NamedSplitAll({self._name}""\n\n    def get_read_instruction(self, split_dict):\n        # Merge all dataset split together\n        read_instructions = [SplitReadInstruction(s) for s in split_dict.values()]\n        return sum(read_instructions, SplitReadInstruction())\n\n\nclass Split(object):\n    # pylint: disable=line-too-long\n    """"""`Enum` for dataset splits.\n\n    Datasets are typically split into different subsets to be used at various\n    stages of training and evaluation.\n\n    * `TRAIN`: the training data.\n    * `VALIDATION`: the validation data. If present, this is typically used as\n        evaluation data while iterating on a model (e.g. changing hyperparameters,\n        model architecture, etc.).\n    * `TEST`: the testing data. This is the data to report metrics on. Typically\n        you do not want to use this during model iteration as you may overfit to it.\n\n    Note: All splits, including compositions inherit from `nlp.SplitBase`\n\n    See the\n    [guide on splits](https://github.com/huggingface/nlp/tree/master/docs/splits.md)\n    for more information.\n    """"""\n    # pylint: enable=line-too-long\n    TRAIN = NamedSplit(""train"")\n    TEST = NamedSplit(""test"")\n    VALIDATION = NamedSplit(""validation"")\n\n    def __new__(cls, name):\n        """"""Create a custom split with nlp.Split(\'custom_name\').""""""\n        return NamedSplit(name)\n\n\n# Similar to SplitInfo, but contain an additional slice info\nSlicedSplitInfo = collections.namedtuple(""SlicedSplitInfo"", [""split_info"", ""slice_value"",])  # noqa: E231\n\n\nclass SplitReadInstruction(object):\n    """"""Object containing the reading instruction for the dataset.\n\n    Similarly to `SplitDescriptor` nodes, this object can be composed with itself,\n    but the resolution happens instantaneously, instead of keeping track of the\n    tree, such as all instructions are compiled and flattened in a single\n    SplitReadInstruction object containing the list of files and slice to use.\n\n    Once resolved, the instructions can be accessed with:\n\n    ```\n    read_instructions.get_list_sliced_split_info()  # List of splits to use\n    ```\n\n    """"""\n\n    def __init__(self, split_info=None):\n        self._splits = NonMutableDict(error_msg=""Overlap between splits. Split {key} has been added with "" ""itself."")\n\n        if split_info:\n            self.add(SlicedSplitInfo(split_info=split_info, slice_value=None))\n\n    def add(self, sliced_split):\n        """"""Add a SlicedSplitInfo the read instructions.""""""\n        # TODO(epot): Check that the number of examples per shard % 100 == 0\n        # Otherwise the slices value may be unbalanced and not exactly reflect the\n        # requested slice.\n        self._splits[sliced_split.split_info.name] = sliced_split\n\n    def __add__(self, other):\n        """"""Merging split together.""""""\n        # Will raise error if a split has already be added (NonMutableDict)\n        # TODO(epot): If a split is already added but there is no overlap between\n        # the slices, should merge the slices (ex: [:10] + [80:])\n        split_instruction = SplitReadInstruction()\n        split_instruction._splits.update(self._splits)  # pylint: disable=protected-access\n        split_instruction._splits.update(other._splits)  # pylint: disable=protected-access\n        return split_instruction\n\n    def __getitem__(self, slice_value):\n        """"""Sub-splits.""""""\n        # Will raise an error if a split has already been sliced\n        split_instruction = SplitReadInstruction()\n        for v in self._splits.values():\n            if v.slice_value is not None:\n                raise ValueError(""Trying to slice Split {} which has already been sliced"".format(v.split_info.name))\n            v = v._asdict()\n            v[""slice_value""] = slice_value\n            split_instruction.add(SlicedSplitInfo(**v))\n        return split_instruction\n\n    def get_list_sliced_split_info(self):\n        return list(sorted(self._splits.values(), key=lambda x: x.split_info.name))\n\n\nclass SplitDict(dict):\n    """"""Split info object.""""""\n\n    def __init__(self, *args, dataset_name=None, **kwargs):\n        super(SplitDict, self).__init__(*args, **kwargs)\n        # super(SplitDict, self).__init__(error_msg=""Split {key} already present"", **kwargs)\n        self.dataset_name = dataset_name\n\n    def __getitem__(self, key: Union[SplitBase, str]):\n        # 1st case: The key exists: `info.splits[\'train\']`\n        if str(key) in self:\n            return super(SplitDict, self).__getitem__(str(key))\n        # 2nd case: Uses instructions: `info.splits[\'train[50%]\']`\n        else:\n            instructions = make_file_instructions(name=self.dataset_name, split_infos=self.values(), instruction=key,)\n            return SubSplitInfo(instructions)\n\n    def __setitem__(self, key: Union[SplitBase, str], value: SplitInfo):\n        raise ValueError(""Cannot add elem. Use .add() instead."")\n\n    def add(self, split_info: SplitInfo):\n        """"""Add the split info.""""""\n        if split_info.name in self:\n            raise ValueError(""Split {} already present"".format(split_info.name))\n        # Forward the dataset name required to build file instructions:\n        # info.splits[\'train\'].file_instructions\n        split_info.dataset_name = self.dataset_name\n        super(SplitDict, self).__setitem__(split_info.name, split_info)\n\n    @property\n    def total_num_examples(self):\n        """"""Return the total number of examples.""""""\n        return sum(s.num_examples for s in self.values())\n\n    @classmethod\n    def from_split_dict(cls, split_infos: Union[List, Dict], dataset_name: Optional[str] = None):\n        """"""Returns a new SplitDict initialized from a Dict or List of `split_infos`.""""""\n        if isinstance(split_infos, dict):\n            split_infos = list(split_infos.values())\n\n        if dataset_name is None:\n            dataset_name = split_infos[0][""dataset_name""]\n\n        split_dict = cls(dataset_name=dataset_name)\n\n        for split_info in split_infos:\n            if isinstance(split_info, dict):\n                split_info = SplitInfo(**split_info)\n            split_dict.add(split_info)\n\n        return split_dict\n\n    def to_split_dict(self):\n        """"""Returns a list of SplitInfo protos that we have.""""""\n        # Return the SplitInfo, sorted by name\n        return sorted([s for s in self.values()], key=lambda s: s.name)\n\n    def copy(self):\n        return SplitDict.from_split_dict(self.to_split_dict(), self.dataset_name)\n\n\n@dataclass\nclass SplitGenerator:\n    """"""Defines the split information for the generator.\n\n    This should be used as returned value of\n    `GeneratorBasedBuilder._split_generators`.\n    See `GeneratorBasedBuilder._split_generators` for more info and example\n    of usage.\n\n    Args:\n        name: `str`, name of the Split for which the generator will\n            create the examples.\n        gen_kwargs: `dict`, kwargs to forward to the _generate_examples() method\n            of the builder.\n    """"""\n\n    name: str\n    gen_kwargs: Dict = field(default_factory=dict)\n    split_info: SplitInfo = field(init=False)\n\n    def __post_init__(self):\n        self.name = str(self.name)  # Make sure we convert NamedSplits in strings\n        self.split_info = SplitInfo(name=self.name)\n'"
tests/utils/test_py_utils.py,0,"b'from unittest import TestCase\n\nfrom nlp.utils.py_utils import (\n    flatten_nest_dict,\n    flatten_nested,\n    map_nested,\n    temporary_assignment,\n    zip_dict,\n    zip_nested,\n)\n\n\nclass PyUtilsTest(TestCase):\n    def test_flatten_nest_dict(self):\n        d1 = {}\n        d2 = {""a"": 1, ""b"": 2}\n        d3 = {""a"": {""1"": 1, ""2"": 2}, ""b"": 3}\n        expected_flatten_d1 = {}\n        expected_flatten_d2 = {""a"": 1, ""b"": 2}\n        expected_flatten_d3 = {""a/1"": 1, ""a/2"": 2, ""b"": 3}\n        self.assertDictEqual(flatten_nest_dict(d1), expected_flatten_d1)\n        self.assertDictEqual(flatten_nest_dict(d2), expected_flatten_d2)\n        self.assertDictEqual(flatten_nest_dict(d3), expected_flatten_d3)\n\n    def test_flatten_nested(self):\n        s1 = {}\n        s2 = []\n        s3 = ""foo""\n        s4 = [""foo"", ""bar""]\n        s5 = {""a"": 1, ""b"": 2}\n        s6 = {""a"": [1, 2], ""b"": [3, 4]}\n        s7 = {""a"": {""1"": 1}, ""b"": 2}\n        expected_flatten_nested_s1 = []\n        expected_flatten_nested_s2 = []\n        expected_flatten_nested_s3 = [""foo""]\n        expected_flatten_nested_s4 = [""foo"", ""bar""]\n        expected_flatten_nested_s5 = [1, 2]\n        expected_flatten_nested_s6 = [1, 2, 3, 4]\n        expected_flatten_nested_s7 = [1, 2]\n        self.assertEqual(flatten_nested(s1), expected_flatten_nested_s1)\n        self.assertEqual(flatten_nested(s2), expected_flatten_nested_s2)\n        self.assertEqual(flatten_nested(s3), expected_flatten_nested_s3)\n        self.assertEqual(flatten_nested(s4), expected_flatten_nested_s4)\n        self.assertEqual(flatten_nested(s5), expected_flatten_nested_s5)\n        self.assertEqual(flatten_nested(s6), expected_flatten_nested_s6)\n        self.assertEqual(flatten_nested(s7), expected_flatten_nested_s7)\n\n    def test_map_mested(self):\n        def add_one(i):\n            return i + 1\n\n        s1 = {}\n        s2 = []\n        s3 = 1\n        s4 = [1, 2]\n        s5 = {""a"": 1, ""b"": 2}\n        s6 = {""a"": [1, 2], ""b"": [3, 4]}\n        s7 = {""a"": {""1"": 1}, ""b"": 2}\n        expected_map_nested_s1 = {}\n        expected_map_nested_s2 = []\n        expected_map_nested_s3 = 2\n        expected_map_nested_s4 = [2, 3]\n        expected_map_nested_s5 = {""a"": 2, ""b"": 3}\n        expected_map_nested_s6 = {""a"": [2, 3], ""b"": [4, 5]}\n        expected_map_nested_s7 = {""a"": {""1"": 2}, ""b"": 3}\n        self.assertEqual(map_nested(add_one, s1), expected_map_nested_s1)\n        self.assertEqual(map_nested(add_one, s2), expected_map_nested_s2)\n        self.assertEqual(map_nested(add_one, s3), expected_map_nested_s3)\n        self.assertEqual(map_nested(add_one, s4), expected_map_nested_s4)\n        self.assertEqual(map_nested(add_one, s5), expected_map_nested_s5)\n        self.assertEqual(map_nested(add_one, s6), expected_map_nested_s6)\n        self.assertEqual(map_nested(add_one, s7), expected_map_nested_s7)\n\n    def test_zip_dict(self):\n        d1 = {""a"": 1, ""b"": 2}\n        d2 = {""a"": 3, ""b"": 4}\n        d3 = {""a"": 5, ""b"": 6}\n        expected_zip_dict_result = sorted([(""a"", (1, 3, 5)), (""b"", (2, 4, 6))])\n        self.assertEqual(sorted(list(zip_dict(d1, d2, d3))), expected_zip_dict_result)\n\n    def test_zip_nested(self):\n        d1 = {""a"": {""1"": 1}, ""b"": 2}\n        d2 = {""a"": {""1"": 3}, ""b"": 4}\n        expected_zip_nested_result = {""a"": {""1"": (1, 3)}, ""b"": (2, 4)}\n        self.assertDictEqual(zip_nested(d1, d2), expected_zip_nested_result)\n\n    def test_temporary_assignment(self):\n        class Foo:\n            my_attr = ""bar""\n\n        foo = Foo()\n        self.assertEqual(foo.my_attr, ""bar"")\n        with temporary_assignment(foo, ""my_attr"", ""BAR""):\n            self.assertEqual(foo.my_attr, ""BAR"")\n        self.assertEqual(foo.my_attr, ""bar"")\n'"
src/nlp/commands/__init__.py,0,"b'from abc import ABC, abstractmethod\nfrom argparse import ArgumentParser\n\n\nclass BaseTransformersCLICommand(ABC):\n    @staticmethod\n    @abstractmethod\n    def register_subcommand(parser: ArgumentParser):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def run(self):\n        raise NotImplementedError()\n'"
src/nlp/commands/convert.py,1,"b'import os\nimport re\nimport shutil\nfrom argparse import ArgumentParser, Namespace\nfrom logging import getLogger\n\nfrom nlp.commands import BaseTransformersCLICommand\n\n\nHIGHLIGHT_MESSAGE_PRE = """"""<<<<<<< This should probably be modified because it mentions: """"""\n\nHIGHLIGHT_MESSAGE_POST = """"""=======\n>>>>>>>\n""""""\n\nTO_HIGHLIGHT = [\n    ""TextEncoderConfig"",\n    ""ByteTextEncoder"",\n    ""SubwordTextEncoder"",\n    ""encoder_config"",\n    ""maybe_build_from_corpus"",\n    ""manual_dir"",\n]\n\nTO_CONVERT = [\n    # (pattern, replacement)\n    # Order is important here for some replacements\n    (r""tfds\\.core"", r""nlp""),\n    (r""tf\\.io\\.gfile\\.GFile"", r""open""),\n    (r""tf\\.([\\w\\d]+)"", r""nlp.Value(\'\\1\')""),\n    (r""tfds\\.features\\.Text\\(\\)"", r""nlp.Value(\'string\')""),\n    (r""tfds\\.features\\.Text\\("", r""nlp.Value(\'string\'),""),\n    (r""features\\s*=\\s*tfds.features.FeaturesDict\\("", r""features=nlp.Features(""),\n    (r""tfds\\.features\\.FeaturesDict\\("", r""dict(""),\n    (r""The TensorFlow Datasets Authors"", r""The TensorFlow Datasets Authors and the HuggingFace NLP Authors""),\n    (r""tfds\\."", r""nlp.""),\n    (r""dl_manager\\.manual_dir"", r""self.config.data_dir""),\n    (r""self\\.builder_config"", r""self.config""),\n]\n\n\ndef convert_command_factory(args: Namespace):\n    """"""\n    Factory function used to convert a model TF 1.0 checkpoint in a PyTorch checkpoint.\n    :return: ServeCommand\n    """"""\n    return ConvertCommand(args.tfds_path, args.nlp_directory)\n\n\nclass ConvertCommand(BaseTransformersCLICommand):\n    @staticmethod\n    def register_subcommand(parser: ArgumentParser):\n        """"""\n        Register this command to argparse so it\'s available for the transformer-cli\n        :param parser: Root parser to register command-specific arguments\n        :return:\n        """"""\n        train_parser = parser.add_parser(\n            ""convert"", help=""CLI tool to convert a (nlp) TensorFlow-Dataset in a HuggingFace-NLP dataset."",\n        )\n        train_parser.add_argument(\n            ""--tfds_path"",\n            type=str,\n            required=True,\n            help=""Path to a TensorFlow Datasets folder to convert or a single tfds file to convert."",\n        )\n        train_parser.add_argument(\n            ""--nlp_directory"", type=str, required=True, help=""Path to the HuggingFace NLP folder.""\n        )\n        train_parser.set_defaults(func=convert_command_factory)\n\n    def __init__(self, tfds_path: str, nlp_directory: str, *args):\n        self._logger = getLogger(""nlp-cli/converting"")\n\n        self._tfds_path = tfds_path\n        self._nlp_directory = nlp_directory\n\n    def run(self):\n        if os.path.isdir(self._tfds_path):\n            abs_tfds_path = os.path.abspath(self._tfds_path)\n        elif os.path.isfile(self._tfds_path):\n            abs_tfds_path = os.path.dirname(self._tfds_path)\n        else:\n            raise ValueError(""--tfds_path is neither a directory nor a file. Please check path."")\n\n        abs_nlp_path = os.path.abspath(self._nlp_directory)\n\n        self._logger.info(""Converting datasets from %s to %s"", abs_tfds_path, abs_nlp_path)\n\n        utils_files = []\n        with_manual_update = []\n        imports_to_builder_map = {}\n\n        if os.path.isdir(self._tfds_path):\n            file_names = os.listdir(abs_tfds_path)\n        else:\n            file_names = [os.path.basename(self._tfds_path)]\n\n        for f_name in file_names:\n            self._logger.info(""Looking at file %s"", f_name)\n            input_file = os.path.join(abs_tfds_path, f_name)\n            output_file = os.path.join(abs_nlp_path, f_name)\n\n            if not os.path.isfile(input_file) or ""__init__"" in f_name or ""_test"" in f_name or "".py"" not in f_name:\n                self._logger.info(""Skipping file"")\n                continue\n\n            with open(input_file, ""r"") as f:\n                lines = f.readlines()\n\n            out_lines = []\n            is_builder = False\n            needs_manual_update = False\n            tfds_imports = []\n            for line in lines:\n                out_line = line\n\n                # Convert imports\n                if ""import tensorflow.compat.v2 as tf"" in out_line:\n                    continue\n                elif ""@tfds.core"" in out_line:\n                    continue\n                elif ""builder=self"" in out_line:\n                    continue\n                elif ""import tensorflow_datasets.public_api as tfds"" in out_line:\n                    out_line = ""import nlp\\n""\n                elif ""import tensorflow"" in out_line:\n                    # order is important here\n                    out_line = """"\n                    continue\n                elif ""from absl import logging"" in out_line:\n                    out_line = ""import logging\\n""\n                elif any(expression in out_line for expression in TO_HIGHLIGHT):\n                    needs_manual_update = True\n                    to_remove = list(filter(lambda e: e in out_line, TO_HIGHLIGHT))\n                    out_lines.append(HIGHLIGHT_MESSAGE_PRE + str(to_remove) + ""\\n"")\n                    out_lines.append(out_line)\n                    out_lines.append(HIGHLIGHT_MESSAGE_POST)\n                    continue\n                else:\n                    for pattern, replacement in TO_CONVERT:\n                        out_line = re.sub(pattern, replacement, out_line)\n\n                # Take care of saving utilities (to later move them together with main script)\n                if ""tensorflow_datasets"" in out_line:\n                    match = re.match(r""from\\stensorflow_datasets.*import\\s([^\\.\\r\\n]+)"", out_line)\n                    tfds_imports.extend(imp.strip() for imp in match.group(1).split("",""))\n                    out_line = ""from . import "" + match.group(1)\n\n                # Check we have not forget anything\n                assert (\n                    ""tf."" not in out_line and ""tfds."" not in out_line and ""tensorflow_datasets"" not in out_line\n                ), f""Error converting {out_line.strip()}""\n\n                if ""GeneratorBasedBuilder"" in out_line or ""BeamBasedBuilder"" in out_line:\n                    is_builder = True\n                out_lines.append(out_line)\n\n            if is_builder or ""wmt"" in f_name:\n                # We create a new directory for each dataset\n                dir_name = f_name.replace("".py"", """")\n                output_dir = os.path.join(abs_nlp_path, dir_name)\n                output_file = os.path.join(output_dir, f_name)\n                os.makedirs(output_dir, exist_ok=True)\n                self._logger.info(""Adding directory %s"", output_dir)\n                imports_to_builder_map.update(dict((imp, output_dir) for imp in tfds_imports))\n            else:\n                # Utilities will be moved at the end\n                utils_files.append(output_file)\n\n            if needs_manual_update:\n                with_manual_update.append(output_file)\n\n            with open(output_file, ""w"") as f:\n                f.writelines(out_lines)\n            self._logger.info(""Converted in %s"", output_file)\n\n        for utils_file in utils_files:\n            try:\n                f_name = os.path.basename(utils_file)\n                dest_folder = imports_to_builder_map[f_name.replace("".py"", """")]\n                self._logger.info(""Moving %s to %s"", utils_file, dest_folder)\n                shutil.copy(utils_file, dest_folder)\n            except KeyError:\n                self._logger.error(f""Cannot find destination folder for {utils_file}. Please copy manually."")\n\n        if with_manual_update:\n            for file_path in with_manual_update:\n                self._logger.warning(\n                    f""You need to manually update file {file_path} to remove configurations using \'TextEncoderConfig\'.""\n                )\n'"
src/nlp/commands/download.py,0,"b'from argparse import ArgumentParser\n\nfrom nlp.commands import BaseTransformersCLICommand\n\n\ndef download_command_factory(args):\n    return DownloadCommand(args.model, args.cache_dir, args.force)\n\n\nclass DownloadCommand(BaseTransformersCLICommand):\n    @staticmethod\n    def register_subcommand(parser: ArgumentParser):\n        download_parser = parser.add_parser(""download"")\n        download_parser.add_argument(\n            ""--cache-dir"", type=str, default=None, help=""Path to location to store the models""\n        )\n        download_parser.add_argument(\n            ""--force"", action=""store_true"", help=""Force the model to be download even if already in cache-dir""\n        )\n        download_parser.add_argument(""model"", type=str, help=""Name of the model to download"")\n        download_parser.set_defaults(func=download_command_factory)\n\n    def __init__(self, model: str, cache: str, force: bool):\n        self._model = model\n        self._cache = cache\n        self._force = force\n\n    def run(self):\n        from transformers import AutoModel, AutoTokenizer\n\n        AutoModel.from_pretrained(self._model, cache_dir=self._cache, force_download=self._force)\n        AutoTokenizer.from_pretrained(self._model, cache_dir=self._cache, force_download=self._force)\n'"
src/nlp/commands/dummy_data.py,0,"b'import logging\nimport os\nfrom argparse import ArgumentParser\n\nfrom nlp.commands import BaseTransformersCLICommand\nfrom nlp.load import import_main_class, prepare_module\nfrom nlp.utils import MockDownloadManager\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_command_factory(args):\n    return DummyDataCommand(args.path_to_dataset, args.requires_manual,)\n\n\nclass DummyDataCommand(BaseTransformersCLICommand):\n    @staticmethod\n    def register_subcommand(parser: ArgumentParser):\n        test_parser = parser.add_parser(""dummy_data"")\n        test_parser.add_argument(""--requires_manual"", action=""store_true"", help=""Dataset requires manual data"")\n        test_parser.add_argument(""path_to_dataset"", type=str, help=""Name of the dataset to download"")\n        test_parser.set_defaults(func=test_command_factory)\n\n    def __init__(\n        self, path_to_dataset: str, requires_manual: bool,\n    ):\n        self._path_to_dataset = path_to_dataset\n        self._requires_manual = requires_manual\n        self._dataset_name = path_to_dataset.split(""/"")[-2]\n\n    def run(self):\n        module_path = prepare_module(self._path_to_dataset)\n        builder_cls = import_main_class(module_path)\n\n        # use `None` as config if no configs\n        configs = builder_cls.BUILDER_CONFIGS or [None]\n\n        for config in configs:\n            if config is None:\n                name = None\n                version = builder_cls.VERSION\n            else:\n                version = config.version\n                name = config.name\n\n            dataset_builder = builder_cls(name=name)\n            mock_dl_manager = MockDownloadManager(\n                dataset_name=self._dataset_name, config=config, version=version, is_local=True\n            )\n\n            dummy_data_folder = os.path.join(self._path_to_dataset, mock_dl_manager.dummy_data_folder)\n            logger.info(f""Creating dummy folder structure for {dummy_data_folder}... "")\n            os.makedirs(dummy_data_folder, exist_ok=True)\n\n            try:\n                generator_splits = dataset_builder._split_generators(mock_dl_manager)\n            except FileNotFoundError as e:\n\n                print(\n                    f""Dataset {self._dataset_name} with config {config} seems to already open files in the method `_split_generators(...)`. You might consider to instead only open files in the method `_generate_examples(...)` instead. If this is not possible the dummy data has to be created with less guidance. Make sure you create the file {e.filename}.""\n                )\n\n            files_to_create = set()\n            split_names = []\n            dummy_file_name = mock_dl_manager.dummy_file_name\n\n            for split in generator_splits:\n                logger.info(f""Collecting dummy data file paths to create for {split.name}"")\n                split_names.append(split.name)\n                gen_kwargs = split.gen_kwargs\n                generator = dataset_builder._generate_examples(**gen_kwargs)\n\n                try:\n                    dummy_data_guidance_print = ""\\n"" + 30 * ""="" + ""DUMMY DATA INSTRUCTIONS"" + 30 * ""="" + ""\\n""\n                    config_string = f""config {config.name} of "" if config is not None else """"\n                    dummy_data_guidance_print += (\n                        ""- In order to create the dummy data for ""\n                        + config_string\n                        + f""{self._dataset_name}, please go into the folder \'{dummy_data_folder}\' with `cd {dummy_data_folder}` . \\n\\n""\n                    )\n\n                    # trigger generate function\n                    for key, record in generator:\n                        pass\n\n                    dummy_data_guidance_print += f""- It appears that the function `_generate_examples(...)` expects one or more files in the folder {dummy_file_name} using the function `glob.glob(...)`. In this case, please refer to the `_generate_examples(...)` method to see under which filename the dummy data files should be created. \\n\\n""\n\n                except FileNotFoundError as e:\n                    files_to_create.add(e.filename)\n\n            split_names = "", "".join(split_names)\n            if len(files_to_create) > 0:\n                # no glob.glob(...) in `_generate_examples(...)`\n                if len(files_to_create) == 1 and next(iter(files_to_create)) == dummy_file_name:\n                    dummy_data_guidance_print += f""- Please create a single dummy data file called \'{next(iter(files_to_create))}\' from the folder \'{dummy_data_folder}\'. Make sure that the dummy data file provides at least one example for the split(s) \'{split_names}\' \\n\\n""\n                    files_string = dummy_file_name\n                else:\n                    files_string = "", "".join(files_to_create)\n                    dummy_data_guidance_print += f""- Please create the following dummy data files \'{files_string}\' from the folder \'{dummy_data_folder}\'\\n\\n""\n\n                    dummy_data_guidance_print += f""- For each of the splits \'{split_names}\', make sure that one or more of the dummy data files provide at least one example \\n\\n""\n\n                dummy_data_guidance_print += f""- If the method `_generate_examples(...)` includes multiple `open()` statements, you might have to create other files in addition to \'{files_string}\'. In this case please refer to the `_generate_examples(...)` method \\n\\n""\n\n            if len(files_to_create) == 1 and next(iter(files_to_create)) == dummy_file_name:\n                dummy_data_guidance_print += f""-After the dummy data file is created, it should be zipped to \'{dummy_file_name}.zip\' with the command `zip {dummy_file_name}.zip {dummy_file_name}` \\n\\n""\n\n                dummy_data_guidance_print += (\n                    f""-You can now delete the file \'{dummy_file_name}\' with the command `rm {dummy_file_name}` \\n\\n""\n                )\n\n                dummy_data_guidance_print += f""- To get the file \'{dummy_file_name}\' back for further changes to the dummy data, simply unzip {dummy_file_name}.zip with the command `unzip {dummy_file_name}.zip` \\n\\n""\n            else:\n                dummy_data_guidance_print += f""-After all dummy data files are created, they should be zipped recursively to \'{dummy_file_name}.zip\' with the command `zip -r {dummy_file_name}.zip {dummy_file_name}/` \\n\\n""\n\n                dummy_data_guidance_print += f""-You can now delete the folder \'{dummy_file_name}\' with the command `rm -r {dummy_file_name}` \\n\\n""\n\n                dummy_data_guidance_print += f""- To get the folder \'{dummy_file_name}\' back for further changes to the dummy data, simply unzip {dummy_file_name}.zip with the command `unzip {dummy_file_name}.zip` \\n\\n""\n\n            dummy_data_guidance_print += (\n                f""- Make sure you have created the file \'{dummy_file_name}.zip\' in \'{dummy_data_folder}\' \\n""\n            )\n\n            dummy_data_guidance_print += 83 * ""="" + ""\\n""\n\n            print(dummy_data_guidance_print)\n'"
src/nlp/commands/env.py,3,"b'import platform\nfrom argparse import ArgumentParser\n\nfrom nlp import __version__ as version\nfrom nlp import is_tf_available, is_torch_available\nfrom nlp.commands import BaseTransformersCLICommand\n\n\ndef info_command_factory(_):\n    return EnvironmentCommand()\n\n\nclass EnvironmentCommand(BaseTransformersCLICommand):\n    @staticmethod\n    def register_subcommand(parser: ArgumentParser):\n        download_parser = parser.add_parser(""env"")\n        download_parser.set_defaults(func=info_command_factory)\n\n    def run(self):\n        pt_version = ""not installed""\n        pt_cuda_available = ""NA""\n        if is_torch_available():\n            import torch\n\n            pt_version = torch.__version__\n            pt_cuda_available = torch.cuda.is_available()\n\n        tf_version = ""not installed""\n        tf_cuda_available = ""NA""\n        if is_tf_available():\n            import tensorflow as tf\n\n            tf_version = tf.__version__\n            try:\n                # deprecated in v2.1\n                tf_cuda_available = tf.test.is_gpu_available()\n            except AttributeError:\n                # returns list of devices, convert to bool\n                tf_cuda_available = bool(tf.config.list_physical_devices(""GPU""))\n\n        info = {\n            ""`nlp` version"": version,\n            ""Platform"": platform.platform(),\n            ""Python version"": platform.python_version(),\n            ""PyTorch version (GPU?)"": ""{} ({})"".format(pt_version, pt_cuda_available),\n            ""Tensorflow version (GPU?)"": ""{} ({})"".format(tf_version, tf_cuda_available),\n            ""Using GPU in script?"": ""<fill in>"",\n            ""Using distributed or parallel set-up in script?"": ""<fill in>"",\n        }\n\n        print(""\\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\\n"")\n        print(self.format_dict(info))\n\n        return info\n\n    @staticmethod\n    def format_dict(d):\n        return ""\\n"".join([""- {}: {}"".format(prop, val) for prop, val in d.items()]) + ""\\n""\n'"
src/nlp/commands/run_beam.py,0,"b'import os\nfrom argparse import ArgumentParser\nfrom shutil import copyfile\nfrom typing import List\n\nimport apache_beam as beam\n\nfrom nlp.builder import FORCE_REDOWNLOAD, HF_DATASETS_CACHE, REUSE_CACHE_IF_EXISTS, DatasetBuilder, DownloadConfig\nfrom nlp.commands import BaseTransformersCLICommand\nfrom nlp.info import DATASET_INFOS_DICT_FILE_NAME\nfrom nlp.load import import_main_class, prepare_module\n\n\ndef run_beam_command_factory(args):\n    return RunBeamCommand(\n        args.dataset,\n        args.name,\n        args.cache_dir,\n        args.beam_pipeline_options,\n        args.data_dir,\n        args.all_configs,\n        args.save_infos,\n        args.ignore_verifications,\n        args.force_redownload,\n    )\n\n\nclass RunBeamCommand(BaseTransformersCLICommand):\n    @staticmethod\n    def register_subcommand(parser: ArgumentParser):\n        run_beam_parser = parser.add_parser(""run_beam"")\n        run_beam_parser.add_argument(""--name"", type=str, default=None, help=""Dataset processing name"")\n        run_beam_parser.add_argument(\n            ""--cache_dir"", type=str, default=None, help=""Cache directory where the datasets are stored."",\n        )\n        run_beam_parser.add_argument(\n            ""--beam_pipeline_options"",\n            type=str,\n            default="""",\n            help=""Beam pipeline options, separated by commas. Example: `--beam_pipeline_options=job_name=my-job,project=my-project`"",\n        )\n        run_beam_parser.add_argument(\n            ""--data_dir"",\n            type=str,\n            default=None,\n            help=""Can be used to specify a manual directory to get the files from."",\n        )\n        run_beam_parser.add_argument(""--all_configs"", action=""store_true"", help=""Test all dataset configurations"")\n        run_beam_parser.add_argument(""--save_infos"", action=""store_true"", help=""Save the dataset infos file"")\n        run_beam_parser.add_argument(\n            ""--ignore_verifications"", action=""store_true"", help=""Run the test without checksums and splits checks""\n        )\n        run_beam_parser.add_argument(""--force_redownload"", action=""store_true"", help=""Force dataset redownload"")\n        run_beam_parser.add_argument(""dataset"", type=str, help=""Name of the dataset to download"")\n        run_beam_parser.set_defaults(func=run_beam_command_factory)\n\n    def __init__(\n        self,\n        dataset: str,\n        name: str,\n        cache_dir: str,\n        beam_pipeline_options: str,\n        data_dir: str,\n        all_configs: bool,\n        save_infos: bool,\n        ignore_verifications: bool,\n        force_redownload: bool,\n    ):\n        self._dataset = dataset\n        self._name = name\n        self._cache_dir = cache_dir\n        self._beam_pipeline_options = beam_pipeline_options\n        self._data_dir = data_dir\n        self._all_configs = all_configs\n        self._save_infos = save_infos\n        self._ignore_verifications = ignore_verifications\n        self._force_redownload = force_redownload\n\n    def run(self):\n        if self._name is not None and self._all_configs:\n            print(""Both parameters `name` and `all_configs` can\'t be used at once."")\n            exit(1)\n        path, name = self._dataset, self._name\n        module_path = prepare_module(path)\n        builder_cls = import_main_class(module_path)\n        builders: List[DatasetBuilder] = []\n        if self._beam_pipeline_options:\n            beam_options = beam.options.pipeline_options.PipelineOptions(\n                flags=[""--%s"" % opt.strip() for opt in self._beam_pipeline_options.split("","") if opt]\n            )\n        else:\n            beam_options = None\n        if self._all_configs and len(builder_cls.BUILDER_CONFIGS) > 0:\n            for config in builder_cls.BUILDER_CONFIGS:\n                builders.append(\n                    builder_cls(\n                        name=config.name, data_dir=self._data_dir, beam_options=beam_options, cache_dir=self._cache_dir\n                    )\n                )\n        else:\n            builders.append(\n                builder_cls(name=name, data_dir=self._data_dir, beam_options=beam_options, cache_dir=self._cache_dir)\n            )\n\n        for builder in builders:\n            builder.download_and_prepare(\n                download_mode=REUSE_CACHE_IF_EXISTS if not self._force_redownload else FORCE_REDOWNLOAD,\n                download_config=DownloadConfig(cache_dir=os.path.join(HF_DATASETS_CACHE, ""downloads"")),\n                save_infos=self._save_infos,\n                ignore_verifications=self._ignore_verifications,\n                try_from_hf_gcs=False,\n            )\n\n        print(""Apache beam run successful."")\n\n        # If save_infos=True, the dataset infos file is created next to the loaded module file.\n        # Let\'s move it to the original directory of the dataset script, to allow the user to\n        # upload them on S3 at the same time afterwards.\n        if self._save_infos:\n            dataset_infos_path = os.path.join(builder_cls.get_imported_module_dir(), DATASET_INFOS_DICT_FILE_NAME)\n\n            name = list(filter(lambda x: x, path.split(""/"")))[-1] + "".py""\n\n            combined_path = os.path.join(path, name)\n            if os.path.isfile(path):\n                dataset_dir = os.path.dirname(path)\n            elif os.path.isfile(combined_path):\n                dataset_dir = path\n            else:  # in case of a remote dataset\n                print(""Dataset Infos file saved at {}"".format(dataset_infos_path))\n                exit(1)\n\n            # Move datasetinfo back to the user\n            user_dataset_infos_path = os.path.join(dataset_dir, DATASET_INFOS_DICT_FILE_NAME)\n            copyfile(dataset_infos_path, user_dataset_infos_path)\n            print(""Dataset Infos file saved at {}"".format(user_dataset_infos_path))\n'"
src/nlp/commands/test.py,0,"b'import os\nfrom argparse import ArgumentParser\nfrom shutil import copyfile\nfrom typing import List\n\nfrom nlp.builder import FORCE_REDOWNLOAD, REUSE_CACHE_IF_EXISTS, DatasetBuilder, DownloadConfig\nfrom nlp.commands import BaseTransformersCLICommand\nfrom nlp.info import DATASET_INFOS_DICT_FILE_NAME\nfrom nlp.load import import_main_class, prepare_module\n\n\ndef test_command_factory(args):\n    return TestCommand(\n        args.dataset,\n        args.name,\n        args.cache_dir,\n        args.data_dir,\n        args.all_configs,\n        args.save_infos,\n        args.ignore_verifications,\n        args.force_redownload,\n    )\n\n\nclass TestCommand(BaseTransformersCLICommand):\n    @staticmethod\n    def register_subcommand(parser: ArgumentParser):\n        test_parser = parser.add_parser(""test"")\n        test_parser.add_argument(""--name"", type=str, default=None, help=""Dataset processing name"")\n        test_parser.add_argument(\n            ""--cache_dir"", type=str, default=None, help=""Cache directory where the datasets are stored."",\n        )\n        test_parser.add_argument(\n            ""--data_dir"",\n            type=str,\n            default=None,\n            help=""Can be used to specify a manual directory to get the files from."",\n        )\n        test_parser.add_argument(""--all_configs"", action=""store_true"", help=""Test all dataset configurations"")\n        test_parser.add_argument(""--save_infos"", action=""store_true"", help=""Save the dataset infos file"")\n        test_parser.add_argument(\n            ""--ignore_verifications"", action=""store_true"", help=""Run the test without checksums and splits checks""\n        )\n        test_parser.add_argument(""--force_redownload"", action=""store_true"", help=""Force dataset redownload"")\n        test_parser.add_argument(""dataset"", type=str, help=""Name of the dataset to download"")\n        test_parser.set_defaults(func=test_command_factory)\n\n    def __init__(\n        self,\n        dataset: str,\n        name: str,\n        cache_dir: str,\n        data_dir: str,\n        all_configs: bool,\n        save_infos: bool,\n        ignore_verifications: bool,\n        force_redownload: bool,\n    ):\n        self._dataset = dataset\n        self._name = name\n        self._cache_dir = cache_dir\n        self._data_dir = data_dir\n        self._all_configs = all_configs\n        self._save_infos = save_infos\n        self._ignore_verifications = ignore_verifications\n        self._force_redownload = force_redownload\n\n    def run(self):\n        if self._name is not None and self._all_configs:\n            print(""Both parameters `config` and `all_configs` can\'t be used at once."")\n            exit(1)\n        path, name = self._dataset, self._name\n        module_path = prepare_module(path)\n        builder_cls = import_main_class(module_path)\n        builders: List[DatasetBuilder] = []\n        if self._all_configs and len(builder_cls.BUILDER_CONFIGS) > 0:\n            for config in builder_cls.BUILDER_CONFIGS:\n                builders.append(builder_cls(name=config.name, data_dir=self._data_dir))\n        else:\n            builders.append(builder_cls(name=name, data_dir=self._data_dir))\n\n        for builder in builders:\n            builder.download_and_prepare(\n                download_config=DownloadConfig(cache_dir=self._cache_dir),\n                download_mode=REUSE_CACHE_IF_EXISTS if not self._force_redownload else FORCE_REDOWNLOAD,\n                save_infos=self._save_infos,\n                ignore_verifications=self._ignore_verifications,\n            )\n\n        print(""Test successful."")\n        # If save_infos=True, the dataset infos file is created next to the loaded module file.\n        # Let\'s move it to the original directory of the dataset script, to allow the user to\n        # upload them on S3 at the same time afterwards.\n        if self._save_infos:\n            dataset_infos_path = os.path.join(builder_cls.get_imported_module_dir(), DATASET_INFOS_DICT_FILE_NAME)\n\n            name = list(filter(lambda x: x, path.split(""/"")))[-1] + "".py""\n\n            combined_path = os.path.join(path, name)\n            if os.path.isfile(path):\n                dataset_dir = os.path.dirname(path)\n            elif os.path.isfile(combined_path):\n                dataset_dir = path\n            else:  # in case of a remote dataset\n                print(""Dataset Infos file saved at {}"".format(dataset_infos_path))\n                exit(1)\n\n            # Move datasetinfo back to the user\n            user_dataset_infos_path = os.path.join(dataset_dir, DATASET_INFOS_DICT_FILE_NAME)\n            copyfile(dataset_infos_path, user_dataset_infos_path)\n            print(""Dataset Infos file saved at {}"".format(user_dataset_infos_path))\n'"
src/nlp/commands/user.py,0,"b'import os\nimport sys\nfrom argparse import ArgumentParser\nfrom getpass import getpass\nfrom typing import List, Union\n\nfrom requests.exceptions import HTTPError\n\nfrom nlp.commands import BaseTransformersCLICommand\nfrom nlp.hf_api import HfApi, HfFolder\n\n\nUPLOAD_MAX_FILES = 15\n\n\nclass UserCommands(BaseTransformersCLICommand):\n    @staticmethod\n    def register_subcommand(parser: ArgumentParser):\n        login_parser = parser.add_parser(""login"", help=""Log in using the same credentials as on huggingface.co"")\n        login_parser.set_defaults(func=lambda args: LoginCommand(args))\n        whoami_parser = parser.add_parser(""whoami"", help=""Find out which huggingface.co account you are logged in as."")\n        whoami_parser.set_defaults(func=lambda args: WhoamiCommand(args))\n        logout_parser = parser.add_parser(""logout"", help=""Log out"")\n        logout_parser.set_defaults(func=lambda args: LogoutCommand(args))\n        # s3 dataset\n        s3_parser = parser.add_parser(\n            ""s3_datasets"", help=""{ls, rm} Commands to interact with the files you upload on S3.""\n        )\n        s3_subparsers = s3_parser.add_subparsers(help=""s3 related commands"")\n        ls_parser = s3_subparsers.add_parser(""ls"")\n        ls_parser.add_argument(""--organization"", type=str, help=""Optional: organization namespace."")\n        ls_parser.set_defaults(func=lambda args: ListObjsCommand(args, file_types=""datasets""))\n        rm_parser = s3_subparsers.add_parser(""rm"")\n        rm_parser.add_argument(""filename"", type=str, help=""individual object filename to delete from S3."")\n        rm_parser.add_argument(""--organization"", type=str, help=""Optional: organization namespace."")\n        rm_parser.set_defaults(func=lambda args: DeleteObjCommand(args, file_types=""datasets""))\n        # s3 metrics\n        s3_parser = parser.add_parser(\n            ""s3_metrics"", help=""{ls, rm} Commands to interact with the files you upload on S3.""\n        )\n        s3_subparsers = s3_parser.add_subparsers(help=""s3 related commands"")\n        ls_parser = s3_subparsers.add_parser(""ls"")\n        ls_parser.add_argument(""--organization"", type=str, help=""Optional: organization namespace."")\n        ls_parser.set_defaults(func=lambda args: ListObjsCommand(args, file_types=""metrics""))\n        rm_parser = s3_subparsers.add_parser(""rm"")\n        rm_parser.add_argument(""filename"", type=str, help=""individual object filename to delete from S3."")\n        rm_parser.add_argument(""--organization"", type=str, help=""Optional: organization namespace."")\n        rm_parser.set_defaults(func=lambda args: DeleteObjCommand(args, file_types=""metrics""))\n        # upload dataset\n        upload_dataset_parser = parser.add_parser(""upload_dataset"", help=""Upload a dataset to S3."")\n        upload_dataset_parser.add_argument(\n            ""path"", type=str, help=""Local path of the dataset folder or individual file to upload.""\n        )\n        upload_dataset_parser.add_argument(""--organization"", type=str, help=""Optional: organization namespace."")\n        upload_dataset_parser.add_argument(\n            ""--filename"", type=str, default=None, help=""Optional: override individual object filename on S3.""\n        )\n        upload_dataset_parser.set_defaults(func=lambda args: UploadCommand(args, file_types=""datasets""))\n        # upload metric\n        upload_metric_parser = parser.add_parser(""upload_metric"", help=""Upload a metric to S3."")\n        upload_metric_parser.add_argument(\n            ""path"", type=str, help=""Local path of the metric folder or individual file to upload.""\n        )\n        upload_metric_parser.add_argument(""--organization"", type=str, help=""Optional: organization namespace."")\n        upload_metric_parser.add_argument(\n            ""--filename"", type=str, default=None, help=""Optional: override individual object filename on S3.""\n        )\n        upload_metric_parser.set_defaults(func=lambda args: UploadCommand(args, file_types=""metrics""))\n\n\nclass ANSI:\n    """"""\n    Helper for en.wikipedia.org/wiki/ANSI_escape_code\n    """"""\n\n    _bold = ""\\u001b[1m""\n    _red = ""\\u001b[31m""\n    _reset = ""\\u001b[0m""\n\n    @classmethod\n    def bold(cls, s):\n        return ""{}{}{}"".format(cls._bold, s, cls._reset)\n\n    @classmethod\n    def red(cls, s):\n        return ""{}{}{}"".format(cls._bold + cls._red, s, cls._reset)\n\n\nclass BaseUserCommand:\n    def __init__(self, args):\n        self.args = args\n        self._api = HfApi()\n\n\nclass LoginCommand(BaseUserCommand):\n    def run(self):\n        print(\n            """"""\n        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n        """"""\n        )\n        username = input(""Username: "")\n        password = getpass()\n        try:\n            token = self._api.login(username, password)\n        except HTTPError as e:\n            # probably invalid credentials, display error message.\n            print(e)\n            print(ANSI.red(e.response.text))\n            exit(1)\n        HfFolder.save_token(token)\n        print(""Login successful"")\n        print(""Your token:"", token, ""\\n"")\n        print(""Your token has been saved to"", HfFolder.path_token)\n\n\nclass WhoamiCommand(BaseUserCommand):\n    def run(self):\n        token = HfFolder.get_token()\n        if token is None:\n            print(""Not logged in"")\n            exit()\n        try:\n            user, orgs = self._api.whoami(token)\n            print(user)\n            if orgs:\n                print(ANSI.bold(""orgs: ""), "","".join(orgs))\n        except HTTPError as e:\n            print(e)\n            print(ANSI.red(e.response.text))\n            exit(1)\n\n\nclass LogoutCommand(BaseUserCommand):\n    def run(self):\n        token = HfFolder.get_token()\n        if token is None:\n            print(""Not logged in"")\n            exit()\n        HfFolder.delete_token()\n        self._api.logout(token)\n        print(""Successfully logged out."")\n\n\nclass ListObjsCommand(BaseUserCommand):\n    def __init__(self, args, file_types: str):\n        super().__init__(args)\n        self.file_types = file_types\n\n    def tabulate(self, rows: List[List[Union[str, int]]], headers: List[str]) -> str:\n        """"""\n        Inspired by:\n        stackoverflow.com/a/8356620/593036\n        stackoverflow.com/questions/9535954/printing-lists-as-tabular-data\n        """"""\n        col_widths = [max(len(str(x)) for x in col) for col in zip(*rows, headers)]\n        row_format = (""{{:{}}} "" * len(headers)).format(*col_widths)\n        lines = []\n        lines.append(row_format.format(*headers))\n        lines.append(row_format.format(*[""-"" * w for w in col_widths]))\n        for row in rows:\n            lines.append(row_format.format(*row))\n        return ""\\n"".join(lines)\n\n    def run(self):\n        token = HfFolder.get_token()\n        if token is None:\n            print(""Not logged in"")\n            exit(1)\n        try:\n            objs = self._api.list_objs(token, organization=self.args.organization, file_types=self.file_types)\n        except HTTPError as e:\n            print(e)\n            print(ANSI.red(e.response.text))\n            exit(1)\n        if len(objs) == 0:\n            print(""No shared file yet"")\n            exit()\n        rows = [[obj.filename, obj.LastModified, obj.ETag, obj.Size] for obj in objs]\n        print(self.tabulate(rows, headers=[""Filename"", ""LastModified"", ""ETag"", ""Size""]))\n\n\nclass DeleteObjCommand(BaseUserCommand):\n    def __init__(self, args, file_types):\n        super().__init__(args)\n        self.file_types = file_types\n\n    def run(self):\n        token = HfFolder.get_token()\n        if token is None:\n            print(""Not logged in"")\n            exit(1)\n        try:\n            self._api.delete_obj(\n                token, filename=self.args.filename, organization=self.args.organization, file_types=self.file_types\n            )\n        except HTTPError as e:\n            print(e)\n            print(ANSI.red(e.response.text))\n            exit(1)\n        print(""Done"")\n\n\nclass UploadCommand(BaseUserCommand):\n    def __init__(self, args, file_types):\n        super().__init__(args)\n        self.file_types = file_types\n\n    def walk_dir(self, rel_path: str):\n        """"""\n        Recursively list all files in a folder.\n        """"""\n        entries: List[os.DirEntry] = list(os.scandir(rel_path))\n        files = [(os.path.join(os.getcwd(), f.path), f.path) for f in entries if f.is_file()]  # (filepath, filename)\n        for f in entries:\n            if f.is_dir():\n                files += self.walk_dir(f.path)\n        return files\n\n    def run(self):\n        token = HfFolder.get_token()\n        if token is None:\n            print(""Not logged in"")\n            exit(1)\n\n        user, _ = self._api.whoami(token)\n        namespace = self.args.organization if self.args.organization is not None else user\n\n        local_path = os.path.abspath(self.args.path)\n        if os.path.isdir(local_path):\n            if self.args.filename is not None:\n                raise ValueError(""Cannot specify a filename override when uploading a folder."")\n            rel_path = os.path.basename(local_path)\n            files = self.walk_dir(rel_path)\n        elif os.path.isfile(local_path):\n            filename = self.args.filename if self.args.filename is not None else os.path.basename(local_path)\n            files = [(local_path, filename)]\n        else:\n            raise ValueError(""Not a valid file or directory: {}"".format(local_path))\n\n        if sys.platform == ""win32"":\n            files = [(filepath, filename.replace(os.sep, ""/"")) for filepath, filename in files]\n\n        if len(files) > UPLOAD_MAX_FILES:\n            print(\n                ""About to upload {} files to S3. This is probably wrong. Please filter files before uploading."".format(\n                    ANSI.bold(len(files))\n                )\n            )\n            exit(1)\n\n        for filepath, filename in files:\n            print(\n                ""About to upload file {} to S3 under filename {} and namespace {}"".format(\n                    ANSI.bold(filepath), ANSI.bold(filename), ANSI.bold(namespace)\n                )\n            )\n\n        choice = input(""Proceed? [Y/n] "").lower()\n        if not (choice == """" or choice == ""y"" or choice == ""yes""):\n            print(""Abort"")\n            exit()\n        print(ANSI.bold(""Uploading... This might take a while if files are large""))\n        for filepath, filename in files:\n            try:\n                access_url = self._api.presign_and_upload(\n                    token=token,\n                    filename=filename,\n                    filepath=filepath,\n                    organization=self.args.organization,\n                    file_types=self.file_types,\n                )\n            except HTTPError as e:\n                print(e)\n                print(ANSI.red(e.response.text))\n                exit(1)\n            print(""Your file now lives at:"")\n            print(access_url)\n'"
src/nlp/datasets/__init__.py,0,b''
src/nlp/metrics/__init__.py,0,b''
src/nlp/utils/__init__.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# flake8: noqa\n# Lint as: python3\n""""""Util import.""""""\n\nfrom .download_manager import DownloadManager, GenerateMode\nfrom .file_utils import (\n    HF_DATASETS_CACHE,\n    HF_METRICS_CACHE,\n    DownloadConfig,\n    cached_path,\n    hf_bucket_url,\n    is_remote_url,\n    is_tf_available,\n    is_torch_available,\n)\nfrom .mock_download_manager import MockDownloadManager\nfrom .py_utils import *\nfrom .tqdm_utils import *\nfrom .version import Version\n'"
src/nlp/utils/beam_utils.py,0,"b'import logging\nimport os\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom apache_beam.io import filebasedsink\nfrom apache_beam.io.filesystem import CompressionTypes\nfrom apache_beam.io.filesystems import FileSystems\nfrom apache_beam.io.iobase import Write\nfrom apache_beam.pipeline import Pipeline\nfrom apache_beam.transforms import PTransform\n\n\nCHUNK_SIZE = 2 << 20  # 2mb\nlogger = logging.getLogger(__name__)\n\n\nclass BeamPipeline(Pipeline):\n    """"""Wrapper over `apache_beam.pipeline.Pipeline` for convenience""""""\n\n    def is_local(self):\n        runner = self._options.get_all_options().get(""runner"")\n        return runner in [None, ""DirectRunner"", ""PortableRunner""]\n\n\ndef upload_local_to_remote(local_file_path, remote_file_path, force_upload=False):\n    """"""Use the Beam Filesystems to upload to a remote directory on gcs/s3/hdfs...""""""\n    fs = FileSystems\n    if fs.exists(remote_file_path):\n        if force_upload:\n            logger.info(""Remote path already exist: {}. Overwriting it as force_upload=True."".format(remote_file_path))\n        else:\n            logger.info(""Remote path already exist: {}. Skipping it as force_upload=False."".format(remote_file_path))\n            return\n    with fs.create(remote_file_path) as remote_file:\n        with open(local_file_path, ""rb"") as local_file:\n            chunk = local_file.read(CHUNK_SIZE)\n            while chunk:\n                remote_file.write(chunk)\n                chunk = local_file.read(CHUNK_SIZE)\n\n\ndef download_remote_to_local(remote_file_path, local_file_path, force_download=False):\n    """"""Use the Beam Filesystems to download from a remote directory on gcs/s3/hdfs...""""""\n    fs = FileSystems\n    if os.path.exists(local_file_path):\n        if force_download:\n            logger.info(""Local path already exist: {}. Overwriting it as force_upload=True."".format(remote_file_path))\n        else:\n            logger.info(""Local path already exist: {}. Skipping it as force_upload=False."".format(remote_file_path))\n            return\n    with fs.open(remote_file_path) as remote_file:\n        with open(local_file_path, ""wb"") as local_file:\n            chunk = remote_file.read(CHUNK_SIZE)\n            while chunk:\n                local_file.write(chunk)\n                chunk = remote_file.read(CHUNK_SIZE)\n\n\nclass WriteToParquet(PTransform):\n    """"""\n    From `apache_beam.io.parquetio.WriteToParquet`, but with a fix for the jira issue `BEAM-10022`.\n    Only the method `_flush_buffer` is different from the original implementation.\n\n        A ``PTransform`` for writing parquet files.\n\n        This ``PTransform`` is currently experimental. No backward-compatibility\n        guarantees.\n    """"""\n\n    def __init__(\n        self,\n        file_path_prefix,\n        schema,\n        row_group_buffer_size=64 * 1024 * 1024,\n        record_batch_size=1000,\n        codec=""none"",\n        use_deprecated_int96_timestamps=False,\n        file_name_suffix="""",\n        num_shards=0,\n        shard_name_template=None,\n        mime_type=""application/x-parquet"",\n    ):\n        """"""Initialize a WriteToParquet transform.\n        (from apache_beam.io.parquetio, only ._flush_buffer() is different)\n\n        Writes parquet files from a :class:`~apache_beam.pvalue.PCollection` of\n        records. Each record is a dictionary with keys of a string type that\n        represent column names. Schema must be specified like the example below.\n\n        .. testsetup::\n\n            from tempfile import NamedTemporaryFile\n            import glob\n            import os\n            import pyarrow\n\n            filename = NamedTemporaryFile(delete=False).name\n\n        .. testcode::\n\n            with beam.Pipeline() as p:\n                records = p | \'Read\' >> beam.Create(\n                        [{\'name\': \'foo\', \'age\': 10}, {\'name\': \'bar\', \'age\': 20}]\n                )\n                _ = records | \'Write\' >> beam.io.WriteToParquet(filename,\n                        pyarrow.schema(\n                                [(\'name\', pyarrow.binary()), (\'age\', pyarrow.int64())]\n                        )\n                )\n\n        .. testcleanup::\n\n            for output in glob.glob(\'{}*\'.format(filename)):\n                os.remove(output)\n\n        For more information on supported types and schema, please see the pyarrow\n        document.\n\n        Args:\n            file_path_prefix: The file path to write to. The files written will begin\n                with this prefix, followed by a shard identifier (see num_shards), and\n                end in a common extension, if given by file_name_suffix. In most cases,\n                only this argument is specified and num_shards, shard_name_template, and\n                file_name_suffix use default values.\n            schema: The schema to use, as type of ``pyarrow.Schema``.\n            row_group_buffer_size: The byte size of the row group buffer. Note that\n                this size is for uncompressed data on the memory and normally much\n                bigger than the actual row group size written to a file.\n            record_batch_size: The number of records in each record batch. Record\n                batch is a basic unit used for storing data in the row group buffer.\n                A higher record batch size implies low granularity on a row group buffer\n                size. For configuring a row group size based on the number of records,\n                set ``row_group_buffer_size`` to 1 and use ``record_batch_size`` to\n                adjust the value.\n            codec: The codec to use for block-level compression. Any string supported\n                by the pyarrow specification is accepted.\n            use_deprecated_int96_timestamps: Write nanosecond resolution timestamps to\n                INT96 Parquet format. Defaults to False.\n            file_name_suffix: Suffix for the files written.\n            num_shards: The number of files (shards) used for output. If not set, the\n                service will decide on the optimal number of shards.\n                Constraining the number of shards is likely to reduce\n                the performance of a pipeline.  Setting this value is not recommended\n                unless you require a specific number of output files.\n            shard_name_template: A template string containing placeholders for\n                the shard number and shard count. When constructing a filename for a\n                particular shard number, the upper-case letters \'S\' and \'N\' are\n                replaced with the 0-padded shard number and shard count respectively.\n                This argument can be \'\' in which case it behaves as if num_shards was\n                set to 1 and only one file will be generated. The default pattern used\n                is \'-SSSSS-of-NNNNN\' if None is passed as the shard_name_template.\n            mime_type: The MIME type to use for the produced files, if the filesystem\n                supports specifying MIME types.\n\n        Returns:\n            A WriteToParquet transform usable for writing.\n        """"""\n        super(WriteToParquet, self).__init__()\n        self._sink = _create_parquet_sink(\n            file_path_prefix,\n            schema,\n            codec,\n            row_group_buffer_size,\n            record_batch_size,\n            use_deprecated_int96_timestamps,\n            file_name_suffix,\n            num_shards,\n            shard_name_template,\n            mime_type,\n        )\n\n    def expand(self, pcoll):\n        return pcoll | Write(self._sink)\n\n    def display_data(self):\n        return {""sink_dd"": self._sink}\n\n\ndef _create_parquet_sink(\n    file_path_prefix,\n    schema,\n    codec,\n    row_group_buffer_size,\n    record_batch_size,\n    use_deprecated_int96_timestamps,\n    file_name_suffix,\n    num_shards,\n    shard_name_template,\n    mime_type,\n):\n    return _ParquetSink(\n        file_path_prefix,\n        schema,\n        codec,\n        row_group_buffer_size,\n        record_batch_size,\n        use_deprecated_int96_timestamps,\n        file_name_suffix,\n        num_shards,\n        shard_name_template,\n        mime_type,\n    )\n\n\nclass _ParquetSink(filebasedsink.FileBasedSink):\n    """"""A sink for parquet files.""""""\n\n    def __init__(\n        self,\n        file_path_prefix,\n        schema,\n        codec,\n        row_group_buffer_size,\n        record_batch_size,\n        use_deprecated_int96_timestamps,\n        file_name_suffix,\n        num_shards,\n        shard_name_template,\n        mime_type,\n    ):\n        super(_ParquetSink, self).__init__(\n            file_path_prefix,\n            file_name_suffix=file_name_suffix,\n            num_shards=num_shards,\n            shard_name_template=shard_name_template,\n            coder=None,\n            mime_type=mime_type,\n            # Compression happens at the block level using the supplied codec, and\n            # not at the file level.\n            compression_type=CompressionTypes.UNCOMPRESSED,\n        )\n        self._schema = schema\n        self._codec = codec\n        self._row_group_buffer_size = row_group_buffer_size\n        self._use_deprecated_int96_timestamps = use_deprecated_int96_timestamps\n        self._buffer = [[] for _ in range(len(schema.names))]\n        self._buffer_size = record_batch_size\n        self._record_batches = []\n        self._record_batches_byte_size = 0\n        self._file_handle = None\n\n    def open(self, temp_path):\n        self._file_handle = super(_ParquetSink, self).open(temp_path)\n        return pq.ParquetWriter(\n            self._file_handle,\n            self._schema,\n            compression=self._codec,\n            use_deprecated_int96_timestamps=self._use_deprecated_int96_timestamps,\n        )\n\n    def write_record(self, writer, value):\n        if len(self._buffer[0]) >= self._buffer_size:\n            self._flush_buffer()\n\n        if self._record_batches_byte_size >= self._row_group_buffer_size:\n            self._write_batches(writer)\n\n        # reorder the data in columnar format.\n        for i, n in enumerate(self._schema.names):\n            self._buffer[i].append(value[n])\n\n    def close(self, writer):\n        if len(self._buffer[0]) > 0:\n            self._flush_buffer()\n        if self._record_batches_byte_size > 0:\n            self._write_batches(writer)\n\n        writer.close()\n        if self._file_handle:\n            self._file_handle.close()\n            self._file_handle = None\n\n    def display_data(self):\n        res = super(_ParquetSink, self).display_data()\n        res[""codec""] = str(self._codec)\n        res[""schema""] = str(self._schema)\n        res[""row_group_buffer_size""] = str(self._row_group_buffer_size)\n        return res\n\n    def _write_batches(self, writer):\n        table = pa.Table.from_batches(self._record_batches)\n        self._record_batches = []\n        self._record_batches_byte_size = 0\n        writer.write_table(table)\n\n    def _flush_buffer(self):\n        arrays = [[] for _ in range(len(self._schema.names))]\n        for x, y in enumerate(self._buffer):\n            arrays[x] = pa.array(y, type=self._schema.types[x])\n            self._buffer[x] = []\n        rb = pa.RecordBatch.from_arrays(arrays, self._schema.names)\n        self._record_batches.append(rb)\n        size = 0\n        for x in arrays:\n            for b in x.buffers():\n                if b is not None:  # only this line is different\n                    size = size + b.size\n        self._record_batches_byte_size = self._record_batches_byte_size + size\n'"
src/nlp/utils/download_manager.py,1,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Download manager interface.""""""\n\nimport enum\nimport logging\nimport os\n\nfrom .file_utils import cached_path, get_from_cache, hash_url_to_filename\nfrom .info_utils import get_size_checksum_dict\nfrom .py_utils import flatten_nested, map_nested, size_str\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass GenerateMode(enum.Enum):\n    """"""`Enum` for how to treat pre-existing downloads and data.\n\n    The default mode is `REUSE_DATASET_IF_EXISTS`, which will reuse both\n    raw downloads and the prepared dataset if they exist.\n\n    The generations modes:\n\n    |                                    | Downloads | Dataset |\n    | -----------------------------------|-----------|---------|\n    | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |\n    | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |\n    | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |\n    """"""\n\n    REUSE_DATASET_IF_EXISTS = ""reuse_dataset_if_exists""\n    REUSE_CACHE_IF_EXISTS = ""reuse_cache_if_exists""\n    FORCE_REDOWNLOAD = ""force_redownload""\n\n\nclass DownloadManager(object):\n    def __init__(\n        self, dataset_name=None, data_dir=None, download_config=None,\n    ):\n        """"""Download manager constructor.\n\n        Args:\n            data_dir: can be used to specify a manual directory to get the files from.\n            cache_dir: `str`, path to directory where downloads are stored.\n            extract_dir: `str`, path to directory where artifacts are extracted.\n            dataset_name: `str`, name of dataset this instance will be used for. If\n                provided, downloads will contain which datasets they were used for.\n            force_download: `bool`, default to False. If True, always [re]download.\n        """"""\n        self._dataset_name = dataset_name\n        self._data_dir = data_dir\n        self._download_config = download_config\n        # To record what is being used: {url: {num_bytes: int, checksum: str}}\n        self._recorded_sizes_checksums = {}\n\n    @property\n    def manual_dir(self):\n        return self._data_dir\n\n    @property\n    def downloaded_size(self):\n        """"""Returns the total size of downloaded files.""""""\n        return sum(checksums_dict[""num_bytes""] for checksums_dict in self._recorded_sizes_checksums.values())\n\n    def ship_files_with_pipeline(self, downloaded_path_or_paths, pipeline):\n        """"""\n        Ship the files using Beam FileSystems to the pipeline temp dir.\n        """"""\n        from nlp.utils.beam_utils import upload_local_to_remote\n\n        remote_dir = pipeline._options.get_all_options().get(""temp_location"")\n        if remote_dir is None:\n            raise ValueError(""You need to specify \'temp_location\' in PipelineOptions to upload files"")\n\n        def upload(local_file_path):\n            remote_file_path = os.path.join(remote_dir, ""downloads"", os.path.basename(local_file_path))\n            logger.info(\n                ""Uploading {} ({}) to {}."".format(\n                    local_file_path, size_str(os.path.getsize(local_file_path)), remote_file_path\n                )\n            )\n            upload_local_to_remote(local_file_path, remote_file_path)\n            return remote_file_path\n\n        uploaded_path_or_paths = map_nested(lambda local_file_path: upload(local_file_path), downloaded_path_or_paths,)\n        return uploaded_path_or_paths\n\n    def _record_sizes_checksums(self, url_or_urls, downloaded_path_or_paths):\n        """"""Record size/checksum of downloaded files.""""""\n        flattened_urls_or_urls = flatten_nested(url_or_urls)\n        flattened_downloaded_path_or_paths = flatten_nested(downloaded_path_or_paths)\n        for url, path in zip(flattened_urls_or_urls, flattened_downloaded_path_or_paths):\n            self._recorded_sizes_checksums[url] = get_size_checksum_dict(path)\n\n    def download_custom(self, url_or_urls, custom_download):\n        """"""\n        Download given urls(s) by calling `custom_download`.\n\n        Args:\n            url_or_urls: url or `list`/`dict` of urls to download and extract. Each\n                url is a `str`.\n            custom_download: Callable with signature (src_url: str, dst_path: str) -> Any\n                as for example `tf.io.gfile.copy`, that lets you download from google storage\n\n        Returns:\n            downloaded_path(s): `str`, The downloaded paths matching the given input\n                url_or_urls.\n        """"""\n\n        def url_to_downloaded_path(url):\n            return os.path.join(self._download_config.cache_dir, hash_url_to_filename(url))\n\n        downloaded_path_or_paths = map_nested(url_to_downloaded_path, url_or_urls)\n        flattened_urls_or_urls = flatten_nested(url_or_urls)\n        flattened_downloaded_path_or_paths = flatten_nested(downloaded_path_or_paths)\n        for url, path in zip(flattened_urls_or_urls, flattened_downloaded_path_or_paths):\n            try:\n                get_from_cache(url, cache_dir=self._download_config.cache_dir, local_files_only=True)\n                cached = True\n            except FileNotFoundError:\n                cached = False\n            if not cached or self._download_config.force_download:\n                custom_download(url, path)\n                get_from_cache(url, cache_dir=self._download_config.cache_dir, local_files_only=True)\n        self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)\n        return downloaded_path_or_paths\n\n    def download(self, url_or_urls):\n        """"""Download given url(s).\n\n        Args:\n            url_or_urls: url or `list`/`dict` of urls to download and extract. Each\n                url is a `str`.\n\n        Returns:\n            downloaded_path(s): `str`, The downloaded paths matching the given input\n                url_or_urls.\n        """"""\n        downloaded_path_or_paths = map_nested(\n            lambda url: cached_path(url, download_config=self._download_config,), url_or_urls,\n        )\n        self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)\n        return downloaded_path_or_paths\n\n    def iter_archive(self, path):\n        """"""Returns iterator over files within archive.\n\n        Args:\n            path: path to archive.\n\n        Returns:\n            Generator yielding tuple (path_within_archive, file_obj).\n            File-Obj are opened in byte mode (io.BufferedReader)\n        """"""\n        logger.info(""Extracting archive at %s"", str(path))\n        extracted_path = self.extract(path)\n        if os.path.isfile(extracted_path):\n            with open(extracted_path, ""rb"") as file_obj:\n                yield (extracted_path, file_obj)\n\n        # We do this complex absolute/relative scheme to reproduce the API of iter_tar of tfds\n        for root, dirs, files in os.walk(extracted_path, topdown=False):\n            relative_dir_path = root.replace(os.path.abspath(extracted_path) + ""/"", """")\n            for name in files:\n                relative_file_path = os.path.join(relative_dir_path, name)\n                absolute_file_path = os.path.join(root, name)\n                with open(absolute_file_path, ""rb"") as file_obj:\n                    yield (relative_file_path, file_obj)\n\n    def extract(self, path_or_paths):\n        """"""Extract given path(s).\n\n        Args:\n            path_or_paths: path or `list`/`dict` of path of file to extract. Each\n                path is a `str`.\n\n        Returns:\n            extracted_path(s): `str`, The extracted paths matching the given input\n                path_or_paths.\n        """"""\n        return map_nested(\n            lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,\n        )\n\n    def download_and_extract(self, url_or_urls):\n        """"""Download and extract given url_or_urls.\n\n        Is roughly equivalent to:\n\n        ```\n        extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))\n        ```\n\n        Args:\n            url_or_urls: url or `list`/`dict` of urls to download and extract. Each\n                url is a `str`.\n\n        Returns:\n            extracted_path(s): `str`, extracted paths of given URL(s).\n        """"""\n        return self.extract(self.download(url_or_urls))\n\n    def get_recorded_sizes_checksums(self):\n        return self._recorded_sizes_checksums.copy()\n'"
src/nlp/utils/file_utils.py,3,"b'""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n""""""\n\nimport gzip\nimport json\nimport logging\nimport os\nimport shutil\nimport sys\nimport tarfile\nimport tempfile\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom hashlib import sha256\nfrom typing import Dict, Optional, Union\nfrom urllib.parse import urlparse\nfrom zipfile import ZipFile, is_zipfile\n\nimport requests\nfrom filelock import FileLock\nfrom tqdm.auto import tqdm\n\nfrom .. import __version__\n\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\ntry:\n    USE_TF = os.environ.get(""USE_TF"", ""AUTO"").upper()\n    USE_TORCH = os.environ.get(""USE_TORCH"", ""AUTO"").upper()\n    if USE_TORCH in (""1"", ""ON"", ""YES"", ""AUTO"") and USE_TF not in (""1"", ""ON"", ""YES""):\n        import torch\n\n        _torch_available = True  # pylint: disable=invalid-name\n        logger.info(""PyTorch version {} available."".format(torch.__version__))\n    else:\n        logger.info(""Disabling PyTorch because USE_TF is set"")\n        _torch_available = False\nexcept ImportError:\n    _torch_available = False  # pylint: disable=invalid-name\n\ntry:\n    USE_TF = os.environ.get(""USE_TF"", ""AUTO"").upper()\n    USE_TORCH = os.environ.get(""USE_TORCH"", ""AUTO"").upper()\n\n    if USE_TF in (""1"", ""ON"", ""YES"", ""AUTO"") and USE_TORCH not in (""1"", ""ON"", ""YES""):\n        import tensorflow as tf\n\n        assert hasattr(tf, ""__version__"") and int(tf.__version__[0]) >= 2\n        _tf_available = True  # pylint: disable=invalid-name\n        logger.info(""TensorFlow version {} available."".format(tf.__version__))\n    else:\n        logger.info(""Disabling Tensorflow because USE_TORCH is set"")\n        _tf_available = False\nexcept (ImportError, AssertionError):\n    _tf_available = False  # pylint: disable=invalid-name\n\n\nhf_cache_home = os.path.expanduser(\n    os.getenv(""HF_HOME"", os.path.join(os.getenv(""XDG_CACHE_HOME"", ""~/.cache""), ""huggingface""))\n)\ndefault_datasets_cache_path = os.path.join(hf_cache_home, ""datasets"")\ntry:\n    from pathlib import Path\n\n    HF_DATASETS_CACHE = Path(os.getenv(""HF_DATASETS_CACHE"", default_datasets_cache_path))\nexcept (AttributeError, ImportError):\n    HF_DATASETS_CACHE = os.getenv(os.getenv(""HF_DATASETS_CACHE"", default_datasets_cache_path))\n\nS3_DATASETS_BUCKET_PREFIX = ""https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets""\nCLOUDFRONT_DATASETS_DISTRIB_PREFIX = ""https://cdn-datasets.huggingface.co/nlp/datasets""\n\n\ndefault_metrics_cache_path = os.path.join(hf_cache_home, ""metrics"")\ntry:\n    from pathlib import Path\n\n    HF_METRICS_CACHE = Path(os.getenv(""HF_METRICS_CACHE"", default_metrics_cache_path))\nexcept (AttributeError, ImportError):\n    HF_METRICS_CACHE = os.getenv(os.getenv(""HF_METRICS_CACHE"", default_metrics_cache_path))\n\nS3_METRICS_BUCKET_PREFIX = ""https://s3.amazonaws.com/datasets.huggingface.co/nlp/metrics""\nCLOUDFRONT_METRICS_DISTRIB_PREFIX = ""https://cdn-datasets.huggingface.co/nlp/metric""\n\nINCOMPLETE_SUFFIX = "".incomplete""\n\n\ndef is_torch_available():\n    return _torch_available\n\n\ndef is_tf_available():\n    return _tf_available\n\n\ndef is_remote_url(url_or_filename):\n    parsed = urlparse(url_or_filename)\n    return parsed.scheme in (""http"", ""https"", ""s3"", ""gs"", ""hdfs"")\n\n\ndef hf_bucket_url(identifier: str, filename: str, use_cdn=False, dataset=True) -> str:\n    if dataset:\n        endpoint = CLOUDFRONT_DATASETS_DISTRIB_PREFIX if use_cdn else S3_DATASETS_BUCKET_PREFIX\n    else:\n        endpoint = CLOUDFRONT_METRICS_DISTRIB_PREFIX if use_cdn else S3_METRICS_BUCKET_PREFIX\n    return ""/"".join((endpoint, identifier, filename))\n\n\ndef hash_url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    If the url ends with .h5 (Keras HDF5 weights) adds \'.h5\' to the name\n    so that TF 2.0 can identify it as a HDF5 file\n    (see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n    """"""\n    url_bytes = url.encode(""utf-8"")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(""utf-8"")\n        etag_hash = sha256(etag_bytes)\n        filename += ""."" + etag_hash.hexdigest()\n\n    if url.endswith("".py""):\n        filename += "".py""\n\n    return filename\n\n\n@dataclass\nclass DownloadConfig:\n    """""" Configuration for our cached path manager\n    Args:\n        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n        force_download: if True, re-dowload the file even if it\'s already cached in the cache dir.\n        resume_download: if True, resume the download if incompletly recieved file is found.\n        user_agent: Optional string or dict that will be appended to the user-agent on remote requests.\n        extract_compressed_file: if True and the path point to a zip or tar file, extract the compressed\n            file in a folder along the archive.\n        force_extract: if True when extract_compressed_file is True and the archive was already extracted,\n            re-extract the archive and overide the folder where it was extracted.\n\n\n    """"""\n\n    cache_dir: Optional[Union[str, Path]] = None\n    force_download: bool = False\n    resume_download: bool = False\n    local_files_only: bool = False\n    proxies: Optional[Dict] = None\n    user_agent: Optional[str] = None\n    extract_compressed_file: bool = False\n    force_extract: bool = False\n\n\ndef cached_path(url_or_filename, download_config=None, **download_kwargs,) -> Optional[str]:\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n\n    Return:\n        Local path (string)\n\n    Raises:\n        FileNotFoundError: in case of non-recoverable file\n            (non-existent or no cache on disk)\n        ConnectionError: in case of unreachable url\n            and no cache on disk\n        ValueError: if it couldn\'t parse the url or filename correctly\n    """"""\n    if download_config is None:\n        download_config = DownloadConfig(**download_kwargs)\n\n    cache_dir = download_config.cache_dir or HF_DATASETS_CACHE\n    if isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n    if isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n\n    if is_remote_url(url_or_filename):\n        # URL, so get it from the cache (downloading if necessary)\n        output_path = get_from_cache(\n            url_or_filename,\n            cache_dir=cache_dir,\n            force_download=download_config.force_download,\n            proxies=download_config.proxies,\n            resume_download=download_config.resume_download,\n            user_agent=download_config.user_agent,\n            local_files_only=download_config.local_files_only,\n        )\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        output_path = url_or_filename\n    elif urlparse(url_or_filename).scheme == """":\n        # File, but it doesn\'t exist.\n        raise FileNotFoundError(""Local file {} doesn\'t exist"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n    if download_config.extract_compressed_file and output_path is not None:\n        if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path) and not is_gzip(output_path):\n            return output_path\n\n        # Path where we extract compressed archives\n        # We extract in the cache dir, and get the extracted path name by hashing the original path""\n        abs_output_path = os.path.abspath(output_path)\n        output_path_extracted = os.path.join(cache_dir, hash_url_to_filename(abs_output_path))\n\n        if (\n            os.path.isdir(output_path_extracted)\n            and os.listdir(output_path_extracted)\n            and not download_config.force_extract\n        ) or (os.path.isfile(output_path_extracted) and not download_config.force_extract):\n            return output_path_extracted\n\n        # Prevent parallel extractions\n        lock_path = output_path + "".lock""\n        with FileLock(lock_path):\n            shutil.rmtree(output_path_extracted, ignore_errors=True)\n            os.makedirs(output_path_extracted, exist_ok=True)\n            if is_zipfile(output_path):\n                with ZipFile(output_path, ""r"") as zip_file:\n                    zip_file.extractall(output_path_extracted)\n                    zip_file.close()\n            elif tarfile.is_tarfile(output_path):\n                tar_file = tarfile.open(output_path)\n                tar_file.extractall(output_path_extracted)\n                tar_file.close()\n            elif is_gzip(output_path):\n                os.rmdir(output_path_extracted)\n                with gzip.open(output_path, ""rb"") as gzip_file:\n                    with open(output_path_extracted, ""wb"") as extracted_file:\n                        shutil.copyfileobj(gzip_file, extracted_file)\n            else:\n                raise EnvironmentError(""Archive format of {} could not be identified"".format(output_path))\n\n        return output_path_extracted\n\n    return output_path\n\n\ndef http_get(url, temp_file, proxies=None, resume_size=0, user_agent=None, cookies=None):\n    ua = ""datasets/{}; python/{}"".format(__version__, sys.version.split()[0])\n    if is_torch_available():\n        ua += ""; torch/{}"".format(torch.__version__)\n    if is_tf_available():\n        ua += ""; tensorflow/{}"".format(tf.__version__)\n    if isinstance(user_agent, dict):\n        ua += ""; "" + ""; "".join(""{}/{}"".format(k, v) for k, v in user_agent.items())\n    elif isinstance(user_agent, str):\n        ua += ""; "" + user_agent\n    headers = {""user-agent"": ua}\n    if resume_size > 0:\n        headers[""Range""] = ""bytes=%d-"" % (resume_size,)\n    response = requests.get(url, stream=True, proxies=proxies, headers=headers, cookies=cookies)\n    if response.status_code == 416:  # Range not satisfiable\n        return\n    content_length = response.headers.get(""Content-Length"")\n    total = resume_size + int(content_length) if content_length is not None else None\n    progress = tqdm(\n        unit=""B"",\n        unit_scale=True,\n        total=total,\n        initial=resume_size,\n        desc=""Downloading"",\n        disable=bool(logger.getEffectiveLevel() == logging.NOTSET),\n    )\n    for chunk in response.iter_content(chunk_size=1024):\n        if chunk:  # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(\n    url,\n    cache_dir=None,\n    force_download=False,\n    proxies=None,\n    etag_timeout=10,\n    resume_download=False,\n    user_agent=None,\n    local_files_only=False,\n) -> Optional[str]:\n    """"""\n    Given a URL, look for the corresponding file in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n\n    Return:\n        Local path (string)\n\n    Raises:\n        FileNotFoundError: in case of non-recoverable file\n            (non-existent or no cache on disk)\n        ConnectionError: in case of unreachable url\n            and no cache on disk\n    """"""\n    if cache_dir is None:\n        cache_dir = HF_DATASETS_CACHE\n    if isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    original_url = url  # Some parameters may be added\n    connected = False\n    cookies = None\n    etag = None\n    if not local_files_only:\n        try:\n            response = requests.head(url, allow_redirects=True, proxies=proxies, timeout=etag_timeout)\n            if response.status_code == 200:  # ok\n                etag = response.headers.get(""ETag"")\n                for k, v in response.cookies.items():\n                    # In some edge cases, we need to get a confirmation token\n                    if k.startswith(""download_warning"") and ""drive.google.com"" in url:\n                        url += ""&confirm="" + v\n                        cookies = response.cookies\n                connected = True\n            # In some edge cases, head request returns 400 but the connection is actually ok\n            elif (response.status_code == 400 and ""firebasestorage.googleapis.com"" in url) or (\n                response.status_code == 405 and ""drive.google.com"" in url\n            ):\n                connected = True\n                logger.info(""Couldn\'t get ETag version for url {}"".format(url))\n        except (EnvironmentError, requests.exceptions.Timeout):\n            # not connected\n            pass\n\n    filename = hash_url_to_filename(original_url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    # connected == False = we don\'t have a connection, or url doesn\'t exist, or is otherwise inaccessible.\n    # try to get the last downloaded one\n    if not connected:\n        if os.path.exists(cache_path):\n            return cache_path\n        if local_files_only:\n            raise FileNotFoundError(\n                ""Cannot find the requested files in the cached path and outgoing traffic has been""\n                "" disabled. To enable model look-ups and downloads online, set \'local_files_only\'""\n                "" to False.""\n            )\n        raise ConnectionError(""Couldn\'t reach {}"".format(url))\n\n    # From now on, connected is True.\n    if os.path.exists(cache_path) and not force_download:\n        return cache_path\n\n    # Prevent parallel downloads of the same file with a lock.\n    lock_path = cache_path + "".lock""\n    with FileLock(lock_path):\n\n        if resume_download:\n            incomplete_path = cache_path + "".incomplete""\n\n            @contextmanager\n            def _resumable_file_manager():\n                with open(incomplete_path, ""a+b"") as f:\n                    yield f\n\n            temp_file_manager = _resumable_file_manager\n            if os.path.exists(incomplete_path):\n                resume_size = os.stat(incomplete_path).st_size\n            else:\n                resume_size = 0\n        else:\n            temp_file_manager = partial(tempfile.NamedTemporaryFile, dir=cache_dir, delete=False)\n            resume_size = 0\n\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with temp_file_manager() as temp_file:\n            logger.info(""%s not found in cache or force_download set to True, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            http_get(url, temp_file, proxies=proxies, resume_size=resume_size, user_agent=user_agent, cookies=cookies)\n\n        logger.info(""storing %s in cache at %s"", url, cache_path)\n        os.rename(temp_file.name, cache_path)\n\n        logger.info(""creating metadata file for %s"", cache_path)\n        meta = {""url"": url, ""etag"": etag}\n        meta_path = cache_path + "".json""\n        with open(meta_path, ""w"") as meta_file:\n            json.dump(meta, meta_file)\n\n    return cache_path\n\n\ndef is_gzip(path: str) -> bool:\n    """"""from https://stackoverflow.com/a/60634210""""""\n    with gzip.open(path, ""r"") as fh:\n        try:\n            fh.read(1)\n            return True\n        except OSError:\n            return False\n'"
src/nlp/utils/info_utils.py,0,"b'import logging\nimport os\nfrom hashlib import sha256\nfrom typing import Optional\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChecksumVerificationException(Exception):\n    """"""Exceptions during checksums verifications of downloaded files.""""""\n\n\nclass UnexpectedDownloadedFile(ChecksumVerificationException):\n    """"""Some downloaded files were not expected.""""""\n\n\nclass ExpectedMoreDownloadedFiles(ChecksumVerificationException):\n    """"""Some files were supposed to be downloaded but were not.""""""\n\n\nclass NonMatchingChecksumError(ChecksumVerificationException):\n    """"""The downloaded file checksum don\'t match the expected checksum.""""""\n\n\ndef verify_checksums(expected_checksums: Optional[dict], recorded_checksums: dict):\n    if expected_checksums is None:\n        logger.info(""Unable to verify checksums."")\n        return\n    if len(set(expected_checksums) - set(recorded_checksums)) > 0:\n        raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\n    if len(set(recorded_checksums) - set(expected_checksums)) > 0:\n        raise UnexpectedDownloadedFile(str(set(recorded_checksums) - set(expected_checksums)))\n    bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]\n    if len(bad_urls) > 0:\n        raise NonMatchingChecksumError(str(bad_urls))\n    logger.info(""All the checksums matched successfully."")\n\n\nclass SplitsVerificationException(Exception):\n    """"""Exceptions during splis verifications""""""\n\n\nclass UnexpectedSplits(SplitsVerificationException):\n    """"""The expected splits of the downloaded file is missing.""""""\n\n\nclass ExpectedMoreSplits(SplitsVerificationException):\n    """"""Some recorded splits are missing.""""""\n\n\nclass NonMatchingSplitsSizesError(SplitsVerificationException):\n    """"""The splits sizes don\'t match the expected splits sizes.""""""\n\n\ndef verify_splits(expected_splits: Optional[dict], recorded_splits: dict):\n    if expected_splits is None:\n        logger.info(""Unable to verify splits sizes."")\n        return\n    if len(set(expected_splits) - set(recorded_splits)) > 0:\n        raise ExpectedMoreSplits(str(set(expected_splits) - set(recorded_splits)))\n    if len(set(recorded_splits) - set(expected_splits)) > 0:\n        raise UnexpectedSplits(str(set(recorded_splits) - set(expected_splits)))\n    bad_splits = [\n        {""expected"": expected_splits[name], ""recorded"": recorded_splits[name]}\n        for name in expected_splits\n        if expected_splits[name].num_examples != recorded_splits[name].num_examples\n    ]\n    if len(bad_splits) > 0:\n        raise NonMatchingSplitsSizesError(str(bad_splits))\n    logger.info(""All the splits matched successfully."")\n\n\ndef get_size_checksum_dict(path: str) -> dict:\n    """"""Compute the file size and the sha256 checksum of a file""""""\n    m = sha256()\n    with open(path, ""rb"") as f:\n        for chunk in iter(lambda: f.read(4096), b""""):\n            m.update(chunk)\n    return {""num_bytes"": os.path.getsize(path), ""checksum"": m.hexdigest()}\n'"
src/nlp/utils/mock_download_manager.py,0,"b'# coding=utf-8\n# Copyright 2020 The TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Mock download manager interface.""""""\n\nimport logging\nimport os\nimport urllib.parse\n\nfrom .file_utils import cached_path, hf_bucket_url\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass MockDownloadManager(object):\n    dummy_file_name = ""dummy_data""\n\n    def __init__(self, dataset_name, config, version, cache_dir=None, is_local=False):\n        self.downloaded_size = 0\n        self.dataset_name = dataset_name\n        self.cache_dir = cache_dir\n        self.is_local = is_local\n        self.config = config\n\n        # TODO(PVP, QUENTIN) might need to make this more general\n        self.version_name = str(version.major) + ""."" + str(version.minor) + ""."" + str(version.patch)\n        # to be downloaded\n        self._dummy_file = None\n        self._bucket_url = None\n\n    @property\n    def dummy_file(self):\n        if self._dummy_file is None:\n            self._dummy_file = self.download_dummy_data()\n        return self._dummy_file\n\n    @property\n    def dummy_data_folder(self):\n        if self.config is not None:\n            # structure is dummy / config_name / version_name\n            return os.path.join(""dummy"", self.config.name, self.version_name)\n        # structure is dummy / version_name\n        return os.path.join(""dummy"", self.version_name)\n\n    @property\n    def dummy_zip_file(self):\n        return os.path.join(self.dummy_data_folder, ""dummy_data.zip"")\n\n    def download_dummy_data(self):\n        path_to_dummy_data_dir = (\n            self.local_path_to_dummy_data if self.is_local is True else self.aws_path_to_dummy_data\n        )\n\n        local_path = cached_path(\n            path_to_dummy_data_dir, cache_dir=self.cache_dir, extract_compressed_file=True, force_extract=True\n        )\n\n        return os.path.join(local_path, self.dummy_file_name)\n\n    @property\n    def local_path_to_dummy_data(self):\n        return os.path.join(""datasets"", self.dataset_name, self.dummy_zip_file)\n\n    @property\n    def aws_path_to_dummy_data(self):\n        if self._bucket_url is None:\n            self._bucket_url = hf_bucket_url(self.dataset_name, filename=self.dummy_zip_file)\n        return self._bucket_url\n\n    @property\n    def manual_dir(self):\n        # return full path if its a dir\n        if os.path.isdir(self.dummy_file):\n            return self.dummy_file\n        # else cut off path to file -> example `xsum`.\n        return ""/"".join(self.dummy_file.split(""/"")[:-1])\n\n    # this function has to be in the manager under this name so that testing works\n    def download_and_extract(self, data_url, *args):\n        if self.cache_dir is not None:\n            # dummy data is downloaded and tested\n            dummy_file = self.dummy_file\n        else:\n            # dummy data cannot be downloaded and only the path to dummy file is returned\n            dummy_file = self.dummy_file_name\n\n        # special case when data_url is a dict\n        if isinstance(data_url, dict):\n            return self.create_dummy_data_dict(dummy_file, data_url)\n        return dummy_file\n\n    # this function has to be in the manager under this name so that testing works\n    def download(self, data_url, *args):\n        return self.download_and_extract(data_url)\n\n    # this function has to be in the manager under this name so that testing works\n    def download_custom(self, data_url, custom_download):\n        return self.download_and_extract(data_url)\n\n    # this function has to be in the manager under this name so that testing works\n    def extract(self, path):\n        return path\n\n    # this function has to be in the manager under this name so that testing works\n    def get_recorded_sizes_checksums(self):\n        return {}\n\n    def create_dummy_data_dict(self, path_to_dummy_data, data_url):\n        dummy_data_dict = {}\n        for key, abs_path in data_url.items():\n            # we force the name of each key to be the last file / folder name of the url path\n            # if the url has arguments, we need to encode them with urllib.parse.quote_plus\n            if isinstance(abs_path, list):\n                value = [os.path.join(path_to_dummy_data, urllib.parse.quote_plus(x.split(""/"")[-1])) for x in abs_path]\n            else:\n                value = os.path.join(path_to_dummy_data, urllib.parse.quote_plus(abs_path.split(""/"")[-1]))\n            dummy_data_dict[key] = value\n\n        # make sure that values are unique\n        first_value = next(iter(dummy_data_dict.values()))\n        if isinstance(first_value, str) and len(set(dummy_data_dict.values())) < len(dummy_data_dict.values()):\n            # append key to value to make its name unique\n            dummy_data_dict = {key: value + key for key, value in dummy_data_dict.items()}\n\n        return dummy_data_dict\n'"
src/nlp/utils/py_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Some python utils function and classes.\n\n""""""\n\nimport contextlib\nimport functools\nimport itertools\nimport os\nfrom io import BytesIO as StringIO\nfrom shutil import disk_usage\nfrom types import CodeType\n\nimport dill\n\n\n# NOTE: When used on an instance method, the cache is shared across all\n# instances and IS NOT per-instance.\n# See\n# https://stackoverflow.com/questions/14946264/python-lru-cache-decorator-per-instance\n# For @property methods, use @memoized_property below.\nmemoize = functools.lru_cache\n\n\ndef convert_tuples_in_lists(data_struct):\n    # Could add support for more exotic data_struct, like OrderedDict\n    if isinstance(data_struct, dict):\n        return {k: convert_tuples_in_lists(v) for k, v in data_struct.items()}\n    else:\n        if isinstance(data_struct, (list, tuple)):\n            return [convert_tuples_in_lists(v) for v in data_struct]\n    return data_struct\n\n\ndef size_str(size_in_bytes):\n    """"""Returns a human readable size string.\n\n    If size_in_bytes is None, then returns ""Unknown size"".\n\n    For example `size_str(1.5 * nlp.units.GiB) == ""1.50 GiB""`.\n\n    Args:\n        size_in_bytes: `int` or `None`, the size, in bytes, that we want to\n            format as a human-readable size string.\n    """"""\n    if not size_in_bytes:\n        return ""Unknown size""\n\n    _NAME_LIST = [(""PiB"", 2 ** 50), (""TiB"", 2 ** 40), (""GiB"", 2 ** 30), (""MiB"", 2 ** 20), (""KiB"", 2 ** 10)]\n\n    size_in_bytes = float(size_in_bytes)\n    for (name, size_bytes) in _NAME_LIST:\n        value = size_in_bytes / size_bytes\n        if value >= 1.0:\n            return ""{:.2f} {}"".format(value, name)\n    return ""{} {}"".format(int(size_in_bytes), ""bytes"")\n\n\ndef is_notebook():\n    """"""Returns True if running in a notebook (Colab, Jupyter) environement.""""""\n    # Inspired from the tfdm autonotebook code\n    try:\n        from IPython import get_ipython  # pylint: disable=import-outside-toplevel,g-import-not-at-top\n\n        if ""IPKernelApp"" not in get_ipython().config:\n            return False  # Run in a IPython terminal\n    except:  # noqa: E722\n        return False\n    else:\n        return True\n\n\n@contextlib.contextmanager\ndef temporary_assignment(obj, attr, value):\n    """"""Temporarily assign obj.attr to value.""""""\n    original = getattr(obj, attr, None)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original)\n\n\ndef zip_dict(*dicts):\n    """"""Iterate over items of dictionaries grouped by their keys.""""""\n    for key in set(itertools.chain(*dicts)):  # set merge all keys\n        # Will raise KeyError if the dict don\'t have the same keys\n        yield key, tuple(d[key] for d in dicts)\n\n\nclass NonMutableDict(dict):\n    """"""Dict where keys can only be added but not modified.\n\n    Will raise an error if the user try to overwrite one key. The error message\n    can be customized during construction. It will be formatted using {key} for\n    the overwritten key.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        self._error_msg = kwargs.pop(""error_msg"", ""Try to overwrite existing key: {key}"",)\n        if kwargs:\n            raise ValueError(""NonMutableDict cannot be initialized with kwargs."")\n        super(NonMutableDict, self).__init__(*args, **kwargs)\n\n    def __setitem__(self, key, value):\n        if key in self:\n            raise ValueError(self._error_msg.format(key=key))\n        return super(NonMutableDict, self).__setitem__(key, value)\n\n    def update(self, other):\n        if any(k in self for k in other):\n            raise ValueError(self._error_msg.format(key=set(self) & set(other)))\n        return super(NonMutableDict, self).update(other)\n\n\nclass classproperty(property):  # pylint: disable=invalid-name\n    """"""Descriptor to be used as decorator for @classmethods.""""""\n\n    def __get__(self, obj, objtype=None):\n        return self.fget.__get__(None, objtype)()\n\n\nclass memoized_property(property):  # pylint: disable=invalid-name\n    """"""Descriptor that mimics @property but caches output in member variable.""""""\n\n    def __get__(self, obj, objtype=None):\n        # See https://docs.python.org/3/howto/descriptor.html#properties\n        if obj is None:\n            return self\n        if self.fget is None:\n            raise AttributeError(""unreadable attribute"")\n        attr = ""__cached_"" + self.fget.__name__\n        cached = getattr(obj, attr, None)\n        if cached is None:\n            cached = self.fget(obj)\n            setattr(obj, attr, cached)\n        return cached\n\n\ndef map_nested(function, data_struct, dict_only=False, map_tuple=False):\n    """"""Apply a function recursively to each element of a nested data struct.""""""\n\n    # Could add support for more exotic data_struct, like OrderedDict\n    if isinstance(data_struct, dict):\n        return {k: map_nested(function, v, dict_only, map_tuple) for k, v in data_struct.items()}\n    elif not dict_only:\n        types = [list]\n        if map_tuple:\n            types.append(tuple)\n        if isinstance(data_struct, tuple(types)):\n            mapped = [map_nested(function, v, dict_only, map_tuple) for v in data_struct]\n            if isinstance(data_struct, list):\n                return mapped\n            else:\n                return tuple(mapped)\n    # Singleton\n    return function(data_struct)\n\n\ndef zip_nested(arg0, *args, **kwargs):\n    """"""Zip data struct together and return a data struct with the same shape.""""""\n    # Python 2 do not support kwargs only arguments\n    dict_only = kwargs.pop(""dict_only"", False)\n    assert not kwargs\n\n    # Could add support for more exotic data_struct, like OrderedDict\n    if isinstance(arg0, dict):\n        return {k: zip_nested(*a, dict_only=dict_only) for k, a in zip_dict(arg0, *args)}\n    elif not dict_only:\n        if isinstance(arg0, list):\n            return [zip_nested(*a, dict_only=dict_only) for a in zip(arg0, *args)]\n    # Singleton\n    return (arg0,) + args\n\n\ndef flatten_nest_dict(d):\n    """"""Return the dict with all nested keys flattened joined with \'/\'.""""""\n    # Use NonMutableDict to ensure there is no collision between features keys\n    flat_dict = NonMutableDict()\n    for k, v in d.items():\n        if isinstance(v, dict):\n            flat_dict.update({""{}/{}"".format(k, k2): v2 for k2, v2 in flatten_nest_dict(v).items()})\n        else:\n            flat_dict[k] = v\n    return flat_dict\n\n\ndef flatten_nested(data_struct):\n    """"""Flatten data struct of obj or `list`/`dict` of obj""""""\n    if isinstance(data_struct, dict):\n        data_struct = list(flatten_nest_dict(data_struct).values())\n        if data_struct and isinstance(data_struct[0], (list, tuple)):\n            data_struct = [x for sublist in data_struct for x in sublist]\n    if isinstance(data_struct, (list, tuple)):\n        return data_struct\n    # Singleton\n    return [data_struct]\n\n\ndef nlp_dir():\n    """"""Path to nlp directory.""""""\n    return os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\n\n\nclass abstractclassmethod(classmethod):  # pylint: disable=invalid-name\n    """"""Decorate a method to mark it as an abstract @classmethod.""""""\n\n    __isabstractmethod__ = True\n\n    def __init__(self, fn):\n        fn.__isabstractmethod__ = True\n        super(abstractclassmethod, self).__init__(fn)\n\n\ndef get_nlp_path(relative_path):\n    """"""Returns absolute path to file given path relative to nlp root.""""""\n    path = os.path.join(nlp_dir(), relative_path)\n    return path\n\n\ndef has_sufficient_disk_space(needed_bytes, directory="".""):\n    try:\n        free_bytes = disk_usage(os.path.abspath(directory)).free\n    except OSError:\n        return True\n    return needed_bytes < free_bytes\n\n\nclass Pickler(dill.Pickler):\n    """"""Same Pickler as the one from dill, but improved for notebooks and shells""""""\n\n    dispatch = dill._dill.MetaCatchingDict(dill.Pickler.dispatch.copy())\n\n\ndef dump(obj, file):\n    """"""pickle an object to a file""""""\n    Pickler(file).dump(obj)\n    return\n\n\ndef dumps(obj):\n    """"""pickle an object to a string""""""\n    file = StringIO()\n    dump(obj, file)\n    return file.getvalue()\n\n\ndef pklregister(t):\n    def proxy(func):\n        Pickler.dispatch[t] = func\n        return func\n\n    return proxy\n\n\n@pklregister(CodeType)\ndef save_code(pickler, obj):\n    """"""\n    From dill._dill.save_code\n    This is a modified version that removes the origin (filename + line no.)\n    of functions created in notebooks or shells for example.\n    """"""\n    dill._dill.log.info(""Co: %s"" % obj)\n    # Filenames of functions created in notebooks or shells start with \'<\'\n    # ex: <ipython-input-13-9ed2afe61d25> for ipython, and <stdin> for shell\n    # Only those two lines are different from the original implementation:\n    co_filename = """" if obj.co_filename.startswith(""<"") else obj.co_filename\n    co_firstlineno = 1 if obj.co_filename.startswith(""<"") else obj.co_firstlineno\n    # The rest is the same as in the original dill implementation\n    if dill._dill.PY3:\n        if hasattr(obj, ""co_posonlyargcount""):\n            args = (\n                obj.co_argcount,\n                obj.co_posonlyargcount,\n                obj.co_kwonlyargcount,\n                obj.co_nlocals,\n                obj.co_stacksize,\n                obj.co_flags,\n                obj.co_code,\n                obj.co_consts,\n                obj.co_names,\n                obj.co_varnames,\n                co_filename,\n                obj.co_name,\n                co_firstlineno,\n                obj.co_lnotab,\n                obj.co_freevars,\n                obj.co_cellvars,\n            )\n        else:\n            args = (\n                obj.co_argcount,\n                obj.co_kwonlyargcount,\n                obj.co_nlocals,\n                obj.co_stacksize,\n                obj.co_flags,\n                obj.co_code,\n                obj.co_consts,\n                obj.co_names,\n                obj.co_varnames,\n                co_filename,\n                obj.co_name,\n                co_firstlineno,\n                obj.co_lnotab,\n                obj.co_freevars,\n                obj.co_cellvars,\n            )\n    else:\n        args = (\n            obj.co_argcount,\n            obj.co_nlocals,\n            obj.co_stacksize,\n            obj.co_flags,\n            obj.co_code,\n            obj.co_consts,\n            obj.co_names,\n            obj.co_varnames,\n            co_filename,\n            obj.co_name,\n            co_firstlineno,\n            obj.co_lnotab,\n            obj.co_freevars,\n            obj.co_cellvars,\n        )\n    pickler.save_reduce(CodeType, args, obj=obj)\n    dill._dill.log.info(""# Co"")\n    return\n'"
src/nlp/utils/tqdm_utils.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Wrapper around tqdm.\n""""""\n\nimport contextlib\n\nfrom tqdm import auto as tqdm_lib\n\n\nclass EmptyTqdm(object):\n    """"""Dummy tqdm which doesn\'t do anything.""""""\n\n    def __init__(self, *args, **kwargs):  # pylint: disable=unused-argument\n        self._iterator = args[0] if args else None\n\n    def __iter__(self):\n        return iter(self._iterator)\n\n    def __getattr__(self, _):\n        """"""Return empty function.""""""\n\n        def empty_fn(*args, **kwargs):  # pylint: disable=unused-argument\n            return\n\n        return empty_fn\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        return\n\n\n_active = True\n\n\ndef tqdm(*args, **kwargs):\n    if _active:\n        return tqdm_lib.tqdm(*args, **kwargs)\n    else:\n        return EmptyTqdm(*args, **kwargs)\n\n\ndef async_tqdm(*args, **kwargs):\n    if _active:\n        return _async_tqdm(*args, **kwargs)\n    else:\n        return EmptyTqdm(*args, **kwargs)\n\n\ndef disable_progress_bar():\n    """"""Disabled Tqdm progress bar.\n\n    Usage:\n\n    nlp.disable_progress_bar()\n    """"""\n    # Replace tqdm\n    global _active\n    _active = False\n\n\n@contextlib.contextmanager\ndef _async_tqdm(*args, **kwargs):\n    """"""Wrapper around Tqdm which can be updated in threads.\n\n    Usage:\n\n    ```\n    with utils.async_tqdm(...) as pbar:\n        # pbar can then be modified inside a thread\n        # pbar.update_total(3)\n        # pbar.update()\n    ```\n\n    Args:\n        *args: args of tqdm\n        **kwargs: kwargs of tqdm\n\n    Yields:\n        pbar: Async pbar which can be shared between threads.\n    """"""\n    with tqdm_lib.tqdm(*args, **kwargs) as pbar:\n        pbar = _TqdmPbarAsync(pbar)\n        yield pbar\n        pbar.clear()  # pop pbar from the active list of pbar\n        print()  # Avoid the next log to overlapp with the bar\n\n\nclass _TqdmPbarAsync(object):\n    """"""Wrapper around Tqdm pbar which be shared between thread.""""""\n\n    _tqdm_bars = []\n\n    def __init__(self, pbar):\n        self._lock = tqdm_lib.tqdm.get_lock()\n        self._pbar = pbar\n        self._tqdm_bars.append(pbar)\n\n    def update_total(self, n=1):\n        """"""Increment total pbar value.""""""\n        with self._lock:\n            self._pbar.total += n\n            self.refresh()\n\n    def update(self, n=1):\n        """"""Increment current value.""""""\n        with self._lock:\n            self._pbar.update(n)\n            self.refresh()\n\n    def refresh(self):\n        """"""Refresh all.""""""\n        for pbar in self._tqdm_bars:\n            pbar.refresh()\n\n    def clear(self):\n        """"""Remove the tqdm pbar from the update.""""""\n        self._tqdm_bars.pop()\n'"
src/nlp/utils/version.py,0,"b'# coding=utf-8\n# Copyright 2020 The HuggingFace NLP Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Version utils.""""""\n\nimport re\nfrom dataclasses import dataclass\n\n\n_VERSION_TMPL = r""^(?P<major>{v})"" r""\\.(?P<minor>{v})"" r""\\.(?P<patch>{v})$""\n_VERSION_WILDCARD_REG = re.compile(_VERSION_TMPL.format(v=r""\\d+|\\*""))\n_VERSION_RESOLVED_REG = re.compile(_VERSION_TMPL.format(v=r""\\d+""))\n\n\n@dataclass()\nclass Version:\n    """""" Dataset version MAJOR.MINOR.PATCH.\n        Args:\n            version_str: string. Eg: ""1.2.3"".\n            description: string, a description of what is new in this version.\n            nlp_version_to_prepare: string, defaults to None. If set, indicates that\n                current version cannot be used to `download_and_prepare` the\n                dataset, but that at version {nlp_version_to_prepare} should be\n                used instead.\n    """"""\n\n    version_str: str\n    description: str = None\n    nlp_version_to_prepare: str = None\n    major: str = None\n    minor: str = None\n    patch: str = None\n\n    def __post_init__(self):\n        self.major, self.minor, self.patch = _str_to_version(self.version_str)\n\n    def __repr__(self):\n        return ""{}.{}.{}"".format(*self.tuple)\n\n    @property\n    def tuple(self):\n        return self.major, self.minor, self.patch\n\n    def _validate_operand(self, other):\n        if isinstance(other, str):\n            return Version(other)\n        elif isinstance(other, Version):\n            return other\n        raise AssertionError(""{} (type {}) cannot be compared to version."".format(other, type(other)))\n\n    def __eq__(self, other):\n        other = self._validate_operand(other)\n        return self.tuple == other.tuple\n\n    def __ne__(self, other):\n        other = self._validate_operand(other)\n        return self.tuple != other.tuple\n\n    def __lt__(self, other):\n        other = self._validate_operand(other)\n        return self.tuple < other.tuple\n\n    def __le__(self, other):\n        other = self._validate_operand(other)\n        return self.tuple <= other.tuple\n\n    def __gt__(self, other):\n        other = self._validate_operand(other)\n        return self.tuple > other.tuple\n\n    def __ge__(self, other):\n        other = self._validate_operand(other)\n        return self.tuple >= other.tuple\n\n    def match(self, other_version):\n        """"""Returns True if other_version matches.\n\n        Args:\n            other_version: string, of the form ""x[.y[.x]]"" where {x,y,z} can be a\n                number or a wildcard.\n        """"""\n        major, minor, patch = _str_to_version(other_version, allow_wildcard=True)\n        return major in [self.major, ""*""] and minor in [self.minor, ""*""] and patch in [self.patch, ""*""]\n\n    @classmethod\n    def from_dict(cls, dic):\n        return cls(**dic)\n\n\ndef _str_to_version(version_str, allow_wildcard=False):\n    """"""Return the tuple (major, minor, patch) version extracted from the str.""""""\n    reg = _VERSION_WILDCARD_REG if allow_wildcard else _VERSION_RESOLVED_REG\n    res = reg.match(version_str)\n    if not res:\n        msg = ""Invalid version \'{}\'. Format should be x.y.z"".format(version_str)\n        if allow_wildcard:\n            msg += "" with {x,y,z} being digits or wildcard.""\n        else:\n            msg += "" with {x,y,z} being digits.""\n        raise ValueError(msg)\n    return tuple(v if v == ""*"" else int(v) for v in [res.group(""major""), res.group(""minor""), res.group(""patch"")])\n'"
