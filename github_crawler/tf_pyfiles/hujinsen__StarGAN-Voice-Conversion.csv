file_path,api_count,code
convert.py,0,"b""import argparse\nimport os\nimport numpy as np\n\nfrom model import StarGANVC\nfrom preprocess import *\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom utility import *\n\n#get all speaker\nall_speaker = get_speakers(trainset='./data/fourspeakers')\nlabel_enc = LabelEncoder()\nlabel_enc.fit(all_speaker)\n\n\ndef conversion(model_dir, test_dir, output_dir, source, target):\n    if not os.path.exists(model_dir) or not os.path.exists(test_dir):\n        raise Exception('model dir or test dir not exist!')\n    model = StarGANVC(num_features=FEATURE_DIM, mode='test')\n\n    model.load(filepath=os.path.join(model_dir, MODEL_NAME))\n    #f'./data/fourspeakers_test/{source}/*.wav'\n    p = os.path.join(test_dir, f'{source}/*.wav')\n    tempfiles = glob.glob(p)\n\n    normlizer = Normalizer()\n\n    for one_file in tempfiles:\n        _, speaker, name = os.path.normpath(one_file).rsplit(os.sep, maxsplit=2)\n        # print(speaker, name)\n        wav_, fs = librosa.load(one_file, sr=SAMPLE_RATE, mono=True, dtype=np.float64)\n        wav, pad_length = pad_wav_to_get_fixed_frames(wav_, frames=FRAMES)\n\n        f0, timeaxis = pyworld.harvest(wav, fs, f0_floor=71.0, f0_ceil=500.0)\n\n        #CheapTrick harmonic spectral envelope estimation algorithm.\n        sp = pyworld.cheaptrick(wav, f0, timeaxis, fs, fft_size=FFTSIZE)\n\n        #D4C aperiodicity estimation algorithm.\n        ap = pyworld.d4c(wav, f0, timeaxis, fs, fft_size=FFTSIZE)\n        #feature reduction\n        coded_sp = pyworld.code_spectral_envelope(sp, fs, FEATURE_DIM)\n\n        coded_sps_mean = np.mean(coded_sp, axis=0, dtype=np.float64, keepdims=True)\n        coded_sps_std = np.std(coded_sp, axis=0, dtype=np.float64, keepdims=True)\n        #normalize\n        # coded_sp = (coded_sp - coded_sps_mean) / coded_sps_std\n        # print(coded_sp.shape, f0.shape, ap.shape)\n\n        #one audio file to multiple slices(that's one_test_sample),every slice is an input\n        one_test_sample = []\n        csp_transpose = coded_sp.T  #36x512 36x128...\n        for i in range(0, csp_transpose.shape[1] - FRAMES + 1, FRAMES):\n            t = csp_transpose[:, i:i + FRAMES]\n            #normalize t\n            t = normlizer.forward_process(t, speaker)\n            t = np.reshape(t, [t.shape[0], t.shape[1], 1])\n            one_test_sample.append(t)\n        # print(f'{len(one_test_sample)} slices appended!')\n\n        #generate target label (one-hot vector)\n        one_test_sample_label = np.zeros([len(one_test_sample), len(all_speaker)])\n        temp_index = label_enc.transform([target])[0]\n        one_test_sample_label[:, temp_index] = 1\n\n        generated_results = model.test(one_test_sample, one_test_sample_label)\n\n        reshpaped_res = []\n        for one in generated_results:\n            t = np.reshape(one, [one.shape[0], one.shape[1]])\n\n            t = normlizer.backward_process(t, target)\n            reshpaped_res.append(t)\n        #collect the generated slices, and concate the array to be a whole representation of the whole audio\n        c = []\n        for one_slice in reshpaped_res:\n            one_slice = np.ascontiguousarray(one_slice.T, dtype=np.float64)\n            # one_slice = one_slice * coded_sps_std + coded_sps_mean\n\n            # print(f'one_slice : {one_slice.shape}')\n            decoded_sp = pyworld.decode_spectral_envelope(one_slice, SAMPLE_RATE, fft_size=FFTSIZE)\n            # print(f'decoded_sp shape: {decoded_sp.shape}')\n            c.append(decoded_sp)\n\n        concated = np.concatenate((c), axis=0)\n        # print(f'concated shape: {concated.shape}')\n        #f0 convert\n        f0 = normlizer.pitch_conversion(f0, speaker, target)\n\n        synwav = pyworld.synthesize(f0, concated, ap, fs)\n        # print(f'origin wav:{len(wav_)} paded wav:{len(wav)} synthesize wav:{len(synwav)}')\n\n        #remove synthesized wav paded length\n        synwav = synwav[:-pad_length]\n\n        #save synthesized wav to file\n        wavname = f'{speaker}-{target}+{name}'\n        wavpath = f'{output_dir}/wavs'\n        if not os.path.exists(wavpath):\n            os.makedirs(wavpath, exist_ok=True)\n        librosa.output.write_wav(f'{wavpath}/{wavname}', synwav, sr=fs)\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='Convert voices using pre-trained CycleGAN model.')\n\n    model_dir = './out/90_2018-10-17-22-58/model/'\n    test_dir = './data/fourspeakers_test/'\n    source_speaker = 'SF1'\n    target_speaker = 'TM1'\n    output_dir = './converted_voices'\n\n    parser.add_argument('--model_dir', type=str, help='Directory for the pre-trained model.', default=model_dir)\n    parser.add_argument('--test_dir', type=str, help='Directory for the voices for conversion.', default=test_dir)\n    parser.add_argument('--output_dir', type=str, help='Directory for the converted voices.', default=output_dir)\n    parser.add_argument('--source_speaker', type=str, help='source_speaker', default=source_speaker)\n    parser.add_argument('--target_speaker', type=str, help='target_speaker', default=target_speaker)\n\n    argv = parser.parse_args()\n\n    model_dir = argv.model_dir\n    test_dir = argv.test_dir\n    output_dir = argv.output_dir\n    source_speaker = argv.source_speaker\n    target_speaker = argv.target_speaker\n\n    conversion(model_dir = model_dir,\\\n     test_dir = test_dir, output_dir = output_dir, source=source_speaker, target=target_speaker)\n"""
download.py,0,"b'import os\nimport random\nimport zipfile\nimport argparse\nimport zipfile\nimport urllib.request as req\nimport ssl\nfrom threading import Thread\nfrom queue import SimpleQueue as Queue\n\n\ndef unzip(zip_filepath, dest_dir=\'./data\'):\n    with zipfile.ZipFile(zip_filepath) as zf:\n        zf.extractall(dest_dir)\n    print(""Extraction complete!"")\n\n\ndef download_vcc2016():\n    datalink = ""https://datashare.is.ed.ac.uk/bitstream/handle/10283/2211/""\n    data_files = [\'vcc2016_training.zip\', \'evaluation_all.zip\']\n\n    if os.path.exists(data_files[0]) or os.path.exists(data_files[1]):\n        print(""File already exists!"")\n        return\n\n    trainset = f\'{datalink}{data_files[0]}\'\n    evalset = f\'{datalink}{data_files[1]}\'\n\n    print(\'Start download dataset...\')\n\n    th = Thread(target=download_file, args=[trainset])\n    th.start()\n    download_file(evalset)\n    th.join()\n\n    unzip(data_files[0])\n    unzip(data_files[1])\n\n    print(\'Finish download dataset...\')\n\ndef download_file(url: str, out_path: str = None, buffer_size: int = 10*(1024**2)):\n    data = Queue()\n    def _download():\n        b = data.get()\n        with open(out_path or url.split(\'/\')[-1], \'wb\') as o:\n            while b:\n                o.write(b)\n                b = data.get()\n    \n    s = ssl.SSLContext()\n    f = req.urlopen(url, context=s)\n    th = Thread(target=_download)\n    th.start()\n    b = f.read(buffer_size)\n    while b:\n        data.put(b)\n        b = f.read(buffer_size)\n    data.put([])\n    th.join()\n\ndef create_dirs(trainset: str = \'./data/fourspeakers\', testset: str = \'./data/fourspeakers_test\'):\n    \'\'\'create train test dirs\'\'\'\n    if not os.path.exists(trainset):\n        print(f\'create train set dir {trainset}\')\n        os.makedirs(trainset, exist_ok=True)\n\n    if not os.path.exists(testset):\n        print(f\'create test set dir {testset}\')\n        os.makedirs(testset, exist_ok=True)\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(description=\'Download  voice conversion datasets.\')\n\n    datasets_default = \'vcc2016\'\n    train_dir = \'./data/fourspeakers\'\n    test_dir = \'./data/fourspeakers_test\'\n    parser.add_argument(\'--datasets\', type=str, help=\'Datasets available: vcc2016\', default=datasets_default)\n\n    parser.add_argument(\'--train_dir\', type=str, help=\'trainset directory\', default=train_dir)\n    parser.add_argument(\'--test_dir\', type=str, help=\'testset directory\', default=test_dir)\n\n    argv = parser.parse_args()\n\n    datasets = argv.datasets\n    create_dirs(train_dir, test_dir)\n\n    if datasets == \'vcc2016\' or datasets == \'VCC2016\':\n        download_vcc2016()\n    else:\n        print(\'Dataset not available.\')\n'"
model.py,45,"b'import os\nimport tensorflow as tf\nfrom module import discriminator, generator_gatedcnn, domain_classifier\nfrom datetime import datetime\nimport numpy as np\nfrom preprocess import *\n\n\nclass StarGANVC(object):\n\n    def __init__(self,\n                 num_features,\n                 frames=FRAMES,\n                 batchsize=8,\n                 discriminator=discriminator,\n                 generator=generator_gatedcnn,\n                 classifier=domain_classifier,\n                 mode=\'train\',\n                 log_dir=\'./log\'):\n        self.num_features = num_features\n        self.batchsize = batchsize\n        self.input_shape = [None, num_features, frames, 1]\n        self.label_shape = [None, SPEAKERS_NUM]\n\n        self.mode = mode\n        self.log_dir = log_dir\n\n        self.discriminator = discriminator\n        self.generator = generator_gatedcnn\n        self.classifier = classifier\n\n        self.build_model()\n\n        self.saver = tf.train.Saver()\n        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n        self.sess.run(tf.global_variables_initializer())\n\n        if self.mode == \'train\':\n            self.train_step = 0\n            now = datetime.now()\n            self.log_dir = os.path.join(log_dir, now.strftime(\'%Y%m%d-%H%M%S\'))\n            self.writer = tf.summary.FileWriter(self.log_dir, tf.get_default_graph())\n            self.generator_summaries, self.discriminator_summaries, self.domain_classifier_summaries = self.summary()\n\n    def build_model(self):\n        # Placeholders for real training samples\n        self.input_real = tf.placeholder(tf.float32, self.input_shape, name=\'input_real\')\n        self.target_real = tf.placeholder(tf.float32, self.input_shape, name=\'target_real\')\n\n        self.source_label = tf.placeholder(tf.float32, self.label_shape, name=\'source_label\')\n        self.target_label = tf.placeholder(tf.float32, self.label_shape, name=\'target_label\')\n        \n        self.target_label_reshaped = tf.placeholder(tf.float32, [None, 1, 1, SPEAKERS_NUM], name=\'reshaped_label_for_classifier\')\n        \n        self.generated_forward = self.generator(self.input_real, self.target_label, reuse=False, scope_name=\'generator\')\n        self.generated_back = self.generator(self.generated_forward, self.source_label, reuse=True, scope_name=\'generator\')\n\n\n        #============================Domain classify loss=============================\n        self.domain_out_real = self.classifier(self.target_real, reuse=False, scope_name=\'classifier\')\n        self.domain_real_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.target_label_reshaped, logits=self.domain_out_real))\n\n\n        # ============================Generator loss================================\n        #Cycle loss\n        self.cycle_loss = tf.reduce_mean(tf.abs(self.input_real - self.generated_back))\n\n        #Identity loss\n        self.identity_map = self.generator(self.input_real, self.source_label, reuse=True, scope_name=\'generator\')\n        self.identity_loss = tf.reduce_mean(tf.abs(self.input_real - self.identity_map))\n        self.discrimination_real = self.discriminator(self.target_real, self.target_label, reuse=False, scope_name=\'discriminator\')\n\n        #combine discriminator and generator\n        self.discirmination = self.discriminator(self.generated_forward, self.target_label, reuse=True, scope_name=\'discriminator\')\n        self.generator_loss = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(self.discirmination), logits=self.discirmination))\n\n        # Place holder for lambda_cycle and lambda_identity\n        self.lambda_cycle = tf.placeholder(tf.float32, None, name=\'lambda_cycle\')\n        self.lambda_identity = tf.placeholder(tf.float32, None, name=\'lambda_identity\')\n        self.lambda_classifier = tf.placeholder(tf.float32, None, name=\'lambda_classifier\')\n\n        self.generator_loss_all = self.generator_loss + self.lambda_cycle * self.cycle_loss + \\\n                                self.lambda_identity * self.identity_loss +\\\n                                 self.lambda_classifier * self.domain_real_loss\n\n\n        # =========================Discriminator loss========================\n        self.discirmination_fake = self.discriminator(self.generated_forward, self.target_label, reuse=True, scope_name=\'discriminator\')\n        self.discrimination_real_loss = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(self.discrimination_real), logits=self.discrimination_real))\n\n        self.discrimination_fake_loss = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(self.discirmination_fake), logits=self.discirmination_fake))\n\n        # calculate `x_hat`\n        epsilon = tf.random_uniform((self.batchsize, 1, 1, 1), 0.0, 1.0)\n        x_hat = epsilon * self.generated_forward + (1.0 - epsilon) * self.input_real\n\n        # gradient penalty\n        gradients = tf.gradients(self.discriminator(x_hat, self.target_label, reuse=True, scope_name=\'discriminator\'), [x_hat])\n        _gradient_penalty = 10.0 * tf.square(tf.norm(gradients[0], ord=2) - 1.0)\n\n        self.domain_out_fake = self.classifier(self.generated_forward, reuse=True, scope_name=\'classifier\')\n        self.domain_fake_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.target_label_reshaped, logits=self.domain_out_fake))\n        self.discrimator_loss = self.discrimination_fake_loss + self.discrimination_real_loss + _gradient_penalty + self.domain_fake_loss\n\n\n        #================================================================================\n        # Categorize variables because we have to optimize the three sets of the variables separately\n        trainable_variables = tf.trainable_variables()\n        self.discriminator_vars = [var for var in trainable_variables if \'discriminator\' in var.name]\n        self.generator_vars = [var for var in trainable_variables if \'generator\' in var.name]\n        self.classifier_vars = [var for var in trainable_variables if \'classifier\' in var.name]\n\n        #optimizer\n        self.generator_learning_rate = tf.placeholder(tf.float32, None, name=\'generator_learning_rate\')\n        self.discriminator_learning_rate = tf.placeholder(tf.float32, None, name=\'discriminator_learning_rate\')\n        self.classifier_learning_rate = tf.placeholder(tf.float32, None, name=""domain_classifier_learning_rate"")\n\n        self.discriminator_optimizer = tf.train.AdamOptimizer(\n            learning_rate=self.discriminator_learning_rate, beta1=0.5).minimize(\n                self.discrimator_loss, var_list=self.discriminator_vars)\n\n        self.generator_optimizer = tf.train.AdamOptimizer(\n            learning_rate=self.generator_learning_rate, beta1=0.5).minimize(\n                self.generator_loss_all, var_list=self.generator_vars)\n\n        self.classifier_optimizer = tf.train.AdamOptimizer(learning_rate=self.classifier_learning_rate).minimize(\n            self.domain_real_loss, var_list=self.classifier_vars)\n\n        # test\n        self.input_test = tf.placeholder(tf.float32, self.input_shape, name=\'input_test\')\n        self.target_label_test = tf.placeholder(tf.float32, self.label_shape, name=\'target_label_test\')\n\n        self.generation_test = self.generator(self.input_test, self.target_label_test, reuse=True, scope_name=\'generator\')\n\n\n    def train(self, input_source, input_target, source_label, target_label,  lambda_cycle=1.0, \\\n                lambda_identity=1.0, lambda_classifier=1.0, \\\n                generator_learning_rate=0.0001, \\\n                discriminator_learning_rate=0.0001, \\\n                classifier_learning_rate=0.0001):\n\n        target_label = np.array(target_label, dtype=np.float32)\n        target_label_reshaped = np.reshape(target_label, [target_label.shape[0], 1, 1, SPEAKERS_NUM])\n\n        domain_classifier_real_loss, _, domain_classifier_summaries = self.sess.run(\\\n        [self.domain_real_loss, self.classifier_optimizer, self.domain_classifier_summaries],\\\n        feed_dict={self.input_real: input_source, self.target_label:target_label, self.target_real:input_target, \\\n            self.target_label_reshaped:target_label_reshaped, \\\n        self.classifier_learning_rate:classifier_learning_rate}\n        )\n        self.writer.add_summary(domain_classifier_summaries, self.train_step)\n\n        generation_f, _, generator_loss, _, generator_summaries = self.sess.run(\n            [self.generated_forward, self.generated_back, self.generator_loss, self.generator_optimizer, self.generator_summaries], \\\n            feed_dict = {self.lambda_cycle: lambda_cycle, self.lambda_identity: lambda_identity, self.lambda_classifier:lambda_classifier ,\\\n            self.input_real: input_source, self.target_real: input_target,\\\n             self.source_label:source_label, self.target_label:target_label, \\\n                 self.target_label_reshaped:target_label_reshaped, \\\n             self.generator_learning_rate: generator_learning_rate})\n\n        self.writer.add_summary(generator_summaries, self.train_step)\n\n        discriminator_loss, _, discriminator_summaries = self.sess.run(\\\n        [self.discrimator_loss, self.discriminator_optimizer, self.discriminator_summaries], \\\n            feed_dict = {self.input_real: input_source, self.target_real: input_target , self.target_label:target_label,\\\n                self.target_label_reshaped:target_label_reshaped, \\\n            self.discriminator_learning_rate: discriminator_learning_rate})\n\n        self.writer.add_summary(discriminator_summaries, self.train_step)\n\n        self.train_step += 1\n\n        return generator_loss, discriminator_loss, domain_classifier_real_loss\n\n    def summary(self):\n        with tf.name_scope(\'generator_summaries\'):\n            cycle_loss_summary = tf.summary.scalar(\'cycle_loss\', self.cycle_loss)\n            identity_loss_summary = tf.summary.scalar(\'identity_loss\', self.identity_loss)\n            generator_loss_summary = tf.summary.scalar(\'generator_loss\', self.generator_loss)\n            generator_summaries = tf.summary.merge([cycle_loss_summary, identity_loss_summary, generator_loss_summary])\n\n        with tf.name_scope(\'discriminator_summaries\'):\n            discriminator_loss_summary = tf.summary.scalar(\'discriminator_loss\', self.discrimator_loss)\n            discriminator_summaries = tf.summary.merge([discriminator_loss_summary])\n\n        with tf.name_scope(\'domain_classifier_summaries\'):\n            domain_real_loss = tf.summary.scalar(\'domain_real_loss\', self.domain_real_loss)\n            domain_classifer_summaries = tf.summary.merge([domain_real_loss])\n\n        return generator_summaries, discriminator_summaries, domain_classifer_summaries\n\n    def test(self, inputs, label):\n        generation = self.sess.run(self.generation_test, feed_dict={self.input_test: inputs, self.target_label_test: label})\n\n        return generation\n\n    def save(self, directory, filename):\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        self.saver.save(self.sess, os.path.join(directory, filename))\n\n        return os.path.join(directory, filename)\n\n    def load(self, filepath):\n        self.saver.restore(self.sess, filepath)\n\n\nif __name__ == \'__main__\':\n    starganvc = StarGANVC(36)'"
module.py,57,"b""import tensorflow as tf\n\n\ndef gated_linear_layer(inputs, gates, name=None):\n\n    activation = tf.multiply(x=inputs, y=tf.sigmoid(gates), name=name)\n\n    return activation\n\n\ndef instance_norm_layer(inputs, epsilon=1e-05, activation_fn=None, name=None):\n\n    instance_norm_layer = tf.contrib.layers.instance_norm(\n        inputs=inputs, center=True, scale=True, epsilon=epsilon, activation_fn=activation_fn, scope=name)\n\n    return instance_norm_layer\n\n\ndef conv1d_layer(inputs, filters, kernel_size, strides=1, padding='same', activation=None, kernel_initializer=None, name=None):\n\n    conv_layer = tf.layers.conv1d(\n        inputs=inputs,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        activation=activation,\n        kernel_initializer=kernel_initializer,\n        name=name)\n\n    return conv_layer\n\n\ndef conv2d_layer(inputs, filters, kernel_size, strides, padding: list = None, activation=None, kernel_initializer=None, name=None):\n\n    p = tf.constant([[0, 0], [padding[0], padding[0]], [padding[1], padding[1]], [0, 0]])\n    out = tf.pad(inputs, p, name=name + 'conv2d_pad')\n\n    conv_layer = tf.layers.conv2d(\n        inputs=out,\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding='valid',\n        activation=activation,\n        kernel_initializer=kernel_initializer,\n        name=name)\n\n    return conv_layer\n\n\ndef residual1d_block(inputs, filters=1024, kernel_size=3, strides=1, name_prefix='residule_block_'):\n\n    h1 = conv1d_layer(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, activation=None, name=name_prefix + 'h1_conv')\n    h1_norm = instance_norm_layer(inputs=h1, activation_fn=None, name=name_prefix + 'h1_norm')\n    h1_gates = conv1d_layer(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, activation=None, name=name_prefix + 'h1_gates')\n    h1_norm_gates = instance_norm_layer(inputs=h1_gates, activation_fn=None, name=name_prefix + 'h1_norm_gates')\n    h1_glu = gated_linear_layer(inputs=h1_norm, gates=h1_norm_gates, name=name_prefix + 'h1_glu')\n    h2 = conv1d_layer(inputs=h1_glu, filters=filters // 2, kernel_size=kernel_size, strides=strides, activation=None, name=name_prefix + 'h2_conv')\n    h2_norm = instance_norm_layer(inputs=h2, activation_fn=None, name=name_prefix + 'h2_norm')\n\n    h3 = inputs + h2_norm\n\n    return h3\n\n\ndef downsample1d_block(inputs, filters, kernel_size, strides, name_prefix='downsample1d_block_'):\n\n    h1 = conv1d_layer(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, activation=None, name=name_prefix + 'h1_conv')\n    h1_norm = instance_norm_layer(inputs=h1, activation_fn=None, name=name_prefix + 'h1_norm')\n    h1_gates = conv1d_layer(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, activation=None, name=name_prefix + 'h1_gates')\n    h1_norm_gates = instance_norm_layer(inputs=h1_gates, activation_fn=None, name=name_prefix + 'h1_norm_gates')\n    h1_glu = gated_linear_layer(inputs=h1_norm, gates=h1_norm_gates, name=name_prefix + 'h1_glu')\n\n    return h1_glu\n\n\ndef downsample2d_block(inputs, filters, kernel_size, strides, padding: list = None, name_prefix='downsample2d_block_'):\n\n    h1 = conv2d_layer(\n        inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, activation=None, name=name_prefix + 'h1_conv')\n    h1_norm = instance_norm_layer(inputs=h1, activation_fn=None, name=name_prefix + 'h1_norm')\n    h1_gates = conv2d_layer(\n        inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, activation=None, name=name_prefix + 'h1_gates')\n    h1_norm_gates = instance_norm_layer(inputs=h1_gates, activation_fn=None, name=name_prefix + 'h1_norm_gates')\n    h1_glu = gated_linear_layer(inputs=h1_norm, gates=h1_norm_gates, name=name_prefix + 'h1_glu')\n\n    return h1_glu\n\n\ndef upsample1d_block(inputs, filters, kernel_size, strides, shuffle_size=2, name_prefix='upsample1d_block_'):\n\n    h1 = conv1d_layer(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, activation=None, name=name_prefix + 'h1_conv')\n    h1_shuffle = pixel_shuffler(inputs=h1, shuffle_size=shuffle_size, name=name_prefix + 'h1_shuffle')\n    h1_norm = instance_norm_layer(inputs=h1_shuffle, activation_fn=None, name=name_prefix + 'h1_norm')\n\n    h1_gates = conv1d_layer(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides, activation=None, name=name_prefix + 'h1_gates')\n    h1_shuffle_gates = pixel_shuffler(inputs=h1_gates, shuffle_size=shuffle_size, name=name_prefix + 'h1_shuffle_gates')\n    h1_norm_gates = instance_norm_layer(inputs=h1_shuffle_gates, activation_fn=None, name=name_prefix + 'h1_norm_gates')\n\n    h1_glu = gated_linear_layer(inputs=h1_norm, gates=h1_norm_gates, name=name_prefix + 'h1_glu')\n\n    return h1_glu\n\n\ndef upsample2d_block(inputs, filters, kernel_size, strides, name_prefix='upsample2d_block_'):\n\n    # t1=tf.layers.Conv2DTranspose(filters,kernel_size,strides, padding='same',name=name_prefix+'conv1')(inputs)\n    # t1 = tf.layers.batch_normalization()\n\n    t1 = tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides, padding='same')(inputs)\n    # t2 = tf.keras.layers.BatchNormalization()(t1)\n    t2 = tf.contrib.layers.instance_norm(t1, scope=name_prefix + 'instance1')\n\n    x1_gates = tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides, padding='same')(inputs)\n\n    # x1_norm_gates = tf.keras.layers.BatchNormalization()(x1_gates)\n    x1_norm_gates = tf.contrib.layers.instance_norm(x1_gates, scope=name_prefix + 'instance2')\n    x1_glu = gated_linear_layer(t2, x1_norm_gates)\n\n    return x1_glu\n\n\ndef pixel_shuffler(inputs, shuffle_size=2, name=None):\n\n    n = tf.shape(inputs)[0]\n    w = tf.shape(inputs)[1]\n    c = inputs.get_shape().as_list()[2]\n\n    oc = c // shuffle_size\n    ow = w * shuffle_size\n\n    outputs = tf.reshape(tensor=inputs, shape=[n, ow, oc], name=name)\n\n    return outputs\n\n\ndef generator_gatedcnn(inputs, speaker_id=None, reuse=False, scope_name='generator_gatedcnn'):\n    #input shape [batchsize, h, w, c]\n    #speaker_id [batchsize, one_hot_vector]\n    #one_hot_vector\xef\xbc\x9a[0,1,0,0]\n    with tf.variable_scope(scope_name) as scope:\n        if reuse:\n            scope.reuse_variables()\n        else:\n            assert scope.reuse is False\n\n        #downsample\n        d1 = downsample2d_block(inputs, filters=32, kernel_size=[3, 9], strides=[1, 1], padding=[1, 4], name_prefix='down_1')\n        print(f'd1: {d1.shape.as_list()}')\n\n        d2 = downsample2d_block(d1, filters=64, kernel_size=[4, 8], strides=[2, 2], padding=[1, 3], name_prefix='down_2')\n        print(f'd2: {d2.shape.as_list()}')\n\n        d3 = downsample2d_block(d2, filters=128, kernel_size=[4, 8], strides=[2, 2], padding=[1, 3], name_prefix='down_3')\n        print(f'd3: {d3.shape.as_list()}')\n\n        d4 = downsample2d_block(d3, filters=64, kernel_size=[3, 5], strides=[1, 1], padding=[1, 2], name_prefix='down_4')\n        print(f'd4: {d4.shape.as_list()}')\n        d5 = downsample2d_block(d4, filters=5, kernel_size=[9, 5], strides=[9, 1], padding=[1, 2], name_prefix='down_5')\n\n        #upsample\n        speaker_id = tf.convert_to_tensor(speaker_id, dtype=tf.float32)\n        c_cast = tf.cast(tf.reshape(speaker_id, [-1, 1, 1, speaker_id.shape.dims[-1].value]), tf.float32)\n        c = tf.tile(c_cast, [1, d5.shape.dims[1].value, d5.shape.dims[2].value, 1])\n        print(c.shape.as_list())\n        concated = tf.concat([d5, c], axis=-1)\n        # print(concated.shape.as_list())\n\n        u1 = upsample2d_block(concated, 64, kernel_size=[9, 5], strides=[9, 1], name_prefix='gen_up_u1')\n        print(f'u1.shape :{u1.shape.as_list()}')\n\n        c1 = tf.tile(c_cast, [1, u1.shape.dims[1].value, u1.shape.dims[2].value, 1])\n        print(f'c1 shape: {c1.shape}')\n        u1_concat = tf.concat([u1, c1], axis=-1)\n        print(f'u1_concat.shape :{u1_concat.shape.as_list()}')\n\n        u2 = upsample2d_block(u1_concat, 128, [3, 5], [1, 1], name_prefix='gen_up_u2')\n        print(f'u2.shape :{u2.shape.as_list()}')\n        c2 = tf.tile(c_cast, [1, u2.shape[1], u2.shape[2], 1])\n        u2_concat = tf.concat([u2, c2], axis=-1)\n\n        u3 = upsample2d_block(u2_concat, 64, [4, 8], [2, 2], name_prefix='gen_up_u3')\n        print(f'u3.shape :{u3.shape.as_list()}')\n        c3 = tf.tile(c_cast, [1, u3.shape[1], u3.shape[2], 1])\n        u3_concat = tf.concat([u3, c3], axis=-1)\n\n        u4 = upsample2d_block(u3_concat, 32, [4, 8], [2, 2], name_prefix='gen_up_u4')\n        print(f'u4.shape :{u4.shape.as_list()}')\n        c4 = tf.tile(c_cast, [1, u4.shape[1], u4.shape[2], 1])\n        u4_concat = tf.concat([u4, c4], axis=-1)\n        print(f'u4_concat.shape :{u4_concat.shape.as_list()}')\n\n        u5 = tf.layers.Conv2DTranspose(filters=1, kernel_size=[3, 9], strides=[1, 1], padding='same', name='generator_last_deconv')(u4_concat)\n        print(f'u5.shape :{u5.shape.as_list()}')\n\n        return u5\n\n\ndef discriminator(inputs, speaker_id, reuse=False, scope_name='discriminator'):\n\n    # inputs has shape [batch_size, height,width, channels]\n\n    with tf.variable_scope(scope_name) as scope:\n        # Discriminator would be reused in CycleGAN\n        if reuse:\n            scope.reuse_variables()\n        else:\n            assert scope.reuse is False\n        #convert data type to float32\n        c_cast = tf.cast(tf.reshape(speaker_id, [-1, 1, 1, speaker_id.shape[-1]]), tf.float32)\n        c = tf.tile(c_cast, [1, inputs.shape[1], inputs.shape[2], 1])\n\n        concated = tf.concat([inputs, c], axis=-1)\n\n        # Downsample\n        d1 = downsample2d_block(\n            inputs=concated, filters=32, kernel_size=[3, 9], strides=[1, 1], padding=[1, 4], name_prefix='downsample2d_dis_block1_')\n        c1 = tf.tile(c_cast, [1, d1.shape[1], d1.shape[2], 1])\n        d1_concat = tf.concat([d1, c1], axis=-1)\n\n        d2 = downsample2d_block(\n            inputs=d1_concat, filters=32, kernel_size=[3, 8], strides=[1, 2], padding=[1, 3], name_prefix='downsample2d_dis_block2_')\n        c2 = tf.tile(c_cast, [1, d2.shape[1], d2.shape[2], 1])\n        d2_concat = tf.concat([d2, c2], axis=-1)\n\n        d3 = downsample2d_block(\n            inputs=d2_concat, filters=32, kernel_size=[3, 8], strides=[1, 2], padding=[1, 3], name_prefix='downsample2d_dis_block3_')\n        c3 = tf.tile(c_cast, [1, d3.shape[1], d3.shape[2], 1])\n        d3_concat = tf.concat([d3, c3], axis=-1)\n\n        d4 = downsample2d_block(\n            inputs=d3_concat, filters=32, kernel_size=[3, 6], strides=[1, 2], padding=[1, 2], name_prefix='downsample2d_diss_block4_')\n        c4 = tf.tile(c_cast, [1, d4.shape[1], d4.shape[2], 1])\n        d4_concat = tf.concat([d4, c4], axis=-1)\n\n        c1 = conv2d_layer(d4_concat, filters=1, kernel_size=[36, 5], strides=[36, 1], padding=[0, 1], name='discriminator-last-conv')\n\n        c1_red = tf.reduce_mean(c1, keepdims=True)\n\n        return c1_red\n\n\ndef domain_classifier(inputs, reuse=False, scope_name='classifier'):\n\n    with tf.variable_scope(scope_name) as scope:\n        if reuse:\n            scope.reuse_variables()\n        else:\n            assert scope.reuse is False\n\n        #   add slice input shape [batchsize, 8, 512, 1]\n        #get one slice\n        one_slice = inputs[:, 0:8, :, :]\n\n        d1 = tf.layers.conv2d(one_slice, 8, kernel_size=[4, 4], padding='same', name=scope_name + '_conv2d01')\n        d1_p = tf.layers.max_pooling2d(d1, [2, 2], strides=[2, 2], name=scope_name + 'p1')\n        print(f'domain_classifier_d1: {d1.shape}')\n        print(f'domain_classifier_d1_p: {d1_p.shape}')\n\n        d2 = tf.layers.conv2d(d1_p, 16, [4, 4], padding='same', name=scope_name + '_conv2d02')\n        d2_p = tf.layers.max_pooling2d(d2, [2, 2], strides=[2, 2], name=scope_name + 'p2')\n        print(f'domain_classifier_d12: {d2.shape}')\n        print(f'domain_classifier_d2_p: {d2_p.shape}')\n\n        d3 = tf.layers.conv2d(d2_p, 32, [4, 4], padding='same', name=scope_name + '_conv2d03')\n        d3_p = tf.layers.max_pooling2d(d3, [2, 2], strides=[2, 2], name=scope_name + 'p3')\n        print(f'domain_classifier_d3: {d3.shape}')\n        print(f'domain_classifier_d3_p: {d3_p.shape}')\n\n        d4 = tf.layers.conv2d(d3_p, 16, [3, 4], padding='same', name=scope_name + '_conv2d04')\n        d4_p = tf.layers.max_pooling2d(d4, [1, 2], strides=[1, 2], name=scope_name + 'p4')\n        print(f'domain_classifier_d4: {d4.shape}')\n        print(f'domain_classifier_d4_p: {d4_p.shape}')\n\n        d5 = tf.layers.conv2d(d4_p, 4, [1, 4], padding='same', name=scope_name + '_conv2d05')\n        d5_p = tf.layers.max_pooling2d(d5, [1, 2], strides=[1, 2], name=scope_name + 'p5')\n        print(f'domain_classifier_d5: {d5.shape}')\n        print(f'domain_classifier_d5_p: {d5_p.shape}')\n\n        p = tf.keras.layers.GlobalAveragePooling2D()(d5_p)\n\n        o_r = tf.reshape(p, [-1, 1, 1, p.shape.dims[1].value])\n        print(f'classifier_output: {o_r.shape}')\n\n        return o_r"""
preprocess.py,0,"b'import librosa\nimport numpy as np\nimport os\nimport pyworld\nimport pyworld as pw\nimport glob\nfrom utility import *\nimport argparse\n\nFEATURE_DIM = 36\nSAMPLE_RATE = 16000\nFRAMES = 512\nFFTSIZE = 1024\nSPEAKERS_NUM = 4  # in our experiment, we use four speakers\n\nEPSILON = 1e-10\nMODEL_NAME = \'starganvc_model\'\n\n\ndef load_wavs(dataset: str, sr):\n    \'\'\'\n    data dict contains all audios file path\n    resdict contains all wav files   \n    \'\'\'\n    data = {}\n    with os.scandir(dataset) as it:\n        for entry in it:\n            if entry.is_dir():\n                data[entry.name] = []\n                # print(entry.name, entry.path)\n                with os.scandir(entry.path) as it_f:\n                    for onefile in it_f:\n                        if onefile.is_file():\n                            # print(onefile.path)\n                            data[entry.name].append(onefile.path)\n    print(f\'loaded keys: {data.keys()}\')\n    # data like {TM1:[xx,xx,xxx,xxx]}\n    resdict = {}\n\n    cnt = 0\n    for key, value in data.items():\n        resdict[key] = {}\n\n        for one_file in value:\n\n            filename = os.path.normpath(one_file).split(os.sep)[-1].split(\'.\')[0]  # like 100061\n            newkey = f\'{filename}\'\n            wav, _ = librosa.load(one_file, sr=sr, mono=True, dtype=np.float64)\n\n            resdict[key][newkey] = wav\n            # resdict[key].append(temp_dict) #like TM1:{100062:[xxxxx], .... }\n            print(\'.\', end=\'\')\n            cnt += 1\n\n    print(f\'\\nTotal {cnt} aduio files!\')\n    return resdict\n\n\ndef wav_to_mcep_file(dataset: str, sr=16000, ispad: bool = False, processed_filepath: str = \'./data/processed\'):\n    \'\'\'convert wavs to mcep feature using image repr\'\'\'\n    # if no processed_filepath, create it ,or delete all npz files\n    if not os.path.exists(processed_filepath):\n        os.makedirs(processed_filepath)\n    else:\n        filelist = glob.glob(os.path.join(processed_filepath, ""*.npy""))\n        for f in filelist:\n            os.remove(f)\n\n    allwavs_cnt = len(glob.glob(f\'{dataset}/*/*.wav\'))\n    # allwavs_cnt = allwavs_cnt//4*3 * 12+200 #about this number not precise\n    print(f\'Total {allwavs_cnt} audio files!\')\n\n    d = load_wavs(dataset, sr)\n    cnt = 1  #\n\n    for one_speaker in d.keys():\n        for audio_name, audio_wav in d[one_speaker].items():\n            # cal source audio feature\n            audio_mcep_dict = cal_mcep(\n                audio_wav, fs=sr, ispad=ispad, frame_period=0.005, dim=FEATURE_DIM)\n            newname = f\'{one_speaker}-{audio_name}\'\n\n            # save the dict as npz\n            file_path_z = f\'{processed_filepath}/{newname}\'\n            print(f\'save file: {file_path_z}\')\n            np.savez(file_path_z, audio_mcep_dict)\n\n            # save every  36*FRAMES blocks\n            print(f\'audio mcep shape {audio_mcep_dict[""coded_sp""].shape}\')\n\n            # TODO step may be FRAMES//2\n            for start_idx in range(0, audio_mcep_dict[""coded_sp""].shape[1] - FRAMES + 1, FRAMES):\n                one_audio_seg = audio_mcep_dict[""coded_sp""][:,\n                                                            start_idx:start_idx + FRAMES]\n\n                if one_audio_seg.shape[1] == FRAMES:\n\n                    temp_name = f\'{newname}_{start_idx}\'\n                    filePath = f\'{processed_filepath}/{temp_name}\'\n\n                    print(f\'[{cnt}:{allwavs_cnt}]svaing file: {filePath}.npy\')\n                    np.save(filePath, one_audio_seg)\n            cnt += 1\n\n\ndef cal_mcep(wav_ori, fs=SAMPLE_RATE, ispad=False, frame_period=0.005, dim=FEATURE_DIM, fft_size=FFTSIZE):\n    \'\'\'cal mcep given wav singnal\n        the frame_period used only for pad_wav_to_get_fixed_frames\n    \'\'\'\n    if ispad:\n        wav, pad_length = pad_wav_to_get_fixed_frames(\n            wav_ori, frames=FRAMES, frame_period=frame_period, sr=fs)\n    else:\n        wav = wav_ori\n    # Harvest F0 extraction algorithm.\n    f0, timeaxis = pyworld.harvest(wav, fs)\n\n    # CheapTrick harmonic spectral envelope estimation algorithm.\n    sp = pyworld.cheaptrick(wav, f0, timeaxis, fs, fft_size=fft_size)\n\n    # D4C aperiodicity estimation algorithm.\n    ap = pyworld.d4c(wav, f0, timeaxis, fs, fft_size=fft_size)\n    # feature reduction nxdim\n    coded_sp = pyworld.code_spectral_envelope(sp, fs, dim)\n    # log\n    coded_sp = coded_sp.T  # dim x n\n\n    res = {\n        \'f0\': f0,  # n\n        \'ap\': ap,  # n*fftsize//2+1\n        \'sp\': sp,  # n*fftsize//2+1\n        \'coded_sp\': coded_sp,  # dim * n\n    }\n    return res\n\n\ndef pad_wav_to_get_fixed_frames(x: np.ndarray, frames: int = 128, frame_period: float = 0.005, sr: int = 16000):\n    # one frame\'s points\n    frame_length = frame_period * sr\n    # frames points\n    frames_points = frames * frame_length\n\n    wav_len = len(x)\n\n    # pad amount\n    pieces = wav_len // frames_points\n\n    need_pad = 0\n    if wav_len % frames_points != 0:\n        # can\'t devide need pad\n        need_pad = int((pieces + 1) * frames_points - wav_len)\n\n    afterpad_len = wav_len + need_pad\n    # print(f\'need pad: {need_pad}, after pad: {afterpad_len}\')\n    # padding process\n    tempx = x.tolist()\n\n    if need_pad <= len(x):\n        tempx.extend(x[:need_pad])\n    else:\n        temp1, temp2 = need_pad // len(x), need_pad / len(x)\n        tempx = tempx * (temp1 + 1)\n        samll_pad_len = int(np.ceil((temp2 - temp1) * len(x)))\n        tempx.extend(x[:samll_pad_len])\n\n        diff = 0\n        if afterpad_len != len(tempx):\n            diff = afterpad_len - len(tempx)\n        if diff > 0:\n            tempx.extend(tempx[:diff])\n        elif diff < 0:\n            tempx = tempx[:diff]\n\n    # print(f\'padding length: {len(x)}-->length: {len(tempx)}\')\n    # remove last point for calculate convience:the frame length are 128*(some integer).\n    tempx = tempx[:-1]\n\n    return np.asarray(tempx, dtype=np.float), need_pad\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Convert the wav waveform to mel-cepstral coefficients(MCCs)\\\n    and calculate the speech statistical characteristics\')\n\n    input_dir = \'./data/fourspeakers\'\n    output_dir = \'./data/processed\'\n    ispad = True\n\n    parser.add_argument(\'--input_dir\', type=str,\n                        help=\'the direcotry contains data need to be processed\', default=input_dir)\n    parser.add_argument(\'--output_dir\', type=str,\n                        help=\'the directory stores the processed data\', default=output_dir)\n    parser.add_argument(\n        \'--ispad\', type=bool, help=\'whether to pad the wavs  to get fixed length MCEP\', default=ispad)\n\n    argv = parser.parse_args()\n    input_dir = argv.input_dir\n    output_dir = argv.output_dir\n    ispad = argv.ispad\n\n    wav_to_mcep_file(input_dir, SAMPLE_RATE, ispad=ispad,\n                     processed_filepath=output_dir)\n\n    # input_dir is train dataset. we need to calculate and save the speech\\\n    # statistical characteristics for each speaker.\n    generator = GenerateStatics(output_dir)\n    generator.generate_stats()\n'"
train.py,0,"b'import os\nimport numpy as np\nimport argparse\nimport time\nimport librosa\nimport glob\nfrom preprocess import *\nfrom model import *\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom utility import *\n\n\ndef get_files_labels(pattern: str):\n    files = glob.glob(pattern)\n    names = []\n    for f in files:\n        t = os.path.normpath(f).rsplit(os.sep, maxsplit=1)[1]  #\'./data/processed/SF2-100008_11.npy\'\n        name = t.rsplit(\'.\', maxsplit=1)[0]\n        names.append(name)\n\n    return files, names\n\n\ndef train(processed_dir: str, test_wav_dir: str):\n    timestr = time.strftime(""%Y-%m-%d-%H-%M"", time.localtime())  #like \'2018-10-10-14-47\'\n\n    all_speaker = get_speakers()\n    label_enc = LabelEncoder()\n    label_enc.fit(all_speaker)\n\n    lambda_cycle = 10\n    lambda_identity = 5\n    lambda_classifier = 3\n\n    generator_learning_rate = 0.0001\n    generator_learning_rate_decay = generator_learning_rate / 20000\n    discriminator_learning_rate = 0.0001\n    discriminator_learning_rate_decay = discriminator_learning_rate / 20000\n    domain_classifier_learning_rate = 0.0001\n    domain_classifier_learning_rate_decay = domain_classifier_learning_rate / 20000\n    #====================load data================#\n    print(\'Loading Data...\')\n\n    files, names = get_files_labels(os.path.join(processed_dir, \'*.npy\'))\n    assert len(files) > 0\n\n    normlizer = Normalizer()\n\n    exclude_dict = {}  #key that not appear in the value list.(eg. SF1:[TM1**.wav,TM2**.wav,SF2**.wav ... ])\n    for s in all_speaker:\n        p = os.path.join(processed_dir, \'*.npy\')  #\'./data/processed/*.npy\'\n        temp = [fn for fn in glob.glob(p) if fn.find(s) == -1]\n        exclude_dict[s] = temp\n\n    print(\'Loading Data Done.\')\n\n    #====================create model=============#\n    BATCHSIZE = 1\n    model = StarGANVC(num_features=FEATURE_DIM, frames=FRAMES, batchsize=BATCHSIZE)\n    #====================start train==============#\n    EPOCH = 201\n\n    num_samples = len(files)\n    for epoch in range(1, EPOCH+1, 1):\n        start_time_epoch = time.time()\n\n        files_shuffled, names_shuffled = shuffle(files, names)\n\n        for i in range(num_samples // BATCHSIZE):\n            num_iterations = num_samples // BATCHSIZE * (epoch-1) + i\n\n            if num_iterations > 100000:\n                domain_classifier_learning_rate = max(0, domain_classifier_learning_rate - domain_classifier_learning_rate_decay)\n                generator_learning_rate = max(0, generator_learning_rate - generator_learning_rate_decay)\n                discriminator_learning_rate = max(0, discriminator_learning_rate - discriminator_learning_rate_decay)\n\n            if discriminator_learning_rate == 0 or generator_learning_rate == 0:\n                print(\'Early stop training.\')\n                break\n\n            start = i * BATCHSIZE\n            end = (i + 1) * BATCHSIZE\n\n            if end > num_samples:\n                end = num_samples\n\n            X, X_t, y, y_t = [], [], [], []\n\n            #get target file paths\n            batchnames = names_shuffled[start:end]\n            pre_targets = []\n            for name in batchnames:\n                name = name.split(sep=\'-\')[0]  #SF1\n                t = np.random.choice(exclude_dict[name], 1)[0]\n                pre_targets.append(t)\n\n            #one batch train data\n            for one_filename, one_name, one_target in zip(files_shuffled[start:end], names_shuffled[start:end], pre_targets):\n\n                #target name\n                t = os.path.normpath(one_target).rsplit(os.sep, maxsplit=1)[1]  #\'./data/processed/SF2-100008_11.npy\'\n                target_speaker_name = t.rsplit(\'.\', maxsplit=1)[0].split(\'-\')[0]\n\n                #source name\n                speaker_name = one_name.split(\'-\')[0]  #SF1\n\n                #shape [36,512]\n                one_file = np.load(one_filename)\n                one_file = normlizer.forward_process(one_file, speaker_name)\n\n                #shape [36,512,1]\n                one_file = np.reshape(one_file, [one_file.shape[0], one_file.shape[1], 1])\n                X.append(one_file)\n\n                #source label\n                temp_index = label_enc.transform([speaker_name])[0]\n                temp_arr_s = np.zeros([\n                    len(all_speaker),\n                ])\n                temp_arr_s[temp_index] = 1\n                y.append(temp_arr_s)\n\n                #load target files and labels\n                one_file_t = np.load(one_target)\n                one_file_t = normlizer.forward_process(one_file_t, target_speaker_name)\n\n                #[36,512,1]\n                one_file_t = np.reshape(one_file_t, [one_file_t.shape[0], one_file_t.shape[1], 1])\n                X_t.append(one_file_t)\n\n                #target label\n                temp_index_t = label_enc.transform([target_speaker_name])[0]\n                temp_arr_t = np.zeros([\n                    len(all_speaker),\n                ])\n                temp_arr_t[temp_index_t] = 1\n                y_t.append(temp_arr_t)\n\n\n            generator_loss, discriminator_loss, domain_classifier_loss = model.train(\\\n            input_source=X, input_target=X_t, source_label=y, \\\n            target_label=y_t, generator_learning_rate=generator_learning_rate,\\\n             discriminator_learning_rate=discriminator_learning_rate,\\\n            classifier_learning_rate=domain_classifier_learning_rate, \\\n            lambda_identity=lambda_identity, lambda_cycle=lambda_cycle,\\\n            lambda_classifier=lambda_classifier\n            )\n\n            if num_iterations % 10 == 0:\n                print(\'Iteration: {:07d},Generator Loss : {:.3f}, Discriminator Loss : {:.3f}, domain_classifier_loss: {:.3f}\'\\\n                .format(num_iterations, generator_loss, discriminator_loss, domain_classifier_loss))\n\n        #=======================test model==========================\n\n        file_path = os.path.join(\'out/\', f\'{epoch}_{timestr}\')\n        if epoch % 10 == 0:\n            print(\'============test model============\')\n            #out put path\n            os.makedirs(file_path, exist_ok=True)                \n\n            tempfiles = []\n            for one_speaker in all_speaker:\n                p = os.path.join(test_wav_dir, f\'{one_speaker}/*.wav\')\n                wavs = glob.glob(p)\n                tempfiles.append(wavs[0])\n                tempfiles.append(wavs[1])  #\'./data/fourspeakers_test/200006.wav\'\n\n            for one_file in tempfiles:\n                _, speaker, name = os.path.normpath(one_file).rsplit(os.sep, maxsplit=2)\n                wav_, fs = librosa.load(one_file, sr=SAMPLE_RATE, mono=True, dtype=np.float64)\n                wav, pad_length = pad_wav_to_get_fixed_frames(wav_, frames=FRAMES)\n\n                f0, timeaxis = pyworld.harvest(wav, fs)\n                sp = pyworld.cheaptrick(wav, f0, timeaxis, fs, fft_size=FFTSIZE)\n                ap = pyworld.d4c(wav, f0, timeaxis, fs, fft_size=FFTSIZE)\n                coded_sp = pyworld.code_spectral_envelope(sp, fs, FEATURE_DIM)\n\n                #one audio file to multiple slices(that\'s one_test_sample),every slice is an input\n                one_test_sample = []\n                csp_transpose = coded_sp.T  #36x512 36x128...\n                for i in range(0, csp_transpose.shape[1] - FRAMES + 1, FRAMES):\n                    t = csp_transpose[:, i:i + FRAMES]\n                    t = normlizer.forward_process(t, speaker)\n                    t = np.reshape(t, [t.shape[0], t.shape[1], 1])\n                    one_test_sample.append(t)\n\n                #target label 1->2, 2->3, 3->0, 0->1\n                one_test_sample_label = np.zeros([len(one_test_sample), len(all_speaker)])\n                temp_index = label_enc.transform([speaker])[0]\n                temp_index = (temp_index + 2) % len(all_speaker)\n\n                for i in range(len(one_test_sample)):\n                    one_test_sample_label[i][temp_index] = 1\n\n                #get conversion target name ,like SF1\n                target_name = label_enc.inverse_transform([temp_index])[0]\n\n                generated_results = model.test(one_test_sample, one_test_sample_label)\n\n                reshpaped_res = []\n                for one in generated_results:\n                    t = np.reshape(one, [one.shape[0], one.shape[1]])\n                    t = normlizer.backward_process(t, target_name)\n                    reshpaped_res.append(t)\n                #collect the generated slices, and concate the array to be a whole representation of the whole audio\n                c = []\n                for one_slice in reshpaped_res:\n                    one_slice = np.ascontiguousarray(one_slice.T, dtype=np.float64)\n                    decoded_sp = pyworld.decode_spectral_envelope(one_slice, SAMPLE_RATE, fft_size=FFTSIZE)\n                    c.append(decoded_sp)\n\n                concated = np.concatenate((c), axis=0)\n\n                #f0 convert\n                f0 = normlizer.pitch_conversion(f0, speaker, target_name)\n                synwav = pyworld.synthesize(f0, concated, ap, fs)\n                #remove synthesized wav paded length\n                synwav = synwav[:-pad_length]\n\n                #save synthesized wav to file\n                wavname = f\'{speaker}-{target_name}+{name}\'\n                wavpath = os.path.join(file_path, \'wavs\')\n                if not os.path.exists(wavpath):\n                    os.makedirs(wavpath, exist_ok=True)\n                librosa.output.write_wav(f\'{wavpath}/{wavname}\', synwav, sr=fs)\n                print(f\'[save]:{wavpath}/{wavname}\')\n\n            print(\'============test finished!============\')\n\n        if epoch % 10 == 0:\n            print(\'============save model============\')\n            model_path = os.path.join(file_path, \'model\')\n            os.makedirs(model_path, exist_ok=True)\n            print(f\'[save]: {model_path}\')\n            model.save(directory=model_path, filename=MODEL_NAME)\n\n        end_time_epoch = time.time()\n        time_elapsed_epoch = end_time_epoch - start_time_epoch\n\n        print(\'Time Elapsed for Epoch %d: %02d:%02d:%02d\' % (epoch, time_elapsed_epoch // 3600, (time_elapsed_epoch % 3600 // 60),\n                                                               (time_elapsed_epoch % 60 // 1)))\n\n\nif __name__ == \'__main__\':\n\n    processed_dir = \'./data/processed\'\n    test_wav_dir = \'./data/fourspeakers_test\'\n\n    parser = argparse.ArgumentParser(description=\'Train StarGAN Voice conversion model.\')\n\n    parser.add_argument(\'--processed_dir\', type=str, help=\'train dataset directory that contains processed npy and npz files\', default=processed_dir)\n    parser.add_argument(\'--test_wav_dir\', type=str, help=\'test directory that contains raw audios\', default=test_wav_dir)\n\n    argv = parser.parse_args()\n\n    processed_dir = argv.processed_dir\n    test_wav_dir = argv.test_wav_dir\n\n    start_time = time.time()\n\n    train(processed_dir, test_wav_dir)\n\n    end_time = time.time()\n    time_elapsed = end_time - start_time\n\n    print(\'Training Time: %02d:%02d:%02d\' % \\\n    (time_elapsed // 3600, (time_elapsed % 3600 // 60), (time_elapsed % 60 // 1)))\n'"
utility.py,0,"b'import numpy as np\nimport pyworld as pw\n# import soundfile as sf\nimport tensorflow as tf\nimport os, shutil\nimport glob\n\n\ndef get_speakers(trainset: str = \'./data/fourspeakers\'):\n    \'\'\'return current selected speakers for training\n        eg. [\'SF2\', \'TM1\', \'SF1\', \'TM2\']\n    \'\'\'\n    p = os.path.join(trainset, ""*"")\n    all_sub_folder = glob.glob(p)\n\n    all_speaker = [os.path.normpath(s).rsplit(os.sep, maxsplit=1)[1] for s in all_sub_folder]\n\n    return all_speaker\n\n\nclass Normalizer(object):\n    \'\'\'Normalizer: convience method for fetch normalize instance\'\'\'\n\n    def __init__(self, statfolderpath: str = \'./etc\'):\n\n        self.all_speaker = get_speakers()\n        self.folderpath = statfolderpath\n\n        self.norm_dict = self.normalizer_dict()\n\n    def forward_process(self, x, speakername):\n        mean = self.norm_dict[speakername][\'coded_sps_mean\']\n        std = self.norm_dict[speakername][\'coded_sps_std\']\n        mean = np.reshape(mean, [-1, 1])\n        std = np.reshape(std, [-1, 1])\n        x = (x - mean) / std\n\n        return x\n\n    def backward_process(self, x, speakername):\n        mean = self.norm_dict[speakername][\'coded_sps_mean\']\n        std = self.norm_dict[speakername][\'coded_sps_std\']\n        mean = np.reshape(mean, [-1, 1])\n        std = np.reshape(std, [-1, 1])\n        x = x * std + mean\n\n        return x\n\n    def normalizer_dict(self):\n        \'\'\'return all speakers normailzer parameter\'\'\'\n\n        d = {}\n        for one_speaker in self.all_speaker:\n\n            p = os.path.join(self.folderpath, \'*.npz\')\n            try:\n                stat_filepath = [fn for fn in glob.glob(p) if one_speaker in fn][0]\n            except:\n                raise Exception(\'====no match files!====\')\n            print(f\'found stat file: {stat_filepath}\')\n            t = np.load(stat_filepath)\n            d_temp = t.f.arr_0.item()\n            # print(d_temp.keys())\n\n            d[one_speaker] = d_temp\n\n        return d\n\n    def pitch_conversion(self, f0, source_speaker, target_speaker):\n        \'\'\'Logarithm Gaussian normalization for Pitch Conversions\'\'\'\n\n        mean_log_src = self.norm_dict[source_speaker][\'log_f0s_mean\']\n        std_log_src = self.norm_dict[source_speaker][\'log_f0s_std\']\n\n        mean_log_target = self.norm_dict[target_speaker][\'log_f0s_mean\']\n        std_log_target = self.norm_dict[target_speaker][\'log_f0s_std\']\n\n        f0_converted = np.exp((np.ma.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n\n        return f0_converted\n\n\nclass GenerateStatics(object):\n\n    def __init__(self, folder: str = \'./data/processed\'):\n        self.folder = folder\n\n        self.all_speaker = get_speakers()\n\n        #key is speaker(SF1, SF2...) and value is corresponding file list\n        self.include_dict = {}\n        for s in self.all_speaker:\n            if not self.include_dict.__contains__(s):\n                self.include_dict[s] = []\n\n            for one_file in os.listdir(folder):\n                if one_file.startswith(s) and one_file.endswith(\'npy\'):\n                    self.include_dict[s].append(one_file)\n        # print(self.include_dict)\n\n        self.include_dict_npz = {}\n        for s in self.all_speaker:\n            if not self.include_dict_npz.__contains__(s):\n                self.include_dict_npz[s] = []\n\n            for one_file in os.listdir(folder):\n                if one_file.startswith(s) and one_file.endswith(\'npz\'):\n                    self.include_dict_npz[s].append(one_file)\n        # print(self.include_dict_npz)\n\n    @staticmethod\n    def coded_sp_statistics(coded_sps):\n        # sp shape (T, D)\n        coded_sps_concatenated = np.concatenate(coded_sps, axis=1)\n        coded_sps_mean = np.mean(coded_sps_concatenated, axis=1, keepdims=False)\n        coded_sps_std = np.std(coded_sps_concatenated, axis=1, keepdims=False)\n        return coded_sps_mean, coded_sps_std\n\n    @staticmethod\n    def logf0_statistics(f0s):\n        log_f0s_concatenated = np.ma.log(np.concatenate(f0s))\n        log_f0s_mean = log_f0s_concatenated.mean()\n        log_f0s_std = log_f0s_concatenated.std()\n\n        return log_f0s_mean, log_f0s_std\n\n    def generate_stats(self, statfolder: str = \'./etc\'):\n        \'\'\'generate all user\'s statitics used for calutate normalized\n           input like sp, f0\n           step 1: generate coded_sp mean std\n           step 2: generate f0 mean std\n         \'\'\'\n        etc_path = os.path.join(os.path.realpath(\'.\'), statfolder)\n        if not os.path.exists(etc_path):\n            os.makedirs(etc_path, exist_ok=True)\n\n        for one_speaker in self.include_dict.keys():\n            coded_sps = []\n\n            arr = self.include_dict[one_speaker]\n            if len(arr) == 0:\n                continue\n            for one_file in arr:\n                t = np.load(os.path.join(self.folder, one_file))\n                # print(t.shape)\n                coded_sps.append(t)\n\n            coded_sps_mean, coded_sps_std = self.coded_sp_statistics(coded_sps)\n            # print(f\'sp_mean: {coded_sps_mean.shape} \\\n            # sp_std: {coded_sps_std.shape}\')\n\n            f0s = []\n            arr01 = self.include_dict_npz[one_speaker]\n            if len(arr01) == 0:\n                continue\n            for one_file in arr01:\n                t = np.load(os.path.join(self.folder, one_file))\n                d = t.f.arr_0.item()\n                f0_ = np.reshape(d[\'f0\'], [-1, 1])\n                # print(f\'f0 shape: {f0_.shape}\')\n                f0s.append(f0_)\n            log_f0s_mean, log_f0s_std = self.logf0_statistics(f0s)\n            print(log_f0s_mean, log_f0s_std)\n\n            tempdict = {\'log_f0s_mean\': log_f0s_mean, \'log_f0s_std\': log_f0s_std, \'coded_sps_mean\': coded_sps_mean, \'coded_sps_std\': coded_sps_std}\n\n            filename = os.path.join(etc_path, f\'{one_speaker}-stats.npz\')\n            print(f\'save: {filename}\')\n            np.savez(filename, tempdict)\n\n\nif __name__ == ""__main__"":\n    pass'"
