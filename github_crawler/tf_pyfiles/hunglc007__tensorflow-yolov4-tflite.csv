file_path,api_count,code
benchmarks.py,12,"b'import numpy as np\nimport tensorflow as tf\nimport time\nimport cv2\nfrom core.yolov4 import YOLOv4, YOLOv3_tiny, YOLOv3, decode\nfrom absl import app, flags, logging\nfrom absl.flags import FLAGS\nfrom tensorflow.python.saved_model import tag_constants\nfrom core import utils\nfrom core.config import cfg\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nflags.DEFINE_boolean(\'tiny\', False, \'yolo or yolo-tiny\')\nflags.DEFINE_string(\'framework\', \'tf\', \'(tf, tflite, trt\')\nflags.DEFINE_string(\'model\', \'yolov4\', \'yolov3 or yolov4\')\nflags.DEFINE_string(\'weights\', \'./data/yolov4.weights\', \'path to weights file\')\nflags.DEFINE_string(\'image\', \'./data/kite.jpg\', \'path to input image\')\nflags.DEFINE_integer(\'size\', 416, \'resize images to\')\n\n\ndef main(_argv):\n    if FLAGS.tiny:\n        STRIDES = np.array(cfg.YOLO.STRIDES_TINY)\n        ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS_TINY, FLAGS.tiny)\n    else:\n        STRIDES = np.array(cfg.YOLO.STRIDES)\n        if FLAGS.model == \'yolov4\':\n            ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS, FLAGS.tiny)\n        else:\n            ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS_V3, FLAGS.tiny)\n    NUM_CLASS = len(utils.read_class_names(cfg.YOLO.CLASSES))\n    XYSCALE = cfg.YOLO.XYSCALE\n\n    config = ConfigProto()\n    config.gpu_options.allow_growth = True\n    session = InteractiveSession(config=config)\n    input_size = FLAGS.size\n    physical_devices = tf.config.experimental.list_physical_devices(\'GPU\')\n    if len(physical_devices) > 0:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    if FLAGS.framework == \'tf\':\n        input_layer = tf.keras.layers.Input([input_size, input_size, 3])\n        if FLAGS.tiny:\n            feature_maps = YOLOv3_tiny(input_layer, NUM_CLASS)\n            bbox_tensors = []\n            for i, fm in enumerate(feature_maps):\n                bbox_tensor = decode(fm, NUM_CLASS, i)\n                bbox_tensors.append(bbox_tensor)\n            model = tf.keras.Model(input_layer, bbox_tensors)\n            utils.load_weights_tiny(model, FLAGS.weights)\n        else:\n            if FLAGS.model == \'yolov3\':\n                feature_maps = YOLOv3(input_layer, NUM_CLASS)\n                bbox_tensors = []\n                for i, fm in enumerate(feature_maps):\n                    bbox_tensor = decode(fm, NUM_CLASS, i)\n                    bbox_tensors.append(bbox_tensor)\n                model = tf.keras.Model(input_layer, bbox_tensors)\n                utils.load_weights_v3(model, FLAGS.weights)\n            elif FLAGS.model == \'yolov4\':\n                feature_maps = YOLOv4(input_layer, NUM_CLASS)\n                bbox_tensors = []\n                for i, fm in enumerate(feature_maps):\n                    bbox_tensor = decode(fm, NUM_CLASS, i)\n                    bbox_tensors.append(bbox_tensor)\n                model = tf.keras.Model(input_layer, bbox_tensors)\n                utils.load_weights(model, FLAGS.weights)\n    elif FLAGS.framework == \'trt\':\n        saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])\n        signature_keys = list(saved_model_loaded.signatures.keys())\n        print(signature_keys)\n        infer = saved_model_loaded.signatures[\'serving_default\']\n\n    logging.info(\'weights loaded\')\n\n    @tf.function\n    def run_model(x):\n        return model(x)\n\n    # Test the TensorFlow Lite model on random input data.\n    sum = 0\n    original_image = cv2.imread(FLAGS.image)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n    original_image_size = original_image.shape[:2]\n    image_data = utils.image_preprocess(np.copy(original_image), [FLAGS.size, FLAGS.size])\n    image_data = image_data[np.newaxis, ...].astype(np.float32)\n    img_raw = tf.image.decode_image(\n        open(FLAGS.image, \'rb\').read(), channels=3)\n    img_raw = tf.expand_dims(img_raw, 0)\n    img_raw = tf.image.resize(img_raw, (FLAGS.size, FLAGS.size))\n    batched_input = tf.constant(image_data)\n    for i in range(1000):\n        prev_time = time.time()\n        # pred_bbox = model.predict(image_data)\n        if FLAGS.framework == \'tf\':\n            pred_bbox = []\n            result = run_model(image_data)\n            for value in result:\n                value = value.numpy()\n                pred_bbox.append(value)\n            if FLAGS.model == \'yolov4\':\n                pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES, XYSCALE)\n            else:\n                pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES)\n            bboxes = utils.postprocess_boxes(pred_bbox, original_image_size, input_size, 0.25)\n            bboxes = utils.nms(bboxes, 0.213, method=\'nms\')\n        elif FLAGS.framework == \'trt\':\n            pred_bbox = []\n            result = infer(batched_input)\n            for key, value in result.items():\n                value = value.numpy()\n                pred_bbox.append(value)\n            if FLAGS.model == \'yolov4\':\n                pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES, XYSCALE)\n            else:\n                pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES)\n            bboxes = utils.postprocess_boxes(pred_bbox, original_image_size, input_size, 0.25)\n            bboxes = utils.nms(bboxes, 0.213, method=\'nms\')\n        # pred_bbox = pred_bbox.numpy()\n        curr_time = time.time()\n        exec_time = curr_time - prev_time\n        if i == 0: continue\n        sum += (1 / exec_time)\n        info = str(i) + "" time:"" + str(round(exec_time, 3)) + "" average FPS:"" + str(round(sum / i, 2)) + "", FPS: "" + str(\n            round((1 / exec_time), 1))\n        print(info)\n\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass\n'"
convert_tflite.py,13,"b'import tensorflow as tf\nfrom absl import app, flags, logging\nfrom absl.flags import FLAGS\nimport numpy as np\nimport cv2\nfrom core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\nimport core.utils as utils\nimport os\nfrom core.config import cfg\n\nflags.DEFINE_string(\'weights\', \'./data/yolov4.weights\', \'path to weights file\')\nflags.DEFINE_string(\'output\', \'./data/yolov4.tflite\', \'path to output\')\nflags.DEFINE_boolean(\'tiny\', False, \'path to output\')\nflags.DEFINE_integer(\'input_size\', 416, \'path to output\')\nflags.DEFINE_string(\'model\', \'yolov4\', \'yolov3 or yolov4\')\nflags.DEFINE_string(\'quantize_mode\', ""int8"", \'quantize mode (int8, float16, full_int8)\')\nflags.DEFINE_string(\'dataset\', ""/media/user/Source/Data/coco_dataset/coco/5k.txt"", \'path to dataset\')\n\ndef representative_data_gen():\n  fimage = open(FLAGS.dataset).read().split()\n  for input_value in range(100):\n    if os.path.exists(fimage[input_value]):\n      original_image=cv2.imread(fimage[input_value])\n      original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n      image_data = utils.image_preprocess(np.copy(original_image), [FLAGS.input_size, FLAGS.input_size])\n      img_in = image_data[np.newaxis, ...].astype(np.float32)\n      print(input_value)\n      yield [img_in]\n    else:\n      continue\n\ndef save_tflite():\n  NUM_CLASS = len(utils.read_class_names(cfg.YOLO.CLASSES))\n  input_layer = tf.keras.layers.Input([FLAGS.input_size, FLAGS.input_size, 3])\n  if FLAGS.tiny:\n    feature_maps = YOLOv3_tiny(input_layer, NUM_CLASS)\n    bbox_tensors = []\n    for i, fm in enumerate(feature_maps):\n      bbox_tensor = decode(fm, NUM_CLASS, i)\n      bbox_tensors.append(bbox_tensor)\n    model = tf.keras.Model(input_layer, bbox_tensors)\n    utils.load_weights_tiny(model, FLAGS.weights)\n  else:\n    if FLAGS.model == \'yolov3\':\n      feature_maps = YOLOv3(input_layer, NUM_CLASS)\n      bbox_tensors = []\n      for i, fm in enumerate(feature_maps):\n        bbox_tensor = decode(fm, NUM_CLASS, i)\n        bbox_tensors.append(bbox_tensor)\n      model = tf.keras.Model(input_layer, bbox_tensors)\n      utils.load_weights_v3(model, FLAGS.weights)\n    elif FLAGS.model == \'yolov4\':\n      feature_maps = YOLOv4(input_layer, NUM_CLASS)\n      bbox_tensors = []\n      for i, fm in enumerate(feature_maps):\n        bbox_tensor = decode(fm, NUM_CLASS, i)\n        bbox_tensors.append(bbox_tensor)\n      model = tf.keras.Model(input_layer, bbox_tensors)\n      utils.load_weights(model, FLAGS.weights)\n\n  model.summary()\n\n  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n  if tf.__version__ >= \'2.2.0\':\n    converter.experimental_new_converter = False\n\n  if FLAGS.quantize_mode == \'int8\':\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n  elif FLAGS.quantize_mode == \'float16\':\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.target_spec.supported_types = [tf.compat.v1.lite.constants.FLOAT16]\n  elif FLAGS.quantize_mode == \'full_int8\':\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n    converter.allow_custom_ops = True\n    converter.representative_dataset = representative_data_gen\n\n  tflite_model = converter.convert()\n  open(FLAGS.output, \'wb\').write(tflite_model)\n\n  logging.info(""model saved to: {}"".format(FLAGS.output))\n\ndef demo():\n  interpreter = tf.lite.Interpreter(model_path=FLAGS.output)\n  interpreter.allocate_tensors()\n  logging.info(\'tflite model loaded\')\n\n  input_details = interpreter.get_input_details()\n  print(input_details)\n  output_details = interpreter.get_output_details()\n  print(output_details)\n\n  input_shape = input_details[0][\'shape\']\n\n  input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n\n  interpreter.set_tensor(input_details[0][\'index\'], input_data)\n  interpreter.invoke()\n  output_data = [interpreter.get_tensor(output_details[i][\'index\']) for i in range(len(output_details))]\n\n  print(output_data)\n\ndef main(_argv):\n  save_tflite()\n  demo()\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass\n\n\n'"
convert_trt.py,2,"b'import tensorflow as tf\nfrom absl import app, flags, logging\nfrom absl.flags import FLAGS\nimport numpy as np\nimport cv2\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\nimport core.utils as utils\nfrom tensorflow.python.saved_model import signature_constants\nimport os\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nflags.DEFINE_string(\'weights\', \'./checkpoints/yolov4-416\', \'path to weights file\')\nflags.DEFINE_string(\'output\', \'./checkpoints/yolov4-trt-fp16-416\', \'path to output\')\nflags.DEFINE_integer(\'input_size\', 416, \'path to output\')\nflags.DEFINE_string(\'quantize_mode\', \'float16\', \'quantize mode (int8, float16)\')\nflags.DEFINE_string(\'dataset\', ""./coco_dataset/coco/5k.txt"", \'path to dataset\')\nflags.DEFINE_integer(\'loop\', 10, \'loop\')\n\ndef representative_data_gen():\n  fimage = open(FLAGS.dataset).read().split()\n  for input_value in range(FLAGS.loop):\n    if os.path.exists(fimage[input_value]):\n      original_image=cv2.imread(fimage[input_value])\n      original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n      image_data = utils.image_preprocess(np.copy(original_image), [FLAGS.input_size, FLAGS.input_size])\n      img_in = image_data[np.newaxis, ...].astype(np.float32)\n      batched_input = tf.constant(img_in)\n      print(input_value)\n      yield (batched_input, )\n    else:\n      continue\n\ndef save_trt():\n  if FLAGS.quantize_mode == \'int8\':\n    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n      precision_mode=trt.TrtPrecisionMode.INT8,\n      max_workspace_size_bytes=8000000000,\n      use_calibration=True,\n      max_batch_size=32)\n    converter = trt.TrtGraphConverterV2(\n      input_saved_model_dir=FLAGS.weights,\n      conversion_params=conversion_params)\n    converter.convert(calibration_input_fn=representative_data_gen)\n  elif FLAGS.quantize_mode == \'float16\':\n    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n      precision_mode=trt.TrtPrecisionMode.FP16,\n      max_workspace_size_bytes=8000000000,\n      max_batch_size=32)\n    converter = trt.TrtGraphConverterV2(\n      input_saved_model_dir=FLAGS.weights, conversion_params=conversion_params)\n    converter.convert()\n  else :\n    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n      precision_mode=trt.TrtPrecisionMode.FP32,\n      max_workspace_size_bytes=8000000000,\n      max_batch_size=32)\n    converter = trt.TrtGraphConverterV2(\n      input_saved_model_dir=FLAGS.weights, conversion_params=conversion_params)\n    converter.convert()\n\n  # converter.build(input_fn=representative_data_gen)\n  converter.save(output_saved_model_dir=FLAGS.output)\n  print(\'Done Converting to TF-TRT\')\n\n  saved_model_loaded = tf.saved_model.load(FLAGS.output)\n  graph_func = saved_model_loaded.signatures[\n    signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n  trt_graph = graph_func.graph.as_graph_def()\n  for n in trt_graph.node:\n    print(n.op)\n    if n.op == ""TRTEngineOp"":\n      print(""Node: %s, %s"" % (n.op, n.name.replace(""/"", ""_"")))\n    else:\n      print(""Exclude Node: %s, %s"" % (n.op, n.name.replace(""/"", ""_"")))\n  logging.info(""model saved to: {}"".format(FLAGS.output))\n\n  trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == \'TRTEngineOp\'])\n  print(""numb. of trt_engine_nodes in TensorRT graph:"", trt_engine_nodes)\n  all_nodes = len([1 for n in trt_graph.node])\n  print(""numb. of all_nodes in TensorRT graph:"", all_nodes)\n\ndef main(_argv):\n  config = ConfigProto()\n  config.gpu_options.allow_growth = True\n  session = InteractiveSession(config=config)\n  save_trt()\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass\n\n\n'"
detect.py,5,"b'import time\nfrom absl import app, flags, logging\nfrom absl.flags import FLAGS\nimport core.utils as utils\nfrom core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\nfrom PIL import Image\nfrom core.config import cfg\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nflags.DEFINE_string(\'framework\', \'tf\', \'(tf, tflite\')\nflags.DEFINE_string(\'weights\', \'./data/yolov4.weights\',\n                    \'path to weights file\')\nflags.DEFINE_integer(\'size\', 608, \'resize images to\')\nflags.DEFINE_boolean(\'tiny\', False, \'yolo or yolo-tiny\')\nflags.DEFINE_string(\'model\', \'yolov4\', \'yolov3 or yolov4\')\nflags.DEFINE_string(\'image\', \'./data/kite.jpg\', \'path to input image\')\nflags.DEFINE_string(\'output\', \'result.png\', \'path to output image\')\n\ndef main(_argv):\n    if FLAGS.tiny:\n        STRIDES = np.array(cfg.YOLO.STRIDES_TINY)\n        ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS_TINY, FLAGS.tiny)\n    else:\n        STRIDES = np.array(cfg.YOLO.STRIDES)\n        if FLAGS.model == \'yolov4\':\n            ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS, FLAGS.tiny)\n        else:\n            ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS_V3, FLAGS.tiny)\n    NUM_CLASS = len(utils.read_class_names(cfg.YOLO.CLASSES))\n    XYSCALE = cfg.YOLO.XYSCALE\n    input_size = FLAGS.size\n    image_path = FLAGS.image\n\n    original_image = cv2.imread(image_path)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n    original_image_size = original_image.shape[:2]\n\n    image_data = utils.image_preprocess(np.copy(original_image), [input_size, input_size])\n    image_data = image_data[np.newaxis, ...].astype(np.float32)\n    if FLAGS.framework == \'tf\':\n        input_layer = tf.keras.layers.Input([input_size, input_size, 3])\n        if FLAGS.tiny:\n            feature_maps = YOLOv3_tiny(input_layer, NUM_CLASS)\n            bbox_tensors = []\n            for i, fm in enumerate(feature_maps):\n                bbox_tensor = decode(fm, NUM_CLASS, i)\n                bbox_tensors.append(bbox_tensor)\n            model = tf.keras.Model(input_layer, bbox_tensors)\n            utils.load_weights_tiny(model, FLAGS.weights)\n        else:\n            if FLAGS.model == \'yolov3\':\n                feature_maps = YOLOv3(input_layer, NUM_CLASS)\n                bbox_tensors = []\n                for i, fm in enumerate(feature_maps):\n                    bbox_tensor = decode(fm, NUM_CLASS, i)\n                    bbox_tensors.append(bbox_tensor)\n                model = tf.keras.Model(input_layer, bbox_tensors)\n                utils.load_weights_v3(model, FLAGS.weights)\n            elif FLAGS.model == \'yolov4\':\n                feature_maps = YOLOv4(input_layer, NUM_CLASS)\n                bbox_tensors = []\n                for i, fm in enumerate(feature_maps):\n                    bbox_tensor = decode(fm, NUM_CLASS, i)\n                    bbox_tensors.append(bbox_tensor)\n                model = tf.keras.Model(input_layer, bbox_tensors)\n\n                if FLAGS.weights.split(""."")[len(FLAGS.weights.split(""."")) - 1] == ""weights"":\n                    utils.load_weights(model, FLAGS.weights)\n                else:\n                    model.load_weights(FLAGS.weights).expect_partial()\n\n        model.summary()\n        pred_bbox = model.predict(image_data)\n    else:\n        # Load TFLite model and allocate tensors.\n        interpreter = tf.lite.Interpreter(model_path=FLAGS.weights)\n        interpreter.allocate_tensors()\n        # Get input and output tensors.\n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n        print(input_details)\n        print(output_details)\n        interpreter.set_tensor(input_details[0][\'index\'], image_data)\n        interpreter.invoke()\n        pred_bbox = [interpreter.get_tensor(output_details[i][\'index\']) for i in range(len(output_details))]\n\n    if FLAGS.model == \'yolov4\':\n        pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES, XYSCALE)\n    else:\n        pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES)\n    bboxes = utils.postprocess_boxes(pred_bbox, original_image_size, input_size, 0.25)\n    bboxes = utils.nms(bboxes, 0.213, method=\'nms\')\n\n    image = utils.draw_bbox(original_image, bboxes)\n    image = Image.fromarray(image)\n    image.show()\n    # image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)\n    # cv2.imwrite(FLAGS.output, image)\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass\n'"
detectvideo.py,5,"b'import time\nfrom absl import app, flags, logging\nfrom absl.flags import FLAGS\nimport core.utils as utils\nfrom core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\nfrom PIL import Image\nfrom core.config import cfg\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nflags.DEFINE_string(\'framework\', \'tf\', \'(tf, tflite\')\nflags.DEFINE_string(\'weights\', \'./data/yolov4.weights\',\n                    \'path to weights file\')\nflags.DEFINE_integer(\'size\', 608, \'resize images to\')\nflags.DEFINE_boolean(\'tiny\', False, \'yolo or yolo-tiny\')\nflags.DEFINE_string(\'model\', \'yolov4\', \'yolov3 or yolov4\')\nflags.DEFINE_string(\'video\', \'./data/road.avi\', \'path to input video\')\n\ndef main(_argv):\n    if FLAGS.tiny:\n        STRIDES = np.array(cfg.YOLO.STRIDES_TINY)\n        ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS_TINY, FLAGS.tiny)\n    else:\n        STRIDES = np.array(cfg.YOLO.STRIDES)\n        if FLAGS.model == \'yolov4\':\n            ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS, FLAGS.tiny)\n        else:\n            ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS_V3, FLAGS.tiny)\n    NUM_CLASS = len(utils.read_class_names(cfg.YOLO.CLASSES))\n    XYSCALE = cfg.YOLO.XYSCALE\n    input_size = FLAGS.size\n    video_path = FLAGS.video\n\n    print(""Video from: "", video_path )\n    vid = cv2.VideoCapture(video_path)\n\n    if FLAGS.framework == \'tf\':\n        input_layer = tf.keras.layers.Input([input_size, input_size, 3])\n        if FLAGS.tiny:\n            feature_maps = YOLOv3_tiny(input_layer, NUM_CLASS)\n            bbox_tensors = []\n            for i, fm in enumerate(feature_maps):\n                bbox_tensor = decode(fm, NUM_CLASS, i)\n                bbox_tensors.append(bbox_tensor)\n            model = tf.keras.Model(input_layer, bbox_tensors)\n            utils.load_weights_tiny(model, FLAGS.weights)\n        else:\n            if FLAGS.model == \'yolov3\':\n                feature_maps = YOLOv3(input_layer, NUM_CLASS)\n                bbox_tensors = []\n                for i, fm in enumerate(feature_maps):\n                    bbox_tensor = decode(fm, NUM_CLASS, i)\n                    bbox_tensors.append(bbox_tensor)\n                model = tf.keras.Model(input_layer, bbox_tensors)\n                utils.load_weights_v3(model, FLAGS.weights)\n            elif FLAGS.model == \'yolov4\':\n                feature_maps = YOLOv4(input_layer, NUM_CLASS)\n                bbox_tensors = []\n                for i, fm in enumerate(feature_maps):\n                    bbox_tensor = decode(fm, NUM_CLASS, i)\n                    bbox_tensors.append(bbox_tensor)\n                model = tf.keras.Model(input_layer, bbox_tensors)\n                \n                if FLAGS.weights.split(""."")[len(FLAGS.weights.split(""."")) - 1] == ""weights"":\n                    utils.load_weights(model, FLAGS.weights)\n                else:\n                    model.load_weights(FLAGS.weights).expect_partial()\n\n        model.summary()\n    else:\n        # Load TFLite model and allocate tensors.\n        interpreter = tf.lite.Interpreter(model_path=FLAGS.weights)\n        interpreter.allocate_tensors()\n        # Get input and output tensors.\n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n        print(input_details)\n        print(output_details)\n\n    while True:\n        return_value, frame = vid.read()\n        if return_value:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            image = Image.fromarray(frame)\n        else:\n            raise ValueError(""No image! Try with another video format"")\n        frame_size = frame.shape[:2]\n        image_data = utils.image_preprocess(np.copy(frame), [input_size, input_size])\n        image_data = image_data[np.newaxis, ...].astype(np.float32)\n        prev_time = time.time()\n\n        if FLAGS.framework == \'tf\':\n            pred_bbox = model.predict(image_data)\n        else:\n            interpreter.set_tensor(input_details[0][\'index\'], image_data)\n            interpreter.invoke()\n            pred_bbox = [interpreter.get_tensor(output_details[i][\'index\']) for i in range(len(output_details))]\n\n        if FLAGS.model == \'yolov4\':\n            pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES, XYSCALE)\n        else:\n            pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES)\n\n        bboxes = utils.postprocess_boxes(pred_bbox, frame_size, input_size, 0.25)\n        bboxes = utils.nms(bboxes, 0.213, method=\'nms\')\n\n        image = utils.draw_bbox(frame, bboxes)\n        curr_time = time.time()\n        exec_time = curr_time - prev_time\n        result = np.asarray(image)\n        info = ""time: %.2f ms"" %(1000*exec_time)\n        print(info)\n        cv2.namedWindow(""result"", cv2.WINDOW_AUTOSIZE)\n        result = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        cv2.imshow(""result"", result)\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'): break\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass\n'"
evaluate.py,6,"b'from absl import app, flags, logging\nfrom absl.flags import FLAGS\nimport cv2\nimport os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom core.config import cfg\nfrom core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\n\nflags.DEFINE_string(\'weights\', \'./data/yolov4.weights\',\n                    \'path to weights file\')\nflags.DEFINE_string(\'framework\', \'tf\', \'select model type in (tf, tflite)\'\n                    \'path to weights file\')\nflags.DEFINE_string(\'model\', \'yolov4\', \'yolov3 or yolov4\')\nflags.DEFINE_boolean(\'tiny\', False, \'yolov3 or yolov3-tiny\')\nflags.DEFINE_integer(\'size\', 512, \'resize images to\')\nflags.DEFINE_string(\'annotation_path\', ""./data/dataset/val2017.txt"", \'annotation path\')\nflags.DEFINE_string(\'write_image_path\', ""./data/detection/"", \'write image path\')\n\ndef main(_argv):\n    INPUT_SIZE = FLAGS.size\n    if FLAGS.tiny:\n        STRIDES = np.array(cfg.YOLO.STRIDES_TINY)\n        ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS_TINY, FLAGS.tiny)\n    else:\n        STRIDES = np.array(cfg.YOLO.STRIDES)\n        if FLAGS.model == \'yolov4\':\n            ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS, FLAGS.tiny)\n        else:\n            ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS_V3, FLAGS.tiny)\n    NUM_CLASS = len(utils.read_class_names(cfg.YOLO.CLASSES))\n    CLASSES = utils.read_class_names(cfg.YOLO.CLASSES)\n    predicted_dir_path = \'./mAP/predicted\'\n    ground_truth_dir_path = \'./mAP/ground-truth\'\n    if os.path.exists(predicted_dir_path): shutil.rmtree(predicted_dir_path)\n    if os.path.exists(ground_truth_dir_path): shutil.rmtree(ground_truth_dir_path)\n    if os.path.exists(cfg.TEST.DECTECTED_IMAGE_PATH): shutil.rmtree(cfg.TEST.DECTECTED_IMAGE_PATH)\n\n    os.mkdir(predicted_dir_path)\n    os.mkdir(ground_truth_dir_path)\n    os.mkdir(cfg.TEST.DECTECTED_IMAGE_PATH)\n\n    # Build Model\n    if FLAGS.framework == \'tf\':\n        input_layer = tf.keras.layers.Input([INPUT_SIZE, INPUT_SIZE, 3])\n        if FLAGS.tiny:\n            feature_maps = YOLOv3_tiny(input_layer, NUM_CLASS)\n            bbox_tensors = []\n            for i, fm in enumerate(feature_maps):\n                bbox_tensor = decode(fm, NUM_CLASS, i)\n                bbox_tensors.append(bbox_tensor)\n            model = tf.keras.Model(input_layer, bbox_tensors)\n            utils.load_weights_tiny(model, FLAGS.weights)\n        else:\n            if FLAGS.model == \'yolov3\':\n                feature_maps = YOLOv3(input_layer, NUM_CLASS)\n                bbox_tensors = []\n                for i, fm in enumerate(feature_maps):\n                    bbox_tensor = decode(fm, NUM_CLASS, i)\n                    bbox_tensors.append(bbox_tensor)\n                model = tf.keras.Model(input_layer, bbox_tensors)\n                utils.load_weights_v3(model, FLAGS.weights)\n            elif FLAGS.model == \'yolov4\':\n                feature_maps = YOLOv4(input_layer, NUM_CLASS)\n                bbox_tensors = []\n                for i, fm in enumerate(feature_maps):\n                    bbox_tensor = decode(fm, NUM_CLASS, i)\n                    bbox_tensors.append(bbox_tensor)\n                model = tf.keras.Model(input_layer, bbox_tensors)\n                utils.load_weights(model, FLAGS.weights)\n\n    else:\n        # Load TFLite model and allocate tensors.\n        interpreter = tf.lite.Interpreter(model_path=FLAGS.weights)\n        interpreter.allocate_tensors()\n        # Get input and output tensors.\n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n        print(input_details)\n        print(output_details)\n\n    num_lines = sum(1 for line in open(FLAGS.annotation_path))\n    with open(cfg.TEST.ANNOT_PATH, \'r\') as annotation_file:\n        for num, line in enumerate(annotation_file):\n            annotation = line.strip().split()\n            image_path = annotation[0]\n            image_name = image_path.split(\'/\')[-1]\n            image = cv2.imread(image_path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            bbox_data_gt = np.array([list(map(int, box.split(\',\'))) for box in annotation[1:]])\n\n            if len(bbox_data_gt) == 0:\n                bboxes_gt = []\n                classes_gt = []\n            else:\n                bboxes_gt, classes_gt = bbox_data_gt[:, :4], bbox_data_gt[:, 4]\n            ground_truth_path = os.path.join(ground_truth_dir_path, str(num) + \'.txt\')\n\n            print(\'=> ground truth of %s:\' % image_name)\n            num_bbox_gt = len(bboxes_gt)\n            with open(ground_truth_path, \'w\') as f:\n                for i in range(num_bbox_gt):\n                    class_name = CLASSES[classes_gt[i]]\n                    xmin, ymin, xmax, ymax = list(map(str, bboxes_gt[i]))\n                    bbox_mess = \' \'.join([class_name, xmin, ymin, xmax, ymax]) + \'\\n\'\n                    f.write(bbox_mess)\n                    print(\'\\t\' + str(bbox_mess).strip())\n            print(\'=> predict result of %s:\' % image_name)\n            predict_result_path = os.path.join(predicted_dir_path, str(num) + \'.txt\')\n            # Predict Process\n            image_size = image.shape[:2]\n            image_data = utils.image_preprocess(np.copy(image), [INPUT_SIZE, INPUT_SIZE])\n            image_data = image_data[np.newaxis, ...].astype(np.float32)\n\n            if FLAGS.framework == ""tf"":\n                pred_bbox = model.predict(image_data)\n            else:\n                interpreter.set_tensor(input_details[0][\'index\'], image_data)\n                interpreter.invoke()\n                pred_bbox = [interpreter.get_tensor(output_details[i][\'index\']) for i in range(len(output_details))]\n            if FLAGS.model == \'yolov3\':\n                pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES)\n            elif FLAGS.model == \'yolov4\':\n                XYSCALE = cfg.YOLO.XYSCALE\n                pred_bbox = utils.postprocess_bbbox(pred_bbox, ANCHORS, STRIDES, XYSCALE=XYSCALE)\n\n            pred_bbox = tf.concat(pred_bbox, axis=0)\n            bboxes = utils.postprocess_boxes(pred_bbox, image_size, INPUT_SIZE, cfg.TEST.SCORE_THRESHOLD)\n            bboxes = utils.nms(bboxes, cfg.TEST.IOU_THRESHOLD, method=\'nms\')\n\n            if cfg.TEST.DECTECTED_IMAGE_PATH is not None:\n                image = utils.draw_bbox(image, bboxes)\n                cv2.imwrite(cfg.TEST.DECTECTED_IMAGE_PATH + image_name, image)\n\n            with open(predict_result_path, \'w\') as f:\n                for bbox in bboxes:\n                    coor = np.array(bbox[:4], dtype=np.int32)\n                    score = bbox[4]\n                    class_ind = int(bbox[5])\n                    class_name = CLASSES[class_ind]\n                    score = \'%.4f\' % score\n                    xmin, ymin, xmax, ymax = list(map(str, coor))\n                    bbox_mess = \' \'.join([class_name, score, xmin, ymin, xmax, ymax]) + \'\\n\'\n                    f.write(bbox_mess)\n                    print(\'\\t\' + str(bbox_mess).strip())\n            print(num, num_lines)\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass\n\n\n'"
save_model.py,4,"b'import tensorflow as tf\nfrom absl import app, flags, logging\nfrom absl.flags import FLAGS\nfrom core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\nimport core.utils as utils\nfrom core.config import cfg\n\nflags.DEFINE_string(\'weights\', \'./data/yolov4.weights\', \'path to weights file\')\nflags.DEFINE_string(\'output\', \'./checkpoints/yolov4-416\', \'path to output\')\nflags.DEFINE_boolean(\'tiny\', False, \'is yolov3-tiny or not\')\nflags.DEFINE_integer(\'input_size\', 416, \'define input size of export model\')\nflags.DEFINE_string(\'model\', \'yolov4\', \'yolov3 or yolov4\')\n\ndef save_tf():\n  NUM_CLASS = len(utils.read_class_names(cfg.YOLO.CLASSES))\n  input_layer = tf.keras.layers.Input([FLAGS.input_size, FLAGS.input_size, 3])\n  if FLAGS.tiny:\n    feature_maps = YOLOv3_tiny(input_layer, NUM_CLASS)\n    bbox_tensors = []\n    for i, fm in enumerate(feature_maps):\n      bbox_tensor = decode(fm, NUM_CLASS, i)\n      bbox_tensors.append(bbox_tensor)\n    model = tf.keras.Model(input_layer, bbox_tensors)\n    utils.load_weights_tiny(model, FLAGS.weights)\n  else:\n    if FLAGS.model == \'yolov3\':\n      feature_maps = YOLOv3(input_layer, NUM_CLASS)\n      bbox_tensors = []\n      for i, fm in enumerate(feature_maps):\n        bbox_tensor = decode(fm, NUM_CLASS, i)\n        bbox_tensors.append(bbox_tensor)\n      model = tf.keras.Model(input_layer, bbox_tensors)\n      utils.load_weights_v3(model, FLAGS.weights)\n    elif FLAGS.model == \'yolov4\':\n      feature_maps = YOLOv4(input_layer, NUM_CLASS)\n      bbox_tensors = []\n      for i, fm in enumerate(feature_maps):\n        bbox_tensor = decode(fm, NUM_CLASS, i)\n        bbox_tensors.append(bbox_tensor)\n      model = tf.keras.Model(input_layer, bbox_tensors)\n      utils.load_weights(model, FLAGS.weights)\n    else:\n      print(""model option can be only \'yolov3\' or \'yolov4\'."")\n      return\n\n  model.summary()\n\n  model.save(FLAGS.output)\n\ndef main(_argv):\n  save_tf()\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass\n\n'"
train.py,19,"b'from absl import app, flags, logging\nfrom absl.flags import FLAGS\nimport os\nimport shutil\nimport tensorflow as tf\nfrom core.yolov4 import YOLOv4,YOLOv3, YOLOv3_tiny, decode, compute_loss, decode_train\nfrom core.dataset import Dataset\nfrom core.config import cfg\nimport numpy as np\nfrom core import utils\nfrom core.utils import freeze_all, unfreeze_all\n\nflags.DEFINE_string(\'model\', \'yolov4\', \'yolov4, yolov3 or yolov3-tiny\')\nflags.DEFINE_string(\'weights\', \'./data/yolov4.weights\', \'pretrained weights\')\nflags.DEFINE_boolean(\'tiny\', False, \'yolo or yolo-tiny\')\n\ndef main(_argv):\n    physical_devices = tf.config.experimental.list_physical_devices(\'GPU\')\n    if len(physical_devices) > 0:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n\n    trainset = Dataset(\'train\')\n    testset = Dataset(\'test\')\n    logdir = ""./data/log""\n    isfreeze = False\n    steps_per_epoch = len(trainset)\n    first_stage_epochs = cfg.TRAIN.FISRT_STAGE_EPOCHS\n    second_stage_epochs = cfg.TRAIN.SECOND_STAGE_EPOCHS\n    global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\n    warmup_steps = cfg.TRAIN.WARMUP_EPOCHS * steps_per_epoch\n    total_steps = (first_stage_epochs + second_stage_epochs) * steps_per_epoch\n    # train_steps = (first_stage_epochs + second_stage_epochs) * steps_per_period\n\n    input_layer = tf.keras.layers.Input([cfg.TRAIN.INPUT_SIZE, cfg.TRAIN.INPUT_SIZE, 3])\n    NUM_CLASS = len(utils.read_class_names(cfg.YOLO.CLASSES))\n    STRIDES         = np.array(cfg.YOLO.STRIDES)\n    IOU_LOSS_THRESH = cfg.YOLO.IOU_LOSS_THRESH\n    XYSCALE = cfg.YOLO.XYSCALE\n    ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS)\n\n    if FLAGS.tiny:\n        feature_maps = YOLOv3_tiny(input_layer, NUM_CLASS)\n        bbox_tensors = []\n        for i, fm in enumerate(feature_maps):\n            bbox_tensor = decode_train(fm, NUM_CLASS, STRIDES, ANCHORS, i)\n            bbox_tensors.append(fm)\n            bbox_tensors.append(bbox_tensor)\n        model = tf.keras.Model(input_layer, bbox_tensors)\n    else:\n        if FLAGS.model == \'yolov3\':\n            feature_maps = YOLOv3(input_layer, NUM_CLASS)\n            bbox_tensors = []\n            for i, fm in enumerate(feature_maps):\n                bbox_tensor = decode_train(fm, NUM_CLASS, STRIDES, ANCHORS, i)\n                bbox_tensors.append(fm)\n                bbox_tensors.append(bbox_tensor)\n            model = tf.keras.Model(input_layer, bbox_tensors)\n        elif FLAGS.model == \'yolov4\':\n            feature_maps = YOLOv4(input_layer, NUM_CLASS)\n            bbox_tensors = []\n            for i, fm in enumerate(feature_maps):\n                bbox_tensor = decode_train(fm, NUM_CLASS, STRIDES, ANCHORS, i, XYSCALE)\n                bbox_tensors.append(fm)\n                bbox_tensors.append(bbox_tensor)\n            model = tf.keras.Model(input_layer, bbox_tensors)\n\n    if FLAGS.weights == None:\n        print(""Training from scratch"")\n    else:\n        if FLAGS.weights.split(""."")[len(FLAGS.weights.split(""."")) - 1] == ""weights"":\n            if FLAGS.tiny:\n                utils.load_weights_tiny(model, FLAGS.weights)\n            else:\n                if FLAGS.model == \'yolov3\':\n                    utils.load_weights_v3(model, FLAGS.weights)\n                else:\n                    utils.load_weights(model, FLAGS.weights)\n        else:\n            model.load_weights(FLAGS.weights)\n        print(\'Restoring weights from: %s ... \' % FLAGS.weights)\n\n\n    optimizer = tf.keras.optimizers.Adam()\n    if os.path.exists(logdir): shutil.rmtree(logdir)\n    writer = tf.summary.create_file_writer(logdir)\n\n    def train_step(image_data, target):\n        with tf.GradientTape() as tape:\n            pred_result = model(image_data, training=True)\n            giou_loss = conf_loss = prob_loss = 0\n\n            # optimizing process\n            for i in range(3):\n                conv, pred = pred_result[i * 2], pred_result[i * 2 + 1]\n                loss_items = compute_loss(pred, conv, target[i][0], target[i][1], STRIDES=STRIDES, NUM_CLASS=NUM_CLASS, IOU_LOSS_THRESH=IOU_LOSS_THRESH, i=i)\n                giou_loss += loss_items[0]\n                conf_loss += loss_items[1]\n                prob_loss += loss_items[2]\n\n            total_loss = giou_loss + conf_loss + prob_loss\n\n            gradients = tape.gradient(total_loss, model.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n            tf.print(""=> STEP %4d   lr: %.6f   giou_loss: %4.2f   conf_loss: %4.2f   ""\n                     ""prob_loss: %4.2f   total_loss: %4.2f"" % (global_steps, optimizer.lr.numpy(),\n                                                               giou_loss, conf_loss,\n                                                               prob_loss, total_loss))\n            # update learning rate\n            global_steps.assign_add(1)\n            if global_steps < warmup_steps:\n                lr = global_steps / warmup_steps * cfg.TRAIN.LR_INIT\n            else:\n                lr = cfg.TRAIN.LR_END + 0.5 * (cfg.TRAIN.LR_INIT - cfg.TRAIN.LR_END) * (\n                    (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi))\n                )\n            optimizer.lr.assign(lr.numpy())\n\n            # writing summary data\n            with writer.as_default():\n                tf.summary.scalar(""lr"", optimizer.lr, step=global_steps)\n                tf.summary.scalar(""loss/total_loss"", total_loss, step=global_steps)\n                tf.summary.scalar(""loss/giou_loss"", giou_loss, step=global_steps)\n                tf.summary.scalar(""loss/conf_loss"", conf_loss, step=global_steps)\n                tf.summary.scalar(""loss/prob_loss"", prob_loss, step=global_steps)\n            writer.flush()\n    def test_step(image_data, target):\n        with tf.GradientTape() as tape:\n            pred_result = model(image_data, training=True)\n            giou_loss = conf_loss = prob_loss = 0\n\n            # optimizing process\n            for i in range(3):\n                conv, pred = pred_result[i * 2], pred_result[i * 2 + 1]\n                loss_items = compute_loss(pred, conv, target[i][0], target[i][1], STRIDES=STRIDES, NUM_CLASS=NUM_CLASS, IOU_LOSS_THRESH=IOU_LOSS_THRESH, i=i)\n                giou_loss += loss_items[0]\n                conf_loss += loss_items[1]\n                prob_loss += loss_items[2]\n\n            total_loss = giou_loss + conf_loss + prob_loss\n\n            tf.print(""=> TEST STEP %4d   giou_loss: %4.2f   conf_loss: %4.2f   ""\n                     ""prob_loss: %4.2f   total_loss: %4.2f"" % (global_steps, giou_loss, conf_loss,\n                                                               prob_loss, total_loss))\n\n    for epoch in range(first_stage_epochs + second_stage_epochs):\n        if epoch < first_stage_epochs:\n            if not isfreeze:\n                isfreeze = True\n                for name in [\'conv2d_93\', \'conv2d_101\', \'conv2d_109\']:\n                    freeze = model.get_layer(name)\n                    freeze_all(freeze)\n        elif epoch >= first_stage_epochs:\n            if isfreeze:\n                isfreeze = False\n                for name in [\'conv2d_93\', \'conv2d_101\', \'conv2d_109\']:\n                    freeze = model.get_layer(name)\n                    unfreeze_all(freeze)\n        for image_data, target in trainset:\n            train_step(image_data, target)\n        for image_data, target in testset:\n            test_step(image_data, target)\n        model.save_weights(""./checkpoints/yolov4"")\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass'"
core/backbone.py,13,"b'#! /usr/bin/env python\n# coding=utf-8\n\nimport tensorflow as tf\nimport core.common as common\n\ndef darknet53(input_data):\n\n    input_data = common.convolutional(input_data, (3, 3,  3,  32))\n    input_data = common.convolutional(input_data, (3, 3, 32,  64), downsample=True)\n\n    for i in range(1):\n        input_data = common.residual_block(input_data,  64,  32, 64)\n\n    input_data = common.convolutional(input_data, (3, 3,  64, 128), downsample=True)\n\n    for i in range(2):\n        input_data = common.residual_block(input_data, 128,  64, 128)\n\n    input_data = common.convolutional(input_data, (3, 3, 128, 256), downsample=True)\n\n    for i in range(8):\n        input_data = common.residual_block(input_data, 256, 128, 256)\n\n    route_1 = input_data\n    input_data = common.convolutional(input_data, (3, 3, 256, 512), downsample=True)\n\n    for i in range(8):\n        input_data = common.residual_block(input_data, 512, 256, 512)\n\n    route_2 = input_data\n    input_data = common.convolutional(input_data, (3, 3, 512, 1024), downsample=True)\n\n    for i in range(4):\n        input_data = common.residual_block(input_data, 1024, 512, 1024)\n\n    return route_1, route_2, input_data\n\ndef cspdarknet53(input_data):\n\n    input_data = common.convolutional(input_data, (3, 3,  3,  32), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (3, 3, 32,  64), downsample=True, activate_type=""mish"")\n\n    route = input_data\n    route = common.convolutional(route, (1, 1, 64, 64), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 64, 64), activate_type=""mish"")\n    for i in range(1):\n        input_data = common.residual_block(input_data,  64,  32, 64, activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 64, 64), activate_type=""mish"")\n\n    input_data = tf.concat([input_data, route], axis=-1)\n    input_data = common.convolutional(input_data, (1, 1, 128, 64), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (3, 3, 64, 128), downsample=True, activate_type=""mish"")\n    route = input_data\n    route = common.convolutional(route, (1, 1, 128, 64), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 128, 64), activate_type=""mish"")\n    for i in range(2):\n        input_data = common.residual_block(input_data, 64,  64, 64, activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 64, 64), activate_type=""mish"")\n    input_data = tf.concat([input_data, route], axis=-1)\n\n    input_data = common.convolutional(input_data, (1, 1, 128, 128), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (3, 3, 128, 256), downsample=True, activate_type=""mish"")\n    route = input_data\n    route = common.convolutional(route, (1, 1, 256, 128), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 256, 128), activate_type=""mish"")\n    for i in range(8):\n        input_data = common.residual_block(input_data, 128, 128, 128, activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 128, 128), activate_type=""mish"")\n    input_data = tf.concat([input_data, route], axis=-1)\n\n    input_data = common.convolutional(input_data, (1, 1, 256, 256), activate_type=""mish"")\n    route_1 = input_data\n    input_data = common.convolutional(input_data, (3, 3, 256, 512), downsample=True, activate_type=""mish"")\n    route = input_data\n    route = common.convolutional(route, (1, 1, 512, 256), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 512, 256), activate_type=""mish"")\n    for i in range(8):\n        input_data = common.residual_block(input_data, 256, 256, 256, activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 256, 256), activate_type=""mish"")\n    input_data = tf.concat([input_data, route], axis=-1)\n\n    input_data = common.convolutional(input_data, (1, 1, 512, 512), activate_type=""mish"")\n    route_2 = input_data\n    input_data = common.convolutional(input_data, (3, 3, 512, 1024), downsample=True, activate_type=""mish"")\n    route = input_data\n    route = common.convolutional(route, (1, 1, 1024, 512), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 1024, 512), activate_type=""mish"")\n    for i in range(4):\n        input_data = common.residual_block(input_data, 512, 512, 512, activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 512, 512), activate_type=""mish"")\n    input_data = tf.concat([input_data, route], axis=-1)\n\n    input_data = common.convolutional(input_data, (1, 1, 1024, 1024), activate_type=""mish"")\n    input_data = common.convolutional(input_data, (1, 1, 1024, 512))\n    input_data = common.convolutional(input_data, (3, 3, 512, 1024))\n    input_data = common.convolutional(input_data, (1, 1, 1024, 512))\n\n    input_data = tf.concat([tf.nn.max_pool(input_data, ksize=13, padding=\'SAME\', strides=1), tf.nn.max_pool(input_data, ksize=9, padding=\'SAME\', strides=1)\n                            , tf.nn.max_pool(input_data, ksize=5, padding=\'SAME\', strides=1), input_data], axis=-1)\n    input_data = common.convolutional(input_data, (1, 1, 2048, 512))\n    input_data = common.convolutional(input_data, (3, 3, 512, 1024))\n    input_data = common.convolutional(input_data, (1, 1, 1024, 512))\n\n    return route_1, route_2, input_data\n\ndef darknet53_tiny(input_data):\n    input_data = common.convolutional(input_data, (3, 3, 3, 16))\n    input_data = tf.keras.layers.MaxPool2D(2, 2, \'same\')(input_data)\n    input_data = common.convolutional(input_data, (3, 3, 16, 32))\n    input_data = tf.keras.layers.MaxPool2D(2, 2, \'same\')(input_data)\n    input_data = common.convolutional(input_data, (3, 3, 32, 64))\n    input_data = tf.keras.layers.MaxPool2D(2, 2, \'same\')(input_data)\n    input_data = common.convolutional(input_data, (3, 3, 64, 128))\n    input_data = tf.keras.layers.MaxPool2D(2, 2, \'same\')(input_data)\n    input_data = common.convolutional(input_data, (3, 3, 128, 256))\n    route_1 = input_data\n    input_data = tf.keras.layers.MaxPool2D(2, 2, \'same\')(input_data)\n    input_data = common.convolutional(input_data, (3, 3, 256, 512))\n    input_data = tf.keras.layers.MaxPool2D(2, 1, \'same\')(input_data)\n    input_data = common.convolutional(input_data, (3, 3, 512, 1024))\n\n    return route_1, input_data\n\n\n'"
core/common.py,26,"b'#! /usr/bin/env python\n# coding=utf-8\n\nimport tensorflow as tf\n# import tensorflow_addons as tfa\nclass BatchNormalization(tf.keras.layers.BatchNormalization):\n    """"""\n    ""Frozen state"" and ""inference mode"" are two separate concepts.\n    `layer.trainable = False` is to freeze the layer, so the layer will use\n    stored moving `var` and `mean` in the ""inference mode"", and both `gama`\n    and `beta` will not be updated !\n    """"""\n    def call(self, x, training=False):\n        if not training:\n            training = tf.constant(False)\n        training = tf.logical_and(training, self.trainable)\n        return super().call(x, training)\n\ndef convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True, activate_type=\'leaky\'):\n    if downsample:\n        input_layer = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n        padding = \'valid\'\n        strides = 2\n    else:\n        strides = 1\n        padding = \'same\'\n\n    conv = tf.keras.layers.Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides, padding=padding,\n                                  use_bias=not bn, kernel_regularizer=tf.keras.regularizers.l2(0.0005),\n                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n                                  bias_initializer=tf.constant_initializer(0.))(input_layer)\n\n    if bn: conv = BatchNormalization()(conv)\n    if activate == True:\n        if activate_type == ""leaky"":\n            conv = tf.nn.leaky_relu(conv, alpha=0.1)\n        elif activate_type == ""mish"":\n            conv = mish(conv)\n            # conv = softplus(conv)\n            # conv = conv * tf.math.tanh(tf.math.softplus(conv))\n            # conv = conv * tf.tanh(softplus(conv))\n            # conv = tf.nn.leaky_relu(conv, alpha=0.1)\n            # conv = tfa.activations.mish(conv)\n            # conv = conv * tf.nn.tanh(tf.keras.activations.relu(tf.nn.softplus(conv), max_value=20))\n            # conv = tf.nn.softplus(conv)\n            # conv = tf.keras.activations.relu(tf.nn.softplus(conv), max_value=20)\n\n    return conv\ndef softplus(x, threshold = 20.):\n    def f1():\n        return x\n    def f2():\n        return tf.exp(x)\n    def f3():\n        return tf.math.log(1 + tf.exp(x))\n    # mask = tf.greater(x, threshold)\n    # x = tf.exp(x[mask])\n    # return tf.exp(x)\n    return tf.case([(tf.greater(x, tf.constant(threshold)), lambda:f1()), (tf.less(x, tf.constant(-threshold)), lambda:f2())], default=lambda:f3())\n    # return tf.case([(tf.greater(x, threshold), lambda:f1())])\ndef mish(x):\n    return tf.keras.layers.Lambda(lambda x: x*tf.tanh(tf.math.log(1+tf.exp(x))))(x)\n    # return tf.keras.layers.Lambda(lambda x: softplus(x))(x)\n    # return tf.keras.layers.Lambda(lambda x: x * tf.tanh(softplus(x)))(x)\n\ndef residual_block(input_layer, input_channel, filter_num1, filter_num2, activate_type=\'leaky\'):\n    short_cut = input_layer\n    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1), activate_type=activate_type)\n    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2), activate_type=activate_type)\n\n    residual_output = short_cut + conv\n    return residual_output\n\ndef upsample(input_layer):\n    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method=\'nearest\')\n\n'"
core/config.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\nfrom easydict import EasyDict as edict\n\n\n__C                           = edict()\n# Consumers can get config by: from config import cfg\n\ncfg                           = __C\n\n# YOLO options\n__C.YOLO                      = edict()\n\n# Set the class name\n__C.YOLO.CLASSES              = ""./data/classes/coco.names""\n__C.YOLO.ANCHORS              = ""./data/anchors/yolov4_anchors.txt""\n__C.YOLO.ANCHORS_V3           = ""./data/anchors/yolov3_anchors.txt""\n__C.YOLO.ANCHORS_TINY         = ""./data/anchors/basline_tiny_anchors.txt""\n__C.YOLO.STRIDES              = [8, 16, 32]\n__C.YOLO.STRIDES_TINY         = [16, 32]\n__C.YOLO.XYSCALE              = [1.2, 1.1, 1.05]\n__C.YOLO.ANCHOR_PER_SCALE     = 3\n__C.YOLO.IOU_LOSS_THRESH      = 0.5\n\n\n# Train options\n__C.TRAIN                     = edict()\n\n__C.TRAIN.ANNOT_PATH          = ""./data/dataset/val2017.txt""\n__C.TRAIN.BATCH_SIZE          = 2\n# __C.TRAIN.INPUT_SIZE            = [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]\n__C.TRAIN.INPUT_SIZE          = 416\n__C.TRAIN.DATA_AUG            = True\n__C.TRAIN.LR_INIT             = 1e-3\n__C.TRAIN.LR_END              = 1e-6\n__C.TRAIN.WARMUP_EPOCHS       = 2\n__C.TRAIN.FISRT_STAGE_EPOCHS    = 20\n__C.TRAIN.SECOND_STAGE_EPOCHS   = 30\n\n\n\n# TEST options\n__C.TEST                      = edict()\n\n__C.TEST.ANNOT_PATH           = ""./data/dataset/val2017.txt""\n__C.TEST.BATCH_SIZE           = 2\n__C.TEST.INPUT_SIZE           = 416\n__C.TEST.DATA_AUG             = False\n__C.TEST.DECTECTED_IMAGE_PATH = ""./data/detection/""\n__C.TEST.SCORE_THRESHOLD      = 0.25\n__C.TEST.IOU_THRESHOLD        = 0.5\n\n\n'"
core/dataset.py,1,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : dataset.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-15 18:05:03\n#   Description :\n#\n#================================================================\n\nimport os\nimport cv2\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom core.config import cfg\n\n\n\nclass Dataset(object):\n    """"""implement Dataset here""""""\n    def __init__(self, dataset_type):\n        self.annot_path  = cfg.TRAIN.ANNOT_PATH if dataset_type == \'train\' else cfg.TEST.ANNOT_PATH\n        self.input_sizes = cfg.TRAIN.INPUT_SIZE if dataset_type == \'train\' else cfg.TEST.INPUT_SIZE\n        self.batch_size  = cfg.TRAIN.BATCH_SIZE if dataset_type == \'train\' else cfg.TEST.BATCH_SIZE\n        self.data_aug    = cfg.TRAIN.DATA_AUG   if dataset_type == \'train\' else cfg.TEST.DATA_AUG\n\n        self.train_input_sizes = cfg.TRAIN.INPUT_SIZE\n        self.strides = np.array(cfg.YOLO.STRIDES)\n        self.classes = utils.read_class_names(cfg.YOLO.CLASSES)\n        self.num_classes = len(self.classes)\n        self.anchors = np.array(utils.get_anchors(cfg.YOLO.ANCHORS))\n        self.anchor_per_scale = cfg.YOLO.ANCHOR_PER_SCALE\n        self.max_bbox_per_scale = 150\n\n        self.annotations = self.load_annotations(dataset_type)\n        self.num_samples = len(self.annotations)\n        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size))\n        self.batch_count = 0\n\n\n    def load_annotations(self, dataset_type):\n        with open(self.annot_path, \'r\') as f:\n            txt = f.readlines()\n            annotations = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]\n        np.random.shuffle(annotations)\n        return annotations\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n\n        with tf.device(\'/cpu:0\'):\n            # self.train_input_size = random.choice(self.train_input_sizes)\n            self.train_input_size = cfg.TRAIN.INPUT_SIZE\n            self.train_output_sizes = self.train_input_size // self.strides\n\n            batch_image = np.zeros((self.batch_size, self.train_input_size, self.train_input_size, 3), dtype=np.float32)\n\n            batch_label_sbbox = np.zeros((self.batch_size, self.train_output_sizes[0], self.train_output_sizes[0],\n                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n            batch_label_mbbox = np.zeros((self.batch_size, self.train_output_sizes[1], self.train_output_sizes[1],\n                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n            batch_label_lbbox = np.zeros((self.batch_size, self.train_output_sizes[2], self.train_output_sizes[2],\n                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n\n            batch_sbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n            batch_mbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n            batch_lbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n\n            num = 0\n            if self.batch_count < self.num_batchs:\n                while num < self.batch_size:\n                    index = self.batch_count * self.batch_size + num\n                    if index >= self.num_samples: index -= self.num_samples\n                    annotation = self.annotations[index]\n                    image, bboxes = self.parse_annotation(annotation)\n                    label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes)\n\n                    batch_image[num, :, :, :] = image\n                    batch_label_sbbox[num, :, :, :, :] = label_sbbox\n                    batch_label_mbbox[num, :, :, :, :] = label_mbbox\n                    batch_label_lbbox[num, :, :, :, :] = label_lbbox\n                    batch_sbboxes[num, :, :] = sbboxes\n                    batch_mbboxes[num, :, :] = mbboxes\n                    batch_lbboxes[num, :, :] = lbboxes\n                    num += 1\n                self.batch_count += 1\n                batch_smaller_target = batch_label_sbbox, batch_sbboxes\n                batch_medium_target  = batch_label_mbbox, batch_mbboxes\n                batch_larger_target  = batch_label_lbbox, batch_lbboxes\n\n                return batch_image, (batch_smaller_target, batch_medium_target, batch_larger_target)\n            else:\n                self.batch_count = 0\n                np.random.shuffle(self.annotations)\n                raise StopIteration\n\n    def random_horizontal_flip(self, image, bboxes):\n\n        if random.random() < 0.5:\n            _, w, _ = image.shape\n            image = image[:, ::-1, :]\n            bboxes[:, [0,2]] = w - bboxes[:, [2,0]]\n\n        return image, bboxes\n\n    def random_crop(self, image, bboxes):\n\n        if random.random() < 0.5:\n            h, w, _ = image.shape\n            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n\n            max_l_trans = max_bbox[0]\n            max_u_trans = max_bbox[1]\n            max_r_trans = w - max_bbox[2]\n            max_d_trans = h - max_bbox[3]\n\n            crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans)))\n            crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans)))\n            crop_xmax = max(w, int(max_bbox[2] + random.uniform(0, max_r_trans)))\n            crop_ymax = max(h, int(max_bbox[3] + random.uniform(0, max_d_trans)))\n\n            image = image[crop_ymin : crop_ymax, crop_xmin : crop_xmax]\n\n            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin\n            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin\n\n        return image, bboxes\n\n    def random_translate(self, image, bboxes):\n\n        if random.random() < 0.5:\n            h, w, _ = image.shape\n            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n\n            max_l_trans = max_bbox[0]\n            max_u_trans = max_bbox[1]\n            max_r_trans = w - max_bbox[2]\n            max_d_trans = h - max_bbox[3]\n\n            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n\n            M = np.array([[1, 0, tx], [0, 1, ty]])\n            image = cv2.warpAffine(image, M, (w, h))\n\n            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n\n        return image, bboxes\n\n    def parse_annotation(self, annotation):\n\n        line = annotation.split()\n        image_path = line[0]\n        if not os.path.exists(image_path):\n            raise KeyError(""%s does not exist ... "" %image_path)\n        image = cv2.imread(image_path)\n        bboxes = np.array([list(map(int, box.split(\',\'))) for box in line[1:]])\n\n        if self.data_aug:\n            image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))\n            image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))\n            image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image, bboxes = utils.image_preprocess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes))\n        return image, bboxes\n\n    def bbox_iou(self, boxes1, boxes2):\n\n        boxes1 = np.array(boxes1)\n        boxes2 = np.array(boxes2)\n\n        boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n        boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n\n        boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                                boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n        boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                                boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n        left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\n        right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n        inter_section = np.maximum(right_down - left_up, 0.0)\n        inter_area = inter_section[..., 0] * inter_section[..., 1]\n        union_area = boxes1_area + boxes2_area - inter_area\n\n        return inter_area / union_area\n\n    def preprocess_true_boxes(self, bboxes):\n\n        label = [np.zeros((self.train_output_sizes[i], self.train_output_sizes[i], self.anchor_per_scale,\n                           5 + self.num_classes)) for i in range(3)]\n        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]\n        bbox_count = np.zeros((3,))\n\n        for bbox in bboxes:\n            bbox_coor = bbox[:4]\n            bbox_class_ind = bbox[4]\n\n            onehot = np.zeros(self.num_classes, dtype=np.float)\n            onehot[bbox_class_ind] = 1.0\n            uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes)\n            deta = 0.01\n            smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution\n\n            bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n            bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]\n\n            iou = []\n            exist_positive = False\n            for i in range(3):\n                anchors_xywh = np.zeros((self.anchor_per_scale, 4))\n                anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n                anchors_xywh[:, 2:4] = self.anchors[i]\n\n                iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)\n                iou.append(iou_scale)\n                iou_mask = iou_scale > 0.3\n\n                if np.any(iou_mask):\n                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\n\n                    label[i][yind, xind, iou_mask, :] = 0\n                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n                    label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n\n                    bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)\n                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n                    bbox_count[i] += 1\n\n                    exist_positive = True\n\n            if not exist_positive:\n                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n                best_detect = int(best_anchor_ind / self.anchor_per_scale)\n                best_anchor = int(best_anchor_ind % self.anchor_per_scale)\n                xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n\n                label[best_detect][yind, xind, best_anchor, :] = 0\n                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n                label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n\n                bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)\n                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n                bbox_count[best_detect] += 1\n        label_sbbox, label_mbbox, label_lbbox = label\n        sbboxes, mbboxes, lbboxes = bboxes_xywh\n        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n\n    def __len__(self):\n        return self.num_batchs\n\n\n\n\n'"
core/utils.py,10,"b'import cv2\nimport random\nimport colorsys\nimport numpy as np\nimport tensorflow as tf\nfrom core.config import cfg\n\ndef load_weights_tiny(model, weights_file):\n    wf = open(weights_file, \'rb\')\n    major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n\n    j = 0\n    for i in range(13):\n        conv_layer_name = \'conv2d_%d\' % i if i > 0 else \'conv2d\'\n        bn_layer_name = \'batch_normalization_%d\' % j if j > 0 else \'batch_normalization\'\n\n        conv_layer = model.get_layer(conv_layer_name)\n        filters = conv_layer.filters\n        k_size = conv_layer.kernel_size[0]\n        in_dim = conv_layer.input_shape[-1]\n\n        if i not in [9, 12]:\n            # darknet weights: [beta, gamma, mean, variance]\n            bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n            # tf weights: [gamma, beta, mean, variance]\n            bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n            bn_layer = model.get_layer(bn_layer_name)\n            j += 1\n        else:\n            conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n\n        # darknet shape (out_dim, in_dim, height, width)\n        conv_shape = (filters, in_dim, k_size, k_size)\n        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n        # tf shape (height, width, in_dim, out_dim)\n        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n\n        if i not in [9, 12]:\n            conv_layer.set_weights([conv_weights])\n            bn_layer.set_weights(bn_weights)\n        else:\n            conv_layer.set_weights([conv_weights, conv_bias])\n\n    assert len(wf.read()) == 0, \'failed to read all data\'\n    wf.close()\n\ndef load_weights_v3(model, weights_file):\n    wf = open(weights_file, \'rb\')\n    major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n\n    j = 0\n    for i in range(75):\n        conv_layer_name = \'conv2d_%d\' % i if i > 0 else \'conv2d\'\n        bn_layer_name = \'batch_normalization_%d\' % j if j > 0 else \'batch_normalization\'\n\n        conv_layer = model.get_layer(conv_layer_name)\n        filters = conv_layer.filters\n        k_size = conv_layer.kernel_size[0]\n        in_dim = conv_layer.input_shape[-1]\n\n        if i not in [58, 66, 74]:\n            # darknet weights: [beta, gamma, mean, variance]\n            bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n            # tf weights: [gamma, beta, mean, variance]\n            bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n            bn_layer = model.get_layer(bn_layer_name)\n            j += 1\n        else:\n            conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n\n        # darknet shape (out_dim, in_dim, height, width)\n        conv_shape = (filters, in_dim, k_size, k_size)\n        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n        # tf shape (height, width, in_dim, out_dim)\n        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n\n        if i not in [58, 66, 74]:\n            conv_layer.set_weights([conv_weights])\n            bn_layer.set_weights(bn_weights)\n        else:\n            conv_layer.set_weights([conv_weights, conv_bias])\n\n    assert len(wf.read()) == 0, \'failed to read all data\'\n    wf.close()\n\ndef load_weights(model, weights_file):\n    wf = open(weights_file, \'rb\')\n    major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n\n    j = 0\n    for i in range(110):\n        conv_layer_name = \'conv2d_%d\' %i if i > 0 else \'conv2d\'\n        bn_layer_name = \'batch_normalization_%d\' %j if j > 0 else \'batch_normalization\'\n\n        conv_layer = model.get_layer(conv_layer_name)\n        filters = conv_layer.filters\n        k_size = conv_layer.kernel_size[0]\n        in_dim = conv_layer.input_shape[-1]\n\n        if i not in [93, 101, 109]:\n            # darknet weights: [beta, gamma, mean, variance]\n            bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n            # tf weights: [gamma, beta, mean, variance]\n            bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n            bn_layer = model.get_layer(bn_layer_name)\n            j += 1\n        else:\n            conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n\n        # darknet shape (out_dim, in_dim, height, width)\n        conv_shape = (filters, in_dim, k_size, k_size)\n        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n        # tf shape (height, width, in_dim, out_dim)\n        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n\n        if i not in [93, 101, 109]:\n            conv_layer.set_weights([conv_weights])\n            bn_layer.set_weights(bn_weights)\n        else:\n            conv_layer.set_weights([conv_weights, conv_bias])\n\n    assert len(wf.read()) == 0, \'failed to read all data\'\n    wf.close()\n\n\ndef read_class_names(class_file_name):\n    \'\'\'loads class name from a file\'\'\'\n    names = {}\n    with open(class_file_name, \'r\') as data:\n        for ID, name in enumerate(data):\n            names[ID] = name.strip(\'\\n\')\n    return names\n\n\ndef get_anchors(anchors_path, tiny=False):\n    \'\'\'loads the anchors from a file\'\'\'\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = np.array(anchors.split(\',\'), dtype=np.float32)\n    if tiny:\n        return anchors.reshape(2, 3, 2)\n    else:\n        return anchors.reshape(3, 3, 2)\n\n\ndef image_preprocess(image, target_size, gt_boxes=None):\n\n    ih, iw    = target_size\n    h,  w, _  = image.shape\n\n    scale = min(iw/w, ih/h)\n    nw, nh  = int(scale * w), int(scale * h)\n    image_resized = cv2.resize(image, (nw, nh))\n\n    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n    image_paded = image_paded / 255.\n\n    if gt_boxes is None:\n        return image_paded\n\n    else:\n        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n        return image_paded, gt_boxes\n\n\ndef draw_bbox(image, bboxes, classes=read_class_names(cfg.YOLO.CLASSES), show_label=True):\n    """"""\n    bboxes: [x_min, y_min, x_max, y_max, probability, cls_id] format coordinates.\n    """"""\n\n    num_classes = len(classes)\n    image_h, image_w, _ = image.shape\n    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n\n    random.seed(0)\n    random.shuffle(colors)\n    random.seed(None)\n\n    for i, bbox in enumerate(bboxes):\n        coor = np.array(bbox[:4], dtype=np.int32)\n        fontScale = 0.5\n        score = bbox[4]\n        class_ind = int(bbox[5])\n        bbox_color = colors[class_ind]\n        bbox_thick = int(0.6 * (image_h + image_w) / 600)\n        c1, c2 = (coor[0], coor[1]), (coor[2], coor[3])\n        cv2.rectangle(image, c1, c2, bbox_color, bbox_thick)\n\n        if show_label:\n            bbox_mess = \'%s: %.2f\' % (classes[class_ind], score)\n            t_size = cv2.getTextSize(bbox_mess, 0, fontScale, thickness=bbox_thick//2)[0]\n            cv2.rectangle(image, c1, (c1[0] + t_size[0], c1[1] - t_size[1] - 3), bbox_color, -1)  # filled\n\n            cv2.putText(image, bbox_mess, (c1[0], c1[1]-2), cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale, (0, 0, 0), bbox_thick//2, lineType=cv2.LINE_AA)\n\n    return image\n\n\n\ndef bboxes_iou(boxes1, boxes2):\n\n    boxes1 = np.array(boxes1)\n    boxes2 = np.array(boxes2)\n\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    inter_section = np.maximum(right_down - left_up, 0.0)\n    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n    union_area    = boxes1_area + boxes2_area - inter_area\n    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n\n    return ious\n\ndef bboxes_ciou(boxes1, boxes2):\n\n    boxes1 = np.array(boxes1)\n    boxes2 = np.array(boxes2)\n\n    left = np.maximum(boxes1[..., 0], boxes2[..., 0])\n    up = np.maximum(boxes1[..., 1], boxes2[..., 1])\n    right = np.maximum(boxes1[..., 2], boxes2[..., 2])\n    down = np.maximum(boxes1[..., 3], boxes2[..., 3])\n\n    c = (right - left) * (right - left) + (up - down) * (up - down)\n    iou = bboxes_iou(boxes1, boxes2)\n\n    ax = (boxes1[..., 0] + boxes1[..., 2]) / 2\n    ay = (boxes1[..., 1] + boxes1[..., 3]) / 2\n    bx = (boxes2[..., 0] + boxes2[..., 2]) / 2\n    by = (boxes2[..., 1] + boxes2[..., 3]) / 2\n\n    u = (ax - bx) * (ax - bx) + (ay - by) * (ay - by)\n    d = u/c\n\n    aw = boxes1[..., 2] - boxes1[..., 0]\n    ah = boxes1[..., 3] - boxes1[..., 1]\n    bw = boxes2[..., 2] - boxes2[..., 0]\n    bh = boxes2[..., 3] - boxes2[..., 1]\n\n    ar_gt = bw/bh\n    ar_pred = aw/ah\n\n    ar_loss = 4 / (np.pi * np.pi) * (np.arctan(ar_gt) - np.arctan(ar_pred)) * (np.arctan(ar_gt) - np.arctan(ar_pred))\n    alpha = ar_loss / (1 - iou + ar_loss + 0.000001)\n    ciou_term = d + alpha * ar_loss\n\n    return iou - ciou_term\n\ndef nms(bboxes, iou_threshold, sigma=0.3, method=\'nms\'):\n    """"""\n    :param bboxes: (xmin, ymin, xmax, ymax, score, class)\n\n    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\n          https://github.com/bharatsingh430/soft-nms\n    """"""\n    classes_in_img = list(set(bboxes[:, 5]))\n    best_bboxes = []\n\n    for cls in classes_in_img:\n        cls_mask = (bboxes[:, 5] == cls)\n        cls_bboxes = bboxes[cls_mask]\n\n        while len(cls_bboxes) > 0:\n            max_ind = np.argmax(cls_bboxes[:, 4])\n            best_bbox = cls_bboxes[max_ind]\n            best_bboxes.append(best_bbox)\n            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n            weight = np.ones((len(iou),), dtype=np.float32)\n\n            assert method in [\'nms\', \'soft-nms\']\n\n            if method == \'nms\':\n                iou_mask = iou > iou_threshold\n                weight[iou_mask] = 0.0\n\n            if method == \'soft-nms\':\n                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n\n            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n            score_mask = cls_bboxes[:, 4] > 0.\n            cls_bboxes = cls_bboxes[score_mask]\n\n    return best_bboxes\n\ndef diounms_sort(bboxes, iou_threshold, sigma=0.3, method=\'nms\', beta_nms=0.6):\n    best_bboxes = []\n    return best_bboxes\ndef postprocess_bbbox(pred_bbox, ANCHORS, STRIDES, XYSCALE=[1,1,1]):\n    for i, pred in enumerate(pred_bbox):\n        conv_shape = pred.shape\n        output_size = conv_shape[1]\n        conv_raw_dxdy = pred[:, :, :, :, 0:2]\n        conv_raw_dwdh = pred[:, :, :, :, 2:4]\n        xy_grid = np.meshgrid(np.arange(output_size), np.arange(output_size))\n        xy_grid = np.expand_dims(np.stack(xy_grid, axis=-1), axis=2)  # [gx, gy, 1, 2]\n\n        xy_grid = np.tile(tf.expand_dims(xy_grid, axis=0), [1, 1, 1, 3, 1])\n        xy_grid = xy_grid.astype(np.float)\n\n        # pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\n        pred_xy = ((tf.sigmoid(conv_raw_dxdy) * XYSCALE[i]) - 0.5 * (XYSCALE[i] - 1) + xy_grid) * STRIDES[i]\n        # pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\n        pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i])\n        pred[:, :, :, :, 0:4] = tf.concat([pred_xy, pred_wh], axis=-1)\n\n    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n    pred_bbox = tf.concat(pred_bbox, axis=0)\n    return pred_bbox\ndef postprocess_boxes(pred_bbox, org_img_shape, input_size, score_threshold):\n\n    valid_scale=[0, np.inf]\n    pred_bbox = np.array(pred_bbox)\n\n    pred_xywh = pred_bbox[:, 0:4]\n    pred_conf = pred_bbox[:, 4]\n    pred_prob = pred_bbox[:, 5:]\n\n    # # (1) (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n    # # (2) (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n    org_h, org_w = org_img_shape\n    resize_ratio = min(input_size / org_w, input_size / org_h)\n\n    dw = (input_size - resize_ratio * org_w) / 2\n    dh = (input_size - resize_ratio * org_h) / 2\n\n    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n\n    # # (3) clip some boxes those are out of range\n    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n    pred_coor[invalid_mask] = 0\n\n    # # (4) discard some invalid boxes\n    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n\n    # # (5) discard some boxes with low scores\n    classes = np.argmax(pred_prob, axis=-1)\n    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n    # scores = pred_prob[np.arange(len(pred_coor)), classes]\n    score_mask = scores > score_threshold\n    mask = np.logical_and(scale_mask, score_mask)\n    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n\n    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)\n\ndef freeze_all(model, frozen=True):\n    model.trainable = not frozen\n    if isinstance(model, tf.keras.Model):\n        for l in model.layers:\n            freeze_all(l, frozen)\ndef unfreeze_all(model, frozen=False):\n    model.trainable = not frozen\n    if isinstance(model, tf.keras.Model):\n        for l in model.layers:\n            unfreeze_all(l, frozen)\n\n'"
core/yolov4.py,64,"b'#! /usr/bin/env python\n# coding=utf-8\n\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nimport core.common as common\nimport core.backbone as backbone\nfrom core.config import cfg\n\n# NUM_CLASS       = len(utils.read_class_names(cfg.YOLO.CLASSES))\n# STRIDES         = np.array(cfg.YOLO.STRIDES)\n# IOU_LOSS_THRESH = cfg.YOLO.IOU_LOSS_THRESH\n# XYSCALE = cfg.YOLO.XYSCALE\n# ANCHORS = utils.get_anchors(cfg.YOLO.ANCHORS)\n\ndef YOLOv3(input_layer, NUM_CLASS):\n    route_1, route_2, conv = backbone.darknet53(input_layer)\n\n    conv = common.convolutional(conv, (1, 1, 1024, 512))\n    conv = common.convolutional(conv, (3, 3, 512, 1024))\n    conv = common.convolutional(conv, (1, 1, 1024, 512))\n    conv = common.convolutional(conv, (3, 3, 512, 1024))\n    conv = common.convolutional(conv, (1, 1, 1024, 512))\n\n    conv_lobj_branch = common.convolutional(conv, (3, 3, 512, 1024))\n    conv_lbbox = common.convolutional(conv_lobj_branch, (1, 1, 1024, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n    conv = common.upsample(conv)\n\n    conv = tf.concat([conv, route_2], axis=-1)\n\n    conv = common.convolutional(conv, (1, 1, 768, 256))\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n\n    conv_mobj_branch = common.convolutional(conv, (3, 3, 256, 512))\n    conv_mbbox = common.convolutional(conv_mobj_branch, (1, 1, 512, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n    conv = common.upsample(conv)\n\n    conv = tf.concat([conv, route_1], axis=-1)\n\n    conv = common.convolutional(conv, (1, 1, 384, 128))\n    conv = common.convolutional(conv, (3, 3, 128, 256))\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n    conv = common.convolutional(conv, (3, 3, 128, 256))\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n\n    conv_sobj_branch = common.convolutional(conv, (3, 3, 128, 256))\n    conv_sbbox = common.convolutional(conv_sobj_branch, (1, 1, 256, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n\n    return [conv_sbbox, conv_mbbox, conv_lbbox]\n\ndef YOLOv4(input_layer, NUM_CLASS):\n    route_1, route_2, conv = backbone.cspdarknet53(input_layer)\n\n    route = conv\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n    conv = common.upsample(conv)\n    route_2 = common.convolutional(route_2, (1, 1, 512, 256))\n    conv = tf.concat([route_2, conv], axis=-1)\n\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n\n    route_2 = conv\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n    conv = common.upsample(conv)\n    route_1 = common.convolutional(route_1, (1, 1, 256, 128))\n    conv = tf.concat([route_1, conv], axis=-1)\n\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n    conv = common.convolutional(conv, (3, 3, 128, 256))\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n    conv = common.convolutional(conv, (3, 3, 128, 256))\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n\n    route_1 = conv\n    conv = common.convolutional(conv, (3, 3, 128, 256))\n    conv_sbbox = common.convolutional(conv, (1, 1, 256, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n\n    conv = common.convolutional(route_1, (3, 3, 128, 256), downsample=True)\n    conv = tf.concat([conv, route_2], axis=-1)\n\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n\n    route_2 = conv\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv_mbbox = common.convolutional(conv, (1, 1, 512, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n\n    conv = common.convolutional(route_2, (3, 3, 256, 512), downsample=True)\n    conv = tf.concat([conv, route], axis=-1)\n\n    conv = common.convolutional(conv, (1, 1, 1024, 512))\n    conv = common.convolutional(conv, (3, 3, 512, 1024))\n    conv = common.convolutional(conv, (1, 1, 1024, 512))\n    conv = common.convolutional(conv, (3, 3, 512, 1024))\n    conv = common.convolutional(conv, (1, 1, 1024, 512))\n\n    conv = common.convolutional(conv, (3, 3, 512, 1024))\n    conv_lbbox = common.convolutional(conv, (1, 1, 1024, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n\n    return [conv_sbbox, conv_mbbox, conv_lbbox]\n\ndef YOLOv3_tiny(input_layer, NUM_CLASS):\n    route_1, conv = backbone.darknet53_tiny(input_layer)\n\n    conv = common.convolutional(conv, (1, 1, 1024, 256))\n\n    conv_lobj_branch = common.convolutional(conv, (3, 3, 256, 512))\n    conv_lbbox = common.convolutional(conv_lobj_branch, (1, 1, 512, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n    conv = common.upsample(conv)\n    conv = tf.concat([conv, route_1], axis=-1)\n\n    conv_mobj_branch = common.convolutional(conv, (3, 3, 128, 256))\n    conv_mbbox = common.convolutional(conv_mobj_branch, (1, 1, 256, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n\n    return [conv_mbbox, conv_lbbox]\n\ndef decode(conv_output, NUM_CLASS, i=0):\n    """"""\n    return tensor of shape [batch_size, output_size, output_size, anchor_per_scale, 5 + num_classes]\n            contains (x, y, w, h, score, probability)\n    """"""\n    conv_shape       = tf.shape(conv_output)\n    batch_size       = conv_shape[0]\n    output_size      = conv_shape[1]\n\n    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n    conv_raw_xywh, conv_raw_conf, conv_raw_prob = tf.split(conv_output, (4, 1, NUM_CLASS), axis=-1)\n\n    pred_conf = tf.sigmoid(conv_raw_conf)\n    pred_prob = tf.sigmoid(conv_raw_prob)\n\n    return tf.concat([conv_raw_xywh, pred_conf, pred_prob], axis=-1)\n\ndef decode_train(conv_output, NUM_CLASS, STRIDES, ANCHORS, i=0, XYSCALE=[1,1,1]):\n    conv_shape = tf.shape(conv_output)\n    batch_size = conv_shape[0]\n    output_size = conv_shape[1]\n\n    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n    conv_raw_dxdy, conv_raw_dwdh, conv_raw_conf, conv_raw_prob = tf.split(conv_output, (2, 2, 1, NUM_CLASS), axis=-1)\n\n    x = tf.tile(tf.expand_dims(tf.range(output_size, dtype=tf.int32), axis=0), [output_size, 1])\n    y = tf.tile(tf.expand_dims(tf.range(output_size, dtype=tf.int32), axis=1), [1, output_size])\n    xy_grid = tf.expand_dims(tf.stack([x, y], axis=-1), axis=2)  # [gx, gy, 1, 2]\n    # xy_grid = np.meshgrid(np.arange(output_size), np.arange(output_size))\n    # xy_grid = np.expand_dims(np.stack(xy_grid, axis=-1), axis=2)  # [gx, gy, 1, 2]\n\n    xy_grid = tf.tile(tf.expand_dims(xy_grid, axis=0), [batch_size, 1, 1, 3, 1])\n    xy_grid = tf.cast(xy_grid, tf.float32)\n\n    pred_xy = ((tf.sigmoid(conv_raw_dxdy) * XYSCALE[i]) - 0.5 * (XYSCALE[i] - 1) + xy_grid) * STRIDES[i]\n    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i])\n    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n\n    pred_conf = tf.sigmoid(conv_raw_conf)\n    pred_prob = tf.sigmoid(conv_raw_prob)\n\n    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n\ndef bbox_iou(boxes1, boxes2):\n\n    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n\n    boxes1_coor = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n    boxes2_coor = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n    left_up = tf.maximum(boxes1_coor[..., :2], boxes1_coor[..., :2])\n    right_down = tf.minimum(boxes2_coor[..., 2:], boxes2_coor[..., 2:])\n\n    inter_section = tf.maximum(right_down - left_up, 0.0)\n    inter_area = inter_section[..., 0] * inter_section[..., 1]\n    union_area = boxes1_area + boxes2_area - inter_area\n\n    return 1.0 * inter_area / union_area\n\ndef bbox_ciou(boxes1, boxes2):\n    boxes1_coor = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n    boxes2_coor = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n    left = tf.maximum(boxes1_coor[..., 0], boxes2_coor[..., 0])\n    up = tf.maximum(boxes1_coor[..., 1], boxes2_coor[..., 1])\n    right = tf.maximum(boxes1_coor[..., 2], boxes2_coor[..., 2])\n    down = tf.maximum(boxes1_coor[..., 3], boxes2_coor[..., 3])\n\n    c = (right - left) * (right - left) + (up - down) * (up - down)\n    iou = bbox_iou(boxes1, boxes2)\n\n    u = (boxes1[..., 0] - boxes2[..., 0]) * (boxes1[..., 0] - boxes2[..., 0]) + (boxes1[..., 1] - boxes2[..., 1]) * (boxes1[..., 1] - boxes2[..., 1])\n    d = u / c\n\n    ar_gt = boxes2[..., 2] / boxes2[..., 3]\n    ar_pred = boxes1[..., 2] / boxes1[..., 3]\n\n    ar_loss = 4 / (np.pi * np.pi) * (tf.atan(ar_gt) - tf.atan(ar_pred)) * (tf.atan(ar_gt) - tf.atan(ar_pred))\n    alpha = ar_loss / (1 - iou + ar_loss + 0.000001)\n    ciou_term = d + alpha * ar_loss\n\n    return iou - ciou_term\n\ndef bbox_giou(boxes1, boxes2):\n\n    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n    boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),\n                        tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)\n    boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),\n                        tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)\n\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    inter_section = tf.maximum(right_down - left_up, 0.0)\n    inter_area = inter_section[..., 0] * inter_section[..., 1]\n    union_area = boxes1_area + boxes2_area - inter_area\n    iou = inter_area / union_area\n\n    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\n    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\n    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\n    enclose_area = enclose[..., 0] * enclose[..., 1]\n    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\n\n    return giou\n\ndef compute_loss(pred, conv, label, bboxes, STRIDES, NUM_CLASS, IOU_LOSS_THRESH, i=0):\n    conv_shape  = tf.shape(conv)\n    batch_size  = conv_shape[0]\n    output_size = conv_shape[1]\n    input_size  = STRIDES[i] * output_size\n    conv = tf.reshape(conv, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n\n    conv_raw_conf = conv[:, :, :, :, 4:5]\n    conv_raw_prob = conv[:, :, :, :, 5:]\n\n    pred_xywh     = pred[:, :, :, :, 0:4]\n    pred_conf     = pred[:, :, :, :, 4:5]\n\n    label_xywh    = label[:, :, :, :, 0:4]\n    respond_bbox  = label[:, :, :, :, 4:5]\n    label_prob    = label[:, :, :, :, 5:]\n\n    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)\n    input_size = tf.cast(input_size, tf.float32)\n\n    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n    giou_loss = respond_bbox * bbox_loss_scale * (1- giou)\n\n    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\n    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\n\n    respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < IOU_LOSS_THRESH, tf.float32 )\n\n    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\n\n    conf_loss = conf_focal * (\n            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n            +\n            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n    )\n\n    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n\n    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4]))\n    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4]))\n    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4]))\n\n    return giou_loss, conf_loss, prob_loss\n\n\n\n\n\n'"
mAP/main.py,0,"b'import glob\nimport json\nimport os\nimport shutil\nimport operator\nimport sys\nimport argparse\nfrom absl import app, flags, logging\nfrom absl.flags import FLAGS\n\nMINOVERLAP = 0.5 # default value (defined in the PASCAL VOC2012 challenge)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-na\', \'--no-animation\',default=True, help=""no animation is shown."", action=""store_true"")\nparser.add_argument(\'-np\', \'--no-plot\', help=""no plot is shown."", action=""store_true"")\nparser.add_argument(\'-q\', \'--quiet\', help=""minimalistic console output."", action=""store_true"")\n# argparse receiving list of classes to be ignored\nparser.add_argument(\'-i\', \'--ignore\', nargs=\'+\', type=str, help=""ignore a list of classes."")\nparser.add_argument(\'-o\', \'--output\', default=""results"", type=str, help=""output path name"")\n# argparse receiving list of classes with specific IoU\nparser.add_argument(\'--set-class-iou\', nargs=\'+\', type=str, help=""set IoU for a specific class."")\nargs = parser.parse_args()\n\n# if there are no classes to ignore then replace None by empty list\nif args.ignore is None:\n  args.ignore = []\n\nspecific_iou_flagged = False\nif args.set_class_iou is not None:\n  specific_iou_flagged = True\n\n# if there are no images then no animation can be shown\nimg_path = \'images\'\nif os.path.exists(img_path): \n  for dirpath, dirnames, files in os.walk(img_path):\n    if not files:\n      # no image files found\n      args.no_animation = True\nelse:\n  args.no_animation = True\n\n# try to import OpenCV if the user didn\'t choose the option --no-animation\nshow_animation = False\nif not args.no_animation:\n  try:\n    import cv2\n    show_animation = True\n  except ImportError:\n    print(""\\""opencv-python\\"" not found, please install to visualize the results."")\n    args.no_animation = True\n\n# try to import Matplotlib if the user didn\'t choose the option --no-plot\ndraw_plot = False\nif not args.no_plot:\n  try:\n    import matplotlib.pyplot as plt\n    draw_plot = True\n  except ImportError:\n    print(""\\""matplotlib\\"" not found, please install it to get the resulting plots."")\n    args.no_plot = True\n\n""""""\n throw error and exit\n""""""\ndef error(msg):\n  print(msg)\n  sys.exit(0)\n\n""""""\n check if the number is a float between 0.0 and 1.0\n""""""\ndef is_float_between_0_and_1(value):\n  try:\n    val = float(value)\n    if val > 0.0 and val < 1.0:\n      return True\n    else:\n      return False\n  except ValueError:\n    return False\n\n""""""\n Calculate the AP given the recall and precision array\n  1st) We compute a version of the measured precision/recall curve with\n       precision monotonically decreasing\n  2nd) We compute the AP as the area under this curve by numerical integration.\n""""""\ndef voc_ap(rec, prec):\n  """"""\n  --- Official matlab code VOC2012---\n  mrec=[0 ; rec ; 1];\n  mpre=[0 ; prec ; 0];\n  for i=numel(mpre)-1:-1:1\n      mpre(i)=max(mpre(i),mpre(i+1));\n  end\n  i=find(mrec(2:end)~=mrec(1:end-1))+1;\n  ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n  """"""\n  rec.insert(0, 0.0) # insert 0.0 at begining of list\n  rec.append(1.0) # insert 1.0 at end of list\n  mrec = rec[:]\n  prec.insert(0, 0.0) # insert 0.0 at begining of list\n  prec.append(0.0) # insert 0.0 at end of list\n  mpre = prec[:]\n  """"""\n   This part makes the precision monotonically decreasing\n    (goes from the end to the beginning)\n    matlab:  for i=numel(mpre)-1:-1:1\n                mpre(i)=max(mpre(i),mpre(i+1));\n  """"""\n  # matlab indexes start in 1 but python in 0, so I have to do:\n  #   range(start=(len(mpre) - 2), end=0, step=-1)\n  # also the python function range excludes the end, resulting in:\n  #   range(start=(len(mpre) - 2), end=-1, step=-1)\n  for i in range(len(mpre)-2, -1, -1):\n    mpre[i] = max(mpre[i], mpre[i+1])\n  """"""\n   This part creates a list of indexes where the recall changes\n    matlab:  i=find(mrec(2:end)~=mrec(1:end-1))+1;\n  """"""\n  i_list = []\n  for i in range(1, len(mrec)):\n    if mrec[i] != mrec[i-1]:\n      i_list.append(i) # if it was matlab would be i + 1\n  """"""\n   The Average Precision (AP) is the area under the curve\n    (numerical integration)\n    matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n  """"""\n  ap = 0.0\n  for i in i_list:\n    ap += ((mrec[i]-mrec[i-1])*mpre[i])\n  return ap, mrec, mpre\n\n\n""""""\n Convert the lines of a file to a list\n""""""\ndef file_lines_to_list(path):\n  # open txt file lines to a list\n  with open(path) as f:\n    content = f.readlines()\n  # remove whitespace characters like `\\n` at the end of each line\n  content = [x.strip() for x in content]\n  return content\n\n""""""\n Draws text in image\n""""""\ndef draw_text_in_image(img, text, pos, color, line_width):\n  font = cv2.FONT_HERSHEY_PLAIN\n  fontScale = 1\n  lineType = 1\n  bottomLeftCornerOfText = pos\n  cv2.putText(img, text,\n      bottomLeftCornerOfText,\n      font,\n      fontScale,\n      color,\n      lineType)\n  text_width, _ = cv2.getTextSize(text, font, fontScale, lineType)[0]\n  return img, (line_width + text_width)\n\n""""""\n Plot - adjust axes\n""""""\ndef adjust_axes(r, t, fig, axes):\n  # get text width for re-scaling\n  bb = t.get_window_extent(renderer=r)\n  text_width_inches = bb.width / fig.dpi\n  # get axis width in inches\n  current_fig_width = fig.get_figwidth()\n  new_fig_width = current_fig_width + text_width_inches\n  propotion = new_fig_width / current_fig_width\n  # get axis limit\n  x_lim = axes.get_xlim()\n  axes.set_xlim([x_lim[0], x_lim[1]*propotion])\n\n""""""\n Draw plot using Matplotlib\n""""""\ndef draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n  # sort the dictionary by decreasing value, into a list of tuples\n  sorted_dic_by_value = sorted(dictionary.items(), key=operator.itemgetter(1))\n  # unpacking the list of tuples into two lists\n  sorted_keys, sorted_values = zip(*sorted_dic_by_value)\n  # \n  if true_p_bar != """":\n    """"""\n     Special case to draw in (green=true predictions) & (red=false predictions)\n    """"""\n    fp_sorted = []\n    tp_sorted = []\n    for key in sorted_keys:\n      fp_sorted.append(dictionary[key] - true_p_bar[key])\n      tp_sorted.append(true_p_bar[key])\n    plt.barh(range(n_classes), fp_sorted, align=\'center\', color=\'crimson\', label=\'False Predictions\')\n    plt.barh(range(n_classes), tp_sorted, align=\'center\', color=\'forestgreen\', label=\'True Predictions\', left=fp_sorted)\n    # add legend\n    plt.legend(loc=\'lower right\')\n    """"""\n     Write number on side of bar\n    """"""\n    fig = plt.gcf() # gcf - get current figure\n    axes = plt.gca()\n    r = fig.canvas.get_renderer()\n    for i, val in enumerate(sorted_values):\n      fp_val = fp_sorted[i]\n      tp_val = tp_sorted[i]\n      fp_str_val = "" "" + str(fp_val)\n      tp_str_val = fp_str_val + "" "" + str(tp_val)\n      # trick to paint multicolor with offset:\n      #   first paint everything and then repaint the first number\n      t = plt.text(val, i, tp_str_val, color=\'forestgreen\', va=\'center\', fontweight=\'bold\')\n      plt.text(val, i, fp_str_val, color=\'crimson\', va=\'center\', fontweight=\'bold\')\n      if i == (len(sorted_values)-1): # largest bar\n        adjust_axes(r, t, fig, axes)\n  else:\n    plt.barh(range(n_classes), sorted_values, color=plot_color)\n    """"""\n     Write number on side of bar\n    """"""\n    fig = plt.gcf() # gcf - get current figure\n    axes = plt.gca()\n    r = fig.canvas.get_renderer()\n    for i, val in enumerate(sorted_values):\n      str_val = "" "" + str(val) # add a space before\n      if val < 1.0:\n        str_val = "" {0:.2f}"".format(val)\n      t = plt.text(val, i, str_val, color=plot_color, va=\'center\', fontweight=\'bold\')\n      # re-set axes to show number inside the figure\n      if i == (len(sorted_values)-1): # largest bar\n        adjust_axes(r, t, fig, axes)\n  # set window title\n  fig.canvas.set_window_title(window_title)\n  # write classes in y axis\n  tick_font_size = 12\n  plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)\n  """"""\n   Re-scale height accordingly\n  """"""\n  init_height = fig.get_figheight()\n  # comput the matrix height in points and inches\n  dpi = fig.dpi\n  height_pt = n_classes * (tick_font_size * 1.4) # 1.4 (some spacing)\n  height_in = height_pt / dpi\n  # compute the required figure height \n  top_margin = 0.15    # in percentage of the figure height\n  bottom_margin = 0.05 # in percentage of the figure height\n  figure_height = height_in / (1 - top_margin - bottom_margin)\n  # set new height\n  if figure_height > init_height:\n    fig.set_figheight(figure_height)\n\n  # set plot title\n  plt.title(plot_title, fontsize=14)\n  # set axis titles\n  # plt.xlabel(\'classes\')\n  plt.xlabel(x_label, fontsize=\'large\')\n  # adjust size of window\n  fig.tight_layout()\n  # save the plot\n  fig.savefig(output_path)\n  # show image\n  if to_show:\n    plt.show()\n  # close the plot\n  plt.close()\n\n""""""\n Create a ""tmp_files/"" and ""results/"" directory\n""""""\ntmp_files_path = ""tmp_files""\nif not os.path.exists(tmp_files_path): # if it doesn\'t exist already\n  os.makedirs(tmp_files_path)\nresults_files_path = args.output\nif os.path.exists(results_files_path): # if it exist already\n  # reset the results directory\n  shutil.rmtree(results_files_path)\n\nos.makedirs(results_files_path)\nif draw_plot:\n  os.makedirs(results_files_path + ""/classes"")\nif show_animation:\n  os.makedirs(results_files_path + ""/images"")\n  os.makedirs(results_files_path + ""/images/single_predictions"")\n\n""""""\n Ground-Truth\n   Load each of the ground-truth files into a temporary "".json"" file.\n   Create a list of all the class names present in the ground-truth (gt_classes).\n""""""\n# get a list with the ground-truth files\nground_truth_files_list = glob.glob(\'ground-truth/*.txt\')\nif len(ground_truth_files_list) == 0:\n  error(""Error: No ground-truth files found!"")\nground_truth_files_list.sort()\n# dictionary with counter per class\ngt_counter_per_class = {}\n\nfor txt_file in ground_truth_files_list:\n  #print(txt_file)\n  file_id = txt_file.split("".txt"",1)[0]\n  file_id = os.path.basename(os.path.normpath(file_id))\n  # check if there is a correspondent predicted objects file\n  if not os.path.exists(\'predicted/\' + file_id + "".txt""):\n    error_msg = ""Error. File not found: predicted/"" +  file_id + "".txt\\n""\n    error_msg += ""(You can avoid this error message by running extra/intersect-gt-and-pred.py)""\n    error(error_msg)\n  lines_list = file_lines_to_list(txt_file)\n  # create ground-truth dictionary\n  bounding_boxes = []\n  is_difficult = False\n  for line in lines_list:\n    try:\n      if ""difficult"" in line:\n          class_name, left, top, right, bottom, _difficult = line.split()\n          is_difficult = True\n      else:\n          class_name, left, top, right, bottom = line.split()\n    except ValueError:\n      error_msg = ""Error: File "" + txt_file + "" in the wrong format.\\n""\n      error_msg += "" Expected: <class_name> <left> <top> <right> <bottom> [\'difficult\']\\n""\n      error_msg += "" Received: "" + line\n      error_msg += ""\\n\\nIf you have a <class_name> with spaces between words you should remove them\\n""\n      error_msg += ""by running the script \\""remove_space.py\\"" or \\""rename_class.py\\"" in the \\""extra/\\"" folder.""\n      error(error_msg)\n    # check if class is in the ignore list, if yes skip\n    if class_name in args.ignore:\n      continue\n    bbox = left + "" "" + top + "" "" + right + "" "" +bottom\n    if is_difficult:\n        bounding_boxes.append({""class_name"":class_name, ""bbox"":bbox, ""used"":False, ""difficult"":True})\n        is_difficult = False\n    else:\n        bounding_boxes.append({""class_name"":class_name, ""bbox"":bbox, ""used"":False})\n        # count that object\n        if class_name in gt_counter_per_class:\n          gt_counter_per_class[class_name] += 1\n        else:\n          # if class didn\'t exist yet\n          gt_counter_per_class[class_name] = 1\n  # dump bounding_boxes into a "".json"" file\n  with open(tmp_files_path + ""/"" + file_id + ""_ground_truth.json"", \'w\') as outfile:\n    json.dump(bounding_boxes, outfile)\n\ngt_classes = list(gt_counter_per_class.keys())\n# let\'s sort the classes alphabetically\ngt_classes = sorted(gt_classes)\nn_classes = len(gt_classes)\n#print(gt_classes)\n#print(gt_counter_per_class)\n\n""""""\n Check format of the flag --set-class-iou (if used)\n  e.g. check if class exists\n""""""\nif specific_iou_flagged:\n  n_args = len(args.set_class_iou)\n  error_msg = \\\n    \'\\n --set-class-iou [class_1] [IoU_1] [class_2] [IoU_2] [...]\'\n  if n_args % 2 != 0:\n    error(\'Error, missing arguments. Flag usage:\' + error_msg)\n  # [class_1] [IoU_1] [class_2] [IoU_2]\n  # specific_iou_classes = [\'class_1\', \'class_2\']\n  specific_iou_classes = args.set_class_iou[::2] # even\n  # iou_list = [\'IoU_1\', \'IoU_2\']\n  iou_list = args.set_class_iou[1::2] # odd\n  if len(specific_iou_classes) != len(iou_list):\n    error(\'Error, missing arguments. Flag usage:\' + error_msg)\n  for tmp_class in specific_iou_classes:\n    if tmp_class not in gt_classes:\n          error(\'Error, unknown class \\""\' + tmp_class + \'\\"". Flag usage:\' + error_msg)\n  for num in iou_list:\n    if not is_float_between_0_and_1(num):\n      error(\'Error, IoU must be between 0.0 and 1.0. Flag usage:\' + error_msg)\n\n""""""\n Predicted\n   Load each of the predicted files into a temporary "".json"" file.\n""""""\n# get a list with the predicted files\npredicted_files_list = glob.glob(\'predicted/*.txt\')\npredicted_files_list.sort()\n\nfor class_index, class_name in enumerate(gt_classes):\n  bounding_boxes = []\n  for txt_file in predicted_files_list:\n    #print(txt_file)\n    # the first time it checks if all the corresponding ground-truth files exist\n    file_id = txt_file.split("".txt"",1)[0]\n    file_id = os.path.basename(os.path.normpath(file_id))\n    if class_index == 0:\n      if not os.path.exists(\'ground-truth/\' + file_id + "".txt""):\n        error_msg = ""Error. File not found: ground-truth/"" +  file_id + "".txt\\n""\n        error_msg += ""(You can avoid this error message by running extra/intersect-gt-and-pred.py)""\n        error(error_msg)\n    lines = file_lines_to_list(txt_file)\n    for line in lines:\n      try:\n        tmp_class_name, confidence, left, top, right, bottom = line.split()\n      except ValueError:\n        error_msg = ""Error: File "" + txt_file + "" in the wrong format.\\n""\n        error_msg += "" Expected: <class_name> <confidence> <left> <top> <right> <bottom>\\n""\n        error_msg += "" Received: "" + line\n        error(error_msg)\n      if tmp_class_name == class_name:\n        #print(""match"")\n        bbox = left + "" "" + top + "" "" + right + "" "" +bottom\n        bounding_boxes.append({""confidence"":confidence, ""file_id"":file_id, ""bbox"":bbox})\n        #print(bounding_boxes)\n  # sort predictions by decreasing confidence\n  bounding_boxes.sort(key=lambda x:float(x[\'confidence\']), reverse=True)\n  with open(tmp_files_path + ""/"" + class_name + ""_predictions.json"", \'w\') as outfile:\n    json.dump(bounding_boxes, outfile)\n\n""""""\n Calculate the AP for each class\n""""""\nsum_AP = 0.0\nap_dictionary = {}\n# open file to store the results\nwith open(results_files_path + ""/results.txt"", \'w\') as results_file:\n  results_file.write(""# AP and precision/recall per class\\n"")\n  count_true_positives = {}\n  for class_index, class_name in enumerate(gt_classes):\n    count_true_positives[class_name] = 0\n    """"""\n     Load predictions of that class\n    """"""\n    predictions_file = tmp_files_path + ""/"" + class_name + ""_predictions.json""\n    predictions_data = json.load(open(predictions_file))\n\n    """"""\n     Assign predictions to ground truth objects\n    """"""\n    nd = len(predictions_data)\n    tp = [0] * nd # creates an array of zeros of size nd\n    fp = [0] * nd\n    for idx, prediction in enumerate(predictions_data):\n      file_id = prediction[""file_id""]\n      if show_animation:\n        # find ground truth image\n        ground_truth_img = glob.glob1(img_path, file_id + "".*"")\n        #tifCounter = len(glob.glob1(myPath,""*.tif""))\n        if len(ground_truth_img) == 0:\n          error(""Error. Image not found with id: "" + file_id)\n        elif len(ground_truth_img) > 1:\n          error(""Error. Multiple image with id: "" + file_id)\n        else: # found image\n          #print(img_path + ""/"" + ground_truth_img[0])\n          # Load image\n          img = cv2.imread(img_path + ""/"" + ground_truth_img[0])\n          # load image with draws of multiple detections\n          img_cumulative_path = results_files_path + ""/images/"" + ground_truth_img[0]\n          if os.path.isfile(img_cumulative_path):\n            img_cumulative = cv2.imread(img_cumulative_path)\n          else:\n            img_cumulative = img.copy()\n          # Add bottom border to image\n          bottom_border = 60\n          BLACK = [0, 0, 0]\n          img = cv2.copyMakeBorder(img, 0, bottom_border, 0, 0, cv2.BORDER_CONSTANT, value=BLACK)\n      # assign prediction to ground truth object if any\n      #   open ground-truth with that file_id\n      gt_file = tmp_files_path + ""/"" + file_id + ""_ground_truth.json""\n      ground_truth_data = json.load(open(gt_file))\n      ovmax = -1\n      gt_match = -1\n      # load prediction bounding-box\n      bb = [ float(x) for x in prediction[""bbox""].split() ]\n      for obj in ground_truth_data:\n        # look for a class_name match\n        if obj[""class_name""] == class_name:\n          bbgt = [ float(x) for x in obj[""bbox""].split() ]\n          bi = [max(bb[0],bbgt[0]), max(bb[1],bbgt[1]), min(bb[2],bbgt[2]), min(bb[3],bbgt[3])]\n          iw = bi[2] - bi[0] + 1\n          ih = bi[3] - bi[1] + 1\n          if iw > 0 and ih > 0:\n            # compute overlap (IoU) = area of intersection / area of union\n            ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + (bbgt[2] - bbgt[0]\n                    + 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih\n            ov = iw * ih / ua\n            if ov > ovmax:\n              ovmax = ov\n              gt_match = obj\n\n      # assign prediction as true positive/don\'t care/false positive\n      if show_animation:\n        status = ""NO MATCH FOUND!"" # status is only used in the animation\n      # set minimum overlap\n      min_overlap = MINOVERLAP\n      if specific_iou_flagged:\n        if class_name in specific_iou_classes:\n          index = specific_iou_classes.index(class_name)\n          min_overlap = float(iou_list[index])\n      if ovmax >= min_overlap:\n        if ""difficult"" not in gt_match:\n            if not bool(gt_match[""used""]):\n              # true positive\n              tp[idx] = 1\n              gt_match[""used""] = True\n              count_true_positives[class_name] += 1\n              # update the "".json"" file\n              with open(gt_file, \'w\') as f:\n                  f.write(json.dumps(ground_truth_data))\n              if show_animation:\n                status = ""MATCH!""\n            else:\n              # false positive (multiple detection)\n              fp[idx] = 1\n              if show_animation:\n                status = ""REPEATED MATCH!""\n      else:\n        # false positive\n        fp[idx] = 1\n        if ovmax > 0:\n          status = ""INSUFFICIENT OVERLAP""\n\n      """"""\n       Draw image to show animation\n      """"""\n      if show_animation:\n        height, widht = img.shape[:2]\n        # colors (OpenCV works with BGR)\n        white = (255,255,255)\n        light_blue = (255,200,100)\n        green = (0,255,0)\n        light_red = (30,30,255)\n        # 1st line\n        margin = 10\n        v_pos = int(height - margin - (bottom_border / 2))\n        text = ""Image: "" + ground_truth_img[0] + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n        text = ""Class ["" + str(class_index) + ""/"" + str(n_classes) + ""]: "" + class_name + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), light_blue, line_width)\n        if ovmax != -1:\n          color = light_red\n          if status == ""INSUFFICIENT OVERLAP"":\n            text = ""IoU: {0:.2f}% "".format(ovmax*100) + ""< {0:.2f}% "".format(min_overlap*100)\n          else:\n            text = ""IoU: {0:.2f}% "".format(ovmax*100) + "">= {0:.2f}% "".format(min_overlap*100)\n            color = green\n          img, _ = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n        # 2nd line\n        v_pos += int(bottom_border / 2)\n        rank_pos = str(idx+1) # rank position (idx starts at 0)\n        text = ""Prediction #rank: "" + rank_pos + "" confidence: {0:.2f}% "".format(float(prediction[""confidence""])*100)\n        img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n        color = light_red\n        if status == ""MATCH!"":\n          color = green\n        text = ""Result: "" + status + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        if ovmax > 0: # if there is intersections between the bounding-boxes\n          bbgt = [ int(x) for x in gt_match[""bbox""].split() ]\n          cv2.rectangle(img,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n          cv2.rectangle(img_cumulative,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n          cv2.putText(img_cumulative, class_name, (bbgt[0],bbgt[1] - 5), font, 0.6, light_blue, 1, cv2.LINE_AA)\n        bb = [int(i) for i in bb]\n        cv2.rectangle(img,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n        cv2.rectangle(img_cumulative,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n        cv2.putText(img_cumulative, class_name, (bb[0],bb[1] - 5), font, 0.6, color, 1, cv2.LINE_AA)\n        # show image\n        cv2.imshow(""Animation"", img)\n        cv2.waitKey(20) # show for 20 ms\n        # save image to results\n        output_img_path = results_files_path + ""/images/single_predictions/"" + class_name + ""_prediction"" + str(idx) + "".jpg""\n        cv2.imwrite(output_img_path, img)\n        # save the image with all the objects drawn to it\n        cv2.imwrite(img_cumulative_path, img_cumulative)\n\n    #print(tp)\n    # compute precision/recall\n    cumsum = 0\n    for idx, val in enumerate(fp):\n      fp[idx] += cumsum\n      cumsum += val\n    cumsum = 0\n    for idx, val in enumerate(tp):\n      tp[idx] += cumsum\n      cumsum += val\n    #print(tp)\n    rec = tp[:]\n    for idx, val in enumerate(tp):\n      rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n    #print(rec)\n    prec = tp[:]\n    for idx, val in enumerate(tp):\n      prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n    #print(prec)\n\n    ap, mrec, mprec = voc_ap(rec, prec)\n    sum_AP += ap\n    text = ""{0:.2f}%"".format(ap*100) + "" = "" + class_name + "" AP  "" #class_name + "" AP = {0:.2f}%"".format(ap*100)\n    """"""\n     Write to results.txt\n    """"""\n    rounded_prec = [ \'%.2f\' % elem for elem in prec ]\n    rounded_rec = [ \'%.2f\' % elem for elem in rec ]\n    results_file.write(text + ""\\n Precision: "" + str(rounded_prec) + ""\\n Recall   :"" + str(rounded_rec) + ""\\n\\n"")\n    if not args.quiet:\n      print(text)\n    ap_dictionary[class_name] = ap\n\n    """"""\n     Draw plot\n    """"""\n    if draw_plot:\n      plt.plot(rec, prec, \'-o\')\n      # add a new penultimate point to the list (mrec[-2], 0.0)\n      # since the last line segment (and respective area) do not affect the AP value\n      area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n      area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n      plt.fill_between(area_under_curve_x, 0, area_under_curve_y, alpha=0.2, edgecolor=\'r\')\n      # set window title\n      fig = plt.gcf() # gcf - get current figure\n      fig.canvas.set_window_title(\'AP \' + class_name)\n      # set plot title\n      plt.title(\'class: \' + text)\n      #plt.suptitle(\'This is a somewhat long figure title\', fontsize=16)\n      # set axis titles\n      plt.xlabel(\'Recall\')\n      plt.ylabel(\'Precision\')\n      # optional - set axes\n      axes = plt.gca() # gca - get current axes\n      axes.set_xlim([0.0,1.0])\n      axes.set_ylim([0.0,1.05]) # .05 to give some extra space\n      # Alternative option -> wait for button to be pressed\n      #while not plt.waitforbuttonpress(): pass # wait for key display\n      # Alternative option -> normal display\n      #plt.show()\n      # save the plot\n      fig.savefig(results_files_path + ""/classes/"" + class_name + "".png"")\n      plt.cla() # clear axes for next plot\n\n  if show_animation:\n    cv2.destroyAllWindows()\n\n  results_file.write(""\\n# mAP of all classes\\n"")\n  mAP = sum_AP / n_classes\n  text = ""mAP = {0:.2f}%"".format(mAP*100)\n  results_file.write(text + ""\\n"")\n  print(text)\n\n# remove the tmp_files directory\nshutil.rmtree(tmp_files_path)\n\n""""""\n Count total of Predictions\n""""""\n# iterate through all the files\npred_counter_per_class = {}\n#all_classes_predicted_files = set([])\nfor txt_file in predicted_files_list:\n  # get lines to list\n  lines_list = file_lines_to_list(txt_file)\n  for line in lines_list:\n    class_name = line.split()[0]\n    # check if class is in the ignore list, if yes skip\n    if class_name in args.ignore:\n      continue\n    # count that object\n    if class_name in pred_counter_per_class:\n      pred_counter_per_class[class_name] += 1\n    else:\n      # if class didn\'t exist yet\n      pred_counter_per_class[class_name] = 1\n#print(pred_counter_per_class)\npred_classes = list(pred_counter_per_class.keys())\n\n\n""""""\n Plot the total number of occurences of each class in the ground-truth\n""""""\nif draw_plot:\n  window_title = ""Ground-Truth Info""\n  plot_title = ""Ground-Truth\\n""\n  plot_title += ""("" + str(len(ground_truth_files_list)) + "" files and "" + str(n_classes) + "" classes)""\n  x_label = ""Number of objects per class""\n  output_path = results_files_path + ""/Ground-Truth Info.png""\n  to_show = False\n  plot_color = \'forestgreen\'\n  draw_plot_func(\n    gt_counter_per_class,\n    n_classes,\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    \'\',\n    )\n\n""""""\n Write number of ground-truth objects per class to results.txt\n""""""\nwith open(results_files_path + ""/results.txt"", \'a\') as results_file:\n  results_file.write(""\\n# Number of ground-truth objects per class\\n"")\n  for class_name in sorted(gt_counter_per_class):\n    results_file.write(class_name + "": "" + str(gt_counter_per_class[class_name]) + ""\\n"")\n\n""""""\n Finish counting true positives\n""""""\nfor class_name in pred_classes:\n  # if class exists in predictions but not in ground-truth then there are no true positives in that class\n  if class_name not in gt_classes:\n    count_true_positives[class_name] = 0\n#print(count_true_positives)\n\n""""""\n Plot the total number of occurences of each class in the ""predicted"" folder\n""""""\nif draw_plot:\n  window_title = ""Predicted Objects Info""\n  # Plot title\n  plot_title = ""Predicted Objects\\n""\n  plot_title += ""("" + str(len(predicted_files_list)) + "" files and ""\n  count_non_zero_values_in_dictionary = sum(int(x) > 0 for x in list(pred_counter_per_class.values()))\n  plot_title += str(count_non_zero_values_in_dictionary) + "" detected classes)""\n  # end Plot title\n  x_label = ""Number of objects per class""\n  output_path = results_files_path + ""/Predicted Objects Info.png""\n  to_show = False\n  plot_color = \'forestgreen\'\n  true_p_bar = count_true_positives\n  draw_plot_func(\n    pred_counter_per_class,\n    len(pred_counter_per_class),\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    true_p_bar\n    )\n\n""""""\n Write number of predicted objects per class to results.txt\n""""""\nwith open(results_files_path + ""/results"", \'a\') as results_file:\n  results_file.write(""\\n# Number of predicted objects per class\\n"")\n  for class_name in sorted(pred_classes):\n    n_pred = pred_counter_per_class[class_name]\n    text = class_name + "": "" + str(n_pred)\n    text += "" (tp:"" + str(count_true_positives[class_name]) + """"\n    text += "", fp:"" + str(n_pred - count_true_positives[class_name]) + "")\\n""\n    results_file.write(text)\n\n""""""\n Draw mAP plot (Show AP\'s of all classes in decreasing order)\n""""""\nif draw_plot:\n  window_title = ""mAP""\n  plot_title = ""mAP = {0:.2f}%"".format(mAP*100)\n  x_label = ""Average Precision""\n  output_path = results_files_path + ""/mAP.png""\n  to_show = True\n  plot_color = \'royalblue\'\n  draw_plot_func(\n    ap_dictionary,\n    n_classes,\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    """"\n    )\n'"
scripts/coco_annotation.py,0,"b'from absl import app, flags, logging\nimport os\nimport pickle\nfrom os import listdir\nfrom os.path import isfile, join\nfrom absl.flags import FLAGS\nimport cv2\n\nflags.DEFINE_string(\'coco_data\', \'./val2017.pkl\', \'path to coco data\')\nflags.DEFINE_string(\'classes\', \'../data/classes/coco.names\', \'path to classes file\')\nflags.DEFINE_string(\'coco_path\', ""/Volumes/Elements/data/coco_dataset/coco"", \'resize images to\')\nflags.DEFINE_string(\'image_path\', ""images/val2017"", \'path to image val\')\nflags.DEFINE_string(\'anno_path_val\', \'../data/dataset/val2017.txt\', \'path to classes file\')\n\ndef convert_annotation(output, data, data_type = ""val""):\n    class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n    replace_dict = {""couch"": ""sofa"", ""airplane"": ""aeroplane"", ""tv"": ""tvmonitor"", ""motorcycle"": ""motorbike""}\n\n    if os.path.exists(output): os.remove(output)\n    directory_path = os.path.join(FLAGS.coco_path, FLAGS.image_path)\n    # if data_type == ""train"":\n    #     anno_path = directory_path + ""/labels/train2014""\n    #     image_path = os.path.join(directory_path, ""trainvalno5k.txt"")\n    # else:\n    #     anno_path = directory_path + ""/labels/val2014""\n    #     image_path = os.path.join(directory_path, ""5k.txt"")\n    # with open(image_path) as f:\n    #     image_paths = f.readlines()\n    # image_paths = [x.strip() for x in image_paths]\n\n    image_paths = [f for f in listdir(directory_path) if isfile(join(directory_path, f))]\n\n    check_classes = []\n    count = 0\n    with open(output, \'a\') as f:\n        for image_path in image_paths:\n            image_inds = image_path.split(""."")[0]\n            annotation = os.path.join(directory_path, image_path)\n            # if os.path.exists(os.path.join(anno_path, image_inds + "".txt"")):\n            if image_inds in data:\n                objects = data[image_inds][""objects""]\n                for key, value in objects.items():\n                    if key == \'num_obj\': continue\n                    if value[""name""] not in class_names:\n                        class_ind = replace_dict[value[""name""]]\n                        class_ind = class_names.index(class_ind)\n                        # if value[""name""] not in check_classes:\n                        #     check_classes.append(value[""name""])\n                        #     print(value[""name""])\n                        # continue\n                    else:\n                        class_ind = class_names.index(value[""name""])\n                    xmin = int(value[""bndbox""][""xmin""])\n                    xmax = int(value[""bndbox""][""xmax""])\n                    ymin = int(value[""bndbox""][""ymin""])\n                    ymax = int(value[""bndbox""][""ymax""])\n                    annotation += \' \' + \',\'.join([str(xmin), str(ymin), str(xmax), str(ymax), str(class_ind)])\n            else: continue\n            f.write(annotation + ""\\n"")\n            count += 1\n            # print(annotation)\n    print(count)\n    return\n\ndef main(_argv):\n    with open(FLAGS.coco_data, ""rb"") as input_file:\n        data = pickle.load(input_file)\n    data = data[1]\n    convert_annotation(FLAGS.anno_path_val, data)\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass'"
scripts/coco_convert.py,0,"b'from absl import app, flags, logging\nfrom absl.flags import FLAGS\nimport cv2\nimport numpy as np\nimport os\nimport json\nimport sys\nimport pickle\n\nflags.DEFINE_string(\'input\', \'/Volumes/Elements/data/coco_dataset/coco/annotations/instances_val2017.json\', \'path to classes file\')\nflags.DEFINE_string(\'output\', \'val2017.pkl\', \'path to classes file\')\n\nclass COCO:\n    """"""\n    Handler Class for COCO Format\n    """"""\n\n    @staticmethod\n    def parse(json_path):\n\n        try:\n            json_data = json.load(open(json_path))\n\n            images_info = json_data[""images""]\n            cls_info = json_data[""categories""]\n\n            data = {}\n\n            progress_length = len(json_data[""annotations""])\n            progress_cnt = 0\n\n            for anno in json_data[""annotations""]:\n\n                image_id = anno[""image_id""]\n                cls_id = anno[""category_id""]\n\n                filename = None\n                img_width = None\n                img_height = None\n                cls = None\n\n                for info in images_info:\n                        if info[""id""] == image_id:\n                            filename, img_width, img_height = \\\n                                info[""file_name""].split(""."")[0], info[""width""], info[""height""]\n\n                for category in cls_info:\n                    if category[""id""] == cls_id:\n                        cls = category[""name""]\n\n                size = {\n                    ""width"": img_width,\n                    ""height"": img_height,\n                    ""depth"": ""3""\n                }\n\n                bndbox = {\n                    ""xmin"": anno[""bbox""][0],\n                    ""ymin"": anno[""bbox""][1],\n                    ""xmax"": anno[""bbox""][2] + anno[""bbox""][0],\n                    ""ymax"": anno[""bbox""][3] + anno[""bbox""][1]\n                }\n\n                obj_info = {\n                    ""name"": cls,\n                    ""bndbox"": bndbox\n                }\n\n                if filename in data:\n                    obj_idx = str(int(data[filename][""objects""][""num_obj""]))\n                    data[filename][""objects""][str(obj_idx)] = obj_info\n                    data[filename][""objects""][""num_obj""] = int(obj_idx) + 1\n\n                elif filename not in data:\n\n                    obj = {\n                        ""num_obj"": ""1"",\n                        ""0"": obj_info\n                    }\n\n                    data[filename] = {\n                        ""size"": size,\n                        ""objects"": obj\n                    }\n\n                percent = (float(progress_cnt) / float(progress_length)) * 100\n                print(str(progress_cnt) + ""/"" + str(progress_length) + "" total: "" + str(round(percent, 2)))\n                progress_cnt += 1\n\n            #print(json.dumps(data, indent=4, sort_keys = True))\n            return True, data\n\n        except Exception as e:\n\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n\n            msg = ""ERROR : {}, moreInfo : {}\\t{}\\t{}"".format(e, exc_type, fname, exc_tb.tb_lineno)\n\n            return False, msg\n\ndef main(_argv):\n    coco = COCO()\n    data = coco.parse(FLAGS.input)\n    with open(FLAGS.output, \'wb\') as handle:\n        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nif __name__ == \'__main__\':\n    try:\n        app.run(main)\n    except SystemExit:\n        pass\n'"
mAP/extra/intersect-gt-and-pred.py,0,"b'import sys\nimport os\nimport glob\n\n## This script ensures same number of files in ground-truth and predicted folder.\n## When you encounter file not found error, it\'s usually because you have\n## mismatched numbers of ground-truth and predicted files.\n## You can use this script to move ground-truth and predicted files that are\n## not in the intersection into a backup folder (backup_no_matches_found).\n## This will retain only files that have the same name in both folders.\n\n# change directory to the one with the files to be changed\npath_to_gt = \'../ground-truth\'\npath_to_pred = \'../predicted\'\nbackup_folder = \'backup_no_matches_found\'  # must end without slash\n\nos.chdir(path_to_gt)\ngt_files = glob.glob(\'*.txt\')\nif len(gt_files) == 0:\n    print(""Error: no .txt files found in"", path_to_gt)\n    sys.exit()\nos.chdir(path_to_pred)\npred_files = glob.glob(\'*.txt\')\nif len(pred_files) == 0:\n    print(""Error: no .txt files found in"", path_to_pred)\n    sys.exit()\n\ngt_files = set(gt_files)\npred_files = set(pred_files)\nprint(\'total ground-truth files:\', len(gt_files))\nprint(\'total predicted files:\', len(pred_files))\nprint()\n\ngt_backup = gt_files - pred_files\npred_backup = pred_files - gt_files\n\n\ndef backup(src_folder, backup_files, backup_folder):\n    # non-intersection files (txt format) will be moved to a backup folder\n    if not backup_files:\n        print(\'No backup required for\', src_folder)\n        return\n    os.chdir(src_folder)\n    ## create the backup dir if it doesn\'t exist already\n    if not os.path.exists(backup_folder):\n        os.makedirs(backup_folder)\n    for file in backup_files:\n        os.rename(file, backup_folder + \'/\' + file)\n\n\nbackup(path_to_gt, gt_backup, backup_folder)\nbackup(path_to_pred, pred_backup, backup_folder)\nif gt_backup:\n    print(\'total ground-truth backup files:\', len(gt_backup))\nif pred_backup:\n    print(\'total predicted backup files:\', len(pred_backup))\n\nintersection = gt_files & pred_files\nprint(\'total intersected files:\', len(intersection))\nprint(""Intersection completed!"")'"
mAP/extra/remove_space.py,0,"b'import sys\nimport os\nimport glob\nimport argparse\n\n# this script will load class_list.txt and find class names with spaces\n# then replace spaces with delimiters inside ground-truth/ and predicted/\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-d\', \'--delimiter\', type=str, help=""delimiter to replace space (default: \'-\')"", default=\'-\')\nparser.add_argument(\'-y\', \'--yes\', action=\'store_true\', help=""force yes confirmation on yes/no query (default: False)"", default=False)\nargs = parser.parse_args()\n\ndef query_yes_no(question, default=""yes"", bypass=False):\n  """"""Ask a yes/no question via raw_input() and return their answer.\n\n  ""question"" is a string that is presented to the user.\n  ""default"" is the presumed answer if the user just hits <Enter>.\n      It must be ""yes"" (the default), ""no"" or None (meaning\n      an answer is required of the user).\n\n  The ""answer"" return value is True for ""yes"" or False for ""no"".\n  """"""\n  valid = {""yes"": True, ""y"": True, ""ye"": True,\n           ""no"": False, ""n"": False}\n  if default is None:\n    prompt = "" [y/n] ""\n  elif default == ""yes"":\n    prompt = "" [Y/n] ""\n  elif default == ""no"":\n    prompt = "" [y/N] ""\n  else:\n    raise ValueError(""invalid default answer: \'%s\'"" % default)\n\n  while True:\n    sys.stdout.write(question + prompt)\n    if bypass:\n        break\n    if sys.version_info[0] == 3:\n      choice = input().lower() # if version 3 of Python\n    else:\n      choice = raw_input().lower()\n    if default is not None and choice == \'\':\n      return valid[default]\n    elif choice in valid:\n      return valid[choice]\n    else:\n      sys.stdout.write(""Please respond with \'yes\' or \'no\' ""\n                         ""(or \'y\' or \'n\').\\n"")\n\n\ndef rename_class(current_class_name, new_class_name):\n  # get list of txt files\n  file_list = glob.glob(\'*.txt\')\n  file_list.sort()\n  # iterate through the txt files\n  for txt_file in file_list:\n    class_found = False\n    # open txt file lines to a list\n    with open(txt_file) as f:\n      content = f.readlines()\n    # remove whitespace characters like `\\n` at the end of each line\n    content = [x.strip() for x in content]\n    new_content = []\n    # go through each line of eache file\n    for line in content:\n      #class_name = line.split()[0]\n      if current_class_name in line:\n        class_found = True\n        line = line.replace(current_class_name, new_class_name)\n      new_content.append(line)\n    if class_found:\n      # rewrite file\n      with open(txt_file, \'w\') as new_f:\n        for line in new_content:\n          new_f.write(""%s\\n"" % line)\n\nwith open(\'../../data/classes/coco.names\') as f:\n    for line in f:\n        current_class_name = line.rstrip(""\\n"")\n        new_class_name = line.replace(\' \', args.delimiter).rstrip(""\\n"")\n        if current_class_name == new_class_name:\n            continue\n        y_n_message = (""Are you sure you want ""\n                       ""to rename the class ""\n                       ""\\"""" + current_class_name + ""\\"" ""\n                       ""into \\"""" + new_class_name + ""\\""?""\n                      )\n\n        if query_yes_no(y_n_message, bypass=args.yes):\n          os.chdir(""../ground-truth"")\n          rename_class(current_class_name, new_class_name)\n          os.chdir(""../predicted"")\n          rename_class(current_class_name, new_class_name)\n\nprint(\'Done!\')\n'"
scripts/voc/voc_convert.py,0,"b'import sys\nimport os\n\nfrom absl import app, flags\nfrom absl.flags import FLAGS\nfrom lxml import etree\n\n\nflags.DEFINE_string(\'image_dir\', \'../../data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/JPEGImages\', \'path to image dir\')\nflags.DEFINE_string(\'anno_dir\', \'../../data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/Annotations\', \'path to anno dir\')\nflags.DEFINE_string(\'train_list_txt\', \'../../data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Main/train.txt\', \'path to a set of train\')\nflags.DEFINE_string(\'val_list_txt\', \'../../data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Main/val.txt\', \'path to a set of val\')\nflags.DEFINE_string(\'classes\', \'../../data/classes/voc2012.names\', \'path to a list of class names\')\nflags.DEFINE_string(\'train_output\', \'../../data/dataset/voc2012_train.txt\', \'path to a file for train\')\nflags.DEFINE_string(\'val_output\', \'../../data/dataset/voc2012_val.txt\', \'path to a file for val\')\n\nflags.DEFINE_boolean(\'no_val\', False, \'if uses this flag, it does not convert a list of val\')\n\n\ndef convert_annotation(list_txt, output_path, image_dir, anno_dir, class_names):\n    IMAGE_EXT = \'.jpg\'\n    ANNO_EXT = \'.xml\'\n\n    with open(list_txt, \'r\') as f, open(output_path, \'w\') as wf:\n        while True:\n            line = f.readline().strip()\n            if line is None or not line:\n                break\n            im_p = os.path.join(image_dir, line + IMAGE_EXT)\n            an_p = os.path.join(anno_dir, line + ANNO_EXT)\n\n            # Get annotation.\n            root = etree.parse(an_p).getroot()\n            bboxes = root.xpath(\'//object/bndbox\')\n            names = root.xpath(\'//object/name\')\n\n            box_annotations = []\n            for b, n in zip(bboxes, names):\n                name = n.text\n                class_idx = class_names.index(name)\n\n                xmin = b.find(\'xmin\').text\n                ymin = b.find(\'ymin\').text\n                xmax = b.find(\'xmax\').text\n                ymax = b.find(\'ymax\').text\n                box_annotations.append(\',\'.join([str(xmin), str(ymin), str(xmax), str(ymax), str(class_idx)]))\n            \n            annotation = os.path.abspath(im_p) + \' \' + \' \'.join(box_annotations) + \'\\n\'\n\n            wf.write(annotation)\n\n\ndef convert_voc(image_dir, anno_dir, train_list_txt, val_list_txt, classes, train_output, val_output, no_val):\n    IMAGE_EXT = \'.jpg\'\n    ANNO_EXT = \'.xml\'\n\n    class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n\n    # Training set.\n    convert_annotation(train_list_txt, train_output, image_dir, anno_dir, class_names)\n\n    if no_val:\n        return\n\n    # Validation set.\n    convert_annotation(val_list_txt, val_output, image_dir, anno_dir, class_names)\n\n\ndef main(_argv):\n    convert_voc(FLAGS.image_dir, FLAGS.anno_dir, FLAGS.train_list_txt, FLAGS.val_list_txt, FLAGS.classes, FLAGS.train_output, FLAGS.val_output, FLAGS.no_val)\n    print(""Complete convert voc data!"")\n\n\nif __name__ == ""__main__"":\n    try:\n        app.run(main)\n    except SystemExit:\n        pass    \n'"
scripts/voc/voc_make_names.py,0,"b'import sys\nimport os\n\nfrom absl import app, flags\nfrom absl.flags import FLAGS\nfrom lxml import etree\n\n\nflags.DEFINE_string(\'anno_dir\', \'../../data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/Annotations\', \'path to anno dir\')\nflags.DEFINE_string(\'output\', \'../../data/classes/voc2012.names\', \'path to anno dir\')\n\n\ndef make_names(anno_dir, output):\n    labels_dict = {}\n\n    anno_list = os.listdir(anno_dir)\n\n    for anno_file in anno_list:\n        p = os.path.join(anno_dir, anno_file)\n        \n        # Get annotation.\n        root = etree.parse(p).getroot()\n        names = root.xpath(\'//object/name\')\n\n        for n in names:\n            labels_dict[n.text] = 0\n    \n    labels = list(labels_dict.keys())\n    labels.sort()\n\n    with open(output, \'w\') as f:\n        for l in labels:\n            f.writelines(l + \'\\n\')\n\n    print(f""Done making a names\'s file ({os.path.abspath(output)})"")\n\n\ndef main(_argv):\n    make_names(FLAGS.anno_dir, FLAGS.output)\n\n\nif __name__ == ""__main__"":\n    try:\n        app.run(main)\n    except SystemExit:\n        pass    \n'"
