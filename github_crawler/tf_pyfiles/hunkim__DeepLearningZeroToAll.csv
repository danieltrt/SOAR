file_path,api_count,code
lab-02-1-linear_regression.py,7,"b'# Lab 2 Linear Regression\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\n# X and Y data\nx_train = [1, 2, 3]\ny_train = [1, 2, 3]\n\n# Try to find values for W and b to compute y_data = x_data * W + b\n# We know that W should be 1 and b should be 0\n# But let TensorFlow figure it out\nW = tf.Variable(tf.random_normal([1]), name=""weight"")\nb = tf.Variable(tf.random_normal([1]), name=""bias"")\n\n# Our hypothesis XW+b\nhypothesis = x_train * W + b\n\n# cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - y_train))\n\n# optimizer\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n\n# Launch the graph in a session.\nwith tf.Session() as sess:\n    # Initializes global variables in the graph.\n    sess.run(tf.global_variables_initializer())\n\n    # Fit the line\n    for step in range(2001):\n        _, cost_val, W_val, b_val = sess.run([train, cost, W, b])\n\n        if step % 20 == 0:\n            print(step, cost_val, W_val, b_val)\n\n# Learns best fit W:[ 1.],  b:[ 0.]\n""""""\n0 2.82329 [ 2.12867713] [-0.85235667]\n20 0.190351 [ 1.53392804] [-1.05059612]\n40 0.151357 [ 1.45725465] [-1.02391243]\n...\n1960 1.46397e-05 [ 1.004444] [-0.01010205]\n1980 1.32962e-05 [ 1.00423515] [-0.00962736]\n2000 1.20761e-05 [ 1.00403607] [-0.00917497]\n""""""\n'"
lab-02-2-linear_regression_feed.py,9,"b'# Lab 2 Linear Regression\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\n# Try to find values for W and b to compute Y = W * X + b\nW = tf.Variable(tf.random_normal([1]), name=""weight"")\nb = tf.Variable(tf.random_normal([1]), name=""bias"")\n\n# placeholders for a tensor that will be always fed using feed_dict\n# See http://stackoverflow.com/questions/36693740/\nX = tf.placeholder(tf.float32, shape=[None])\nY = tf.placeholder(tf.float32, shape=[None])\n\n# Our hypothesis is X * W + b\nhypothesis = X * W + b\n\n# cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# optimizer\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n\n# Launch the graph in a session.\nwith tf.Session() as sess:\n    # Initializes global variables in the graph.\n    sess.run(tf.global_variables_initializer())\n\n    # Fit the line\n    for step in range(2001):\n        _, cost_val, W_val, b_val = sess.run(\n            [train, cost, W, b], feed_dict={X: [1, 2, 3], Y: [1, 2, 3]}\n        )\n        if step % 20 == 0:\n            print(step, cost_val, W_val, b_val)\n\n    # Testing our model\n    print(sess.run(hypothesis, feed_dict={X: [5]}))\n    print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n    print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))\n\n    # Learns best fit W:[ 1.],  b:[ 0]\n    """"""\n    0 3.5240757 [2.2086694] [-0.8204183]\n    20 0.19749963 [1.5425726] [-1.0498911]\n    ...\n    1980 1.3360998e-05 [1.0042454] [-0.00965055]\n    2000 1.21343355e-05 [1.0040458] [-0.00919707]\n    [5.0110054]\n    [2.500915]\n    [1.4968792 3.5049512]\n    """"""\n\n    # Fit the line with new training data\n    for step in range(2001):\n        _, cost_val, W_val, b_val = sess.run(\n            [train, cost, W, b],\n            feed_dict={X: [1, 2, 3, 4, 5], Y: [2.1, 3.1, 4.1, 5.1, 6.1]},\n        )\n        if step % 20 == 0:\n            print(step, cost_val, W_val, b_val)\n\n    # Testing our model\n    print(sess.run(hypothesis, feed_dict={X: [5]}))\n    print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n    print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))\n\n    # Learns best fit W:[ 1.],  b:[ 1.1]\n    """"""\n    0 1.2035878 [1.0040361] [-0.00917497]\n    20 0.16904518 [1.2656431] [0.13599995]\n    ...\n    1980 2.9042917e-07 [1.00035] [1.0987366]\n    2000 2.5372992e-07 [1.0003271] [1.0988194]\n    [6.1004534]\n    [3.5996385]\n    [2.5993123 4.599964 ]\n    """"""\n'"
lab-02-3-linear_regression_tensorflow.org.py,8,"b'# From https://www.tensorflow.org/get_started/get_started\nimport tensorflow as tf\n\n# training data\nx_train = [1, 2, 3, 4]\ny_train = [0, -1, -2, -3]\n\n# Model parameters\nW = tf.Variable([0.3], tf.float32)\nb = tf.Variable([-0.3], tf.float32)\n\n# Model input and output\nx = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\n\nhypothesis = x * W + b\n\n# cost/loss function\ncost = tf.reduce_sum(tf.square(hypothesis - y))  # sum of the squares\n\n# optimizer\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n\n# training\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(1000):\n        sess.run(train, {x: x_train, y: y_train})\n\n    # evaluate training accuracy\n    W_val, b_val, cost_val = sess.run([W, b, cost], feed_dict={x: x_train, y: y_train})\n    print(f""W: {W_val} b: {b_val} cost: {cost_val}"")\n\n""""""\nW: [-0.9999969] b: [0.9999908] cost: 5.699973826267524e-11\n""""""\n'"
lab-03-1-minimizing_cost_show_graph.py,3,"b'# Lab 3 Minimizing Cost\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nX = [1, 2, 3]\nY = [1, 2, 3]\n\nW = tf.placeholder(tf.float32)\n\n# Our hypothesis for linear model X * W\nhypothesis = X * W\n\n# cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Variables for plotting cost function\nW_history = []\ncost_history = []\n\n# Launch the graph in a session.\nwith tf.Session() as sess:\n    for i in range(-30, 50):\n        curr_W = i * 0.1\n        curr_cost = sess.run(cost, feed_dict={W: curr_W})\n\n        W_history.append(curr_W)\n        cost_history.append(curr_cost)\n\n# Show the cost function\nplt.plot(W_history, cost_history)\nplt.show()\n'"
lab-03-2-minimizing_cost_gradient_update.py,8,"b'# Lab 3 Minimizing Cost\nimport tensorflow as tf\n\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = [1, 2, 3]\ny_data = [1, 2, 3]\n\n# Try to find values for W and b to compute y_data = W * x_data\n# We know that W should be 1\n# But let\'s use TensorFlow to figure it out\nW = tf.Variable(tf.random_normal([1]), name=""weight"")\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\n# Our hypothesis for linear model X * W\nhypothesis = X * W\n\n# cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\nlearning_rate = 0.1\ngradient = tf.reduce_mean((W * X - Y) * X)\ndescent = W - learning_rate * gradient\nupdate = W.assign(descent)\n\n# Launch the graph in a session.\nwith tf.Session() as sess:\n    # Initializes global variables in the graph.\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(21):\n        _, cost_val, W_val = sess.run(\n            [update, cost, W], feed_dict={X: x_data, Y: y_data}\n        )\n        print(step, cost_val, W_val)\n\n""""""\n0 1.93919 [ 1.64462376]\n1 0.551591 [ 1.34379935]\n2 0.156897 [ 1.18335962]\n3 0.0446285 [ 1.09779179]\n4 0.0126943 [ 1.05215561]\n5 0.00361082 [ 1.0278163]\n6 0.00102708 [ 1.01483536]\n7 0.000292144 [ 1.00791216]\n8 8.30968e-05 [ 1.00421977]\n9 2.36361e-05 [ 1.00225055]\n10 6.72385e-06 [ 1.00120032]\n11 1.91239e-06 [ 1.00064015]\n12 5.43968e-07 [ 1.00034142]\n13 1.54591e-07 [ 1.00018203]\n14 4.39416e-08 [ 1.00009704]\n15 1.24913e-08 [ 1.00005174]\n16 3.5322e-09 [ 1.00002754]\n17 9.99824e-10 [ 1.00001466]\n18 2.88878e-10 [ 1.00000787]\n19 8.02487e-11 [ 1.00000417]\n20 2.34053e-11 [ 1.00000226]\n""""""\n'"
lab-03-3-minimizing_cost_tf_optimizer.py,5,"b'# Lab 3 Minimizing Cost\nimport tensorflow as tf\n\n# tf Graph Input\nX = [1, 2, 3]\nY = [1, 2, 3]\n\n# Set wrong model weights\nW = tf.Variable(5.0)\n\n# Linear model\nhypothesis = X * W\n\n# cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize: Gradient Descent Optimizer\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n# Launch the graph in a session.\nwith tf.Session() as sess:\n    # Initializes global variables in the graph.\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(101):\n        _, W_val = sess.run([train, W])\n        print(step, W_val)\n\n""""""\n0 5.0\n1 1.2666664\n2 1.0177778\n3 1.0011852\n4 1.000079\n...\n97 1.0\n98 1.0\n99 1.0\n100 1.0\n""""""\n'"
lab-03-X-minimizing_cost_tf_gradient.py,7,"b""# Lab 3 Minimizing Cost\n# This is optional\nimport tensorflow as tf\n\n# tf Graph Input\nX = [1, 2, 3]\nY = [1, 2, 3]\n\n# Set wrong model weights\nW = tf.Variable(5.)\n\n# Linear model\nhypothesis = X * W\n\n# Manual gradient\ngradient = tf.reduce_mean((W * X - Y) * X) * 2\n\n# cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize: Gradient Descent Optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n\n# Get gradients\ngvs = optimizer.compute_gradients(cost)\n\n# Optional: modify gradient if necessary\n# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n\n# Apply gradients\napply_gradients = optimizer.apply_gradients(gvs)\n\n# Launch the graph in a session.\nwith tf.Session() as sess:\n    # Initializes global variables in the graph.\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(101):\n        gradient_val, gvs_val, _ = sess.run([gradient, gvs, apply_gradients])\n        print(step, gradient_val, gvs_val)\n\n'''\n0 37.333332 [(37.333336, 5.0)]\n1 33.84889 [(33.84889, 4.6266665)]\n2 30.689657 [(30.689657, 4.2881775)]\n3 27.825289 [(27.825289, 3.981281)]\n...\n97 0.0027837753 [(0.0027837753, 1.0002983)]\n98 0.0025234222 [(0.0025234222, 1.0002704)]\n99 0.0022875469 [(0.0022875469, 1.0002451)]\n100 0.0020739238 [(0.0020739238, 1.0002222)]\n'''\n"""
lab-04-1-multi_variable_linear_regression.py,13,"b'# Lab 4 Multi-variable linear regression\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\nx1_data = [73., 93., 89., 96., 73.]\nx2_data = [80., 88., 91., 98., 66.]\nx3_data = [75., 93., 90., 100., 70.]\n\ny_data = [152., 185., 180., 196., 142.]\n\n# placeholders for a tensor that will be always fed.\nx1 = tf.placeholder(tf.float32)\nx2 = tf.placeholder(tf.float32)\nx3 = tf.placeholder(tf.float32)\n\nY = tf.placeholder(tf.float32)\n\nw1 = tf.Variable(tf.random_normal([1]), name=\'weight1\')\nw2 = tf.Variable(tf.random_normal([1]), name=\'weight2\')\nw3 = tf.Variable(tf.random_normal([1]), name=\'weight3\')\nb = tf.Variable(tf.random_normal([1]), name=\'bias\')\n\nhypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n\n# cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize. Need a very small learning rate for this data set\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\ntrain = optimizer.minimize(cost)\n\n# Launch the graph in a session.\nsess = tf.Session()\n# Initializes global variables in the graph.\nsess.run(tf.global_variables_initializer())\n\nfor step in range(2001):\n    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n                                   feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n    if step % 10 == 0:\n        print(step, ""Cost: "", cost_val, ""\\nPrediction:\\n"", hy_val)\n\n\'\'\'\n0 Cost:  19614.8\nPrediction:\n [ 21.69748688  39.10213089  31.82624626  35.14236832  32.55316544]\n10 Cost:  14.0682\nPrediction:\n [ 145.56100464  187.94958496  178.50236511  194.86721802  146.08096313]\n\n ...\n\n1990 Cost:  4.9197\nPrediction:\n [ 148.15084839  186.88632202  179.6293335   195.81796265  144.46044922]\n2000 Cost:  4.89449\nPrediction:\n [ 148.15931702  186.8805542   179.63194275  195.81971741  144.45298767]\n\'\'\'\n'"
lab-04-2-multi_variable_matmul_linear_regression.py,10,"b'# Lab 4 Multi-variable linear regression\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = [[73., 80., 75.],\n          [93., 88., 93.],\n          [89., 91., 90.],\n          [96., 98., 100.],\n          [73., 66., 70.]]\ny_data = [[152.],\n          [185.],\n          [180.],\n          [196.],\n          [142.]]\n\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 3])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([3, 1]), name=\'weight\')\nb = tf.Variable(tf.random_normal([1]), name=\'bias\')\n\n# Hypothesis\nhypothesis = tf.matmul(X, W) + b\n\n# Simplified cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\ntrain = optimizer.minimize(cost)\n\n# Launch the graph in a session.\nsess = tf.Session()\n# Initializes global variables in the graph.\nsess.run(tf.global_variables_initializer())\n\nfor step in range(2001):\n    cost_val, hy_val, _ = sess.run(\n        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n    if step % 10 == 0:\n        print(step, ""Cost: "", cost_val, ""\\nPrediction:\\n"", hy_val)\n\n\'\'\'\n0 Cost:  7105.46\nPrediction:\n [[ 80.82241058]\n [ 92.26364136]\n [ 93.70250702]\n [ 98.09217834]\n [ 72.51759338]]\n10 Cost:  5.89726\nPrediction:\n [[ 155.35159302]\n [ 181.85691833]\n [ 181.97254944]\n [ 194.21760559]\n [ 140.85707092]]\n\n...\n\n1990 Cost:  3.18588\nPrediction:\n [[ 154.36352539]\n [ 182.94833374]\n [ 181.85189819]\n [ 194.35585022]\n [ 142.03240967]]\n2000 Cost:  3.1781\nPrediction:\n [[ 154.35881042]\n [ 182.95147705]\n [ 181.85035706]\n [ 194.35533142]\n [ 142.036026  ]]\n\n\'\'\'\n'"
lab-04-3-file_input_linear_regression.py,10,"b'# Lab 4 Multi-variable linear regression\nimport tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility\n\nxy = np.loadtxt(\'data-01-test-score.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\n# Make sure the shape and data are OK\nprint(x_data, ""\\nx_data shape:"", x_data.shape)\nprint(y_data, ""\\ny_data shape:"", y_data.shape)\n\n# data output\n\'\'\'\n[[ 73.  80.  75.]\n [ 93.  88.  93.]\n ...\n [ 76.  83.  71.]\n [ 96.  93.  95.]] \nx_data shape: (25, 3)\n[[152.]\n [185.]\n ...\n [149.]\n [192.]] \ny_data shape: (25, 1)\n\'\'\'\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 3])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([3, 1]), name=\'weight\')\nb = tf.Variable(tf.random_normal([1]), name=\'bias\')\n\n# Hypothesis\nhypothesis = tf.matmul(X, W) + b\n\n# Simplified cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\ntrain = optimizer.minimize(cost)\n\n# Launch the graph in a session.\nsess = tf.Session()\n# Initializes global variables in the graph.\nsess.run(tf.global_variables_initializer())\n\nfor step in range(2001):\n    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], \n                                   feed_dict={X: x_data, Y: y_data})\n    if step % 10 == 0:\n        print(step, ""Cost:"", cost_val, ""\\nPrediction:\\n"", hy_val)\n\n# train output\n\'\'\'\n0 Cost: 21027.0 \nPrediction:\n [[22.048063 ]\n [21.619772 ]\n ...\n [31.36112  ]\n [24.986364 ]]\n10 Cost: 95.976326 \nPrediction:\n [[157.11063 ]\n [183.99283 ]\n ...\n [167.48862 ]\n [193.25117 ]]\n 1990 Cost: 24.863274 \nPrediction:\n [[154.4393  ]\n [185.5584  ]\n ...\n [158.27443 ]\n [192.79778 ]]\n2000 Cost: 24.722485 \nPrediction:\n [[154.42894 ]\n [185.5586  ]\n ...\n [158.24257 ]\n [192.79166 ]]\n\'\'\'\n\n# Ask my score\nprint(""Your score will be "", sess.run(hypothesis, \n                                      feed_dict={X: [[100, 70, 101]]}))\n\nprint(""Other scores will be "", sess.run(hypothesis,\n                                        feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))\n\n\'\'\'\nYour score will be  [[ 181.73277283]]\nOther scores will be  [[ 145.86265564]\n [ 187.23129272]]\n\n\'\'\'\n'"
lab-04-4-tf_reader_linear_regression.py,16,"b'# Lab 4 Multi-variable linear regression\n# https://www.tensorflow.org/programmers_guide/reading_data\n\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\nfilename_queue = tf.train.string_input_producer(\n    [\'data-01-test-score.csv\'], shuffle=False, name=\'filename_queue\')\n\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\n\n# Default values, in case of empty columns. Also specifies the type of the\n# decoded result.\nrecord_defaults = [[0.], [0.], [0.], [0.]]\nxy = tf.decode_csv(value, record_defaults=record_defaults)\n\n# collect batches of csv in\ntrain_x_batch, train_y_batch = \\\n    tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 3])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([3, 1]), name=\'weight\')\nb = tf.Variable(tf.random_normal([1]), name=\'bias\')\n\n# Hypothesis\nhypothesis = tf.matmul(X, W) + b\n\n# Simplified cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\ntrain = optimizer.minimize(cost)\n\n# Launch the graph in a session.\nsess = tf.Session()\n# Initializes global variables in the graph.\nsess.run(tf.global_variables_initializer())\n\n# Start populating the filename queue.\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\nfor step in range(2001):\n    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n    cost_val, hy_val, _ = sess.run(\n        [cost, hypothesis, train], feed_dict={X: x_batch, Y: y_batch})\n    if step % 10 == 0:\n        print(step, ""Cost: "", cost_val, ""\\nPrediction:\\n"", hy_val)\n\ncoord.request_stop()\ncoord.join(threads)\n\n# Ask my score\nprint(""Your score will be "",\n      sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))\n\nprint(""Other scores will be "",\n      sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))\n\n\'\'\'\nYour score will be  [[185.33531]]\nOther scores will be  [[178.36246]\n [177.03687]]\n\'\'\'\n'"
lab-05-1-logistic_regression.py,14,"b'# Lab 5 Logistic Regression Classifier\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = [[1, 2],\n          [2, 3],\n          [3, 1],\n          [4, 3],\n          [5, 3],\n          [6, 2]]\ny_data = [[0],\n          [0],\n          [0],\n          [1],\n          [1],\n          [1]]\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 2])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([2, 1]), name=\'weight\')\nb = tf.Variable(tf.random_normal([1]), name=\'bias\')\n\n# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\nhypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n\n# cost/loss function\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n                       tf.log(1 - hypothesis))\n\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(10001):\n        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n        if step % 200 == 0:\n            print(step, cost_val)\n\n    # Accuracy report\n    h, c, a = sess.run([hypothesis, predicted, accuracy],\n                       feed_dict={X: x_data, Y: y_data})\n    print(""\\nHypothesis: "", h, ""\\nCorrect (Y): "", c, ""\\nAccuracy: "", a)\n\n\'\'\'\n0 1.73078\n200 0.571512\n400 0.507414\n600 0.471824\n800 0.447585\n...\n9200 0.159066\n9400 0.15656\n9600 0.154132\n9800 0.151778\n10000 0.149496\n\nHypothesis:  [[ 0.03074029]\n [ 0.15884677]\n [ 0.30486736]\n [ 0.78138196]\n [ 0.93957496]\n [ 0.98016882]]\nCorrect (Y):  [[ 0.]\n [ 0.]\n [ 0.]\n [ 1.]\n [ 1.]\n [ 1.]]\nAccuracy:  1.0\n\'\'\'\n'"
lab-05-2-logistic_regression_diabetes.py,14,"b'# Lab 5 Logistic Regression Classifier\nimport tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility\n\nxy = np.loadtxt(\'data-03-diabetes.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(x_data.shape, y_data.shape)\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 8])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([8, 1]), name=\'weight\')\nb = tf.Variable(tf.random_normal([1]), name=\'bias\')\n\n# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(-tf.matmul(X, W)))\nhypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n\n# cost/loss function\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n                       tf.log(1 - hypothesis))\n\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(10001):\n        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n        if step % 200 == 0:\n            print(step, cost_val)\n\n    # Accuracy report\n    h, c, a = sess.run([hypothesis, predicted, accuracy],\n                       feed_dict={X: x_data, Y: y_data})\n    print(""\\nHypothesis: "", h, ""\\nCorrect (Y): "", c, ""\\nAccuracy: "", a)\n\n\'\'\'\n0 0.82794\n200 0.755181\n400 0.726355\n600 0.705179\n800 0.686631\n...\n9600 0.492056\n9800 0.491396\n10000 0.490767\n\n...\n\n [ 1.]\n [ 1.]\n [ 1.]]\nAccuracy:  0.762846\n\'\'\'\n'"
lab-06-1-softmax_classifier.py,15,"b'# Lab 6 Softmax Classifier\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = [[1, 2, 1, 1],\n          [2, 1, 3, 2],\n          [3, 1, 3, 4],\n          [4, 1, 5, 5],\n          [1, 7, 5, 5],\n          [1, 2, 5, 6],\n          [1, 6, 6, 6],\n          [1, 7, 7, 7]]\ny_data = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1],\n          [0, 1, 0],\n          [0, 1, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [1, 0, 0]]\n\nX = tf.placeholder(""float"", [None, 4])\nY = tf.placeholder(""float"", [None, 3])\nnb_classes = 3\n\nW = tf.Variable(tf.random_normal([4, nb_classes]), name=\'weight\')\nb = tf.Variable(tf.random_normal([nb_classes]), name=\'bias\')\n\n# tf.nn.softmax computes softmax activations\n# softmax = exp(logits) / reduce_sum(exp(logits), dim)\nhypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n\n# Cross entropy cost/loss\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n# Launch graph\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(2001):\n            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n\n            if step % 200 == 0:\n                print(step, cost_val)\n\n    print(\'--------------\')\n    # Testing & One-hot encoding\n    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n    print(a, sess.run(tf.argmax(a, 1)))\n\n    print(\'--------------\')\n    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n    print(b, sess.run(tf.argmax(b, 1)))\n\n    print(\'--------------\')\n    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n    print(c, sess.run(tf.argmax(c, 1)))\n\n    print(\'--------------\')\n    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n    print(all, sess.run(tf.argmax(all, 1)))\n\n\'\'\'\n0 6.926112\n200 0.6005015\n400 0.47295815\n600 0.37342924\n800 0.28018373\n1000 0.23280522\n1200 0.21065344\n1400 0.19229904\n1600 0.17682323\n1800 0.16359556\n2000 0.15216158\n-------------\n[[1.3890490e-03 9.9860185e-01 9.0613084e-06]] [1]\n-------------\n[[0.9311919  0.06290216 0.00590591]] [0]\n-------------\n[[1.2732815e-08 3.3411323e-04 9.9966586e-01]] [2]\n-------------\n[[1.3890490e-03 9.9860185e-01 9.0613084e-06]\n [9.3119192e-01 6.2902197e-02 5.9059085e-03]\n [1.2732815e-08 3.3411323e-04 9.9966586e-01]] [1 0 2]\n\'\'\'\n'"
lab-06-2-softmax_zoo_classifier.py,18,"b'# Lab 6 Softmax Classifier\nimport tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility\n\n# Predicting animal type based on various features\nxy = np.loadtxt(\'data-04-zoo.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(x_data.shape, y_data.shape)\n\n\'\'\'\n(101, 16) (101, 1)\n\'\'\'\n\nnb_classes = 7  # 0 ~ 6\n\nX = tf.placeholder(tf.float32, [None, 16])\nY = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n\nY_one_hot = tf.one_hot(Y, nb_classes)  # one hot\nprint(""one_hot:"", Y_one_hot)\nY_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\nprint(""reshape one_hot:"", Y_one_hot)\n\n\'\'\'\none_hot: Tensor(""one_hot:0"", shape=(?, 1, 7), dtype=float32)\nreshape one_hot: Tensor(""Reshape:0"", shape=(?, 7), dtype=float32)\n\'\'\'\n\nW = tf.Variable(tf.random_normal([16, nb_classes]), name=\'weight\')\nb = tf.Variable(tf.random_normal([nb_classes]), name=\'bias\')\n\n# tf.nn.softmax computes softmax activations\n# softmax = exp(logits) / reduce_sum(exp(logits), dim)\nlogits = tf.matmul(X, W) + b\nhypothesis = tf.nn.softmax(logits)\n\n# Cross entropy cost/loss\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n                                                                 labels=tf.stop_gradient([Y_one_hot])))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\nprediction = tf.argmax(hypothesis, 1)\ncorrect_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(2001):\n        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n                                        \n        if step % 100 == 0:\n            print(""Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}"".format(step, cost_val, acc_val))\n\n    # Let\'s see if we can predict\n    pred = sess.run(prediction, feed_dict={X: x_data})\n    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n    for p, y in zip(pred, y_data.flatten()):\n        print(""[{}] Prediction: {} True Y: {}"".format(p == int(y), p, int(y)))\n\n\'\'\'\nStep:     0 Loss: 5.106 Acc: 37.62%\nStep:   100 Loss: 0.800 Acc: 79.21%\nStep:   200 Loss: 0.486 Acc: 88.12%\n...\nStep:  1800\tLoss: 0.060\tAcc: 100.00%\nStep:  1900\tLoss: 0.057\tAcc: 100.00%\nStep:  2000\tLoss: 0.054\tAcc: 100.00%\n[True] Prediction: 0 True Y: 0\n[True] Prediction: 0 True Y: 0\n[True] Prediction: 3 True Y: 3\n...\n[True] Prediction: 0 True Y: 0\n[True] Prediction: 6 True Y: 6\n[True] Prediction: 1 True Y: 1\n\'\'\'\n'"
lab-07-1-learning_rate_and_evaluation.py,14,"b'# Lab 7 Learning rate and Evaluation\nimport tensorflow as tf\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = [[1, 2, 1],\n          [1, 3, 2],\n          [1, 3, 4],\n          [1, 5, 5],\n          [1, 7, 5],\n          [1, 2, 5],\n          [1, 6, 6],\n          [1, 7, 7]]\ny_data = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1],\n          [0, 1, 0],\n          [0, 1, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [1, 0, 0]]\n\n# Evaluation our model using this test dataset\nx_test = [[2, 1, 1],\n          [3, 1, 2],\n          [3, 3, 4]]\ny_test = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1]]\n\nX = tf.placeholder(""float"", [None, 3])\nY = tf.placeholder(""float"", [None, 3])\n\nW = tf.Variable(tf.random_normal([3, 3]))\nb = tf.Variable(tf.random_normal([3]))\n\n# tf.nn.softmax computes softmax activations\n# softmax = exp(logits) / reduce_sum(exp(logits), dim)\nhypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n\n# Cross entropy cost/loss\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n# Try to change learning_rate to small numbers\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n# Correct prediction Test model\nprediction = tf.argmax(hypothesis, 1)\nis_correct = tf.equal(prediction, tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(201):\n        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n        print(step, cost_val, W_val)\n\n    # predict\n    print(""Prediction:"", sess.run(prediction, feed_dict={X: x_test}))\n    # Calculate the accuracy\n    print(""Accuracy: "", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n\n\'\'\'\nwhen lr = 1.5\n\n0 5.73203 [[-0.30548954  1.22985029 -0.66033536]\n [-4.39069986  2.29670858  2.99386835]\n [-3.34510708  2.09743214 -0.80419564]]\n1 23.1494 [[ 0.06951046  0.29449689 -0.0999819 ]\n [-1.95319986 -1.63627958  4.48935604]\n [-0.90760708 -1.65020132  0.50593793]]\n2 27.2798 [[ 0.44451016  0.85699677 -1.03748143]\n [ 0.48429942  0.98872018 -0.57314301]\n [ 1.52989244  1.16229868 -4.74406147]]\n3 8.668 [[ 0.12396193  0.61504567 -0.47498202]\n [ 0.22003263 -0.2470119   0.9268558 ]\n [ 0.96035379  0.41933775 -3.43156195]]\n4 5.77111 [[-0.9524312   1.13037777  0.08607888]\n [-3.78651619  2.26245379  2.42393875]\n [-3.07170963  3.14037919 -2.12054014]]\n5 inf [[ nan  nan  nan]\n [ nan  nan  nan]\n [ nan  nan  nan]]\n6 nan [[ nan  nan  nan]\n [ nan  nan  nan]\n [ nan  nan  nan]]\n ...\nPrediction: [0 0 0]\nAccuracy:  0.0\n-------------------------------------------------\nWhen lr = 1e-10\n\n0 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\n1 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\n...\n199 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\n200 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n [-0.3051686  -0.3032113   1.50825703]\n [ 0.75722361 -0.7008909  -2.10820389]]\nPrediction: [0 0 0]\nAccuracy:  0.0\n-------------------------------------------------\nWhen lr = 0.1\n\n0 5.73203 [[ 0.72881663  0.71536207 -1.18015325]\n [-0.57753736 -0.12988332  1.60729778]\n [ 0.48373488 -0.51433605 -2.02127004]]\n1 3.318 [[ 0.66219079  0.74796319 -1.14612854]\n [-0.81948912  0.03000021  1.68936598]\n [ 0.23214608 -0.33772916 -1.94628811]]\n...\n199 0.672261 [[-1.15377033  0.28146935  1.13632679]\n [ 0.37484586  0.18958236  0.33544877]\n [-0.35609841 -0.43973011 -1.25604188]]\n200 0.670909 [[-1.15885413  0.28058422  1.14229572]\n [ 0.37609792  0.19073224  0.33304682]\n [-0.35536593 -0.44033223 -1.2561723 ]]\nPrediction: [2 2 2]\nAccuracy:  1.0\n\'\'\'\n'"
lab-07-2-linear_regression_without_min_max.py,10,"b'import tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility\n\n\nxy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n               [816, 820.958984, 1008100, 815.48999, 819.23999],\n               [819.359985, 823, 1188100, 818.469971, 818.97998],\n               [819, 823, 1198100, 816, 820.450012],\n               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 4])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([4, 1]), name=\'weight\')\nb = tf.Variable(tf.random_normal([1]), name=\'bias\')\n\n# Hypothesis\nhypothesis = tf.matmul(X, W) + b\n\n# Simplified cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\ntrain = optimizer.minimize(cost)\n\n# Launch the graph in a session.\nsess = tf.Session()\n# Initializes global variables in the graph.\nsess.run(tf.global_variables_initializer())\n\nfor step in range(101):\n    cost_val, hy_val, _ = sess.run(\n        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n    print(step, ""Cost: "", cost_val, ""\\nPrediction:\\n"", hy_val)\n\n\n\'\'\'\n0 Cost:  2.45533e+12\nPrediction:\n [[-1104436.375]\n [-2224342.75 ]\n [-1749606.75 ]\n [-1226179.375]\n [-1445287.125]\n [-1457459.5  ]\n [-1335740.5  ]\n [-1700924.625]]\n1 Cost:  2.69762e+27\nPrediction:\n [[  3.66371490e+13]\n [  7.37543360e+13]\n [  5.80198785e+13]\n [  4.06716290e+13]\n [  4.79336847e+13]\n [  4.83371348e+13]\n [  4.43026590e+13]\n [  5.64060907e+13]]\n2 Cost:  inf\nPrediction:\n [[ -1.21438790e+21]\n [ -2.44468702e+21]\n [ -1.92314724e+21]\n [ -1.34811610e+21]\n [ -1.58882674e+21]\n [ -1.60219962e+21]\n [ -1.46847142e+21]\n [ -1.86965602e+21]]\n3 Cost:  inf\nPrediction:\n [[  4.02525216e+28]\n [  8.10324465e+28]\n [  6.37453079e+28]\n [  4.46851237e+28]\n [  5.26638074e+28]\n [  5.31070676e+28]\n [  4.86744608e+28]\n [  6.19722623e+28]]\n4 Cost:  inf\nPrediction:\n [[ -1.33422428e+36]\n [ -2.68593010e+36]\n [ -2.11292430e+36]\n [ -1.48114879e+36]\n [ -1.74561303e+36]\n [ -1.76030542e+36]\n [ -1.61338091e+36]\n [ -2.05415459e+36]]\n5 Cost:  inf\nPrediction:\n [[ inf]\n [ inf]\n [ inf]\n [ inf]\n [ inf]\n [ inf]\n [ inf]\n [ inf]]\n6 Cost:  nan\nPrediction:\n [[ nan]\n [ nan]\n [ nan]\n [ nan]\n [ nan]\n [ nan]\n [ nan]\n [ nan]]\n\'\'\'\n'"
lab-07-3-linear_regression_min_max.py,10,"b'import tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility\n\n\ndef min_max_scaler(data):\n    numerator = data - np.min(data, 0)\n    denominator = np.max(data, 0) - np.min(data, 0)\n    # noise term prevents the zero division\n    return numerator / (denominator + 1e-7)\n\n\nxy = np.array(\n    [\n        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n        [816, 820.958984, 1008100, 815.48999, 819.23999],\n        [819.359985, 823, 1188100, 818.469971, 818.97998],\n        [819, 823, 1198100, 816, 820.450012],\n        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n    ]\n)\n\n# very important. It does not work without it.\nxy = min_max_scaler(xy)\nprint(xy)\n\n\'\'\'\n[[0.99999999 0.99999999 0.         1.         1.        ]\n [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n [0.         0.07747099 0.5326087  0.         0.        ]]\n\'\'\'\n\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 4])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.Variable(tf.random_normal([4, 1]), name=\'weight\')\nb = tf.Variable(tf.random_normal([1]), name=\'bias\')\n\n# Hypothesis\nhypothesis = tf.matmul(X, W) + b\n\n# Simplified cost/loss function\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\n\n# Minimize\ntrain = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n\n# Launch the graph in a session.\nwith tf.Session() as sess:\n    # Initializes global variables in the graph.\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(101):\n        _, cost_val, hy_val = sess.run(\n            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n        )\n        print(step, ""Cost: "", cost_val, ""\\nPrediction:\\n"", hy_val)\n\n\'\'\'\n0 Cost: 0.15230925 \nPrediction:\n [[ 1.6346191 ]\n [ 0.06613699]\n [ 0.3500818 ]\n [ 0.6707252 ]\n [ 0.61130744]\n [ 0.61464405]\n [ 0.23171967]\n [-0.1372836 ]]\n1 Cost: 0.15230872 \nPrediction:\n [[ 1.634618  ]\n [ 0.06613836]\n [ 0.35008252]\n [ 0.670725  ]\n [ 0.6113076 ]\n [ 0.6146443 ]\n [ 0.23172   ]\n [-0.13728246]]\n...\n99 Cost: 0.1522546 \nPrediction:\n [[ 1.6345041 ]\n [ 0.06627947]\n [ 0.35014683]\n [ 0.670706  ]\n [ 0.6113161 ]\n [ 0.61466044]\n [ 0.23175153]\n [-0.13716647]]\n100 Cost: 0.15225402 \nPrediction:\n [[ 1.6345029 ]\n [ 0.06628093]\n [ 0.35014752]\n [ 0.67070574]\n [ 0.61131614]\n [ 0.6146606 ]\n [ 0.23175186]\n [-0.13716528]]\n\'\'\'\n'"
lab-07-4-mnist_introduction.py,14,"b'# Lab 7 Learning rate and Evaluation\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport random\n\ntf.set_random_seed(777)  # for reproducibility\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\nnb_classes = 10\n\n# MNIST data image of shape 28 * 28 = 784\nX = tf.placeholder(tf.float32, [None, 784])\n# 0 - 9 digits recognition = 10 classes\nY = tf.placeholder(tf.float32, [None, nb_classes])\n\nW = tf.Variable(tf.random_normal([784, nb_classes]))\nb = tf.Variable(tf.random_normal([nb_classes]))\n\n# Hypothesis (using softmax)\nhypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n\ncost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n# Test model\nis_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n# Calculate accuracy\naccuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n\n# parameters\nnum_epochs = 15\nbatch_size = 100\nnum_iterations = int(mnist.train.num_examples / batch_size)\n\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n    # Training cycle\n    for epoch in range(num_epochs):\n        avg_cost = 0\n\n        for i in range(num_iterations):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n            avg_cost += cost_val / num_iterations\n\n        print(""Epoch: {:04d}, Cost: {:.9f}"".format(epoch + 1, avg_cost))\n\n    print(""Learning finished"")\n\n    # Test the model using test sets\n    print(\n        ""Accuracy: "",\n        accuracy.eval(\n            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n        ),\n    )\n\n    # Get one and predict\n    r = random.randint(0, mnist.test.num_examples - 1)\n    print(""Label: "", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n    print(\n        ""Prediction: "",\n        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n    )\n\n    plt.imshow(\n        mnist.test.images[r : r + 1].reshape(28, 28),\n        cmap=""Greys"",\n        interpolation=""nearest"",\n    )\n    plt.show()\n\n\n\'\'\'\nEpoch: 0001, Cost: 2.826302672\nEpoch: 0002, Cost: 1.061668952\nEpoch: 0003, Cost: 0.838061315\nEpoch: 0004, Cost: 0.733232745\nEpoch: 0005, Cost: 0.669279885\nEpoch: 0006, Cost: 0.624611836\nEpoch: 0007, Cost: 0.591160344\nEpoch: 0008, Cost: 0.563868987\nEpoch: 0009, Cost: 0.541745171\nEpoch: 0010, Cost: 0.522673578\nEpoch: 0011, Cost: 0.506782325\nEpoch: 0012, Cost: 0.492447643\nEpoch: 0013, Cost: 0.479955837\nEpoch: 0014, Cost: 0.468893674\nEpoch: 0015, Cost: 0.458703488\nLearning finished\nAccuracy:  0.8951\n\'\'\'\n'"
lab-09-1-xor.py,13,"b'# Lab 9 XOR\nimport tensorflow as tf\nimport numpy as np\n\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nX = tf.placeholder(tf.float32, [None, 2])\nY = tf.placeholder(tf.float32, [None, 1])\n\nW = tf.Variable(tf.random_normal([2, 1]), name=""weight"")\nb = tf.Variable(tf.random_normal([1]), name=""bias"")\n\n# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\nhypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n\n# cost/loss function\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(10001):\n        _, cost_val, w_val = sess.run(\n                  [train, cost, W], feed_dict={X: x_data, Y: y_data}\n        )\n        if step % 100 == 0:\n            print(step, cost_val, w_val)\n\n    # Accuracy report\n    h, c, a = sess.run(\n              [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n    )\n    print(""\\nHypothesis: "", h, ""\\nCorrect: "", c, ""\\nAccuracy: "", a)\n\n\'\'\'\nHypothesis:  [[ 0.5]\n [ 0.5]\n [ 0.5]\n [ 0.5]]\nCorrect:  [[ 0.]\n [ 0.]\n [ 0.]\n [ 0.]]\nAccuracy:  0.5\n\'\'\'\n'"
lab-09-2-xor-nn.py,15,"b'# Lab 9 XOR\nimport tensorflow as tf\nimport numpy as np\n\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nX = tf.placeholder(tf.float32, [None, 2])\nY = tf.placeholder(tf.float32, [None, 1])\n\nW1 = tf.Variable(tf.random_normal([2, 2]), name=\'weight1\')\nb1 = tf.Variable(tf.random_normal([2]), name=\'bias1\')\nlayer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n\nW2 = tf.Variable(tf.random_normal([2, 1]), name=\'weight2\')\nb2 = tf.Variable(tf.random_normal([1]), name=\'bias2\')\nhypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n\n# cost/loss function\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(10001):\n        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n        if step % 100 == 0:\n            print(step, cost_val)\n\n    # Accuracy report\n    h, p, a = sess.run(\n        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n    )\n    \n    print(f""\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}"")\n\n\n\'\'\'\nHypothesis:\n[[0.01338216]\n [0.98166394]\n [0.98809403]\n [0.01135799]] \nPredicted:\n[[0.]\n [1.]\n [1.]\n [0.]] \nAccuracy:\n1.0\n\'\'\'\n'"
lab-09-3-xor-nn-wide-deep.py,21,"b'# Lab 9 XOR\nimport tensorflow as tf\nimport numpy as np\n\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nX = tf.placeholder(tf.float32, [None, 2])\nY = tf.placeholder(tf.float32, [None, 1])\n\nW1 = tf.Variable(tf.random_normal([2, 10]), name=\'weight1\')\nb1 = tf.Variable(tf.random_normal([10]), name=\'bias1\')\nlayer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n\nW2 = tf.Variable(tf.random_normal([10, 10]), name=\'weight2\')\nb2 = tf.Variable(tf.random_normal([10]), name=\'bias2\')\nlayer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n\nW3 = tf.Variable(tf.random_normal([10, 10]), name=\'weight3\')\nb3 = tf.Variable(tf.random_normal([10]), name=\'bias3\')\nlayer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n\nW4 = tf.Variable(tf.random_normal([10, 1]), name=\'weight4\')\nb4 = tf.Variable(tf.random_normal([1]), name=\'bias4\')\nhypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n\n# cost/loss function\ncost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\ntrain = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(10001):\n        _, cost_val = sess.run([train, cost], feed_dict={X: x_data, Y: y_data})\n        if step % 100 == 0:\n            print(step, cost_val)\n\n    # Accuracy report\n    h, c, a = sess.run(\n        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n    )\n    print(""\\nHypothesis: "", h, ""\\nCorrect: "", c, ""\\nAccuracy: "", a)\n\n\n\'\'\'\nHypothesis:  [[  7.80511764e-04]\n [  9.99238133e-01]\n [  9.98379230e-01]\n [  1.55659032e-03]]\nCorrect:  [[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\nAccuracy:  1.0\n\'\'\'\n'"
lab-09-4-xor_tensorboard.py,29,"b'# Lab 9 XOR\nimport tensorflow as tf\nimport numpy as np\n\ntf.set_random_seed(777)  # for reproducibility\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nX = tf.placeholder(tf.float32, [None, 2], name=""x"")\nY = tf.placeholder(tf.float32, [None, 1], name=""y"")\n\nwith tf.name_scope(""Layer1""):\n    W1 = tf.Variable(tf.random_normal([2, 2]), name=""weight_1"")\n    b1 = tf.Variable(tf.random_normal([2]), name=""bias_1"")\n    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n\n    tf.summary.histogram(""W1"", W1)\n    tf.summary.histogram(""b1"", b1)\n    tf.summary.histogram(""Layer1"", layer1)\n\n\nwith tf.name_scope(""Layer2""):\n    W2 = tf.Variable(tf.random_normal([2, 1]), name=""weight_2"")\n    b2 = tf.Variable(tf.random_normal([1]), name=""bias_2"")\n    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n\n    tf.summary.histogram(""W2"", W2)\n    tf.summary.histogram(""b2"", b2)\n    tf.summary.histogram(""Hypothesis"", hypothesis)\n\n# cost/loss function\nwith tf.name_scope(""Cost""):\n    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n    tf.summary.scalar(""Cost"", cost)\n\nwith tf.name_scope(""Train""):\n    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\ntf.summary.scalar(""accuracy"", accuracy)\n\n# Launch graph\nwith tf.Session() as sess:\n    # tensorboard --logdir=./logs/xor_logs\n    merged_summary = tf.summary.merge_all()\n    writer = tf.summary.FileWriter(""./logs/xor_logs_r0_01"")\n    writer.add_graph(sess.graph)  # Show the graph\n\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(10001):\n        _, summary, cost_val = sess.run(\n            [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}\n        )\n        writer.add_summary(summary, global_step=step)\n\n        if step % 100 == 0:\n            print(step, cost_val)\n\n    # Accuracy report\n    h, p, a = sess.run(\n        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n    )\n    \n    print(f""\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}"")\n\n""""""\nHypothesis:\n[[6.1310326e-05]\n [9.9993694e-01]\n [9.9995077e-01]\n [5.9751470e-05]] \nPredicted:\n[[0.]\n [1.]\n [1.]\n [0.]] \nAccuracy:\n1.0\n""""""\n'"
lab-09-5-linear_back_prop.py,12,"b'# http://blog.aloni.org/posts/backprop-with-tensorflow/\n# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.b3rvzhx89\n# WIP\nimport tensorflow as tf\n\ntf.set_random_seed(777)  # reproducibility\n\n# tf Graph Input\nx_data = [[1.],\n          [2.],\n          [3.]]\ny_data = [[1.],\n          [2.],\n          [3.]]\n\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 1])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\n# Set wrong model weights\nW = tf.Variable(tf.truncated_normal([1, 1]))\nb = tf.Variable(5.)\n\n# Forward prop\nhypothesis = tf.matmul(X, W) + b\n\n# diff\nassert hypothesis.shape.as_list() == Y.shape.as_list()\ndiff = (hypothesis - Y)\n\n# Back prop (chain rule)\nd_l1 = diff\nd_b = d_l1\nd_w = tf.matmul(tf.transpose(X), d_l1)\n\nprint(X, W, d_l1, d_w)\n\n# Updating network using gradients\nlearning_rate = 0.1\nstep = [\n    tf.assign(W, W - learning_rate * d_w),\n    tf.assign(b, b - learning_rate * tf.reduce_mean(d_b)),\n]\n\n# 7. Running and testing the training process\nRMSE = tf.reduce_mean(tf.square((Y - hypothesis)))\n\nsess = tf.InteractiveSession()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor i in range(1000):\n    print(i, sess.run([step, RMSE], feed_dict={X: x_data, Y: y_data}))\n\nprint(sess.run(hypothesis, feed_dict={X: x_data}))\n'"
lab-09-6-multi-linear_back_prop.py,12,"b'# http://blog.aloni.org/posts/backprop-with-tensorflow/\n# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.b3rvzhx89\n# WIP\nimport tensorflow as tf\n\ntf.set_random_seed(777)  # reproducibility\n\n# tf Graph Input\nx_data = [[73., 80., 75.],\n          [93., 88., 93.],\n          [89., 91., 90.],\n          [96., 98., 100.],\n          [73., 66., 70.]]\ny_data = [[152.],\n          [185.],\n          [180.],\n          [196.],\n          [142.]]\n\n# placeholders for a tensor that will be always fed.\nX = tf.placeholder(tf.float32, shape=[None, 3])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\n# Set wrong model weights\nW = tf.Variable(tf.truncated_normal([3, 1]))\nb = tf.Variable(5.)\n\n# Forward prop\nhypothesis = tf.matmul(X, W) + b\n\nprint(hypothesis.shape, Y.shape)\n\n# diff\nassert hypothesis.shape.as_list() == Y.shape.as_list()\ndiff = (hypothesis - Y)\n\n# Back prop (chain rule)\nd_l1 = diff\nd_b = d_l1\nd_w = tf.matmul(tf.transpose(X), d_l1)\n\nprint(X, d_l1, d_w)\n\n# Updating network using gradients\nlearning_rate = 1e-6\nstep = [\n    tf.assign(W, W - learning_rate * d_w),\n    tf.assign(b, b - learning_rate * tf.reduce_mean(d_b)),\n]\n\n# 7. Running and testing the training process\nRMSE = tf.reduce_mean(tf.square((Y - hypothesis)))\n\nsess = tf.InteractiveSession()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor i in range(10000):\n    print(i, sess.run([step, RMSE], feed_dict={X: x_data, Y: y_data}))\n\nprint(sess.run(hypothesis, feed_dict={X: x_data}))\n'"
lab-09-7-sigmoid_back_prop.py,23,"b'""""""\nIn this file, we will implement back propagations by hands\n\nWe will use the Sigmoid Cross Entropy loss function.\nThis is equivalent to tf.nn.sigmoid_softmax_with_logits(logits, labels)\n\n[References]\n\n1) Tensorflow Document (tf.nn.sigmoid_softmax_with_logits)\n    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n\n2) Neural Net Backprop in one slide! by Sung Kim\n    https://docs.google.com/presentation/d/1_ZmtfEjLmhbuM_PqbDYMXXLAqeWN0HwuhcSKnUQZ6MM/edit#slide=id.g1ec1d04b5a_1_83\n\n3) Back Propagation with Tensorflow by Dan Aloni\n    http://blog.aloni.org/posts/backprop-with-tensorflow/\n\n4) Yes you should understand backprop by Andrej Karpathy\n    https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.cockptkn7\n\n\n[Network Architecture]\n\nInput: x\nLayer1: x * W + b\nOutput layer = \xcf\x83(Layer1)\n\nLoss_i = - y * log(\xcf\x83(Layer1)) - (1 - y) * log(1 - \xcf\x83(Layer1))\nLoss = tf.reduce_sum(Loss_i)\n\nWe want to compute that\n\ndLoss/dW = ???\ndLoss/db = ???\n\nplease read ""Neural Net Backprop in one slide!"" for deriving formulas\n\n""""""\nimport tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # for reproducibility\n\n# Predicting animal type based on various features\nxy = np.loadtxt(\'data-04-zoo.csv\', delimiter=\',\', dtype=np.float32)\nX_data = xy[:, 0:-1]\nN = X_data.shape[0]\ny_data = xy[:, [-1]]\n\n# y_data has labels from 0 ~ 6\nprint(""y has one of the following values"")\nprint(np.unique(y_data))\n\n# X_data.shape = (101, 16) => 101 samples, 16 features\n# y_data.shape = (101, 1)  => 101 samples, 1 label\nprint(""Shape of X data: "", X_data.shape)\nprint(""Shape of y data: "", y_data.shape)\n\nnb_classes = 7  # 0 ~ 6\n\nX = tf.placeholder(tf.float32, [None, 16])\ny = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n\ntarget = tf.one_hot(y, nb_classes)  # one hot\ntarget = tf.reshape(target, [-1, nb_classes])\ntarget = tf.cast(target, tf.float32)\n\nW = tf.Variable(tf.random_normal([16, nb_classes]), name=\'weight\')\nb = tf.Variable(tf.random_normal([nb_classes]), name=\'bias\')\n\n\ndef sigma(x):\n    # sigmoid function\n    # \xcf\x83(x) = 1 / (1 + exp(-x))\n    return 1. / (1. + tf.exp(-x))\n\n\ndef sigma_prime(x):\n    # derivative of the sigmoid function\n    # \xcf\x83\'(x) = \xcf\x83(x) * (1 - \xcf\x83(x))\n    return sigma(x) * (1. - sigma(x))\n\n\n# Forward propagtion\nlayer_1 = tf.matmul(X, W) + b\ny_pred = sigma(layer_1)\n\n# Loss Function (end of forwad propagation)\nloss_i = - target * tf.log(y_pred) - (1. - target) * tf.log(1. - y_pred)\nloss = tf.reduce_sum(loss_i)\n\n# Dimension Check\nassert y_pred.shape.as_list() == target.shape.as_list()\n\n\n# Back prop (chain rule)\n# How to derive? please read ""Neural Net Backprop in one slide!""\nd_loss = (y_pred - target) / (y_pred * (1. - y_pred) + 1e-7)\nd_sigma = sigma_prime(layer_1)\nd_layer = d_loss * d_sigma\nd_b = d_layer\nd_W = tf.matmul(tf.transpose(X), d_layer)\n\n# Updating network using gradients\nlearning_rate = 0.01\ntrain_step = [\n    tf.assign(W, W - learning_rate * d_W),\n    tf.assign(b, b - learning_rate * tf.reduce_sum(d_b)),\n]\n\n# Prediction and Accuracy\nprediction = tf.argmax(y_pred, 1)\nacct_mat = tf.equal(tf.argmax(y_pred, 1), tf.argmax(target, 1))\nacct_res = tf.reduce_mean(tf.cast(acct_mat, tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for step in range(500):\n        sess.run(train_step, feed_dict={X: X_data, y: y_data})\n\n        if step % 10 == 0:\n            # Within 300 steps, you should see an accuracy of 100%\n            step_loss, acc = sess.run([loss, acct_res], feed_dict={\n                                      X: X_data, y: y_data})\n            print(""Step: {:5}\\t Loss: {:10.5f}\\t Acc: {:.2%}"" .format(\n                step, step_loss, acc))\n\n    # Let\'s see if we can predict\n    pred = sess.run(prediction, feed_dict={X: X_data})\n    for p, y in zip(pred, y_data):\n        msg = ""[{}]\\t Prediction: {:d}\\t True y: {:d}""\n        print(msg.format(p == int(y[0]), p, int(y[0])))\n\n""""""\nOutput Example\n\nStep:     0      Loss:  453.74799        Acc: 38.61%\nStep:    20      Loss:   95.05664        Acc: 88.12%\nStep:    40      Loss:   66.43570        Acc: 93.07%\nStep:    60      Loss:   53.09288        Acc: 94.06%\n...\nStep:   290      Loss:   18.72972        Acc: 100.00%\nStep:   300      Loss:   18.24953        Acc: 100.00%\nStep:   310      Loss:   17.79592        Acc: 100.00%\n...\n[True]   Prediction: 0   True y: 0\n[True]   Prediction: 0   True y: 0\n[True]   Prediction: 3   True y: 3\n[True]   Prediction: 0   True y: 0\n...\n""""""\n'"
lab-09-x-xor-nn-back_prop.py,27,"b'# Lab 9 XOR-back_prop\nimport tensorflow as tf\nimport numpy as np\n\ntf.set_random_seed(777)  # for reproducibility\nlearning_rate = 0.1\n\nx_data = [[0, 0],\n          [0, 1],\n          [1, 0],\n          [1, 1]]\ny_data = [[0],\n          [1],\n          [1],\n          [0]]\n\nx_data = np.array(x_data, dtype=np.float32)\ny_data = np.array(y_data, dtype=np.float32)\n\nX = tf.placeholder(tf.float32, [None, 2])\nY = tf.placeholder(tf.float32, [None, 1])\n\nW1 = tf.Variable(tf.random_normal([2, 2]), name=\'weight1\')\nb1 = tf.Variable(tf.random_normal([2]), name=\'bias1\')\nl1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n\nW2 = tf.Variable(tf.random_normal([2, 1]), name=\'weight2\')\nb2 = tf.Variable(tf.random_normal([1]), name=\'bias2\')\nY_pred = tf.sigmoid(tf.matmul(l1, W2) + b2)\n\n# cost/loss function\ncost = -tf.reduce_mean(Y * tf.log(Y_pred) + (1 - Y) *\n                       tf.log(1 - Y_pred))\n\n# Network\n#          p1     a1           l1     p2     a2           l2 (y_pred)\n# X -> (*) -> (+) -> (sigmoid) -> (*) -> (+) -> (sigmoid) -> (loss)\n#       ^      ^                   ^      ^\n#       |      |                   |      |\n#       W1     b1                  W2     b2\n\n# Loss derivative\nd_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n\n# Layer 2\nd_sigma2 = Y_pred * (1 - Y_pred)\nd_a2 = d_Y_pred * d_sigma2\nd_p2 = d_a2\nd_b2 = d_a2\nd_W2 = tf.matmul(tf.transpose(l1), d_p2)\n\n# Mean\nd_b2_mean = tf.reduce_mean(d_b2, axis=[0])\nd_W2_mean = d_W2 / tf.cast(tf.shape(l1)[0], dtype=tf.float32)\n\n# Layer 1\nd_l1 = tf.matmul(d_p2, tf.transpose(W2))\nd_sigma1 = l1 * (1 - l1)\nd_a1 = d_l1 * d_sigma1\nd_b1 = d_a1\nd_p1 = d_a1\nd_W1 = tf.matmul(tf.transpose(X), d_a1)\n\n# Mean\nd_W1_mean = d_W1 / tf.cast(tf.shape(X)[0], dtype=tf.float32)\nd_b1_mean = tf.reduce_mean(d_b1, axis=[0])\n\n# Weight update\nstep = [\n  tf.assign(W2, W2 - learning_rate * d_W2_mean),\n  tf.assign(b2, b2 - learning_rate * d_b2_mean),\n  tf.assign(W1, W1 - learning_rate * d_W1_mean),\n  tf.assign(b1, b1 - learning_rate * d_b1_mean)\n]\n\n# Accuracy computation\n# True if hypothesis > 0.5 else False\npredicted = tf.cast(Y_pred > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n\n# Launch graph\nwith tf.Session() as sess:\n    # Initialize TensorFlow variables\n    sess.run(tf.global_variables_initializer())\n\n    print(""shape"", sess.run(tf.shape(X)[0], feed_dict={X: x_data}))\n\n\n    for i in range(10001):\n        sess.run([step, cost], feed_dict={X: x_data, Y: y_data})\n        if i % 1000 == 0:\n            print(i, sess.run([cost, d_W1], feed_dict={\n                  X: x_data, Y: y_data}), sess.run([W1, W2]))\n\n    # Accuracy report\n    h, c, a = sess.run([Y_pred, predicted, accuracy],\n                       feed_dict={X: x_data, Y: y_data})\n    print(""\\nHypothesis: "", h, ""\\nCorrect: "", c, ""\\nAccuracy: "", a)\n\n\n\'\'\'\nHypothesis:  [[ 0.01338224]\n [ 0.98166382]\n [ 0.98809403]\n [ 0.01135806]]\nCorrect:  [[ 0.]\n [ 1.]\n [ 1.]\n [ 0.]]\nAccuracy:  1.0\n\'\'\'\n'"
lab-10-1-mnist_softmax.py,16,"b'# Lab 7 Learning rate and Evaluation\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport random\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# weights & bias for nn layers\nW = tf.Variable(tf.random_normal([784, 10]))\nb = tf.Variable(tf.random_normal([10]))\n\n# parameters\nlearning_rate = 0.001\nbatch_size = 100\nnum_epochs = 50\nnum_iterations = int(mnist.train.num_examples / batch_size)\n\nhypothesis = tf.matmul(X, W) + b\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=hypothesis, labels=tf.stop_gradient(Y)\n    )\n)\ntrain = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# train my model\nwith tf.Session() as sess:\n    # initialize\n    sess.run(tf.global_variables_initializer())\n\n    for epoch in range(num_epochs):\n        avg_cost = 0\n\n        for iteration in range(num_iterations):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n            avg_cost += cost_val / num_iterations\n\n        print(f""Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}"")\n\n    print(""Learning Finished!"")\n\n    # Test model and check accuracy\n    print(\n        ""Accuracy:"",\n        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n    )\n\n    # Get one and predict\n    r = random.randint(0, mnist.test.num_examples - 1)\n\n    print(""Label: "", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n    print(\n        ""Prediction: "",\n        sess.run(\n            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n        ),\n    )\n\n    plt.imshow(\n        mnist.test.images[r : r + 1].reshape(28, 28),\n        cmap=""Greys"",\n        interpolation=""nearest"",\n    )\n    plt.show()\n\n\'\'\'\nEpoch: 0001 Cost: 5.745170949\nEpoch: 0002 Cost: 1.780056722\nEpoch: 0003 Cost: 1.122778654\n...\nEpoch: 0048 Cost: 0.271918680\nEpoch: 0049 Cost: 0.270640434\nEpoch: 0050 Cost: 0.269054370\nLearning Finished!\nAccuracy: 0.9194\n\'\'\'\n'"
lab-10-2-mnist_nn.py,20,"b'# Lab 10 MNIST and NN\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# weights & bias for nn layers\nW1 = tf.Variable(tf.random_normal([784, 256]))\nb1 = tf.Variable(tf.random_normal([256]))\nL1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n\nW2 = tf.Variable(tf.random_normal([256, 256]))\nb2 = tf.Variable(tf.random_normal([256]))\nL2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n\nW3 = tf.Variable(tf.random_normal([256, 10]))\nb3 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L2, W3) + b3\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys}\n        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nEpoch: 0001 cost = 141.207671860\nEpoch: 0002 cost = 38.788445864\nEpoch: 0003 cost = 23.977515479\nEpoch: 0004 cost = 16.315132428\nEpoch: 0005 cost = 11.702554882\nEpoch: 0006 cost = 8.573139748\nEpoch: 0007 cost = 6.370995680\nEpoch: 0008 cost = 4.537178684\nEpoch: 0009 cost = 3.216900532\nEpoch: 0010 cost = 2.329708954\nEpoch: 0011 cost = 1.715552875\nEpoch: 0012 cost = 1.189857912\nEpoch: 0013 cost = 0.820965160\nEpoch: 0014 cost = 0.624131458\nEpoch: 0015 cost = 0.454633765\nLearning Finished!\nAccuracy: 0.9455\n\'\'\'\n'"
lab-10-3-mnist_nn_xavier.py,23,"b'# Lab 10 MNIST and Xavier\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# weights & bias for nn layers\n# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\nW1 = tf.get_variable(""W1"", shape=[784, 256],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.Variable(tf.random_normal([256]))\nL1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n\nW2 = tf.get_variable(""W2"", shape=[256, 256],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb2 = tf.Variable(tf.random_normal([256]))\nL2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n\nW3 = tf.get_variable(""W3"", shape=[256, 10],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb3 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L2, W3) + b3\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys}\n        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nEpoch: 0001 cost = 0.301498963\nEpoch: 0002 cost = 0.107252513\nEpoch: 0003 cost = 0.064888892\nEpoch: 0004 cost = 0.044463030\nEpoch: 0005 cost = 0.029951642\nEpoch: 0006 cost = 0.020663404\nEpoch: 0007 cost = 0.015853033\nEpoch: 0008 cost = 0.011764387\nEpoch: 0009 cost = 0.008598264\nEpoch: 0010 cost = 0.007383116\nEpoch: 0011 cost = 0.006839140\nEpoch: 0012 cost = 0.004672963\nEpoch: 0013 cost = 0.003979437\nEpoch: 0014 cost = 0.002714260\nEpoch: 0015 cost = 0.004707661\nLearning Finished!\nAccuracy: 0.9783\n\'\'\'\n'"
lab-10-4-mnist_nn_deep.py,31,"b'# Lab 10 MNIST and Deep learning\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# weights & bias for nn layers\n# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\nW1 = tf.get_variable(""W1"", shape=[784, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.Variable(tf.random_normal([512]))\nL1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n\nW2 = tf.get_variable(""W2"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb2 = tf.Variable(tf.random_normal([512]))\nL2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n\nW3 = tf.get_variable(""W3"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb3 = tf.Variable(tf.random_normal([512]))\nL3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n\nW4 = tf.get_variable(""W4"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb4 = tf.Variable(tf.random_normal([512]))\nL4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n\nW5 = tf.get_variable(""W5"", shape=[512, 10],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb5 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L4, W5) + b5\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys}\n        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nEpoch: 0001 cost = 0.266061549\nEpoch: 0002 cost = 0.080796588\nEpoch: 0003 cost = 0.049075800\nEpoch: 0004 cost = 0.034772298\nEpoch: 0005 cost = 0.024780529\nEpoch: 0006 cost = 0.017072763\nEpoch: 0007 cost = 0.014031383\nEpoch: 0008 cost = 0.013763446\nEpoch: 0009 cost = 0.009164047\nEpoch: 0010 cost = 0.008291388\nEpoch: 0011 cost = 0.007319742\nEpoch: 0012 cost = 0.006434021\nEpoch: 0013 cost = 0.005684378\nEpoch: 0014 cost = 0.004781207\nEpoch: 0015 cost = 0.004342310\nLearning Finished!\nAccuracy: 0.9742\n\'\'\'\n'"
lab-10-5-mnist_nn_dropout.py,36,"b'# Lab 10 MNIST and Dropout\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\ntotal_batch = int(mnist.train.num_examples / batch_size)\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n# weights & bias for nn layers\n# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\nW1 = tf.get_variable(""W1"", shape=[784, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.Variable(tf.random_normal([512]))\nL1 = tf.nn.relu(tf.matmul(X, W1) + b1)\nL1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n\nW2 = tf.get_variable(""W2"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb2 = tf.Variable(tf.random_normal([512]))\nL2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\nL2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n\nW3 = tf.get_variable(""W3"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb3 = tf.Variable(tf.random_normal([512]))\nL3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\nL3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n\nW4 = tf.get_variable(""W4"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb4 = tf.Variable(tf.random_normal([512]))\nL4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\nL4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n\nW5 = tf.get_variable(""W5"", shape=[512, 10],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb5 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L4, W5) + b5\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nEpoch: 0001 cost = 0.447322626\nEpoch: 0002 cost = 0.157285590\nEpoch: 0003 cost = 0.121884535\nEpoch: 0004 cost = 0.098128681\nEpoch: 0005 cost = 0.082901778\nEpoch: 0006 cost = 0.075337573\nEpoch: 0007 cost = 0.069752543\nEpoch: 0008 cost = 0.060884363\nEpoch: 0009 cost = 0.055276413\nEpoch: 0010 cost = 0.054631256\nEpoch: 0011 cost = 0.049675195\nEpoch: 0012 cost = 0.049125314\nEpoch: 0013 cost = 0.047231930\nEpoch: 0014 cost = 0.041290121\nEpoch: 0015 cost = 0.043621063\nLearning Finished!\nAccuracy: 0.9804\n\'\'\'\n'"
lab-10-7-mnist_nn_higher_level_API.py,14,"b'# Lab 10 MNIST and High-level TF API\nfrom tensorflow.contrib.layers import fully_connected, batch_norm, dropout\nfrom tensorflow.contrib.framework import arg_scope\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.01  # we can use large learning rate using Batch Normalization\ntraining_epochs = 15\nbatch_size = 100\nkeep_prob = 0.7\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\ntrain_mode = tf.placeholder(tf.bool, name=\'train_mode\')\n\n# layer output size\nhidden_output_size = 512\nfinal_output_size = 10\n\nxavier_init = tf.contrib.layers.xavier_initializer()\nbn_params = {\n    \'is_training\': train_mode,\n    \'decay\': 0.9,\n    \'updates_collections\': None\n}\n\n# We can build short code using \'arg_scope\' to avoid duplicate code\n# same function with different arguments\nwith arg_scope([fully_connected],\n               activation_fn=tf.nn.relu,\n               weights_initializer=xavier_init,\n               biases_initializer=None,\n               normalizer_fn=batch_norm,\n               normalizer_params=bn_params\n               ):\n    hidden_layer1 = fully_connected(X, hidden_output_size, scope=""h1"")\n    h1_drop = dropout(hidden_layer1, keep_prob, is_training=train_mode)\n    hidden_layer2 = fully_connected(h1_drop, hidden_output_size, scope=""h2"")\n    h2_drop = dropout(hidden_layer2, keep_prob, is_training=train_mode)\n    hidden_layer3 = fully_connected(h2_drop, hidden_output_size, scope=""h3"")\n    h3_drop = dropout(hidden_layer3, keep_prob, is_training=train_mode)\n    hidden_layer4 = fully_connected(h3_drop, hidden_output_size, scope=""h4"")\n    h4_drop = dropout(hidden_layer4, keep_prob, is_training=train_mode)\n    hypothesis = fully_connected(h4_drop, final_output_size, activation_fn=None, scope=""hypothesis"")\n\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict_train = {X: batch_xs, Y: batch_ys, train_mode: True}\n        feed_dict_cost = {X: batch_xs, Y: batch_ys, train_mode: False}\n        opt = sess.run(optimizer, feed_dict=feed_dict_train)\n        c = sess.run(cost, feed_dict=feed_dict_cost)\n        avg_cost += c / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost))\n    #print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels, train_mode: False}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], train_mode: False}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\n[Epoch:    1] cost = 0.519417209\n[Epoch:    2] cost = 0.432551052\n[Epoch:    3] cost = 0.404978843\n[Epoch:    4] cost = 0.392039919\n[Epoch:    5] cost = 0.382165317\n[Epoch:    6] cost = 0.377987834\n[Epoch:    7] cost = 0.372577601\n[Epoch:    8] cost = 0.367208552\n[Epoch:    9] cost = 0.365525589\n[Epoch:   10] cost = 0.361964276\n[Epoch:   11] cost = 0.359540287\n[Epoch:   12] cost = 0.356423751\n[Epoch:   13] cost = 0.354478216\n[Epoch:   14] cost = 0.353212552\n[Epoch:   15] cost = 0.35230893\nLearning Finished!\nAccuracy: 0.9826\n\'\'\'\n'"
lab-10-8-mnist_nn_selu(wip).py,34,"b'# Lab 10 MNIST and Dropout\n# SELU implementation from https://github.com/bioinf-jku/SNNs/blob/master/selu.py\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n# -*- coding: utf-8 -*-\n\'\'\'\nTensorflow Implementation of the Scaled ELU function and Dropout\n\'\'\'\n\nimport numbers\nfrom tensorflow.contrib import layers\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.layers import utils\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\ndef selu(x):\n    with ops.name_scope(\'elu\') as scope:\n        alpha = 1.6732632423543772848170429916717\n        scale = 1.0507009873554804934193349852946\n        return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n\n\ndef dropout_selu(x, keep_prob, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0,\n                 noise_shape=None, seed=None, name=None, training=False):\n    """"""Dropout to a value with rescaling.""""""\n\n    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n        keep_prob = 1.0 - rate\n        x = ops.convert_to_tensor(x, name=""x"")\n        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n            raise ValueError(""keep_prob must be a scalar tensor or a float in the ""\n                                             ""range (0, 1], got %g"" % keep_prob)\n        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=""keep_prob"")\n        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n\n        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=""alpha"")\n        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n\n        if tensor_util.constant_value(keep_prob) == 1:\n            return x\n\n        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n        random_tensor = keep_prob\n        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n        binary_tensor = math_ops.floor(random_tensor)\n        ret = x * binary_tensor + alpha * (1-binary_tensor)\n\n        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n\n        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n        ret = a * ret + b\n        ret.set_shape(x.get_shape())\n        return ret\n\n    with ops.name_scope(name, ""dropout"", [x]) as name:\n        return utils.smart_cond(training,\n                                lambda: dropout_selu_impl(x, keep_prob, alpha, noise_shape, seed, name),\n                                lambda: array_ops.identity(x))\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 50\nbatch_size = 100\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n# weights & bias for nn layers\n# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\nW1 = tf.get_variable(""W1"", shape=[784, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.Variable(tf.random_normal([512]))\nL1 = selu(tf.matmul(X, W1) + b1)\nL1 = dropout_selu(L1, keep_prob=keep_prob)\n\nW2 = tf.get_variable(""W2"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb2 = tf.Variable(tf.random_normal([512]))\nL2 = selu(tf.matmul(L1, W2) + b2)\nL2 = dropout_selu(L2, keep_prob=keep_prob)\n\nW3 = tf.get_variable(""W3"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb3 = tf.Variable(tf.random_normal([512]))\nL3 = selu(tf.matmul(L2, W3) + b3)\nL3 = dropout_selu(L3, keep_prob=keep_prob)\n\nW4 = tf.get_variable(""W4"", shape=[512, 512],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb4 = tf.Variable(tf.random_normal([512]))\nL4 = selu(tf.matmul(L3, W4) + b4)\nL4 = dropout_selu(L4, keep_prob=keep_prob)\n\nW5 = tf.get_variable(""W5"", shape=[512, 10],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb5 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L4, W5) + b5\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nEpoch: 0001 cost = 0.447322626\nEpoch: 0002 cost = 0.157285590\nEpoch: 0003 cost = 0.121884535\nEpoch: 0004 cost = 0.098128681\nEpoch: 0005 cost = 0.082901778\nEpoch: 0006 cost = 0.075337573\nEpoch: 0007 cost = 0.069752543\nEpoch: 0008 cost = 0.060884363\nEpoch: 0009 cost = 0.055276413\nEpoch: 0010 cost = 0.054631256\nEpoch: 0011 cost = 0.049675195\nEpoch: 0012 cost = 0.049125314\nEpoch: 0013 cost = 0.047231930\nEpoch: 0014 cost = 0.041290121\nEpoch: 0015 cost = 0.043621063\nLearning Finished!\nAccuracy: 0.9804\n\'\'\'\n'"
lab-10-X1-mnist_back_prop.py,25,"b'# http://blog.aloni.org/posts/backprop-with-tensorflow/\n# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.b3rvzhx89\nimport tensorflow as tf\n\ntf.set_random_seed(777)  # reproducibility\n\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\nw1 = tf.Variable(tf.truncated_normal([784, 30]))\nb1 = tf.Variable(tf.truncated_normal([1, 30]))\nw2 = tf.Variable(tf.truncated_normal([30, 10]))\nb2 = tf.Variable(tf.truncated_normal([1, 10]))\n\n\ndef sigma(x):\n    #  sigmoid function\n    return tf.div(tf.constant(1.0),\n                  tf.add(tf.constant(1.0), tf.exp(-x)))\n\n\ndef sigma_prime(x):\n    # derivative of the sigmoid function\n    return sigma(x) * (1 - sigma(x))\n\n# Forward prop\nl1 = tf.add(tf.matmul(X, w1), b1)\na1 = sigma(l1)\nl2 = tf.add(tf.matmul(a1, w2), b2)\ny_pred = sigma(l2)\n\n# diff\nassert y_pred.shape.as_list() == Y.shape.as_list()\ndiff = (y_pred - Y)\n\n\n# Back prop (chain rule)\nd_l2 = diff * sigma_prime(l2)\nd_b2 = d_l2\nd_w2 = tf.matmul(tf.transpose(a1), d_l2)\n\nd_a1 = tf.matmul(d_l2, tf.transpose(w2))\nd_l1 = d_a1 * sigma_prime(l1)\nd_b1 = d_l1\nd_w1 = tf.matmul(tf.transpose(X), d_l1)\n\n\n# Updating network using gradients\nlearning_rate = 0.5\nstep = [\n    tf.assign(w1, w1 - learning_rate * d_w1),\n    tf.assign(b1, b1 - learning_rate *\n              tf.reduce_mean(d_b1, reduction_indices=[0])),\n    tf.assign(w2, w2 - learning_rate * d_w2),\n    tf.assign(b2, b2 - learning_rate *\n              tf.reduce_mean(d_b2, reduction_indices=[0]))\n]\n\n# 7. Running and testing the training process\nacct_mat = tf.equal(tf.argmax(y_pred, 1), tf.argmax(Y, 1))\nacct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\nfor i in range(10000):\n    batch_xs, batch_ys = mnist.train.next_batch(10)\n    sess.run(step, feed_dict={X: batch_xs,\n                              Y: batch_ys})\n    if i % 1000 == 0:\n        res = sess.run(acct_res, feed_dict={X: mnist.test.images[:1000],\n                                            Y: mnist.test.labels[:1000]})\n        print(res)\n\n# 8. Automatic differentiation in TensorFlow\ncost = diff * diff\nstep = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n'"
lab-11-1-mnist_cnn.py,25,"b'# Lab 11 MNIST and Convolutional Neural Network\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nX_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\nY = tf.placeholder(tf.float32, [None, 10])\n\n# L1 ImgIn shape=(?, 28, 28, 1)\nW1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n#    Conv     -> (?, 28, 28, 32)\n#    Pool     -> (?, 14, 14, 32)\nL1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding=\'SAME\')\nL1 = tf.nn.relu(L1)\nL1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1], padding=\'SAME\')\n\'\'\'\nTensor(""Conv2D:0"", shape=(?, 28, 28, 32), dtype=float32)\nTensor(""Relu:0"", shape=(?, 28, 28, 32), dtype=float32)\nTensor(""MaxPool:0"", shape=(?, 14, 14, 32), dtype=float32)\n\'\'\'\n\n# L2 ImgIn shape=(?, 14, 14, 32)\nW2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n#    Conv      ->(?, 14, 14, 64)\n#    Pool      ->(?, 7, 7, 64)\nL2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\'SAME\')\nL2 = tf.nn.relu(L2)\nL2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1], padding=\'SAME\')\nL2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n\'\'\'\nTensor(""Conv2D_1:0"", shape=(?, 14, 14, 64), dtype=float32)\nTensor(""Relu_1:0"", shape=(?, 14, 14, 64), dtype=float32)\nTensor(""MaxPool_1:0"", shape=(?, 7, 7, 64), dtype=float32)\nTensor(""Reshape_1:0"", shape=(?, 3136), dtype=float32)\n\'\'\'\n\n# Final FC 7x7x64 inputs -> 10 outputs\nW3 = tf.get_variable(""W3"", shape=[7 * 7 * 64, 10],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb = tf.Variable(tf.random_normal([10]))\nlogits = tf.matmul(L2_flat, W3) + b\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=logits, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nprint(\'Learning started. It takes sometime.\')\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys}\n        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nEpoch: 0001 cost = 0.340291267\nEpoch: 0002 cost = 0.090731326\nEpoch: 0003 cost = 0.064477619\nEpoch: 0004 cost = 0.050683064\nEpoch: 0005 cost = 0.041864835\nEpoch: 0006 cost = 0.035760704\nEpoch: 0007 cost = 0.030572132\nEpoch: 0008 cost = 0.026207981\nEpoch: 0009 cost = 0.022622454\nEpoch: 0010 cost = 0.019055919\nEpoch: 0011 cost = 0.017758641\nEpoch: 0012 cost = 0.014156652\nEpoch: 0013 cost = 0.012397016\nEpoch: 0014 cost = 0.010693789\nEpoch: 0015 cost = 0.009469977\nLearning Finished!\nAccuracy: 0.9885\n\'\'\'\n'"
lab-11-2-mnist_deep_cnn.py,38,"b'# Lab 11 MNIST and Deep learning CNN\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nX_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\nY = tf.placeholder(tf.float32, [None, 10])\n\n# L1 ImgIn shape=(?, 28, 28, 1)\nW1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n#    Conv     -> (?, 28, 28, 32)\n#    Pool     -> (?, 14, 14, 32)\nL1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding=\'SAME\')\nL1 = tf.nn.relu(L1)\nL1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1], padding=\'SAME\')\nL1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n\'\'\'\nTensor(""Conv2D:0"", shape=(?, 28, 28, 32), dtype=float32)\nTensor(""Relu:0"", shape=(?, 28, 28, 32), dtype=float32)\nTensor(""MaxPool:0"", shape=(?, 14, 14, 32), dtype=float32)\nTensor(""dropout/mul:0"", shape=(?, 14, 14, 32), dtype=float32)\n\'\'\'\n\n# L2 ImgIn shape=(?, 14, 14, 32)\nW2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n#    Conv      ->(?, 14, 14, 64)\n#    Pool      ->(?, 7, 7, 64)\nL2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\'SAME\')\nL2 = tf.nn.relu(L2)\nL2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1], padding=\'SAME\')\nL2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n\'\'\'\nTensor(""Conv2D_1:0"", shape=(?, 14, 14, 64), dtype=float32)\nTensor(""Relu_1:0"", shape=(?, 14, 14, 64), dtype=float32)\nTensor(""MaxPool_1:0"", shape=(?, 7, 7, 64), dtype=float32)\nTensor(""dropout_1/mul:0"", shape=(?, 7, 7, 64), dtype=float32)\n\'\'\'\n\n# L3 ImgIn shape=(?, 7, 7, 64)\nW3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n#    Conv      ->(?, 7, 7, 128)\n#    Pool      ->(?, 4, 4, 128)\n#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\nL3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding=\'SAME\')\nL3 = tf.nn.relu(L3)\nL3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n                    1, 2, 2, 1], padding=\'SAME\')\nL3 = tf.nn.dropout(L3, keep_prob=keep_prob)\nL3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n\'\'\'\nTensor(""Conv2D_2:0"", shape=(?, 7, 7, 128), dtype=float32)\nTensor(""Relu_2:0"", shape=(?, 7, 7, 128), dtype=float32)\nTensor(""MaxPool_2:0"", shape=(?, 4, 4, 128), dtype=float32)\nTensor(""dropout_2/mul:0"", shape=(?, 4, 4, 128), dtype=float32)\nTensor(""Reshape_1:0"", shape=(?, 2048), dtype=float32)\n\'\'\'\n\n# L4 FC 4x4x128 inputs -> 625 outputs\nW4 = tf.get_variable(""W4"", shape=[128 * 4 * 4, 625],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb4 = tf.Variable(tf.random_normal([625]))\nL4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\nL4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n\'\'\'\nTensor(""Relu_3:0"", shape=(?, 625), dtype=float32)\nTensor(""dropout_3/mul:0"", shape=(?, 625), dtype=float32)\n\'\'\'\n\n# L5 Final FC 625 inputs -> 10 outputs\nW5 = tf.get_variable(""W5"", shape=[625, 10],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb5 = tf.Variable(tf.random_normal([10]))\nlogits = tf.matmul(L4, W5) + b5\n\'\'\'\nTensor(""add_1:0"", shape=(?, 10), dtype=float32)\n\'\'\'\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=logits, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nprint(\'Learning started. It takes sometime.\')\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\n\n# if you have a OOM error, please refer to lab-11-X-mnist_deep_cnn_low_memory.py\n\ncorrect_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nLearning stared. It takes sometime.\nEpoch: 0001 cost = 0.385748474\nEpoch: 0002 cost = 0.092017397\nEpoch: 0003 cost = 0.065854684\nEpoch: 0004 cost = 0.055604566\nEpoch: 0005 cost = 0.045996377\nEpoch: 0006 cost = 0.040913645\nEpoch: 0007 cost = 0.036924479\nEpoch: 0008 cost = 0.032808939\nEpoch: 0009 cost = 0.031791007\nEpoch: 0010 cost = 0.030224456\nEpoch: 0011 cost = 0.026849916\nEpoch: 0012 cost = 0.026826763\nEpoch: 0013 cost = 0.027188021\nEpoch: 0014 cost = 0.023604777\nEpoch: 0015 cost = 0.024607201\nLearning Finished!\nAccuracy: 0.9938\n\'\'\''"
lab-11-3-mnist_cnn_class.py,38,"b'# Lab 11 MNIST and Deep learning CNN\nimport tensorflow as tf\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n\nclass Model:\n\n    def __init__(self, sess, name):\n        self.sess = sess\n        self.name = name\n        self._build_net()\n\n    def _build_net(self):\n        with tf.variable_scope(self.name):\n            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n            # for testing\n            self.keep_prob = tf.placeholder(tf.float32)\n\n            # input place holders\n            self.X = tf.placeholder(tf.float32, [None, 784])\n            # img 28x28x1 (black/white)\n            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n            self.Y = tf.placeholder(tf.float32, [None, 10])\n\n            # L1 ImgIn shape=(?, 28, 28, 1)\n            W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n            #    Conv     -> (?, 28, 28, 32)\n            #    Pool     -> (?, 14, 14, 32)\n            L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding=\'SAME\')\n            L1 = tf.nn.relu(L1)\n            L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n                                strides=[1, 2, 2, 1], padding=\'SAME\')\n            L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n            \'\'\'\n            Tensor(""Conv2D:0"", shape=(?, 28, 28, 32), dtype=float32)\n            Tensor(""Relu:0"", shape=(?, 28, 28, 32), dtype=float32)\n            Tensor(""MaxPool:0"", shape=(?, 14, 14, 32), dtype=float32)\n            Tensor(""dropout/mul:0"", shape=(?, 14, 14, 32), dtype=float32)\n            \'\'\'\n\n            # L2 ImgIn shape=(?, 14, 14, 32)\n            W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n            #    Conv      ->(?, 14, 14, 64)\n            #    Pool      ->(?, 7, 7, 64)\n            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\'SAME\')\n            L2 = tf.nn.relu(L2)\n            L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n                                strides=[1, 2, 2, 1], padding=\'SAME\')\n            L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n            \'\'\'\n            Tensor(""Conv2D_1:0"", shape=(?, 14, 14, 64), dtype=float32)\n            Tensor(""Relu_1:0"", shape=(?, 14, 14, 64), dtype=float32)\n            Tensor(""MaxPool_1:0"", shape=(?, 7, 7, 64), dtype=float32)\n            Tensor(""dropout_1/mul:0"", shape=(?, 7, 7, 64), dtype=float32)\n            \'\'\'\n\n            # L3 ImgIn shape=(?, 7, 7, 64)\n            W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n            #    Conv      ->(?, 7, 7, 128)\n            #    Pool      ->(?, 4, 4, 128)\n            #    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n            L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding=\'SAME\')\n            L3 = tf.nn.relu(L3)\n            L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n                                1, 2, 2, 1], padding=\'SAME\')\n            L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)\n\n            L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n            \'\'\'\n            Tensor(""Conv2D_2:0"", shape=(?, 7, 7, 128), dtype=float32)\n            Tensor(""Relu_2:0"", shape=(?, 7, 7, 128), dtype=float32)\n            Tensor(""MaxPool_2:0"", shape=(?, 4, 4, 128), dtype=float32)\n            Tensor(""dropout_2/mul:0"", shape=(?, 4, 4, 128), dtype=float32)\n            Tensor(""Reshape_1:0"", shape=(?, 2048), dtype=float32)\n            \'\'\'\n\n            # L4 FC 4x4x128 inputs -> 625 outputs\n            W4 = tf.get_variable(""W4"", shape=[128 * 4 * 4, 625],\n                                 initializer=tf.contrib.layers.xavier_initializer())\n            b4 = tf.Variable(tf.random_normal([625]))\n            L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n            L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n            \'\'\'\n            Tensor(""Relu_3:0"", shape=(?, 625), dtype=float32)\n            Tensor(""dropout_3/mul:0"", shape=(?, 625), dtype=float32)\n            \'\'\'\n\n            # L5 Final FC 625 inputs -> 10 outputs\n            W5 = tf.get_variable(""W5"", shape=[625, 10],\n                                 initializer=tf.contrib.layers.xavier_initializer())\n            b5 = tf.Variable(tf.random_normal([10]))\n            self.logits = tf.matmul(L4, W5) + b5\n            \'\'\'\n            Tensor(""add_1:0"", shape=(?, 10), dtype=float32)\n            \'\'\'\n\n        # define cost/loss & optimizer\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n            logits=self.logits, labels=self.Y))\n        self.optimizer = tf.train.AdamOptimizer(\n            learning_rate=learning_rate).minimize(self.cost)\n\n        correct_prediction = tf.equal(\n            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    def predict(self, x_test, keep_prop=1.0):\n        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.keep_prob: keep_prop})\n\n    def get_accuracy(self, x_test, y_test, keep_prop=1.0):\n        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.keep_prob: keep_prop})\n\n    def train(self, x_data, y_data, keep_prop=0.7):\n        return self.sess.run([self.cost, self.optimizer], feed_dict={\n            self.X: x_data, self.Y: y_data, self.keep_prob: keep_prop})\n\n# initialize\nsess = tf.Session()\nm1 = Model(sess, ""m1"")\n\nsess.run(tf.global_variables_initializer())\n\nprint(\'Learning Started!\')\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        c, _ = m1.train(batch_xs, batch_ys)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nprint(\'Accuracy:\', m1.get_accuracy(mnist.test.images, mnist.test.labels))\n'"
lab-11-4-mnist_cnn_layers.py,30,"b'# Lab 11 MNIST and Deep learning CNN\nimport tensorflow as tf\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n\nclass Model:\n\n    def __init__(self, sess, name):\n        self.sess = sess\n        self.name = name\n        self._build_net()\n\n    def _build_net(self):\n        with tf.variable_scope(self.name):\n            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n            # for testing\n            self.training = tf.placeholder(tf.bool)\n\n            # input place holders\n            self.X = tf.placeholder(tf.float32, [None, 784])\n\n            # img 28x28x1 (black/white), Input Layer\n            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n            self.Y = tf.placeholder(tf.float32, [None, 10])\n\n            # Convolutional Layer #1\n            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n                                     padding=""SAME"", activation=tf.nn.relu)\n            # Pooling Layer #1\n            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n                                            padding=""SAME"", strides=2)\n            dropout1 = tf.layers.dropout(inputs=pool1,\n                                         rate=0.3, training=self.training)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n                                     padding=""SAME"", activation=tf.nn.relu)\n            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n                                            padding=""SAME"", strides=2)\n            dropout2 = tf.layers.dropout(inputs=pool2,\n                                         rate=0.3, training=self.training)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n                                     padding=""same"", activation=tf.nn.relu)\n            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n                                            padding=""same"", strides=2)\n            dropout3 = tf.layers.dropout(inputs=pool3,\n                                         rate=0.3, training=self.training)\n\n            # Dense Layer with Relu\n            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n            dense4 = tf.layers.dense(inputs=flat,\n                                     units=625, activation=tf.nn.relu)\n            dropout4 = tf.layers.dropout(inputs=dense4,\n                                         rate=0.5, training=self.training)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n\n        # define cost/loss & optimizer\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n            logits=self.logits, labels=self.Y))\n        self.optimizer = tf.train.AdamOptimizer(\n            learning_rate=learning_rate).minimize(self.cost)\n\n        correct_prediction = tf.equal(\n            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    def predict(self, x_test, training=False):\n        return self.sess.run(self.logits,\n                             feed_dict={self.X: x_test, self.training: training})\n\n    def get_accuracy(self, x_test, y_test, training=False):\n        return self.sess.run(self.accuracy,\n                             feed_dict={self.X: x_test,\n                                        self.Y: y_test, self.training: training})\n\n    def train(self, x_data, y_data, training=True):\n        return self.sess.run([self.cost, self.optimizer], feed_dict={\n            self.X: x_data, self.Y: y_data, self.training: training})\n\n# initialize\nsess = tf.Session()\nm1 = Model(sess, ""m1"")\n\nsess.run(tf.global_variables_initializer())\n\nprint(\'Learning Started!\')\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        c, _ = m1.train(batch_xs, batch_ys)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nprint(\'Accuracy:\', m1.get_accuracy(mnist.test.images, mnist.test.labels))\n'"
lab-11-5-mnist_cnn_ensemble_layers.py,34,"b'# Lab 11 MNIST and Deep learning CNN\n# https://www.tensorflow.org/tutorials/layers\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 20\nbatch_size = 100\n\n\nclass Model:\n\n    def __init__(self, sess, name):\n        self.sess = sess\n        self.name = name\n        self._build_net()\n\n    def _build_net(self):\n        with tf.variable_scope(self.name):\n            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n            # for testing\n            self.training = tf.placeholder(tf.bool)\n\n            # input place holders\n            self.X = tf.placeholder(tf.float32, [None, 784])\n\n            # img 28x28x1 (black/white), Input Layer\n            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n            self.Y = tf.placeholder(tf.float32, [None, 10])\n\n            # Convolutional Layer #1\n            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n                                     padding=""SAME"", activation=tf.nn.relu)\n            # Pooling Layer #1\n            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n                                            padding=""SAME"", strides=2)\n            dropout1 = tf.layers.dropout(inputs=pool1,\n                                         rate=0.3, training=self.training)\n\n            # Convolutional Layer #2 and Pooling Layer #2\n            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n                                     padding=""SAME"", activation=tf.nn.relu)\n            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n                                            padding=""SAME"", strides=2)\n            dropout2 = tf.layers.dropout(inputs=pool2,\n                                         rate=0.3, training=self.training)\n\n            # Convolutional Layer #3 and Pooling Layer #3\n            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n                                     padding=""SAME"", activation=tf.nn.relu)\n            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n                                            padding=""SAME"", strides=2)\n            dropout3 = tf.layers.dropout(inputs=pool3,\n                                         rate=0.3, training=self.training)\n\n            # Dense Layer with Relu\n            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n            dense4 = tf.layers.dense(inputs=flat,\n                                     units=625, activation=tf.nn.relu)\n            dropout4 = tf.layers.dropout(inputs=dense4,\n                                         rate=0.5, training=self.training)\n\n            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n\n        # define cost/loss & optimizer\n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n            logits=self.logits, labels=self.Y))\n        self.optimizer = tf.train.AdamOptimizer(\n            learning_rate=learning_rate).minimize(self.cost)\n\n        correct_prediction = tf.equal(\n            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    def predict(self, x_test, training=False):\n        return self.sess.run(self.logits,\n                             feed_dict={self.X: x_test, self.training: training})\n\n    def get_accuracy(self, x_test, y_test, training=False):\n        return self.sess.run(self.accuracy,\n                             feed_dict={self.X: x_test,\n                                        self.Y: y_test, self.training: training})\n\n    def train(self, x_data, y_data, training=True):\n        return self.sess.run([self.cost, self.optimizer], feed_dict={\n            self.X: x_data, self.Y: y_data, self.training: training})\n\n# initialize\nsess = tf.Session()\n\nmodels = []\nnum_models = 2\nfor m in range(num_models):\n    models.append(Model(sess, ""model"" + str(m)))\n\nsess.run(tf.global_variables_initializer())\n\nprint(\'Learning Started!\')\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost_list = np.zeros(len(models))\n    total_batch = int(mnist.train.num_examples / batch_size)\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n\n        # train each model\n        for m_idx, m in enumerate(models):\n            c, _ = m.train(batch_xs, batch_ys)\n            avg_cost_list[m_idx] += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', avg_cost_list)\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ntest_size = len(mnist.test.labels)\npredictions = np.zeros([test_size, 10])\nfor m_idx, m in enumerate(models):\n    print(m_idx, \'Accuracy:\', m.get_accuracy(\n        mnist.test.images, mnist.test.labels))\n    p = m.predict(mnist.test.images)\n    predictions += p\n\nensemble_correct_prediction = tf.equal(\n    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\nensemble_accuracy = tf.reduce_mean(\n    tf.cast(ensemble_correct_prediction, tf.float32))\nprint(\'Ensemble accuracy:\', sess.run(ensemble_accuracy))\n\n\'\'\'\n0 Accuracy: 0.9933\n1 Accuracy: 0.9946\n2 Accuracy: 0.9934\n3 Accuracy: 0.9935\n4 Accuracy: 0.9935\n5 Accuracy: 0.9949\n6 Accuracy: 0.9941\n\nEnsemble accuracy: 0.9952\n\'\'\'\n'"
lab-11-X-mnist_cnn_low_memory.py,38,"b'# Lab 10 MNIST and Deep learning CNN\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nX_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\nY = tf.placeholder(tf.float32, [None, 10])\n\n# L1 ImgIn shape=(?, 28, 28, 1)\nW1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n#    Conv     -> (?, 28, 28, 32)\n#    Pool     -> (?, 14, 14, 32)\nL1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding=\'SAME\')\nL1 = tf.nn.relu(L1)\nL1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1], padding=\'SAME\')\nL1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n\'\'\'\nTensor(""Conv2D:0"", shape=(?, 28, 28, 32), dtype=float32)\nTensor(""Relu:0"", shape=(?, 28, 28, 32), dtype=float32)\nTensor(""MaxPool:0"", shape=(?, 14, 14, 32), dtype=float32)\nTensor(""dropout/mul:0"", shape=(?, 14, 14, 32), dtype=float32)\n\'\'\'\n\n# L2 ImgIn shape=(?, 14, 14, 32)\nW2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n#    Conv      ->(?, 14, 14, 64)\n#    Pool      ->(?, 7, 7, 64)\nL2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\'SAME\')\nL2 = tf.nn.relu(L2)\nL2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1], padding=\'SAME\')\nL2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n\'\'\'\nTensor(""Conv2D_1:0"", shape=(?, 14, 14, 64), dtype=float32)\nTensor(""Relu_1:0"", shape=(?, 14, 14, 64), dtype=float32)\nTensor(""MaxPool_1:0"", shape=(?, 7, 7, 64), dtype=float32)\nTensor(""dropout_1/mul:0"", shape=(?, 7, 7, 64), dtype=float32)\n\'\'\'\n\n# L3 ImgIn shape=(?, 7, 7, 64)\nW3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n#    Conv      ->(?, 7, 7, 128)\n#    Pool      ->(?, 4, 4, 128)\n#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\nL3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding=\'SAME\')\nL3 = tf.nn.relu(L3)\nL3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n                    1, 2, 2, 1], padding=\'SAME\')\nL3 = tf.nn.dropout(L3, keep_prob=keep_prob)\nL3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n\'\'\'\nTensor(""Conv2D_2:0"", shape=(?, 7, 7, 128), dtype=float32)\nTensor(""Relu_2:0"", shape=(?, 7, 7, 128), dtype=float32)\nTensor(""MaxPool_2:0"", shape=(?, 4, 4, 128), dtype=float32)\nTensor(""dropout_2/mul:0"", shape=(?, 4, 4, 128), dtype=float32)\nTensor(""Reshape_1:0"", shape=(?, 2048), dtype=float32)\n\'\'\'\n\n# L4 FC 4x4x128 inputs -> 625 outputs\nW4 = tf.get_variable(""W4"", shape=[128 * 4 * 4, 625],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb4 = tf.Variable(tf.random_normal([625]))\nL4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\nL4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n\'\'\'\nTensor(""Relu_3:0"", shape=(?, 625), dtype=float32)\nTensor(""dropout_3/mul:0"", shape=(?, 625), dtype=float32)\n\'\'\'\n\n# L5 Final FC 625 inputs -> 10 outputs\nW5 = tf.get_variable(""W5"", shape=[625, 10],\n                     initializer=tf.contrib.layers.xavier_initializer())\nb5 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L4, W5) + b5\n\'\'\'\nTensor(""add_1:0"", shape=(?, 10), dtype=float32)\n\'\'\'\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nprint(\'Learning stared. It takes sometime.\')\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\ndef evaluate(X_sample, y_sample, batch_size=512):\n    """"""Run a minibatch accuracy op""""""\n\n    N = X_sample.shape[0]\n    correct_sample = 0\n\n    for i in range(0, N, batch_size):\n        X_batch = X_sample[i: i + batch_size]\n        y_batch = y_sample[i: i + batch_size]\n        N_batch = X_batch.shape[0]\n\n        feed = {\n            X: X_batch,\n            Y: y_batch,\n            keep_prob: 1\n        }\n\n        correct_sample += sess.run(accuracy, feed_dict=feed) * N_batch\n\n    return correct_sample / N\n\nprint(""\\nAccuracy Evaluates"")\nprint(""-------------------------------"")\nprint(\'Train Accuracy:\', evaluate(mnist.train.images, mnist.train.labels))\nprint(\'Test Accuracy:\', evaluate(mnist.test.images, mnist.test.labels))\n\n\n# Get one and predict\nprint(""\\nGet one and predict"")\nprint(""-------------------------------"")\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), {X: mnist.test.images[r:r + 1], keep_prob: 1}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nLearning stared. It takes sometime.\nEpoch: 0001 cost = 0.385748474\nEpoch: 0002 cost = 0.092017397\nEpoch: 0003 cost = 0.065854684\nEpoch: 0004 cost = 0.055604566\nEpoch: 0005 cost = 0.045996377\nEpoch: 0006 cost = 0.040913645\nEpoch: 0007 cost = 0.036924479\nEpoch: 0008 cost = 0.032808939\nEpoch: 0009 cost = 0.031791007\nEpoch: 0010 cost = 0.030224456\nEpoch: 0011 cost = 0.026849916\nEpoch: 0012 cost = 0.026826763\nEpoch: 0013 cost = 0.027188021\nEpoch: 0014 cost = 0.023604777\nEpoch: 0015 cost = 0.024607201\nLearning Finished!\nAccuracy: 0.9938\n\'\'\'\n'"
lab-12-1-hello-rnn.py,21,"b'# Lab 12 RNN\nimport tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # reproducibility\n\nidx2char = [\'h\', \'i\', \'e\', \'l\', \'o\']\n# Teach hello: hihell -> ihello\nx_data = [[0, 1, 0, 2, 3, 3]]   # hihell\nx_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n              [0, 1, 0, 0, 0],   # i 1\n              [1, 0, 0, 0, 0],   # h 0\n              [0, 0, 1, 0, 0],   # e 2\n              [0, 0, 0, 1, 0],   # l 3\n              [0, 0, 0, 1, 0]]]  # l 3\n\ny_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n\nnum_classes = 5\ninput_dim = 5  # one-hot size\nhidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\nbatch_size = 1   # one sentence\nsequence_length = 6  # |ihello| == 6\nlearning_rate = 0.1\n\nX = tf.placeholder(\n    tf.float32, [None, sequence_length, input_dim])  # X one-hot\nY = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n\ncell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\ninitial_state = cell.zero_state(batch_size, tf.float32)\noutputs, _states = tf.nn.dynamic_rnn(\n    cell, X, initial_state=initial_state, dtype=tf.float32)\n\n# FC layer\nX_for_fc = tf.reshape(outputs, [-1, hidden_size])\n# fc_w = tf.get_variable(""fc_w"", [hidden_size, num_classes])\n# fc_b = tf.get_variable(""fc_b"", [num_classes])\n# outputs = tf.matmul(X_for_fc, fc_w) + fc_b\noutputs = tf.contrib.layers.fully_connected(\n    inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)\n\n# reshape out for sequence_loss\noutputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n\nweights = tf.ones([batch_size, sequence_length])\nsequence_loss = tf.contrib.seq2seq.sequence_loss(\n    logits=outputs, targets=Y, weights=weights)\nloss = tf.reduce_mean(sequence_loss)\ntrain = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\nprediction = tf.argmax(outputs, axis=2)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(50):\n        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n        result = sess.run(prediction, feed_dict={X: x_one_hot})\n        print(i, ""loss:"", l, ""prediction: "", result, ""true Y: "", y_data)\n\n        # print char using dic\n        result_str = [idx2char[c] for c in np.squeeze(result)]\n        print(""\\tPrediction str: "", \'\'.join(result_str))\n\n\'\'\'\n0 loss: 1.71584 prediction:  [[2 2 2 3 3 2]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  eeelle\n1 loss: 1.56447 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  llllll\n2 loss: 1.46284 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  llllll\n3 loss: 1.38073 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  llllll\n4 loss: 1.30603 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  llllll\n5 loss: 1.21498 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  llllll\n6 loss: 1.1029 prediction:  [[3 0 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  lhlllo\n7 loss: 0.982386 prediction:  [[1 0 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  ihlllo\n8 loss: 0.871259 prediction:  [[1 0 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  ihlllo\n9 loss: 0.774338 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  ihello\n10 loss: 0.676005 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n\tPrediction str:  ihello\n\n...\n\n\'\'\'\n'"
lab-12-2-char-seq-rnn.py,18,"b'# Lab 12 Character Sequence RNN\nimport tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # reproducibility\n\nsample = "" if you want you""\nidx2char = list(set(sample))  # index -> char\nchar2idx = {c: i for i, c in enumerate(idx2char)}  # char -> index\n\n# hyper parameters\ndic_size = len(char2idx)  # RNN input size (one hot size)\nhidden_size = len(char2idx)  # RNN output size\nnum_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\nbatch_size = 1  # one sample data, one batch\nsequence_length = len(sample) - 1  # number of lstm rollings (unit #)\nlearning_rate = 0.1\n\nsample_idx = [char2idx[c] for c in sample]  # char to index\nx_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\ny_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n\nX = tf.placeholder(tf.int32, [None, sequence_length])  # X data\nY = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n\nx_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\ncell = tf.contrib.rnn.BasicLSTMCell(\n    num_units=hidden_size, state_is_tuple=True)\ninitial_state = cell.zero_state(batch_size, tf.float32)\noutputs, _states = tf.nn.dynamic_rnn(\n    cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)\n\n# FC layer\nX_for_fc = tf.reshape(outputs, [-1, hidden_size])\noutputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n\n# reshape out for sequence_loss\noutputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n\nweights = tf.ones([batch_size, sequence_length])\nsequence_loss = tf.contrib.seq2seq.sequence_loss(\n    logits=outputs, targets=Y, weights=weights)\nloss = tf.reduce_mean(sequence_loss)\ntrain = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\nprediction = tf.argmax(outputs, axis=2)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(50):\n        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n        result = sess.run(prediction, feed_dict={X: x_data})\n\n        # print char using dic\n        result_str = [idx2char[c] for c in np.squeeze(result)]\n\n        print(i, ""loss:"", l, ""Prediction:"", \'\'.join(result_str))\n\n\n\'\'\'\n0 loss: 2.35377 Prediction: uuuuuuuuuuuuuuu\n1 loss: 2.21383 Prediction: yy you y    you\n2 loss: 2.04317 Prediction: yy yoo       ou\n3 loss: 1.85869 Prediction: yy  ou      uou\n4 loss: 1.65096 Prediction: yy you  a   you\n5 loss: 1.40243 Prediction: yy you yan  you\n6 loss: 1.12986 Prediction: yy you wann you\n7 loss: 0.907699 Prediction: yy you want you\n8 loss: 0.687401 Prediction: yf you want you\n9 loss: 0.508868 Prediction: yf you want you\n10 loss: 0.379423 Prediction: yf you want you\n11 loss: 0.282956 Prediction: if you want you\n12 loss: 0.208561 Prediction: if you want you\n\n...\n\n\'\'\'\n'"
lab-12-3-char-seq-softmax-only.py,16,"b'# Lab 12 Character Sequence Softmax only\nimport tensorflow as tf\nimport numpy as np\ntf.set_random_seed(777)  # reproducibility\n\nsample = "" if you want you""\nidx2char = list(set(sample))  # index -> char\nchar2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n\n# hyper parameters\ndic_size = len(char2idx)  # RNN input size (one hot size)\nrnn_hidden_size = len(char2idx)  # RNN output size\nnum_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\nbatch_size = 1  # one sample data, one batch\nsequence_length = len(sample) - 1  # number of lstm rollings (unit #)\nlearning_rate = 0.1\n\nsample_idx = [char2idx[c] for c in sample]  # char to index\nx_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\ny_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n\nX = tf.placeholder(tf.int32, [None, sequence_length])  # X data\nY = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n\n# flatten the data (ignore batches for now). No effect if the batch size is 1\nX_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\nX_for_softmax = tf.reshape(X_one_hot, [-1, rnn_hidden_size])\n\n# softmax layer (rnn_hidden_size -> num_classes)\nsoftmax_w = tf.get_variable(""softmax_w"", [rnn_hidden_size, num_classes])\nsoftmax_b = tf.get_variable(""softmax_b"", [num_classes])\noutputs = tf.matmul(X_for_softmax, softmax_w) + softmax_b\n\n# expend the data (revive the batches)\noutputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\nweights = tf.ones([batch_size, sequence_length])\n\n# Compute sequence cost/loss\nsequence_loss = tf.contrib.seq2seq.sequence_loss(\n    logits=outputs, targets=Y, weights=weights)\nloss = tf.reduce_mean(sequence_loss)  # mean all sequence loss\ntrain = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\nprediction = tf.argmax(outputs, axis=2)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(3000):\n        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n        result = sess.run(prediction, feed_dict={X: x_data})\n\n        # print char using dic\n        result_str = [idx2char[c] for c in np.squeeze(result)]\n        print(i, ""loss:"", l, ""Prediction:"", \'\'.join(result_str))\n\n\'\'\'\n0 loss: 2.29513 Prediction: yu yny y y oyny\n1 loss: 2.10156 Prediction: yu ynu y y oynu\n2 loss: 1.92344 Prediction: yu you y u  you\n\n..\n\n2997 loss: 0.277323 Prediction: yf you yant you\n2998 loss: 0.277323 Prediction: yf you yant you\n2999 loss: 0.277323 Prediction: yf you yant you\n\'\'\'\n'"
lab-12-4-rnn_long_char.py,14,"b'from __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib import rnn\n\ntf.set_random_seed(777)  # reproducibility\n\nsentence = (""if you want to build a ship, don\'t drum up people together to ""\n            ""collect wood and don\'t assign them tasks and work, but rather ""\n            ""teach them to long for the endless immensity of the sea."")\n\nchar_set = list(set(sentence))\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\ndata_dim = len(char_set)\nhidden_size = len(char_set)\nnum_classes = len(char_set)\nsequence_length = 10  # Any arbitrary number\nlearning_rate = 0.1\n\ndataX = []\ndataY = []\nfor i in range(0, len(sentence) - sequence_length):\n    x_str = sentence[i:i + sequence_length]\n    y_str = sentence[i + 1: i + sequence_length + 1]\n    print(i, x_str, \'->\', y_str)\n\n    x = [char_dic[c] for c in x_str]  # x str to index\n    y = [char_dic[c] for c in y_str]  # y str to index\n\n    dataX.append(x)\n    dataY.append(y)\n\nbatch_size = len(dataX)\n\nX = tf.placeholder(tf.int32, [None, sequence_length])\nY = tf.placeholder(tf.int32, [None, sequence_length])\n\n# One-hot encoding\nX_one_hot = tf.one_hot(X, num_classes)\nprint(X_one_hot)  # check out the shape\n\n\n# Make a lstm cell with hidden_size (each unit output vector size)\ndef lstm_cell():\n    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n    return cell\n\nmulti_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple=True)\n\n# outputs: unfolding size x hidden size, state = hidden size\noutputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n\n# FC layer\nX_for_fc = tf.reshape(outputs, [-1, hidden_size])\noutputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n\n# reshape out for sequence_loss\noutputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n\n# All weights are 1 (equal weights)\nweights = tf.ones([batch_size, sequence_length])\n\nsequence_loss = tf.contrib.seq2seq.sequence_loss(\n    logits=outputs, targets=Y, weights=weights)\nmean_loss = tf.reduce_mean(sequence_loss)\ntrain_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nfor i in range(500):\n    _, l, results = sess.run(\n        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})\n    for j, result in enumerate(results):\n        index = np.argmax(result, axis=1)\n        print(i, j, \'\'.join([char_set[t] for t in index]), l)\n\n# Let\'s print the last char of each result to check it works\nresults = sess.run(outputs, feed_dict={X: dataX})\nfor j, result in enumerate(results):\n    index = np.argmax(result, axis=1)\n    if j is 0:  # print all for the first result to make a sentence\n        print(\'\'.join([char_set[t] for t in index]), end=\'\')\n    else:\n        print(char_set[index[-1]], end=\'\')\n\n\'\'\'\n0 167 tttttttttt 3.23111\n0 168 tttttttttt 3.23111\n0 169 tttttttttt 3.23111\n\xe2\x80\xa6\n499 167  of the se 0.229616\n499 168 tf the sea 0.229616\n499 169   the sea. 0.229616\n\ng you want to build a ship, don\'t drum up people together to collect wood and don\'t assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n\n\'\'\'\n'"
lab-12-5-rnn_stock_prediction.py,14,"b'\'\'\'\nThis script shows how to predict stock prices using a basic RNN\n\'\'\'\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib\nimport os\n\ntf.set_random_seed(777)  # reproducibility\n\nif ""DISPLAY"" not in os.environ:\n    # remove Travis CI Error\n    matplotlib.use(\'Agg\')\n\nimport matplotlib.pyplot as plt\n\n\ndef MinMaxScaler(data):\n    \'\'\' Min Max Normalization\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        input data to be normalized\n        shape: [Batch size, dimension]\n\n    Returns\n    ----------\n    data : numpy.ndarry\n        normalized data\n        shape: [Batch size, dimension]\n\n    References\n    ----------\n    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n\n    \'\'\'\n    numerator = data - np.min(data, 0)\n    denominator = np.max(data, 0) - np.min(data, 0)\n    # noise term prevents the zero division\n    return numerator / (denominator + 1e-7)\n\n\n# train Parameters\nseq_length = 7\ndata_dim = 5\nhidden_dim = 10\noutput_dim = 1\nlearning_rate = 0.01\niterations = 500\n\n# Open, High, Low, Volume, Close\nxy = np.loadtxt(\'data-02-stock_daily.csv\', delimiter=\',\')\nxy = xy[::-1]  # reverse order (chronically ordered)\n\n# train/test split\ntrain_size = int(len(xy) * 0.7)\ntrain_set = xy[0:train_size]\ntest_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n\n# Scale each\ntrain_set = MinMaxScaler(train_set)\ntest_set = MinMaxScaler(test_set)\n\n# build datasets\ndef build_dataset(time_series, seq_length):\n    dataX = []\n    dataY = []\n    for i in range(0, len(time_series) - seq_length):\n        _x = time_series[i:i + seq_length, :]\n        _y = time_series[i + seq_length, [-1]]  # Next close price\n        print(_x, ""->"", _y)\n        dataX.append(_x)\n        dataY.append(_y)\n    return np.array(dataX), np.array(dataY)\n\ntrainX, trainY = build_dataset(train_set, seq_length)\ntestX, testY = build_dataset(test_set, seq_length)\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, seq_length, data_dim])\nY = tf.placeholder(tf.float32, [None, 1])\n\n# build a LSTM network\ncell = tf.contrib.rnn.BasicLSTMCell(\n    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\noutputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\nY_pred = tf.contrib.layers.fully_connected(\n    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell\'s output\n\n# cost/loss\nloss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n# optimizer\noptimizer = tf.train.AdamOptimizer(learning_rate)\ntrain = optimizer.minimize(loss)\n\n# RMSE\ntargets = tf.placeholder(tf.float32, [None, 1])\npredictions = tf.placeholder(tf.float32, [None, 1])\nrmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    # Training step\n    for i in range(iterations):\n        _, step_loss = sess.run([train, loss], feed_dict={\n                                X: trainX, Y: trainY})\n        print(""[step: {}] loss: {}"".format(i, step_loss))\n\n    # Test step\n    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n    rmse_val = sess.run(rmse, feed_dict={\n                    targets: testY, predictions: test_predict})\n    print(""RMSE: {}"".format(rmse_val))\n\n    # Plot predictions\n    plt.plot(testY)\n    plt.plot(test_predict)\n    plt.xlabel(""Time Period"")\n    plt.ylabel(""Stock Price"")\n    plt.show()\n'"
lab-13-1-mnist_using_scope.py,41,"b'# Lab 13 Using Scope\nimport tensorflow as tf\nimport random\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n# weights & bias for nn layers\n# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\nwith tf.variable_scope(\'layer1\') as scope:\n    W1 = tf.get_variable(""W"", shape=[784, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.Variable(tf.random_normal([512]))\n    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n\nwith tf.variable_scope(\'layer2\') as scope:\n    W2 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.Variable(tf.random_normal([512]))\n    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n\nwith tf.variable_scope(\'layer3\') as scope:\n    W3 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b3 = tf.Variable(tf.random_normal([512]))\n    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n\nwith tf.variable_scope(\'layer4\') as scope:\n    W4 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b4 = tf.Variable(tf.random_normal([512]))\n    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n\nwith tf.variable_scope(\'layer5\') as scope:\n    W5 = tf.get_variable(""W"", shape=[512, 10],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b5 = tf.Variable(tf.random_normal([10]))\n    hypothesis = tf.matmul(L4, W5) + b5\n\nprint(W1, W5)\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        sess.run(optimizer, feed_dict=feed_dict)\n        avg_cost += sess.run(cost, feed_dict=feed_dict) / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\nEpoch: 0001 cost = 0.447322626\nEpoch: 0002 cost = 0.157285590\nEpoch: 0003 cost = 0.121884535\nEpoch: 0004 cost = 0.098128681\nEpoch: 0005 cost = 0.082901778\nEpoch: 0006 cost = 0.075337573\nEpoch: 0007 cost = 0.069752543\nEpoch: 0008 cost = 0.060884363\nEpoch: 0009 cost = 0.055276413\nEpoch: 0010 cost = 0.054631256\nEpoch: 0011 cost = 0.049675195\nEpoch: 0012 cost = 0.049125314\nEpoch: 0013 cost = 0.047231930\nEpoch: 0014 cost = 0.041290121\nEpoch: 0015 cost = 0.043621063\nLearning Finished!\nAccuracy: 0.9804\n\'\'\'\n'"
lab-13-2-mnist_tensorboard.py,62,"b'# Lab 13 Tensorboard\nimport tensorflow as tf\nimport random\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\nTB_SUMMARY_DIR = \'./tb/mnist\'\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# Image input\nx_image = tf.reshape(X, [-1, 28, 28, 1])\ntf.summary.image(\'input\', x_image, 3)\n\n# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n# weights & bias for nn layers\n# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\nwith tf.variable_scope(\'layer1\') as scope:\n    W1 = tf.get_variable(""W"", shape=[784, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.Variable(tf.random_normal([512]))\n    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n\n    tf.summary.histogram(""X"", X)\n    tf.summary.histogram(""weights"", W1)\n    tf.summary.histogram(""bias"", b1)\n    tf.summary.histogram(""layer"", L1)\n\nwith tf.variable_scope(\'layer2\') as scope:\n    W2 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.Variable(tf.random_normal([512]))\n    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n\n    tf.summary.histogram(""weights"", W2)\n    tf.summary.histogram(""bias"", b2)\n    tf.summary.histogram(""layer"", L2)\n\nwith tf.variable_scope(\'layer3\') as scope:\n    W3 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b3 = tf.Variable(tf.random_normal([512]))\n    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n\n    tf.summary.histogram(""weights"", W3)\n    tf.summary.histogram(""bias"", b3)\n    tf.summary.histogram(""layer"", L3)\n\nwith tf.variable_scope(\'layer4\') as scope:\n    W4 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b4 = tf.Variable(tf.random_normal([512]))\n    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n\n    tf.summary.histogram(""weights"", W4)\n    tf.summary.histogram(""bias"", b4)\n    tf.summary.histogram(""layer"", L4)\n\nwith tf.variable_scope(\'layer5\') as scope:\n    W5 = tf.get_variable(""W"", shape=[512, 10],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b5 = tf.Variable(tf.random_normal([10]))\n    hypothesis = tf.matmul(L4, W5) + b5\n\n    tf.summary.histogram(""weights"", W5)\n    tf.summary.histogram(""bias"", b5)\n    tf.summary.histogram(""hypothesis"", hypothesis)\n\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\ntf.summary.scalar(""loss"", cost)\n\n# Summary\nsummary = tf.summary.merge_all()\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# Create summary writer\nwriter = tf.summary.FileWriter(TB_SUMMARY_DIR)\nwriter.add_graph(sess.graph)\nglobal_step = 0\n\nprint(\'Start learning!\')\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n        writer.add_summary(s, global_step=global_step)\n        global_step += 1\n\n        avg_cost += sess.run(cost, feed_dict=feed_dict) / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\ntensorboard --logdir tb/\nStarting TensorBoard b\'41\' on port 6006\n(You can navigate to http://10.0.1.4:6006)\n\n\'\'\'\n\'\'\'\nEpoch: 0001 cost = 0.447322626\nEpoch: 0002 cost = 0.157285590\nEpoch: 0003 cost = 0.121884535\nEpoch: 0004 cost = 0.098128681\nEpoch: 0005 cost = 0.082901778\nEpoch: 0006 cost = 0.075337573\nEpoch: 0007 cost = 0.069752543\nEpoch: 0008 cost = 0.060884363\nEpoch: 0009 cost = 0.055276413\nEpoch: 0010 cost = 0.054631256\nEpoch: 0011 cost = 0.049675195\nEpoch: 0012 cost = 0.049125314\nEpoch: 0013 cost = 0.047231930\nEpoch: 0014 cost = 0.041290121\nEpoch: 0015 cost = 0.043621063\nLearning Finished!\nAccuracy: 0.9804\n\'\'\'\n'"
lab-13-3-mnist_save_restore.py,65,"b'# Lab 13 Saver and Restore\nimport tensorflow as tf\nimport random\n# import matplotlib.pyplot as plt\nimport os\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(777)  # reproducibility\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\nCHECK_POINT_DIR = TB_SUMMARY_DIR = \'./tb/mnist2\'\n\n\n# input place holders\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\n# Image input\nx_image = tf.reshape(X, [-1, 28, 28, 1])\ntf.summary.image(\'input\', x_image, 3)\n\n# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n# weights & bias for nn layers\n# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\nwith tf.variable_scope(\'layer1\'):\n    W1 = tf.get_variable(""W"", shape=[784, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.Variable(tf.random_normal([512]))\n    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n\n    tf.summary.histogram(""X"", X)\n    tf.summary.histogram(""weights"", W1)\n    tf.summary.histogram(""bias"", b1)\n    tf.summary.histogram(""layer"", L1)\n\nwith tf.variable_scope(\'layer2\'):\n    W2 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.Variable(tf.random_normal([512]))\n    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n\n    tf.summary.histogram(""weights"", W2)\n    tf.summary.histogram(""bias"", b2)\n    tf.summary.histogram(""layer"", L2)\n\nwith tf.variable_scope(\'layer3\'):\n    W3 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b3 = tf.Variable(tf.random_normal([512]))\n    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n\n    tf.summary.histogram(""weights"", W3)\n    tf.summary.histogram(""bias"", b3)\n    tf.summary.histogram(""layer"", L3)\n\nwith tf.variable_scope(\'layer4\'):\n    W4 = tf.get_variable(""W"", shape=[512, 512],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b4 = tf.Variable(tf.random_normal([512]))\n    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n\n    tf.summary.histogram(""weights"", W4)\n    tf.summary.histogram(""bias"", b4)\n    tf.summary.histogram(""layer"", L4)\n\nwith tf.variable_scope(\'layer5\'):\n    W5 = tf.get_variable(""W"", shape=[512, 10],\n                         initializer=tf.contrib.layers.xavier_initializer())\n    b5 = tf.Variable(tf.random_normal([10]))\n    hypothesis = tf.matmul(L4, W5) + b5\n\n    tf.summary.histogram(""weights"", W5)\n    tf.summary.histogram(""bias"", b5)\n    tf.summary.histogram(""hypothesis"", hypothesis)\n\n\n# define cost/loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\ntf.summary.scalar(""loss"", cost)\n\nlast_epoch = tf.Variable(0, name=\'last_epoch\')\n\n# Summary\nsummary = tf.summary.merge_all()\n\n# initialize\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# Create summary writer\nwriter = tf.summary.FileWriter(TB_SUMMARY_DIR)\nwriter.add_graph(sess.graph)\nglobal_step = 0\n\n# Saver and Restore\nsaver = tf.train.Saver()\ncheckpoint = tf.train.get_checkpoint_state(CHECK_POINT_DIR)\n\nif checkpoint and checkpoint.model_checkpoint_path:\n    try:\n        saver.restore(sess, checkpoint.model_checkpoint_path)\n        print(""Successfully loaded:"", checkpoint.model_checkpoint_path)\n    except:\n        print(""Error on loading old network weights"")\nelse:\n    print(""Could not find old network weights"")\n\nstart_from = sess.run(last_epoch)\n\n# train my model\nprint(\'Start learning from:\', start_from)\n\nfor epoch in range(start_from, training_epochs):\n    print(\'Start Epoch:\', epoch)\n\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n        writer.add_summary(s, global_step=global_step)\n        global_step += 1\n\n        avg_cost += sess.run(cost, feed_dict=feed_dict) / total_batch\n\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\n\n    print(""Saving network..."")\n    sess.run(last_epoch.assign(epoch + 1))\n    if not os.path.exists(CHECK_POINT_DIR):\n        os.makedirs(CHECK_POINT_DIR)\n    saver.save(sess, CHECK_POINT_DIR + ""/model"", global_step=i)\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\ncorrect_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\'Accuracy:\', sess.run(accuracy, feed_dict={\n      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n\n# Get one and predict\nr = random.randint(0, mnist.test.num_examples - 1)\nprint(""Label: "", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\nprint(""Prediction: "", sess.run(\n    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n\n# plt.imshow(mnist.test.images[r:r + 1].\n#           reshape(28, 28), cmap=\'Greys\', interpolation=\'nearest\')\n# plt.show()\n\n\'\'\'\n\n...\n\nSuccessfully loaded: ./tb/mnist/model-549\nStart learning from: 2\nEpoch: 2\n\n...\ntensorboard --logdir tb/\nStarting TensorBoard b\'41\' on port 6006\n(You can navigate to http://10.0.1.4:6006)\n\n\'\'\'\n'"
chainer/chlab-01-1-basics.py,0,"b'#!/usr/bin/env python\n# Lab 1-1 Basics\n\nimport numpy as np\nimport chainer\n\n\n# Create Variable object.\na = chainer.Variable(np.array([1], dtype=np.float32))\nb = chainer.Variable(np.array([2], dtype=np.float32))\n\n# Variable object has basic arithmetic operators.\ny = a * b\n\n# Now y is a Variable object, with attribute ""data"".\n# http://docs.chainer.org/en/latest/reference/core/variable.html?highlight=Variable\nprint(""{} should equal [ 2.]"".format(y.data))\n\na = chainer.Variable(np.array([3], dtype=np.float32))\nb = chainer.Variable(np.array([3], dtype=np.float32))\n\ny = a * b\nprint(""{} should equal [ 9.]"".format(y.data))\n\n""""""\nExpected output.\n---\n[ 2.] should equal [ 2.]\n[ 9.] should equal [ 9.]\n""""""\n'"
chainer/chlab-02-1-linear_regression.py,0,"b'#!/usr/bin/env python\n# Lab 2-1 Linear Regression\n\nimport numpy as np\nimport chainer\nfrom chainer import training\nfrom chainer import datasets\nfrom chainer.training import extensions\n\nimport chainer.functions as F\nimport chainer.links as L\n\n\nclass MyModel(chainer.Chain):\n    # Define model to be called later by L.Classifier()\n\n    def __init__(self, n_out):\n        super(MyModel, self).__init__(\n            l1=L.Linear(None, n_out),\n        )\n\n    def __call__(self, x):\n        return self.l1(x)\n\n\ndef generate_data():\n    # Need to reshape so that each input is an array.\n    reshape = lambda x: np.reshape(x, (len(x), 1))\n\n    # Notice the type specification (np.float32)\n    # For regression, use np.float32 for both input & output, while for\n    # classification using softmax_cross_entropy, the output(label) needs to be\n    # of type np.int32.\n    X = np.linspace(-1, 1, 101).astype(np.float32)\n    Y = (2 * X + np.random.randn(*X.shape) * 0.33).astype(np.float32)\n    return reshape(X), reshape(Y)\n\n\ndef main():\n    epoch = 100\n    batch_size = 1\n\n    data = generate_data()\n\n    # Convert to set of tuples (target, label).\n    train = datasets.TupleDataset(*data)\n\n    model = L.Classifier(MyModel(1), lossfun=F.mean_squared_error)\n\n    # Set compute_accuracy=False when using MSE.\n    model.compute_accuracy = False\n\n    # Define optimizer (Adam, RMSProp, etc)\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(model)\n\n    # Define iterators.\n    train_iter = chainer.iterators.SerialIterator(train, batch_size)\n\n    updater = training.StandardUpdater(train_iter, optimizer)\n    trainer = training.Trainer(updater, (epoch, \'epoch\'))\n\n    # Helper functions (extensions) to monitor progress on stdout.\n    report_params = [\n        \'epoch\',\n        \'main/loss\',\n    ]\n    trainer.extend(extensions.LogReport())\n    trainer.extend(extensions.PrintReport(report_params))\n    trainer.extend(extensions.ProgressBar())\n\n    # Run trainer\n    trainer.run()\n\n    # Should print out value close to 2.\n    print(model.predictor(np.array([[1]]).astype(np.float32)).data)\n\nif __name__ == ""__main__"":\n    main()\n\n\n""""""\nExpected output.\n---\n\nepoch       main/loss\n...\n90          0.104054\n91          0.104079\n92          0.104037\n93          0.104005\n94          0.104142\n95          0.104292\n96          0.103934\n97          0.104091\n98          0.103952\n99          0.104034\n100         0.103947\n[[ 1.98888695]]\n""""""\n'"
chainer/chlab-05-2-logistic_regression_mnist.py,0,"b'#!/usr/bin/env python\n# Lab 5 Logistic Regression Classifier\n\nimport numpy as np\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\n\nfrom chainer import training, Variable\nfrom chainer import datasets, iterators, optimizers\nfrom chainer import Chain\nfrom chainer.training import extensions\n\n\nclass MyModel(chainer.Chain):\n    # Define model to be called later by L.Classifer()\n\n    def __init__(self, n_out):\n        super(MyModel, self).__init__(\n            l1=L.Linear(None, n_out),\n        )\n\n    def __call__(self, x):\n        return self.l1(x)\n\n\ndef main():\n    epoch = 100\n    batch_size = 128\n\n    # Load mnist data\n    # http://docs.chainer.org/en/latest/reference/datasets.html\n    train, test = chainer.datasets.get_mnist()\n\n    # Define iterators.\n    train_iter = chainer.iterators.SerialIterator(train, batch_size)\n    test_iter = chainer.iterators.SerialIterator(test, batch_size,\n                                                 repeat=False, shuffle=False)\n\n    # Initialize model: Loss function defaults to softmax_cross_entropy.\n    # Can keep same model used in linear regression.\n    model = L.Classifier(MyModel(10))\n\n    # Define optimizer (SGD, Adam, RMSProp, etc)\n    # http://docs.chainer.org/en/latest/reference/optimizers.html\n    optimizer = chainer.optimizers.SGD()\n    optimizer.setup(model)\n\n    # Set up trainer\n    updater = training.StandardUpdater(train_iter, optimizer)\n    trainer = training.Trainer(updater, (epoch, \'epoch\'))\n\n    # Evaluate the model at end of each epoch\n    trainer.extend(extensions.Evaluator(test_iter, model))\n\n    # Helper functions (extensions) to monitor progress on stdout.\n    report_params = [\n        \'epoch\',\n        \'main/loss\',\n        \'validation/main/loss\',\n        \'main/accuracy\',\n        \'validation/main/accuracy\',\n        \'elapsed_time\'\n    ]\n    trainer.extend(extensions.LogReport())\n    trainer.extend(extensions.PrintReport(report_params))\n    trainer.extend(extensions.ProgressBar())\n\n    # Run trainer\n    trainer.run()\n\n\nif __name__ == ""__main__"":\n    main()\n\n\n""""""\nExpected output.\n\nepoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n...\n90          0.289549    0.282843              0.919443       0.920293                  153.543\n91          0.289916    0.282415              0.919027       0.920688                  155.247\n92          0.288972    0.282059              0.919655       0.920589                  156.955\n93          0.288909    0.281913              0.919426       0.920688                  158.662\n94          0.288564    0.281732              0.91946        0.920886                  160.377\n95          0.28832     0.281716              0.919826       0.921084                  162.081\n96          0.287921    0.281607              0.919538       0.920688                  163.785\n97          0.287806    0.281264              0.919759       0.921381                  165.491\n98          0.287321    0.281151              0.919926       0.921479                  167.2\n99          0.287164    0.280869              0.919926       0.921084                  168.897\n100         0.286772    0.280792              0.920072       0.921282                  170.597\n""""""\n'"
chainer/chlab-10-2-mnist_nn.py,0,"b'#!/usr/bin/env python\n# Lab 10 MNIST and NN\n\nimport argparse\nimport numpy as np\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\n\nfrom chainer import training, Variable\nfrom chainer import datasets, iterators, optimizers\nfrom chainer import Chain\nfrom chainer.training import extensions\n\n\nclass MLP(chainer.Chain):\n    # Define model to be called later by L.Classifer()\n    # Basic MLP\n\n    def __init__(self, n_unit, n_out):\n        super(MLP, self).__init__(\n            l1=L.Linear(None, n_unit),\n            l2=L.Linear(None, n_out)\n        )\n\n    def __call__(self, x):\n        h = F.sigmoid(self.l1(x))\n        return self.l2(h)\n\n\ndef main():\n    # Introduce argparse for clarity and organization.\n    # Starting to use higher capacity models, thus set up for GPU.\n    parser = argparse.ArgumentParser(description=\'Chainer-Tutorial: MLP\')\n    parser.add_argument(\'--batch_size\', \'-b\', type=int, default=128,\n                        help=\'Number of samples in each mini-batch\')\n    parser.add_argument(\'--epoch\', \'-e\', type=int, default=100,\n                        help=\'Number of times to train on data set\')\n    parser.add_argument(\'--gpu\', \'-g\', type=int, default=-1,\n                        help=\'GPU ID: -1 indicates CPU\')\n    args = parser.parse_args()\n\n    # Load mnist data\n    # http://docs.chainer.org/en/latest/reference/datasets.html\n    train, test = chainer.datasets.get_mnist()\n\n    # Define iterators.\n    train_iter = chainer.iterators.SerialIterator(train, args.batch_size)\n    test_iter = chainer.iterators.SerialIterator(test, args.batch_size,\n                                                 repeat=False, shuffle=False)\n\n    # Initialize model: Loss function defaults to softmax_cross_entropy.\n    # 784 is dimension of the inputs, 625 is n_units in hidden layer\n    # and 10 is the output dimension.\n    model = L.Classifier(MLP(625, 10))\n\n    # Set up GPU usage if necessary. args.gpu is a condition as well as an\n    # identification when passed to get_device().\n    if args.gpu >= 0:\n        chainer.cuda.get_device(args.gpu).use()\n        model.to_gpu()\n\n    # Define optimizer (SGD, Adam, RMSProp, etc)\n    # http://docs.chainer.org/en/latest/reference/optimizers.html\n    optimizer = chainer.optimizers.SGD()\n    optimizer.setup(model)\n\n    # Set up trainer\n    updater = training.StandardUpdater(train_iter, optimizer, device=args.gpu)\n    trainer = training.Trainer(updater, (args.epoch, \'epoch\'))\n\n    # Evaluate the model at end of each epoch\n    trainer.extend(extensions.Evaluator(test_iter, model, device=args.gpu))\n\n    # Helper functions (extensions) to monitor progress on stdout.\n    report_params = [\n        \'epoch\',\n        \'main/loss\',\n        \'validation/main/loss\',\n        \'main/accuracy\',\n        \'validation/main/accuracy\',\n        \'elapsed_time\'\n    ]\n    trainer.extend(extensions.LogReport())\n    trainer.extend(extensions.PrintReport(report_params))\n    trainer.extend(extensions.ProgressBar())\n\n    # Run trainer\n    trainer.run()\n\n\nif __name__ == ""__main__"":\n    main()\n\n\n""""""\n# Expected output with 1 gpu.\n\nepoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n...\n90          0.285929    0.277819              0.918177       0.921084                  198.132\n91          0.285159    0.277838              0.918077       0.920194                  200.3\n92          0.28487     0.277246              0.918453       0.9196                    202.472\n93          0.284443    0.276658              0.918643       0.920589                  204.645\n94          0.283882    0.276925              0.918877       0.920985                  206.818\n95          0.283553    0.276153              0.91906        0.920688                  209.031\n96          0.283272    0.275503              0.919071       0.921282                  211.219\n97          0.282494    0.274468              0.91941        0.921084                  213.428\n98          0.282246    0.274534              0.91936        0.921381                  215.617\n99          0.2818      0.274671              0.919543       0.921875                  217.821\n100         0.281342    0.27406               0.919772       0.922567                  220.023\n""""""\n'"
chainer/chlab-10-3-mnist_modern.py,0,"b'#!/usr/bin/env python\n# Lab 10 MNIST and MLP with dropout\n\nimport argparse\nimport numpy as np\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\n\nfrom chainer import training, Variable\nfrom chainer import datasets, iterators, optimizers\nfrom chainer import Chain\nfrom chainer.training import extensions\n\n\nclass ModernMLP(chainer.Chain):\n    # Define model to be called later by L.Classifer()\n    # Basic MLP\n\n    def __init__(self, n_units, n_out):\n        super(ModernMLP, self).__init__(\n            l1=L.Linear(None, n_units),\n            l2=L.Linear(None, n_units),\n            l3=L.Linear(None, n_out)\n        )\n\n    def __call__(self, x):\n        # Add dropout, and use ReLU for activation function.\n        # dropout:\n        # This function drops input elements randomly with probability\n        # ``ratio`` and scales the remaining elements by factor\n        # ``1 / (1 - ratio)``. In testing mode, it does nothing and\n        # just returns ``x``.\n        # source: http://docs.chainer.org/en/latest/_modules/chainer/functions/noise/dropout.html?highlight=dropout\n        h = F.dropout(F.relu(self.l1(x)), ratio=0.3, train=True)\n        h = F.dropout(F.relu(self.l2(h)), ratio=0.3, train=True)\n        return self.l3(h)\n\n\ndef main():\n    # Introduce argparse for clarity and organization.\n    # Starting to use higher capacity models, thus set up for GPU.\n    parser = argparse.ArgumentParser(description=\'Chainer-Tutorial: MLP\')\n    parser.add_argument(\'--batch_size\', \'-b\', type=int, default=128,\n                        help=\'Number of samples in each mini-batch\')\n    parser.add_argument(\'--epoch\', \'-e\', type=int, default=100,\n                        help=\'Number of times to train on data set\')\n    parser.add_argument(\'--gpu\', \'-g\', type=int, default=-1,\n                        help=\'GPU ID: -1 indicates CPU\')\n    parser.add_argument(\'--frequency\', \'-f\', type=int, default=-1,\n                        help=\'Frequency of taking a snapshot\')\n    parser.add_argument(\'--resume\', \'-r\', default=\'\',\n                        help=\'Resume the training from snapshot\')\n    args = parser.parse_args()\n\n    # Load mnist data\n    # http://docs.chainer.org/en/latest/reference/datasets.html\n    train, test = chainer.datasets.get_mnist()\n\n    # Define iterators.\n    train_iter = chainer.iterators.SerialIterator(train, args.batch_size)\n    test_iter = chainer.iterators.SerialIterator(test, args.batch_size,\n                                                 repeat=False, shuffle=False)\n\n    # Initialize model: Loss function defaults to softmax_cross_entropy.\n    # 784 is dimension of the inputs, 625 is n_units in hidden layer\n    # and 10 is the output dimension.\n    model = L.Classifier(ModernMLP(625, 10))\n\n    # Set up GPU usage if necessary. args.gpu is a condition as well as an\n    # identification when passed to get_device().\n    if args.gpu >= 0:\n        chainer.cuda.get_device(args.gpu).use()\n        model.to_gpu()\n\n    # Define optimizer (SGD, Adam, RMSprop, etc)\n    # http://docs.chainer.org/en/latest/reference/optimizers.html\n    # RMSprop default parameter setting:\n    # lr=0.01, alpha=0.99, eps=1e-8\n    optimizer = chainer.optimizers.RMSprop()\n    optimizer.setup(model)\n\n    # Set up trainer\n    updater = training.StandardUpdater(train_iter, optimizer, device=args.gpu)\n    trainer = training.Trainer(updater, (args.epoch, \'epoch\'))\n\n    # Evaluate the model at end of each epoch\n    trainer.extend(extensions.Evaluator(test_iter, model, device=args.gpu))\n\n    # Dump a computational graph from \'loss\' variable at the first iteration\n    # The ""main"" refers to the target link of the ""main"" optimizer.\n    trainer.extend(extensions.dump_graph(\'main/loss\'))\n\n    # Helper functions (extensions) to monitor progress on stdout.\n    report_params = [\n        \'epoch\',\n        \'main/loss\',\n        \'validation/main/loss\',\n        \'main/accuracy\',\n        \'validation/main/accuracy\',\n        \'elapsed_time\'\n    ]\n    trainer.extend(extensions.LogReport())\n    trainer.extend(extensions.PrintReport(report_params))\n    trainer.extend(extensions.ProgressBar())\n\n    # Here we add a bit more boiler plate code to help in output of useful\n    # information in related to training. Very intuitive and great for post\n    # analysis.\n    # source:\n    # https://github.com/pfnet/chainer/blob/master/examples/mnist/train_mnist.py\n\n    # Take a snapshot for each specified epoch\n    frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)\n    trainer.extend(extensions.snapshot(), trigger=(frequency, \'epoch\'))\n\n    # Write a log of evaluation statistics for each epoch\n    trainer.extend(extensions.LogReport())\n\n    # Save two plot images to the result dir\n    if extensions.PlotReport.available():\n        trainer.extend(\n            extensions.PlotReport(\n                [\'main/loss\', \'validation/main/loss\'],\n                \'epoch\', file_name=\'loss.png\'))\n        trainer.extend(\n            extensions.PlotReport(\n                [\'main/accuracy\', \'validation/main/accuracy\'],\n                \'epoch\', file_name=\'accuracy.png\'))\n\n    if args.resume:\n        # Resume from a snapshot (NumPy NPZ format and HDF5 format available)\n        # http://docs.chainer.org/en/latest/reference/serializers.html\n        chainer.serializers.load_npz(args.resume, trainer)\n\n    # Run trainer\n    trainer.run()\n\n\nif __name__ == ""__main__"":\n    main()\n\n\n""""""\nExpected output with 1 gpu.\n\nepoch       main/loss   validation/main/loss  main/accuracy validation/main/accuracy  elapsed_time\n...\n90          0.217452    0.965264              0.958189       0.941456                 294.61\n91          0.196134    1.14531               0.959089       0.944917                 297.859\n92          0.203648    0.956059              0.957148       0.943928                 301.109\n93          0.20284     1.02199               0.960021       0.948378                 304.362\n94          0.195888    1.18072               0.958905       0.945609                 307.619\n95          0.199831    1.2245                0.958356       0.94195                  310.879\n96          0.200486    1.10434               0.960186       0.943038                 314.151\n97          0.202059    1.43919               0.960421       0.943335                 317.447\n98          0.221666    0.947955              0.959305       0.946994                 320.745\n99          0.200717    1.35896               0.961504       0.943137                 324.038\n100         0.182234    0.935365              0.962039       0.946301                 327.323\n""""""\n'"
keras/klab-02-1-linear_regression.py,0,"b""import numpy as np\nfrom keras import optimizers\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\n\nx_train = [1, 2, 3, 4]\ny_train = [0, -1, -2, -3]\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=1))\n\nsgd = optimizers.SGD(lr=0.1)\nmodel.compile(loss='mse', optimizer=sgd)\n\n# prints summary of the model to the terminal\nmodel.summary()\n\nmodel.fit(x_train, y_train, epochs=200)\n\ny_predict = model.predict(np.array([5]))\nprint(y_predict)\n"""
keras/klab-04-1-multi_input_linear_regression.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import RMSprop\nimport numpy as np\n\nx_data = np.array([[73., 80., 75.],\n          [93., 88., 93.],\n          [89., 91., 90.],\n          [96., 98., 100.],\n          [73., 66., 70.]])\ny_data = np.array([[152.],\n          [185.],\n          [180.],\n          [196.],\n          [142.]])\n\nprint(x_data.shape)\nprint(y_data.shape)          \n\nmodel = Sequential()\nmodel.add(Dense(input_dim=3, units=1))\nmodel.add(Activation('linear'))\n\nrmsprop = RMSprop(lr=0.1)\nmodel.compile(loss='mse', optimizer=rmsprop,  metrics=['accuracy'])\nmodel.fit(x_data, y_data, epochs=1000)\n\ny_predict = model.predict(np.array([[95., 100., 80]]))\nprint(y_predict)"""
keras/klab-04-2-multi_input_linear_regression.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\n\nx_data = np.array([[73., 80., 75.],\n          [93., 88., 93.],\n          [89., 91., 90.],\n          [96., 98., 100.],\n          [73., 66., 70.]])\ny_data = np.array([[152.],\n          [185.],\n          [180.],\n          [196.],\n          [142.]])\n\nmodel = Sequential()\nmodel.add(Dense(input_dim=3, units=1))\n\nmodel.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_data, y_data, epochs=500)\n\ny_predict = model.predict(np.array([[0, 2, 1]]))\nprint(y_predict)"""
keras/klab-04-3-file_input_linear_regression.py,0,"b'from keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\nxy = np.loadtxt(\'data-01-test-score.csv\', delimiter=\',\')\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(""x_data"", x_data)\nprint(""y_data"", y_data)\n\nmodel = Sequential()\nmodel.add(Dense(input_dim=3, units=1))\n\nmodel.compile(loss=\'mse\', optimizer=\'rmsprop\')\nmodel.fit(x_data, y_data, epochs=2000)\n\nprint(""0, 2, 1"", model.predict(np.array([[0, 2, 1]])))\nprint(""0, 9, -1"", model.predict(np.array([[0, 9, -1]])))\n'"
keras/klab-04-4-stock_linear_regression.py,0,"b'from keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\n\nxy = np.loadtxt(\'data-02-stock_daily.csv\', delimiter=\',\')\n\n# very important. It does not work without it.\nscaler = MinMaxScaler(feature_range=(0, 1))\nxy = scaler.fit_transform(xy)\n\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\n# before deletion\nprint(x_data[0], y_data[0])\nprint(x_data[1], y_data[1])\n\n# predict tomorrow\nx_data = np.delete(x_data, -1, 0)\ny_data = np.delete(y_data, 0)\n\nprint(""== Predict tomorrow"")\nprint(x_data[0], ""->"", y_data[0])\n\nmodel = Sequential()\nmodel.add(Dense(input_dim=4, units=1))\n\nmodel.compile(loss=\'mse\', optimizer=\'sgd\', metrics=[\'mse\'])\nmodel.fit(x_data, y_data, epochs=100)\n\ntest = x_data[10].reshape(-1, 4)\nprint(""y="", y_data[10], ""prediction="", model.predict(test))\n\ntest = x_data[30].reshape(-1, 4)\nprint(""y="", y_data[30], ""prediction="", model.predict(test))\n\n# ---------------------------\n# Test\n# split to train and testing\n\ntrain_size = int(len(x_data) * 0.7)\ntest_size = len(x_data) - train_size\nx_train, x_test = x_data[0:train_size], x_data[train_size:len(x_data)]\ny_train, y_test = y_data[0:train_size], y_data[train_size:len(y_data)]\n\nmodel = Sequential()\nmodel.add(Dense(input_dim=4, units=1))\nmodel.compile(loss=\'mse\', optimizer=\'sgd\', metrics=[\'mse\'])\n\n# Train a model\nmodel.fit(x_train, y_train, epochs=200)\n\n# evaluate\nresults = model.evaluate(x_test, y_test, verbose=1)\nprint(results)\n\npredictions = model.predict(x_test)\n\nplt.plot(y_test)\nplt.plot(predictions)\nplt.show()\n'"
keras/klab-05-1-logistic_regression.py,0,"b'from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nimport numpy as np\n\nx_data = np.array([[1, 2],\n          [2, 3],\n          [3, 1],\n          [4, 3],\n          [5, 3],\n          [6, 2]])\ny_data = np.array([[0],\n          [0],\n          [0],\n          [1],\n          [1],\n          [1]])\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=2, activation=\'sigmoid\'))\n\nsgd = SGD(lr=0.1)\nmodel.compile(loss=\'binary_crossentropy\', optimizer=sgd, metrics=[\'accuracy\'])\n\nmodel.summary()\nmodel.fit(x_data, y_data, epochs=2000)\n\nprint(""2,1"", model.predict_classes(np.array([[2, 1]])))\nprint(""6,5"", model.predict_classes(np.array([[6, 5]])))'"
keras/klab-05-2-logistic_regression_diabetes.py,0,"b'from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nimport numpy as np\n\nx_data = np.array([[1, 2],\n          [2, 3],\n          [3, 1],\n          [4, 3],\n          [5, 3],\n          [6, 2]])\ny_data = np.array([[0],\n          [0],\n          [0],\n          [1],\n          [1],\n          [1]])\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=2, activation=\'sigmoid\'))\n\nsgd = SGD(lr=0.1)\nmodel.compile(loss=\'binary_crossentropy\', optimizer=sgd)\n\nmodel.summary()\nmodel.fit(x_data, y_data, epochs=2000)\n\nprint(""2,1"", model.predict_classes(np.array([[2, 1]])))\nprint(""6,5"", model.predict_classes(np.array([[6, 5]])))\n'"
keras/klab-06-1-softmax.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nimport numpy as np\n\nx_data = np.array([[1, 2, 1, 1],\n                   [2, 1, 3, 2],\n                   [3, 1, 3, 4],\n                   [4, 1, 5, 5],\n                   [1, 7, 5, 5],\n                   [1, 2, 5, 6],\n                   [1, 6, 6, 6],\n                   [1, 7, 7, 7]],\n                  dtype=np.float32)\ny_data = np.array([[0, 0, 1],\n                   [0, 0, 1],\n                   [0, 0, 1],\n                   [0, 1, 0],\n                   [0, 1, 0],\n                   [0, 1, 0],\n                   [1, 0, 0],\n                   [1, 0, 0]],\n                  dtype=np.float32)\n\nnb_classes = 3\n\nmodel = Sequential()\nmodel.add(Dense(3, input_shape=(4,)))\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n\nhistory = model.fit(x_data, y_data, epochs=1000)\n\nprint(model.predict_classes(np.array([[1, 2, 1, 1]])))\nprint(model.predict_classes(np.array([[1, 2, 5, 6]])))\n"""
keras/klab-06-2-softmax_zoo.py,0,"b'# https://github.com/fchollet/keras/tree/master/examples\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.utils import np_utils\nimport numpy as np\n\n# Predicting animal type based on various features\nxy = np.loadtxt(\'data-04-zoo.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]] - 1\nprint(x_data.shape, y_data.shape)\n\nnb_classes = 7\ny_one_hot = np_utils.to_categorical(y_data, nb_classes)\n\nmodel = Sequential()\nmodel.add(Dense(nb_classes, input_shape=(16,)))\nmodel.add(Activation(\'softmax\'))\n\nmodel.summary()\n\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'sgd\',\n              metrics=[\'accuracy\'])\n\nhistory = model.fit(x_data, y_one_hot, epochs=1000)\n\n# Let\'s see if we can predict\npred = model.predict_classes(x_data)\nfor p, y in zip(pred, y_data):\n    print(""prediction: "", p, "" true Y: "", y)\n'"
keras/klab-07-1-learning_rate_and_evaluation.py,0,"b""# Lab 7 Learning rate and Evaluation\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import SGD\nimport numpy as np\nnp.random.seed(777)  # for reproducibility\n\nx_data = np.array([[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5],\n          [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]])\ny_data = np.array([[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],\n          [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]])\n\n# Evaluation our model using this test dataset\nx_test = np.array([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\ny_test = np.array([[0, 0, 1], [0, 0, 1], [0, 0, 1]])\n\n\nmodel = Sequential()\nmodel.add(Dense(3, input_dim=3))\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=SGD(lr=0.1),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_data, y_data, epochs=200)\n\npredictions = model.predict(x_test)\nscore = model.evaluate(x_test, y_test)\n\nprint('Prediction: ', [np.argmax(prediction) for prediction in predictions])\nprint('Accuracy: ', score[1])\n"""
keras/klab-07-2-linear_regression_without_min_max.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import SGD\nimport numpy as np\nnp.random.seed(777)  # for reproducibility\n\n\nxy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n               [816, 820.958984, 1008100, 815.48999, 819.23999],\n               [819.359985, 823, 1188100, 818.469971, 818.97998],\n               [819, 823, 1198100, 816, 820.450012],\n               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=4))\nmodel.add(Activation('linear'))\n\nmodel.summary()\n\nmodel.compile(loss='mse',\n              optimizer=SGD(lr=1e-5),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_data, y_data, epochs=100)\n\npredictions = model.predict(x_data)\nscore = model.evaluate(x_data, y_data)\n\nprint('Prediction: ', predictions)\nprint('Accuracy: ', score[1])\n"""
keras/klab-07-3-linear_regression_min_max.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nnp.random.seed(777)  # for reproducibility\n\n\nxy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n               [816, 820.958984, 1008100, 815.48999, 819.23999],\n               [819.359985, 823, 1188100, 818.469971, 818.97998],\n               [819, 823, 1198100, 816, 820.450012],\n               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n\n# very important. It does not work without it.\nscaler = MinMaxScaler(feature_range=(0, 1))\nxy = scaler.fit_transform(xy)\nprint(xy)\n\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=4))\nmodel.add(Activation('linear'))\n\nmodel.summary()\n\nmodel.compile(loss='mse',\n              optimizer='sgd',\n              metrics=['accuracy'])\n\nhistory = model.fit(x_data, y_data, epochs=100)\n\npredictions = model.predict(x_data)\nscore = model.evaluate(x_data, y_data)\n\nprint('Prediction: \\n', predictions)\nprint('Cost: ', score[0])\n"""
keras/klab-07-4-mnist_introduction.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.utils import np_utils\nimport numpy as np\nnp.random.seed(777)  # for reproducibility\n\nfrom keras.datasets import mnist\n\nnb_classes = 10\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# for Using TensorFlow backend.\nx_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\nx_train = x_train.astype('float32') / 255\n# one_hot\ny_train = np_utils.to_categorical(y_train, nb_classes)\n\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\nx_test = x_test.astype('float32') / 255\n# one_hot\ny_test = np_utils.to_categorical(y_test, nb_classes)\n\nmodel = Sequential()\n# MNIST data image of shape 28 * 28 = 784\nmodel.add(Dense(nb_classes, input_dim=784))\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n\n\nhistory = model.fit(x_train, y_train, epochs=15)\nscore = model.evaluate(x_test, y_test)\nprint('\\nAccuracy:', score[1])\n\n'''\nAccuracy: 0.9192\n'''\n"""
keras/klab-09-1-xor.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\nx_data = np.array([[0., 0.],\n          [0., 1.],\n          [1., 0.],\n          [1., 1.]])\ny_data = np.array([[0.],\n          [1.],\n          [1.],\n          [0.]])\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=2, activation='sigmoid'))\nsgd = SGD(lr=0.1)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd,\n              metrics=['accuracy'])\nmodel.summary()\nmodel.fit(x_data, y_data, epochs=50000)\n\nscore = model.evaluate(x_data, y_data, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n"""
keras/klab-09-2-xor-nn.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\nx_data = np.array([[0., 0.],\n          [0., 1.],\n          [1., 0.],\n          [1., 1.]])\ny_data = np.array([[0.],\n          [1.],\n          [1.],\n          [0.]])\n\nmodel = Sequential()\nmodel.add(Dense(2, input_dim=2, activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\nsgd = SGD(lr=0.1)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd,\n              metrics=['accuracy'])\n\nmodel.summary()\nmodel.fit(x_data, y_data, epochs=50000)\n\nprint(model.predict_classes(x_data))\n\nscore = model.evaluate(x_data, y_data, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n"""
keras/klab-10-1-mnist_softmax.py,0,"b""from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import Adam\nfrom keras import initializers\nfrom keras.utils import np_utils\nfrom keras import backend as K\n\nfrom keras.datasets import mnist\n\n# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n# more information about the mnist dataset\n\n# parameters\nlearning_rate = 0.001\nbatch_size = 100\ntraining_epochs = 15\nnb_classes = 10\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nif K.image_dim_ordering() == 'th':\n    X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols)\n    X_test = X_test.reshape(X_test.shape[0], img_rows * img_cols)\nelse:\n    X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols)\n    X_test = X_test.reshape(X_test.shape[0], img_rows * img_cols)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nmodel = Sequential()\nmodel.add(Dense(units=nb_classes, input_dim=img_rows * img_cols,\n                kernel_initializer=initializers.random_normal(stddev=0.01),\n                use_bias=True))\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\nadam = Adam(lr=learning_rate)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=adam,\n              metrics=['accuracy'])\n\nmodel.fit(X_train, Y_train, batch_size=batch_size, epochs=training_epochs)\n\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\n'''\nTest score: 0.265781824633\nTest accuracy: 0.9268\n'''\n"""
keras/klab-10-2-X1-mnist_conv_model_with_ensemble.py,0,"b""#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import *\nimport numpy as np\nfrom keras.utils import to_categorical\n\n\n# In[2]:\n#load mnist data\n\n(train_data,test_data) = mnist.load_data()\n\n\n# In[3]:\n#preprocess mnist data\n\nx_train = train_data[0]\nx_train =np.asarray([np.reshape(data,(28,28,1)) for data in x_train])\ny_train = to_categorical(train_data[1])\n\n\nx_test = test_data[0]\nx_test = np.asarray([np.reshape(data,(28,28,1)) for data in x_test])\ny_test_pure = test_data[1]\ny_test =to_categorical(test_data[1])\n\n\n# In[4]:\n#defined model \n\ndef make_conv_model_list():\n    model = Sequential()\n    model.add(Conv2D(32,(3,3),use_bias=True,activation='relu',padding='same',input_shape=(28,28,1)))\n    model.add(MaxPool2D(2,2))\n    model.add(ZeroPadding2D(padding=(1, 1), data_format=None))\n    model.add(Conv2D(64,(3,3),use_bias=True,padding='same',activation='relu'))\n    model.add(MaxPool2D(2,2))\n    model.add(Flatten())\n    model.add(Dense(64,activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc'])\n    model.summary()\n    return model\n\n\n# In[5]:\n#make  ensemble list 5\n\nensemble_model_list = []\nensemble_size=5\n\nfor i in range(ensemble_size):\n    this_model = make_conv_model_list()\n    print('i_th model training')\n    this_model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=5,\n          verbose=1,\n          validation_data=(x_test, y_test))\n    \n    ensemble_model_list.append(this_model)\n\n\n# In[6]:\n\n#results of all models\n    \nall_predicted = np.zeros(y_test.shape)\n\n\nfor one in ensemble_model_list:\n    all_predicted =all_predicted+one.predict(x_test)\n\n\n#get final res\n\nfinal_res = []\nfor one in all_predicted:\n    final_res.append(np.argmax(one))\n\n\n# In[9]:\n\n\nfinal_res = np.asarray(final_res)\n\n\n# In[10]:\n\n\nentire = y_test_pure.shape[0]\n\n\n# In[11]:\n\n\nele = 0\nfor one in range(entire):\n    if(y_test_pure[one]==final_res[one]):\n        ele=ele+1\n    \nres_acc = ele/entire\n\n\n# In[12]:\n\n\nprint('fianl result test accuracy:',res_acc)\n\n#99.3%\n\n\n\n"""
keras/klab-10-2-mnist_nn.py,0,"b""from __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n\n# ==============================================================================\n# prepare data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\n# ==============================================================================\n# build model\n# (model code from http://iostream.tistory.com/111)\nmodel = Sequential()\n\nmodel.add(Dense(256, input_dim=784,\n                kernel_initializer='glorot_uniform', activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(256, kernel_initializer='glorot_uniform', activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(256, kernel_initializer='glorot_uniform', activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(256, kernel_initializer='glorot_uniform', activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_split=0.2)\n\n\n# ==============================================================================\n# predict\nscore = model.evaluate(X_test, y_test, batch_size=batch_size)\nprint('\\nTest loss:', score[0])\nprint('Test accuracy:', score[1])\n'''\nTest loss: 0.0742975851574\nTest accuracy: 0.9811\n'''\n"""
keras/klab-11-1-cnn_mnist.py,0,"b""# https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n'''Trains a simple convnet on the MNIST dataset.\nGets to 99.25% test accuracy after 12 epochs\n(there is still a lot of margin for parameter tuning).\n16 seconds per epoch on a GRID K520 GPU.\n'''\n\nfrom __future__ import print_function\nimport numpy as np\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\n\nnp.random.seed(1337)  # for reproducibility\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nnb_filters = 32\n# size of pooling area for max pooling\npool_size = (2, 2)\n# convolution kernel size\nkernel_size = (3, 3)\n\n# the data, shuffled and split between train and test sets\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nif K.image_dim_ordering() == 'th':\n    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(nb_filters, kernel_size, padding='valid', input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(nb_filters, kernel_size))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=pool_size))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n          verbose=1, validation_data=(X_test, Y_test))\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n"""
keras/klab-11-X-cnn_cifar100_with_img_preprocess_and augumentation.py,0,"b'from keras.datasets import cifar100\nfrom keras.models import *\nfrom keras.layers import *\nimport numpy as np\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import *\nfrom keras.preprocessing import image\nfrom keras import regularizers,optimizers\nfrom keras.callbacks import LearningRateScheduler\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\n \ndef lr_schedule(epoch):\n    lrate = 0.001\n    if epoch < 2:\n        lrate = 0.005\n    if epoch > 5:\n        lrate = 0.0001\n    return lrate\n \n(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n#c\ny_train=to_categorical(y_train)\ny_test=to_categorical(y_test)\n\n#image augumentation\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator( rotation_range=30,\n                 width_shift_range=0.1, height_shift_range=0.1,\n                 horizontal_flip=True)\ndatagen.fit(x_train)\n\n#cofirm train data\ndef show_imgs(X):\n    plt.figure(1)\n    k = 0\n    for i in range(0,4):\n        for j in range(0,4):\n            plt.subplot2grid((4,4),(i,j))\n            plt.imshow((X[k]))\n            k = k+1\n    # show the plot\n    plt.show()\nshow_imgs(x_train[:16])\n\n\n#modeling\nmodel  = Sequential()\nmodel.add(Conv2D(32,(2,2),padding=\'same\',input_shape=(32,32,3),activation=\'elu\'))\nmodel.add(MaxPool2D(2,2))    \nmodel.add(Conv2D(64, (2, 2), padding=\'same\',activation=\'elu\'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(2,2))\nmodel.add(Flatten())\nmodel.add(Dense(128,activation=\'relu\'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100,activation=\'softmax\'))\nmodel.compile(loss=\'binary_crossentropy\', optimizer=optimizers.rmsprop(lr=0.001, decay=0.00005), metrics=[\'accuracy\'])\n\n#model structure\nprint(model.summary())\n\ncallbacks = [EarlyStopping(monitor=\'val_loss\', patience=1, mode=\'min\', verbose=1)]\n\nmodel.fit(x_train, y_train,epochs=10,validation_split=0.2,verbose=1,callbacks=callbacks)\n\nres_acc = model.evaluate(x_test,y_test)\n\n#res_acc: 0.9901830131530762\nprint(""res_acc:"",res_acc[1])\n#res_acc: 0.043311127352714536\nprint(""res_socre:"",res_acc[0])'"
keras/klab-12-1-rnn_hello_char.py,0,"b'import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, TimeDistributed, Activation, LSTM\nfrom keras.utils import np_utils\n\nimport os\n\n# brew install graphviz\n# pip3 install graphviz\n# pip3 install pydot-ng\nfrom keras.utils.vis_utils import plot_model\n\n# sample text\nsample = ""hihello""\n\nchar_set = list(set(sample))  # id -> char [\'i\', \'l\', \'e\', \'o\', \'h\']\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\nx_str = sample[:-1]\ny_str = sample[1:]\n\ndata_dim = len(char_set)\ntimesteps = len(y_str)\nnum_classes = len(char_set)\n\nprint(x_str, y_str)\n\nx = [char_dic[c] for c in x_str]  # char to index\ny = [char_dic[c] for c in y_str]  # char to index\n\n# One-hot encoding\nx = np_utils.to_categorical(x, num_classes=num_classes)\n# reshape X to be [samples, time steps, features]\nx = np.reshape(x, (-1, len(x), data_dim))\nprint(x.shape)\n\n# One-hot encoding\ny = np_utils.to_categorical(y, num_classes=num_classes)\n# time steps\ny = np.reshape(y, (-1, len(y), data_dim))\nprint(y.shape)\n\nmodel = Sequential()\nmodel.add(LSTM(num_classes, input_shape=(\n    timesteps, data_dim), return_sequences=True))\nmodel.add(TimeDistributed(Dense(num_classes)))\nmodel.add(Activation(\'softmax\'))\nmodel.summary()\n# Store model graph in png\n# (Error occurs on in python interactive shell)\nplot_model(model, to_file=os.path.basename(__file__) + \'.png\', show_shapes=True)\n\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'rmsprop\', metrics=[\'accuracy\'])\nmodel.fit(x, y, epochs=1)\n\npredictions = model.predict(x, verbose=0)\nfor i, prediction in enumerate(predictions):\n    print(prediction)\n    x_index = np.argmax(x[i], axis=1)\n    x_str = [char_set[j] for j in x_index]\n    print(x_index, \'\'.join(x_str))\n\n    index = np.argmax(prediction, axis=1)\n    result = [char_set[j] for j in index]\n    print(index, \'\'.join(result))\n'"
keras/klab-12-1-rnn_hello_char_acc1.0.py,0,"b'# -*- coding: utf-8 -*-\n""""""12_1_2_hello_rnn_keras.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/12FLhRlwyQNX-XLx1Ix6BfNkCCga7xZiY\n""""""\n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, TimeDistributed, Activation, LSTM\nfrom keras.utils import np_utils\n\nimport os\n\n# brew install graphviz\n# pip3 install graphviz\n# pip3 install pydot-ng\nfrom keras.utils.vis_utils import plot_model\n\n# sample text\nsample = ""hihello""\n\nchar_set = list(set(sample))  # id -> char [\'i\', \'l\', \'e\', \'o\', \'h\']\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\nx_str = sample[:-1]\ny_str = sample[1:]\n\ndata_dim = len(char_set)\ntimesteps = len(y_str)\nnum_classes = len(char_set)\n\nprint(data_dim)\nprint(timesteps)\nprint(num_classes)\nprint(x_str, y_str)\n\nx = [char_dic[c] for c in x_str]  # char to index\ny = [char_dic[c] for c in y_str]  # char to index\nprint(x)\nprint(y)\n\n# One-hot encoding\nx = np_utils.to_categorical(x, num_classes=num_classes)\n# reshape X to be [samples, time steps, features]\nx = np.reshape(x, (-1, len(x), data_dim))\nprint(x.shape)\n\n# One-hot encoding\ny = np_utils.to_categorical(y, num_classes=num_classes)\n# time steps\ny = np.reshape(y, (-1, len(y), data_dim))\nprint(y.shape)\n\nmodel = Sequential()\n\n#model.add(LSTM(num_classes, input_shape=(timesteps, data_dim), return_sequences=True))\n\n#wise and deep\xed\x95\x98\xea\xb2\x8c layer\xeb\xa5\xbc \xea\xb5\xac\xec\x84\xb1\xed\x95\x98\xeb\xa9\xb4 200 epochs\xec\x95\x88\xec\x97\x90 1.0\xec\x9d\x98 acc \xeb\x82\x98\xec\x98\xb4\nmodel.add(LSTM(num_classes*128, input_shape=(timesteps, data_dim), return_sequences=True))\nmodel.add(LSTM(num_classes*64, return_sequences=True))\nmodel.add(LSTM(num_classes*16, return_sequences=True))\nmodel.add(LSTM(num_classes*4, return_sequences=True))\nmodel.add(TimeDistributed(Dense(num_classes)))\nmodel.add(Activation(\'softmax\')) \nmodel.summary()\n\n# Store model graph in png\n# (Error occurs on in python interactive shell)\n__file__ = \'hello-rnn-keras-img\'\nplot_model(model, to_file=os.path.basename(__file__) + \'.png\', show_shapes=True)\n\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\', metrics=[\'accuracy\'])\nhistory = model.fit(x, y, epochs=200)\n\npredictions = model.predict(x, verbose=0)\n\nfor i, prediction in enumerate(predictions):\n    print(prediction)\n    x_index = np.argmax(x[i], axis=1)\n    x_str = [char_set[j] for j in x_index]\n    print(x_index, \'\'.join(x_str))\n\n    index = np.argmax(prediction, axis=1)\n    result = [char_set[j] for j in index]\n    print(index, \'\'.join(result))\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history[\'loss\'])\nplt.show()'"
keras/klab-12-1-softmax_hello_char.py,0,"b'import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\n\nimport os\n\n# brew install graphviz\n# pip3 install graphviz\n# pip3 install pydot-ng\nfrom keras.utils.vis_utils import plot_model\n\n# sample text\nsample = ""hihello""\n\nchar_set = list(set(sample))  # id -> char [\'i\', \'l\', \'e\', \'o\', \'h\']\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\nx_str = sample[:-1]\ny_str = sample[1:]\n\ndata_dim = len(char_set)\ntimesteps = len(y_str)\nnum_classes = len(char_set)\n\nprint(x_str, y_str)\n\nx = [char_dic[c] for c in x_str]  # char to index\ny = [char_dic[c] for c in y_str]  # char to index\n\n# One-hot encoding\nx = np_utils.to_categorical(x, num_classes=num_classes)\n# reshape X to be [samples, time steps, features]\nx = np.reshape(x, (-1, len(x), data_dim))\nprint(x.shape)\n\n# One-hot encoding\ny = np_utils.to_categorical(y, num_classes=num_classes)\n# time steps\ny = np.reshape(y, (-1, len(y), data_dim))\nprint(y.shape)\n\nmodel = Sequential()\nmodel.add(Dense(num_classes, input_shape=(\n    timesteps, data_dim)))\nmodel.add(Activation(\'softmax\'))\nmodel.summary()\n# Store model graph in png\n# (Error occurs on in python interactive shell)\nplot_model(model, to_file=os.path.basename(__file__) + \'.png\', show_shapes=True)\n\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'rmsprop\', metrics=[\'accuracy\'])\nmodel.fit(x, y, epochs=100)\n\npredictions = model.predict(x, verbose=0)\nfor i, prediction in enumerate(predictions):\n    x_index = np.argmax(x[i], axis=1)\n    x_str = [char_set[j] for j in x_index]\n    print(x_index, \'\'.join(x_str))\n\n    index = np.argmax(prediction, axis=1)\n    result = [char_set[j] for j in index]\n    print(index, \'\'.join(result))\n'"
keras/klab-12-2-rnn_long_char.py,0,"b'import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, TimeDistributed, Activation, LSTM\nfrom keras.utils import np_utils\n\nimport os\n\n# brew install graphviz\n# pip3 install graphviz\n# pip3 install pydot-ng\nfrom keras.utils.vis_utils import plot_model\n\n# sample sentence\nsentence = (""if you want to build a ship, don\'t drum up people together to ""\n            ""collect wood and don\'t assign them tasks and work, but rather ""\n            ""teach them to long for the endless immensity of the sea."")\n\nchar_set = list(set(sentence))  # id -> char [\'i\', \'l\', \'e\', \'o\', \'h\', ...]\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\ndata_dim = len(char_set)\nseq_length = timesteps = 10\nnum_classes = len(char_set)\n\ndataX = []\ndataY = []\nfor i in range(0, len(sentence) - seq_length):\n    x_str = sentence[i:i + seq_length]\n    y_str = sentence[i + 1: i + seq_length + 1]\n    print(x_str, \'->\', y_str)\n\n    x = [char_dic[c] for c in x_str]  # char to index\n    y = [char_dic[c] for c in y_str]  # char to index\n\n    dataX.append(x)\n    dataY.append(y)\n\n# One-hot encoding\ndataX = np_utils.to_categorical(dataX, num_classes=num_classes)\n# reshape X to be [samples, time steps, features]\ndataX = np.reshape(dataX, (-1, seq_length, data_dim))\nprint(dataX.shape)\n\n# One-hot encoding\ndataY = np_utils.to_categorical(dataY, num_classes=num_classes)\n# time steps\ndataY = np.reshape(dataY, (-1, seq_length, data_dim))\nprint(dataY.shape)\n\nmodel = Sequential()\nmodel.add(LSTM(num_classes, input_shape=(\n    timesteps, data_dim), return_sequences=True))\nmodel.add(LSTM(num_classes, return_sequences=True))\nmodel.add(TimeDistributed(Dense(num_classes)))\n\nmodel.add(Activation(\'softmax\'))\nmodel.summary()\n\n# Store model graph in png\n# (Error occurs on in python interactive shell)\nplot_model(model, to_file=os.path.basename(__file__) + \'.png\', show_shapes=True)\n\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'rmsprop\', metrics=[\'accuracy\'])\nmodel.fit(dataX, dataY, epochs=1000)\n\npredictions = model.predict(dataX, verbose=0)\nfor i, prediction in enumerate(predictions):\n    # print(prediction)\n    x_index = np.argmax(dataX[i], axis=1)\n    x_str = [char_set[j] for j in x_index]\n\n    index = np.argmax(prediction, axis=1)\n    result = [char_set[j] for j in index]\n\n    print(\'\'.join(x_str), \' -> \', \'\'.join(result))\n'"
keras/klab-12-3-rnn_stock_prediction.py,0,"b'# http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\n\n# brew install graphviz\n# pip3 install graphviz\n# pip3 install pydot-ng\nfrom keras.utils.vis_utils import plot_model\n\nimport matplotlib.pyplot as plt\n\ntimesteps = seq_length = 7\ndata_dim = 5\n\n# Open,High,Low,Close,Volume\nxy = np.loadtxt(\'data-02-stock_daily.csv\', delimiter=\',\')\nxy = xy[::-1]  # reverse order (chronically ordered)\n\n# very important. It does not work without it.\nscaler = MinMaxScaler(feature_range=(0, 1))\nxy = scaler.fit_transform(xy)\n\nx = xy\ny = xy[:, [-1]]  # Close as label\n\ndataX = []\ndataY = []\nfor i in range(0, len(y) - seq_length):\n    _x = x[i:i + seq_length]\n    _y = y[i + seq_length]  # Next close price\n    print(_x, ""->"", _y)\n    dataX.append(_x)\n    dataY.append(_y)\n\n# split to train and testing\ntrain_size = int(len(dataY) * 0.7)\ntest_size = len(dataY) - train_size\ntrainX, testX = np.array(dataX[0:train_size]), np.array(\n    dataX[train_size:len(dataX)])\ntrainY, testY = np.array(dataY[0:train_size]), np.array(\n    dataY[train_size:len(dataY)])\n\nmodel = Sequential()\nmodel.add(LSTM(1, input_shape=(seq_length, data_dim), return_sequences=False))\n# model.add(Dense(1))\nmodel.add(Activation(""linear""))\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\nmodel.summary()\n\n# Store model graph in png\n# (Error occurs on in python interactive shell)\nplot_model(model, to_file=os.path.basename(__file__) + \'.png\', show_shapes=True)\n\nprint(trainX.shape, trainY.shape)\nmodel.fit(trainX, trainY, epochs=200)\n\n# make predictions\ntestPredict = model.predict(testX)\n\n# inverse values\n# testPredict = scaler.transform(testPredict)\n# testY = scaler.transform(testY)\n\n# print(testPredict)\nplt.plot(testY)\nplt.plot(testPredict)\nplt.show()\n'"
keras/klab-12-4-rnn_deep_prediction.py,0,"b'# http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n# Video: https://www.youtube.com/watch?v=ftMq5ps503w\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Activation\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\n\n# brew install graphviz\n# pip3 install graphviz\n# pip3 install pydot-ng\nfrom keras.utils.vis_utils import plot_model\n\nimport matplotlib.pyplot as plt\n\ntimesteps = seq_length = 7\ndata_dim = 5\n\n# Open,High,Low,Close,Volume\nxy = np.loadtxt(\'data-02-stock_daily.csv\', delimiter=\',\')\nxy = xy[::-1]  # reverse order (chronically ordered)\n\n# very important. It does not work without it.\nscaler = MinMaxScaler(feature_range=(0, 1))\nxy = scaler.fit_transform(xy)\n\nx = xy\ny = xy[:, [-1]]  # Close as label\n\ndataX = []\ndataY = []\nfor i in range(0, len(y) - seq_length):\n    _x = x[i:i + seq_length]\n    _y = y[i + seq_length]  # Next close price\n    print(_x, ""->"", _y)\n    dataX.append(_x)\n    dataY.append(_y)\n\n# split to train and testing\ntrain_size = int(len(dataY) * 0.7)\ntest_size = len(dataY) - train_size\ntrainX, testX = np.array(dataX[0:train_size]), np.array(\n    dataX[train_size:len(dataX)])\ntrainY, testY = np.array(dataY[0:train_size]), np.array(\n    dataY[train_size:len(dataY)])\n\nmodel = Sequential()\nmodel.add(LSTM(5, input_shape=(timesteps, data_dim), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(10, return_sequences=False))\nmodel.add(Dense(1))\nmodel.add(Activation(\'linear\'))\n\nmodel.summary()\n\n# Store model graph in png\n# (Error occurs on in python interactive shell)\nplot_model(model, to_file=os.path.basename(__file__) + \'.png\', show_shapes=True)\n\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\nprint(trainX.shape, trainY.shape)\nmodel.fit(trainX, trainY, epochs=200)\n\n# make predictions\ntestPredict = model.predict(testX)\n\n# inverse values\n# testPredict = scaler.transform(testPredict)\n# testY = scaler.transform(testY)\n\nprint(testPredict)\nplt.plot(testY)\nplt.plot(testPredict)\nplt.show()\n'"
keras/klab-12-5-seq2seq.py,0,"b'# https://gist.github.com/rouseguy/1122811f2375064d009dac797d59bae9\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Activation, TimeDistributed, Dense, RepeatVector, LSTM\nfrom keras.utils import np_utils\nfrom keras.callbacks import TensorBoard\nimport os\n\n# brew install graphviz\n# pip3 install graphviz\n# pip3 install pydot-ng\nfrom keras.utils.vis_utils import plot_model\n\ndigit = ""0123456789""\nalpha = ""abcdefghij""\n\nchar_set = list(set(digit + alpha))  # id -> char\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\ndata_dim = len(char_set)  # one hot encoding size\nseq_length = time_steps = 7\nnum_classes = len(char_set)\n\n# Build training date set\ndataX = []\ndataY = []\n\nfor i in range(1000):\n    rand_pick = np.random.choice(10, 7)\n    x = [char_dic[digit[c]] for c in rand_pick]\n    y = [char_dic[alpha[c]] for c in rand_pick]\n    dataX.append(x)\n    dataY.append(y)\n\n# One-hot encoding\ndataX = np_utils.to_categorical(dataX, num_classes=num_classes)\n# reshape X to be [samples, time steps, features]\ndataX = np.reshape(dataX, (-1, seq_length, data_dim))\n\n# One-hot encoding\ndataY = np_utils.to_categorical(dataY, num_classes=num_classes)\n# time steps\ndataY = np.reshape(dataY, (-1, seq_length, data_dim))\n\n\nprint(\'Build model...\')\nTensorBoard(log_dir=\'./logs\', histogram_freq=1,\n            write_graph=True, write_images=False)\n\nmodel = Sequential()\n# ""Encode"" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n# note: in a situation where your input sequences have a variable length,\n# use input_shape=(None, nb_feature).\nmodel.add(LSTM(32, input_shape=(time_steps, data_dim), return_sequences=False))\n\n# For the decoder\'s input, we repeat the encoded input for each time step\nmodel.add(RepeatVector(time_steps))\n# The decoder RNN could be multiple layers stacked or a single layer\n\nmodel.add(LSTM(32, return_sequences=True))\n\n# For each of step of the output sequence, decide which character should\n# be chosen\nmodel.add(TimeDistributed(Dense(data_dim)))\nmodel.add(Activation(\'softmax\'))\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=\'adam\',\n              metrics=[\'accuracy\'])\nmodel.fit(dataX, dataY, epochs=1000)\n\n# Store model graph in png\n# (Error occurs on in python interactive shell)\nplot_model(model, to_file=os.path.basename(__file__) + \'.png\', show_shapes=True)\n\n\n# Create test data set for fun\ntestX = []\ntestY = []\nfor i in range(10):\n    rand_pick = np.random.choice(10, 7)\n    x = [char_dic[digit[c]] for c in rand_pick]\n    y = [alpha[c] for c in rand_pick]\n    testX.append(x)\n    testY.append(y)\n\n\n# One-hot encoding\ntestX = np_utils.to_categorical(testX, num_classes=num_classes)\n# reshape X to be [samples, time steps, features]\ntestX = np.reshape(testX, (-1, seq_length, data_dim))\n\n\npredictions = model.predict(testX, verbose=0)\nfor i, prediction in enumerate(predictions):\n    # print(prediction)\n    x_index = np.argmax(testX[i], axis=1)\n    x_str = [char_set[j] for j in x_index]\n\n    index = np.argmax(prediction, axis=1)\n    result = [char_set[j] for j in index]\n\n    print(\'\'.join(x_str), \' -> \', \'\'.join(result),\n          "" true: "", \'\'.join(testY[i]))\n'"
keras/klab-13-2-mnist_tensorboard.py,0,"b""# https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n'''Trains a simple convnet on the MNIST dataset.\nGets to 99.25% test accuracy after 12 epochs\n(there is still a lot of margin for parameter tuning).\n16 seconds per epoch on a GRID K520 GPU.\n'''\n\nfrom __future__ import print_function\nimport numpy as np\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\nfrom keras.callbacks import TensorBoard\n\nnp.random.seed(1337)  # for reproducibility\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 12\n\nimg_rows, img_cols = 28, 28\nnb_filters = 32\npool_size = (2, 2)\nkernel_size = (3, 3)\n\n\n# ==============================================================================\n# load data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nif K.image_dim_ordering() == 'th':\n    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n\n# ==============================================================================\n# build model\nmodel = Sequential()\n\nmodel.add(Conv2D(nb_filters,\n                 kernel_size,\n                 padding='valid',\n                 input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(nb_filters, kernel_size))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=pool_size))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n\ntensorboard = TensorBoard(log_dir='./logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=False)\n\nmodel.fit(X_train, Y_train,\n          batch_size=batch_size,\n          epochs=nb_epoch,\n          verbose=1,\n          validation_data=(X_test, Y_test),\n          callbacks=[tensorboard])\n\nprint('Learning Finished!')\n\n\n# ==============================================================================\n# evaluate\nscore = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n'''\nTest score: 0.0317909693908\nTest accuracy: 0.9894\n'''\n\n'''\ntensorboard --logdir logs/\nStarting TensorBoard b'47' at http://0.0.0.0:6006\n(Press CTRL+C to quit)\n'''\n"""
mxnet/mxlab-04-3-file_input_linear_regression.py,0,"b'# Lab 4 Linear Regression\nimport mxnet as mx\nimport mxnet.ndarray as nd\nimport numpy as np\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)  # Config the logging\nnp.random.seed(777)\n\n# 1. Prepare Data\nxy = np.loadtxt(\'data-01-test-score.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(""x_data"", x_data)\nprint(""y_data"", y_data)\n\n# hyper-parameters\nsample_num = x_data.shape[0]\ndimension = x_data.shape[1]\nbatch_size = 25\n\n# 2. Build the Linear Regression Symbol\ndata = mx.sym.Variable(""data"")\ntarget = mx.sym.Variable(""target"")\nfc = mx.sym.FullyConnected(data=data, num_hidden=1, name=\'fc\')\npred = mx.sym.LinearRegressionOutput(data=fc, label=target)\n\n# 3. Construct the Module based on the symbol.\nnet = mx.mod.Module(symbol=pred,\n                    data_names=[\'data\'],\n                    label_names=[\'target\'],\n                    context=mx.gpu(0))\nnet.bind(data_shapes=[mx.io.DataDesc(name=\'data\', shape=(batch_size, dimension), layout=\'NC\')],\n         label_shapes=[mx.io.DataDesc(name=\'target\', shape=(batch_size, 1), layout=\'NC\')])\nnet.init_params(initializer=mx.init.Normal(sigma=0.01))\nnet.init_optimizer(optimizer=\'sgd\', optimizer_params={\'learning_rate\': 1E-4, \'momentum\': 0.9})\n\n# 4. Train the model\n# First constructing the training iterator and then fit the model\ntrain_iter = mx.io.NDArrayIter(x_data, y_data, batch_size, shuffle=True, label_name=\'target\')\nnet.fit(train_data=train_iter, eval_metric=""mse"", num_epoch=2000)\n\n# 5. Test the model\ntest_net = mx.mod.Module(symbol=fc,\n                         data_names=[\'data\'],\n                         label_names=None,\n                         context=mx.gpu(0))\ntest_net.bind(data_shapes=[mx.io.DataDesc(name=\'data\', shape=(1, dimension), layout=\'NC\')],\n              label_shapes=None,\n              for_training=False,\n              shared_module=net)\ntest_net.forward(mx.io.DataBatch(data=[nd.array([[60, 70, 110]])], label=None))\nprint(""input = [60, 70, 110], score ="", test_net.get_outputs()[0].asnumpy())\ntest_net.forward(mx.io.DataBatch(data=[nd.array([[90, 100, 80]])], label=None))\nprint(""input = [90, 100, 80], score ="", test_net.get_outputs()[0].asnumpy())\n\'\'\'\ninput = [60, 70, 110], score = [[ 182.48858643]]\ninput = [90, 100, 80], score = [[ 175.24279785]]\n\'\'\'\n'"
mxnet/mxlab-05-2-logistic_regression_diabetes.py,0,"b'# Lab 5 Logistic Regression Classifier\nimport mxnet as mx\nimport numpy as np\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)  # Config the logging\nnp.random.seed(777)\n\nxy = np.loadtxt(\'data-03-diabetes.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(x_data.shape, y_data.shape)\n\n# hyper-parameters\nsample_num = x_data.shape[0]\ndimension = x_data.shape[1]\nbatch_size = 32\n\n# 2. Build the Logistic Regression Symbol\ndata = mx.sym.Variable(""data"")\ntarget = mx.sym.Variable(""target"")\nfc = mx.sym.FullyConnected(data=data, num_hidden=1, name=\'fc\')\npred = mx.sym.LogisticRegressionOutput(data=fc, label=target)\n\n# 3. Construct the Module based on the symbol.\nnet = mx.mod.Module(symbol=pred,\n                    data_names=[\'data\'],\n                    label_names=[\'target\'],\n                    context=mx.gpu(0))\nnet.bind(data_shapes=[mx.io.DataDesc(name=\'data\', shape=(batch_size, dimension), layout=\'NC\')],\n         label_shapes=[mx.io.DataDesc(name=\'target\', shape=(batch_size, 1), layout=\'NC\')])\nnet.init_params(initializer=mx.init.Normal(sigma=0.01))\nnet.init_optimizer(optimizer=\'sgd\', optimizer_params={\'learning_rate\': 1E-3, \'momentum\': 0.9})\n\n# 4. Train the model\n# First constructing the training iterator and then fit the model\ntrain_iter = mx.io.NDArrayIter(x_data, y_data, batch_size, shuffle=True, label_name=\'target\')\nmetric = mx.metric.CustomMetric(feval=lambda labels, pred: ((pred > 0.5) == labels).mean(),\n                                name=""acc"")\nnet.fit(train_data=train_iter, eval_metric=metric, num_epoch=200)\n\n\'\'\'\n...\nINFO:root:Epoch[195] Train-acc=0.770833\nINFO:root:Epoch[195] Time cost=0.015\nINFO:root:Epoch[196] Train-acc=0.770833\nINFO:root:Epoch[196] Time cost=0.015\nINFO:root:Epoch[197] Train-acc=0.770833\nINFO:root:Epoch[197] Time cost=0.013\nINFO:root:Epoch[198] Train-acc=0.769531\nINFO:root:Epoch[198] Time cost=0.012\nINFO:root:Epoch[199] Train-acc=0.769531\nINFO:root:Epoch[199] Time cost=0.012\n\'\'\'\n\n# 5. Test the model\ntest_iter = mx.io.NDArrayIter(x_data, None, batch_size, shuffle=False, label_name=None)\n\npred_class = (fc > 0)\ntest_net = mx.mod.Module(symbol=pred_class,\n                         data_names=[\'data\'],\n                         label_names=None,\n                         context=mx.gpu(0))\ntest_net.bind(data_shapes=[mx.io.DataDesc(name=\'data\', shape=(batch_size, dimension), layout=\'NC\')],\n              label_shapes=None,\n              for_training=False,\n              shared_module=net)\nout = test_net.predict(eval_data=test_iter)\nprint(out.asnumpy())\n\'\'\'\n...\n[ 1.]\n[ 1.]\n[ 1.]\n[ 0.]\n[ 1.]\n[ 0.]\n[ 1.]\n[ 1.]\n[ 1.]\n[ 1.]\n[ 1.]\n[ 1.]]\n\'\'\'\n'"
mxnet/mxlab-06-2-softmax_zoo_classifier.py,0,"b'# Lab 6 Softmax Classifier\nimport mxnet as mx\nimport numpy as np\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)  # Config the logging\nnp.random.seed(777)\n\n# Predicting animal type based on various features\nxy = np.loadtxt(\'data-04-zoo.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]].reshape((-1,))\n\nprint(x_data.shape, y_data.shape)\n\n# hyper-parameters\nnb_classes = 7  # 0 ~ 6\nsample_num = x_data.shape[0]\ndimension = x_data.shape[1]\nbatch_size = 32\n\n# 2. Build the Softmax Classification Symbol\ndata = mx.sym.Variable(""data"")\ntarget = mx.sym.Variable(""target"")\nlogits = mx.sym.FullyConnected(data=data, num_hidden=nb_classes, name=\'logits\')\npred = mx.sym.SoftmaxOutput(data=logits, label=target)\n\n# 3. Construct the Module based on the symbol.\nnet = mx.mod.Module(symbol=pred,\n                    data_names=[\'data\'],\n                    label_names=[\'target\'],\n                    context=mx.gpu(0))\nnet.bind(data_shapes=[mx.io.DataDesc(name=\'data\', shape=(batch_size, dimension), layout=\'NC\')],\n         label_shapes=[mx.io.DataDesc(name=\'target\', shape=(batch_size,), layout=\'NC\')])\nnet.init_params(initializer=mx.init.Normal(sigma=0.01))\nnet.init_optimizer(optimizer=\'sgd\', optimizer_params={\'learning_rate\': 1E-1, \'momentum\': 0.9})\n\n# 4. Train the model\n# First constructing the training iterator and then fit the model\ntrain_iter = mx.io.NDArrayIter(x_data, y_data, batch_size, shuffle=True, label_name=\'target\')\nnet.fit(train_data=train_iter, eval_metric=\'acc\', num_epoch=40)\n\n\'\'\'\nINFO:root:Epoch[27] Train-accuracy=0.992188\nINFO:root:Epoch[27] Time cost=0.003\nINFO:root:Epoch[28] Train-accuracy=0.992188\nINFO:root:Epoch[28] Time cost=0.003\nINFO:root:Epoch[29] Train-accuracy=0.992188\nINFO:root:Epoch[29] Time cost=0.003\nINFO:root:Epoch[30] Train-accuracy=1.000000\nINFO:root:Epoch[30] Time cost=0.003\nINFO:root:Epoch[31] Train-accuracy=1.000000\nINFO:root:Epoch[31] Time cost=0.003\nINFO:root:Epoch[32] Train-accuracy=1.000000\nINFO:root:Epoch[32] Time cost=0.003\nINFO:root:Epoch[33] Train-accuracy=1.000000\nINFO:root:Epoch[33] Time cost=0.004\nINFO:root:Epoch[34] Train-accuracy=1.000000\nINFO:root:Epoch[34] Time cost=0.003\nINFO:root:Epoch[35] Train-accuracy=1.000000\nINFO:root:Epoch[35] Time cost=0.003\nINFO:root:Epoch[36] Train-accuracy=1.000000\nINFO:root:Epoch[36] Time cost=0.003\nINFO:root:Epoch[37] Train-accuracy=1.000000\nINFO:root:Epoch[37] Time cost=0.004\nINFO:root:Epoch[38] Train-accuracy=1.000000\nINFO:root:Epoch[38] Time cost=0.003\nINFO:root:Epoch[39] Train-accuracy=1.000000\nINFO:root:Epoch[39] Time cost=0.003\n\'\'\'\n# 5. Test the model\ntest_iter = mx.io.NDArrayIter(x_data, None, batch_size, shuffle=False, label_name=None)\n\npred_class = mx.sym.argmax(logits, axis=-1, name=""pred_class"")\ntest_net = mx.mod.Module(symbol=pred_class,\n                         data_names=[\'data\'],\n                         label_names=None,\n                         context=mx.gpu(0))\ntest_net.bind(data_shapes=[mx.io.DataDesc(name=\'data\', shape=(batch_size, dimension), layout=\'NC\')],\n              label_shapes=None,\n              for_training=False,\n              shared_module=net)\nout = test_net.predict(eval_data=test_iter)\nprint(out.asnumpy())\n\'\'\'\n[ 0.  0.  3.  0.  0.  0.  0.  3.  3.  0.  0.  1.  3.  6.  6.  6.  1.  0.\n  3.  0.  1.  1.  0.  1.  5.  4.  4.  0.  0.  0.  5.  0.  0.  1.  3.  0.\n  0.  1.  3.  5.  5.  1.  5.  1.  0.  0.  6.  0.  0.  0.  0.  5.  4.  6.\n  0.  0.  1.  1.  1.  1.  3.  3.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n  6.  3.  0.  0.  2.  6.  1.  1.  2.  6.  3.  1.  0.  6.  3.  1.  5.  4.\n  2.  2.  3.  0.  0.  1.  0.  5.  0.  6.  1.]\n\'\'\'\n'"
mxnet/mxlab-11-2-mnist_deep_cnn.py,0,"b'# Lab 11 MNIST and Deep learning CNN\nimport math\nimport mxnet as mx\nimport mxnet.ndarray as nd\nimport numpy as np\nimport random\nfrom sklearn.datasets import fetch_mldata\n\n# set the seeds. However, this does not guarantee that the result will always be the same since CUDNN is non-deterministic\nnp.random.seed(777)\nmx.random.seed(77)\nrandom.seed(7777)\n\n# 1. Loading MNIST\nmnist = fetch_mldata(dataname=\'MNIST original\')\nX, y = mnist.data, mnist.target\nX = X.astype(np.float32) / 255.0\nX_train, X_valid, X_test = X[:55000].reshape((-1, 1, 28, 28)),\\\n    X[55000:60000].reshape((-1, 1, 28, 28)),\\\n    X[60000:].reshape((-1, 1, 28, 28))\ny_train, y_valid, y_test = y[:55000], y[55000:60000], y[60000:]\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\ndrop_out_prob = 0.3  # The keep probability is 0.7\n\n# 2. Build symbol\ndata = mx.sym.var(name=""data"")\nlabel = mx.sym.var(name=""label"")\n\nL1 = mx.sym.Convolution(data=data, kernel=(3, 3), pad=(1, 1), num_filter=32, name=\'L1_conv\')\nL1 = mx.sym.Activation(data=L1, act_type=\'relu\', name=\'L1_relu\')\nL1 = mx.sym.Pooling(data=L1, kernel=(2, 2), stride=(2, 2), pool_type=\'max\', name=\'L1_pool\')\nL1 = mx.sym.Dropout(L1, p=drop_out_prob, name=""L1_dropout"")\n\nL2 = mx.sym.Convolution(data=L1, kernel=(3, 3), pad=(1, 1), num_filter=64, name=\'L2_conv\')\nL2 = mx.sym.Activation(data=L2, act_type=\'relu\', name=\'L2_relu\')\nL2 = mx.sym.Pooling(data=L2, kernel=(2, 2), stride=(2, 2), pool_type=\'max\', name=\'L2_pool\')\nL2 = mx.sym.Dropout(L2, p=drop_out_prob, name=""L2_dropout"")\n\nL3 = mx.sym.Convolution(data=L2, kernel=(3, 3), pad=(1, 1), num_filter=128, name=\'L3_conv\')\nL3 = mx.sym.Activation(data=L3, act_type=\'relu\', name=\'L3_relu\')\nL3 = mx.sym.Pooling(data=L3, kernel=(2, 2), stride=(2, 2), pad=(1, 1), pool_type=\'max\', name=\'L3_pool\')\nL3 = mx.sym.flatten(L3)\nL3 = mx.sym.Dropout(L3, p=drop_out_prob, name=""L3_dropout"")\n\nL4 = mx.sym.FullyConnected(data=L3, num_hidden=625, name=\'L4_fc\')\nL4 = mx.sym.Dropout(L4, p=drop_out_prob)\n\nlogits = mx.sym.FullyConnected(data=L4, num_hidden=10, name=\'logits\')\n\nloss = mx.sym.mean(-mx.sym.pick(mx.sym.log_softmax(logits), label, axis=-1))\nloss = mx.sym.make_loss(loss)\n\n# 3. Build network handler\ndata_desc = mx.io.DataDesc(name=\'data\', shape=(batch_size, 1, 28, 28), layout=\'NCHW\')\nlabel_desc = mx.io.DataDesc(name=\'label\', shape=(batch_size, ), layout=\'N\')\nnet = mx.mod.Module(symbol=loss,\n                    data_names=[data_desc.name],\n                    label_names=[label_desc.name],\n                    context=mx.gpu())\nnet.bind(data_shapes=[data_desc], label_shapes=[label_desc])\nnet.init_params(initializer=mx.init.Xavier())\nnet.init_optimizer(optimizer=""adam"",\n                   optimizer_params={\'learning_rate\': learning_rate,\n                                     \'rescale_grad\': 1.0},\n                   kvstore=None)\n\n# We build another testing network that outputs the logits.\ntest_net = mx.mod.Module(symbol=logits,\n                         data_names=[data_desc.name],\n                         label_names=None,\n                         context=mx.gpu())\n# Setting the `shared_module` to ensure that the test network shares the same parameters and\n#  allocated memory of the training network\ntest_net.bind(data_shapes=[data_desc],\n              label_shapes=None,\n              for_training=False,\n              grad_req=\'null\',\n              shared_module=net)\n\n# 4. Train the network\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(math.ceil(X_train.shape[0] / batch_size))\n    shuffle_ind = np.random.permutation(np.arange(X_train.shape[0]))\n    X_train = X_train[shuffle_ind, :]\n    y_train = y_train[shuffle_ind]\n    for i in range(total_batch):\n        # Slice the data batch and label batch.\n        # Note that we use np.take to ensure that the batch will be padded correctly.\n        data_npy = np.take(X_train,\n                           indices=np.arange(i * batch_size, (i + 1) * batch_size),\n                           axis=0,\n                           mode=""clip"")\n        label_npy = np.take(y_train,\n                            indices=np.arange(i * batch_size, (i + 1) * batch_size),\n                            axis=0,\n                            mode=""clip"")\n        net.forward(data_batch=mx.io.DataBatch(data=[nd.array(data_npy)],\n                                               label=[nd.array(label_npy)]),\n                    is_train=True)\n        loss_nd = net.get_outputs()[0]\n        net.backward()\n        net.update()\n        avg_cost += loss_nd.asnumpy()[0] / total_batch\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\nprint(\'Learning Finished!\')\n\n# 5. Test the network\ntotal_batch = int(np.ceil(X_test.shape[0] / batch_size))\ncorrect_count = 0\ntotal_num = 0\nfor i in range(total_batch):\n    num_valid = batch_size if (i + 1) * batch_size <= X_test.shape[0]\\\n        else X_test.shape[0] - i * batch_size\n    data_npy = np.take(X_test,\n                       indices=np.arange(i * batch_size, (i + 1) * batch_size),\n                       axis=0,\n                       mode=""clip"")\n    label_npy = np.take(y_test,\n                        indices=np.arange(i * batch_size, (i + 1) * batch_size),\n                        axis=0,\n                        mode=""clip"")\n    test_net.forward(data_batch=mx.io.DataBatch(data=[nd.array(data_npy)],\n                                                label=None),\n                     is_train=False)\n    logits_nd = test_net.get_outputs()[0]\n    pred_cls = nd.argmax(logits_nd, axis=-1).asnumpy()\n    correct_count += (pred_cls[:num_valid] == label_npy[:num_valid]).sum()\nacc = correct_count / float(X_test.shape[0])\nprint(\'Accuracy:\', acc)\n\n# 6. Get one and predict\ntest_net.reshape(data_shapes=[mx.io.DataDesc(name=\'data\', shape=(1, 1, 28, 28), layout=\'NCHW\')],\n                 label_shapes=None)\nr = np.random.randint(0, X_test.shape[0])\ntest_net.forward(data_batch=mx.io.DataBatch(data=[nd.array(X_test[r:r + 1])],\n                                            label=None))\nlogits_nd = test_net.get_outputs()[0]\nprint(""Label: "", int(y_test[r]))\nprint(""Prediction: "", int(nd.argmax(logits_nd, axis=1).asnumpy()[0]))\n\'\'\'\nEpoch: 0001 cost = 0.222577997\nEpoch: 0002 cost = 0.072177568\nEpoch: 0003 cost = 0.055896563\nEpoch: 0004 cost = 0.049281721\nEpoch: 0005 cost = 0.042741676\nEpoch: 0006 cost = 0.040903398\nEpoch: 0007 cost = 0.037740686\nEpoch: 0008 cost = 0.035468433\nEpoch: 0009 cost = 0.035284971\nEpoch: 0010 cost = 0.032539701\nEpoch: 0011 cost = 0.031544954\nEpoch: 0012 cost = 0.032199799\nEpoch: 0013 cost = 0.030421943\nEpoch: 0014 cost = 0.028435153\nEpoch: 0015 cost = 0.028335706\nLearning Finished!\nAccuracy: 0.989\nLabel:  2\nPrediction:  2\n\'\'\'\n'"
mxnet/mxlab-11-5-mnist_cnn_ensemble_layers.py,0,"b'# Lab 11 MNIST and Deep learning CNN\nimport math\nimport mxnet as mx\nimport mxnet.ndarray as nd\nimport numpy as np\nimport random\nfrom sklearn.datasets import fetch_mldata\n\n# set the seeds. However, this does not guarantee that the result will always be the same since CUDNN is non-deterministic\nnp.random.seed(777)\nmx.random.seed(77)\nrandom.seed(7777)\n\n# 1. Loading MNIST\nmnist = fetch_mldata(dataname=\'MNIST original\')\nX, y = mnist.data, mnist.target\nX = X.astype(np.float32) / 255.0\nX_train, X_valid, X_test = X[:55000].reshape((-1, 1, 28, 28)),\\\n    X[55000:60000].reshape((-1, 1, 28, 28)),\\\n    X[60000:].reshape((-1, 1, 28, 28))\ny_train, y_valid, y_test = y[:55000], y[55000:60000], y[60000:]\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 20\nbatch_size = 100\nnum_models = 2\n\n\ndef build_symbol():\n    data = mx.sym.var(name=""data"")\n    label = mx.sym.var(name=""label"")\n\n    L1 = mx.sym.Convolution(data=data, kernel=(3, 3), pad=(1, 1), num_filter=32, name=\'L1_conv\')\n    L1 = mx.sym.Activation(data=L1, act_type=\'relu\', name=\'L1_relu\')\n    L1 = mx.sym.Pooling(data=L1, kernel=(2, 2), stride=(2, 2), pool_type=\'max\', name=\'L1_pool\')\n    L1 = mx.sym.Dropout(L1, p=0.3, name=""L1_dropout"")\n\n    L2 = mx.sym.Convolution(data=L1, kernel=(3, 3), pad=(1, 1), num_filter=64, name=\'L2_conv\')\n    L2 = mx.sym.Activation(data=L2, act_type=\'relu\', name=\'L2_relu\')\n    L2 = mx.sym.Pooling(data=L2, kernel=(2, 2), stride=(2, 2), pool_type=\'max\', name=\'L2_pool\')\n    L2 = mx.sym.Dropout(L2, p=0.3, name=""L2_dropout"")\n\n    L3 = mx.sym.Convolution(data=L2, kernel=(3, 3), pad=(1, 1), num_filter=128, name=\'L3_conv\')\n    L3 = mx.sym.Activation(data=L3, act_type=\'relu\', name=\'L3_relu\')\n    L3 = mx.sym.Pooling(data=L3, kernel=(2, 2), stride=(2, 2), pad=(1, 1), pool_type=\'max\', name=\'L3_pool\')\n    L3 = mx.sym.flatten(L3)\n    L3 = mx.sym.Dropout(L3, p=0.3, name=""L3_dropout"")\n\n    L4 = mx.sym.FullyConnected(data=L3, num_hidden=625, name=\'L4_fc\')\n    L4 = mx.sym.Dropout(L4, p=0.5)\n\n    logits = mx.sym.FullyConnected(data=L4, num_hidden=10, name=\'logits\')\n\n    loss = mx.sym.mean(-mx.sym.pick(mx.sym.log_softmax(logits), label, axis=-1))\n    loss = mx.sym.make_loss(loss)\n    return loss, logits\n\n\ndef get_batch(p, batch_size, X, y):\n    data_npy = np.take(X,\n                       indices=np.arange(p * batch_size, (p + 1) * batch_size),\n                       axis=0,\n                       mode=""clip"")\n    label_npy = np.take(y,\n                        indices=np.arange(p * batch_size, (p + 1) * batch_size),\n                        axis=0,\n                        mode=""clip"")\n    num_valid = batch_size if (p + 1) * batch_size <= X.shape[0] else X.shape[0] - p * batch_size\n    return data_npy, label_npy, num_valid\n\n\ntrain_nets = []\ntest_nets = []\n# 1. Get the symbol\nloss, logits = build_symbol()\n\n# 2. Build the training nets and testing nets\ndata_desc = mx.io.DataDesc(name=\'data\', shape=(batch_size, 1, 28, 28), layout=\'NCHW\')\nlabel_desc = mx.io.DataDesc(name=\'label\', shape=(batch_size, ), layout=\'N\')\nfor i in range(num_models):\n    net = mx.mod.Module(symbol=loss,\n                        data_names=[data_desc.name],\n                        label_names=[label_desc.name],\n                        context=mx.gpu())\n    net.bind(data_shapes=[data_desc], label_shapes=[label_desc])\n    net.init_params(initializer=mx.init.Xavier())\n    net.init_optimizer(optimizer=""adam"",\n                       optimizer_params={\'learning_rate\': learning_rate,\n                                         \'rescale_grad\': 1.0},\n                       kvstore=None)\n\n    # We build another testing network that outputs the logits.\n    test_net = mx.mod.Module(symbol=logits,\n                             data_names=[data_desc.name],\n                             label_names=None,\n                             context=mx.gpu())\n    # Setting the `shared_module` to ensure that the test network shares the same parameters and\n    #  allocated memory of the training network\n    test_net.bind(data_shapes=[data_desc],\n                  label_shapes=None,\n                  for_training=False,\n                  grad_req=\'null\',\n                  shared_module=net)\n    train_nets.append(net)\n    test_nets.append(test_net)\n\nprint(\'Learning Started!\')\n\n# 3. Train all the models\nfor epoch in range(training_epochs):\n    avg_cost_list = np.zeros(num_models)\n    total_batch = int(math.ceil(X_train.shape[0] / batch_size))\n    shuffle_ind = np.random.permutation(np.arange(X_train.shape[0]))\n    X_train = X_train[shuffle_ind, :]\n    y_train = y_train[shuffle_ind]\n    for i in range(total_batch):\n        data_npy, label_npy, _ = get_batch(i, batch_size, X_train, y_train)\n        for i, net in enumerate(train_nets):\n            net.forward(data_batch=mx.io.DataBatch(data=[nd.array(data_npy)],\n                                                   label=[nd.array(label_npy)]),\n                        is_train=True)\n            loss_nd = net.get_outputs()[0]\n            net.backward()\n            net.update()\n            avg_cost_list[i] += loss_nd.asnumpy()[0] / total_batch\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', avg_cost_list)\n\nprint(\'Learning Finished!\')\n\n# 5. Test the networks\ntotal_batch = int(np.ceil(X_test.shape[0] / batch_size))\ncorrect_counts = [0 for i in range(num_models)]\nensemble_correct_count = 0\ntotal_num = 0\nfor i in range(total_batch):\n    num_valid = batch_size if (i + 1) * batch_size <= X_test.shape[0]\\\n        else X_test.shape[0] - i * batch_size\n    data_npy, label_npy, num_valid = get_batch(i, batch_size, X_test, y_test)\n    prob_ensemble = nd.zeros(shape=(label_npy.shape[0], 10), ctx=mx.gpu())\n    for i, test_net in enumerate(test_nets):\n        test_net.forward(data_batch=mx.io.DataBatch(data=[nd.array(data_npy)],\n                                                    label=None),\n                         is_train=False)\n        logits_nd = test_net.get_outputs()[0]\n        prob_nd = nd.softmax(logits_nd)\n        prob_ensemble += prob_nd\n        pred_cls = nd.argmax(prob_nd, axis=-1).asnumpy()\n        correct_counts[i] += (pred_cls[:num_valid] == label_npy[:num_valid]).sum()\n    prob_ensemble /= num_models\n    ensemble_pred_cls = nd.argmax(prob_ensemble, axis=-1).asnumpy()\n    ensemble_correct_count += (ensemble_pred_cls[:num_valid] == label_npy[:num_valid]).sum()\nfor i in range(num_models):\n    print(i, \'Accuracy:\', correct_counts[i] / float(X_test.shape[0]))\nprint(\'Ensemble accuracy:\', ensemble_correct_count / float(X_test.shape[0]))\n\'\'\'\nLearning Started!\nEpoch: 0001 cost = [ 0.23813407  0.23717315]\nEpoch: 0002 cost = [ 0.07455271  0.07434764]\nEpoch: 0003 cost = [ 0.05925059  0.06024327]\nEpoch: 0004 cost = [ 0.05032205  0.04895757]\nEpoch: 0005 cost = [ 0.04573197  0.0439943 ]\nEpoch: 0006 cost = [ 0.04143022  0.0416003 ]\nEpoch: 0007 cost = [ 0.03805082  0.03796253]\nEpoch: 0008 cost = [ 0.03668946  0.03679928]\nEpoch: 0009 cost = [ 0.03688032  0.03588339]\nEpoch: 0010 cost = [ 0.03180911  0.03446447]\nEpoch: 0011 cost = [ 0.03293695  0.03334761]\nEpoch: 0012 cost = [ 0.03255253  0.03101865]\nEpoch: 0013 cost = [ 0.03044157  0.03092092]\nEpoch: 0014 cost = [ 0.02833735  0.02996393]\nEpoch: 0015 cost = [ 0.03077925  0.02817958]\nEpoch: 0016 cost = [ 0.02788243  0.02807305]\nEpoch: 0017 cost = [ 0.02659359  0.02851706]\nEpoch: 0018 cost = [ 0.02834567  0.02659114]\nEpoch: 0019 cost = [ 0.02670252  0.02910407]\nEpoch: 0020 cost = [ 0.02647125  0.02396415]\nLearning Finished!\n0 Accuracy: 0.9924\n1 Accuracy: 0.9912\nEnsemble accuracy: 0.9938\n\'\'\'\n'"
mxnet/mxlab-12-4-rnn_deep_prediction.py,0,"b'# http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n# Video: https://www.youtube.com/watch?v=ftMq5ps503w\nimport numpy as np\nimport mxnet as mx\nimport logging\nimport sys\nfrom sklearn.preprocessing import MinMaxScaler\n\n# brew install graphviz\n# pip3 install graphviz\n# pip3 install pydot-ng\nimport matplotlib.pyplot as plt\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)  # Config the logging\nnp.random.seed(777)\nmx.random.seed(777)\n\ntimesteps = seq_length = 7\nbatch_size = 32\ndata_dim = 5\n\n\ndef build_sym(seq_len, use_cudnn=False):\n    """"""Build the symbol for stock-price prediction\n\n    Parameters\n    ----------\n    seq_len : int\n    use_cudnn : bool, optional\n        Whether to use the LSTM implemented in cudnn, will be faster than the original version\n\n    Returns\n    -------\n    pred : mx.sym.Symbol\n        The prediction result\n    """"""\n    data = mx.sym.var(""data"")  # Shape: (N, T, C)\n    target = mx.sym.var(""target"")  # Shape: (N, T, C)\n    data = mx.sym.transpose(data, axes=(1, 0, 2))  # Shape: (T, N, C)\n    if use_cudnn:\n        lstm1 = mx.rnn.FusedRNNCell(num_hidden=5, mode=""lstm"", prefix=""lstm1_"")\n        lstm2 = mx.rnn.FusedRNNCell(num_hidden=10, mode=""lstm"", prefix=""lstm2_"",\n                                    get_next_state=True)\n    else:\n        lstm1 = mx.rnn.LSTMCell(num_hidden=5, prefix=""lstm1_"")\n        lstm2 = mx.rnn.LSTMCell(num_hidden=10, prefix=""lstm2_"")\n    L1, _ = lstm1.unroll(length=seq_len, inputs=data, merge_outputs=True,\n                         layout=""TNC"")  # Shape: (T, N, 5)\n    L1 = mx.sym.Dropout(L1, p=0.2)  # Shape: (T, N, 5)\n    _, L2_states = lstm2.unroll(length=seq_len, inputs=L1, merge_outputs=True,\n                                layout=""TNC"")  # Shape: (T, N, 10)\n    L2 = mx.sym.reshape(L2_states[0], shape=(-1, 0), reverse=True)  # Shape: (T * N, 10)\n    pred = mx.sym.FullyConnected(L2, num_hidden=1, name=""pred"")\n    pred = mx.sym.LinearRegressionOutput(data=pred, label=target)\n    return pred\n\n# Open,High,Low,Close,Volume\nxy = np.loadtxt(\'data-02-stock_daily.csv\', delimiter=\',\')\nxy = xy[::-1]  # reverse order (chronically ordered)\n\n# very important. It does not work without it.\nscaler = MinMaxScaler(feature_range=(0, 1))\nxy = scaler.fit_transform(xy)\n\nx = xy\ny = xy[:, [-1]]  # Close as label\n\ndataX = []\ndataY = []\nfor i in range(0, len(y) - seq_length):\n    _x = x[i:i + seq_length]\n    _y = y[i + seq_length]  # Next close price\n    print(_x, ""->"", _y)\n    dataX.append(_x)\n    dataY.append(_y)\n\n# split to train and testing\ntrain_size = int(len(dataY) * 0.7)\ntest_size = len(dataY) - train_size\ntrainX, testX = np.array(dataX[0:train_size]), np.array(\n    dataX[train_size:len(dataX)])\ntrainY, testY = np.array(dataY[0:train_size]), np.array(\n    dataY[train_size:len(dataY)])\n\n\ndef train_eval_net(use_cudnn):\n    pred = build_sym(seq_len=seq_length, use_cudnn=use_cudnn)\n    net = mx.mod.Module(symbol=pred, data_names=[\'data\'], label_names=[\'target\'], context=mx.gpu())\n\n    train_iter = mx.io.NDArrayIter(data=trainX, label=trainY,\n                                   data_name=""data"", label_name=""target"",\n                                   batch_size=batch_size,\n                                   shuffle=True)\n    test_iter = mx.io.NDArrayIter(data=testX, label=testY,\n                                  data_name=""data"", label_name=""target"",\n                                  batch_size=batch_size)\n    net.fit(train_data=train_iter, eval_data=test_iter,\n            initializer=mx.init.Xavier(rnd_type=""gaussian"", magnitude=1),\n            optimizer=""adam"",\n            optimizer_params={""learning_rate"": 1E-3},\n            eval_metric=""mse"", num_epoch=200)\n\n    # make predictions\n    testPredict = net.predict(test_iter).asnumpy()\n    mse = np.mean((testPredict - testY)**2)\n    return testPredict, mse\n\n# inverse values\n# testPredict = scaler.transform(testPredict)\n# testY = scaler.transform(testY)\nimport time\nprint(""Begin to train LSTM with CUDNN acceleration..."")\nbegin = time.time()\ncudnn_pred, cudnn_mse = train_eval_net(use_cudnn=True)\nend = time.time()\ncudnn_time_spent = end - begin\nprint(""Done!"")\n\nprint(""Begin to train LSTM without CUDNN acceleration..."")\nbegin = time.time()\nnormal_pred, normal_mse = train_eval_net(use_cudnn=False)\nend = time.time()\nnormal_time_spent = end - begin\nprint(""Done!"")\n\n\nprint(""CUDNN time spent: %g, test mse: %g"" % (cudnn_time_spent, cudnn_mse))\nprint(""NoCUDNN time spent: %g, test mse: %g"" % (normal_time_spent, normal_mse))\n\nplt.close(\'all\')\nfig = plt.figure()\nplt.plot(testY, label=\'Groud Truth\')\nplt.plot(cudnn_pred, label=\'With cuDNN\')\nplt.plot(normal_pred, label=\'Without cuDNN\')\nplt.legend()\nplt.show()\n\'\'\'\nCUDNN time spent: 10.0955, test mse: 0.00721571\nNoCUDNN time spent: 38.9882, test mse: 0.00565724\n\'\'\'\n'"
mxnet/mxlab-12-5-seq2seq.py,0,"b'# https://gist.github.com/rouseguy/1122811f2375064d009dac797d59bae9\nimport numpy as np\nimport math\nimport mxnet as mx\nimport mxnet.ndarray as nd\nimport logging\nimport sys\n\n# pip3 install tqdm\nfrom tqdm import tqdm\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)  # Config the logging\nnp.random.seed(777)\nmx.random.seed(777)\n\ndigit = ""0123456789""\nalpha = ""abcdefghij""\n\nchar_set = list(set(digit + alpha))  # id -> char\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\ndata_dim = len(char_set)  # one hot encoding size\nseq_length = time_steps = 7\nnum_classes = len(char_set)\nbatch_size = 32\nseq_num = 1000\n\n# Build training date set\ndataX = np.empty(shape=(seq_num, seq_length), dtype=np.int)\ndataY = np.empty(shape=(seq_num, seq_length), dtype=np.int)\n\nfor i in range(1000):\n    rand_pick = np.random.choice(10, seq_length)\n    dataX[i, :] = [char_dic[digit[c]] for c in rand_pick]\n    dataY[i, :] = [char_dic[alpha[c]] for c in rand_pick]\n\n# Build the symbol\ndata = mx.sym.var(\'data\')  # Shape: (N, T)\ntarget = mx.sym.var(\'target\')  # Shape: (N, T)\nlstm1 = mx.rnn.FusedRNNCell(num_hidden=32, prefix=""lstm1_"", get_next_state=True)\nlstm2 = mx.rnn.FusedRNNCell(num_hidden=32, prefix=""lstm2_"", get_next_state=False)\ndata_one_hot = mx.sym.one_hot(data, depth=data_dim)  # Shape: (N, T, C)\ndata_one_hot = mx.sym.transpose(data_one_hot, axes=(1, 0, 2))  # Shape: (T, N, C)\n_, encode_state = lstm1.unroll(length=seq_length, inputs=data_one_hot, layout=""TNC"")\nencode_state_h = encode_state[0]  # Shape: (1, N, C)\nencode_state_h = mx.sym.broadcast_to(encode_state_h, shape=(seq_length, 0, 0))  # Shape: (T, N, C)\ndecode_out, _ = lstm2.unroll(length=seq_length, inputs=encode_state_h, layout=""TNC"")\ndecode_out = mx.sym.reshape(decode_out, shape=(-1, 32))\nlogits = mx.sym.FullyConnected(decode_out, num_hidden=data_dim, name=""logits"")\nlogits = mx.sym.reshape(logits, shape=(seq_length, -1, data_dim))\nlogits = mx.sym.transpose(logits, axes=(1, 0, 2))\nloss = mx.sym.mean(-mx.sym.pick(mx.sym.log_softmax(logits), target, axis=-1))\nloss = mx.sym.make_loss(loss)\n\n# Construct the training and testing modules\ndata_desc = mx.io.DataDesc(name=\'data\', shape=(batch_size, seq_length), layout=\'NT\')\nlabel_desc = mx.io.DataDesc(name=\'target\', shape=(batch_size, seq_length), layout=\'NT\')\nnet = mx.mod.Module(symbol=loss,\n                    data_names=[\'data\'],\n                    label_names=[\'target\'],\n                    context=mx.gpu())\nnet.bind(data_shapes=[data_desc], label_shapes=[label_desc])\nnet.init_params(initializer=mx.init.Xavier())\nnet.init_optimizer(optimizer=""adam"",\n                   optimizer_params={\'learning_rate\': 1E-3,\n                                     \'rescale_grad\': 1.0},\n                   kvstore=None)\n\n# We build another testing network that outputs the logits.\ntest_net = mx.mod.Module(symbol=logits,\n                         data_names=[data_desc.name],\n                         label_names=None,\n                         context=mx.gpu())\n# Setting the `shared_module` to ensure that the test network shares the same parameters and\n#  allocated memory of the training network\ntest_net.bind(data_shapes=[data_desc],\n              label_shapes=None,\n              for_training=False,\n              grad_req=\'null\',\n              shared_module=net)\n\n\nfor epoch in range(1000):\n    avg_cost = 0\n    total_batch = int(math.ceil(dataX.shape[0] / batch_size))\n    shuffle_ind = np.random.permutation(np.arange(dataX.shape[0]))\n    dataX = dataX[shuffle_ind, :]\n    dataY = dataY[shuffle_ind]\n    for i in tqdm(range(total_batch)):\n        # Slice the data batch and target batch.\n        # Note that we use np.take to ensure that the batch will be padded correctly.\n        data_npy = np.take(dataX,\n                           indices=np.arange(i * batch_size, (i + 1) * batch_size),\n                           axis=0,\n                           mode=""clip"")\n        target_npy = np.take(dataY,\n                             indices=np.arange(i * batch_size, (i + 1) * batch_size),\n                             axis=0,\n                             mode=""clip"")\n        net.forward_backward(data_batch=mx.io.DataBatch(data=[nd.array(data_npy)],\n                                                        label=[nd.array(target_npy)]))\n        loss = net.get_outputs()[0].asscalar()\n        avg_cost += loss / total_batch\n        net.update()\n    print(\'Epoch:\', \'%04d\' % (epoch + 1), \'cost =\', \'{:.9f}\'.format(avg_cost))\nprint(\'Learning Finished!\')\n\n\n# Create test data set for fun\ntestX = []\ntestY = []\nfor i in range(10):\n    rand_pick = np.random.choice(10, 7)\n    x = [char_dic[digit[c]] for c in rand_pick]\n    y = [alpha[c] for c in rand_pick]\n    testX.append(x)\n    testY.append(y)\ntextX = np.array(testX, dtype=np.int)\n\ntest_net.reshape(data_shapes=[mx.io.DataDesc(\'data\', (10, seq_length))])\npredictions = test_net.predict(mx.io.NDArrayIter(textX, batch_size=10)).asnumpy()\n\nfor i, prediction in enumerate(predictions):\n    x_str = [char_set[j] for j in testX[i]]\n    index = np.argmax(prediction, axis=1)\n    result = [char_set[j] for j in index]\n\n    print(\'\'.join(x_str), \' -> \', \'\'.join(result),\n          "" true: "", \'\'.join(testY[i]))\n\'\'\'\n...\n100%|#########################################| 32/32 [00:00<00:00, 433.79it/s]\nEpoch: 1000 cost = 0.010169438\nLearning Finished!\n0112842  ->  abbcejh  true:  abbciec\n7014889  ->  habbeii  true:  habeiij\n8394636  ->  idjeggg  true:  idjegdg\n7609424  ->  hgajeee  true:  hgajece\n4537462  ->  efdhegc  true:  efdhegc\n2485396  ->  ceidfjj  true:  ceifdjg\n9744693  ->  heeejje  true:  jheegjd\n4527849  ->  efhccde  true:  efchiej\n5321099  ->  fddabjj  true:  fdcbajj\n2173620  ->  cbhdgba  true:  cbhdgca\n\'\'\'\n'"
numpy/lab-05-1-logistic_regression.py,0,"b'""""""\nLogistic Regression\n\ny = sigmoid(X @ W + b)\n\n""""""\nimport numpy as np\n\nx_data = [[1, 2],\n          [2, 3],\n          [3, 1],\n          [4, 3],\n          [5, 3],\n          [6, 2]]\ny_data = [[0],\n          [0],\n          [0],\n          [1],\n          [1],\n          [1]]\n\nX_train = np.array(x_data, dtype=np.float32)\ny_train = np.array(y_data).reshape(-1, 1)\n\nN = X_train.shape[0]\nD = X_train.shape[1]\n\nC = 1\nLEARNING_RATE = 0.1\nMAX_ITER = 1000\n\nW = np.random.standard_normal((D, C))\nb = np.zeros((C,))\n\n\ndef sigmoid(x):\n    """"""Sigmoid function """"""\n    sigmoid = 1 / (1 + np.exp(-x))\n\n    return sigmoid\n\n\ndef sigmoid_cross_entropy(logit, labels):\n    """"""Compute a binary cross entropy loss\n\n    z = logit = X @ W + b\n    p = sigmoid(z)\n    loss_i = y * - log(p) + (1 - y) * - log(1 - p)\n\n    Args:\n        logit (2-D Array): Logit array of shape (N, 1)\n        labels (2-D Array): Binary Label array of shape (N, 1)\n\n    Returns:\n        float: mean(loss_i)\n    """"""\n    assert logit.shape == (N, C)\n    assert labels.shape == (N, C)\n\n    probs = sigmoid(logit)\n    loss_i = labels * -np.log(probs + 1e-8)\n    loss_i += (1 - labels) * -np.log(1 - probs + 1e-8)\n\n    loss = np.mean(loss_i)\n\n    return loss\n\n\ndef grad_sigmoid_cross_entropy(logit, labels):\n    """"""Returns\n\n    d_loss_i       d_sigmoid\n    --------   *   ---------\n    d_sigmoid      d_z\n\n    z = logit = X * W + b\n\n    Args:\n        logit (2-D Array): Logit array of shape (N, 1)\n        labels (2-D Array): Binary Label array of shape (N, 1)\n\n    Returns:\n        2-D Array: Backpropagated gradients of loss (N, 1)\n    """"""\n    return sigmoid(logit) - labels\n\n\ndef affine_forward(X, W, b):\n    """"""Returns a logit\n\n    logit = X @ W + b\n\n    Args:\n        X (2-D Array): Input array of shape (N, D)\n        W (2-D Array): Weight array of shape (D, 1)\n        b (1-D Array): Bias array of shape (1,)\n\n    Returns:\n        logit (2-D Array): Logit array of shape (N, 1)\n    """"""\n    return np.dot(X, W) + b\n\n\nfor i in range(MAX_ITER):\n\n    logit = affine_forward(X_train, W, b)\n    loss = sigmoid_cross_entropy(logit, y_train)\n    d_loss = grad_sigmoid_cross_entropy(logit, y_train)\n\n    d_W = np.dot(X_train.T, d_loss) / N\n    d_b = np.sum(d_loss) / N\n\n    W -= LEARNING_RATE * d_W\n    b -= LEARNING_RATE * d_b\n\n    if i % (MAX_ITER // 10) == 0:\n        prob = affine_forward(X_train, W, b)\n        prob = sigmoid(prob)\n        pred = prob > 0.5\n        acc = (pred == y_train).mean()\n\n        print(""[Step: {:5}] Loss: {:<5.3} Accuracy: {:>5.2%}"".format(i, loss, acc))\n\n""""""\n[Step:     0] Loss: 2.35  Accuracy: 50.00%\n[Step:   100] Loss: 0.523 Accuracy: 83.33%\n[Step:   200] Loss: 0.435 Accuracy: 83.33%\n[Step:   300] Loss: 0.368 Accuracy: 83.33%\n[Step:   400] Loss: 0.316 Accuracy: 83.33%\n[Step:   500] Loss: 0.275 Accuracy: 83.33%\n[Step:   600] Loss: 0.243 Accuracy: 100.00%\n[Step:   700] Loss: 0.217 Accuracy: 100.00%\n[Step:   800] Loss: 0.196 Accuracy: 100.00%\n[Step:   900] Loss: 0.178 Accuracy: 100.00%\n""""""\n'"
numpy/lab-06-2-softmax_zoo_classifier.py,0,"b'import numpy as np\n\ndata = np.loadtxt(""../data-04-zoo.csv"",\n                  delimiter="","",\n                  dtype=np.float32)\n\nX_train = data[:, :-1]\ny_train = data[:, -1].astype(np.int8)\nassert X_train.shape == (101, 16)\nassert y_train.shape == (101,)\n\nN, D = X_train.shape\nC = np.max(y_train) + 1\n\ny_train_onehot = np.zeros(shape=(N, C))\ny_train_onehot[np.arange(N), y_train] = 1\n\nassert C == 7, ""There are 7 classes to predict""\n\nW = np.random.standard_normal((D, C))\nb = np.zeros((C,))\n\n\ndef affine_forward(X, W, b):\n    """"""Returns a logit\n\n    logit = X @ W + b\n\n    Args:\n        X (2-D Array): Input array of shape (N, D)\n        W (2-D Array): Weight array of shape (D, C)\n        b (1-D Array): Bias array of shape (C,)\n\n    Returns:\n        logit (2-D Array): Logit array of shape (N, C)\n    """"""\n    return np.dot(X, W) + b\n\n\ndef softmax(z):\n    """"""Softmax Function\n\n    Subtract max for numerical stability\n\n    Args:\n        z (2-D Array): Array of shape (N, C)\n\n    Returns:\n        2-D Array: Softmax output of (N, C)\n    """"""\n    z -= np.max(z)\n    numerator = np.exp(z)\n    denominator = np.sum(numerator, axis=1).reshape(-1, 1) + 1e-7\n\n    return numerator / denominator\n\n\ndef softmax_cross_entropy_loss(logit, labels):\n    """"""Returns a softmax cross entropy loss\n\n    loss_i = - log(P(y_i | x_i))\n\n    Args:\n        logit (2-D Array): Logit array of shape (N, C)\n        labels (2-D Array): Label Onehot array of shape (N, C)\n\n    Returns:\n        float: mean(loss_i)\n    """"""\n    p = softmax(logit)\n    loss_i = - labels * np.log(p + 1e-8)\n    return np.mean(loss_i)\n\n\ndef grad_softmax_cross_entropy_loss(logit, labels):\n    """"""Returns\n\n    d_loss_i       d_softmax\n    --------   *   ---------\n    d_softmax      d_z\n\n    z = logit = X * W + b\n\n    Args:\n        logit (2-D Array): Logit array of shape (N, C)\n        labels (2-D Array): Onehot label array of shape (N, C)\n\n    Returns:\n        2-D Array: Backpropagated gradients of loss (N, C)\n\n    Notes:\n        [1] Neural Net Backprop in one slide! by Sung Kim\n        https://docs.google.com/presentation/d/1_ZmtfEjLmhbuM_PqbDYMXXLAqeWN0HwuhcSKnUQZ6MM/edit#slide=id.g1ec1d04b5a_1_83\n\n    """"""\n    return softmax(logit) - labels\n\n\ndef get_accuracy(logit, labels):\n    """"""Returna an accracy\n\n    Args:\n        logit (2-D Array): Logit array of shape (N, C)\n        labels (2-D Array): Onehot label array of shape (N, C)\n\n    Returns:\n        float: Accuracy\n    """"""\n\n    probs = softmax(logit)\n    pred = np.argmax(probs, axis=1)\n    true = np.argmax(labels, axis=1)\n\n    return np.mean(pred == true)\n\n\nLEARNING_RATE = 0.1\nMAX_ITER = 2000\nPRINT_N = 10\n\nfor i in range(MAX_ITER):\n\n    logit = affine_forward(X_train, W, b)\n    loss = softmax_cross_entropy_loss(logit, y_train_onehot)\n    d_loss = grad_softmax_cross_entropy_loss(logit, y_train_onehot)\n\n    d_W = np.dot(X_train.T, d_loss) / N\n    d_b = np.sum(d_loss) / N\n\n    W -= LEARNING_RATE * d_W\n    b -= LEARNING_RATE * d_b\n\n    if i % (MAX_ITER // PRINT_N) == 0:\n        acc = get_accuracy(logit, y_train_onehot)\n        print(""[Step: {:5}] Loss: {:<10.5} Acc: {:.2%}"".format(i, loss, acc))\n\n""""""\n[Step:     0] Loss: 0.76726    Acc: 31.68%\n[Step:   200] Loss: 0.057501   Acc: 87.13%\n[Step:   400] Loss: 0.034893   Acc: 92.08%\n[Step:   600] Loss: 0.025472   Acc: 97.03%\n[Step:   800] Loss: 0.020099   Acc: 97.03%\n[Step:  1000] Loss: 0.016562   Acc: 99.01%\n[Step:  1200] Loss: 0.014058   Acc: 100.00%\n[Step:  1400] Loss: 0.012204   Acc: 100.00%\n[Step:  1600] Loss: 0.010784   Acc: 100.00%\n[Step:  1800] Loss: 0.0096631  Acc: 100.00%\n""""""\n'"
pytorch/lab-02-1&2-linear_regression.py,0,"b'# Lab 2 Linear Regression\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\ntorch.manual_seed(777)   # for reproducibility\n\n# X and Y data\nx_train = [[1], [2], [3]]\ny_train = [[1], [2], [3]]\nX = Variable(torch.Tensor(x_train))\nY = Variable(torch.Tensor(y_train))\n\n# Our hypothesis XW+b\nmodel = nn.Linear(1, 1, bias=True)\n\n# cost criterion\ncriterion = nn.MSELoss()\n\n# Minimize\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Train the model\nfor step in range(2001):\n    optimizer.zero_grad()\n    # Our hypothesis\n    hypothesis = model(X)\n    cost = criterion(hypothesis, Y)\n    cost.backward()\n    optimizer.step()\n\n    if step % 20 == 0:\n        print(step, cost.data.numpy(), model.weight.data.numpy(), model.bias.data.numpy())\n\n\n# Testing our model\npredicted = model(Variable(torch.Tensor([[5]])))\nprint(predicted.data.numpy())\npredicted = model(Variable(torch.Tensor([[2.5]])))\nprint(predicted.data.numpy())\npredicted = model(Variable(torch.Tensor([[1.5], [3.5]])))\nprint(predicted.data.numpy())\n'"
pytorch/lab-04-2-multi_variable_linear_regression.py,0,"b'# Lab 4 Multi-variable linear regression\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\ntorch.manual_seed(777)   # for reproducibility\n\n# X and Y data\nx_data = [[73., 80., 75.], [93., 88., 93.],\n          [89., 91., 90.], [96., 98., 100.], [73., 66., 70.]]\ny_data = [[152.], [185.], [180.], [196.], [142.]]\n\nX = Variable(torch.Tensor(x_data))\nY = Variable(torch.Tensor(y_data))\n\n# Our hypothesis XW+b\nmodel = nn.Linear(3, 1, bias=True)\n\n# cost criterion\ncriterion = nn.MSELoss()\n\n# Minimize\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n\n# Train the model\nfor step in range(2001):\n    optimizer.zero_grad()\n    # Our hypothesis\n    hypothesis = model(X)\n    cost = criterion(hypothesis, Y)\n    cost.backward()\n    optimizer.step()\n\n    if step % 10 == 0:\n        print(step, ""Cost: "", cost.data.numpy(), ""\\nPrediction:\\n"", hypothesis.data.numpy())\n'"
pytorch/lab-04-3-file_input_linear_regression.py,0,"b'# Lab 4 Multi-variable linear regression\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\ntorch.manual_seed(777)   # for reproducibility\n\nxy = np.loadtxt(\'data-01-test-score.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\n# Make sure the shape and data are OK\nprint(x_data.shape, x_data, len(x_data))\nprint(y_data.shape, y_data)\n\nx_data = Variable(torch.from_numpy(x_data))\ny_data = Variable(torch.from_numpy(y_data))\n\n# Our hypothesis XW+b\nmodel = nn.Linear(3, 1, bias=True)\n\n# cost criterion\ncriterion = nn.MSELoss()\n\n# Minimize\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n\n# Train the model\nfor step in range(2001):\n    optimizer.zero_grad()\n    # Our hypothesis\n    hypothesis = model(x_data)\n    cost = criterion(hypothesis, y_data)\n    cost.backward()\n    optimizer.step()\n\n    if step % 10 == 0:\n        print(step, ""Cost: "", cost.data.numpy(), ""\\nPrediction:\\n"", hypothesis.data.numpy())\n\n# Ask my score\nprint(""Your score will be "", model(Variable(torch.Tensor([[100, 70, 101]]))).data.numpy())\nprint(""Other scores will be "", model(Variable(torch.Tensor([[60, 70, 110], [90, 100, 80]]))).data.numpy())\n'"
pytorch/lab-05-1-logistic_regression.py,1,"b'# Lab 5 Logistic Regression Classifier\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\ntorch.manual_seed(777)\n\nx_data = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]], dtype=np.float32)\ny_data = np.array([[0], [0], [0], [1], [1], [1]], dtype=np.float32)\n\nX = Variable(torch.from_numpy(x_data))\nY = Variable(torch.from_numpy(y_data))\n\n# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\nlinear = torch.nn.Linear(2, 1, bias=True)\nsigmoid = torch.nn.Sigmoid()\nmodel = torch.nn.Sequential(linear, sigmoid)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nfor step in range(10001):\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    # cost/loss function\n    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n             * torch.log(1 - hypothesis)).mean()\n    cost.backward()\n    optimizer.step()\n\n    if step % 200 == 0:\n        print(step, cost.data.numpy())\n\n# Accuracy computation\npredicted = (model(X).data > 0.5).float()\naccuracy = (predicted == Y.data).float().mean()\nprint(""\\nHypothesis: "", hypothesis.data.numpy(), ""\\nCorrect (Y): "", predicted.numpy(), ""\\nAccuracy: "", accuracy)\n'"
pytorch/lab-05-2-logistic_regression_diabetes.py,0,"b'# Lab 5 Logistic Regression Classifier\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\ntorch.manual_seed(777)  # for reproducibility\n\nxy = np.loadtxt(\'data-03-diabetes.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\n# Make sure the shape and data are OK\nprint(x_data.shape, y_data.shape)\n\nX = Variable(torch.from_numpy(x_data))\nY = Variable(torch.from_numpy(y_data))\n\n# Hypothesis using sigmoid\nlinear = torch.nn.Linear(8, 1, bias=True)\nsigmoid = torch.nn.Sigmoid()\nmodel = torch.nn.Sequential(linear, sigmoid)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nfor step in range(10001):\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    # cost/loss function\n    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n             * torch.log(1 - hypothesis)).mean()\n    cost.backward()\n    optimizer.step()\n\n    if step % 200 == 0:\n        print(step, cost.data.numpy())\n\n# Accuracy computation\npredicted = (model(X).data > 0.5).float()\naccuracy = (predicted == Y.data).float().mean()\nprint(""\\nHypothesis: "", hypothesis.data.numpy(), ""\\nCorrect (Y): "", predicted.numpy(), ""\\nAccuracy: "", accuracy)\n'"
pytorch/lab-06-1-softmax_classifier.py,1,"b""# Lab 6 Softmax Classifier\nimport torch\nfrom torch.autograd import Variable\n\ntorch.manual_seed(777)  # for reproducibility\n\nx_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5],\n          [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\ny_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],\n          [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n\nX = Variable(torch.Tensor(x_data))\nY = Variable(torch.Tensor(y_data))\nnb_classes = 3\n\n# tf.nn.softmax computes softmax activations\n# softmax = exp(logits) / reduce_sum(exp(logits), dim)\nsoftmax = torch.nn.Softmax()\nlinear = torch.nn.Linear(4, nb_classes, bias=True)\nmodel = torch.nn.Sequential(linear, softmax)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nfor step in range(2001):\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    # Cross entropy cost/loss\n    cost = -Y * torch.log(hypothesis)\n    cost = torch.sum(cost, 1).mean()\n    cost.backward()\n    optimizer.step()\n\n    if step % 200 == 0:\n        print(step, cost.data.numpy())\n\n# Testing & One-hot encoding\nprint('--------------')\na = model(Variable(torch.Tensor([[1, 11, 7, 9]])))\nprint(a.data.numpy(), torch.max(a, 1)[1].data.numpy())\n\nprint('--------------')\nb = model(Variable(torch.Tensor([[1, 3, 4, 3]])))\nprint(b.data.numpy(), torch.max(b, 1)[1].data.numpy())\n\nprint('--------------')\nc = model(Variable(torch.Tensor([[1, 1, 0, 1]])))\nprint(c.data.numpy(), torch.max(c, 1)[1].data.numpy())\n\nprint('--------------')\nall = model(Variable(torch.Tensor([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]])))\nprint(all.data.numpy(), torch.max(all, 1)[1].data.numpy())\n"""
pytorch/lab-06-2-softmax_zoo_classifier.py,0,"b'# Lab 6 Softmax Classifier\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\ntorch.manual_seed(777)  # for reproducibility\n\n# Predicting animal type based on various features\nxy = np.loadtxt(\'data-04-zoo.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(x_data.shape, y_data.shape)\n\nnb_classes = 7  # 0 ~ 6\n\nX = Variable(torch.from_numpy(x_data))\nY = Variable(torch.from_numpy(y_data))\n\n# one hot encoding\nY_one_hot = torch.zeros(Y.size()[0], nb_classes)\nY_one_hot.scatter_(1, Y.long().data, 1)\nY_one_hot = Variable(Y_one_hot)\nprint(""one_hot"", Y_one_hot.data)\n\nsoftmax = torch.nn.Softmax()\nmodel = torch.nn.Linear(16, nb_classes, bias=True)\n\n# Cross entropy cost/loss\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nfor step in range(2001):\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    # Label has to be 1D LongTensor\n    cost = criterion(hypothesis, Y.long().view(-1))\n    cost.backward()\n    optimizer.step()\n\n    prediction = torch.max(softmax(hypothesis), 1)[1].float()\n\n    correct_prediction = (prediction.data == Y.data)\n    accuracy = correct_prediction.float().mean()\n\n    if step % 100 == 0:\n        print(""Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}"".format(step, cost.data[0], accuracy))\n\n\n# Let\'s see if we can predict\npred = torch.max(softmax(hypothesis), 1)[1].float()\n\nfor p, y in zip(pred, Y):\n    print(""[{}] Prediction: {} True Y: {}"".format(bool(p.data[0] == y.data[0]), p.data.int()[0], y.data.int()[0]))\n'"
pytorch/lab-09-1-xor.py,0,"b'# Lab 9 XOR\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\ntorch.manual_seed(777)  # for reproducibility\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nX = Variable(torch.from_numpy(x_data))\nY = Variable(torch.from_numpy(y_data))\n\n# Hypothesis using sigmoid\nlinear = torch.nn.Linear(2, 1, bias=True)\nsigmoid = torch.nn.Sigmoid()\nmodel = torch.nn.Sequential(linear, sigmoid)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nfor step in range(10001):\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    # cost/loss function\n    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n             * torch.log(1 - hypothesis)).mean()\n    cost.backward()\n    optimizer.step()\n\n    if step % 100 == 0:\n        print(step, cost.data.numpy())\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = (model(X).data > 0.5).float()\naccuracy = (predicted == Y.data).float().mean()\nprint(""\\nHypothesis: "", hypothesis.data.numpy(), ""\\nCorrect: "", predicted.numpy(), ""\\nAccuracy: "", accuracy)\n'"
pytorch/lab-09-2-xor-nn.py,0,"b'# Lab 9 XOR\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\ntorch.manual_seed(777)  # for reproducibility\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nX = Variable(torch.from_numpy(x_data))\nY = Variable(torch.from_numpy(y_data))\n\nlinear1 = torch.nn.Linear(2, 2, bias=True)\nlinear2 = torch.nn.Linear(2, 1, bias=True)\nsigmoid = torch.nn.Sigmoid()\nmodel = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nfor step in range(10001):\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    # cost/loss function\n    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n             * torch.log(1 - hypothesis)).mean()\n    cost.backward()\n    optimizer.step()\n\n    if step % 100 == 0:\n        print(step, cost.data.numpy())\n\n# Accuracy computation\n# True if hypothesis>0.5 else False\npredicted = (model(X).data > 0.5).float()\naccuracy = (predicted == Y.data).float().mean()\nprint(""\\nHypothesis: "", hypothesis.data.numpy(), ""\\nCorrect: "", predicted.numpy(), ""\\nAccuracy: "", accuracy)\n'"
pytorch/lab-10-1-mnist_softmax.py,0,"b'# Lab 10 MNIST and softmax\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport random\n\ntorch.manual_seed(777)  # reproducibility\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# MNIST dataset\nmnist_train = dsets.MNIST(root=\'MNIST_data/\',\n                          train=True,\n                          transform=transforms.ToTensor(),\n                          download=True)\n\nmnist_test = dsets.MNIST(root=\'MNIST_data/\',\n                         train=False,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# dataset loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\n# model\nmodel = torch.nn.Linear(784, 10, bias=True)\n\n# define cost/loss & optimizer\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = len(mnist_train) // batch_size\n\n    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n        # reshape input image into [batch_size by 784]\n        X = Variable(batch_xs.view(-1, 28 * 28))\n        Y = Variable(batch_ys)    # label is not one-hot encoded\n\n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = criterion(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost.data[0]))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nX_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\nY_test = Variable(mnist_test.test_labels)\n\nprediction = model(X_test)\ncorrect_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\naccuracy = correct_prediction.float().mean()\nprint(\'Accuracy:\', accuracy)\n\n# Get one and predict\nr = random.randint(0, len(mnist_test) - 1)\nX_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\nY_single_data = Variable(mnist_test.test_labels[r:r + 1])\n\nprint(""Label: "", Y_single_data.data)\nsingle_prediction = model(X_single_data)\nprint(""Prediction: "", torch.max(single_prediction.data, 1)[1])\n'"
pytorch/lab-10-2-mnist_nn.py,0,"b'# Lab 10 MNIST and nn\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport random\n\ntorch.manual_seed(777)  # reproducibility\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# MNIST dataset\nmnist_train = dsets.MNIST(root=\'MNIST_data/\',\n                          train=True,\n                          transform=transforms.ToTensor(),\n                          download=True)\n\nmnist_test = dsets.MNIST(root=\'MNIST_data/\',\n                         train=False,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# dataset loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n# nn layers\nlinear1 = torch.nn.Linear(784, 256, bias=True)\nlinear2 = torch.nn.Linear(256, 256, bias=True)\nlinear3 = torch.nn.Linear(256, 10, bias=True)\nrelu = torch.nn.ReLU()\n\n# model\nmodel = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n\n# define cost/loss & optimizer\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = len(mnist_train) // batch_size\n\n    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n        # reshape input image into [batch_size by 784]\n        X = Variable(batch_xs.view(-1, 28 * 28))\n        Y = Variable(batch_ys)    # label is not one-hot encoded\n\n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = criterion(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost.data[0]))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nX_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\nY_test = Variable(mnist_test.test_labels)\n\nprediction = model(X_test)\ncorrect_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\naccuracy = correct_prediction.float().mean()\nprint(\'Accuracy:\', accuracy)\n\n# Get one and predict\nr = random.randint(0, len(mnist_test) - 1)\nX_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\nY_single_data = Variable(mnist_test.test_labels[r:r + 1])\n\nprint(""Label: "", Y_single_data.data)\nsingle_prediction = model(X_single_data)\nprint(""Prediction: "", torch.max(single_prediction.data, 1)[1])\n'"
pytorch/lab-10-3-mnist_nn_xavier.py,0,"b'# Lab 10 MNIST and Xavier\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport random\nimport torch.nn.init\n\ntorch.manual_seed(777)  # reproducibility\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# MNIST dataset\nmnist_train = dsets.MNIST(root=\'MNIST_data/\',\n                          train=True,\n                          transform=transforms.ToTensor(),\n                          download=True)\n\nmnist_test = dsets.MNIST(root=\'MNIST_data/\',\n                         train=False,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# dataset loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n# nn layers\nlinear1 = torch.nn.Linear(784, 256, bias=True)\nlinear2 = torch.nn.Linear(256, 256, bias=True)\nlinear3 = torch.nn.Linear(256, 10, bias=True)\nrelu = torch.nn.ReLU()\n\n# xavier initializer\ntorch.nn.init.xavier_uniform(linear1.weight)\ntorch.nn.init.xavier_uniform(linear2.weight)\ntorch.nn.init.xavier_uniform(linear3.weight)\n\n# model\nmodel = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n\n# define cost/loss & optimizer\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = len(mnist_train) // batch_size\n\n    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n        # reshape input image into [batch_size by 784]\n        X = Variable(batch_xs.view(-1, 28 * 28))\n        Y = Variable(batch_ys)    # label is not one-hot encoded\n\n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = criterion(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost.data[0]))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nX_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\nY_test = Variable(mnist_test.test_labels)\n\nprediction = model(X_test)\ncorrect_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\naccuracy = correct_prediction.float().mean()\nprint(\'Accuracy:\', accuracy)\n\n# Get one and predict\nr = random.randint(0, len(mnist_test) - 1)\nX_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\nY_single_data = Variable(mnist_test.test_labels[r:r + 1])\n\nprint(""Label: "", Y_single_data.data)\nsingle_prediction = model(X_single_data)\nprint(""Prediction: "", torch.max(single_prediction.data, 1)[1])\n'"
pytorch/lab-10-4-mnist_nn_deep.py,0,"b'# Lab 10 MNIST and Deep learning\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport random\nimport torch.nn.init\n\ntorch.manual_seed(777)  # reproducibility\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# MNIST dataset\nmnist_train = dsets.MNIST(root=\'MNIST_data/\',\n                          train=True,\n                          transform=transforms.ToTensor(),\n                          download=True)\n\nmnist_test = dsets.MNIST(root=\'MNIST_data/\',\n                         train=False,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# dataset loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n# nn layers\nlinear1 = torch.nn.Linear(784, 512, bias=True)\nlinear2 = torch.nn.Linear(512, 512, bias=True)\nlinear3 = torch.nn.Linear(512, 512, bias=True)\nlinear4 = torch.nn.Linear(512, 512, bias=True)\nlinear5 = torch.nn.Linear(512, 10, bias=True)\nrelu = torch.nn.ReLU()\n\n# xavier initializer\ntorch.nn.init.xavier_uniform(linear1.weight)\ntorch.nn.init.xavier_uniform(linear2.weight)\ntorch.nn.init.xavier_uniform(linear3.weight)\ntorch.nn.init.xavier_uniform(linear4.weight)\ntorch.nn.init.xavier_uniform(linear5.weight)\n\n# model\nmodel = torch.nn.Sequential(linear1, relu,\n                            linear2, relu,\n                            linear3, relu,\n                            linear4, relu,\n                            linear5)\n\n# define cost/loss & optimizer\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = len(mnist_train) // batch_size\n\n    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n        # reshape input image into [batch_size by 784]\n        X = Variable(batch_xs.view(-1, 28 * 28))\n        Y = Variable(batch_ys)    # label is not one-hot encoded\n\n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = criterion(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost.data[0]))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nX_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\nY_test = Variable(mnist_test.test_labels)\n\nprediction = model(X_test)\ncorrect_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\naccuracy = correct_prediction.float().mean()\nprint(\'Accuracy:\', accuracy)\n\n# Get one and predict\nr = random.randint(0, len(mnist_test) - 1)\nX_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\nY_single_data = Variable(mnist_test.test_labels[r:r + 1])\n\nprint(""Label: "", Y_single_data.data)\nsingle_prediction = model(X_single_data)\nprint(""Prediction: "", torch.max(single_prediction.data, 1)[1])\n'"
pytorch/lab-10-5-mnist_nn_dropout.py,0,"b'# Lab 10 MNIST and Dropout\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport random\nimport torch.nn.init\n\ntorch.manual_seed(777)  # reproducibility\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\nkeep_prob = 0.7\n\n# MNIST dataset\nmnist_train = dsets.MNIST(root=\'MNIST_data/\',\n                          train=True,\n                          transform=transforms.ToTensor(),\n                          download=True)\n\nmnist_test = dsets.MNIST(root=\'MNIST_data/\',\n                         train=False,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# dataset loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n# nn layers\nlinear1 = torch.nn.Linear(784, 512, bias=True)\nlinear2 = torch.nn.Linear(512, 512, bias=True)\nlinear3 = torch.nn.Linear(512, 512, bias=True)\nlinear4 = torch.nn.Linear(512, 512, bias=True)\nlinear5 = torch.nn.Linear(512, 10, bias=True)\n\nrelu = torch.nn.ReLU()\n# p is the probability of being dropped in PyTorch\ndropout = torch.nn.Dropout(p=1 - keep_prob)\n\n# xavier initializer\ntorch.nn.init.xavier_uniform(linear1.weight)\ntorch.nn.init.xavier_uniform(linear2.weight)\ntorch.nn.init.xavier_uniform(linear3.weight)\ntorch.nn.init.xavier_uniform(linear4.weight)\ntorch.nn.init.xavier_uniform(linear5.weight)\n\n# model\nmodel = torch.nn.Sequential(linear1, relu, dropout,\n                            linear2, relu, dropout,\n                            linear3, relu, dropout,\n                            linear4, relu, dropout,\n                            linear5)\n\n# define cost/loss & optimizer\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# train my model\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = len(mnist_train) // batch_size\n\n    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n        # reshape input image into [batch_size by 784]\n        X = Variable(batch_xs.view(-1, 28 * 28))\n        Y = Variable(batch_ys)    # label is not one-hot encoded\n\n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = criterion(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost.data[0]))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nmodel.eval()    # set the model to evaluation mode (dropout=False)\n\nX_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\nY_test = Variable(mnist_test.test_labels)\n\nprediction = model(X_test)\ncorrect_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\naccuracy = correct_prediction.float().mean()\nprint(\'Accuracy:\', accuracy)\n\n# Get one and predict\nr = random.randint(0, len(mnist_test) - 1)\nX_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\nY_single_data = Variable(mnist_test.test_labels[r:r + 1])\n\nprint(""Label: "", Y_single_data.data)\nsingle_prediction = model(X_single_data)\nprint(""Prediction: "", torch.max(single_prediction.data, 1)[1])\n'"
pytorch/lab-11-1-mnist_cnn.py,0,"b'# Lab 11 MNIST and Convolutional Neural Network\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport torch.nn.init\n\ntorch.manual_seed(777)  # reproducibility\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# MNIST dataset\nmnist_train = dsets.MNIST(root=\'MNIST_data/\',\n                          train=True,\n                          transform=transforms.ToTensor(),\n                          download=True)\n\nmnist_test = dsets.MNIST(root=\'MNIST_data/\',\n                         train=False,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# dataset loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\n# CNN Model (2 conv layers)\n\n\nclass CNN(torch.nn.Module):\n\n    def __init__(self):\n        super(CNN, self).__init__()\n        # L1 ImgIn shape=(?, 28, 28, 1)\n        #    Conv     -> (?, 28, 28, 32)\n        #    Pool     -> (?, 14, 14, 32)\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        # L2 ImgIn shape=(?, 14, 14, 32)\n        #    Conv      ->(?, 14, 14, 64)\n        #    Pool      ->(?, 7, 7, 64)\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        # Final FC 7x7x64 inputs -> 10 outputs\n        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n        torch.nn.init.xavier_uniform(self.fc.weight)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)   # Flatten them for FC\n        out = self.fc(out)\n        return out\n\n\n# instantiate CNN model\nmodel = CNN()\n\n# define cost/loss & optimizer\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# train my model\nprint(\'Learning started. It takes sometime.\')\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = len(mnist_train) // batch_size\n\n    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n        X = Variable(batch_xs)    # image is already size of (28x28), no reshape\n        Y = Variable(batch_ys)    # label is not one-hot encoded\n\n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = criterion(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost.data / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost[0]))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nmodel.eval()    # set the model to evaluation mode (dropout=False)\n\nX_test = Variable(mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float())\nY_test = Variable(mnist_test.test_labels)\n\nprediction = model(X_test)\ncorrect_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\naccuracy = correct_prediction.float().mean()\nprint(\'Accuracy:\', accuracy)\n'"
pytorch/lab-11-2-mnist_deep_cnn.py,0,"b'# Lab 11 MNIST and Deep learning CNN\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport torch.nn.init\n\ntorch.manual_seed(777)  # reproducibility\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\nkeep_prob = 0.7\n\n# MNIST dataset\nmnist_train = dsets.MNIST(root=\'MNIST_data/\',\n                          train=True,\n                          transform=transforms.ToTensor(),\n                          download=True)\n\nmnist_test = dsets.MNIST(root=\'MNIST_data/\',\n                         train=False,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# dataset loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\n# CNN Model\n\n\nclass CNN(torch.nn.Module):\n\n    def __init__(self):\n        super(CNN, self).__init__()\n        # L1 ImgIn shape=(?, 28, 28, 1)\n        #    Conv     -> (?, 28, 28, 32)\n        #    Pool     -> (?, 14, 14, 32)\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n            torch.nn.Dropout(p=1 - keep_prob))\n        # L2 ImgIn shape=(?, 14, 14, 32)\n        #    Conv      ->(?, 14, 14, 64)\n        #    Pool      ->(?, 7, 7, 64)\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n            torch.nn.Dropout(p=1 - keep_prob))\n        # L3 ImgIn shape=(?, 7, 7, 64)\n        #    Conv      ->(?, 7, 7, 128)\n        #    Pool      ->(?, 4, 4, 128)\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n            torch.nn.Dropout(p=1 - keep_prob))\n\n        # L4 FC 4x4x128 inputs -> 625 outputs\n        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n        torch.nn.init.xavier_uniform(self.fc1.weight)\n        self.layer4 = torch.nn.Sequential(\n            self.fc1,\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=1 - keep_prob))\n        # L5 Final FC 625 inputs -> 10 outputs\n        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n        torch.nn.init.xavier_uniform(self.fc2.weight)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = out.view(out.size(0), -1)   # Flatten them for FC\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\n\n# instantiate CNN model\nmodel = CNN()\n\n# define cost/loss & optimizer\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# train my model\nprint(\'Learning started. It takes sometime.\')\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = len(mnist_train) // batch_size\n\n    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n        X = Variable(batch_xs)    # image is already size of (28x28), no reshape\n        Y = Variable(batch_ys)    # label is not one-hot encoded\n\n        optimizer.zero_grad()\n        hypothesis = model(X)\n        cost = criterion(hypothesis, Y)\n        cost.backward()\n        optimizer.step()\n\n        avg_cost += cost.data / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost[0]))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nmodel.eval()    # set the model to evaluation mode (dropout=False)\n\nX_test = Variable(mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float())\nY_test = Variable(mnist_test.test_labels)\n\nprediction = model(X_test)\ncorrect_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\naccuracy = correct_prediction.float().mean()\nprint(\'Accuracy:\', accuracy)\n'"
pytorch/lab-11-3-mnist_cnn_class.py,0,"b'# Lab 11 MNIST and Deep learning CNN\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport torch.nn.init\n\ntorch.manual_seed(777)  # reproducibility\n\n# parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# MNIST dataset\nmnist_train = dsets.MNIST(root=\'MNIST_data/\',\n                          train=True,\n                          transform=transforms.ToTensor(),\n                          download=True)\n\nmnist_test = dsets.MNIST(root=\'MNIST_data/\',\n                         train=False,\n                         transform=transforms.ToTensor(),\n                         download=True)\n\n# dataset loader\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\n# CNN Model\n\n\nclass CNN(torch.nn.Module):\n\n    def __init__(self):\n        super(CNN, self).__init__()\n        self._build_net()\n\n    def _build_net(self):\n        # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n        self.keep_prob = 0.7\n        # L1 ImgIn shape=(?, 28, 28, 1)\n        #    Conv     -> (?, 28, 28, 32)\n        #    Pool     -> (?, 14, 14, 32)\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n            torch.nn.Dropout(p=1 - self.keep_prob))\n        # L2 ImgIn shape=(?, 14, 14, 32)\n        #    Conv      ->(?, 14, 14, 64)\n        #    Pool      ->(?, 7, 7, 64)\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n            torch.nn.Dropout(p=1 - self.keep_prob))\n        # L3 ImgIn shape=(?, 7, 7, 64)\n        #    Conv      ->(?, 7, 7, 128)\n        #    Pool      ->(?, 4, 4, 128)\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n            torch.nn.Dropout(p=1 - self.keep_prob))\n        # L4 FC 4x4x128 inputs -> 625 outputs\n        self.keep_prob = 0.5\n        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n        torch.nn.init.xavier_uniform(self.fc1.weight)\n        self.layer4 = torch.nn.Sequential(\n            self.fc1,\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=1 - self.keep_prob))\n        # L5 Final FC 625 inputs -> 10 outputs\n        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n        torch.nn.init.xavier_uniform(self.fc2.weight)\n\n        # define cost/loss & optimizer\n        self.criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = out.view(out.size(0), -1)   # Flatten them for FC\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\n    def predict(self, x):\n        self.eval()\n        return self.forward(x)\n\n    def get_accuracy(self, x, y):\n        prediction = self.predict(x)\n        correct_prediction = (torch.max(prediction.data, 1)[1] == y.data)\n        self.accuracy = correct_prediction.float().mean()\n        return self.accuracy\n\n    def train_model(self, x, y):\n        self.train()\n        self.optimizer.zero_grad()\n        hypothesis = self.forward(x)\n        self.cost = self.criterion(hypothesis, y)\n        self.cost.backward()\n        self.optimizer.step()\n        return self.cost\n\n\n# instantiate CNN model\nmodel = CNN()\n\n# train my model\nprint(\'Learning started. It takes sometime.\')\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = len(mnist_train) // batch_size\n\n    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n        X = Variable(batch_xs)    # image is already size of (28x28), no reshape\n        Y = Variable(batch_ys)    # label is not one-hot encoded\n\n        cost = model.train_model(X, Y)\n\n        avg_cost += cost.data / total_batch\n\n    print(""[Epoch: {:>4}] cost = {:>.9}"".format(epoch + 1, avg_cost[0]))\n\nprint(\'Learning Finished!\')\n\n# Test model and check accuracy\nX_test = Variable(mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float())\nY_test = Variable(mnist_test.test_labels)\n\nprint(\'Accuracy:\', model.get_accuracy(X_test, Y_test))\n'"
pytorch/lab-12-1-hello-rnn.py,0,"b'# Lab 12 RNN\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ntorch.manual_seed(777)  # reproducibility\r\n\r\n# hyperparameters\r\nlearning_rate = 0.1\r\nnum_epochs = 15\r\n\r\nidx2char = [\'h\', \'i\', \'e\', \'l\', \'o\']\r\n\r\n# Teach hello: hihell -> ihello\r\nx_data = [[0, 1, 0, 2, 3, 3]]   # hihell\r\nx_one_hot = [[[1, 0, 0, 0, 0],   # h 0\r\n              [0, 1, 0, 0, 0],   # i 1\r\n              [1, 0, 0, 0, 0],   # h 0\r\n              [0, 0, 1, 0, 0],   # e 2\r\n              [0, 0, 0, 1, 0],   # l 3\r\n              [0, 0, 0, 1, 0]]]  # l 3\r\n\r\ny_data = [1, 0, 2, 3, 3, 4]    # ihello\r\n\r\n# As we have one batch of samples, we will change them to variables only once\r\ninputs = torch.Tensor(x_one_hot)\r\nlabels = torch.LongTensor(y_data)\r\n\r\ninputs = Variable(inputs)\r\nlabels = Variable(labels)\r\n\r\nnum_classes = 5\r\ninput_size = 5  # one-hot size\r\nhidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\r\nbatch_size = 1   # one sentence\r\nsequence_length = 6  # |ihello| == 6\r\nnum_layers = 1  # one-layer rnn\r\n\r\n\r\nclass RNN(nn.Module):\r\n\r\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\r\n        super(RNN, self).__init__()\r\n        self.num_classes = num_classes\r\n        self.num_layers = num_layers\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.sequence_length = sequence_length\r\n        # Set parameters for RNN block\r\n        # Note: batch_first=False by default.\r\n        # When true, inputs are (batch_size, sequence_length, input_dimension)\r\n        # instead of (sequence_length, batch_size, input_dimension)\r\n        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size,\r\n                          num_layers=num_layers, batch_first=True)\r\n        # Fully connected layer to obtain outputs corresponding to the number\r\n        # of classes\r\n        self.fc = nn.Linear(hidden_size, num_classes)\r\n\r\n    def forward(self, x):\r\n        # Initialize hidden and cell states\r\n        h_0 = Variable(torch.zeros(\r\n            x.size(0), self.num_layers, self.hidden_size))\r\n\r\n        # Reshape input\r\n        x.view(x.size(0), self.sequence_length, self.input_size)\r\n\r\n        # Propagate input through RNN\r\n        # Input: (batch, seq_len, input_size)\r\n        # h_0: (batch, num_layers * num_directions, hidden_size)\r\n\r\n        out, _ = self.rnn(x, h_0)\r\n\r\n        # Reshape output from (batch, seq_len, hidden_size) to (batch *\r\n        # seq_len, hidden_size)\r\n        out = out.view(-1, self.hidden_size)\r\n        # Return outputs applied to fully connected layer\r\n        out = self.fc(out)\r\n        return out\r\n\r\n\r\n# Instantiate RNN model\r\nrnn = RNN(num_classes, input_size, hidden_size, num_layers)\r\n\r\n# Set loss and optimizer function\r\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\r\n\r\n# Train the model\r\nfor epoch in range(num_epochs):\r\n    outputs = rnn(inputs)\r\n    optimizer.zero_grad()\r\n    loss = criterion(outputs, labels)\r\n    loss.backward()\r\n    optimizer.step()\r\n    _, idx = outputs.max(1)\r\n    idx = idx.data.numpy()\r\n    result_str = [idx2char[c] for c in idx.squeeze()]\r\n    print(""epoch: %d, loss: %1.3f"" % (epoch + 1, loss.data[0]))\r\n    print(""Predicted string: "", \'\'.join(result_str))\r\n\r\nprint(""Learning finished!"")\r\n'"
pytorch/lab-12-2-char-seq-rnn.py,0,"b'# Lab 12 Character Sequence RNN\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ntorch.manual_seed(777)  # reproducibility\r\n\r\nsample = "" if you want you""\r\nidx2char = list(set(sample))  # index -> char\r\nchar2idx = {c: i for i, c in enumerate(idx2char)}  # char -> index\r\n\r\n# hyperparameters\r\nlearning_rate = 0.1\r\nnum_epochs = 50\r\ninput_size = len(char2idx)  # RNN input size (one hot size)\r\nhidden_size = len(char2idx)  # RNN output size\r\nnum_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\r\nbatch_size = 1  # one sample data, one batch\r\nsequence_length = len(sample) - 1  # number of lstm rollings (unit #)\r\nnum_layers = 1  # number of layers in RNN\r\n\r\nsample_idx = [char2idx[c] for c in sample]  # char to index\r\nx_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\r\ny_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\r\n\r\nx_data = torch.Tensor(x_data)\r\ny_data = torch.LongTensor(y_data)\r\n\r\n# one hot encoding\r\n\r\n\r\ndef one_hot(x, num_classes):\r\n    idx = x.long()\r\n    idx = idx.view(-1, 1)\r\n    x_one_hot = torch.zeros(x.size()[0] * x.size()[1], num_classes)\r\n    x_one_hot.scatter_(1, idx, 1)\r\n    x_one_hot = x_one_hot.view(x.size()[0], x.size()[1], num_classes)\r\n    return x_one_hot\r\n\r\n\r\nx_one_hot = one_hot(x_data, num_classes)\r\n\r\ninputs = Variable(x_one_hot)\r\nlabels = Variable(y_data)\r\n\r\n\r\nclass LSTM(nn.Module):\r\n\r\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\r\n        super(LSTM, self).__init__()\r\n        self.num_classes = num_classes\r\n        self.num_layers = num_layers\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.sequence_length = sequence_length\r\n        # Set parameters for RNN block\r\n        # Note: batch_first=False by default.\r\n        # When true, inputs are (batch_size, sequence_length, input_dimension)\r\n        # instead of (sequence_length, batch_size, input_dimension)\r\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\r\n                            num_layers=num_layers, batch_first=True)\r\n        # Fully connected layer to obtain outputs corresponding to the number\r\n        # of classes\r\n        self.fc = nn.Linear(hidden_size, num_classes)\r\n\r\n    def forward(self, x):\r\n        # Initialize hidden and cell states\r\n        h_0 = Variable(torch.zeros(\r\n            self.num_layers, x.size(0), self.hidden_size))\r\n        c_0 = Variable(torch.zeros(\r\n            self.num_layers, x.size(0), self.hidden_size))\r\n\r\n        # Reshape input\r\n        x.view(x.size(0), self.sequence_length, self.input_size)\r\n\r\n        # Propagate input through RNN\r\n        # Input: (batch, seq_len, input_size)\r\n        # h_0: (num_layers * num_directions, batch, hidden_size)\r\n        out, _ = self.lstm(x, (h_0, c_0))\r\n\r\n        # Reshape output from (batch, seq_len, hidden_size) to (batch *\r\n        # seq_len, hidden_size)\r\n        out = out.view(-1, self.hidden_size)\r\n        # Return outputs applied to fully connected layer\r\n        out = self.fc(out)\r\n        return out\r\n\r\n\r\n# Instantiate RNN model\r\nlstm = LSTM(num_classes, input_size, hidden_size, num_layers)\r\n\r\n# Set loss and optimizer function\r\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\r\n\r\n# Train the model\r\nfor epoch in range(num_epochs):\r\n    outputs = lstm(inputs)\r\n    optimizer.zero_grad()\r\n    loss = criterion(outputs, labels.view(-1))\r\n    loss.backward()\r\n    optimizer.step()\r\n    _, idx = outputs.max(1)\r\n    idx = idx.data.numpy()\r\n    result_str = [idx2char[c] for c in idx.squeeze()]\r\n    print(""epoch: %d, loss: %1.3f"" % (epoch + 1, loss.data[0]))\r\n    print(""Predicted string: "", \'\'.join(result_str))\r\n\r\nprint(""Learning finished!"")\r\n'"
pytorch/lab-12-4-rnn-long_char.py,0,"b'# Lab 12 Character Sequence RNN\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ntorch.manual_seed(777)  # reproducibility\r\n\r\nsentence = (""if you want to build a ship, don\'t drum up people together to ""\r\n            ""collect wood and don\'t assign them tasks and work, but rather ""\r\n            ""teach them to long for the endless immensity of the sea."")\r\n\r\nchar_set = list(set(sentence))\r\nchar_dic = {w: i for i, w in enumerate(char_set)}\r\n\r\n# hyperparameters\r\nlearning_rate = 0.1\r\nnum_epochs = 500\r\ninput_size = len(char_set)  # RNN input size (one hot size)\r\nhidden_size = len(char_set)  # RNN output size\r\nnum_classes = len(char_set)  # final output size (RNN or softmax, etc.)\r\nsequence_length = 10  # any arbitrary number\r\nnum_layers = 2  # number of layers in RNN\r\n\r\ndataX = []\r\ndataY = []\r\nfor i in range(0, len(sentence) - sequence_length):\r\n    x_str = sentence[i:i + sequence_length]\r\n    y_str = sentence[i + 1: i + sequence_length + 1]\r\n    print(i, x_str, \'->\', y_str)\r\n\r\n    x = [char_dic[c] for c in x_str]  # x str to index\r\n    y = [char_dic[c] for c in y_str]  # y str to index\r\n\r\n    dataX.append(x)\r\n    dataY.append(y)\r\n\r\nbatch_size = len(dataX)\r\n\r\nx_data = torch.Tensor(dataX)\r\ny_data = torch.LongTensor(dataY)\r\n\r\n# one hot encoding\r\n\r\n\r\ndef one_hot(x, num_classes):\r\n    idx = x.long()\r\n    idx = idx.view(-1, 1)\r\n    x_one_hot = torch.zeros(x.size()[0] * x.size()[1], num_classes)\r\n    x_one_hot.scatter_(1, idx, 1)\r\n    x_one_hot = x_one_hot.view(x.size()[0], x.size()[1], num_classes)\r\n    return x_one_hot\r\n\r\n\r\nx_one_hot = one_hot(x_data, num_classes)\r\n\r\ninputs = Variable(x_one_hot)\r\nlabels = Variable(y_data)\r\n\r\n\r\nclass LSTM(nn.Module):\r\n\r\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\r\n        super(LSTM, self).__init__()\r\n        self.num_classes = num_classes\r\n        self.num_layers = num_layers\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.sequence_length = sequence_length\r\n        # Set parameters for RNN block\r\n        # Note: batch_first=False by default.\r\n        # When true, inputs are (batch_size, sequence_length, input_dimension)\r\n        # instead of (sequence_length, batch_size, input_dimension)\r\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\r\n                            num_layers=num_layers, batch_first=True)\r\n        # Fully connected layer\r\n        self.fc = nn.Linear(hidden_size, num_classes)\r\n\r\n    def forward(self, x):\r\n        # Initialize hidden and cell states\r\n        h_0 = Variable(torch.zeros(\r\n            self.num_layers, x.size(0), self.hidden_size))\r\n        c_0 = Variable(torch.zeros(\r\n            self.num_layers, x.size(0), self.hidden_size))\r\n        # h_0 = Variable(torch.zeros(\r\n        # self.num_layers, x.size(0), self.hidden_size))\r\n        # c_0 = Variable(torch.zeros(\r\n        # self.num_layers, x.size(0), self.hidden_size))\r\n\r\n        # Propagate input through LSTM\r\n        # Input: (batch, seq_len, input_size)\r\n        out, _ = self.lstm(x, (h_0, c_0))\r\n        # Note: the output tensor of LSTM in this case is a block with holes\r\n        # > add .contiguous() to apply view()\r\n        out = out.contiguous().view(-1, self.hidden_size)\r\n        # Return outputs applied to fully connected layer\r\n        out = self.fc(out)\r\n        return out\r\n\r\n\r\n# Instantiate RNN model\r\nlstm = LSTM(num_classes, input_size, hidden_size, num_layers)\r\n\r\n# Set loss and optimizer function\r\ncriterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\r\n\r\n# Train the model\r\nfor epoch in range(num_epochs):\r\n    outputs = lstm(inputs)\r\n    optimizer.zero_grad()\r\n    # obtain the loss function\r\n    # flatten target labels to match output\r\n    loss = criterion(outputs, labels.view(-1))\r\n    loss.backward()\r\n    optimizer.step()\r\n    # obtain the predicted indices of the next character\r\n    _, idx = outputs.max(1)\r\n    idx = idx.data.numpy()\r\n    idx = idx.reshape(-1, sequence_length)  # (170,10)\r\n    # display the prediction of the last sequence\r\n    result_str = [char_set[c] for c in idx[-1]]\r\n    print(""epoch: %d, loss: %1.3f"" % (epoch + 1, loss.data[0]))\r\n    print(""Predicted string: "", \'\'.join(result_str))\r\n\r\nprint(""Learning finished!"")\r\n'"
pytorch/lab-12-5-stock_prediction.py,0,"b'\'\'\'\r\nThis script shows how to predict stock prices using a basic RNN\r\n\'\'\'\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport os\r\nimport matplotlib\r\n\r\ntorch.manual_seed(777)  # reproducibility\r\n\r\nif ""DISPLAY"" not in os.environ:\r\n    # remove Travis CI Error\r\n    matplotlib.use(\'Agg\')\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\ndef MinMaxScaler(data):\r\n    \'\'\' Min Max Normalization\r\n\r\n    Parameters\r\n    ----------\r\n    data : numpy.ndarray\r\n        input data to be normalized\r\n        shape: [Batch size, dimension]\r\n\r\n    Returns\r\n    ----------\r\n    data : numpy.ndarry\r\n        normalized data\r\n        shape: [Batch size, dimension]\r\n\r\n    References\r\n    ----------\r\n    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\r\n\r\n    \'\'\'\r\n    numerator = data - np.min(data, 0)\r\n    denominator = np.max(data, 0) - np.min(data, 0)\r\n    # noise term prevents the zero division\r\n    return numerator / (denominator + 1e-7)\r\n\r\n\r\n# train Parameters\r\nlearning_rate = 0.01\r\nnum_epochs = 500\r\ninput_size = 5\r\nhidden_size = 5\r\nnum_classes = 1\r\ntimesteps = seq_length = 7\r\nnum_layers = 1  # number of layers in RNN\r\n\r\n# Open, High, Low, Volume, Close\r\nxy = np.loadtxt(\'data-02-stock_daily.csv\', delimiter=\',\')\r\nxy = xy[::-1]  # reverse order (chronically ordered)\r\nxy = MinMaxScaler(xy)\r\nx = xy\r\ny = xy[:, [-1]]  # Close as label\r\n\r\n# build a dataset\r\ndataX = []\r\ndataY = []\r\nfor i in range(0, len(y) - seq_length):\r\n    _x = x[i:i + seq_length]\r\n    _y = y[i + seq_length]  # Next close price\r\n    print(_x, ""->"", _y)\r\n    dataX.append(_x)\r\n    dataY.append(_y)\r\n\r\n# train/test split\r\ntrain_size = int(len(dataY) * 0.7)\r\ntest_size = len(dataY) - train_size\r\ntrainX = torch.Tensor(np.array(dataX[0:train_size]))\r\ntrainX = Variable(trainX)\r\ntestX = torch.Tensor(np.array(dataX[train_size:len(dataX)]))\r\ntestX = Variable(testX)\r\ntrainY = torch.Tensor(np.array(dataY[0:train_size]))\r\ntrainY = Variable(trainY)\r\ntestY = torch.Tensor(np.array(dataY[train_size:len(dataY)]))\r\ntestY = Variable(testY)\r\n\r\n\r\nclass LSTM(nn.Module):\r\n\r\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\r\n        super(LSTM, self).__init__()\r\n        self.num_classes = num_classes\r\n        self.num_layers = num_layers\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.seq_length = seq_length\r\n        # Set parameters for RNN block\r\n        # Note: batch_first=False by default.\r\n        # When true, inputs are (batch_size, sequence_length, input_dimension)\r\n        # instead of (sequence_length, batch_size, input_dimension)\r\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\r\n                            num_layers=num_layers, batch_first=True)\r\n        # Fully connected layer\r\n        self.fc = nn.Linear(hidden_size, num_classes)\r\n\r\n    def forward(self, x):\r\n        # Initialize hidden and cell states\r\n        h_0 = Variable(torch.zeros(\r\n            self.num_layers, x.size(0), self.hidden_size))\r\n        c_0 = Variable(torch.zeros(\r\n            self.num_layers, x.size(0), self.hidden_size))\r\n\r\n        # Propagate input through LSTM\r\n        _, (h_out, _) = self.lstm(x, (h_0, c_0))\r\n        h_out = h_out.view(-1, self.hidden_size)\r\n        out = self.fc(h_out)\r\n        return out\r\n\r\n\r\n# Instantiate RNN model\r\nlstm = LSTM(num_classes, input_size, hidden_size, num_layers)\r\n\r\n# Set loss and optimizer function\r\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\r\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\r\n\r\n# Train the model\r\nfor epoch in range(num_epochs):\r\n    outputs = lstm(trainX)\r\n    optimizer.zero_grad()\r\n    # obtain the loss function\r\n    loss = criterion(outputs, trainY)\r\n    loss.backward()\r\n    optimizer.step()\r\n    print(""Epoch: %d, loss: %1.5f"" % (epoch, loss.data[0]))\r\n\r\nprint(""Learning finished!"")\r\n\r\n# Test the model\r\nlstm.eval()\r\ntest_predict = lstm(testX)\r\n\r\n# Plot predictions\r\ntest_predict = test_predict.data.numpy()\r\ntestY = testY.data.numpy()\r\nplt.plot(testY)\r\nplt.plot(test_predict)\r\nplt.xlabel(""Time Period"")\r\nplt.ylabel(""Stock Price"")\r\nplt.show()\r\n'"
tests/test_square.py,6,"b""# https://www.tensorflow.org/api_guides/python/test\n\nimport tensorflow as tf\nimport numpy as np\n\n\nclass SquareTest(tf.test.TestCase):\n\n    def testSquare(self):\n        with self.test_session():\n            x = tf.square([2, 3])\n            self.assertAllEqual(x, [4, 9])\n\n    def testBroadcast(self):\n        with self.test_session():\n            hypothesis = np.array([[1], [2], [3]])\n            y = np.array([1, 2, 3])\n            print(hypothesis - y)\n            cost = tf.reduce_mean(tf.square(hypothesis - y))\n            self.assertNotEqual(cost, 0)\n\n            y = y.reshape(-1, 1)\n            print(y, hypothesis - y)\n            cost = tf.reduce_mean(tf.square(hypothesis - y))\n            self.assertAllEqual(cost, 0)\n\n    def testNormalize(self):\n        with self.test_session():\n            values = np.array([[10, 20], [1000, -100]], dtype=np.float32)\n            norm_values = tf.nn.l2_normalize(values, axis=1)\n            print(norm_values)\n\nif __name__ == '__main__':\n    tf.test.main()\n"""
tf2/tf2-02-1-linear_regression.py,7,"b""import numpy as np\nimport tensorflow as tf\n\nx_train = [1, 2, 3, 4]\ny_train = [0, -1, -2, -3]\n\ntf.model = tf.keras.Sequential()\n# units == output shape, input_dim == input shape\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=1))\n\nsgd = tf.keras.optimizers.SGD(lr=0.1)  # SGD == standard gradient descendent, lr == learning rate\ntf.model.compile(loss='mse', optimizer=sgd)  # mse == mean_squared_error, 1/m * sig (y'-y)^2\n\n# prints summary of the model to the terminal\ntf.model.summary()\n\n# fit() executes training\ntf.model.fit(x_train, y_train, epochs=200)\n\n# predict() returns predicted value\ny_predict = tf.model.predict(np.array([5, 4]))\nprint(y_predict)\n"""
tf2/tf2-03-1-minimizing_cost_show_graph.py,7,"b""# Lab 3 Minimizing Cost\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nx_train = [1, 2, 3, 4]\ny_train = [0, -1, -2, -3]\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=1))\n\nsgd = tf.keras.optimizers.SGD(lr=0.1)\ntf.model.compile(loss='mse', optimizer=sgd)\n\ntf.model.summary()\n\n# fit() trains the model and returns history of train\nhistory = tf.model.fit(x_train, y_train, epochs=100)\n\ny_predict = tf.model.predict(np.array([5, 4]))\nprint(y_predict)\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n"""
tf2/tf2-04-1-multi_variable_linear_regression.py,7,"b""# Lab 4 Multi-variable linear regression\nimport tensorflow as tf\nimport numpy as np\n\nx_data = [[73., 80., 75.],\n          [93., 88., 93.],\n          [89., 91., 90.],\n          [96., 98., 100.],\n          [73., 66., 70.]]\ny_data = [[152.],\n          [185.],\n          [180.],\n          [196.],\n          [142.]]\n\ntf.model = tf.keras.Sequential()\n\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=3))  # input_dim=3 gives multi-variable regression\ntf.model.add(tf.keras.layers.Activation('linear'))  # this line can be omitted, as linear activation is default\n# advanced reading https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n\ntf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5))\ntf.model.summary()\nhistory = tf.model.fit(x_data, y_data, epochs=100)\n\ny_predict = tf.model.predict(np.array([[72., 93., 90.]]))\nprint(y_predict)\n"""
tf2/tf2-04-3-file_input_linear_regression.py,8,"b'# Lab 4 Multi-variable linear regression\nimport tensorflow as tf\nimport numpy as np\n\nxy = np.loadtxt(\'../data-01-test-score.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\n# Make sure the shape and data are OK\nprint(x_data, ""\\nx_data shape:"", x_data.shape)\nprint(y_data, ""\\ny_data shape:"", y_data.shape)\n\n# data output\n\'\'\'\n[[ 73.  80.  75.]\n [ 93.  88.  93.]\n ...\n [ 76.  83.  71.]\n [ 96.  93.  95.]] \nx_data shape: (25, 3)\n[[152.]\n [185.]\n ...\n [149.]\n [192.]] \ny_data shape: (25, 1)\n\'\'\'\ntf.model = tf.keras.Sequential()\n# activation function doesn\'t have to be added as a separate layer. Add it as an argument of Dense() layer\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=3, activation=\'linear\'))\n# tf.model.add(tf.keras.layers.Activation(\'linear\'))\ntf.model.summary()\n\ntf.model.compile(loss=\'mse\', optimizer=tf.keras.optimizers.SGD(lr=1e-5))\nhistory = tf.model.fit(x_data, y_data, epochs=2000)\n\n# Ask my score\nprint(""Your score will be "", tf.model.predict([[100, 70, 101]]))\nprint(""Other scores will be "", tf.model.predict([[60, 70, 110], [90, 100, 80]]))\n'"
tf2/tf2-05-1-logistic_regression.py,6,"b'# Lab 5 Logistic Regression Classifier\nimport tensorflow as tf\n\nx_data = [[1, 2],\n          [2, 3],\n          [3, 1],\n          [4, 3],\n          [5, 3],\n          [6, 2]]\ny_data = [[0],\n          [0],\n          [0],\n          [1],\n          [1],\n          [1]]\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=2))\n# use sigmoid activation for 0~1 problem\ntf.model.add(tf.keras.layers.Activation(\'sigmoid\'))\n\n\'\'\' \nbetter result with loss function == \'binary_crossentropy\', try \'mse\' for yourself\nadding accuracy metric to get accuracy report during training\n\'\'\'\ntf.model.compile(loss=\'binary_crossentropy\', optimizer=tf.keras.optimizers.SGD(lr=0.01), metrics=[\'accuracy\'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_data, epochs=5000)\n\n# Accuracy report\nprint(""Accuracy: "", history.history[\'accuracy\'][-1])\n'"
tf2/tf2-05-2-logistic_regression_diabetes.py,7,"b'# Lab 5 Logistic Regression Classifier\nimport tensorflow as tf\nimport numpy as np\n\nxy = np.loadtxt(\'../data-03-diabetes.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(x_data.shape, y_data.shape)\n\ntf.model = tf.keras.Sequential()\n# multi-variable, x_data.shape[1] == feature counts == 8 in this case\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=x_data.shape[1], activation=\'sigmoid\'))\ntf.model.compile(loss=\'binary_crossentropy\', optimizer=tf.keras.optimizers.SGD(lr=0.01),  metrics=[\'accuracy\'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_data, epochs=500)\n\n# accuracy!\nprint(""Accuracy: {0}"".format(history.history[\'accuracy\'][-1]))\n\n# predict a single data point\ny_predict = tf.model.predict([[0.176471, 0.155779, 0, 0, 0, 0.052161, -0.952178, -0.733333]])\nprint(""Predict: {0}"".format(y_predict))\n\n# evaluating model\nevaluate = tf.model.evaluate(x_data, y_data)\nprint(""loss: {0}, accuracy: {1}"".format(evaluate[0], evaluate[1]))\n'"
tf2/tf2-06-1-softmax_classifier.py,14,"b""# Lab 6 Softmax Classifier\nimport tensorflow as tf\nimport numpy as np\n\nx_raw = [[1, 2, 1, 1],\n          [2, 1, 3, 2],\n          [3, 1, 3, 4],\n          [4, 1, 5, 5],\n          [1, 7, 5, 5],\n          [1, 2, 5, 6],\n          [1, 6, 6, 6],\n          [1, 7, 7, 7]]\ny_raw = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1],\n          [0, 1, 0],\n          [0, 1, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [1, 0, 0]]\n\nx_data = np.array(x_raw, dtype=np.float32)\ny_data = np.array(y_raw, dtype=np.float32)\n\nnb_classes = 3\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(input_dim=4, units=nb_classes, use_bias=True))  # use_bias is True, by default\n\n# use softmax activations: softmax = exp(logits) / reduce_sum(exp(logits), dim)\ntf.model.add(tf.keras.layers.Activation('softmax'))\n\n# use loss == categorical_crossentropy\ntf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=0.1), metrics=['accuracy'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_data, epochs=2000)\n\nprint('--------------')\n# Testing & One-hot encoding\na = tf.model.predict(np.array([[1, 11, 7, 9]]))\nprint(a, tf.keras.backend.eval(tf.argmax(a, axis=1)))\n\nprint('--------------')\nb = tf.model.predict(np.array([[1, 3, 4, 3]]))\nprint(b, tf.keras.backend.eval(tf.argmax(b, axis=1)))\n\nprint('--------------')\n# or use argmax embedded method, predict_classes\nc = tf.model.predict(np.array([[1, 1, 0, 1]]))\nc_onehot = tf.model.predict_classes(np.array([[1, 1, 0, 1]]))\nprint(c, c_onehot)\n\nprint('--------------')\nall = tf.model.predict(np.array([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]))\nall_onehot = tf.model.predict_classes(np.array([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]))\nprint(all, all_onehot)\n"""
tf2/tf2-06-2-softmax_zoo_classifier.py,8,"b'# Lab 6 Softmax Classifier\nimport tensorflow as tf\nimport numpy as np\n\n# Predicting animal type based on various features\nxy = np.loadtxt(\'..\\data-04-zoo.csv\', delimiter=\',\', dtype=np.float32)\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\nprint(x_data.shape, y_data.shape)\n\n\'\'\'\n(101, 16) (101, 1)\n\'\'\'\n\nnb_classes = 7  # 0 ~ 6\n\n# Convert y_data to one_hot\ny_one_hot = tf.keras.utils.to_categorical(y_data, nb_classes)\nprint(""one_hot:"", y_one_hot)\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=nb_classes, input_dim=16, activation=\'softmax\'))\ntf.model.compile(loss=\'categorical_crossentropy\', optimizer=tf.keras.optimizers.SGD(lr=0.1), metrics=[\'accuracy\'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_one_hot, epochs=1000)\n\n# Single data test\ntest_data = np.array([[0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]]) # expected prediction == 3 (feathers)\nprint(tf.model.predict(test_data), tf.model.predict_classes(test_data))\n\n# Full x_data test\npred = tf.model.predict_classes(x_data)\nfor p, y in zip(pred, y_data.flatten()):\n    print(""[{}] Prediction: {} True Y: {}"".format(p == int(y), p, int(y)))\n'"
tf2/tf2-07-1-learning_rate_and_evaluation.py,6,"b'# Lab 7 Learning rate and Evaluation\nimport tensorflow as tf\n\nx_data = [[1, 2, 1],\n          [1, 3, 2],\n          [1, 3, 4],\n          [1, 5, 5],\n          [1, 7, 5],\n          [1, 2, 5],\n          [1, 6, 6],\n          [1, 7, 7]]\ny_data = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1],\n          [0, 1, 0],\n          [0, 1, 0],\n          [0, 1, 0],\n          [1, 0, 0],\n          [1, 0, 0]]\n\n# Evaluation our model using this test dataset\nx_test = [[2, 1, 1],\n          [3, 1, 2],\n          [3, 3, 4]]\ny_test = [[0, 0, 1],\n          [0, 0, 1],\n          [0, 0, 1]]\n\n# try different learning_rate\n# learning_rate = 65535  # ? it works too hahaha\nlearning_rate = 0.1\n# learning_rate = 1e-10  # small learning rate won\'t work either\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=3, input_dim=3, activation=\'softmax\'))\ntf.model.compile(loss=\'categorical_crossentropy\', optimizer=tf.keras.optimizers.SGD(lr=learning_rate), metrics=[\'accuracy\'])\n\ntf.model.fit(x_data, y_data, epochs=1000)\n\n# predict\nprint(""Prediction: "", tf.model.predict_classes(x_test))\n\n# Calculate the accuracy\nprint(""Accuracy: "", tf.model.evaluate(x_test, y_test)[1])\n'"
tf2/tf2-07-2-linear_regression_without_min_max.py,6,"b""import tensorflow as tf\nimport numpy as np\n\nxy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n               [816, 820.958984, 1008100, 815.48999, 819.23999],\n               [819.359985, 823, 1188100, 818.469971, 818.97998],\n               [819, 823, 1198100, 816, 820.450012],\n               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=4))\ntf.model.add(tf.keras.layers.Activation('linear'))\ntf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5))\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_data, epochs=100)\n\nprint(history.history['loss']) # loss == nan"""
tf2/tf2-07-3-linear_regression_min_max.py,8,"b""import tensorflow as tf\nimport numpy as np\n\n\ndef min_max_scaler(data):\n    numerator = data - np.min(data, 0)\n    denominator = np.max(data, 0) - np.min(data, 0)\n    # noise term prevents the zero division\n    return numerator / (denominator + 1e-7)\n\n\nxy = np.array(\n    [\n        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n        [816, 820.958984, 1008100, 815.48999, 819.23999],\n        [819.359985, 823, 1188100, 818.469971, 818.97998],\n        [819, 823, 1198100, 816, 820.450012],\n        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n    ]\n)\n\n# very important. It does not work without it.\nxy = min_max_scaler(xy)\nprint(xy)\n\n'''\n[[0.99999999 0.99999999 0.         1.         1.        ]\n [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n [0.         0.07747099 0.5326087  0.         0.        ]]\n'''\n\nx_data = xy[:, 0:-1]\ny_data = xy[:, [-1]]\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=4))\ntf.model.add(tf.keras.layers.Activation('linear'))\ntf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5))\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_data, epochs=1000)\n\npredictions = tf.model.predict(x_data)\nscore = tf.model.evaluate(x_data, y_data)\n\nprint('Prediction: \\n', predictions)\nprint('Cost: ', score)\n"""
tf2/tf2-09-1-xor.py,7,"b""# Lab 9 XOR\nimport tensorflow as tf\nimport numpy as np\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=2, activation='sigmoid'))\ntf.model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.SGD(lr=0.01),  metrics=['accuracy'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_data, epochs=1000)\n\npredictions = tf.model.predict(x_data)\nprint('Prediction: \\n', predictions)\n\nscore = tf.model.evaluate(x_data, y_data)\nprint('Accuracy: ', score[1])\n\n"""
tf2/tf2-09-2-xor-nn.py,10,"b""# Lab 9 XOR\nimport tensorflow as tf\nimport numpy as np\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=2, input_dim=2))\ntf.model.add(tf.keras.layers.Activation('sigmoid'))\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=2))\ntf.model.add(tf.keras.layers.Activation('sigmoid'))\ntf.model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.SGD(lr=0.1),  metrics=['accuracy'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_data, epochs=10000)\n\npredictions = tf.model.predict(x_data)\nprint('Prediction: \\n', predictions)\n\nscore = tf.model.evaluate(x_data, y_data)\nprint('Accuracy: ', score[1])\n"""
tf2/tf2-09-3-xor-nn-wide-deep.py,11,"b""# Lab 9 XOR\n# 9-3 deep and wide\nimport tensorflow as tf\nimport numpy as np\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=10, input_dim=2, activation='sigmoid'))\ntf.model.add(tf.keras.layers.Dense(units=10, activation='sigmoid'))\ntf.model.add(tf.keras.layers.Dense(units=10, activation='sigmoid'))\ntf.model.add(tf.keras.layers.Dense(units=10, activation='sigmoid'))\ntf.model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\n# SGD not working very well due to vanishing gradient problem, switched to Adam for now\n# or you may use activation='relu', study chapter 10 to know more on vanishing gradient problem.\ntf.model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(lr=0.1), metrics=['accuracy'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_data, y_data, epochs=5000)\n\npredictions = tf.model.predict(x_data)\nprint('Prediction: \\n', predictions)\n\nscore = tf.model.evaluate(x_data, y_data)\nprint('Accuracy: ', score[1])\n"""
tf2/tf2-09-4-xor_tensorboard.py,11,"b'# Lab 9 XOR\nimport datetime\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nx_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=2, input_dim=2))\ntf.model.add(tf.keras.layers.Activation(\'sigmoid\'))\ntf.model.add(tf.keras.layers.Dense(units=1, input_dim=2))\ntf.model.add(tf.keras.layers.Activation(\'sigmoid\'))\ntf.model.compile(loss=\'binary_crossentropy\', optimizer=tf.optimizers.SGD(lr=0.1),  metrics=[\'accuracy\'])\ntf.model.summary()\n\n# prepare callback\nlog_dir = os.path.join(""."", ""logs"", ""fit"", datetime.datetime.now().strftime(""%Y%m%d-%H%M%S""))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n# add callback param to fit()\nhistory = tf.model.fit(x_data, y_data, epochs=10000, callbacks=[tensorboard_callback])\n\npredictions = tf.model.predict(x_data)\nprint(\'Prediction: \\n\', predictions)\n\nscore = tf.model.evaluate(x_data, y_data)\nprint(\'Accuracy: \', score[1])\n\n\'\'\'\nat the end of the run, open terminal / command window\ncd to the source directory\ntensorboard --logdir logs/fit\n\nread more on tensorboard: https://www.tensorflow.org/tensorboard/get_started\n\'\'\''"
tf2/tf2-10-1-mnist_softmax.py,10,"b'# Lab 7 Learning rate and Evaluation\nimport tensorflow as tf\n\nlearning_rate = 0.001\nbatch_size = 100\ntraining_epochs = 15\nnb_classes = 10\n\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n# normalizing data\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# change data shape\nprint(x_train.shape)  # (60000, 28, 28)\nx_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n\n# change result to one-hot encoding\n# in tf1, one_hot= True in read_data_sets(""MNIST_data/"", one_hot=True)\n# took care of it, but here we need to manually convert them\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n# # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n# array([0, 2, 1, 2, 0])\n# `to_categorical` converts this into a matrix with as many columns as there are classes. The number of rows\n#  stays the same. to_categorical(labels)\n# array([[ 1.,  0.,  0.],\n#        [ 0.,  0.,  1.],\n#        [ 0.,  1.,  0.],\n#        [ 0.,  0.,  1.],\n#        [ 1.,  0.,  0.]], dtype=float32)\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(units=10, input_dim=784, activation=\'softmax\'))\ntf.model.compile(loss=\'categorical_crossentropy\', optimizer=tf.optimizers.Adam(0.001), metrics=[\'accuracy\'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n\npredictions = tf.model.predict(x_test)\nprint(\'Prediction: \\n\', predictions)\nx_train\nscore = tf.model.evaluate(x_train, y_train)\nprint(\'Accuracy: \', score[1])'"
tf2/tf2-10-2-mnist_nn.py,13,"b'# Lab 10 MNIST and NN\nimport numpy as np\nimport random\nimport tensorflow as tf\n\nrandom.seed(777)  # for reproducibility\nlearning_rate = 0.001\nbatch_size = 100\ntraining_epochs = 15\nnb_classes = 10\n\n(x_train, y_train), (x_test2, y_test) = tf.keras.datasets.mnist.load_data()\nprint(x_train.shape)\n\nx_train = x_train.reshape(x_train.shape[0], 28 * 28)\nx_test = x_test2.reshape(x_test2.shape[0], 28 * 28)\n\ny_train = tf.keras.utils.to_categorical(y_train, nb_classes)\ny_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n\ntf.model = tf.keras.Sequential()\ntf.model.add(tf.keras.layers.Dense(input_dim=784, units=256, activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dense(units=256, activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dense(units=nb_classes, activation=\'softmax\'))\ntf.model.compile(loss=\'categorical_crossentropy\',\n                 optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\'accuracy\'])\ntf.model.summary()\n\ntf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n\n# predict 10 random hand-writing data\ny_predicted = tf.model.predict(x_test)\nfor x in range(0, 10):\n    random_index = random.randint(0, x_test.shape[0]-1)\n    print(""index: "", random_index,\n          ""actual y: "", np.argmax(y_test[random_index]),\n          ""predicted y: "", np.argmax(y_predicted[random_index]))\n\n# evaluate test set\nevaluation = tf.model.evaluate(x_test, y_test)\nprint(\'loss: \', evaluation[0])\nprint(\'accuracy\', evaluation[1])\n'"
tf2/tf2-10-3-mnist_nn_xavier.py,13,"b'# Lab 10 MNIST and NN\nimport numpy as np\nimport random\nimport tensorflow as tf\n\nrandom.seed(777)  # for reproducibility\nlearning_rate = 0.001\nbatch_size = 100\ntraining_epochs = 15\nnb_classes = 10\n\n(x_train, y_train), (x_test2, y_test) = tf.keras.datasets.mnist.load_data()\nprint(x_train.shape)\n\nx_train = x_train.reshape(x_train.shape[0], 28 * 28)\nx_test = x_test2.reshape(x_test2.shape[0], 28 * 28)\n\ny_train = tf.keras.utils.to_categorical(y_train, nb_classes)\ny_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n\ntf.model = tf.keras.Sequential()\n# Glorot normal initializer, also called Xavier normal initializer.\n# see https://www.tensorflow.org/api_docs/python/tf/initializers\n\ntf.model.add(tf.keras.layers.Dense(input_dim=784, units=256, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dense(units=256, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dense(units=nb_classes, kernel_initializer=\'glorot_normal\', activation=\'softmax\'))\ntf.model.compile(loss=\'categorical_crossentropy\',\n                 optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\'accuracy\'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n\n# predict 10 random hand-writing data\ny_predicted = tf.model.predict(x_test)\nfor x in range(0, 10):\n    random_index = random.randint(0, x_test.shape[0]-1)\n    print(""index: "", random_index,\n          ""actual y: "", np.argmax(y_test[random_index]),\n          ""predicted y: "", np.argmax(y_predicted[random_index]))\n\n# evaluate test set\nevaluation = tf.model.evaluate(x_test, y_test)\nprint(\'loss: \', evaluation[0])\nprint(\'accuracy\', evaluation[1])\n'"
tf2/tf2-10-4-mnist_nn_deep.py,15,"b'# Lab 10 MNIST and NN\nimport numpy as np\nimport random\nimport tensorflow as tf\n\nrandom.seed(777)  # for reproducibility\nlearning_rate = 0.001\nbatch_size = 100\ntraining_epochs = 15\nnb_classes = 10\n\n(x_train, y_train), (x_test2, y_test) = tf.keras.datasets.mnist.load_data()\nprint(x_train.shape)\n\nx_train = x_train.reshape(x_train.shape[0], 28 * 28)\nx_test = x_test2.reshape(x_test2.shape[0], 28 * 28)\n\ny_train = tf.keras.utils.to_categorical(y_train, nb_classes)\ny_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n\ntf.model = tf.keras.Sequential()\n# Glorot normal initializer, also called Xavier normal initializer.\n# see https://www.tensorflow.org/api_docs/python/tf/initializers\n\ntf.model.add(tf.keras.layers.Dense(input_dim=784, units=512, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dense(units=nb_classes, kernel_initializer=\'glorot_normal\', activation=\'softmax\'))\ntf.model.compile(loss=\'categorical_crossentropy\',\n                 optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\'accuracy\'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n\n# predict 10 random hand-writing data\ny_predicted = tf.model.predict(x_test)\nfor x in range(0, 10):\n    random_index = random.randint(0, x_test.shape[0]-1)\n    print(""index: "", random_index,\n          ""actual y: "", np.argmax(y_test[random_index]),\n          ""predicted y: "", np.argmax(y_predicted[random_index]))\n\n# evaluate test set\nevaluation = tf.model.evaluate(x_test, y_test)\nprint(\'loss: \', evaluation[0])\nprint(\'accuracy\', evaluation[1])\n'"
tf2/tf2-10-5-mnist_nn_dropout.py,19,"b'# Lab 10 MNIST and NN\nimport numpy as np\nimport random\nimport tensorflow as tf\n\nrandom.seed(777)  # for reproducibility\nlearning_rate = 0.001\nbatch_size = 100\ntraining_epochs = 15\nnb_classes = 10\ndrop_rate = 0.3\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nprint(x_train.shape)\n\nx_train = x_train.reshape(x_train.shape[0], 28 * 28)\nx_test = x_test.reshape(x_test.shape[0], 28 * 28)\n\ny_train = tf.keras.utils.to_categorical(y_train, nb_classes)\ny_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n\ntf.model = tf.keras.Sequential()\n# Glorot normal initializer, also called Xavier normal initializer.\n# see https://www.tensorflow.org/api_docs/python/tf/initializers\n\ntf.model.add(tf.keras.layers.Dense(input_dim=784, units=512, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dropout(drop_rate))\ntf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dropout(drop_rate))\ntf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dropout(drop_rate))\ntf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer=\'glorot_normal\', activation=\'relu\'))\ntf.model.add(tf.keras.layers.Dropout(drop_rate))\ntf.model.add(tf.keras.layers.Dense(units=nb_classes, kernel_initializer=\'glorot_normal\', activation=\'softmax\'))\ntf.model.compile(loss=\'categorical_crossentropy\',\n                 optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\'accuracy\'])\ntf.model.summary()\n\nhistory = tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n\n# predict 10 random hand-writing data\ny_predicted = tf.model.predict(x_test)\nfor x in range(0, 10):\n    random_index = random.randint(0, x_test.shape[0]-1)\n    print(""index: "", random_index,\n          ""actual y: "", np.argmax(y_test[random_index]),\n          ""predicted y: "", np.argmax(y_predicted[random_index]))\n\n# evaluate test set\nevaluation = tf.model.evaluate(x_test, y_test)\nprint(\'loss: \', evaluation[0])\nprint(\'accuracy\', evaluation[1])\n'"
tf2/tf2-11-1-mnist_cnn.py,15,"b'# Lab 11 MNIST and Convolutional Neural Network\nimport numpy as np\nimport tensorflow as tf\nimport random\n\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_test = x_test / 255\nx_train = x_train / 255\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n\n# one hot encode y data\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n# hyper parameters\nlearning_rate = 0.001\ntraining_epochs = 12\nbatch_size = 128\n\ntf.model = tf.keras.Sequential()\n# L1\ntf.model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), input_shape=(28, 28, 1), activation=\'relu\'))\ntf.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n# L2\ntf.model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\'relu\'))\ntf.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n# L3 fully connected\ntf.model.add(tf.keras.layers.Flatten())\ntf.model.add(tf.keras.layers.Dense(units=10, kernel_initializer=\'glorot_normal\', activation=\'softmax\'))\n\ntf.model.compile(loss=\'categorical_crossentropy\', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\'accuracy\'])\ntf.model.summary()\n\ntf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n\n# predict 10 random hand-writing data\ny_predicted = tf.model.predict(x_test)\nfor x in range(0, 10):\n    random_index = random.randint(0, x_test.shape[0]-1)\n    print(""index: "", random_index,\n          ""actual y: "", np.argmax(y_test[random_index]),\n          ""predicted y: "", np.argmax(y_predicted[random_index]))\n\nevaluation = tf.model.evaluate(x_test, y_test)\nprint(\'loss: \', evaluation[0])\nprint(\'accuracy\', evaluation[1])'"
tf2/tf2-12-1-hello-rnn.py,10,"b'# Lab 12 RNN\nimport numpy as np\nimport tensorflow as tf\n\nidx2char = [\'h\', \'i\', \'e\', \'l\', \'o\']\n# Teach hello: hihell -> ihello\n# x_data = [[0, 1, 0, 2, 3, 3]]  # hihell\ny_data = [[1, 0, 2, 3, 3, 4]]  # ihello\n\nnum_classes = 5\ninput_dim = 5  # one-hot size, same as hidden_size to directly predict one-hot\nsequence_length = 6  # |ihello| == 6\nlearning_rate = 0.1\n\nx_one_hot = np.array([[[1, 0, 0, 0, 0],    # h 0\n                       [0, 1, 0, 0, 0],    # i 1\n                       [1, 0, 0, 0, 0],    # h 0\n                       [0, 0, 1, 0, 0],    # e 2\n                       [0, 0, 0, 1, 0],    # l 3\n                       [0, 0, 0, 1, 0]]],  # l 3\n                     dtype=np.float32)\n\ny_one_hot = tf.keras.utils.to_categorical(y_data, num_classes=num_classes)\nprint(x_one_hot.shape)\nprint(y_one_hot.shape)\n\ntf.model = tf.keras.Sequential()\n\n# make cell and add it to RNN layer\n# input_shape = (1,6,5) => number of sequence (batch), length of sequence, size of input dim\ncell = tf.keras.layers.LSTMCell(units=num_classes, input_shape=(sequence_length, input_dim))\ntf.model.add(tf.keras.layers.RNN(cell=cell, return_sequences=True))\n\n# single LSTM layer can be used as well instead of creating LSTMCell\n# tf.model.add(tf.keras.layers.LSTM(units=num_classes, input_shape=(sequence_length, input_dim), return_sequences=True))\n\n# fully connected layer\ntf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation=\'softmax\')))\n\ntf.model.compile(loss=\'categorical_crossentropy\', optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n                 metrics=[\'accuracy\'])\n\n# train\ntf.model.fit(x_one_hot, y_one_hot, epochs=50)\ntf.model.summary()\n\npredictions = tf.model.predict(x_one_hot)\nfor i, prediction in enumerate(predictions):\n    print(prediction)\n    # print char using argmax, dict\n    result_str = [idx2char[c] for c in np.argmax(prediction, axis=1)]\n    print(""\\tPrediction str: "", \'\'.join(result_str))\n'"
tf2/tf2-12-2-char-seq-rnn.py,10,"b'# Lab 12 Character Sequence RNN\nimport tensorflow as tf\nimport numpy as np\n\nsample = "" if you want you""\nidx2char = list(set(sample))  # index -> char\nchar2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n\n# hyper parameters\ndic_size = len(char2idx)  # RNN input size (one hot size)\nhidden_size = len(char2idx)  # RNN output size\nnum_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\nbatch_size = 1  # one sample data, one batch\nsequence_length = len(sample) - 1  # number of lstm rollings (unit #)\nlearning_rate = 0.1\n\nsample_idx = [char2idx[c] for c in sample]  # char to index\nx_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\ny_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n\nx_one_hot_eager = tf.one_hot(x_data, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\nx_one_hot_numpy = tf.keras.utils.to_categorical(x_data, num_classes)  # it\'ll generate numpy array, either way works\ny_one_hot_eager = tf.one_hot(y_data, num_classes)\n\ntf.model = tf.keras.Sequential();\ntf.model.add(tf.keras.layers.\n             LSTM(units=num_classes, input_shape=(sequence_length, x_one_hot_eager.shape[2]), return_sequences=True))\ntf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation=\'softmax\')))\ntf.model.summary()\ntf.model.compile(loss=\'categorical_crossentropy\', optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n                 metrics=[\'accuracy\'])\ntf.model.fit(x_one_hot_eager, y_one_hot_eager, epochs=50)\n\n\npredictions = tf.model.predict(x_one_hot_eager)\n\nfor i, prediction in enumerate(predictions):\n    # print char using argmax, dict\n    result_str = [idx2char[c] for c in np.argmax(prediction, axis=1)]\n    print(""\\tPrediction str: "", \'\'.join(result_str))\n'"
tf2/tf2-12-4-rnn_long_char.py,10,"b'import tensorflow as tf\nimport numpy as np\n\nsentence = (""if you want to build a ship, don\'t drum up people together to ""\n            ""collect wood and don\'t assign them tasks and work, but rather ""\n            ""teach them to long for the endless immensity of the sea."")\n\nchar_set = list(set(sentence))\nchar_dic = {w: i for i, w in enumerate(char_set)}\n\ndata_dim = len(char_set)\nhidden_size = len(char_set)\nnum_classes = len(char_set)\nsequence_length = 10  # Any arbitrary number\nlearning_rate = 0.1\n\ndataX = []\ndataY = []\nfor i in range(0, len(sentence) - sequence_length):\n    x_str = sentence[i:i + sequence_length]\n    y_str = sentence[i + 1: i + sequence_length + 1]\n    print(i, x_str, \'->\', y_str)\n\n    x = [char_dic[c] for c in x_str]  # x str to index\n    y = [char_dic[c] for c in y_str]  # y str to index\n\n    dataX.append(x)\n    dataY.append(y)\n\nbatch_size = len(dataX)\n\n# One-hot encoding\nX_one_hot = tf.one_hot(dataX, num_classes)\nY_one_hot = tf.one_hot(dataY, num_classes)\n\nprint(X_one_hot.shape)  # check out the shape (170, 10, 25)\nprint(Y_one_hot.shape)  # check out the shape\n\n\ntf.model = tf.keras.Sequential();\ntf.model.add(tf.keras.layers.\n             LSTM(units=num_classes, input_shape=(sequence_length, X_one_hot.shape[2]), return_sequences=True))\ntf.model.add(tf.keras.layers.LSTM(units=num_classes, return_sequences=True))\ntf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation=\'softmax\')))\ntf.model.summary()\ntf.model.compile(loss=\'categorical_crossentropy\', optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n                 metrics=[\'accuracy\'])\ntf.model.fit(X_one_hot, Y_one_hot, epochs=100)\n\n\nresults = tf.model.predict(X_one_hot)\nfor j, result in enumerate(results):\n    index = np.argmax(result, axis=1)\n    if j is 0:  # print all for the first result to make a sentence\n        print(\'\'.join([char_set[t] for t in index]), end=\'\')\n    else:\n        print(char_set[index[-1]], end=\'\')\n\n'"
tf2/tf2-12-5-rnn_stock_prediction.py,7,"b'\'\'\'\nThis script shows how to predict stock prices using a basic RNN\n\'\'\'\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef MinMaxScaler(data):\n    \'\'\' Min Max Normalization\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        input data to be normalized\n        shape: [Batch size, dimension]\n\n    Returns\n    ----------\n    data : numpy.ndarry\n        normalized data\n        shape: [Batch size, dimension]\n\n    References\n    ----------\n    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n\n    \'\'\'\n    numerator = data - np.min(data, 0)\n    denominator = np.max(data, 0) - np.min(data, 0)\n    # noise term prevents the zero division\n    return numerator / (denominator + 1e-7)\n\n\n# train Parameters\nseq_length = 7\ndata_dim = 5\noutput_dim = 1\nlearning_rate = 0.01\niterations = 500\n\n# Open, High, Low, Volume, Close\nxy = np.loadtxt(\'../data-02-stock_daily.csv\', delimiter=\',\')\nxy = xy[::-1]  # reverse order (chronically ordered)\n\n# train/test split\ntrain_size = int(len(xy) * 0.7)\ntrain_set = xy[0:train_size]\ntest_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n\n# Scale each\ntrain_set = MinMaxScaler(train_set)\ntest_set = MinMaxScaler(test_set)\n\n# build datasets\ndef build_dataset(time_series, seq_length):\n    dataX = []\n    dataY = []\n    for i in range(0, len(time_series) - seq_length):\n        x = time_series[i:i + seq_length, :]\n        y = time_series[i + seq_length, [-1]]  # Next close price\n        print(x, ""->"", y)\n        dataX.append(x)\n        dataY.append(y)\n    return np.array(dataX), np.array(dataY)\n\ntrainX, trainY = build_dataset(train_set, seq_length)\ntestX, testY = build_dataset(test_set, seq_length)\n\nprint(trainX.shape)  # (505, 7, 5)\nprint(trainY.shape)\n\ntf.model = tf.keras.Sequential();\ntf.model.add(tf.keras.layers.LSTM(units=1, input_shape=(seq_length, data_dim)))\ntf.model.add(tf.keras.layers.Dense(units=output_dim, activation=\'tanh\'))\ntf.model.summary()\n\ntf.model.compile(loss=\'mean_squared_error\', optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\ntf.model.fit(trainX, trainY, epochs=iterations)\n\n\n# Test step\ntest_predict = tf.model.predict(testX)\n\n# Plot predictions\nplt.plot(testY)\nplt.plot(test_predict)\nplt.xlabel(""Time Period"")\nplt.ylabel(""Stock Price"")\nplt.show()\n'"
