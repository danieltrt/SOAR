file_path,api_count,code
beam.py,0,"b'import tensorflow as tf\nimport numpy as np\n\n\nclass BeamSearch():\n    def __init__(self, predict, initial_state, prime_labels):\n        """"""Initializes the beam search.\n\n        Args:\n            predict:\n                A function that takes a `sample` and a `state`. It then performs\n                the computation on the last word in `sample`.\n            initial_state:\n                The initial state of the RNN.\n            prime_labels:\n                A list of labels corresponding to the priming text. This must\n                not be empty.\n        """"""\n\n        if not prime_labels:\n            raise ValueError(\'prime_labels must be a non-empty list.\')\n        self.predict = predict\n        self.initial_state = initial_state\n        self.prime_labels = prime_labels\n\n    def predict_samples(self, samples, states):\n        probs = []\n        next_states = []\n        for i in range(len(samples)):\n            prob, next_state = self.predict(samples[i], states[i])\n            probs.append(prob.squeeze())\n            next_states.append(next_state)\n        return np.array(probs), next_states\n\n    def search(self, oov, eos, k=1, maxsample=4000, use_unk=False):\n        """"""Return k samples (beams) and their NLL scores.\n\n        Each sample is a sequence of labels, either ending with `eos` or\n        truncated to length of `maxsample`. `use_unk` allow usage of `oov`\n        (out-of-vocabulary) label in samples\n        """"""\n\n        # A list of probabilities of our samples.\n        probs = []\n\n        prime_sample = []\n        prime_score = 0\n        prime_state = self.initial_state\n\n        # Initialize the live sample with the prime.\n        for i, label in enumerate(self.prime_labels):\n            prime_sample.append(label)\n\n            # The first word does not contribute to the score as the probs have\n            # not yet been determined.\n            if i > 0:\n                prime_score = prime_score - np.log(probs[0, label])\n            probs, prime_state = self.predict(prime_sample, prime_state)\n\n        dead_k = 0  # samples that reached eos\n        dead_samples = []\n        dead_scores = []\n        dead_states = []\n\n        live_k = 1  # samples that did not yet reached eos\n        live_samples = [prime_sample]\n        live_scores = [prime_score]\n        live_states = [prime_state]\n\n        while live_k and dead_k < k:\n            # total score for every sample is sum of -log of word prb\n            cand_scores = np.array(live_scores)[:, None] - np.log(probs)\n            if not use_unk and oov is not None:\n                cand_scores[:, oov] = 1e20\n            cand_flat = cand_scores.flatten()\n\n            # find the best (lowest) scores we have from all possible samples and new words\n            ranks_flat = cand_flat.argsort()[:(k - dead_k)]\n            live_scores = cand_flat[ranks_flat]\n\n            # append the new words to their appropriate live sample\n            voc_size = probs.shape[1]\n            live_samples = [live_samples[r // voc_size] + [r % voc_size] for r in ranks_flat]\n            live_states = [live_states[r // voc_size] for r in ranks_flat]\n\n            # live samples that should be dead are...\n            zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n\n            # add zombies to the dead\n            dead_samples += [s for s, z in zip(live_samples, zombie) if z]  # remove first label == empty\n            dead_scores += [s for s, z in zip(live_scores, zombie) if z]\n            dead_states += [s for s, z in zip(live_states, zombie) if z]\n            dead_k = len(dead_samples)\n            # remove zombies from the living\n            live_samples = [s for s, z in zip(live_samples, zombie) if not z]\n            live_scores = [s for s, z in zip(live_scores, zombie) if not z]\n            live_states = [s for s, z in zip(live_states, zombie) if not z]\n            live_k = len(live_samples)\n\n            # Finally, compute the next-step probabilities and states.\n            probs, live_states = self.predict_samples(live_samples, live_states)\n\n        return dead_samples + live_samples, dead_scores + live_scores\n'"
model.py,40,"b'import tensorflow as tf\nfrom tensorflow.contrib import rnn\nfrom tensorflow.contrib import legacy_seq2seq\nimport random\nimport numpy as np\n\nfrom beam import BeamSearch\n\nclass Model():\n    def __init__(self, args, infer=False):\n        self.args = args\n        if infer:\n            args.batch_size = 1\n            args.seq_length = 1\n\n        if args.model == \'rnn\':\n            cell_fn = rnn.BasicRNNCell\n        elif args.model == \'gru\':\n            cell_fn = rnn.GRUCell\n        elif args.model == \'lstm\':\n            cell_fn = rnn.BasicLSTMCell\n        else:\n            raise Exception(""model type not supported: {}"".format(args.model))\n\n        cells = []\n        for _ in range(args.num_layers):\n            cell = cell_fn(args.rnn_size)\n            cells.append(cell)\n\n        self.cell = cell = rnn.MultiRNNCell(cells)\n\n        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n        self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n        self.batch_pointer = tf.Variable(0, name=""batch_pointer"", trainable=False, dtype=tf.int32)\n        self.inc_batch_pointer_op = tf.assign(self.batch_pointer, self.batch_pointer + 1)\n        self.epoch_pointer = tf.Variable(0, name=""epoch_pointer"", trainable=False)\n        self.batch_time = tf.Variable(0.0, name=""batch_time"", trainable=False)\n        tf.summary.scalar(""time_batch"", self.batch_time)\n\n        def variable_summaries(var):\n            """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n            with tf.name_scope(\'summaries\'):\n                mean = tf.reduce_mean(var)\n                tf.summary.scalar(\'mean\', mean)\n                #with tf.name_scope(\'stddev\'):\n                #   stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n                #tf.summary.scalar(\'stddev\', stddev)\n                tf.summary.scalar(\'max\', tf.reduce_max(var))\n                tf.summary.scalar(\'min\', tf.reduce_min(var))\n                #tf.summary.histogram(\'histogram\', var)\n\n        with tf.variable_scope(\'rnnlm\'):\n            softmax_w = tf.get_variable(""softmax_w"", [args.rnn_size, args.vocab_size])\n            variable_summaries(softmax_w)\n            softmax_b = tf.get_variable(""softmax_b"", [args.vocab_size])\n            variable_summaries(softmax_b)\n            with tf.device(""/cpu:0""):\n                embedding = tf.get_variable(""embedding"", [args.vocab_size, args.rnn_size])\n                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), args.seq_length, 1)\n                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n\n        def loop(prev, _):\n            prev = tf.matmul(prev, softmax_w) + softmax_b\n            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n            return tf.nn.embedding_lookup(embedding, prev_symbol)\n\n        outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if infer else None, scope=\'rnnlm\')\n        output = tf.reshape(tf.concat(outputs, 1), [-1, args.rnn_size])\n        self.logits = tf.matmul(output, softmax_w) + softmax_b\n        self.probs = tf.nn.softmax(self.logits)\n        loss = legacy_seq2seq.sequence_loss_by_example([self.logits],\n                [tf.reshape(self.targets, [-1])],\n                [tf.ones([args.batch_size * args.seq_length])],\n                args.vocab_size)\n        self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n        tf.summary.scalar(""cost"", self.cost)\n        self.final_state = last_state\n        self.lr = tf.Variable(0.0, trainable=False)\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n                args.grad_clip)\n        optimizer = tf.train.AdamOptimizer(self.lr)\n        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n    def sample(self, sess, words, vocab, num=200, prime=\'first all\', sampling_type=1, pick=0, width=4, quiet=False):\n        def weighted_pick(weights):\n            t = np.cumsum(weights)\n            s = np.sum(weights)\n            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n\n        def beam_search_predict(sample, state):\n            """"""Returns the updated probability distribution (`probs`) and\n            `state` for a given `sample`. `sample` should be a sequence of\n            vocabulary labels, with the last word to be tested against the RNN.\n            """"""\n\n            x = np.zeros((1, 1))\n            x[0, 0] = sample[-1]\n            feed = {self.input_data: x, self.initial_state: state}\n            [probs, final_state] = sess.run([self.probs, self.final_state],\n                                            feed)\n            return probs, final_state\n\n        def beam_search_pick(prime, width):\n            """"""Returns the beam search pick.""""""\n            if not len(prime) or prime == \' \':\n                prime = random.choice(list(vocab.keys()))\n            prime_labels = [vocab.get(word, 0) for word in prime.split()]\n            bs = BeamSearch(beam_search_predict,\n                            sess.run(self.cell.zero_state(1, tf.float32)),\n                            prime_labels)\n            samples, scores = bs.search(None, None, k=width, maxsample=num)\n            return samples[np.argmin(scores)]\n\n        ret = \'\'\n        if pick == 1:\n            state = sess.run(self.cell.zero_state(1, tf.float32))\n            if not len(prime) or prime == \' \':\n                prime  = random.choice(list(vocab.keys()))\n            if not quiet:\n                print(prime)\n            for word in prime.split()[:-1]:\n                if not quiet:\n                    print(word)\n                x = np.zeros((1, 1))\n                x[0, 0] = vocab.get(word,0)\n                feed = {self.input_data: x, self.initial_state:state}\n                [state] = sess.run([self.final_state], feed)\n\n            ret = prime\n            word = prime.split()[-1]\n            for n in range(num):\n                x = np.zeros((1, 1))\n                x[0, 0] = vocab.get(word, 0)\n                feed = {self.input_data: x, self.initial_state:state}\n                [probs, state] = sess.run([self.probs, self.final_state], feed)\n                p = probs[0]\n\n                if sampling_type == 0:\n                    sample = np.argmax(p)\n                elif sampling_type == 2:\n                    if word == \'\\n\':\n                        sample = weighted_pick(p)\n                    else:\n                        sample = np.argmax(p)\n                else: # sampling_type == 1 default:\n                    sample = weighted_pick(p)\n\n                pred = words[sample]\n                ret += \' \' + pred\n                word = pred\n        elif pick == 2:\n            pred = beam_search_pick(prime, width)\n            for i, label in enumerate(pred):\n                ret += \' \' + words[label] if i > 0 else words[label]\n        return ret\n'"
sample.py,4,"b""from __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\n\nimport argparse\nimport time\nimport os\nfrom six.moves import cPickle\n\nfrom utils import TextLoader\nfrom model import Model\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--save_dir', type=str, default='save',\n                       help='model directory to load stored checkpointed models from')\n    parser.add_argument('-n', type=int, default=200,\n                       help='number of words to sample')\n    parser.add_argument('--prime', type=str, default=' ',\n                       help='prime text')\n    parser.add_argument('--pick', type=int, default=1,\n                       help='1 = weighted pick, 2 = beam search pick')\n    parser.add_argument('--width', type=int, default=4,\n                       help='width of the beam search')\n    parser.add_argument('--sample', type=int, default=1,\n                       help='0 to use max at each timestep, 1 to sample at each timestep, 2 to sample on spaces')\n    parser.add_argument('--count', '-c', type=int, default=1,\n                       help='number of samples to print')\n    parser.add_argument('--quiet', '-q', default=False, action='store_true',\n                       help='suppress printing the prime text (default false)')\n\n    args = parser.parse_args()\n    sample(args)\n\ndef sample(args):\n    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n        saved_args = cPickle.load(f)\n    with open(os.path.join(args.save_dir, 'words_vocab.pkl'), 'rb') as f:\n        words, vocab = cPickle.load(f)\n    model = Model(saved_args, True)\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n        saver = tf.train.Saver(tf.global_variables())\n        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            for _ in range(args.count):\n              print(model.sample(sess, words, vocab, args.n, args.prime, args.sample, args.pick, args.width, args.quiet))\n\nif __name__ == '__main__':\n    main()\n"""
train.py,8,"b'from __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\n\nimport argparse\nimport time\nimport os\nfrom six.moves import cPickle\n\nfrom utils import TextLoader\nfrom model import Model\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/tinyshakespeare\',\n                       help=\'data directory containing input.txt\')\n    parser.add_argument(\'--input_encoding\', type=str, default=None,\n                       help=\'character encoding of input.txt, from https://docs.python.org/3/library/codecs.html#standard-encodings\')\n    parser.add_argument(\'--log_dir\', type=str, default=\'logs\',\n                       help=\'directory containing tensorboard logs\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'save\',\n                       help=\'directory to store checkpointed models\')\n    parser.add_argument(\'--rnn_size\', type=int, default=256,\n                       help=\'size of RNN hidden state\')\n    parser.add_argument(\'--num_layers\', type=int, default=2,\n                       help=\'number of layers in the RNN\')\n    parser.add_argument(\'--model\', type=str, default=\'lstm\',\n                       help=\'rnn, gru, or lstm\')\n    parser.add_argument(\'--batch_size\', type=int, default=50,\n                       help=\'minibatch size\')\n    parser.add_argument(\'--seq_length\', type=int, default=25,\n                       help=\'RNN sequence length\')\n    parser.add_argument(\'--num_epochs\', type=int, default=50,\n                       help=\'number of epochs\')\n    parser.add_argument(\'--save_every\', type=int, default=1000,\n                       help=\'save frequency\')\n    parser.add_argument(\'--grad_clip\', type=float, default=5.,\n                       help=\'clip gradients at this value\')\n    parser.add_argument(\'--learning_rate\', type=float, default=0.002,\n                       help=\'learning rate\')\n    parser.add_argument(\'--decay_rate\', type=float, default=0.97,\n                       help=\'decay rate for rmsprop\')\n    parser.add_argument(\'--gpu_mem\', type=float, default=0.666,\n                       help=\'%% of gpu memory to be allocated to this process. Default is 66.6%%\')\n    parser.add_argument(\'--init_from\', type=str, default=None,\n                       help=""""""continue training from saved model at this path. Path must contain files saved by previous training process:\n                            \'config.pkl\'        : configuration;\n                            \'words_vocab.pkl\'   : vocabulary definitions;\n                            \'checkpoint\'        : paths to model file(s) (created by tf).\n                                                  Note: this file contains absolute paths, be careful when moving files around;\n                            \'model.ckpt-*\'      : file(s) with model definition (created by tf)\n                        """""")\n    args = parser.parse_args()\n    train(args)\n\ndef train(args):\n    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length, args.input_encoding)\n    args.vocab_size = data_loader.vocab_size\n\n    # check compatibility if training is continued from previously saved model\n    if args.init_from is not None:\n        # check if all necessary files exist\n        assert os.path.isdir(args.init_from),"" %s must be a path"" % args.init_from\n        assert os.path.isfile(os.path.join(args.init_from,""config.pkl"")),""config.pkl file does not exist in path %s""%args.init_from\n        assert os.path.isfile(os.path.join(args.init_from,""words_vocab.pkl"")),""words_vocab.pkl.pkl file does not exist in path %s"" % args.init_from\n        ckpt = tf.train.get_checkpoint_state(args.init_from)\n        assert ckpt,""No checkpoint found""\n        assert ckpt.model_checkpoint_path,""No model path found in checkpoint""\n\n        # open old config and check if models are compatible\n        with open(os.path.join(args.init_from, \'config.pkl\'), \'rb\') as f:\n            saved_model_args = cPickle.load(f)\n        need_be_same=[""model"",""rnn_size"",""num_layers"",""seq_length""]\n        for checkme in need_be_same:\n            assert vars(saved_model_args)[checkme]==vars(args)[checkme],""Command line argument and saved model disagree on \'%s\' ""%checkme\n\n        # open saved vocab/dict and check if vocabs/dicts are compatible\n        with open(os.path.join(args.init_from, \'words_vocab.pkl\'), \'rb\') as f:\n            saved_words, saved_vocab = cPickle.load(f)\n        assert saved_words==data_loader.words, ""Data and loaded model disagree on word set!""\n        assert saved_vocab==data_loader.vocab, ""Data and loaded model disagree on dictionary mappings!""\n\n    with open(os.path.join(args.save_dir, \'config.pkl\'), \'wb\') as f:\n        cPickle.dump(args, f)\n    with open(os.path.join(args.save_dir, \'words_vocab.pkl\'), \'wb\') as f:\n        cPickle.dump((data_loader.words, data_loader.vocab), f)\n\n    model = Model(args)\n\n    merged = tf.summary.merge_all()\n    train_writer = tf.summary.FileWriter(args.log_dir)\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_mem)\n\n    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n        train_writer.add_graph(sess.graph)\n        tf.global_variables_initializer().run()\n        saver = tf.train.Saver(tf.global_variables())\n        # restore model\n        if args.init_from is not None:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        for e in range(model.epoch_pointer.eval(), args.num_epochs):\n            sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n            data_loader.reset_batch_pointer()\n            state = sess.run(model.initial_state)\n            speed = 0\n            if args.init_from is None:\n                assign_op = model.epoch_pointer.assign(e)\n                sess.run(assign_op)\n            if args.init_from is not None:\n                data_loader.pointer = model.batch_pointer.eval()\n                args.init_from = None\n            for b in range(data_loader.pointer, data_loader.num_batches):\n                start = time.time()\n                x, y = data_loader.next_batch()\n                feed = {model.input_data: x, model.targets: y, model.initial_state: state,\n                        model.batch_time: speed}\n                summary, train_loss, state, _, _ = sess.run([merged, model.cost, model.final_state,\n                                                             model.train_op, model.inc_batch_pointer_op], feed)\n                train_writer.add_summary(summary, e * data_loader.num_batches + b)\n                speed = time.time() - start\n                if (e * data_loader.num_batches + b) % args.batch_size == 0:\n                    print(""{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}"" \\\n                        .format(e * data_loader.num_batches + b,\n                                args.num_epochs * data_loader.num_batches,\n                                e, train_loss, speed))\n                if (e * data_loader.num_batches + b) % args.save_every == 0 \\\n                        or (e==args.num_epochs-1 and b == data_loader.num_batches-1): # save for the last result\n                    checkpoint_path = os.path.join(args.save_dir, \'model.ckpt\')\n                    saver.save(sess, checkpoint_path, global_step = e * data_loader.num_batches + b)\n                    print(""model saved to {}"".format(checkpoint_path))\n        train_writer.close()\n\nif __name__ == \'__main__\':\n    main()\n'"
utils.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport codecs\nimport collections\nfrom six.moves import cPickle\nimport numpy as np\nimport re\nimport itertools\n\nclass TextLoader():\n    def __init__(self, data_dir, batch_size, seq_length, encoding=None):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.seq_length = seq_length\n\n        input_file = os.path.join(data_dir, ""input.txt"")\n        vocab_file = os.path.join(data_dir, ""vocab.pkl"")\n        tensor_file = os.path.join(data_dir, ""data.npy"")\n\n        # Let\'s not read voca and data from file. We many change them.\n        if True or not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n            print(""reading text file"")\n            self.preprocess(input_file, vocab_file, tensor_file, encoding)\n        else:\n            print(""loading preprocessed files"")\n            self.load_preprocessed(vocab_file, tensor_file)\n        self.create_batches()\n        self.reset_batch_pointer()\n\n    def clean_str(self, string):\n        """"""\n        Tokenization/string cleaning for all datasets except for SST.\n        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data\n        """"""\n        string = re.sub(r""[^\xea\xb0\x80-\xed\x9e\xa3A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n        string = re.sub(r""\\\'s"", "" \\\'s"", string)\n        string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n        string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n        string = re.sub(r""\\\'re"", "" \\\'re"", string)\n        string = re.sub(r""\\\'d"", "" \\\'d"", string)\n        string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n        string = re.sub(r"","", "" , "", string)\n        string = re.sub(r""!"", "" ! "", string)\n        string = re.sub(r""\\("", "" \\( "", string)\n        string = re.sub(r""\\)"", "" \\) "", string)\n        string = re.sub(r""\\?"", "" \\? "", string)\n        string = re.sub(r""\\s{2,}"", "" "", string)\n        return string.strip().lower()\n\n    def build_vocab(self, sentences):\n        """"""\n        Builds a vocabulary mapping from word to index based on the sentences.\n        Returns vocabulary mapping and inverse vocabulary mapping.\n        """"""\n        # Build vocabulary\n        word_counts = collections.Counter(sentences)\n        # Mapping from index to word\n        vocabulary_inv = [x[0] for x in word_counts.most_common()]\n        vocabulary_inv = list(sorted(vocabulary_inv))\n        # Mapping from word to index\n        vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n        return [vocabulary, vocabulary_inv]\n\n    def preprocess(self, input_file, vocab_file, tensor_file, encoding):\n        with codecs.open(input_file, ""r"", encoding=encoding) as f:\n            data = f.read()\n\n        # Optional text cleaning or make them lower case, etc.\n        #data = self.clean_str(data)\n        x_text = data.split()\n\n        self.vocab, self.words = self.build_vocab(x_text)\n        self.vocab_size = len(self.words)\n\n        with open(vocab_file, \'wb\') as f:\n            cPickle.dump(self.words, f)\n\n        #The same operation like this [self.vocab[word] for word in x_text]\n        # index of words as our basic data\n        self.tensor = np.array(list(map(self.vocab.get, x_text)))\n        # Save the data to data.npy\n        np.save(tensor_file, self.tensor)\n\n    def load_preprocessed(self, vocab_file, tensor_file):\n        with open(vocab_file, \'rb\') as f:\n            self.words = cPickle.load(f)\n        self.vocab_size = len(self.words)\n        self.vocab = dict(zip(self.words, range(len(self.words))))\n        self.tensor = np.load(tensor_file)\n        self.num_batches = int(self.tensor.size / (self.batch_size *\n                                                   self.seq_length))\n\n    def create_batches(self):\n        self.num_batches = int(self.tensor.size / (self.batch_size *\n                                                   self.seq_length))\n        if self.num_batches==0:\n            assert False, ""Not enough data. Make seq_length and batch_size small.""\n\n        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n        xdata = self.tensor\n        ydata = np.copy(self.tensor)\n\n        ydata[:-1] = xdata[1:]\n        ydata[-1] = xdata[0]\n        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n\n    def next_batch(self):\n        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n        self.pointer += 1\n        return x, y\n\n    def reset_batch_pointer(self):\n        self.pointer = 0\n'"
tests/test_beam.py,0,"b'import unittest\nimport numpy as np\n\nfrom beam import BeamSearch\n\n\ndef naive_predict(sample, state):\n    """"""Fake predict function.\n\n    For our model, let\'s assume a vocabulary of size 5. Furthermore, let\'s say\n    that the `state` is exactly the probability that each vocabulary occurs,\n    and these probabilities never change.\n    """"""\n\n    return np.array(state)[None, :], state\n\n\nclass TestBeamMethods(unittest.TestCase):\n    def setUp(self):\n        self.prime_labels = [0, 1]\n        self.initial_state = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n    def test_single_beam(self):\n        bs = BeamSearch(naive_predict, self.initial_state, self.prime_labels)\n        samples, scores = bs.search(None, None, k=1, maxsample=5)\n        self.assertEqual(samples, [[0, 1, 4, 4, 4]])\n\n    def test_multiple_beams(self):\n        bs = BeamSearch(naive_predict, self.initial_state, self.prime_labels)\n        samples, scores = bs.search(None, None, k=4, maxsample=5)\n\n        self.assertIn([0, 1, 4, 4, 4], samples)\n\n        # All permutations of this form must be in the results.\n        self.assertIn([0, 1, 4, 4, 3], samples)\n        self.assertIn([0, 1, 4, 3, 4], samples)\n        self.assertIn([0, 1, 3, 4, 4], samples)\n\n        # Make sure that the best beam has the lowest score.\n        self.assertEqual(samples[np.argmin(scores)], [0, 1, 4, 4, 4])\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_example.py,0,"b""import unittest\n\nfrom utils import TextLoader\n\n\nclass TestStringMethods(unittest.TestCase):\n\n  def test_upper(self):\n      self.assertEqual('foo'.upper(), 'FOO')\n\n  def test_isupper(self):\n      self.assertTrue('FOO'.isupper())\n      self.assertFalse('Foo'.isupper())\n\n  def test_split(self):\n      s = 'hello world'\n      self.assertEqual(s.split(), ['hello', 'world'])\n      # check that s.split fails when the separator is not a string\n      with self.assertRaises(TypeError):\n          s.split(2)\n\nif __name__ == '__main__':\n    unittest.main()"""
tests/test_train.py,0,"b'import unittest\nfrom utils import TextLoader\nimport numpy as np\n\nclass TestUtilsMethods(unittest.TestCase):\n    def setUp(self):\n        self.data_loader = TextLoader(""tests/test_data"", batch_size=2, seq_length=5)\n\n    def test_init(self):\n        print (""Work in progress."")\n\n\nif __name__ == \'__main__\':\n    unittest.main()'"
tests/test_utils.py,0,"b'import unittest\nfrom utils import TextLoader\nimport numpy as np\nfrom collections import Counter\n\nclass TestUtilsMethods(unittest.TestCase):\n    def setUp(self):\n        self.data_loader = TextLoader(""tests/test_data"", batch_size=2, seq_length=5)\n\n    def test_init(self):\n      print (self.data_loader.vocab)\n      print (self.data_loader.tensor)\n      print (self.data_loader.vocab_size)\n\n    def test_build_vocab(self):\n        sentences = [""I"", ""love"", ""cat"", ""cat""]\n        vocab, vocab_inv = self.data_loader.build_vocab(sentences)\n        print (vocab, vocab_inv)\n\n        # Must include I, love, and cat\n        self.assertEqual(Counter(list(vocab)), Counter(list([""I"", ""love"", ""cat""])))\n        self.assertDictEqual(vocab, {\'I\': 0, \'love\': 2, \'cat\': 1})\n\n        self.assertEqual(Counter(list(vocab_inv)), Counter(list([""I"", ""love"", ""cat""])))\n\n    def test_batch_vocab(self):\n        print (np.array(self.data_loader.x_batches).shape)\n        self.assertEqual(Counter(list(self.data_loader.x_batches[0][0][1:])),\n                              Counter(list(self.data_loader.y_batches[0][0][:-1])))\n        self.assertEqual(Counter(list(self.data_loader.x_batches[0][1][1:])),\n                              Counter(list(self.data_loader.y_batches[0][1][:-1])))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
