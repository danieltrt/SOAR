file_path,api_count,code
advanced_east.py,0,"b'import os\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import Adam\n\nimport cfg\nfrom network import East\nfrom losses import quad_loss\nfrom data_generator import gen\n\neast = East()\neast_network = east.east_network()\neast_network.summary()\neast_network.compile(loss=quad_loss, optimizer=Adam(lr=cfg.lr,\n                                                    # clipvalue=cfg.clipvalue,\n                                                    decay=cfg.decay))\nif cfg.load_weights and os.path.exists(cfg.saved_model_weights_file_path):\n    east_network.load_weights(cfg.saved_model_weights_file_path)\n\neast_network.fit_generator(generator=gen(),\n                           steps_per_epoch=cfg.steps_per_epoch,\n                           epochs=cfg.epoch_num,\n                           validation_data=gen(is_val=True),\n                           validation_steps=cfg.validation_steps,\n                           verbose=1,\n                           initial_epoch=cfg.initial_epoch,\n                           callbacks=[\n                               EarlyStopping(patience=cfg.patience, verbose=1),\n                               ModelCheckpoint(filepath=cfg.model_weights_path,\n                                               save_best_only=True,\n                                               save_weights_only=True,\n                                               verbose=1)])\neast_network.save(cfg.saved_model_file_path)\neast_network.save_weights(cfg.saved_model_weights_file_path)\n'"
cfg.py,0,"b""import os\n\ntrain_task_id = '3T256'\ninitial_epoch = 0\nepoch_num = 24\nlr = 1e-3\ndecay = 5e-4\n# clipvalue = 0.5  # default 0.5, 0 means no clip\npatience = 5\nload_weights = False\nlambda_inside_score_loss = 4.0\nlambda_side_vertex_code_loss = 1.0\nlambda_side_vertex_coord_loss = 1.0\n\ntotal_img = 10000\nvalidation_split_ratio = 0.1\nmax_train_img_size = int(train_task_id[-3:])\nmax_predict_img_size = int(train_task_id[-3:])  # 2400\nassert max_train_img_size in [256, 384, 512, 640, 736], \\\n    'max_train_img_size must in [256, 384, 512, 640, 736]'\nif max_train_img_size == 256:\n    batch_size = 8\nelif max_train_img_size == 384:\n    batch_size = 4\nelif max_train_img_size == 512:\n    batch_size = 2\nelse:\n    batch_size = 1\nsteps_per_epoch = total_img * (1 - validation_split_ratio) // batch_size\nvalidation_steps = total_img * validation_split_ratio // batch_size\n\ndata_dir = 'icpr/'\norigin_image_dir_name = 'image_10000/'\norigin_txt_dir_name = 'txt_10000/'\ntrain_image_dir_name = 'images_%s/' % train_task_id\ntrain_label_dir_name = 'labels_%s/' % train_task_id\nshow_gt_image_dir_name = 'show_gt_images_%s/' % train_task_id\nshow_act_image_dir_name = 'show_act_images_%s/' % train_task_id\ngen_origin_img = True\ndraw_gt_quad = True\ndraw_act_quad = True\nval_fname = 'val_%s.txt' % train_task_id\ntrain_fname = 'train_%s.txt' % train_task_id\n# in paper it's 0.3, maybe to large to this problem\nshrink_ratio = 0.2\n# pixels between 0.2 and 0.6 are side pixels\nshrink_side_ratio = 0.6\nepsilon = 1e-4\n\nnum_channels = 3\nfeature_layers_range = range(5, 1, -1)\n# feature_layers_range = range(3, 0, -1)\nfeature_layers_num = len(feature_layers_range)\n# pixel_size = 4\npixel_size = 2 ** feature_layers_range[-1]\nlocked_layers = False\n\nif not os.path.exists('model'):\n    os.mkdir('model')\nif not os.path.exists('saved_model'):\n    os.mkdir('saved_model')\n\nmodel_weights_path = 'model/weights_%s.{epoch:03d}-{val_loss:.3f}.h5' \\\n                     % train_task_id\nsaved_model_file_path = 'saved_model/east_model_%s.h5' % train_task_id\nsaved_model_weights_file_path = 'saved_model/east_model_weights_%s.h5'\\\n                                % train_task_id\n\npixel_threshold = 0.9\nside_vertex_pixel_threshold = 0.9\ntrunc_threshold = 0.1\npredict_cut_text_line = False\npredict_write2txt = True\n"""
data_generator.py,0,"b""import os\nimport numpy as np\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\n\nimport cfg\n\n\ndef gen(batch_size=cfg.batch_size, is_val=False):\n    img_h, img_w = cfg.max_train_img_size, cfg.max_train_img_size\n    x = np.zeros((batch_size, img_h, img_w, cfg.num_channels), dtype=np.float32)\n    pixel_num_h = img_h // cfg.pixel_size\n    pixel_num_w = img_w // cfg.pixel_size\n    y = np.zeros((batch_size, pixel_num_h, pixel_num_w, 7), dtype=np.float32)\n    if is_val:\n        with open(os.path.join(cfg.data_dir, cfg.val_fname), 'r') as f_val:\n            f_list = f_val.readlines()\n    else:\n        with open(os.path.join(cfg.data_dir, cfg.train_fname), 'r') as f_train:\n            f_list = f_train.readlines()\n    while True:\n        for i in range(batch_size):\n            # random gen an image name\n            random_img = np.random.choice(f_list)\n            img_filename = str(random_img).strip().split(',')[0]\n            # load img and img anno\n            img_path = os.path.join(cfg.data_dir,\n                                    cfg.train_image_dir_name,\n                                    img_filename)\n            img = image.load_img(img_path)\n            img = image.img_to_array(img)\n            x[i] = preprocess_input(img, mode='tf')\n            gt_file = os.path.join(cfg.data_dir,\n                                   cfg.train_label_dir_name,\n                                   img_filename[:-4] + '_gt.npy')\n            y[i] = np.load(gt_file)\n        yield x, y\n"""
label.py,0,"b""import numpy as np\nimport os\nfrom PIL import Image, ImageDraw\nfrom tqdm import tqdm\nimport cfg\n\n\ndef point_inside_of_quad(px, py, quad_xy_list, p_min, p_max):\n    if (p_min[0] <= px <= p_max[0]) and (p_min[1] <= py <= p_max[1]):\n        xy_list = np.zeros((4, 2))\n        xy_list[:3, :] = quad_xy_list[1:4, :] - quad_xy_list[:3, :]\n        xy_list[3] = quad_xy_list[0, :] - quad_xy_list[3, :]\n        yx_list = np.zeros((4, 2))\n        yx_list[:, :] = quad_xy_list[:, -1:-3:-1]\n        a = xy_list * ([py, px] - yx_list)\n        b = a[:, 0] - a[:, 1]\n        if np.amin(b) >= 0 or np.amax(b) <= 0:\n            return True\n        else:\n            return False\n    else:\n        return False\n\n\ndef point_inside_of_nth_quad(px, py, xy_list, shrink_1, long_edge):\n    nth = -1\n    vs = [[[0, 0, 3, 3, 0], [1, 1, 2, 2, 1]],\n          [[0, 0, 1, 1, 0], [2, 2, 3, 3, 2]]]\n    for ith in range(2):\n        quad_xy_list = np.concatenate((\n            np.reshape(xy_list[vs[long_edge][ith][0]], (1, 2)),\n            np.reshape(shrink_1[vs[long_edge][ith][1]], (1, 2)),\n            np.reshape(shrink_1[vs[long_edge][ith][2]], (1, 2)),\n            np.reshape(xy_list[vs[long_edge][ith][3]], (1, 2))), axis=0)\n        p_min = np.amin(quad_xy_list, axis=0)\n        p_max = np.amax(quad_xy_list, axis=0)\n        if point_inside_of_quad(px, py, quad_xy_list, p_min, p_max):\n            if nth == -1:\n                nth = ith\n            else:\n                nth = -1\n                break\n    return nth\n\n\ndef shrink(xy_list, ratio=cfg.shrink_ratio):\n    if ratio == 0.0:\n        return xy_list, xy_list\n    diff_1to3 = xy_list[:3, :] - xy_list[1:4, :]\n    diff_4 = xy_list[3:4, :] - xy_list[0:1, :]\n    diff = np.concatenate((diff_1to3, diff_4), axis=0)\n    dis = np.sqrt(np.sum(np.square(diff), axis=-1))\n    # determine which are long or short edges\n    long_edge = int(np.argmax(np.sum(np.reshape(dis, (2, 2)), axis=0)))\n    short_edge = 1 - long_edge\n    # cal r length array\n    r = [np.minimum(dis[i], dis[(i + 1) % 4]) for i in range(4)]\n    # cal theta array\n    diff_abs = np.abs(diff)\n    diff_abs[:, 0] += cfg.epsilon\n    theta = np.arctan(diff_abs[:, 1] / diff_abs[:, 0])\n    # shrink two long edges\n    temp_new_xy_list = np.copy(xy_list)\n    shrink_edge(xy_list, temp_new_xy_list, long_edge, r, theta, ratio)\n    shrink_edge(xy_list, temp_new_xy_list, long_edge + 2, r, theta, ratio)\n    # shrink two short edges\n    new_xy_list = np.copy(temp_new_xy_list)\n    shrink_edge(temp_new_xy_list, new_xy_list, short_edge, r, theta, ratio)\n    shrink_edge(temp_new_xy_list, new_xy_list, short_edge + 2, r, theta, ratio)\n    return temp_new_xy_list, new_xy_list, long_edge\n\n\ndef shrink_edge(xy_list, new_xy_list, edge, r, theta, ratio=cfg.shrink_ratio):\n    if ratio == 0.0:\n        return\n    start_point = edge\n    end_point = (edge + 1) % 4\n    long_start_sign_x = np.sign(\n        xy_list[end_point, 0] - xy_list[start_point, 0])\n    new_xy_list[start_point, 0] = \\\n        xy_list[start_point, 0] + \\\n        long_start_sign_x * ratio * r[start_point] * np.cos(theta[start_point])\n    long_start_sign_y = np.sign(\n        xy_list[end_point, 1] - xy_list[start_point, 1])\n    new_xy_list[start_point, 1] = \\\n        xy_list[start_point, 1] + \\\n        long_start_sign_y * ratio * r[start_point] * np.sin(theta[start_point])\n    # long edge one, end point\n    long_end_sign_x = -1 * long_start_sign_x\n    new_xy_list[end_point, 0] = \\\n        xy_list[end_point, 0] + \\\n        long_end_sign_x * ratio * r[end_point] * np.cos(theta[start_point])\n    long_end_sign_y = -1 * long_start_sign_y\n    new_xy_list[end_point, 1] = \\\n        xy_list[end_point, 1] + \\\n        long_end_sign_y * ratio * r[end_point] * np.sin(theta[start_point])\n\n\ndef process_label(data_dir=cfg.data_dir):\n    with open(os.path.join(data_dir, cfg.val_fname), 'r') as f_val:\n        f_list = f_val.readlines()\n    with open(os.path.join(data_dir, cfg.train_fname), 'r') as f_train:\n        f_list.extend(f_train.readlines())\n    for line, _ in zip(f_list, tqdm(range(len(f_list)))):\n        line_cols = str(line).strip().split(',')\n        img_name, width, height = \\\n            line_cols[0].strip(), int(line_cols[1].strip()), \\\n            int(line_cols[2].strip())\n        gt = np.zeros((height // cfg.pixel_size, width // cfg.pixel_size, 7))\n        train_label_dir = os.path.join(data_dir, cfg.train_label_dir_name)\n        xy_list_array = np.load(os.path.join(train_label_dir,\n                                             img_name[:-4] + '.npy'))\n        train_image_dir = os.path.join(data_dir, cfg.train_image_dir_name)\n        with Image.open(os.path.join(train_image_dir, img_name)) as im:\n            draw = ImageDraw.Draw(im)\n            for xy_list in xy_list_array:\n                _, shrink_xy_list, _ = shrink(xy_list, cfg.shrink_ratio)\n                shrink_1, _, long_edge = shrink(xy_list, cfg.shrink_side_ratio)\n                p_min = np.amin(shrink_xy_list, axis=0)\n                p_max = np.amax(shrink_xy_list, axis=0)\n                # floor of the float\n                ji_min = (p_min / cfg.pixel_size - 0.5).astype(int) - 1\n                # +1 for ceil of the float and +1 for include the end\n                ji_max = (p_max / cfg.pixel_size - 0.5).astype(int) + 3\n                imin = np.maximum(0, ji_min[1])\n                imax = np.minimum(height // cfg.pixel_size, ji_max[1])\n                jmin = np.maximum(0, ji_min[0])\n                jmax = np.minimum(width // cfg.pixel_size, ji_max[0])\n                for i in range(imin, imax):\n                    for j in range(jmin, jmax):\n                        px = (j + 0.5) * cfg.pixel_size\n                        py = (i + 0.5) * cfg.pixel_size\n                        if point_inside_of_quad(px, py,\n                                                shrink_xy_list, p_min, p_max):\n                            gt[i, j, 0] = 1\n                            line_width, line_color = 1, 'red'\n                            ith = point_inside_of_nth_quad(px, py,\n                                                           xy_list,\n                                                           shrink_1,\n                                                           long_edge)\n                            vs = [[[3, 0], [1, 2]], [[0, 1], [2, 3]]]\n                            if ith in range(2):\n                                gt[i, j, 1] = 1\n                                if ith == 0:\n                                    line_width, line_color = 2, 'yellow'\n                                else:\n                                    line_width, line_color = 2, 'green'\n                                gt[i, j, 2:3] = ith\n                                gt[i, j, 3:5] = \\\n                                    xy_list[vs[long_edge][ith][0]] - [px, py]\n                                gt[i, j, 5:] = \\\n                                    xy_list[vs[long_edge][ith][1]] - [px, py]\n                            draw.line([(px - 0.5 * cfg.pixel_size,\n                                        py - 0.5 * cfg.pixel_size),\n                                       (px + 0.5 * cfg.pixel_size,\n                                        py - 0.5 * cfg.pixel_size),\n                                       (px + 0.5 * cfg.pixel_size,\n                                        py + 0.5 * cfg.pixel_size),\n                                       (px - 0.5 * cfg.pixel_size,\n                                        py + 0.5 * cfg.pixel_size),\n                                       (px - 0.5 * cfg.pixel_size,\n                                        py - 0.5 * cfg.pixel_size)],\n                                      width=line_width, fill=line_color)\n            act_image_dir = os.path.join(cfg.data_dir,\n                                         cfg.show_act_image_dir_name)\n            if cfg.draw_act_quad:\n                im.save(os.path.join(act_image_dir, img_name))\n        train_label_dir = os.path.join(data_dir, cfg.train_label_dir_name)\n        np.save(os.path.join(train_label_dir,\n                             img_name[:-4] + '_gt.npy'), gt)\n\n\nif __name__ == '__main__':\n    process_label()\n"""
losses.py,26,"b'import tensorflow as tf\n\nimport cfg\n\n\ndef quad_loss(y_true, y_pred):\n    # loss for inside_score\n    logits = y_pred[:, :, :, :1]\n    labels = y_true[:, :, :, :1]\n    # balance positive and negative samples in an image\n    beta = 1 - tf.reduce_mean(labels)\n    # first apply sigmoid activation\n    predicts = tf.nn.sigmoid(logits)\n    # log +epsilon for stable cal\n    inside_score_loss = tf.reduce_mean(\n        -1 * (beta * labels * tf.log(predicts + cfg.epsilon) +\n              (1 - beta) * (1 - labels) * tf.log(1 - predicts + cfg.epsilon)))\n    inside_score_loss *= cfg.lambda_inside_score_loss\n\n    # loss for side_vertex_code\n    vertex_logits = y_pred[:, :, :, 1:3]\n    vertex_labels = y_true[:, :, :, 1:3]\n    vertex_beta = 1 - (tf.reduce_mean(y_true[:, :, :, 1:2])\n                       / (tf.reduce_mean(labels) + cfg.epsilon))\n    vertex_predicts = tf.nn.sigmoid(vertex_logits)\n    pos = -1 * vertex_beta * vertex_labels * tf.log(vertex_predicts +\n                                                    cfg.epsilon)\n    neg = -1 * (1 - vertex_beta) * (1 - vertex_labels) * tf.log(\n        1 - vertex_predicts + cfg.epsilon)\n    positive_weights = tf.cast(tf.equal(y_true[:, :, :, 0], 1), tf.float32)\n    side_vertex_code_loss = \\\n        tf.reduce_sum(tf.reduce_sum(pos + neg, axis=-1) * positive_weights) / (\n                tf.reduce_sum(positive_weights) + cfg.epsilon)\n    side_vertex_code_loss *= cfg.lambda_side_vertex_code_loss\n\n    # loss for side_vertex_coord delta\n    g_hat = y_pred[:, :, :, 3:]\n    g_true = y_true[:, :, :, 3:]\n    vertex_weights = tf.cast(tf.equal(y_true[:, :, :, 1], 1), tf.float32)\n    pixel_wise_smooth_l1norm = smooth_l1_loss(g_hat, g_true, vertex_weights)\n    side_vertex_coord_loss = tf.reduce_sum(pixel_wise_smooth_l1norm) / (\n            tf.reduce_sum(vertex_weights) + cfg.epsilon)\n    side_vertex_coord_loss *= cfg.lambda_side_vertex_coord_loss\n    return inside_score_loss + side_vertex_code_loss + side_vertex_coord_loss\n\n\ndef smooth_l1_loss(prediction_tensor, target_tensor, weights):\n    n_q = tf.reshape(quad_norm(target_tensor), tf.shape(weights))\n    diff = prediction_tensor - target_tensor\n    abs_diff = tf.abs(diff)\n    abs_diff_lt_1 = tf.less(abs_diff, 1)\n    pixel_wise_smooth_l1norm = (tf.reduce_sum(\n        tf.where(abs_diff_lt_1, 0.5 * tf.square(abs_diff), abs_diff - 0.5),\n        axis=-1) / n_q) * weights\n    return pixel_wise_smooth_l1norm\n\n\ndef quad_norm(g_true):\n    shape = tf.shape(g_true)\n    delta_xy_matrix = tf.reshape(g_true, [-1, 2, 2])\n    diff = delta_xy_matrix[:, 0:1, :] - delta_xy_matrix[:, 1:2, :]\n    square = tf.square(diff)\n    distance = tf.sqrt(tf.reduce_sum(square, axis=-1))\n    distance *= 4.0\n    distance += cfg.epsilon\n    return tf.reshape(distance, shape[:-1])\n'"
network.py,0,"b'# coding=utf-8\nfrom keras import Input, Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Concatenate, Conv2D, UpSampling2D, BatchNormalization\n\nimport cfg\n\n""""""\ninput_shape=(img.height, img.width, 3), height and width must scaled by 32.\nSo images\'s height and width need to be pre-processed to the nearest num that\nscaled by 32.And the annotations xy need to be scaled by the same ratio \nas height and width respectively.\n""""""\n\n\nclass East:\n\n    def __init__(self):\n        self.input_img = Input(name=\'input_img\',\n                               shape=(None, None, cfg.num_channels),\n                               dtype=\'float32\')\n        vgg16 = VGG16(input_tensor=self.input_img,\n                      weights=\'imagenet\',\n                      include_top=False)\n        if cfg.locked_layers:\n            # locked first two conv layers\n            locked_layers = [vgg16.get_layer(\'block1_conv1\'),\n                             vgg16.get_layer(\'block1_conv2\')]\n            for layer in locked_layers:\n                layer.trainable = False\n        self.f = [vgg16.get_layer(\'block%d_pool\' % i).output\n                  for i in cfg.feature_layers_range]\n        self.f.insert(0, None)\n        self.diff = cfg.feature_layers_range[0] - cfg.feature_layers_num\n\n    def g(self, i):\n        # i+diff in cfg.feature_layers_range\n        assert i + self.diff in cfg.feature_layers_range, \\\n            (\'i=%d+diff=%d not in \' % (i, self.diff)) + \\\n            str(cfg.feature_layers_range)\n        if i == cfg.feature_layers_num:\n            bn = BatchNormalization()(self.h(i))\n            return Conv2D(32, 3, activation=\'relu\', padding=\'same\')(bn)\n        else:\n            return UpSampling2D((2, 2))(self.h(i))\n\n    def h(self, i):\n        # i+diff in cfg.feature_layers_range\n        assert i + self.diff in cfg.feature_layers_range, \\\n            (\'i=%d+diff=%d not in \' % (i, self.diff)) + \\\n            str(cfg.feature_layers_range)\n        if i == 1:\n            return self.f[i]\n        else:\n            concat = Concatenate(axis=-1)([self.g(i - 1), self.f[i]])\n            bn1 = BatchNormalization()(concat)\n            conv_1 = Conv2D(128 // 2 ** (i - 2), 1,\n                            activation=\'relu\', padding=\'same\',)(bn1)\n            bn2 = BatchNormalization()(conv_1)\n            conv_3 = Conv2D(128 // 2 ** (i - 2), 3,\n                            activation=\'relu\', padding=\'same\',)(bn2)\n            return conv_3\n\n    def east_network(self):\n        before_output = self.g(cfg.feature_layers_num)\n        inside_score = Conv2D(1, 1, padding=\'same\', name=\'inside_score\'\n                              )(before_output)\n        side_v_code = Conv2D(2, 1, padding=\'same\', name=\'side_vertex_code\'\n                             )(before_output)\n        side_v_coord = Conv2D(4, 1, padding=\'same\', name=\'side_vertex_coord\'\n                              )(before_output)\n        east_detect = Concatenate(axis=-1,\n                                  name=\'east_detect\')([inside_score,\n                                                       side_v_code,\n                                                       side_v_coord])\n        return Model(inputs=self.input_img, outputs=east_detect)\n\n\nif __name__ == \'__main__\':\n    east = East()\n    east_network = east.east_network()\n    east_network.summary()\n'"
nms.py,0,"b'# coding=utf-8\nimport numpy as np\n\nimport cfg\n\n\ndef should_merge(region, i, j):\n    neighbor = {(i, j - 1)}\n    return not region.isdisjoint(neighbor)\n\n\ndef region_neighbor(region_set):\n    region_pixels = np.array(list(region_set))\n    j_min = np.amin(region_pixels, axis=0)[1] - 1\n    j_max = np.amax(region_pixels, axis=0)[1] + 1\n    i_m = np.amin(region_pixels, axis=0)[0] + 1\n    region_pixels[:, 0] += 1\n    neighbor = {(region_pixels[n, 0], region_pixels[n, 1]) for n in\n                range(len(region_pixels))}\n    neighbor.add((i_m, j_min))\n    neighbor.add((i_m, j_max))\n    return neighbor\n\n\ndef region_group(region_list):\n    S = [i for i in range(len(region_list))]\n    D = []\n    while len(S) > 0:\n        m = S.pop(0)\n        if len(S) == 0:\n            # S has only one element, put it to D\n            D.append([m])\n        else:\n            D.append(rec_region_merge(region_list, m, S))\n    return D\n\n\ndef rec_region_merge(region_list, m, S):\n    rows = [m]\n    tmp = []\n    for n in S:\n        if not region_neighbor(region_list[m]).isdisjoint(region_list[n]) or \\\n                not region_neighbor(region_list[n]).isdisjoint(region_list[m]):\n            # \xe7\xac\xacm\xe4\xb8\x8en\xe7\x9b\xb8\xe4\xba\xa4\n            tmp.append(n)\n    for d in tmp:\n        S.remove(d)\n    for e in tmp:\n        rows.extend(rec_region_merge(region_list, e, S))\n    return rows\n\n\ndef nms(predict, activation_pixels, threshold=cfg.side_vertex_pixel_threshold):\n    region_list = []\n    for i, j in zip(activation_pixels[0], activation_pixels[1]):\n        merge = False\n        for k in range(len(region_list)):\n            if should_merge(region_list[k], i, j):\n                region_list[k].add((i, j))\n                merge = True\n                # Fixme \xe9\x87\x8d\xe5\x8f\xa0\xe6\x96\x87\xe6\x9c\xac\xe5\x8c\xba\xe5\x9f\x9f\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\xad\x98\xe5\x9c\xa8\xe5\x92\x8c\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8c\xba\xe5\x9f\x9f\xe9\x82\xbb\xe6\x8e\xa5\xe7\x9a\x84pixels\xef\xbc\x8c\xe5\x85\x88\xe9\x83\xbdmerge\xe8\xaf\x95\xe8\xaf\x95\n                # break\n        if not merge:\n            region_list.append({(i, j)})\n    D = region_group(region_list)\n    quad_list = np.zeros((len(D), 4, 2))\n    score_list = np.zeros((len(D), 4))\n    for group, g_th in zip(D, range(len(D))):\n        total_score = np.zeros((4, 2))\n        for row in group:\n            for ij in region_list[row]:\n                score = predict[ij[0], ij[1], 1]\n                if score >= threshold:\n                    ith_score = predict[ij[0], ij[1], 2:3]\n                    if not (cfg.trunc_threshold <= ith_score < 1 -\n                            cfg.trunc_threshold):\n                        ith = int(np.around(ith_score))\n                        total_score[ith * 2:(ith + 1) * 2] += score\n                        px = (ij[1] + 0.5) * cfg.pixel_size\n                        py = (ij[0] + 0.5) * cfg.pixel_size\n                        p_v = [px, py] + np.reshape(predict[ij[0], ij[1], 3:7],\n                                              (2, 2))\n                        quad_list[g_th, ith * 2:(ith + 1) * 2] += score * p_v\n        score_list[g_th] = total_score[:, 0]\n        quad_list[g_th] /= (total_score + cfg.epsilon)\n    return score_list, quad_list\n'"
predict.py,0,"b'import argparse\n\nimport numpy as np\nfrom PIL import Image, ImageDraw\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\n\nimport cfg\nfrom label import point_inside_of_quad\nfrom network import East\nfrom preprocess import resize_image\nfrom nms import nms\n\n\ndef sigmoid(x):\n    """"""`y = 1 / (1 + exp(-x))`""""""\n    return 1 / (1 + np.exp(-x))\n\n\ndef cut_text_line(geo, scale_ratio_w, scale_ratio_h, im_array, img_path, s):\n    geo /= [scale_ratio_w, scale_ratio_h]\n    p_min = np.amin(geo, axis=0)\n    p_max = np.amax(geo, axis=0)\n    min_xy = p_min.astype(int)\n    max_xy = p_max.astype(int) + 2\n    sub_im_arr = im_array[min_xy[1]:max_xy[1], min_xy[0]:max_xy[0], :].copy()\n    for m in range(min_xy[1], max_xy[1]):\n        for n in range(min_xy[0], max_xy[0]):\n            if not point_inside_of_quad(n, m, geo, p_min, p_max):\n                sub_im_arr[m - min_xy[1], n - min_xy[0], :] = 255\n    sub_im = image.array_to_img(sub_im_arr, scale=False)\n    sub_im.save(img_path + \'_subim%d.jpg\' % s)\n\n\ndef predict(east_detect, img_path, pixel_threshold, quiet=False):\n    img = image.load_img(img_path)\n    d_wight, d_height = resize_image(img, cfg.max_predict_img_size)\n    img = img.resize((d_wight, d_height), Image.NEAREST).convert(\'RGB\')\n    img = image.img_to_array(img)\n    img = preprocess_input(img, mode=\'tf\')\n    x = np.expand_dims(img, axis=0)\n    y = east_detect.predict(x)\n\n    y = np.squeeze(y, axis=0)\n    y[:, :, :3] = sigmoid(y[:, :, :3])\n    cond = np.greater_equal(y[:, :, 0], pixel_threshold)\n    activation_pixels = np.where(cond)\n    quad_scores, quad_after_nms = nms(y, activation_pixels)\n    with Image.open(img_path) as im:\n        im_array = image.img_to_array(im.convert(\'RGB\'))\n        d_wight, d_height = resize_image(im, cfg.max_predict_img_size)\n        scale_ratio_w = d_wight / im.width\n        scale_ratio_h = d_height / im.height\n        im = im.resize((d_wight, d_height), Image.NEAREST).convert(\'RGB\')\n        quad_im = im.copy()\n        draw = ImageDraw.Draw(im)\n        for i, j in zip(activation_pixels[0], activation_pixels[1]):\n            px = (j + 0.5) * cfg.pixel_size\n            py = (i + 0.5) * cfg.pixel_size\n            line_width, line_color = 1, \'red\'\n            if y[i, j, 1] >= cfg.side_vertex_pixel_threshold:\n                if y[i, j, 2] < cfg.trunc_threshold:\n                    line_width, line_color = 2, \'yellow\'\n                elif y[i, j, 2] >= 1 - cfg.trunc_threshold:\n                    line_width, line_color = 2, \'green\'\n            draw.line([(px - 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size),\n                       (px + 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size),\n                       (px + 0.5 * cfg.pixel_size, py + 0.5 * cfg.pixel_size),\n                       (px - 0.5 * cfg.pixel_size, py + 0.5 * cfg.pixel_size),\n                       (px - 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size)],\n                      width=line_width, fill=line_color)\n        im.save(img_path + \'_act.jpg\')\n        quad_draw = ImageDraw.Draw(quad_im)\n        txt_items = []\n        for score, geo, s in zip(quad_scores, quad_after_nms,\n                                 range(len(quad_scores))):\n            if np.amin(score) > 0:\n                quad_draw.line([tuple(geo[0]),\n                                tuple(geo[1]),\n                                tuple(geo[2]),\n                                tuple(geo[3]),\n                                tuple(geo[0])], width=2, fill=\'red\')\n                if cfg.predict_cut_text_line:\n                    cut_text_line(geo, scale_ratio_w, scale_ratio_h, im_array,\n                                  img_path, s)\n                rescaled_geo = geo / [scale_ratio_w, scale_ratio_h]\n                rescaled_geo_list = np.reshape(rescaled_geo, (8,)).tolist()\n                txt_item = \',\'.join(map(str, rescaled_geo_list))\n                txt_items.append(txt_item + \'\\n\')\n            elif not quiet:\n                print(\'quad invalid with vertex num less then 4.\')\n        quad_im.save(img_path + \'_predict.jpg\')\n        if cfg.predict_write2txt and len(txt_items) > 0:\n            with open(img_path[:-4] + \'.txt\', \'w\') as f_txt:\n                f_txt.writelines(txt_items)\n\n\ndef predict_txt(east_detect, img_path, txt_path, pixel_threshold, quiet=False):\n    img = image.load_img(img_path)\n    d_wight, d_height = resize_image(img, cfg.max_predict_img_size)\n    scale_ratio_w = d_wight / img.width\n    scale_ratio_h = d_height / img.height\n    img = img.resize((d_wight, d_height), Image.NEAREST).convert(\'RGB\')\n    img = image.img_to_array(img)\n    img = preprocess_input(img, mode=\'tf\')\n    x = np.expand_dims(img, axis=0)\n    y = east_detect.predict(x)\n\n    y = np.squeeze(y, axis=0)\n    y[:, :, :3] = sigmoid(y[:, :, :3])\n    cond = np.greater_equal(y[:, :, 0], pixel_threshold)\n    activation_pixels = np.where(cond)\n    quad_scores, quad_after_nms = nms(y, activation_pixels)\n\n    txt_items = []\n    for score, geo in zip(quad_scores, quad_after_nms):\n        if np.amin(score) > 0:\n            rescaled_geo = geo / [scale_ratio_w, scale_ratio_h]\n            rescaled_geo_list = np.reshape(rescaled_geo, (8,)).tolist()\n            txt_item = \',\'.join(map(str, rescaled_geo_list))\n            txt_items.append(txt_item + \'\\n\')\n        elif not quiet:\n            print(\'quad invalid with vertex num less then 4.\')\n    if cfg.predict_write2txt and len(txt_items) > 0:\n        with open(txt_path, \'w\') as f_txt:\n            f_txt.writelines(txt_items)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--path\', \'-p\',\n                        default=\'demo/012.png\',\n                        help=\'image path\')\n    parser.add_argument(\'--threshold\', \'-t\',\n                        default=cfg.pixel_threshold,\n                        help=\'pixel activation threshold\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    img_path = args.path\n    threshold = float(args.threshold)\n    print(img_path, threshold)\n\n    east = East()\n    east_detect = east.east_network()\n    east_detect.load_weights(cfg.saved_model_weights_file_path)\n    predict(east_detect, img_path, threshold)\n'"
preprocess.py,0,"b""import numpy as np\nfrom PIL import Image, ImageDraw\nimport os\nimport random\nfrom tqdm import tqdm\n\nimport cfg\nfrom label import shrink\n\n\ndef batch_reorder_vertexes(xy_list_array):\n    reorder_xy_list_array = np.zeros_like(xy_list_array)\n    for xy_list, i in zip(xy_list_array, range(len(xy_list_array))):\n        reorder_xy_list_array[i] = reorder_vertexes(xy_list)\n    return reorder_xy_list_array\n\n\ndef reorder_vertexes(xy_list):\n    reorder_xy_list = np.zeros_like(xy_list)\n    # determine the first point with the smallest x,\n    # if two has same x, choose that with smallest y,\n    ordered = np.argsort(xy_list, axis=0)\n    xmin1_index = ordered[0, 0]\n    xmin2_index = ordered[1, 0]\n    if xy_list[xmin1_index, 0] == xy_list[xmin2_index, 0]:\n        if xy_list[xmin1_index, 1] <= xy_list[xmin2_index, 1]:\n            reorder_xy_list[0] = xy_list[xmin1_index]\n            first_v = xmin1_index\n        else:\n            reorder_xy_list[0] = xy_list[xmin2_index]\n            first_v = xmin2_index\n    else:\n        reorder_xy_list[0] = xy_list[xmin1_index]\n        first_v = xmin1_index\n    # connect the first point to others, the third point on the other side of\n    # the line with the middle slope\n    others = list(range(4))\n    others.remove(first_v)\n    k = np.zeros((len(others),))\n    for index, i in zip(others, range(len(others))):\n        k[i] = (xy_list[index, 1] - xy_list[first_v, 1]) \\\n                    / (xy_list[index, 0] - xy_list[first_v, 0] + cfg.epsilon)\n    k_mid = np.argsort(k)[1]\n    third_v = others[k_mid]\n    reorder_xy_list[2] = xy_list[third_v]\n    # determine the second point which on the bigger side of the middle line\n    others.remove(third_v)\n    b_mid = xy_list[first_v, 1] - k[k_mid] * xy_list[first_v, 0]\n    second_v, fourth_v = 0, 0\n    for index, i in zip(others, range(len(others))):\n        # delta = y - (k * x + b)\n        delta_y = xy_list[index, 1] - (k[k_mid] * xy_list[index, 0] + b_mid)\n        if delta_y > 0:\n            second_v = index\n        else:\n            fourth_v = index\n    reorder_xy_list[1] = xy_list[second_v]\n    reorder_xy_list[3] = xy_list[fourth_v]\n    # compare slope of 13 and 24, determine the final order\n    k13 = k[k_mid]\n    k24 = (xy_list[second_v, 1] - xy_list[fourth_v, 1]) / (\n                xy_list[second_v, 0] - xy_list[fourth_v, 0] + cfg.epsilon)\n    if k13 < k24:\n        tmp_x, tmp_y = reorder_xy_list[3, 0], reorder_xy_list[3, 1]\n        for i in range(2, -1, -1):\n            reorder_xy_list[i + 1] = reorder_xy_list[i]\n        reorder_xy_list[0, 0], reorder_xy_list[0, 1] = tmp_x, tmp_y\n    return reorder_xy_list\n\n\ndef resize_image(im, max_img_size=cfg.max_train_img_size):\n    im_width = np.minimum(im.width, max_img_size)\n    if im_width == max_img_size < im.width:\n        im_height = int((im_width / im.width) * im.height)\n    else:\n        im_height = im.height\n    o_height = np.minimum(im_height, max_img_size)\n    if o_height == max_img_size < im_height:\n        o_width = int((o_height / im_height) * im_width)\n    else:\n        o_width = im_width\n    d_wight = o_width - (o_width % 32)\n    d_height = o_height - (o_height % 32)\n    return d_wight, d_height\n\n\ndef preprocess():\n    data_dir = cfg.data_dir\n    origin_image_dir = os.path.join(data_dir, cfg.origin_image_dir_name)\n    origin_txt_dir = os.path.join(data_dir, cfg.origin_txt_dir_name)\n    train_image_dir = os.path.join(data_dir, cfg.train_image_dir_name)\n    train_label_dir = os.path.join(data_dir, cfg.train_label_dir_name)\n    if not os.path.exists(train_image_dir):\n        os.mkdir(train_image_dir)\n    if not os.path.exists(train_label_dir):\n        os.mkdir(train_label_dir)\n    draw_gt_quad = cfg.draw_gt_quad\n    show_gt_image_dir = os.path.join(data_dir, cfg.show_gt_image_dir_name)\n    if not os.path.exists(show_gt_image_dir):\n        os.mkdir(show_gt_image_dir)\n    show_act_image_dir = os.path.join(cfg.data_dir, cfg.show_act_image_dir_name)\n    if not os.path.exists(show_act_image_dir):\n        os.mkdir(show_act_image_dir)\n\n    o_img_list = os.listdir(origin_image_dir)\n    print('found %d origin images.' % len(o_img_list))\n    train_val_set = []\n    for o_img_fname, _ in zip(o_img_list, tqdm(range(len(o_img_list)))):\n        with Image.open(os.path.join(origin_image_dir, o_img_fname)) as im:\n            # d_wight, d_height = resize_image(im)\n            d_wight, d_height = cfg.max_train_img_size, cfg.max_train_img_size\n            scale_ratio_w = d_wight / im.width\n            scale_ratio_h = d_height / im.height\n            im = im.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n            show_gt_im = im.copy()\n            # draw on the img\n            draw = ImageDraw.Draw(show_gt_im)\n            with open(os.path.join(origin_txt_dir,\n                                   o_img_fname[:-4] + '.txt'), 'r') as f:\n                anno_list = f.readlines()\n            xy_list_array = np.zeros((len(anno_list), 4, 2))\n            for anno, i in zip(anno_list, range(len(anno_list))):\n                anno_colums = anno.strip().split(',')\n                anno_array = np.array(anno_colums)\n                xy_list = np.reshape(anno_array[:8].astype(float), (4, 2))\n                xy_list[:, 0] = xy_list[:, 0] * scale_ratio_w\n                xy_list[:, 1] = xy_list[:, 1] * scale_ratio_h\n                xy_list = reorder_vertexes(xy_list)\n                xy_list_array[i] = xy_list\n                _, shrink_xy_list, _ = shrink(xy_list, cfg.shrink_ratio)\n                shrink_1, _, long_edge = shrink(xy_list, cfg.shrink_side_ratio)\n                if draw_gt_quad:\n                    draw.line([tuple(xy_list[0]), tuple(xy_list[1]),\n                               tuple(xy_list[2]), tuple(xy_list[3]),\n                               tuple(xy_list[0])\n                               ],\n                              width=2, fill='green')\n                    draw.line([tuple(shrink_xy_list[0]),\n                               tuple(shrink_xy_list[1]),\n                               tuple(shrink_xy_list[2]),\n                               tuple(shrink_xy_list[3]),\n                               tuple(shrink_xy_list[0])\n                               ],\n                              width=2, fill='blue')\n                    vs = [[[0, 0, 3, 3, 0], [1, 1, 2, 2, 1]],\n                          [[0, 0, 1, 1, 0], [2, 2, 3, 3, 2]]]\n                    for q_th in range(2):\n                        draw.line([tuple(xy_list[vs[long_edge][q_th][0]]),\n                                   tuple(shrink_1[vs[long_edge][q_th][1]]),\n                                   tuple(shrink_1[vs[long_edge][q_th][2]]),\n                                   tuple(xy_list[vs[long_edge][q_th][3]]),\n                                   tuple(xy_list[vs[long_edge][q_th][4]])],\n                                  width=3, fill='yellow')\n            if cfg.gen_origin_img:\n                im.save(os.path.join(train_image_dir, o_img_fname))\n            np.save(os.path.join(\n                train_label_dir,\n                o_img_fname[:-4] + '.npy'),\n                xy_list_array)\n            if draw_gt_quad:\n                show_gt_im.save(os.path.join(show_gt_image_dir, o_img_fname))\n            train_val_set.append('{},{},{}\\n'.format(o_img_fname,\n                                                     d_wight,\n                                                     d_height))\n\n    train_img_list = os.listdir(train_image_dir)\n    print('found %d train images.' % len(train_img_list))\n    train_label_list = os.listdir(train_label_dir)\n    print('found %d train labels.' % len(train_label_list))\n\n    random.shuffle(train_val_set)\n    val_count = int(cfg.validation_split_ratio * len(train_val_set))\n    with open(os.path.join(data_dir, cfg.val_fname), 'w') as f_val:\n        f_val.writelines(train_val_set[:val_count])\n    with open(os.path.join(data_dir, cfg.train_fname), 'w') as f_train:\n        f_train.writelines(train_val_set[val_count:])\n\n\nif __name__ == '__main__':\n    preprocess()\n"""
tianchi_check.py,0,"b""import numpy as np\nfrom PIL import Image, ImageDraw\n\n\ndef test():\n    with Image.open('demo/LB1xbbUGVXXXXaIXFXXXXXXXXXX.jpg') as im:\n        # draw on the origin img\n        draw = ImageDraw.Draw(im)\n        with open('demo/LB1xbbUGVXXXXaIXFXXXXXXXXXX.txt', 'r') as f:\n            anno_list = f.readlines()\n        for anno in anno_list:\n            anno_colums = anno.strip().split(',')\n            anno_array = np.array(anno_colums)\n            xy_list = np.reshape(anno_array.astype(float), (4, 2))\n            draw.line([tuple(xy_list[0]), tuple(xy_list[1]), tuple(xy_list[2]),\n                       tuple(xy_list[3]), tuple(xy_list[0])],\n                      width=1,\n                      fill='red')\n        im.save('demo/LB1xbbUGVXXXXaIXFXXXXXXXXXX_anno.jpg')\n\n\ntest()\n"""
tianchi_submit.py,0,"b""import os\n\nfrom tqdm import tqdm\nfrom network import East\nfrom predict import predict_txt\nimport cfg\n\n\nif __name__ == '__main__':\n    east = East()\n    east_detect = east.east_network()\n    east_detect.load_weights(cfg.saved_model_weights_file_path)\n\n    image_test_dir = os.path.join(cfg.data_dir, 'icpr_mtwi_task3/image_test/')\n    txt_test_dir = os.path.join(cfg.data_dir, 'icpr_mtwi_task3/txt_test/')\n    test_imgname_list = os.listdir(image_test_dir)\n    print('found %d test images.' % len(test_imgname_list))\n    for test_img_name, _ in zip(test_imgname_list,\n                                tqdm(range(len(test_imgname_list)))):\n        img_path = os.path.join(image_test_dir, test_img_name)\n        txt_path = os.path.join(txt_test_dir, test_img_name[:-4] + '.txt')\n        predict_txt(east_detect, img_path, txt_path, cfg.pixel_threshold, True)\n"""
