file_path,api_count,code
0.percepton/autoencoder.py,0,"b'#/usr/bin/python\n#coding:utf-8\n\nimport pandas as pd\nimport numpy as np\nimport random\n\n#\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe6\x95\xb0\xe6\x8d\xae\ndef reader(filename):\n    data = []\n    f = open(filename,\'r\')\n    for line in f.readlines():\n        row = line.strip(\'\\n\').split(\'\\t\')\n        data.append([float(i) for i in row])\n    data = np.array(data)\n    f.close()\n    return data\n\n# \xe7\x94\x9f\xe6\x88\x90batch\xe6\x95\xb0\xe6\x8d\xae\ndef batch(data,batch_size):\n    x = []\n    for i in data:\n        x.append(i)\n        if len(x) == batch_size:\n            yield x\n            x = []\n    if x:\n        yield x\n\n# \xe7\x94\x9f\xe6\x88\x90\xe5\x86\x85\xe7\xbd\xaebatch\xe6\x95\xb0\xe6\x8d\xae\ndef input(data,batch_size):\n    #count = len(data)/batch_size\n    #input_data = np.array(list(batch(data,batch_size)))[random.randint(0,count)]\n    input_data = np.array(list(batch(data,batch_size)))\n    return input_data\n\n# \xe7\x94\x9f\xe6\x88\x90\xe6\x96\x87\xe4\xbb\xb6batch\xe6\x95\xb0\xe6\x8d\xae\ndef file_input(filename,batch_size):\n    data = reader(filename)\n    input_data = input(data,batch_size)\n    return input_data\n\n\nclass AutoEncoder():\n    \'\'\'\n    Auto Encoder  \n    layer      1     2    ...    ...    L-1    L\n      W        0     1    ...    ...    L-2\n      B        0     1    ...    ...    L-2\n      Z              0     1     ...    L-3    L-2\n      A              0     1     ...    L-3    L-2\n    \'\'\'\n    def __init__(self, X, Y, nNodes):\n        # training samples\n        self.X = X\n        self.Y = Y\n        # number of samples\n        self.M = len(self.X)\n        # layers of networks\n        self.nLayers = len(nNodes)\n        # nodes at layers\n        self.nNodes = nNodes\n        # parameters of networks\n        self.W = list()\n        self.B = list()\n        self.dW = list()\n        self.dB = list()\n        self.A = list()\n        self.Z = list()\n        self.delta = list()\n        for iLayer in range(self.nLayers - 1):\n            self.W.append( np.random.rand(nNodes[iLayer]*nNodes[iLayer+1]).reshape(nNodes[iLayer],nNodes[iLayer+1]) ) \n            self.B.append( np.random.rand(nNodes[iLayer+1]) )\n            self.dW.append( np.zeros([nNodes[iLayer], nNodes[iLayer+1]]) )\n            self.dB.append( np.zeros(nNodes[iLayer+1]) )\n            self.A.append( np.zeros(nNodes[iLayer+1]) )\n            self.Z.append( np.zeros(nNodes[iLayer+1]) )\n            self.delta.append( np.zeros(nNodes[iLayer+1]) )\n            \n        # value of cost function\n        self.Jw = 0.0\n        # active function (logistic function)\n        self.sigmod = lambda z: 1.0 / (1.0 + np.exp(-z))\n        # learning rate 1.2\n        self.alpha = 2.5\n        # steps of iteration 30000\n        self.steps = 100\n        \n    def BackPropAlgorithm(self):\n        # \xe5\xae\x9a\xe4\xb9\x89loss\xe6\x96\xb9\xe5\xbc\x8f\n        self.Jw -= self.Jw\n        for iLayer in range(self.nLayers-1):\n            self.dW[iLayer] -= self.dW[iLayer]\n            self.dB[iLayer] -= self.dB[iLayer]\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x92\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad \n        for i in range(self.M):\n            # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n            for iLayer in range(self.nLayers - 1):\n                # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\n                if iLayer==0: \n                    self.Z[iLayer] = np.dot(self.X[i], self.W[iLayer])\n                else:\n                    self.Z[iLayer] = np.dot(self.A[iLayer-1], self.W[iLayer])\n                self.A[iLayer] = self.sigmod(self.Z[iLayer] + self.B[iLayer])            \n            # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n            for iLayer in range(self.nLayers - 1)[::-1]: # reserve\n                if iLayer==self.nLayers-2:# \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\n                    self.delta[iLayer] = -(self.X[i] - self.A[iLayer]) * (self.A[iLayer]*(1-self.A[iLayer]))#sigmoid\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\n                    self.Jw += np.dot(self.Y[i] - self.A[iLayer], self.Y[i] - self.A[iLayer])/self.M\n                else:\n                    self.delta[iLayer] = np.dot(self.W[iLayer].T, self.delta[iLayer+1]) * (self.A[iLayer]*(1-self.A[iLayer]))\n                # \xe8\xae\xa1\xe7\xae\x97\xe6\x9d\x83\xe5\x80\xbc\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\xe7\x9a\x84\xe5\x81\x8f\xe5\xaf\xbc\n                if iLayer==0:\n                    self.dW[iLayer] += self.X[i][:, np.newaxis] * self.delta[iLayer][:, np.newaxis].T\n                else:\n                    self.dW[iLayer] += self.A[iLayer-1][:, np.newaxis] * self.delta[iLayer][:, np.newaxis].T\n                self.dB[iLayer] += self.delta[iLayer] \n        # \xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\n        for iLayer in range(self.nLayers-1):\n            self.W[iLayer] -= (self.alpha/self.M)*self.dW[iLayer]\n            self.B[iLayer] -= (self.alpha/self.M)*self.dB[iLayer]\n\n        return self.W,self.B\n\n    \n    # AutoEncoder\xe8\xae\xa1\xe7\xae\x97\xe8\xbf\x87\xe7\xa8\x8b\xef\xbc\x8c\xe6\x89\x93\xe5\x8d\xb0loss\xe5\x80\xbc\n    def PlainAutoEncoder(self):\n        for i in range(self.steps):\n            self.BackPropAlgorithm()\n            print ""step:%d"" % i, ""loss=%f"" % self.Jw\n\n    # \xe8\xbe\x93\xe5\x87\xba\xe6\xaf\x8f\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xbf\xa1\xe6\x81\xaf\n    def ValidateAutoEncoder(self):\n        for i in range(self.M):\n            print self.X[i]\n            for iLayer in range(self.nLayers - 1):\n                if iLayer==0: \n                    self.Z[iLayer] = np.dot(self.X[i], self.W[iLayer])\n                else:\n                    self.Z[iLayer] = np.dot(self.A[iLayer-1], self.W[iLayer])\n                self.A[iLayer] = self.sigmod(self.Z[iLayer] + self.B[iLayer])\n                print ""\\t layer=%d"" % iLayer, self.A[iLayer]\n\ndef batch_train(data,batch_size):\n    input_data = input(data,batch_size)\n    count  = len(data)/batch_size\n    nNodes =  np.array([ 200, 10, 200])\n    for i in range(count):\n        x = input_data[i]\n        attr = AutoEncoder(data,data,nNodes)\n        print ""\xe7\xac\xac%d\xe4\xb8\xaabatch"" %i, attr.PlainAutoEncoder()\n\ndef main(filename):\n    file = reader(filename)\n    data = batch_reader(file)\n    ae2 = AutoEncoder(data,data,nNodes)\n    ae2.PlainAutoEncoder()\n    ae2.ValidateAutoEncoder()\n\nif __name__==\'__main__\':\n    # \xe5\x86\x85\xe7\xbd\xae\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84array\xe6\x95\xb0\xe6\x8d\xae\n    raw_data = np.random.random(size=(1000,200))\n    # data = input(raw_data,100)\n    # \xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe6\x95\xb0\xe6\x8d\xae\n    # data = file_input(\'data.txt\',100)\n    batch_train(raw_data,100)\n    nNodes =  np.array([ 200, 10, 200])\n    # ae2 = AutoEncoder(data,data,nNodes)\n    # ae2.PlainAutoEncoder()\n    # ae2.ValidateAutoEncoder()    \n\n    # \xe5\xa4\x96\xe9\x83\xa8\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\x84\xe7\x90\x86\n    #main(sys.argv[1])\n    #data_1 = reader(data)\n    #data_1 = np.random.random(size=(100000,2000))\n\n\n'"
0.percepton/backpropagation.py,0,"b'#coding:utf-8\n\'\'\'\nCopyright by huxioaman 2017.11.16\nbackpropagation.py: a simple 3-layer network including forward and backward process\n\'\'\'\n\nimport random\nimport math\n\n\n#   \xe5\x8f\x82\xe6\x95\xb0\xe8\xa7\xa3\xe9\x87\x8a\xef\xbc\x9a\n#   ""pd_"" \xef\xbc\x9a\xe5\x81\x8f\xe5\xaf\xbc\xe7\x9a\x84\xe5\x89\x8d\xe7\xbc\x80\n#   ""d_"" \xef\xbc\x9a\xe5\xaf\xbc\xe6\x95\xb0\xe7\x9a\x84\xe5\x89\x8d\xe7\xbc\x80\n#   ""w_ho"" \xef\xbc\x9a\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe7\xb3\xbb\xe6\x95\xb0\xe7\xb4\xa2\xe5\xbc\x95\n#   ""w_ih"" \xef\xbc\x9a\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe5\x88\xb0\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe7\xb3\xbb\xe6\x95\xb0\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\n\nclass NeuralNetwork:\n    LEARNING_RATE = 0.5\n\n    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n        self.num_inputs = num_inputs\n\n        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n\n        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n\n    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n        weight_num = 0\n        for h in range(len(self.hidden_layer.neurons)):\n            for i in range(self.num_inputs):\n                if not hidden_layer_weights:\n                    self.hidden_layer.neurons[h].weights.append(random.random())\n                else:\n                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n                weight_num += 1\n\n    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n        weight_num = 0\n        for o in range(len(self.output_layer.neurons)):\n            for h in range(len(self.hidden_layer.neurons)):\n                if not output_layer_weights:\n                    self.output_layer.neurons[o].weights.append(random.random())\n                else:\n                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n                weight_num += 1\n\n    def inspect(self):\n        print(\'------\')\n        print(\'* Inputs: {}\'.format(self.num_inputs))\n        print(\'------\')\n        print(\'Hidden Layer\')\n        self.hidden_layer.inspect()\n        print(\'------\')\n        print(\'* Output Layer\')\n        self.output_layer.inspect()\n        print(\'------\')\n\n    def feed_forward(self, inputs):\n        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n        return self.output_layer.feed_forward(hidden_layer_outputs)\n\n    def train(self, training_inputs, training_outputs):\n        self.feed_forward(training_inputs)\n\n        # 1. \xe8\xbe\x93\xe5\x87\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe5\x80\xbc\n        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n        for o in range(len(self.output_layer.neurons)):\n\n            # \xe2\x88\x82E/\xe2\x88\x82z\xe2\xb1\xbc\n            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n\n        # 2. \xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe5\x80\xbc\n        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n        for h in range(len(self.hidden_layer.neurons)):\n\n            # dE/dy\xe2\xb1\xbc = \xce\xa3 \xe2\x88\x82E/\xe2\x88\x82z\xe2\xb1\xbc * \xe2\x88\x82z/\xe2\x88\x82y\xe2\xb1\xbc = \xce\xa3 \xe2\x88\x82E/\xe2\x88\x82z\xe2\xb1\xbc * w\xe1\xb5\xa2\xe2\xb1\xbc\n            d_error_wrt_hidden_neuron_output = 0\n            for o in range(len(self.output_layer.neurons)):\n                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n\n            # \xe2\x88\x82E/\xe2\x88\x82z\xe2\xb1\xbc = dE/dy\xe2\xb1\xbc * \xe2\x88\x82z\xe2\xb1\xbc/\xe2\x88\x82\n            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n\n        # 3. \xe6\x9b\xb4\xe6\x96\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\xe7\xb3\xbb\xe6\x95\xb0\n        for o in range(len(self.output_layer.neurons)):\n            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n\n                # \xe2\x88\x82E\xe2\xb1\xbc/\xe2\x88\x82w\xe1\xb5\xa2\xe2\xb1\xbc = \xe2\x88\x82E/\xe2\x88\x82z\xe2\xb1\xbc * \xe2\x88\x82z\xe2\xb1\xbc/\xe2\x88\x82w\xe1\xb5\xa2\xe2\xb1\xbc\n                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n\n                # \xce\x94w = \xce\xb1 * \xe2\x88\x82E\xe2\xb1\xbc/\xe2\x88\x82w\xe1\xb5\xa2\n                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n\n        # 4. \xe6\x9b\xb4\xe6\x96\xb0\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe7\xb3\xbb\xe6\x95\xb0\n        for h in range(len(self.hidden_layer.neurons)):\n            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n\n                # \xe2\x88\x82E\xe2\xb1\xbc/\xe2\x88\x82w\xe1\xb5\xa2 = \xe2\x88\x82E/\xe2\x88\x82z\xe2\xb1\xbc * \xe2\x88\x82z\xe2\xb1\xbc/\xe2\x88\x82w\xe1\xb5\xa2\n                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n\n                # \xce\x94w = \xce\xb1 * \xe2\x88\x82E\xe2\xb1\xbc/\xe2\x88\x82w\xe1\xb5\xa2\n                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n\n    def calculate_total_error(self, training_sets):\n        total_error = 0\n        for t in range(len(training_sets)):\n            training_inputs, training_outputs = training_sets[t]\n            self.feed_forward(training_inputs)\n            for o in range(len(training_outputs)):\n                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n        return total_error\n\nclass NeuronLayer:\n    def __init__(self, num_neurons, bias):\n\n        # \xe5\x90\x8c\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe5\x85\xb1\xe4\xba\xab\xe4\xb8\x80\xe4\xb8\xaa\xe6\x88\xaa\xe8\xb7\x9d\xe9\xa1\xb9b\n        self.bias = bias if bias else random.random()\n\n        self.neurons = []\n        for i in range(num_neurons):\n            self.neurons.append(Neuron(self.bias))\n\n    def inspect(self):\n        print(\'Neurons:\', len(self.neurons))\n        for n in range(len(self.neurons)):\n            print(\' Neuron\', n)\n            for w in range(len(self.neurons[n].weights)):\n                print(\'  Weight:\', self.neurons[n].weights[w])\n            print(\'  Bias:\', self.bias)\n\n    def feed_forward(self, inputs):\n        outputs = []\n        for neuron in self.neurons:\n            outputs.append(neuron.calculate_output(inputs))\n        return outputs\n\n    def get_outputs(self):\n        outputs = []\n        for neuron in self.neurons:\n            outputs.append(neuron.output)\n        return outputs\n\nclass Neuron:\n    def __init__(self, bias):\n        self.bias = bias\n        self.weights = []\n\n    def calculate_output(self, inputs):\n        self.inputs = inputs\n        self.output = self.squash(self.calculate_total_net_input())\n        return self.output\n\n    def calculate_total_net_input(self):\n        total = 0\n        for i in range(len(self.inputs)):\n            total += self.inputs[i] * self.weights[i]\n        return total + self.bias\n\n    # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0sigmoid\n    def squash(self, total_net_input):\n        return 1 / (1 + math.exp(-total_net_input))\n\n\n    def calculate_pd_error_wrt_total_net_input(self, target_output):\n        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n\n    # \xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe6\x98\xaf\xe7\x94\xb1\xe5\xb9\xb3\xe6\x96\xb9\xe5\xb7\xae\xe5\x85\xac\xe5\xbc\x8f\xe8\xae\xa1\xe7\xae\x97\xe7\x9a\x84\n    def calculate_error(self, target_output):\n        return 0.5 * (target_output - self.output) ** 2\n\n    \n    def calculate_pd_error_wrt_output(self, target_output):\n        return -(target_output - self.output)\n\n    # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xe5\x85\xac\xe5\xbc\x8f    \n    def calculate_pd_total_net_input_wrt_input(self):\n        return self.output * (1 - self.output)\n\n\n    def calculate_pd_total_net_input_wrt_weight(self, index):\n        return self.inputs[index]\n\n\n# \xe6\x96\x87\xe4\xb8\xad\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90:\n\nnn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\nfor i in range(10000):\n    nn.train([0.05, 0.1], [0.01, 0.09])\n    print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.09]]]), 9))\n\n\n#\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8a\x8a\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe6\xb3\xa8\xe9\x87\x8a\xe6\x8e\x89\xe5\x86\x8d\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\x8b:\n\n# training_sets = [\n#     [[0, 0], [0]],\n#     [[0, 1], [1]],\n#     [[1, 0], [1]],\n#     [[1, 1], [0]]\n# ]\n\n# nn = NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1]))\n# for i in range(10000):\n#     training_inputs, training_outputs = random.choice(training_sets)\n#     nn.train(training_inputs, training_outputs)\n#     print(i, nn.calculate_total_error(training_sets))\n'"
1.mnist/activators.py,0,"b""#codinf:utf-8\n'''\nCopyright by huxiaoman 2017.11.21\nactivators.py: some common activation functions, relu, sigmoid, tanh included\n'''\n\nimport numpy as np\n\n\nclass ReluActivator(object):\n    def forward(self, weighted_input):\n        #return weighted_input\n        return max(0, weighted_input)\n\n    def backward(self, output):\n        return 1 if output > 0 else 0\n\n\nclass IdentityActivator(object):\n    def forward(self, weighted_input):\n        return weighted_input\n\n    def backward(self, output):\n        return 1\n\n\nclass SigmoidActivator(object):\n    def forward(self, weighted_input):\n        return 1.0 / (1.0 + np.exp(-weighted_input))\n    #the partial of sigmoid\n    def backward(self, output):\n        return output * (1 - output)\n\n\nclass TanhActivator(object):\n    def forward(self, weighted_input):\n        return 2.0 / (1.0 + np.exp(-2 * weighted_input)) - 1.0\n\n    def backward(self, output):\n        return 1 - output * output\n"""
1.mnist/cnn.py,0,"b'# -*- coding: UTF-8 -*-\n\'\'\'\nCreated by huxiaoman 2017.11.22\n\'\'\'\nimport numpy as np\nfrom activators import ReluActivator, IdentityActivator\n\n\n# \xe8\x8e\xb7\xe5\x8f\x96\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8c\xba\xe5\x9f\x9f\ndef get_patch(input_array, i, j, filter_width,\n              filter_height, stride):\n    \'\'\'\n    \xe4\xbb\x8e\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe7\xbb\x84\xe4\xb8\xad\xe8\x8e\xb7\xe5\x8f\x96\xe6\x9c\xac\xe6\xac\xa1\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\xef\xbc\x8c\n    \xe8\x87\xaa\xe5\x8a\xa8\xe9\x80\x82\xe9\x85\x8d\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba2D\xe5\x92\x8c3D\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\n    \'\'\'\n    start_i = i * stride\n    start_j = j * stride\n    if input_array.ndim == 2:\n\tinput_array_conv = input_array[\n            start_i : start_i + filter_height,\n            start_j : start_j + filter_width]\n\treturn input_array_conv\n    elif input_array.ndim == 3:\n        input_array_conv = input_array[:,\n            start_i : start_i + filter_height,\n            start_j : start_j + filter_width]\n\treturn input_array_conv\n       \n\n# \xe8\x8e\xb7\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaa2D\xe5\x8c\xba\xe5\x9f\x9f\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\ndef get_max_index(array):\n    max_i = 0\n    max_j = 0\n    max_value = array[0,0]\n    for i in range(array.shape[0]):\n        for j in range(array.shape[1]):\n            if array[i,j] > max_value:\n                max_value = array[i,j]\n                max_i, max_j = i, j\n    return max_i, max_j\n\n\n# \xe8\xae\xa1\xe7\xae\x97\xe5\x8d\xb7\xe7\xa7\xaf\ndef conv(input_array, \n         kernel_array,\n         output_array, \n         stride, bias):\n    \'\'\'\n    \xe8\xae\xa1\xe7\xae\x97\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe9\x80\x82\xe9\x85\x8d\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba2D\xe5\x92\x8c3D\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\n    \'\'\'\n    channel_number = input_array.ndim\n    output_width = output_array.shape[1]\n    output_height = output_array.shape[0]\n    kernel_width = kernel_array.shape[-1]\n    kernel_height = kernel_array.shape[-2]\n    for i in range(output_height):\n        for j in range(output_width):\n            output_array[i][j] = (    \n                get_patch(input_array, i, j, kernel_width, \n                    kernel_height, stride) * kernel_array\n                ).sum() + bias\n\n\n# \xe4\xb8\xba\xe6\x95\xb0\xe7\xbb\x84\xe5\xa2\x9e\xe5\x8a\xa0Zero padding\ndef padding(input_array, zp):\n    \'\'\'\n    \xe4\xb8\xba\xe6\x95\xb0\xe7\xbb\x84\xe5\xa2\x9e\xe5\x8a\xa0Zero padding\xef\xbc\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe9\x80\x82\xe9\x85\x8d\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba2D\xe5\x92\x8c3D\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\n    \'\'\'\n    if zp == 0:\n        return input_array\n    else:\n        if input_array.ndim == 3:\n            input_width = input_array.shape[2]\n            input_height = input_array.shape[1]\n            input_depth = input_array.shape[0]\n            padded_array = np.zeros((\n                input_depth, \n                input_height + 2 * zp,\n                input_width + 2 * zp))\n            padded_array[:,\n                zp : zp + input_height,\n                zp : zp + input_width] = input_array\n            return padded_array\n        elif input_array.ndim == 2:\n            input_width = input_array.shape[1]\n            input_height = input_array.shape[0]\n            padded_array = np.zeros((\n                input_height + 2 * zp,\n                input_width + 2 * zp))\n            padded_array[zp : zp + input_height,\n                zp : zp + input_width] = input_array\n            return padded_array\n\n\n# \xe5\xaf\xb9numpy\xe6\x95\xb0\xe7\xbb\x84\xe8\xbf\x9b\xe8\xa1\x8celement wise\xe6\x93\x8d\xe4\xbd\x9c\ndef element_wise_op(array, op):\n    for i in np.nditer(array,\n                       op_flags=[\'readwrite\']):\n        i[...] = op(i)\n\n\nclass Filter(object):\n    def __init__(self, width, height, depth):\n        self.weights = np.random.uniform(-1e-4, 1e-4,\n            (depth, height, width))\n        self.bias = 0\n        self.weights_grad = np.zeros(\n            self.weights.shape)\n        self.bias_grad = 0\n\n    def __repr__(self):\n        return \'filter weights:\\n%s\\nbias:\\n%s\' % (\n            repr(self.weights), repr(self.bias))\n\n    def get_weights(self):\n        return self.weights\n\n    def get_bias(self):\n        return self.bias\n\n    def update(self, learning_rate):\n        self.weights -= learning_rate * self.weights_grad\n        self.bias -= learning_rate * self.bias_grad\n\n\nclass ConvLayer(object):\n    def __init__(self, input_width, input_height, \n                 channel_number, filter_width, \n                 filter_height, filter_number, \n                 zero_padding, stride, activator,\n                 learning_rate):\n        self.input_width = input_width\n        self.input_height = input_height\n        self.channel_number = channel_number\n        self.filter_width = filter_width\n        self.filter_height = filter_height\n        self.filter_number = filter_number\n        self.zero_padding = zero_padding\n        self.stride = stride\n        self.output_width = \\\n            ConvLayer.calculate_output_size(\n            self.input_width, filter_width, zero_padding,\n            stride)\n        self.output_height = \\\n            ConvLayer.calculate_output_size(\n            self.input_height, filter_height, zero_padding,\n            stride)\n        self.output_array = np.zeros((self.filter_number, \n            self.output_height, self.output_width))\n        self.filters = []\n        for i in range(filter_number):\n            self.filters.append(Filter(filter_width, \n                filter_height, self.channel_number))\n        self.activator = activator\n        self.learning_rate = learning_rate\n\n    def forward(self, input_array):\n        \'\'\'\n        \xe8\xae\xa1\xe7\xae\x97\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n        \xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8self.output_array\n        \'\'\'\n        self.input_array = input_array\n        self.padded_input_array = padding(input_array,\n            self.zero_padding)\n        for f in range(self.filter_number):\n            filter = self.filters[f]\n            conv(self.padded_input_array, \n                filter.get_weights(), self.output_array[f],\n                self.stride, filter.get_bias())\n        element_wise_op(self.output_array, \n                        self.activator.forward)\n        \n    def backward(self, input_array, sensitivity_array, \n                 activator):\n        \'\'\'\n        \xe8\xae\xa1\xe7\xae\x97\xe4\xbc\xa0\xe9\x80\x92\xe7\xbb\x99\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe9\xa1\xb9\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x9d\x83\xe9\x87\x8d\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        \xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe9\xa1\xb9\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8self.delta_array\n        \xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8Filter\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84weights_grad\n        \'\'\'\n        self.forward(input_array)\n        self.bp_sensitivity_map(sensitivity_array,\n                                activator)\n        self.bp_gradient(sensitivity_array)\n\n    def update(self):\n        \'\'\'\n        \xe6\x8c\x89\xe7\x85\xa7\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\n        \'\'\'\n        for filter in self.filters:\n            filter.update(self.learning_rate)\n\n    def bp_sensitivity_map(self, sensitivity_array,\n                           activator):\n        \'\'\'\n        \xe8\xae\xa1\xe7\xae\x97\xe4\xbc\xa0\xe9\x80\x92\xe5\x88\xb0\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84sensitivity map\n        sensitivity_array: \xe6\x9c\xac\xe5\xb1\x82\xe7\x9a\x84sensitivity map\n        activator: \xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n        \'\'\'\n        # \xe5\xa4\x84\xe7\x90\x86\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xad\xa5\xe9\x95\xbf\xef\xbc\x8c\xe5\xaf\xb9\xe5\x8e\x9f\xe5\xa7\x8bsensitivity map\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x89\xa9\xe5\xb1\x95\n        expanded_array = self.expand_sensitivity_map(\n            sensitivity_array)\n        # full\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\xaf\xb9sensitivitiy map\xe8\xbf\x9b\xe8\xa1\x8czero padding\n        # \xe8\x99\xbd\xe7\x84\xb6\xe5\x8e\x9f\xe5\xa7\x8b\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84zero padding\xe5\x8d\x95\xe5\x85\x83\xe4\xb9\x9f\xe4\xbc\x9a\xe8\x8e\xb7\xe5\xbe\x97\xe6\xae\x8b\xe5\xb7\xae\n        # \xe4\xbd\x86\xe8\xbf\x99\xe4\xb8\xaa\xe6\xae\x8b\xe5\xb7\xae\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe7\xbb\xa7\xe7\xbb\xad\xe5\x90\x91\xe4\xb8\x8a\xe4\xbc\xa0\xe9\x80\x92\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\xb0\xb1\xe4\xb8\x8d\xe8\xae\xa1\xe7\xae\x97\xe4\xba\x86\n        expanded_width = expanded_array.shape[2]\n        zp = (self.input_width +  \n              self.filter_width - 1 - expanded_width) / 2\n        padded_array = padding(expanded_array, zp)\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96delta_array\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe4\xbf\x9d\xe5\xad\x98\xe4\xbc\xa0\xe9\x80\x92\xe5\x88\xb0\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\n        # sensitivity map\n        self.delta_array = self.create_delta_array()\n        # \xe5\xaf\xb9\xe4\xba\x8e\xe5\x85\xb7\xe6\x9c\x89\xe5\xa4\x9a\xe4\xb8\xaafilter\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8c\xe6\x9c\x80\xe7\xbb\x88\xe4\xbc\xa0\xe9\x80\x92\xe5\x88\xb0\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\n        # sensitivity map\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84filter\xe7\x9a\x84\n        # sensitivity map\xe4\xb9\x8b\xe5\x92\x8c\n        for f in range(self.filter_number):\n            filter = self.filters[f]\n            # \xe5\xb0\x86filter\xe6\x9d\x83\xe9\x87\x8d\xe7\xbf\xbb\xe8\xbd\xac180\xe5\xba\xa6\n            flipped_weights = np.array(map(\n                lambda i: np.rot90(i, 2), \n                filter.get_weights()))\n            # \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x8e\xe4\xb8\x80\xe4\xb8\xaafilter\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84delta_array\n            delta_array = self.create_delta_array()\n            for d in range(delta_array.shape[0]):\n                conv(padded_array[f], flipped_weights[d],\n                    delta_array[d], 1, 0)\n            self.delta_array += delta_array\n        # \xe5\xb0\x86\xe8\xae\xa1\xe7\xae\x97\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x8e\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x81\x8f\xe5\xaf\xbc\xe6\x95\xb0\xe5\x81\x9aelement-wise\xe4\xb9\x98\xe6\xb3\x95\xe6\x93\x8d\xe4\xbd\x9c\n        derivative_array = np.array(self.input_array)\n        element_wise_op(derivative_array, \n                        activator.backward)\n        self.delta_array *= derivative_array\n\n    def bp_gradient(self, sensitivity_array):\n        # \xe5\xa4\x84\xe7\x90\x86\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xad\xa5\xe9\x95\xbf\xef\xbc\x8c\xe5\xaf\xb9\xe5\x8e\x9f\xe5\xa7\x8bsensitivity map\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x89\xa9\xe5\xb1\x95\n        expanded_array = self.expand_sensitivity_map(\n            sensitivity_array)\n        for f in range(self.filter_number):\n            # \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x9d\x83\xe9\x87\x8d\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n            filter = self.filters[f]\n            for d in range(filter.weights.shape[0]):\n                conv(self.padded_input_array[d], \n                     expanded_array[f],\n                     filter.weights_grad[d], 1, 0)\n            # \xe8\xae\xa1\xe7\xae\x97\xe5\x81\x8f\xe7\xbd\xae\xe9\xa1\xb9\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n            filter.bias_grad = expanded_array[f].sum()\n\n    def expand_sensitivity_map(self, sensitivity_array):\n        depth = sensitivity_array.shape[0]\n        # \xe7\xa1\xae\xe5\xae\x9a\xe6\x89\xa9\xe5\xb1\x95\xe5\x90\x8esensitivity map\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n        # \xe8\xae\xa1\xe7\xae\x97stride\xe4\xb8\xba1\xe6\x97\xb6sensitivity map\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n        expanded_width = (self.input_width - \n            self.filter_width + 2 * self.zero_padding + 1)\n        expanded_height = (self.input_height - \n            self.filter_height + 2 * self.zero_padding + 1)\n        # \xe6\x9e\x84\xe5\xbb\xba\xe6\x96\xb0\xe7\x9a\x84sensitivity_map\n        expand_array = np.zeros((depth, expanded_height, \n                                 expanded_width))\n        # \xe4\xbb\x8e\xe5\x8e\x9f\xe5\xa7\x8bsensitivity map\xe6\x8b\xb7\xe8\xb4\x9d\xe8\xaf\xaf\xe5\xb7\xae\xe5\x80\xbc\n        for i in range(self.output_height):\n            for j in range(self.output_width):\n                i_pos = i * self.stride\n                j_pos = j * self.stride\n                expand_array[:,i_pos,j_pos] = \\\n                    sensitivity_array[:,i,j]\n        return expand_array\n\n    def create_delta_array(self):\n        return np.zeros((self.channel_number,\n            self.input_height, self.input_width))\n\n    @staticmethod\n    def calculate_output_size(input_size,\n            filter_size, zero_padding, stride):\n        return (input_size - filter_size + \n            2 * zero_padding) / stride + 1\n\n\nclass MaxPoolingLayer(object):\n    def __init__(self, input_width, input_height, \n                 channel_number, filter_width, \n                 filter_height, stride):\n        self.input_width = input_width\n        self.input_height = input_height\n        self.channel_number = channel_number\n        self.filter_width = filter_width\n        self.filter_height = filter_height\n        self.stride = stride\n        self.output_width = (input_width - \n            filter_width) / self.stride + 1\n        self.output_height = (input_height -\n            filter_height) / self.stride + 1\n        self.output_array = np.zeros((self.channel_number,\n            self.output_height, self.output_width))\n\n    def forward(self, input_array):\n        for d in range(self.channel_number):\n            for i in range(self.output_height):\n                for j in range(self.output_width):\n                    self.output_array[d,i,j] = (    \n                        get_patch(input_array[d], i, j,\n                            self.filter_width, \n                            self.filter_height, \n                            self.stride).max())\n\n    def backward(self, input_array, sensitivity_array):\n        self.delta_array = np.zeros(input_array.shape)\n        for d in range(self.channel_number):\n            for i in range(self.output_height):\n                for j in range(self.output_width):\n                    patch_array = get_patch(\n                        input_array[d], i, j,\n                        self.filter_width, \n                        self.filter_height, \n                        self.stride)\n                    k, l = get_max_index(patch_array)\n                    self.delta_array[d, \n                        i * self.stride + k, \n                        j * self.stride + l] = \\\n                        sensitivity_array[d,i,j]\n\n###\xe6\xb5\x8b\xe8\xaf\x95\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x92\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x90\x8e\xe8\xb7\x9f\xe6\x96\xb0\xe7\x9a\x84filter\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xaf\xe5\x90\xa6\xe6\xad\xa3\xe7\xa1\xae\ndef init_test():\n    a = np.array(\n        [[[0,1,1,0,2],\n          [2,2,2,2,1],\n          [1,0,0,2,0],\n          [0,1,1,0,0],\n          [1,2,0,0,2]],\n         [[1,0,2,2,0],\n          [0,0,0,2,0],\n          [1,2,1,2,1],\n          [1,0,0,0,0],\n          [1,2,1,1,1]],\n         [[2,1,2,0,0],\n          [1,0,0,1,0],\n          [0,2,1,0,1],\n          [0,1,2,2,2],\n          [2,1,0,0,1]]])\n    b = np.array(\n        [[[0,1,1],\n          [2,2,2],\n          [1,0,0]],\n         [[1,0,2],\n          [0,0,0],\n          [1,2,1]]])\n    cl = ConvLayer(5,5,3,3,3,2,1,2,IdentityActivator(),0.001)\n    cl.filters[0].weights = np.array(\n        [[[-1,1,0],\n          [0,1,0],\n          [0,1,1]],\n         [[-1,-1,0],\n          [0,0,0],\n          [0,-1,0]],\n         [[0,0,-1],\n          [0,1,0],\n          [1,-1,-1]]], dtype=np.float64)\n    cl.filters[0].bias=1\n    cl.filters[1].weights = np.array(\n        [[[1,1,-1],\n          [-1,-1,1],\n          [0,-1,1]],\n         [[0,1,0],\n         [-1,0,-1],\n          [-1,1,0]],\n         [[-1,0,0],\n          [-1,0,1],\n          [-1,0,0]]], dtype=np.float64)\n    return a, b, cl\n\n\ndef test():\n    a, b, cl = init_test()\n    cl.forward(a)\n    print ""\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xbb\x93\xe6\x9e\x9c:"", cl.output_array\n    cl.backward(a, b, IdentityActivator())\n    cl.update()\n    print ""\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x90\x8e\xe6\x9b\xb4\xe6\x96\xb0\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84filter1:"",cl.filters[0]\n    print ""\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x90\x8e\xe6\x9b\xb4\xe6\x96\xb0\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84filter2:"",cl.filters[1]\n\n\n\ndef gradient_check():\n    \'\'\'\n    \xe6\xa2\xaf\xe5\xba\xa6\xe6\xa3\x80\xe6\x9f\xa5\n    \'\'\'\n    # \xe8\xae\xbe\xe8\xae\xa1\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\xaf\xe5\xb7\xae\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89\xe8\x8a\x82\xe7\x82\xb9\xe8\xbe\x93\xe5\x87\xba\xe9\xa1\xb9\xe4\xb9\x8b\xe5\x92\x8c\n    error_function = lambda o: o.sum()\n    \n    # \xe8\xae\xa1\xe7\xae\x97forward\xe5\x80\xbc\n    a, b, cl = init_test()\n    cl.forward(a)\n    \n    # \xe6\xb1\x82\xe5\x8f\x96sensitivity map\n    sensitivity_array = np.ones(cl.output_array.shape,\n                                dtype=np.float64)\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n    cl.backward(a, sensitivity_array,\n                  IdentityActivator())\n    # \xe6\xa3\x80\xe6\x9f\xa5\xe6\xa2\xaf\xe5\xba\xa6\n    epsilon = 10e-4\n    for d in range(cl.filters[0].weights_grad.shape[0]):\n        for i in range(cl.filters[0].weights_grad.shape[1]):\n            for j in range(cl.filters[0].weights_grad.shape[2]):\n                cl.filters[0].weights[d,i,j] += epsilon\n                cl.forward(a)\n                err1 = error_function(cl.output_array)\n                cl.filters[0].weights[d,i,j] -= 2*epsilon\n                cl.forward(a)\n                err2 = error_function(cl.output_array)\n                expect_grad = (err1 - err2) / (2 * epsilon)\n                cl.filters[0].weights[d,i,j] += epsilon\n                print \'weights(%d,%d,%d): expected - actural %f - %f\' % (\n                    d, i, j, expect_grad, cl.filters[0].weights_grad[d,i,j])\n\n\ndef init_pool_test():\n    a = np.array(\n        [[[1,1,2,4],\n          [5,6,7,8],\n          [3,2,1,0],\n          [1,2,3,4]],\n         [[0,1,2,3],\n          [4,5,6,7],\n          [8,9,0,1],\n          [3,4,5,6]]], dtype=np.float64)\n\n    b = np.array(\n        [[[1,2],\n          [2,4]],\n         [[3,5],\n          [8,2]]], dtype=np.float64)\n\n    mpl = MaxPoolingLayer(4,4,2,2,2,2)\n\n    return a, b, mpl\n\n\ndef test_pool():\n    a, b, mpl = init_pool_test()\n    mpl.forward(a)\n    print \'input array:\\n%s\\noutput array:\\n%s\' % (a, mpl.output_array)\n    mpl.backward(a, b)\n    print \'input array:\\n%s\\nsensitivity array:\\n%s\\ndelta array:\\n%s\' % (a, b, mpl.delta_array)\n\n\ndef test_example():\n    a = np.array(\n        [[1,0,1,0],\n          [1,1,1,0],\n          [1,0,1,0],\n          [0,0,1,0]],\n          dtype=np.float64)\n    b = np.array(\n        [[[1,-1],\n          [1,-1]],\n         [[1,1],\n          [-1,-1]]], dtype=np.float64)\n\n\n\nif __name__ == ""__main__"":\n\ttest()\n'"
1.mnist/conv.py,0,"b""#coding:utf-8\n'''\nCreated by huxiaoman 2017.10.31\nUpdate by huxiaoman 2017.11.15\nCopyright huxiaoman\nconv.py:to implement a simple convolutional network,includig convolutional,padding,maxpolling,forwarf_propogation and backpropogation process.\n###\xe6\x9a\x82\xe6\x97\xb6\xe4\xb8\x8d\xe8\xa6\x81\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe7\xa8\x8b\xe5\xba\x8f\xef\xbc\x8c\xe5\x86\x99\xe7\x9a\x84\xe6\x9c\x89\xe7\x82\xb9\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x8d\xe7\xa1\xae\xe5\xae\x9a\xe6\x98\xaf\xe5\xaf\xb9\xe7\x9a\x84 = =\n'''\n\nimport numpy as np\nclass Conv:\n    '''\n    \xe5\x8f\x82\xe6\x95\xb0\xe5\x90\xab\xe4\xb9\x89\xef\xbc\x9a\n    c:channel,\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    w:width,\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xae\xbd\xe5\xba\xa6\n    h:height,\xe5\x9b\xbe\xe7\x89\x87\xe9\x95\xbf\xe5\xba\xa6\n    k_x:kernel_x,\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe9\x95\xbf\xe5\xba\xa6\n    k_y:kernel_y,\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xae\xbd\xe5\xba\xa6\n    s_x:stride_x,\xe6\xb0\xb4\xe5\xb9\xb3\xe6\xad\xa5\xe9\x95\xbf\xe9\x95\xbf\xe5\xba\xa6\n    s_y:stride_y,\xe5\x9e\x82\xe7\x9b\xb4\xe6\xad\xa5\xe9\x95\xbf\xe9\x95\xbf\xe5\xba\xa6\n    p_x:zero_padding_x,\xe6\xb0\xb4\xe5\xb9\xb3\xe8\xa1\xa5\xe9\x9b\xb6\xe9\x95\xbf\xe5\xba\xa6\n    p_y:zero_padding_y,\xe5\x9e\x82\xe7\x9b\xb4\xe8\xa1\xa5\xe9\x9b\xb6\xe9\x95\xbf\xe5\xba\xa6\n    f:feature,\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\xe7\x9b\xae\n    '''\n    def __init__(self, c, w, h, k_x, k_y, s_x, s_y, p_x, p_y, f):\n\tself.c, self.w, self.h = c, w, h\n\tself.k_x, self.k_y = k_x,k_y\n        self.s_x, self.s_y = s_x, s_y\n\tself.p_x, self.p_y = p_x, p_y\n\tself.f = f\n\t# \xe5\x88\xa4\xe6\x96\xad\xe6\xb0\xb4\xe5\xb9\xb3\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xe6\x95\xb4\xe6\x95\xb0\n\tassert ((w - k_x + 2 * p_x) % s_x ==0)\n\t# \xe5\x88\xa4\xe6\x96\xad\xe5\x9e\x82\xe7\x9b\xb4\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xe6\x95\xb4\xe6\x95\xb0\n\tassert ((h - k_y + 2 * p_y) % s_y ==0)\n\tself.w_num = (w - k_x + 2 * p_x) / s_x + 1\n\tself.h_num = (h - k_y + 2 * p_y) / s_y + 1\n\tself.weights = np.random.randn(f, c * k_x * k_y) / np.sqrt(c * k_x * k_y)\n\tself.bias = np.random.randn(f)\n\tself.lr,self.lamb = 0.0, 0.0\n\n    # zero padding\n    def padding(self, x):\n\tzero_num = x.shape\n\tzero = np.zeros((zero_num[0], zero_num[1], zero_num[2] + 2* self.p_x, zero_num[3] + 2* self.p_y))\n\tx = zero[:, :, self.p_x:self.p_x + self.w, self.p_y:self.p_y + self.h]\n\treturn zero\n\n    def eval(self,x):\n\t# \xe8\xa1\xa5\xe9\x9b\xb6\xe5\x90\x8e\xe7\x9a\x84\xe5\xae\xbd\xe5\xba\xa6ww\xe5\x92\x8c\xe9\xab\x98\xe5\xba\xa6hh\n\tww = self.h - self.k_x + 2 * self.p_x + 1\n\thh = self.h - self.k_y + 2 * self.p_y + 1\n\tret = np.array([[[np.ravel(xx[:, a:a + self.k_x, b:b + self.k_y]) for b in range(0, hh, self.s_y)]\n                         for a in range(0, ww, self.s_x)] for xx in x])\n\t#ret = np.array([[[np.ravel(xx[:,a:a +self.k_x, b:b, self.k_y]) for b in range(0,hh,self.s_y)] for a in range(0,ww,self.s_x)] for xx in x])# here using np.ravel rather than np.flatten to save memory\n\treturn ret\n    \n    def de_eval(self,x):\n\ta1, a2, a3, a4 = x.shape\n        x = x.reshape(a1, a2, a3, self.c, self.k_x, self.k_y)\n        ret = np.zeros_like(self.px)\n        for i in range(a1):\n            for j in range(a2):\n                w = j * self.s_x\n                for t in range(a3):\n                    h = t * self.s_y\n                    ret[i][:, w:w + self.k_x, h:h + self.k_y] += x[i][j][t]\n        return ret[:, :, self.p_x:self.p_x + self.w, self.p_y:self.p_y + self.h]\n\n\n    def forward(self,x):\n\tassert (x.shape[1] == self.c)\n\tassert (x.shape[2] == self.w)\n\tassert (x.shape[3] == self.h)\n\tself.px = self.padding(x)\n\tself.evalx = self.eval(self.px)\n\tself.y = (self.evalx.dot(self.weights.T) + self.bias).transpose(0,3,1,2)\n\tprint self. y\t\n\treturn self.y\n\n    def backward(self,b):\n        a1, a2, a3, a4 = b.shape\n\tself.ddx = np.array([dd.T.dot(self.weights) for dd in b])\n        self.dx = self.de_eval(self.ddx)\n        b = b.reshape(a1, a2, a3 * a4)\n        self.evalx = self.evalx.reshape(a1, a3 * a4, self.c * self.k_x * self.k_y)\n        self.dw = np.sum([dd.dot(x) for x, dd in zip(self.evalx, b)], axis=0) / a1 / a3 / a4\n        self.db = np.sum(b, axis=(0, 2)) / a1 / a3 / a4\n\n        self.weights -= self.lr * (self.dw + self.lamb * np.sum(np.square(self.weights)) / a1)\n        self.bias -= self.lr * self.db\n\treturn self.dx\n\nif __name__ == '__main__':\n\t# \xe5\xb8\xa6\xe5\x85\xa5\xe6\x95\xb0\xe5\x80\xbc\xe8\xae\xa1\xe7\xae\x97\n\ttest = Conv(1,5,5,1,1,1,1,3,3,32)\n\tdata = np.ones((6,1,5,5))\n\t# \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n\tx1 = test.forward(data)\n\t# \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n\tx2 = test.backward(x1)\n\tprint x1.shape\n\tprint x2.shape\n\n\t\n"""
1.mnist/mnist.py,0,"b'#coding:utf-8\n\'\'\'\nCreated by huxiaoman 2017.10.28\nCopyright huxiaoman \nmnist.py:using paddlepaddle framework to train a simple cnn network,and improve the baseline network,the accuracy of improved network is 99.28% \n\'\'\'\nimport os\nfrom PIL import Image\nimport numpy as np\nimport paddle.v2 as paddle\n\n# \xe8\xae\xbe\xe7\xbd\xae\xe6\x98\xaf\xe5\x90\xa6\xe7\x94\xa8gpu\xef\xbc\x8c0\xe4\xb8\xba\xe5\x90\xa6\xef\xbc\x8c1\xe4\xb8\xba\xe6\x98\xaf\nwith_gpu = os.getenv(\'WITH_GPU\', \'0\') != \'0\'\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\ndef convolutional_neural_network_org(img):\n    # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n    conv_pool_1 = paddle.networks.simple_img_conv_pool(\n        input=img,\n        filter_size=5,\n        num_filters=20,\n        num_channel=1,\n        pool_size=2,\n        pool_stride=2,\n        act=paddle.activation.Relu())\n    # \xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n    conv_pool_2 = paddle.networks.simple_img_conv_pool(\n        input=conv_pool_1,\n        filter_size=5,\n        num_filters=50,\n        num_channel=20,\n        pool_size=2,\n        pool_stride=2,\n        act=paddle.activation.Relu())\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    predict = paddle.layer.fc(\n        input=conv_pool_2, size=10, act=paddle.activation.Softmax())\n    return predict\n\n# \xe6\x94\xb9\xe8\xbf\x9b\xe7\x89\x88\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\ndef convolutional_neural_network(img):\n    # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n    conv_pool_1 = paddle.networks.simple_img_conv_pool(\n        input=img,\n        filter_size=5,\n        num_filters=20,\n        num_channel=1,\n        pool_size=2,\n        pool_stride=2,\n        act=paddle.activation.Relu())\n    # \xe5\x8a\xa0\xe4\xb8\x80\xe5\xb1\x82dropout\xe5\xb1\x82\n    drop_1 = paddle.layer.dropout(input=conv_pool_1, dropout_rate=0.2)\n    # \xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n    conv_pool_2 = paddle.networks.simple_img_conv_pool(\n        input=drop_1,\n        filter_size=5,\n        num_filters=50,\n        num_channel=20,\n        pool_size=2,\n        pool_stride=2,\n        act=paddle.activation.Relu())\n    # \xe5\x8a\xa0\xe4\xb8\x80\xe5\xb1\x82dropout\xe5\xb1\x82\n    drop_2 = paddle.layer.dropout(input=conv_pool_2, dropout_rate=0.5)\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    fc1 = paddle.layer.fc(input=drop_2, size=10, act=paddle.activation.Linear())\n    bn = paddle.layer.batch_norm(input=fc1,act=paddle.activation.Relu(),\n         layer_attr=paddle.attr.Extra(drop_rate=0.2))\n    predict = paddle.layer.fc(input=bn, size=10, act=paddle.activation.Softmax())\n    return predict\n\n\ndef main():\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xae\x9a\xe4\xb9\x89\xe8\xb7\x91\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xae\xbe\xe5\xa4\x87\n    paddle.init(use_gpu=with_gpu, trainer_count=1)\n\n    # \xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n    images = paddle.layer.data(\n        name=\'pixel\', type=paddle.data_type.dense_vector(784))\n    label = paddle.layer.data(\n        name=\'label\', type=paddle.data_type.integer_value(10))\n    print label\n\n    # \xe8\xb0\x83\xe7\x94\xa8\xe4\xb9\x8b\xe5\x89\x8d\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n    predict = convolutional_neural_network_org(images)#\xe5\x8e\x9f\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84    \n    # predict = convolutional_neural_network(images)\n\n    # \xe5\xae\x9a\xe4\xb9\x89\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n    cost = paddle.layer.classification_cost(input=predict, label=label)\n\n    # \xe6\x8c\x87\xe5\xae\x9a\xe8\xae\xad\xe7\xbb\x83\xe7\x9b\xb8\xe5\x85\xb3\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    parameters = paddle.parameters.create(cost)\n\n    # \xe5\xae\x9a\xe4\xb9\x89\xe8\xae\xad\xe7\xbb\x83\xe6\x96\xb9\xe6\xb3\x95\n    optimizer = paddle.optimizer.Momentum(\n        learning_rate=0.1 / 128.0,\n        momentum=0.9,\n        regularization=paddle.optimizer.L2Regularization(rate=0.0005 * 128))\n\n    # \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n    trainer = paddle.trainer.SGD(\n        cost=cost, parameters=parameters, update_equation=optimizer)\n\n\n    lists = []\n\n    # \xe5\xae\x9a\xe4\xb9\x89event_handler\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n    def event_handler(event):\n        if isinstance(event, paddle.event.EndIteration):\n            if event.batch_id % 100 == 0:\n                print ""Pass %d, Batch %d, Cost %f, %s"" % (\n                    event.pass_id, event.batch_id, event.cost, event.metrics)\n        if isinstance(event, paddle.event.EndPass):\n            # \xe4\xbf\x9d\xe5\xad\x98\xe5\x8f\x82\xe6\x95\xb0\n            with open(\'params_pass_%d.tar\' % event.pass_id, \'w\') as f:\n                parameters.to_tar(f)\n\n            result = trainer.test(reader=paddle.batch(\n                paddle.dataset.mnist.test(), batch_size=128))\n            print ""Test with Pass %d, Cost %f, %s\\n"" % (\n                event.pass_id, result.cost, result.metrics)\n            lists.append((event.pass_id, result.cost,\n                          result.metrics[\'classification_error_evaluator\']))\n\n    trainer.train(\n        reader=paddle.batch(\n            paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=8192),\n            batch_size=64),\n        event_handler=event_handler,\n        num_passes=50)\n\n    # \xe6\x89\xbe\xe5\x88\xb0\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\xaf\xe5\xb7\xae\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe4\xb8\x80\xe6\xac\xa1\xe7\xbb\x93\xe6\x9e\x9c\n    best = sorted(lists, key=lambda list: float(list[1]))[0]\n    print \'Best pass is %s, testing Avgcost is %s\' % (best[0], best[1])\n    print \'The classification accuracy is %.2f%%\' % (100 - float(best[2]) * 100)\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae   \n    def load_image(file):\n        im = Image.open(file).convert(\'L\')\n        im = im.resize((28, 28), Image.ANTIALIAS)\n        im = np.array(im).astype(np.float32).flatten()\n        im = im / 255.0\n        return im\n\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe7\xbb\x93\xe6\x9e\x9c\n    test_data = []\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    test_data.append((load_image(cur_dir + \'/image/infer_3.png\'), ))\n\n    probs = paddle.infer(\n        output_layer=predict, parameters=parameters, input=test_data)\n    lab = np.argsort(-probs)  # probs and lab are the results of one batch data\n    print ""Label of image/infer_3.png is: %d"" % lab[0][0]\n\n\nif __name__ == \'__main__\':\n    main()\n'"
2.data_processing/mnsit_data.py,0,"b'#coding:utf-8\n\'\'\'\nCreated by huxiaoman 2017.11.08\nmnist_data.py: extracing mnist dataset and create reader_creator to convert data format to train and test.\nmore can see in the paddlepaddle source code : https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/dataset/mnist.py\n\'\'\'\nimport paddle.v2.dataset.common\nimport subprocess\nimport numpy\nimport platform\n__all__ = [\'train\', \'test\', \'convert\']\n\nURL_PREFIX = \'http://yann.lecun.com/exdb/mnist/\'\nTEST_IMAGE_URL = URL_PREFIX + \'t10k-images-idx3-ubyte.gz\'\nTEST_IMAGE_MD5 = \'9fb629c4189551a2d022fa330f9573f3\'\nTEST_LABEL_URL = URL_PREFIX + \'t10k-labels-idx1-ubyte.gz\'\nTEST_LABEL_MD5 = \'ec29112dd5afa0611ce80d1b7f02629c\'\nTRAIN_IMAGE_URL = URL_PREFIX + \'train-images-idx3-ubyte.gz\'\nTRAIN_IMAGE_MD5 = \'f68b3c2dcbeaaa9fbdd348bbdeb94873\'\nTRAIN_LABEL_URL = URL_PREFIX + \'train-labels-idx1-ubyte.gz\'\nTRAIN_LABEL_MD5 = \'d53e105ee54ea40749a09fcbcd1e9432\'\n\ndef reader_creator(image_filename, label_filename, buffer_size):\n    # \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaareader\n    def reader():\n        if platform.system() == \'Darwin\':\n            zcat_cmd = \'gzcat\'\n        elif platform.system() == \'Linux\':\n            zcat_cmd = \'zcat\'\n        else:\n            raise NotImplementedError()\n\n        m = subprocess.Popen([zcat_cmd, image_filename], stdout=subprocess.PIPE)\n        m.stdout.read(16)  \n\n        l = subprocess.Popen([zcat_cmd, label_filename], stdout=subprocess.PIPE)\n        l.stdout.read(8)  \n\n        try:  # reader could be break.\n            while True:\n                labels = numpy.fromfile(\n                    l.stdout, \'ubyte\', count=buffer_size).astype(""int"")\n\n                if labels.size != buffer_size:\n                    break  # numpy.fromfile returns empty slice after EOF.\n\n                images = numpy.fromfile(\n                    m.stdout, \'ubyte\', count=buffer_size * 28 * 28).reshape(\n                        (buffer_size, 28 * 28)).astype(\'float32\')\n\n                images = images / 255.0 * 2.0 - 1.0\n\n                for i in xrange(buffer_size):\n                    yield images[i, :], int(labels[i])\n        finally:\n            m.terminate()\n            l.terminate()\n\n    return reader\n\ndef train():\n    """"""\n    \xe5\x88\x9b\xe5\xbb\xbamnsit\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86 reader creator\n    \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaareador creator\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaareader\xe9\x87\x8c\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe9\x83\xbd\xe6\x98\xaf\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe5\x80\xbc\xef\xbc\x8c\xe5\x9c\xa8\xe5\x8c\xba\xe9\x97\xb4[0,1]\xe5\x86\x85\xef\xbc\x8clabel\xe4\xb8\xba0~9\n    \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x9atraining reader creator\n    """"""\n    return reader_creator(\n        paddle.v2.dataset.common.download(TRAIN_IMAGE_URL, \'mnist\',\n                                          TRAIN_IMAGE_MD5),\n        paddle.v2.dataset.common.download(TRAIN_LABEL_URL, \'mnist\',\n                                          TRAIN_LABEL_MD5), 100)\n\n\ndef test():\n    """"""\n    \xe5\x88\x9b\xe5\xbb\xbamnsit\xe7\x9a\x84\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86 reader creator\n    \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaareador creator\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaareader\xe9\x87\x8c\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe9\x83\xbd\xe6\x98\xaf\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe5\x80\xbc\xef\xbc\x8c\xe5\x9c\xa8\xe5\x8c\xba\xe9\x97\xb4[0,1]\xe5\x86\x85\xef\xbc\x8clabel\xe4\xb8\xba0~9\n    \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x9atestreader creator\n    """"""\n    return reader_creator(\n        paddle.v2.dataset.common.download(TEST_IMAGE_URL, \'mnist\',\n                                          TEST_IMAGE_MD5),\n        paddle.v2.dataset.common.download(TEST_LABEL_URL, \'mnist\',\n                                          TEST_LABEL_MD5), 100)\n\ndef fetch():\n    paddle.v2.dataset.common.download(TRAIN_IMAGE_URL, \'mnist\', TRAIN_IMAGE_MD5)\n    paddle.v2.dataset.common.download(TRAIN_LABEL_URL, \'mnist\', TRAIN_LABEL_MD5)\n    paddle.v2.dataset.common.download(TEST_IMAGE_URL, \'mnist\', TEST_IMAGE_MD5)\n    paddle.v2.dataset.common.download(TEST_LABEL_URL, \'mnist\', TRAIN_LABEL_MD5)\n\n\ndef convert(path):\n    """"""\n    \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\xbc\xe5\xbc\x8f\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba recordio format\n    """"""\n    paddle.v2.dataset.common.convert(path, train(), 1000, ""minist_train"")\n    paddle.v2.dataset.common.convert(path, test(), 1000, ""minist_test"")\n\n\n'"
2.data_processing/sentiment_analyze.py,0,"b'#coding:utf-8\n\'\'\'\nCreated by huxiaoman 2017.11.15\nsentiment_analyze.py: create reader to convert text data to train and test\n\'\'\'\nimport os\n\ndef train_reader(data_dir, word_dict, label_dict):\n    """"""\n   \xe5\x88\x9b\xe5\xbb\xba\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xaereader\n    :param data_dir: \xe6\x95\xb0\xe6\x8d\xae\xe5\x9c\xb0\xe5\x9d\x80.\n    :type data_dir: str\n    :param word_dict: \xe8\xaf\x8d\xe5\x85\xb8\xe5\x9c\xb0\xe5\x9d\x80,\n        \xe8\xaf\x8d\xe5\x85\xb8\xe9\x87\x8c\xe5\xbf\x85\xe9\xa1\xbb\xe6\x9c\x89 ""UNK"" .\n    :type word_dict:python dict\n    :param label_dict: label \xe5\xad\x97\xe5\x85\xb8\xe7\x9a\x84\xe5\x9c\xb0\xe5\x9d\x80\n    :type label_dict: Python dict\n    """"""\n\n    def reader():\n        UNK_ID = word_dict[""<UNK>""]\n        word_col = 1\n        lbl_col = 0\n\n        for file_name in os.listdir(data_dir):\n            with open(os.path.join(data_dir, file_name), ""r"") as f:\n                for line in f:\n                    line_split = line.strip().split(""\\t"")\n                    word_ids = [\n                        word_dict.get(w, UNK_ID)\n                        for w in line_split[word_col].split()\n                    ]\n                    yield word_ids, label_dict[line_split[lbl_col]]\n\n    return reader\n\n\ndef test_reader(data_dir, word_dict):\n    """"""\n    \xe5\x88\x9b\xe5\xbb\xba\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xaereader\n    :param data_dir: \xe6\x95\xb0\xe6\x8d\xae\xe5\x9c\xb0\xe5\x9d\x80.\n    :type data_dir: str\n    :param word_dict: \xe8\xaf\x8d\xe5\x85\xb8\xe5\x9c\xb0\xe5\x9d\x80,\n        \xe8\xaf\x8d\xe5\x85\xb8\xe9\x87\x8c\xe5\xbf\x85\xe9\xa1\xbb\xe6\x9c\x89 ""UNK"" .\n    :type word_dict:python dict\n    """"""\n\n    def reader():\n        UNK_ID = word_dict[""<UNK>""]\n        word_col = 1\n\n        for file_name in os.listdir(data_dir):\n            with open(os.path.join(data_dir, file_name), ""r"") as f:\n                for line in f:\n                    line_split = line.strip().split(""\\t"")\n                    if len(line_split) < word_col: continue\n                    word_ids = [\n                        word_dict.get(w, UNK_ID)\n                        for w in line_split[word_col].split()\n                    ]\n                    yield word_ids, line_split[word_col]\n\n    return reader\n'"
3.image_classification/alexnet_tf.py,50,"b'#coding:utf-8\n\'\'\'\nCreated by huxiaoman 2017.12.05\nalexnet_tf.py:\xe8\xae\xad\xe7\xbb\x83tensorflo\xe7\x89\x88\xe7\x9a\x84alexnet\xe7\xbd\x91\xe7\xbb\x9c\n\'\'\'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nfrom datetime import datetime\nimport math\nimport sys\nimport time\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nFLAGS = None\n\n\ndef print_activations(t):\n  print(t.op.name, \' \', t.get_shape().as_list())\n\n\ndef inference(images):\n  \'\'\'\n  Alexnet\xe6\xa8\xa1\xe5\x9e\x8b\n  \xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x9aimages\xe7\x9a\x84tensor\n  \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x9aAlexnet\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n\n  \'\'\'\n  parameters = []\n  # conv1\n  with tf.name_scope(\'conv1\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([11, 11, 3, 64], dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(images, kernel, [1, 4, 4, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(bias, name=scope)\n    print_activations(conv1)\n    parameters += [kernel, biases]\n\n  # lrn1\n  with tf.name_scope(\'lrn1\') as scope:\n    lrn1 = tf.nn.local_response_normalization(conv1,\n                                              alpha=1e-4,\n                                              beta=0.75,\n                                              depth_radius=2,\n                                              bias=2.0)\n\n  # pool1\n  pool1 = tf.nn.max_pool(lrn1,\n                         ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1],\n                         padding=\'VALID\',\n                         name=\'pool1\')\n  print_activations(pool1)\n\n  # conv2\n  with tf.name_scope(\'conv2\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([5, 5, 64, 192], dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[192], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(bias, name=scope)\n    parameters += [kernel, biases]\n  print_activations(conv2)\n\n  # lrn2\n  with tf.name_scope(\'lrn2\') as scope:\n    lrn2 = tf.nn.local_response_normalization(conv2,\n                                              alpha=1e-4,\n                                              beta=0.75,\n                                              depth_radius=2,\n                                              bias=2.0)\n\n  # pool2\n  pool2 = tf.nn.max_pool(lrn2,\n                         ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1],\n                         padding=\'VALID\',\n                         name=\'pool2\')\n  print_activations(pool2)\n\n  # conv3\n  with tf.name_scope(\'conv3\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 192, 384],\n                                             dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv3 = tf.nn.relu(bias, name=scope)\n    parameters += [kernel, biases]\n    print_activations(conv3)\n\n  # conv4\n  with tf.name_scope(\'conv4\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n                                             dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv4 = tf.nn.relu(bias, name=scope)\n    parameters += [kernel, biases]\n    print_activations(conv4)\n\n  # conv5\n  with tf.name_scope(\'conv5\') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256],\n                                             dtype=tf.float32,\n                                             stddev=1e-1), name=\'weights\')\n    conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name=\'biases\')\n    bias = tf.nn.bias_add(conv, biases)\n    conv5 = tf.nn.relu(bias, name=scope)\n    parameters += [kernel, biases]\n    print_activations(conv5)\n\n  # pool5\n  pool5 = tf.nn.max_pool(conv5,\n                         ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1],\n                         padding=\'VALID\',\n                         name=\'pool5\')\n  print_activations(pool5)\n\n  return pool5, parameters\n\n\ndef time_tensorflow_run(session, target, info_string):\n  num_steps_burn_in = 10\n  total_duration = 0.0\n  total_duration_squared = 0.0\n  for i in xrange(FLAGS.num_batches + num_steps_burn_in):\n    start_time = time.time()\n    _ = session.run(target)\n    duration = time.time() - start_time\n    if i >= num_steps_burn_in:\n      if not i % 10:\n        print (\'%s: step %d, duration = %.3f\' %\n               (datetime.now(), i - num_steps_burn_in, duration))\n      total_duration += duration\n      total_duration_squared += duration * duration\n  mn = total_duration / FLAGS.num_batches\n  vr = total_duration_squared / FLAGS.num_batches - mn * mn\n  sd = math.sqrt(vr)\n  print (\'%s: %s across %d steps, %.3f +/- %.3f sec / batch\' %\n         (datetime.now(), info_string, FLAGS.num_batches, mn, sd))\n\n\n\ndef run_benchmark():\n  """"""Run the benchmark on AlexNet.""""""\n  with tf.Graph().as_default():\n    image_size = 224\n    images = tf.Variable(tf.random_normal([FLAGS.batch_size,\n                                           image_size,\n                                           image_size, 3],\n                                          dtype=tf.float32,\n                                          stddev=1e-1))\n\n    pool5, parameters = inference(images)\n\n    init = tf.global_variables_initializer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allocator_type = \'BFC\'\n    sess = tf.Session(config=config)\n    sess.run(init)\n\n    time_tensorflow_run(sess, pool5, ""Forward"")\n\n    objective = tf.nn.l2_loss(pool5)\n    grad = tf.gradients(objective, parameters)\n    time_tensorflow_run(sess, grad, ""Forward-backward"")\n\n\ndef main(_):\n  run_benchmark()\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--batch_size\',\n      type=int,\n      default=128,\n      help=\'Batch size.\'\n  )\n  parser.add_argument(\n      \'--num_batches\',\n      type=int,\n      default=200,\n      help=\'Number of batches to run.\'\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
3.image_classification/alextnet.py,0,"b""#coding:utf-8\n'''\nCreated by huxiaoman 2017.12.5\nalexnet.py:alexnet\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n'''\n\nimport paddle.v2 as paddle\nimport os\n\n\nwith_gpu = os.getenv('WITH_GPU', '0') != '1'\n\ndef alexnet_lrn(img):\n    conv1 = paddle.layer.img_conv(\n        input=img,\n        filter_size=11,\n        num_channels=3,\n        num_filters=96,\n        stride=4,\n        padding=1)\n    cmrnorm1 = paddle.layer.img_cmrnorm(\n        input=conv1, size=5, scale=0.0001, power=0.75)\n    pool1 = paddle.layer.img_pool(input=cmrnorm1, pool_size=3, stride=2)\n\n    conv2 = paddle.layer.img_conv(\n        input=pool1,\n        filter_size=5,\n        num_filters=256,\n        stride=1,\n        padding=2,\n        groups=1)\n    cmrnorm2 = paddle.layer.img_cmrnorm(\n        input=conv2, size=5, scale=0.0001, power=0.75)\n    pool2 = paddle.layer.img_pool(input=cmrnorm2, pool_size=3, stride=2)\n\n    pool3 = paddle.networks.img_conv_group(\n        input=pool2,\n        pool_size=3,\n        pool_stride=2,\n        conv_num_filter=[384, 384, 256],\n        conv_filter_size=3,\n        pool_type=paddle.pooling.Max())\n\n    fc1 = paddle.layer.fc(\n        input=pool3,\n        size=4096,\n        act=paddle.activation.Relu(),\n        layer_attr=paddle.attr.Extra(drop_rate=0.5))\n    fc2 = paddle.layer.fc(\n        input=fc1,\n        size=4096,\n        act=paddle.activation.Relu(),\n        layer_attr=paddle.attr.Extra(drop_rate=0.5))\n    return fc2\n\ndef alexnet(img):\n    conv1 = paddle.layer.img_conv(\n        input=img,\n        filter_size=11,\n        num_channels=3,\n        num_filters=96,\n        stride=4,\n        padding=1)\n    cmrnorm1 = paddle.layer.img_cmrnorm(\n        input=conv1, size=5, scale=0.0001, power=0.75)\n    pool1 = paddle.layer.img_pool(input=cmrnorm1, pool_size=3, stride=2)\n\n    conv2 = paddle.layer.img_conv(\n        input=pool1,\n        filter_size=5,\n        num_filters=256,\n        stride=1,\n        padding=2,\n        groups=1)\n    cmrnorm2 = paddle.layer.img_cmrnorm(\n        input=conv2, size=5, scale=0.0001, power=0.75)\n    pool2 = paddle.layer.img_pool(input=cmrnorm2, pool_size=3, stride=2)\n\n    pool3 = paddle.networks.img_conv_group(\n        input=pool2,\n        pool_size=3,\n        pool_stride=2,\n        conv_num_filter=[384, 384, 256],\n        conv_filter_size=3,\n        pool_type=paddle.pooling.Max())\n\n    fc1 = paddle.layer.fc(\n        input=pool3,\n        size=4096,\n        act=paddle.activation.Relu(),\n        layer_attr=paddle.attr.Extra(drop_rate=0.5))\n    fc2 = paddle.layer.fc(\n        input=fc1,\n        size=4096,\n        act=paddle.activation.Relu(),\n        layer_attr=paddle.attr.Extra(drop_rate=0.5))\n    return fc3\n"""
3.image_classification/googlenet.py,0,"b'import paddle.v2 as paddle\n\n__all__ = [\'googlenet\']\n\n\ndef inception(name, input, channels, filter1, filter3R, filter3, filter5R,\n              filter5, proj):\n    cov1 = paddle.layer.img_conv(\n        name=name + \'_1\',\n        input=input,\n        filter_size=1,\n        num_channels=channels,\n        num_filters=filter1,\n        stride=1,\n        padding=0)\n\n    cov3r = paddle.layer.img_conv(\n        name=name + \'_3r\',\n        input=input,\n        filter_size=1,\n        num_channels=channels,\n        num_filters=filter3R,\n        stride=1,\n        padding=0)\n    cov3 = paddle.layer.img_conv(\n        name=name + \'_3\',\n        input=cov3r,\n        filter_size=3,\n        num_filters=filter3,\n        stride=1,\n        padding=1)\n\n    cov5r = paddle.layer.img_conv(\n        name=name + \'_5r\',\n        input=input,\n        filter_size=1,\n        num_channels=channels,\n        num_filters=filter5R,\n        stride=1,\n        padding=0)\n    cov5 = paddle.layer.img_conv(\n        name=name + \'_5\',\n        input=cov5r,\n        filter_size=5,\n        num_filters=filter5,\n        stride=1,\n        padding=2)\n\n    pool1 = paddle.layer.img_pool(\n        name=name + \'_max\',\n        input=input,\n        pool_size=3,\n        num_channels=channels,\n        stride=1,\n        padding=1)\n    covprj = paddle.layer.img_conv(\n        name=name + \'_proj\',\n        input=pool1,\n        filter_size=1,\n        num_filters=proj,\n        stride=1,\n        padding=0)\n\n    cat = paddle.layer.concat(name=name, input=[cov1, cov3, cov5, covprj])\n    return cat\n\n\ndef googlenet(input, class_dim):\n    # stage 1\n    conv1 = paddle.layer.img_conv(\n        name=""conv1"",\n        input=input,\n        filter_size=7,\n        num_channels=3,\n        num_filters=64,\n        stride=2,\n        padding=3)\n    pool1 = paddle.layer.img_pool(\n        name=""pool1"", input=conv1, pool_size=3, num_channels=64, stride=2)\n\n    # stage 2\n    conv2_1 = paddle.layer.img_conv(\n        name=""conv2_1"",\n        input=pool1,\n        filter_size=1,\n        num_filters=64,\n        stride=1,\n        padding=0)\n    conv2_2 = paddle.layer.img_conv(\n        name=""conv2_2"",\n        input=conv2_1,\n        filter_size=3,\n        num_filters=192,\n        stride=1,\n        padding=1)\n    pool2 = paddle.layer.img_pool(\n        name=""pool2"", input=conv2_2, pool_size=3, num_channels=192, stride=2)\n\n    # stage 3\n    ince3a = inception(""ince3a"", pool2, 192, 64, 96, 128, 16, 32, 32)\n    ince3b = inception(""ince3b"", ince3a, 256, 128, 128, 192, 32, 96, 64)\n    pool3 = paddle.layer.img_pool(\n        name=""pool3"", input=ince3b, num_channels=480, pool_size=3, stride=2)\n\n    # stage 4\n    ince4a = inception(""ince4a"", pool3, 480, 192, 96, 208, 16, 48, 64)\n    ince4b = inception(""ince4b"", ince4a, 512, 160, 112, 224, 24, 64, 64)\n    ince4c = inception(""ince4c"", ince4b, 512, 128, 128, 256, 24, 64, 64)\n    ince4d = inception(""ince4d"", ince4c, 512, 112, 144, 288, 32, 64, 64)\n    ince4e = inception(""ince4e"", ince4d, 528, 256, 160, 320, 32, 128, 128)\n    pool4 = paddle.layer.img_pool(\n        name=""pool4"", input=ince4e, num_channels=832, pool_size=3, stride=2)\n\n    # stage 5\n    ince5a = inception(""ince5a"", pool4, 832, 256, 160, 320, 32, 128, 128)\n    ince5b = inception(""ince5b"", ince5a, 832, 384, 192, 384, 48, 128, 128)\n    pool5 = paddle.layer.img_pool(\n        name=""pool5"",\n        input=ince5b,\n        num_channels=1024,\n        pool_size=7,\n        stride=7,\n        pool_type=paddle.pooling.Avg())\n    dropout = paddle.layer.addto(\n        input=pool5,\n        layer_attr=paddle.attr.Extra(drop_rate=0.4),\n        act=paddle.activation.Linear())\n\n    out = paddle.layer.fc(\n        input=dropout, size=class_dim, act=paddle.activation.Softmax())\n\n    # fc for output 1\n    pool_o1 = paddle.layer.img_pool(\n        name=""pool_o1"",\n        input=ince4a,\n        num_channels=512,\n        pool_size=5,\n        stride=3,\n        pool_type=paddle.pooling.Avg())\n    conv_o1 = paddle.layer.img_conv(\n        name=""conv_o1"",\n        input=pool_o1,\n        filter_size=1,\n        num_filters=128,\n        stride=1,\n        padding=0)\n    fc_o1 = paddle.layer.fc(\n        name=""fc_o1"",\n        input=conv_o1,\n        size=1024,\n        layer_attr=paddle.attr.Extra(drop_rate=0.7),\n        act=paddle.activation.Relu())\n    out1 = paddle.layer.fc(\n        input=fc_o1, size=class_dim, act=paddle.activation.Softmax())\n\n    # fc for output 2\n    pool_o2 = paddle.layer.img_pool(\n        name=""pool_o2"",\n        input=ince4d,\n        num_channels=528,\n        pool_size=5,\n        stride=3,\n        pool_type=paddle.pooling.Avg())\n    conv_o2 = paddle.layer.img_conv(\n        name=""conv_o2"",\n        input=pool_o2,\n        filter_size=1,\n        num_filters=128,\n        stride=1,\n        padding=0)\n    fc_o2 = paddle.layer.fc(\n        name=""fc_o2"",\n        input=conv_o2,\n        size=1024,\n        layer_attr=paddle.attr.Extra(drop_rate=0.7),\n        act=paddle.activation.Relu())\n    out2 = paddle.layer.fc(\n        input=fc_o2, size=class_dim, act=paddle.activation.Softmax())\n\n    return out, out1, out2\n'"
3.image_classification/keras_model_visualization.py,0,"b'#coding:utf-8\n\'\'\'\nCreated by huxiaoman 2018.1.23\nkeras_model_visualization.py:\xe7\x94\xa8keras\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xbf\x87\xe7\xa8\x8b\n\'\'\'\n\nfrom keras.applications import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.applications.imagenet_utils import decode_predictions\nimport numpy as np\nimport cv2\nfrom cv2 import *\nimport matplotlib.pyplot as plt\nimport scipy as sp\nfrom scipy.misc import toimage\n\ndef test_opencv():\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x91\x84\xe5\x83\x8f\xe5\xa4\xb4\n    cam = VideoCapture(0)  # 0 -> \xe6\x91\x84\xe5\x83\x8f\xe5\xa4\xb4\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe4\xb8\x89\xe4\xb8\xaa\xe5\x9b\x9b\xe4\xb8\xaa\xe6\x91\x84\xe5\x83\x8f\xe5\xa4\xb4\xef\xbc\x8c\xe8\xa6\x81\xe8\xb0\x83\xe7\x94\xa8\xe5\x93\xaa\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe5\xbe\x80\xe4\xb8\x8a\xe5\x8a\xa0\xe5\x98\x9b\n    # \xe6\x8a\x93\xe6\x8b\x8d 5 \xe5\xbc\xa0\xe5\xb0\x8f\xe5\x9b\xbe\xe7\x89\x87\n    for x in range(0, 5):\n        s, img = cam.read()\n        if s:\n            imwrite(""o-"" + str(x) + "".jpg"", img)\n\ndef load_original(img_path):\n    # \xe6\x8a\x8a\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe7\x89\x87\xe5\x8e\x8b\xe7\xbc\xa9\xe4\xb8\xba 299*299\xe5\xa4\xa7\xe5\xb0\x8f\n    im_original = cv2.resize(cv2.imread(img_path), (299, 299))\n    im_converted = cv2.cvtColor(im_original, cv2.COLOR_BGR2RGB)\n    plt.figure(0)\n    plt.subplot(211)\n    plt.imshow(im_converted)\n    return im_original\n\ndef load_fine_tune_googlenet_v3(img):\n    # \xe5\x8a\xa0\xe8\xbd\xbdfine-tuning googlenet v3\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe5\x81\x9a\xe9\xa2\x84\xe6\xb5\x8b\n    model = InceptionV3(include_top=True, weights=\'imagenet\')\n    model.summary()\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    preds = model.predict(x)\n    print(\'Predicted:\', decode_predictions(preds))\n    plt.subplot(212)\n    plt.plot(preds.ravel())\n    plt.show()\n    return model, x\n\ndef extract_features(ins, layer_id, filters, layer_num):\n    \'\'\'\n    \xe6\x8f\x90\xe5\x8f\x96\xe6\x8c\x87\xe5\xae\x9a\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x8c\x87\xe5\xae\x9a\xe5\xb1\x82\xe6\x8c\x87\xe5\xae\x9a\xe6\x95\xb0\xe7\x9b\xae\xe7\x9a\x84feature map\xe5\xb9\xb6\xe8\xbe\x93\xe5\x87\xba\xe5\x88\xb0\xe4\xb8\x80\xe5\xb9\x85\xe5\x9b\xbe\xe4\xb8\x8a.\n    :param ins: \xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x9e\xe4\xbe\x8b\n    :param layer_id: \xe6\x8f\x90\xe5\x8f\x96\xe6\x8c\x87\xe5\xae\x9a\xe5\xb1\x82\xe7\x89\xb9\xe5\xbe\x81\n    :param filters: \xe6\xaf\x8f\xe5\xb1\x82\xe6\x8f\x90\xe5\x8f\x96\xe7\x9a\x84feature map\xe6\x95\xb0\n    :param layer_num: \xe4\xb8\x80\xe5\x85\xb1\xe6\x8f\x90\xe5\x8f\x96\xe5\xa4\x9a\xe5\xb0\x91\xe5\xb1\x82feature map\n    :return: None\n    \'\'\'\n    if len(ins) != 2:\n        print(\'parameter error:(model, instance)\')\n        return None\n    model = ins[0]\n    x = ins[1]\n    if type(layer_id) == type(1):\n        model_extractfeatures = Model(input=model.input, output=model.get_layer(index=layer_id).output)\n    else:\n        model_extractfeatures = Model(input=model.input, output=model.get_layer(name=layer_id).output)\n    fc2_features = model_extractfeatures.predict(x)\n    if filters > len(fc2_features[0][0][0]):\n        print(\'layer number error.\', len(fc2_features[0][0][0]),\',\',filters)\n        return None\n    for i in range(filters):\n        plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n        plt.subplot(filters, layer_num, layer_id + 1 + i * layer_num)\n        plt.axis(""off"")\n        if i < len(fc2_features[0][0][0]):\n            plt.imshow(fc2_features[0, :, :, i])\n\n# \xe5\xb1\x82\xe6\x95\xb0\xe3\x80\x81\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x81\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\ndef extract_features_batch(layer_num, model, filters):\n    \'\'\'\n    \xe6\x89\xb9\xe9\x87\x8f\xe6\x8f\x90\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\n    :param layer_num: \xe5\xb1\x82\xe6\x95\xb0\n    :param model: \xe6\xa8\xa1\xe5\x9e\x8b\n    :param filters: feature map\xe6\x95\xb0\n    :return: None\n    \'\'\'\n    plt.figure(figsize=(filters, layer_num))\n    plt.subplot(filters, layer_num, 1)\n    for i in range(layer_num):\n        extract_features(model, i, filters, layer_num)\n    plt.savefig(\'sample.jpg\')\n    plt.show()\n\ndef extract_features_with_layers(layers_extract):\n    \'\'\'\n    \xe6\x8f\x90\xe5\x8f\x96hypercolumn\xe5\xb9\xb6\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96.\n    :param layers_extract: \xe6\x8c\x87\xe5\xae\x9a\xe5\xb1\x82\xe5\x88\x97\xe8\xa1\xa8\n    :return: None\n    \'\'\'\n    hc = extract_hypercolumn(x[0], layers_extract, x[1])\n    ave = np.average(hc.transpose(1, 2, 0), axis=2)\n    plt.imshow(ave)\n    plt.show()\n\ndef extract_hypercolumn(model, layer_indexes, instance):\n    \'\'\'\n    \xe6\x8f\x90\xe5\x8f\x96\xe6\x8c\x87\xe5\xae\x9a\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x8c\x87\xe5\xae\x9a\xe5\xb1\x82\xe7\x9a\x84hypercolumn\xe5\x90\x91\xe9\x87\x8f\n    :param model: \xe6\xa8\xa1\xe5\x9e\x8b\n    :param layer_indexes: \xe5\xb1\x82id\n    :param instance: \xe6\xa8\xa1\xe5\x9e\x8b\n    :return:\n    \'\'\'\n    feature_maps = []\n    for i in layer_indexes:\n        feature_maps.append(Model(input=model.input, output=model.get_layer(index=i).output).predict(instance))\n    hypercolumns = []\n    for convmap in feature_maps:\n        for i in convmap[0][0][0]:\n            upscaled = sp.misc.imresize(convmap[0, :, :, i], size=(299, 299), mode=""F"", interp=\'bilinear\')\n            hypercolumns.append(upscaled)\n    return np.asarray(hypercolumns)\n\nif __name__ == \'__main__\':\n    img_path = \'~/auto1.jpg\'\n    img = load_original(img_path)\n    x = load_fine_tune_googlenet_v3(img)\n    extract_features_batch(15, x, 3)\n    extract_features_with_layers([1, 4, 7])\n    extract_features_with_layers([1, 4, 7, 10, 11, 14, 17])\n'"
3.image_classification/lenet.py,0,"b""#coding:utf-8\n'''\nCreated by huxiaoman 2017.11.27\nlenet.py:LeNet-5\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n'''\nimport os\nfrom PIL import Image\nimport numpy as np\nimport paddle.v2 as paddle\nfrom paddle.trainer_config_helpers import *\n\nwith_gpu = os.getenv('WITH_GPU', '0') != '1'\n\ndef lenet(img):\n    conv_pool_1 = paddle.networks.simple_img_conv_pool(\n        input=img,\n        filter_size=5,\n        num_filters=6,\n        num_channel=3,\n        pool_size=2,\n        pool_stride=2,\n        act=paddle.activation.Relu())\n    conv_pool_2 = paddle.networks.simple_img_conv_pool(\n        input=conv_pool_1,\n        filter_size=5,\n        num_filters=16,\n\tnum_channel=120,\n        pool_size=2,\n        pool_stride=2,\n        act=paddle.activation.Relu())\n    conv_3 = img_conv_layer(\n        input = conv_pool_2,\n        filter_size = 1,\n        num_filters = 120,\n        stride = 1)\n    fc = paddle.layer.fc(\n        input=conv_3, size=512, act=paddle.activation.Softmax())\n    return fc\n"""
3.image_classification/simple_cnn.py,0,"b""#coding:utf-8\n'''\nCreated by huxiaoman 2017.11.27\nsimple_cnn.py:\xe8\x87\xaa\xe5\xb7\xb1\xe8\xae\xbe\xe8\xae\xa1\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84cnn\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n'''\n\nimport os\nfrom PIL import Image\nimport numpy as np\nimport paddle.v2 as paddle\nfrom paddle.trainer_config_helpers import *\n\nwith_gpu = os.getenv('WITH_GPU', '0') != '1'\n\ndef simple_cnn(img):\n    conv_pool_1 = paddle.networks.simple_img_conv_pool(\n        input=img,\n        filter_size=5,\n        num_filters=20,\n        num_channel=3,\n        pool_size=2,\n        pool_stride=2,\n        act=paddle.activation.Relu())\n    conv_pool_2 = paddle.networks.simple_img_conv_pool(\n        input=conv_pool_1,\n        filter_size=5,\n        num_filters=50,\n        num_channel=20,\n        pool_size=2,\n        pool_stride=2,\n        act=paddle.activation.Relu())\n    fc = paddle.layer.fc(\n        input=conv_pool_2, size=512, act=paddle.activation.Softmax())\n    return fc\n~\n"""
3.image_classification/train.py,0,"b'# coding:utf-8\nimport gzip\nimport paddle.v2.dataset.flowers as flowers\nimport paddle.v2 as paddle\nimport reader\nimport vgg\nimport resnet\nimport alexnet\nimport googlenet\nimport argparse\n\nDATA_DIM = 3 * 224 * 224\nCLASS_DIM = 102\nBATCH_SIZE = 128\n\n\ndef main():\n    # parse the argument\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'model\',\n        help=\'The model for image classification\',\n        choices=[\'alexnet\', \'vgg13\', \'vgg16\', \'vgg19\', \'resnet\', \'googlenet\'])\n    args = parser.parse_args()\n\n    # PaddlePaddle init\n    paddle.init(use_gpu=True, trainer_count=7)\n\n    image = paddle.layer.data(\n        name=""image"", type=paddle.data_type.dense_vector(DATA_DIM))\n    lbl = paddle.layer.data(\n        name=""label"", type=paddle.data_type.integer_value(CLASS_DIM))\n\n    extra_layers = None\n    learning_rate = 0.01\n    if args.model == \'alexnet\':\n        out = alexnet.alexnet(image, class_dim=CLASS_DIM)\n    elif args.model == \'vgg13\':\n        out = vgg.vgg13(image, class_dim=CLASS_DIM)\n    elif args.model == \'vgg16\':\n        out = vgg.vgg16(image, class_dim=CLASS_DIM)\n    elif args.model == \'vgg19\':\n        out = vgg.vgg19(image, class_dim=CLASS_DIM)\n    elif args.model == \'resnet\':\n        out = resnet.resnet_imagenet(image, class_dim=CLASS_DIM)\n        learning_rate = 0.1\n    elif args.model == \'googlenet\':\n        out, out1, out2 = googlenet.googlenet(image, class_dim=CLASS_DIM)\n        loss1 = paddle.layer.cross_entropy_cost(\n            input=out1, label=lbl, coeff=0.3)\n        paddle.evaluator.classification_error(input=out1, label=lbl)\n        loss2 = paddle.layer.cross_entropy_cost(\n            input=out2, label=lbl, coeff=0.3)\n        paddle.evaluator.classification_error(input=out2, label=lbl)\n        extra_layers = [loss1, loss2]\n\n    cost = paddle.layer.classification_cost(input=out, label=lbl)\n\n    # Create parameters\n    parameters = paddle.parameters.create(cost)\n\n    # Create optimizer\n    optimizer = paddle.optimizer.Momentum(\n        momentum=0.9,\n        regularization=paddle.optimizer.L2Regularization(rate=0.0005 *\n                                                         BATCH_SIZE),\n        learning_rate=learning_rate / BATCH_SIZE,\n        learning_rate_decay_a=0.1,\n        learning_rate_decay_b=128000 * 35,\n        learning_rate_schedule=""discexp"", )\n\n    train_reader = paddle.batch(\n        paddle.reader.shuffle(\n            flowers.train(),\n            # To use other data, replace the above line with:\n            # reader.train_reader(\'train.list\'),\n            buf_size=1000),\n        batch_size=BATCH_SIZE)\n    test_reader = paddle.batch(\n        flowers.valid(),\n        # To use other data, replace the above line with:\n        # reader.test_reader(\'val.list\'),\n        batch_size=BATCH_SIZE)\n\n    # Create trainer\n    trainer = paddle.trainer.SGD(\n        cost=cost,\n        parameters=parameters,\n        update_equation=optimizer,\n        extra_layers=extra_layers)\n\n    # End batch and end pass event handler\n    def event_handler(event):\n        if isinstance(event, paddle.event.EndIteration):\n            if event.batch_id % 1 == 0:\n                print ""\\nPass %d, Batch %d, Cost %f, %s"" % (\n                    event.pass_id, event.batch_id, event.cost, event.metrics)\n        if isinstance(event, paddle.event.EndPass):\n            with gzip.open(\'params_pass_%d.tar.gz\' % event.pass_id, \'w\') as f:\n                trainer.save_parameter_to_tar(f)\n\n            result = trainer.test(reader=test_reader)\n            print ""\\nTest with Pass %d, %s"" % (event.pass_id, result.metrics)\n\n    trainer.train(\n        reader=train_reader, num_passes=200, event_handler=event_handler)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
3.image_classification/train_alexnet.py,0,"b'# Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License\n\nimport sys, os\n\nimport paddle.v2 as paddle\n\nfrom alexnet import alexnet\n#from alextnet import alexnet_lrn\nwith_gpu = os.getenv(\'WITH_GPU\', \'0\') != \'1\'\n\n\ndef main():\n    datadim = 3 * 32 * 32\n    classdim = 10\n\n    # PaddlePaddle init\n    paddle.init(use_gpu=with_gpu, trainer_count=7)\n\n    image = paddle.layer.data(\n        name=""image"", type=paddle.data_type.dense_vector(datadim))\n\n    # Add neural network config\n    # option 1. resnet\n    # net = resnet_cifar10(image, depth=32)\n    # option 2. vgg\n    #net = alexnet_lrn(image)\n    net = alexnet(image)\n    out = paddle.layer.fc(\n        input=net, size=classdim, act=paddle.activation.Softmax())\n\n    lbl = paddle.layer.data(\n        name=""label"", type=paddle.data_type.integer_value(classdim))\n    cost = paddle.layer.classification_cost(input=out, label=lbl)\n\n    # Create parameters\n    parameters = paddle.parameters.create(cost)\n\n    # Create optimizer\n    momentum_optimizer = paddle.optimizer.Momentum(\n        momentum=0.9,\n        regularization=paddle.optimizer.L2Regularization(rate=0.0002 * 128),\n        learning_rate=0.1 / 128.0,\n        learning_rate_decay_a=0.1,\n        learning_rate_decay_b=50000 * 100,\n        learning_rate_schedule=\'discexp\')\n\n    # End batch and end pass event handler\n    def event_handler(event):\n        if isinstance(event, paddle.event.EndIteration):\n            if event.batch_id % 100 == 0:\n                print ""\\nPass %d, Batch %d, Cost %f, %s"" % (\n                    event.pass_id, event.batch_id, event.cost, event.metrics)\n            else:\n                sys.stdout.write(\'.\')\n                sys.stdout.flush()\n        if isinstance(event, paddle.event.EndPass):\n            # save parameters\n            with open(\'params_pass_%d.tar\' % event.pass_id, \'w\') as f:\n                parameters.to_tar(f)\n\n            result = trainer.test(\n                reader=paddle.batch(\n                    paddle.dataset.cifar.test10(), batch_size=128),\n                feeding={\'image\': 0,\n                         \'label\': 1})\n            print ""\\nTest with Pass %d, %s"" % (event.pass_id, result.metrics)\n\n    # Create trainer\n    trainer = paddle.trainer.SGD(\n        cost=cost, parameters=parameters, update_equation=momentum_optimizer)\n\n    # Save the inference topology to protobuf.\n    inference_topology = paddle.topology.Topology(layers=out)\n    with open(""inference_topology.pkl"", \'wb\') as f:\n        inference_topology.serialize_for_inference(f)\n\n    trainer.train(\n        reader=paddle.batch(\n            paddle.reader.shuffle(\n                paddle.dataset.cifar.train10(), buf_size=50000),\n            batch_size=128),\n        num_passes=200,\n        event_handler=event_handler,\n        feeding={\'image\': 0,\n                 \'label\': 1})\n\n    # inference\n    from PIL import Image\n    import numpy as np\n    import os\n\n    def load_image(file):\n        im = Image.open(file)\n        im = im.resize((32, 32), Image.ANTIALIAS)\n        im = np.array(im).astype(np.float32)\n        # The storage order of the loaded image is W(widht),\n        # H(height), C(channel). PaddlePaddle requires\n        # the CHW order, so transpose them.\n        im = im.transpose((2, 0, 1))  # CHW\n        # In the training phase, the channel order of CIFAR\n        # image is B(Blue), G(green), R(Red). But PIL open\n        # image in RGB mode. It must swap the channel order.\n        im = im[(2, 1, 0), :, :]  # BGR\n        im = im.flatten()\n        im = im / 255.0\n        return im\n\n    test_data = []\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    test_data.append((load_image(cur_dir + \'/image/dog.png\'), ))\n\n    # users can remove the comments and change the model name\n    # with open(\'params_pass_50.tar\', \'r\') as f:\n    #    parameters = paddle.parameters.Parameters.from_tar(f)\n\n    probs = paddle.infer(\n        output_layer=out, parameters=parameters, input=test_data)\n    lab = np.argsort(-probs)  # probs and lab are the results of one batch data\n    print ""Label of image/dog.png is: %d"" % lab[0][0]\n\n\nif __name__ == \'__main__\':\n    main()\n'"
3.image_classification/train_fluid.py,0,"b'from __future__ import print_function\n\nimport os\nimport paddle\nimport paddle.fluid as fluid\nimport numpy\nimport sys\nfrom vgg import vgg_bn_drop\nfrom resnet import resnet_cifar10\n\n\ndef inference_network():\n    # The image is 32 * 32 with RGB representation.\n    data_shape = [3, 32, 32]\n    images = fluid.layers.data(name=\'pixel\', shape=data_shape, dtype=\'float32\')\n\n    predict = resnet_cifar10(images, 32)\n    # predict = vgg_bn_drop(images) # un-comment to use vgg net\n    return predict\n\n\ndef train_network(predict):\n    label = fluid.layers.data(name=\'label\', shape=[1], dtype=\'int64\')\n    cost = fluid.layers.cross_entropy(input=predict, label=label)\n    avg_cost = fluid.layers.mean(cost)\n    accuracy = fluid.layers.accuracy(input=predict, label=label)\n    return [avg_cost, accuracy]\n\n\ndef optimizer_program():\n    return fluid.optimizer.Adam(learning_rate=0.001)\n\n\ndef train(use_cuda, params_dirname):\n    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n    BATCH_SIZE = 128\n    train_reader = paddle.batch(\n        paddle.reader.shuffle(\n            paddle.dataset.cifar.train10(), buf_size=128 * 100),\n        batch_size=BATCH_SIZE)\n\n    test_reader = paddle.batch(\n        paddle.dataset.cifar.test10(), batch_size=BATCH_SIZE)\n\n    feed_order = [\'pixel\', \'label\']\n\n    main_program = fluid.default_main_program()\n    star_program = fluid.default_startup_program()\n\n    predict = inference_network()\n    avg_cost, acc = train_network(predict)\n\n    # Test program\n    test_program = main_program.clone(for_test=True)\n\n    optimizer = optimizer_program()\n    optimizer.minimize(avg_cost)\n\n    exe = fluid.Executor(place)\n\n    EPOCH_NUM = 1\n\n    # For training test cost\n    def train_test(program, reader):\n        count = 0\n        feed_var_list = [\n            program.global_block().var(var_name) for var_name in feed_order\n        ]\n        feeder_test = fluid.DataFeeder(feed_list=feed_var_list, place=place)\n        test_exe = fluid.Executor(place)\n        accumulated = len([avg_cost, acc]) * [0]\n        for tid, test_data in enumerate(reader()):\n            avg_cost_np = test_exe.run(\n                program=program,\n                feed=feeder_test.feed(test_data),\n                fetch_list=[avg_cost, acc])\n            accumulated = [\n                x[0] + x[1][0] for x in zip(accumulated, avg_cost_np)\n            ]\n            count += 1\n        return [x / count for x in accumulated]\n\n    # main train loop.\n    def train_loop():\n        feed_var_list_loop = [\n            main_program.global_block().var(var_name) for var_name in feed_order\n        ]\n        feeder = fluid.DataFeeder(feed_list=feed_var_list_loop, place=place)\n        exe.run(star_program)\n\n        step = 0\n        for pass_id in range(EPOCH_NUM):\n            for step_id, data_train in enumerate(train_reader()):\n                avg_loss_value = exe.run(\n                    main_program,\n                    feed=feeder.feed(data_train),\n                    fetch_list=[avg_cost, acc])\n                if step_id % 100 == 0:\n                    print(""\\nPass %d, Batch %d, Cost %f, Acc %f"" % (\n                        step_id, pass_id, avg_loss_value[0], avg_loss_value[1]))\n                else:\n                    sys.stdout.write(\'.\')\n                    sys.stdout.flush()\n                step += 1\n\n            avg_cost_test, accuracy_test = train_test(\n                test_program, reader=test_reader)\n            print(\'\\nTest with Pass {0}, Loss {1:2.2}, Acc {2:2.2}\'.format(\n                pass_id, avg_cost_test, accuracy_test))\n\n            if params_dirname is not None:\n                fluid.io.save_inference_model(params_dirname, [""pixel""],\n                                              [predict], exe)\n\n    train_loop()\n\n\ndef infer(use_cuda, params_dirname=None):\n    from PIL import Image\n    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n    exe = fluid.Executor(place)\n    inference_scope = fluid.core.Scope()\n\n    def load_image(infer_file):\n        im = Image.open(infer_file)\n        im = im.resize((32, 32), Image.ANTIALIAS)\n\n        im = numpy.array(im).astype(numpy.float32)\n        # The storage order of the loaded image is W(width),\n        # H(height), C(channel). PaddlePaddle requires\n        # the CHW order, so transpose them.\n        im = im.transpose((2, 0, 1))  # CHW\n        im = im / 255.0\n\n        # Add one dimension to mimic the list format.\n        im = numpy.expand_dims(im, axis=0)\n        return im\n\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    img = load_image(cur_dir + \'/image/dog.png\')\n\n    with fluid.scope_guard(inference_scope):\n        # Use fluid.io.load_inference_model to obtain the inference program desc,\n        # the feed_target_names (the names of variables that will be feeded\n        # data using feed operators), and the fetch_targets (variables that\n        # we want to obtain data from using fetch operators).\n        [inference_program, feed_target_names,\n         fetch_targets] = fluid.io.load_inference_model(params_dirname, exe)\n\n        # The input\'s dimension of conv should be 4-D or 5-D.\n        # Use inference_transpiler to speedup\n        inference_transpiler_program = inference_program.clone()\n        t = fluid.transpiler.InferenceTranspiler()\n        t.transpile(inference_transpiler_program, place)\n\n        # Construct feed as a dictionary of {feed_target_name: feed_target_data}\n        # and results will contain a list of data corresponding to fetch_targets.\n        results = exe.run(\n            inference_program,\n            feed={feed_target_names[0]: img},\n            fetch_list=fetch_targets)\n\n        transpiler_results = exe.run(\n            inference_transpiler_program,\n            feed={feed_target_names[0]: img},\n            fetch_list=fetch_targets)\n\n        assert len(results[0]) == len(transpiler_results[0])\n        for i in range(len(results[0])):\n            numpy.testing.assert_almost_equal(\n                results[0][i], transpiler_results[0][i], decimal=5)\n\n        # infer label\n        label_list = [\n            ""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"",\n            ""horse"", ""ship"", ""truck""\n        ]\n\n        print(""infer results: %s"" % label_list[numpy.argmax(results[0])])\n\n\ndef main(use_cuda):\n    if use_cuda and not fluid.core.is_compiled_with_cuda():\n        return\n    save_path = ""image_classification_resnet.inference.model""\n\n    train(use_cuda=use_cuda, params_dirname=save_path)\n\n    infer(use_cuda=use_cuda, params_dirname=save_path)\n\n\nif __name__ == \'__main__\':\n    # For demo purpose, the training runs on CPU\n    # Please change accordingly.\n    main(use_cuda=False)\n'"
3.image_classification/train_lenet.py,0,"b'#coding:utf-8\n\'\'\'\nCreated by huxiaoman 2017.11.27\ntrain_lenet.py:\xe8\xae\xad\xe7\xbb\x83lenet\xe5\xaf\xb9cifar10\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\n\'\'\'\n\nimport sys, os\n\nimport paddle.v2 as paddle\nfrom lenet import lenet\n\nwith_gpu = os.getenv(\'WITH_GPU\', \'0\') != \'1\'\n\n\ndef main():\n    datadim = 3 * 32 * 32\n    classdim = 10\n\n    # PaddlePaddle init\n    paddle.init(use_gpu=with_gpu, trainer_count=7)\n\n    image = paddle.layer.data(\n        name=""image"", type=paddle.data_type.dense_vector(datadim))\n\n    # Add neural network config\n    # option 1. resnet\n    # net = resnet_cifar10(image, depth=32)\n    # option 2. vgg\n    net = lenet(image)\n\n    out = paddle.layer.fc(\n        input=net, size=classdim, act=paddle.activation.Softmax())\n\n    lbl = paddle.layer.data(\n        name=""label"", type=paddle.data_type.integer_value(classdim))\n    cost = paddle.layer.classification_cost(input=out, label=lbl)\n\n    # Create parameters\n    parameters = paddle.parameters.create(cost)\n\n    # Create optimizer\n    momentum_optimizer = paddle.optimizer.Momentum(\n        momentum=0.9,\n        regularization=paddle.optimizer.L2Regularization(rate=0.0002 * 128),\n        learning_rate=0.1 / 128.0,\n        learning_rate_decay_a=0.1,\n        learning_rate_decay_b=50000 * 100,\n        learning_rate_schedule=\'discexp\')\n\n    # End batch and end pass event handler\n    def event_handler(event):\n        if isinstance(event, paddle.event.EndIteration):\n            if event.batch_id % 100 == 0:\n                print ""\\nPass %d, Batch %d, Cost %f, %s"" % (\n                    event.pass_id, event.batch_id, event.cost, event.metrics)\n            else:\n                sys.stdout.write(\'.\')\n                sys.stdout.flush()\n        if isinstance(event, paddle.event.EndPass):\n            # save parameters\n            with open(\'params_pass_%d.tar\' % event.pass_id, \'w\') as f:\n                parameters.to_tar(f)\n\n            result = trainer.test(\n                reader=paddle.batch(\n                    paddle.dataset.cifar.test10(), batch_size=128),\n                feeding={\'image\': 0,\n                         \'label\': 1})\n            print ""\\nTest with Pass %d, %s"" % (event.pass_id, result.metrics)\n\n    # Create trainer\n    trainer = paddle.trainer.SGD(\n        cost=cost, parameters=parameters, update_equation=momentum_optimizer)\n\n    # Save the inference topology to protobuf.\n    inference_topology = paddle.topology.Topology(layers=out)\n    with open(""inference_topology.pkl"", \'wb\') as f:\n        inference_topology.serialize_for_inference(f)\n\n    trainer.train(\n        reader=paddle.batch(\n            paddle.reader.shuffle(\n                paddle.dataset.cifar.train10(), buf_size=50000),\n            batch_size=128),\n        num_passes=200,\n        event_handler=event_handler,\n        feeding={\'image\': 0,\n                 \'label\': 1})\n\n    # inference\n    from PIL import Image\n    import numpy as np\n    import os\n\n    def load_image(file):\n        im = Image.open(file)\n        im = im.resize((32, 32), Image.ANTIALIAS)\n        im = np.array(im).astype(np.float32)\n        # The storage order of the loaded image is W(widht),\n        # H(height), C(channel). PaddlePaddle requires\n        # the CHW order, so transpose them.\n        im = im.transpose((2, 0, 1))  # CHW\n        # In the training phase, the channel order of CIFAR\n        # image is B(Blue), G(green), R(Red). But PIL open\n        # image in RGB mode. It must swap the channel order.\n        im = im[(2, 1, 0), :, :]  # BGR\n        im = im.flatten()\n        im = im / 255.0\n        return im\n\n    test_data = []\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    test_data.append((load_image(cur_dir + \'/image/dog.png\'), ))\n\n    # users can remove the comments and change the model name\n    # with open(\'params_pass_50.tar\', \'r\') as f:\n    #    parameters = paddle.parameters.Parameters.from_tar(f)\n\n    probs = paddle.infer(\n        output_layer=out, parameters=parameters, input=test_data)\n    lab = np.argsort(-probs)  # probs and lab are the results of one batch data\n    print ""Label of image/dog.png is: %d"" % lab[0][0]\n\n\nif __name__ == \'__main__\':\n    main()\n'"
3.image_classification/train_simple_cnn.py,0,"b'#coding:utf-8\n\'\'\'\nCreated by huxiaoman 2017.11.27\ntrain_simple\xe2\x80\x94_cnn.py:\xe8\xae\xad\xe7\xbb\x83simple_cnn\xe5\xaf\xb9cifar10\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\n\'\'\'\nimport sys, os\n\nimport paddle.v2 as paddle\nfrom simple_cnn import simple_cnn\n\nwith_gpu = os.getenv(\'WITH_GPU\', \'0\') != \'1\'\n\n\ndef main():\n    datadim = 3 * 32 * 32\n    classdim = 10\n\n    # PaddlePaddle init\n    paddle.init(use_gpu=with_gpu, trainer_count=7)\n\n    image = paddle.layer.data(\n        name=""image"", type=paddle.data_type.dense_vector(datadim))\n\n    # Add neural network config\n    # option 1. resnet\n    # net = resnet_cifar10(image, depth=32)\n    # option 2. vgg\n    net = simple_cnn(image)\n\n    out = paddle.layer.fc(\n        input=net, size=classdim, act=paddle.activation.Softmax())\n\n    lbl = paddle.layer.data(\n        name=""label"", type=paddle.data_type.integer_value(classdim))\n    cost = paddle.layer.classification_cost(input=out, label=lbl)\n\n    # Create parameters\n    parameters = paddle.parameters.create(cost)\n\n    # Create optimizer\n    momentum_optimizer = paddle.optimizer.Momentum(\n        momentum=0.9,\n        regularization=paddle.optimizer.L2Regularization(rate=0.0002 * 128),\n        learning_rate=0.1 / 128.0,\n        learning_rate_decay_a=0.1,\n        learning_rate_decay_b=50000 * 100,\n        learning_rate_schedule=\'discexp\')\n\n    # End batch and end pass event handler\n    def event_handler(event):\n        if isinstance(event, paddle.event.EndIteration):\n            if event.batch_id % 100 == 0:\n                print ""\\nPass %d, Batch %d, Cost %f, %s"" % (\n                    event.pass_id, event.batch_id, event.cost, event.metrics)\n            else:\n                sys.stdout.write(\'.\')\n                sys.stdout.flush()\n        if isinstance(event, paddle.event.EndPass):\n            # save parameters\n            with open(\'params_pass_%d.tar\' % event.pass_id, \'w\') as f:\n                parameters.to_tar(f)\n\n            result = trainer.test(\n                reader=paddle.batch(\n                    paddle.dataset.cifar.test10(), batch_size=128),\n                feeding={\'image\': 0,\n                         \'label\': 1})\n            print ""\\nTest with Pass %d, %s"" % (event.pass_id, result.metrics)\n\n    # Create trainer\n    trainer = paddle.trainer.SGD(\n        cost=cost, parameters=parameters, update_equation=momentum_optimizer)\n\n    # Save the inference topology to protobuf.\n    inference_topology = paddle.topology.Topology(layers=out)\n    with open(""inference_topology.pkl"", \'wb\') as f:\n        inference_topology.serialize_for_inference(f)\n\n    trainer.train(\n        reader=paddle.batch(\n            paddle.reader.shuffle(\n                paddle.dataset.cifar.train10(), buf_size=50000),\n            batch_size=128),\n        num_passes=200,\n        event_handler=event_handler,\n        feeding={\'image\': 0,\n                 \'label\': 1})\n\n    # inference\n    from PIL import Image\n    import numpy as np\n    import os\n\n    def load_image(file):\n        im = Image.open(file)\n        im = im.resize((32, 32), Image.ANTIALIAS)\n        im = np.array(im).astype(np.float32)\n        # The storage order of the loaded image is W(widht),\n        # H(height), C(channel). PaddlePaddle requires\n        # the CHW order, so transpose them.\n        im = im.transpose((2, 0, 1))  # CHW\n        # In the training phase, the channel order of CIFAR\n        # image is B(Blue), G(green), R(Red). But PIL open\n        # image in RGB mode. It must swap the channel order.\n        im = im[(2, 1, 0), :, :]  # BGR\n        im = im.flatten()\n        im = im / 255.0\n        return im\n\n    test_data = []\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    test_data.append((load_image(cur_dir + \'/image/dog.png\'), ))\n\n    # users can remove the comments and change the model name\n    # with open(\'params_pass_50.tar\', \'r\') as f:\n    #    parameters = paddle.parameters.Parameters.from_tar(f)\n\n    probs = paddle.infer(\n        output_layer=out, parameters=parameters, input=test_data)\n    lab = np.argsort(-probs)  # probs and lab are the results of one batch data\n    print ""Label of image/dog.png is: %d"" % lab[0][0]\n\n\nif __name__ == \'__main__\':\n    main()\n'"
3.image_classification/train_vgg.py,0,"b'#coding:utf-8\n\'\'\'\nCreated by huxiaoman 2017.12.12\ntrain_vgg.py:\xe8\xae\xad\xe7\xbb\x83vgg16\xe5\xaf\xb9cifar10\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\n\'\'\'\n\nimport sys, os\nimport paddle.v2 as paddle\nfrom vggnet import vgg\n\nwith_gpu = os.getenv(\'WITH_GPU\', \'0\') != \'1\'\n\ndef main():\n    datadim = 3 * 32 * 32\n    classdim = 10\n\n    # PaddlePaddle init\n    paddle.init(use_gpu=with_gpu, trainer_count=8)\n\n    image = paddle.layer.data(\n        name=""image"", type=paddle.data_type.dense_vector(datadim))\n    \n    net = vgg(image)\n\n    out = paddle.layer.fc(\n        input=net, size=classdim, act=paddle.activation.Softmax())\n\n    lbl = paddle.layer.data(\n        name=""label"", type=paddle.data_type.integer_value(classdim))\n    cost = paddle.layer.classification_cost(input=out, label=lbl)\n\n    # Create parameters\n    parameters = paddle.parameters.create(cost)\n\n    # Create optimizer\n    momentum_optimizer = paddle.optimizer.Momentum(\n        momentum=0.9,\n        regularization=paddle.optimizer.L2Regularization(rate=0.0002 * 128),\n        learning_rate=0.1 / 128.0,\n        learning_rate_decay_a=0.1,\n        learning_rate_decay_b=50000 * 100,\n        learning_rate_schedule=\'discexp\')\n\n    # End batch and end pass event handler\n    def event_handler(event):\n        if isinstance(event, paddle.event.EndIteration):\n            if event.batch_id % 100 == 0:\n                print ""\\nPass %d, Batch %d, Cost %f, %s"" % (\n                    event.pass_id, event.batch_id, event.cost, event.metrics)\n            else:\n                sys.stdout.write(\'.\')\n                sys.stdout.flush()\n        if isinstance(event, paddle.event.EndPass):\n            # save parameters\n            with open(\'params_pass_%d.tar\' % event.pass_id, \'w\') as f:\n                parameters.to_tar(f)\n\n            result = trainer.test(\n                reader=paddle.batch(\n                    paddle.dataset.cifar.test10(), batch_size=128),\n                feeding={\'image\': 0,\n                         \'label\': 1})\n            print ""\\nTest with Pass %d, %s"" % (event.pass_id, result.metrics)\n\n    # Create trainer\n    trainer = paddle.trainer.SGD(\n        cost=cost, parameters=parameters, update_equation=momentum_optimizer)\n\n    # Save the inference topology to protobuf.\n    inference_topology = paddle.topology.Topology(layers=out)\n    with open(""inference_topology.pkl"", \'wb\') as f:\n        inference_topology.serialize_for_inference(f)\n\n    trainer.train(\n        reader=paddle.batch(\n            paddle.reader.shuffle(\n                paddle.dataset.cifar.train10(), buf_size=50000),\n            batch_size=128),\n        num_passes=200,\n        event_handler=event_handler,\n        feeding={\'image\': 0,\n                 \'label\': 1})\n\n    # inference\n    from PIL import Image\n    import numpy as np\n    import os\n\n    def load_image(file):\n        im = Image.open(file)\n        im = im.resize((32, 32), Image.ANTIALIAS)\n        im = np.array(im).astype(np.float32)\n        im = im.transpose((2, 0, 1))  # CHW\n        im = im[(2, 1, 0), :, :]  # BGR\n        im = im.flatten()\n        im = im / 255.0\n        return im\n\n    test_data = []\n    cur_dir = os.path.dirname(os.path.realpath(__file__))\n    test_data.append((load_image(cur_dir + \'/image/dog.png\'), ))\n\n    probs = paddle.infer(\n        output_layer=out, parameters=parameters, input=test_data)\n    lab = np.argsort(-probs)  # probs and lab are the results of one batch data\n    print ""Label of image/dog.png is: %d"" % lab[0][0]\n\n\nif __name__ == \'__main__\':\n    main()\n'"
3.image_classification/vggnet.py,0,"b""#coding:utf-8\n'''\nCreated by huxiaoman 2017.12.12\nvggnet.py:\xe7\x94\xa8vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\xae\x9e\xe7\x8e\xb0cifar-10\xe5\x88\x86\xe7\xb1\xbb\n'''\n\nimport paddle.v2 as paddle\n\n\ndef vgg(input):\n    def conv_block(ipt, num_filter, groups, dropouts, num_channels=None):\n        return paddle.networks.img_conv_group(\n            input=ipt,\n            num_channels=num_channels,\n            pool_size=2,\n            pool_stride=2,\n            conv_num_filter=[num_filter] * groups,\n            conv_filter_size=3,\n            conv_act=paddle.activation.Relu(),\n            conv_with_batchnorm=True,\n            conv_batchnorm_drop_rate=dropouts,\n            pool_type=paddle.pooling.Max())\n\n    conv1 = conv_block(input, 64, 2, [0.3, 0], 3)\n    conv2 = conv_block(conv1, 128, 2, [0.4, 0])\n    conv3 = conv_block(conv2, 256, 3, [0.4, 0.4, 0])\n    conv4 = conv_block(conv3, 512, 3, [0.4, 0.4, 0])\n    conv5 = conv_block(conv4, 512, 3, [0.4, 0.4, 0])\n\n    drop = paddle.layer.dropout(input=conv5, dropout_rate=0.5)\n    fc1 = paddle.layer.fc(input=drop, size=512, act=paddle.activation.Linear())\n    bn = paddle.layer.batch_norm(\n        input=fc1,\n        act=paddle.activation.Relu(),\n        layer_attr=paddle.attr.Extra(drop_rate=0.5))\n    fc2 = paddle.layer.fc(input=bn, size=512, act=paddle.activation.Linear())\n    return fc2\n"""
5.plate_recognition/genGreenplate.py,0,"b'#coding=utf-8\n"""""" \n   genGreenPlate.py:\xe7\x94\x9f\xe6\x88\x90\xe6\x96\xb0\xe8\x83\xbd\xe6\xba\x90\xe7\xbb\xbf\xe8\x89\xb2\xe8\xbd\xa6\xe7\x89\x8c\n""""""\n\n__author__ = ""Huxiaoman""\n__copyright__ = ""Copyright (c) 2017 ""\n\nimport PIL\nfrom PIL import ImageFont\nfrom PIL import Image\nfrom PIL import ImageDraw\nimport cv2;\nimport numpy as np;\nimport os;\nfrom math import *\nimport sys\n\n\nindex = {""\xe4\xba\xac"": 0, ""\xe6\xb2\xaa"": 1, ""\xe6\xb4\xa5"": 2, ""\xe6\xb8\x9d"": 3, ""\xe5\x86\x80"": 4, ""\xe6\x99\x8b"": 5, ""\xe8\x92\x99"": 6, ""\xe8\xbe\xbd"": 7, ""\xe5\x90\x89"": 8, ""\xe9\xbb\x91"": 9, ""\xe8\x8b\x8f"": 10, ""\xe6\xb5\x99"": 11, ""\xe7\x9a\x96"": 12,\n         ""\xe9\x97\xbd"": 13, ""\xe8\xb5\xa3"": 14, ""\xe9\xb2\x81"": 15, ""\xe8\xb1\xab"": 16, ""\xe9\x84\x82"": 17, ""\xe6\xb9\x98"": 18, ""\xe7\xb2\xa4"": 19, ""\xe6\xa1\x82"": 20, ""\xe7\x90\xbc"": 21, ""\xe5\xb7\x9d"": 22, ""\xe8\xb4\xb5"": 23, ""\xe4\xba\x91"": 24,\n         ""\xe8\x97\x8f"": 25, ""\xe9\x99\x95"": 26, ""\xe7\x94\x98"": 27, ""\xe9\x9d\x92"": 28, ""\xe5\xae\x81"": 29, ""\xe6\x96\xb0"": 30, ""0"": 31, ""1"": 32, ""2"": 33, ""3"": 34, ""4"": 35, ""5"": 36,\n         ""6"": 37, ""7"": 38, ""8"": 39, ""9"": 40, ""A"": 41, ""B"": 42, ""C"": 43, ""D"": 44, ""E"": 45, ""F"": 46, ""G"": 47, ""H"": 48,\n         ""J"": 49, ""K"": 50, ""L"": 51, ""M"": 52, ""N"": 53, ""P"": 54, ""Q"": 55, ""R"": 56, ""S"": 57, ""T"": 58, ""U"": 59, ""V"": 60,\n         ""W"": 61, ""X"": 62, ""Y"": 63, ""Z"": 64};\n\n\nchars = [""\xe4\xba\xac"", ""\xe6\xb2\xaa"", ""\xe6\xb4\xa5"", ""\xe6\xb8\x9d"", ""\xe5\x86\x80"", ""\xe6\x99\x8b"", ""\xe8\x92\x99"", ""\xe8\xbe\xbd"", ""\xe5\x90\x89"", ""\xe9\xbb\x91"", ""\xe8\x8b\x8f"", ""\xe6\xb5\x99"", ""\xe7\x9a\x96"", ""\xe9\x97\xbd"", ""\xe8\xb5\xa3"", ""\xe9\xb2\x81"", ""\xe8\xb1\xab"", ""\xe9\x84\x82"", ""\xe6\xb9\x98"", ""\xe7\xb2\xa4"", ""\xe6\xa1\x82"",\n             ""\xe7\x90\xbc"", ""\xe5\xb7\x9d"", ""\xe8\xb4\xb5"", ""\xe4\xba\x91"", ""\xe8\x97\x8f"", ""\xe9\x99\x95"", ""\xe7\x94\x98"", ""\xe9\x9d\x92"", ""\xe5\xae\x81"", ""\xe6\x96\xb0"", ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""A"",\n             ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""J"", ""K"", ""L"", ""M"", ""N"", ""P"", ""Q"", ""R"", ""S"", ""T"", ""U"", ""V"", ""W"", ""X"",\n             ""Y"", ""Z""\n             ];\n\ndef AddSmudginess(img, Smu):\n    rows = r(Smu.shape[0] - 50)\n    cols = r(Smu.shape[1] - 50)\n    adder = Smu[rows:rows + 50, cols:cols + 50];\n    adder = cv2.resize(adder, (50, 50));\n    #adder = cv2.bitwise_not(adder)\n    img = cv2.resize(img,(50,50))\n    img = cv2.bitwise_not(img)\n    img = cv2.bitwise_and(adder, img)\n    img = cv2.bitwise_not(img)\n    return img\n\ndef rot(img,angel,shape,max_angel):\n    """""" \n        \xe6\xb7\xbb\xe5\x8a\xa0\xe6\x94\xbe\xe5\xb0\x84\xe7\x95\xb8\xe5\x8f\x98\n        img \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\n        factor \xe7\x95\xb8\xe5\x8f\x98\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n        size \xe4\xb8\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xe5\xb0\xba\xe5\xaf\xb8\n    """"""\n    size_o = [shape[1],shape[0]]\n    size = (shape[1]+ int(shape[0]*cos((float(max_angel )/180) * 3.14)),shape[0])\n    #print size\n    interval = abs( int( sin((float(angel) /180) * 3.14)* shape[0]));\n    pts1 = np.float32([[0,0],[0,size_o[1]],[size_o[0],0],[size_o[0],size_o[1]]])\n    if(angel>0):\n        pts2 = np.float32([[interval,0],[0,size[1]  ],[size[0],0  ],[size[0]-interval,size_o[1]]])\n    else:\n        pts2 = np.float32([[0,0],[interval,size[1]  ],[size[0]-interval,0  ],[size[0],size_o[1]]])\n    M  = cv2.getPerspectiveTransform(pts1,pts2);\n    dst = cv2.warpPerspective(img,M,size);\n    return dst\n\ndef rotRandrom(img, factor, size):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe9\x80\x8f\xe8\xa7\x86\xe7\x95\xb8\xe5\x8f\x98\n    """"""\n    shape = size;\n    pts1 = np.float32([[0, 0], [0, shape[0]], [shape[1], 0], [shape[1], shape[0]]])\n    pts2 = np.float32([[r(factor), r(factor)], [ r(factor), shape[0] - r(factor)], [shape[1] - r(factor),  r(factor)],\n                       [shape[1] - r(factor), shape[0] - r(factor)]])\n    M = cv2.getPerspectiveTransform(pts1, pts2);\n    dst = cv2.warpPerspective(img, M, size);\n    return dst\n\ndef tfactor(img):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe9\xa5\xb1\xe5\x92\x8c\xe5\xba\xa6\xe5\x85\x89\xe7\x85\xa7\xe7\x9a\x84\xe5\x99\xaa\xe5\xa3\xb0\n    """"""\n    hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV);\n    hsv[:,:,0] = hsv[:,:,0]*(0.8+ np.random.random()*0.2);\n    hsv[:,:,1] = hsv[:,:,1]*(0.3+ np.random.random()*0.7);\n    hsv[:,:,2] = hsv[:,:,2]*(0.2+ np.random.random()*0.8);\n\n    img = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR);\n    return img\n\ndef random_envirment(img,data_set):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe8\x87\xaa\xe7\x84\xb6\xe7\x8e\xaf\xe5\xa2\x83\xe7\x9a\x84\xe5\x99\xaa\xe5\xa3\xb0    \n    """"""\n    index=r(len(data_set))\n    env = cv2.imread(data_set[index])\n    env = cv2.resize(env,(img.shape[1],img.shape[0]))\n    bak = (img==0);\n    bak = bak.astype(np.uint8)*255;\n    inv = cv2.bitwise_and(bak,env)\n    img = cv2.bitwise_or(inv,img)\n    return img\n\ndef GenCh(f,val):\n    """"""\n    \xe7\x94\x9f\xe6\x88\x90\xe4\xb8\xad\xe6\x96\x87\xe5\xad\x97\xe7\xac\xa6\n    """"""\n    # \xe6\xad\xa4\xe5\xa4\x84\xe9\x9c\x80\xe8\xa6\x81\xe5\xb0\x86\xe5\xad\x97\xe4\xbd\x93\xe9\xa2\x9c\xe8\x89\xb2\xe6\x8d\xa2\xe4\xb8\xba\xe9\xbb\x91\xe8\x89\xb2\n    img=Image.new(""RGB"", (45,70),(255,255,255))\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 3),val,(0,0,0),font=f)\n    img =  img.resize((23,70))\n    A = np.array(img)\n    return A\n\ndef GenCh1(f,val):\n    """"""\n    \xe7\x94\x9f\xe6\x88\x90\xe8\x8b\xb1\xe6\x96\x87\xe5\xad\x97\xe7\xac\xa6\n    """"""\n    # \xe6\xad\xa4\xe5\xa4\x84\xe9\x9c\x80\xe8\xa6\x81\xe5\xb0\x86\xe5\xad\x97\xe4\xbd\x93\xe9\xa2\x9c\xe8\x89\xb2\xe6\x8d\xa2\xe4\xb8\xba\xe9\xbb\x91\xe8\x89\xb2\n    img=Image.new(""RGB"", (23,70),(255,255,255))\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 2),val.decode(\'utf-8\'),(0,0,0),font=f)\n    A = np.array(img)\n    return A\n\ndef AddGauss(img, level):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe9\xab\x98\xe6\x96\xaf\xe6\xa8\xa1\xe7\xb3\x8a\n    """""" \n    return cv2.blur(img, (level * 2 + 1, level * 2 + 1));\n\ndef r(val):\n    return int(np.random.random() * val)\n\ndef AddNoiseSingleChannel(single):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe9\xab\x98\xe6\x96\xaf\xe5\x99\xaa\xe5\xa3\xb0\n    """"""\n    diff = 255-single.max();\n    noise = np.random.normal(0,1+r(6),single.shape);\n    noise = (noise - noise.min())/(noise.max()-noise.min())\n    noise= diff*noise;\n    noise= noise.astype(np.uint8)\n    dst = single + noise\n    return dst\n\ndef addNoise(img,sdev = 0.5,avg=10):\n    img[:,:,0] =  AddNoiseSingleChannel(img[:,:,0]);\n    img[:,:,1] =  AddNoiseSingleChannel(img[:,:,1]);\n    img[:,:,2] =  AddNoiseSingleChannel(img[:,:,2]);\n    return img\n\n\nclass GenPlate:\n\n    def __init__(self,fontCh,fontEng,NoPlates):\n        self.fontC =  ImageFont.truetype(fontCh,43,0);\n        self.fontE =  ImageFont.truetype(fontEng,60,0);\n        self.img=np.array(Image.new(""RGB"", (226,70),(255,255,255)))\n        self.bg  = cv2.resize(cv2.imread(""./images/g1.jpg""),(226,70));\n        self.smu = cv2.imread(""./images/smu2.jpg"");\n        self.noplates_path = [];\n        for parent,parent_folder,filenames in os.walk(NoPlates):\n            for filename in filenames:\n                path = parent+""/""+filename;\n                self.noplates_path.append(path);\n\n\n    def draw(self,val):\n        offset= 2 ;\n        self.img[0:70,offset+8:offset+8+23]= GenCh(self.fontC,val[0]);\n        self.img[0:70,offset+8+23+6:offset+8+23+6+23]= GenCh1(self.fontE,val[1]);\n\tfor i in range(6):\n\t    #\xe7\x94\x9f\xe6\x88\x90\xe7\xbb\xbf\xe8\x89\xb2\xe8\xbd\xa6\xe7\x89\x8c\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe8\xbf\x99\xe9\x87\x8c\xe9\x9c\x80\xe8\xa6\x81\xe6\x94\xb9\xe4\xb8\x80\xe4\xb8\x8b\n            base = offset+8+23+6+23+17 +i*23 ;\n\t    #print base\n            #print self.img[0:70, base  : base+23].shape\n\t    self.img[0:70, base  : base+23]= GenCh1(self.fontE,val[i+2]);\n        return self.img\n    \n    def generate(self,text):\n        #print len(text)\n        if len(text) == 10:\n\t    # \xe6\xb3\xa8\xe6\x84\x8f\xe6\xad\xa4\xe5\xa4\x84\xe5\x92\x8c\xe7\x94\x9f\xe6\x88\x90\xe8\x93\x9d\xe7\x89\x8c\xe8\xbd\xa6\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe5\xad\x97\xe7\xac\xa6\xe9\xa2\x9c\xe8\x89\xb2\xe4\xb8\xba\xe9\xbb\x91\xe8\x89\xb2\n            fg = self.draw(text.decode(encoding=""utf-8""));\n            com = cv2.bitwise_and(fg,self.bg);\n            #com = rot(com,r(60)-30,com.shape,30);\n            #com = rotRandrom(com,10,(com.shape[1],com.shape[0]));\n            #com = tfactor(com)\n            #com = random_envirment(com,self.noplates_path);\n            # \xe6\xb7\xbb\xe5\x8a\xa0\xe9\xab\x98\xe6\x96\xaf\xe6\xa8\xa1\xe7\xb3\x8a\n\t    #com = AddGauss(com, 1+r(2));\n            com = addNoise(com);\n            return com\n\n    def genPlateString(self,pos,val):\n        \'\'\'\n\t    \xe7\x94\x9f\xe6\x88\x90\xe8\xbd\xa6\xe7\x89\x8cString,\xe5\xad\x98\xe4\xb8\xba\xe5\x9b\xbe\xe7\x89\x87\n        \xe7\x94\x9f\xe6\x88\x90\xe8\xbd\xa6\xe7\x89\x8clist,\xe5\xad\x98\xe4\xb8\xbalabel\n        \'\'\'\n        plateStr = """";\n        plateList=[]\n        box = [0,0,0,0,0,0,0,0];\n        if(pos!=-1):\n            box[pos]=1;\n        for unit,cpos in zip(box,range(len(box))):\n            if unit == 1:\n                plateStr += val\n                #print plateStr\n                plateList.append(val)\n            else:\n                if cpos == 0:\n                    plateStr += chars[r(31)]\n                    plateList.append(plateStr)\n                elif cpos == 1:\n                    plateStr += chars[41+r(24)]\n                    plateList.append(plateStr)\n\t\t#\xe6\x96\xb0\xe8\x83\xbd\xe6\xba\x90\xe8\xbd\xa6\xe7\x89\x8c\xe7\x9a\x84\xe7\x94\x9f\xe6\x88\x90\xe6\x9c\x89\xe8\xa7\x84\xe5\x88\x99,\xe5\xb0\x8f\xe5\x9e\x8b\xe5\xae\xa2\xe8\xbd\xa6\xe7\xac\xac\xe4\xb8\x89\xe4\xbd\x8d\xe6\x98\xafD/F\xef\xbc\x8c\xe5\xa4\xa7\xe5\x9e\x8b\xe5\xae\xa2\xe8\xbd\xa6\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xbd\x8d\xe6\x98\xafD/F\n                elif cpos == 2:\n\t\t    plateStr += [\'D\',\'F\'][r(2)]\n                    plateList.append(plateStr)\n\t\t# \xe5\xaf\xb9\xe7\xac\xac3~6\xe4\xbd\x8d\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xbb\xbb\xe6\x84\x8f\xe5\xad\x97\xe7\xac\xa6\n                #elif cpos == range(2,8)[r(6)]:\n                #    plateStr += chars[41+r(24)]\n                #    plateList.append(plateStr)\n                else:\n                    plateStr += chars[31:41][r(10)]\n                    plateList.append(plateStr)\n        plate = [plateList[0]]\n        b = [plateList[i][-1] for i in range(len(plateList))]\n        plate.extend(b[1:8])\n        return plateStr,plate\n\n    # \xe5\xb0\x86\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe8\xbd\xa6\xe7\x89\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84label\xe5\x86\x99\xe5\x85\xa5label.txt\n    def genBatch(self, batchSize,pos,charRange, outputPath,size):\n        if (not os.path.exists(outputPath)):\n            os.mkdir(outputPath)\n\toutfile = open(\'label.txt\',\'w\')\n        for i in xrange(batchSize):\n                plateStr,plate = G.genPlateString(-1,-1)\n                print plateStr,plate\n\t\timg =  G.generate(plateStr);\n                #print type(img)\n\t\timg = cv2.resize(img,size);\n                cv2.imwrite(outputPath + ""/"" + str(i).zfill(2) + "".jpg"", img);\n\t\toutfile.write(str(plate)+""\\n"")\nG = GenPlate(""./font/platech.ttf"",\'./font/platechar.ttf\',""./NoPlates"")\n#G.genBatch(100,2,range(31,65),""./plate_100"",(272,72))\n\nif __name__==\'__main__\':\n    G.genBatch(int(sys.argv[1]),2,range(31,65),sys.argv[2],(272,72))\n'"
5.plate_recognition/genplate.py,0,"b'#coding=utf-8\n"""""" \n   genPlate.py:\xe7\x94\x9f\xe6\x88\x90\xe9\x9a\x8f\xe6\x9c\xba\xe8\xbd\xa6\xe7\x89\x8c\n""""""\n\n__author__ = ""Huxiaoman""\n__copyright__ = ""Copyright (c) 2017 ""\n\nimport PIL\nfrom PIL import ImageFont\nfrom PIL import Image\nfrom PIL import ImageDraw\nimport cv2;\nimport numpy as np;\nimport os;\nfrom math import *\nimport sys\n\n\nindex = {""\xe4\xba\xac"": 0, ""\xe6\xb2\xaa"": 1, ""\xe6\xb4\xa5"": 2, ""\xe6\xb8\x9d"": 3, ""\xe5\x86\x80"": 4, ""\xe6\x99\x8b"": 5, ""\xe8\x92\x99"": 6, ""\xe8\xbe\xbd"": 7, ""\xe5\x90\x89"": 8, ""\xe9\xbb\x91"": 9, ""\xe8\x8b\x8f"": 10, ""\xe6\xb5\x99"": 11, ""\xe7\x9a\x96"": 12,\n         ""\xe9\x97\xbd"": 13, ""\xe8\xb5\xa3"": 14, ""\xe9\xb2\x81"": 15, ""\xe8\xb1\xab"": 16, ""\xe9\x84\x82"": 17, ""\xe6\xb9\x98"": 18, ""\xe7\xb2\xa4"": 19, ""\xe6\xa1\x82"": 20, ""\xe7\x90\xbc"": 21, ""\xe5\xb7\x9d"": 22, ""\xe8\xb4\xb5"": 23, ""\xe4\xba\x91"": 24,\n         ""\xe8\x97\x8f"": 25, ""\xe9\x99\x95"": 26, ""\xe7\x94\x98"": 27, ""\xe9\x9d\x92"": 28, ""\xe5\xae\x81"": 29, ""\xe6\x96\xb0"": 30, ""0"": 31, ""1"": 32, ""2"": 33, ""3"": 34, ""4"": 35, ""5"": 36,\n         ""6"": 37, ""7"": 38, ""8"": 39, ""9"": 40, ""A"": 41, ""B"": 42, ""C"": 43, ""D"": 44, ""E"": 45, ""F"": 46, ""G"": 47, ""H"": 48,\n         ""J"": 49, ""K"": 50, ""L"": 51, ""M"": 52, ""N"": 53, ""P"": 54, ""Q"": 55, ""R"": 56, ""S"": 57, ""T"": 58, ""U"": 59, ""V"": 60,\n         ""W"": 61, ""X"": 62, ""Y"": 63, ""Z"": 64};\n\nchars = [""\xe4\xba\xac"", ""\xe6\xb2\xaa"", ""\xe6\xb4\xa5"", ""\xe6\xb8\x9d"", ""\xe5\x86\x80"", ""\xe6\x99\x8b"", ""\xe8\x92\x99"", ""\xe8\xbe\xbd"", ""\xe5\x90\x89"", ""\xe9\xbb\x91"", ""\xe8\x8b\x8f"", ""\xe6\xb5\x99"", ""\xe7\x9a\x96"", ""\xe9\x97\xbd"", ""\xe8\xb5\xa3"", ""\xe9\xb2\x81"", ""\xe8\xb1\xab"", ""\xe9\x84\x82"", ""\xe6\xb9\x98"", ""\xe7\xb2\xa4"", ""\xe6\xa1\x82"",\n             ""\xe7\x90\xbc"", ""\xe5\xb7\x9d"", ""\xe8\xb4\xb5"", ""\xe4\xba\x91"", ""\xe8\x97\x8f"", ""\xe9\x99\x95"", ""\xe7\x94\x98"", ""\xe9\x9d\x92"", ""\xe5\xae\x81"", ""\xe6\x96\xb0"", ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""A"",\n             ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H"", ""J"", ""K"", ""L"", ""M"", ""N"", ""P"", ""Q"", ""R"", ""S"", ""T"", ""U"", ""V"", ""W"", ""X"",\n             ""Y"", ""Z""\n             ];\n\ndef AddSmudginess(img, Smu):\n    rows = r(Smu.shape[0] - 50)\n    cols = r(Smu.shape[1] - 50)\n    adder = Smu[rows:rows + 50, cols:cols + 50];\n    adder = cv2.resize(adder, (50, 50));\n    #adder = cv2.bitwise_not(adder)\n    img = cv2.resize(img,(50,50))\n    img = cv2.bitwise_not(img)\n    img = cv2.bitwise_and(adder, img)\n    img = cv2.bitwise_not(img)\n    return img\n\ndef rot(img,angel,shape,max_angel):\n    """""" \n        \xe6\xb7\xbb\xe5\x8a\xa0\xe6\x94\xbe\xe5\xb0\x84\xe7\x95\xb8\xe5\x8f\x98\n        img \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\n        factor \xe7\x95\xb8\xe5\x8f\x98\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n        size \xe4\xb8\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xe5\xb0\xba\xe5\xaf\xb8\n    """"""\n    size_o = [shape[1],shape[0]]\n    size = (shape[1]+ int(shape[0]*cos((float(max_angel )/180) * 3.14)),shape[0])\n    interval = abs( int( sin((float(angel) /180) * 3.14)* shape[0]));\n    pts1 = np.float32([[0,0],[0,size_o[1]],[size_o[0],0],[size_o[0],size_o[1]]])\n    if(angel>0):\n        pts2 = np.float32([[interval,0],[0,size[1]  ],[size[0],0  ],[size[0]-interval,size_o[1]]])\n    else:\n        pts2 = np.float32([[0,0],[interval,size[1]  ],[size[0]-interval,0  ],[size[0],size_o[1]]])\n    M  = cv2.getPerspectiveTransform(pts1,pts2);\n    dst = cv2.warpPerspective(img,M,size);\n    return dst\n\ndef rotRandrom(img, factor, size):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe9\x80\x8f\xe8\xa7\x86\xe7\x95\xb8\xe5\x8f\x98\n    """"""\n    shape = size;\n    pts1 = np.float32([[0, 0], [0, shape[0]], [shape[1], 0], [shape[1], shape[0]]])\n    pts2 = np.float32([[r(factor), r(factor)], [ r(factor), shape[0] - r(factor)], [shape[1] - r(factor),  r(factor)],\n                       [shape[1] - r(factor), shape[0] - r(factor)]])\n    M = cv2.getPerspectiveTransform(pts1, pts2);\n    dst = cv2.warpPerspective(img, M, size);\n    return dst\n\ndef tfactor(img):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe9\xa5\xb1\xe5\x92\x8c\xe5\xba\xa6\xe5\x85\x89\xe7\x85\xa7\xe7\x9a\x84\xe5\x99\xaa\xe5\xa3\xb0\n    """"""\n    hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV);\n    hsv[:,:,0] = hsv[:,:,0]*(0.8+ np.random.random()*0.2);\n    hsv[:,:,1] = hsv[:,:,1]*(0.3+ np.random.random()*0.7);\n    hsv[:,:,2] = hsv[:,:,2]*(0.2+ np.random.random()*0.8);\n\n    img = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR);\n    return img\n\ndef random_envirment(img,data_set):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe8\x87\xaa\xe7\x84\xb6\xe7\x8e\xaf\xe5\xa2\x83\xe7\x9a\x84\xe5\x99\xaa\xe5\xa3\xb0    \n    """"""\n    index=r(len(data_set))\n    env = cv2.imread(data_set[index])\n    env = cv2.resize(env,(img.shape[1],img.shape[0]))\n    bak = (img==0);\n    bak = bak.astype(np.uint8)*255;\n    inv = cv2.bitwise_and(bak,env)\n    img = cv2.bitwise_or(inv,img)\n    return img\n\ndef GenCh(f,val):\n    """"""\n    \xe7\x94\x9f\xe6\x88\x90\xe4\xb8\xad\xe6\x96\x87\xe5\xad\x97\xe7\xac\xa6\n    """"""\n    img=Image.new(""RGB"", (45,70),(255,255,255))\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 3),val,(0,0,0),font=f)\n    img =  img.resize((23,70))\n    A = np.array(img)\n    return A\n\ndef GenCh1(f,val):\n    """"""\n    \xe7\x94\x9f\xe6\x88\x90\xe8\x8b\xb1\xe6\x96\x87\xe5\xad\x97\xe7\xac\xa6\n    """"""\n    img=Image.new(""RGB"", (23,70),(255,255,255))\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 2),val.decode(\'utf-8\'),(0,0,0),font=f)\n    A = np.array(img)\n    return A\n\ndef AddGauss(img, level):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe9\xab\x98\xe6\x96\xaf\xe6\xa8\xa1\xe7\xb3\x8a\n    """""" \n    return cv2.blur(img, (level * 2 + 1, level * 2 + 1));\n\ndef r(val):\n    return int(np.random.random() * val)\n\ndef AddNoiseSingleChannel(single):\n    """"""\n    \xe6\xb7\xbb\xe5\x8a\xa0\xe9\xab\x98\xe6\x96\xaf\xe5\x99\xaa\xe5\xa3\xb0\n    """"""\n    diff = 255-single.max();\n    noise = np.random.normal(0,1+r(6),single.shape);\n    noise = (noise - noise.min())/(noise.max()-noise.min())\n    noise= diff*noise;\n    noise= noise.astype(np.uint8)\n    dst = single + noise\n    return dst\n\ndef addNoise(img,sdev = 0.5,avg=10):\n    img[:,:,0] =  AddNoiseSingleChannel(img[:,:,0]);\n    img[:,:,1] =  AddNoiseSingleChannel(img[:,:,1]);\n    img[:,:,2] =  AddNoiseSingleChannel(img[:,:,2]);\n    return img\n\n\nclass GenPlate:\n\n    def __init__(self,fontCh,fontEng,NoPlates):\n        self.fontC =  ImageFont.truetype(fontCh,43,0);\n        self.fontE =  ImageFont.truetype(fontEng,60,0);\n        self.img=np.array(Image.new(""RGB"", (226,70),(255,255,255)))\n        self.bg  = cv2.resize(cv2.imread(""./images/template.bmp""),(226,70));\n        self.smu = cv2.imread(""./images/smu2.jpg"");\n        self.noplates_path = [];\n        for parent,parent_folder,filenames in os.walk(NoPlates):\n            for filename in filenames:\n                path = parent+""/""+filename;\n                self.noplates_path.append(path);\n\n\n    def draw(self,val):\n        offset= 2 ;\n        self.img[0:70,offset+8:offset+8+23]= GenCh(self.fontC,val[0]);\n        self.img[0:70,offset+8+23+6:offset+8+23+6+23]= GenCh1(self.fontE,val[1]);\n        for i in range(5):\n            base = offset+8+23+6+23+17 +i*23 + i*6 ;\n            self.img[0:70, base  : base+23]= GenCh1(self.fontE,val[i+2]);\n        return self.img\n    \n    def generate(self,text):\n        if len(text) == 9:\n            fg = self.draw(text.decode(encoding=""utf-8""));\n            fg = cv2.bitwise_not(fg);\n            com = cv2.bitwise_or(fg,self.bg);\n            com = rot(com,r(60)-30,com.shape,30);\n            com = rotRandrom(com,10,(com.shape[1],com.shape[0]));\n            com = tfactor(com)\n            com = random_envirment(com,self.noplates_path);\n            com = AddGauss(com, 1+r(4));\n            com = addNoise(com);\n            return com\n\n    def genPlateString(self,pos,val):\n        \'\'\'\n\t\xe7\x94\x9f\xe6\x88\x90\xe8\xbd\xa6\xe7\x89\x8cString,\xe5\xad\x98\xe4\xb8\xba\xe5\x9b\xbe\xe7\x89\x87\n        \xe7\x94\x9f\xe6\x88\x90\xe8\xbd\xa6\xe7\x89\x8clist,\xe5\xad\x98\xe4\xb8\xbalabel\n        \'\'\'\n        plateStr = """";\n        plateList=[]\n        box = [0,0,0,0,0,0,0];\n        if(pos!=-1):\n            box[pos]=1;\n        for unit,cpos in zip(box,range(len(box))):\n            if unit == 1:\n                plateStr += val\n                #print plateStr\n                plateList.append(val)\n            else:\n                if cpos == 0:\n                    plateStr += chars[r(31)]\n                    plateList.append(plateStr)\n                elif cpos == 1:\n                    plateStr += chars[41+r(24)]\n                    plateList.append(plateStr)\n                else:\n                    plateStr += chars[31 + r(34)]\n                    plateList.append(plateStr)\n        plate = [plateList[0]]\n        b = [plateList[i][-1] for i in range(len(plateList))]\n        plate.extend(b[1:7])\n        return plateStr,plate\n\n    # \xe5\xb0\x86\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe8\xbd\xa6\xe7\x89\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84label\xe5\x86\x99\xe5\x85\xa5label.txt\n    def genBatch(self, batchSize,pos,charRange, outputPath,size):\n        if (not os.path.exists(outputPath)):\n            os.mkdir(outputPath)\n\toutfile = open(\'label.txt\',\'w\')\n        for i in xrange(batchSize):\n                plateStr,plate = G.genPlateString(-1,-1)\n                print plateStr,plate\n\t\timg =  G.generate(plateStr);\n                img = cv2.resize(img,size);\n                cv2.imwrite(outputPath + ""/"" + str(i).zfill(2) + "".jpg"", img);\n\t\toutfile.write(str(plate)+""\\n"")\nG = GenPlate(""./font/platech.ttf"",\'./font/platechar.ttf\',""./NoPlates"")\n#G.genBatch(100,2,range(31,65),""./plate_100"",(272,72))\n\nif __name__==\'__main__\':\n    G.genBatch(int(sys.argv[1]),2,range(31,65),sys.argv[2],(272,72))\n'"
3.image_classification/cifar_tf/cifar10.py,62,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builds the CIFAR-10 network.\n\nSummary of available functions:\n\n # Compute input images and labels for training. If you would like to run\n # evaluations, use inputs() instead.\n inputs, labels = distorted_inputs()\n\n # Compute inference on the model inputs to make a prediction.\n predictions = inference(inputs)\n\n # Compute the total loss of the prediction with respect to the labels.\n loss = loss(predictions, labels)\n\n # Create a graph to run one step of training with respect to the loss.\n train_op = train(loss, global_step)\n""""""\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport re\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nimport cifar10_input\n\nparser = argparse.ArgumentParser()\n\n# Basic model parameters.\nparser.add_argument(\'--batch_size\', type=int, default=128,\n                    help=\'Number of images to process in a batch.\')\n\nparser.add_argument(\'--data_dir\', type=str, default=\'/tmp/cifar10_data\',\n                    help=\'Path to the CIFAR-10 data directory.\')\n\nparser.add_argument(\'--use_fp16\', type=bool, default=False,\n                    help=\'Train the model using fp16.\')\n\nFLAGS = parser.parse_args()\n\n# Global constants describing the CIFAR-10 data set.\nIMAGE_SIZE = cifar10_input.IMAGE_SIZE\nNUM_CLASSES = cifar10_input.NUM_CLASSES\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n\n\n# Constants describing the training process.\nMOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\nNUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\nLEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\nINITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n\n# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n# to differentiate the operations. Note that this prefix is removed from the\n# names of the summaries when visualizing a model.\nTOWER_NAME = \'tower\'\n\nDATA_URL = \'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\'\n\n\ndef _activation_summary(x):\n  """"""Helper to create summaries for activations.\n\n  Creates a summary that provides a histogram of activations.\n  Creates a summary that measures the sparsity of activations.\n\n  Args:\n    x: Tensor\n  Returns:\n    nothing\n  """"""\n  # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n  # session. This helps the clarity of presentation on tensorboard.\n  tensor_name = re.sub(\'%s_[0-9]*/\' % TOWER_NAME, \'\', x.op.name)\n  tf.summary.histogram(tensor_name + \'/activations\', x)\n  tf.summary.scalar(tensor_name + \'/sparsity\',\n                                       tf.nn.zero_fraction(x))\n\n\ndef _variable_on_cpu(name, shape, initializer):\n  """"""Helper to create a Variable stored on CPU memory.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n\n  Returns:\n    Variable Tensor\n  """"""\n  with tf.device(\'/cpu:0\'):\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var\n\n\ndef _variable_with_weight_decay(name, shape, stddev, wd):\n  """"""Helper to create an initialized Variable with weight decay.\n\n  Note that the Variable is initialized with a truncated normal distribution.\n  A weight decay is added only if one is specified.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    stddev: standard deviation of a truncated Gaussian\n    wd: add L2Loss weight decay multiplied by this float. If None, weight\n        decay is not added for this Variable.\n\n  Returns:\n    Variable Tensor\n  """"""\n  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n  var = _variable_on_cpu(\n      name,\n      shape,\n      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n  if wd is not None:\n    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n    tf.add_to_collection(\'losses\', weight_decay)\n  return var\n\n\ndef distorted_inputs():\n  """"""Construct distorted input for CIFAR training using the Reader ops.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n\n  Raises:\n    ValueError: If no data_dir\n  """"""\n  if not FLAGS.data_dir:\n    raise ValueError(\'Please supply a data_dir\')\n  data_dir = os.path.join(FLAGS.data_dir, \'cifar-10-batches-bin\')\n  images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,\n                                                  batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels\n\n\ndef inputs(eval_data):\n  """"""Construct input for CIFAR evaluation using the Reader ops.\n\n  Args:\n    eval_data: bool, indicating if one should use the train or eval data set.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n\n  Raises:\n    ValueError: If no data_dir\n  """"""\n  if not FLAGS.data_dir:\n    raise ValueError(\'Please supply a data_dir\')\n  data_dir = os.path.join(FLAGS.data_dir, \'cifar-10-batches-bin\')\n  images, labels = cifar10_input.inputs(eval_data=eval_data,\n                                        data_dir=data_dir,\n                                        batch_size=FLAGS.batch_size)\n  if FLAGS.use_fp16:\n    images = tf.cast(images, tf.float16)\n    labels = tf.cast(labels, tf.float16)\n  return images, labels\n\n\ndef inference(images):\n  """"""Build the CIFAR-10 model.\n\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n\n  Returns:\n    Logits.\n  """"""\n  # We instantiate all variables using tf.get_variable() instead of\n  # tf.Variable() in order to share variables across multiple GPU training runs.\n  # If we only ran this model on a single GPU, we could simplify this function\n  # by replacing all instances of tf.get_variable() with tf.Variable().\n  #\n  # conv1\n  with tf.variable_scope(\'conv1\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[5, 5, 3, 64],\n                                         stddev=5e-2,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [64], tf.constant_initializer(0.0))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv1)\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding=\'SAME\', name=\'pool1\')\n  # norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name=\'norm1\')\n\n  # conv2\n  with tf.variable_scope(\'conv2\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[5, 5, 64, 64],\n                                         stddev=5e-2,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [64], tf.constant_initializer(0.1))\n    pre_activation = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n    _activation_summary(conv2)\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name=\'norm2\')\n  # pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool2\')\n\n  # local3\n  with tf.variable_scope(\'local3\') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n    dim = reshape.get_shape()[1].value\n    weights = _variable_with_weight_decay(\'weights\', shape=[dim, 384],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu(\'biases\', [384], tf.constant_initializer(0.1))\n    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n    _activation_summary(local3)\n\n  # local4\n  with tf.variable_scope(\'local4\') as scope:\n    weights = _variable_with_weight_decay(\'weights\', shape=[384, 192],\n                                          stddev=0.04, wd=0.004)\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.1))\n    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n    _activation_summary(local4)\n\n  # linear layer(WX + b),\n  # We don\'t apply softmax here because\n  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n  # and performs the softmax internally for efficiency.\n  with tf.variable_scope(\'softmax_linear\') as scope:\n    weights = _variable_with_weight_decay(\'weights\', [192, NUM_CLASSES],\n                                          stddev=1/192.0, wd=0.0)\n    biases = _variable_on_cpu(\'biases\', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n    _activation_summary(softmax_linear)\n\n  return softmax_linear\n\n\ndef loss(logits, labels):\n  """"""Add L2Loss to all the trainable variables.\n\n  Add summary for ""Loss"" and ""Loss/avg"".\n  Args:\n    logits: Logits from inference().\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n\n  Returns:\n    Loss tensor of type float.\n  """"""\n  # Calculate the average cross entropy loss across the batch.\n  labels = tf.cast(labels, tf.int64)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      labels=labels, logits=logits, name=\'cross_entropy_per_example\')\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n  tf.add_to_collection(\'losses\', cross_entropy_mean)\n\n  # The total loss is defined as the cross entropy loss plus all of the weight\n  # decay terms (L2 loss).\n  return tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\')\n\n\ndef _add_loss_summaries(total_loss):\n  """"""Add summaries for losses in CIFAR-10 model.\n\n  Generates moving average for all losses and associated summaries for\n  visualizing the performance of the network.\n\n  Args:\n    total_loss: Total loss from loss().\n  Returns:\n    loss_averages_op: op for generating moving averages of losses.\n  """"""\n  # Compute the moving average of all individual losses and the total loss.\n  loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n  losses = tf.get_collection(\'losses\')\n  loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n  # Attach a scalar summary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Name each loss as \'(raw)\' and name the moving average version of the loss\n    # as the original loss name.\n    tf.summary.scalar(l.op.name + \' (raw)\', l)\n    tf.summary.scalar(l.op.name, loss_averages.average(l))\n\n  return loss_averages_op\n\n\ndef train(total_loss, global_step):\n  """"""Train CIFAR-10 model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n  Returns:\n    train_op: op for training.\n  """"""\n  # Variables that affect learning rate.\n  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n\n  # Decay the learning rate exponentially based on the number of steps.\n  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n                                  global_step,\n                                  decay_steps,\n                                  LEARNING_RATE_DECAY_FACTOR,\n                                  staircase=True)\n  tf.summary.scalar(\'learning_rate\', lr)\n\n  # Generate moving averages of all losses and associated summaries.\n  loss_averages_op = _add_loss_summaries(total_loss)\n\n  # Compute gradients.\n  with tf.control_dependencies([loss_averages_op]):\n    opt = tf.train.GradientDescentOptimizer(lr)\n    grads = opt.compute_gradients(total_loss)\n\n  # Apply gradients.\n  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n  # Add histograms for trainable variables.\n  for var in tf.trainable_variables():\n    tf.summary.histogram(var.op.name, var)\n\n  # Add histograms for gradients.\n  for grad, var in grads:\n    if grad is not None:\n      tf.summary.histogram(var.op.name + \'/gradients\', grad)\n\n  # Track the moving averages of all trainable variables.\n  variable_averages = tf.train.ExponentialMovingAverage(\n      MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name=\'train\')\n\n  return train_op\n\n\ndef maybe_download_and_extract():\n  """"""Download and extract the tarball from Alex\'s website.""""""\n  dest_directory = FLAGS.data_dir\n  if not os.path.exists(dest_directory):\n    os.makedirs(dest_directory)\n  filename = DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dest_directory, filename)\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (filename,\n          float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  extracted_dir_path = os.path.join(dest_directory, \'cifar-10-batches-bin\')\n  if not os.path.exists(extracted_dir_path):\n    tarfile.open(filepath, \'r:gz\').extractall(dest_directory)\n'"
3.image_classification/cifar_tf/cifar10_eval.py,15,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Evaluation for CIFAR-10.\n\nAccuracy:\ncifar10_train.py achieves 83.0% accuracy after 100K steps (256 epochs\nof data) as judged by cifar10_eval.py.\n\nSpeed:\nOn a single Tesla K40, cifar10_train.py processes a single batch of 128 images\nin 0.25-0.35 sec (i.e. 350 - 600 images /sec). The model reaches ~86%\naccuracy after 100K steps in 8 hours of training time.\n\nUsage:\nPlease see the tutorial and website for how to download the CIFAR-10\ndata set, compile the program and train the model.\n\nhttp://tensorflow.org/tutorials/deep_cnn/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport math\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport cifar10\n\nparser = cifar10.parser\n\nparser.add_argument(\'--eval_dir\', type=str, default=\'/tmp/cifar10_eval\',\n                    help=\'Directory where to write event logs.\')\n\nparser.add_argument(\'--eval_data\', type=str, default=\'test\',\n                    help=\'Either `test` or `train_eval`.\')\n\nparser.add_argument(\'--checkpoint_dir\', type=str, default=\'/tmp/cifar10_train\',\n                    help=\'Directory where to read model checkpoints.\')\n\nparser.add_argument(\'--eval_interval_secs\', type=int, default=60*5,\n                    help=\'How often to run the eval.\')\n\nparser.add_argument(\'--num_examples\', type=int, default=10000,\n                    help=\'Number of examples to run.\')\n\nparser.add_argument(\'--run_once\', type=bool, default=False,\n                    help=\'Whether to run eval only once.\')\n\n\ndef eval_once(saver, summary_writer, top_k_op, summary_op):\n  """"""Run Eval once.\n\n  Args:\n    saver: Saver.\n    summary_writer: Summary writer.\n    top_k_op: Top K op.\n    summary_op: Summary op.\n  """"""\n  with tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      # Restores from checkpoint\n      saver.restore(sess, ckpt.model_checkpoint_path)\n      # Assuming model_checkpoint_path looks something like:\n      #   /my-favorite-path/cifar10_train/model.ckpt-0,\n      # extract global_step from it.\n      global_step = ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1]\n    else:\n      print(\'No checkpoint file found\')\n      return\n\n    # Start the queue runners.\n    coord = tf.train.Coordinator()\n    try:\n      threads = []\n      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n                                         start=True))\n\n      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n      true_count = 0  # Counts the number of correct predictions.\n      total_sample_count = num_iter * FLAGS.batch_size\n      step = 0\n      while step < num_iter and not coord.should_stop():\n        predictions = sess.run([top_k_op])\n        true_count += np.sum(predictions)\n        step += 1\n\n      # Compute precision @ 1.\n      precision = true_count / total_sample_count\n      print(\'%s: precision @ 1 = %.3f\' % (datetime.now(), precision))\n\n      summary = tf.Summary()\n      summary.ParseFromString(sess.run(summary_op))\n      summary.value.add(tag=\'Precision @ 1\', simple_value=precision)\n      summary_writer.add_summary(summary, global_step)\n    except Exception as e:  # pylint: disable=broad-except\n      coord.request_stop(e)\n\n    coord.request_stop()\n    coord.join(threads, stop_grace_period_secs=10)\n\n\ndef evaluate():\n  """"""Eval CIFAR-10 for a number of steps.""""""\n  with tf.Graph().as_default() as g:\n    # Get images and labels for CIFAR-10.\n    eval_data = FLAGS.eval_data == \'test\'\n    images, labels = cifar10.inputs(eval_data=eval_data)\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    logits = cifar10.inference(images)\n\n    # Calculate predictions.\n    top_k_op = tf.nn.in_top_k(logits, labels, 1)\n\n    # Restore the moving average version of the learned variables for eval.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        cifar10.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # Build the summary operation based on the TF collection of Summaries.\n    summary_op = tf.summary.merge_all()\n\n    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)\n\n    while True:\n      eval_once(saver, summary_writer, top_k_op, summary_op)\n      if FLAGS.run_once:\n        break\n      time.sleep(FLAGS.eval_interval_secs)\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  cifar10.maybe_download_and_extract()\n  if tf.gfile.Exists(FLAGS.eval_dir):\n    tf.gfile.DeleteRecursively(FLAGS.eval_dir)\n  tf.gfile.MakeDirs(FLAGS.eval_dir)\n  evaluate()\n\n\nif __name__ == \'__main__\':\n  FLAGS = parser.parse_args()\n  tf.app.run()\n'"
3.image_classification/cifar_tf/cifar10_input.py,24,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Routine for decoding the CIFAR-10 binary file format.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n# Process images of this size. Note that this differs from the original CIFAR\n# image size of 32 x 32. If one alters this number, then the entire model\n# architecture will change and any model would need to be retrained.\nIMAGE_SIZE = 24\n\n# Global constants describing the CIFAR-10 data set.\nNUM_CLASSES = 10\nNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\nNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n\n\ndef read_cifar10(filename_queue):\n  """"""Reads and parses examples from CIFAR10 data files.\n\n  Recommendation: if you want N-way read parallelism, call this function\n  N times.  This will give you N independent Readers reading different\n  files & positions within those files, which will give better mixing of\n  examples.\n\n  Args:\n    filename_queue: A queue of strings with the filenames to read from.\n\n  Returns:\n    An object representing a single example, with the following fields:\n      height: number of rows in the result (32)\n      width: number of columns in the result (32)\n      depth: number of color channels in the result (3)\n      key: a scalar string Tensor describing the filename & record number\n        for this example.\n      label: an int32 Tensor with the label in the range 0..9.\n      uint8image: a [height, width, depth] uint8 Tensor with the image data\n  """"""\n\n  class CIFAR10Record(object):\n    pass\n  result = CIFAR10Record()\n\n  # Dimensions of the images in the CIFAR-10 dataset.\n  # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n  # input format.\n  label_bytes = 1  # 2 for CIFAR-100\n  result.height = 32\n  result.width = 32\n  result.depth = 3\n  image_bytes = result.height * result.width * result.depth\n  # Every record consists of a label followed by the image, with a\n  # fixed number of bytes for each.\n  record_bytes = label_bytes + image_bytes\n\n  # Read a record, getting filenames from the filename_queue.  No\n  # header or footer in the CIFAR-10 format, so we leave header_bytes\n  # and footer_bytes at their default of 0.\n  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n  result.key, value = reader.read(filename_queue)\n\n  # Convert from a string to a vector of uint8 that is record_bytes long.\n  record_bytes = tf.decode_raw(value, tf.uint8)\n\n  # The first bytes represent the label, which we convert from uint8->int32.\n  result.label = tf.cast(\n      tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n\n  # The remaining bytes after the label represent the image, which we reshape\n  # from [depth * height * width] to [depth, height, width].\n  depth_major = tf.reshape(\n      tf.strided_slice(record_bytes, [label_bytes],\n                       [label_bytes + image_bytes]),\n      [result.depth, result.height, result.width])\n  # Convert from [depth, height, width] to [height, width, depth].\n  result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n\n  return result\n\n\ndef _generate_image_and_label_batch(image, label, min_queue_examples,\n                                    batch_size, shuffle):\n  """"""Construct a queued batch of images and labels.\n\n  Args:\n    image: 3-D Tensor of [height, width, 3] of type.float32.\n    label: 1-D Tensor of type.int32\n    min_queue_examples: int32, minimum number of samples to retain\n      in the queue that provides of batches of examples.\n    batch_size: Number of images per batch.\n    shuffle: boolean indicating whether to use a shuffling queue.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, height, width, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  """"""\n  # Create a queue that shuffles the examples, and then\n  # read \'batch_size\' images + labels from the example queue.\n  num_preprocess_threads = 16\n  if shuffle:\n    images, label_batch = tf.train.shuffle_batch(\n        [image, label],\n        batch_size=batch_size,\n        num_threads=num_preprocess_threads,\n        capacity=min_queue_examples + 3 * batch_size,\n        min_after_dequeue=min_queue_examples)\n  else:\n    images, label_batch = tf.train.batch(\n        [image, label],\n        batch_size=batch_size,\n        num_threads=num_preprocess_threads,\n        capacity=min_queue_examples + 3 * batch_size)\n\n  # Display the training images in the visualizer.\n  tf.summary.image(\'images\', images)\n\n  return images, tf.reshape(label_batch, [batch_size])\n\n\ndef distorted_inputs(data_dir, batch_size):\n  """"""Construct distorted input for CIFAR training using the Reader ops.\n\n  Args:\n    data_dir: Path to the CIFAR-10 data directory.\n    batch_size: Number of images per batch.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  """"""\n  filenames = [os.path.join(data_dir, \'data_batch_%d.bin\' % i)\n               for i in xrange(1, 6)]\n  for f in filenames:\n    if not tf.gfile.Exists(f):\n      raise ValueError(\'Failed to find file: \' + f)\n\n  # Create a queue that produces the filenames to read.\n  filename_queue = tf.train.string_input_producer(filenames)\n\n  # Read examples from files in the filename queue.\n  read_input = read_cifar10(filename_queue)\n  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n\n  height = IMAGE_SIZE\n  width = IMAGE_SIZE\n\n  # Image processing for training the network. Note the many random\n  # distortions applied to the image.\n\n  # Randomly crop a [height, width] section of the image.\n  distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n\n  # Randomly flip the image horizontally.\n  distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n  # Because these operations are not commutative, consider randomizing\n  # the order their operation.\n  # NOTE: since per_image_standardization zeros the mean and makes\n  # the stddev unit, this likely has no effect see tensorflow#1458.\n  distorted_image = tf.image.random_brightness(distorted_image,\n                                               max_delta=63)\n  distorted_image = tf.image.random_contrast(distorted_image,\n                                             lower=0.2, upper=1.8)\n\n  # Subtract off the mean and divide by the variance of the pixels.\n  float_image = tf.image.per_image_standardization(distorted_image)\n\n  # Set the shapes of tensors.\n  float_image.set_shape([height, width, 3])\n  read_input.label.set_shape([1])\n\n  # Ensure that the random shuffling has good mixing properties.\n  min_fraction_of_examples_in_queue = 0.4\n  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n                           min_fraction_of_examples_in_queue)\n  print (\'Filling queue with %d CIFAR images before starting to train. \'\n         \'This will take a few minutes.\' % min_queue_examples)\n\n  # Generate a batch of images and labels by building up a queue of examples.\n  return _generate_image_and_label_batch(float_image, read_input.label,\n                                         min_queue_examples, batch_size,\n                                         shuffle=True)\n\n\ndef inputs(eval_data, data_dir, batch_size):\n  """"""Construct input for CIFAR evaluation using the Reader ops.\n\n  Args:\n    eval_data: bool, indicating if one should use the train or eval data set.\n    data_dir: Path to the CIFAR-10 data directory.\n    batch_size: Number of images per batch.\n\n  Returns:\n    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n    labels: Labels. 1D tensor of [batch_size] size.\n  """"""\n  if not eval_data:\n    filenames = [os.path.join(data_dir, \'data_batch_%d.bin\' % i)\n                 for i in xrange(1, 6)]\n    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n  else:\n    filenames = [os.path.join(data_dir, \'test_batch.bin\')]\n    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n\n  for f in filenames:\n    if not tf.gfile.Exists(f):\n      raise ValueError(\'Failed to find file: \' + f)\n\n  # Create a queue that produces the filenames to read.\n  filename_queue = tf.train.string_input_producer(filenames)\n\n  # Read examples from files in the filename queue.\n  read_input = read_cifar10(filename_queue)\n  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n\n  height = IMAGE_SIZE\n  width = IMAGE_SIZE\n\n  # Image processing for evaluation.\n  # Crop the central [height, width] of the image.\n  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n                                                         height, width)\n\n  # Subtract off the mean and divide by the variance of the pixels.\n  float_image = tf.image.per_image_standardization(resized_image)\n\n  # Set the shapes of tensors.\n  float_image.set_shape([height, width, 3])\n  read_input.label.set_shape([1])\n\n  # Ensure that the random shuffling has good mixing properties.\n  min_fraction_of_examples_in_queue = 0.4\n  min_queue_examples = int(num_examples_per_epoch *\n                           min_fraction_of_examples_in_queue)\n\n  # Generate a batch of images and labels by building up a queue of examples.\n  return _generate_image_and_label_batch(float_image, read_input.label,\n                                         min_queue_examples, batch_size,\n                                         shuffle=False)\n'"
3.image_classification/cifar_tf/cifar10_multi_gpu_train.py,34,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A binary to train CIFAR-10 using multiple GPUs with synchronous updates.\n\nAccuracy:\ncifar10_multi_gpu_train.py achieves ~86% accuracy after 100K steps (256\nepochs of data) as judged by cifar10_eval.py.\n\nSpeed: With batch_size 128.\n\nSystem        | Step Time (sec/batch)  |     Accuracy\n--------------------------------------------------------------------\n1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n2 Tesla K20m  | 0.13-0.20              | ~84% at 30K steps  (2.5 hours)\n3 Tesla K20m  | 0.13-0.18              | ~84% at 30K steps\n4 Tesla K20m  | ~0.10                  | ~84% at 30K steps\n\nUsage:\nPlease see the tutorial and website for how to download the CIFAR-10\ndata set, compile the program and train the model.\n\nhttp://tensorflow.org/tutorials/deep_cnn/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os.path\nimport re\nimport time\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport cifar10\n\nparser = cifar10.parser\n\nparser.add_argument(\'--train_dir\', type=str, default=\'/tmp/cifar10_train\',\n                    help=\'Directory where to write event logs and checkpoint.\')\n\nparser.add_argument(\'--max_steps\', type=int, default=1000000,\n                    help=\'Number of batches to run.\')\n\nparser.add_argument(\'--num_gpus\', type=int, default=1,\n                    help=\'How many GPUs to use.\')\n\nparser.add_argument(\'--log_device_placement\', type=bool, default=False,\n                    help=\'Whether to log device placement.\')\n\n\ndef tower_loss(scope, images, labels):\n  """"""Calculate the total loss on a single tower running the CIFAR model.\n\n  Args:\n    scope: unique prefix string identifying the CIFAR tower, e.g. \'tower_0\'\n    images: Images. 4D tensor of shape [batch_size, height, width, 3].\n    labels: Labels. 1D tensor of shape [batch_size].\n\n  Returns:\n     Tensor of shape [] containing the total loss for a batch of data\n  """"""\n\n  # Build inference Graph.\n  logits = cifar10.inference(images)\n\n  # Build the portion of the Graph calculating the losses. Note that we will\n  # assemble the total_loss using a custom function below.\n  _ = cifar10.loss(logits, labels)\n\n  # Assemble all of the losses for the current tower only.\n  losses = tf.get_collection(\'losses\', scope)\n\n  # Calculate the total loss for the current tower.\n  total_loss = tf.add_n(losses, name=\'total_loss\')\n\n  # Attach a scalar summary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on tensorboard.\n    loss_name = re.sub(\'%s_[0-9]*/\' % cifar10.TOWER_NAME, \'\', l.op.name)\n    tf.summary.scalar(loss_name, l)\n\n  return total_loss\n\n\ndef average_gradients(tower_grads):\n  """"""Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  """"""\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n      # Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n      # Append on a \'tower\' dimension which we will average over below.\n      grads.append(expanded_g)\n\n    # Average over the \'tower\' dimension.\n    grad = tf.concat(axis=0, values=grads)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower\'s pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\n\ndef train():\n  """"""Train CIFAR-10 for a number of steps.""""""\n  with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n    # Create a variable to count the number of train() calls. This equals the\n    # number of batches processed * FLAGS.num_gpus.\n    global_step = tf.get_variable(\n        \'global_step\', [],\n        initializer=tf.constant_initializer(0), trainable=False)\n\n    # Calculate the learning rate schedule.\n    num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /\n                             FLAGS.batch_size)\n    decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)\n\n    # Decay the learning rate exponentially based on the number of steps.\n    lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE,\n                                    global_step,\n                                    decay_steps,\n                                    cifar10.LEARNING_RATE_DECAY_FACTOR,\n                                    staircase=True)\n\n    # Create an optimizer that performs gradient descent.\n    opt = tf.train.GradientDescentOptimizer(lr)\n\n    # Get images and labels for CIFAR-10.\n    images, labels = cifar10.distorted_inputs()\n    batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * FLAGS.num_gpus)\n    # Calculate the gradients for each model tower.\n    tower_grads = []\n    with tf.variable_scope(tf.get_variable_scope()):\n      for i in xrange(FLAGS.num_gpus):\n        with tf.device(\'/gpu:%d\' % i):\n          with tf.name_scope(\'%s_%d\' % (cifar10.TOWER_NAME, i)) as scope:\n            # Dequeues one batch for the GPU\n            image_batch, label_batch = batch_queue.dequeue()\n            # Calculate the loss for one tower of the CIFAR model. This function\n            # constructs the entire CIFAR model but shares the variables across\n            # all towers.\n            loss = tower_loss(scope, image_batch, label_batch)\n\n            # Reuse variables for the next tower.\n            tf.get_variable_scope().reuse_variables()\n\n            # Retain the summaries from the final tower.\n            summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n\n            # Calculate the gradients for the batch of data on this CIFAR tower.\n            grads = opt.compute_gradients(loss)\n\n            # Keep track of the gradients across all towers.\n            tower_grads.append(grads)\n\n    # We must calculate the mean of each gradient. Note that this is the\n    # synchronization point across all towers.\n    grads = average_gradients(tower_grads)\n\n    # Add a summary to track the learning rate.\n    summaries.append(tf.summary.scalar(\'learning_rate\', lr))\n\n    # Add histograms for gradients.\n    for grad, var in grads:\n      if grad is not None:\n        summaries.append(tf.summary.histogram(var.op.name + \'/gradients\', grad))\n\n    # Apply the gradients to adjust the shared variables.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    # Add histograms for trainable variables.\n    for var in tf.trainable_variables():\n      summaries.append(tf.summary.histogram(var.op.name, var))\n\n    # Track the moving averages of all trainable variables.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        cifar10.MOVING_AVERAGE_DECAY, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    # Group all updates to into a single train op.\n    train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.global_variables())\n\n    # Build the summary operation from the last tower summaries.\n    summary_op = tf.summary.merge(summaries)\n\n    # Build an initialization operation to run below.\n    init = tf.global_variables_initializer()\n\n    # Start running operations on the Graph. allow_soft_placement must be set to\n    # True to build towers on GPU, as some of the ops do not have GPU\n    # implementations.\n    sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=FLAGS.log_device_placement))\n    sess.run(init)\n\n    # Start the queue runners.\n    tf.train.start_queue_runners(sess=sess)\n\n    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)\n\n    for step in xrange(FLAGS.max_steps):\n      start_time = time.time()\n      _, loss_value = sess.run([train_op, loss])\n      duration = time.time() - start_time\n\n      assert not np.isnan(loss_value), \'Model diverged with loss = NaN\'\n\n      if step % 10 == 0:\n        num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus\n        examples_per_sec = num_examples_per_step / duration\n        sec_per_batch = duration / FLAGS.num_gpus\n\n        format_str = (\'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f \'\n                      \'sec/batch)\')\n        print (format_str % (datetime.now(), step, loss_value,\n                             examples_per_sec, sec_per_batch))\n\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n\n      # Save the model checkpoint periodically.\n      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n        checkpoint_path = os.path.join(FLAGS.train_dir, \'model.ckpt\')\n        saver.save(sess, checkpoint_path, global_step=step)\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  cifar10.maybe_download_and_extract()\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  train()\n\n\nif __name__ == \'__main__\':\n  FLAGS = parser.parse_args()\n  tf.app.run()\n'"
3.image_classification/cifar_tf/cifar10_train.py,13,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A binary to train CIFAR-10 using a single GPU.\n\nAccuracy:\ncifar10_train.py achieves ~86% accuracy after 100K steps (256 epochs of\ndata) as judged by cifar10_eval.py.\n\nSpeed: With batch_size 128.\n\nSystem        | Step Time (sec/batch)  |     Accuracy\n------------------------------------------------------------------\n1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n\nUsage:\nPlease see the tutorial and website for how to download the CIFAR-10\ndata set, compile the program and train the model.\n\nhttp://tensorflow.org/tutorials/deep_cnn/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport time\n\nimport tensorflow as tf\n\nimport cifar10\n\nparser = cifar10.parser\n\nparser.add_argument(\'--train_dir\', type=str, default=\'/tmp/cifar10_train\',\n                    help=\'Directory where to write event logs and checkpoint.\')\n\nparser.add_argument(\'--max_steps\', type=int, default=1000000,\n                    help=\'Number of batches to run.\')\n\nparser.add_argument(\'--log_device_placement\', type=bool, default=False,\n                    help=\'Whether to log device placement.\')\n\nparser.add_argument(\'--log_frequency\', type=int, default=10,\n                    help=\'How often to log results to the console.\')\n\n\ndef train():\n  """"""Train CIFAR-10 for a number of steps.""""""\n  with tf.Graph().as_default():\n    global_step = tf.train.get_or_create_global_step()\n\n    # Get images and labels for CIFAR-10.\n    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n    # GPU and resulting in a slow down.\n    with tf.device(\'/cpu:0\'):\n      images, labels = cifar10.distorted_inputs()\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    logits = cifar10.inference(images)\n\n    # Calculate loss.\n    loss = cifar10.loss(logits, labels)\n\n    # Build a Graph that trains the model with one batch of examples and\n    # updates the model parameters.\n    train_op = cifar10.train(loss, global_step)\n\n    class _LoggerHook(tf.train.SessionRunHook):\n      """"""Logs loss and runtime.""""""\n\n      def begin(self):\n        self._step = -1\n        self._start_time = time.time()\n\n      def before_run(self, run_context):\n        self._step += 1\n        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n\n      def after_run(self, run_context, run_values):\n        if self._step % FLAGS.log_frequency == 0:\n          current_time = time.time()\n          duration = current_time - self._start_time\n          self._start_time = current_time\n\n          loss_value = run_values.results\n          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n          sec_per_batch = float(duration / FLAGS.log_frequency)\n\n          format_str = (\'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f \'\n                        \'sec/batch)\')\n          print (format_str % (datetime.now(), self._step, loss_value,\n                               examples_per_sec, sec_per_batch))\n\n    with tf.train.MonitoredTrainingSession(\n        checkpoint_dir=FLAGS.train_dir,\n        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n               tf.train.NanTensorHook(loss),\n               _LoggerHook()],\n        config=tf.ConfigProto(\n            log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n      while not mon_sess.should_stop():\n        mon_sess.run(train_op)\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  cifar10.maybe_download_and_extract()\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  train()\n\n\nif __name__ == \'__main__\':\n  FLAGS = parser.parse_args()\n  tf.app.run()\n'"
3.image_classification/cifar_tf/vgg_tf.py,33,"b'# -*- coding: utf-8 -*-\n""""""\nCreated by huxiaoman 2017.12.12\nvgg_tf.py:\xe8\xae\xad\xe7\xbb\x83tensorflow\xe7\x89\x88\xe7\x9a\x84vgg16\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\xaf\xb9cifar-10shuju\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\n""""""\nfrom datetime import datetime\nimport math\nimport time\nimport tensorflow as tf\nimport cifar10\n\nbatch_size = 16\nnum_batches = 100\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe5\x87\xbd\xe6\x95\xb0\xe5\xaf\xb9\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n# input_op : \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae \n# name : \xe8\xaf\xa5\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xef\xbc\x8c\xe7\x94\xa8tf.name_scope()\xe6\x9d\xa5\xe5\x91\xbd\xe5\x90\x8d\n# kh,kw : \xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\n# n_out : \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n# dh,dw : \xe6\xad\xa5\xe9\x95\xbf\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\n# p \xef\xbc\x9a \xe6\x98\xaf\xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe5\xad\x98\xe5\x82\xa8VGG\xe6\x89\x80\xe7\x94\xa8\xe5\x88\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n# \xe9\x87\x87\xe7\x94\xa8xavier\xe6\x96\xb9\xe6\xb3\x95\xe5\xaf\xb9\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x9d\x83\xe5\x80\xbc\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\ndef conv_op(input_op, name, kh, kw, n_out, dh, dw, p):\n    n_in = input_op.get_shape()[-1].value # \xe8\x8e\xb7\xe5\xbe\x97\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    with tf.name_scope(name) as scope:\n        kernel = tf.get_variable(scope+\'w\',\n            shape = [kh, kw, n_in, n_out], dtype = tf.float32,\n            initializer = tf.contrib.layers.xavier_initializer_conv2d())\n        #  \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe8\xae\xa1\xe7\xae\x97\n        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding = \'SAME\')\n        bias_init_val = tf.constant(0.0, shape = [n_out], dtype = tf.float32)\n        biases = tf.Variable(bias_init_val, trainable = True, name = \'b\')\n        z = tf.nn.bias_add(conv, biases)\n        activation = tf.nn.relu(z, name = scope)\n        p += [kernel, biases]\n        return activation\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe5\x87\xbd\xe6\x95\xb0\xe5\xaf\xb9\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n# input_op : \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\n# name : \xe8\xaf\xa5\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\n# n_out : \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n# p : \xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x97\xe8\xa1\xa8 \n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xe7\x94\xa8 xavier\xe6\x96\xb9\xe6\xb3\x95\ndef fc_op(input_op, name, n_out, p):\n    n_in = input_op.get_shape()[-1].value\n\n    with tf.name_scope(name) as scope:\n        kernel = tf.get_variable(scope+\'w\',\n            shape = [n_in, n_out], dtype = tf.float32,\n            initializer = tf.contrib.layers.xavier_initializer())\n        biases = tf.Variable(tf.constant(0.1, shape = [n_out],\n            dtype = tf.float32), name = \'b\')\n        activation = tf.nn.relu_layer(input_op, kernel,  #  ???????????????\n            biases, name = scope)\n        p += [kernel, biases]\n        return activation \n\n# \xe5\xae\x9a\xe4\xb9\x89\xe5\x87\xbd\xe6\x95\xb0 \xe5\x88\x9b\xe5\xbb\xba maxpool\xe5\xb1\x82\n# input_op : \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae \n# name : \xe8\xaf\xa5\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xef\xbc\x8c\xe7\x94\xa8tf.name_scope()\xe6\x9d\xa5\xe5\x91\xbd\xe5\x90\x8d\n# kh,kw : \xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\n# dh,dw : \xe6\xad\xa5\xe9\x95\xbf\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\ndef mpool_op(input_op, name, kh, kw, dh, dw):\n    return tf.nn.max_pool(input_op, ksize = [1,kh,kw,1],\n        strides = [1, dh, dw, 1], padding = \'SAME\', name = name)\n\n#---------------\xe5\x88\x9b\xe5\xbb\xba VGG-16------------------\n\ndef inference_op(input_op, keep_prob):\n    p = []\n    # \xe7\xac\xac\xe4\xb8\x80\xe5\x9d\x97 conv1_1-conv1_2-pool1\n    conv1_1 = conv_op(input_op, name=\'conv1_1\', kh=3, kw=3,\n                n_out = 64, dh = 1, dw = 1, p = p)\n    conv1_2 = conv_op(conv1_1, name=\'conv1_2\', kh=3, kw=3,\n                n_out = 64, dh = 1, dw = 1, p = p)\n    pool1 = mpool_op(conv1_2, name = \'pool1\', kh = 2, kw = 2,\n                dw = 2, dh = 2)\n    # \xe7\xac\xac\xe4\xba\x8c\xe5\x9d\x97 conv2_1-conv2_2-pool2\n    conv2_1 = conv_op(pool1, name=\'conv2_1\', kh=3, kw=3,\n                n_out = 128, dh = 1, dw = 1, p = p)\n    conv2_2 = conv_op(conv2_1, name=\'conv2_2\', kh=3, kw=3,\n                n_out = 128, dh = 1, dw = 1, p = p)\n    pool2 = mpool_op(conv2_2, name = \'pool2\', kh = 2, kw = 2,\n                dw = 2, dh = 2)\n    # \xe7\xac\xac\xe4\xb8\x89\xe5\x9d\x97 conv3_1-conv3_2-conv3_3-pool3\n    conv3_1 = conv_op(pool2, name=\'conv3_1\', kh=3, kw=3,\n                n_out = 256, dh = 1, dw = 1, p = p)\n    conv3_2 = conv_op(conv3_1, name=\'conv3_2\', kh=3, kw=3,\n                n_out = 256, dh = 1, dw = 1, p = p)\n    conv3_3 = conv_op(conv3_2, name=\'conv3_3\', kh=3, kw=3,\n                n_out = 256, dh = 1, dw = 1, p = p)\n    pool3 = mpool_op(conv3_3, name = \'pool3\', kh = 2, kw = 2,\n                dw = 2, dh = 2)\n    # \xe7\xac\xac\xe5\x9b\x9b\xe5\x9d\x97 conv4_1-conv4_2-conv4_3-pool4\n    conv4_1 = conv_op(pool3, name=\'conv4_1\', kh=3, kw=3,\n                n_out = 512, dh = 1, dw = 1, p = p)\n    conv4_2 = conv_op(conv4_1, name=\'conv4_2\', kh=3, kw=3,\n                n_out = 512, dh = 1, dw = 1, p = p)\n    conv4_3 = conv_op(conv4_2, name=\'conv4_3\', kh=3, kw=3,\n                n_out = 512, dh = 1, dw = 1, p = p)\n    pool4 = mpool_op(conv4_3, name = \'pool4\', kh = 2, kw = 2,\n                dw = 2, dh = 2)\n    # \xe7\xac\xac\xe4\xba\x94\xe5\x9d\x97 conv5_1-conv5_2-conv5_3-pool5\n    conv5_1 = conv_op(pool4, name=\'conv5_1\', kh=3, kw=3,\n                n_out = 512, dh = 1, dw = 1, p = p)\n    conv5_2 = conv_op(conv5_1, name=\'conv5_2\', kh=3, kw=3,\n                n_out = 512, dh = 1, dw = 1, p = p)\n    conv5_3 = conv_op(conv5_2, name=\'conv5_3\', kh=3, kw=3,\n                n_out = 512, dh = 1, dw = 1, p = p)\n    pool5 = mpool_op(conv5_3, name = \'pool5\', kh = 2, kw = 2,\n                dw = 2, dh = 2)\n    # \xe6\x8a\x8apool5 ( [7, 7, 512] )  \xe6\x8b\x89\xe6\x88\x90\xe5\x90\x91\xe9\x87\x8f\n    shp  = pool5.get_shape()\n    flattened_shape = shp[1].value * shp[2].value * shp[3].value\n    resh1 = tf.reshape(pool5, [-1, flattened_shape], name = \'resh1\')\n\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x821 \xe6\xb7\xbb\xe5\x8a\xa0\xe4\xba\x86 Droput\xe6\x9d\xa5\xe9\x98\xb2\xe6\xad\xa2\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88    \n    fc1 = fc_op(resh1, name = \'fc1\', n_out = 2048, p = p)\n    fc1_drop = tf.nn.dropout(fc1, keep_prob, name = \'fc1_drop\')\n\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x822 \xe6\xb7\xbb\xe5\x8a\xa0\xe4\xba\x86 Droput\xe6\x9d\xa5\xe9\x98\xb2\xe6\xad\xa2\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88    \n    fc2 = fc_op(fc1_drop, name = \'fc2\', n_out = 2048, p = p)\n    fc2_drop = tf.nn.dropout(fc2, keep_prob, name = \'fc2_drop\')\n\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x823 \xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaasoftmax\xe6\xb1\x82\xe7\xbb\x99\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\n    fc3 = fc_op(fc2_drop, name = \'fc3\', n_out = 1000, p = p)\n    softmax = tf.nn.softmax(fc3)\n    predictions = tf.argmax(softmax, 1)\n    return predictions, softmax, fc3, p\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe8\xaf\x84\xe6\xb5\x8b\xe5\x87\xbd\xe6\x95\xb0\n\ndef time_tensorflow_run(session, target, feed, info_string):\n    num_steps_burn_in = 10\n    total_duration = 0.0\n    total_duration_squared = 0.0\n\n    for i in range(num_batches + num_steps_burn_in):\n        start_time = time.time()\n        _ = session.run(target, feed_dict = feed)\n        duration = time.time() - start_time\n        if i >= num_steps_burn_in:\n            if not i  % 10: \n                print(\'%s: step %d, duration = %.3f\' % \n                    (datetime.now(), i-num_steps_burn_in, duration))\n            total_duration += duration\n            total_duration_squared += duration * duration\n    mean_dur = total_duration / num_batches \n    var_dur = total_duration_squared / num_batches - mean_dur * mean_dur\n    std_dur = math.sqrt(var_dur)\n    print(\'%s: %s across %d steps, %.3f +/- %.3f sec / batch\' %(datetime.now(), info_string, num_batches, mean_dur, std_dur))\n\n\ndef train_vgg16():\n    with tf.Graph().as_default():\n        image_size = 224  # \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb0\xba\xe5\xaf\xb8\n        # \xe7\x94\x9f\xe6\x88\x90\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe6\xb5\x8b\xe8\xaf\x95\xe6\x98\xaf\xe5\x90\xa6\xe8\x83\xbd\xe8\xb7\x91\xe9\x80\x9a\n        #images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3], dtype=tf.float32, stddev=1e-1))\n        with tf.device(\'/cpu:0\'):\n            images, labels = cifar10.distorted_inputs()\n        keep_prob = tf.placeholder(tf.float32)\n        prediction,softmax,fc8,p = inference_op(images,keep_prob)\n        init = tf.global_variables_initializer()\n        sess = tf.Session()\n        sess.run(init)\n        time_tensorflow_run(sess, prediction,{keep_prob:1.0}, ""Forward"")\n        # \xe7\x94\xa8\xe4\xbb\xa5\xe6\xa8\xa1\xe6\x8b\x9f\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xbf\x87\xe7\xa8\x8b\n        objective = tf.nn.l2_loss(fc8)  # \xe7\xbb\x99\xe4\xb8\x80\xe4\xb8\xaaloss\n        grad = tf.gradients(objective, p)  # \xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8eloss\xe7\x9a\x84 \xe6\x89\x80\xe6\x9c\x89\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        time_tensorflow_run(sess, grad, {keep_prob:0.5},""Forward-backward"")\n\n\n\n\nif __name__ == \'__main__\':\n    train_vgg16()\n'"
4.image_project/face_recognition/genImage.py,0,"b""import cv2\nimport dlib\nimport os\nimport sys\nimport random\n\noutput_dir = './my_faces'\nsize = 64\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# \xe6\x94\xb9\xe5\x8f\x98\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe4\xba\xae\xe5\xba\xa6\xe4\xb8\x8e\xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\ndef relight(img, light=1, bias=0):\n    w = img.shape[1]\n    h = img.shape[0]\n    #image = []\n    for i in range(0,w):\n        for j in range(0,h):\n            for c in range(3):\n                tmp = int(img[j,i,c]*light + bias)\n                if tmp > 255:\n                    tmp = 255\n                elif tmp < 0:\n                    tmp = 0\n                img[j,i,c] = tmp\n    return img\n\n#\xe4\xbd\xbf\xe7\x94\xa8dlib\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84frontal_face_detector\xe4\xbd\x9c\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe5\x99\xa8\ndetector = dlib.get_frontal_face_detector()\n# \xe6\x89\x93\xe5\xbc\x80\xe6\x91\x84\xe5\x83\x8f\xe5\xa4\xb4 \xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe6\xb5\x81\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xb8\xba\xe6\x91\x84\xe5\x83\x8f\xe5\xa4\xb4\xe6\x88\x96\xe8\xa7\x86\xe9\xa2\x91\xe6\x96\x87\xe4\xbb\xb6\ncamera = cv2.VideoCapture(0)\n\nindex = 1\nwhile True:\n    if (index <= 10000):\n        print('Being processed picture %s' % index)\n        # \xe4\xbb\x8e\xe6\x91\x84\xe5\x83\x8f\xe5\xa4\xb4\xe8\xaf\xbb\xe5\x8f\x96\xe7\x85\xa7\xe7\x89\x87\n        success, img = camera.read()\n        # \xe8\xbd\xac\xe4\xb8\xba\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xe7\x89\x87\n        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        # \xe4\xbd\xbf\xe7\x94\xa8detector\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xba\xba\xe8\x84\xb8\xe6\xa3\x80\xe6\xb5\x8b\n        dets = detector(gray_img, 1)\n\n        for i, d in enumerate(dets):\n            x1 = d.top() if d.top() > 0 else 0\n            y1 = d.bottom() if d.bottom() > 0 else 0\n            x2 = d.left() if d.left() > 0 else 0\n            y2 = d.right() if d.right() > 0 else 0\n\n            face = img[x1:y1,x2:y2]\n            # \xe8\xb0\x83\xe6\x95\xb4\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\xe4\xb8\x8e\xe4\xba\xae\xe5\xba\xa6\xef\xbc\x8c \xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\xe4\xb8\x8e\xe4\xba\xae\xe5\xba\xa6\xe5\x80\xbc\xe9\x83\xbd\xe5\x8f\x96\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe8\x83\xbd\xe5\xa2\x9e\xe5\x8a\xa0\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe5\xa4\x9a\xe6\xa0\xb7\xe6\x80\xa7\n            face = relight(face, random.uniform(0.5, 1.5), random.randint(-50, 50))\n\n            face = cv2.resize(face, (size,size))\n\n            cv2.imshow('image', face)\n\n            cv2.imwrite(output_dir+'/'+str(index)+'.jpg', face)\n\n            index += 1\n        key = cv2.waitKey(30) & 0xff\n        if key == 27:\n            break\n    else:\n        print('Finished!')\n        break\n"""
4.image_project/face_recognition/genImageMore.py,0,"b""# -*- codeing: utf-8 -*-\nimport sys\nimport os\nimport cv2\nimport dlib\n\ninput_dir = './input_img'\noutput_dir = './other_faces'\nsize = 64\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n#\xe4\xbd\xbf\xe7\x94\xa8dlib\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84frontal_face_detector\xe4\xbd\x9c\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe5\x99\xa8\ndetector = dlib.get_frontal_face_detector()\n\nindex = 1\nfor (path, dirnames, filenames) in os.walk(input_dir):\n    for filename in filenames:\n        if filename.endswith('.jpg'):\n            print('Being processed picture %s' % index)\n            img_path = path+'/'+filename\n            # \xe4\xbb\x8e\xe6\x96\x87\xe4\xbb\xb6\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\n            img = cv2.imread(img_path)\n            # \xe8\xbd\xac\xe4\xb8\xba\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xe7\x89\x87\n            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            # \xe4\xbd\xbf\xe7\x94\xa8detector\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xba\xba\xe8\x84\xb8\xe6\xa3\x80\xe6\xb5\x8b dets\xe4\xb8\xba\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n            dets = detector(gray_img, 1)\n\n            #\xe4\xbd\xbf\xe7\x94\xa8enumerate \xe5\x87\xbd\xe6\x95\xb0\xe9\x81\x8d\xe5\x8e\x86\xe5\xba\x8f\xe5\x88\x97\xe4\xb8\xad\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xae\x83\xe4\xbb\xac\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\n            #\xe4\xb8\x8b\xe6\xa0\x87i\xe5\x8d\xb3\xe4\xb8\xba\xe4\xba\xba\xe8\x84\xb8\xe5\xba\x8f\xe5\x8f\xb7\n            #left\xef\xbc\x9a\xe4\xba\xba\xe8\x84\xb8\xe5\xb7\xa6\xe8\xbe\xb9\xe8\xb7\x9d\xe7\xa6\xbb\xe5\x9b\xbe\xe7\x89\x87\xe5\xb7\xa6\xe8\xbe\xb9\xe7\x95\x8c\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb \xef\xbc\x9bright\xef\xbc\x9a\xe4\xba\xba\xe8\x84\xb8\xe5\x8f\xb3\xe8\xbe\xb9\xe8\xb7\x9d\xe7\xa6\xbb\xe5\x9b\xbe\xe7\x89\x87\xe5\xb7\xa6\xe8\xbe\xb9\xe7\x95\x8c\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb \n            #top\xef\xbc\x9a\xe4\xba\xba\xe8\x84\xb8\xe4\xb8\x8a\xe8\xbe\xb9\xe8\xb7\x9d\xe7\xa6\xbb\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\xe8\xbe\xb9\xe7\x95\x8c\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb \xef\xbc\x9bbottom\xef\xbc\x9a\xe4\xba\xba\xe8\x84\xb8\xe4\xb8\x8b\xe8\xbe\xb9\xe8\xb7\x9d\xe7\xa6\xbb\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\xe8\xbe\xb9\xe7\x95\x8c\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\n            for i, d in enumerate(dets):\n                x1 = d.top() if d.top() > 0 else 0\n                y1 = d.bottom() if d.bottom() > 0 else 0\n                x2 = d.left() if d.left() > 0 else 0\n                y2 = d.right() if d.right() > 0 else 0\n                # img[y:y+h,x:x+w]\n                face = img[x1:y1,x2:y2]\n                # \xe8\xb0\x83\xe6\x95\xb4\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\n                face = cv2.resize(face, (size,size))\n                cv2.imshow('image',face)\n                # \xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\n                cv2.imwrite(output_dir+'/'+str(index)+'.jpg', face)\n                index += 1\n\n            key = cv2.waitKey(30) & 0xff\n            if key == 27:\n                sys.exit(0)\n"""
4.image_project/face_recognition/model.py,21,"b""import tensorflow as tf\nimport cv2\nimport dlib\nimport numpy as np\nimport os\nimport random\nimport sys\nfrom sklearn.model_selection import train_test_split\n\nmy_faces_path = './my_faces'\nother_faces_path = './other_faces'\nsize = 64\n\nimgs = []\nlabs = []\n\ndef getPaddingSize(img):\n    h, w, _ = img.shape\n    top, bottom, left, right = (0,0,0,0)\n    longest = max(h, w)\n\n    if w < longest:\n        tmp = longest - w\n        # //\xe8\xa1\xa8\xe7\xa4\xba\xe6\x95\xb4\xe9\x99\xa4\xe7\xac\xa6\xe5\x8f\xb7\n        left = tmp // 2\n        right = tmp - left\n    elif h < longest:\n        tmp = longest - h\n        top = tmp // 2\n        bottom = tmp - top\n    else:\n        pass\n    return top, bottom, left, right\n\ndef readData(path , h=size, w=size):\n    for filename in os.listdir(path):\n        if filename.endswith('.jpg'):\n            filename = path + '/' + filename\n\n            img = cv2.imread(filename)\n\n            top,bottom,left,right = getPaddingSize(img)\n            # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x94\xbe\xe5\xa4\xa7\xef\xbc\x8c \xe6\x89\xa9\xe5\x85\x85\xe5\x9b\xbe\xe7\x89\x87\xe8\xbe\xb9\xe7\xbc\x98\xe9\x83\xa8\xe5\x88\x86\n            img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0,0,0])\n            img = cv2.resize(img, (h, w))\n\n            imgs.append(img)\n            labs.append(path)\n\nreadData(my_faces_path)\nreadData(other_faces_path)\n# \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbe\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe6\x95\xb0\xe7\xbb\x84\nimgs = np.array(imgs)\nlabs = np.array([[0,1] if lab == my_faces_path else [1,0] for lab in labs])\n# \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x92\xe5\x88\x86\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8e\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\ntrain_x,test_x,train_y,test_y = train_test_split(imgs, labs, test_size=0.05, random_state=random.randint(0,100))\n# \xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x80\xbb\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe9\xab\x98\xe3\x80\x81\xe5\xae\xbd\xe3\x80\x81\xe9\x80\x9a\xe9\x81\x93\ntrain_x = train_x.reshape(train_x.shape[0], size, size, 3)\ntest_x = test_x.reshape(test_x.shape[0], size, size, 3)\n# \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe5\xb0\x8f\xe4\xba\x8e1\xe7\x9a\x84\xe6\x95\xb0\ntrain_x = train_x.astype('float32')/255.0\ntest_x = test_x.astype('float32')/255.0\n\nprint('train size:%s, test size:%s' % (len(train_x), len(test_x)))\n# \xe5\x9b\xbe\xe7\x89\x87\xe5\x9d\x97\xef\xbc\x8c\xe6\xaf\x8f\xe6\xac\xa1\xe5\x8f\x96128\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\nbatch_size = 128\nnum_batch = len(train_x) // 128\n\nx = tf.placeholder(tf.float32, [None, size, size, 3])\ny_ = tf.placeholder(tf.float32, [None, 2])\n\nkeep_prob_5 = tf.placeholder(tf.float32)\nkeep_prob_75 = tf.placeholder(tf.float32)\n\ndef weightVariable(shape):\n    init = tf.random_normal(shape, stddev=0.01)\n    return tf.Variable(init)\n\ndef biasVariable(shape):\n    init = tf.random_normal(shape)\n    return tf.Variable(init)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n\ndef maxPool(x):\n    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n\ndef dropout(x, keep):\n    return tf.nn.dropout(x, keep)\n\ndef cnnLayer():\n    # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\n    W1 = weightVariable([3,3,3,32]) # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f(3,3)\xef\xbc\x8c \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93(3)\xef\xbc\x8c \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93(32)\n    b1 = biasVariable([32])\n    # \xe5\x8d\xb7\xe7\xa7\xaf\n    conv1 = tf.nn.relu(conv2d(x, W1) + b1)\n    # \xe6\xb1\xa0\xe5\x8c\x96\n    pool1 = maxPool(conv1)\n    # \xe5\x87\x8f\xe5\xb0\x91\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\xef\xbc\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe8\xae\xa9\xe6\x9f\x90\xe4\xba\x9b\xe6\x9d\x83\xe9\x87\x8d\xe4\xb8\x8d\xe6\x9b\xb4\xe6\x96\xb0\n    drop1 = dropout(pool1, keep_prob_5)\n\n    # \xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\n    W2 = weightVariable([3,3,32,64])\n    b2 = biasVariable([64])\n    conv2 = tf.nn.relu(conv2d(drop1, W2) + b2)\n    pool2 = maxPool(conv2)\n    drop2 = dropout(pool2, keep_prob_5)\n\n    # \xe7\xac\xac\xe4\xb8\x89\xe5\xb1\x82\n    W3 = weightVariable([3,3,64,64])\n    b3 = biasVariable([64])\n    conv3 = tf.nn.relu(conv2d(drop2, W3) + b3)\n    pool3 = maxPool(conv3)\n    drop3 = dropout(pool3, keep_prob_5)\n\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    Wf = weightVariable([8*16*32, 512])\n    bf = biasVariable([512])\n    drop3_flat = tf.reshape(drop3, [-1, 8*16*32])\n    dense = tf.nn.relu(tf.matmul(drop3_flat, Wf) + bf)\n    dropf = dropout(dense, keep_prob_75)\n\n    # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n    Wout = weightVariable([512,2])\n    bout = weightVariable([2])\n    out = tf.add(tf.matmul(dropf, Wout), bout)\n    return out\n\noutput = cnnLayer()  \npredict = tf.argmax(output, 1)  \n   \nsaver = tf.train.Saver()  \nsess = tf.Session()  \nsaver.restore(sess, tf.train.latest_checkpoint('.'))  \n   \ndef is_my_face(image):  \n    res = sess.run(predict, feed_dict={x: [image/255.0], keep_prob_5:1.0, keep_prob_75: 1.0})  \n    if res[0] == 1:  \n        return True  \n    else:  \n        return False  \n\n#\xe4\xbd\xbf\xe7\x94\xa8dlib\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84frontal_face_detector\xe4\xbd\x9c\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe5\x99\xa8\ndetector = dlib.get_frontal_face_detector()\n\ncam = cv2.VideoCapture(0)  \n   \nwhile True:  \n    _, img = cam.read()  \n    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    dets = detector(gray_image, 1)\n    if not len(dets):\n        #print('Can`t get face.')\n        cv2.imshow('img', img)\n        key = cv2.waitKey(30) & 0xff  \n        if key == 27:\n            sys.exit(0)\n            \n    for i, d in enumerate(dets):\n        x1 = d.top() if d.top() > 0 else 0\n        y1 = d.bottom() if d.bottom() > 0 else 0\n        x2 = d.left() if d.left() > 0 else 0\n        y2 = d.right() if d.right() > 0 else 0\n        face = img[x1:y1,x2:y2]\n        # \xe8\xb0\x83\xe6\x95\xb4\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\n        face = cv2.resize(face, (size,size))\n        print('Is this my face? %s' % is_my_face(face))\n\n        cv2.rectangle(img, (x2,x1),(y2,y1), (255,0,0),3)\n        cv2.imshow('image',img)\n        key = cv2.waitKey(30) & 0xff\n        if key == 27:\n            sys.exit(0)\n  \nsess.close() \n"""
4.image_project/face_recognition/train.py,29,"b""import tensorflow as tf\nimport cv2\nimport numpy as np\nimport os\nimport random\nimport sys\nfrom sklearn.model_selection import train_test_split\n\nmy_faces_path = './my_faces'\nother_faces_path = './other_faces'\nsize = 64\n\nimgs = []\nlabs = []\n\ndef getPaddingSize(img):\n    h, w, _ = img.shape\n    top, bottom, left, right = (0,0,0,0)\n    longest = max(h, w)\n\n    if w < longest:\n        tmp = longest - w\n        # //\xe8\xa1\xa8\xe7\xa4\xba\xe6\x95\xb4\xe9\x99\xa4\xe7\xac\xa6\xe5\x8f\xb7\n        left = tmp // 2\n        right = tmp - left\n    elif h < longest:\n        tmp = longest - h\n        top = tmp // 2\n        bottom = tmp - top\n    else:\n        pass\n    return top, bottom, left, right\n\ndef readData(path , h=size, w=size):\n    for filename in os.listdir(path):\n        if filename.endswith('.jpg'):\n            filename = path + '/' + filename\n\n            img = cv2.imread(filename)\n\n            top,bottom,left,right = getPaddingSize(img)\n            # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x94\xbe\xe5\xa4\xa7\xef\xbc\x8c \xe6\x89\xa9\xe5\x85\x85\xe5\x9b\xbe\xe7\x89\x87\xe8\xbe\xb9\xe7\xbc\x98\xe9\x83\xa8\xe5\x88\x86\n            img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0,0,0])\n            img = cv2.resize(img, (h, w))\n\n            imgs.append(img)\n            labs.append(path)\n\nreadData(my_faces_path)\nreadData(other_faces_path)\n# \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbe\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe6\x95\xb0\xe7\xbb\x84\nimgs = np.array(imgs)\nlabs = np.array([[0,1] if lab == my_faces_path else [1,0] for lab in labs])\n# \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x92\xe5\x88\x86\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8e\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\ntrain_x,test_x,train_y,test_y = train_test_split(imgs, labs, test_size=0.05, random_state=random.randint(0,100))\n# \xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x80\xbb\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe9\xab\x98\xe3\x80\x81\xe5\xae\xbd\xe3\x80\x81\xe9\x80\x9a\xe9\x81\x93\ntrain_x = train_x.reshape(train_x.shape[0], size, size, 3)\ntest_x = test_x.reshape(test_x.shape[0], size, size, 3)\n# \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe5\xb0\x8f\xe4\xba\x8e1\xe7\x9a\x84\xe6\x95\xb0\ntrain_x = train_x.astype('float32')/255.0\ntest_x = test_x.astype('float32')/255.0\n\nprint('train size:%s, test size:%s' % (len(train_x), len(test_x)))\n# \xe5\x9b\xbe\xe7\x89\x87\xe5\x9d\x97\xef\xbc\x8c\xe6\xaf\x8f\xe6\xac\xa1\xe5\x8f\x96100\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\nbatch_size = 100\nnum_batch = len(train_x) // batch_size\n\nx = tf.placeholder(tf.float32, [None, size, size, 3])\ny_ = tf.placeholder(tf.float32, [None, 2])\n\nkeep_prob_5 = tf.placeholder(tf.float32)\nkeep_prob_75 = tf.placeholder(tf.float32)\n\ndef weightVariable(shape):\n    init = tf.random_normal(shape, stddev=0.01)\n    return tf.Variable(init)\n\ndef biasVariable(shape):\n    init = tf.random_normal(shape)\n    return tf.Variable(init)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n\ndef maxPool(x):\n    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n\ndef dropout(x, keep):\n    return tf.nn.dropout(x, keep)\n\ndef cnnLayer():\n    # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\n    W1 = weightVariable([3,3,3,32]) # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f(3,3)\xef\xbc\x8c \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93(3)\xef\xbc\x8c \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93(32)\n    b1 = biasVariable([32])\n    # \xe5\x8d\xb7\xe7\xa7\xaf\n    conv1 = tf.nn.relu(conv2d(x, W1) + b1)\n    # \xe6\xb1\xa0\xe5\x8c\x96\n    pool1 = maxPool(conv1)\n    # \xe5\x87\x8f\xe5\xb0\x91\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\xef\xbc\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe8\xae\xa9\xe6\x9f\x90\xe4\xba\x9b\xe6\x9d\x83\xe9\x87\x8d\xe4\xb8\x8d\xe6\x9b\xb4\xe6\x96\xb0\n    drop1 = dropout(pool1, keep_prob_5)\n\n    # \xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\n    W2 = weightVariable([3,3,32,64])\n    b2 = biasVariable([64])\n    conv2 = tf.nn.relu(conv2d(drop1, W2) + b2)\n    pool2 = maxPool(conv2)\n    drop2 = dropout(pool2, keep_prob_5)\n\n    # \xe7\xac\xac\xe4\xb8\x89\xe5\xb1\x82\n    W3 = weightVariable([3,3,64,64])\n    b3 = biasVariable([64])\n    conv3 = tf.nn.relu(conv2d(drop2, W3) + b3)\n    pool3 = maxPool(conv3)\n    drop3 = dropout(pool3, keep_prob_5)\n\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    Wf = weightVariable([8*8*64, 512])\n    bf = biasVariable([512])\n    drop3_flat = tf.reshape(drop3, [-1, 8*8*64])\n    dense = tf.nn.relu(tf.matmul(drop3_flat, Wf) + bf)\n    dropf = dropout(dense, keep_prob_75)\n\n    # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n    Wout = weightVariable([512,2])\n    bout = weightVariable([2])\n    #out = tf.matmul(dropf, Wout) + bout\n    out = tf.add(tf.matmul(dropf, Wout), bout)\n    return out\n\ndef cnnTrain():\n    out = cnnLayer()\n\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=y_))\n\n    train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n    # \xe6\xaf\x94\xe8\xbe\x83\xe6\xa0\x87\xe7\xad\xbe\xe6\x98\xaf\xe5\x90\xa6\xe7\x9b\xb8\xe7\xad\x89\xef\xbc\x8c\xe5\x86\x8d\xe6\xb1\x82\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x95\xb0\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x8ctf.cast(\xe5\xbc\xba\xe5\x88\xb6\xe8\xbd\xac\xe6\x8d\xa2\xe7\xb1\xbb\xe5\x9e\x8b)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out, 1), tf.argmax(y_, 1)), tf.float32))\n    # \xe5\xb0\x86loss\xe4\xb8\x8eaccuracy\xe4\xbf\x9d\xe5\xad\x98\xe4\xbb\xa5\xe4\xbe\x9btensorboard\xe4\xbd\xbf\xe7\x94\xa8\n    tf.summary.scalar('loss', cross_entropy)\n    tf.summary.scalar('accuracy', accuracy)\n    merged_summary_op = tf.summary.merge_all()\n    # \xe6\x95\xb0\xe6\x8d\xae\xe4\xbf\x9d\xe5\xad\x98\xe5\x99\xa8\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n\n        sess.run(tf.global_variables_initializer())\n\n        summary_writer = tf.summary.FileWriter('./tmp', graph=tf.get_default_graph())\n\n        for n in range(10):\n             # \xe6\xaf\x8f\xe6\xac\xa1\xe5\x8f\x96128(batch_size)\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\n            for i in range(num_batch):\n                batch_x = train_x[i*batch_size : (i+1)*batch_size]\n                batch_y = train_y[i*batch_size : (i+1)*batch_size]\n                # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe8\xae\xad\xe7\xbb\x83\xe4\xb8\x89\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x89\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\n                _,loss,summary = sess.run([train_step, cross_entropy, merged_summary_op],\n                                           feed_dict={x:batch_x,y_:batch_y, keep_prob_5:0.5,keep_prob_75:0.75})\n                summary_writer.add_summary(summary, n*num_batch+i)\n                # \xe6\x89\x93\xe5\x8d\xb0\xe6\x8d\x9f\xe5\xa4\xb1\n                print(n*num_batch+i, loss)\n\n                if (n*num_batch+i) % 100 == 0:\n                    # \xe8\x8e\xb7\xe5\x8f\x96\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n                    acc = accuracy.eval({x:test_x, y_:test_y, keep_prob_5:1.0, keep_prob_75:1.0})\n                    print(n*num_batch+i, acc)\n                    # \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe5\xa4\xa7\xe4\xba\x8e0.98\xe6\x97\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\xb9\xb6\xe9\x80\x80\xe5\x87\xba\n                    if acc > 0.98 and n > 2:\n                        saver.save(sess, './train_faces.model', global_step=n*num_batch+i)\n                        sys.exit(0)\n        print('accuracy less 0.98, exited!')\n\ncnnTrain()\n"""
