file_path,api_count,code
run_test.py,2,"b'import tensorflow as tf\nimport os\nimport utils\nimport style_transfer_tester\nimport argparse\nimport time\n\n""""""parsing and configuration""""""\ndef parse_args():\n    desc = ""Tensorflow implementation of \'Perceptual Losses for Real-Time Style Transfer and Super-Resolution\'""\n    parser = argparse.ArgumentParser(description=desc)\n\n    parser.add_argument(\'--style_model\', type=str, default=\'models/wave.ckpt\', help=\'location for model file (*.ckpt)\',\n                        required=True)\n\n    parser.add_argument(\'--content\', type=str, default=\'content/female_knight.jpg\',\n                        help=\'File path of content image (notation in the paper : x)\', required=True)\n\n    parser.add_argument(\'--output\', type=str, default=\'result.jpg\',\n                        help=\'File path of output image (notation in the paper : y_c)\', required=True)\n\n    parser.add_argument(\'--max_size\', type=int, default=None, help=\'The maximum width or height of input images\')\n\n    return check_args(parser.parse_args())\n\n""""""checking arguments""""""\ndef check_args(args):\n    # --style_model\n    try:\n        #Tensorflow r0.12 requires 3 files related to *.ckpt\n        assert os.path.exists(args.style_model + \'.index\') and os.path.exists(args.style_model + \'.meta\') and os.path.exists(\n            args.style_model + \'.data-00000-of-00001\')\n    except:\n        print(\'There is no %s\'%args.style_model)\n        print(\'Tensorflow r0.12 requires 3 files related to *.ckpt\')\n        print(\'If you want to restore any models generated from old tensorflow versions, this assert might be ignored\')\n        return None\n\n    # --content\n    try:\n        assert os.path.exists(args.content)\n    except:\n        print(\'There is no %s\' % args.content)\n        return None\n\n    # --max_size\n    try:\n        if args.max_size is not None:\n            assert args.max_size > 0\n    except:\n        print(\'The maximum width or height of input image must be positive\')\n        return None\n\n    # --output\n    dirname = os.path.dirname(args.output)\n    try:\n        if len(dirname) > 0:\n            os.stat(dirname)\n    except:\n        os.mkdir(dirname)\n\n    return args\n\n""""""main""""""\ndef main():\n\n    # parse arguments\n    args = parse_args()\n    if args is None:\n        exit()\n\n    # load content image\n    content_image = utils.load_image(args.content, max_size=args.max_size)\n\n    # open session\n    soft_config = tf.ConfigProto(allow_soft_placement=True)\n    soft_config.gpu_options.allow_growth = True # to deal with large image\n    sess = tf.Session(config=soft_config)\n\n    # build the graph\n    transformer = style_transfer_tester.StyleTransferTester(session=sess,\n                                                            model_path=args.style_model,\n                                                            content_image=content_image,\n                                                            )\n    # execute the graph\n    start_time = time.time()\n    output_image = transformer.test()\n    end_time = time.time()\n\n    # save result\n    utils.save_image(output_image, args.output)\n\n    # report execution time\n    shape = content_image.shape #(batch, width, height, channel)\n    print(\'Execution time for a %d x %d image : %f msec\' % (shape[0], shape[1], 1000.*float(end_time - start_time)/60))\n\nif __name__ == \'__main__\':\n    main()\n'"
run_train.py,1,"b'import tensorflow as tf\nimport numpy as np\nimport utils\nimport vgg19\nimport style_transfer_trainer\nimport os\n\nimport argparse\n\n""""""parsing and configuration""""""\ndef parse_args():\n    desc = ""Tensorflow implementation of \'Image Style Transfer Using Convolutional Neural Networks""\n    parser = argparse.ArgumentParser(description=desc)\n\n    parser.add_argument(\'--vgg_model\', type=str, default=\'pre_trained_model\', help=\'The directory where the pre-trained model was saved\', required=True)\n    parser.add_argument(\'--trainDB_path\', type=str, default=\'train2014\',\n                        help=\'The directory where MSCOCO DB was saved\', required=True)\n    parser.add_argument(\'--style\', type=str, default=\'style/wave.jpg\', help=\'File path of style image (notation in the paper : a)\', required=True)\n    parser.add_argument(\'--output\', type=str, default=\'models\', help=\'File path for trained-model. Train-log is also saved here.\', required=True)\n\t\n    parser.add_argument(\'--content_weight\', type=float, default=7.5e0, help=\'Weight of content-loss\')\n    parser.add_argument(\'--style_weight\', type=float, default=5e2, help=\'Weight of style-loss\')\n    parser.add_argument(\'--tv_weight\', type=float, default=2e2, help=\'Weight of total-variance-loss\')\n\n    parser.add_argument(\'--content_layers\', nargs=\'+\', type=str, default=[\'relu4_2\'], help=\'VGG19 layers used for content loss\')\n    parser.add_argument(\'--style_layers\', nargs=\'+\', type=str, default=[\'relu1_1\', \'relu2_1\', \'relu3_1\', \'relu4_1\', \'relu5_1\'],\n                        help=\'VGG19 layers used for style loss\')\n\n    parser.add_argument(\'--content_layer_weights\', nargs=\'+\', type=float, default=[1.0], help=\'Content loss for each content is multiplied by corresponding weight\')\n    parser.add_argument(\'--style_layer_weights\', nargs=\'+\', type=float, default=[.2,.2,.2,.2,.2],\n                        help=\'Style loss for each content is multiplied by corresponding weight\')\n\n    parser.add_argument(\'--learn_rate\', type=float, default=1e-3, help=\'Learning rate for Adam optimizer\')\n    parser.add_argument(\'--num_epochs\', type=int, default=2, help=\'The number of epochs to run\')\n    parser.add_argument(\'--batch_size\', type=int, default=4, help=\'Batch size\')\n\n    parser.add_argument(\'--checkpoint_every\', type=int, default=1000, help=\'save a trained model every after this number of iterations\')\n\n    parser.add_argument(\'--test\', type=str, default=None,\n                        help=\'File path of content image (notation in the paper : x)\')\n\n    parser.add_argument(\'--max_size\', type=int, default=None, help=\'The maximum width or height of input images\')\n\n    return check_args(parser.parse_args())\n\n""""""checking arguments""""""\ndef check_args(args):\n\n    # --vgg_model\n    model_file_path = args.vgg_model + \'/\' + vgg19.MODEL_FILE_NAME\n    try:\n        assert os.path.exists(model_file_path)\n    except:\n        print(\'There is no %s\' % model_file_path)\n        return None\n    try:\n        size_in_KB = os.path.getsize(model_file_path)\n        assert abs(size_in_KB - 534904783) < 10\n    except:\n        print(\'check file size of \\\'imagenet-vgg-verydeep-19.mat\\\'\')\n        print(\'there are some files with the same name\')\n        print(\'pre_trained_model used here can be downloaded from bellow\')\n        print(\'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\')\n        return None\n\n    # --trainDB_path\n    try:\n        assert os.path.exists(args.trainDB_path)\n    except:\n        print(\'There is no %s\' % args.trainDB_path)\n        return None\n\n    # --style\n    try:\n        assert os.path.exists(args.style)\n    except:\n        print(\'There is no %s\' % args.style)\n        return None\n\n    # --output\n    dirname = os.path.dirname(args.output)\n    try:\n        if len(dirname) > 0:\n            os.stat(dirname)\n    except:\n        os.mkdir(dirname)\n\n    # --content_weight\n    try:\n        assert args.content_weight > 0\n    except:\n        print(\'content weight must be positive\')\n\n    # --style_weight\n    try:\n        assert args.style_weight > 0\n    except:\n        print(\'style weight must be positive\')\n\n    # --tv_weight\n    try:\n        assert args.tv_weight > 0\n    except:\n        print(\'total variance weight must be positive\')\n\n    # --content_layer_weights\n    try:\n        assert len(args.content_layers) == len(args.content_layer_weights)\n    except:\n        print (\'content layer info and weight info must be matched\')\n        return None\n\n    # --style_layer_weights\n    try:\n        assert len(args.style_layers) == len(args.style_layer_weights)\n    except:\n        print(\'style layer info and weight info must be matched\')\n        return None\n\n    # --learn_rate\n    try:\n        assert args.learn_rate > 0\n    except:\n        print(\'learning rate must be positive\')\n\n    # --num_epochs\n    try:\n        assert args.num_epochs >= 1\n    except:\n        print(\'number of epochs must be larger than or equal to one\')\n\n    # --batch_size\n    try:\n        assert args.batch_size >= 1\n    except:\n        print(\'batch size must be larger than or equal to one\')\n\n    # --checkpoint_every\n    try:\n        assert args.checkpoint_every >= 1\n    except:\n        print(\'checkpoint period must be larger than or equal to one\')\n\n    # --test\n    try:\n        if args.test is not None:\n            assert os.path.exists(args.test)\n    except:\n        print(\'There is no %s\' % args.test)\n        return None\n\n    # --max_size\n    try:\n        if args.max_size is not None:\n            assert args.max_size > 0\n    except:\n        print(\'The maximum width or height of input image must be positive\')\n        return None\n\n    return args\n\n""""""add one dim for batch""""""\n# VGG19 requires input dimension to be (batch, height, width, channel)\ndef add_one_dim(image):\n    shape = (1,) + image.shape\n    return np.reshape(image, shape)\n\n""""""main""""""\ndef main():\n\n    # parse arguments\n    args = parse_args()\n    if args is None:\n        exit()\n\n    # initiate VGG19 model\n    model_file_path = args.vgg_model + \'/\' + vgg19.MODEL_FILE_NAME\n    vgg_net = vgg19.VGG19(model_file_path)\n\n    # get file list for training\n    content_images = utils.get_files(args.trainDB_path)\n\n    # load style image\n    style_image = utils.load_image(args.style)\n\n    # create a map for content layers info\n    CONTENT_LAYERS = {}\n    for layer, weight in zip(args.content_layers,args.content_layer_weights):\n        CONTENT_LAYERS[layer] = weight\n\n    # create a map for style layers info\n    STYLE_LAYERS = {}\n    for layer, weight in zip(args.style_layers, args.style_layer_weights):\n        STYLE_LAYERS[layer] = weight\n\n    # open session\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\n    # build the graph for train\n    trainer = style_transfer_trainer.StyleTransferTrainer(session=sess,\n                                                          content_layer_ids=CONTENT_LAYERS,\n                                                          style_layer_ids=STYLE_LAYERS,\n                                                          content_images=content_images,\n                                                          style_image=add_one_dim(style_image),\n                                                          net=vgg_net,\n                                                          num_epochs=args.num_epochs,\n                                                          batch_size=args.batch_size,\n                                                          content_weight=args.content_weight,\n                                                          style_weight=args.style_weight,\n                                                          tv_weight=args.tv_weight,\n                                                          learn_rate=args.learn_rate,\n                                                          save_path=args.output,\n                                                          check_period=args.checkpoint_every,\n                                                          test_image=args.test,\n                                                          max_size=args.max_size,\n                                                          )\n    # launch the graph in a session\n    trainer.train()\n\n    # close session\n    sess.close()\n\nif __name__ == \'__main__\':\n    main()\n'"
style_transfer_tester.py,6,"b""import tensorflow as tf\nimport transform\n\nclass StyleTransferTester:\n\n    def __init__(self, session, content_image, model_path):\n        # session\n        self.sess = session\n\n        # input images\n        self.x0 = content_image\n\n        # input model\n        self.model_path = model_path\n\n        # image transform network\n        self.transform = transform.Transform()\n\n        # build graph for style transfer\n        self._build_graph()\n\n    def _build_graph(self):\n\n        # graph input\n        self.x = tf.placeholder(tf.float32, shape=self.x0.shape, name='input')\n        self.xi = tf.expand_dims(self.x, 0) # add one dim for batch\n\n        # result image from transform-net\n        self.y_hat = self.transform.net(self.xi/255.0)\n        self.y_hat = tf.squeeze(self.y_hat) # remove one dim for batch\n        self.y_hat = tf.clip_by_value(self.y_hat, 0., 255.)\n\n    def test(self):\n\n        # initialize parameters\n        self.sess.run(tf.global_variables_initializer())\n\n        # load pre-trained model\n        saver = tf.train.Saver()\n        saver.restore(self.sess, self.model_path)\n\n        # get transformed image\n        output = self.sess.run(self.y_hat, feed_dict={self.x: self.x0})\n\n        return output\n\n\n\n\n\n"""
style_transfer_trainer.py,27,"b'import tensorflow as tf\nimport numpy as np\nimport collections\nimport transform\nimport utils\nimport style_transfer_tester\n\nclass StyleTransferTrainer:\n    def __init__(self, content_layer_ids, style_layer_ids, content_images, style_image, session, net, num_epochs,\n                 batch_size, content_weight, style_weight, tv_weight, learn_rate, save_path, check_period, test_image,\n                 max_size):\n\n        self.net = net\n        self.sess = session\n\n        # sort layers info\n        self.CONTENT_LAYERS = collections.OrderedDict(sorted(content_layer_ids.items()))\n        self.STYLE_LAYERS = collections.OrderedDict(sorted(style_layer_ids.items()))\n\n        # input images\n        self.x_list = content_images\n        mod = len(content_images) % batch_size\n        self.x_list = self.x_list[:-mod]\n        self.y_s0 = style_image\n        self.content_size = len(self.x_list)\n\n        # parameters for optimization\n        self.num_epochs = num_epochs\n        self.content_weight = content_weight\n        self.style_weight = style_weight\n        self.tv_weight = tv_weight\n        self.learn_rate = learn_rate\n        self.batch_size = batch_size\n        self.check_period = check_period\n\n        # path for model to be saved\n        self.save_path = save_path\n\n        # image transform network\n        self.transform = transform.Transform()\n        self.tester = transform.Transform(\'test\')\n\n        # build graph for style transfer\n        self._build_graph()\n\n        # test during training\n        if test_image is not None:\n            self.TEST = True\n\n            # load content image\n            self.test_image = utils.load_image(test_image, max_size=max_size)\n\n            # build graph\n            self.x_test = tf.placeholder(tf.float32, shape=self.test_image.shape, name=\'test_input\')\n            self.xi_test = tf.expand_dims(self.x_test, 0)  # add one dim for batch\n\n            # result image from transform-net\n            self.y_hat_test = self.tester.net(\n                self.xi_test / 255.0)  # please build graph for train first. tester.net reuses variables.\n\n        else:\n            self.TEST = False\n\n    def _build_graph(self):\n\n        """""" prepare data """"""\n\n        self.batch_shape = (self.batch_size,256,256,3)\n\n        # graph input\n        self.y_c = tf.placeholder(tf.float32, shape=self.batch_shape, name=\'content\')\n        self.y_s = tf.placeholder(tf.float32, shape=self.y_s0.shape, name=\'style\')\n\n        # preprocess for VGG\n        self.y_c_pre = self.net.preprocess(self.y_c)\n        self.y_s_pre = self.net.preprocess(self.y_s)\n\n        # get content-layer-feature for content loss\n        content_layers = self.net.feed_forward(self.y_c_pre, scope=\'content\')\n        self.Ps = {}\n        for id in self.CONTENT_LAYERS:\n            self.Ps[id] = content_layers[id]\n\n        # get style-layer-feature for style loss\n        style_layers = self.net.feed_forward(self.y_s_pre, scope=\'style\')\n        self.As = {}\n        for id in self.STYLE_LAYERS:\n            self.As[id] = self._gram_matrix(style_layers[id])\n\n        # result of image transform net\n        self.x = self.y_c/255.0\n        self.y_hat = self.transform.net(self.x)\n        \n        # get layer-values for x\n        self.y_hat_pre = self.net.preprocess(self.y_hat)\n        self.Fs = self.net.feed_forward(self.y_hat_pre, scope=\'mixed\')\n\n        """""" compute loss """"""\n\n        # style & content losses\n        L_content = 0\n        L_style = 0\n        for id in self.Fs:\n            if id in self.CONTENT_LAYERS:\n                ## content loss ##\n\n                F = self.Fs[id]             # content feature of x\n                P = self.Ps[id]             # content feature of p\n\n                b, h, w, d = F.get_shape()  # first return value is batch size (must be one)\n                b = b.value                 # batch size\n                N = h.value*w.value         # product of width and height\n                M = d.value                 # number of filters\n\n                w = self.CONTENT_LAYERS[id] # weight for this layer\n\n                L_content += w * 2 * tf.nn.l2_loss(F-P) / (b*N*M)\n\n            elif id in self.STYLE_LAYERS:\n                ## style loss ##\n\n                F = self.Fs[id]\n\n                b, h, w, d = F.get_shape()          # first return value is batch size (must be one)\n                b = b.value                         # batch size\n                N = h.value * w.value               # product of width and height\n                M = d.value                         # number of filters\n\n                w = self.STYLE_LAYERS[id]           # weight for this layer\n\n                G = self._gram_matrix(F, (b,N,M))   # style feature of x\n                A = self.As[id]                     # style feature of a\n\n                L_style += w * 2 * tf.nn.l2_loss(G - A) / (b * (M ** 2))\n\n        # total variation loss\n        L_tv = self._get_total_variation_loss(self.y_hat)\n\n        """""" compute total loss """"""\n\n        # Loss of total variation regularization\n        alpha = self.content_weight\n        beta = self.style_weight\n        gamma = self.tv_weight\n\n        self.L_content = alpha*L_content\n        self.L_style = beta*L_style\n        self.L_tv = gamma*L_tv\n        self.L_total = self.L_content + self.L_style + self.L_tv\n\n        # add summary for each loss\n        tf.summary.scalar(\'L_content\', self.L_content)\n        tf.summary.scalar(\'L_style\', self.L_style)\n        tf.summary.scalar(\'L_tv\', self.L_tv)\n        tf.summary.scalar(\'L_total\', self.L_total)\n\n    # borrowed from https://github.com/lengstrom/fast-style-transfer/blob/master/src/optimize.py\n    def _get_total_variation_loss(self, img):\n        b, h, w, d = img.get_shape()\n        b = b.value\n        h = h.value\n        w = w.value\n        d = d.value\n        tv_y_size = (h-1) * w * d\n        tv_x_size = h * (w-1) * d\n        y_tv = tf.nn.l2_loss(img[:, 1:, :, :] - img[:, :self.batch_shape[1] - 1, :, :])\n        x_tv = tf.nn.l2_loss(img[:, :, 1:, :] - img[:, :, :self.batch_shape[2] - 1, :])\n        loss = 2. * (x_tv / tv_x_size + y_tv / tv_y_size) / b\n\n        loss = tf.cast(loss, tf.float32)\n        return loss\n\n    def train(self):\n        """""" define optimizer Adam """"""\n        global_step = tf.contrib.framework.get_or_create_global_step()\n\n        trainable_variables = tf.trainable_variables()\n        grads = tf.gradients(self.L_total, trainable_variables)\n\n        optimizer = tf.train.AdamOptimizer(self.learn_rate)\n        train_op = optimizer.apply_gradients(zip(grads, trainable_variables), global_step=global_step,\n                                             name=\'train_step\')\n\n        """""" tensor board """"""\n        # merge all summaries into a single op\n        merged_summary_op = tf.summary.merge_all()\n\n        # op to write logs to Tensorboard\n        summary_writer = tf.summary.FileWriter(self.save_path, graph=tf.get_default_graph())\n\n        """""" session run """"""\n        self.sess.run(tf.global_variables_initializer())\n\n        # saver to save model\n        saver = tf.train.Saver()\n\n        # restore check-point if it exits\n        checkpoint_exists = True\n        try:\n            ckpt_state = tf.train.get_checkpoint_state(self.save_path)\n        except tf.errors.OutOfRangeError as e:\n            print(\'Cannot restore checkpoint: %s\' % e)\n            checkpoint_exists = False\n        if not (ckpt_state and ckpt_state.model_checkpoint_path):\n            print(\'No model to restore at %s\' % self.save_path)\n            checkpoint_exists = False\n\n        if checkpoint_exists:\n            tf.logging.info(\'Loading checkpoint %s\', ckpt_state.model_checkpoint_path)\n            saver.restore(self.sess, ckpt_state.model_checkpoint_path)\n\n        """""" loop for train """"""\n        num_examples = len(self.x_list)\n\n        # get iteration info\n        if checkpoint_exists:\n            iterations = self.sess.run(global_step)\n            epoch = (iterations * self.batch_size) // num_examples\n            iterations = iterations - epoch*(num_examples // self.batch_size)\n        else:\n            epoch = 0\n            iterations = 0\n\n        while epoch < self.num_epochs:\n            while iterations * self.batch_size < num_examples:\n\n                curr = iterations * self.batch_size\n                step = curr + self.batch_size\n                x_batch = np.zeros(self.batch_shape, dtype=np.float32)\n                for j, img_p in enumerate(self.x_list[curr:step]):\n                    x_batch[j] = utils.get_img(img_p, (256, 256, 3)).astype(np.float32)\n\n                iterations += 1\n\n                assert x_batch.shape[0] == self.batch_size\n\n                _, summary, L_total, L_content, L_style, L_tv, step = self.sess.run(\n                    [train_op, merged_summary_op, self.L_total, self.L_content, self.L_style, self.L_tv, global_step],\n                    feed_dict={self.y_c: x_batch, self.y_s: self.y_s0})\n\n                print(\'epoch : %d, iter : %4d, \' % (epoch, step),\n                      \'L_total : %g, L_content : %g, L_style : %g, L_tv : %g\' % (L_total, L_content, L_style, L_tv))\n\n                # write logs at every iteration\n                summary_writer.add_summary(summary, iterations)\n\n                if step % self.check_period == 0:\n                    res = saver.save(self.sess, self.save_path + \'/final.ckpt\', step)\n\n                    if self.TEST:\n                        output_image = self.sess.run([self.y_hat_test], feed_dict={self.x_test: self.test_image})\n                        output_image = np.squeeze(output_image[0])  # remove one dim for batch\n                        output_image = np.clip(output_image, 0., 255.)\n\n                        utils.save_image(output_image, self.save_path + \'/result_\' + ""%05d"" % step + \'.jpg\')\n            epoch += 1\n            iterations = 0\n        res = saver.save(self.sess,self.save_path+\'/final.ckpt\')\n\n    def _gram_matrix(self, tensor, shape=None):\n\n        if shape is not None:\n            B = shape[0]  # batch size\n            HW = shape[1] # height x width\n            C = shape[2]  # channels\n            CHW = C*HW\n        else:\n            B, H, W, C = map(lambda i: i.value, tensor.get_shape())\n            HW = H*W\n            CHW = W*H*C\n\n        # reshape the tensor so it is a (B, 2-dim) matrix\n        # so that \'B\'th gram matrix can be computed\n        feats = tf.reshape(tensor, (B, HW, C))\n\n        # leave dimension of batch as it is\n        feats_T = tf.transpose(feats, perm=[0, 2, 1])\n\n        # paper suggests to normalize gram matrix by its number of elements\n        gram = tf.matmul(feats_T, feats) / CHW\n\n        return gram\n\n\n\n\n\n\n\n\n\n\n'"
transform.py,14,"b'# Most code in this file was borrowed from https://github.com/lengstrom/fast-style-transfer/blob/master/src/transform.py\n\nimport tensorflow as tf\n\nclass Transform:\n    def __init__(self, mode=\'train\'):\n        if mode == \'train\':\n            self.reuse = None\n        else:\n            self.reuse = True\n\n    def net(self, image):\n        image_p = self._reflection_padding(image)\n        conv1 = self._conv_layer(image_p, 32, 9, 1, name=\'conv1\')\n        conv2 = self._conv_layer(conv1, 64, 3, 2, name=\'conv2\')\n        conv3 = self._conv_layer(conv2, 128, 3, 2, name=\'conv3\')\n        resid1 = self._residual_block(conv3, 3, name=\'resid1\')\n        resid2 = self._residual_block(resid1, 3, name=\'resid2\')\n        resid3 = self._residual_block(resid2, 3, name=\'resid3\')\n        resid4 = self._residual_block(resid3, 3, name=\'resid4\')\n        resid5 = self._residual_block(resid4, 3, name=\'resid5\')\n        conv_t1 = self._conv_tranpose_layer(resid5, 64, 3, 2, name=\'convt1\')\n        conv_t2 = self._conv_tranpose_layer(conv_t1, 32, 3, 2, name=\'convt2\')\n        conv_t3 = self._conv_layer(conv_t2, 3, 9, 1, relu=False, name=\'convt3\')\n        preds = (tf.nn.tanh(conv_t3) + 1) * (255. / 2)\n        return preds\n\n    def _reflection_padding(self, net):\n        return tf.pad(net,[[0, 0],[40, 40],[40, 40], [0, 0]], ""REFLECT"")\n\n    def _conv_layer(self, net, num_filters, filter_size, strides, padding=\'SAME\', relu=True, name=None):\n        weights_init = self._conv_init_vars(net, num_filters, filter_size, name=name)\n        strides_shape = [1, strides, strides, 1]\n        net = tf.nn.conv2d(net, weights_init, strides_shape, padding=padding)\n        net = self._instance_norm(net, name=name)\n        if relu:\n            net = tf.nn.relu(net)\n\n        return net\n\n    def _conv_tranpose_layer(self, net, num_filters, filter_size, strides, name=None):\n        weights_init = self._conv_init_vars(net, num_filters, filter_size, transpose=True, name=name)\n\n        batch_size, rows, cols, in_channels = [i.value for i in net.get_shape()]\n        new_rows, new_cols = int(rows * strides), int(cols * strides)\n\n        new_shape = [batch_size, new_rows, new_cols, num_filters]\n        tf_shape = tf.stack(new_shape)\n        strides_shape = [1,strides,strides,1]\n\n        net = tf.nn.conv2d_transpose(net, weights_init, tf_shape, strides_shape, padding=\'SAME\')\n        net = self._instance_norm(net, name=name)\n        return tf.nn.relu(net)\n\n    def _residual_block(self, net, filter_size=3, name=None):\n        batch, rows, cols, channels = [i.value for i in net.get_shape()]\n        tmp = self._conv_layer(net, 128, filter_size, 1, padding=\'VALID\', relu=True, name=name+\'_1\')\n        return self._conv_layer(tmp, 128, filter_size, 1, padding=\'VALID\', relu=False, name=name+\'_2\') + tf.slice(net, [0,2,2,0], [batch,rows-4,cols-4,channels])\n\n    def _instance_norm(self, net, name=None):\n        batch, rows, cols, channels = [i.value for i in net.get_shape()]\n        var_shape = [channels]\n        mu, sigma_sq = tf.nn.moments(net, [1,2], keep_dims=True)\n        with tf.variable_scope(name, reuse=self.reuse):\n            shift = tf.get_variable(\'shift\', initializer=tf.zeros(var_shape), dtype=tf.float32)\n            scale = tf.get_variable(\'scale\', initializer=tf.ones(var_shape), dtype=tf.float32)\n        epsilon = 1e-3\n        normalized = (net-mu)/(sigma_sq + epsilon)**(.5)\n        return scale * normalized + shift\n\n    def _conv_init_vars(self, net, out_channels, filter_size, transpose=False, name=None):\n        _, rows, cols, in_channels = [i.value for i in net.get_shape()]\n        if not transpose:\n            weights_shape = [filter_size, filter_size, in_channels, out_channels]\n        else:\n            weights_shape = [filter_size, filter_size, out_channels, in_channels]\n        with tf.variable_scope(name, reuse=self.reuse):\n            weights_init = tf.get_variable(\'weight\', shape=weights_shape, initializer=tf.contrib.layers.variance_scaling_initializer(), dtype=tf.float32)\n        return weights_init'"
utils.py,0,"b'import numpy as np\nimport PIL.Image\nimport os\nimport scipy\n\n""""""Helper-functions to load MSCOCO DB""""""\n# borrowed from https://github.com/lengstrom/fast-style-transfer/blob/master/src/utils.py\ndef get_img(src, img_size=False):\n   img = scipy.misc.imread(src, mode=\'RGB\')\n   if not (len(img.shape) == 3 and img.shape[2] == 3):\n       img = np.dstack((img,img,img))\n   if img_size != False:\n       img = scipy.misc.imresize(img, img_size)\n   return img\n\ndef get_files(img_dir):\n    files = list_files(img_dir)\n    return list(map(lambda x: os.path.join(img_dir,x), files))\n\ndef list_files(in_path):\n    files = []\n    for (dirpath, dirnames, filenames) in os.walk(in_path):\n        files.extend(filenames)\n        break\n    return files\n\n""""""Helper-functions for image manipulation""""""\n# borrowed from https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb\n\n# This function loads an image and returns it as a numpy array of floating-points.\n# The image can be automatically resized so the largest of the height or width equals max_size.\n# or resized to the given shape\ndef load_image(filename, shape=None, max_size=None):\n    image = PIL.Image.open(filename)\n\n    if max_size is not None:\n        # Calculate the appropriate rescale-factor for\n        # ensuring a max height and width, while keeping\n        # the proportion between them.\n        factor = float(max_size) / np.max(image.size)\n\n        # Scale the image\'s height and width.\n        size = np.array(image.size) * factor\n\n        # The size is now floating-point because it was scaled.\n        # But PIL requires the size to be integers.\n        size = size.astype(int)\n\n        # Resize the image.\n        image = image.resize(size, PIL.Image.LANCZOS) # PIL.Image.LANCZOS is one of resampling filter\n\n    if shape is not None:\n        image = image.resize(shape, PIL.Image.LANCZOS) # PIL.Image.LANCZOS is one of resampling filter\n\n    # Convert to numpy floating-point array.\n    return np.float32(image)\n\n# Save an image as a jpeg-file.\n# The image is given as a numpy array with pixel-values between 0 and 255.\ndef save_image(image, filename):\n    # Ensure the pixel-values are between 0 and 255.\n    image = np.clip(image, 0.0, 255.0)\n\n    # Convert to bytes.\n    image = image.astype(np.uint8)\n\n    # Write the image-file in jpeg-format.\n    with open(filename, \'wb\') as file:\n        PIL.Image.fromarray(image).save(file, \'jpeg\')'"
vgg19.py,5,"b""# Copyright (c) 2015-2016 Anish Athalye. Released under GPLv3.\n# Most code in this file was borrowed from https://github.com/anishathalye/neural-style/blob/master/vgg.py\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.io\n\n# download URL : http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\nMODEL_FILE_NAME = 'imagenet-vgg-verydeep-19.mat'\n\ndef _conv_layer(input, weights, bias):\n    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n            padding='SAME')\n    return tf.nn.bias_add(conv, bias)\n\ndef _pool_layer(input):\n    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n            padding='SAME')\n\ndef preprocess(image, mean_pixel):\n    return image - mean_pixel\n\ndef undo_preprocess(image, mean_pixel):\n    return image + mean_pixel\n\nclass VGG19:\n    layers = (\n        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n\n        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n\n        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n\n        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n\n        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n        'relu5_3', 'conv5_4', 'relu5_4'\n    )\n\n    def __init__(self, data_path):\n        data = scipy.io.loadmat(data_path)\n\n        self.mean_pixel = np.array([123.68, 116.779, 103.939])\n\n        self.weights = data['layers'][0]\n\n    def preprocess(self, image):\n        return image-self.mean_pixel\n\n    def undo_preprocess(self,image):\n        return image+self.mean_pixel\n\n    def feed_forward(self, input_image, scope=None):\n        net = {}\n        current = input_image\n\n        with tf.variable_scope(scope):\n            for i, name in enumerate(self.layers):\n                kind = name[:4]\n                if kind == 'conv':\n                    kernels = self.weights[i][0][0][2][0][0]\n                    bias = self.weights[i][0][0][2][0][1]\n\n                    # matconvnet: weights are [width, height, in_channels, out_channels]\n                    # tensorflow: weights are [height, width, in_channels, out_channels]\n                    kernels = np.transpose(kernels, (1, 0, 2, 3))\n                    bias = bias.reshape(-1)\n\n                    current = _conv_layer(current, kernels, bias)\n                elif kind == 'relu':\n                    current = tf.nn.relu(current)\n                elif kind == 'pool':\n                    current = _pool_layer(current)\n                net[name] = current\n\n        assert len(net) == len(self.layers)\n        return net"""
