file_path,api_count,code
cnn_model.py,1,"b""# Some code was borrowed from https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/mnist/convolutional.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n# Create model of CNN with slim api\ndef CNN(inputs, is_training=True):\n    batch_norm_params = {'is_training': is_training, 'decay': 0.9, 'updates_collections': None}\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params):\n        x = tf.reshape(inputs, [-1, 28, 28, 1])\n\n        # For slim.conv2d, default argument values are like\n        # normalizer_fn = None, normalizer_params = None, <== slim.arg_scope changes these arguments\n        # padding='SAME', activation_fn=nn.relu,\n        # weights_initializer = initializers.xavier_initializer(),\n        # biases_initializer = init_ops.zeros_initializer,\n        net = slim.conv2d(x, 32, [5, 5], scope='conv1')\n        net = slim.max_pool2d(net, [2, 2], scope='pool1')\n        net = slim.conv2d(net, 64, [5, 5], scope='conv2')\n        net = slim.max_pool2d(net, [2, 2], scope='pool2')\n        net = slim.flatten(net, scope='flatten3')\n\n        # For slim.fully_connected, default argument values are like\n        # activation_fn = nn.relu,\n        # normalizer_fn = None, normalizer_params = None, <== slim.arg_scope changes these arguments\n        # weights_initializer = initializers.xavier_initializer(),\n        # biases_initializer = init_ops.zeros_initializer,\n        net = slim.fully_connected(net, 1024, scope='fc3')\n        net = slim.dropout(net, is_training=is_training, scope='dropout3')  # 0.5 by default\n        outputs = slim.fully_connected(net, 10, activation_fn=None, normalizer_fn=None, scope='fco')\n    return outputs\n"""
mnist_cnn_test.py,18,"b'# Some code was borrowed from https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/mnist/convolutional.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport mnist_data\nimport cnn_model\n\n# user input\nfrom argparse import ArgumentParser\n\n# refernce argument values\nMODEL_DIRECTORY = ""model""\nTEST_BATCH_SIZE = 5000\nENSEMBLE = True\n\n# build parser\ndef build_parser():\n    parser = ArgumentParser()\n\n    parser.add_argument(\'--model-dir\',\n                        dest=\'model_directory\', help=\'directory where model to be tested is stored\',\n                        metavar=\'MODEL_DIRECTORY\', required=True)\n    parser.add_argument(\'--batch-size\', type=int,\n                        dest=\'batch_size\', help=\'batch size for test\',\n                        metavar=\'TEST_BATCH_SIZE\', required=True)\n    parser.add_argument(\'--use-ensemble\',\n                        dest=\'ensemble\', help=\'boolean for usage of ensemble\',\n                        metavar=\'ENSEMBLE\', required=True)\n    return parser\n\n# test with test data given by mnist_data.py\ndef test(model_directory, batch_size):\n    # Import data\n    PIXEL_DEPTH = mnist_data.PIXEL_DEPTH\n    mnist = input_data.read_data_sets(\'data/\', one_hot=True)\n\n    is_training = tf.placeholder(tf.bool, name=\'MODE\')\n\n    # tf Graph input\n    x = tf.placeholder(tf.float32, [None, 784])\n    y_ = tf.placeholder(tf.float32, [None, 10])  # answer\n    y = cnn_model.CNN(x, is_training=is_training)\n\n    # Add ops to save and restore all the variables\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer(), feed_dict={is_training: True})\n\n    # Restore variables from disk\n    saver = tf.train.Saver()\n\n    # Calculate accuracy for all mnist test images\n    test_size = mnist.test.num_examples\n    total_batch = int(test_size / batch_size)\n\n    saver.restore(sess, model_directory)\n\n    acc_buffer = []\n    # Loop over all batches\n    for i in range(total_batch):\n\n        batch = mnist.test.next_batch(batch_size)\n        batch_xs = (batch[0] - (PIXEL_DEPTH / 2.0) / PIXEL_DEPTH)  # make zero-centered distribution as in mnist_data.extract_data()\n        batch_ys = batch[1]\n\n        y_final = sess.run(y, feed_dict={x: batch_xs, y_: batch_ys, is_training: False})\n\n        correct_prediction = numpy.equal(numpy.argmax(y_final, 1), numpy.argmax(batch_ys, 1))\n\n        acc_buffer.append(numpy.sum(correct_prediction) / batch_size)\n\n    print(""test accuracy for the stored model: %g"" % numpy.mean(acc_buffer))\n\n# test with test data given by mnist_data.py\ndef test_org(model_directory, batch_size):\n    # Import data\n    PIXEL_DEPTH = mnist_data.PIXEL_DEPTH\n    train_total_data, train_size, validation_data, validation_labels, test_data, test_labels = mnist_data.prepare_MNIST_data(\n        False)\n\n    is_training = tf.placeholder(tf.bool, name=\'MODE\')\n\n    # tf Graph input\n    x = tf.placeholder(tf.float32, [None, 784])\n    y_ = tf.placeholder(tf.float32, [None, 10])  # answer\n    y = cnn_model.CNN(x, is_training=is_training)\n\n    # Add ops to save and restore all the variables\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer(), feed_dict={is_training: True})\n\n    # Restore variables from disk\n    saver = tf.train.Saver()\n\n    # Calculate accuracy for all mnist test images\n    test_size = test_labels.shape[0]\n    total_batch = int(test_size / batch_size)\n\n    saver.restore(sess, model_directory)\n\n    acc_buffer = []\n\n    # Loop over all batches\n    for i in range(total_batch):\n        # Compute the offset of the current minibatch in the data.\n        offset = (i * batch_size) % (test_size)\n        batch_xs = test_data[offset:(offset + batch_size), :]\n        batch_ys = test_labels[offset:(offset + batch_size), :]\n\n        y_final = sess.run(y, feed_dict={x: batch_xs, y_: batch_ys, is_training: False})\n\n        correct_prediction = numpy.equal(numpy.argmax(y_final, 1), numpy.argmax(batch_ys, 1))\n\n        acc_buffer.append(numpy.sum(correct_prediction) / batch_size)\n\n    print(""test accuracy for the stored model: %g"" % numpy.mean(acc_buffer))\n\n# For a given matrix, each row is converted into a one-hot row vector\ndef one_hot_matrix(a):\n    a_ = numpy.zeros_like(a)\n    for i, j in zip(numpy.arange(a.shape[0]), numpy.argmax(a, 1)): a_[i, j] = 1\n    return a_\n\n# test with test data given by mnist_data.py\ndef test_ensemble(model_directory_list, batch_size):\n    # Import data\n    PIXEL_DEPTH = mnist_data.PIXEL_DEPTH\n    mnist = input_data.read_data_sets(\'data/\', one_hot=True)\n\n    is_training = tf.placeholder(tf.bool, name=\'MODE\')\n\n    # tf Graph input\n    x = tf.placeholder(tf.float32, [None, 784])\n    y_ = tf.placeholder(tf.float32, [None, 10])  # answer\n    y = cnn_model.CNN(x, is_training=is_training)\n\n    # Add ops to save and restore all the variables\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer(), feed_dict={is_training: True})\n\n    # Restore variables from disk\n    saver = tf.train.Saver()\n\n    # Calculate accuracy for all mnist test images\n    test_size = mnist.test.num_examples\n    total_batch = int(test_size / batch_size)\n\n    acc_buffer = []\n    # Loop over all batches\n    for i in range(total_batch):\n\n        batch = mnist.test.next_batch(batch_size)\n        batch_xs = (batch[0] - (PIXEL_DEPTH / 2.0) / PIXEL_DEPTH)  # make zero-centered distribution as in mnist_data.extract_data()\n        batch_ys = batch[1]\n\n        y_final = numpy.zeros_like(batch_ys)\n\n        for dir in model_directory_list:\n            saver.restore(sess, dir+\'/model.ckpt\')\n            pred = sess.run(y, feed_dict={x: batch_xs, y_: batch_ys, is_training: False})\n            y_final += one_hot_matrix(pred) # take a majority vote as an answer\n\n        correct_prediction = numpy.equal(numpy.argmax(y_final, 1), numpy.argmax(batch_ys, 1))\n\n        acc_buffer.append(numpy.sum(correct_prediction) / batch_size)\n\n    print(""test accuracy for the stored model: %g"" % numpy.mean(acc_buffer))\n\nif __name__ == \'__main__\':\n    # Parse argument\n    parser = build_parser()\n    options = parser.parse_args()\n    ensemble = options.ensemble\n    model_directory = options.model_directory\n    batch_size = options.batch_size\n\n    # Select ensemble test or a single model test\n    if ensemble==\'True\': # use ensemble model\n        model_directory_list = [x[0] for x in os.walk(model_directory)]\n        test_ensemble(model_directory_list[1:], batch_size)\n    else: # test a single model\n        # test_org(model_directory, batch_size) #test with test data given by mnist_data.py\n        test(model_directory+\'/model.ckpt\',\n             batch_size)  # test with test data given by tensorflow.examples.tutorials.mnist.input_data()\n\n'"
mnist_cnn_train.py,19,"b'# Some code was borrowed from https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/mnist/convolutional.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nimport mnist_data\nimport cnn_model\n\n\nMODEL_DIRECTORY = ""model/model.ckpt""\nLOGS_DIRECTORY = ""logs/train""\n\n# Params for Train\ntraining_epochs = 10# 10 for augmented training data, 20 for training data\nTRAIN_BATCH_SIZE = 50\ndisplay_step = 100\nvalidation_step = 500\n\n# Params for test\nTEST_BATCH_SIZE = 5000\n\ndef train():\n\n    # Some parameters\n    batch_size = TRAIN_BATCH_SIZE\n    num_labels = mnist_data.NUM_LABELS\n\n    # Prepare mnist data\n    train_total_data, train_size, validation_data, validation_labels, test_data, test_labels = mnist_data.prepare_MNIST_data(True)\n\n    # Boolean for MODE of train or test\n    is_training = tf.placeholder(tf.bool, name=\'MODE\')\n\n    # tf Graph input\n    x = tf.placeholder(tf.float32, [None, 784])\n    y_ = tf.placeholder(tf.float32, [None, 10]) #answer\n\n    # Predict\n    y = cnn_model.CNN(x)\n\n    # Get loss of model\n    with tf.name_scope(""LOSS""):\n        loss = slim.losses.softmax_cross_entropy(y,y_)\n\n    # Create a summary to monitor loss tensor\n    tf.scalar_summary(\'loss\', loss)\n\n    # Define optimizer\n    with tf.name_scope(""ADAM""):\n        # Optimizer: set up a variable that\'s incremented once per batch and\n        # controls the learning rate decay.\n        batch = tf.Variable(0)\n\n        learning_rate = tf.train.exponential_decay(\n            1e-4,  # Base learning rate.\n            batch * batch_size,  # Current index into the dataset.\n            train_size,  # Decay step.\n            0.95,  # Decay rate.\n            staircase=True)\n        # Use simple momentum for the optimization.\n        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=batch)\n\n    # Create a summary to monitor learning_rate tensor\n    tf.scalar_summary(\'learning_rate\', learning_rate)\n\n    # Get accuracy of model\n    with tf.name_scope(""ACC""):\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    # Create a summary to monitor accuracy tensor\n    tf.scalar_summary(\'acc\', accuracy)\n\n    # Merge all summaries into a single op\n    merged_summary_op = tf.merge_all_summaries()\n\n    # Add ops to save and restore all the variables\n    saver = tf.train.Saver()\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer(), feed_dict={is_training: True})\n\n    # Training cycle\n    total_batch = int(train_size / batch_size)\n\n    # op to write logs to Tensorboard\n    summary_writer = tf.train.SummaryWriter(LOGS_DIRECTORY, graph=tf.get_default_graph())\n\n    # Save the maximum accuracy value for validation data\n    max_acc = 0.\n\n    # Loop for epoch\n    for epoch in range(training_epochs):\n\n        # Random shuffling\n        numpy.random.shuffle(train_total_data)\n        train_data_ = train_total_data[:, :-num_labels]\n        train_labels_ = train_total_data[:, -num_labels:]\n\n        # Loop over all batches\n        for i in range(total_batch):\n\n            # Compute the offset of the current minibatch in the data.\n            offset = (i * batch_size) % (train_size)\n            batch_xs = train_data_[offset:(offset + batch_size), :]\n            batch_ys = train_labels_[offset:(offset + batch_size), :]\n\n            # Run optimization op (backprop), loss op (to get loss value)\n            # and summary nodes\n            _, train_accuracy, summary = sess.run([train_step, accuracy, merged_summary_op] , feed_dict={x: batch_xs, y_: batch_ys, is_training: True})\n\n            # Write logs at every iteration\n            summary_writer.add_summary(summary, epoch * total_batch + i)\n\n            # Display logs\n            if i % display_step == 0:\n                print(""Epoch:"", \'%04d,\' % (epoch + 1),\n                ""batch_index %4d/%4d, training accuracy %.5f"" % (i, total_batch, train_accuracy))\n\n            # Get accuracy for validation data\n            if i % validation_step == 0:\n                # Calculate accuracy\n                validation_accuracy = sess.run(accuracy,\n                feed_dict={x: validation_data, y_: validation_labels, is_training: False})\n\n                print(""Epoch:"", \'%04d,\' % (epoch + 1),\n                ""batch_index %4d/%4d, validation accuracy %.5f"" % (i, total_batch, validation_accuracy))\n\n            # Save the current model if the maximum accuracy is updated\n            if validation_accuracy > max_acc:\n                max_acc = validation_accuracy\n                save_path = saver.save(sess, MODEL_DIRECTORY)\n                print(""Model updated and saved in file: %s"" % save_path)\n\n    print(""Optimization Finished!"")\n\n    # Restore variables from disk\n    saver.restore(sess, MODEL_DIRECTORY)\n\n    # Calculate accuracy for all mnist test images\n    test_size = test_labels.shape[0]\n    batch_size = TEST_BATCH_SIZE\n    total_batch = int(test_size / batch_size)\n\n    acc_buffer = []\n\n    # Loop over all batches\n    for i in range(total_batch):\n        # Compute the offset of the current minibatch in the data.\n        offset = (i * batch_size) % (test_size)\n        batch_xs = test_data[offset:(offset + batch_size), :]\n        batch_ys = test_labels[offset:(offset + batch_size), :]\n\n        y_final = sess.run(y, feed_dict={x: batch_xs, y_: batch_ys, is_training: False})\n        correct_prediction = numpy.equal(numpy.argmax(y_final, 1), numpy.argmax(batch_ys, 1))\n        acc_buffer.append(numpy.sum(correct_prediction) / batch_size)\n\n    print(""test accuracy for the stored model: %g"" % numpy.mean(acc_buffer))\n\nif __name__ == \'__main__\':\n    train()\n'"
mnist_data.py,4,"b'# Some code was borrowed from https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/mnist/convolutional.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\n\nimport numpy\nfrom scipy import ndimage\n\nfrom six.moves import urllib\n\nimport tensorflow as tf\n\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\nDATA_DIRECTORY = ""data""\n\n# Params for MNIST\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\n\n# Download MNIST data\ndef maybe_download(filename):\n    """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n    if not tf.gfile.Exists(DATA_DIRECTORY):\n        tf.gfile.MakeDirs(DATA_DIRECTORY)\n    filepath = os.path.join(DATA_DIRECTORY, filename)\n    if not tf.gfile.Exists(filepath):\n        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n        with tf.gfile.GFile(filepath) as f:\n            size = f.size()\n        print(\'Successfully downloaded\', filename, size, \'bytes.\')\n    return filepath\n\n# Extract the images\ndef extract_data(filename, num_images):\n    """"""Extract the images into a 4D tensor [image index, y, x, channels].\n\n    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n    """"""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(16)\n        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n        data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n        data = numpy.reshape(data, [num_images, -1])\n    return data\n\n# Extract the labels\ndef extract_labels(filename, num_images):\n    """"""Extract the labels into a vector of int64 label IDs.""""""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(8)\n        buf = bytestream.read(1 * num_images)\n        labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n        num_labels_data = len(labels)\n        one_hot_encoding = numpy.zeros((num_labels_data,NUM_LABELS))\n        one_hot_encoding[numpy.arange(num_labels_data),labels] = 1\n        one_hot_encoding = numpy.reshape(one_hot_encoding, [-1, NUM_LABELS])\n    return one_hot_encoding\n\n# Augment training data\ndef expend_training_data(images, labels):\n\n    expanded_images = []\n    expanded_labels = []\n\n    j = 0 # counter\n    for x, y in zip(images, labels):\n        j = j+1\n        if j%100==0:\n            print (\'expanding data : %03d / %03d\' % (j,numpy.size(images,0)))\n\n        # register original data\n        expanded_images.append(x)\n        expanded_labels.append(y)\n\n        # get a value for the background\n        # zero is the expected value, but median() is used to estimate background\'s value \n        bg_value = numpy.median(x) # this is regarded as background\'s value        \n        image = numpy.reshape(x, (-1, 28))\n\n        for i in range(4):\n            # rotate the image with random degree\n            angle = numpy.random.randint(-15,15,1)\n            new_img = ndimage.rotate(image,angle,reshape=False, cval=bg_value)\n\n            # shift the image with random distance\n            shift = numpy.random.randint(-2, 2, 2)\n            new_img_ = ndimage.shift(new_img,shift, cval=bg_value)\n\n            # register new training data\n            expanded_images.append(numpy.reshape(new_img_, 784))\n            expanded_labels.append(y)\n\n    # images and labels are concatenated for random-shuffle at each epoch\n    # notice that pair of image and label should not be broken\n    expanded_train_total_data = numpy.concatenate((expanded_images, expanded_labels), axis=1)\n    numpy.random.shuffle(expanded_train_total_data)\n\n    return expanded_train_total_data\n\n# Prepare MNISt data\ndef prepare_MNIST_data(use_data_augmentation=True):\n    # Get the data.\n    train_data_filename = maybe_download(\'train-images-idx3-ubyte.gz\')\n    train_labels_filename = maybe_download(\'train-labels-idx1-ubyte.gz\')\n    test_data_filename = maybe_download(\'t10k-images-idx3-ubyte.gz\')\n    test_labels_filename = maybe_download(\'t10k-labels-idx1-ubyte.gz\')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, :]\n    validation_labels = train_labels[:VALIDATION_SIZE,:]\n    train_data = train_data[VALIDATION_SIZE:, :]\n    train_labels = train_labels[VALIDATION_SIZE:,:]\n\n    # Concatenate train_data & train_labels for random shuffle\n    if use_data_augmentation:\n        train_total_data = expend_training_data(train_data, train_labels)\n    else:\n        train_total_data = numpy.concatenate((train_data, train_labels), axis=1)\n\n    train_size = train_total_data.shape[0]\n\n    return train_total_data, train_size, validation_data, validation_labels, test_data, test_labels\n\n\n'"
