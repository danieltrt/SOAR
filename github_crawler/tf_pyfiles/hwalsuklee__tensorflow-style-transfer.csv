file_path,api_count,code
run_main.py,1,"b'import tensorflow as tf\nimport numpy as np\nimport utils\nimport vgg19\nimport style_transfer\nimport os\n\nimport argparse\n\n""""""parsing and configuration""""""\ndef parse_args():\n    desc = ""Tensorflow implementation of \'Image Style Transfer Using Convolutional Neural Networks""\n    parser = argparse.ArgumentParser(description=desc)\n\n    parser.add_argument(\'--model_path\', type=str, default=\'pre_trained_model\', help=\'The directory where the pre-trained model was saved\')\n    parser.add_argument(\'--content\', type=str, default=\'images/tubingen.jpg\', help=\'File path of content image (notation in the paper : p)\', required = True)\n    parser.add_argument(\'--style\', type=str, default=\'images/starry-night.jpg\', help=\'File path of style image (notation in the paper : a)\', required = True)\n    parser.add_argument(\'--output\', type=str, default=\'result.jpg\', help=\'File path of output image\', required = True)\n\t\n    parser.add_argument(\'--loss_ratio\', type=float, default=1e-3, help=\'Weight of content-loss relative to style-loss\')\n\n    parser.add_argument(\'--content_layers\', nargs=\'+\', type=str, default=[\'conv4_2\'], help=\'VGG19 layers used for content loss\')\n    parser.add_argument(\'--style_layers\', nargs=\'+\', type=str, default=[\'relu1_1\', \'relu2_1\', \'relu3_1\', \'relu4_1\', \'relu5_1\'],\n                        help=\'VGG19 layers used for style loss\')\n\n    parser.add_argument(\'--content_layer_weights\', nargs=\'+\', type=float, default=[1.0], help=\'Content loss for each content is multiplied by corresponding weight\')\n    parser.add_argument(\'--style_layer_weights\', nargs=\'+\', type=float, default=[.2,.2,.2,.2,.2],\n                        help=\'Style loss for each content is multiplied by corresponding weight\')\n\n    parser.add_argument(\'--initial_type\', type=str, default=\'content\', choices=[\'random\',\'content\',\'style\'], help=\'The initial image for optimization (notation in the paper : x)\')\n    parser.add_argument(\'--max_size\', type=int, default=512, help=\'The maximum width or height of input images\')\n    parser.add_argument(\'--content_loss_norm_type\', type=int, default=3, choices=[1,2,3], help=\'Different types of normalization for content loss\')\n    parser.add_argument(\'--num_iter\', type=int, default=1000, help=\'The number of iterations to run\')\n\n    return check_args(parser.parse_args())\n\n""""""checking arguments""""""\ndef check_args(args):\n    try:\n        assert len(args.content_layers) == len(args.content_layer_weights)\n    except:\n        print (\'content layer info and weight info must be matched\')\n        return None\n    try:\n        assert len(args.style_layers) == len(args.style_layer_weights)\n    except:\n        print(\'style layer info and weight info must be matched\')\n        return None\n\n    try:\n        assert args.max_size > 100\n    except:\n        print (\'Too small size\')\n        return None\n\n    model_file_path = args.model_path + \'/\' + vgg19.MODEL_FILE_NAME\n    try:\n        assert os.path.exists(model_file_path)\n    except:\n        print (\'There is no %s\'%model_file_path)\n        return None\n\n    try:\n        size_in_KB = os.path.getsize(model_file_path)\n        assert abs(size_in_KB - 534904783) < 10\n    except:\n        print(\'check file size of \\\'imagenet-vgg-verydeep-19.mat\\\'\')\n        print(\'there are some files with the same name\')\n        print(\'pre_trained_model used here can be downloaded from bellow\')\n        print(\'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\')\n        return None\n\n    try:\n        assert os.path.exists(args.content)\n    except:\n        print(\'There is no %s\'%args.content)\n        return None\n\n    try:\n        assert os.path.exists(args.style)\n    except:\n        print(\'There is no %s\' % args.style)\n        return None\n\n    return args\n\n""""""add one dim for batch""""""\n# VGG19 requires input dimension to be (batch, height, width, channel)\ndef add_one_dim(image):\n    shape = (1,) + image.shape\n    return np.reshape(image, shape)\n\n""""""main""""""\ndef main():\n\n    # parse arguments\n    args = parse_args()\n    if args is None:\n        exit()\n\n    # initiate VGG19 model\n    model_file_path = args.model_path + \'/\' + vgg19.MODEL_FILE_NAME\n    vgg_net = vgg19.VGG19(model_file_path)\n\n    # load content image and style image\n    content_image = utils.load_image(args.content, max_size=args.max_size)\n    style_image = utils.load_image(args.style, shape=(content_image.shape[1],content_image.shape[0]))\n\n    # initial guess for output\n    if args.initial_type == \'content\':\n        init_image = content_image\n    elif args.initial_type == \'style\':\n        init_image = style_image\n    elif args.initial_type == \'random\':\n        init_image = np.random.normal(size=content_image.shape, scale=np.std(content_image))\n\n    # check input images for style-transfer\n    # utils.plot_images(content_image,style_image, init_image)\n\n    # create a map for content layers info\n    CONTENT_LAYERS = {}\n    for layer, weight in zip(args.content_layers,args.content_layer_weights):\n        CONTENT_LAYERS[layer] = weight\n\n    # create a map for style layers info\n    STYLE_LAYERS = {}\n    for layer, weight in zip(args.style_layers, args.style_layer_weights):\n        STYLE_LAYERS[layer] = weight\n\n\n    # open session\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\n    # build the graph\n    st = style_transfer.StyleTransfer(session = sess,\n                                      content_layer_ids = CONTENT_LAYERS,\n                                      style_layer_ids = STYLE_LAYERS,\n                                      init_image = add_one_dim(init_image),\n                                      content_image = add_one_dim(content_image),\n                                      style_image = add_one_dim(style_image),\n                                      net = vgg_net,\n                                      num_iter = args.num_iter,\n                                      loss_ratio = args.loss_ratio,\n                                      content_loss_norm_type = args.content_loss_norm_type,\n                                      )\n    # launch the graph in a session\n    result_image = st.update()\n\n    # close session\n    sess.close()\n\n    # remove batch dimension\n    shape = result_image.shape\n    result_image = np.reshape(result_image,shape[1:])\n\n    # save result\n    utils.save_image(result_image,args.output)\n\n    # utils.plot_images(content_image,style_image, result_image)\n\nif __name__ == \'__main__\':\n    main()\n'"
style_transfer.py,11,"b'import tensorflow as tf\nimport numpy as np\nimport collections\n\nclass StyleTransfer:\n\n    def __init__(self, content_layer_ids, style_layer_ids, init_image, content_image,\n                 style_image, session, net, num_iter, loss_ratio, content_loss_norm_type):\n\n        self.net = net\n        self.sess = session\n\n        # sort layers info\n        self.CONTENT_LAYERS = collections.OrderedDict(sorted(content_layer_ids.items()))\n        self.STYLE_LAYERS = collections.OrderedDict(sorted(style_layer_ids.items()))\n\n        # preprocess input images\n        self.p0 = np.float32(self.net.preprocess(content_image))\n        self.a0 = np.float32(self.net.preprocess(style_image))\n        self.x0 = np.float32(self.net.preprocess(init_image))\n\n        # parameters for optimization\n        self.content_loss_norm_type = content_loss_norm_type\n        self.num_iter = num_iter\n        self.loss_ratio = loss_ratio\n\n        # build graph for style transfer\n        self._build_graph()\n\n    def _build_graph(self):\n\n        """""" prepare data """"""\n        # this is what must be trained\n        self.x = tf.Variable(self.x0, trainable=True, dtype=tf.float32)\n\n        # graph input\n        self.p = tf.placeholder(tf.float32, shape=self.p0.shape, name=\'content\')\n        self.a = tf.placeholder(tf.float32, shape=self.a0.shape, name=\'style\')\n\n        # get content-layer-feature for content loss\n        content_layers = self.net.feed_forward(self.p, scope=\'content\')\n        self.Ps = {}\n        for id in self.CONTENT_LAYERS:\n            self.Ps[id] = content_layers[id]\n\n        # get style-layer-feature for style loss\n        style_layers = self.net.feed_forward(self.a, scope=\'style\')\n        self.As = {}\n        for id in self.STYLE_LAYERS:\n            self.As[id] = self._gram_matrix(style_layers[id])\n        \n        # get layer-values for x\n        self.Fs = self.net.feed_forward(self.x, scope=\'mixed\')\n\n        """""" compute loss """"""\n        L_content = 0\n        L_style = 0\n        for id in self.Fs:\n            if id in self.CONTENT_LAYERS:\n                ## content loss ##\n\n                F = self.Fs[id]            # content feature of x\n                P = self.Ps[id]            # content feature of p\n\n                _, h, w, d = F.get_shape() # first return value is batch size (must be one)\n                N = h.value*w.value        # product of width and height\n                M = d.value                # number of filters\n\n                w = self.CONTENT_LAYERS[id]# weight for this layer\n\n                # You may choose different normalization constant\n                if self.content_loss_norm_type==1:\n                    L_content += w * tf.reduce_sum(tf.pow((F-P), 2)) / 2 # original paper\n                elif self.content_loss_norm_type == 2:\n                    L_content += w * tf.reduce_sum(tf.pow((F-P), 2)) / (N*M) #artistic style transfer for videos\n                elif self.content_loss_norm_type == 3: # this is from https://github.com/cysmith/neural-style-tf/blob/master/neural_style.py\n                    L_content += w * (1. / (2. * np.sqrt(M) * np.sqrt(N))) * tf.reduce_sum(tf.pow((F - P), 2))\n\n            elif id in self.STYLE_LAYERS:\n                ## style loss ##\n\n                F = self.Fs[id]\n\n                _, h, w, d = F.get_shape()  # first return value is batch size (must be one)\n                N = h.value * w.value       # product of width and height\n                M = d.value                 # number of filters\n\n                w = self.STYLE_LAYERS[id]   # weight for this layer\n\n                G = self._gram_matrix(F)    # style feature of x\n                A = self.As[id]             # style feature of a\n\n                L_style += w * (1. / (4 * N ** 2 * M ** 2)) * tf.reduce_sum(tf.pow((G-A), 2))\n\n\n        # fix beta as 1\n        alpha = self.loss_ratio\n        beta = 1\n\n        self.L_content = L_content\n        self.L_style = L_style\n        self.L_total = alpha*L_content + beta*L_style\n\n    def update(self):\n        """""" define optimizer L-BFGS """"""\n        # this call back function is called every after loss is updated\n        global _iter\n        _iter = 0\n        def callback(tl, cl, sl):\n            global _iter\n            print(\'iter : %4d, \' % _iter, \'L_total : %g, L_content : %g, L_style : %g\' % (tl, cl, sl))\n            _iter += 1\n\n        optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.L_total, method=\'L-BFGS-B\', options={\'maxiter\': self.num_iter})\n\n        """""" session run """"""\n        # initialize variables\n        init_op = tf.global_variables_initializer()\n        self.sess.run(init_op)\n\n        # optmization\n        optimizer.minimize(self.sess,feed_dict={self.a:self.a0, self.p:self.p0},\n                           fetches=[self.L_total, self.L_content, self.L_style], loss_callback=callback)\n\n        """""" get final result """"""\n        final_image = self.sess.run(self.x)\n\n        # ensure the image has valid pixel-values between 0 and 255\n        final_image = np.clip(self.net.undo_preprocess(final_image), 0.0, 255.0)\n\n        return final_image\n\n    def _gram_matrix(self, tensor):\n\n        shape = tensor.get_shape()\n\n        # Get the number of feature channels for the input tensor,\n        # which is assumed to be from a convolutional layer with 4-dim.\n        num_channels = int(shape[3])\n\n        # Reshape the tensor so it is a 2-dim matrix. This essentially\n        # flattens the contents of each feature-channel.\n        matrix = tf.reshape(tensor, shape=[-1, num_channels])\n\n        # Calculate the Gram-matrix as the matrix-product of\n        # the 2-dim matrix with itself. This calculates the\n        # dot-products of all combinations of the feature-channels.\n        gram = tf.matmul(tf.transpose(matrix), matrix)\n\n        return gram\n\n\n\n\n\n\n\n\n\n\n'"
utils.py,0,"b'# Most code in this file was borrowed from https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\n\n""""""Helper-functions for image manipulation""""""\n# This function loads an image and returns it as a numpy array of floating-points.\n# The image can be automatically resized so the largest of the height or width equals max_size.\n# or resized to the given shape\ndef load_image(filename, shape=None, max_size=None):\n    image = PIL.Image.open(filename)\n\n    if max_size is not None:\n        # Calculate the appropriate rescale-factor for\n        # ensuring a max height and width, while keeping\n        # the proportion between them.\n        factor = float(max_size) / np.max(image.size)\n\n        # Scale the image\'s height and width.\n        size = np.array(image.size) * factor\n\n        # The size is now floating-point because it was scaled.\n        # But PIL requires the size to be integers.\n        size = size.astype(int)\n\n        # Resize the image.\n        image = image.resize(size, PIL.Image.LANCZOS) # PIL.Image.LANCZOS is one of resampling filter\n\n    if shape is not None:\n        image = image.resize(shape, PIL.Image.LANCZOS) # PIL.Image.LANCZOS is one of resampling filter\n\n    # Convert to numpy floating-point array.\n    return np.float32(image)\n\n# Save an image as a jpeg-file.\n# The image is given as a numpy array with pixel-values between 0 and 255.\ndef save_image(image, filename):\n    # Ensure the pixel-values are between 0 and 255.\n    image = np.clip(image, 0.0, 255.0)\n\n    # Convert to bytes.\n    image = image.astype(np.uint8)\n\n    # Write the image-file in jpeg-format.\n    with open(filename, \'wb\') as file:\n        PIL.Image.fromarray(image).save(file, \'jpeg\')\n\n# This function plots the content-, mixed- and style-images.\ndef plot_images(content_image, style_image, mixed_image):\n    # Create figure with sub-plots.\n    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n\n    # Adjust vertical spacing.\n    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n\n    # Plot the content-image.\n    # Note that the pixel-values are normalized to\n    # the [0.0, 1.0] range by dividing with 255.\n    ax = axes.flat[0]\n    ax.imshow(content_image / 255.0, interpolation=\'sinc\')\n    ax.set_xlabel(""Content"")\n\n    # Plot the mixed-image.\n    ax = axes.flat[1]\n    ax.imshow(mixed_image / 255.0, interpolation=\'sinc\')\n    ax.set_xlabel(""Output"")\n\n    # Plot the style-image\n    ax = axes.flat[2]\n    ax.imshow(style_image / 255.0, interpolation=\'sinc\')\n    ax.set_xlabel(""Style"")\n\n    # Remove ticks from all the plots.\n    for ax in axes.flat:\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    plt.show()\n'"
vgg19.py,5,"b""# Copyright (c) 2015-2016 Anish Athalye. Released under GPLv3.\n# Most code in this file was borrowed from https://github.com/anishathalye/neural-style/blob/master/vgg.py\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.io\n\n# download URL : http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\nMODEL_FILE_NAME = 'imagenet-vgg-verydeep-19.mat'\n\ndef _conv_layer(input, weights, bias):\n    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n            padding='SAME')\n    return tf.nn.bias_add(conv, bias)\n\ndef _pool_layer(input):\n    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n            padding='SAME')\n\ndef preprocess(image, mean_pixel):\n    return image - mean_pixel\n\ndef undo_preprocess(image, mean_pixel):\n    return image + mean_pixel\n\nclass VGG19:\n    layers = (\n        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n\n        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n\n        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n\n        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n\n        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n        'relu5_3', 'conv5_4', 'relu5_4'\n    )\n\n    def __init__(self, data_path):\n        data = scipy.io.loadmat(data_path)\n\n        self.mean_pixel = np.array([123.68, 116.779, 103.939])\n\n        self.weights = data['layers'][0]\n\n    def preprocess(self, image):\n        return image-self.mean_pixel\n\n    def undo_preprocess(self,image):\n        return image+self.mean_pixel\n\n    def feed_forward(self, input_image, scope=None):\n        net = {}\n        current = input_image\n\n        with tf.variable_scope(scope):\n            for i, name in enumerate(self.layers):\n                kind = name[:4]\n                if kind == 'conv':\n                    kernels = self.weights[i][0][0][2][0][0]\n                    bias = self.weights[i][0][0][2][0][1]\n\n                    # matconvnet: weights are [width, height, in_channels, out_channels]\n                    # tensorflow: weights are [height, width, in_channels, out_channels]\n                    kernels = np.transpose(kernels, (1, 0, 2, 3))\n                    bias = bias.reshape(-1)\n\n                    current = _conv_layer(current, kernels, bias)\n                elif kind == 'relu':\n                    current = tf.nn.relu(current)\n                elif kind == 'pool':\n                    current = _pool_layer(current)\n                net[name] = current\n\n        assert len(net) == len(self.layers)\n        return net"""
