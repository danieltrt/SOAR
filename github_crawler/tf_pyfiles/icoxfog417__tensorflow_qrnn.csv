file_path,api_count,code
test_tf_qrnn_forward.py,12,"b'import unittest\nimport numpy as np\nimport tensorflow as tf\nfrom tf_qrnn import QRNN\n\n\nclass TestQRNNForward(unittest.TestCase):\n\n    def test_qrnn_linear_forward(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_linear:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=1)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def test_qrnn_with_previous(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_with_previous:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=2)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def test_qrnn_convolution(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_conv:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=3)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def create_test_data(self, batch_size, sentence_length, word_size):\n        batch = []\n        for b in range(batch_size):\n            sentence = np.random.rand(sentence_length, word_size)\n            batch.append(sentence)\n        return np.array(batch)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test_tf_qrnn_work.py,36,"b'import os\nimport unittest\nimport time\nimport functools\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell\nfrom tensorflow import nn\nfrom tf_qrnn import QRNN\n\ndef measure_time(func):\n    @functools.wraps(func)\n    def _measure_time(*args, **kwargs):\n        start = time.time()\n        func(*args, **kwargs)\n        elapse = time.time() - start\n        print(""takes {} seconds."".format(elapse))\n    return _measure_time\n\n\nclass TestQRNNWork(unittest.TestCase):\n\n    @measure_time\n    def test_qrnn(self):\n        print(""QRNN Working check"")\n        with tf.Graph().as_default() as qrnn:\n            self.check_by_digits(qrnn, qrnn=5)\n\n    @measure_time\n    def test_baseline(self):\n        print(""Baseline(LSTM) Working check"")\n        with tf.Graph().as_default() as baseline:\n            self.check_by_digits(baseline, baseline=True)\n\n    @measure_time\n    def test_random(self):\n        print(""Random Working check"")\n        with tf.Graph().as_default() as random:\n            self.check_by_digits(random, random=True)\n\n    def check_by_digits(self, graph, qrnn=-1, baseline=False, random=False):\n        digits = load_digits()\n        horizon, vertical, n_class = (8, 8, 10)  # 8 x 8 image, 0~9 number(=10 class)\n        size = 128  # state vector size\n        batch_size = 128\n        images = digits.images / np.max(digits.images)  # simple normalization\n        target = np.array([[1 if t == i else 0 for i in range(n_class)] for t in digits.target])  # to 1 hot vector\n        learning_rate = 0.001\n        train_iter = 1000\n        summary_dir = os.path.join(os.path.dirname(__file__), ""./summary"")\n\n        with tf.name_scope(""placeholder""):\n            X = tf.placeholder(tf.float32, [batch_size, vertical, horizon])\n            y = tf.placeholder(tf.float32, [batch_size, n_class])\n\n        if qrnn > 0:\n            pred = self.qrnn_forward(X, size, n_class, batch_size, conv_size=qrnn)\n            summary_dir += ""/qrnn""\n        elif baseline:\n            pred = self.baseline_forward(X, size, n_class)\n            summary_dir += ""/lstm""\n        else:\n            pred = self.random_forward(X, size, n_class)            \n            summary_dir += ""/random""\n        \n        with tf.name_scope(""optimization""):\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n        with tf.name_scope(""evaluation""):\n            correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n        with tf.name_scope(""summary""):\n            tf.summary.scalar(""loss"", loss)\n            tf.summary.scalar(""accuracy"", accuracy)\n            merged = tf.summary.merge_all()\n        writer = tf.summary.FileWriter(summary_dir, graph)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            for i in range(train_iter):\n                indices = np.random.randint(len(digits.target) - batch_size, size=batch_size)\n                _X = images[indices]\n                _y = target[indices]\n                sess.run(optimizer, feed_dict={X: _X, y: _y})\n\n                if i % 100 == 0:\n                    _loss, _accuracy, _merged = sess.run([loss, accuracy, merged], feed_dict={X: _X, y: _y})\n                    writer.add_summary(_merged, i)\n                    print(""Iter {}: loss={}, accuracy={}"".format(i, _loss, _accuracy))\n            \n            with tf.name_scope(""test-evaluation""):\n                acc = sess.run(accuracy, feed_dict={X: images[-batch_size:], y: target[-batch_size:]})\n                print(""Testset Accuracy={}"".format(acc))\n    \n    def baseline_forward(self, X, size, n_class):\n        shape = X.get_shape()\n        seq = tf.transpose(X, [1, 0, 2]) \n\n        with tf.name_scope(""LSTM""):\n            lstm_cell = LSTMCell(size, forget_bias=1.0)\n            outputs, states = nn.dynamic_rnn(time_major=True, cell=lstm_cell, inputs=seq, dtype=tf.float32)\n\n        with tf.name_scope(""LSTM-Classifier""):\n            W = tf.Variable(tf.random_normal([size, n_class]), name=""W"")\n            b = tf.Variable(tf.random_normal([n_class]), name=""b"")\n            output = tf.matmul(outputs[-1], W) + b\n\n        return output\n\n    def random_forward(self, X, size, n_class):\n        batch_size = int(X.get_shape()[0])\n\n        with tf.name_scope(""Random-Classifier""):\n            rand_vector = tf.random_normal([batch_size, size])  # batch_size x size random vector\n            W = tf.Variable(tf.random_normal([size, n_class]), name=""W"")\n            b = tf.Variable(tf.random_normal([n_class]), name=""b"")\n            output = tf.matmul(rand_vector, W) + b\n        return output\n\n    def qrnn_forward(self, X, size, n_class, batch_size, conv_size):\n        in_size = int(X.get_shape()[2])\n\n        qrnn = QRNN(in_size=in_size, size=size, conv_size=conv_size)\n        hidden = qrnn.forward(X)\n\n        with tf.name_scope(""QRNN-Classifier""):\n            W = tf.Variable(tf.random_normal([size, n_class]), name=""W"")\n            b = tf.Variable(tf.random_normal([n_class]), name=""b"")\n            output = tf.add(tf.matmul(hidden, W), b)\n\n        return output\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tf_qrnn.py,33,"b'import tensorflow as tf\n\n\nclass QRNN():\n\n    def __init__(self, in_size, size, conv_size=2):\n        self.kernel = None\n        self.batch_size = -1\n        self.conv_size = conv_size\n        self.c = None\n        self.h = None\n        self._x = None\n        if conv_size == 1:\n            self.kernel = QRNNLinear(in_size, size)\n        elif conv_size == 2:\n            self.kernel = QRNNWithPrevious(in_size, size)\n        else:\n            self.kernel = QRNNConvolution(in_size, size, conv_size)\n\n    def _step(self, f, z, o):\n        with tf.variable_scope(""fo-Pool""):\n            # f,z,o is batch_size x size\n            f = tf.sigmoid(f)\n            z = tf.tanh(z)\n            o = tf.sigmoid(o)\n            self.c = tf.multiply(f, self.c) + tf.multiply(1 - f, z)\n            self.h = tf.multiply(o, self.c)  # h is size vector\n\n        return self.h\n\n    def forward(self, x):\n        length = lambda mx: int(mx.get_shape()[0])\n\n        with tf.variable_scope(""QRNN/Forward""):\n            if self.c is None:\n                # init context cell\n                self.c = tf.zeros([length(x), self.kernel.size], dtype=tf.float32)\n\n            if self.conv_size <= 2:\n                # x is batch_size x sentence_length x word_length\n                # -> now, transpose it to sentence_length x batch_size x word_length\n                _x = tf.transpose(x, [1, 0, 2])\n\n                for i in range(length(_x)):\n                    t = _x[i] # t is batch_size x word_length matrix\n                    f, z, o = self.kernel.forward(t)\n                    self._step(f, z, o)\n            else:\n                c_f, c_z, c_o = self.kernel.conv(x)\n                for i in range(length(c_f)):\n                    f, z, o = c_f[i], c_z[i], c_o[i]\n                    self._step(f, z, o)\n\n        return self.h\n\n\nclass QRNNLinear():\n\n    def __init__(self, in_size, size):\n        self.in_size = in_size\n        self.size = size\n        self._weight_size = self.size * 3  # z, f, o\n        with tf.variable_scope(""QRNN/Variable/Linear""):\n            initializer = tf.random_normal_initializer()\n            self.W = tf.get_variable(""W"", [self.in_size, self._weight_size], initializer=initializer)\n            self.b = tf.get_variable(""b"", [self._weight_size], initializer=initializer)\n\n    def forward(self, t):\n        # x is batch_size x word_length matrix\n        _weighted = tf.matmul(t, self.W)\n        _weighted = tf.add(_weighted, self.b)\n\n        # now, _weighted is batch_size x weight_size\n        f, z, o = tf.split(_weighted, num_or_size_splits=3, axis=1)  # split to f, z, o. each matrix is batch_size x size\n        return f, z, o\n\n\nclass QRNNWithPrevious():\n\n    def __init__(self, in_size, size):\n        self.in_size = in_size\n        self.size = size\n        self._weight_size = self.size * 3  # z, f, o\n        self._previous = None\n        with tf.variable_scope(""QRNN/Variable/WithPrevious""):\n            initializer = tf.random_normal_initializer()\n            self.W = tf.get_variable(""W"", [self.in_size, self._weight_size], initializer=initializer)\n            self.V = tf.get_variable(""V"", [self.in_size, self._weight_size], initializer=initializer)\n            self.b = tf.get_variable(""b"", [self._weight_size], initializer=initializer)\n\n    def forward(self, t):\n        if self._previous is None:\n            self._previous = tf.get_variable(""previous"", [t.get_shape()[0], self.in_size], initializer=tf.random_normal_initializer())\n\n        _current = tf.matmul(t, self.W)\n        _previous = tf.matmul(self._previous, self.V)\n        _previous = tf.add(_previous, self.b)\n        _weighted = tf.add(_current, _previous)\n\n        f, z, o = tf.split(_weighted, num_or_size_splits=3, axis=1)  # split to f, z, o. each matrix is batch_size x size\n        self._previous = t\n        return f, z, o\n\n\nclass QRNNConvolution():\n\n    def __init__(self, in_size, size, conv_size):\n        self.in_size = in_size\n        self.size = size\n        self.conv_size = conv_size\n        self._weight_size = self.size * 3  # z, f, o\n\n        with tf.variable_scope(""QRNN/Variable/Convolution""):\n            initializer = tf.random_normal_initializer()\n            self.conv_filter = tf.get_variable(""conv_filter"", [conv_size, in_size, self._weight_size], initializer=initializer)\n\n    def conv(self, x):\n        # !! x is batch_size x sentence_length x word_length(=channel) !!\n        _weighted = tf.nn.conv1d(x, self.conv_filter, stride=1, padding=""SAME"", data_format=""NWC"")\n\n        # _weighted is batch_size x conved_size x output_channel\n        _w = tf.transpose(_weighted, [1, 0, 2])  # conved_size x  batch_size x output_channel\n        _ws = tf.split(_w, num_or_size_splits=3, axis=2) # make 3(f, z, o) conved_size x  batch_size x size\n        return _ws\n'"
