file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\nlong_description = '''\nImage ATM is a one-click tool that automates the workflow of a typical\nimage classification pipeline in an opinionated way, this includes:\n\n- Preprocessing and validating input images and labels\n- Starting/terminating cloud instance with GPU support\n- Training\n- Model evaluation\n\nRead the documentation at: https://idealo.github.io/imageatm/\n\nImage ATM is compatible with Python 3.6 and is distributed under the Apache 2.0 license.\n'''\n\nsetup(\n    name='imageatm',\n    version='0.1.1',\n    author='Christopher Lennan, Malgorzata Adamczyk, Gunar Maiwald, Dat Tran',\n    author_email='christopherlennan@gmail.com, m.adamczyk.berlin@gmail.com, gunar.maiwald@web.de, datitran@gmail.com',\n    description='Image classification for everyone',\n    long_description=long_description,\n    license='Apache 2.0',\n    install_requires=[\n        'Keras==2.3.*',\n        'keras-vis>=0.4.1',\n        'tensorflow==2.0.*',\n        'awscli',\n        'Click',\n        'h5py',\n        'matplotlib',\n        'papermill',\n        'Pillow',\n        'scikit-learn',\n        'scipy==1.1.*',\n        'tqdm',\n        'yarl',\n    ],\n    extras_require={\n        'tests': ['pytest==4.3.0', 'pytest-cov==2.6.1', 'pytest-mock', 'mock', 'mypy'],\n        'docs': ['mkdocs==1.0.4', 'mkdocs-material==4.0.2'],\n        'dev': ['bumpversion==0.5.3'],\n    },\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Education',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n    packages=find_packages(exclude=('tests',)),\n    entry_points={'console_scripts': ['imageatm=imageatm.client.client:cli']},\n    include_package_data=True,\n)\n"""
imageatm/__init__.py,0,"b""__version__ = '0.1.1'\n"""
mkdocs/autogen.py,0,"b'# Heavily borrowed from the Auto-Keras project:\n# https://github.com/jhfjhfj1/autokeras/blob/master/mkdocs/autogen.py\n\nimport ast\nimport os\nimport re\n\n\ndef delete_space(parts, start, end):\n    if start > end or end >= len(parts):\n        return None\n    count = 0\n    while count < len(parts[start]):\n        if parts[start][count] == \' \':\n            count += 1\n        else:\n            break\n    return \'\\n\'.join(y for y in [x[count:] for x in parts[start : end + 1] if len(x) > count])\n\n\ndef change_args_to_dict(string):\n    if string is None:\n        return None\n    ans = []\n    strings = string.split(\'\\n\')\n    ind = 1\n    start = 0\n    while ind <= len(strings):\n        if ind < len(strings) and strings[ind].startswith("" ""):\n            ind += 1\n        else:\n            if start < ind:\n                ans.append(\'\\n\'.join(strings[start:ind]))\n            start = ind\n            ind += 1\n    d = {}\n    for line in ans:\n        if "":"" in line and len(line) > 0:\n            lines = line.split("":"")\n            d[lines[0]] = lines[1].strip()\n    return d\n\n\ndef remove_next_line(comments):\n    for x in comments:\n        if comments[x] is not None and \'\\n\' in comments[x]:\n            comments[x] = \' \'.join(comments[x].split(\'\\n\'))\n    return comments\n\n\ndef skip_space_line(parts, ind):\n    while ind < len(parts):\n        if re.match(r\'^\\s*$\', parts[ind]):\n            ind += 1\n        else:\n            break\n    return ind\n\n\n# check if comment is None or len(comment) == 0 return {}\ndef parse_func_string(comment):\n    if comment is None or len(comment) == 0:\n        return {}\n    comments = {}\n    paras = (\'Args\', \'Attributes\', \'Returns\', \'Raises\')\n    comment_parts = [\n        \'short_description\',\n        \'long_description\',\n        \'Args\',\n        \'Attributes\',\n        \'Returns\',\n        \'Raises\',\n    ]\n    for x in comment_parts:\n        comments[x] = None\n\n    parts = re.split(r\'\\n\', comment)\n    ind = 1\n    while ind < len(parts):\n        if re.match(r\'^\\s*$\', parts[ind]):\n            break\n        else:\n            ind += 1\n\n    comments[\'short_description\'] = \'\\n\'.join(\n        [\'\\n\'.join(re.split(\'\\n\\s+\', x.strip())) for x in parts[0:ind]]\n    ).strip(\':\\n\\t \')\n    ind = skip_space_line(parts, ind)\n\n    start = ind\n    while ind < len(parts):\n        if parts[ind].strip().startswith(paras):\n            break\n        else:\n            ind += 1\n    long_description = \'\\n\'.join(\n        [\'\\n\'.join(re.split(\'\\n\\s+\', x.strip())) for x in parts[start:ind]]\n    ).strip(\':\\n\\t \')\n    comments[\'long_description\'] = long_description\n\n    ind = skip_space_line(paras, ind)\n    while ind < len(parts):\n        if parts[ind].strip().startswith(paras):\n            start = ind\n            start_with = parts[ind].strip()\n            ind += 1\n            while ind < len(parts):\n                if parts[ind].strip().startswith(paras):\n                    break\n                else:\n                    ind += 1\n            part = delete_space(parts, start + 1, ind - 1)\n            if start_with.startswith(paras[0]):\n                comments[paras[0]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[1]):\n                comments[paras[1]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[2]):\n                comments[paras[2]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[3]):\n                comments[paras[3]] = part\n            ind = skip_space_line(parts, ind)\n        else:\n            ind += 1\n\n    remove_next_line(comments)\n    return comments\n\n\ndef md_parse_line_break(comment):\n    comment = comment.replace(\'  \', \'\\n\\n\')\n    return comment.replace(\' - \', \'\\n\\n- \')\n\n\ndef to_md(comment_dict):\n    doc = \'\'\n    if \'short_description\' in comment_dict:\n        doc += comment_dict[\'short_description\']\n        doc += \'\\n\\n\'\n\n    if \'long_description\' in comment_dict:\n        doc += md_parse_line_break(comment_dict[\'long_description\'])\n        doc += \'\\n\'\n\n    if \'Args\' in comment_dict and comment_dict[\'Args\'] is not None:\n        doc += \'##### Args\\n\'\n        for arg, des in comment_dict[\'Args\'].items():\n            doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n\n    if \'Attributes\' in comment_dict and comment_dict[\'Attributes\'] is not None:\n        doc += \'##### Attributes\\n\'\n        for arg, des in comment_dict[\'Attributes\'].items():\n            doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n\n    if \'Returns\' in comment_dict and comment_dict[\'Returns\'] is not None:\n        doc += \'##### Returns\\n\'\n        if isinstance(comment_dict[\'Returns\'], str):\n            doc += comment_dict[\'Returns\']\n            doc += \'\\n\'\n        else:\n            for arg, des in comment_dict[\'Returns\'].items():\n                doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n    return doc\n\n\ndef parse_func_args(function):\n    args = [a.arg for a in function.args.args if a.arg != \'self\']\n    kwargs = []\n    if function.args.kwarg:\n        kwargs = [\'**\' + function.args.kwarg.arg]\n\n    return \'(\' + \', \'.join(args + kwargs) + \')\'\n\n\ndef get_func_comments(function_definitions):\n    doc = \'\'\n    for f in function_definitions:\n        temp_str = to_md(parse_func_string(ast.get_docstring(f)))\n        doc += \'\'.join(\n            [\n                \'### \',\n                f.name.replace(\'_\', \'\\\\_\'),\n                \'\\n\',\n                \'```python\',\n                \'\\n\',\n                \'def \',\n                f.name,\n                parse_func_args(f),\n                \'\\n\',\n                \'```\',\n                \'\\n\',\n                temp_str,\n                \'\\n\',\n            ]\n        )\n\n    return doc\n\n\ndef get_comments_str(file_name):\n    with open(file_name) as fd:\n        file_contents = fd.read()\n    module = ast.parse(file_contents)\n\n    function_definitions = [node for node in module.body if isinstance(node, ast.FunctionDef)]\n\n    doc = get_func_comments(function_definitions)\n\n    class_definitions = [node for node in module.body if isinstance(node, ast.ClassDef)]\n    for class_def in class_definitions:\n        temp_str = to_md(parse_func_string(ast.get_docstring(class_def)))\n\n        # excludes private methods (start with \'_\')\n        method_definitions = [\n            node\n            for node in class_def.body\n            if isinstance(node, ast.FunctionDef) and (node.name[0] != \'_\' or node.name[:2] == \'__\')\n        ]\n\n        temp_str += get_func_comments(method_definitions)\n        doc += \'## class \' + class_def.name + \'\\n\' + temp_str\n    return doc\n\n\ndef extract_comments(directory):\n    for parent, dir_names, file_names in os.walk(directory):\n        for file_name in file_names:\n            if os.path.splitext(file_name)[1] == \'.py\' and file_name != \'__init__.py\':\n                # with open\n                doc = get_comments_str(os.path.join(parent, file_name))\n                directory = os.path.join(\'docs\', parent.replace(\'../imageatm/\', \'\'))\n                if not os.path.exists(directory):\n                    os.makedirs(directory)\n\n                output_file = open(os.path.join(directory, file_name[:-3] + \'.md\'), \'w\')\n                output_file.write(doc)\n                output_file.close()\n\n\nextract_comments(\'../imageatm/\')\n'"
imageatm/client/__init__.py,0,b''
imageatm/client/client.py,0,"b'import click\nfrom imageatm.client import commands\nfrom imageatm.client.config import Config\n\n\n# creates config object that will be passed from cli to subcommands\npass_config = click.make_pass_decorator(Config, ensure=True)\n\n\n# cli is the group parent, it gets run before any of the subcommands are run\n@click.group()\n@pass_config\ndef cli(config: Config):\n    pass\n\n\n@cli.command()\n@click.argument(\'config-file\', type=click.Path())\n@click.option(\'--image-dir\', type=click.Path(), help=\'Directory with image files.\')\n@click.option(\'--samples-file\', type=click.Path(), help=\'JSON file with samples.\')\n@click.option(\n    \'--job-dir\',\n    type=click.Path(),\n    help=(\'Directory with train, val, and test samples files and class_mapping file.\'),\n)\n@click.option(\'--provider\', help=\'Cloud provider, currently supported: [aws].\')\n@click.option(\'--instance-type\', help=\'Cloud instance_type [aws].\')\n@click.option(\'--region\', help=\'Cloud region [aws].\')\n@click.option(\'--vpc-id\', help=\'Cloud VPC id [aws].\')\n@click.option(\'--bucket\', help=\'Cloud bucket used for persistence [aws].\')\n@click.option(\'--tf-dir\', help=\'Directory with Terraform configs [aws].\')\n@click.option(\'--train-cloud\', is_flag=True, required=False, help=\'Run training in cloud [aws].\')\n@click.option(\'--destroy\', is_flag=True, required=False, help=\'Destroys cloud.\')\n@click.option(\'--resize\', is_flag=True, required=False, help=\'Resizes images in dataprep.\')\n@click.option(\'--batch-size\', type=int, help=\'Batch size.\', required=False)\n@click.option(\n    \'--epochs-train-dense\',\n    type=int,\n    help=\'Number of epochs train only dense layer.\',\n    required=False,\n)\n@click.option(\n    \'--epochs-train-all\', type=int, help=\'Number of epochs train all layers.\', required=False\n)\n@click.option(\n    \'--learning-rate-dense\', type=float, help=\'Learning rate dense layers.\', required=False\n)\n@click.option(\'--learning-rate-all\', type=float, help=\'Learning rate all layers.\', required=False)\n@click.option(\n    \'--base-model-name\', help=\'Pretrained CNN to be used for transfer learning.\', required=False\n)\n@click.option(\n    \'--create-report\', is_flag=True, required=False, help=\'Create evaluation report via jupyter notebook.\'\n)\n@click.option(\n    \'--kernel-name\',  required=False, help=\'Kernel-name for juypter notebook.\'\n)\n@click.option(\n    \'--export-html\', is_flag=True, required=False, help=\'Export evaluation report to html.\'\n)\n@click.option(\n    \'--export-pdf\', is_flag=True, required=False, help=\'Export evaluation report to pdf.\'\n)\n@click.option(\n    \'--cloud-tag\', help=\'Tag under which all cloud resources are created.\', required=False\n)\n@pass_config\ndef pipeline(config: Config, **kwargs):\n    """"""Runs all components for which run=True in config file.\n\n    All activated (run=True) components from config file will be run in sequence. Options overwrite the config file.\n    The config file is the only way to define pipeline components.\n\n    Args:\n        config-file: Central configuration file.\n    """"""\n    commands.pipeline(config, **kwargs)\n\n\n@cli.command()\n@click.option(\'--config-file\', type=click.Path(), help=\'Central configuration file.\')\n@click.option(\'--image-dir\', type=click.Path(), help=\'Directory with image files.\')\n@click.option(\'--samples-file\', type=click.Path(), help=\'JSON file with samples.\')\n@click.option(\n    \'--job-dir\',\n    type=click.Path(),\n    help=(\'Directory with train, val, and test samples files and class_mapping file.\'),\n)\n@click.option(\n    \'--resize\',\n    is_flag=True,\n    required=False,\n    help=\'Resizes images and stores them in _resized subfolder.\',\n)\n@pass_config\ndef dataprep(config: Config, **kwargs):\n    """"""Run data preparation and create job dir.\n\n    Creates a directory (job_dir) with the following files:\n\n        - train_samples.json\n\n        - val_samples.json\n\n        - test_samples.json\n\n        - class_mapping.json\n    """"""\n    commands.dataprep(config, **kwargs)\n\n\n@cli.command()\n@click.option(\'--config-file\', type=click.Path(), help=\'Central configuration file.\')\n@click.option(\'--image-dir\', type=click.Path(), help=\'Directory with image files.\')\n@click.option(\n    \'--job-dir\',\n    type=click.Path(),\n    help=(\'Directory with train, val, and test samples files and class_mapping file.\'),\n)\n@click.option(\'--provider\', help=\'Cloud provider, currently supported: [aws].\')\n@click.option(\'--instance-type\', help=\'Cloud instance_type [aws].\')\n@click.option(\'--region\', help=\'Cloud region [aws].\')\n@click.option(\'--vpc-id\', help=\'Cloud VPC id [aws].\')\n@click.option(\'--bucket\', help=\'Cloud bucket used for persistence [aws].\')\n@click.option(\'--tf-dir\', help=\'Directory with Terraform configs [aws].\')\n@click.option(\'--train-cloud\', is_flag=True, required=False, help=\'Run training in cloud [aws].\')\n@click.option(\'--destroy\', is_flag=True, required=False, help=\'Destroys cloud.\')\n@click.option(\'--batch-size\', type=int, help=\'Batch size.\', required=False)\n@click.option(\n    \'--epochs-train-dense\',\n    type=int,\n    help=\'Number of epochs train only dense layer.\',\n    required=False,\n)\n@click.option(\n    \'--epochs-train-all\', type=int, help=\'Number of epochs train all layers.\', required=False\n)\n@click.option(\n    \'--learning-rate-dense\', type=float, help=\'Learning rate dense layers.\', required=False\n)\n@click.option(\'--learning-rate-all\', type=float, help=\'Learning rate all layers.\', required=False)\n@click.option(\n    \'--base-model-name\', help=\'Pretrained CNN to be used for transfer learning.\', required=False\n)\n@click.option(\n    \'--cloud-tag\', help=\'Tag under which all cloud resources are created.\', required=False\n)\n@pass_config\ndef train(config: Config, **kwargs):\n    """"""Train a CNN.\n\n    Fine-tunes an ImageNet pre-trained CNN. The number of classes are derived from train_samples.json.\n    After each epoch the model will be evaluated on val_samples.json.\n\n    The best model (based on valuation accuracy) will be saved.\n\n    Args:\n        image_dir: Directory with image files.\n        job_dir: Directory with train_samples, val_samples, and class_mapping.json.\n\n    """"""\n    commands.train(config, **kwargs)\n\n\n@cli.command()\n@click.option(\'--config-file\', type=click.Path(), help=\'Central configuration file.\')\n@click.option(\'--image-dir\', type=click.Path(), help=\'Directory with image files.\')\n@click.option(\n    \'--job-dir\', type=click.Path(), help=(\'Directory with test samples files and trained model.\')\n)\n@click.option(\n    \'--create-report\', is_flag=True, required=False, help=\'Create evaluation report via jupyter notebook.\'\n)\n@click.option(\n    \'--kernel-name\',  required=False, help=\'Kernel-name for juypter notebook.\'\n)\n@click.option(\n    \'--export-html\', is_flag=True, required=False, help=\'Export evaluation report to html.\'\n)\n@click.option(\n    \'--export-pdf\', is_flag=True, required=False, help=\'Export evaluation report to pdf.\'\n)\n@pass_config\ndef evaluate(config: Config, **kwargs):\n    """"""Evaluate a trained model.\n\n    Evaluation will be performed on test_samples.json.\n\n    Args:\n        image_dir: Directory with image files.\n        job_dir: Directory with test_samples.json and class_mapping.json.\n    """"""\n    commands.evaluate(config, **kwargs)\n\n\n@cli.command()\n@click.option(\'--config-file\', type=click.Path(), help=\'Central configuration file.\')\n@click.option(\n    \'--job-dir\', type=click.Path(), help=(\'Directory with test samples files and trained model.\')\n)\n@click.option(\'--provider\', help=\'Cloud provider, currently supported: [aws].\')\n@click.option(\'--instance-type\', help=\'Cloud instance_type [aws].\')\n@click.option(\'--region\', help=\'Cloud region [aws].\')\n@click.option(\'--vpc-id\', help=\'Cloud VPC id [aws].\')\n@click.option(\'--bucket\', help=\'Cloud bucket used for persistence [aws].\')\n@click.option(\'--tf-dir\', help=\'Directory with Terraform configs [aws].\')\n@click.option(\'--train-cloud\', is_flag=True, required=False, help=\'Run training in cloud [aws].\')\n@click.option(\'--destroy\', is_flag=True, required=False, help=\'Destroys cloud.\')\n@click.option(\'--no-destroy\', is_flag=True, required=False, help=\'Keeps cloud.\')\n@click.option(\n    \'--cloud-tag\', help=\'Tag under which all cloud resources are created.\', required=False\n)\n@pass_config\ndef cloud(config: Config, **kwargs):\n    """"""Launch/destroy a cloud compute instance.\n\n    Launch/destroy cloud instances with Terraform based on Terraform files in tf_dir.\n    """"""\n    commands.cloud(config, **kwargs)\n'"
imageatm/client/commands.py,0,"b'from typing import Optional\nfrom pathlib import Path\nfrom imageatm.utils.logger import get_logger\nfrom imageatm.client.config import Config, validate_config, update_config, update_component_configs\n\n\ndef pipeline(\n    config: Config,\n    config_file: Path,\n    job_dir: Optional[Path] = None,\n    image_dir: Optional[Path] = None,\n    samples_file: Optional[Path] = None,\n    provider: Optional[str] = None,\n    instance_type: Optional[str] = None,\n    region: Optional[str] = None,\n    vpc_id: Optional[str] = None,\n    bucket: Optional[str] = None,\n    tf_dir: Optional[Path] = None,\n    train_cloud: Optional[bool] = None,\n    destroy: Optional[bool] = None,\n    resize: Optional[bool] = None,\n    batch_size: Optional[int] = None,\n    learning_rate_dense: Optional[float] = None,\n    learning_rate_all: Optional[float] = None,\n    epochs_train_dense: Optional[int] = None,\n    epochs_train_all: Optional[int] = None,\n    base_model_name: Optional[str] = None,\n    cloud_tag: Optional[str] = None,\n    create_report: Optional[bool] = None,\n    kernel_name: Optional[str] = None,\n    export_html: Optional[bool] = None,\n    export_pdf: Optional[bool] = None,\n):\n    """"""Runs the entire pipeline based on config file.""""""\n    config = update_config(\n        config=config,\n        config_file=config_file,\n        job_dir=job_dir,\n        image_dir=image_dir,\n        samples_file=samples_file,\n        provider=provider,\n        instance_type=instance_type,\n        region=region,\n        vpc_id=vpc_id,\n        bucket=bucket,\n        tf_dir=tf_dir,\n        train_cloud=train_cloud,\n        destroy=destroy,\n        resize=resize,\n        batch_size=batch_size,\n        learning_rate_dense=learning_rate_dense,\n        learning_rate_all=learning_rate_all,\n        epochs_train_dense=epochs_train_dense,\n        epochs_train_all=epochs_train_all,\n        base_model_name=base_model_name,\n        cloud_tag=cloud_tag,\n        create_report=create_report,\n        kernel_name=kernel_name,\n        export_html=export_html,\n        export_pdf=export_pdf,\n    )\n\n    validate_config(config, config.pipeline)\n\n    Path(config.job_dir).resolve().mkdir(parents=True, exist_ok=True)\n\n    logger = get_logger(__name__, config.job_dir)  # type: ignore\n\n    if \'dataprep\' in config.pipeline:\n        from imageatm.scripts import run_dataprep\n\n        logger.info(\n            \'\\n********************************\\n\'\n            \'******* Data preparation *******\\n\'\n            \'********************************\'\n        )\n\n        dp = run_dataprep(**config.dataprep)\n\n        # update image_dir if images were resized\n        if config.dataprep.get(\'resize\', False):\n            config.image_dir = dp.image_dir  # type: ignore\n            config = update_component_configs(config)\n\n    if \'train\' in config.pipeline:\n        logger.info(\n            \'\\n********************************\\n\'\n            \'*********** Training ***********\\n\'\n            \'********************************\'\n        )\n\n        if config.train.get(\'cloud\'):\n            from imageatm.scripts import run_training_cloud\n\n            run_training_cloud(**{**config.cloud, **config.train})\n        else:\n            from imageatm.scripts import run_training\n\n            run_training(**config.train)\n\n    if \'evaluate\' in config.pipeline:\n        from imageatm.scripts import run_evaluation\n\n        logger.info(\n            \'\\n********************************\\n\'\n            \'********** Evaluation **********\\n\'\n            \'********************************\'\n        )\n\n        run_evaluation(**config.evaluate)\n\n    if \'cloud\' in config.pipeline:\n        from imageatm.scripts import run_cloud\n\n        run_cloud(**config.cloud)\n\n\ndef dataprep(\n    config: Config,\n    config_file: Optional[Path] = None,\n    image_dir: Optional[Path] = None,\n    samples_file: Optional[Path] = None,\n    job_dir: Optional[Path] = None,\n    resize: Optional[bool] = None,\n):\n    config = update_config(\n        config=config,\n        config_file=config_file,\n        job_dir=job_dir,\n        image_dir=image_dir,\n        samples_file=samples_file,\n        resize=resize,\n    )\n\n    config.dataprep[\'run\'] = True\n    validate_config(config, [\'dataprep\'])\n\n    from imageatm.scripts import run_dataprep\n\n    run_dataprep(**config.dataprep)\n\n\ndef train(\n    config,\n    config_file: Optional[Path] = None,\n    job_dir: Optional[Path] = None,\n    image_dir: Optional[Path] = None,\n    provider: Optional[str] = None,\n    instance_type: Optional[str] = None,\n    region: Optional[str] = None,\n    vpc_id: Optional[str] = None,\n    bucket: Optional[str] = None,\n    tf_dir: Optional[Path] = None,\n    train_cloud: Optional[bool] = None,\n    destroy: Optional[bool] = None,\n    batch_size: Optional[int] = None,\n    learning_rate_dense: Optional[float] = None,\n    learning_rate_all: Optional[float] = None,\n    epochs_train_dense: Optional[int] = None,\n    epochs_train_all: Optional[int] = None,\n    base_model_name: Optional[str] = None,\n    cloud_tag: Optional[str] = None,\n):\n    config = update_config(\n        config=config,\n        config_file=config_file,\n        job_dir=job_dir,\n        image_dir=image_dir,\n        provider=provider,\n        instance_type=instance_type,\n        region=region,\n        vpc_id=vpc_id,\n        bucket=bucket,\n        tf_dir=tf_dir,\n        train_cloud=train_cloud,\n        destroy=destroy,\n        batch_size=batch_size,\n        learning_rate_dense=learning_rate_dense,\n        learning_rate_all=learning_rate_all,\n        epochs_train_dense=epochs_train_dense,\n        epochs_train_all=epochs_train_all,\n        base_model_name=base_model_name,\n        cloud_tag=cloud_tag,\n    )\n\n    config.train[\'run\'] = True\n\n    validate_config(config, [\'train\'])\n\n    if config.train.get(\'cloud\'):\n        from imageatm.scripts import run_training_cloud\n\n        run_training_cloud(**{**config.cloud, **config.train})\n    else:\n        from imageatm.scripts import run_training\n\n        run_training(**config.train)\n\n\ndef evaluate(\n    config: Config,\n    config_file: Optional[Path] = None,\n    image_dir: Optional[Path] = None,\n    job_dir: Optional[Path] = None,\n    create_report: Optional[bool] = None,\n    kernel_name: Optional[str] = None,\n    export_html: Optional[bool] = None,\n    export_pdf: Optional[bool] = None,\n):\n    config = update_config(\n        config=config,\n        config_file=config_file,\n        job_dir=job_dir,\n        image_dir=image_dir,\n        create_report=create_report,\n        kernel_name=kernel_name,\n        export_html=export_html,\n        export_pdf=export_pdf,\n    )\n\n    config.evaluate[\'run\'] = True\n    validate_config(config, [\'evaluate\'])\n\n    from imageatm.scripts import run_evaluation\n\n    run_evaluation(**config.evaluate)\n\n\ndef cloud(\n    config,\n    job_dir: Optional[Path] = None,\n    config_file: Optional[Path] = None,\n    provider: Optional[str] = None,\n    instance_type: Optional[str] = None,\n    region: Optional[str] = None,\n    vpc_id: Optional[str] = None,\n    bucket: Optional[str] = None,\n    tf_dir: Optional[Path] = None,\n    train_cloud: Optional[bool] = None,\n    destroy: Optional[bool] = None,\n    no_destroy: Optional[bool] = None,\n    cloud_tag: Optional[str] = None,\n):\n    config = update_config(\n        config=config,\n        job_dir=job_dir,\n        config_file=config_file,\n        provider=provider,\n        instance_type=instance_type,\n        region=region,\n        vpc_id=vpc_id,\n        bucket=bucket,\n        tf_dir=tf_dir,\n        train_cloud=train_cloud,\n        destroy=destroy,\n        no_destroy=no_destroy,\n        cloud_tag=cloud_tag,\n    )\n\n    config.cloud[\'run\'] = True\n    validate_config(config, [\'cloud\'])\n\n    from imageatm.scripts import run_cloud\n\n    run_cloud(**config.cloud)\n'"
imageatm/client/config.py,0,"b'from pathlib import Path\nfrom typing import Optional, List\nfrom imageatm.utils.io import load_yaml\n\n\nclass Config:\n    def __init__(self) -> None:\n        # components\n        self.dataprep: dict = {}\n        self.train: dict = {}\n        self.cloud: dict = {}\n        self.evaluate: dict = {}\n\n        # central parameters\n        self.image_dir: Optional[Path] = None\n        self.job_dir: Optional[Path] = None\n\n        self.pipeline: list = []\n\n\ndef update_component_configs(config: Config) -> Config:\n    """"""Populate central parameters to component configs.""""""\n    if config.image_dir:\n        config.dataprep[\'image_dir\'] = config.image_dir\n        config.train[\'image_dir\'] = config.image_dir\n        config.evaluate[\'image_dir\'] = config.image_dir\n\n    if config.job_dir:\n        config.dataprep[\'job_dir\'] = config.job_dir\n        config.train[\'job_dir\'] = config.job_dir\n        config.evaluate[\'job_dir\'] = config.job_dir\n        config.cloud[\'job_dir\'] = config.job_dir\n\n    return config\n\n\ndef update_config(\n    config: Config,\n    config_file: Optional[Path] = None,\n    job_dir: Optional[Path] = None,\n    image_dir: Optional[Path] = None,\n    samples_file: Optional[Path] = None,\n    provider: Optional[str] = None,\n    instance_type: Optional[str] = None,\n    region: Optional[str] = None,\n    vpc_id: Optional[str] = None,\n    bucket: Optional[str] = None,\n    tf_dir: Optional[Path] = None,\n    train_cloud: Optional[bool] = None,\n    ## TODO: check if destroy == !no_destroy\n    destroy: Optional[bool] = None,\n    no_destroy: Optional[bool] = None,\n    resize: Optional[bool] = None,\n    batch_size: Optional[int] = None,\n    learning_rate_dense: Optional[float] = None,\n    learning_rate_all: Optional[float] = None,\n    epochs_train_dense: Optional[int] = None,\n    epochs_train_all: Optional[int] = None,\n    base_model_name: Optional[str] = None,\n    cloud_tag: Optional[str] = None,\n    create_report: Optional[bool] = None,\n    kernel_name: Optional[str] = None,\n    export_html: Optional[bool] = None,\n    export_pdf: Optional[bool] = None,\n) -> Config:\n\n    # set defaults\n    config.train[\'cloud\'] = False\n    config.dataprep[\'resize\'] = False\n    config.evaluate[\'report\'] = {\n        \'create\': False,\n        \'kernel_name\': \'imageatm\',\n        \'export_html\': False,\n        \'export_pdf\': False\n    }\n    config.pipeline = []\n\n    # load central config file\n    if config_file:\n        config_yml: dict = load_yaml(config_file)  # type: ignore\n\n        # populate parameters from config file\n        # parameters from config file overwrite defaults\n        config.image_dir = config_yml.get(\'image_dir\')\n        config.job_dir = config_yml.get(\'job_dir\')\n\n        config.dataprep = {**config.dataprep, **config_yml.get(\'dataprep\', {})}\n        config.train = {**config.train, **config_yml.get(\'train\', {})}\n        config.evaluate = {**config.evaluate, **config_yml.get(\'evaluate\', {})}\n        config.cloud = {**config.cloud, **config_yml.get(\'cloud\', {})}\n\n        # set pipeline\n        components = [\'dataprep\', \'train\', \'evaluate\', \'cloud\']\n        config.pipeline = [i for i in components if config_yml.get(i, {}).get(\'run\')]\n\n    # set options\n    if job_dir is not None:\n        config.job_dir = job_dir\n\n    if image_dir is not None:\n        config.image_dir = image_dir\n\n    if samples_file is not None:\n        config.dataprep[\'samples_file\'] = samples_file\n\n    if provider is not None:\n        config.cloud[\'provider\'] = provider\n\n    if instance_type is not None:\n        config.cloud[\'instance_type\'] = instance_type\n\n    if region is not None:\n        config.cloud[\'region\'] = region\n\n    if vpc_id is not None:\n        config.cloud[\'vpc_id\'] = vpc_id\n\n    if bucket is not None:\n        config.cloud[\'bucket\'] = bucket\n\n    if tf_dir is not None:\n        config.cloud[\'tf_dir\'] = tf_dir\n\n    if train_cloud is True:\n        config.train[\'cloud\'] = True\n\n    if destroy is True:\n        config.cloud[\'destroy\'] = True\n\n    if no_destroy is True:\n        config.cloud[\'destroy\'] = False\n\n    if resize is True:\n        config.dataprep[\'resize\'] = True\n\n    if batch_size is not None:\n        config.train[\'batch_size\'] = batch_size\n\n    if learning_rate_dense is not None:\n        config.train[\'learning_rate_dense\'] = learning_rate_dense\n\n    if learning_rate_all is not None:\n        config.train[\'learning_rate_all\'] = learning_rate_all\n\n    if epochs_train_dense is not None:\n        config.train[\'epochs_train_dense\'] = epochs_train_dense\n\n    if epochs_train_all is not None:\n        config.train[\'epochs_train_all\'] = epochs_train_all\n\n    if base_model_name is not None:\n        config.train[\'base_model_name\'] = base_model_name\n\n    if create_report is True:\n        config.evaluate[\'report\'][\'create\'] = create_report\n\n    if kernel_name is not None:\n        config.evaluate[\'report\'][\'kernel_name\'] = kernel_name\n\n    if kernel_name is not None:\n        config.evaluate[\'report\'][\'export_html\'] = export_html\n\n    if kernel_name is not None:\n        config.evaluate[\'report\'][\'export_pdf\'] = export_pdf\n\n    if cloud_tag is not None:\n        config.cloud[\'cloud_tag\'] = cloud_tag\n\n    config = update_component_configs(config)\n\n    return config\n\n\ndef get_diff(\n    name: str, config: dict, required_keys: List[str], optional_keys: List[str]\n) -> List[str]:\n    allowed_keys = required_keys + optional_keys\n\n    msg = []\n    # check that all required keys are in config keys\n    diff = list(set(required_keys).difference(config.keys()))\n    if diff:\n        msg.append(\'{} config: missing required parameters [{}]\\n\'.format(name, \', \'.join(diff)))\n\n    # check that config keys are in allowed keys\n    diff = list(set(config.keys()).difference(allowed_keys))\n    if diff:\n        msg.append(\n            \'{} config: [{}] not in allowed parameters [{}]\\n\'.format(\n                name, \', \'.join(diff), \', \'.join(allowed_keys)\n            )\n        )\n\n    return msg\n\n\ndef val_dataprep(config: dict) -> List[str]:\n    required_keys = [\'image_dir\', \'job_dir\', \'samples_file\', \'run\']\n    optional_keys = [\'resize\']\n\n    return get_diff(\'dataprep\', config, required_keys, optional_keys)\n\n\ndef val_train(config: dict) -> List[str]:\n    required_keys = [\'image_dir\', \'job_dir\', \'cloud\', \'run\']\n    optional_keys = [\n        \'batch_size\',\n        \'learning_rate_dense\',\n        \'learning_rate_all\',\n        \'epochs_train_dense\',\n        \'epochs_train_all\',\n        \'base_model_name\',\n    ]\n\n    return get_diff(\'train\', config, required_keys, optional_keys)\n\n\ndef val_evaluate(config: dict) -> List[str]:\n    required_keys = [\'image_dir\', \'job_dir\', \'report\', \'run\']\n    optional_keys: list = []\n    msg = get_diff(\'evaluate\', config, required_keys, optional_keys)\n\n    required_keys = [\'create\', \'kernel_name\', \'export_pdf\', \'export_html\']\n    optional_keys: list = []\n    msg += get_diff(\'evaluate\', config[\'report\'], required_keys, optional_keys)\n    return msg\n\n\ndef val_cloud(config: dict) -> List[str]:\n    allowed_providers = [\'aws\']\n\n    assert config.get(\'provider\') is not None, \'Config error: cloud config: missing provider\'\n\n    provider = config.get(\'provider\')\n    assert (\n        provider in allowed_providers\n    ), \'Config error: cloud config: {} not in allowed providers [{}]\'.format(\n        provider, *allowed_providers\n    )\n\n    required_keys = {\n        \'aws\': [\n            \'run\',\n            \'job_dir\',\n            \'tf_dir\',\n            \'region\',\n            \'vpc_id\',\n            \'instance_type\',\n            \'bucket\',\n            \'destroy\',\n            \'provider\',\n            \'cloud_tag\',\n        ]\n    }\n    optional_keys: list = []\n\n    return get_diff(\'cloud\', config, required_keys[provider], optional_keys)\n\n\ndef validate_config(config: Config, components: Optional[list]):\n    msgs: list = []\n\n    if components:\n        for component in components:\n            config_component = getattr(config, component)\n\n            if component == \'dataprep\':\n                msgs += val_dataprep(config_component)\n\n            if component == \'train\':\n                msgs += val_train(config_component)\n\n            if component == \'evaluate\':\n                msgs += val_evaluate(config_component)\n\n            if component == \'cloud\':\n                msgs += val_cloud(config_component)\n\n    assert len(msgs) == 0, \'\\nConfig error:\\n{}\'.format(\'\'.join(msgs))\n'"
imageatm/components/__init__.py,0,b'from .dataprep import DataPrep\nfrom .training import Training\nfrom .evaluation import Evaluation\nfrom .cloud import AWS\n'
imageatm/components/cloud.py,0,"b'import time\nfrom pathlib import Path\nfrom typing import Optional\nfrom yarl import URL\nfrom imageatm.utils.process import run_cmd\nfrom imageatm.utils.logger import get_logger\n\n\nclass AWS:\n    """"""Cloud provider class that allows to run training on AWS.\n\n    Cloud instances are created and destroyed using Terraform. The instance is provisioned\n    with nvidia-docker and training is run in a Docker container using the public Docker\n    image [idealo/tensorflow-image-atm:1.13.1](https://hub.docker.com/r/idealo/tensorflow-image-atm/tags).\n\n    All commands on EC2 instance will be run via SSH.\n\n    For training the local image and job directories will be synced with S3. After training\n    the trained models will be synced with S3 and the local job directory.\n\n    Attributes:\n        tf_dir: Directory with Terraform files for AWS setup.\n        region: AWS region [eu-west-1, eu-central-1].\n        instance_type: AWS GPU instance type [g2.\\*, p2.\\*, p3.\\*].\n        vpc_id: AWS Virtual Private Cloud ID.\n        s3_bucket: AWS S3 bucket where all training files will be stored (is not created by Terraform).\n        job_dir: Job directory on local system (needed for logging).\n        cloud_tag: Name under which all AWS resources will be set up.\n    """"""\n\n    def __init__(\n        self,\n        tf_dir: str,\n        region: str,\n        instance_type: str,\n        vpc_id: str,\n        s3_bucket: str,\n        job_dir: str,\n        cloud_tag: str,\n        **kwargs\n    ) -> None:\n        """"""Inits cloud component.\n\n        Sets *remote workdir* and ensures that *s3 bucket prefix* is correct.\n        """"""\n        self.tf_dir = tf_dir\n        self.region = region\n        self.instance_type = instance_type\n        self.vpc_id = vpc_id\n        self.s3_bucket = URL(\n            s3_bucket\n        )  # needed for IAM setup; bucket will not be created by terraform\n        self.cloud_tag = cloud_tag\n        if \'s3://\' in str(job_dir):\n            self.job_dir = URL(job_dir)\n        else:\n            self.job_dir = Path(job_dir).resolve()\n\n        self.image_dir: Optional[Path] = None\n        self.ssh: Optional[str] = None\n        self.remote_workdir = Path(\'/home/ec2-user/image-atm\').resolve()\n\n        self._check_s3_prefix()\n\n        self.logger = get_logger(__name__, Path(self.job_dir))\n\n    def _check_s3_prefix(self):\n        # ensure that s3 bucket prefix is correct\n        self.s3_bucket_wo = str(self.s3_bucket).split(\'s3://\')[-1]  # without s3:// prefix\n        self.s3_bucket = URL(\'s3://\' + self.s3_bucket_wo)\n\n    def _set_ssh(self):\n        cmd = \'cd {} && terraform output public_ip\'.format(self.tf_dir)\n        output = run_cmd(cmd, logger=self.logger, return_output=True)\n        self.ssh = \'ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@{}\'.format(output)\n\n    def _set_remote_dirs(self):\n        cmd = \'{} mkdir -p {}\'.format(self.ssh, self.remote_workdir)\n        run_cmd(cmd, logger=self.logger)\n\n        self.remote_image_dir = self.remote_workdir.joinpath(self.image_dir.name)\n        self.remote_job_dir = self.remote_workdir.joinpath(self.job_dir.name)\n\n    def _set_s3_dirs(self):\n        if \'s3://\' in str(self.image_dir):\n            self.s3_image_dir = self.image_dir\n        else:\n            self.s3_image_dir = self.s3_bucket / \'image_dirs\' / self.image_dir.name\n\n        if \'s3://\' in str(self.job_dir):\n            self.s3_job_dir = self.job_dir\n        else:\n            self.s3_job_dir = self.s3_bucket / \'job_dirs\' / self.job_dir.name\n\n    def _sync_local_s3(self):\n        self.logger.info(\'Syncing files local <> s3...\')\n        self._set_s3_dirs()\n\n        # only sync if image dir is local dir\n        if \'s3://\' not in str(self.image_dir):\n            cmd = \'aws s3 sync --quiet --exclude logs {} {}\'.format(\n                self.image_dir, self.s3_image_dir\n            )\n            run_cmd(cmd, logger=self.logger)\n\n        # only sync if job dir is local dir\n        if \'s3://\' not in str(self.job_dir):\n            cmd = \'aws s3 sync --quiet --exclude logs {} {}\'.format(self.job_dir, self.s3_job_dir)\n            run_cmd(cmd, logger=self.logger)\n\n    def _sync_s3_local(self):\n        self.logger.info(\'Syncing files s3 <> local...\')\n        self._set_s3_dirs()\n\n        # only sync if job dir is local dir\n        if \'s3://\' not in str(self.job_dir):\n            cmd = \'aws s3 sync --exclude logs --quiet {} {}\'.format(self.s3_job_dir, self.job_dir)\n            run_cmd(cmd, logger=self.logger, level=\'info\')\n\n    def _sync_remote_s3(self):\n        self.logger.info(\'Syncing files remote <> s3...\')\n        self._set_s3_dirs()\n        self._set_remote_dirs()\n\n        cmd = \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n            self.ssh, self.remote_image_dir, self.s3_image_dir\n        )\n        run_cmd(cmd, logger=self.logger)\n\n        cmd = \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n            self.ssh, self.remote_job_dir, self.s3_job_dir\n        )\n        run_cmd(cmd, logger=self.logger)\n\n    def _sync_s3_remote(self):\n        self.logger.info(\'Syncing files s3 <> remote...\')\n        self._set_s3_dirs()\n        self._set_remote_dirs()\n\n        cmd = \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n            self.ssh, self.s3_image_dir, self.remote_image_dir\n        )\n        run_cmd(cmd, logger=self.logger)\n\n        cmd = \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n            self.ssh, self.s3_job_dir, self.remote_job_dir\n        )\n        run_cmd(cmd, logger=self.logger)\n\n    def _launch_train_container(self, **kwargs):\n        self.logger.info(\'Launching training container...\')\n        # training parameters are passed to container through environment variables\n        envs = [\'-e {}={}\'.format(key, value) for key, value in kwargs.items() if value is not None]\n\n        cmd = (\n            \'{} docker run -d -v {}:$WORKDIR/image_dir -v {}:$WORKDIR/job_dir {} \'\n            \'idealo/tensorflow-image-atm:1.13.1\'\n        ).format(self.ssh, self.remote_image_dir, self.remote_job_dir, \' \'.join(envs))\n\n        run_cmd(cmd, logger=self.logger)\n\n    def _stream_docker_logs(self):\n        time.sleep(5)\n        cmd = \'{} docker ps -l -q\'.format(self.ssh)\n        output = run_cmd(cmd, logger=self.logger, return_output=True)\n\n        cmd = \'{} docker logs {} --follow\'.format(self.ssh, output)\n        run_cmd(cmd, logger=self.logger, level=\'info\')\n\n    def init(self):\n        """"""Runs Terraform initialization.""""""\n        self.logger.info(\'Running terraform init...\')\n        cmd = \'cd {} && terraform init\'.format(self.tf_dir)\n        run_cmd(cmd, logger=self.logger)\n\n    def apply(self):\n        """"""Runs Terraform apply.""""""\n        self.logger.info(\'Running terraform apply...\')\n        cmd = (\n            \'cd {} && terraform apply -auto-approve -var ""region={}"" -var ""instance_type={}"" \'\n            \'-var ""vpc_id={}"" -var ""s3_bucket={}"" -var ""name={}""\'\n        ).format(\n            self.tf_dir,\n            self.region,\n            self.instance_type,\n            self.vpc_id,\n            self.s3_bucket_wo,\n            self.cloud_tag,\n        )\n\n        run_cmd(cmd, logger=self.logger)\n\n        self._set_ssh()\n\n    def train(self, image_dir: str = None, job_dir: str = None, **kwargs):\n        """"""Runs training on EC2 instance.\n\n        The following steps will be performed in sequence:\n            - syncs local image and job directory with S3\n            - syncs S3 with EC2 instance\n            - launches Docker training container on EC2\n            - syncs EC2 with S3\n            - syncs S3 with local.\n\n        Any of the pre-trained CNNs in Keras can be used.\n\n        Args:\n            image_dir: Directory with images used for training.\n            job_dir: Directory with train_samples.json, val_samples.json,\n                     and class_mapping.json.\n            epochs_train_dense: Number of epochs to train dense layers.\n            epochs_train_all: Number of epochs to train all layers.\n            learning_rate_dense: Learning rate for dense training phase.\n            self.learning_rate_all: Learning rate for all training phase.\n            batch_size: Number of images per batch.\n            dropout_rate: Fraction set randomly.\n            base_model_name: Name of pretrained CNN.\n        """"""\n        self.logger.info(\'Setting up remote instance...\')\n        if image_dir is not None:\n            self.image_dir = Path(image_dir).resolve()\n\n        if job_dir is not None:\n            self.job_dir = Path(job_dir).resolve()\n\n        self._sync_local_s3()\n        self._sync_s3_remote()\n        self._launch_train_container(**kwargs)\n        self._stream_docker_logs()\n        self._sync_remote_s3()\n        self._sync_s3_local()\n\n    def destroy(self):\n        """"""Runs Terraform destroy.""""""\n        self.logger.info(\'Running terraform destroy...\')\n        cmd = (\n            \'cd {} && terraform destroy -auto-approve -var ""region={}"" -var ""instance_type={}"" \'\n            \'-var ""vpc_id={}"" -var ""s3_bucket={}"" -var ""name={}""\'\n        ).format(\n            self.tf_dir,\n            self.region,\n            self.instance_type,\n            self.vpc_id,\n            self.s3_bucket_wo,\n            self.cloud_tag,\n        )\n\n        run_cmd(cmd, logger=self.logger)\n'"
imageatm/components/dataprep.py,0,"b'from typing import Counter as Counter_type\nfrom typing import Optional, List, Callable\nfrom collections import Counter\nfrom pathlib import Path\nfrom imageatm.utils.images import resize_image_mp, validate_image\nfrom imageatm.utils.io import load_json, save_json\nfrom imageatm.utils.process import parallelise\nfrom imageatm.utils.logger import get_logger\nfrom sklearn.model_selection import train_test_split\n\n\nMIN_CLASS_SIZE = 2\nVALIDATION_SIZE = 0.15\nTEST_SIZE = 0.2\nPART_SIZE = 1.0\n\n\nclass DataPrep:\n    """"""Prepares data for training.\n\n    Based on a *samples file* and an *image directory* a train, validation, and test set will be created.\n    Before data preparation starts samples file and image directory will be validated.\n\n    DataPrep creates a job directory with the following set of files\n\n    - class_mapping.json\n    - train_samples.json\n    - val_samples.json\n    - test_samples.json\n\n    These files are required for subsequent components (training and evaluation).\n\n\n    The samples file must be in JSON format with the following keys\n\n    [\n    {\n        ""image_id"": ""image_1.jpg"",\n        ""label"": ""Class 1""\n    },\n    {\n        ""image_id"": ""image_2.jpg"",\n        ""label"": ""Class 1""\n    },\n    ...\n    ]\n\n\n\n    Attributes:\n        image_dir: path of image directory.\n        job_dir: path to job directory with samples.\n        image_dir: path of image directory.\n        samples_file: path to samples file.\n        min_class_size: minimal number of samples per label (default 2).\n        test_size: represent the proportion of the dataset to include in the test set (default 0.2).\n        val_size: represent the proportion of the dataset to include in the val set (default 0.1).\n        part_size: represent the proportion of the dataset to include in all sets (default 1).\n    """"""\n\n    def __init__(\n        self,\n        job_dir: str,\n        image_dir: str,\n        samples_file: str,\n        min_class_size: int = MIN_CLASS_SIZE,\n        test_size: float = TEST_SIZE,\n        val_size: float = VALIDATION_SIZE,\n        part_size: float = PART_SIZE,\n        **kwargs\n    ) -> None:\n        """"""Inits data preparation component.\n\n        Loads samples file. Initializes variables for further operations:\n        *valid_image_ids*, *class_mapping*, *train_samples*, *val_samples*, *test_samples*.\n        """"""\n        self.job_dir = Path(job_dir).resolve()\n        self.job_dir.mkdir(parents=True, exist_ok=True)\n\n        self.image_dir = Path(image_dir)\n        self.samples_file = Path(samples_file)\n        self.samples_file = Path(samples_file)\n        self.min_class_size = min_class_size\n        self.test_size = test_size\n        self.val_size = val_size\n        self.part_size = part_size\n\n        self.class_mapping: Optional[dict] = None\n        self.valid_image_ids: Optional[List[str]] = None\n        self.train_samples = None\n        self.val_samples = None\n        self.test_samples = None\n\n        self.logger = get_logger(__name__, self.job_dir)\n        self.samples = load_json(self.samples_file)\n\n    def _get_counter(self, list_to_count: list, print_count: bool = False) -> Counter_type:\n        """"""Retrieves samples count for each label from loaded samples and prints distribution when parameter is set.""""""\n        counter = Counter(list_to_count)\n        total = len(list_to_count)\n\n        if print_count:\n            for key, val in sorted(counter.items()):\n                self.logger.info(\'{}: {} ({}%)\'.format(key, val, round(val * 100 / total, 1)))\n\n        return counter\n\n    def _validate_images(self):\n        """"""Checks if all files in *image_dir* are valid images.\n\n        Checks if files are images with extention \'JPEG\' or \'PNG\' and if they are not truncated.\n        self.logger.infos filenames that didn\'t pass the validation.\n\n        Sets:\n            self.valid_image_ids: a list of valid image.\n        """"""\n        self.logger.info(\'\\n****** Running image validation ******\\n\')\n\n        # validate images, use multiprocessing\n        files = [str(i.absolute()) for i in self.image_dir.glob(\'*\')]\n        files.sort()\n\n        results = parallelise(validate_image, files)\n\n        valid_image_files = [j for i, j in enumerate(files) if results[i][0]]\n        self.valid_image_ids = [Path(i).name for i in valid_image_files]\n\n        # return list of invalid images to user and save them if there are more than 10\n        invalid_image_files = [\n            (j, str(results[i][1])) for i, j in enumerate(files) if not results[i][0]\n        ]\n\n        if invalid_image_files:\n            self.logger.info(\'The following files are not valid image files:\')\n            for file_name, error_msg in invalid_image_files[:10]:\n                self.logger.info(\'- {} ({})\'.format(file_name, error_msg))\n            if len(invalid_image_files) > 10:\n                save_json(invalid_image_files, self.job_dir / \'invalid_image_files.json\')\n                self.logger.info(\n                    (\n                        \'NOTE: More than 10 files were identified as invalid image files.\\n\'\n                        \'The full list of those files has been saved here:\\n{}\'.format(\n                            self.job_dir / \'invalid_image_files.json\'\n                        )\n                    )\n                )\n\n    @staticmethod\n    def _validate_sample(sample: dict, valid_image_ids: list) -> bool:\n        return all(\n            [\n                (\'label\' in sample),\n                (\'image_id\' in sample),\n                (sample.get(\'image_id\') in valid_image_ids),\n            ]\n        )\n\n    def _validate_samples(self):\n        """"""Validates the samples.\n\n        Compares samples with valid image ids. Checks if:\n\n            - samples have \'label\' and \'image_id\' keys\n            - there is more than 1 class\n            - there are enough samples in each class\n\n        Reassigns self.samples to valid samples.\n        Sets counter of each class in samples.\n\n        self.logger.infos:\n            - samples that didn\'t pass the validation\n            - distribution of valid samples for each label\n        """"""\n        self.logger.info(\'\\n****** Running samples validation ******\\n\')\n\n        assert self.valid_image_ids, \'Images have to be validated before samples validation.\'\n        valid_image_ids = set(self.valid_image_ids)  # convert to set to optimise lookup\n\n        # exclude samples with invalid image or invalid keys\n        valid_samples = []\n        self.invalid_samples = []\n        for sample in self.samples:\n            if self._validate_sample(sample, valid_image_ids):\n                valid_samples.append(sample)\n            else:\n                self.invalid_samples.append(sample)\n\n        # replace self.samples with valid samples\n        assert valid_samples, \'Program ended. No valid samples found.\'\n        self.samples = valid_samples\n\n        # self.logger.infos invalid images to user and saves them in json if there are more than 10\n        if self.invalid_samples:\n            self.logger.info(\'The following samples were dropped:\')\n            for sample in self.invalid_samples[:10]:\n                self.logger.info(\'- {}\'.format(sample))\n\n            if len(self.invalid_samples) > 10:\n                self.logger.info(\n                    (\n                        \'NOTE: {} samples were identified as invalid.\\n\'\n                        \'The full list of invalid samples will be saved in job dir.\\n\'.format(\n                            len(self.invalid_samples)\n                        )\n                    )\n                )\n\n        self.logger.info(\'Class distribution after validation:\')\n        self.samples_count = self._get_counter([i[\'label\'] for i in self.samples], print_count=True)\n\n        # check if each class has sufficient samples\n        warnings_count = 0\n        for key, val in self.samples_count.items():\n            if val < self.min_class_size:\n                warnings_count += 1\n                self.logger.info(\'Not enough samples for label {}\'.format(key))\n\n        assert warnings_count == 0, \'Program ended. Collect more samples.\'\n        assert len(self.samples_count) > 1, \'Program ended. Only one label in the dataset.\'\n\n    def _create_class_mapping(self, print_mapping: bool = True):\n        """"""Produces a class-mapping.""""""\n        self.class_mapping = {}\n        labels = list(self.samples_count.keys())\n        labels.sort()\n\n        for i, j in enumerate(labels):\n            self.class_mapping[i] = j\n\n        if print_mapping:\n            self.logger.info(\'Class mapping:\\n{}\'.format(self.class_mapping))\n\n    def _apply_class_mapping(self):\n        """"""Applies the class-mapping.""""""\n        class_mapping_inv = {v: k for k, v in self.class_mapping.items()}\n\n        samples_int = [\n            {\'label\': class_mapping_inv[sample[\'label\']], \'image_id\': sample[\'image_id\']}\n            for sample in self.samples\n        ]\n\n        # replace self.samples with samples int\n        self.samples = samples_int\n\n    def _split_samples(self):\n        """"""Produces stratified train, val, test sets.\n\n        Sets:\n            self.train_samples\n            self.val_samples\n            self.test_samples\n        """"""\n        self.logger.info(\'\\n****** Creating train/val/test sets ******\\n\')\n\n        self.train_size = 1 - (self.test_size + self.val_size)\n\n        self.logger.info(\n            \'Split distribution: train: {:.2f}, val: {}, test: {:}\\n\'.format(\n                self.train_size, self.val_size, self.test_size\n            )\n        )\n\n        self.logger.info(\n            \'Partial split distribution: train: {:.2f}, val: {:.2f}, test: {:.2f}\\n\'.format(\n                self.train_size * self.part_size,\n                self.val_size * self.part_size,\n                self.test_size * self.part_size,\n            )\n        )\n\n        labels = [i[\'label\'] for i in self.samples]  # need label list for stratification\n\n        split_test_size = self.test_size * self.part_size\n        split_train_size = (1 - self.test_size) * self.part_size\n\n        train_samples, self.test_samples, train_labels, _ = train_test_split(\n            self.samples,\n            labels,\n            test_size=split_test_size,\n            train_size=split_train_size,\n            shuffle=True,\n            random_state=10207,\n            stratify=labels,\n        )\n\n        split_test_size = self.val_size / (1 - self.test_size)\n        self.train_samples, self.val_samples, _, _ = train_test_split(\n            train_samples,\n            train_labels,\n            test_size=split_test_size,\n            shuffle=True,\n            random_state=10207,\n            stratify=train_labels,\n        )\n\n        self.logger.info(\'Train set:\')\n        self._get_counter([i[\'label\'] for i in self.train_samples], print_count=True)\n\n        self.logger.info(\'Val set:\')\n        self._get_counter([i[\'label\'] for i in self.val_samples], print_count=True)\n\n        self.logger.info(\'Test set:\')\n        self._get_counter([i[\'label\'] for i in self.test_samples], print_count=True)\n\n    def _resize_images(self, resize_image_mp: Callable = resize_image_mp):\n        self.logger.info(\'\\n****** Resizing images ******\\n\')\n        new_image_dir = \'_\'.join([str(self.image_dir), \'resized\'])\n        Path(new_image_dir).resolve().mkdir(parents=True, exist_ok=True)\n\n        args = [(self.image_dir, new_image_dir, i[\'image_id\']) for i in self.samples]\n        parallelise(resize_image_mp, args)\n        self.logger.info(\n            \'Stored {} resized images under {}\'.format(len(self.samples), new_image_dir)\n        )\n\n        self.image_dir = Path(new_image_dir)\n        self.logger.info(\'Changed image directory to {}\'.format(self.image_dir))\n\n    def _save_files(self):\n        save_json(self.train_samples, self.job_dir / \'train_samples.json\')\n        save_json(self.val_samples, self.job_dir / \'val_samples.json\')\n        save_json(self.test_samples, self.job_dir / \'test_samples.json\')\n        save_json(self.class_mapping, self.job_dir / \'class_mapping.json\')\n\n        if len(self.invalid_samples) > 10:\n            save_json(self.invalid_samples, self.job_dir / \'invalid_samples.json\')\n            self.logger.info(\n                (\n                    \'NOTE: More than 10 samples were identified as invalid.\\n\'\n                    \'The full list of invalid samples has been saved here:\\n{}\'.format(\n                        self.job_dir / \'invalid_samples.json\'\n                    )\n                )\n            )\n\n    def run(self, resize: bool = False):\n        """"""Executes all steps of data preparation.\n\n            - Validates samples and images\n            - Creates class-mapping (string to integer)\n            - Applies class-mapping on samples\n            - Splits sample into train, validation and test sets\n            - Resizes images\n            - Saves files (class-mapping, train-, validation- and test-set)\n\n        Args:\n            resize: boolean (creates a subfolder of resized images, default False).\n        """"""\n        self._validate_images()\n        self._validate_samples()\n\n        self._create_class_mapping()\n        self._apply_class_mapping()\n        self._split_samples()\n\n        if resize:\n            self._resize_images()\n\n        self._save_files()\n'"
imageatm/components/evaluation.py,0,"b'import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nbformat\nimport papermill as pm\nfrom nbconvert import HTMLExporter, PDFExporter\nfrom os.path import dirname\n\nplt.style.use(\'ggplot\')\nfrom typing import List, Union, Tuple, Any\nfrom pathlib import Path\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom vis.visualization import visualize_cam\nfrom imageatm.handlers.image_classifier import ImageClassifier\nfrom imageatm.handlers.data_generator import ValDataGenerator\nfrom imageatm.utils.io import load_json\nfrom imageatm.utils.images import load_image\nfrom imageatm.utils.logger import get_logger\nfrom imageatm.utils.tf_keras import use_multiprocessing, load_model\nimport imageatm.notebooks\n\n\nBATCH_SIZE = 16\nBASE_MODEL_NAME = \'MobileNet\'\nMAX_N_CLASSES = 20\n\nUSE_MULTIPROCESSING, WORKERS = use_multiprocessing()\n\nTYPE_IMAGE_LIST = List[List[Tuple[int, np.array, dict]]]  # used for type hinting\n\n\nclass Evaluation:\n    """"""Calculates performance metrics for trained models.\n\n    Loads the best model (validation accuracy) from *models* directory in job directory.\n    All metrics and graphs are based on *test_samples.json* in job directory.\n    Plots will only be shown if number of classes 20 or less.\n\n    Attributes:\n        image_dir: Path of image directory.\n        job_dir: Path to job directory with samples.\n        batch_size: Number of images per batch (default 64).\n        base_model_name: Name of pretrained CNN (default MobileNet).\n    """"""\n\n    def __init__(\n        self,\n        image_dir: str,\n        job_dir: str,\n        batch_size: int = BATCH_SIZE,\n        base_model_name: str = BASE_MODEL_NAME,\n        **kwargs\n    ) -> None:\n        """"""Inits evaluation component.\n\n        Loads the best model from job directory.\n        Creates evaluation directory if app was started from commandline.\n        """"""\n        self.image_dir = Path(image_dir).resolve()\n        self.job_dir = Path(job_dir).resolve()\n        self.batch_size = batch_size\n        self.base_model_name = base_model_name\n\n        self.logger = get_logger(__name__, self.job_dir)\n        self.samples_test: list = load_json(self.job_dir / \'test_samples.json\')  # type: ignore\n        self.class_mapping: dict = load_json(self.job_dir / \'class_mapping.json\')  # type: ignore\n        self.n_classes = len(self.class_mapping)\n        self.classes = [str(self.class_mapping[str(i)]) for i in range(self.n_classes)]\n        self.y_true = np.array([i[\'label\'] for i in self.samples_test])\n        self.figures = []\n\n        self._determine_plot_params()\n        self._load_best_model()\n        self._create_evaluation_dir()\n\n    def _determine_plot_params(self):\n        """"""Determines fontsizes and checks whether ipython kernel is present.\n\n        Plots will only be shown if in ipython, otherwise saved as files.\n        """"""\n        self.fontsize_title = 18 if self.n_classes < 4 else 18\n        self.fontsize_label = 14 if self.n_classes < 4 else 14\n        self.fontsize_ticks = 12 if self.n_classes < 4 else 12\n        self.mode_ipython = True if self._is_in_ipython_mode() else False\n\n    def _is_in_ipython_mode(self):\n        try:\n            __IPYTHON__\n            return True\n\n        except NameError:\n            ## TODO: Is this obsolete? Please remove!\n            # Suppress figure window in terminal\n            # https://matplotlib.org/faq/howto_faq.html#generate-images-without-having-a-window-appear\n            import matplotlib\n            matplotlib.use(\'Agg\')\n\n            return False\n\n    def _load_best_model(self):\n        """"""Loads best performing model from job_dir.""""""\n        self.logger.info(\'\\n****** Load model ******\\n\')\n        best_model_file = self._determine_best_modelfile()\n        self.best_model_file = Path(best_model_file).resolve()\n        self.best_model = load_model(self.best_model_file)\n\n        self.logger.info(\'loaded {}\'.format(self.best_model_file))\n\n    def _determine_best_modelfile(self):\n        """"""Determines best performing model from job_dir.""""""\n        job_path = self.job_dir / \'models\'\n        model_files = list(job_path.glob(\'**/*.hdf5\'))\n        max_acc_idx = np.argmax([m.name.split(\'_\')[3][:5] for m in model_files])\n        return model_files[max_acc_idx]\n\n    def _create_evaluation_dir(self):\n        """"""Creates evaluation dir for reporting.""""""\n        if not self.mode_ipython:\n            evaluation_dir_name = self.best_model_file.name.split(\'.hdf5\')[0]\n            self.evaluation_dir = self.job_dir / \'evaluation_{}\'.format(evaluation_dir_name)\n\n            self.evaluation_dir.mkdir(parents=True, exist_ok=True)\n\n    @staticmethod\n    def _get_probabilities_prediction(predictions_dist: List[List[float]]) -> List[float]:\n        index = np.argmax(predictions_dist, axis=1)\n        prob = [pred[index] for pred, index in zip(predictions_dist, index)]\n        return prob\n\n    def _make_prediction_on_test_set(self):\n        """"""Makes prediction on test set.""""""\n\n        self.classifier = ImageClassifier(\n            base_model_name=self.base_model_name,\n            n_classes=self.n_classes,\n            weights=None,\n            dropout_rate=None,\n            learning_rate=None,\n            loss=None,\n        )\n        self.classifier.model = self.best_model\n\n        self.data_generator = ValDataGenerator(\n            samples=self.samples_test,\n            image_dir=self.image_dir,\n            batch_size=self.batch_size,\n            n_classes=self.n_classes,\n            basenet_preprocess=self.classifier.get_preprocess_input(),\n        )\n\n        predictions_dist = self.classifier.predict_generator(\n            data_generator=self.data_generator,\n            workers=WORKERS,\n            use_multiprocessing=USE_MULTIPROCESSING,\n            verbose=1,\n        )\n\n        self.y_pred = np.argmax(predictions_dist, axis=1)\n        self.y_pred_prob = self._get_probabilities_prediction(predictions_dist=predictions_dist)\n\n    def _plot_test_set_distribution(self, figsize: (float, float) = [8, 5]):\n        """"""Plots bars with number of samples for each label in test set.""""""\n        assert self.mode_ipython, \'Plotting is only possible when in ipython-mode\'\n\n        if self.n_classes > MAX_N_CLASSES:\n            self.logger.info(\'\\nPlotting only for max {} classes\\n\'.format(MAX_N_CLASSES))\n            return\n\n        x_tick_marks = np.arange(self.n_classes)\n        y_values = np.bincount(self.y_true)\n        title = \'Number of images in test set: {}\'.format(len(self.samples_test))\n\n        fig = plt.figure(figsize=figsize)\n        plt.rcParams[""axes.grid""] = True\n        plt.bar(x_tick_marks, y_values)\n        plt.title(title, fontsize=self.fontsize_title)\n        plt.xlabel(\'Label\', fontsize=self.fontsize_label)\n        plt.ylabel(\'Number of images\', fontsize=self.fontsize_label)\n        plt.xticks(x_tick_marks, self.classes, fontsize=self.fontsize_ticks, rotation=30)\n\n        plt.tight_layout()\n        plt.show()\n\n    def _print_test_set_distribution(self):\n        """"""Prints distribution for labels in test set.""""""\n        assert not self.mode_ipython, \'Printing is recommended when not in ipython-mode\'\n\n        max_length = len(max(self.classes, key=len))\n        y_values = np.bincount(self.y_true)\n        for i, c in enumerate(self.classes):\n            label = c + \' \' * (max_length - len(c))\n            self.logger.info(""{}\\t{}"". format(label, y_values[i]))\n\n    def _print_classification_report(self):\n        """"""Prints classification for labels in test set.""""""\n        cr = classification_report(\n            y_true=self.y_true, y_pred=self.y_pred, target_names=self.classes, output_dict=True\n        )\n\n        metrics = [\'precision\', \'recall\', \'f1-score\', \'support\']\n        categories = self.classes.copy()\n        categories.extend([\'macro avg\', \'weighted avg\'])\n\n        max_length = len(max(self.classes, key=len))\n        self.logger.info(""{}\\t{}\\t{}\\t{}\\t{}"". format(\' \' * max_length, \'prec\', ""rec"", ""f1"", ""support""))\n        for c in categories:\n            label = c + \' \' * (max_length - len(c))\n            line_output = ""{}\\t"".format(label)\n            for m in metrics:\n                if m == \'support\':\n                    line_output += ""{}\\t"".format(cr[c][m])\n                else:\n                    line_output += ""{0:.2f}\\t"".format(cr[c][m])\n            self.logger.info(line_output)\n\n    def _plot_confusion_matrix(self, figsize: (float, float) = [9, 9], precision: bool = False):\n        """"""Plots normalized confusion matrix.""""""\n        assert self.mode_ipython, \'Plotting is only possible when in ipython-mode\'\n\n        if self.n_classes > MAX_N_CLASSES:\n            self.logger.info(\'\\nPlotting only for max {} classes\\n\'.format(MAX_N_CLASSES))\n            return\n\n        (title, xlabel, ylabel) = \\\n            (\'Confusion matrix (precision)\', \'True label\', \'Predicted label\') if precision \\\n            else (\'Confusion matrix (recall)\', \'Predicted label\', \'True label\')\n\n        cm = confusion_matrix(y_true=self.y_pred, y_pred=self.y_true) if precision \\\n            else confusion_matrix(y_true=self.y_true, y_pred=self.y_pred)\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        tick_marks = np.arange(self.n_classes)\n\n        fig = plt.figure(figsize=figsize)\n        plt.rcParams[""axes.grid""] = False\n        plt.imshow(cm, interpolation=\'nearest\', cmap=plt.cm.Blues, vmin=0, vmax=1)\n        plt.colorbar()\n        plt.title(title, fontsize=self.fontsize_title)\n        plt.xlabel(xlabel, fontsize=self.fontsize_label)\n        plt.ylabel(ylabel, fontsize=self.fontsize_label)\n        plt.xticks(tick_marks, self.classes, rotation=45, fontsize=self.fontsize_ticks, ha=\'right\')\n        plt.yticks(tick_marks, self.classes, fontsize=self.fontsize_ticks)\n\n        thresh = cm.max() / 2.0\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(\n                j,\n                i,\n                \'{:.2f}\'.format(cm[i, j]),\n                horizontalalignment=\'center\',\n                color=\'white\' if cm[i, j] > thresh else \'black\',\n                fontsize=self.fontsize_ticks,\n            )\n\n        plt.tight_layout()\n        plt.show()\n\n    def _print_confusion_matrix(self, precision: bool = False):\n        """"""Prints normalized confusion matrix.""""""\n        assert not self.mode_ipython, \'Printing is recommended when not in ipython-mode\'\n\n        cm = confusion_matrix(y_true=self.y_pred, y_pred=self.y_true) if precision \\\n            else confusion_matrix(y_true=self.y_true, y_pred=self.y_pred)\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n\n        max_length = len(max(self.classes, key=len))\n        for i, c in enumerate(self.classes):\n            label = c + \' \' * (max_length - len(c))\n            line_output = ""{}\\t"".format(label)\n            for x in cm[i].tolist():\n                line_output += ""{0:.2f}\\t"".format(x)\n            self.logger.info(line_output)\n\n    def _plot_correct_wrong_examples(self):\n        """"""Plots correct and wrong examples for each label in test set.""""""\n        assert self.mode_ipython, \'Plotting is only possible when in ipython-mode\'\n\n        if self.n_classes > MAX_N_CLASSES:\n            self.logger.info(\'\\nPlotting only for max {} classes\\n\'.format(MAX_N_CLASSES))\n            return\n\n        for i in range(len(self.classes)):\n            c, w = self.get_correct_wrong_examples(label=i)\n            self.visualize_images(c, title=\'Label: ""{}"" (correct predicted)\'.format(self.classes[i]), show_heatmap=True, n_plot=3)\n            self.visualize_images(w, title=\'Label: ""{}"" (wrong predicted)\'.format(self.classes[i]), show_heatmap=True, n_plot=3)\n\n    def _create_report(self, report_kernel_name:str, report_export_html:bool, report_export_pdf:bool):\n        """"""Creates report from notebook-template and stores it in different formats all figures.\n\n            - Jupyter Notebook\n            - HTML\n            - PDF\n        """"""\n        assert not self.mode_ipython, \'Create report is only possible when not in ipython mode\'\n\n        filepath_template = dirname(imageatm.notebooks.__file__) + \'/evaluation_template.ipynb\'\n        filepath_notebook = self.evaluation_dir / \'evaluation_report.ipynb\'\n        filepath_html = self.evaluation_dir / \'evaluation_report.html\'\n        filepath_pdf = self.evaluation_dir / \'evaluation_report.pdf\'\n\n        pm.execute_notebook(\n            str(filepath_template),\n            str(filepath_notebook),\n            parameters=dict(\n                image_dir=str(self.image_dir),\n                job_dir=str(self.job_dir)\n            ),\n            kernel_name=report_kernel_name\n        )\n\n        with open(filepath_notebook) as f:\n            nb = nbformat.read(f, as_version=4)\n\n        if report_export_html:\n            self.logger.info(\'\\n****** Create HTML ******\\n\')\n            with open(filepath_notebook) as f:\n                nb = nbformat.read(f, as_version=4)\n\n            html_exporter = HTMLExporter()\n            html_data, resources = html_exporter.from_notebook_node(nb)\n\n            with open(filepath_html, \'w\') as f:\n                f.write(html_data)\n                f.close()\n\n        if report_export_pdf:\n            self.logger.info(\'\\n****** Create PDF ******\\n\')\n\n            pdf_exporter = PDFExporter()\n            pdf_exporter.template_file = dirname(imageatm.notebooks.__file__) + \'/tex_templates/evaluation_report.tplx\'\n            pdf_data, resources = pdf_exporter.from_notebook_node(nb, resources={\n                \'metadata\': {\'name\': \'Evaluation Report\'}\n            })\n\n            with open(filepath_pdf, \'wb\') as f:\n                f.write(pdf_data)\n                f.close()\n\n    # TO-DO: Enforce string or integer but not both at the same time\n    def get_correct_wrong_examples(\n        self, label: Union[int, str]\n    ) -> Tuple[TYPE_IMAGE_LIST, TYPE_IMAGE_LIST]:\n        """"""Gets correctly and wrongly predicted samples for a given label.\n\n        Args:\n            label: int or str (label for which the predictions should be considered).\n\n        Returns:\n            (correct, wrong): Tuple of two image lists.\n        """"""\n        correct = []\n        wrong = []\n\n        if type(label) == str:\n            class_mapping_inv = {v: k for k, v in self.class_mapping.items()}\n            label = int(class_mapping_inv[label])\n\n        for i, sample in enumerate(self.samples_test):\n            if self.y_true[i] == label:\n                image_file = self.image_dir / sample[\'image_id\']\n                if self.y_true[i] == self.y_pred[i]:\n                    correct.append([i, load_image(image_file, target_size=(224, 224)), sample])\n                else:\n                    wrong.append([i, load_image(image_file, target_size=(224, 224)), sample])\n\n        return correct, wrong\n\n    def visualize_images(\n        self, image_list: TYPE_IMAGE_LIST, title: str = \'Images for visualisation\', show_heatmap: bool = False, n_plot: int = 20\n    ):\n        """"""Visualizes images in a sample list.\n\n        Args:\n            image_list: sample list.\n            show_heatmap: boolean (generates a gradient based class activation map (grad-CAM), default False).\n            n_plot: maximum number of plots to be shown (default 20).\n        """"""\n        assert self.mode_ipython, \'Plotting is only possible when in ipython-mode\'\n\n        if len(image_list) == 0:\n            print(\'Empty list.\')\n            return\n        else:\n            n_rows = min(n_plot, len(image_list))\n            n_cols = 2 if show_heatmap else 1\n\n            figsize = [5 * n_cols, 4 * n_rows]\n            fig = plt.figure(figsize=figsize)\n            fig.suptitle(title, fontsize=self.fontsize_title)\n\n            plot_count = 1\n            for (i, img, sample) in image_list[:n_rows]:\n                plt.subplot(n_rows, n_cols, plot_count)\n                plt.imshow(img)\n                plt.axis(\'off\')\n                plt.title(\n                    \'true: {}, predicted: {} ({})\'.format(\n                        self.class_mapping[str(self.y_true[i])],\n                        self.class_mapping[str(self.y_pred[i])],\n                        str(round(self.y_pred_prob[i], 2)),\n                    )\n                )\n                plot_count += 1\n\n                if show_heatmap is True:\n                    heatmap = visualize_cam(\n                        model=self.classifier.model,\n                        layer_idx=89,\n                        filter_indices=[self.y_pred[i]],\n                        seed_input=self.classifier.get_preprocess_input()(\n                            np.array(img).astype(np.float32)\n                        ),\n                    )\n                    plt.subplot(n_rows, n_cols, plot_count)\n                    plt.imshow(img)\n                    plt.imshow(heatmap, alpha=0.7)\n                    plt.axis(\'off\')\n                    plot_count += 1\n\n            plt.show()\n\n    def run(self,\n            report_create: bool = False,\n            report_kernel_name: str = \'imageatm\',\n            report_export_html: bool = False,\n            report_export_pdf: bool = False\n        ):\n        """"""Runs evaluation pipeline on the best model found in job directory for the specific test set:\n\n            - Makes prediction on test set\n            - Plots test set distribution\n            - Plots classification report (accuracy, precision, recall)\n            - Plots confusion matrix (on precsion and on recall)\n            - Plots correct and wrong examples\n\n           If not in ipython mode an evaluation report is created.\n\n        Args:\n        \treport_create: boolean (create ipython kernel)\n            report_kernel_name: str (name of ipython kernel)\n            report_export_html: boolean (exports report to html).\n            report_export_pdf: boolean (exports report to pdf).\n        """"""\n        if self.mode_ipython:\n            self.logger.info(\'\\n****** Make prediction on test set ******\\n\')\n            self._make_prediction_on_test_set()\n\n            self.logger.info(\'\\n****** Plot distribution on test set ******\\n\')\n            self._plot_test_set_distribution(figsize=[8, 5])\n\n            self.logger.info(\'\\n****** Plot classification report ******\\n\')\n            # self._plot_classification_report(figsize=[4 + self.n_classes*0.5, 4 + self.n_classes*0.5])\n            self._print_classification_report()\n\n            self.logger.info(\'\\n****** Plot confusion matrix (recall) ******\\n\')\n            self._plot_confusion_matrix(figsize=[4 + self.n_classes*0.5, 4 + self.n_classes*0.5])\n\n            self.logger.info(\'\\n****** Plot confusion matrix (precision) ******\\n\')\n            self._plot_confusion_matrix(figsize=[4 + self.n_classes*0.5, 4 + self.n_classes*0.5], precision=True)\n\n            self.logger.info(\'\\n****** Plot correct and wrong examples ******\\n\')\n            self._plot_correct_wrong_examples()\n\n        elif report_create:\n            self.logger.info(\'\\n****** Create Jupyter Notebook (this may take a while) ******\\n\')\n            self._create_report(report_kernel_name, report_export_html, report_export_pdf)\n\n        else:\n            self.logger.info(\'\\n****** Make prediction on test set ******\\n\')\n            self._make_prediction_on_test_set()\n\n            self.logger.info(\'\\n****** Print distribution on test set ******\\n\')\n            self._print_test_set_distribution()\n\n            self.logger.info(\'\\n****** Print classification report ******\\n\')\n            self._print_classification_report()\n\n            self.logger.info(\'\\n****** Print confusion matrix (recall) ******\\n\')\n            self._print_confusion_matrix()\n\n            self.logger.info(\'\\n****** Print confusion matrix (precision) ******\\n\')\n            self._print_confusion_matrix(precision=True)\n'"
imageatm/components/training.py,1,"b'import typing\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom pathlib import Path\nfrom imageatm.utils.io import load_json\nfrom imageatm.utils.logger import get_logger\nfrom imageatm.utils.tf_keras import use_multiprocessing, LoggingMetrics, LoggingModels\nfrom imageatm.handlers.data_generator import TrainDataGenerator, ValDataGenerator\nfrom imageatm.handlers.image_classifier import ImageClassifier\n\n\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\nBATCH_SIZE = 64\nDROPOUT_RATE = 0.75\nBASE_MODEL_NAME = \'MobileNet\'\nLOSS = \'categorical_crossentropy\'\nEPOCHS_TRAIN_DENSE = 100\nEPOCHS_TRAIN_ALL = 100\nLEARNING_RATE_DENSE = 0.001\nLEARNING_RATE_ALL = 0.0003\n\n\nclass Training:\n    """"""Builds model and runs training.\n\n    The following pretrained CNNs from Keras can be used for transfer learning:\n\n    - Xception\n    - VGG16\n    - VGG19\n    - ResNet50, ResNet101, ResNet152\n    - ResNet50V2, ResNet101V2, ResNet152V2\n    - ResNeXt50, ResNeXt101\n    - InceptionV3\n    - InceptionResNetV2\n    - MobileNet\n    - MobileNetV2\n    - DenseNet121, DenseNet169, DenseNet201\n    - NASNetLarge, NASNetMobile\n\n    Training is split into two phases, at first only the last dense layer gets\n    trained, and then all layers are trained. The maximum number of epochs for\n    each phase is set by *epochs_train_dense* (default: 100) and\n    *epochs_train_all* (default: 100), respectively. Similarly,\n    *learning_rate_dense* (default: 0.001) and *learning_rate_all*\n    (default: 0.0003) can be set.\n\n    For each phase the learning rate is reduced after a patience period if no\n    improvement in validation accuracy has been observed. The patience period\n    depends on the average number of samples per class:\n\n    - if n_per_class < 200: patience = 5 epochs\n    - if n_per_class >= 200 and < 500: patience = 4 epochs\n    - if n_per_class >= 500: patience = 2 epochs\n\n    The training is stopped early after a patience period that is three times\n    the learning rate patience to allow for two learning rate adjustments\n    with no validation accuracy improvement before stopping training.\n\n    Attributes:\n        image_dir: Directory with images used for training.\n        job_dir: Directory with train_samples.json, val_samples.json,\n                 and class_mapping.json.\n        epochs_train_dense: Maximum number of epochs to train dense layers (default 100).\n        epochs_train_all: Maximum number of epochs to train all layers (default 100).\n        learning_rate_dense: Learning rate for dense training phase (default 0.001).\n        learning_rate_all: Learning rate for all training phase (default 0.0003).\n        batch_size: Number of images per batch (default 64).\n        dropout_rate: Fraction of nodes before output layer set to random value (default 0.75).\n        base_model_name: Name of pretrained CNN (default MobileNet).\n    """"""\n\n    def __init__(\n        self,\n        image_dir: str,\n        job_dir: str,\n        epochs_train_dense: typing.Union[int, str] = EPOCHS_TRAIN_DENSE,\n        epochs_train_all: typing.Union[int, str] = EPOCHS_TRAIN_ALL,\n        learning_rate_dense: typing.Union[float, str] = LEARNING_RATE_DENSE,\n        learning_rate_all: typing.Union[float, str] = LEARNING_RATE_ALL,\n        batch_size: typing.Union[int, str] = BATCH_SIZE,\n        dropout_rate: typing.Union[float, str] = DROPOUT_RATE,\n        base_model_name: str = BASE_MODEL_NAME,\n        loss: str = LOSS,\n        **kwargs,\n    ) -> None:\n        """"""Inits training component.\n\n        Checks whether multiprocessing is available and sets number of workers for training.\n        """"""\n        self.image_dir = Path(image_dir).resolve()\n        self.job_dir = Path(job_dir).resolve()\n\n        self.logger = get_logger(__name__, self.job_dir)\n        self.samples_train = load_json(self.job_dir / \'train_samples.json\')\n        self.samples_val = load_json(self.job_dir / \'val_samples.json\')\n        self.class_mapping = load_json(self.job_dir / \'class_mapping.json\')\n        self.n_classes = len(self.class_mapping)\n\n        self.epochs_train_dense = int(epochs_train_dense)\n        self.epochs_train_all = int(epochs_train_all)\n        self.learning_rate_dense = float(learning_rate_dense)\n        self.learning_rate_all = float(learning_rate_all)\n        self.batch_size = int(batch_size)\n        self.dropout_rate = float(dropout_rate)\n        self.base_model_name = base_model_name\n        self.loss = loss\n        self.use_multiprocessing, self.workers = use_multiprocessing()\n\n    def _set_patience(self):\n        """"""Adjust patience for early stopping and learning rate schedule\n        based on training set size.\n        """"""\n        n_per_class = int(len(self.samples_train) / self.n_classes)\n\n        self.patience_learning_rate = 5\n        if n_per_class >= 200:\n            self.patience_learning_rate = 4\n\n        if n_per_class >= 500:\n            self.patience_learning_rate = 2\n\n        self.patience_early_stopping = 3 * self.patience_learning_rate\n\n        self.logger.info(\'Early stopping patience: {}\'.format(self.patience_early_stopping))\n        self.logger.info(\'Learning rate patience: {}\'.format(self.patience_learning_rate))\n\n    def _build_model(self):\n        self.classifier = ImageClassifier(\n            self.base_model_name,\n            self.n_classes,\n            self.learning_rate_dense,\n            self.dropout_rate,\n            self.loss,\n        )\n        self.classifier.build()\n\n    def _fit_model(self):\n        training_generator = TrainDataGenerator(\n            self.samples_train,\n            self.image_dir,\n            self.batch_size,\n            self.n_classes,\n            self.classifier.get_preprocess_input(),\n        )\n\n        validation_generator = ValDataGenerator(\n            self.samples_val,\n            self.image_dir,\n            self.batch_size,\n            self.n_classes,\n            self.classifier.get_preprocess_input(),\n        )\n\n        # TODO: initialize callbacks TensorBoardBatch\n        # tensorboard = TensorBoardBatch(log_dir=Path(job_dir).resolve() / \'logs\')\n\n        model_save_name = (\n            \'model_\' + self.base_model_name.lower() + \'_{epoch:02d}_{val_accuracy:.3f}.hdf5\'\n        )\n        model_dir = self.job_dir / \'models\'\n        model_dir.mkdir(parents=True, exist_ok=True)\n\n        logging_metrics = LoggingMetrics(logger=self.logger)\n        logging_models = LoggingModels(\n            logger=self.logger,\n            filepath=str(model_dir / model_save_name),\n            monitor=\'val_accuracy\',\n            verbose=1,\n            save_best_only=True,\n            save_weights_only=False,\n        )\n\n        def _train_dense_layers():\n            if self.epochs_train_dense > 0:\n                self.logger.info(\'\\n****** Train dense layers ******\\n\')\n\n                min_lr = self.learning_rate_dense / 10\n                reduce_lr = ReduceLROnPlateau(\n                    monitor=\'val_accuracy\',\n                    factor=0.3162,\n                    patience=self.patience_learning_rate,\n                    min_lr=min_lr,\n                    verbose=1,\n                )\n\n                early_stopping = EarlyStopping(\n                    monitor=\'val_accuracy\',\n                    min_delta=0.002,\n                    patience=self.patience_early_stopping,\n                    verbose=1,\n                    mode=\'auto\',\n                    baseline=None,\n                    restore_best_weights=True,\n                )\n\n                # freeze convolutional layers in base net\n                for layer in self.classifier.get_base_layers():\n                    layer.trainable = False\n\n                self.classifier.compile()\n                # self.classifier.summary()\n\n                self.hist_dense = self.classifier.fit_generator(\n                    generator=training_generator,\n                    validation_data=validation_generator,\n                    epochs=self.epochs_train_dense,\n                    verbose=1,\n                    use_multiprocessing=self.use_multiprocessing,\n                    workers=self.workers,\n                    max_queue_size=30,\n                    callbacks=[logging_metrics, logging_models, reduce_lr, early_stopping],\n                )\n\n        def _train_all_layers():\n            if self.epochs_train_all > 0:\n                self.logger.info(\'\\n****** Train all layers ******\\n\')\n\n                min_lr = self.learning_rate_all / 10\n                reduce_lr = ReduceLROnPlateau(\n                    monitor=\'val_accuracy\',\n                    factor=0.3162,\n                    patience=self.patience_learning_rate,\n                    min_lr=min_lr,\n                    verbose=1,\n                )\n\n                early_stopping = EarlyStopping(\n                    monitor=\'val_accuracy\',\n                    min_delta=0.002,\n                    patience=self.patience_early_stopping,\n                    verbose=1,\n                    mode=\'auto\',\n                    baseline=None,\n                    restore_best_weights=False,\n                )\n\n                # unfreeze all layers\n                for layer in self.classifier.get_base_layers():\n                    layer.trainable = True\n\n                self.classifier.set_learning_rate(self.learning_rate_all)\n\n                self.classifier.compile()\n                # self.classifier.summary()\n\n                self.hist_all = self.classifier.fit_generator(\n                    generator=training_generator,\n                    validation_data=validation_generator,\n                    epochs=self.epochs_train_dense + self.epochs_train_all,\n                    initial_epoch=self.epochs_train_dense,\n                    verbose=1,\n                    use_multiprocessing=self.use_multiprocessing,\n                    workers=self.workers,\n                    max_queue_size=30,\n                    callbacks=[logging_metrics, logging_models, reduce_lr, early_stopping],\n                )\n\n        self._set_patience()\n        _train_dense_layers()\n        _train_all_layers()\n\n        K.clear_session()\n\n    def run(self):\n        """"""Builds the model and runs training.\n        """"""\n        self._build_model()\n        self._fit_model()\n'"
imageatm/handlers/__init__.py,0,b'\n'
imageatm/handlers/data_generator.py,0,"b'import numpy as np\nfrom typing import List, Callable, Tuple\nfrom pathlib import Path\nfrom imageatm.utils.images import load_image, random_crop\nfrom keras.utils import Sequence, to_categorical\n\n\nclass DataGenerator(Sequence):\n    """"""Class inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator.\n\n    DataGenerator is extended by these classes:\n        - TrainDataGenerator\n        - ValDataGenerator.\n\n    Attributes:\n        samples: Dictionary of samples to generate data from.\n        image_dir: Path of image directory.\n        batch_size: Number of images per batch.\n        n_classes: Number of classes in dataset.\n        basenet_preprocess: Basenet specific preprocessing function.\n        img_load_dims: Dimensions that images get resized into when loaded.\n        train: If set to True samples are shuffled before each epoch and images are cropped once.\n    """"""\n\n    def __init__(\n        self,\n        samples: List[dict],\n        image_dir: str,\n        batch_size: int,\n        n_classes: int,\n        basenet_preprocess: Callable,\n        img_load_dims: Tuple[int, int],\n        train: bool,\n    ) -> None:\n        """"""Inits DataGenerator object.\n\n        If *train* set *True* then samples are shuffled on init.\n        """"""\n        self.samples = samples\n        self.image_dir = Path(image_dir)\n        self.batch_size = batch_size\n        self.n_classes = n_classes\n        self.basenet_preprocess = basenet_preprocess\n        self.img_load_dims = img_load_dims\n        self.train = train\n        self.on_epoch_end()\n\n    def on_epoch_end(self):\n        """"""Method called at the end of every epoch.\n\n        If *train* set *True* then samples are shuffled.\n        """"""\n        self.indexes = np.arange(len(self.samples))\n        if self.train is True:\n            np.random.shuffle(self.indexes)\n\n    def __len__(self):\n        """"""Number of batches in the Sequence.""""""\n        return int(np.ceil(len(self.samples) / self.batch_size))\n\n    def __getitem__(self, index: int) -> Tuple[np.array, np.array]:\n        """"""Gets batch at position `index`.\n\n        If *train* set *True* then images will be cropped by *img_crop_dims*.\n        """"""\n        batch_indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        batch_samples = [self.samples[i] for i in batch_indexes]\n        X, y = self._data_generator(batch_samples)\n        return X, y\n\n    def _data_generator(self, batch_samples: List[dict]) -> Tuple[np.array, np.array]:\n        """"""Generates data from samples in specified batch.""""""\n        #  initialize images and labels tensors for faster processing\n        dims = self.img_crop_dims if self.train == True else self.img_load_dims\n        X = np.empty((len(batch_samples), *dims, 3))\n        y = np.empty((len(batch_samples), self.n_classes))\n\n        for i, sample in enumerate(batch_samples):\n            # load and randomly augment image\n            img_file = self.image_dir / sample[\'image_id\']\n            img = np.asarray(load_image(img_file, self.img_load_dims))\n            if self.train == True:\n                img = random_crop(img, self.img_crop_dims)\n            X[i,] = img\n\n            # TODO: more efficient by preprocessing\n            y[i,] = to_categorical([sample[\'label\']], num_classes=self.n_classes)\n\n        # apply basenet specific preprocessing\n        # input is 4D numpy array of RGB values within [0, 255]\n        X = self.basenet_preprocess(X)\n\n        return X, y\n\n\nclass TrainDataGenerator(DataGenerator):\n    """"""Class inherits from DataGenerator.\n\n    Per default images will be cropped and samples are shuffled before each epoch.\n\n    Attributes:\n        samples: Dictionary of samples to generate data from.\n        image_dir: Path of image directory.\n        batch_size: Number of images per batch.\n        n_classes: Number of classes in dataset.\n        basenet_preprocess: Basenet specific preprocessing function.\n        img_load_dims: Dimensions that images get resized into when loaded (default (256, 256)).\n        img_crop_dims: Dimensions that images get resized into when loaded (default (224, 224)).\n        train: If set to True samples are shuffled before each epoch and images are cropped once (default True).\n    """"""\n\n    def __init__(\n        self,\n        samples: List[dict],\n        image_dir: str,\n        batch_size: int,\n        n_classes: int,\n        basenet_preprocess: Callable,\n        img_load_dims: Tuple[int, int] = (256, 256),\n        img_crop_dims: Tuple[int, int] = (224, 224),\n        train: bool = True,\n    ) -> None:\n        """"""Inits TrainDataGenerator object.\n\n        Per default samples are shuffled on init.""""""\n        super(TrainDataGenerator, self).__init__(\n            samples, image_dir, batch_size, n_classes, basenet_preprocess, img_load_dims, train\n        )\n        self.img_crop_dims = img_crop_dims  # dimensions that images get randomly cropped to\n\n\nclass ValDataGenerator(DataGenerator):\n    """"""Class inherits from DataGenerator.\n\n    Per default neither images are cropped nor samples are shuffled.\n\n    Attributes:\n        samples: Dictionary of samples to generate data from.\n        image_dir: Path of image directory.\n        batch_size: Number of images per batch.\n        n_classes: Number of classes in dataset.\n        basenet_preprocess: Basenet specific preprocessing function.\n        img_load_dims: Dimensions that images get resized into when loaded (default (224, 224)).\n        train: If set to True samples are shuffled before each epoch and images are cropped once (default False).\n    """"""\n\n    def __init__(\n        self,\n        samples: List[dict],\n        image_dir: str,\n        batch_size: int,\n        n_classes: int,\n        basenet_preprocess: Callable,\n        img_load_dims: Tuple[int, int] = (224, 224),\n        train: bool = False,\n    ) -> None:\n        """"""Inits TrainDataGenerator object.""""""\n        super(ValDataGenerator, self).__init__(\n            samples, image_dir, batch_size, n_classes, basenet_preprocess, img_load_dims, train\n        )\n'"
imageatm/handlers/image_classifier.py,0,"b'import importlib\nfrom typing import Callable\nfrom keras.models import Model\nfrom keras.layers import Dropout, Dense\nfrom keras.optimizers import Adam\nfrom keras.callbacks import History\nfrom imageatm.handlers.data_generator import DataGenerator\n\n\nclass ImageClassifier:\n    """"""Class that represents the classifier for images.\n\n    The following pretrained CNNs from Keras can be used as base model:\n    - Xception\n    - VGG16\n    - VGG19\n    - ResNet50, ResNet101, ResNet152\n    - ResNet50V2, ResNet101V2, ResNet152V2\n    - ResNeXt50, ResNeXt101\n    - InceptionV3\n    - InceptionResNetV2\n    - MobileNet\n    - MobileNetV2\n    - DenseNet121, DenseNet169, DenseNet201\n    - NASNetLarge, NASNetMobile\n\n    Attributes:\n        base_model_name: Name of Keras base model.\n        n_classes: Number of classes.\n        learning_rate: Learning rate for training phase.\n        dropout_rate: Fraction set randomly.\n        loss: A loss function as one of two parameters to compile the model.\n        weights: Pretrained weights the model architecture is loaded with (default imagenet).\n    """"""\n\n    def __init__(\n        self,\n        base_model_name: str,\n        n_classes: int,\n        learning_rate: float,\n        dropout_rate: float,\n        loss: str,\n        weights: str = \'imagenet\',\n    ) -> None:\n        """"""Inits ImageClassifier object.\n\n        Loads Keras base_module specified by base_model_name.\n        """"""\n        self.n_classes = n_classes\n        self.base_model_name = base_model_name\n        self.learning_rate = learning_rate\n        self.dropout_rate = dropout_rate\n        self.loss = loss\n        self.weights = weights\n        self._load_base_module()\n\n    def _load_base_module(self):\n        """"""Loads Keras base_module specified by base_model_name. """"""\n        if self.base_model_name == \'InceptionV3\':\n            self.base_module = importlib.import_module(\'keras.applications.inception_v3\')\n\n        elif self.base_model_name == \'InceptionResNetV2\':\n            self.base_module = importlib.import_module(\'keras.applications.inception_resnet_v2\')\n\n        elif self.base_model_name in [\'NASNetLarge\', \'NASNetMobile\']:\n            self.base_module = importlib.import_module(\'keras.applications.nasnet\')\n\n        elif self.base_model_name in [\'DenseNet121\', \'DenseNet169\', \'DenseNet201\']:\n            self.base_module = importlib.import_module(\'keras.applications.densenet\')\n\n        elif self.base_model_name in [\'ResNet50\', \'ResNet101\', \'ResNet152\']:\n            self.base_module = importlib.import_module(\'keras.applications.resnet\')\n\n        elif self.base_model_name in [\'ResNet50V2\', \'ResNet101V2\', \'ResNet152V2\']:\n            self.base_module = importlib.import_module(\'keras.applications.resnet_v2\')\n\n        elif self.base_model_name in [\'ResNeXt50\', \'ResNeXt101\']:\n            self.base_module = importlib.import_module(\'keras.applications.resnext\')\n\n        else:\n            self.base_module = importlib.import_module(\n                \'keras.applications.\' + self.base_model_name.lower()\n            )\n\n    def get_base_layers(self) -> list:\n        """""" Gets layers of classifiers\' base model\n\n        Returns:\n            base_layers: list of layers\n        """"""\n        return self.base_model.layers\n\n    def get_preprocess_input(self) -> Callable:\n        """""" Gets preprocess_input of classifiers\' base_module\n\n        Returns:\n            preprocess_input: Callable\n        """"""\n        return self.base_module.preprocess_input\n\n    def set_learning_rate(self, learning_rate: float):\n        """"""\n        sets classifiers\' learning_rate\n\n        Args:\n            learning_rate: the learning_rate.\n        """"""\n        self.learning_rate = learning_rate\n\n    def build(self):\n        """"""\n        Builds classifiers\' model.\n\n        The following steps will be performed in sequence:\n            - Loads a pretrained base model.\n            - Adds dropout and dense layer to the model.\n            - Sets classifiers\' model.\n        """"""\n        BaseCnn = getattr(self.base_module, self.base_model_name)\n        self.base_model = BaseCnn(\n            input_shape=(224, 224, 3), weights=self.weights, include_top=False, pooling=\'avg\'\n        )\n\n        x = Dropout(self.dropout_rate)(self.base_model.output)\n        x = Dense(units=self.n_classes, activation=\'softmax\')(x)\n\n        self.model = Model(self.base_model.inputs, x)\n\n    def compile(self):\n        """""" Configures classifiers\' model for training.""""""\n        self.model.compile(\n            optimizer=Adam(lr=self.learning_rate), loss=self.loss, metrics=[\'accuracy\']\n        )\n\n    def fit_generator(self, **kwargs) -> History:\n        """"""\n        Trains classifiers\' model on data generated by a Python generator.\n\n        Args:\n            generator: Input samples from a data generator on which to train the model.\n            validation_data: Input samples from a data generator on which to evaluate the model.\n            epochs: Number of epochs to train the model.\n            initial_epoch: Epoch at which to start training.\n            verbose: Verbosity mode.\n            use_multiprocessing: Use process based threading.\n            workers: Maximum number of processes.\n            max_queue_size: Maximum size for the generator queue.\n            callbacks: List of callbacks to apply during training.\n\n        Returns:\n            history: A `History` object.\n\n        """"""\n        return self.model.fit_generator(**kwargs)\n\n    def predict_generator(self, data_generator: DataGenerator, **kwargs) -> History:\n        """"""\n        Generates predictions for the input samples from a data generator.\n\n        Args:\n            data_generator: Input samples from a data generator.\n            workers: Maximum number of processes.\n            use_multiprocessing: Use process based threading.\n            verbose: Verbosity mode.\n\n        Returns:\n            history: A `History` object.\n        """"""\n        return self.model.predict_generator(data_generator, **kwargs)\n\n    def summary(self):\n        """""" Summarizes classifiers\' model.""""""\n        self.model.summary()\n'"
imageatm/notebooks/__init__.py,0,b''
imageatm/scripts/__init__.py,0,b'from .run_dataprep import run_dataprep\nfrom .run_training import run_training\nfrom .run_training_cloud import run_training_cloud\nfrom .run_evaluation import run_evaluation\nfrom .run_cloud import run_cloud\n'
imageatm/scripts/run_cloud.py,0,"b""from imageatm.components import AWS\n\n\ndef run_cloud(\n    provider: str,\n    tf_dir: str,\n    region: str,\n    instance_type: str,\n    vpc_id: str,\n    bucket: str,\n    destroy: bool,\n    job_dir: str,\n    cloud_tag: str,\n    **kwargs\n):\n    if provider == 'aws':\n        cloud = AWS(\n            tf_dir=tf_dir,\n            region=region,\n            instance_type=instance_type,\n            vpc_id=vpc_id,\n            s3_bucket=bucket,\n            job_dir=job_dir,\n            cloud_tag=cloud_tag,\n        )\n\n    if destroy:\n        cloud.destroy()\n\n    else:\n        cloud.init()\n        cloud.apply()\n"""
imageatm/scripts/run_dataprep.py,0,"b'from imageatm.components import DataPrep\n\n\ndef run_dataprep(\n    image_dir: str, samples_file: str, job_dir: str, resize: bool = False, **kwargs\n) -> DataPrep:\n    dp = DataPrep(job_dir=job_dir, image_dir=image_dir, samples_file=samples_file, **kwargs)\n    dp.run(resize=resize)\n\n    return dp\n'"
imageatm/scripts/run_evaluation.py,0,"b""from imageatm.components import Evaluation\n\n\ndef run_evaluation(\n\timage_dir: str,\n\tjob_dir: str,\n\treport: object,\n\t**kwargs,\n):\n    eval = Evaluation(image_dir=image_dir, job_dir=job_dir, **kwargs)\n    eval.run(\n\t\treport_create = report['create'],\n\t\treport_kernel_name = report['kernel_name'],\n\t\treport_export_html = report['export_html'],\n\t\treport_export_pdf = report['export_pdf']\n\t)\n"""
imageatm/scripts/run_training.py,0,"b""import os\nimport argparse\nfrom imageatm.components import Training\n\n\ndef run_training(image_dir: str, job_dir: str, **kwargs):\n    trainer = Training(image_dir=image_dir, job_dir=job_dir, **kwargs)\n    trainer.run()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-j', '--job-dir', help='Directory with job files.', required=True)\n    parser.add_argument('-i', '--image-dir', help='Directory with image files.', required=True)\n\n    args = parser.parse_args()\n\n    run_training(**{**os.environ, **args.__dict__})\n"""
imageatm/scripts/run_training_cloud.py,0,"b""from imageatm.components import AWS\n\n\ndef run_training_cloud(\n    image_dir: str,\n    job_dir: str,\n    provider: str,\n    tf_dir: str,\n    region: str,\n    instance_type: str,\n    vpc_id: str,\n    bucket: str,\n    destroy: bool,\n    cloud_tag: str,\n    **kwargs\n):\n    if provider == 'aws':\n        cloud = AWS(\n            tf_dir=tf_dir,\n            region=region,\n            instance_type=instance_type,\n            vpc_id=vpc_id,\n            s3_bucket=bucket,\n            job_dir=job_dir,\n            cloud_tag=cloud_tag,\n        )\n\n    cloud.init()\n    cloud.apply()\n    cloud.train(image_dir=image_dir, job_dir=job_dir, **kwargs)\n\n    if destroy:\n        cloud.destroy()\n"""
imageatm/utils/__init__.py,0,b'\n'
imageatm/utils/images.py,0,"b'import numpy as np\nfrom typing import List, Tuple, Optional\nfrom pathlib import Path\nfrom PIL import Image\n\nIMG_FORMATS = [\'JPEG\', \'PNG\']\n\n\ndef load_image(path: Path, target_size=None) -> Image:\n    img = Image.open(path)\n    f = img.format  # store format after opening as it gets lost after conversion\n\n    if img.mode != \'RGB\':\n        # convert to RGBA first to avoid warning\n        # we ignore alpha channel if available\n        img = img.convert(\'RGBA\').convert(\'RGB\')\n\n    if target_size:\n        img = img.resize(target_size)\n\n    img.format = f  # reassign format for later validation checks\n\n    return img\n\n\ndef save_image(img, path: Path):\n    img.convert(\'RGB\').save(path)\n\n\ndef validate_image(\n    file_name: Path, img_formats: List[str] = IMG_FORMATS\n) -> Tuple[bool, Optional[Exception]]:\n    """"""\n    Checks whether File is valid image file:\n        - file exists\n        - file is readable\n        - file is an image\n\n    Args:\n        file_name: Absolute path of file.\n\n     Returns:\n        True if file is valid image file.\n        False else.\n    """"""\n\n    valid_image = False\n    error = None\n\n    try:\n        img = load_image(file_name)\n\n        if img.format in img_formats:\n            img.load()  # Pillow uses lazy loading, so need to explicitly load\n\n            valid_image = True\n\n    except Exception as e:\n        error = e\n\n    return valid_image, error\n\n\ndef resize_image(img: Image, max_size: int, upscale: bool = False) -> Image:\n    """"""Resizes image while keeping aspect ratio.\n\n    The smaller dimension will be resized to max_size, i.e. a 400x500px image with max_size=300\n    will be resized to 300x375px.\n\n    Args:\n        img: Pillow image object.\n        max_size: Maximum width or height of resized image.\n        upscale: If True will upscale small images to max_size.\n\n     Returns:\n        Pillow image object.\n    """"""\n    width, height = img.size\n\n    if (max_size >= min(width, height)) and not upscale:\n        return img\n\n    min_dim = min(width, height)\n    new_width = int((max_size / min_dim) * width)\n    new_height = int((max_size / min_dim) * height)\n\n    return img.resize((new_width, new_height))\n\n\ndef resize_image_mp(data_tuple: Tuple[str, str, str]):\n    image_dir, new_image_dir, image_id = data_tuple\n    img = load_image(Path(image_dir) / image_id)\n    img = resize_image(img, max_size=300, upscale=False)\n    save_image(img, Path(new_image_dir) / image_id)\n\n\ndef random_crop(img: np.array, crop_dims: Tuple[int, int]):\n    h, w = img.shape[0], img.shape[1]\n    ch, cw = crop_dims[0], crop_dims[1]\n    assert h >= ch, \'image height is less than crop height\'\n    assert w >= cw, \'image width is less than crop width\'\n    x = np.random.randint(0, w - cw + 1)\n    y = np.random.randint(0, h - ch + 1)\n    return img[y : (y + ch), x : (x + cw), :]\n'"
imageatm/utils/io.py,0,"b""import json\nimport yaml\nfrom pathlib import Path\nfrom typing import Union\n\n\ndef load_json(file_path: Path) -> Union[dict, list]:\n    with open(file_path, 'r') as f:\n        return json.load(f)\n\n\ndef save_json(data: Union[dict, list], target_file: str):\n    with open(target_file, 'w') as f:\n        json.dump(data, f, indent=2, sort_keys=True)\n\n\ndef load_yaml(file_path: str) -> str:\n    with open(file_path, 'r') as f:\n        return yaml.safe_load(f)\n"""
imageatm/utils/logger.py,0,"b""import sys\nimport logging\nfrom pathlib import Path\nfrom typing import Union\n\n\ndef get_logger(name: str, job_dir: Union[Path, str]) -> logging.Logger:\n    if isinstance(job_dir, str):\n        job_dir = Path(job_dir)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.handlers = []\n\n    if not logger.handlers:\n        # stream handler ensures that logging events are passed to stdout\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(logging.INFO)\n        ch_formatter = logging.Formatter('%(message)s')\n        ch.setFormatter(ch_formatter)\n        logger.addHandler(ch)\n\n        # file handler ensures that logging events are passed to log file\n        fh = logging.FileHandler(filename=job_dir / 'logs')\n        fh.setLevel(logging.DEBUG)\n        fh_formatter = logging.Formatter(\n            '%(asctime)s - %(module)s - %(levelname)s - %(message)s', '%Y-%m-%d %H:%M:%S'\n        )\n        fh.setFormatter(fh_formatter)\n        logger.addHandler(fh)\n\n    return logger\n"""
imageatm/utils/process.py,0,"b""import re\nimport sys\nimport tqdm\nimport subprocess\nimport logging\nfrom typing import Callable\nfrom multiprocessing import Pool, cpu_count\n\n\ndef parallelise(function: Callable, data: list) -> list:\n    processes = cpu_count()\n    pool = Pool(processes=processes)\n    results = list(tqdm.tqdm(pool.imap(function, data), total=len(data)))\n    pool.close()\n    pool.join()\n    return results\n\n\ndef run_cmd(cmd: str, logger: logging.Logger, level: str = 'debug', return_output: bool = False):\n    # filter out ANSI color and font formatting\n    ansi_re = re.compile(r'\\x1b\\[[0-9;]*m')\n\n    p = subprocess.Popen(\n        cmd,\n        stderr=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        universal_newlines=True,\n        shell=True,\n        bufsize=1,\n    )\n\n    # stream process stdout to logger if available\n    stdout = []\n    line = ''\n    while True:\n        inchar = p.stdout.read(1)\n        line += inchar\n\n        if line[-1:] == '\\n':\n            new_line = re.sub(ansi_re, '', line[:-1])\n\n            if level == 'debug':\n                logger.debug(new_line)\n            else:\n                logger.info(new_line)\n\n            stdout.append(new_line)\n            line = ''\n\n        if not inchar:\n            sys.stdout.flush()\n            break\n\n    output, error = p.communicate()\n\n    if p.returncode != 0 and error:\n        logger.error(error, exc_info=True)\n        raise Exception('{}'.format(error))\n    if return_output:\n        return stdout[-1]\n"""
imageatm/utils/tf_keras.py,0,"b'import numpy as np\nfrom six import iteritems\nfrom multiprocessing import cpu_count\nfrom typing import Tuple, List\nfrom logging import Logger\nfrom tensorflow.python.client import device_lib\nfrom keras.models import load_model as load_model_keras\nfrom keras.callbacks import Callback\nfrom keras.engine.training import Model\nfrom pathlib import Path\n\n\ndef load_model(model_path: Path) -> Model:\n    return load_model_keras(str(model_path))\n\n\ndef use_multiprocessing() -> Tuple[bool, int]:\n    if _get_available_gpus():\n        # if GPU is available, use all available CPUs for batch preprocessing\n        use_multiprocessing = True\n        num_workers = cpu_count()\n    else:\n        # device = \'CPU\'\n        use_multiprocessing = False\n        num_workers = 1\n    return use_multiprocessing, num_workers\n\n\ndef _get_available_gpus() -> List[str]:\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == \'GPU\']\n\n\nclass LoggingMetrics(Callback):\n    """"""Callback for logging metrics at the end of each epoch.\n\n    Args:\n        logger: Root logger.\n    """"""\n\n    def __init__(self, logger: Logger) -> None:\n        Callback.__init__(self)\n        self.logger = logger\n        self.format_epoch = \'Epoch: {} - {}\'\n        self.format_keyvalue = \'{}: {:0.4f}\'\n        self.format_separator = \' - \'\n\n    def on_epoch_end(self, epoch: int, logs: dict = {}):\n        values = self.format_separator.join(\n            self.format_keyvalue.format(k, v) for k, v in iteritems(logs)\n        )\n        msg = self.format_epoch.format(epoch + 1, values)\n        self.logger.debug(msg)\n\n\nclass LoggingModels(Callback):\n    def __init__(\n        self,\n        filepath: Path,\n        logger: Logger,\n        monitor: str = \'val_loss\',\n        verbose: int = 0,\n        save_best_only: bool = False,\n        save_weights_only: bool = False,\n        mode: str = \'auto\',\n        period: int = 1,\n    ) -> None:\n        self.monitor = monitor\n        self.verbose = verbose\n        self.filepath = filepath\n        self.save_best_only = save_best_only\n        self.save_weights_only = save_weights_only\n        self.period = period\n        self.epochs_since_last_save = 0\n        self.logger = logger\n\n        if mode not in [\'auto\', \'min\', \'max\']:\n            self.logger.warning(\n                \'ModelCheckpoint mode {} is unknown, fallback to auto mode.\'.format(mode)\n            )\n            mode = \'auto\'\n\n        if mode == \'min\':\n            self.monitor_op = np.less\n            self.best = np.Inf\n        elif mode == \'max\':\n            self.monitor_op = np.greater\n            self.best = -np.Inf\n        else:\n            if \'acc\' in self.monitor or self.monitor.startswith(\'fmeasure\'):\n                self.monitor_op = np.greater\n                self.best = -np.Inf\n            else:\n                self.monitor_op = np.less\n                self.best = np.Inf\n\n    def on_epoch_end(self, epoch: int, logs: dict = None):\n        logs = logs or {}\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n            filepath = Path(str(self.filepath).format(epoch=epoch + 1, **logs))\n            if self.save_best_only:\n                current = logs.get(self.monitor)\n                if current is None:\n                    self.logger.warning(\n                        \'Can save best model only with {} available, skipping.\'.format(self.monitor)\n                    )\n                else:\n                    if self.monitor_op(current, self.best):\n                        if self.verbose > 0:\n                            self.logger.info(\n                                \'\\nEpoch {:05d} {} improved from {:0.5f} to {:0.5f},\\nsaving model to {}\'.format(\n                                    epoch + 1, self.monitor, self.best, current, filepath\n                                )\n                            )\n                        self.best = current\n                        if self.save_weights_only:\n                            self.model.save_weights(str(filepath), overwrite=True)\n                        else:\n                            self.model.save(str(filepath), overwrite=True)\n                    else:\n                        if self.verbose > 0:\n                            self.logger.info(\n                                \'\\nEpoch {:05d} {} did not improve from {:0.5f}\'.format(\n                                    epoch + 1, self.monitor, self.best\n                                )\n                            )\n            else:\n                if self.verbose > 0:\n                    self.logger.debug(\n                        \'\\nEpoch {:05d} saving model to {}\'.format(epoch + 1, filepath)\n                    )\n                if self.save_weights_only:\n                    self.model.save_weights(str(filepath), overwrite=True)\n                else:\n                    self.model.save(str(filepath), overwrite=True)\n'"
tests/client/test_arg_flow.py,0,"b""import pytest\nimport shutil\nfrom pathlib import Path\nfrom imageatm.client.client import Config\nfrom imageatm.client.commands import pipeline, train, evaluate, dataprep\n\np = Path(__file__)\nTEST_CONFIG_PIPE = p.resolve().parent / 'test_configs' / 'config_arg_flow_all.yml'\nTEST_CONFIG_DATAPREP = p.resolve().parent / 'test_configs' / 'config_arg_flow_dataprep.yml'\nTEST_CONFIG_TRAIN = p.resolve().parent / 'test_configs' / 'config_arg_flow_train.yml'\nTEST_CONFIG_EVAL = p.resolve().parent / 'test_configs' / 'config_arg_flow_eval.yml'\nTEST_NB_TEMPLATE = p.resolve().parent / 'test_notebooks' / 'evaluation_template.ipynb'\n\nTEST_SAMPLES = Path('tests/data/test_samples/test_arg_flow.json')\nTEST_IMAGE_DIR = Path('tests/data/test_images')\nTEST_IMAGE_DIR_RES = Path('tests/data/test_images_resized')\nTEST_JOB_DIR = Path('tests/data/test_arg_flow')\n\n\n@pytest.fixture(scope='class', autouse=True)\ndef tear_down(request):\n    def remove_job_dir():\n        shutil.rmtree(TEST_JOB_DIR)\n        shutil.rmtree(TEST_IMAGE_DIR_RES)\n\n    request.addfinalizer(remove_job_dir)\n\n\nclass TestArgFlow(object):\n    def test_dataprep(self):\n        config = Config()\n\n        assert not TEST_IMAGE_DIR_RES.exists()\n        assert not Path(TEST_JOB_DIR / 'class_mapping.json').exists()\n        assert not Path(TEST_JOB_DIR / 'test_samples.json').exists()\n        assert not Path(TEST_JOB_DIR / 'train_samples.json').exists()\n        assert not Path(TEST_JOB_DIR / 'val_samples.json').exists()\n\n        dataprep(config, config_file=TEST_CONFIG_DATAPREP)\n\n        assert config.dataprep['run'] == True\n        assert config.dataprep['job_dir'] == str(TEST_JOB_DIR)\n        assert config.dataprep['samples_file'] == str(TEST_SAMPLES)\n        assert config.dataprep['image_dir'] == str(TEST_IMAGE_DIR)\n        assert config.dataprep['resize'] == True\n\n        assert config.train['run'] == False\n        assert config.evaluate['run'] == False\n        assert config.cloud['run'] == False\n\n        assert TEST_IMAGE_DIR_RES.exists()\n        assert Path(TEST_JOB_DIR / 'class_mapping.json').exists()\n        assert Path(TEST_JOB_DIR / 'test_samples.json').exists()\n        assert Path(TEST_JOB_DIR / 'train_samples.json').exists()\n        assert Path(TEST_JOB_DIR / 'val_samples.json').exists()\n\n    def test_train(self):\n        config = Config()\n\n        assert not list(Path(TEST_JOB_DIR / 'models').glob('*.hdf5'))\n\n        train(config, config_file=TEST_CONFIG_TRAIN)\n\n        assert config.train['run'] == True\n        assert config.train['cloud'] == False\n        assert config.train['job_dir'] == str(TEST_JOB_DIR)\n        assert config.train['image_dir'] == str(TEST_IMAGE_DIR_RES)\n\n        assert config.dataprep['run'] == False\n        assert config.evaluate['run'] == False\n        assert config.cloud['run'] == False\n\n        assert list(Path(TEST_JOB_DIR / 'models').glob('*.hdf5'))\n\n    def test_evaluate(self, mocker):\n\n        BEST_MODEL_FILE = list(Path(TEST_JOB_DIR / 'models').glob('*.hdf5'))[-1]\n        BEST_MODEL = 'evaluation_' + BEST_MODEL_FILE.stem\n        NB_FILEPATH = TEST_JOB_DIR / BEST_MODEL / 'evaluation_report.ipynb'\n\n        def fake_execute_notebook(*args, **kwargs):\n            shutil.copy(TEST_NB_TEMPLATE, NB_FILEPATH)\n\n        mocker.patch('papermill.execute_notebook', side_effect=fake_execute_notebook)\n        mocker.patch('imageatm.components.evaluation.Evaluation._determine_best_modelfile',\n                     return_value=BEST_MODEL_FILE)\n        mocker.patch('nbconvert.PDFExporter.from_notebook_node',\n                     return_value=('ANY_DATA'.encode(), None))\n\n        config = Config()\n\n        evaluate(config, config_file=TEST_CONFIG_EVAL)\n\n        assert config.dataprep['run'] == False\n        assert config.train['run'] == False\n        assert config.cloud['run'] == False\n\n        assert config.evaluate['run'] == True\n        assert config.evaluate['job_dir'] == str(TEST_JOB_DIR)\n        assert config.evaluate['image_dir'] == str(TEST_IMAGE_DIR_RES)\n\n    def test_pipeline(self, mocker):\n\n        BEST_MODEL_FILE = list(Path(TEST_JOB_DIR / 'models').glob('*.hdf5'))[-1]\n        BEST_MODEL = 'evaluation_' + BEST_MODEL_FILE.stem\n        NB_FILEPATH = TEST_JOB_DIR / BEST_MODEL / 'evaluation_report.ipynb'\n\n        def fake_execute_notebook(*args, **kwargs):\n            shutil.copy(TEST_NB_TEMPLATE, NB_FILEPATH)\n\n        mocker.patch('papermill.execute_notebook', side_effect=fake_execute_notebook)\n        mocker.patch('imageatm.components.evaluation.Evaluation._determine_best_modelfile',\n                     return_value=BEST_MODEL_FILE)\n        mocker.patch('nbconvert.PDFExporter.from_notebook_node',\n                     return_value=('ANY_DATA'.encode(), None))\n\n        config = Config()\n\n        pipeline(config, config_file=TEST_CONFIG_PIPE)\n\n        assert config.dataprep['run'] == True\n        assert config.dataprep['job_dir'] == str(TEST_JOB_DIR)\n        assert config.dataprep['samples_file'] == str(TEST_SAMPLES)\n        assert config.dataprep['image_dir'] == TEST_IMAGE_DIR_RES\n        assert config.dataprep['resize'] == True\n\n        assert config.train['run'] == True\n        assert config.train['cloud'] == False\n\n        assert config.evaluate['run'] == True\n        assert config.evaluate['report']['create'] == True\n        assert config.evaluate['report']['kernel_name'] == 'any_kernel'\n        assert config.evaluate['report']['export_html'] == True\n        assert config.evaluate['report']['export_pdf'] == True\n\n        assert config.cloud['run'] == False\n        assert config.cloud['provider'] == 'aws'\n        assert config.cloud['tf_dir'] == 'cloud/aws'\n        assert config.cloud['region'] == 'eu-west-1'\n        assert config.cloud['vpc_id'] == 'abc'\n        assert config.cloud['instance_type'] == 't2.micro'\n        assert config.cloud['bucket'] == 's3://test_bucket'\n        assert config.cloud['destroy'] == True\n        assert config.cloud['cloud_tag'] == 'test_user'\n\n        assert list(Path(TEST_JOB_DIR / 'models').glob('*.hdf5'))\n"""
tests/client/test_client.py,0,"b""from pathlib import Path\nfrom click.testing import CliRunner\nfrom imageatm.client.client import cli\n\np = Path(__file__)\n\nTEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_1.yml'\n\nTEST_SAMPLES_FILE = p.resolve().parent / 'test_samples' / 'test_int_labels.json'\n\nTEST_IMG_DIR = p.resolve().parent / 'test_images' / 'test_image_dir'\n\n\ndef test_help():\n    runner = CliRunner()\n\n    result = runner.invoke(cli, ['--help'])\n    assert result.exit_code == 0\n\n    result = runner.invoke(cli, ['pipeline', '--help'])\n    assert result.exit_code == 0\n\n    result = runner.invoke(cli, ['dataprep', '--help'])\n    assert result.exit_code == 0\n\n    result = runner.invoke(cli, ['train', '--help'])\n    assert result.exit_code == 0\n\n    result = runner.invoke(cli, ['evaluate', '--help'])\n    assert result.exit_code == 0\n\n    result = runner.invoke(cli, ['cloud', '--help'])\n    assert result.exit_code == 0\n\n\ndef test_options_available():\n    runner = CliRunner()\n\n    expected = 'Options:\\n  --help  Show this message and exit.\\n\\nCommands'\n    result = runner.invoke(cli, ['--help'])\n    assert expected in result.stdout\n\n    expected = (\n        'Options:\\n'\n        '  --image-dir PATH              Directory with image files.\\n'\n        '  --samples-file PATH           JSON file with samples.\\n'\n        '  --job-dir PATH                Directory with train, val, and test samples\\n'\n        '                                files and class_mapping file.\\n'\n        '  --provider TEXT               Cloud provider, currently supported: [aws].\\n'\n        '  --instance-type TEXT          Cloud instance_type [aws].\\n'\n        '  --region TEXT                 Cloud region [aws].\\n'\n        '  --vpc-id TEXT                 Cloud VPC id [aws].\\n'\n        '  --bucket TEXT                 Cloud bucket used for persistence [aws].\\n'\n        '  --tf-dir TEXT                 Directory with Terraform configs [aws].\\n'\n        '  --train-cloud                 Run training in cloud [aws].\\n'\n        '  --destroy                     Destroys cloud.\\n'\n        '  --resize                      Resizes images in dataprep.\\n'\n        '  --batch-size INTEGER          Batch size.\\n'\n        '  --epochs-train-dense INTEGER  Number of epochs train only dense layer.\\n'\n        '  --epochs-train-all INTEGER    Number of epochs train all layers.\\n'\n        '  --learning-rate-dense FLOAT   Learning rate dense layers.\\n'\n        '  --learning-rate-all FLOAT     Learning rate all layers.\\n'\n        '  --base-model-name TEXT        Pretrained CNN to be used for transfer learning.\\n'\n        '  --create-report               Create evaluation report via jupyter notebook.\\n'\n        '  --kernel-name TEXT            Kernel-name for juypter notebook.\\n'\n        '  --export-html                 Export evaluation report to html.\\n'\n        '  --export-pdf                  Export evaluation report to pdf.\\n'\n        '  --cloud-tag TEXT              Tag under which all cloud resources are created.\\n'\n        '  --help                        Show this message and exit.\\n'\n    )\n    result = runner.invoke(cli, ['pipeline', '--help'])\n    print(result.stdout)\n    assert expected in result.stdout\n\n    expected = (\n        'Options:\\n'\n        '  --config-file PATH   Central configuration file.\\n'\n        '  --image-dir PATH     Directory with image files.\\n'\n        '  --samples-file PATH  JSON file with samples.\\n'\n        '  --job-dir PATH       Directory with train, val, and test samples files and\\n'\n        '                       class_mapping file.\\n'\n        '  --resize             Resizes images and stores them in _resized subfolder.\\n'\n        '  --help               Show this message and exit.\\n'\n    )\n    result = runner.invoke(cli, ['dataprep', '--help'])\n    assert expected in result.stdout\n\n    expected = (\n        'Options:\\n'\n        '  --config-file PATH            Central configuration file.\\n'\n        '  --image-dir PATH              Directory with image files.\\n'\n        '  --job-dir PATH                Directory with train, val, and test samples\\n'\n        '                                files and class_mapping file.\\n'\n        '  --provider TEXT               Cloud provider, currently supported: [aws].\\n'\n        '  --instance-type TEXT          Cloud instance_type [aws].\\n'\n        '  --region TEXT                 Cloud region [aws].\\n'\n        '  --vpc-id TEXT                 Cloud VPC id [aws].\\n'\n        '  --bucket TEXT                 Cloud bucket used for persistence [aws].\\n'\n        '  --tf-dir TEXT                 Directory with Terraform configs [aws].\\n'\n        '  --train-cloud                 Run training in cloud [aws].\\n'\n        '  --destroy                     Destroys cloud.\\n'\n        '  --batch-size INTEGER          Batch size.\\n'\n        '  --epochs-train-dense INTEGER  Number of epochs train only dense layer.\\n'\n        '  --epochs-train-all INTEGER    Number of epochs train all layers.\\n'\n        '  --learning-rate-dense FLOAT   Learning rate dense layers.\\n'\n        '  --learning-rate-all FLOAT     Learning rate all layers.\\n'\n        '  --base-model-name TEXT        Pretrained CNN to be used for transfer learning.\\n'\n        '  --cloud-tag TEXT              Tag under which all cloud resources are created.\\n'\n        '  --help                        Show this message and exit.\\n'\n    )\n    result = runner.invoke(cli, ['train', '--help'])\n    assert expected in result.stdout\n\n    expected = (\n        'Options:\\n'\n        '  --config-file PATH  Central configuration file.\\n'\n        '  --image-dir PATH    Directory with image files.\\n'\n        '  --job-dir PATH      Directory with test samples files and trained model.\\n'\n        '  --create-report     Create evaluation report via jupyter notebook.\\n'\n        '  --kernel-name TEXT  Kernel-name for juypter notebook.\\n'\n        '  --export-html       Export evaluation report to html.\\n'\n        '  --export-pdf        Export evaluation report to pdf.\\n'\n        '  --help              Show this message and exit.\\n'\n    )\n    result = runner.invoke(cli, ['evaluate', '--help'])\n    assert expected in result.stdout\n\n    expected = (\n        'Options:\\n'\n        '  --config-file PATH    Central configuration file.\\n'\n        '  --job-dir PATH        Directory with test samples files and trained model.\\n'\n        '  --provider TEXT       Cloud provider, currently supported: [aws].\\n'\n        '  --instance-type TEXT  Cloud instance_type [aws].\\n'\n        '  --region TEXT         Cloud region [aws].\\n'\n        '  --vpc-id TEXT         Cloud VPC id [aws].\\n'\n        '  --bucket TEXT         Cloud bucket used for persistence [aws].\\n'\n        '  --tf-dir TEXT         Directory with Terraform configs [aws].\\n'\n        '  --train-cloud         Run training in cloud [aws].\\n'\n        '  --destroy             Destroys cloud.\\n'\n        '  --no-destroy          Keeps cloud.\\n'\n        '  --cloud-tag TEXT      Tag under which all cloud resources are created.\\n'\n        '  --help                Show this message and exit.\\n'\n    )\n    result = runner.invoke(cli, ['cloud', '--help'])\n    assert expected in result.stdout\n"""
tests/client/test_commands.py,0,"b""import pytest\nimport shutil\nimport logging\nfrom pathlib import Path\nfrom imageatm.client.client import Config\nfrom imageatm.client.commands import pipeline, train, evaluate, cloud\n\np = Path(__file__)\n\nTEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_train.yml'\n\n# TODO: Add relative path\nTEST_JOB_DIR = 'test_dataprep'\n\n\n@pytest.fixture(scope='module', autouse=True)\ndef tear_down(request):\n    def remove_job_dir():\n        shutil.rmtree(TEST_JOB_DIR)\n\n    request.addfinalizer(remove_job_dir)\n\n\ndef mock_scripts(mocker):\n    m_dp = mocker.patch('imageatm.scripts.run_dataprep')\n    m_tc = mocker.patch('imageatm.scripts.run_training_cloud')\n    m_t = mocker.patch('imageatm.scripts.run_training')\n    m_e = mocker.patch('imageatm.scripts.run_evaluation')\n    m_c = mocker.patch('imageatm.scripts.run_cloud')\n\n    m_l = mocker.patch('imageatm.client.commands.get_logger')\n    logger = logging.getLogger()\n    m_l.return_value = logger\n\n    return m_dp, m_tc, m_t, m_e, m_c, m_l\n\n\ndef test_pipeline(mocker):\n    # assert that only dataprep gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_dataprep.yml'\n\n    config = Config()\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    pipeline(config, config_file=TEST_CONFIG_FILE)\n\n    m_dp.assert_called()\n    m_tc.assert_not_called()\n    m_t.assert_not_called()\n    m_e.assert_not_called()\n    m_c.assert_not_called()\n\n    # assert that only train gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_train.yml'\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    pipeline(config, config_file=TEST_CONFIG_FILE)\n\n    m_dp.assert_not_called()\n    m_tc.assert_not_called()\n    m_t.assert_called()\n    m_e.assert_not_called()\n    m_c.assert_not_called()\n\n    # assert that only train cloud gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_train_cloud.yml'\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    pipeline(config, config_file=TEST_CONFIG_FILE)\n\n    m_dp.assert_not_called()\n    m_tc.assert_called()\n    m_t.assert_not_called()\n    m_e.assert_not_called()\n    m_c.assert_not_called()\n\n    # assert that only evaluate gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_evaluate.yml'\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    pipeline(config, config_file=TEST_CONFIG_FILE)\n\n    m_dp.assert_not_called()\n    m_tc.assert_not_called()\n    m_t.assert_not_called()\n    m_e.assert_called()\n    m_c.assert_not_called()\n\n    # assert that only cloud gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_cloud.yml'\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    pipeline(config, config_file=TEST_CONFIG_FILE)\n\n    m_dp.assert_not_called()\n    m_tc.assert_not_called()\n    m_t.assert_not_called()\n    m_e.assert_not_called()\n    m_c.assert_called()\n\n    # assert that all components get run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_all.yml'\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    pipeline(config, config_file=TEST_CONFIG_FILE)\n\n    m_dp.assert_called()\n    m_tc.assert_not_called()\n    m_t.assert_called()\n    m_e.assert_called()\n    m_c.assert_called()\n\n\ndef test_train(mocker):\n    # assert that train gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_train.yml'\n\n    config = Config()\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    train(config, config_file=TEST_CONFIG_FILE)\n\n    m_tc.assert_not_called()\n    m_t.assert_called()\n\n    # assert that train cloud gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_train_cloud.yml'\n\n    config = Config()\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    train(config, config_file=TEST_CONFIG_FILE)\n\n    m_tc.assert_called()\n    m_t.assert_not_called()\n\n\ndef test_evaluate(mocker):\n    # assert that evaluate gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_evaluate.yml'\n\n    config = Config()\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    evaluate(config, config_file=TEST_CONFIG_FILE)\n\n    m_e.assert_called()\n\n    # assert that train gets not run\n    # even though run=False in config, if user calls evaluate command we want it to run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_evaluate.yml'\n\n    config = Config()\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    evaluate(config, config_file=TEST_CONFIG_FILE)\n\n    m_e.assert_called()\n\n\ndef test_cloud(mocker):\n    # assert that evaluate gets run\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_cloud.yml'\n\n    config = Config()\n\n    m_dp, m_tc, m_t, m_e, m_c, m_l = mock_scripts(mocker)\n\n    cloud(config, config_file=TEST_CONFIG_FILE)\n\n    m_c.assert_called()\n"""
tests/client/test_config.py,0,"b""from pathlib import Path\nfrom imageatm.client.client import Config\nfrom imageatm.client.config import update_component_configs, update_config, get_diff\n\np = Path(__file__)\n\n\ndef test_update_component_configs():\n    config = Config()\n    config.image_dir = 'test_image'\n    config.job_dir = 'test_job'\n\n    result = update_component_configs(config)\n\n    assert result.dataprep['image_dir'] == 'test_image'\n    assert result.train['image_dir'] == 'test_image'\n    assert result.evaluate['image_dir'] == 'test_image'\n\n    assert result.dataprep['job_dir'] == 'test_job'\n    assert result.train['job_dir'] == 'test_job'\n    assert result.evaluate['job_dir'] == 'test_job'\n\n\ndef test_update_config():\n    # check that defaults are being set\n    config = Config()\n\n    result = update_config(config)\n\n    assert result.train == {\n        'cloud': False\n    }\n    assert result.dataprep == {\n        'resize': False\n    }\n    assert result.cloud == {}\n    assert result.evaluate == {\n        'report': {\n            'create': False,\n            'kernel_name': 'imageatm',\n            'export_html': False,\n            'export_pdf': False\n        }\n    }\n\n    # check that defaults, image_dir, and job_dir are being set\n    config = Config()\n    config.image_dir = 'test_image'\n    config.job_dir = 'test_job'\n\n    result = update_config(config)\n\n    assert result.train == {\n        'cloud': False,\n        'image_dir': 'test_image',\n        'job_dir': 'test_job'\n    }\n    assert result.dataprep == {\n        'resize': False,\n        'image_dir': 'test_image',\n        'job_dir': 'test_job'\n    }\n    assert result.cloud == {\n        'job_dir': 'test_job'\n    }\n    assert result.evaluate == {\n        'image_dir': 'test_image',\n        'job_dir': 'test_job',\n        'report': {\n            'create': False,\n            'kernel_name': 'imageatm',\n            'export_html': False,\n            'export_pdf': False\n        }\n    }\n\n    # check that config file gets populated correctly\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_train.yml'\n\n    config = Config()\n\n    result = update_config(config, config_file=TEST_CONFIG_FILE)\n\n    assert result.train == {\n        'run': True,\n        'cloud': False,\n        'image_dir': 'test_train/images',\n        'job_dir': 'test_train/job_dir',\n    }\n    assert result.dataprep == {\n        'run': False,\n        'resize': True,\n        'image_dir': 'test_train/images',\n        'job_dir': 'test_train/job_dir',\n        'samples_file': 'test_dataprep/samples.json',\n    }\n    assert result.cloud == {\n        'run': False,\n        'provider': 'aws',  # supported providers ['aws']\n        'tf_dir': 'cloud/aws',\n        'region': 'eu-west-1',  # supported regions ['eu-west-1', 'eu-central-1']\n        'vpc_id': 'abc',\n        'instance_type': 't2.micro',  # supported instances ['p2.xlarge']\n        'bucket': 's3://test_bucket',  # s3 bucket needs to exist, will not be created/destroyed by terraform\n        'destroy': True,\n        'job_dir': 'test_train/job_dir',\n        'cloud_tag': 'test_user',\n    }\n    assert result.evaluate == {\n        'run': False,\n        'image_dir': 'test_train/images',\n        'job_dir': 'test_train/job_dir',\n        'report': {\n            'create': False,\n            'kernel_name': 'imageatm',\n            'export_html': False,\n            'export_pdf': False\n        }\n    }\n\n    # test that options overwrite config file\n    TEST_CONFIG_FILE = p.resolve().parent / 'test_configs' / 'config_train.yml'\n\n    config = Config()\n\n    result = update_config(\n        config,\n        config_file=TEST_CONFIG_FILE,\n        image_dir='test_image',\n        job_dir='test_job',\n        region='eu-central-1',\n    )\n\n    assert result.train == {\n        'run': True,\n        'cloud': False,\n        'image_dir': 'test_image',\n        'job_dir': 'test_job',\n    }\n\n    assert result.dataprep == {\n        'run': False,\n        'resize': True,\n        'image_dir': 'test_image',\n        'job_dir': 'test_job',\n        'samples_file': 'test_dataprep/samples.json',\n    }\n\n    assert result.cloud == {\n        'run': False,\n        'provider': 'aws',\n        'tf_dir': 'cloud/aws',\n        'region': 'eu-central-1',\n        'vpc_id': 'abc',\n        'instance_type': 't2.micro',\n        'bucket': 's3://test_bucket',\n        'destroy': True,\n        'bucket': 's3://test_bucket',\n        'job_dir': 'test_job',\n        'cloud_tag': 'test_user',\n    }\n\n    assert result.evaluate == {\n        'run': False,\n        'image_dir': 'test_image',\n        'job_dir': 'test_job',\n        'report': {\n            'create': False,\n            'kernel_name': 'imageatm',\n            'export_html': False,\n            'export_pdf': False\n        }\n    }\n\n\ndef test_get_diff():\n    # test required fields missing\n    required_keys = ['a', 'b']\n    optional_keys = ['c', 'd']\n\n    config = {'a': 124}\n\n    expected = ['train config: missing required parameters [b]\\n']\n    result = get_diff('train', config, required_keys, optional_keys)\n\n    assert result == expected\n\n    # test no required fields missing\n    required_keys = ['a', 'b']\n    optional_keys = ['c', 'd']\n\n    config = {'a': 124, 'b': 234}\n\n    expected = []\n    result = get_diff('train', config, required_keys, optional_keys)\n\n    assert result == expected\n\n    # test all keys allowed\n    required_keys = ['a', 'b']\n    optional_keys = ['c', 'd']\n\n    config = {'a': 124, 'b': 234, 'c': 456, 'd': 678}\n\n    expected = []\n    result = get_diff('train', config, required_keys, optional_keys)\n\n    assert result == expected\n\n    # test not all keys allowed\n    required_keys = ['a', 'b']\n    optional_keys = ['c', 'd']\n\n    config = {'a': 124, 'b': 234, 'c': 456, 'e': 678}\n\n    expected = ['train config: [e] not in allowed parameters [a, b, c, d]\\n']\n    result = get_diff('train', config, required_keys, optional_keys)\n\n    assert result == expected\n\n    # test both allowed and rrequired keys\n    required_keys = ['a', 'b']\n    optional_keys = ['c', 'd']\n\n    config = {'a': 124, 'c': 456, 'e': 678}\n\n    expected = [\n        'train config: missing required parameters [b]\\n',\n        'train config: [e] not in allowed parameters [a, b, c, d]\\n',\n    ]\n    result = get_diff('train', config, required_keys, optional_keys)\n\n    assert result == expected\n"""
tests/components/test_cloud.py,0,"b'import pytest\nfrom mock import call\nfrom pathlib import Path\nfrom yarl import URL\nfrom imageatm.components.cloud import AWS\n\nTEST_TF_DIR = Path(\'./tests/data/test_train_job\').resolve()\nTEST_REGION = \'test-region\'\nTEST_INSTANCE_TYPE = \'test_instance_type\'\nTEST_VPC_ID = \'test_vpc_id\'\nTEST_S3_BUCKET = URL(\'s3://test_s3_bucket\')\nTEST_JOB_DIR = Path(\'./tests/data/test_train_job\').resolve()\nTEST_CLOUD_TAG = \'test_cloud_tag\'\nTEST_IMG_DIR = Path(\'./tests/data/test_images\').resolve()\n\n\n@pytest.fixture(scope=\'class\', autouse=True)\ndef tear_down(request):\n    def remove_logs():\n        (TEST_JOB_DIR / \'logs\').unlink()\n\n    request.addfinalizer(remove_logs)\n\n\nclass TestAWS(object):\n\n    aws = None\n\n    def test__init__(self, mocker):\n        mp__check_s3_prefix = mocker.patch(\'imageatm.components.cloud.AWS._check_s3_prefix\')\n\n        global aws\n        aws = AWS(\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            s3_bucket=TEST_S3_BUCKET,\n            job_dir=TEST_JOB_DIR,\n            cloud_tag=TEST_CLOUD_TAG,\n        )\n\n        assert aws.tf_dir == TEST_TF_DIR\n        assert aws.region == TEST_REGION\n        assert aws.instance_type == TEST_INSTANCE_TYPE\n        assert aws.vpc_id == TEST_VPC_ID\n        assert aws.s3_bucket == TEST_S3_BUCKET\n        assert aws.job_dir == Path(TEST_JOB_DIR)\n        assert aws.cloud_tag == TEST_CLOUD_TAG\n        assert aws.remote_workdir == Path(\'/home/ec2-user/image-atm\').resolve()\n        mp__check_s3_prefix.assert_called_once()\n\n    def test__check_s3_prefix(self):\n        global aws\n\n        aws.s3_bucket = TEST_S3_BUCKET\n        aws._check_s3_prefix()\n        assert aws.s3_bucket == URL(\'s3://test_s3_bucket\')\n\n        aws.s3_bucket = \'test_s3_bucket\'\n        aws._check_s3_prefix()\n        assert aws.s3_bucket == URL(\'s3://test_s3_bucket\')\n\n    def test__set_ssh(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        assert aws.ssh is None\n        aws._set_ssh()\n        assert aws.ssh is not None\n        mp_run_cmd.assert_called_with(\n            \'cd {} && terraform output public_ip\'.format(TEST_TF_DIR),\n            logger=aws.logger,\n            return_output=True,\n        )\n\n    def test__set_remote_dirs(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        global aws\n        aws.image_dir = TEST_IMG_DIR\n        aws.job_dir = TEST_JOB_DIR\n\n        assert not hasattr(aws, \'remote_image_dir\')\n        assert not hasattr(aws, \'remote_job_dir\')\n\n        aws._set_remote_dirs()\n        assert hasattr(aws, \'remote_image_dir\')\n        assert hasattr(aws, \'remote_job_dir\')\n        assert aws.remote_image_dir == Path(\'/home/ec2-user/image-atm/test_images\').resolve()\n        assert aws.remote_job_dir == Path(\'/home/ec2-user/image-atm/test_train_job\').resolve()\n\n    def test__set_s3_dirs(self):\n        aws.image_dir = TEST_IMG_DIR\n        aws.job_dir = TEST_JOB_DIR\n\n        assert not hasattr(aws, \'s3_image_dir\')\n        assert not hasattr(aws, \'s3_job_dir\')\n\n        aws._set_s3_dirs()\n\n        assert aws.s3_image_dir == URL(\'s3://test_s3_bucket/image_dirs/test_images\')\n        assert aws.s3_job_dir == URL(\'s3://test_s3_bucket/job_dirs/test_train_job\')\n\n        aws.image_dir = URL(\'s3://test_s3_bucket/image_dirs/test_images2\')\n        aws.job_dir = URL(\'s3://test_s3_bucket/job_dirs/test_train_job2\')\n\n        aws._set_s3_dirs()\n\n        assert aws.s3_image_dir == URL(\'s3://test_s3_bucket/image_dirs/test_images2\')\n        assert aws.s3_job_dir == URL(\'s3://test_s3_bucket/job_dirs/test_train_job2\')\n\n    def test__sync_local_s3_1(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.image_dir = TEST_IMG_DIR\n        aws.job_dir = TEST_JOB_DIR\n        calls = [\n            call(\n                \'aws s3 sync --quiet --exclude logs {} {}\'.format(\n                    TEST_IMG_DIR, URL(\'s3://test_s3_bucket/image_dirs/test_images\')\n                ),\n                logger=aws.logger,\n            ),\n            call(\n                \'aws s3 sync --quiet --exclude logs {} {}\'.format(\n                    TEST_JOB_DIR, URL(\'s3://test_s3_bucket/job_dirs/test_train_job\')\n                ),\n                logger=aws.logger,\n            ),\n        ]\n\n        aws._sync_local_s3()\n        mp_run_cmd.assert_has_calls(calls)\n\n    def test__sync_local_s3_2(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.image_dir = URL(\'s3://test_s3_bucket/image_dirs/test_images\')\n        aws.job_dir = URL(\'s3://test_s3_bucket/job_dirs/test_train_job\')\n\n        aws._sync_local_s3()\n        mp_run_cmd.assert_not_called()\n\n    def test__sync_s3_local_1(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.job_dir = TEST_JOB_DIR\n        aws._sync_s3_local()\n        mp_run_cmd.assert_called_with(\n            \'aws s3 sync --exclude logs --quiet {} {}\'.format(\n                URL(\'s3://test_s3_bucket/job_dirs/test_train_job\'), TEST_JOB_DIR\n            ),\n            logger=aws.logger,\n            level=\'info\',\n        )\n\n    def test__sync_s3_local_2(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.job_dir = URL(\'s3://test_s3_bucket/job_dirs/test_train_job\')\n        aws._sync_s3_local()\n        mp_run_cmd.assert_not_called()\n\n    def test__sync_remote_s3_1(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.image_dir = TEST_IMG_DIR\n        aws.job_dir = TEST_JOB_DIR\n        aws.ssh = \'test_ssh\'\n        calls = [\n            call(\n                \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n                    \'test_ssh\',\n                    Path(\'/home/ec2-user/image-atm/test_images\'),\n                    URL(\'s3://test_s3_bucket/image_dirs/test_images\'),\n                ),\n                logger=aws.logger,\n            ),\n            call(\n                \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n                    \'test_ssh\',\n                    Path(\'/home/ec2-user/image-atm/test_train_job\'),\n                    URL(\'s3://test_s3_bucket/job_dirs/test_train_job\'),\n                ),\n                logger=aws.logger,\n            ),\n        ]\n\n        aws._sync_remote_s3()\n        mp_run_cmd.assert_has_calls(calls)\n\n    def test__sync_remote_s3_2(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.image_dir = URL(\'s3://test_s3_bucket/image_dirs/test_images\')\n        aws.job_dir = URL(\'s3://test_s3_bucket/job_dirs/test_train_job\')\n        aws.ssh = \'test_ssh\'\n        calls = [\n            call(\n                \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n                    \'test_ssh\',\n                    Path(\'/home/ec2-user/image-atm/test_images\'),\n                    URL(\'s3://test_s3_bucket/image_dirs/test_images\'),\n                ),\n                logger=aws.logger,\n            ),\n            call(\n                \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n                    \'test_ssh\',\n                    Path(\'/home/ec2-user/image-atm/test_train_job\'),\n                    URL(\'s3://test_s3_bucket/job_dirs/test_train_job\'),\n                ),\n                logger=aws.logger,\n            ),\n        ]\n\n        aws._sync_remote_s3()\n        mp_run_cmd.assert_has_calls(calls)\n\n    def test__sync_s3_remote_1(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.image_dir = TEST_IMG_DIR\n        aws.job_dir = TEST_JOB_DIR\n        aws.ssh = \'test_ssh\'\n        calls = [\n            call(\n                \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n                    \'test_ssh\',\n                    URL(\'s3://test_s3_bucket/image_dirs/test_images\'),\n                    Path(\'/home/ec2-user/image-atm/test_images\'),\n                ),\n                logger=aws.logger,\n            ),\n            call(\n                \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n                    \'test_ssh\',\n                    URL(\'s3://test_s3_bucket/job_dirs/test_train_job\'),\n                    Path(\'/home/ec2-user/image-atm/test_train_job\'),\n                ),\n                logger=aws.logger,\n            ),\n        ]\n        aws._sync_s3_remote()\n        mp_run_cmd.assert_has_calls(calls)\n\n    def test__sync_s3_remote_2(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.image_dir = URL(\'s3://test_s3_bucket/image_dirs/test_images\')\n        aws.job_dir = URL(\'s3://test_s3_bucket/job_dirs/test_train_job\')\n        aws.ssh = \'test_ssh\'\n        calls = [\n            call(\n                \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n                    \'test_ssh\',\n                    URL(\'s3://test_s3_bucket/image_dirs/test_images\'),\n                    Path(\'/home/ec2-user/image-atm/test_images\'),\n                ),\n                logger=aws.logger,\n            ),\n            call(\n                \'{} aws s3 sync --exclude logs --quiet {} {}\'.format(\n                    \'test_ssh\',\n                    URL(\'s3://test_s3_bucket/job_dirs/test_train_job\'),\n                    Path(\'/home/ec2-user/image-atm/test_train_job\'),\n                ),\n                logger=aws.logger,\n            ),\n        ]\n        aws._sync_s3_remote()\n        mp_run_cmd.assert_has_calls(calls)\n\n    def test__launch_train_container(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        kwargs = {\'key_1\': \'test\', \'key_2\': 1, \'key_3\': None}\n\n        aws._launch_train_container(**kwargs)\n\n        mp_run_cmd.assert_called_with(\n            \'{} docker run -d -v {}:$WORKDIR/image_dir -v {}:$WORKDIR/job_dir {} \'\n            \'idealo/tensorflow-image-atm:1.13.1\'.format(\n                \'test_ssh\',\n                Path(\'/home/ec2-user/image-atm/test_images\'),\n                Path(\'/home/ec2-user/image-atm/test_train_job\'),\n                \'-e key_1=test -e key_2=1\',\n            ),\n            logger=aws.logger,\n        )\n\n    def test__stream_docker_logs(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\', return_value=\'cmd_output\')\n\n        calls = [\n            call(\'test_ssh docker ps -l -q\', logger=aws.logger, return_output=True),\n            call(\'test_ssh docker logs cmd_output --follow\', logger=aws.logger, level=\'info\'),\n        ]\n        aws._stream_docker_logs()\n        mp_run_cmd.assert_has_calls(calls)\n\n    def test_init(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.init()\n        mp_run_cmd.assert_called_with(\n            \'cd {} && terraform init\'.format(TEST_TF_DIR), logger=aws.logger\n        )\n\n    def test_apply(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        calls = [\n            call(\n                (\n                    \'cd {} && terraform apply -auto-approve -var ""region={}"" -var ""instance_type={}"" \'\n                    \'-var ""vpc_id={}"" -var ""s3_bucket={}"" -var ""name={}""\'\n                ).format(\n                    TEST_TF_DIR,\n                    TEST_REGION,\n                    TEST_INSTANCE_TYPE,\n                    TEST_VPC_ID,\n                    URL(\'test_s3_bucket\'),\n                    TEST_CLOUD_TAG,\n                ),\n                logger=aws.logger,\n            ),\n            call(\n                \'cd {} && terraform output public_ip\'.format(TEST_TF_DIR),\n                logger=aws.logger,\n                return_output=True,\n            ),\n        ]\n\n        aws.apply()\n        mp_run_cmd.assert_has_calls(calls)\n\n    def test_train_1(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n        mp__sync_local_s3 = mocker.patch(\'imageatm.components.cloud.AWS._sync_local_s3\')\n        mp__sync_s3_remote = mocker.patch(\'imageatm.components.cloud.AWS._sync_s3_remote\')\n        mp__launch_train_container = mocker.patch(\n            \'imageatm.components.cloud.AWS._launch_train_container\'\n        )\n        mp__stream_docker_logs = mocker.patch(\'imageatm.components.cloud.AWS._stream_docker_logs\')\n        mp__sync_remote_s3 = mocker.patch(\'imageatm.components.cloud.AWS._sync_remote_s3\')\n        mp__sync_s3_local = mocker.patch(\'imageatm.components.cloud.AWS._sync_s3_local\')\n\n        aws.image_dir = TEST_IMG_DIR\n        aws.job_dir = TEST_JOB_DIR\n\n        aws.train()\n        assert aws.image_dir == Path(TEST_IMG_DIR).resolve()\n        mp__sync_local_s3.assert_called_once()\n        mp__sync_s3_remote.assert_called_once()\n        mp__launch_train_container.assert_called_once()\n        mp__stream_docker_logs.assert_called_once()\n        mp__sync_remote_s3.assert_called_once()\n        mp__sync_s3_local.assert_called_once()\n\n    def test_train_2(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n        mp__sync_local_s3 = mocker.patch(\'imageatm.components.cloud.AWS._sync_local_s3\')\n        mp__sync_s3_remote = mocker.patch(\'imageatm.components.cloud.AWS._sync_s3_remote\')\n        mp__launch_train_container = mocker.patch(\n            \'imageatm.components.cloud.AWS._launch_train_container\'\n        )\n        mp__stream_docker_logs = mocker.patch(\'imageatm.components.cloud.AWS._stream_docker_logs\')\n        mp__sync_remote_s3 = mocker.patch(\'imageatm.components.cloud.AWS._sync_remote_s3\')\n        mp__sync_s3_local = mocker.patch(\'imageatm.components.cloud.AWS._sync_s3_local\')\n\n        aws.train(image_dir=\'./tests/data/test_no_images\')\n        assert aws.image_dir == Path(\'./tests/data/test_no_images\').resolve()\n        mp__sync_local_s3.assert_called_once()\n        mp__sync_s3_remote.assert_called_once()\n        mp__launch_train_container.assert_called_once()\n        mp__stream_docker_logs.assert_called_once()\n        mp__sync_remote_s3.assert_called_once()\n        mp__sync_s3_local.assert_called_once()\n\n    def test_destroy(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n\n        aws.destroy()\n        mp_run_cmd.assert_called_with(\n            (\n                \'cd {} && terraform destroy -auto-approve -var ""region={}"" -var ""instance_type={}"" \'\n                \'-var ""vpc_id={}"" -var ""s3_bucket={}"" -var ""name={}""\'\n            ).format(\n                TEST_TF_DIR,\n                TEST_REGION,\n                TEST_INSTANCE_TYPE,\n                TEST_VPC_ID,\n                URL(\'test_s3_bucket\'),\n                TEST_CLOUD_TAG,\n            ),\n            logger=aws.logger,\n        )\n\n    def test_integration(self, mocker):\n        mp_run_cmd = mocker.patch(\'imageatm.components.cloud.run_cmd\')\n        mp__sync_local_s3 = mocker.patch(\'imageatm.components.cloud.AWS._sync_local_s3\')\n        mp__sync_s3_remote = mocker.patch(\'imageatm.components.cloud.AWS._sync_s3_remote\')\n        mp__launch_train_container = mocker.patch(\n            \'imageatm.components.cloud.AWS._launch_train_container\'\n        )\n        mp__stream_docker_logs = mocker.patch(\'imageatm.components.cloud.AWS._stream_docker_logs\')\n        mp__sync_remote_s3 = mocker.patch(\'imageatm.components.cloud.AWS._sync_remote_s3\')\n        mp__sync_s3_local = mocker.patch(\'imageatm.components.cloud.AWS._sync_s3_local\')\n\n        aws2 = AWS(\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            s3_bucket=TEST_S3_BUCKET,\n            job_dir=TEST_JOB_DIR,\n            cloud_tag=TEST_CLOUD_TAG,\n        )\n\n        aws2.init()\n        aws2.apply()\n\n        NEW_JOB_DIR = Path(\'./tests/data/test_train_job_copy\').resolve()\n\n        aws2.train(job_dir=NEW_JOB_DIR)\n        aws2.destroy()\n\n        assert aws2.job_dir == NEW_JOB_DIR\n        mp__sync_local_s3.assert_called_once()\n        mp__sync_s3_remote.assert_called_once()\n        mp__launch_train_container.assert_called_once()\n        mp__stream_docker_logs.assert_called_once()\n        mp__sync_remote_s3.assert_called_once()\n        mp__sync_s3_local.assert_called_once()\n'"
tests/components/test_dataprep.py,0,"b'import shutil\nimport pytest\nimport numpy.testing as npt\nfrom collections import Counter\nfrom mock import call\nfrom pathlib import Path\nfrom imageatm.components.dataprep import DataPrep\nfrom imageatm.utils.io import load_json\nfrom imageatm.utils.images import resize_image_mp\n\np = Path(__file__)\n""""""Files for test_valid_images.""""""\nINVALID_IMG_PATH = p.parent / \'../data/test_images\' / \'image_invalid.jpg\'\n\nVALID_IMG_PATH = p.parent / \'../data/test_images\' / \'image_960x540.jpg\'\n\n""""""Files for sample validation.""""""\nTEST_STR_FILE = p.parent / \'../data/test_samples\' / \'test_str_labels.json\'\n\nTEST_INT_FILE = p.parent / \'../data/test_samples\' / \'test_samples_int.json\'\n\nTEST_FILE_STR2INT = p.parent / \'../data/test_samples\' / \'test_int_labels.json\'\n\nTEST_IMG_DIR = p.parent / \'../data/test_images\'\n\nTEST_NO_IMG_DIR = p.parent / \'../data/test_no_images\'\n\nTEST_SPLIT_FILE = p.parent / \'../data/test_samples\' / \'test_split.json\'\n\nTEST_STR_FILE_CORRUPTED = p.parent / \'../data/test_samples\' / \'test_str_labels_corrupted.json\'\n\nTEST_JOB_DIR = p.parent / \'test_job_dir\'\n\n\n@pytest.fixture(scope=\'class\', autouse=True)\ndef tear_down(request):\n    def remove_job_dir():\n        shutil.rmtree(TEST_JOB_DIR)\n        shutil.rmtree(p.parent / \'../data/test_images_resized\')\n\n    request.addfinalizer(remove_job_dir)\n\n\nclass TestDataPrep(object):\n    dp = None\n\n    def test__init(self, mocker):\n        mocker.patch(\'imageatm.utils.io.load_json\', return_value={})\n        global dp\n        dp = DataPrep(image_dir=TEST_IMG_DIR, job_dir=TEST_JOB_DIR, samples_file=TEST_STR_FILE)\n\n        assert dp.image_dir == TEST_IMG_DIR\n        assert dp.job_dir == TEST_JOB_DIR\n        assert dp.samples_file == TEST_STR_FILE\n\n    def test__validate_images_1(self):\n        expected = [\n            \'helmet_1.jpg\',\n            \'helmet_10.jpg\',\n            \'helmet_2.jpg\',\n            \'helmet_3.jpg\',\n            \'helmet_4.jpg\',\n            \'helmet_5.jpg\',\n            \'helmet_6.jpg\',\n            \'helmet_7.jpg\',\n            \'helmet_8.jpg\',\n            \'helmet_9.jpg\',\n            \'image_960x540.jpg\',\n            \'image_png.png\',\n        ]\n        global dp\n        dp._validate_images()\n\n        assert sorted(dp.valid_image_ids) == expected\n\n    def test__validate_images_2(self, mocker):\n        mp_save_json = mocker.patch(\'imageatm.components.dataprep.save_json\')\n\n        global dp\n        dp.image_dir = TEST_NO_IMG_DIR\n\n        invalid_image_files = []\n        files = [str(i.absolute()) for i in dp.image_dir.glob(\'*\')]\n        files.sort()\n        for file in files:\n            inv_tuple = (\n                str(file),\n                str(OSError(""cannot identify image file \'{}\'"".format(str(file)))),\n            )\n            invalid_image_files.append(inv_tuple)\n\n        dp._validate_images()\n\n        mp_save_json.assert_called_with(\n            invalid_image_files, dp.job_dir / \'invalid_image_files.json\'\n        )\n\n    def test__validate_sample(self):\n        global dp\n\n        valid_image_ids = [\'helmet_1.jpg\', \'helmet_2.jpg\', \'image_png.png\']\n\n        sample = {\'image_id\': \'helmet_3.jpg\', \'blabla\': \'left\'}\n        result = dp._validate_sample(sample, valid_image_ids)\n        assert result == False\n\n        sample = {\'blabla\': \'helmet_1.jpg\', \'label\': \'right\'}\n        result = dp._validate_sample(sample, valid_image_ids)\n        assert result == False\n\n        sample = {\'image_id\': \'truncated.jpg\', \'label\': \'left\'}\n        result = dp._validate_sample(sample, valid_image_ids)\n        assert result == False\n\n        sample = {\'image_id\': \'helmet_1.jpg\', \'label\': \'left\'}\n        result = dp._validate_sample(sample, valid_image_ids)\n        assert result == True\n\n        sample = {\'image_id\': \'helmet_5.jpg\', \'label\': \'left\'}\n        result = dp._validate_sample(sample, valid_image_ids)\n        assert result == False\n\n    def test__validate_samples_1(self):\n        global dp\n        expected = load_json(TEST_STR_FILE)\n\n        dp.min_class_size = 1\n        dp.valid_image_ids = [\'helmet_1.jpg\', \'helmet_2.jpg\', \'helmet_3.jpg\', \'image_png.png\']\n        dp._validate_samples()\n\n        assert dp.samples == expected\n        assert dp.invalid_samples == []\n        assert dp.samples_count == {\'left\': 3, \'right\': 1}\n\n        # exclude first 3 samples as they are corrupted\n        expected = [\n            {\'image_id\': \'helmet_1.jpg\', \'label\': \'left\'},\n            {\'image_id\': \'helmet_2.jpg\', \'label\': \'left\'},\n            {\'image_id\': \'image_png.png\', \'label\': \'right\'},\n        ]\n\n        dp.valid_image_ids = [\'helmet_1.jpg\', \'helmet_2.jpg\', \'image_png.png\']\n        dp.samples = load_json(TEST_STR_FILE_CORRUPTED)\n        dp._validate_samples()\n\n        assert dp.samples == expected\n\n    def test__validate_samples_2(self, mocker):\n        mp_logger_info = mocker.patch(\'logging.Logger.info\')\n\n        global dp\n        dp.valid_image_ids = [\'1.jpg\', \'2.jpg\', \'3.jpg\', \'4.jpg\']\n        dp.samples = load_json(TEST_INT_FILE)\n        dp._validate_samples()\n        calls = [\n            call(\'\\n****** Running samples validation ******\\n\'),\n            call(\'The following samples were dropped:\'),\n            call(""- {\'image_id\': \'5.jpg\', \'label\': 1}""),\n            call(""- {\'image_id\': \'6.jpg\', \'label\': 1}""),\n            call(""- {\'image_id\': \'7.jpg\', \'label\': 2}""),\n            call(""- {\'image_id\': \'8.jpg\', \'label\': 1}""),\n            call(""- {\'image_id\': \'9.jpg\', \'label\': 1}""),\n            call(""- {\'image_id\': \'10.jpg\', \'label\': 2}""),\n            call(""- {\'image_id\': \'11.jpg\', \'label\': 1}""),\n            call(""- {\'image_id\': \'12.jpg\', \'label\': 1}""),\n            call(""- {\'image_id\': \'13.jpg\', \'label\': 1}""),\n            call(""- {\'image_id\': \'14.jpg\', \'label\': 2}""),\n            call(\n                \'NOTE: 26 samples were identified as invalid.\\n\'\n                \'The full list of invalid samples will be saved in job dir.\\n\'\n            ),\n            call(\'Class distribution after validation:\'),\n            call(\'1: 2 (50.0%)\'),\n            call(\'2: 2 (50.0%)\'),\n        ]\n\n        mp_logger_info.assert_has_calls(calls)\n\n    def test__validate_samples_3(self):\n        global dp\n\n        dp.min_class_size = 1000\n        dp.valid_image_ids = [\'1.jpg\', \'2.jpg\', \'3.jpg\', \'4.jpg\']\n\n        with pytest.raises(AssertionError) as excinfo:\n            dp._validate_samples()\n\n        assert \'Program ended. Collect more samples.\' in str(excinfo)\n\n        dp.min_class_size = 1\n        dp.valid_image_ids = [\'2.jpg\', \'3.jpg\']\n\n        with pytest.raises(AssertionError) as excinfo:\n            dp._validate_samples()\n\n        assert \'Program ended. Only one label in the dataset.\' in str(excinfo)\n\n    def test__create_class_mapping(self):\n        global dp\n        dp.samples = load_json(TEST_STR_FILE)\n        dp.samples_count = {\'left\': 3, \'right\': 1}\n        dp._create_class_mapping()\n        expected = {0: \'left\', 1: \'right\'}\n        assert dp.class_mapping == expected\n\n        dp.samples = dp.samples[::-1]\n        dp._create_class_mapping()\n        expected = {0: \'left\', 1: \'right\'}\n        assert dp.class_mapping == expected\n\n        dp.samples = load_json(TEST_INT_FILE)\n        dp.samples_count = {1: 10, 2: 20}\n        dp._create_class_mapping()\n        print(dp.class_mapping)\n        expected = {0: 1, 1: 2}\n        assert dp.class_mapping == expected\n\n        dp.samples = dp.samples[::-1]\n        dp._create_class_mapping()\n        expected = {0: 1, 1: 2}\n        assert dp.class_mapping == expected\n\n    def test__apply_class_mapping(self):\n        global dp\n        dp.samples = load_json(TEST_STR_FILE)\n        dp.class_mapping = {0: \'left\', 1: \'right\'}\n        dp._apply_class_mapping()\n        expected = load_json(TEST_FILE_STR2INT)\n\n        assert dp.samples == expected\n\n        dp.samples = [\n            {\'image_id\': \'helmet_2.jpg\', \'label\': \'left\'},\n            {\'image_id\': \'image_png.png\', \'label\': \'right\', \'test\': \'abc\'},\n        ]\n        expected = [\n            {\'image_id\': \'helmet_2.jpg\', \'label\': 0},\n            {\'image_id\': \'image_png.png\', \'label\': 1},\n        ]\n\n        dp._apply_class_mapping()\n\n        assert dp.samples == expected\n\n        dp.samples = [\n            {\'image_id\': \'helmet_2.jpg\', \'label\': \'left\'},\n            {\'image_id\': \'image_png.png\', \'label\': \'right\'},\n        ]\n        expected = [\n            {\'image_id\': \'helmet_2.jpg\', \'label\': 1},\n            {\'image_id\': \'image_png.png\', \'label\': 0},\n        ]\n\n        assert dp.samples != expected\n\n        dp.samples = [\n            {\'image_id\': \'helmet_2.jpg\', \'label\': 1},\n            {\'image_id\': \'image_png.png\', \'label\': 0},\n        ]\n        expected = [\n            {\'image_id\': \'helmet_2.jpg\', \'label\': 1},\n            {\'image_id\': \'image_png.png\', \'label\': 0},\n        ]\n\n        assert dp.samples == expected\n\n    def test_split_samples_full(self):\n        global dp\n        dp.samples = load_json(TEST_SPLIT_FILE)  ##100\n        dp.test_size = 0.2\n        dp.val_size = 0.5\n        dp.part_size = 1.0\n\n        dp._split_samples()\n\n        npt.assert_almost_equal(dp.train_size, 0.3)\n        assert dp.test_size + dp.val_size + dp.train_size == 1.0\n\n        assert len(dp.test_samples) == 40\n        assert len(dp.val_samples) == 100\n        assert len(dp.train_samples) == 60\n\n        train_labels_count = Counter([i[\'label\'] for i in dp.train_samples])\n        val_labels_count = Counter([i[\'label\'] for i in dp.val_samples])\n        test_labels_count = Counter([i[\'label\'] for i in dp.test_samples])\n\n        assert test_labels_count[1] == 20\n        assert test_labels_count[2] == 12\n        assert test_labels_count[3] == 8\n\n        assert val_labels_count[1] == 50\n        assert val_labels_count[2] == 30\n        assert val_labels_count[3] == 20\n\n        assert train_labels_count[1] == 30\n        assert train_labels_count[2] == 18\n        assert train_labels_count[3] == 12\n\n    def test_split_samples_full_2(self):\n        global dp\n        dp.samples = load_json(TEST_SPLIT_FILE)\n        dp.test_size = 0.2\n        dp.val_size = 0.1\n        dp.part_size = 1.0\n\n        dp._split_samples()\n\n        npt.assert_almost_equal(dp.train_size, 0.7)\n        assert dp.test_size + dp.val_size + dp.train_size == 1.0\n\n        assert len(dp.test_samples) == 40\n        assert len(dp.val_samples) == 20\n        assert len(dp.train_samples) == 140\n\n        train_labels_count = Counter([i[\'label\'] for i in dp.train_samples])\n        val_labels_count = Counter([i[\'label\'] for i in dp.val_samples])\n        test_labels_count = Counter([i[\'label\'] for i in dp.test_samples])\n\n        assert test_labels_count[1] == 20\n        assert test_labels_count[2] == 12\n        assert test_labels_count[3] == 8\n\n        assert val_labels_count[1] == 10\n        assert val_labels_count[2] == 6\n        assert val_labels_count[3] == 4\n\n        assert train_labels_count[1] == 70\n        assert train_labels_count[2] == 42\n        assert train_labels_count[3] == 28\n\n    def test_split_samples_half(self):\n        global dp\n        dp.samples = load_json(TEST_SPLIT_FILE)\n        dp.test_size = 0.2\n        dp.val_size = 0.5\n        dp.part_size = 1 / 2\n\n        dp._split_samples()\n\n        npt.assert_almost_equal(dp.train_size, 0.3)\n        assert dp.test_size + dp.val_size + dp.train_size == 1.0\n\n        assert len(dp.test_samples) == 20\n        assert len(dp.val_samples) == 50\n        assert len(dp.train_samples) == 30\n\n        train_labels_count = Counter([i[\'label\'] for i in dp.train_samples])\n        val_labels_count = Counter([i[\'label\'] for i in dp.val_samples])\n        test_labels_count = Counter([i[\'label\'] for i in dp.test_samples])\n\n        assert test_labels_count[1] == 10\n        assert test_labels_count[2] == 6\n        assert test_labels_count[3] == 4\n\n        assert val_labels_count[1] == 25\n        assert val_labels_count[2] == 15\n        assert val_labels_count[3] == 10\n\n        assert train_labels_count[1] == 15\n        assert train_labels_count[2] == 9\n        assert train_labels_count[3] == 6\n\n    def test_split_samples_half_2(self):\n        global dp\n        dp.samples = load_json(TEST_SPLIT_FILE)\n        dp.test_size = 0.2\n        dp.val_size = 0.4\n        dp.part_size = 2 / 3\n\n        dp._split_samples()\n\n        npt.assert_almost_equal(dp.train_size, 0.4)\n        assert dp.test_size + dp.val_size + dp.train_size == 1.0\n\n        assert len(dp.test_samples) == 27\n        assert len(dp.val_samples) == 53\n        assert len(dp.train_samples) == 53\n\n        train_labels_count = Counter([i[\'label\'] for i in dp.train_samples])\n        val_labels_count = Counter([i[\'label\'] for i in dp.val_samples])\n        test_labels_count = Counter([i[\'label\'] for i in dp.test_samples])\n\n        assert test_labels_count[1] == 14\n        assert test_labels_count[2] == 8\n        assert test_labels_count[3] == 5\n\n        assert val_labels_count[1] == 26\n        assert val_labels_count[2] == 16\n        assert val_labels_count[3] == 11\n\n        assert train_labels_count[1] == 27\n        assert train_labels_count[2] == 16\n        assert train_labels_count[3] == 10\n\n    def test_run_1(self, mocker):\n        mp_validate_images = mocker.patch(\'imageatm.components.dataprep.DataPrep._validate_images\')\n        mp_validate_samples = mocker.patch(\n            \'imageatm.components.dataprep.DataPrep._validate_samples\'\n        )\n        mp_create_class_mapping = mocker.patch(\n            \'imageatm.components.dataprep.DataPrep._create_class_mapping\'\n        )\n        mp_apply_class_mapping = mocker.patch(\n            \'imageatm.components.dataprep.DataPrep._apply_class_mapping\'\n        )\n        mp_split_samples = mocker.patch(\'imageatm.components.dataprep.DataPrep._split_samples\')\n        mp_resize_images = mocker.patch(\'imageatm.components.dataprep.DataPrep._resize_images\')\n        mp_save_files = mocker.patch(\'imageatm.components.dataprep.DataPrep._save_files\')\n\n        global dp\n        dp.image_dir == TEST_IMG_DIR\n        dp.job_dir == TEST_JOB_DIR\n        dp.samples_file == TEST_STR_FILE\n\n        dp.run(resize=False)\n\n        mp_validate_images.assert_called_once()\n        mp_validate_samples.assert_called_once()\n        mp_create_class_mapping.assert_called_once()\n        mp_apply_class_mapping.assert_called_once()\n        mp_split_samples.assert_called_once()\n        mp_resize_images.assert_not_called()\n        mp_save_files.assert_called_once()\n\n    def test_run_2(self, mocker):\n        mp_validate_images = mocker.patch(\'imageatm.components.dataprep.DataPrep._validate_images\')\n        mp_validate_samples = mocker.patch(\n            \'imageatm.components.dataprep.DataPrep._validate_samples\'\n        )\n        mp_create_class_mapping = mocker.patch(\n            \'imageatm.components.dataprep.DataPrep._create_class_mapping\'\n        )\n        mp_apply_class_mapping = mocker.patch(\n            \'imageatm.components.dataprep.DataPrep._apply_class_mapping\'\n        )\n        mp_split_samples = mocker.patch(\'imageatm.components.dataprep.DataPrep._split_samples\')\n        mp_resize_images = mocker.patch(\'imageatm.components.dataprep.DataPrep._resize_images\')\n        mp_save_files = mocker.patch(\'imageatm.components.dataprep.DataPrep._save_files\')\n\n        global dp\n        dp.run(resize=True)\n\n        mp_validate_images.assert_called_once()\n        mp_validate_samples.assert_called_once()\n        mp_create_class_mapping.assert_called_once()\n        mp_apply_class_mapping.assert_called_once()\n        mp_split_samples.assert_called_once()\n        mp_resize_images.assert_called_once()\n        mp_save_files.assert_called_once()\n\n    def test__save_files_1(self, mocker):\n        mp_save_json = mocker.patch(\'imageatm.components.dataprep.save_json\')\n\n        global dp\n        dp.job_dir = TEST_JOB_DIR\n\n        dp._save_files()\n\n        calls = [\n            call(dp.train_samples, dp.job_dir / \'train_samples.json\'),\n            call(dp.val_samples, dp.job_dir / \'val_samples.json\'),\n            call(dp.test_samples, dp.job_dir / \'test_samples.json\'),\n            call(dp.class_mapping, dp.job_dir / \'class_mapping.json\'),\n        ]\n\n        mp_save_json.assert_has_calls(calls)\n\n    def test__save_files_2(self, mocker):\n        mp_save_json = mocker.patch(\'imageatm.components.dataprep.save_json\')\n        mp_logger_info = mocker.patch(\'logging.Logger.info\')\n\n        global dp\n        dp.job_dir = TEST_JOB_DIR\n        dp.invalid_samples = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n\n        dp._save_files()\n\n        calls = [\n            call(dp.train_samples, dp.job_dir / \'train_samples.json\'),\n            call(dp.val_samples, dp.job_dir / \'val_samples.json\'),\n            call(dp.test_samples, dp.job_dir / \'test_samples.json\'),\n            call(dp.class_mapping, dp.job_dir / \'class_mapping.json\'),\n            call(dp.invalid_samples, dp.job_dir / \'invalid_samples.json\'),\n        ]\n\n        mp_save_json.assert_has_calls(calls)\n        mp_logger_info.assert_called_with(\n            \'NOTE: More than 10 samples were identified as invalid.\\n\'\n            \'The full list of invalid samples has been saved here:\\n{}\'.format(\n                dp.job_dir / \'invalid_samples.json\'\n            )\n        )\n\n    def test__resize_images(self, mocker):\n        mp_parallelise = mocker.patch(\'imageatm.components.dataprep.parallelise\')\n\n        global dp\n        dp.image_dir = TEST_IMG_DIR\n        dp.job_dir = TEST_JOB_DIR\n        dp.samples_file = TEST_STR_FILE\n        print(dp.image_dir)\n        new_image_dir = \'_\'.join([str(dp.image_dir), \'resized\'])\n        args = [(dp.image_dir, new_image_dir, i[\'image_id\']) for i in dp.samples]\n\n        dp._resize_images()\n\n        mp_parallelise.assert_called_with(resize_image_mp, args)\n        assert str(dp.image_dir) == new_image_dir\n'"
tests/components/test_evaluation.py,0,"b""import pytest\nimport shutil\nimport numpy.testing as npt\nfrom pathlib import Path\nfrom imageatm.components.evaluation import Evaluation\nfrom imageatm.handlers.image_classifier import ImageClassifier\n\n\nTEST_IMAGE_DIR = Path('./tests/data/test_images').resolve()\nTEST_JOB_DIR = Path('./tests/data/test_train_job').resolve()\nTEST_BATCH_SIZE = 16\nTEST_BASE_MODEL_NAME = 'MobileNet'\n\n\n@pytest.fixture(scope='class', autouse=True)\ndef tear_down(request):\n    def remove_evaluation_dir():\n        shutil.rmtree(TEST_JOB_DIR / 'evaluation_model_mobilenet_15_0.375')\n\n    def remove_logs():\n        (TEST_JOB_DIR / 'logs').unlink()\n\n    request.addfinalizer(remove_evaluation_dir)\n    request.addfinalizer(remove_logs)\n\n\nclass TestEvaluation(object):\n    eval = None\n\n    def test__init__(self, mocker):\n        mocker.patch('imageatm.components.evaluation.load_model', return_value={})\n        global eval\n        eval = Evaluation(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            batch_size=TEST_BATCH_SIZE,\n            base_model_name=TEST_BASE_MODEL_NAME,\n        )\n\n        eval.show_plots = True\n\n        assert eval.image_dir == TEST_IMAGE_DIR\n        assert eval.job_dir == TEST_JOB_DIR\n\n        assert eval.batch_size == 16\n        assert eval.base_model_name == 'MobileNet'\n\n        assert len(eval.samples_test) == 4\n        assert len(eval.class_mapping) == 2\n        assert eval.n_classes == 2\n        assert eval.classes == ['0', '1']\n        assert eval.y_true[0] == 0\n        assert eval.y_true[1] == 0\n        assert eval.y_true[2] == 0\n        assert eval.y_true[3] == 1\n\n    def test_run(self, mocker):\n        mp_make_pred = mocker.patch(\n            'imageatm.components.evaluation.Evaluation._make_prediction_on_test_set'\n        )\n        mp_plot_dist = mocker.patch(\n            'imageatm.components.evaluation.Evaluation._plot_test_set_distribution'\n        )\n        mp_calc_cr = mocker.patch(\n            'imageatm.components.evaluation.Evaluation._print_classification_report'\n        )\n        mp_plot_cm = mocker.patch(\n            'imageatm.components.evaluation.Evaluation._plot_confusion_matrix'\n        )\n        mp_plot_cw = mocker.patch(\n            'imageatm.components.evaluation.Evaluation._plot_correct_wrong_examples'\n        )\n\n        global eval\n        eval.mode_ipython = True\n        eval.run()\n\n        mp_make_pred.assert_called()\n        mp_plot_dist.assert_called()\n        mp_calc_cr.assert_called()\n        mp_plot_cm.assert_called()\n        mp_plot_cw.assert_called()\n\n    def test__load_best_model(self, mocker):\n        mp = mocker.patch('imageatm.components.evaluation.load_model')\n\n        global eval\n        eval._load_best_model()\n\n        mp.assert_called_with(TEST_JOB_DIR / 'models/model_mobilenet_15_0.375.hdf5')\n\n    def test__plot_test_set_distribution(self, mocker):\n        mock_plt_bar = mocker.patch('matplotlib.pyplot.bar')\n        mock_plt_xlabel = mocker.patch('matplotlib.pyplot.xlabel')\n        mock_plt_ylabel = mocker.patch('matplotlib.pyplot.ylabel')\n        mock_plt_xticks = mocker.patch('matplotlib.pyplot.xticks')\n        mock_plt_title = mocker.patch('matplotlib.pyplot.title')\n        mock_plt_show = mocker.patch('matplotlib.pyplot.show')\n\n        global eval\n        eval.mode_ipython = True\n        eval._plot_test_set_distribution()\n\n        mock_plt_bar.assert_called()\n        mock_plt_xlabel.assert_called_with('Label', fontsize=14)\n        mock_plt_ylabel.assert_called_with('Number of images', fontsize=14)\n        mock_plt_xticks.assert_called()\n        mock_plt_title.assert_called_with('Number of images in test set: 4', fontsize=18)\n        mock_plt_show.assert_called()\n\n    def test__plot_confusion_matrix(self, mocker):\n        mock_plt_tight_layout = mocker.patch('matplotlib.pyplot.tight_layout')\n        mock_plt_xlabel = mocker.patch('matplotlib.pyplot.xlabel')\n        mock_plt_ylabel = mocker.patch('matplotlib.pyplot.ylabel')\n        mock_plt_title = mocker.patch('matplotlib.pyplot.title')\n        mock_plt_figure = mocker.patch('matplotlib.pyplot.figure')\n        mock_plt_imshow = mocker.patch('matplotlib.pyplot.imshow')\n        mock_plt_colorbar = mocker.patch('matplotlib.pyplot.colorbar')\n        mock_plt_xticks = mocker.patch('matplotlib.pyplot.xticks')\n        mock_plt_yticks = mocker.patch('matplotlib.pyplot.yticks')\n        mock_plt_show = mocker.patch('matplotlib.pyplot.show')\n\n        global eval\n        eval.mode_ipython = True\n        eval.y_pred = [1, 0, 0, 0]\n        eval._plot_confusion_matrix()\n\n        mock_plt_figure.assert_called()\n        mock_plt_imshow.assert_called()\n        mock_plt_title.assert_called_with('Confusion matrix (recall)', fontsize=18)\n        mock_plt_colorbar.assert_called()\n        mock_plt_xticks.assert_called()\n        mock_plt_yticks.assert_called()\n        mock_plt_xlabel.assert_called_with('Predicted label', fontsize=14)\n        mock_plt_ylabel.assert_called_with('True label', fontsize=14)\n        mock_plt_tight_layout.assert_called()\n        mock_plt_show.assert_called()\n\n    def test_get_correct_wrong_examples(self):\n        def paramized_test_correct_wrong_function(y_pred, label, num_correct, num_wrong):\n            global eval\n            eval.y_pred = y_pred\n            eval.class_mapping = {0: '0', 1: '1'}\n            correct, wrong = eval.get_correct_wrong_examples(label)\n            assert len(correct) == num_correct\n            assert len(wrong) == num_wrong\n\n        paramized_test_correct_wrong_function([0, 0, 0, 1], 0, 3, 0)\n        paramized_test_correct_wrong_function([0, 0, 0, 1], 1, 1, 0)\n        paramized_test_correct_wrong_function([1, 0, 0, 0], 0, 2, 1)\n        paramized_test_correct_wrong_function([1, 0, 0, 0], 1, 0, 1)\n        paramized_test_correct_wrong_function([0, 0, 0, 1], '0', 3, 0)\n        paramized_test_correct_wrong_function([0, 0, 0, 1], '1', 1, 0)\n\n    def test__get_probabilities_prediction(self):\n        global eval\n        eval.y_pred_prob = [[0.4, 0.6], [0.2, 0.8], [0.7, 0.3]]\n        assert eval._get_probabilities_prediction(predictions_dist=eval.y_pred_prob) == [\n            0.6,\n            0.8,\n            0.7,\n        ]\n\n    def test__make_prediction_on_test_set(self, mocker):\n        mocker.patch(\n            'imageatm.handlers.image_classifier.ImageClassifier.__init__', return_value=None\n        )\n        mocker.patch(\n            'imageatm.handlers.data_generator.ValDataGenerator.__init__', return_value=None\n        )\n        mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.get_preprocess_input')\n        mocker.patch.object(\n            ImageClassifier, 'predict_generator', return_value=[[0.3, 0.85], [0.7, 0.2], [0.5, 0.3]]\n        )\n        mp_proba = mocker.patch(\n            'imageatm.components.evaluation.Evaluation._get_probabilities_prediction'\n        )\n\n        global eval\n        eval._make_prediction_on_test_set()\n\n        npt.assert_array_equal(eval.y_pred, [1, 0, 0])\n        mp_proba.assert_called_with(predictions_dist=[[0.3, 0.85], [0.7, 0.2], [0.5, 0.3]])\n\n    def test_visualize_images_empty_image_list(self, mocker):\n        mock_plt_title = mocker.patch('matplotlib.pyplot.title')\n        mock_plt_figure = mocker.patch('matplotlib.pyplot.figure')\n        mock_plt_imshow = mocker.patch('matplotlib.pyplot.imshow')\n        mock_plt_show = mocker.patch('matplotlib.pyplot.show')\n        mock_plt_subplot = mocker.patch('matplotlib.pyplot.subplot')\n        mock_plt_axis = mocker.patch('matplotlib.pyplot.axis')\n        mock_plt_savefig = mocker.patch('matplotlib.pyplot.savefig')\n\n        global eval\n        eval.mode_ipython = True\n\n        assert eval.visualize_images(image_list=[]) is None\n        mock_plt_title.assert_not_called()\n        mock_plt_figure.assert_not_called()\n        mock_plt_imshow.assert_not_called()\n        mock_plt_show.assert_not_called()\n        mock_plt_subplot.assert_not_called()\n        mock_plt_axis.assert_not_called()\n        mock_plt_savefig.assert_not_called()\n\n    def test_visualize_images_1(self, mocker):\n        mock_plt_title = mocker.patch('matplotlib.pyplot.title')\n        mock_plt_figure = mocker.patch('matplotlib.pyplot.figure')\n        mock_plt_imshow = mocker.patch('matplotlib.pyplot.imshow')\n        mock_plt_subplot = mocker.patch('matplotlib.pyplot.subplot')\n        mock_plt_axis = mocker.patch('matplotlib.pyplot.axis')\n        mock_plt_savefig = mocker.patch('matplotlib.pyplot.savefig')\n        mock_plt_show = mocker.patch('matplotlib.pyplot.show')\n\n        global eval\n        eval.mode_ipython = True\n        eval.class_mapping = {'0': 0, '1': 1}\n        eval.y_true = [0, 0, 0, 1]\n        eval.y_pred = [1, 0, 0, 0]\n        eval.y_pred_prob = [0.9, 0.8, 0.7, 0.8]\n        correct, wrong = eval.get_correct_wrong_examples(0)\n\n        eval.visualize_images(correct, show_heatmap=False)\n        mock_plt_title.call_count == 3\n        mock_plt_figure.call_count == 3\n        mock_plt_imshow.call_count == 3\n        mock_plt_subplot.call_count == 3\n        mock_plt_axis.call_count == 3\n        mock_plt_savefig.assert_not_called()\n        mock_plt_show.assert_called_once()\n\n    def test_visualize_images_2(self, mocker):\n        mock_plt_title = mocker.patch('matplotlib.pyplot.title')\n        mock_plt_figure = mocker.patch('matplotlib.pyplot.figure')\n        mock_plt_imshow = mocker.patch('matplotlib.pyplot.imshow')\n        mock_plt_subplot = mocker.patch('matplotlib.pyplot.subplot')\n        mock_plt_axis = mocker.patch('matplotlib.pyplot.axis')\n        mock_plt_savefig = mocker.patch('matplotlib.pyplot.savefig')\n        mock_plt_show = mocker.patch('matplotlib.pyplot.show')\n\n        global eval\n        eval.mode_ipython = True\n        correct, wrong = eval.get_correct_wrong_examples(0)\n\n        eval.visualize_images(correct, show_heatmap=False)\n        mock_plt_title.call_count == 3\n        mock_plt_figure.call_count == 3\n        mock_plt_imshow.call_count == 3\n        mock_plt_subplot.call_count == 3\n        mock_plt_axis.call_count == 3\n        mock_plt_savefig.assert_not_called()\n        mock_plt_show.assert_called_once()\n\n    def test_visualize_images_3(self, mocker):\n        mock_plt_title = mocker.patch('matplotlib.pyplot.title')\n        mock_plt_figure = mocker.patch('matplotlib.pyplot.figure')\n        mock_plt_imshow = mocker.patch('matplotlib.pyplot.imshow')\n        mock_plt_subplot = mocker.patch('matplotlib.pyplot.subplot')\n        mock_plt_axis = mocker.patch('matplotlib.pyplot.axis')\n        mock_plt_savefig = mocker.patch('matplotlib.pyplot.savefig')\n        mock_plt_show = mocker.patch('matplotlib.pyplot.show')\n        mock_vc = mocker.patch('imageatm.components.evaluation.visualize_cam')\n        mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.get_preprocess_input')\n\n        global eval\n        eval.mode_ipython = True\n        correct, wrong = eval.get_correct_wrong_examples(0)\n\n        eval.visualize_images(correct, show_heatmap=True)\n        mock_vc.call_count == 3\n        mock_plt_title.call_count == 3\n        mock_plt_figure.call_count == 3\n        mock_plt_imshow.call_count == 9\n        mock_plt_subplot.call_count == 3\n        mock_plt_axis.call_count == 6\n        mock_plt_savefig.assert_not_called()\n        mock_plt_show.assert_called_once()\n"""
tests/components/test_training.py,0,"b""import pytest\nimport shutil\nfrom pathlib import Path\nfrom imageatm.components.training import Training\nfrom imageatm.handlers.data_generator import TrainDataGenerator, ValDataGenerator\nfrom imageatm.handlers.image_classifier import ImageClassifier\n\np = Path(__file__)\n\nTEST_IMAGE_DIR = p.resolve().parent / '../data/test_images'\nTEST_JOB_DIR = p.resolve().parent / '../data/test_train_job'\n\n\n@pytest.fixture(autouse=True)\ndef common_patches(mocker):\n    mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.__init__')\n    mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.build')\n    mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.compile')\n    mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.fit_generator')\n    mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.summary')\n    mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.get_preprocess_input')\n    mocker.patch('imageatm.handlers.image_classifier.ImageClassifier.get_base_layers')\n    ImageClassifier.__init__.return_value = None\n\n    mocker.patch('imageatm.handlers.data_generator.TrainDataGenerator.__init__')\n    mocker.patch('imageatm.handlers.data_generator.ValDataGenerator.__init__')\n    TrainDataGenerator.__init__.return_value = None\n    ValDataGenerator.__init__.return_value = None\n\n@pytest.fixture(scope='class', autouse=True)\ndef tear_down(request):\n    def remove_logs():\n        (TEST_JOB_DIR / 'logs').unlink()\n\n    request.addfinalizer(remove_logs)\n\nclass TestTraining(object):\n    train = None\n\n    def test__init(self):\n        global train\n        train = Training(image_dir=TEST_IMAGE_DIR, job_dir=TEST_JOB_DIR)\n\n        assert train.n_classes == 2\n        assert train.epochs_train_dense == 100\n\n    def test__set_patience(self):\n        global train\n\n        n_per_class = int(len(train.samples_train) / train.n_classes)\n\n        assert n_per_class == 2\n\n        train._set_patience()\n\n        assert train.patience_learning_rate == 5\n        assert train.patience_early_stopping == 15\n\n        train_400 = train\n        train_400.samples_train = train_400.samples_train * 100\n\n        train_400._set_patience()\n\n        assert train_400.patience_learning_rate == 4\n        assert train_400.patience_early_stopping == 12\n\n        train_2000 = train\n        train_2000.samples_train = train_2000.samples_train * 1000\n\n        train_2000._set_patience()\n\n        assert train_2000.patience_learning_rate == 2\n        assert train_2000.patience_early_stopping == 6\n\n    def test__build_model(self):\n        global train\n        train._build_model()\n\n        ImageClassifier.__init__.assert_called_once_with(\n            'MobileNet', 2, 0.001, 0.75, 'categorical_crossentropy'\n        )\n        ImageClassifier.build.assert_called_once_with()\n\n    def test__fit_model(self):\n        global train\n        train._fit_model()\n\n        TrainDataGenerator.__init__.assert_called_once()\n        ValDataGenerator.__init__.assert_called_once()\n        ImageClassifier.get_preprocess_input.assert_called_with()\n\n        # TODO: this tests are only rudimentary\n        ImageClassifier.compile.assert_called()\n        ImageClassifier.fit_generator.assert_called()\n        ImageClassifier.get_base_layers.assert_called()\n"""
tests/handlers/test_data_generator.py,0,"b""import pytest\nimport numpy as np\nfrom pathlib import Path\nfrom imageatm.handlers.data_generator import TrainDataGenerator, ValDataGenerator\n\nTEST_CONFIG = {'batch_size': 3, 'n_classes': 2}\n\nTEST_SAMPLES = [\n    {'image_id': 'helmet_1.jpg', 'label': 0},\n    {'image_id': 'helmet_2.jpg', 'label': 0},\n    {'image_id': 'helmet_3.jpg', 'label': 0},\n    {'image_id': 'helmet_4.jpg', 'label': 1},\n    {'image_id': 'helmet_5.jpg', 'label': 1},\n]\n\n\n@pytest.fixture(autouse=True)\ndef common_patches(mocker):\n    mocker.patch.object(TrainDataGenerator, '_data_generator')\n    TrainDataGenerator._data_generator.return_value = 'X', 'y'\n\n    mocker.patch.object(ValDataGenerator, '_data_generator')\n    ValDataGenerator._data_generator.return_value = 'X', 'y'\n    # Setting seed for np.random.shuffle in Train-mode\n    np.random.seed(10247)\n\n\nclass TestTrainDataGenerator(object):\n    generator = None\n\n    def test__init(self):\n        global generator\n        generator = TrainDataGenerator(\n            TEST_SAMPLES,\n            'image_dir',\n            TEST_CONFIG['batch_size'],\n            TEST_CONFIG['n_classes'],\n            'preprocess_input',\n        )\n\n        assert generator.samples == TEST_SAMPLES\n        assert generator.image_dir == Path('image_dir')\n        assert generator.batch_size == 3\n        assert generator.n_classes == 2\n        assert generator.basenet_preprocess == 'preprocess_input'\n        assert generator.img_load_dims == (256, 256)\n        assert generator.img_crop_dims == (224, 224)\n        assert generator.train is True\n\n    def test__len(self):\n        global generator\n        x = generator.__len__()\n\n        assert x == 2\n\n    def test__get_item(self):\n        global generator\n        generator.__getitem__(1)\n\n        generator._data_generator.assert_called_with(\n            [{'image_id': 'helmet_2.jpg', 'label': 0}, {'image_id': 'helmet_5.jpg', 'label': 1}]\n        )\n\n\nclass TestValDataGenerator(object):\n    generator = None\n\n    def test__init(self):\n        global generator\n        generator = ValDataGenerator(\n            TEST_SAMPLES,\n            'image_dir',\n            TEST_CONFIG['batch_size'],\n            TEST_CONFIG['n_classes'],\n            'preprocess_input',\n        )\n\n        assert generator.samples == TEST_SAMPLES\n        assert generator.image_dir == Path('image_dir')\n        assert generator.batch_size == 3\n        assert generator.n_classes == 2\n        assert generator.basenet_preprocess == 'preprocess_input'\n        assert generator.img_load_dims == (224, 224)\n        assert generator.train is False\n        assert not hasattr(generator, 'img_crop_dims')\n\n    def test__len(self):\n        global generator\n        assert generator.__len__() == 2\n\n    def test__get_item(self):\n        global generator\n        generator.__getitem__(1)\n\n        generator._data_generator.assert_called_with(\n            [{'image_id': 'helmet_4.jpg', 'label': 1}, {'image_id': 'helmet_5.jpg', 'label': 1}]\n        )\n"""
tests/handlers/test_image_classifier.py,0,"b""import pytest\nimport importlib\nfrom keras.models import Model\nfrom imageatm.handlers.image_classifier import ImageClassifier\n\nTEST_CONFIG = {\n    'base_model_name': 'MobileNet',\n    'batch_size': 16,\n    'decay_all': 0,\n    'decay_dense': 0,\n    'dropout_rate': 0.75,\n    'epochs_train_all': 5,\n    'epochs_train_dense': 5,\n    'learning_rate_all': 0.0000003,\n    'learning_rate_dense': 0.001,\n    'multiprocessing_data_load': True,\n    'num_workers_data_load': 8,\n    'n_classes': 10,\n    'loss': 'categorical_crossentropy',\n}\n\n\n@pytest.fixture(autouse=True)\ndef common_patches(mocker):\n    mocker.patch('keras.models.Model.compile')\n    mocker.patch('keras.models.Model.fit_generator')\n    mocker.patch('keras.models.Model.summary')\n\n\nclass TestImageClassifier(object):\n    classifier = None\n\n    def test__init(self):\n        global classifier\n        classifier = ImageClassifier(\n            TEST_CONFIG['base_model_name'],\n            TEST_CONFIG['n_classes'],\n            TEST_CONFIG['learning_rate_dense'],\n            TEST_CONFIG['dropout_rate'],\n            TEST_CONFIG['loss'],\n        )\n\n        assert classifier.weights == 'imagenet'\n        assert classifier.base_module is importlib.import_module('keras.applications.mobilenet')\n\n    def test__build(self):\n        global classifier\n        classifier.build()\n\n        assert isinstance(classifier.base_model, Model)\n        assert isinstance(classifier.model, Model)\n\n    def test__get_preprocess_input(self):\n        global classifier\n        classifier.get_preprocess_input()\n        pass\n\n    def test__get_base_layers(self):\n        global classifier\n        classifier.get_base_layers()\n        pass\n\n    def test__set_learning_rate(self):\n        global classifier\n        classifier.set_learning_rate(5)\n\n        assert classifier.learning_rate == 5\n\n    def test__compile(self):\n        global classifier\n        classifier.compile()\n\n        Model.compile.assert_called()\n\n    def test__summary(self):\n        global classifier\n        classifier.summary()\n\n        Model.summary.assert_called()\n\n    def test__fit_generator(self):\n        global classifier\n        classifier.fit_generator(\n            generator='training_generator',\n            validation_data='validation_generator',\n            epochs=TEST_CONFIG['epochs_train_dense'],\n            verbose=1,\n            use_multiprocessing=TEST_CONFIG['multiprocessing_data_load'],\n            workers=TEST_CONFIG['num_workers_data_load'],\n            max_queue_size=30,\n            callbacks=[],\n        )\n\n        Model.fit_generator.assert_called_once_with(\n            callbacks=[],\n            epochs=5,\n            generator='training_generator',\n            max_queue_size=30,\n            use_multiprocessing=True,\n            validation_data='validation_generator',\n            verbose=1,\n            workers=8,\n        )\n"""
tests/scripts/test_run_cloud.py,0,"b""import pytest\nimport shutil\nfrom pathlib import Path\nfrom imageatm.scripts import run_cloud\nfrom imageatm.components.cloud import AWS\n\n\nTEST_JOB_DIR = Path('./tests/data/test_train_job').resolve()\nTEST_TF_DIR = 'test_tf_dir'\nTEST_REGION = 'test_region'\nTEST_INSTANCE_TYPE = 'test_instance_type'\nTEST_VPC_ID = 'test_vpc_id'\nTEST_S3_BUCKET = 'test_s3_bucket'\nTEST_CLOUD_TAG = 'test_cloud_tag'\n\n\nclass TestRunEvaluation(object):\n    def test_run_cloud_1(self, mocker):\n        mp_init = mocker.patch('imageatm.components.cloud.AWS.init')\n        mp_apply = mocker.patch('imageatm.components.cloud.AWS.apply')\n        mp_destroy = mocker.patch('imageatm.components.cloud.AWS.destroy')\n        mocker.patch('imageatm.components.cloud.AWS.__init__')\n        AWS.__init__.return_value = None\n\n        run_cloud(\n            provider='aws',\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            bucket=TEST_S3_BUCKET,\n            destroy=False,\n            job_dir=TEST_JOB_DIR,\n            cloud_tag=TEST_CLOUD_TAG,\n            image_dir='test',\n        )\n        mp_init.assert_called_once()\n        mp_apply.assert_called_once()\n        mp_destroy.assert_not_called()\n\n        AWS.__init__.assert_called_with(\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            s3_bucket=TEST_S3_BUCKET,\n            job_dir=TEST_JOB_DIR,\n            cloud_tag=TEST_CLOUD_TAG,\n        )\n\n    def test_run_cloud_2(self, mocker):\n        mp_init = mocker.patch('imageatm.components.cloud.AWS.init')\n        mp_apply = mocker.patch('imageatm.components.cloud.AWS.apply')\n        mp_train = mocker.patch('imageatm.components.cloud.AWS.train')\n        mp_destroy = mocker.patch('imageatm.components.cloud.AWS.destroy')\n        mocker.patch('imageatm.components.cloud.AWS.__init__')\n        AWS.__init__.return_value = None\n\n        run_cloud(\n            provider='aws',\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            bucket=TEST_S3_BUCKET,\n            destroy=True,\n            job_dir=TEST_JOB_DIR,\n            cloud_tag=TEST_CLOUD_TAG,\n        )\n        mp_init.assert_not_called()\n        mp_apply.assert_not_called()\n        mp_destroy.assert_called_once()\n\n        AWS.__init__.assert_called_with(\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            s3_bucket=TEST_S3_BUCKET,\n            job_dir=TEST_JOB_DIR,\n            cloud_tag=TEST_CLOUD_TAG,\n        )\n"""
tests/scripts/test_run_data_prep.py,0,"b""from pathlib import Path\nfrom imageatm.scripts import run_dataprep\nfrom imageatm.components.dataprep import DataPrep\n\nTEST_IMAGE_DIR = Path('./tests/data/test_images').resolve()\nTEST_JOB_DIR = Path('./tests/data/test_train_job').resolve()\nTEST_STR_FILE = Path('./tests/data/test_samples/test_str_labels.json').resolve()\nTEST_BATCH_SIZE = 16\nTEST_BASE_MODEL_NAME = 'MobileNet'\n\n\nclass TestRunDataPrep(object):\n    def test_run_dataprep(self, mocker):\n        mp_run = mocker.patch('imageatm.components.dataprep.DataPrep.run')\n        mocker.patch('imageatm.components.dataprep.DataPrep.__init__')\n        DataPrep.__init__.return_value = None\n\n        run_dataprep(\n            image_dir=TEST_IMAGE_DIR, job_dir=TEST_JOB_DIR, samples_file=TEST_STR_FILE, resize=True\n        )\n        mp_run.assert_called_with(resize=True)\n        DataPrep.__init__.assert_called_with(\n            image_dir=TEST_IMAGE_DIR, job_dir=TEST_JOB_DIR, samples_file=TEST_STR_FILE\n        )\n\n        run_dataprep(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            samples_file=TEST_STR_FILE,\n            resize=True,\n            min_class_size=1,\n            test_size=1,\n            val_size=1,\n            part_size=1,\n        )\n        DataPrep.__init__.assert_called_with(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            samples_file=TEST_STR_FILE,\n            min_class_size=1,\n            test_size=1,\n            val_size=1,\n            part_size=1,\n        )\n\n        run_dataprep(image_dir=TEST_IMAGE_DIR, job_dir=TEST_JOB_DIR, samples_file=TEST_STR_FILE)\n        mp_run.assert_called_with(resize=False)\n"""
tests/scripts/test_run_evaluation.py,0,"b'import pytest\nimport shutil\nfrom pathlib import Path\nfrom imageatm.scripts import run_evaluation\nfrom imageatm.components.evaluation import Evaluation\n\n\nTEST_IMAGE_DIR = Path(\'./tests/data/test_images\').resolve()\nTEST_JOB_DIR = Path(\'./tests/data/test_train_job\').resolve()\nTEST_BATCH_SIZE = 16\nTEST_BASE_MODEL_NAME = \'MobileNet\'\nTEST_CONFIG_EVALUATION_REPORT = {\n    ""create"": True,\n    ""kernel_name"": ""any_kernel"",\n    ""export_html"": True,\n    ""export_pdf"": True,\n}\n\n\nclass TestRunEvaluation(object):\n    def test_run_evaluation(self, mocker):\n        mocker.patch(\'imageatm.components.evaluation.load_model\', return_value={})\n        mp_run = mocker.patch(\'imageatm.components.evaluation.Evaluation.run\')\n        mocker.patch(\'imageatm.components.evaluation.Evaluation.__init__\')\n        Evaluation.__init__.return_value = None\n\n        run_evaluation(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            report=TEST_CONFIG_EVALUATION_REPORT,\n            batch_size=TEST_BATCH_SIZE,\n            base_model_name=TEST_BASE_MODEL_NAME,\n        )\n        mp_run.assert_called_once()\n        Evaluation.__init__.assert_called_with(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            batch_size=TEST_BATCH_SIZE,\n            base_model_name=TEST_BASE_MODEL_NAME,\n        )\n\n        run_evaluation(image_dir=TEST_IMAGE_DIR, job_dir=TEST_JOB_DIR, report=TEST_CONFIG_EVALUATION_REPORT)\n        Evaluation.__init__.assert_called_with(image_dir=TEST_IMAGE_DIR, job_dir=TEST_JOB_DIR)\n'"
tests/scripts/test_run_training.py,0,"b""import pytest\nimport shutil\nfrom pathlib import Path\nfrom imageatm.scripts import run_training\nfrom imageatm.components.training import Training\n\n\nTEST_IMAGE_DIR = Path('./tests/data/test_images').resolve()\nTEST_JOB_DIR = Path('./tests/data/test_train_job').resolve()\nTEST_BATCH_SIZE = 64\nTEST_DROPOUT_RATE = 0.75\nTEST_BASE_MODEL_NAME = 'MobileNet'\nTEST_LOSS = 'test_loss'\nTEST_EPOCHS_TRAIN_DENSE = 2\nTEST_EPOCHS_TRAIN_ALL = 10\nTEST_LEARNING_RATE_DENSE = 0.001\nTEST_LEARNING_RATE_ALL = 0.0001\n\n\nclass TestRunEvaluation(object):\n    def test_run_training_1(self, mocker):\n        mp_run = mocker.patch('imageatm.components.training.Training.run')\n        mocker.patch('imageatm.components.training.Training.__init__')\n        Training.__init__.return_value = None\n\n        run_training(image_dir=TEST_IMAGE_DIR, job_dir=TEST_JOB_DIR)\n        mp_run.assert_called_once()\n        Training.__init__.assert_called_with(image_dir=TEST_IMAGE_DIR, job_dir=TEST_JOB_DIR)\n\n    def test_run_training_2(self, mocker):\n        mp_run = mocker.patch('imageatm.components.training.Training.run')\n        mocker.patch('imageatm.components.training.Training.__init__')\n        Training.__init__.return_value = None\n        run_training(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            epochs_train_dense=TEST_EPOCHS_TRAIN_DENSE,\n            epochs_train_all=TEST_EPOCHS_TRAIN_ALL,\n            learning_rate_dense=TEST_LEARNING_RATE_DENSE,\n            learning_rate_all=TEST_LEARNING_RATE_ALL,\n            batch_size=TEST_BATCH_SIZE,\n            dropout_rate=TEST_DROPOUT_RATE,\n            base_model_name=TEST_BASE_MODEL_NAME,\n            loss=TEST_LOSS,\n        )\n        mp_run.assert_called_once()\n        Training.__init__.assert_called_with(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            epochs_train_dense=TEST_EPOCHS_TRAIN_DENSE,\n            epochs_train_all=TEST_EPOCHS_TRAIN_ALL,\n            learning_rate_dense=TEST_LEARNING_RATE_DENSE,\n            learning_rate_all=TEST_LEARNING_RATE_ALL,\n            batch_size=TEST_BATCH_SIZE,\n            dropout_rate=TEST_DROPOUT_RATE,\n            base_model_name=TEST_BASE_MODEL_NAME,\n            loss=TEST_LOSS,\n        )\n"""
tests/scripts/test_run_training_cloud.py,0,"b""import pytest\nimport shutil\nfrom pathlib import Path\nfrom imageatm.scripts import run_training_cloud\nfrom imageatm.components.cloud import AWS\n\n\nTEST_JOB_DIR = Path('./tests/data/test_train_job').resolve()\nTEST_TF_DIR = 'test_tf_dir'\nTEST_REGION = 'test_region'\nTEST_INSTANCE_TYPE = 'test_instance_type'\nTEST_VPC_ID = 'test_vpc_id'\nTEST_S3_BUCKET = 'test_s3_bucket'\nTEST_CLOUD_TAG = 'test_cloud_tag'\n\nTEST_IMAGE_DIR = Path('./tests/data/test_images').resolve()\nTEST_BATCH_SIZE = 64\nTEST_DROPOUT_RATE = 0.75\nTEST_BASE_MODEL_NAME = 'MobileNet'\nTEST_LOSS = 'test_loss'\nTEST_EPOCHS_TRAIN_DENSE = 2\nTEST_EPOCHS_TRAIN_ALL = 10\nTEST_LEARNING_RATE_DENSE = 0.001\nTEST_LEARNING_RATE_ALL = 0.0001\n\n\nclass TestRunEvaluation(object):\n    def test_run_training_cloud_1(self, mocker):\n        mp_init = mocker.patch('imageatm.components.cloud.AWS.init')\n        mp_apply = mocker.patch('imageatm.components.cloud.AWS.apply')\n        mp_train = mocker.patch('imageatm.components.cloud.AWS.train')\n        mp_destroy = mocker.patch('imageatm.components.cloud.AWS.destroy')\n        mocker.patch('imageatm.components.cloud.AWS.__init__')\n        AWS.__init__.return_value = None\n\n        run_training_cloud(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            provider='aws',\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            bucket=TEST_S3_BUCKET,\n            destroy=False,\n            cloud_tag=TEST_CLOUD_TAG,\n        )\n        mp_init.assert_called_once()\n        mp_apply.assert_called_once()\n        mp_train.assert_called_with(job_dir=TEST_JOB_DIR, image_dir=TEST_IMAGE_DIR)\n        mp_destroy.assert_not_called()\n\n        AWS.__init__.assert_called_with(\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            s3_bucket=TEST_S3_BUCKET,\n            job_dir=TEST_JOB_DIR,\n            cloud_tag=TEST_CLOUD_TAG,\n        )\n\n    def test_run_training_cloud_2(self, mocker):\n        mp_init = mocker.patch('imageatm.components.cloud.AWS.init')\n        mp_apply = mocker.patch('imageatm.components.cloud.AWS.apply')\n        mp_train = mocker.patch('imageatm.components.cloud.AWS.train')\n        mp_destroy = mocker.patch('imageatm.components.cloud.AWS.destroy')\n        mocker.patch('imageatm.components.cloud.AWS.__init__')\n        AWS.__init__.return_value = None\n\n        run_training_cloud(\n            image_dir=TEST_IMAGE_DIR,\n            job_dir=TEST_JOB_DIR,\n            provider='aws',\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            bucket=TEST_S3_BUCKET,\n            destroy=True,\n            cloud_tag=TEST_CLOUD_TAG,\n            epochs_train_dense=TEST_EPOCHS_TRAIN_DENSE,\n            epochs_train_all=TEST_EPOCHS_TRAIN_ALL,\n            learning_rate_dense=TEST_LEARNING_RATE_DENSE,\n            learning_rate_all=TEST_LEARNING_RATE_ALL,\n            batch_size=TEST_BATCH_SIZE,\n            dropout_rate=TEST_DROPOUT_RATE,\n            base_model_name=TEST_BASE_MODEL_NAME,\n            loss=TEST_LOSS,\n        )\n        mp_init.assert_called_once()\n        mp_apply.assert_called_once()\n        mp_train.assert_called_with(\n            job_dir=TEST_JOB_DIR,\n            image_dir=TEST_IMAGE_DIR,\n            epochs_train_dense=TEST_EPOCHS_TRAIN_DENSE,\n            epochs_train_all=TEST_EPOCHS_TRAIN_ALL,\n            learning_rate_dense=TEST_LEARNING_RATE_DENSE,\n            learning_rate_all=TEST_LEARNING_RATE_ALL,\n            batch_size=TEST_BATCH_SIZE,\n            dropout_rate=TEST_DROPOUT_RATE,\n            base_model_name=TEST_BASE_MODEL_NAME,\n            loss=TEST_LOSS,\n        )\n        mp_destroy.assert_called_once()\n\n        AWS.__init__.assert_called_with(\n            tf_dir=TEST_TF_DIR,\n            region=TEST_REGION,\n            instance_type=TEST_INSTANCE_TYPE,\n            vpc_id=TEST_VPC_ID,\n            s3_bucket=TEST_S3_BUCKET,\n            job_dir=TEST_JOB_DIR,\n            cloud_tag=TEST_CLOUD_TAG,\n        )\n"""
tests/utils/test_images.py,0,"b""from pathlib import Path\nfrom imageatm.utils import images as im\n\np = Path(__file__)\n\nNON_EXISTENT_FILE_PATH = p.resolve().parent / '../data/test_images' / 'foo_bar.jpg'\n\nINVALID_IMG_PATH = p.resolve().parent / '../data/test_images' / 'image_invalid.jpg'\n\nTRUNCATED_IMG_PATH = p.resolve().parent / '../data/test_images' / 'truncated.jpg'\n\nJPG_IMG_PATH = p.resolve().parent / '../data/test_images' / 'image_960x540.jpg'\n\nPNG_IMG_PATH = p.resolve().parent / '../data/test_images' / 'image_png.png'\n\nBMP_IMG_PATH = p.resolve().parent / '../data/test_images' / 'image_bmp.bmp'\n\n\ndef test_validate_image():\n    valid, error = im.validate_image(NON_EXISTENT_FILE_PATH)\n    assert valid is False\n    assert isinstance(error, FileNotFoundError)\n\n    valid, error = im.validate_image(INVALID_IMG_PATH)\n    assert valid is False\n    assert isinstance(error, OSError)\n\n    valid, error = im.validate_image(TRUNCATED_IMG_PATH)\n    assert valid is False\n    assert isinstance(error, OSError)\n\n    valid, error = im.validate_image(JPG_IMG_PATH)\n    assert valid is True\n    assert error is None\n\n    valid, error = im.validate_image(PNG_IMG_PATH)\n    assert valid is True\n    assert error is None\n\n    valid, error = im.validate_image(BMP_IMG_PATH)\n    assert valid is False\n    assert error is None\n\n\ndef test_resize_image():\n    img_960x540 = im.load_image(JPG_IMG_PATH)\n\n    img_pp = im.resize_image(img_960x540, max_size=300, upscale=False)\n    w, h = img_pp.size\n    assert w == 533\n    assert h == 300\n\n    img_pp = im.resize_image(img_960x540, max_size=300, upscale=True)\n    w, h = img_pp.size\n    assert w == 533\n    assert h == 300\n\n    img_pp = im.resize_image(img_960x540, max_size=600, upscale=False)\n    w, h = img_pp.size\n    assert w == 960\n    assert h == 540\n\n    img_pp = im.resize_image(img_960x540, max_size=600, upscale=True)\n    w, h = img_pp.size\n    assert w == 1066\n    assert h == 600\n"""
tests/utils/test_io.py,0,"b""import logging\nfrom pathlib import Path\nfrom imageatm.utils.io import save_json, load_json\n\n\nTEST_TARGET_FILE = Path('./tests/data/test_samples/test_target_file.json').resolve()\n\n\nclass TestIo(object):\n    def test_save_json(self):\n        data = [\n            {'image_id': 'helmet_1.jpg', 'label': 'left'},\n            {'image_id': 'helmet_2.jpg', 'label': 'left'},\n            {'image_id': 'image_png.png', 'label': 'right'},\n        ]\n\n        target_file = TEST_TARGET_FILE\n        assert target_file.exists() is False\n        save_json(data, target_file)\n        assert target_file.exists()\n        assert load_json(target_file) == data\n        target_file.unlink()\n"""
tests/utils/test_process.py,0,"b""import logging\nimport pytest\nfrom imageatm.utils.process import run_cmd\n\n\nclass TestProcess(object):\n    def test_run_cmd_1(self, mocker):\n        mp_debug = mocker.patch('logging.Logger.debug')\n        mp_info = mocker.patch('logging.Logger.info')\n\n        cmd = 'echo Hello world'\n        logger = logging.Logger(__name__)\n        level = 'debug'\n        return_output = False\n\n        assert run_cmd(cmd, logger, level, return_output) == None\n\n        mp_debug.assert_called_once()\n        mp_info.assert_not_called()\n\n    def test_run_cmd_2(self, mocker):\n        mp_debug = mocker.patch('logging.Logger.debug')\n        mp_info = mocker.patch('logging.Logger.info')\n\n        cmd = 'echo Hello world'\n        logger = logging.Logger(__name__)\n        level = 'info'\n        return_output = False\n\n        run_cmd(cmd, logger, level, return_output)\n\n        mp_debug.assert_not_called()\n        mp_info.assert_called_once()\n\n    def test_run_cmd_3(self, mocker):\n        mp_debug = mocker.patch('logging.Logger.debug')\n        mp_info = mocker.patch('logging.Logger.info')\n\n        cmd = 'echo Hello world'\n        logger = logging.Logger(__name__)\n        level = 'debug'\n        return_output = True\n\n        assert run_cmd(cmd, logger, level, return_output) == 'Hello world'\n        mp_debug.assert_called_once()\n        mp_info.assert_not_called()\n\n    def test_run_cmd_4(self, mocker):\n        mp_debug = mocker.patch('logging.Logger.debug')\n        mp_info = mocker.patch('logging.Logger.info')\n        mp_error = mocker.patch('logging.Logger.error')\n\n        cmd = 'echo2 Hello world'\n        logger = logging.Logger(__name__)\n        level = 'debug'\n        return_output = False\n\n        with pytest.raises(Exception) as excinfo:\n            run_cmd(cmd, logger, level, return_output)\n        mp_debug.assert_not_called()\n        mp_info.assert_not_called()\n        mp_error.assert_called_once()\n"""
tests/utils/test_tf_keras.py,0,"b""import logging\nimport mock\nimport pytest\nimport shutil\nimport numpy as np\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.utils.test_utils import get_test_data\nfrom pathlib import Path\nfrom imageatm.utils.tf_keras import use_multiprocessing, LoggingModels\n\ninput_dim = 2\nnum_hidden = 4\nnum_classes = 2\nbatch_size = 5\ntrain_samples = 20\ntest_samples = 20\n\nTEST_DIR = Path('tests/data/test_callbacks/').resolve()\n\nTEST_DIR.mkdir(parents=True, exist_ok=True)\n\n\n@pytest.fixture(scope='class', autouse=True)\ndef tear_down(request):\n    def remove_job_dir():\n        shutil.rmtree(TEST_DIR)\n\n    request.addfinalizer(remove_job_dir)\n\n\nclass TestTfKeras(object):\n    @mock.patch('imageatm.utils.tf_keras._get_available_gpus', return_value=[])\n    def test__use_multiprocessing_false(self, mock):\n        use_multi, num_worker = use_multiprocessing()\n\n        assert use_multi == False\n        assert num_worker == 1\n\n    @mock.patch('imageatm.utils.tf_keras.cpu_count', return_value=4711)\n    @mock.patch('imageatm.utils.tf_keras._get_available_gpus', return_value=['foo', 'bar'])\n    def test__use_multiprocessing_true(self, mock1, mock2):\n        use_multi, num_worker = use_multiprocessing()\n\n        assert use_multi == True\n        assert num_worker == 4711\n\n    def test_LoggingModels(self, mocker):\n        mp_logger_warning = mocker.patch('logging.Logger.warning')\n\n        def get_data_callbacks(\n            num_train=train_samples,\n            num_test=test_samples,\n            input_shape=(input_dim,),\n            classification=True,\n            num_classes=num_classes,\n        ):\n            return get_test_data(\n                num_train=num_train,\n                num_test=num_test,\n                input_shape=input_shape,\n                classification=classification,\n                num_classes=num_classes,\n            )\n\n        tmpdir = TEST_DIR\n        np.random.seed(1337)\n        filepath = tmpdir / 'checkpoint.h5'\n        (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n        y_test = np_utils.to_categorical(y_test)\n        y_train = np_utils.to_categorical(y_train)\n        # case 1\n        monitor = 'val_loss'\n        save_best_only = False\n        mode = 'auto'\n\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n        cbks = [\n            LoggingModels(\n                filepath,\n                logger=logging.Logger(__name__),\n                monitor=monitor,\n                save_best_only=save_best_only,\n                mode=mode,\n            )\n        ]\n        model.fit(\n            X_train,\n            y_train,\n            batch_size=batch_size,\n            validation_data=(X_test, y_test),\n            callbacks=cbks,\n            epochs=1,\n        )\n        assert filepath.is_file()\n        filepath.unlink()\n\n        # case 2\n        mode = 'min'\n        cbks = [\n            LoggingModels(\n                filepath,\n                logger=logging.Logger(__name__),\n                monitor=monitor,\n                save_best_only=save_best_only,\n                mode=mode,\n            )\n        ]\n        model.fit(\n            X_train,\n            y_train,\n            batch_size=batch_size,\n            validation_data=(X_test, y_test),\n            callbacks=cbks,\n            epochs=1,\n        )\n        assert filepath.is_file()\n        filepath.unlink()\n\n        # case 3\n        mode = 'max'\n        monitor = 'val_accuracy'\n        cbks = [\n            LoggingModels(\n                filepath,\n                logger=logging.Logger(__name__),\n                monitor=monitor,\n                save_best_only=save_best_only,\n                mode=mode,\n            )\n        ]\n        model.fit(\n            X_train,\n            y_train,\n            batch_size=batch_size,\n            validation_data=(X_test, y_test),\n            callbacks=cbks,\n            epochs=1,\n        )\n        assert filepath.is_file()\n        filepath.unlink()\n\n        # case 4\n        save_best_only = True\n        cbks = [\n            LoggingModels(\n                filepath,\n                logger=logging.Logger(__name__),\n                monitor=monitor,\n                save_best_only=save_best_only,\n                mode=mode,\n            )\n        ]\n        model.fit(\n            X_train,\n            y_train,\n            batch_size=batch_size,\n            validation_data=(X_test, y_test),\n            callbacks=cbks,\n            epochs=1,\n        )\n        assert filepath.is_file()\n        filepath.unlink()\n\n        # case 5\n        monitor = 'val_loss'\n        save_best_only = False\n        mode = 'test'\n\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n        cbks = [\n            LoggingModels(\n                filepath,\n                logger=logging.Logger(__name__),\n                monitor=monitor,\n                save_best_only=save_best_only,\n                mode=mode,\n            )\n        ]\n        model.fit(\n            X_train,\n            y_train,\n            batch_size=batch_size,\n            validation_data=(X_test, y_test),\n            callbacks=cbks,\n            epochs=1,\n        )\n        assert filepath.is_file()\n        mp_logger_warning.assert_called()\n        filepath.unlink()\n\n        # case 6\n        save_best_only = False\n        period = 2\n        mode = 'auto'\n        filepath = 'checkpoint.{epoch:02d}.h5'\n        cbks = [\n            LoggingModels(\n                filepath,\n                logger=logging.Logger(__name__),\n                monitor=monitor,\n                save_best_only=save_best_only,\n                mode=mode,\n                period=period,\n            )\n        ]\n        model.fit(\n            X_train,\n            y_train,\n            batch_size=batch_size,\n            validation_data=(X_test, y_test),\n            callbacks=cbks,\n            epochs=4,\n        )\n        assert Path(filepath.format(epoch=2)).resolve().is_file()\n        assert Path(filepath.format(epoch=4)).resolve().is_file()\n        assert not Path(filepath.format(epoch=1)).resolve().exists()\n        assert not Path(filepath.format(epoch=3)).resolve().exists()\n        Path(filepath.format(epoch=2)).unlink()\n        Path(filepath.format(epoch=4)).unlink()\n        assert not list(tmpdir.glob('*'))\n"""
