file_path,api_count,code
setup.py,0,"b""import sys\nfrom setuptools import setup, find_packages, Extension\n\nlong_description = '''\nimagededup is a python package that provides functionality to find duplicates in a collection of images using a variety\nof algorithms. Additionally, an evaluation and experimentation framework, is also provided. Following details the\nfunctionality provided by the package:\n\n* Finding duplicates in a directory using one of the following algorithms:\n    - Convolutional Neural Network\n    - Perceptual hashing\n    - Difference hashing\n    - Wavelet hashing\n    - Average hashing\n* Generation of features for images using one of the above stated algorithms.\n* Framework to evaluate effectiveness of deduplication given a ground truth mapping.\n* Plotting duplicates found for a given image file.\n\nRead the documentation at: https://idealo.github.io/imagededup/\n\nimagededup is compatible with Python 3.6+ and runs on Linux, MacOS X and Windows. \nIt is distributed under the Apache 2.0 license.\n'''\n\n# Cython compilation is not enabled by default\n# http://docs.cython.org/en/latest/src/userguide/source_files_and_compilation.html#distributing-cython-modules\n\n\ntry:\n    from Cython.Build import cythonize\nexcept ImportError:\n    use_cython = False\nelse:\n    use_cython = True\n\non_mac = sys.platform.startswith('darwin')\non_windows = sys.platform.startswith('win')\n\nMOD_NAME = 'brute_force_cython_ext'\nMOD_PATH = 'imagededup/handlers/search/brute_force_cython_ext'\nCOMPILE_LINK_ARGS = ['-O3', '-march=native', '-mtune=native']\n# On Mac, use libc++ because Apple deprecated use of libstdc\nCOMPILE_ARGS_OSX = ['-stdlib=libc++']\nLINK_ARGS_OSX = ['-lc++', '-nodefaultlibs']\n\next_modules = []\nif use_cython and on_mac:\n    ext_modules += cythonize([\n        Extension(\n            MOD_NAME,\n            [MOD_PATH + '.pyx'],\n            language='c++',\n            extra_compile_args=COMPILE_LINK_ARGS + COMPILE_ARGS_OSX,\n            extra_link_args=COMPILE_LINK_ARGS + LINK_ARGS_OSX,\n        )\n    ])\nelif use_cython and on_windows:\n    ext_modules += cythonize([\n        Extension(\n            MOD_NAME,\n            [MOD_PATH + '.pyx'],\n            language='c++',\n        )\n    ])\nelif use_cython:\n    ext_modules += cythonize([\n        Extension(\n            MOD_NAME,\n            [MOD_PATH + '.pyx'],\n            language='c++',\n            extra_compile_args=COMPILE_LINK_ARGS,\n            extra_link_args=COMPILE_LINK_ARGS,\n        )\n    ])\nelse:\n    if on_mac:\n        ext_modules += [Extension(MOD_NAME,\n                                  [MOD_PATH + '.cpp'],\n                                  extra_compile_args=COMPILE_ARGS_OSX,\n                                  extra_link_args=LINK_ARGS_OSX,\n                                  )\n                        ]\n    else:\n        ext_modules += [Extension(MOD_NAME,\n                                  [MOD_PATH + '.cpp'],\n                                  )\n                        ]\n\n\nsetup(\n    name='imagededup',\n    version='0.2.2',\n    author='Tanuj Jain, Christopher Lennan, Zubin John, Dat Tran',\n    author_email='tanuj.jain.10@gmail.com, christopherlennan@gmail.com, zrjohn@yahoo.com, datitran@gmail.com',\n    description='Package for image deduplication',\n    long_description=long_description,\n    license='Apache 2.0',\n    install_requires=[\n        'numpy<1.17',\n        'Pillow<7.0.0',\n        'PyWavelets~=1.0.3',\n        'scipy',\n        'tensorflow>1.0',\n        'tqdm',\n        'scikit-learn',\n        'matplotlib',\n    ],\n    extras_require={\n        'tests': ['pytest', 'pytest-cov', 'pytest-mock', 'codecov'],\n        'docs': ['mkdocs', 'mkdocs-material'],\n        'dev': ['bumpversion', 'twine'],\n    },\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: Apache Software License',\n        'Operating System :: POSIX :: Linux',\n        'Operating System :: MacOS :: MacOS X',\n        'Operating System :: Microsoft :: Windows',\n        'Programming Language :: Cython',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n    ],\n    packages=find_packages(exclude=('tests',)),\n    ext_modules=ext_modules\n)"""
imagededup/__init__.py,0,"b""__version__ = '0.2.2'\n"""
mkdocs/autogen.py,0,"b'# Heavily borrowed from the Auto-Keras project:\n# https://github.com/jhfjhfj1/autokeras/blob/master/mkdocs/autogen.py\n\nimport ast\nimport os\nimport re\n\n\ndef delete_space(parts, start, end):\n    if start > end or end >= len(parts):\n        return None\n    count = 0\n    while count < len(parts[start]):\n        if parts[start][count] == \' \':\n            count += 1\n        else:\n            break\n    return \'\\n\'.join(y for y in [x[count:] for x in parts[start: end + 1] if len(x) > count])\n\n\ndef change_args_to_dict(string):\n    if string is None:\n        return None\n    ans = []\n    strings = string.split(\'\\n\')\n    ind = 1\n    start = 0\n    while ind <= len(strings):\n        if ind < len(strings) and strings[ind].startswith("" ""):\n            ind += 1\n        else:\n            if start < ind:\n                ans.append(\'\\n\'.join(strings[start:ind]))\n            start = ind\n            ind += 1\n    d = {}\n    for line in ans:\n        if ("":"" in line) and (line.count(\':\') == 1) and (len(line) > 0):\n            lines = line.split("":"")\n            d[lines[0]] = lines[1].strip()\n        elif ("":"" in line) and (line.count(\':\') > 1) and (len(line) > 0):\n            lines = line.split("":"")\n            keyname = lines[0]\n            val = \':\'.join(lines[1:])\n            d[keyname] = val\n    return d\n\n\ndef remove_next_line(comments):\n    for x in comments:\n        if comments[x] is not None and \'\\n\' in comments[x]:\n            comments[x] = \' \'.join(comments[x].split(\'\\n\'))\n    return comments\n\n\ndef skip_space_line(parts, ind):\n    while ind < len(parts):\n        if re.match(r\'^\\s*$\', parts[ind]):\n            ind += 1\n        else:\n            break\n    return ind\n\n\n# check if comment is None or len(comment) == 0 return {}\ndef parse_func_string(comment):\n    if comment is None or len(comment) == 0:\n        return {}\n    comments = {}\n    paras = (\'Args\', \'Attributes\', \'Returns\', \'Raises\', \'Example\')\n    comment_parts = [\n        \'short_description\',\n        \'long_description\',\n        \'Args\',\n        \'Attributes\',\n        \'Returns\',\n        \'Raises\',\n        \'Example\',\n    ]\n    for x in comment_parts:\n        comments[x] = None\n\n    parts_init = re.split(r\'\\n\', comment)\n\n    parts = []\n\n    for i in range(len(parts_init)):\n        if parts_init[i] == \'Example:\':\n            parts.append(parts_init[i])\n            code_part = \'<sep>\'.join([i.strip() for i in parts_init[len(parts):]]).replace(\'```\', \'\')\n            parts.append(code_part)\n            break\n        else:\n            parts.append(parts_init[i])\n\n    ind = 1\n    while ind < len(parts):\n        if re.match(r\'^\\s*$\', parts[ind]):\n            break\n        else:\n            ind += 1\n\n    comments[\'short_description\'] = \'\\n\'.join(\n        [\'\\n\'.join(re.split(\'\\n\\s+\', x.strip())) for x in parts[0:ind]]\n    ).strip(\':\\n\\t \')\n    ind = skip_space_line(parts, ind)\n\n    start = ind\n    while ind < len(parts):\n        if parts[ind].strip().startswith(paras):\n            break\n        else:\n            ind += 1\n    long_description = \'\\n\'.join(\n        [\'\\n\'.join(re.split(\'\\n\\s+\', x.strip())) for x in parts[start:ind]]\n    ).strip(\':\\n\\t \')\n    comments[\'long_description\'] = long_description\n\n    ind = skip_space_line(paras, ind)\n    while ind < len(parts):\n        if parts[ind].strip().startswith(paras):\n            start = ind\n            start_with = parts[ind].strip()\n            ind += 1\n            while ind < len(parts):\n                if parts[ind].strip().startswith(paras):\n                    break\n                else:\n                    ind += 1\n            part = delete_space(parts, start + 1, ind - 1)\n            if start_with.startswith(paras[0]):\n                comments[paras[0]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[1]):\n                comments[paras[1]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[2]):\n                comments[paras[2]] = change_args_to_dict(part)\n            elif start_with.startswith(paras[3]):\n                comments[paras[3]] = part\n            elif start_with.startswith(paras[4]):\n                comments[paras[4]] = part\n            ind = skip_space_line(parts, ind)\n        else:\n            ind += 1\n\n    remove_next_line(comments)\n    return comments\n\n\ndef md_parse_line_break(comment):\n    comment = comment.replace(\'  \', \'\\n\\n\')\n    return comment.replace(\' - \', \'\\n\\n- \')\n\n\ndef to_md(comment_dict):\n    doc = \'\'\n    if \'short_description\' in comment_dict:\n        doc += comment_dict[\'short_description\']\n        doc += \'\\n\\n\'\n\n    if \'long_description\' in comment_dict:\n        doc += md_parse_line_break(comment_dict[\'long_description\'])\n        doc += \'\\n\'\n\n    if \'Args\' in comment_dict and comment_dict[\'Args\'] is not None:\n        doc += \'##### Args\\n\'\n        for arg, des in comment_dict[\'Args\'].items():\n            doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n\n    if \'Attributes\' in comment_dict and comment_dict[\'Attributes\'] is not None:\n        doc += \'##### Attributes\\n\'\n        for arg, des in comment_dict[\'Attributes\'].items():\n            doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n\n    if \'Returns\' in comment_dict and comment_dict[\'Returns\'] is not None:\n        doc += \'##### Returns\\n\'\n        if isinstance(comment_dict[\'Returns\'], str):\n            doc += comment_dict[\'Returns\']\n            doc += \'\\n\'\n        else:\n            for arg, des in comment_dict[\'Returns\'].items():\n                doc += \'* **\' + arg + \'**: \' + des + \'\\n\\n\'\n\n    if \'Example\' in comment_dict and comment_dict[\'Example\'] is not None:\n        doc += \'##### Example usage\\n\'\n        doc += \'```python\\n\'\n        if isinstance(comment_dict[\'Example\'], str):\n            for i in comment_dict[\'Example\'].split(\'<sep>\'):\n                doc = doc + i\n                doc += \'\\n\'\n        doc += \'```\\n\'\n    return doc\n\n\ndef parse_func_args(function):\n    args = [a.arg for a in function.args.args if a.arg != \'self\']\n    kwargs = []\n    if function.args.kwarg:\n        kwargs = [\'**\' + function.args.kwarg.arg]\n\n    return \'(\' + \', \'.join(args + kwargs) + \')\'\n\n\ndef get_func_comments(function_definitions):\n    doc = \'\'\n    for f in function_definitions:\n        temp_str = to_md(parse_func_string(ast.get_docstring(f)))\n        doc += \'\'.join(\n            [\n                \'### \',\n                f.name.replace(\'_\', \'\\\\_\'),\n                \'\\n\',\n                \'```python\',\n                \'\\n\',\n                \'def \',\n                f.name,\n                parse_func_args(f),\n                \'\\n\',\n                \'```\',\n                \'\\n\',\n                temp_str,\n                \'\\n\',\n            ]\n        )\n\n    return doc\n\n\ndef get_comments_str(file_name):\n    with open(file_name) as fd:\n        file_contents = fd.read()\n    module = ast.parse(file_contents)\n\n    function_definitions = [node for node in module.body if (isinstance(node, ast.FunctionDef)) and (node.name[0] != \'_\' or node.name[:2] == \'__\')]\n\n    doc = get_func_comments(function_definitions)\n\n    class_definitions = [node for node in module.body if isinstance(node, ast.ClassDef)]\n    for class_def in class_definitions:\n        temp_str = to_md(parse_func_string(ast.get_docstring(class_def)))\n\n        # excludes private methods (start with \'_\')\n        method_definitions = [\n            node\n            for node in class_def.body\n            if isinstance(node, ast.FunctionDef) and (node.name[0] != \'_\' or node.name[:2] == \'__\')\n        ]\n\n        temp_str += get_func_comments(method_definitions)\n        doc += \'## class \' + class_def.name + \'\\n\' + temp_str\n    return doc\n\n\ndef extract_comments(directory):\n    for parent, dir_names, file_names in os.walk(directory):\n        for file_name in file_names:\n            if os.path.splitext(file_name)[1] == \'.py\' and file_name != \'__init__.py\':\n                # with open\n                doc = get_comments_str(os.path.join(parent, file_name))\n                directory = os.path.join(\'docs\', parent.replace(\'../imagededup/\', \'\'))\n                if not os.path.exists(directory):\n                    os.makedirs(directory)\n\n                output_file = open(os.path.join(directory, file_name[:-3] + \'.md\'), \'w\')\n                output_file.write(doc)\n                output_file.close()\n\n\nextract_comments(\'../imagededup/\')\n'"
tests/test_bktree.py,0,"b""from collections import OrderedDict\n\nfrom imagededup.methods.hashing import Hashing\nfrom imagededup.handlers.search.bktree import BKTree, BkTreeNode\n\n# Test BkTreeNode\n\n\ndef initialize_for_bktree():\n    hash_dict = OrderedDict(\n        {'a': '9', 'b': 'D', 'c': 'A', 'd': 'F', 'e': '2', 'f': '6', 'g': '7', 'h': 'E'}\n    )\n    dist_func = Hashing.hamming_distance\n    return hash_dict, dist_func\n\n\ndef test_bktreenode_correct_initialization():\n    node_name, node_value, parent_name = 'test_node', '1aef', None\n    node = BkTreeNode(node_name, node_value, parent_name)\n    assert node.node_name == 'test_node'\n    assert node.node_value == '1aef'\n    assert node.parent_name is None\n    assert len(node.children) == 0\n\n\n# test BKTree class\n\n\ndef test_insert_tree():\n    # initialize root node and add 1 new node, check it goes as root's child and has it's parent as root\n    _, dist_func = initialize_for_bktree()\n    hash_dict = {'a': '9', 'b': 'D'}\n    bk = BKTree(hash_dict, dist_func)\n    assert bk.ROOT == 'a'\n    assert 'b' in list(bk.dict_all['a'].children.keys())\n    assert bk.dict_all['b'].parent_name == 'a'\n\n\ndef test_insert_tree_collision():\n    # initialize root node, add 1 new node and enter another node with same distance from root, check it goes not as\n    # root's child but the other node's child\n    _, dist_func = initialize_for_bktree()\n    hash_dict = OrderedDict(\n        {'a': '9', 'b': 'D', 'c': '8'}\n    )  # to guarantee that 'a' is the root of the tree\n    bk = BKTree(hash_dict, dist_func)\n    assert bk.ROOT == 'a'\n    assert len(bk.dict_all[bk.ROOT].children) == 1\n    assert 'c' in list(bk.dict_all['b'].children.keys())\n\n\ndef test_insert_tree_different_nodes():\n    # initialize root node, add 1 new node and enter another node with different distance from root, check it goes as\n    # root's child and not as the other node's child\n    _, dist_func = initialize_for_bktree()\n    hash_dict = OrderedDict(\n        {'a': '9', 'b': 'D', 'c': 'F'}\n    )  # to guarantee that 'a' is the root of the tree\n    bk = BKTree(hash_dict, dist_func)\n    assert bk.ROOT == 'a'\n    assert len(bk.dict_all[bk.ROOT].children) == 2\n    assert set(['b', 'c']) <= set(bk.dict_all[bk.ROOT].children.keys())\n\n\ndef test_insert_tree_check_distance():\n    # initialize root node, add 1 new node and enter another node with different distance from root, check that the\n    # distance recorded in the root's children dictionary is as expected\n    _, dist_func = initialize_for_bktree()\n    hash_dict = OrderedDict(\n        {'a': '9', 'b': 'D', 'c': 'F'}\n    )  # to guarantee that 'a' is the root of the tree\n    bk = BKTree(hash_dict, dist_func)\n    assert bk.ROOT == 'a'\n    assert bk.dict_all[bk.ROOT].children['b'] == 1\n    assert bk.dict_all[bk.ROOT].children['c'] == 2\n\n\ndef test_construct_tree():\n    # Input a complete tree and check for each node the children and parents\n    hash_dict, dist_func = initialize_for_bktree()\n    bk = BKTree(hash_dict, dist_func)\n    # check root\n    assert bk.ROOT == 'a'\n    # check that expected leaf nodes have no children (they're actually leaf nodes)\n    leaf_nodes = set(\n        [k for k in bk.dict_all.keys() if len(bk.dict_all[k].children) == 0]\n    )\n    expected_leaf_nodes = set(['b', 'd', 'f', 'h'])\n    assert leaf_nodes == expected_leaf_nodes\n    # check that root node ('a') has 4 children\n    assert len(bk.dict_all[bk.ROOT].children) == 4\n    # check that 'c' has 'd' as it's child at distance 2\n    assert bk.dict_all['c'].children['d'] == 2\n\n\ndef test_search():\n    # Input a tree and send a search query, check whether correct number of retrievals are returned\n    hash_dict, dist_func = initialize_for_bktree()\n    bk = BKTree(hash_dict, dist_func)\n    query = '5'\n    valid_retrievals = bk.search(query, tol=2)\n    assert len(valid_retrievals) == 5\n\n\ndef test_search_correctness():\n    # Input a tree and send a search query, check whether correct retrievals are returned\n    hash_dict, dist_func = initialize_for_bktree()\n    bk = BKTree(hash_dict, dist_func)\n    query = '5'\n    valid_retrievals = bk.search(query, tol=2)\n    assert set([i[0] for i in valid_retrievals]) == set(['a', 'f', 'g', 'd', 'b'])\n\n\ndef test_search_zero_tolerance():\n    # Input a tree and send a search query, check whether zero retrievals are returned for zero tolerance\n    hash_dict, dist_func = initialize_for_bktree()\n    bk = BKTree(hash_dict, dist_func)\n    query = '5'\n    valid_retrievals = bk.search(query, tol=0)\n    assert len(valid_retrievals) == 0\n\n\ndef test_search_dist():\n    # Input a tree and send a search query, check whether correct distance for a retrieval is returned\n    hash_dict, dist_func = initialize_for_bktree()\n    bk = BKTree(hash_dict, dist_func)\n    query = '5'\n    valid_retrievals = bk.search(query, tol=2)\n    assert [i for i in valid_retrievals if i[0] == 'a'][0][1] == 2\n\n\ndef test_get_next_candidates_valid():\n    # Give a partial tree as input and check that for a query, expected candidates and validity flag are obtained\n    hash_dict, dist_func = initialize_for_bktree()\n    bk = BKTree(hash_dict, dist_func)\n    assert bk.ROOT == 'a'\n    query = '5'\n    candidates, validity, dist = bk._get_next_candidates(\n        query, bk.dict_all[bk.ROOT], tolerance=2\n    )\n    candidates = set(candidates)\n    assert candidates <= set(['b', 'c', 'e', 'f'])\n    assert validity\n\n\ndef test_get_next_candidates_invalid():\n    # Give a tree as input and check that for a query, validity flag is 0\n    hash_dict, dist_func = initialize_for_bktree()\n    bk = BKTree(hash_dict, dist_func)\n    assert bk.ROOT == 'a'\n    query = '5'\n    _, validity, _ = bk._get_next_candidates(query, bk.dict_all[bk.ROOT], tolerance=1)\n    assert not validity\n\n\ndef test_tolerance_affects_retrievals():\n    # Give a partial tree as input and check that for a query, increased tolerance gives more retrievals as expected for\n    # the input tree\n    hash_dict, dist_func = initialize_for_bktree()\n    bk = BKTree(hash_dict, dist_func)\n    assert bk.ROOT == 'a'\n    query = '5'\n    candidates, _, _ = bk._get_next_candidates(query, bk.dict_all[bk.ROOT], tolerance=1)\n    low_tolerance_candidate_len = len(candidates)\n    candidates, _, _ = bk._get_next_candidates(query, bk.dict_all[bk.ROOT], tolerance=2)\n    high_tolerance_candidate_len = len(candidates)\n    assert high_tolerance_candidate_len > low_tolerance_candidate_len\n"""
tests/test_brute_force.py,0,"b""from collections import OrderedDict\n\nfrom imagededup.handlers.search.brute_force import BruteForce\nfrom imagededup.methods.hashing import Hashing\n\n\ndef initialize():\n    hash_dict = OrderedDict(\n        {'a': '9', 'b': 'D', 'c': 'A', 'd': 'F', 'e': '2', 'f': '6', 'g': '7', 'h': 'E'}\n    )\n    dist_func = Hashing.hamming_distance\n    return hash_dict, dist_func\n\n\ndef test_search_correctness():\n    hash_dict, dist_func = initialize()\n    bf = BruteForce(hash_dict, dist_func)\n    query = '5'\n    valid_retrievals = bf.search(query, tol=2)\n    assert set([i[0] for i in valid_retrievals]) == set(['a', 'f', 'g', 'd', 'b'])\n\n\ndef test_tolerance_value_effect():\n    hash_dict, dist_func = initialize()\n    bf = BruteForce(hash_dict, dist_func)\n    query = '5'\n    valid_retrievals_2 = bf.search(query, tol=2)\n    valid_retrievals_3 = bf.search(query, tol=3)\n    assert set([i[0] for i in valid_retrievals_2]) != set(\n        [i[0] for i in valid_retrievals_3]\n    )\n"""
tests/test_brute_force_cython.py,0,"b""from collections import OrderedDict\n\nfrom imagededup.handlers.search.brute_force_cython import BruteForceCython\nfrom imagededup.methods.hashing import Hashing\n\n\ndef initialize():\n    hash_dict = OrderedDict(\n        {'a': '9', 'b': 'D', 'c': 'A', 'd': 'F', 'e': '2', 'f': '6', 'g': '7', 'h': 'E'}\n    )\n    dist_func = Hashing.hamming_distance\n    return hash_dict, dist_func\n\n\ndef test_search_correctness():\n    hash_dict, dist_func = initialize()\n    bf = BruteForceCython(hash_dict, dist_func)\n    query = '5'\n    valid_retrievals = bf.search(query, tol=2)\n    assert set([i[0] for i in valid_retrievals]) == set(['a', 'f', 'g', 'd', 'b'])\n\n\ndef test_tolerance_value_effect():\n    hash_dict, dist_func = initialize()\n    bf = BruteForceCython(hash_dict, dist_func)\n    query = '5'\n    valid_retrievals_2 = bf.search(query, tol=2)\n    valid_retrievals_3 = bf.search(query, tol=3)\n    assert set([i[0] for i in valid_retrievals_2]) != set(\n        [i[0] for i in valid_retrievals_3]\n    )\n"""
tests/test_classification.py,0,"b""import numpy as np\n\nfrom imagededup.handlers.metrics.classification import (\n    _get_unique_ordered_tuples,\n    _make_all_unique_possible_pairs,\n    _make_positive_duplicate_pairs,\n    _prepare_labels,\n    classification_metrics,\n)\n\n\ndef test__get_unique_ordered_tuples():\n    non_unique_pairs = [('1', '3'), ('1', '4'), ('3', '1'), ('1', '3'), ('3', '4')]\n    obtained_unique_ordered_pairs = _get_unique_ordered_tuples(non_unique_pairs)\n\n    # test order\n    for i in obtained_unique_ordered_pairs:\n        assert i[0] <= i[1]\n\n    # test membership\n    set_list = []\n    for j in obtained_unique_ordered_pairs:\n        if set(j) not in set_list:\n            set_list.append(set(j))\n    assert len(set_list) == 3\n\n\ndef test__make_all_unique_possible_pairs(mocker):\n    ground_truth = {'1': ['2', '3', '4'], '2': ['1', '3'], '3': ['1', '2'], '4': ['1']}\n    all_pairs = [\n        ('1', '2'),\n        ('1', '3'),\n        ('1', '4'),\n        ('2', '1'),\n        ('2', '3'),\n        ('2', '4'),\n        ('3', '1'),\n        ('3', '2'),\n        ('3', '4'),\n        ('4', '1'),\n        ('4', '2'),\n        ('4', '3'),\n    ]\n    get_unique_ordered_tuples_mocker = mocker.patch(\n        'imagededup.handlers.metrics.classification._get_unique_ordered_tuples'\n    )\n    _make_all_unique_possible_pairs(ground_truth)\n    get_unique_ordered_tuples_mocker.assert_called_once_with(all_pairs)\n\n\ndef test__make_positive_duplicate_pairs(mocker):\n    ground_truth = {'1': ['2', '3', '4'], '2': ['1', '3'], '3': ['1', '2'], '4': ['1']}\n    valid_pairs = [\n        ('1', '2'),\n        ('1', '3'),\n        ('1', '4'),\n        ('2', '1'),\n        ('2', '3'),\n        ('3', '1'),\n        ('3', '2'),\n        ('4', '1'),\n    ]\n    get_unique_ordered_tuples_mocker = mocker.patch(\n        'imagededup.handlers.metrics.classification._get_unique_ordered_tuples'\n    )\n    _make_positive_duplicate_pairs(ground_truth, ground_truth)\n    get_unique_ordered_tuples_mocker.assert_called_with(valid_pairs)\n\n\ndef test__prepare_labels():\n    all_possible_pairs = [\n        ('1', '3'),\n        ('2', '3'),\n        ('1', '4'),\n        ('2', '4'),\n        ('1', '2'),\n        ('3', '4'),\n    ]\n    ground_truth_pairs = [('1', '3'), ('2', '3'), ('1', '2'), ('1', '4')]\n    retrieved_pairs = [('1', '3'), ('1', '2')]\n    y_true_obtained, y_pred_obtained = _prepare_labels(\n        all_possible_pairs, ground_truth_pairs, retrieved_pairs\n    )\n    assert y_true_obtained == [1, 1, 1, 0, 1, 0]\n    assert y_pred_obtained == [1, 0, 0, 0, 1, 0]\n\n\ndef test_classification_metrics(mocker):\n    ground_truth = {'1': ['2', '3', '4'], '2': ['1', '3'], '3': ['1', '2'], '4': ['1']}\n    retrieved = {'1': ['2', '3'], '2': ['1'], '3': ['1'], '4': []}\n    all_possible_pairs_ret = [\n        ('1', '3'),\n        ('2', '3'),\n        ('1', '4'),\n        ('2', '4'),\n        ('1', '2'),\n        ('3', '4'),\n    ]\n    ground_truth_pairs_ret = [('1', '3'), ('2', '3'), ('1', '2'), ('1', '4')]\n    retrieved_pairs_ret = [('1', '3'), ('1', '2')]\n    y_true_ret = [1, 1, 1, 0, 1, 0]\n    y_pred_ret = [1, 0, 0, 0, 1, 0]\n\n    make_all_unique_possible_pairs_mocker = mocker.patch(\n        'imagededup.handlers.metrics.classification._make_all_unique_possible_pairs'\n    )\n    make_all_unique_possible_pairs_mocker.return_value = all_possible_pairs_ret\n\n    make_positive_duplicate_pairs_mocker = mocker.patch(\n        'imagededup.handlers.metrics.classification._make_positive_duplicate_pairs'\n    )\n    make_positive_duplicate_pairs_mocker.return_value = (\n        ground_truth_pairs_ret,\n        retrieved_pairs_ret,\n    )\n    prepare_labels_mocker = mocker.patch(\n        'imagededup.handlers.metrics.classification._prepare_labels'\n    )\n    prepare_labels_mocker.return_value = y_true_ret, y_pred_ret\n    precision_recall_fscore_support_mocker = mocker.patch(\n        'imagededup.handlers.metrics.classification.precision_recall_fscore_support'\n    )\n    classification_metrics(ground_truth, retrieved)\n    make_all_unique_possible_pairs_mocker.assert_called_once_with(ground_truth)\n    make_positive_duplicate_pairs_mocker.assert_called_once_with(\n        ground_truth, retrieved\n    )\n    prepare_labels_mocker.assert_called_once_with(\n        all_possible_pairs_ret, ground_truth_pairs_ret, retrieved_pairs_ret\n    )\n    precision_recall_fscore_support_mocker.assert_called_once_with(\n        y_true_ret, y_pred_ret\n    )\n\n\n# Integration test\n\n\ndef test_classification_metrics_integrated():\n    ground_truth = {'1': ['2', '3', '4'], '2': ['1', '3'], '3': ['1', '2'], '4': ['1']}\n    retrieved = {'1': ['2', '3'], '2': ['1'], '3': ['1'], '4': []}\n    expected_return = {\n        'precision': np.array([0.5, 1.0]),\n        'recall': np.array([1.0, 0.5]),\n        'f1_score': np.array([0.66666667, 0.66666667]),\n        'support': np.array([2, 4]),\n    }\n    metrics = classification_metrics(ground_truth, retrieved)\n    assert isinstance(metrics, dict)\n\n    for k, v in metrics.items():\n        assert isinstance(v, np.ndarray)\n        np.testing.assert_almost_equal(metrics[k], expected_return[k])\n"""
tests/test_cnn.py,0,"b'from pathlib import Path\n\nimport os\nimport json\nimport numpy as np\nimport pytest\nfrom tensorflow.keras.models import Model\n\nfrom imagededup.methods.cnn import CNN\nfrom imagededup.utils.image_utils import load_image\n\n\np = Path(__file__)\nTEST_IMAGE = p.parent / \'data\' / \'base_images\' / \'ukbench00120.jpg\'\nTEST_IMAGE_DIR = p.parent / \'data\' / \'base_images\'\nTEST_IMAGE_FORMATS_DIR = p.parent / \'data\' / \'formats_images\'\nTEST_IMAGE_DIR_MIXED = p.parent / \'data\' / \'mixed_images\'\n\nTEST_BATCH_SIZE = 64\nTEST_TARGET_SIZE = (224, 224)\n\n\ndef data_encoding_map():\n    return {\n        \'ukbench00002.jpg\': np.array([1, 0, 0, 1]),\n        \'ukbench00003.jpg\': np.array([1, 1, 0, 1]),\n        \'ukbench00002_dup.jpg\': np.array([1, 0, 0, 1]),\n    }\n\n\n@pytest.fixture(scope=\'module\')\ndef cnn():\n    return CNN()\n\n\n@pytest.fixture\ndef mocker_save_json(mocker):\n    return mocker.patch(\'imagededup.methods.cnn.save_json\')\n\n\ndef test__init(cnn):\n    assert cnn.batch_size == TEST_BATCH_SIZE\n    assert cnn.target_size == TEST_TARGET_SIZE\n    assert isinstance(cnn.model, Model)\n\n\ndef test__get_cnn_features_single(cnn):\n    img = load_image(TEST_IMAGE, target_size=(224, 224))\n\n    result = cnn._get_cnn_features_single(img)\n\n    assert isinstance(result, np.ndarray)\n    assert result.shape == (1, 1024)\n\n\ndef test__get_cnn_features_batch(cnn):\n\n    result = cnn._get_cnn_features_batch(TEST_IMAGE_DIR)\n\n    expected_predicted_files = [\n        \'ukbench00120.jpg\',\n        \'ukbench01380.jpg\',\n        \'ukbench08976.jpg\',\n        \'ukbench08996.jpg\',\n        \'ukbench09012.jpg\',\n        \'ukbench09040.jpg\',\n        \'ukbench09060.jpg\',\n        \'ukbench09268.jpg\',\n        \'ukbench09348.jpg\',\n        \'ukbench09380.jpg\',\n    ]\n\n    assert list(sorted(result.keys(), key=str.lower)) == expected_predicted_files\n\n    for i in result.values():\n        assert isinstance(i, np.ndarray)\n        assert i.shape == (1024,)\n\n    result = cnn._get_cnn_features_batch(TEST_IMAGE_FORMATS_DIR)\n\n    expected_predicted_files = [\n        \'baboon.pgm\',\n        \'copyleft.tiff\',\n        \'giphy.gif\',\n        \'Iggy.1024.ppm\',\n        \'marbles.pbm\',\n        \'mpo_image.MPO\',\n        \'ukbench09380.bmp\',\n        \'ukbench09380.jpeg\',\n        \'ukbench09380.png\',\n        \'ukbench09380.svg\',\n    ]\n\n    assert list(sorted(result.keys(), key=str.lower)) == expected_predicted_files\n\n    for i in result.values():\n        assert isinstance(i, np.ndarray)\n        assert i.shape == (1024,)\n\n\ndef test_encode_image(cnn):\n    result = cnn.encode_image(TEST_IMAGE)\n\n    assert isinstance(result, np.ndarray)\n    assert result.shape == (1, 1024)  # 1024 = 3*3*1024*2\n\n    result = cnn.encode_image(str(TEST_IMAGE))\n\n    assert isinstance(result, np.ndarray)\n    assert result.shape == (1, 1024)  # 1024 = 3*3*1024*2\n\n    with pytest.raises(ValueError):\n        cnn.encode_image("""")\n\n    image_array = load_image(TEST_IMAGE)\n    result = cnn.encode_image(image_array=image_array)\n    assert result.shape == (1, 1024)  # 1024 = 3*3*1024*2\n\n\ndef test_encode_images(cnn):\n    result = cnn.encode_images(TEST_IMAGE_DIR)\n\n    expected_predicted_files = [\n        \'ukbench00120.jpg\',\n        \'ukbench01380.jpg\',\n        \'ukbench08976.jpg\',\n        \'ukbench08996.jpg\',\n        \'ukbench09012.jpg\',\n        \'ukbench09040.jpg\',\n        \'ukbench09060.jpg\',\n        \'ukbench09268.jpg\',\n        \'ukbench09348.jpg\',\n        \'ukbench09380.jpg\',\n    ]\n\n    assert list(sorted(result.keys(), key=str.lower)) == expected_predicted_files\n\n    for i in result.values():\n        assert isinstance(i, np.ndarray)\n        assert i.shape == (1024,)\n\n    result = cnn.encode_images(TEST_IMAGE_FORMATS_DIR)\n\n    expected_predicted_files = [\n        \'baboon.pgm\',\n        \'copyleft.tiff\',\n        \'giphy.gif\',\n        \'Iggy.1024.ppm\',\n        \'marbles.pbm\',\n        \'mpo_image.MPO\',\n        \'ukbench09380.bmp\',\n        \'ukbench09380.jpeg\',\n        \'ukbench09380.png\',\n        \'ukbench09380.svg\',\n    ]\n\n    assert list(sorted(result.keys(), key=str.lower)) == expected_predicted_files\n\n    for i in result.values():\n        assert isinstance(i, np.ndarray)\n        assert i.shape == (1024,)\n\n    result = cnn.encode_images(str(TEST_IMAGE_FORMATS_DIR))\n\n    expected_predicted_files = [\n        \'baboon.pgm\',\n        \'copyleft.tiff\',\n        \'giphy.gif\',\n        \'Iggy.1024.ppm\',\n        \'marbles.pbm\',\n        \'mpo_image.MPO\',\n        \'ukbench09380.bmp\',\n        \'ukbench09380.jpeg\',\n        \'ukbench09380.png\',\n        \'ukbench09380.svg\',\n    ]\n\n    assert list(sorted(result.keys(), key=str.lower)) == expected_predicted_files\n\n    for i in result.values():\n        assert isinstance(i, np.ndarray)\n        assert i.shape == (1024,)\n\n    with pytest.raises(ValueError):\n        cnn.encode_images(\'abc\')\n\n\ndef test__check_threshold_bounds_input_not_float(cnn):\n    with pytest.raises(TypeError):\n        cnn._check_threshold_bounds(thresh=1)\n\n\ndef test__check_threshold_bounds_input_out_of_range(cnn):\n    with pytest.raises(ValueError):\n        cnn._check_threshold_bounds(thresh=1.1)\n\n\n# _find_duplicates_dict\n\n\ndef test__find_duplicates_dict_scores_false(cnn):\n    # check correctness\n    encoding_map = data_encoding_map()\n    dict_ret = cnn._find_duplicates_dict(\n        encoding_map, min_similarity_threshold=0.9, scores=False\n    )\n    assert isinstance(dict_ret[\'ukbench00002.jpg\'], list)\n    assert len(dict_ret[\'ukbench00002.jpg\']) == 1\n    assert not isinstance(dict_ret[\'ukbench00002.jpg\'][0], tuple)\n    assert dict_ret[\'ukbench00002.jpg\'][0] == \'ukbench00002_dup.jpg\'\n\n\ndef test__find_duplicates_dict_scores_true(cnn, mocker_save_json):\n    # check correctness, also check that saving file is not triggered as outfile default value is False\n    encoding_map = data_encoding_map()\n    dict_ret = cnn._find_duplicates_dict(\n        encoding_map, min_similarity_threshold=0.9, scores=True\n    )\n\n    assert isinstance(dict_ret[\'ukbench00002.jpg\'], list)\n    assert len(dict_ret[\'ukbench00002.jpg\']) == 1\n    assert isinstance(dict_ret[\'ukbench00002.jpg\'][0], tuple)\n    assert dict_ret[\'ukbench00002.jpg\'][0][0] == \'ukbench00002_dup.jpg\'\n    assert isinstance(dict_ret[\'ukbench00002.jpg\'][0][1], float)\n    np.testing.assert_almost_equal(dict_ret[\'ukbench00002.jpg\'][0][1], 1.0)\n    mocker_save_json.assert_not_called()\n\n\ndef test__find_duplicates_dict_outfile_true(cnn, mocker_save_json):\n    encoding_map = data_encoding_map()\n    threshold = 0.8\n    scores = True\n    outfile = True\n    cnn._find_duplicates_dict(\n        encoding_map=encoding_map,\n        min_similarity_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n    )\n    mocker_save_json.assert_called_once_with(results=cnn.results, filename=outfile, float_scores=True)\n\n\n# _find_duplicates_dir\n\n\ndef test__find_duplicates_dir(cnn, mocker):\n    encoding_map = data_encoding_map()\n    threshold = 0.8\n    scores = True\n    outfile = True\n    ret_val_find_dup_dict = {\n        \'filename1.jpg\': [(\'dup1.jpg\', 0.82)],\n        \'filename2.jpg\': [(\'dup2.jpg\', 0.90)],\n    }\n    encode_images_mocker = mocker.patch(\'imagededup.methods.cnn.CNN.encode_images\')\n    cnn.encoding_map = encoding_map\n    find_dup_dict_mocker = mocker.patch(\n        \'imagededup.methods.cnn.CNN._find_duplicates_dict\',\n        return_value=ret_val_find_dup_dict,\n    )\n    cnn._find_duplicates_dir(\n        image_dir=TEST_IMAGE_DIR,\n        min_similarity_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n    )\n    encode_images_mocker.assert_called_once_with(image_dir=TEST_IMAGE_DIR)\n    find_dup_dict_mocker.assert_called_once_with(\n        encoding_map=cnn.encoding_map,\n        min_similarity_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n    )\n\n\n# find_duplicates\n\n\ndef test_find_duplicates_dir(cnn, mocker):\n    threshold = 0.9\n    scores = True\n    outfile = True\n    find_dup_dir_mocker = mocker.patch(\n        \'imagededup.methods.cnn.CNN._find_duplicates_dir\'\n    )\n    cnn.find_duplicates(\n        image_dir=TEST_IMAGE_DIR,\n        min_similarity_threshold=threshold,\n        outfile=outfile,\n        scores=scores,\n    )\n    find_dup_dir_mocker.assert_called_once_with(\n        image_dir=TEST_IMAGE_DIR,\n        min_similarity_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n    )\n\n\ndef test_find_duplicates_dict(cnn, mocker):\n    encoding_map = data_encoding_map()\n    threshold = 0.9\n    scores = True\n    outfile = True\n    find_dup_dict_mocker = mocker.patch(\n        \'imagededup.methods.cnn.CNN._find_duplicates_dict\'\n    )\n    cnn.find_duplicates(\n        encoding_map=encoding_map,\n        min_similarity_threshold=threshold,\n        outfile=outfile,\n        scores=scores,\n    )\n    find_dup_dict_mocker.assert_called_once_with(\n        encoding_map=encoding_map,\n        min_similarity_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n    )\n\n\ndef test_find_duplicates_wrong_threhsold_input(cnn):\n    with pytest.raises(ValueError):\n        cnn.find_duplicates(min_similarity_threshold=1.3)\n\n\ndef test_find_duplicates_wrong_input(cnn):\n    with pytest.raises(ValueError):\n        cnn.find_duplicates()\n\n\n# find_duplicates_to_remove\n\n\ndef test_find_duplicates_to_remove_outfile_false(cnn, mocker, mocker_save_json):\n    threshold = 0.9\n    outfile = False\n    ret_val_find_dup_dict = {\n        \'filename.jpg\': [(\'dup1.jpg\', 3)],\n        \'filename2.jpg\': [(\'dup2.jpg\', 10)],\n    }\n    find_duplicates_mocker = mocker.patch(\n        \'imagededup.methods.cnn.CNN.find_duplicates\', return_value=ret_val_find_dup_dict\n    )\n    get_files_to_remove_mocker = mocker.patch(\n        \'imagededup.methods.cnn.get_files_to_remove\'\n    )\n    cnn.find_duplicates_to_remove(\n        image_dir=TEST_IMAGE_DIR, min_similarity_threshold=threshold, outfile=outfile\n    )\n    find_duplicates_mocker.assert_called_once_with(\n        image_dir=TEST_IMAGE_DIR,\n        encoding_map=None,\n        min_similarity_threshold=threshold,\n        scores=False,\n    )\n    get_files_to_remove_mocker.assert_called_once_with(ret_val_find_dup_dict)\n    mocker_save_json.assert_not_called()\n\n\ndef test_find_duplicates_to_remove_outfile_true(cnn, mocker, mocker_save_json):\n    threshold = 0.9\n    outfile = True\n    ret_val_find_dup_dict = {\n        \'filename.jpg\': [\'dup1.jpg\'],\n        \'filename2.jpg\': [\'dup2.jpg\'],\n    }\n    ret_val_get_files_to_remove = [\'1.jpg\', \'2.jpg\']\n\n    find_duplicates_mocker = mocker.patch(\n        \'imagededup.methods.cnn.CNN.find_duplicates\', return_value=ret_val_find_dup_dict\n    )\n    get_files_to_remove_mocker = mocker.patch(\n        \'imagededup.methods.cnn.get_files_to_remove\',\n        return_value=ret_val_get_files_to_remove,\n    )\n    cnn.find_duplicates_to_remove(\n        image_dir=TEST_IMAGE_DIR, min_similarity_threshold=threshold, outfile=outfile\n    )\n    find_duplicates_mocker.assert_called_once_with(\n        image_dir=TEST_IMAGE_DIR,\n        encoding_map=None,\n        min_similarity_threshold=threshold,\n        scores=False,\n    )\n    get_files_to_remove_mocker.assert_called_once_with(ret_val_find_dup_dict)\n    mocker_save_json.assert_called_once_with(ret_val_get_files_to_remove, outfile)\n\n\ndef test_find_duplicates_to_remove_encoding_map(cnn, mocker, mocker_save_json):\n    threshold = 0.9\n    outfile = True\n    ret_val_find_dup_dict = {\n        \'filename.jpg\': [\'dup1.jpg\'],\n        \'filename2.jpg\': [\'dup2.jpg\'],\n    }\n    ret_val_get_files_to_remove = [\'1.jpg\', \'2.jpg\']\n    encoding_map = data_encoding_map()\n    find_duplicates_mocker = mocker.patch(\n        \'imagededup.methods.cnn.CNN.find_duplicates\', return_value=ret_val_find_dup_dict\n    )\n    get_files_to_remove_mocker = mocker.patch(\n        \'imagededup.methods.cnn.get_files_to_remove\',\n        return_value=ret_val_get_files_to_remove,\n    )\n    cnn.find_duplicates_to_remove(\n        encoding_map=encoding_map, min_similarity_threshold=threshold, outfile=outfile\n    )\n    find_duplicates_mocker.assert_called_once_with(\n        encoding_map=encoding_map,\n        image_dir=None,\n        min_similarity_threshold=threshold,\n        scores=False,\n    )\n    get_files_to_remove_mocker.assert_called_once_with(ret_val_find_dup_dict)\n    mocker_save_json.assert_called_once_with(ret_val_get_files_to_remove, outfile)\n\n\n# Integration tests\n\n\n# test find_duplicates with directory path\ndef test_find_duplicates_dir_integration(cnn):\n    expected_duplicates = {\n        \'ukbench00120.jpg\': [\n            (\'ukbench00120_hflip.jpg\', 0.9672552),\n            (\'ukbench00120_resize.jpg\', 0.98120844),\n        ],\n        \'ukbench00120_hflip.jpg\': [\n            (\'ukbench00120.jpg\', 0.9672552),\n            (\'ukbench00120_resize.jpg\', 0.95676106),\n        ],\n        \'ukbench00120_resize.jpg\': [\n            (\'ukbench00120.jpg\', 0.98120844),\n            (\'ukbench00120_hflip.jpg\', 0.95676106),\n        ],\n        \'ukbench00120_rotation.jpg\': [],\n        \'ukbench09268.jpg\': [],\n    }\n    duplicates = cnn.find_duplicates(\n        image_dir=TEST_IMAGE_DIR_MIXED,\n        min_similarity_threshold=0.9,\n        scores=True,\n        outfile=False,\n    )\n    # verify variable type\n    assert isinstance(duplicates[\'ukbench00120.jpg\'][0][1], np.float32)\n\n    # verify that all files have been considered for deduplication\n    assert len(duplicates) == len(expected_duplicates)\n\n    # verify for each file that expected files have been received as duplicates\n    for k in duplicates.keys():\n        dup_val = duplicates[k]\n        expected_val = expected_duplicates[k]\n        dup_ret = set(map(lambda x: x[0], dup_val))\n        expected_ret = set(map(lambda x: x[0], expected_val))\n        assert dup_ret == expected_ret\n\n\n# test find_duplicates with encoding map\ndef test_find_duplicates_encoding_integration(cnn):\n    expected_duplicates = {\n        \'ukbench00120.jpg\': [\n            (\'ukbench00120_hflip.jpg\', 0.9672552),\n            (\'ukbench00120_resize.jpg\', 0.98120844),\n        ],\n        \'ukbench00120_hflip.jpg\': [\n            (\'ukbench00120.jpg\', 0.9672552),\n            (\'ukbench00120_resize.jpg\', 0.95676106),\n        ],\n        \'ukbench00120_resize.jpg\': [\n            (\'ukbench00120.jpg\', 0.98120844),\n            (\'ukbench00120_hflip.jpg\', 0.95676106),\n        ],\n        \'ukbench00120_rotation.jpg\': [],\n        \'ukbench09268.jpg\': [],\n    }\n\n    encodings = cnn.encode_images(TEST_IMAGE_DIR_MIXED)\n    duplicates = cnn.find_duplicates(\n        encoding_map=encodings, min_similarity_threshold=0.9, scores=True, outfile=False\n    )\n    # verify variable type\n    assert isinstance(duplicates[\'ukbench00120.jpg\'][0][1], np.float32)\n\n    # verify that all files have been considered for deduplication\n    assert len(duplicates) == len(expected_duplicates)\n\n    # verify for each file that expected files have been received as duplicates\n    for k in duplicates.keys():\n        dup_val = duplicates[k]\n        expected_val = expected_duplicates[k]\n        dup_ret = set(map(lambda x: x[0], dup_val))\n        expected_ret = set(map(lambda x: x[0], expected_val))\n        assert dup_ret == expected_ret\n\n\n# test find_duplicates_to_remove with directory path\ndef test_find_duplicates_to_remove_dir_integration(cnn):\n    duplicates_list = cnn.find_duplicates_to_remove(\n        image_dir=TEST_IMAGE_DIR_MIXED, min_similarity_threshold=0.9, outfile=False\n    )\n    assert set(duplicates_list) == set(\n        [\'ukbench00120_resize.jpg\', \'ukbench00120_hflip.jpg\']\n    )\n\n\n# test find_duplicates_to_remove with encoding map\ndef test_find_duplicates_to_remove_encoding_integration(cnn):\n    encodings = cnn.encode_images(TEST_IMAGE_DIR_MIXED)\n    duplicates_list = cnn.find_duplicates_to_remove(\n        encoding_map=encodings, min_similarity_threshold=0.9, outfile=False\n    )\n    assert set(duplicates_list) == set(\n        [\'ukbench00120_resize.jpg\', \'ukbench00120_hflip.jpg\']\n    )\n\n\n# test verbose\ndef test_encode_images_verbose_true(capsys):\n    cnn = CNN(verbose=True)\n    cnn.encode_images(image_dir=TEST_IMAGE_DIR)\n    out, err = capsys.readouterr()\n\n    assert \'[==============================]\' in out\n    assert \'\' == err\n\n\ndef test_encode_images_verbose_false(capsys):\n    cnn = CNN(verbose=False)\n    cnn.encode_images(image_dir=TEST_IMAGE_DIR)\n    out, err = capsys.readouterr()\n\n    assert \'\' == out\n    assert \'\' == err\n\n\ndef test_find_duplicates_verbose_true(capsys):\n    cnn = CNN(verbose=True)\n    cnn.find_duplicates(\n        image_dir=TEST_IMAGE_DIR,\n        min_similarity_threshold=0.8,\n        scores=False,\n        outfile=False,\n    )\n    out, err = capsys.readouterr()\n\n    assert \'[==============================]\' in out\n    assert \'\' == err\n\n\ndef test_find_duplicates_verbose_false(capsys):\n    cnn = CNN(verbose=False)\n    cnn.find_duplicates(\n        image_dir=TEST_IMAGE_DIR,\n        min_similarity_threshold=0.8,\n        scores=False,\n        outfile=False,\n    )\n    out, err = capsys.readouterr()\n\n    assert \'\' == out\n    assert \'\' == err\n\n\ndef test_scores_saving(cnn):\n    save_file = \'myduplicates.json\'\n    cnn.find_duplicates(\n        image_dir=TEST_IMAGE_DIR_MIXED,\n        min_similarity_threshold=0.6,\n        scores=True,\n        outfile=save_file,\n    )\n    with open(save_file, \'r\') as f:\n        saved_json = json.load(f)\n\n    assert len(saved_json) == 5  # all valid files present as keys\n    assert len(saved_json[\'ukbench00120.jpg\']) == 3  # file with duplicates have all entries\n    assert len(saved_json[\'ukbench09268.jpg\']) == 0  # file with no duplicates have no entries\n    assert isinstance(saved_json[\'ukbench00120.jpg\'], list)  # a list of files is returned\n    assert isinstance(saved_json[\'ukbench00120.jpg\'][0], list) # each entry in the duplicate list is a list (not a tuple, since json can\'t save tuples)\n    assert isinstance(saved_json[\'ukbench00120.jpg\'][0][1], float) # saved score is of type \'float\'\n\n    os.remove(save_file)  # clean up\n\n'"
tests/test_data_generator.py,0,"b""import pytest\nfrom pathlib import Path\n\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\n\nfrom imagededup.utils.data_generator import DataGenerator\n\np = Path(__file__)\nIMAGE_DIR = p.parent / 'data/base_images'\nFORMATS_IMAGE_DIR = p.parent / 'data/formats_images'\n\nTEST_BATCH_SIZE = 3\nTEST_TARGET_SIZE = (224, 224)\n\n\n@pytest.fixture(autouse=True)\ndef run_before_tests():\n    global generator\n    generator = DataGenerator(\n        image_dir=IMAGE_DIR,\n        batch_size=TEST_BATCH_SIZE,\n        basenet_preprocess=preprocess_input,\n        target_size=TEST_TARGET_SIZE,\n    )\n\n\ndef test__init():\n    assert generator.image_dir == IMAGE_DIR\n    assert generator.batch_size == TEST_BATCH_SIZE\n    assert generator.target_size == TEST_TARGET_SIZE\n    assert generator.basenet_preprocess == preprocess_input\n\n\ndef test__len():\n    assert generator.__len__() == 4\n\n\ndef test__get_item(mocker):\n    mocker.patch.object(DataGenerator, '_data_generator')\n\n    generator.__getitem__(0)\n\n    generator._data_generator.assert_called_with(\n        [\n            IMAGE_DIR / 'ukbench00120.jpg',\n            IMAGE_DIR / 'ukbench01380.jpg',\n            IMAGE_DIR / 'ukbench08976.jpg',\n        ]\n    )\n\n\ndef test__data_generator():\n    image_files = [\n        IMAGE_DIR / 'ukbench09348.jpg',\n        IMAGE_DIR / 'ukbench09012.jpg',\n        IMAGE_DIR / 'ukbench09380.jpg',\n    ]\n\n    result = generator._data_generator(image_files=image_files)\n\n    assert result.shape == tuple([TEST_BATCH_SIZE, *TEST_TARGET_SIZE, 3])\n\n\ndef test_on_epoch_end_1():\n    generator.on_epoch_end()\n\n    assert generator.valid_image_files == sorted(\n        [x for x in IMAGE_DIR.glob('*') if x.is_file()]\n    )\n\n\ndef test_on_epoch_end_2():\n    expected = [\n        FORMATS_IMAGE_DIR / 'baboon.pgm',\n        FORMATS_IMAGE_DIR / 'copyleft.tiff',\n        FORMATS_IMAGE_DIR / 'giphy.gif',\n        FORMATS_IMAGE_DIR / 'Iggy.1024.ppm',\n        FORMATS_IMAGE_DIR / 'marbles.pbm',\n        FORMATS_IMAGE_DIR / 'mpo_image.MPO',\n        FORMATS_IMAGE_DIR / 'ukbench09380.bmp',\n        FORMATS_IMAGE_DIR / 'ukbench09380.jpeg',\n        FORMATS_IMAGE_DIR / 'ukbench09380.png',\n        FORMATS_IMAGE_DIR / 'ukbench09380.svg',\n    ]\n\n    generator = DataGenerator(\n        image_dir=FORMATS_IMAGE_DIR,\n        batch_size=TEST_BATCH_SIZE,\n        basenet_preprocess=preprocess_input,\n        target_size=TEST_TARGET_SIZE,\n    )\n\n    generator.__getitem__(0)\n    generator.__getitem__(1)\n    generator.__getitem__(2)\n\n    generator.on_epoch_end()\n    assert sorted(generator.valid_image_files, key=lambda x: str(x).lower()) == expected\n"""
tests/test_evaluator.py,0,"b'import pytest\n\nimport numpy as np\n\nfrom imagededup.evaluation.evaluation import (\n    evaluate,\n    _check_map_correctness,\n    _transpose_checker,\n)\n\n\ndef return_ground_all_correct_retrievals():\n    ground_truth = {\'1\': [\'2\', \'3\', \'4\'], \'2\': [\'1\', \'3\'], \'3\': [\'1\', \'2\'], \'4\': [\'1\']}\n    retrieved = ground_truth\n    return ground_truth, retrieved\n\n\ndef return_ground_incorrect_retrievals():\n    ground_truth = {\'1\': [\'2\', \'3\', \'4\'], \'2\': [\'1\', \'3\'], \'3\': [\'1\', \'2\'], \'4\': [\'1\']}\n    retrieved = {\'1\': [\'2\', \'3\'], \'2\': [\'1\'], \'3\': [\'1\'], \'4\': []}\n    return ground_truth, retrieved\n\n\ndef test__transpose_checker():\n    mapping_non_transpose = {\n        \'1\': [\'2\', \'3\', \'4\'],\n        \'2\': [\'1\', \'3\'],\n        \'3\': [\'1\', \'2\'],\n        \'4\': [],\n    }\n    with pytest.raises(AssertionError):\n        _transpose_checker(mapping_non_transpose)\n\n\ndef test__check_map_correctness_extra_gt_vals():\n    ground_truth_map, retrieved_map = return_ground_all_correct_retrievals()\n    ground_truth_map[\'1\'].append(\'20\')\n    with pytest.raises(AssertionError):\n        _check_map_correctness(ground_truth_map, retrieved_map)\n\n\ndef test__check_map_correctness_extra_ret_vals():\n    ground_truth_map, retrieved_map = return_ground_all_correct_retrievals()\n    retrieved_map[\'1\'].append(\'20\')\n    with pytest.raises(AssertionError):\n        _check_map_correctness(ground_truth_map, retrieved_map)\n\n\ndef test__check_map_correctness_different_keys():\n    ground_truth_map = {\'1\': [\'2\'], \'2\': [\'1\']}\n    retrieve_map = {\'2\': [\'3\'], \'3\': [\'2\']}\n    with pytest.raises(Exception):\n        _check_map_correctness(ground_truth_map, retrieve_map)\n\n\ndef test_default_returns_all_metrics(mocker):\n    ground_truth_map, retrieve_map = return_ground_all_correct_retrievals()\n    get_all_metrics_mocker = mocker.patch(\n        \'imagededup.evaluation.evaluation.get_all_metrics\'\n    )\n    classification_metrics_mocker = mocker.patch(\n        \'imagededup.evaluation.evaluation.classification_metrics\'\n    )\n    classification_metrics_mocker = mocker.patch(\n        \'imagededup.evaluation.evaluation.classification_metrics\'\n    )\n    evaluate(ground_truth_map, retrieve_map)\n    get_all_metrics_mocker.assert_called_once_with(ground_truth_map, retrieve_map)\n    classification_metrics_mocker.assert_called_once_with(\n        ground_truth_map, retrieve_map\n    )\n\n\ndef test_wrong_metric_raises_valueerror():\n    ground_truth_map, retrieve_map = return_ground_all_correct_retrievals()\n    with pytest.raises(ValueError):\n        evaluate(ground_truth_map, retrieve_map, metric=\'bla\')\n\n\n@pytest.mark.parametrize(\'metric_name\', [\'map\', \'ndcg\', \'jaccard\'])\ndef test_correct_call_to_mean_metric(mocker, metric_name):\n    ground_truth_map, retrieve_map = return_ground_all_correct_retrievals()\n    mean_metric_mocker = mocker.patch(\'imagededup.evaluation.evaluation.mean_metric\')\n    evaluate(ground_truth_map, retrieve_map, metric=metric_name)\n    mean_metric_mocker.assert_called_once_with(\n        ground_truth_map, retrieve_map, metric=metric_name\n    )\n\n\ndef test_correct_call_to_classification_metric(mocker):\n    ground_truth_map, retrieve_map = return_ground_all_correct_retrievals()\n    classification_metrics_mocker = mocker.patch(\n        \'imagededup.evaluation.evaluation.classification_metrics\'\n    )\n    evaluate(ground_truth_map, retrieve_map, metric=\'classification\')\n    classification_metrics_mocker.assert_called_once_with(\n        ground_truth_map, retrieve_map\n    )\n\n\n@pytest.mark.parametrize(\'metric_name\', [\'MAP\', \'Ndcg\', \'JacCard\'])\ndef test_correct_call_to_mean_metric_mixed_cases(mocker, metric_name):\n    ground_truth_map, retrieve_map = return_ground_all_correct_retrievals()\n    mean_metric_mocker = mocker.patch(\'imagededup.evaluation.evaluation.mean_metric\')\n    evaluate(ground_truth_map, retrieve_map, metric=metric_name)\n    mean_metric_mocker.assert_called_once_with(\n        ground_truth_map, retrieve_map, metric=metric_name.lower()\n    )\n\n\ndef test_correct_call_to_classification_metric_mixed_case(mocker):\n    ground_truth_map, retrieve_map = return_ground_all_correct_retrievals()\n    classification_metrics_mocker = mocker.patch(\n        \'imagededup.evaluation.evaluation.classification_metrics\'\n    )\n    evaluate(ground_truth_map, retrieve_map, metric=\'Classification\')\n    classification_metrics_mocker.assert_called_once_with(\n        ground_truth_map, retrieve_map\n    )\n\n\n# Integration tests\n\n\n@pytest.mark.parametrize(\n    \'metric, expected_value\',\n    [(\'map\', 0.41666666666666663), (\'ndcg\', 0.75), (\'jaccard\', 0.41666666666666663)],\n)\ndef test_correct_values_ir(metric, expected_value):\n    """"""Tests if correct MAP values are computed\n    Load ground truth and dict for incorrect map prediction to have a Map less than 1.0""""""\n    ground_truth, retrieved = return_ground_incorrect_retrievals()\n    score = evaluate(ground_truth, retrieved, metric=metric)\n    assert isinstance(score, dict)\n    assert list(score.keys())[0] == metric\n    assert score[metric] == expected_value\n\n\ndef test_correct_values_classification():\n    ground_truth, retrieved = return_ground_incorrect_retrievals()\n    expected_return = {\n        \'precision\': np.array([0.5, 1.0]),\n        \'recall\': np.array([1.0, 0.5]),\n        \'f1_score\': np.array([0.66666667, 0.66666667]),\n        \'support\': np.array([2, 4]),\n    }\n    score = evaluate(ground_truth, retrieved, metric=\'classification\')\n    assert isinstance(score, dict)\n    assert set(score.keys()) == set([\'precision\', \'recall\', \'f1_score\', \'support\'])\n    for k, v in score.items():\n        assert isinstance(v, np.ndarray)\n        np.testing.assert_almost_equal(score[k], expected_return[k])\n\n\ndef test_correct_values_all():\n    ground_truth, retrieved = return_ground_incorrect_retrievals()\n    score = evaluate(ground_truth, retrieved)\n    assert isinstance(score, dict)\n    assert set(score.keys()) == set(\n        [\'map\', \'ndcg\', \'jaccard\', \'precision\', \'recall\', \'f1_score\', \'support\']\n    )\n    for v in score.values():\n        assert v is not None\n'"
tests/test_general_utils.py,0,"b""import os\nimport json\n\nimport numpy as np\n\nfrom imagededup.utils import general_utils\n\n\ndef test_get_files_to_remove():\n    from collections import OrderedDict\n\n    dict_a = OrderedDict({'1': ['2'], '2': ['1', '3'], '3': ['4'], '4': ['3'], '5': []})\n    dups_to_remove = general_utils.get_files_to_remove(dict_a)\n    assert set(dups_to_remove) == set(['2', '4'])\n\n\ndef test_correct_saving_floats():\n    res = {\n        'image1.jpg': [\n            ('image1_duplicate1.jpg', np.float16(0.324)),\n            ('image1_duplicate2.jpg', np.float16(0.324)),\n        ],\n        'image2.jpg': [],\n        'image3.jpg': [('image1_duplicate1.jpg', np.float32(0.324))],\n    }\n    save_file = 'myduplicates.json'\n    general_utils.save_json(results=res, filename=save_file, float_scores=True)\n    with open(save_file, 'r') as f:\n        saved_json = json.load(f)\n\n    assert len(saved_json) == 3  # all valid files present as keys\n    assert isinstance(\n        saved_json['image1.jpg'][0][1], float\n    )  # saved score is of type 'float' for np.float16 score\n    assert isinstance(\n        saved_json['image3.jpg'][0][1], float\n    )  # saved score is of type 'float' for np.float32 score\n\n    os.remove(save_file)  # clean up\n\n\ndef test_correct_saving_ints():\n    res = {\n        'image1.jpg': [('image1_duplicate1.jpg', 2), ('image1_duplicate2.jpg', 22)],\n        'image2.jpg': [],\n        'image3.jpg': [('image1_duplicate1.jpg', 43)],\n    }\n    save_file = 'myduplicates.json'\n    general_utils.save_json(results=res, filename=save_file)\n    with open(save_file, 'r') as f:\n        saved_json = json.load(f)\n\n    assert len(saved_json) == 3  # all valid files present as keys\n    assert isinstance(\n        saved_json['image1.jpg'][0][1], int\n    )  # saved score is of type 'int'\n\n    os.remove(save_file)  # clean up\n"""
tests/test_hashing.py,0,"b""import os\nimport sys\nfrom pathlib import Path\nfrom PIL import Image\n\nimport pytest\nimport numpy as np\n\nfrom imagededup.methods.hashing import Hashing, PHash, DHash, AHash, WHash\n\np = Path(__file__)\n\nPATH_IMAGE_DIR = p.parent / 'data/mixed_images'\nPATH_IMAGE_DIR_STRING = os.path.join(os.getcwd(), 'tests/data/mixed_images')\nPATH_SINGLE_IMAGE = p.parent / 'data/mixed_images/ukbench00120.jpg'\nPATH_SINGLE_IMAGE_STRING = p.parent / 'data/mixed_images/ukbench00120.jpg'\nPATH_SINGLE_IMAGE_CORRUPT = p.parent / 'data/mixed_images/ukbench09268_corrupt.jpg'\nPATH_SINGLE_IMAGE_RESIZED = p.parent / 'data/mixed_images/ukbench00120_resize.jpg'\n\n\n# Test parent class (static methods/class attributes initialization)\n\n\n@pytest.fixture\ndef hasher():\n    hashobj = Hashing()\n    return hashobj\n\n\ndef test_correct_init_hashing(hasher):\n    assert hasher.target_size == (8, 8)\n\n\ndef test_hamming_distance(hasher):\n    # Put two numbers and check if hamming distance is correct\n    number_1 = '1a'\n    number_2 = '1f'\n    hamdist = hasher.hamming_distance(number_1, number_2)\n    assert hamdist == 2\n\n\ndef test__array_to_hash(hasher):\n    hash_mat = np.array(\n        [1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n    )\n    assert hasher._array_to_hash(hash_mat) == '9191fa'\n\n\ndef test__check_hamming_distance_bounds_input_not_int(hasher):\n    with pytest.raises(TypeError):\n        hasher._check_hamming_distance_bounds(thresh=1.0)\n\n\ndef test__check_hamming_distance_bounds_out_of_bound(hasher):\n    with pytest.raises(ValueError):\n        hasher._check_hamming_distance_bounds(thresh=68)\n\n\ndef test__check_hamming_distance_bounds_correct(hasher):\n    assert hasher._check_hamming_distance_bounds(thresh=20) is None\n\n\n# encode_image\n\n\n@pytest.fixture\ndef mocker_preprocess_image(mocker):\n    ret_val = np.zeros((2, 2))\n    preprocess_image_mocker = mocker.patch(\n        'imagededup.methods.hashing.preprocess_image', return_value=ret_val\n    )\n    return preprocess_image_mocker\n\n\n@pytest.fixture\ndef mocker_hash_func(mocker):\n    ret_val = np.zeros((2, 2))\n    hash_func_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing._hash_func', return_value=ret_val\n    )\n    return hash_func_mocker\n\n\n@pytest.fixture\ndef mocker_load_image(mocker):\n    ret_val = np.zeros((2, 2))\n    load_image_mocker = mocker.patch(\n        'imagededup.methods.hashing.load_image', return_value=ret_val, autospec=True\n    )\n    return load_image_mocker\n\n\ndef test_encode_image_accepts_image_posixpath(\n    hasher, mocker_load_image, mocker_hash_func\n):\n    ret_val = np.zeros((2, 2))\n    hasher.encode_image(image_file=PATH_SINGLE_IMAGE)\n    mocker_load_image.assert_called_with(\n        image_file=PATH_SINGLE_IMAGE, grayscale=True, target_size=(8, 8)\n    )\n    np.testing.assert_array_equal(ret_val, mocker_hash_func.call_args[0][0])\n\n\ndef test_encode_image_accepts_numpy_array(\n    hasher, mocker_preprocess_image, mocker_hash_func\n):\n    ret_val = np.zeros((2, 2))\n    hasher.encode_image(image_array=ret_val)\n    mocker_preprocess_image.assert_called_with(\n        image=ret_val, target_size=(8, 8), grayscale=True\n    )\n    np.testing.assert_array_equal(ret_val, mocker_hash_func.call_args[0][0])\n\n\ndef test_encode_image_valerror_wrong_input(hasher):\n    pil_im = Image.open(PATH_SINGLE_IMAGE)\n    with pytest.raises(ValueError):\n        hasher.encode_image(image_file=pil_im)\n\n\ndef test_encode_image_valerror_wrong_input_array(hasher):\n    pil_im = Image.open(PATH_SINGLE_IMAGE)\n    with pytest.raises(ValueError):\n        hasher.encode_image(image_array=pil_im)\n\n\ndef test_encode_image_returns_none_image_pp_not_array(hasher, mocker):\n    mocker.patch('imagededup.methods.hashing.load_image', return_value=None)\n    assert hasher.encode_image(PATH_SINGLE_IMAGE) is None\n\n\ndef test_encode_image_returns_none_image_pp_not_array_array_input(hasher, mocker):\n    mocker.patch('imagededup.methods.hashing.preprocess_image', return_value=None)\n    assert hasher.encode_image(image_array=np.zeros((2, 2))) is None\n\n\ndef test_encode_image_accepts_non_posixpath(\n    hasher, mocker_load_image, mocker_hash_func\n):\n    ret_val = np.zeros((2, 2))\n    hasher.encode_image(image_file=PATH_SINGLE_IMAGE_STRING)\n    mocker_load_image.assert_called_with(\n        image_file=PATH_SINGLE_IMAGE, grayscale=True, target_size=(8, 8)\n    )\n    np.testing.assert_array_equal(ret_val, mocker_hash_func.call_args[0][0])\n\n\n# _encoder\n\n\n@pytest.fixture\ndef mocker_encode_image(mocker):\n    mocker.patch(\n        'imagededup.methods.hashing.parallelise', return_value='123456789ABCDEFA'\n    )\n\n\n# encode_images\n\n\ndef test_encode_images_accepts_valid_posixpath(hasher, mocker_encode_image):\n    assert len(hasher.encode_images(PATH_IMAGE_DIR)) == 6  # 6 files in the directory\n\n\ndef test_encode_images_accepts_non_posixpath(hasher, mocker_encode_image):\n    assert len(hasher.encode_images(PATH_IMAGE_DIR_STRING)) == 6\n\n\ndef test_encode_images_rejects_non_directory_paths(hasher):\n    with pytest.raises(ValueError):\n        hasher.encode_images(PATH_SINGLE_IMAGE)\n\n\ndef test_encode_images_return_vals(hasher, mocker_encode_image):\n    encoded_val = '123456789ABCDEFA'\n    hashes = hasher.encode_images(PATH_IMAGE_DIR)\n    assert isinstance(hashes, dict)\n    assert list(hashes.values())[0] == encoded_val[0]\n    assert PATH_SINGLE_IMAGE.name in hashes.keys()\n\n\ndef test_hash_func(hasher, mocker):\n    inp_array = np.array((3, 3))\n    ret_arr = np.array((2, 2))\n    hash_algo_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing._hash_algo', return_value=ret_arr\n    )\n    array_mocker = mocker.patch('imagededup.methods.hashing.Hashing._array_to_hash')\n    hasher._hash_func(inp_array)\n    np.testing.assert_array_equal(inp_array, hash_algo_mocker.call_args[0][0])\n    array_mocker.assert_called_with(ret_arr)\n\n\n# _find_duplicates_dict\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='Does not run on Windows.')\ndef test__find_duplicates_dict_outfile_none(mocker):\n    encoding_map = {'1.jpg': '123456'}\n    threshold = 10\n    scores = True\n    outfile = None\n    verbose = False\n    myhasher = PHash(verbose=verbose)\n    hasheval_mocker = mocker.patch('imagededup.methods.hashing.HashEval')\n    save_json_mocker = mocker.patch('imagededup.methods.hashing.save_json')\n    myhasher._find_duplicates_dict(\n        encoding_map=encoding_map,\n        max_distance_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n    )\n    hasheval_mocker.assert_called_with(\n        test=encoding_map,\n        queries=encoding_map,\n        distance_function=Hashing.hamming_distance,\n        verbose=verbose,\n        threshold=threshold,\n        search_method='brute_force_cython',\n    )\n    hasheval_mocker.return_value.retrieve_results.assert_called_once_with(scores=scores)\n    save_json_mocker.assert_not_called()\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='Does not run on Windows.')\ndef test__find_duplicates_dict_outfile_none_verbose(hasher, mocker):\n    encoding_map = {'1.jpg': '123456'}\n    threshold = 10\n    scores = True\n    outfile = None\n    hasheval_mocker = mocker.patch('imagededup.methods.hashing.HashEval')\n    save_json_mocker = mocker.patch('imagededup.methods.hashing.save_json')\n    hasher._find_duplicates_dict(\n        encoding_map=encoding_map,\n        max_distance_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n    )\n    hasheval_mocker.assert_called_with(\n        test=encoding_map,\n        queries=encoding_map,\n        distance_function=Hashing.hamming_distance,\n        verbose=True,\n        threshold=threshold,\n        search_method='brute_force_cython',\n    )\n    hasheval_mocker.return_value.retrieve_results.assert_called_once_with(scores=scores)\n    save_json_mocker.assert_not_called()\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='Does not run on Windows.')\ndef test__find_duplicates_dict_outfile_true(hasher, mocker):\n    encoding_map = {'1.jpg': '123456'}\n    threshold = 10\n    scores = True\n    outfile = True\n    verbose = True\n    hasheval_mocker = mocker.patch('imagededup.methods.hashing.HashEval')\n    hasheval_mocker.return_value.retrieve_results.return_value = {\n        'filename.jpg': [('dup1.jpg', 3)],\n        'filename2.jpg': [('dup2.jpg', 10)],\n    }\n    save_json_mocker = mocker.patch('imagededup.methods.hashing.save_json')\n    hasher._find_duplicates_dict(\n        encoding_map=encoding_map,\n        max_distance_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n    )\n    hasheval_mocker.assert_called_with(\n        test=encoding_map,\n        queries=encoding_map,\n        distance_function=Hashing.hamming_distance,\n        verbose=verbose,\n        threshold=threshold,\n        search_method='brute_force_cython',\n    )\n    hasheval_mocker.return_value.retrieve_results.assert_called_once_with(scores=scores)\n    save_json_mocker.assert_called_once_with(\n        hasheval_mocker.return_value.retrieve_results.return_value, outfile\n    )\n\n\n# _find_duplicates_dir\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='Does not run on Windows.')\ndef test__find_duplicates_dir(hasher, mocker):\n    encoding_map = {'1.jpg': '123456'}\n    threshold = 10\n    scores = True\n    outfile = True\n    ret_val_find_dup_dict = {\n        'filename.jpg': [('dup1.jpg', 3)],\n        'filename2.jpg': [('dup2.jpg', 10)],\n    }\n    encode_images_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing.encode_images', return_value=encoding_map\n    )\n    find_dup_dict_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing._find_duplicates_dict',\n        return_value=ret_val_find_dup_dict,\n    )\n    hasher._find_duplicates_dir(\n        image_dir=PATH_IMAGE_DIR,\n        max_distance_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n        search_method='brute_force_cython',\n    )\n    encode_images_mocker.assert_called_once_with(PATH_IMAGE_DIR)\n    find_dup_dict_mocker.assert_called_once_with(\n        encoding_map=encoding_map,\n        max_distance_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n        search_method='brute_force_cython',\n    )\n\n\n# find_duplicates\n\n\n@pytest.fixture\ndef mocker_hamming_distance(mocker):\n    return mocker.patch(\n        'imagededup.methods.hashing.Hashing._check_hamming_distance_bounds'\n    )\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='Does not run on Windows.')\ndef test_find_duplicates_dir(hasher, mocker, mocker_hamming_distance):\n    threshold = 10\n    scores = True\n    outfile = True\n    find_dup_dir_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing._find_duplicates_dir'\n    )\n    hasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR,\n        max_distance_threshold=threshold,\n        outfile=outfile,\n        scores=scores,\n        search_method='brute_force_cython',\n    )\n    mocker_hamming_distance.assert_called_once_with(thresh=threshold)\n    find_dup_dir_mocker.assert_called_once_with(\n        image_dir=PATH_IMAGE_DIR,\n        max_distance_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n        search_method='brute_force_cython',\n    )\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='Does not run on Windows.')\ndef test_find_duplicates_dict(hasher, mocker, mocker_hamming_distance):\n    encoding_map = {'1.jpg': '123456'}\n    threshold = 10\n    scores = True\n    outfile = True\n    find_dup_dict_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing._find_duplicates_dict'\n    )\n    hasher.find_duplicates(\n        encoding_map=encoding_map,\n        max_distance_threshold=threshold,\n        outfile=outfile,\n        scores=scores,\n        search_method='brute_force_cython',\n    )\n    mocker_hamming_distance.assert_called_once_with(thresh=threshold)\n    find_dup_dict_mocker.assert_called_once_with(\n        encoding_map=encoding_map,\n        max_distance_threshold=threshold,\n        scores=scores,\n        outfile=outfile,\n        search_method='brute_force_cython',\n    )\n\n\ndef test_find_duplicates_wrong_input(hasher):\n    with pytest.raises(ValueError):\n        hasher.find_duplicates(max_distance_threshold=10)\n\n\n# find_duplicates_to_remove\n\n\ndef test_find_duplicates_to_remove_outfile_false(hasher, mocker):\n    threshold = 10\n    outfile = False\n    ret_val_find_dup_dict = {\n        'filename.jpg': [('dup1.jpg', 3)],\n        'filename2.jpg': [('dup2.jpg', 10)],\n    }\n    find_duplicates_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing.find_duplicates',\n        return_value=ret_val_find_dup_dict,\n    )\n    get_files_to_remove_mocker = mocker.patch(\n        'imagededup.methods.hashing.get_files_to_remove'\n    )\n    save_json_mocker = mocker.patch('imagededup.methods.hashing.save_json')\n    hasher.find_duplicates_to_remove(\n        image_dir=PATH_IMAGE_DIR, max_distance_threshold=threshold, outfile=outfile\n    )\n    find_duplicates_mocker.assert_called_once_with(\n        image_dir=PATH_IMAGE_DIR,\n        encoding_map=None,\n        max_distance_threshold=threshold,\n        scores=False,\n    )\n    get_files_to_remove_mocker.assert_called_once_with(ret_val_find_dup_dict)\n    save_json_mocker.assert_not_called()\n\n\ndef test_find_duplicates_to_remove_outfile_true(hasher, mocker):\n    threshold = 10\n    outfile = True\n    ret_val_find_dup_dict = {\n        'filename.jpg': [('dup1.jpg', 3)],\n        'filename2.jpg': [('dup2.jpg', 10)],\n    }\n    ret_val_get_files_to_remove = ['1.jpg', '2.jpg']\n    find_duplicates_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing.find_duplicates',\n        return_value=ret_val_find_dup_dict,\n    )\n    get_files_to_remove_mocker = mocker.patch(\n        'imagededup.methods.hashing.get_files_to_remove',\n        return_value=ret_val_get_files_to_remove,\n    )\n    save_json_mocker = mocker.patch('imagededup.methods.hashing.save_json')\n    hasher.find_duplicates_to_remove(\n        image_dir=PATH_IMAGE_DIR, max_distance_threshold=threshold, outfile=outfile\n    )\n    find_duplicates_mocker.assert_called_once_with(\n        image_dir=PATH_IMAGE_DIR,\n        encoding_map=None,\n        max_distance_threshold=threshold,\n        scores=False,\n    )\n    get_files_to_remove_mocker.assert_called_once_with(ret_val_find_dup_dict)\n    save_json_mocker.assert_called_once_with(ret_val_get_files_to_remove, outfile)\n\n\ndef test_find_duplicates_to_remove_encoding_map(hasher, mocker):\n    encoding_map = {'1.jpg': '123456'}\n    threshold = 10\n    outfile = False\n    ret_val_find_dup_dict = {\n        'filename.jpg': [('dup1.jpg', 3)],\n        'filename2.jpg': [('dup2.jpg', 10)],\n    }\n    find_duplicates_mocker = mocker.patch(\n        'imagededup.methods.hashing.Hashing.find_duplicates',\n        return_value=ret_val_find_dup_dict,\n    )\n    get_files_to_remove_mocker = mocker.patch(\n        'imagededup.methods.hashing.get_files_to_remove'\n    )\n    save_json_mocker = mocker.patch('imagededup.methods.hashing.save_json')\n    hasher.find_duplicates_to_remove(\n        encoding_map=encoding_map, max_distance_threshold=threshold, outfile=outfile\n    )\n    find_duplicates_mocker.assert_called_once_with(\n        encoding_map=encoding_map,\n        image_dir=None,\n        max_distance_threshold=threshold,\n        scores=False,\n    )\n    get_files_to_remove_mocker.assert_called_once_with(ret_val_find_dup_dict)\n    save_json_mocker.assert_not_called()\n\n\n# Integration tests\n\nphasher = PHash()\ndhasher = DHash()\nahasher = AHash()\nwhasher = WHash()\n\ncommon_test_parameters = [\n    phasher.encode_image,\n    dhasher.encode_image,\n    ahasher.encode_image,\n    whasher.encode_image,\n]\n\n\n@pytest.mark.parametrize('hash_function', common_test_parameters)\nclass TestCommon:\n    def test_len_hash(self, hash_function):\n        hash_im = hash_function(PATH_SINGLE_IMAGE)\n        assert len(hash_im) == 16\n\n    def test_hash_resize(self, hash_function):\n        # Resize one image to (300, 300) and check that hamming distance between hashes is not too large\n        hash_im_1 = hash_function(PATH_SINGLE_IMAGE)\n        hash_im_2 = hash_function(PATH_SINGLE_IMAGE_RESIZED)\n        hamdist = Hashing.hamming_distance(hash_im_1, hash_im_2)\n        assert hamdist < 3\n\n    def test_hash_small_rotation(self, hash_function):\n        # Rotate image slightly (1 degree) and check that hamming distance between hashes is not too large\n        orig_image = Image.open(PATH_SINGLE_IMAGE)\n        rotated_image = np.array(orig_image.rotate(1))\n        hash_im_1 = hash_function(image_array=np.array(orig_image))\n        hash_im_2 = hash_function(image_array=rotated_image)\n        hamdist = Hashing.hamming_distance(hash_im_1, hash_im_2)\n        assert hamdist < 3\n\n    def test_hash_distinct_images(self, hash_function):\n        # Put in distinct images and check that hamming distance between hashes is large\n        hash_im_1 = hash_function(PATH_SINGLE_IMAGE)\n        hash_im_2 = hash_function(p.parent / 'data/mixed_images/ukbench09268.jpg')\n        hamdist = Hashing.hamming_distance(hash_im_1, hash_im_2)\n        assert hamdist > 20\n\n    def test_same_hashes_with_different_inputs(self, hash_function):\n        arr_inp = np.array(Image.open(PATH_SINGLE_IMAGE))\n        assert hash_function(image_array=arr_inp) == hash_function(PATH_SINGLE_IMAGE)\n\n\ndef test_encode_images_returns_dict():\n    hash_dict = phasher.encode_images(PATH_IMAGE_DIR)\n    assert isinstance(hash_dict, dict)\n\n\ndef test_encode_images_return_non_none_hashes():\n    hash_dict = dhasher.encode_images(PATH_IMAGE_DIR)\n    for v in hash_dict.values():\n        assert v is not None\n\n\n# For each of the hash types, check correctness of hashes for known images\n# Check encode_image(s)\n\n\n@pytest.mark.parametrize(\n    'hash_object, expected_hash',\n    [\n        (phasher, '9fee256239984d71'),\n        (dhasher, '2b69707551f1b87a'),\n        (ahasher, '81b8bc3c3c3c1e0a'),\n        (whasher, '89b8bc3c3c3c5e0e'),\n    ],\n)\ndef test_encode_image_hash(hash_object, expected_hash):\n    assert hash_object.encode_image(PATH_SINGLE_IMAGE) == expected_hash\n\n\ndef test_encode_image_corrupt_file():\n    whasher = WHash()\n    assert whasher.encode_image(PATH_SINGLE_IMAGE_CORRUPT) is None\n\n\ndef test_encode_images_corrupt_and_good_images():\n    ahasher = AHash()\n    hashes = ahasher.encode_images(PATH_IMAGE_DIR)\n    assert len(hashes) == 5  # 5 non-corrupt files in the directory, 1 corrupt\n    assert isinstance(hashes, dict)\n\n\ndef test_find_duplicates_correctness():\n    phasher = PHash()\n    duplicate_dict = phasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR, max_distance_threshold=10\n    )\n    assert isinstance(duplicate_dict, dict)\n    assert isinstance(list(duplicate_dict.values())[0], list)\n    assert len(duplicate_dict['ukbench09268.jpg']) == 0\n    assert duplicate_dict['ukbench00120.jpg'] == ['ukbench00120_resize.jpg']\n\n\ndef test_find_duplicates_correctness_score():\n    phasher = PHash()\n    duplicate_dict = phasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR, max_distance_threshold=10, scores=True\n    )\n    assert isinstance(duplicate_dict, dict)\n    duplicates = list(duplicate_dict.values())\n    assert isinstance(duplicates[0], list)\n    assert isinstance(duplicates[0][0], tuple)\n    assert duplicate_dict['ukbench09268.jpg'] == []\n    assert duplicate_dict['ukbench00120.jpg'] == [('ukbench00120_resize.jpg', 0)]\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='Does not run on Windows.')\ndef test_find_duplicates_clearing():\n    phasher = PHash()\n    duplicate_dict = phasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR,\n        max_distance_threshold=10,\n        scores=True,\n        search_method='brute_force_cython',\n    )\n\n    duplicate_dict = phasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR,\n        max_distance_threshold=10,\n        scores=True,\n        search_method='brute_force_cython',\n    )\n\n    assert isinstance(duplicate_dict, dict)\n    duplicates = list(duplicate_dict.values())\n    assert isinstance(duplicates[0], list)\n    assert isinstance(duplicates[0][0], tuple)\n    assert duplicate_dict['ukbench09268.jpg'] == []\n    assert duplicate_dict['ukbench00120.jpg'] == [('ukbench00120_resize.jpg', 0)]\n\n\ndef test_find_duplicates_outfile():\n    dhasher = DHash()\n    outfile_name = 'score_output.json'\n    if os.path.exists(outfile_name):\n        os.remove(outfile_name)\n    _ = dhasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR,\n        max_distance_threshold=10,\n        scores=True,\n        outfile=outfile_name,\n    )\n    assert os.path.exists(outfile_name)\n    # clean up\n    if os.path.exists(outfile_name):\n        os.remove(outfile_name)\n\n\ndef test_find_duplicates_encoding_map_input():\n    encoding = {\n        'ukbench00120_resize.jpg': '9fee256239984d71',\n        'ukbench00120_rotation.jpg': '850d513c4fdcbb72',\n        'ukbench00120.jpg': '9fee256239984d71',\n        'ukbench00120_hflip.jpg': 'cabb7237e8cd3824',\n        'ukbench09268.jpg': 'c73c36c2da2f29c9',\n    }\n    phasher = PHash()\n    duplicate_dict = phasher.find_duplicates(\n        encoding_map=encoding, max_distance_threshold=10\n    )\n    assert isinstance(duplicate_dict, dict)\n    assert isinstance(list(duplicate_dict.values())[0], list)\n    assert len(duplicate_dict['ukbench09268.jpg']) == 0\n    assert duplicate_dict['ukbench00120.jpg'] == ['ukbench00120_resize.jpg']\n\n\ndef test_find_duplicates_to_remove_dir():\n    phasher = PHash()\n    removal_list = phasher.find_duplicates_to_remove(\n        image_dir=PATH_IMAGE_DIR, max_distance_threshold=10\n    )\n    assert isinstance(removal_list, list)\n    assert removal_list == ['ukbench00120.jpg'] or removal_list == [\n        'ukbench00120_resize.jpg'\n    ]\n\n\ndef test_find_duplicates_to_remove_encoding():\n    encoding = {\n        'ukbench00120_resize.jpg': '9fee256239984d71',\n        'ukbench00120_rotation.jpg': '850d513c4fdcbb72',\n        'ukbench00120.jpg': '9fee256239984d71',\n        'ukbench00120_hflip.jpg': 'cabb7237e8cd3824',\n        'ukbench09268.jpg': 'c73c36c2da2f29c9',\n    }\n    phasher = PHash()\n    removal_list = phasher.find_duplicates_to_remove(\n        encoding_map=encoding, max_distance_threshold=10\n    )\n    assert isinstance(removal_list, list)\n    assert removal_list == ['ukbench00120.jpg'] or removal_list == [\n        'ukbench00120_resize.jpg'\n    ]\n\n\ndef test_find_duplicates_to_remove_outfile():\n    dhasher = DHash()\n    outfile_name = 'removal_list.json'\n    if os.path.exists(outfile_name):\n        os.remove(outfile_name)\n    _ = dhasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR, max_distance_threshold=10, outfile=outfile_name\n    )\n    assert os.path.exists(outfile_name)\n    # clean up\n    if os.path.exists(outfile_name):\n        os.remove(outfile_name)\n\n\n# test verbose\ndef test_encode_images_verbose_true(capsys):\n    phasher = PHash(verbose=True)\n    phasher.encode_images(image_dir=PATH_IMAGE_DIR)\n    out, err = capsys.readouterr()\n\n    assert '%' in err\n    assert '' == out\n\n\ndef test_encode_images_verbose_false(capsys):\n    phasher = PHash(verbose=False)\n    phasher.encode_images(image_dir=PATH_IMAGE_DIR)\n    out, err = capsys.readouterr()\n\n    assert '' == err\n    assert '' == out\n\n\ndef test_find_duplicates_verbose_true(capsys):\n    phasher = PHash(verbose=True)\n    phasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR, max_distance_threshold=10, scores=False, outfile=False\n    )\n    out, err = capsys.readouterr()\n\n    assert '%' in err\n    assert '' == out\n\n\ndef test_find_duplicates_verbose_false(capsys):\n    phasher = PHash(verbose=False)\n    phasher.find_duplicates(\n        image_dir=PATH_IMAGE_DIR, max_distance_threshold=10, scores=False, outfile=False\n    )\n    out, err = capsys.readouterr()\n\n    assert '' == out\n    assert '' == err\n"""
tests/test_image_utils.py,0,"b""import pytest\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\n\nfrom imagededup.utils.image_utils import preprocess_image, load_image\n\np = Path(__file__)\nPATH_SINGLE_IMAGE = p.parent / 'data/mixed_images/ukbench00120.jpg'\n\n\ndef test_preprocess_image_accepts_array_input():\n    inp_x = Image.open(PATH_SINGLE_IMAGE)\n    inp_x = np.array(inp_x)\n    target_size = (2, 2)\n    ret_array = preprocess_image(inp_x, target_size=target_size, grayscale=True)\n    assert isinstance(ret_array, np.ndarray)\n    assert ret_array.shape == target_size\n\n\ndef test_preprocess_image_accepts_pil_input():\n    inp_x = Image.open(PATH_SINGLE_IMAGE)\n    target_size = (2, 2)\n    ret_array = preprocess_image(inp_x, target_size=target_size, grayscale=True)\n    assert isinstance(ret_array, np.ndarray)\n    assert ret_array.shape == target_size\n\n\ndef test_preprocess_image_wrong_input():\n    inp = 'test_string'\n    with pytest.raises(ValueError):\n        preprocess_image(inp, target_size=(2, 2))\n\n\ndef test_preprocess_image_grayscale_false():\n    inp_x = Image.open(PATH_SINGLE_IMAGE)\n    target_size = (2, 2)\n    ret_array = preprocess_image(inp_x, target_size=target_size, grayscale=False)\n    assert isinstance(ret_array, np.ndarray)\n    assert ret_array.shape == target_size + (3,)  # 3 for RGB\n\n\n# load_image\n\n\ndef test_load_image_accepts_pil(mocker):\n    preprocess_mocker = mocker.patch('imagededup.utils.image_utils.preprocess_image')\n    load_image(PATH_SINGLE_IMAGE)\n    preprocess_mocker.assert_called_once_with(\n        Image.open(PATH_SINGLE_IMAGE), target_size=None, grayscale=False\n    )\n\n\ndef test_load_image_returns_none_wrong_input():\n    inp = 'test_string'\n    assert load_image(inp) is None\n\n\n@pytest.fixture\ndef preprocess_mocker(mocker):\n    return mocker.patch('imagededup.utils.image_utils.preprocess_image')\n\n\ndef test_load_image_alpha_channel_image_converts(preprocess_mocker):\n    PATH_ALPHA_IMAGE = p.parent / 'data/alpha_channel_image.png'\n    alpha_converted = Image.open(PATH_ALPHA_IMAGE).convert('RGBA').convert('RGB')\n    load_image(PATH_ALPHA_IMAGE)\n    preprocess_mocker.assert_called_once_with(\n        alpha_converted, target_size=None, grayscale=False\n    )\n\n\ndef test_load_image_target_size_grayscale_true(preprocess_mocker):\n    load_image(image_file=PATH_SINGLE_IMAGE, target_size=(8, 8), grayscale=True)\n    preprocess_mocker.assert_called_once_with(\n        Image.open(PATH_SINGLE_IMAGE), target_size=(8, 8), grayscale=True\n    )\n\n\n# Integration test\n\n\ndef test_load_image_all_inputs_correct():\n    target_size = (8, 8)\n    loaded_image = load_image(\n        image_file=PATH_SINGLE_IMAGE, target_size=target_size, grayscale=True\n    )\n    assert isinstance(loaded_image, np.ndarray)\n    assert loaded_image.shape == target_size\n    assert np.issubdtype(\n        np.uint8, loaded_image.dtype\n    )  # return numpy array dtype is uint8\n"""
tests/test_information_retrieval.py,0,"b'import pickle\nimport pytest\nfrom pathlib import Path\n\nfrom imagededup.handlers.metrics.information_retrieval import *\n\np = Path(__file__)\n\nPATH_GROUND_TRUTH = p.parent / \'data/evaluation_files/ground_truth.pkl\'\nPATH_ALL_CORRECT_RETRIEVALS = (\n    p.parent / \'data/evaluation_files/all_correct_retrievals.pkl\'\n)\nPATH_INCORRECT_RETRIEVALS = p.parent / \'data/evaluation_files/incorrect_retrievals.pkl\'\n\n\ndef return_ground_all_correct_retrievals():\n    return load_pickle(PATH_GROUND_TRUTH), load_pickle(PATH_ALL_CORRECT_RETRIEVALS)\n\n\ndef return_ground_incorrect_retrievals():\n    return load_pickle(PATH_GROUND_TRUTH), load_pickle(PATH_INCORRECT_RETRIEVALS)\n\n\ndef load_pickle(filename):\n    """"""The path of the file below is set since the test suite is run using python -m pytest command from the image-dedup\n    directory""""""\n    with open(filename, \'rb\') as f:\n        dict_loaded = pickle.load(f)\n    return dict_loaded\n\n\ndef initialize_fake_data_retrieved_same():\n    """"""Number of retrievals = Number of ground truth retrievals""""""\n    corr_dup = [\'1.jpg\', \'2.jpg\', \'3.jpg\', \'4.jpg\']\n    ret_dups = [\'1.jpg\', \'33.jpg\', \'2.jpg\', \'4.jpg\']\n    return corr_dup, ret_dups\n\n\ndef initialize_fake_data_retrieved_less():\n    """"""Number of retrievals < Number of ground truth retrievals""""""\n    corr_dup = [\'1.jpg\', \'2.jpg\', \'3.jpg\', \'4.jpg\']\n    ret_dups = [\'1.jpg\', \'42.jpg\']\n    return corr_dup, ret_dups\n\n\ndef initialize_fake_data_retrieved_more():\n    """"""Number of retrievals > Number of ground truth retrievals""""""\n    corr_dup = [\'1.jpg\', \'2.jpg\', \'3.jpg\', \'4.jpg\']\n    ret_dups = [\'1.jpg\', \'42.jpg\', \'2.jpg\', \'3.jpg\', \'4.jpg\']\n    return corr_dup, ret_dups\n\n\n@pytest.mark.parametrize(\'metric_function, expected_value\', [(avg_prec, 0.6041666666666666), (ndcg, 0.9060254355346823),\n                                                        (jaccard_similarity, 0.6)])\ndef test_metrics_same_number_of_retrievals(metric_function, expected_value):\n    """"""Number of retrievals = Number of ground truth retrievals""""""\n    corr_dup, ret_dups = initialize_fake_data_retrieved_same()\n    avg_val = metric_function(corr_dup, ret_dups)\n    assert avg_val == expected_value\n\n\n@pytest.mark.parametrize(\'metric_function, expected_value\', [(avg_prec, 0.25), (ndcg, 1.0),\n                                                        (jaccard_similarity, 0.2)])\ndef test_metrics_less_number_of_retrievals(metric_function, expected_value):\n    """"""Number of retrievals < Number of ground truth retrievals""""""\n    corr_dup, ret_dups = initialize_fake_data_retrieved_less()\n    avg_val = metric_function(corr_dup, ret_dups)\n    assert avg_val == expected_value\n\n@pytest.mark.parametrize(\'metric_function, expected_value\', [(avg_prec, 0.8041666666666667), (ndcg, 0.9047172294870751),\n                                                        (jaccard_similarity, 0.8)])\ndef test_metrics_more_number_of_retrievals(metric_function, expected_value):\n    """"""Number of retrievals > Number of ground truth retrievals""""""\n    corr_dup, ret_dups = initialize_fake_data_retrieved_more()\n    avg_val = metric_function(corr_dup, ret_dups)\n    assert avg_val == expected_value\n\n\n@pytest.mark.parametrize(\'metric_func\', [avg_prec, ndcg, jaccard_similarity])\ndef test_zero_retrieval(metric_func):\n    corr_dup = [\'1.jpg\', \'2.jpg\', \'3.jpg\', \'4.jpg\']\n    ret_dups = []\n    av_val = metric_func(corr_dup, ret_dups)\n    assert av_val == 0.0\n\n@pytest.mark.parametrize(\'metric_function, expected_value\', [(avg_prec, 1.0), (ndcg, 1.0),\n                                                        (jaccard_similarity, 1.0)])\ndef test_zero_correct_and_zero_retrieved(metric_function, expected_value):\n    corr_dup = []\n    ret_dups = []\n    assert metric_function(corr_dup, ret_dups) == expected_value\n\n\n@pytest.mark.parametrize(\'metric_function, expected_value\', [(avg_prec, 0.0), (ndcg, 0.0),\n                                                        (jaccard_similarity, 0.0)])\ndef test_zero_correct_and_one_retrieved(metric_function, expected_value):\n    corr_dup = []\n    ret_dups = [\'1\']\n    assert metric_function(corr_dup, ret_dups) == expected_value\n\n@pytest.mark.parametrize(\'metric, expected_value\', [(\'map\', 0.5555555555555556), (\'ndcg\', 0.75),\n                                                        (\'jaccard\', 0.6)])\ndef test_metric_is_not_1_for_incorrect(metric, expected_value):\n    """"""Tests if correct MAP values are computed\n    Load ground truth and dict for incorrect map prediction to have a Map less than 1.0""""""\n    ground_truth, retrieved = return_ground_incorrect_retrievals()\n    metric_val = mean_metric(ground_truth, retrieved, metric=metric)\n    assert metric_val == expected_value\n\n\ndef test_all_metrics_1_for_all_correct_retrievals():\n    ground_truth, retrieved = return_ground_all_correct_retrievals()\n    metrics = get_all_metrics(ground_truth, retrieved)\n    for i in metrics.values():\n        assert i == 1.0\n\n\ndef test_get_metrics_returns_dict():\n    ground_truth, retrieved = return_ground_incorrect_retrievals()\n    assert isinstance(get_all_metrics(ground_truth, retrieved), dict)\n    assert len(get_all_metrics(ground_truth, retrieved).values()) == 3  # 3 metrics\n\n\n\n\n'"
tests/test_plotter.py,0,"b""import os\nimport pytest\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom imagededup.utils.plotter import _formatter, _validate_args, plot_duplicates\n\np = Path(__file__)\n\nPATH_DIR_POSIX = p.parent / 'data/mixed_images'\nPATH_DIR = os.path.join(os.getcwd(), 'tests/data/mixed_images')\nPATH_DIR_INVALID = 'some_invalid_path/'\n\n\ndef test__formatter_float():\n    assert _formatter(np.float32(0.81342537)) == '0.813'\n\n\ndef test__formatter_int():\n    assert _formatter(3) == 3\n\n\ndef test__formatter_float_not_npfloat32():\n    assert _formatter(0.81342537) == 0.81342537\n\n\n# test_validate_args (n tcs)\n\ndef test__validate_args_nonposixpath():\n    assert _validate_args(image_dir=PATH_DIR, duplicate_map={'1': ['2']}, filename='1') == PATH_DIR_POSIX\n\n\ndef test__validate_args_image_dir():\n    with pytest.raises(AssertionError) as e:\n        _validate_args(image_dir=PATH_DIR_INVALID, duplicate_map=None, filename=None)\n    assert (\n        str(e.value)\n        == 'Provided image directory does not exist! Please provide the image directory where all files are present!'\n    )\n\n\ndef test__validate_args_duplicate_map():\n    with pytest.raises(ValueError) as e:\n        _validate_args(image_dir=PATH_DIR, duplicate_map=None, filename=None)\n    assert str(e.value) == 'Please provide a valid Duplicate map!'\n\n\ndef test__validate_args_filename():\n    with pytest.raises(ValueError) as e:\n        _validate_args(image_dir=PATH_DIR, duplicate_map={'1': ['2']}, filename='2')\n    assert (\n        str(e.value)\n        == 'Please provide a valid filename present as a key in the duplicate_map!'\n    )\n\n\n# test plot_duplicates, assert calls\n@pytest.fixture\ndef mocker_validate_args(mocker):\n    return mocker.patch('imagededup.utils.plotter._validate_args', return_value=PATH_DIR_POSIX)\n\n\n@pytest.fixture\ndef mocker_plot_images(mocker):\n    return mocker.patch('imagededup.utils.plotter._plot_images')\n\n\ndef test_plot_duplicates(mocker_validate_args, mocker_plot_images):\n    plot_duplicates(image_dir=PATH_DIR_POSIX, duplicate_map={'1': ['2']}, filename='1')\n    mocker_validate_args.assert_called_once_with(\n        image_dir=PATH_DIR_POSIX, duplicate_map={'1': ['2']}, filename='1'\n    )\n    mocker_plot_images.assert_called_once_with(\n        image_dir=PATH_DIR_POSIX, orig='1', image_list=['2'], scores=False, outfile=None\n    )\n\n\ndef test_plot_duplicates_outfile(mocker_validate_args, mocker_plot_images):\n    plot_duplicates(\n        image_dir=PATH_DIR_POSIX,\n        duplicate_map={'1': ['2']},\n        filename='1',\n        outfile='bla.png',\n    )\n    mocker_validate_args.assert_called_once_with(\n        image_dir=PATH_DIR_POSIX, duplicate_map={'1': ['2']}, filename='1'\n    )\n    mocker_plot_images.assert_called_once_with(\n        image_dir=PATH_DIR_POSIX,\n        orig='1',\n        image_list=['2'],\n        scores=False,\n        outfile='bla.png',\n    )\n\n\ndef test_plot_duplicates_scores(mocker_validate_args, mocker_plot_images):\n    plot_duplicates(\n        image_dir=PATH_DIR_POSIX, duplicate_map={'1': [('2', 0.6)]}, filename='1'\n    )\n    mocker_validate_args.assert_called_once_with(\n        image_dir=PATH_DIR_POSIX, duplicate_map={'1': [('2', 0.6)]}, filename='1'\n    )\n    mocker_plot_images.assert_called_once_with(\n        image_dir=PATH_DIR_POSIX,\n        orig='1',\n        image_list=[('2', 0.6)],\n        scores=True,\n        outfile=None,\n    )\n\n\ndef test_plot_duplicates_no_duplicates():\n    with pytest.raises(AssertionError) as e:\n        plot_duplicates(\n            image_dir=PATH_DIR_POSIX, duplicate_map={'1': [], '2': []}, filename='2'\n        )\n    assert str(e.value) == 'Provided filename has no duplicates!'\n\n\n# def test_plot_duplicates_integrated():\n#     plot_duplicates(image_dir=PATH_DIR_POSIX, duplicate_map={'ukbench00120.jpg': ['ukbench00120_hflip.jpg']}, filename='ukbench00120.jpg')\n#     plt.gcf().canvas.draw()\n"""
tests/test_retrieval.py,0,"b""import sys\nimport pytest\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom imagededup.handlers.search.retrieval import (\n    HashEval,\n    cosine_similarity_chunk,\n    get_cosine_similarity,\n)\nfrom imagededup.methods.hashing import Hashing\n\nHAMMING_DISTANCE_FUNCTION = Hashing().hamming_distance\n\n\ndef test_cosine_similarity_chunk():\n    X = np.random.rand(333, 100)\n    start_idx = 10\n    end_idx = 100\n\n    input_tuple = (X, (start_idx, end_idx))\n\n    result = cosine_similarity_chunk(input_tuple)\n\n    np.testing.assert_array_almost_equal(\n        result, cosine_similarity(X[start_idx:end_idx, :], X).astype('float16')\n    )\n\n\ndef test_get_cosine_similarity():\n    X = np.random.rand(333, 10)\n    expected = cosine_similarity(X)\n\n    # threshold not triggered\n    result = get_cosine_similarity(X)\n\n    np.testing.assert_array_almost_equal(result, expected)\n\n    # threshold triggered\n    result = get_cosine_similarity(X, threshold=20)\n\n    np.testing.assert_array_almost_equal(result, expected.astype('float16'))\n\n    # multiple chunks\n    result = get_cosine_similarity(X, threshold=20, chunk_size=10)\n\n    np.testing.assert_array_almost_equal(result, expected.astype('float16'))\n\n\ndef test_initialization():\n    db = {'ukbench09060.jpg': 'e064ece078d7c96a'}\n    threshold = 10\n    hasheval_obj = HashEval(\n        test=db,\n        queries=db,\n        distance_function=HAMMING_DISTANCE_FUNCTION,\n        threshold=threshold,\n    )\n    assert hasheval_obj.queries and hasheval_obj.test\n    assert hasheval_obj.threshold == threshold\n    assert hasheval_obj.distance_invoker('e064ece078d7c96a', 'a064ece078d7c96e') == 2\n\n\ndef test_retrieve_results_dtypes():\n    db = {'ukbench09060.jpg': 'e064ece078d7c96a'}\n    result = HashEval(db, db, HAMMING_DISTANCE_FUNCTION).retrieve_results()\n    assert isinstance(result, dict)\n    assert isinstance(list(result.values())[0], list)\n\n\ndef test_retrieve_results_dtypes_scores():\n    query = {\n        'ukbench00120.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n\n    db = {\n        'ukbench00120_hflip.jpg': '2b69f1517570e2a1',\n        'ukbench00120_resize.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n    out_map = HashEval(db, query, HAMMING_DISTANCE_FUNCTION).retrieve_results(\n        scores=True\n    )\n    assert isinstance(out_map, dict)\n    assert isinstance(list(out_map.values())[0], list)\n    assert isinstance(list(out_map.values())[0][0], tuple)\n    assert len(out_map) == len(query)\n\n\ndef test_resultset_correctness():\n    query = {\n        'ukbench00120.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n    db = {\n        'ukbench00120_fake.jpg': '2b69707551f1b87d',\n        'ukbench00120_resize.jpg': '2b69707551f1b87a',\n        'ukbench09268_2.jpg': 'ac9c72f8e1c2c448',\n    }\n    hasheval_obj = HashEval(db, query, HAMMING_DISTANCE_FUNCTION, threshold=3)\n    results = hasheval_obj.retrieve_results(scores=True)\n    distances = [i[1] for v in results.values() for i in v]\n    assert max(distances) == 3\n\n\ndef test_result_consistency_across_search_methods():\n    query = {\n        'ukbench00120.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n    db = {\n        'ukbench00120_hflip.jpg': '2b69f1517570e2a1',\n        'ukbench00120_resize.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n    brute_force_result = HashEval(\n        db, query, HAMMING_DISTANCE_FUNCTION, search_method='brute_force'\n    ).retrieve_results()\n\n    bktree_result = HashEval(db, query, HAMMING_DISTANCE_FUNCTION).retrieve_results()\n    assert brute_force_result == bktree_result\n\n\n@pytest.mark.skipif(sys.platform == 'win32', reason='Does not run on Windows.')\ndef test_result_consistency_across_search_methods_scores():\n    query = {\n        'ukbench00120.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n\n    db = {\n        'ukbench00120_hflip.jpg': '2b69f1517570e2a1',\n        'ukbench00120_resize.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n    brute_force_result = HashEval(\n        db, query, HAMMING_DISTANCE_FUNCTION, search_method='brute_force'\n    ).retrieve_results(scores=True)\n\n    bktree_result = HashEval(db, query, HAMMING_DISTANCE_FUNCTION,search_method='bktree').retrieve_results(\n        scores=True\n    )\n\n    brute_force_cython_result = HashEval(\n        db, query, HAMMING_DISTANCE_FUNCTION, search_method='brute_force_cython'\n    ).retrieve_results(scores=True)\n\n    assert brute_force_result == bktree_result\n    assert brute_force_cython_result == brute_force_result\n\n\ndef test_no_self_retrieval():\n    query = {\n        'ukbench00120.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n    db = {\n        'ukbench00120_hflip.jpg': '2b69f1517570e2a1',\n        'ukbench00120_resize.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n    brute_res = HashEval(\n        db, query, HAMMING_DISTANCE_FUNCTION, search_method='brute_force'\n    ).retrieve_results()\n\n    bktree_res = HashEval(db, query, HAMMING_DISTANCE_FUNCTION).retrieve_results()\n    assert len(brute_res['ukbench09268.jpg']) == 0\n    assert len(bktree_res['ukbench09268.jpg']) == 0\n\n\ndef test_max_hamming_threshold_not_violated():\n    query = {\n        'ukbench00120.jpg': '2b69707551f1b87a',\n        'ukbench09268.jpg': 'ac9c72f8e1c2c448',\n    }\n    db = {\n        'ukbench00120_hflip.jpg': '2b69f1517570e2a1',\n        'ukbench00120_resize.jpg': '2b69707551f1b870',\n        'ukbench09268_2.jpg': 'ac9c72f8e1c2c448',\n    }\n    result = HashEval(\n        db, query, HAMMING_DISTANCE_FUNCTION, search_method='bktree'\n    ).retrieve_results(scores=True)\n    distances = [i[1] for v in result.values() for i in v]\n    assert max(distances) < 5  # 5 is the default threshold value\n\n\ndef test_results_sorted_in_ascending_distance_order():\n    query = {'ukbench00120.jpg': '2b69707551f1b87a'}\n    db = {\n        'ukbench00120_hflip.jpg': '2b69707551f1b87f',\n        'ukbench00120_resize.jpg': '2b69707551f1b87b',\n        'ukbench09268_2.jpg': '2b69707551f1b870',\n        'ukbench09268_3.jpg': '2c89709251f1b870',\n    }\n    result = HashEval(\n        db, query, HAMMING_DISTANCE_FUNCTION, threshold=30, search_method='bktree'\n    ).retrieve_results(scores=True)\n    distances = [i[1] for v in result.values() for i in v]\n\n    assert sorted(distances, reverse=False) == distances\n"""
imagededup/evaluation/__init__.py,0,b'from .evaluation import evaluate\n'
imagededup/evaluation/evaluation.py,0,"b'import itertools\nfrom typing import Dict\n\nfrom imagededup.handlers.metrics.classification import classification_metrics\nfrom imagededup.handlers.metrics.information_retrieval import (\n    mean_metric,\n    get_all_metrics,\n)\nfrom imagededup.utils.logger import return_logger\n\nlogger = return_logger(__name__)\n\n\ndef _transpose_checker(mapping):\n    """"""\n    Check for the given dictionary that transpose (symmetric) relationship holds.\n\n    Args:\n        mapping: Dictionary representing a mapping of filenames to the list of respective duplicate filenames.\n    """"""\n    for key, val in mapping.items():\n        # check for each value in the list if the key is present as its value\n        for v in val:\n            assert key in mapping[v], (\n                f\'Transpose relationship violated, file {key} not present as a duplicate for file {v} in the provided\'\n                f\' mapping dictionary\'\n            )\n\n\ndef _check_map_correctness(ground_truth_map: Dict, retrieved_map: Dict):\n    """"""\n    Perform following validation checks for both ground truth and retrieved maps:\n    - Each duplicate filename should be one of the keys (no files that are not keys of the map)\n    - Transpose relationships are present. Eg: if \'file1.jpg\' is a duplicate for \'file2.jpg\', then both relationships\n    should be present in the respective map i.e., {\'file1.jpg\': [\'file2.jpg\'], \'file2.jpg\': [\'file1.jpg\']}\n    - Ground truth as well as retrieved map have exactly the same keys.\n\n    Args:\n        ground_truth_map: A dictionary representing ground truth with filenames as key and a list of duplicate filenames\n        as value.\n        retrieved_map: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved\n        duplicate filenames as value.\n    """"""\n    logger.info(\'Validating ground truth map ..\')\n    ground_truth_keys_set = set(ground_truth_map.keys())\n    ground_truth_val_set = set(itertools.chain(*list(ground_truth_map.values())))\n    assert (\n        len(ground_truth_val_set.difference(ground_truth_keys_set)) == 0\n    ), \'Ground truth map validation failed, Ground truth has filenames that are not in the key filename of the map!\'\n    _transpose_checker(\n        ground_truth_map\n    )  # transpose relationships important for Information Retrieval metrics\n    logger.info(\'Ground truth map validated\')\n\n    logger.info(\'Validating retrieved map ..\')\n    duplicate_map_keys_set = set(retrieved_map.keys())\n    duplicate_val_set = set(itertools.chain(*list(retrieved_map.values())))\n    assert (\n        len(duplicate_val_set.difference(duplicate_map_keys_set)) == 0\n    ), \'Retrieved map validation failed, Retrieved map has filenames that are not in the key filename of the map!\'\n    _transpose_checker(retrieved_map)\n    logger.info(\'Duplicate map validated\')\n\n    logger.info(\'Validating ground truth map and retrieved map consistency..\')\n    if not ground_truth_keys_set == duplicate_map_keys_set:\n        diff = ground_truth_keys_set.symmetric_difference(duplicate_map_keys_set)\n        raise Exception(\n            f\'Please ensure that ground truth and retrieved map have the same keys!\'\n            f\' Following keys uncommon between ground truth and retrieved maps:\\n{diff}\'\n        )\n    logger.info(\'Ground truth map and retrieved map found to be consistent.\')\n\n\ndef evaluate(\n    ground_truth_map: Dict = None, retrieved_map: Dict = None, metric: str = \'all\'\n):\n    """"""\n    Given a ground truth map and a duplicate map retrieved from a deduplication algorithm, get metrics to evaluate the\n    effectiveness of the applied deduplication algorithm.\n\n    Args:\n        ground_truth_map: A dictionary representing ground truth with filenames as key and a list of duplicate filenames\n                          as value.\n        retrieved_map: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved\n                       duplicate filenames as value.\n        metric: Name of metric to be evaluated and returned. Accepted values are: \'map\', \'ndcg\', \'jaccard\',\n                \'classification\', \'all\'(default, returns every metric).\n\n    Returns:\n        dictionary: A dictionary with metric name as key and corresponding calculated metric as the value. \'map\', \'ndcg\'\n                    and \'jaccard\' return a single number denoting the corresponding information retrieval metric.\n                    \'classification\' metrics include \'precision\', \'recall\' and \'f1-score\' which are returned in the form\n                    of individual entries in the returned dictionary. The value for each of the classification metric\n                    is a numpy array with first entry as the score for non-duplicate file pairs(class-0) and second\n                    entry as the score for duplicate file pairs (class-1). Additionally, a support is also returned as\n                    another key with first entry denoting number of non-duplicate file pairs and second entry having\n                    duplicate file pairs.\n    """"""\n    metric = metric.lower()\n    _check_map_correctness(ground_truth_map, retrieved_map)\n\n    if metric in [\'map\', \'ndcg\', \'jaccard\']:\n        return {metric: mean_metric(ground_truth_map, retrieved_map, metric=metric)}\n    elif metric == \'classification\':\n        return classification_metrics(ground_truth_map, retrieved_map)\n    elif metric == \'all\':\n        ir_metrics = get_all_metrics(ground_truth_map, retrieved_map)\n        class_metrics = classification_metrics(ground_truth_map, retrieved_map)\n        ir_metrics.update(class_metrics)\n        return ir_metrics\n    else:\n        raise ValueError(\n            \'Acceptable metrics are: \\\'map\\\', \\\'ndcg\\\', \\\'jaccard\\\', \\\'classification\\\', \\\'all\\\'\'\n        )\n'"
imagededup/handlers/__init__.py,0,b''
imagededup/methods/__init__.py,0,"b'from .hashing import PHash, AHash, DHash, WHash\nfrom .cnn import CNN\n'"
imagededup/methods/cnn.py,0,"b'from pathlib import Path, PurePath\nfrom typing import Dict, List, Optional, Union\n\nimport numpy as np\n\nfrom imagededup.handlers.search.retrieval import get_cosine_similarity\nfrom imagededup.utils.general_utils import save_json, get_files_to_remove\nfrom imagededup.utils.image_utils import load_image, preprocess_image\nfrom imagededup.utils.logger import return_logger\n\n\nclass CNN:\n    """"""\n    Find duplicates using CNN and/or generate CNN encodings given a single image or a directory of images.\n\n    The module can be used for 2 purposes: Encoding generation and duplicate detection.\n    - Encodings generation:\n    To propagate an image through a Convolutional Neural Network architecture and generate encodings. The generated\n    encodings can be used at a later time for deduplication. Using the method \'encode_image\', the CNN encodings for a\n    single image can be obtained while the \'encode_images\' method can be used to get encodings for all images in a\n    directory.\n\n    - Duplicate detection:\n    Find duplicates either using the encoding mapping generated previously using \'encode_images\' or using a Path to the\n    directory that contains the images that need to be deduplicated. \'find_duplciates\' and \'find_duplicates_to_remove\'\n    methods are provided to accomplish these tasks.\n    """"""\n\n    def __init__(self, verbose: bool = True) -> None:\n        """"""\n        Initialize a keras MobileNet model that is sliced at the last convolutional layer.\n        Set the batch size for keras generators to be 64 samples. Set the input image size to (224, 224) for providing\n        as input to MobileNet model.\n\n        Args:\n            verbose: Display progress bar if True else disable it. Default value is True.\n        """"""\n        from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n        from imagededup.utils.data_generator import DataGenerator\n\n        self.MobileNet = MobileNet\n        self.preprocess_input = preprocess_input\n        self.DataGenerator = DataGenerator\n\n        self.target_size = (224, 224)\n        self.batch_size = 64\n        self.logger = return_logger(__name__)\n        self._build_model()\n        self.verbose = 1 if verbose is True else 0\n\n    def _build_model(self):\n        """"""\n        Build MobileNet model sliced at the last convolutional layer with global average pooling added.\n        """"""\n        self.model = self.MobileNet(\n            input_shape=(224, 224, 3), include_top=False, pooling=\'avg\'\n        )\n\n        self.logger.info(\n            \'Initialized: MobileNet pretrained on ImageNet dataset sliced at last conv layer and added \'\n            \'GlobalAveragePooling\'\n        )\n\n    def _get_cnn_features_single(self, image_array: np.ndarray) -> np.ndarray:\n        """"""\n        Generate CNN encodings for a single image.\n\n        Args:\n            image_array: Image typecast to numpy array.\n\n        Returns:\n            Encodings for the image in the form of numpy array.\n        """"""\n        image_pp = self.preprocess_input(image_array)\n        image_pp = np.array(image_pp)[np.newaxis, :]\n        return self.model.predict(image_pp)\n\n    def _get_cnn_features_batch(self, image_dir: PurePath) -> Dict[str, np.ndarray]:\n        """"""\n        Generate CNN encodings for all images in a given directory of images.\n        Args:\n            image_dir: Path to the image directory.\n\n        Returns:\n            A dictionary that contains a mapping of filenames and corresponding numpy array of CNN encodings.\n        """"""\n        self.logger.info(\'Start: Image encoding generation\')\n        self.data_generator = self.DataGenerator(\n            image_dir=image_dir,\n            batch_size=self.batch_size,\n            target_size=self.target_size,\n            basenet_preprocess=self.preprocess_input,\n        )\n\n        feat_vec = self.model.predict_generator(\n            self.data_generator, len(self.data_generator), verbose=self.verbose\n        )\n        self.logger.info(\'End: Image encoding generation\')\n\n        filenames = [i.name for i in self.data_generator.valid_image_files]\n\n        self.encoding_map = {j: feat_vec[i, :] for i, j in enumerate(filenames)}\n        return self.encoding_map\n\n    def encode_image(\n        self,\n        image_file: Optional[Union[PurePath, str]] = None,\n        image_array: Optional[np.ndarray] = None,\n    ) -> np.ndarray:\n        """"""\n        Generate CNN encoding for a single image.\n\n        Args:\n            image_file: Path to the image file.\n            image_array: Optional, used instead of image_file. Image typecast to numpy array.\n\n        Returns:\n            encoding: Encodings for the image in the form of numpy array.\n\n        Example:\n        ```\n        from imagededup.methods import CNN\n        myencoder = CNN()\n        encoding = myencoder.encode_image(image_file=\'path/to/image.jpg\')\n        OR\n        encoding = myencoder.encode_image(image_array=<numpy array of image>)\n        ```\n        """"""\n        if isinstance(image_file, str):\n            image_file = Path(image_file)\n\n        if isinstance(image_file, PurePath):\n            if not image_file.is_file():\n                raise ValueError(\n                    \'Please provide either image file path or image array!\'\n                )\n\n            image_pp = load_image(\n                image_file=image_file, target_size=self.target_size, grayscale=False\n            )\n\n        elif isinstance(image_array, np.ndarray):\n            image_pp = preprocess_image(\n                image=image_array, target_size=self.target_size, grayscale=False\n            )\n        else:\n            raise ValueError(\'Please provide either image file path or image array!\')\n\n        return (\n            self._get_cnn_features_single(image_pp)\n            if isinstance(image_pp, np.ndarray)\n            else None\n        )\n\n    def encode_images(self, image_dir: Union[PurePath, str]) -> Dict:\n        """"""Generate CNN encodings for all images in a given directory of images.\n\n        Args:\n            image_dir: Path to the image directory.\n        Returns:\n            dictionary: Contains a mapping of filenames and corresponding numpy array of CNN encodings.\n        Example:\n            ```\n            from imagededup.methods import CNN\n            myencoder = CNN()\n            encoding_map = myencoder.encode_images(image_dir=\'path/to/image/directory\')\n            ```\n        """"""\n        if isinstance(image_dir, str):\n            image_dir = Path(image_dir)\n\n        if not image_dir.is_dir():\n            raise ValueError(\'Please provide a valid directory path!\')\n\n        return self._get_cnn_features_batch(image_dir)\n\n    @staticmethod\n    def _check_threshold_bounds(thresh: float) -> None:\n        """"""\n        Check if provided threshold is valid. Raises TypeError if wrong threshold variable type is passed or a\n        ValueError if an out of range value is supplied.\n\n        Args:\n            thresh: Threshold value (must be float between -1.0 and 1.0)\n\n        Raises:\n            TypeError: If wrong variable type is provided.\n            ValueError: If wrong value is provided.\n        """"""\n        if not isinstance(thresh, float):\n            raise TypeError(\'Threshold must be a float between -1.0 and 1.0\')\n        if thresh < -1.0 or thresh > 1.0:\n            raise ValueError(\'Threshold must be a float between -1.0 and 1.0\')\n\n    def _find_duplicates_dict(\n        self,\n        encoding_map: Dict[str, list],\n        min_similarity_threshold: float,\n        scores: bool,\n        outfile: Optional[str] = None,\n    ) -> Dict:\n        """"""\n        Take in dictionary {filename: encoded image}, detects duplicates above the given cosine similarity threshold\n        and returns a dictionary containing key as filename and value as a list of duplicate filenames. Optionally,\n        the cosine distances could be returned instead of just duplicate filenames for each query file.\n\n        Args:\n            encoding_map: Dictionary with keys as file names and values as encoded images.\n            min_similarity_threshold: Cosine similarity above which retrieved duplicates are valid.\n            scores: Boolean indicating whether similarity scores are to be returned along with retrieved duplicates.\n\n        Returns:\n            if scores is True, then a dictionary of the form {\'image1.jpg\': [(\'image1_duplicate1.jpg\',\n            score), (\'image1_duplicate2.jpg\', score)], \'image2.jpg\': [] ..}\n            if scores is False, then a dictionary of the form {\'image1.jpg\': [\'image1_duplicate1.jpg\',\n            \'image1_duplicate2.jpg\'], \'image2.jpg\':[\'image1_duplicate1.jpg\',..], ..}\n        """"""\n\n        # get all image ids\n        # we rely on dictionaries preserving insertion order in Python >=3.6\n        image_ids = np.array([*encoding_map.keys()])\n\n        # put image encodings into feature matrix\n        features = np.array([*encoding_map.values()])\n\n        self.logger.info(\'Start: Calculating cosine similarities...\')\n\n        self.cosine_scores = get_cosine_similarity(features, self.verbose)\n\n        np.fill_diagonal(\n            self.cosine_scores, 2.0\n        )  # allows to filter diagonal in results, 2 is a placeholder value\n\n        self.logger.info(\'End: Calculating cosine similarities.\')\n\n        self.results = {}\n        for i, j in enumerate(self.cosine_scores):\n            duplicates_bool = (j >= min_similarity_threshold) & (j < 2)\n\n            if scores:\n                tmp = np.array([*zip(image_ids, j)], dtype=object)\n                duplicates = list(map(tuple, tmp[duplicates_bool]))\n\n            else:\n                duplicates = list(image_ids[duplicates_bool])\n\n            self.results[image_ids[i]] = duplicates\n\n        if outfile and scores:\n            save_json(results=self.results, filename=outfile, float_scores=True)\n        elif outfile:\n            save_json(results=self.results, filename=outfile)\n        return self.results\n\n    def _find_duplicates_dir(\n        self,\n        image_dir: Union[PurePath, str],\n        min_similarity_threshold: float,\n        scores: bool,\n        outfile: Optional[str] = None,\n    ) -> Dict:\n        """"""\n        Take in path of the directory in which duplicates are to be detected above the given threshold.\n        Returns dictionary containing key as filename and value as a list of duplicate file names.  Optionally,\n        the cosine distances could be returned instead of just duplicate filenames for each query file.\n\n        Args:\n            image_dir: Path to the directory containing all the images.\n            min_similarity_threshold: Optional, hamming distance above which retrieved duplicates are valid. Default 0.9\n            scores: Optional, boolean indicating whether Hamming distances are to be returned along with retrieved\n                    duplicates.\n            outfile: Optional, name of the file the results should be written to.\n\n        Returns:\n            if scores is True, then a dictionary of the form {\'image1.jpg\': [(\'image1_duplicate1.jpg\',\n            score), (\'image1_duplicate2.jpg\', score)], \'image2.jpg\': [] ..}\n            if scores is False, then a dictionary of the form {\'image1.jpg\': [\'image1_duplicate1.jpg\',\n            \'image1_duplicate2.jpg\'], \'image2.jpg\':[\'image1_duplicate1.jpg\',..], ..}\n        """"""\n        self.encode_images(image_dir=image_dir)\n\n        return self._find_duplicates_dict(\n            encoding_map=self.encoding_map,\n            min_similarity_threshold=min_similarity_threshold,\n            scores=scores,\n            outfile=outfile,\n        )\n\n    def find_duplicates(\n        self,\n        image_dir: Union[PurePath, str] = None,\n        encoding_map: Dict[str, list] = None,\n        min_similarity_threshold: float = 0.9,\n        scores: bool = False,\n        outfile: Optional[str] = None,\n    ) -> Dict:\n        """"""\n        Find duplicates for each file. Take in path of the directory or encoding dictionary in which duplicates are to\n        be detected above the given threshold. Return dictionary containing key as filename and value as a list of\n        duplicate file names. Optionally, the cosine distances could be returned instead of just duplicate filenames for\n        each query file.\n\n        Args:\n            image_dir: Path to the directory containing all the images or dictionary with keys as file names\n            and values as numpy arrays which represent the CNN encoding for the key image file.\n            encoding_map: Optional, used instead of image_dir, a dictionary containing mapping of filenames and\n                          corresponding CNN encodings.\n            min_similarity_threshold: Optional, threshold value (must be float between -1.0 and 1.0). Default is 0.9\n            scores: Optional, boolean indicating whether similarity scores are to be returned along with retrieved\n                    duplicates.\n            outfile: Optional, name of the file to save the results, must be a json. Default is None.\n\n        Returns:\n            dictionary: if scores is True, then a dictionary of the form {\'image1.jpg\': [(\'image1_duplicate1.jpg\',\n                        score), (\'image1_duplicate2.jpg\', score)], \'image2.jpg\': [] ..}. if scores is False, then a\n                        dictionary of the form {\'image1.jpg\': [\'image1_duplicate1.jpg\', \'image1_duplicate2.jpg\'],\n                        \'image2.jpg\':[\'image1_duplicate1.jpg\',..], ..}\n\n        Example:\n        ```\n        from imagededup.methods import CNN\n        myencoder = CNN()\n        duplicates = myencoder.find_duplicates(image_dir=\'path/to/directory\', min_similarity_threshold=0.85, scores=True,\n        outfile=\'results.json\')\n\n        OR\n\n        from imagededup.methods import CNN\n        myencoder = CNN()\n        duplicates = myencoder.find_duplicates(encoding_map=<mapping filename to cnn encodings>,\n        min_similarity_threshold=0.85, scores=True, outfile=\'results.json\')\n        ```\n        """"""\n        self._check_threshold_bounds(min_similarity_threshold)\n\n        if image_dir:\n            result = self._find_duplicates_dir(\n                image_dir=image_dir,\n                min_similarity_threshold=min_similarity_threshold,\n                scores=scores,\n                outfile=outfile,\n            )\n        elif encoding_map:\n            result = self._find_duplicates_dict(\n                encoding_map=encoding_map,\n                min_similarity_threshold=min_similarity_threshold,\n                scores=scores,\n                outfile=outfile,\n            )\n\n        else:\n            raise ValueError(\'Provide either an image directory or encodings!\')\n\n        return result\n\n    def find_duplicates_to_remove(\n        self,\n        image_dir: PurePath = None,\n        encoding_map: Dict[str, np.ndarray] = None,\n        min_similarity_threshold: float = 0.9,\n        outfile: Optional[str] = None,\n    ) -> List:\n        """"""\n        Give out a list of image file names to remove based on the similarity threshold. Does not remove the mentioned\n        files.\n\n        Args:\n            image_dir: Path to the directory containing all the images or dictionary with keys as file names\n                       and values as numpy arrays which represent the CNN encoding for the key image file.\n            encoding_map: Optional, used instead of image_dir, a dictionary containing mapping of filenames and\n                          corresponding CNN encodings.\n            min_similarity_threshold: Optional, threshold value (must be float between -1.0 and 1.0). Default is 0.9\n            outfile: Optional, name of the file to save the results, must be a json. Default is None.\n\n        Returns:\n            duplicates: List of image file names that should be removed.\n\n        Example:\n        ```\n        from imagededup.methods import CNN\n        myencoder = CNN()\n        duplicates = myencoder.find_duplicates_to_remove(image_dir=\'path/to/images/directory\'),\n        min_similarity_threshold=0.85)\n\n        OR\n\n        from imagededup.methods import CNN\n        myencoder = CNN()\n        duplicates = myencoder.find_duplicates_to_remove(encoding_map=<mapping filename to cnn encodings>,\n        min_similarity_threshold=0.85, outfile=\'results.json\')\n        ```\n        """"""\n        if image_dir or encoding_map:\n            duplicates = self.find_duplicates(\n                image_dir=image_dir,\n                encoding_map=encoding_map,\n                min_similarity_threshold=min_similarity_threshold,\n                scores=False,\n            )\n\n        files_to_remove = get_files_to_remove(duplicates)\n\n        if outfile:\n            save_json(files_to_remove, outfile)\n\n        return files_to_remove\n'"
imagededup/methods/hashing.py,0,"b'import os\nimport sys\nfrom pathlib import PurePath, Path\nfrom typing import Dict, List, Optional\n\nimport pywt\nimport numpy as np\nfrom scipy.fftpack import dct\n\nfrom imagededup.handlers.search.retrieval import HashEval\nfrom imagededup.utils.general_utils import get_files_to_remove, save_json, parallelise\nfrom imagededup.utils.image_utils import load_image, preprocess_image\nfrom imagededup.utils.logger import return_logger\n\nlogger = return_logger(__name__)\n\n""""""\nTODO:\nWavelet hash: Zero the LL coeff, reconstruct image, then get wavelet transform\nWavelet hash: Allow possibility of different wavelet functions\n\n""""""\n\n\nclass Hashing:\n    """"""\n    Find duplicates using hashing algorithms and/or generate hashes given a single image or a directory of images.\n\n    The module can be used for 2 purposes: Encoding generation and duplicate detection.\n    - Encoding generation:\n    To generate hashes using specific hashing method. The generated hashes can be used at a later time for\n    deduplication. Using the method \'encode_image\' from the specific hashing method object, the hash for a\n    single image can be obtained while the \'encode_images\' method can be used to get hashes for all images in a\n    directory.\n\n    - Duplicate detection:\n    Find duplicates either using the encoding mapping generated previously using \'encode_images\' or using a Path to the\n    directory that contains the images that need to be deduplicated. \'find_duplciates\' and \'find_duplicates_to_remove\'\n    methods are provided to accomplish these tasks.\n    """"""\n\n    def __init__(self, verbose: bool = True) -> None:\n        """"""\n        Initialize hashing class.\n\n        Args:\n            verbose: Display progress bar if True else disable it. Default value is True.\n        """"""\n        self.target_size = (8, 8)  # resizing to dims\n        self.verbose = verbose\n\n    @staticmethod\n    def hamming_distance(hash1: str, hash2: str) -> float:\n        """"""\n        Calculate the hamming distance between two hashes. If length of hashes is not 64 bits, then pads the length\n        to be 64 for each hash and then calculates the hamming distance.\n\n        Args:\n            hash1: hash string\n            hash2: hash string\n\n        Returns:\n            hamming_distance: Hamming distance between the two hashes.\n        """"""\n        hash1_bin = bin(int(hash1, 16))[2:].zfill(\n            64\n        )  # zfill ensures that len of hash is 64 and pads MSB if it is < A\n        hash2_bin = bin(int(hash2, 16))[2:].zfill(64)\n        return np.sum([i != j for i, j in zip(hash1_bin, hash2_bin)])\n\n    @staticmethod\n    def _array_to_hash(hash_mat: np.ndarray) -> str:\n        """"""\n        Convert a matrix of binary numerals to 64 character hash.\n\n        Args:\n            hash_mat: A numpy array consisting of 0/1 values.\n\n        Returns:\n            An hexadecimal hash string.\n        """"""\n        return \'\'.join(\'%0.2x\' % x for x in np.packbits(hash_mat))\n\n    def encode_image(\n        self, image_file=None, image_array: Optional[np.ndarray] = None\n    ) -> str:\n        """"""\n        Generate hash for a single image.\n\n        Args:\n            image_file: Path to the image file.\n            image_array: Optional, used instead of image_file. Image typecast to numpy array.\n\n        Returns:\n            hash: A 16 character hexadecimal string hash for the image.\n\n        Example:\n        ```\n        from imagededup.methods import <hash-method>\n        myencoder = <hash-method>()\n        myhash = myencoder.encode_image(image_file=\'path/to/image.jpg\')\n        OR\n        myhash = myencoder.encode_image(image_array=<numpy array of image>)\n        ```\n        """"""\n        try:\n            if image_file and os.path.exists(image_file):\n                image_file = Path(image_file)\n                image_pp = load_image(\n                    image_file=image_file, target_size=self.target_size, grayscale=True\n                )\n\n            elif isinstance(image_array, np.ndarray):\n                image_pp = preprocess_image(\n                    image=image_array, target_size=self.target_size, grayscale=True\n                )\n            else:\n                raise ValueError\n        except (ValueError, TypeError):\n            raise ValueError(\'Please provide either image file path or image array!\')\n\n        return self._hash_func(image_pp) if isinstance(image_pp, np.ndarray) else None\n\n    def encode_images(self, image_dir=None):\n        """"""\n        Generate hashes for all images in a given directory of images.\n\n        Args:\n            image_dir: Path to the image directory.\n\n        Returns:\n            dictionary: A dictionary that contains a mapping of filenames and corresponding 64 character hash string\n                        such as {\'Image1.jpg\': \'hash_string1\', \'Image2.jpg\': \'hash_string2\', ...}\n\n        Example:\n        ```\n        from imagededup.methods import <hash-method>\n        myencoder = <hash-method>()\n        mapping = myencoder.encode_images(\'path/to/directory\')\n        ```\n        """"""\n        if not os.path.isdir(image_dir):\n            raise ValueError(\'Please provide a valid directory path!\')\n\n        image_dir = Path(image_dir)\n\n        files = [\n            i.absolute() for i in image_dir.glob(\'*\') if not i.name.startswith(\'.\')\n        ]  # ignore hidden files\n\n        logger.info(f\'Start: Calculating hashes...\')\n\n        hashes = parallelise(self.encode_image, files, self.verbose)\n        hash_initial_dict = dict(zip([f.name for f in files], hashes))\n        hash_dict = {\n            k: v for k, v in hash_initial_dict.items() if v\n        }  # To ignore None (returned if some probelm with image file)\n\n        logger.info(f\'End: Calculating hashes!\')\n        return hash_dict\n\n    def _hash_algo(self, image_array: np.ndarray):\n        pass\n\n    def _hash_func(self, image_array: np.ndarray):\n        hash_mat = self._hash_algo(image_array)\n        return self._array_to_hash(hash_mat)\n\n    # search part\n\n    @staticmethod\n    def _check_hamming_distance_bounds(thresh: int) -> None:\n        """"""\n        Check if provided threshold is valid. Raises TypeError if wrong threshold variable type is passed or a\n        ValueError if an out of range value is supplied.\n\n        Args:\n            thresh: Threshold value (must be int between 0 and 64)\n\n        Raises:\n            TypeError: If wrong variable type is provided.\n            ValueError: If invalid value is provided.\n        """"""\n        if not isinstance(thresh, int):\n            raise TypeError(\'Threshold must be an int between 0 and 64\')\n        elif thresh < 0 or thresh > 64:\n            raise ValueError(\'Threshold must be an int between 0 and 64\')\n        else:\n            return None\n\n    def _find_duplicates_dict(\n        self,\n        encoding_map: Dict[str, str],\n        max_distance_threshold: int = 10,\n        scores: bool = False,\n        outfile: Optional[str] = None,\n        search_method: str = \'brute_force_cython\' if not sys.platform == \'win32\' else \'bktree\',\n    ) -> Dict:\n        """"""\n        Take in dictionary {filename: encoded image}, detects duplicates below the given hamming distance threshold\n        and returns a dictionary containing key as filename and value as a list of duplicate filenames. Optionally,\n        the hamming distances could be returned instead of just duplicate filenames for each query file.\n\n        Args:\n            encoding_map: Dictionary with keys as file names and values as encoded images (hashes).\n            max_distance_threshold: Hamming distance between two images below which retrieved duplicates are valid.\n            scores: Boolean indicating whether hamming distance scores are to be returned along with retrieved\n            duplicates.\n            outfile: Optional, name of the file to save the results. Default is None.\n            search_method: Algorithm used to retrieve duplicates. Default is brute_force_cython for Unix else bktree.\n\n        Returns:\n            if scores is True, then a dictionary of the form {\'image1.jpg\': [(\'image1_duplicate1.jpg\',\n            score), (\'image1_duplicate2.jpg\', score)], \'image2.jpg\': [] ..}\n            if scores is False, then a dictionary of the form {\'image1.jpg\': [\'image1_duplicate1.jpg\',\n            \'image1_duplicate2.jpg\'], \'image2.jpg\':[\'image1_duplicate1.jpg\',..], ..}\n        """"""\n        logger.info(\'Start: Evaluating hamming distances for getting duplicates\')\n\n        result_set = HashEval(\n            test=encoding_map,\n            queries=encoding_map,\n            distance_function=self.hamming_distance,\n            verbose=self.verbose,\n            threshold=max_distance_threshold,\n            search_method=search_method,\n        )\n\n        logger.info(\'End: Evaluating hamming distances for getting duplicates\')\n\n        self.results = result_set.retrieve_results(scores=scores)\n        if outfile:\n            save_json(self.results, outfile)\n        return self.results\n\n    def _find_duplicates_dir(\n        self,\n        image_dir: PurePath,\n        max_distance_threshold: int = 10,\n        scores: bool = False,\n        outfile: Optional[str] = None,\n        search_method: str = \'brute_force_cython\' if not sys.platform == \'win32\' else \'bktree\',\n    ) -> Dict:\n        """"""\n        Take in path of the directory in which duplicates are to be detected below the given hamming distance\n        threshold. Returns dictionary containing key as filename and value as a list of duplicate file names.\n        Optionally, the hamming distances could be returned instead of just duplicate filenames for each query file.\n\n        Args:\n            image_dir: Path to the directory containing all the images.\n            max_distance_threshold: Hamming distance between two images below which retrieved duplicates are valid.\n            scores: Boolean indicating whether Hamming distances are to be returned along with retrieved duplicates.\n            outfile: Name of the file the results should be written to.\n            search_method: Algorithm used to retrieve duplicates. Default is brute_force_cython for Unix else bktree.\n\n        Returns:\n            if scores is True, then a dictionary of the form {\'image1.jpg\': [(\'image1_duplicate1.jpg\',\n            score), (\'image1_duplicate2.jpg\', score)], \'image2.jpg\': [] ..}\n            if scores is False, then a dictionary of the form {\'image1.jpg\': [\'image1_duplicate1.jpg\',\n            \'image1_duplicate2.jpg\'], \'image2.jpg\':[\'image1_duplicate1.jpg\',..], ..}\n        """"""\n        encoding_map = self.encode_images(image_dir)\n        results = self._find_duplicates_dict(\n            encoding_map=encoding_map,\n            max_distance_threshold=max_distance_threshold,\n            scores=scores,\n            outfile=outfile,\n            search_method=search_method,\n        )\n        return results\n\n    def find_duplicates(\n        self,\n        image_dir: PurePath = None,\n        encoding_map: Dict[str, str] = None,\n        max_distance_threshold: int = 10,\n        scores: bool = False,\n        outfile: Optional[str] = None,\n        search_method: str = \'brute_force_cython\' if not sys.platform == \'win32\' else \'bktree\',\n    ) -> Dict:\n        """"""\n        Find duplicates for each file. Takes in path of the directory or encoding dictionary in which duplicates are to\n        be detected. All images with hamming distance less than or equal to the max_distance_threshold are regarded as\n        duplicates. Returns dictionary containing key as filename and value as a list of duplicate file names.\n        Optionally, the below the given hamming distance could be returned instead of just duplicate filenames for each\n        query file.\n\n        Args:\n            image_dir: Path to the directory containing all the images or dictionary with keys as file names\n                       and values as hash strings for the key image file.\n            encoding_map: Optional,  used instead of image_dir, a dictionary containing mapping of filenames and\n                          corresponding hashes.\n            max_distance_threshold: Optional, hamming distance between two images below which retrieved duplicates are\n                                    valid. (must be an int between 0 and 64). Default is 10.\n            scores: Optional, boolean indicating whether Hamming distances are to be returned along with retrieved duplicates.\n            outfile: Optional, name of the file to save the results, must be a json. Default is None.\n            search_method: Algorithm used to retrieve duplicates. Default is brute_force_cython for Unix else bktree.\n\n        Returns:\n            duplicates dictionary: if scores is True, then a dictionary of the form {\'image1.jpg\': [(\'image1_duplicate1.jpg\',\n                        score), (\'image1_duplicate2.jpg\', score)], \'image2.jpg\': [] ..}. if scores is False, then a\n                        dictionary of the form {\'image1.jpg\': [\'image1_duplicate1.jpg\', \'image1_duplicate2.jpg\'],\n                        \'image2.jpg\':[\'image1_duplicate1.jpg\',..], ..}\n\n        Example:\n        ```\n        from imagededup.methods import <hash-method>\n        myencoder = <hash-method>()\n        duplicates = myencoder.find_duplicates(image_dir=\'path/to/directory\', max_distance_threshold=15, scores=True,\n        outfile=\'results.json\')\n\n        OR\n\n        from imagededup.methods import <hash-method>\n        myencoder = <hash-method>()\n        duplicates = myencoder.find_duplicates(encoding_map=<mapping filename to hashes>,\n        max_distance_threshold=15, scores=True, outfile=\'results.json\')\n        ```\n        """"""\n        self._check_hamming_distance_bounds(thresh=max_distance_threshold)\n        if image_dir:\n            result = self._find_duplicates_dir(\n                image_dir=image_dir,\n                max_distance_threshold=max_distance_threshold,\n                scores=scores,\n                outfile=outfile,\n                search_method=search_method,\n            )\n        elif encoding_map:\n            result = self._find_duplicates_dict(\n                encoding_map=encoding_map,\n                max_distance_threshold=max_distance_threshold,\n                scores=scores,\n                outfile=outfile,\n                search_method=search_method,\n            )\n        else:\n            raise ValueError(\'Provide either an image directory or encodings!\')\n        return result\n\n    def find_duplicates_to_remove(\n        self,\n        image_dir: PurePath = None,\n        encoding_map: Dict[str, str] = None,\n        max_distance_threshold: int = 10,\n        outfile: Optional[str] = None,\n    ) -> List:\n        """"""\n        Give out a list of image file names to remove based on the hamming distance threshold threshold. Does not\n        remove the mentioned files.\n\n        Args:\n            image_dir: Path to the directory containing all the images or dictionary with keys as file names\n                       and values as hash strings for the key image file.\n            encoding_map: Optional, used instead of image_dir, a dictionary containing mapping of filenames and\n                          corresponding hashes.\n            max_distance_threshold: Optional, hamming distance between two images below which retrieved duplicates are\n                                    valid. (must be an int between 0 and 64). Default is 10.\n            outfile: Optional, name of the file to save the results, must be a json. Default is None.\n\n        Returns:\n            duplicates: List of image file names that are found to be duplicate of me other file in the directory.\n\n        Example:\n        ```\n        from imagededup.methods import <hash-method>\n        myencoder = <hash-method>()\n        duplicates = myencoder.find_duplicates_to_remove(image_dir=\'path/to/images/directory\'),\n        max_distance_threshold=15)\n\n        OR\n\n        from imagededup.methods import <hash-method>\n        myencoder = <hash-method>()\n        duplicates = myencoder.find_duplicates(encoding_map=<mapping filename to hashes>,\n        max_distance_threshold=15, outfile=\'results.json\')\n        ```\n        """"""\n        result = self.find_duplicates(\n            image_dir=image_dir,\n            encoding_map=encoding_map,\n            max_distance_threshold=max_distance_threshold,\n            scores=False,\n        )\n        files_to_remove = get_files_to_remove(result)\n        if outfile:\n            save_json(files_to_remove, outfile)\n        return files_to_remove\n\n\nclass PHash(Hashing):\n    """"""\n    Inherits from Hashing base class and implements perceptual hashing (Implementation reference:\n    http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html).\n\n    Offers all the functionality mentioned in hashing class.\n\n    Example:\n    ```\n    # Perceptual hash for images\n    from imagededup.methods import PHash\n    phasher = PHash()\n    perceptual_hash = phasher.encode_image(image_file = \'path/to/image.jpg\')\n    OR\n    perceptual_hash = phasher.encode_image(image_array = <numpy image array>)\n    OR\n    perceptual_hashes = phasher.encode_images(image_dir = \'path/to/directory\')  # for a directory of images\n\n    # Finding duplicates:\n    from imagededup.methods import PHash\n    phasher = PHash()\n    duplicates = phasher.find_duplicates(image_dir=\'path/to/directory\', max_distance_threshold=15, scores=True)\n    OR\n    duplicates = phasher.find_duplicates(encoding_map=encoding_map, max_distance_threshold=15, scores=True)\n\n    # Finding duplicates to return a single list of duplicates in the image collection\n    from imagededup.methods import PHash\n    phasher = PHash()\n    files_to_remove = phasher.find_duplicates_to_remove(image_dir=\'path/to/images/directory\',\n                      max_distance_threshold=15)\n    OR\n    files_to_remove = phasher.find_duplicates_to_remove(encoding_map=encoding_map, max_distance_threshold=15)\n    ```\n    """"""\n\n    def __init__(self, verbose: bool = True) -> None:\n        """"""\n        Initialize perceptual hashing class.\n\n        Args:\n            verbose: Display progress bar if True else disable it. Default value is True.\n        """"""\n        super().__init__(verbose)\n        self.__coefficient_extract = (8, 8)\n        self.target_size = (32, 32)\n\n    def _hash_algo(self, image_array):\n        """"""\n        Get perceptual hash of the input image.\n\n        Args:\n            image_array: numpy array that corresponds to the image.\n\n        Returns:\n            A string representing the perceptual hash of the image.\n        """"""\n        dct_coef = dct(dct(image_array, axis=0), axis=1)\n\n        # retain top left 8 by 8 dct coefficients\n        dct_reduced_coef = dct_coef[\n            : self.__coefficient_extract[0], : self.__coefficient_extract[1]\n        ]\n\n        # median of coefficients excluding the DC term (0th term)\n        # mean_coef_val = np.mean(np.ndarray.flatten(dct_reduced_coef)[1:])\n        median_coef_val = np.median(np.ndarray.flatten(dct_reduced_coef)[1:])\n\n        # return mask of all coefficients greater than mean of coefficients\n        hash_mat = dct_reduced_coef >= median_coef_val\n        return hash_mat\n\n\nclass AHash(Hashing):\n    """"""\n    Inherits from Hashing base class and implements average hashing. (Implementation reference:\n    http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html)\n\n    Offers all the functionality mentioned in hashing class.\n\n    Example:\n    ```\n    # Average hash for images\n    from imagededup.methods import AHash\n    ahasher = AHash()\n    average_hash = ahasher.encode_image(image_file = \'path/to/image.jpg\')\n    OR\n    average_hash = ahasher.encode_image(image_array = <numpy image array>)\n    OR\n    average_hashes = ahasher.encode_images(image_dir = \'path/to/directory\')  # for a directory of images\n\n    # Finding duplicates:\n    from imagededup.methods import AHash\n    ahasher = AHash()\n    duplicates = ahasher.find_duplicates(image_dir=\'path/to/directory\', max_distance_threshold=15, scores=True)\n    OR\n    duplicates = ahasher.find_duplicates(encoding_map=encoding_map, max_distance_threshold=15, scores=True)\n\n    # Finding duplicates to return a single list of duplicates in the image collection\n    from imagededup.methods import AHash\n    ahasher = AHash()\n    files_to_remove = ahasher.find_duplicates_to_remove(image_dir=\'path/to/images/directory\',\n                      max_distance_threshold=15)\n    OR\n    files_to_remove = ahasher.find_duplicates_to_remove(encoding_map=encoding_map, max_distance_threshold=15)\n    ```\n    """"""\n\n    def __init__(self, verbose: bool = True) -> None:\n        """"""\n        Initialize average hashing class.\n\n        Args:\n            verbose: Display progress bar if True else disable it. Default value is True.\n        """"""\n        super().__init__(verbose)\n        self.target_size = (8, 8)\n\n    def _hash_algo(self, image_array: np.ndarray):\n        """"""\n        Get average hash of the input image.\n\n        Args:\n            image_array: numpy array that corresponds to the image.\n\n        Returns:\n            A string representing the average hash of the image.\n        """"""\n        avg_val = np.mean(image_array)\n        hash_mat = image_array >= avg_val\n        return hash_mat\n\n\nclass DHash(Hashing):\n    """"""\n    Inherits from Hashing base class and implements difference hashing. (Implementation reference:\n    http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html)\n\n    Offers all the functionality mentioned in hashing class.\n\n    Example:\n    ```\n    # Difference hash for images\n    from imagededup.methods import DHash\n    dhasher = DHash()\n    difference_hash = dhasher.encode_image(image_file = \'path/to/image.jpg\')\n    OR\n    difference_hash = dhasher.encode_image(image_array = <numpy image array>)\n    OR\n    difference_hashes = dhasher.encode_images(image_dir = \'path/to/directory\')  # for a directory of images\n\n    # Finding duplicates:\n    from imagededup.methods import DHash\n    dhasher = DHash()\n    duplicates = dhasher.find_duplicates(image_dir=\'path/to/directory\', max_distance_threshold=15, scores=True)\n    OR\n    duplicates = dhasher.find_duplicates(encoding_map=encoding_map, max_distance_threshold=15, scores=True)\n\n    # Finding duplicates to return a single list of duplicates in the image collection\n    from imagededup.methods import DHash\n    dhasher = DHash()\n    files_to_remove = dhasher.find_duplicates_to_remove(image_dir=\'path/to/images/directory\',\n                      max_distance_threshold=15)\n    OR\n    files_to_remove = dhasher.find_duplicates_to_remove(encoding_map=encoding_map, max_distance_threshold=15)\n    ```\n    """"""\n\n    def __init__(self, verbose: bool = True) -> None:\n        """"""\n        Initialize difference hashing class.\n\n        Args:\n            verbose: Display progress bar if True else disable it. Default value is True.\n        """"""\n        super().__init__(verbose)\n        self.target_size = (9, 8)\n\n    def _hash_algo(self, image_array):\n        """"""\n        Get difference hash of the input image.\n\n        Args:\n            image_array: numpy array that corresponds to the image.\n\n        Returns:\n            A string representing the difference hash of the image.\n        """"""\n        # Calculates difference between consecutive columns and return mask\n        hash_mat = image_array[:, 1:] > image_array[:, :-1]\n        return hash_mat\n\n\nclass WHash(Hashing):\n    """"""\n    Inherits from Hashing base class and implements wavelet hashing. (Implementation reference:\n    https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5)\n\n    Offers all the functionality mentioned in hashing class.\n\n    Example:\n    ```\n    # Wavelet hash for images\n    from imagededup.methods import WHash\n    whasher = WHash()\n    wavelet_hash = whasher.encode_image(image_file = \'path/to/image.jpg\')\n    OR\n    wavelet_hash = whasher.encode_image(image_array = <numpy image array>)\n    OR\n    wavelet_hashes = whasher.encode_images(image_dir = \'path/to/directory\')  # for a directory of images\n\n    # Finding duplicates:\n    from imagededup.methods import WHash\n    whasher = WHash()\n    duplicates = whasher.find_duplicates(image_dir=\'path/to/directory\', max_distance_threshold=15, scores=True)\n    OR\n    duplicates = whasher.find_duplicates(encoding_map=encoding_map, max_distance_threshold=15, scores=True)\n\n    # Finding duplicates to return a single list of duplicates in the image collection\n    from imagededup.methods import WHash\n    whasher = WHash()\n    files_to_remove = whasher.find_duplicates_to_remove(image_dir=\'path/to/images/directory\',\n                      max_distance_threshold=15)\n    OR\n    files_to_remove = whasher.find_duplicates_to_remove(encoding_map=encoding_map, max_distance_threshold=15)\n    ```\n    """"""\n\n    def __init__(self, verbose: bool = True) -> None:\n        """"""\n        Initialize wavelet hashing class.\n\n        Args:\n            verbose: Display progress bar if True else disable it. Default value is True.\n        """"""\n        super().__init__(verbose)\n        self.target_size = (256, 256)\n        self.__wavelet_func = \'haar\'\n\n    def _hash_algo(self, image_array):\n        """"""\n        Get wavelet hash of the input image.\n\n        Args:\n            image_array: numpy array that corresponds to the image.\n\n        Returns:\n            A string representing the wavelet hash of the image.\n        """"""\n        # decomposition level set to 5 to get 8 by 8 hash matrix\n        image_array = image_array / 255\n        coeffs = pywt.wavedec2(data=image_array, wavelet=self.__wavelet_func, level=5)\n        LL_coeff = coeffs[0]\n\n        # median of LL coefficients\n        median_coef_val = np.median(np.ndarray.flatten(LL_coeff))\n\n        # return mask of all coefficients greater than mean of coefficients\n        hash_mat = LL_coeff >= median_coef_val\n        return hash_mat\n'"
imagededup/utils/__init__.py,0,b'from .plotter import plot_duplicates'
imagededup/utils/data_generator.py,0,"b'from pathlib import PurePath\nfrom typing import Tuple, List, Callable\n\nimport numpy as np\nfrom tensorflow.keras.utils import Sequence\n\nfrom imagededup.utils.image_utils import load_image\n\n\nclass DataGenerator(Sequence):\n    """"""Class inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator.\n\n    Attributes:\n        image_dir: Path of image directory.\n        batch_size: Number of images per batch.\n        basenet_preprocess: Basenet specific preprocessing function.\n        target_size: Dimensions that images get resized into when loaded.\n    """"""\n\n    def __init__(\n        self,\n        image_dir: PurePath,\n        batch_size: int,\n        basenet_preprocess: Callable,\n        target_size: Tuple[int, int],\n    ) -> None:\n        """"""Init DataGenerator object.\n        """"""\n        self.image_dir = image_dir\n        self.batch_size = batch_size\n        self.basenet_preprocess = basenet_preprocess\n        self.target_size = target_size\n        self.counter = 0\n\n        self._get_image_files()\n        self.on_epoch_end()\n\n    def _get_image_files(self) -> None:\n        self.invalid_image_idx = []\n        self.image_files = sorted(\n            [\n                i.absolute()\n                for i in self.image_dir.glob(\'*\')\n                if not i.name.startswith(\'.\')]\n        )  # ignore hidden files\n\n    def on_epoch_end(self) -> None:\n        """"""Method called at the end of every epoch.\n        """"""\n        self.indexes = np.arange(len(self.image_files))\n        self.valid_image_files = [\n            j for i, j in enumerate(self.image_files) if i not in self.invalid_image_idx\n        ]\n\n    def __len__(self) -> int:\n        """"""Number of batches in the Sequence.""""""\n        return int(np.ceil(len(self.image_files) / self.batch_size))\n\n    def __getitem__(self, index: int) -> Tuple[np.array, np.array]:\n        """"""Get batch at position `index`.\n        """"""\n        batch_indexes = self.indexes[\n            index * self.batch_size : (index + 1) * self.batch_size\n        ]\n        batch_samples = [self.image_files[i] for i in batch_indexes]\n        X = self._data_generator(batch_samples)\n        return X\n\n    def _data_generator(\n        self, image_files: List[PurePath]\n    ) -> Tuple[np.array, np.array]:\n        """"""Generate data from samples in specified batch.""""""\n        #  initialize images and labels tensors for faster processing\n        X = np.empty((len(image_files), *self.target_size, 3))\n\n        invalid_image_idx = []\n        for i, image_file in enumerate(image_files):\n            # load and randomly augment image\n            img = load_image(\n                image_file=image_file, target_size=self.target_size, grayscale=False\n            )\n\n            if img is not None:\n                X[i, :] = img\n\n            else:\n                invalid_image_idx.append(i)\n                self.invalid_image_idx.append(self.counter)\n\n            self.counter += 1\n\n        if invalid_image_idx:\n            X = np.delete(X, invalid_image_idx, axis=0)\n\n        # apply basenet specific preprocessing\n        # input is 4D numpy array of RGB values within [0, 255]\n        X = self.basenet_preprocess(X)\n\n        return X\n'"
imagededup/utils/general_utils.py,0,"b'import json\nimport tqdm\nfrom multiprocessing import cpu_count, Pool\nfrom typing import Callable, Dict, List\nfrom imagededup.utils.logger import return_logger\n\nlogger = return_logger(__name__)\n\n\ndef get_files_to_remove(duplicates: Dict[str, List]) -> List:\n    """"""\n    Get a list of files to remove.\n\n    Args:\n        duplicates: A dictionary with file name as key and a list of duplicate file names as value.\n\n    Returns:\n        A list of files that should be removed.\n    """"""\n    # iterate over dict_ret keys, get value for the key and delete the dict keys that are in the value list\n    files_to_remove = set()\n\n    for k, v in duplicates.items():\n        tmp = [\n            i[0] if isinstance(i, tuple) else i for i in v\n        ]  # handle tuples (image_id, score)\n\n        if k not in files_to_remove:\n            files_to_remove.update(tmp)\n\n    return list(files_to_remove)\n\n\ndef save_json(results: Dict, filename: str, float_scores: bool = False) -> None:\n    """"""\n    Save results with a filename.\n\n    Args:\n        results: Dictionary of results to be saved.\n        filename: Name of the file to be saved.\n        float_scores: boolean to indicate if scores are floats.\n    """"""\n    logger.info(\'Start: Saving duplicates as json!\')\n\n    if float_scores:\n        for _file, dup_list in results.items():\n            if dup_list:\n                typecasted_dup_list = []\n                for dup in dup_list:\n                    typecasted_dup_list.append((dup[0], float(dup[1])))\n\n                results[_file] = typecasted_dup_list\n\n    with open(filename, \'w\') as f:\n        json.dump(results, f, indent=2, sort_keys=True)\n\n    logger.info(\'End: Saving duplicates as json!\')\n\n\ndef parallelise(function: Callable, data: List, verbose: bool) -> List:\n    pool = Pool(processes=cpu_count())\n    results = list(\n        tqdm.tqdm(pool.imap(function, data, 100), total=len(data), disable=not verbose)\n    )\n    pool.close()\n    pool.join()\n    return results\n'"
imagededup/utils/image_utils.py,0,"b'from pathlib import PurePath\nfrom typing import List, Union, Tuple\n\nimport numpy as np\nfrom PIL import Image\n\nfrom imagededup.utils.logger import return_logger\n\n\nIMG_FORMATS = [\'JPEG\', \'PNG\', \'BMP\', \'MPO\', \'PPM\', \'TIFF\', \'GIF\']\nlogger = return_logger(__name__)\n\n\ndef preprocess_image(\n    image, target_size: Tuple[int, int] = None, grayscale: bool = False\n) -> np.ndarray:\n    """"""\n    Take as input an image as numpy array or Pillow format. Returns an array version of optionally resized and grayed\n    image.\n\n    Args:\n        image: numpy array or a pillow image.\n        target_size: Size to resize the input image to.\n        grayscale: A boolean indicating whether to grayscale the image.\n\n    Returns:\n        A numpy array of the processed image.\n    """"""\n    if isinstance(image, np.ndarray):\n        image = image.astype(\'uint8\')\n        image_pil = Image.fromarray(image)\n\n    elif isinstance(image, Image.Image):\n        image_pil = image\n    else:\n        raise ValueError(\'Input is expected to be a numpy array or a pillow object!\')\n\n    if target_size:\n        image_pil = image_pil.resize(target_size, Image.ANTIALIAS)\n\n    if grayscale:\n        image_pil = image_pil.convert(\'L\')\n\n    return np.array(image_pil).astype(\'uint8\')\n\n\ndef load_image(\n    image_file: Union[PurePath, str],\n    target_size: Tuple[int, int] = None,\n    grayscale: bool = False,\n    img_formats: List[str] = IMG_FORMATS,\n) -> np.ndarray:\n    """"""\n    Load an image given its path. Returns an array version of optionally resized and grayed image. Only allows images\n    of types described by img_formats argument.\n\n    Args:\n        image_file: Path to the image file.\n        target_size: Size to resize the input image to.\n        grayscale: A boolean indicating whether to grayscale the image.\n        img_formats: List of allowed image formats that can be loaded.\n    """"""\n    try:\n        img = Image.open(image_file)\n\n        # validate image format\n        if img.format not in img_formats:\n            logger.warning(f\'Invalid image format {img.format}!\')\n            return None\n\n        else:\n            if img.mode != \'RGB\':\n                # convert to RGBA first to avoid warning\n                # we ignore alpha channel if available\n                img = img.convert(\'RGBA\').convert(\'RGB\')\n\n            img = preprocess_image(img, target_size=target_size, grayscale=grayscale)\n\n            return img\n\n    except Exception as e:\n        logger.warning(f\'Invalid image file {image_file}:\\n{e}\')\n        return None\n'"
imagededup/utils/logger.py,0,"b""import logging\n\n\ndef return_logger(name: str) -> logging.Logger:\n    # Set log message format\n\n    logger = logging.getLogger(name)\n\n    if not len(logger.handlers):\n        log_formatter = logging.Formatter('%(asctime)-s: %(levelname)-s %(message)s')\n        # set logging level\n        logger.setLevel(logging.DEBUG)\n\n        # Direct logs to stdout\n        console_handler = logging.StreamHandler()\n        console_handler.setFormatter(log_formatter)\n        logger.addHandler(console_handler)\n\n    return logger\n"""
imagededup/utils/plotter.py,0,"b'import matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nfrom matplotlib import figure\nfrom pathlib import Path, PurePath\nfrom typing import Dict, Union, List\n\nimport numpy as np\nfrom PIL import Image\n\n\ndef _formatter(val: Union[int, np.float32]):\n    """"""\n    For printing floats only upto 3rd precision. Ints are unchanged.\n    """"""\n    if isinstance(val, np.float32):\n        return f\'{val:.3f}\'\n    else:\n        return val\n\n\ndef _plot_images(\n    image_dir: PurePath,\n    orig: str,\n    image_list: List,\n    scores: bool = False,\n    outfile: str = None,\n) -> None:\n    """"""\n    Plotting function for plot_duplicates() defined below.\n\n    Args:\n        image_dir: image directory where all files in duplicate_map are present.\n        orig: filename for which duplicates are to be plotted.\n        image_list: List of duplicate filenames, could also be with scores (filename, score).\n        scores: Whether only filenames are present in the image_list or scores as well.\n        outfile:  Name of the file to save the plot.\n    """"""\n    n_ims = len(image_list)\n    ncols = 4  # fixed for a consistent layout\n    nrows = int(np.ceil(n_ims / ncols)) + 1\n    fig = figure.Figure(figsize=(10, 14))\n\n    gs = gridspec.GridSpec(nrows=nrows, ncols=ncols)\n    ax = plt.subplot(\n        gs[0, 1:3]\n    )  # Always plot the original image in the middle of top row\n    ax.imshow(Image.open(image_dir / orig))\n    ax.set_title(\'Original Image: {}\'.format(orig))\n    ax.axis(\'off\')\n\n    for i in range(0, n_ims):\n        row_num = (i // ncols) + 1\n        col_num = i % ncols\n\n        ax = plt.subplot(gs[row_num, col_num])\n        if scores:\n            ax.imshow(Image.open(image_dir / image_list[i][0]))\n            val = _formatter(image_list[i][1])\n            title = \' \'.join([image_list[i][0], f\'({val})\'])\n        else:\n            ax.imshow(Image.open(image_dir / image_list[i]))\n            title = image_list[i]\n\n        ax.set_title(title, fontsize=6)\n        ax.axis(\'off\')\n    gs.tight_layout(fig)\n\n    if outfile:\n        plt.savefig(outfile)\n\n    plt.show()\n\n\ndef _validate_args(\n    image_dir: Union[PurePath, str], duplicate_map: Dict, filename: str\n) -> PurePath:\n    """"""Argument validator for plot_duplicates() defined below.\n    Return PurePath to the image directory""""""\n\n    image_dir = Path(image_dir)\n    assert (\n        image_dir.is_dir()\n    ), \'Provided image directory does not exist! Please provide the image directory where all files are present!\'\n\n    if not isinstance(duplicate_map, dict):\n        raise ValueError(\'Please provide a valid Duplicate map!\')\n    if filename not in duplicate_map.keys():\n        raise ValueError(\n            \'Please provide a valid filename present as a key in the duplicate_map!\'\n        )\n    return image_dir\n\n\ndef plot_duplicates(\n    image_dir: Union[PurePath, str],\n    duplicate_map: Dict,\n    filename: str,\n    outfile: str = None,\n) -> None:\n    """"""\n    Given filename for an image, plot duplicates along with the original image using the duplicate map obtained using\n    find_duplicates method.\n\n    Args:\n        image_dir: image directory where all files in duplicate_map are present.\n        duplicate_map: mapping of filename to found duplicates (could be with or without scores).\n        filename: Name of the file for which duplicates are to be plotted, must be a key in the duplicate_map.\n        dictionary.\n        outfile: Optional, name of the file to save the plot. Default is None.\n\n    Example:\n    ```\n        from imagededup.utils import plot_duplicates\n        plot_duplicates(image_dir=\'path/to/image/directory\',\n                        duplicate_map=duplicate_map,\n                        filename=\'path/to/image.jpg\')\n    ```\n    """"""\n    # validate args\n    image_dir = _validate_args(image_dir=image_dir, duplicate_map=duplicate_map, filename=filename)\n\n    retrieved = duplicate_map[filename]\n    assert len(retrieved) != 0, \'Provided filename has no duplicates!\'\n\n    # plot\n    if isinstance(retrieved[0], tuple):\n        _plot_images(\n            image_dir=image_dir,\n            orig=filename,\n            image_list=retrieved,\n            scores=True,\n            outfile=outfile,\n        )\n    else:\n        _plot_images(\n            image_dir=image_dir,\n            orig=filename,\n            image_list=retrieved,\n            scores=False,\n            outfile=outfile,\n        )\n'"
imagededup/handlers/metrics/__init__.py,0,b''
imagededup/handlers/metrics/classification.py,0,"b'import itertools\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\n\nfrom sklearn.metrics import (\n    classification_report,\n    precision_score,\n    recall_score,\n    precision_recall_fscore_support,\n)\nfrom imagededup.utils.logger import return_logger\n\nlogger = return_logger(__name__)\n\n\ndef _get_unique_ordered_tuples(unique_tuples: List[Tuple]) -> List[Tuple]:\n    """"""Sort each tuple given a list of tuples and retain only unique pairs regardless of order within the tuple.\n    Eg: [(2, 1), (1, 2), (3, 4)]  becomes [(1, 2), (3, 4)]""""""\n\n    return list(set([tuple(sorted(i)) for i in unique_tuples]))\n\n\ndef _make_all_unique_possible_pairs(ground_truth_dict: Dict) -> List[Tuple]:\n    """"""\n    Given a ground truth dictionary, generate all possible unique image pairs (both negative and positive pairs).\n    """"""\n    # get all elements of the dictionary\n    all_files = list(ground_truth_dict.keys())\n\n    # make all possible pairs (remove pairs with same elements)\n    all_tuples = [i for i in itertools.product(all_files, all_files) if i[0] != i[1]]\n    return _get_unique_ordered_tuples(all_tuples)\n\n\ndef _make_positive_duplicate_pairs(ground_truth: Dict, retrieved: Dict) -> List[Tuple]:\n    """"""\n    Given ground_truth and retrieved dictionary, generate all unique positive pairs.\n    """"""\n    pairs = []\n\n    for mapping in [ground_truth, retrieved]:\n        valid_pairs = []\n\n        for k, v in mapping.items():\n            valid_pairs.extend(list(zip([k]*len(v), v)))\n        pairs.append(_get_unique_ordered_tuples(valid_pairs))\n\n    return pairs[0], pairs[1]\n\n\ndef _prepare_labels(\n    complete_pairs: List[Tuple],\n    ground_truth_pairs: List[Tuple],\n    retrieved_pairs: List[Tuple],\n) -> Tuple[List, List]:\n    """"""\n    Given all possible unique pairs, ground truth positive pairs and retrieved positive pairs, generate true and\n    predicted labels to feed into classification metrics functions.\n    """"""\n    ground_truth_pairs = set(ground_truth_pairs)\n    retrieved_pairs = set(retrieved_pairs)\n\n    y_true = [1 if i in ground_truth_pairs else 0 for i in complete_pairs]\n    y_pred = [1 if i in retrieved_pairs else 0 for i in complete_pairs]\n    return y_true, y_pred\n\n\ndef classification_metrics(ground_truth: Dict, retrieved: Dict) -> np.ndarray:\n    """"""\n    Given ground truth dictionary and retrieved dictionary, return per class precision, recall and f1 score. Class 1 is\n    assigned to duplicate file pairs while class 0 is for non-duplicate file pairs.\n\n    Args:\n        ground_truth: A dictionary representing ground truth with filenames as key and a list of duplicate filenames\n        as value.\n        retrieved: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved\n        duplicate filenames as value.\n\n    Returns:\n        Dictionary of precision, recall and f1 score for both classes.\n    """"""\n    all_pairs = _make_all_unique_possible_pairs(ground_truth)\n    ground_truth_duplicate_pairs, retrieved_duplicate_pairs = _make_positive_duplicate_pairs(\n        ground_truth, retrieved\n    )\n    y_true, y_pred = _prepare_labels(\n        all_pairs, ground_truth_duplicate_pairs, retrieved_duplicate_pairs\n    )\n    logger.info(classification_report(y_true, y_pred))\n    prec_rec_fscore_support = dict(\n        zip(\n            (\'precision\', \'recall\', \'f1_score\', \'support\'),\n            precision_recall_fscore_support(y_true, y_pred),\n        )\n    )\n    return prec_rec_fscore_support\n'"
imagededup/handlers/metrics/information_retrieval.py,0,"b'from typing import List, Dict\n\nimport numpy as np\n\n\ndef avg_prec(correct_duplicates: List, retrieved_duplicates: List) -> float:\n    """"""\n    Get average precision(AP) for a single query given correct and retrieved file names.\n\n    Args:\n        correct_duplicates: List of correct duplicates i.e., ground truth)\n        retrieved_duplicates: List of retrieved duplicates for one single query\n\n    Returns:\n        Average precision for this query.\n    """"""\n    if len(retrieved_duplicates) == 0 and len(correct_duplicates) == 0:\n        return 1.0\n\n    if not len(retrieved_duplicates) or not len(correct_duplicates):\n        return 0.0\n\n    count_real_correct = len(correct_duplicates)\n    relevance = np.array(\n        [1 if i in correct_duplicates else 0 for i in retrieved_duplicates]\n    )\n    relevance_cumsum = np.cumsum(relevance)\n    prec_k = [relevance_cumsum[k] / (k + 1) for k in range(len(relevance))]\n    prec_and_relevance = [relevance[k] * prec_k[k] for k in range(len(relevance))]\n    avg_precision = np.sum(prec_and_relevance) / count_real_correct\n    return avg_precision\n\n\ndef ndcg(correct_duplicates: List, retrieved_duplicates: List) -> float:\n    """"""\n    Get Normalized discounted cumulative gain(NDCG) for a single query given correct and retrieved file names.\n\n    Args:\n        correct_duplicates: List of correct duplicates i.e., ground truth)\n        retrieved_duplicates: List of retrieved duplicates for one single query\n\n    Returns:\n        NDCG for this query.\n    """"""\n    if len(retrieved_duplicates) == 0 and len(correct_duplicates) == 0:\n        return 1.0\n\n    if not len(retrieved_duplicates) or not len(correct_duplicates):\n        return 0.0\n\n    def dcg(rel):\n        relevance_numerator = [2 ** (k) - 1 for k in rel]\n        relevance_denominator = [\n            np.log2(k + 2) for k in range(len(rel))\n        ]  # first value of denominator term should be 2\n\n        dcg_terms = [\n            relevance_numerator[k] / relevance_denominator[k] for k in range(len(rel))\n        ]\n        dcg_at_k = np.sum(dcg_terms)\n\n        return dcg_at_k\n\n    relevance = np.array(\n        [1 if i in correct_duplicates else 0 for i in retrieved_duplicates]\n    )\n    dcg_k = dcg(relevance)\n\n    if dcg_k == 0:\n        return 0.0\n\n    idcg_k = dcg(sorted(relevance, reverse=True))\n    return dcg_k / idcg_k\n\n\ndef jaccard_similarity(correct_duplicates: List, retrieved_duplicates: List) -> float:\n    """"""\n    Get jaccard similarity for a single query given correct and retrieved file names.\n\n    Args:\n        correct_duplicates: List of correct duplicates i.e., ground truth)\n        retrieved_duplicates: List of retrieved duplicates for one single query\n\n    Returns:\n        Jaccard similarity for this query.\n    """"""\n    if len(retrieved_duplicates) == 0 and len(correct_duplicates) == 0:\n        return 1.0\n\n    if not len(retrieved_duplicates) or not len(correct_duplicates):\n        return 0.0\n\n    set_correct_duplicates = set(correct_duplicates)\n    set_retrieved_duplicates = set(retrieved_duplicates)\n\n    intersection_dups = set_retrieved_duplicates.intersection(set_correct_duplicates)\n    union_dups = set_retrieved_duplicates.union(set_correct_duplicates)\n\n    jacc_sim = len(intersection_dups) / len(union_dups)\n    return jacc_sim\n\n\ndef mean_metric(ground_truth: Dict, retrieved: Dict, metric: str = None) -> float:\n    """"""\n    Get mean of specified metric.\n\n    Args:\n        metric_func: metric function on which mean is to be calculated across all queries\n\n    Returns:\n        float representing mean of the metric across all queries\n    """"""\n    metric = metric.lower()\n    metric_lookup = {\'map\': avg_prec, \'ndcg\': ndcg, \'jaccard\': jaccard_similarity}\n\n    metric_func = metric_lookup[metric]\n    metric_vals = []\n\n    for k in ground_truth.keys():\n        metric_vals.append(metric_func(ground_truth[k], retrieved[k]))\n    return np.mean(metric_vals)\n\n\ndef get_all_metrics(ground_truth: Dict, retrieved: Dict) -> Dict:\n    """"""\n    Get mean of all information retrieval metrics across all queries.\n\n    Args:\n        ground_truth: A dictionary representing ground truth with filenames as key and a list of duplicate filenames\n        as value.\n        retrieved: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved\n        duplicate filenames as value.\n\n    Returns:\n        Dictionary of all mean metrics.\n    """"""\n\n    all_average_metrics = {\n        \'map\': mean_metric(ground_truth, retrieved, metric=\'map\'),\n        \'ndcg\': mean_metric(ground_truth, retrieved, metric=\'ndcg\'),\n        \'jaccard\': mean_metric(ground_truth, retrieved, metric=\'jaccard\'),\n    }\n    return all_average_metrics\n'"
imagededup/handlers/search/__init__.py,0,b''
imagededup/handlers/search/bktree.py,0,"b'import copy\nfrom typing import Callable, Dict, Tuple\n\n# Implementation reference: https://signal-to-noise.xyz/post/bk-tree/\n\n\nclass BkTreeNode:\n    """"""\n    Class to contain the attributes of a single node in the BKTree.\n    """"""\n\n    def __init__(\n        self, node_name: str, node_value: str, parent_name: str = None\n    ) -> None:\n        self.node_name = node_name\n        self.node_value = node_value\n        self.parent_name = parent_name\n        self.children = {}\n\n\nclass BKTree:\n    """"""\n    Class to construct and perform search using a BKTree.\n    """"""\n\n    def __init__(self, hash_dict: Dict, distance_function: Callable) -> None:\n        """"""\n        Initialize a root for the BKTree and triggers the tree construction using the dictionary for mapping file names\n        and corresponding hashes.\n\n        Args:\n            hash_dict: Dictionary mapping file names to corresponding hash strings {filename: hash}\n            distance_function: A function for calculating distance between the hashes.\n        """"""\n        self.hash_dict = hash_dict  # database\n        self.distance_function = distance_function\n        self.all_keys = list(self.hash_dict.keys())\n        self.ROOT = self.all_keys[0]\n        self.all_keys.remove(self.ROOT)\n        self.dict_all = {self.ROOT: BkTreeNode(self.ROOT, self.hash_dict[self.ROOT])}\n        self.candidates = [self.dict_all[self.ROOT].node_name]  # Initial value is root\n        self.construct_tree()\n\n    def _insert_in_tree(self, k: str, current_node: str) -> int:\n        """"""\n        Function to insert a new node into the BKTree.\n\n        Args:\n            k: filename for inserting into the BKTree.\n            current_node: Node of the tree to which the new node should be added.\n\n        Return:\n            0 for successful execution.\n        """"""\n        dist_current_node = self.distance_function(\n            self.hash_dict[k], self.dict_all[current_node].node_value\n        )\n        condition_insert_current_node_child = (\n            not self.dict_all[current_node].children\n        ) or (\n            dist_current_node not in list(self.dict_all[current_node].children.values())\n        )\n        if condition_insert_current_node_child:\n            self.dict_all[current_node].children[k] = dist_current_node\n            self.dict_all[k] = BkTreeNode(\n                k, self.hash_dict[k], parent_name=current_node\n            )\n        else:\n            for i, val in self.dict_all[current_node].children.items():\n                if val == dist_current_node:\n                    node_to_add_to = i\n                    break\n            self._insert_in_tree(k, node_to_add_to)\n        return 0\n\n    def construct_tree(self) -> None:\n        """"""\n        Construct the BKTree.\n        """"""\n        for k in self.all_keys:\n            self._insert_in_tree(k, self.ROOT)\n\n    def _get_next_candidates(\n        self, query: str, candidate_obj: BkTreeNode, tolerance: int\n    ) -> Tuple[list, int, float]:\n        """"""\n        Get candidates for checking if the query falls within the distance tolerance. Sets a validity flag if the input\n        candidate BKTree node is valid (distance to this candidate is within the distance tolerance from the query.)\n\n        Args:\n            query: The hash for which retrievals are needed.\n            candidate_obj: A BKTree object which is a candidate for being checked as valid.\n            tolerance: Distance within which the candidate is considered valid.\n\n        Returns:\n            new candidates to examine, validity flag indicating whether current candidate is within the distance\n            tolerance, distance of the current candidate from the query hash.\n        """"""\n        dist = self.distance_function(candidate_obj.node_value, query)\n        if dist <= tolerance:\n            validity = 1\n        else:\n            validity = 0\n        search_range_dist = list(range(dist - tolerance, dist + tolerance + 1))\n        candidate_children = candidate_obj.children\n        candidates = [\n            k\n            for k in candidate_children.keys()\n            if candidate_children[k] in search_range_dist\n        ]\n        return candidates, validity, dist\n\n    def search(self, query: str, tol: int = 5) -> Dict:\n        """"""\n        Function to search the bktree given a hash of the query image.\n\n        Args:\n            query: hash string for which BKTree needs to be searched.\n            tol: distance upto which duplicate is valid.\n\n        Returns:\n            List of tuples of the form [(valid_retrieval_filename1: distance), (valid_retrieval_filename2: distance)]\n        """"""\n\n        valid_retrievals = []\n        candidates_local = copy.deepcopy(self.candidates)\n        while len(candidates_local) != 0:\n            candidate_name = candidates_local.pop()\n            cand_list, valid_flag, dist = self._get_next_candidates(\n                query, self.dict_all[candidate_name], tolerance=tol\n            )\n            if valid_flag:\n                valid_retrievals.append(\n                    (candidate_name, int(dist))\n                )  # typecast dist to int to save later as np.int64\n                # can\'t be saved by json\n            candidates_local.extend(cand_list)\n        return valid_retrievals\n'"
imagededup/handlers/search/brute_force.py,0,"b'from typing import Callable, Dict\n\n\nclass BruteForce:\n    """"""\n    Class to perform search using a Brute force.\n    """"""\n\n    def __init__(self, hash_dict: Dict, distance_function: Callable) -> None:\n        """"""\n        Initialize a dictionary for mapping file names and corresponding hashes and a distance function to be used for\n        getting distance between two hash strings.\n\n        Args:\n            hash_dict: Dictionary mapping file names to corresponding hash strings {filename: hash}\n            distance_function:  A function for calculating distance between the hashes.\n        """"""\n        self.distance_function = distance_function\n        self.hash_dict = hash_dict  # database\n\n    def search(self, query: str, tol: int = 10) -> Dict[str, int]:\n        """"""\n        Function for searching using brute force.\n\n        Args:\n            query: hash string for which brute force needs to work.\n            tol: distance upto which duplicate is valid.\n\n        Returns:\n            List of tuples of the form [(valid_retrieval_filename1: distance), (valid_retrieval_filename2: distance)]\n        """"""\n        return [\n            (item, self.distance_function(query, self.hash_dict[item]))\n            for item in self.hash_dict\n            if self.distance_function(query, self.hash_dict[item]) <= tol\n        ]\n'"
imagededup/handlers/search/brute_force_cython.py,0,"b'from typing import Callable, Dict\n\nimport brute_force_cython_ext\n\n\nclass BruteForceCython:\n    """"""\n    Class to perform search using a Brute force.\n    """"""\n\n    def __init__(self, hash_dict: Dict, distance_function: Callable) -> None:\n        """"""\n        Initialize a dictionary for mapping file names and corresponding hashes and a distance function to be used for\n        getting distance between two hash strings.\n\n        Args:\n            hash_dict: Dictionary mapping file names to corresponding hash strings {filename: hash}\n            distance_function:  A function for calculating distance between the hashes.\n        """"""\n        self.distance_function = distance_function\n        self.hash_dict = hash_dict  # database\n\n        brute_force_cython_ext.clear()\n\n        for filename, hash_val in self.hash_dict.items():\n            brute_force_cython_ext.add(\n                int(hash_val, 16), filename.encode(\'utf-8\')\n            )  # cast hex hash_val to decimals for __builtin_popcountll function\n\n    def search(self, query: str, tol: int = 10) -> Dict[str, int]:\n        """"""\n        Function for searching using brute force.\n\n        Args:\n            query: hash string for which brute force needs to work.\n            tol: distance upto which duplicate is valid.\n\n        Returns:\n            List of tuples of the form [(valid_retrieval_filename1: distance), (valid_retrieval_filename2: distance)]\n        """"""\n\n        return brute_force_cython_ext.query(\n            int(query, 16), tol\n        )  # cast hex hash_val to decimals for __builtin_popcountll function\n'"
imagededup/handlers/search/retrieval.py,0,"b'import sys\nfrom typing import Callable, Dict, Union, Tuple\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom imagededup.handlers.search.bktree import BKTree\nfrom imagededup.handlers.search.brute_force import BruteForce\nfrom imagededup.handlers.search.brute_force_cython import BruteForceCython\nfrom imagededup.utils.general_utils import parallelise\nfrom imagededup.utils.logger import return_logger\n\nlogger = return_logger(__name__)\n\n\ndef cosine_similarity_chunk(t: Tuple) -> np.ndarray:\n    return cosine_similarity(t[0][t[1][0] : t[1][1]], t[0]).astype(\'float16\')\n\n\ndef get_cosine_similarity(\n    X: np.ndarray, verbose: bool = True, chunk_size: int = 1000, threshold: int = 10000\n) -> np.ndarray:\n    n_rows = X.shape[0]\n\n    if n_rows <= threshold:\n        return cosine_similarity(X)\n\n    else:\n        logger.info(\n            \'Large feature matrix thus calculating cosine similarities in chunks...\'\n        )\n        start_idxs = list(range(0, n_rows, chunk_size))\n        end_idxs = start_idxs[1:] + [n_rows]\n        cos_sim = parallelise(\n            cosine_similarity_chunk,\n            [(X, idxs) for i, idxs in enumerate(zip(start_idxs, end_idxs))],\n            verbose,\n        )\n\n        return np.vstack(cos_sim)\n\n\nclass HashEval:\n    def __init__(\n        self,\n        test: Dict,\n        queries: Dict,\n        distance_function: Callable,\n        verbose: bool = True,\n        threshold: int = 5,\n        search_method: str = \'brute_force_cython\' if not sys.platform == \'win32\' else \'bktree\',\n    ) -> None:\n        """"""\n        Initialize a HashEval object which offers an interface to control hashing and search methods for desired\n        dataset. Compute a map of duplicate images in the document space given certain input control parameters.\n        """"""\n        self.test = test  # database\n        self.queries = queries\n        self.distance_invoker = distance_function\n        self.verbose = verbose\n        self.threshold = threshold\n        self.query_results_map = None\n\n        if search_method == \'bktree\':\n            self._fetch_nearest_neighbors_bktree()\n        elif search_method == \'brute_force\':\n            self._fetch_nearest_neighbors_brute_force()\n        else:\n            self._fetch_nearest_neighbors_brute_force_cython()\n\n    def _searcher(self, data_tuple) -> None:\n        """"""\n        Perform search on a query passed in by _get_query_results multiprocessing part.\n\n        Args:\n            data_tuple: Tuple of (query_key, query_val, search_method_object, thresh)\n\n        Returns:\n           List of retrieved duplicate files and corresponding hamming distance for the query file.\n        """"""\n        query_key, query_val, search_method_object, thresh = data_tuple\n        res = search_method_object.search(query=query_val, tol=thresh)\n        res = [i for i in res if i[0] != query_key]  # to avoid self retrieval\n        return res\n\n    def _get_query_results(\n        self, search_method_object: Union[BruteForce, BKTree]\n    ) -> None:\n        """"""\n        Get result for the query using specified search object. Populate the global query_results_map.\n\n        Args:\n            search_method_object: BruteForce or BKTree object to get results for the query.\n        """"""\n        args = list(\n            zip(\n                list(self.queries.keys()),\n                list(self.queries.values()),\n                [search_method_object] * len(self.queries),\n                [self.threshold] * len(self.queries),\n            )\n        )\n        result_map_list = parallelise(self._searcher, args, self.verbose)\n        result_map = dict(zip(list(self.queries.keys()), result_map_list))\n\n        self.query_results_map = {\n            k: [i for i in sorted(v, key=lambda tup: tup[1], reverse=False)]\n            for k, v in result_map.items()\n        }  # {\'filename.jpg\': [(\'dup1.jpg\', 3)], \'filename2.jpg\': [(\'dup2.jpg\', 10)]}\n\n    def _fetch_nearest_neighbors_brute_force(self) -> None:\n        """"""\n        Wrapper function to retrieve results for all queries in dataset using brute-force search.\n        """"""\n        logger.info(\'Start: Retrieving duplicates using Brute force algorithm\')\n        brute_force = BruteForce(self.test, self.distance_invoker)\n        self._get_query_results(brute_force)\n        logger.info(\'End: Retrieving duplicates using Brute force algorithm\')\n\n    def _fetch_nearest_neighbors_brute_force_cython(self) -> None:\n        """"""\n        Wrapper function to retrieve results for all queries in dataset using brute-force search.\n        """"""\n        logger.info(\'Start: Retrieving duplicates using Cython Brute force algorithm\')\n        brute_force_cython = BruteForceCython(self.test, self.distance_invoker)\n        self._get_query_results(brute_force_cython)\n        logger.info(\'End: Retrieving duplicates using Cython Brute force algorithm\')\n\n    def _fetch_nearest_neighbors_bktree(self) -> None:\n        """"""\n        Wrapper function to retrieve results for all queries in dataset using a BKTree search.\n        """"""\n        logger.info(\'Start: Retrieving duplicates using BKTree algorithm\')\n        built_tree = BKTree(self.test, self.distance_invoker)  # construct bktree\n        self._get_query_results(built_tree)\n        logger.info(\'End: Retrieving duplicates using BKTree algorithm\')\n\n    def retrieve_results(self, scores: bool = False) -> Dict:\n        """"""\n        Return results with or without scores.\n\n        Args:\n            scores: Boolean indicating whether results are to eb returned with or without scores.\n\n        Returns:\n            if scores is True, then a dictionary of the form {\'image1.jpg\': [(\'image1_duplicate1.jpg\',\n            score), (\'image1_duplicate2.jpg\', score)], \'image2.jpg\': [] ..}\n            if scores is False, then a dictionary of the form {\'image1.jpg\': [\'image1_duplicate1.jpg\',\n            \'image1_duplicate2.jpg\'], \'image2.jpg\':[\'image1_duplicate1.jpg\',..], ..}\n        """"""\n        if scores:\n            return self.query_results_map\n        else:\n            return {k: [i[0] for i in v] for k, v in self.query_results_map.items()}\n'"
