file_path,api_count,code
src/bilstm.py,61,"b'from __future__ import print_function\nfrom __future__ import division\nimport tensorflow as tf\nimport numpy as np\nimport tf_utils\n\nclass BiLSTM(object):\n\n    """"""\n    A bidirectional LSTM for text classification.\n    """"""\n    def __init__(self, num_classes, vocab_size, shape_domain_size, char_domain_size, char_size,\n            embedding_size, shape_size, nonlinearity, viterbi, hidden_dim, char_embeddings, embeddings=None):\n\n        self.num_classes = num_classes\n        self.shape_domain_size = shape_domain_size\n        self.char_domain_size = char_domain_size\n        self.char_size = char_size\n        self.embedding_size = embedding_size\n        self.shape_size = shape_size\n        self.hidden_dim = hidden_dim\n        self.nonlinearity = nonlinearity\n        self.char_embeddings = char_embeddings\n        self.viterbi = viterbi\n\n        # word embedding input\n        self.input_x1 = tf.placeholder(tf.int64, [None, None], name=""input_x1"")\n\n        # shape embedding input\n        self.input_x2 = tf.placeholder(tf.int64, [None, None], name=""input_x2"")\n\n        # labels\n        self.input_y = tf.placeholder(tf.int64, [None, None], name=""input_y"")\n\n        # padding mask\n        self.input_mask = tf.placeholder(tf.float32, [None, None], name=""input_mask"")\n\n        self.batch_size = tf.placeholder(tf.int32, None, name=""batch_size"")\n\n        self.max_seq_len = tf.placeholder(tf.int32, None, name=""max_seq_len"")\n\n        # sequence lengths\n        self.sequence_lengths = tf.placeholder(tf.int32, [None, None], name=""sequence_lengths"")\n\n        # dropout and l2 penalties\n        self.middle_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""middle_dropout_keep_prob"")\n        self.hidden_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""hidden_dropout_keep_prob"")\n        self.input_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""input_dropout_keep_prob"")\n        self.word_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""word_dropout_keep_prob"")\n\n        self.l2_penalty = tf.placeholder_with_default(0.0, [], name=""l2_penalty"")\n\n        self.projection = tf.placeholder_with_default(False, [], name=""projection"")\n\n        self.drop_penalty = tf.placeholder_with_default(0.0, [], name=""drop_penalty"")\n\n        # Keeping track of l2 regularization loss (optional)\n        self.l2_loss = tf.constant(0.0)\n\n        # set the pad token to a constant 0 vector\n        self.word_zero_pad = tf.constant(0.0, dtype=tf.float32, shape=[1, embedding_size])\n        self.shape_zero_pad = tf.constant(0.0, dtype=tf.float32, shape=[1, shape_size])\n        self.char_zero_pad = tf.constant(0.0, dtype=tf.float32, shape=[1, char_size])\n\n        self.use_characters = char_size != 0\n        self.use_shape = shape_size != 0\n\n        if self.viterbi:\n            self.transition_params = tf.get_variable(""transitions"", [num_classes, num_classes])\n\n        # Embedding layer\n        # with tf.device(\'/cpu:0\'), tf.name_scope(""embedding""):\n        word_embeddings_shape = (vocab_size - 1, embedding_size)\n        self.w_e = tf_utils.initialize_embeddings(word_embeddings_shape, name=""w_e"", pretrained=embeddings)\n\n        nonzero_elements = tf.not_equal(self.sequence_lengths, tf.zeros_like(self.sequence_lengths))\n        count_nonzero_per_row = tf.reduce_sum(tf.to_int32(nonzero_elements), axis=1)\n        self.flat_sequence_lengths = tf.add(tf.reduce_sum(self.sequence_lengths, 1), tf.scalar_mul(2, count_nonzero_per_row))\n\n        self.unflat_scores, self.hidden_layer = self.forward(self.input_x1, self.input_x2, self.max_seq_len,\n                                          self.hidden_dropout_keep_prob,\n                                          self.input_dropout_keep_prob, self.middle_dropout_keep_prob, reuse=False)\n\n        # Calculate mean cross-entropy loss\n        with tf.name_scope(""loss""):\n            labels = tf.cast(self.input_y, \'int32\')\n            if viterbi:\n                log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(self.unflat_scores, labels, self.flat_sequence_lengths, transition_params=self.transition_params)\n                # self.transition_params = transition_params\n                self.loss = tf.reduce_mean(-log_likelihood)\n            else:\n                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.unflat_scores, labels=labels)\n                masked_losses = tf.multiply(losses, self.input_mask)\n                self.loss = tf.div(tf.reduce_sum(masked_losses), tf.reduce_sum(self.input_mask))\n            self.loss += self.l2_penalty * self.l2_loss\n\n            self.unflat_no_dropout_scores, _ = self.forward(self.input_x1, self.input_x2, self.max_seq_len,\n                                                         1.0, 1.0, 1.0)\n\n            drop_loss = tf.nn.l2_loss(tf.subtract(self.unflat_scores, self.unflat_no_dropout_scores))\n            self.loss += self.drop_penalty * drop_loss\n\n        # Accuracy\n        with tf.name_scope(""predictions""):\n            if viterbi:\n                self.predictions = self.unflat_scores\n            else:\n                self.predictions = tf.argmax(self.unflat_scores, 2)\n\n    def forward(self, input_x1, input_x2, max_seq_len, hidden_dropout_keep_prob,\n                input_dropout_keep_prob, middle_dropout_keep_prob, reuse=True):\n        word_embeddings = tf.nn.embedding_lookup(self.w_e, input_x1)\n\n        with tf.variable_scope(""forward"", reuse=reuse):\n\n            input_list = [word_embeddings]\n            input_size = self.embedding_size\n            if self.use_characters:\n                input_list.append(self.char_embeddings)\n                input_size += self.char_size\n            if self.use_shape:\n                shape_embeddings_shape = (self.shape_domain_size - 1, self.shape_size)\n                w_s = tf_utils.initialize_embeddings(shape_embeddings_shape, name=""w_s"")\n                shape_embeddings = tf.nn.embedding_lookup(w_s, input_x2)\n                input_list.append(shape_embeddings)\n                input_size += self.shape_size\n\n            input_feats = tf.concat(axis=2, values=input_list)\n            # self.input_feats_expanded = tf.expand_dims(self.input_feats, 1)\n            input_feats_expanded_drop = tf.nn.dropout(input_feats, input_dropout_keep_prob)\n\n            total_output_width = 2*self.hidden_dim\n\n            with tf.name_scope(""bilstm""):\n                # selected_col_embeddings = tf.nn.embedding_lookup(token_embeddings, self.token_batch)\n                fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(self.hidden_dim, state_is_tuple=True)\n                bwd_cell = tf.nn.rnn_cell.BasicLSTMCell(self.hidden_dim, state_is_tuple=True)\n                lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fwd_cell, cell_bw=bwd_cell, dtype=tf.float32,\n                                                                 inputs=input_feats_expanded_drop,\n                                                                 parallel_iterations=50,\n                                                                 sequence_length=self.flat_sequence_lengths)\n                hidden_outputs = tf.concat(axis=2, values=lstm_outputs)\n\n            h_concat_flat = tf.reshape(hidden_outputs, [-1, total_output_width])\n\n            # Add dropout\n            with tf.name_scope(""middle_dropout""):\n                h_drop = tf.nn.dropout(h_concat_flat, middle_dropout_keep_prob)\n\n            # second projection\n            with tf.name_scope(""tanh_proj""):\n                w_tanh = tf_utils.initialize_weights([total_output_width, self.hidden_dim], ""w_tanh"", init_type=""xavier"")\n                b_tanh = tf.get_variable(initializer=tf.constant(0.01, shape=[self.hidden_dim]), name=""b_tanh"")\n                self.l2_loss += tf.nn.l2_loss(w_tanh)\n                self.l2_loss += tf.nn.l2_loss(b_tanh)\n                h2_concat_flat = tf.nn.xw_plus_b(h_drop, w_tanh, b_tanh, name=""h2_tanh"")\n                h2_tanh = tf_utils.apply_nonlinearity(h2_concat_flat, self.nonlinearity)\n\n            # Add dropout\n            with tf.name_scope(""hidden_dropout""):\n                h2_drop = tf.nn.dropout(h2_tanh, hidden_dropout_keep_prob)\n\n            # Final (unnormalized) scores and predictions\n            with tf.name_scope(""output""):\n                w_o = tf_utils.initialize_weights([self.hidden_dim, self.num_classes], ""w_o"", init_type=""xavier"")\n                b_o = tf.get_variable(initializer=tf.constant(0.01, shape=[self.num_classes]), name=""b_o"")\n                self.l2_loss += tf.nn.l2_loss(w_o)\n                self.l2_loss += tf.nn.l2_loss(b_o)\n                scores = tf.nn.xw_plus_b(h2_drop, w_o, b_o, name=""scores"")\n                unflat_scores = tf.reshape(scores, tf.stack([self.batch_size, max_seq_len, self.num_classes]))\n        return unflat_scores, hidden_outputs\n'"
src/bilstm_char.py,24,"b'from __future__ import print_function\nfrom __future__ import division\nimport tensorflow as tf\nimport numpy as np\nimport tf_utils\n\nclass BiLSTMChar(object):\n\n    """"""\n    A bidirectional LSTM for embedding tokens.\n    """"""\n    def __init__(self, char_domain_size, char_embedding_dim, hidden_dim, embeddings=None):\n\n        self.char_domain_size = char_domain_size\n        self.embedding_size = char_embedding_dim\n        self.hidden_dim = hidden_dim\n\n        # char embedding input\n        self.input_chars = tf.placeholder(tf.int64, [None, None], name=""input_chars"")\n\n        # padding mask\n        # self.input_mask = tf.placeholder(tf.float32, [None, None], name=""input_mask"")\n\n        self.batch_size = tf.placeholder(tf.int32, None, name=""batch_size"")\n\n        self.max_seq_len = tf.placeholder(tf.int32, None, name=""max_seq_len"")\n\n        self.max_tok_len = tf.placeholder(tf.int32, None, name=""max_tok_len"")\n\n        self.input_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""input_dropout_keep_prob"")\n\n        # sequence lengths\n        self.sequence_lengths = tf.placeholder(tf.int32, [None, None], name=""sequence_lengths"")\n        self.token_lengths = tf.placeholder(tf.int32, [None, None], name=""tok_lengths"")\n\n        self.output_size = 2*self.hidden_dim\n\n        print(""LSTM char embedding model"")\n        print(""embedding dim: "", self.embedding_size)\n        print(""out dim: "", self.output_size)\n\n        # set the pad token to a constant 0 vector\n        # self.char_zero_pad = tf.constant(0.0, dtype=tf.float32, shape=[1, self.embedding_size])\n\n        # Embedding layer\n        shape = (char_domain_size-1, self.embedding_size)\n        self.char_embeddings = tf_utils.initialize_embeddings(shape, name=""char_embeddings"", pretrained=embeddings)\n\n        self.outputs = self.forward(self.input_chars, self.input_dropout_keep_prob, reuse=False)\n\n    def forward(self, input_x1, input_dropout_keep_prob, reuse=True):\n        with tf.variable_scope(""char-forward"", reuse=reuse):\n\n            char_embeddings_lookup = tf.nn.embedding_lookup(self.char_embeddings, input_x1)\n            char_embeddings_flat = tf.reshape(char_embeddings_lookup, tf.stack([self.batch_size*self.max_seq_len, self.max_tok_len, self.embedding_size]))\n            tok_lens_flat = tf.reshape(self.token_lengths, [self.batch_size*self.max_seq_len])\n\n            #\n            # input_list = [char_embeddings_lookup]\n            # # input_size = self.embedding_size\n            #\n            # input_feats = tf.concat(2, input_list)\n\n            input_feats_drop = tf.nn.dropout(char_embeddings_flat, input_dropout_keep_prob)\n\n            # total_output_width = 2*self.hidden_dim\n\n            with tf.name_scope(""char-bilstm""):\n                # selected_col_embeddings = tf.nn.embedding_lookup(token_embeddings, self.token_batch)\n                fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(self.hidden_dim, state_is_tuple=True)\n                bwd_cell = tf.nn.rnn_cell.BasicLSTMCell(self.hidden_dim, state_is_tuple=True)\n                lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fwd_cell, cell_bw=bwd_cell, dtype=tf.float32,\n                                                                 inputs=input_feats_drop,\n                                                                 parallel_iterations=32, swap_memory=False,\n                                                                 sequence_length=tok_lens_flat)\n                outputs_fw = lstm_outputs[0]\n                outputs_bw = lstm_outputs[1]\n\n                # this is batch*output_size (flat)\n                fw_output = tf_utils.last_relevant(outputs_fw, tok_lens_flat)\n                # this is batch * max_seq_len * output_size\n                bw_output = outputs_bw[:, 0, :]\n                hidden_outputs = tf.concat(axis=1, values=[fw_output, bw_output])\n                # hidden_outputs = tf.concat(2, lstm_outputs)\n                # hidden_outputs = tf.Print(hidden_outputs, [tf.shape(hidden_outputs)], message=\'hidden outputs:\')\n                hidden_outputs_unflat = tf.reshape(hidden_outputs, tf.stack([self.batch_size, self.max_seq_len, self.output_size]))\n\n        return hidden_outputs_unflat\n'"
src/cnn.py,82,"b'from __future__ import print_function\nfrom __future__ import division\nimport tensorflow as tf\nimport numpy as np\nimport tf_utils\n\n\nclass CNN(object):\n\n    def __init__(self, num_classes, vocab_size, shape_domain_size, char_domain_size, char_size, embedding_size,\n                 shape_size, nonlinearity, layers_map, viterbi, projection, loss, margin, repeats, share_repeats,\n                 char_embeddings, embeddings=None):\n\n        self.num_classes = num_classes\n        self.shape_domain_size = shape_domain_size\n        self.char_domain_size = char_domain_size\n        self.char_size = char_size\n        self.embedding_size = embedding_size\n        self.shape_size = shape_size\n        self.nonlinearity = nonlinearity\n        self.layers_map = layers_map\n        self.projection = projection\n        self.which_loss = loss\n        self.margin = margin\n        self.char_embeddings = char_embeddings\n        self.repeats = repeats\n        self.viterbi = viterbi\n        self.share_repeats = share_repeats\n\n        # word embedding input\n        self.input_x1 = tf.placeholder(tf.int64, [None, None], name=""input_x1"")\n\n        # shape embedding input\n        self.input_x2 = tf.placeholder(tf.int64, [None, None], name=""input_x2"")\n\n        # labels\n        self.input_y = tf.placeholder(tf.int64, [None, None], name=""input_y"")\n\n        # padding mask\n        self.input_mask = tf.placeholder(tf.float32, [None, None], name=""input_mask"")\n\n        # dims\n        self.batch_size = tf.placeholder(tf.int32, None, name=""batch_size"")\n        self.max_seq_len = tf.placeholder(tf.int32, None, name=""max_seq_len"")\n\n        # sequence lengths\n        self.sequence_lengths = tf.placeholder(tf.int32, [None, None], name=""sequence_lengths"")\n\n        # dropout and l2 penalties\n        self.hidden_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""hidden_dropout_keep_prob"")\n        self.input_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""input_dropout_keep_prob"")\n        self.middle_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""middle_dropout_keep_prob"")\n        self.training = tf.placeholder_with_default(False, [], name=""training"")\n\n        self.l2_penalty = tf.placeholder_with_default(0.0, [], name=""l2_penalty"")\n        self.drop_penalty = tf.placeholder_with_default(0.0, [], name=""drop_penalty"")\n\n        self.l2_loss = tf.constant(0.0)\n\n        self.use_characters = char_size != 0\n        self.use_shape = shape_size != 0\n\n        self.ones = tf.ones([self.batch_size, self.max_seq_len, self.num_classes])\n\n        if self.viterbi:\n            self.transition_params = tf.get_variable(""transitions"", [num_classes, num_classes])\n\n        word_embeddings_shape = (vocab_size-1, embedding_size)\n        self.w_e = tf_utils.initialize_embeddings(word_embeddings_shape, name=""w_e"", pretrained=embeddings, old=False)\n\n        self.block_unflat_scores, self.hidden_layer = self.forward(self.input_x1, self.input_x2, self.max_seq_len,\n                                          self.hidden_dropout_keep_prob,\n                                          self.input_dropout_keep_prob, self.middle_dropout_keep_prob, reuse=False)\n\n        # CalculateMean cross-entropy loss\n        with tf.name_scope(""loss""):\n\n            self.loss = tf.constant(0.0)\n\n            self.block_unflat_no_dropout_scores, _ = self.forward(self.input_x1, self.input_x2, self.max_seq_len, 1.0, 1.0, 1.0)\n\n            labels = tf.cast(self.input_y, \'int32\')\n\n            if self.which_loss == ""block"":\n                for unflat_scores, unflat_no_dropout_scores in zip(self.block_unflat_scores, self.block_unflat_no_dropout_scores):\n                    self.loss += self.compute_loss(unflat_scores, unflat_no_dropout_scores, labels)\n                self.unflat_scores = self.block_unflat_scores[-1]\n            else:\n                self.unflat_scores = self.block_unflat_scores[-1]\n                self.unflat_no_dropout_scores = self.block_unflat_no_dropout_scores[-1]\n                self.loss = self.compute_loss(self.unflat_scores, self.unflat_no_dropout_scores, labels)\n\n        with tf.name_scope(""predictions""):\n            if viterbi:\n                self.predictions = self.unflat_scores\n            else:\n                self.predictions = tf.argmax(self.unflat_scores, 2)\n\n    def compute_loss(self, scores, scores_no_dropout, labels):\n\n        loss = tf.constant(0.0)\n\n        if self.viterbi:\n            zero_elements = tf.equal(self.sequence_lengths, tf.zeros_like(self.sequence_lengths))\n            count_zeros_per_row = tf.reduce_sum(tf.to_int32(zero_elements), axis=1)\n            flat_sequence_lengths = tf.add(tf.reduce_sum(self.sequence_lengths, 1),\n                                           tf.scalar_mul(2, count_zeros_per_row))\n\n            log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(scores, labels, flat_sequence_lengths,\n                                                                                  transition_params=self.transition_params)\n            loss += tf.reduce_mean(-log_likelihood)\n        else:\n            if self.which_loss == ""mean"" or self.which_loss == ""block"":\n                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=labels)\n                masked_losses = tf.multiply(losses, self.input_mask)\n                loss += tf.div(tf.reduce_sum(masked_losses), tf.reduce_sum(self.input_mask))\n            elif self.which_loss == ""sum"":\n                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=labels)\n                masked_losses = tf.multiply(losses, self.input_mask)\n                loss += tf.reduce_sum(masked_losses)\n            elif self.which_loss == ""margin"":\n                # todo put into utils\n                # also todo put idx-into-3d as sep func\n                flat_labels = tf.reshape(labels, [-1])\n                batch_offsets = tf.multiply(tf.range(self.batch_size), self.max_seq_len * self.num_classes)\n                repeated_batch_offsets = tf_utils.repeat(batch_offsets, self.max_seq_len)\n                tok_offsets = tf.multiply(tf.range(self.max_seq_len), self.num_classes)\n                tiled_tok_offsets = tf.tile(tok_offsets, [self.batch_size])\n                indices = tf.add(tf.add(repeated_batch_offsets, tiled_tok_offsets), flat_labels)\n\n                # scores w/ true label set to -inf\n                sparse = tf.sparse_to_dense(indices, [self.batch_size * self.max_seq_len * self.num_classes], np.NINF)\n                loss_augmented_flat = tf.add(tf.reshape(scores, [-1]), sparse)\n                loss_augmented = tf.reshape(loss_augmented_flat, [self.batch_size, self.max_seq_len, self.num_classes])\n\n                # maxes excluding true label\n                max_scores = tf.reshape(tf.reduce_max(loss_augmented, [-1]), [-1])\n                sparse = tf.sparse_to_dense(indices, [self.batch_size * self.max_seq_len * self.num_classes],\n                                            -self.margin)\n                loss_augmented_flat = tf.add(tf.reshape(scores, [-1]), sparse)\n                label_scores = tf.gather(loss_augmented_flat, indices)\n                # margin + max_logit - correct_logit == max_logit - (correct - margin)\n                max2_diffs = tf.subtract(max_scores, label_scores)\n                mask = tf.reshape(self.input_mask, [-1])\n                loss += tf.reduce_mean(tf.multiply(mask, tf.nn.relu(max2_diffs)))\n        loss += self.l2_penalty * self.l2_loss\n\n        drop_loss = tf.nn.l2_loss(tf.subtract(scores, scores_no_dropout))\n        loss += self.drop_penalty * drop_loss\n        return loss\n\n    def forward(self, input_x1, input_x2, max_seq_len, hidden_dropout_keep_prob, input_dropout_keep_prob,\n                middle_dropout_keep_prob, reuse=True):\n\n        block_unflat_scores = []\n\n        with tf.variable_scope(""forward"", reuse=reuse):\n            word_embeddings = tf.nn.embedding_lookup(self.w_e, input_x1)\n\n            input_list = [word_embeddings]\n            input_size = self.embedding_size\n            if self.use_characters:\n                char_embeddings_masked = tf.multiply(self.char_embeddings, tf.expand_dims(self.input_mask, 2))\n                input_list.append(char_embeddings_masked)\n                input_size += self.char_size\n            if self.use_shape:\n                shape_embeddings_shape = (self.shape_domain_size-1, self.shape_size)\n                w_s = tf_utils.initialize_embeddings(shape_embeddings_shape, name=""w_s"")\n                shape_embeddings = tf.nn.embedding_lookup(w_s, input_x2)\n                input_list.append(shape_embeddings)\n                input_size += self.shape_size\n\n            initial_filter_width = self.layers_map[0][1][\'width\']\n            initial_num_filters = self.layers_map[0][1][\'filters\']\n            filter_shape = [1, initial_filter_width, input_size, initial_num_filters]\n            initial_layer_name = ""conv0""\n\n            if not reuse:\n                print(input_list)\n                print(""Adding initial layer %s: width: %d; filters: %d"" % (\n                    initial_layer_name, initial_filter_width, initial_num_filters))\n\n            input_feats = tf.concat(axis=2, values=input_list)\n            input_feats_expanded = tf.expand_dims(input_feats, 1)\n            input_feats_expanded_drop = tf.nn.dropout(input_feats_expanded, input_dropout_keep_prob)\n            print(""input feats expanded drop"", input_feats_expanded_drop.get_shape())\n\n            # first projection of embeddings\n            w = tf_utils.initialize_weights(filter_shape, initial_layer_name + ""_w"", init_type=\'xavier\', gain=\'relu\')\n            b = tf.get_variable(initial_layer_name + ""_b"", initializer=tf.constant(0.01, shape=[initial_num_filters]))\n            conv0 = tf.nn.conv2d(input_feats_expanded_drop, w, strides=[1, 1, 1, 1], padding=""SAME"", name=initial_layer_name)\n            h0 = tf_utils.apply_nonlinearity(tf.nn.bias_add(conv0, b), \'relu\')\n\n            initial_inputs = [h0]\n            last_dims = initial_num_filters\n\n            # Stacked atrous convolutions\n            last_output = tf.concat(axis=3, values=initial_inputs)\n\n            for block in range(self.repeats):\n                print(""last out shape"", last_output.get_shape())\n                print(""last dims"", last_dims)\n                hidden_outputs = []\n                total_output_width = 0\n                reuse_block = (block != 0 and self.share_repeats) or reuse\n                block_name_suff = """" if self.share_repeats else str(block)\n                inner_last_dims = last_dims\n                inner_last_output = last_output\n                with tf.variable_scope(""block"" + block_name_suff, reuse=reuse_block):\n                    for layer_name, layer in self.layers_map:\n                        dilation = layer[\'dilation\']\n                        filter_width = layer[\'width\']\n                        num_filters = layer[\'filters\']\n                        initialization = layer[\'initialization\']\n                        take_layer = layer[\'take\']\n                        if not reuse:\n                            print(""Adding layer %s: dilation: %d; width: %d; filters: %d; take: %r"" % (\n                            layer_name, dilation, filter_width, num_filters, take_layer))\n                        with tf.name_scope(""atrous-conv-%s"" % layer_name):\n                            # [filter_height, filter_width, in_channels, out_channels]\n                            filter_shape = [1, filter_width, inner_last_dims, num_filters]\n                            w = tf_utils.initialize_weights(filter_shape, layer_name + ""_w"", init_type=initialization, gain=self.nonlinearity, divisor=self.num_classes)\n                            b = tf.get_variable(layer_name + ""_b"", initializer=tf.constant(0.0 if initialization == ""identity"" or initialization == ""varscale"" else 0.001, shape=[num_filters]))\n                            # h = tf_utils.residual_layer(inner_last_output, w, b, dilation, self.nonlinearity, self.batch_norm, layer_name + ""_r"",\n                            #                             self.batch_size, max_seq_len, self.res_activation, self.training) \\\n                            #     if last_output != input_feats_expanded_drop \\\n                            #     else tf_utils.residual_layer(inner_last_output, w, b, dilation, self.nonlinearity, False, layer_name + ""_r"",\n                            #                             self.batch_size, max_seq_len, 0, self.training)\n\n                            conv = tf.nn.atrous_conv2d(inner_last_output, w, rate=dilation, padding=""SAME"", name=layer_name)\n                            conv_b = tf.nn.bias_add(conv, b)\n                            h = tf_utils.apply_nonlinearity(conv_b, self.nonlinearity)\n\n                            # so, only apply ""take"" to last block (may want to change this later)\n                            if take_layer:\n                                hidden_outputs.append(h)\n                                total_output_width += num_filters\n                            inner_last_dims = num_filters\n                            inner_last_output = h\n\n                    h_concat = tf.concat(axis=3, values=hidden_outputs)\n                    last_output = tf.nn.dropout(h_concat, middle_dropout_keep_prob)\n                    last_dims = total_output_width\n\n                    h_concat_squeeze = tf.squeeze(h_concat, [1])\n                    h_concat_flat = tf.reshape(h_concat_squeeze, [-1, total_output_width])\n\n                    # Add dropout\n                    with tf.name_scope(""hidden_dropout""):\n                        h_drop = tf.nn.dropout(h_concat_flat, hidden_dropout_keep_prob)\n\n                    def do_projection():\n                        # Project raw outputs down\n                        with tf.name_scope(""projection""):\n                            projection_width = int(total_output_width/(2*len(hidden_outputs)))\n                            w_p = tf_utils.initialize_weights([total_output_width, projection_width], ""w_p"", init_type=""xavier"")\n                            b_p = tf.get_variable(""b_p"", initializer=tf.constant(0.01, shape=[projection_width]))\n                            projected = tf.nn.xw_plus_b(h_drop, w_p, b_p, name=""projected"")\n                            projected_nonlinearity = tf_utils.apply_nonlinearity(projected, self.nonlinearity)\n                        return projected_nonlinearity, projection_width\n\n                    # only use projection if we wanted to, and only apply middle dropout here if projection\n                    input_to_pred, proj_width = do_projection() if self.projection else (h_drop, total_output_width)\n                    input_to_pred_drop = tf.nn.dropout(input_to_pred, middle_dropout_keep_prob) if self.projection else input_to_pred\n\n                    # Final (unnormalized) scores and predictions\n                    with tf.name_scope(""output""+block_name_suff):\n                        w_o = tf_utils.initialize_weights([proj_width, self.num_classes], ""w_o"", init_type=""xavier"")\n                        b_o = tf.get_variable(""b_o"", initializer=tf.constant(0.01, shape=[self.num_classes]))\n                        self.l2_loss += tf.nn.l2_loss(w_o)\n                        self.l2_loss += tf.nn.l2_loss(b_o)\n                        scores = tf.nn.xw_plus_b(input_to_pred_drop, w_o, b_o, name=""scores"")\n                        unflat_scores = tf.reshape(scores, tf.stack([self.batch_size, max_seq_len, self.num_classes]))\n                        block_unflat_scores.append(unflat_scores)\n\n        return block_unflat_scores, h_concat_squeeze\n'"
src/cnn_char.py,20,"b'from __future__ import print_function\nfrom __future__ import division\nimport tensorflow as tf\nimport tf_utils\n\nclass CNNChar(object):\n\n    """"""\n    A CNN for embedding tokens.\n    """"""\n    def __init__(self, char_domain_size, char_embedding_dim, hidden_dim, filter_width, embeddings=None):\n\n        self.char_domain_size = char_domain_size\n        self.embedding_size = char_embedding_dim\n        self.hidden_dim = hidden_dim\n        self.filter_width = filter_width\n\n        # char embedding input\n        self.input_chars = tf.placeholder(tf.int64, [None, None], name=""input_chars"")\n\n        # padding mask\n        # self.input_mask = tf.placeholder(tf.float32, [None, None], name=""input_mask"")\n\n        self.batch_size = tf.placeholder(tf.int32, None, name=""batch_size"")\n\n        self.max_seq_len = tf.placeholder(tf.int32, None, name=""max_seq_len"")\n\n        self.max_tok_len = tf.placeholder(tf.int32, None, name=""max_tok_len"")\n\n        self.input_dropout_keep_prob = tf.placeholder_with_default(1.0, [], name=""input_dropout_keep_prob"")\n\n        # sequence lengths\n        self.sequence_lengths = tf.placeholder(tf.int32, [None, None], name=""sequence_lengths"")\n        self.token_lengths = tf.placeholder(tf.int32, [None, None], name=""tok_lengths"")\n\n        print(""CNN char embedding model:"")\n        print(""embedding dim: "", self.embedding_size)\n        print(""out dim: "", self.hidden_dim)\n\n        char_embeddings_shape = (self.char_domain_size-1, self.embedding_size)\n        self.char_embeddings = tf_utils.initialize_embeddings(char_embeddings_shape, name=""char_embeddings"", pretrained=embeddings)\n\n        self.outputs = self.forward(self.input_chars, self.input_dropout_keep_prob, reuse=False)\n\n    def forward(self, input_x1, input_dropout_keep_prob, reuse=True):\n        with tf.variable_scope(""char-forward"", reuse=reuse):\n\n            char_embeddings_lookup = tf.nn.embedding_lookup(self.char_embeddings, input_x1)\n            print(char_embeddings_lookup.get_shape())\n\n            char_embeddings_flat = tf.reshape(char_embeddings_lookup, tf.stack([self.batch_size*self.max_seq_len, self.max_tok_len, self.embedding_size]))\n            print(char_embeddings_flat.get_shape())\n            tok_lens_flat = tf.reshape(self.token_lengths, [self.batch_size*self.max_seq_len])\n            print(tok_lens_flat.get_shape())\n\n            input_feats_expanded = tf.expand_dims(char_embeddings_flat, 1)\n            input_feats_expanded_drop = tf.nn.dropout(input_feats_expanded, input_dropout_keep_prob)\n\n\n            with tf.name_scope(""char-cnn""):\n                filter_shape = [1, self.filter_width, self.embedding_size, self.hidden_dim]\n                w = tf_utils.initialize_weights(filter_shape, ""conv0_w"", init_type=\'xavier\',  gain=\'relu\')\n                b = tf.get_variable(""conv0_b"", initializer=tf.constant(0.01, shape=[self.hidden_dim]))\n                conv0 = tf.nn.conv2d(input_feats_expanded_drop, w, strides=[1, 1, 1, 1], padding=""SAME"", name=""conv0"")\n                print(""conv0"", conv0.get_shape())\n                h_squeeze = tf.squeeze(conv0, [1])\n                print(""squeeze"", h_squeeze.get_shape())\n                hidden_outputs = tf.reduce_max(h_squeeze, 1)\n                print(""max"", hidden_outputs.get_shape())\n                hidden_outputs_unflat = tf.reshape(hidden_outputs, tf.stack([self.batch_size, self.max_seq_len, self.hidden_dim]))\n\n        return hidden_outputs_unflat\n'"
src/data_utils.py,25,"b""import numpy as np\nimport tensorflow as tf\nfrom collections import defaultdict\nfrom random import shuffle\nimport sys\nimport os\n\nFLAGS = tf.app.flags.FLAGS\n\nclass Batcher(object):\n    def __init__(self, in_dir, batch_size, num_epochs=None):\n        self._batch_size = batch_size\n        self._epoch = 0\n        self._step = 0.\n        self._data = defaultdict(list)\n        self._starts = {}\n        self._ends = {}\n        self._bucket_probs = {}\n        self.sequence_batcher = SeqBatcher(in_dir, batch_size, 0, num_epochs=1)\n\n    def load_and_bucket_data(self, sess):\n        done = False\n        i = 0\n        while not done:\n            try:\n                batch = sess.run(self.sequence_batcher.next_batch_op)\n                self._data[batch[0].shape[1]].append(batch)\n                i += 1\n            except Exception as e:\n                done = True\n        # now flatten\n        for seq_len, batches in self._data.items():\n            self._data[seq_len] = [(label_batch[i], token_batch[i], shape_batch[i], char_batch[i], seq_len_batch[i], tok_len_batch[i])\n                                  for (label_batch, token_batch, shape_batch, char_batch, seq_len_batch, tok_len_batch) in batches\n                                  for i in range(label_batch.shape[0])]\n        self.reset_batch_pointer()\n\n    def next_batch(self):\n        if sum(self._bucket_probs.values()) == 0:\n            self.reset_batch_pointer()\n        # select bucket to create batch from\n        self._step += 1\n        bucket = self.select_bucket()\n        batch = self._data[bucket][self._starts[bucket]:self._ends[bucket]]\n        # update pointers\n        self._starts[bucket] = self._ends[bucket]\n        self._ends[bucket] = min(self._ends[bucket] + self._batch_size, len(self._data[bucket]))\n        self._bucket_probs[bucket] = max(0, self._ends[bucket] - self._starts[bucket])\n\n        _label_batch = np.array([b[0] for b in batch])\n        _token_batch = np.array([b[1] for b in batch])\n        _shape_batch = np.array([b[2] for b in batch])\n        _char_batch = np.array([b[3] for b in batch])\n        _seq_len_batch = np.array([b[4] for b in batch])\n        _tok_len_batch = np.array([b[5] for b in batch])\n        batch = (_label_batch, _token_batch, _shape_batch, _char_batch, _seq_len_batch, _tok_len_batch)\n\n        return batch\n\n    def reset_batch_pointer(self):\n        # shuffle each bucket\n        for bucket in self._data.values():\n            shuffle(bucket)\n        self._epoch += 1\n        self._step = 0.\n        # print('\\nStarting epoch ' + str(self._epoch))\n        self._starts = {i: 0 for i in self._data.keys()}\n        self._ends = {i: min(self._batch_size, len(examples)) for i, examples in self._data.items()}\n        self._bucket_probs = {i: len(l) for (i, l) in self._data.items()}\n\n    def select_bucket(self):\n        buckets, weights = zip(*[(i, p) for i, p in self._bucket_probs.items() if p > 0])\n        total = float(sum(weights))\n        probs = [w / total for w in weights]\n        bucket = np.random.choice(buckets, p=probs)\n        return bucket\n\nclass SeqBatcher(object):\n    def __init__(self, in_pattern, batch_size, num_buckets=0, num_epochs=None):\n        self._batch_size = batch_size\n        self.num_buckets = num_buckets\n        self._epoch = 0\n        self._step = 1.\n        self.num_epochs = num_epochs\n        file_pattern = in_pattern + '/examples.proto' if os.path.isdir(in_pattern) else in_pattern\n        filenames = tf.matching_files(file_pattern)\n        # filenames = tf.Print(filenames, [filenames], message='filenames: ')\n        self.next_batch_op = self.input_pipeline(filenames, self._batch_size, self.num_buckets, self.num_epochs)\n\n    def example_parser(self, filename_queue):\n        reader = tf.TFRecordReader()\n        key, record_string = reader.read(filename_queue)\n        features = {\n            'labels': tf.FixedLenSequenceFeature([], tf.int64),\n            'tokens': tf.FixedLenSequenceFeature([], tf.int64),\n            'shapes': tf.FixedLenSequenceFeature([], tf.int64),\n            'chars': tf.FixedLenSequenceFeature([], tf.int64),\n            'seq_len': tf.FixedLenSequenceFeature([], tf.int64),\n            'tok_len': tf.FixedLenSequenceFeature([], tf.int64),\n        }\n\n        _, example = tf.parse_single_sequence_example(serialized=record_string, sequence_features=features)\n        labels = example['labels']\n        tokens = example['tokens']\n        shapes = example['shapes']\n        chars = example['chars']\n        seq_len = example['seq_len']\n        tok_len = example['tok_len']\n        # context = c['context']\n        return labels, tokens, shapes, chars, seq_len, tok_len\n        # return labels, tokens, labels, labels, labels\n\n    def input_pipeline(self, filenames, batch_size, num_buckets, num_epochs=None):\n        filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=True)\n        labels, tokens, shapes, chars, seq_len, tok_len = self.example_parser(filename_queue)\n        # min_after_dequeue defines how big a buffer we will randomly sample\n        #   from -- bigger means better shuffling but slower start up and more\n        #   memory used.\n        # capacity must be larger than min_after_dequeue and the amount larger\n        #   determines the maximum we will prefetch.  Recommendation:\n        #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n        min_after_dequeue = 10000\n        capacity = min_after_dequeue + 12 * batch_size\n\n        # next_batch = tf.train.batch([labels, tokens, shapes, chars, seq_len], batch_size=batch_size, capacity=capacity,\n        #                                 dynamic_pad=True, allow_smaller_final_batch=True)\n\n        if num_buckets == 0:\n            next_batch = tf.train.batch([labels, tokens, shapes, chars, seq_len, tok_len], batch_size=batch_size, capacity=capacity,\n                                        dynamic_pad=True, allow_smaller_final_batch=True)\n        else:\n            bucket, next_batch = tf.contrib.training.bucket([labels, tokens, shapes, chars, seq_len, tok_len], np.random.randint(num_buckets),\n                                                        batch_size, num_buckets, num_threads=1, capacity=capacity,\n                                                        dynamic_pad=True, allow_smaller_final_batch=False)\n        return next_batch\n\n\n# class NodeBatcher(object):\n#     def __init__(self, in_dir, max_seq, max_word, batch_size, num_epochs=None):\n#         self._batch_size = batch_size\n#         self._max_seq = max_seq\n#         self._max_word = max_word\n#         self._epoch = 0\n#         self._step = 1.\n#         self.num_epochs = num_epochs\n#         in_file = [in_dir + '/examples.proto']\n#         self.next_batch_op = self.input_pipeline(in_file, self._max_seq, self._max_word, self._batch_size, self.num_epochs)\n#\n#     def example_parser(self, filename_queue, max_seq, max_word):\n#         reader = tf.TFRecordReader()\n#         key, record_string = reader.read(filename_queue)\n#         features = {\n#             'labels': tf.FixedLenFeature([max_seq], tf.int64),\n#             'tokens': tf.FixedLenFeature([max_seq], tf.int64),\n#             'shapes': tf.FixedLenFeature([max_seq], tf.int64),\n#             'chars': tf.FixedLenFeature([], tf.int64),\n#             'seq_len': tf.FixedLenFeature([], tf.int64),\n#             'tok_len': tf.FixedLenFeature([], tf.int64)\n#         }\n#\n#         example = tf.parse_single_example(record_string, features)\n#         labels = example['labels']\n#         tokens = example['tokens']\n#         shapes = example['shapes']\n#         chars = example['chars'][0]\n#         seq_len = example['seq_len'][0]\n#         tok_len = example['tok_len'][0]\n#         return labels, tokens, shapes, chars, seq_len, tok_len\n#\n#     def input_pipeline(self, filenames, max_seq, max_word, batch_size, num_epochs=None):\n#         filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=True)\n#         labels, tokens, shapes, chars, seq_len, tok_len = self.example_parser(filename_queue, max_seq, max_word)\n#         # min_after_dequeue defines how big a buffer we will randomly sample\n#         #   from -- bigger means better shuffling but slower start up and more\n#         #   memory used.\n#         # capacity must be larger than min_after_dequeue and the amount larger\n#         #   determines the maximum we will prefetch.  Recommendation:\n#         #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n#         min_after_dequeue = 10000\n#         capacity = min_after_dequeue + 12 * batch_size\n#         next_batch = tf.train.shuffle_batch(\n#             [labels, tokens, shapes, chars, seq_len, tok_len], batch_size=batch_size, capacity=capacity,\n#             min_after_dequeue=min_after_dequeue, allow_smaller_final_batch=True)\n#         return next_batch\n"""
src/eval_f1.py,0,"b'from __future__ import division\nfrom __future__ import print_function\nimport time\nimport numpy as np\nimport sys\n\n\ndef is_start(curr):\n    return curr[0] == ""B"" or curr[0] == ""U""\n\n\ndef is_continue(curr):\n    return curr[0] == ""I"" or curr[0] == ""L""\n\n\ndef is_background(curr):\n    return not is_start(curr) and not is_continue(curr)\n\n\ndef is_seg_start(curr, prev):\n    return (is_start(curr) and not is_continue(curr)) or (is_continue(curr) and (prev is None or is_background(prev) or prev[1:] != curr[1:]))\n\n\ndef segment_eval(batches, predictions, label_map, type_int_int_map, labels_id_str_map, vocab_id_str_map, outside_idx, pad_width, start_end, extra_text="""", verbose=False):\n    if extra_text != """":\n        print(extra_text)\n\n    def print_context(width, start, tok_list, pred_list, gold_list):\n        for offset in range(-width, width+1):\n            idx = offset + start\n            if 0 <= idx < len(tok_list):\n                print(""%s\\t%s\\t%s"" % (vocab_id_str_map[tok_list[idx]], labels_id_str_map[pred_list[idx]], labels_id_str_map[gold_list[idx]]))\n        print()\n\n    pred_counts = {t: 0 for t in label_map.values()}\n    gold_counts = {t: 0 for t in label_map.values()}\n    correct_counts = {t: 0 for t in label_map.values()}\n    token_count = 0\n    # iterate over batches\n    for predictions, (dev_label_batch, dev_token_batch, _, _, dev_seq_len_batch, _, _) in zip(predictions, batches):\n        # iterate over examples in batch\n        for preds, labels, tokens, seq_lens in zip(predictions, dev_label_batch, dev_token_batch, dev_seq_len_batch):\n            start = pad_width\n            for seq_len in seq_lens:\n                predicted = preds[start:seq_len+start]\n                golds = labels[start:seq_len+start]\n                toks = tokens[start:seq_len+start]\n                for i in range(seq_len):\n                    token_count += 1\n                    pred = predicted[i]\n                    gold = golds[i]\n                    gold_str = labels_id_str_map[gold]\n                    pred_str = labels_id_str_map[pred]\n                    gold_prev = None if i == 0 else labels_id_str_map[golds[i - 1]]\n                    pred_prev = None if i == 0 else labels_id_str_map[predicted[i - 1]]\n                    pred_type = type_int_int_map[pred]\n                    gold_type = type_int_int_map[gold]\n                    pred_start = False\n                    gold_start = False\n                    if is_seg_start(pred_str, pred_prev):\n                        pred_counts[pred_type] += 1\n                        pred_start = True\n                    if is_seg_start(gold_str, gold_prev):\n                        gold_counts[gold_type] += 1\n                        gold_start = True\n\n                    if pred_start and gold_start and pred_type == gold_type:\n                        if i == seq_len - 1:\n                            correct_counts[gold_type] += 1\n                        else:\n                            j = i + 1\n                            stop_search = False\n                            while j < seq_len and not stop_search:\n                                pred2 = labels_id_str_map[predicted[j]]\n                                gold2 = labels_id_str_map[golds[j]]\n                                pred_type2 = type_int_int_map[predicted[j]]\n                                pred_continue = is_continue(pred2)\n                                gold_continue = is_continue(gold2)\n\n                                if not pred_continue or not gold_continue or pred_type2 != gold_type or j == seq_len - 1:\n                                    # if pred_continue == gold_continue:\n                                    if (not pred_continue and not gold_continue) or (pred_continue and gold_continue and pred_type2 == gold_type):\n                                        correct_counts[gold_type] += 1\n                                    stop_search = True\n                                j += 1\n                start += seq_len + (2 if start_end else 1)*pad_width\n\n    all_correct = np.sum([p if i not in outside_idx else 0 for i, p in enumerate(correct_counts.values())])\n    all_pred = np.sum([p if i not in outside_idx else 0 for i, p in enumerate(pred_counts.values())])\n    all_gold = np.sum([p if i not in outside_idx else 0 for i, p in enumerate(gold_counts.values())])\n\n    precisions = np.array([correct_counts[i] / pred_counts[i] if pred_counts[i] != 0 else 0.0 for i in pred_counts.keys()])\n    recalls = np.array([correct_counts[i] / gold_counts[i] if gold_counts[i] != 0 else 1.0 for i in gold_counts.keys()])\n    f1s = [2 * precision * recall / (recall + precision) if recall + precision != 0 else 0.0 for precision, recall in\n           zip(precisions, recalls)]\n\n    in_indices = np.where(np.array(gold_counts.values()) != 0)[0]\n    precision_macro = np.mean(precisions[in_indices])\n    recall_macro = np.mean(recalls[in_indices])\n    f1_macro = 2 * precision_macro * recall_macro / (precision_macro + recall_macro)\n\n    precision_micro = all_correct / all_pred\n    recall_micro = all_correct / all_gold\n    f1_micro = 2 * precision_micro * recall_micro / (precision_micro + recall_micro)\n\n    accuracy = all_correct / all_gold\n\n    print(""\\t%10s\\tPrec\\tRecall"" % (""F1""))\n    print(""%10s\\t%2.2f\\t%2.2f\\t%2.2f"" % (""Micro (Seg)"", f1_micro * 100, precision_micro * 100, recall_micro * 100))\n    print(""%10s\\t%2.2f\\t%2.2f\\t%2.2f"" % (""Macro (Seg)"", f1_macro * 100, precision_macro * 100, recall_macro * 100))\n    print(""-------"")\n    for t in label_map:\n        idx = label_map[t]\n        if idx not in outside_idx:\n            print(""%10s\\t%2.2f\\t%2.2f\\t%2.2f"" % (t, f1s[idx] * 100, precisions[idx] * 100, recalls[idx] * 100))\n    print(""Processed %d tokens with %d phrases; found: %d phrases; correct: %d."" % (token_count, all_gold, all_pred, all_correct))\n    sys.stdout.flush()\n    return f1_micro, precision_micro\n\n\ndef print_training_error(num_examples, start_time, epoch_losses, step):\n    losses_str = \' \'.join([""%5.5f""]*len(epoch_losses)) % tuple(map(lambda l: l/step, epoch_losses))\n    print(""%20d examples at %5.2f examples/sec. Error: %s"" %\n          (num_examples, num_examples / (time.time() - start_time), losses_str))\n    sys.stdout.flush()\n\n\ndef print_conlleval_format(out_filename, eval_batches, predictions, labels_id_str_map, vocab_id_str_map, pad_width):\n    with open(out_filename, \'w\') as conll_preds_file:\n        token_count = 0\n        sentence_count = 0\n        for prediction, (label_batch, token_batch, shape_batch, char_batch, seq_len_batch, tok_len_batch, eval_mask_batch) in zip(\n                predictions, eval_batches):\n            for preds, labels, tokens, seq_lens in zip(prediction, label_batch, token_batch, seq_len_batch):\n                start = pad_width\n                for seq_len in seq_lens:\n                    if seq_len != 0:\n                        preds_nopad = map(lambda t: labels_id_str_map[t], preds[start:seq_len + start])\n                        labels_nopad = map(lambda t: labels_id_str_map[t], labels[start:seq_len + start])\n                        tokens_nopad = map(lambda t: vocab_id_str_map[t], tokens[start:seq_len + start])\n                        start += pad_width + seq_len\n                        labels_converted = []\n                        preds_converted = []\n                        for idx, (pred, label) in enumerate(zip(preds_nopad, labels_nopad)):\n                            token_count += 1\n\n                            if pred[0] == ""L"":\n                                preds_converted.append(""I"" + pred[1:])\n                            elif pred[0] == ""U"":\n                                preds_converted.append(""B"" + pred[1:])\n                            else:\n                                preds_converted.append(pred)\n\n                            if label[0] == ""L"":\n                                labels_converted.append(""I"" + label[1:])\n                            elif label[0] == ""U"":\n                                labels_converted.append(""B"" + label[1:])\n                            else:\n                                labels_converted.append(label)\n\n                        for pred_conv, label_conv, pred, label, token in zip(preds_converted, labels_converted, preds_nopad,\n                                                                             labels_nopad, tokens_nopad):\n                            print(""%s %s %s %s %s"" % (token, label, pred, label_conv, pred_conv), file=conll_preds_file)\n                        print("""", file=conll_preds_file)\n                        sentence_count += 1\n'"
src/preprocess.py,21,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport sys\nimport tensorflow as tf\nimport numpy as np\n\ntf.app.flags.DEFINE_string(\'in_file\', \'naacl-data.tsv\', \'tsv file containing string data\')\ntf.app.flags.DEFINE_string(\'vocab\', \'\', \'file containing vocab (empty means make new vocab)\')\ntf.app.flags.DEFINE_string(\'labels\', \'\', \'file containing labels (but always add new labels)\')\ntf.app.flags.DEFINE_string(\'shapes\', \'\', \'file containing shapes (add new shapes only when adding new vocab)\')\ntf.app.flags.DEFINE_string(\'chars\', \'\', \'file containing characters\')\n\ntf.app.flags.DEFINE_string(\'embeddings\', \'\', \'pretrained embeddings\')\n\ntf.app.flags.DEFINE_string(\'out_dir\', \'\', \'export tf protos\')\n\ntf.app.flags.DEFINE_integer(\'window_size\', 3, \'window size (for computing padding)\')\ntf.app.flags.DEFINE_boolean(\'lowercase\', False, \'whether to lowercase\')\n\ntf.app.flags.DEFINE_boolean(\'start_end\', False, \'whether to use distinct start/end padding\')\ntf.app.flags.DEFINE_boolean(\'debug\', False, \'print debugging output\')\n\ntf.app.flags.DEFINE_boolean(\'predict_pad\', False, \'whether to predict padding labels\')\n\ntf.app.flags.DEFINE_boolean(\'documents\', False, \'whether to grab documents rather than sentences\')\n\ntf.app.flags.DEFINE_boolean(\'update_maps\', False, \'whether to update maps\')\n\ntf.app.flags.DEFINE_string(\'update_vocab\', \'\', \'file to update vocab with tokens from training data\')\n\n\nFLAGS = tf.app.flags.FLAGS\n\nZERO_STR = ""<ZERO>""\n# PAD_STR = ""PADDING""\n# OOV_STR = ""UNKNOWN""\nPAD_STR = ""<PAD>""\nOOV_STR = ""<OOV>""\nNONE_STR = ""<NONE>""\nSENT_START = ""<S>""\nSENT_END = ""</S>""\n\npad_strs = [PAD_STR, SENT_START, SENT_END, ZERO_STR, NONE_STR]\n\nDOC_MARKER = ""-DOCSTART-""\n\n# indices of characters to grab: first and last 4\n# char_indices = [0, 1, 2, 3, -1, -2, -3, -4]\n\nlabel_int_str_map = {}\ntoken_int_str_map = {}\nchar_int_str_map = {}\n\npad_width = int(FLAGS.window_size/2)\n\n\ndef _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef shape(string):\n    if all(c.isupper() for c in string):\n        return ""AA""\n    if string[0].isupper():\n        return ""Aa""\n    if any(c for c in string if c.isupper()):\n        return ""aAa""\n    else:\n        return ""a""\n\n    # upper = \'A\'\n    # lower = \'a\'\n    # digit = \'0\'\n    # symbol = \'.\'\n    # shape_str = """"\n    # for c in string:\n    #     if c.isupper():\n    #         shape_c = upper\n    #     elif c.islower():\n    #         shape_c = lower\n    #     elif c == digit:\n    #         shape_c = digit\n    #     else:\n    #         shape_c = symbol\n    #     if shape_str == """" or shape_c != shape_str[-1]:\n    #         shape_str += shape_c\n    # return shape_str\n\n\ndef make_example(writer, lines, label_map, token_map, shape_map, char_map, update_vocab, update_chars):\n    # data format is:\n    # token pos phrase ner\n    # LONDON NNP I-NP I-LOC\n    # 1996-08-30 CD I-NP O\n\n    # West NNP I-NP I-MISC\n    # Indian NNP I-NP I-MISC\n    # all-rounder NN I-NP O\n    # ...\n\n    sent_len = len(lines)\n    num_breaks = sum([1 if line.strip() == """" else 0 for line in lines])\n    max_len_with_pad = pad_width * (num_breaks + (1 if FLAGS.start_end else 2)) * (2 if FLAGS.start_end else 1) + (sent_len - num_breaks)\n    max_word_len = max(map(len, lines))\n    # print(""Processing %s w/ %d lines %d breaks; max len w/ pad: %d"" % (""doc"" if FLAGS.documents else ""sent"", sent_len, num_breaks, max_len_with_pad))\n\n    # max_len_with_pad = FLAGS.max_len if FLAGS.documents else FLAGS.max_len + (FLAGS.window_size - 1) # assumes odd window size\n\n\n    oov_count = 0\n    if sent_len == 0:\n        return 0, 0, 0\n\n    # if sent_len > FLAGS.max_len:\n    #     print(""Skipping sentence w/ %d tokens ( > max len %d)"" % (sent_len, FLAGS.max_len))\n    #     return 0, 0, 0\n    # else:\n    # zero pad\n    tokens = np.zeros(max_len_with_pad, dtype=np.int64)\n    shapes = np.zeros(max_len_with_pad, dtype=np.int64)\n    chars = np.zeros(max_len_with_pad*max_word_len, dtype=np.int64)\n    intmapped_labels = np.zeros(max_len_with_pad, dtype=np.int64)\n    sent_lens = []\n    tok_lens = []\n\n    # initial padding\n    if FLAGS.start_end:\n        tokens[:pad_width] = token_map[SENT_START]\n        shapes[:pad_width] = shape_map[SENT_START]\n        chars[:pad_width] = char_map[SENT_START]\n        if FLAGS.predict_pad:\n            intmapped_labels[:pad_width] = label_map[SENT_START]\n    else:\n        tokens[:pad_width] = token_map[PAD_STR]\n        shapes[:pad_width] = shape_map[PAD_STR]\n        chars[:pad_width] = char_map[PAD_STR]\n        if FLAGS.predict_pad:\n            intmapped_labels[:pad_width] = label_map[PAD_STR]\n    tok_lens.extend([1]*pad_width)\n\n    last_label = ""O""\n    labels = []\n    # for k in range(pad_width):\n    #     intmapped_labels[k] = label_map[SENT_START]\n    current_sent_len = 0\n    char_start = pad_width\n    idx = pad_width\n    for i, line in enumerate(lines):\n\n        if line:\n            split_line = line.strip().split(\'\\t\')\n            # print(""line: "", split_line)\n            token_str = split_line[0]\n            label_str = split_line[1]\n\n            label_str = ""O"" if label_str == ""null"" else label_str\n\n            # skip docstart markers\n            if token_str == DOC_MARKER:\n                # print(""doc marker"")\n                return 0, 0, 0\n\n            current_sent_len += 1\n\n            # process tokens to match Collobert embedding preprocessing:\n            # - normalize the digits to 0\n            # - lowercase\n            # token_str_digits = re.sub(""\\d"", ""0"", token_str)\n\n            # get capitalization features\n            token_shape = shape(token_str)\n\n            # print(token_str_digits, token_shape)\n\n            token_str_normalized = token_str.lower() if FLAGS.lowercase else token_str\n\n            if token_shape not in shape_map:#  and update_vocab:\n                shape_map[token_shape] = len(shape_map)\n\n            # token_str_normalized_char = re.sub(r\'\\W\', \'.\', token_str_normalized)\n            # num_chars = len(token_str_normalized_char)\n\n            # for c_i in char_indices:\n            #     if c_i < num_chars and -c_i <= num_chars and token_str_normalized_char[c_i] not in char_map:\n            #         char_map[token_str_normalized_char[c_i]] = len(char_map)\n            # Don\'t use normalized token str -- want digits\n            for char in token_str:\n                if char not in char_map and update_chars:\n                    char_map[char] = len(char_map)\n                    char_int_str_map[char_map[char]] = char\n            tok_lens.append(len(token_str))\n\n            # convert label to BILOU encoding\n            label_bilou = label_str\n            # handle cases where we need to update the last token we processed\n            if label_str == ""O"" or label_str[0] == ""B"" or (last_label != ""O"" and label_str[2] != last_label[2]):\n                if last_label[0] == ""I"":\n                    labels[-1] = ""L"" + labels[-1][1:]\n                elif last_label[0] == ""B"":\n                    labels[-1] = ""U"" + labels[-1][1:]\n            if label_str[0] == ""I"":\n                if last_label == ""O"" or label_str[2] != last_label[2]:\n                    label_bilou = ""B-"" + label_str[2:]\n\n            if token_str_normalized not in token_map:\n                oov_count += 1\n                if update_vocab:\n                    token_map[token_str_normalized] = len(token_map)\n                    token_int_str_map[token_map[token_str_normalized]] = token_str_normalized\n\n            tokens[idx] = token_map.get(token_str_normalized, token_map[OOV_STR])\n            shapes[idx] = shape_map[token_shape] # if update_vocab else shape_map.get(token_shape, shape_map[token_shape[0]])\n            chars[char_start:char_start+tok_lens[-1]] = [char_map.get(char, char_map[OOV_STR]) for char in token_str]\n            char_start += tok_lens[-1]\n            labels.append(label_bilou)\n            last_label = label_bilou\n            # tmp.append(label_str)\n            # toks_tmp.append(token_str_normalized)\n            idx += 1\n        elif current_sent_len > 0:\n            sent_lens.append(current_sent_len)\n            current_sent_len = 0\n\n            if FLAGS.start_end:\n                tokens[idx:idx+pad_width] = token_map[SENT_END]\n                shapes[idx:idx+pad_width] = shape_map[SENT_END]\n                chars[char_start:char_start+pad_width] = char_map[SENT_END]\n                char_start += pad_width\n                tok_lens.extend([1] * pad_width)\n                labels.extend([SENT_END] * pad_width)\n                idx += pad_width\n\n                if i != len(lines)-1:\n                    tokens[idx:idx+pad_width] = token_map[SENT_START]\n                    shapes[idx:idx+pad_width] = shape_map[SENT_START]\n                    chars[char_start:char_start + pad_width] = char_map[SENT_START]\n                    char_start += pad_width\n                    tok_lens.extend([1] * pad_width)\n                    labels.extend([SENT_START]*pad_width)\n                    idx += pad_width\n            else:\n                tokens[idx:idx + pad_width] = token_map[PAD_STR]\n                shapes[idx:idx + pad_width] = shape_map[PAD_STR]\n                chars[char_start:char_start + pad_width] = char_map[PAD_STR]\n                char_start += pad_width\n                tok_lens.extend([1] * pad_width)\n                labels.extend([PAD_STR if FLAGS.predict_pad else ""O""] * pad_width)\n                idx += pad_width\n\n            last_label = ""O""\n\n    if last_label[0] == ""I"":\n        labels[-1] = ""L"" + labels[-1][1:]\n    elif last_label[0] == ""B"":\n        labels[-1] = ""U"" + labels[-1][1:]\n\n    if not FLAGS.documents:\n        sent_lens.append(sent_len)\n\n    # final padding\n    if not FLAGS.documents and FLAGS.start_end:\n        tokens[idx:idx+pad_width] = token_map[SENT_END]\n        shapes[idx:idx+pad_width] = shape_map[SENT_END]\n        chars[char_start:char_start+pad_width] = char_map[SENT_END]\n        char_start += pad_width\n        tok_lens.extend([1] * pad_width)\n        if FLAGS.predict_pad:\n            intmapped_labels[idx:idx+pad_width] = label_map[SENT_END]\n    elif not FLAGS.documents:\n        tokens[idx:idx+pad_width] = token_map[PAD_STR]\n        shapes[idx:idx+pad_width] = shape_map[PAD_STR]\n        chars[char_start:char_start+pad_width] = char_map[PAD_STR]\n        char_start += pad_width\n        tok_lens.extend([1] * pad_width)\n        if FLAGS.predict_pad:\n            intmapped_labels[idx:idx + pad_width] = label_map[PAD_STR]\n\n    for label in labels:\n        if label not in label_map:\n            label_map[label] = len(label_map)\n            label_int_str_map[label_map[label]] = label\n\n    intmapped_labels[pad_width:pad_width+len(labels)] = map(lambda s: label_map[s], labels)\n\n    # chars = chars.flatten()\n\n    # print(sent_lens)\n\n    padded_len = (2 if FLAGS.start_end else 1)*(len(sent_lens)+(0 if FLAGS.start_end else 1))*pad_width+sum(sent_lens)\n    intmapped_labels = intmapped_labels[:padded_len]\n    tokens = tokens[:padded_len]\n    shapes = shapes[:padded_len]\n    chars = chars[:sum(tok_lens)]\n\n    if FLAGS.debug:\n        print(""sent lens: "", sent_lens)\n        print(""tok lens: "", tok_lens, len(tok_lens), sum(tok_lens))\n        print(""labels"", map(lambda t: label_int_str_map[t], intmapped_labels), len(intmapped_labels))\n        print(""tokens"", map(lambda t: token_int_str_map[t], tokens), len(tokens))\n        print(""chars"", map(lambda t: char_int_str_map[t], chars), len(chars))\n\n    example = tf.train.SequenceExample()\n\n    fl_labels = example.feature_lists.feature_list[""labels""]\n    for l in intmapped_labels:\n        fl_labels.feature.add().int64_list.value.append(l)\n\n    fl_tokens = example.feature_lists.feature_list[""tokens""]\n    for t in tokens:\n        fl_tokens.feature.add().int64_list.value.append(t)\n\n    fl_shapes = example.feature_lists.feature_list[""shapes""]\n    for s in shapes:\n        fl_shapes.feature.add().int64_list.value.append(s)\n\n    fl_chars = example.feature_lists.feature_list[""chars""]\n    for c in chars:\n        fl_chars.feature.add().int64_list.value.append(c)\n\n    fl_seq_len = example.feature_lists.feature_list[""seq_len""]\n    for seq_len in sent_lens:\n        fl_seq_len.feature.add().int64_list.value.append(seq_len)\n\n    fl_tok_len = example.feature_lists.feature_list[""tok_len""]\n    for tok_len in tok_lens:\n        fl_tok_len.feature.add().int64_list.value.append(tok_len)\n\n    # example = tf.train.Example(features=tf.train.Features(feature={\n    #     \'labels\': _int64_feature(intmapped_labels),\n    #     \'tokens\': _int64_feature(tokens),\n    #     \'shapes\': _int64_feature(shapes),\n    #     \'chars\': _int64_feature(chars),\n    #     \'seq_len\': _int64_feature(sent_lens if FLAGS.documents else [sent_len])\n    # }))\n\n    writer.write(example.SerializeToString())\n    return sum(sent_lens), oov_count, 1\n\n\ndef tsv_to_examples():\n    # label_map = {ZERO_STR: 0}\n    # label_int_str_map[0] = ZERO_STR\n    # # embedding_map = {ZERO_STR: 0, PAD_STR: 1, OOV_STR: 2}\n    # token_map = {ZERO_STR: 0, OOV_STR: 1}\n    # token_int_str_map[0] = ZERO_STR\n    # token_int_str_map[1] = OOV_STR\n    # shape_map = {ZERO_STR: 0}\n    # char_map = {ZERO_STR: 0, NONE_STR: 1}\n    label_map = {}\n    token_map = {}\n    shape_map = {}\n    char_map = {}\n\n    update_vocab = True\n    update_chars = True\n\n    if FLAGS.start_end:\n        token_map[SENT_START] = len(token_map)\n        token_int_str_map[token_map[SENT_START]] = SENT_START\n        shape_map[SENT_START] = len(shape_map)\n        char_map[SENT_START] = len(char_map)\n        char_int_str_map[char_map[SENT_START]] = SENT_START\n        if FLAGS.predict_pad:\n            label_map[SENT_START] = len(label_map)\n            label_int_str_map[label_map[SENT_START]] = SENT_START\n        token_map[SENT_END] = len(token_map)\n        token_int_str_map[token_map[SENT_END]] = SENT_END\n        shape_map[SENT_END] = len(shape_map)\n        char_map[SENT_END] = len(char_map)\n        char_int_str_map[char_map[SENT_END]] = SENT_END\n        if FLAGS.predict_pad:\n            label_map[SENT_END] = len(label_map)\n            label_int_str_map[label_map[SENT_END]] = SENT_END\n\n    else:\n        token_map[PAD_STR] = len(token_map)\n        token_int_str_map[token_map[PAD_STR]] = PAD_STR\n        char_map[PAD_STR] = len(char_map)\n        char_int_str_map[char_map[PAD_STR]] = PAD_STR\n        shape_map[PAD_STR] = len(shape_map)\n        if FLAGS.predict_pad:\n            label_map[PAD_STR] = len(label_map)\n            label_int_str_map[label_map[PAD_STR]] = PAD_STR\n\n    token_map[OOV_STR] = len(token_map)\n    token_int_str_map[token_map[OOV_STR]] = OOV_STR\n    char_map[OOV_STR] = len(char_map)\n    char_int_str_map[char_map[OOV_STR]] = OOV_STR\n\n    # load embeddings if we have them\n    # if FLAGS.embeddings != \'\':\n    #     with open(FLAGS.embeddings, \'r\') as f:\n    #         for line in f.readlines():\n    #             # word, idx = line.strip().split(""\\t"")\n    #             # token_map[word] = int(idx)\n    #             word = line.strip().split("" "")[0]\n    #             if word not in token_map:\n    #                 # print(""adding word %s"" % word)\n    #                 embedding_map[word] = len(embedding_map)\n\n\n    # load vocab if we have one\n    if FLAGS.vocab != \'\':\n        update_vocab = False\n        with open(FLAGS.vocab, \'r\') as f:\n            for line in f.readlines():\n                word = line.strip().split("" "")[0]\n                # token_map[word] = int(idx)\n                # word = line.strip().split(""\\t"")[0]\n                if word not in token_map:\n                    # print(""adding word %s"" % word)\n                    token_map[word] = len(token_map)\n                    token_int_str_map[token_map[word]] = word\n    if FLAGS.update_vocab != \'\':\n        with open(FLAGS.update_vocab, \'r\') as f:\n            for line in f.readlines():\n                word = line.strip().split("" "")[0]\n                if word not in token_map:\n                    # print(""adding word %s"" % word)\n                    token_map[word] = len(token_map)\n                    token_int_str_map[token_map[word]] = word\n\n    # load labels if given\n    if FLAGS.labels != \'\':\n        with open(FLAGS.labels, \'r\') as f:\n            for line in f.readlines():\n                label, idx = line.strip().split(""\\t"")\n                label_map[label] = int(idx)\n                label_int_str_map[label_map[label]] = label\n\n    # load shapes if given\n    if FLAGS.shapes != \'\':\n        with open(FLAGS.shapes, \'r\') as f:\n            for line in f.readlines():\n                shape, idx = line.strip().split(""\\t"")\n                shape_map[shape] = int(idx)\n\n    # load chars if given\n    if FLAGS.chars != \'\':\n        update_chars = FLAGS.update_maps\n        with open(FLAGS.chars, \'r\') as f:\n            for line in f.readlines():\n                char, idx = line.strip().split(""\\t"")\n                char_map[char] = int(idx)\n                char_int_str_map[char_map[char]] = char\n\n    num_tokens = 0\n    num_sentences = 0\n    num_oov = 0\n    num_docs = 0\n    if not os.path.exists(FLAGS.out_dir):\n        print(""Output directory not found: %s"" % FLAGS.out_dir)\n    writer = tf.python_io.TFRecordWriter(FLAGS.out_dir + \'/examples.proto\')\n    with open(FLAGS.in_file) as f:\n        line_buf = []\n        line = f.readline()\n        line_idx = 1\n        while line:\n            line = line.strip()\n\n            if FLAGS.documents:\n                if line.split("" "")[0] == DOC_MARKER:\n                    if line_buf:\n                        # reached the end of a document; process the lines\n                        toks, oov, sent = make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n                        num_tokens += toks\n                        num_oov += oov\n                        num_sentences += sent\n                        num_docs += 1\n                        line_buf = []\n                else:\n                    # print(line)\n                    line_buf.append(line)\n                    line_idx += 1\n\n            else:\n                # if the line is not empty, add it to the buffer\n                if line:\n                    line_buf.append(line)\n                    line_idx += 1\n                # otherwise, if there\'s stuff in the buffer, process it\n                elif line_buf:\n                    # reached the end of a sentence; process the line\n                    toks, oov, sent = make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n                    num_tokens += toks\n                    num_oov += oov\n                    num_sentences += sent\n                    line_buf = []\n            # print(""reading line %d"" % line_idx)\n            line = f.readline()\n        if line_buf:\n            make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n        writer.close()\n\n    print(""Embeddings coverage: %2.2f%%"" % ((1-(num_oov/num_tokens)) * 100))\n\n    # remove padding from label domain\n    # filtered_label_map = label_map if FLAGS.predict_pad else \\\n    #     {label_str: label_map[label_str] for label_str in label_map.keys() if label_str not in pad_strs}\n\n    # export the string->int maps to file\n    for f_str, id_map in [(\'label\', label_map), (\'token\', token_map), (\'shape\', shape_map), (\'char\', char_map)]:\n        with open(FLAGS.out_dir + \'/\' + f_str + \'.txt\', \'w\') as f:\n            [f.write(s + \'\\t\' + str(i) + \'\\n\') for (s, i) in id_map.items()]\n\n    # export data sizes to file\n    with open(FLAGS.out_dir + ""/sizes.txt"", \'w\') as f:\n        print(num_sentences, file=f)\n        print(num_tokens, file=f)\n        print(num_docs, file=f)\n\n\ndef main(argv):\n    if FLAGS.out_dir == \'\':\n        print(\'Must supply out_dir\')\n        sys.exit(1)\n    tsv_to_examples()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
src/tf_utils.py,30,"b'from __future__ import division\nimport tensorflow as tf\nimport numpy as np\n\neps = 1e-5\n\n\ndef gather_nd(params, indices, shape=None, name=None):\n    if shape is None:\n        shape = params.get_shape().as_list()\n    rank = len(shape)\n    flat_params = tf.reshape(params, [-1])\n    multipliers = [reduce(lambda x, y: x * y, shape[i + 1:], 1) for i in range(0, rank)]\n    indices_unpacked = tf.unstack(tf.cast(tf.transpose(indices, [rank - 1] + range(0, rank - 1), name), \'int32\'))\n    flat_indices = sum([a * b for a, b in zip(multipliers, indices_unpacked)])\n    return tf.gather(flat_params, flat_indices, name=name)\n\n\ndef repeat(tensor, reps):\n    flat_tensor = tf.reshape(tensor, [-1, 1])  # Convert to a len(yp) x 1 matrix.\n    repeated = tf.tile(flat_tensor, [1, reps])  # Create multiple columns.\n    repeated_flat = tf.reshape(repeated, [-1])  # Convert back to a vector.\n    return repeated_flat\n\n\ndef last_relevant(output, length):\n    batch_size = tf.shape(output)[0]\n    max_length = tf.shape(output)[1]\n    out_size = int(output.get_shape()[2])\n    index = tf.range(0, batch_size) * max_length + (length - 1)\n    flat = tf.reshape(output, [-1, out_size])\n    relevant = tf.gather(flat, index)\n    return relevant\n\n\ndef apply_nonlinearity(parameters, nonlinearity_type):\n    if nonlinearity_type == ""relu"":\n        return tf.nn.relu(parameters, name=""relu"")\n    elif nonlinearity_type == ""tanh"":\n        return tf.nn.tanh(parameters, name=""tanh"")\n    elif nonlinearity_type == ""sigmoid"":\n        return tf.nn.sigmoid(parameters, name=""sigmoid"")\n\n\ndef embedding_values(shape, old=False):\n    if old:\n        embeddings = np.multiply(np.add(np.random.rand(shape[0], shape[1]).astype(\'float32\'), -0.1), 0.01)\n    else:\n        # xavier init\n        drange = np.sqrt(6.0 / (np.sum(shape)))\n        embeddings = drange * np.random.uniform(low=-1.0, high=1.0, size=shape).astype(\'float32\')\n    return embeddings\n\n\ndef initialize_embeddings(shape, name, pretrained=None, old=False):\n    zero_pad = tf.constant(0.0, dtype=tf.float32, shape=[1, shape[1]])\n    if pretrained is None:\n        embeddings = embedding_values(shape, old)\n    else:\n        embeddings = pretrained\n    return tf.concat(axis=0, values=[zero_pad, tf.get_variable(name=name, initializer=embeddings)])\n\n\ndef initialize_weights(shape, name, init_type, gain=""1.0"", divisor=1.0):\n    if init_type == ""random"":\n        return tf.get_variable(name, initializer=tf.truncated_normal(shape, stddev=0.1))\n    if init_type == ""xavier"":\n        return tf.get_variable(name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n    if init_type == ""identity"":\n        middle0 = int(shape[0] / 2)\n        middle1 = int(shape[1] / 2)\n        if shape[2] == shape[3]:\n            array = np.zeros(shape, dtype=\'float32\')\n            identity = np.eye(shape[2], shape[3])\n            array[middle0, middle1] = identity\n        else:\n            m1 = divisor / shape[2]\n            m2 = divisor / shape[3]\n            sigma = eps*m2\n            array = np.random.normal(loc=0, scale=sigma, size=shape).astype(\'float32\')\n            for i in range(shape[2]):\n                for j in range(shape[3]):\n                    if int(i*m1) == int(j*m2):\n                        array[middle0, middle1, i, j] = m2\n        return tf.get_variable(name, initializer=array)\n    if init_type == ""varscale"":\n        return tf.get_variable(name, shape=shape, initializer=tf.contrib.layers.variance_scaling_initializer())\n    if init_type == ""orthogonal"":\n        gain = np.sqrt(2) if gain == ""relu"" else 1.0\n        array = np.zeros(shape, dtype=\'float32\')\n        random = np.random.normal(0.0, 1.0, (shape[2], shape[3])).astype(\'float32\')\n        u, _, v_t = np.linalg.svd(random, full_matrices=False)\n        middle = int(shape[1] / 2)\n        array[0, middle] = gain * v_t\n        return tf.get_variable(name, initializer=array)\n\n\ndef residual_layer(input, w, b, dilation, nonlinearity, batch_norm, name, batch_size, max_sequence_len, activation, training):\n    # if activation == ""pre"" (2): BN -> relu -> weight -> BN -> relu -> weight -> addition\n    conv_in_bn = tf.contrib.layers.batch_norm(input, decay=0.995, scale=False, is_training=training, trainable=True) \\\n                    if batch_norm and activation == 2 else input\n    conv_in = apply_nonlinearity(conv_in_bn, nonlinearity) if activation == 2 else conv_in_bn\n    conv = tf.nn.atrous_conv2d(\n        conv_in,\n        w,\n        rate=dilation,\n        padding=""SAME"",\n        name=name)\n    conv_b = tf.nn.bias_add(conv, b)\n\n    # if activation == ""post"" (1): weight -> BN -> relu -> weight -> BN -> addition -> relu\n    conv_out_bn = tf.contrib.layers.batch_norm(conv_b, decay=0.995, scale=False, is_training=training, trainable=True) \\\n                    if batch_norm and activation != 2 else conv_b\n    conv_out = apply_nonlinearity(conv_out_bn, nonlinearity) if activation != 2 else conv_out_bn\n\n    # if activation == ""none"" (0): weight -> BN -> relu\n    conv_shape = w.get_shape()\n    if conv_shape[-1] != conv_shape[-2] and activation != 0:\n        # if len(input_shape) != 2:\n        input = tf.reshape(input, [-1, tf.to_int32(conv_shape[-2])])\n        w_r = initialize_weights([conv_shape[-2], conv_shape[-1]], ""w_o_"" + name, init_type=""xavier"")\n        b_r = tf.get_variable(""b_r_"" + name, initializer=tf.constant(0.01, shape=[conv_shape[-1]]))\n        input_projected = tf.nn.xw_plus_b(input, w_r, b_r, name=""proj_r_"" + name)\n        # if len(output_shape) != 2:\n        input_projected = tf.reshape(input_projected, tf.stack([batch_size, 1, max_sequence_len, tf.to_int32(conv_shape[-1])]))\n        return tf.add(input_projected, conv_out)\n    else:\n        return conv_out\n'"
src/train.py,72,"b'from __future__ import division\nfrom __future__ import print_function\nimport sys\nimport time\nimport tensorflow as tf\nimport numpy as np\nfrom data_utils import SeqBatcher, Batcher\nfrom cnn import CNN\nfrom bilstm import BiLSTM\nfrom bilstm_char import BiLSTMChar\nfrom cnn_char import CNNChar\nimport eval_f1 as evaluation\nimport json\nimport tf_utils\nfrom os import listdir\nimport os\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(argv):\n    print(""CUDA_VISIBLE_DEVICES="", os.environ[\'CUDA_VISIBLE_DEVICES\'])\n\n    train_dir = FLAGS.train_dir\n    dev_dir = FLAGS.dev_dir\n    maps_dir = FLAGS.maps_dir\n\n    if train_dir == \'\':\n        print(\'Must supply input data directory generated from tsv_to_tfrecords.py\')\n        sys.exit(1)\n\n    # Doesn\'t work in newer versions of tf. TODO: fix\n    \n    # print(\'\\n\'.join(sorted([""%s : %s"" % (str(k), str(v)) for k, v in FLAGS.__dict__[\'__flags\'].items()])))\n\n    with open(maps_dir + \'/label.txt\', \'r\') as f:\n        labels_str_id_map = {l.split(\'\\t\')[0]: int(l.split(\'\\t\')[1].strip()) for l in f.readlines()}\n        labels_id_str_map = {i: s for s, i in labels_str_id_map.items()}\n        labels_size = len(labels_id_str_map)\n    with open(maps_dir + \'/token.txt\', \'r\') as f:\n        vocab_str_id_map = {l.split(\'\\t\')[0]: int(l.split(\'\\t\')[1].strip()) for l in f.readlines()}\n        vocab_id_str_map = {i: s for s, i in vocab_str_id_map.items()}\n        vocab_size = len(vocab_id_str_map)\n    with open(maps_dir + \'/shape.txt\', \'r\') as f:\n        shape_str_id_map = {l.split(\'\\t\')[0]: int(l.split(\'\\t\')[1].strip()) for l in f.readlines()}\n        shape_id_str_map = {i: s for s, i in shape_str_id_map.items()}\n        shape_domain_size = len(shape_id_str_map)\n    with open(maps_dir + \'/char.txt\', \'r\') as f:\n        char_str_id_map = {l.split(\'\\t\')[0]: int(l.split(\'\\t\')[1].strip()) for l in f.readlines()}\n        char_id_str_map = {i: s for s, i in char_str_id_map.items()}\n        char_domain_size = len(char_id_str_map)\n\n    # with open(maps_dir + \'/sizes.txt\', \'r\') as f:\n    #     num_train_examples = int(f.readline()[:-1])\n\n    print(""num classes: %d"" % labels_size)\n\n    size_files = [maps_dir + ""/"" + fname for fname in listdir(maps_dir) if fname.find(""sizes"") != -1]\n    num_train_examples = 0\n    num_tokens = 0\n    for size_file in size_files:\n        print(size_file)\n        with open(size_file, \'r\') as f:\n            num_train_examples += int(f.readline()[:-1])\n            num_tokens += int(f.readline()[:-1])\n\n    print(""num train examples: %d"" % num_train_examples)\n    print(""num train tokens: %d"" % num_tokens)\n\n    dev_top_dir = \'/\'.join(dev_dir.split(""/"")[:-2]) if dev_dir.find(""*"") != -1 else dev_dir\n    print(dev_top_dir)\n    dev_size_files = [dev_top_dir + ""/"" + fname for fname in listdir(dev_top_dir) if fname.find(""sizes"") != -1]\n    num_dev_examples = 0\n    num_dev_tokens = 0\n    for size_file in dev_size_files:\n        print(size_file)\n        with open(size_file, \'r\') as f:\n            num_dev_examples += int(f.readline()[:-1])\n            num_dev_tokens += int(f.readline()[:-1])\n\n    print(""num dev examples: %d"" % num_dev_examples)\n    print(""num dev tokens: %d"" % num_dev_tokens)\n\n    # with open(dev_dir + \'/sizes.txt\', \'r\') as f:\n    #     num_dev_examples = int(f.readline()[:-1])\n\n    type_set = {}\n    type_int_int_map = {}\n    outside_set = [""O"", ""<PAD>"",  ""<S>"",  ""</S>"", ""<ZERO>""]\n    for label, id in labels_str_id_map.items():\n        label_type = label if label in outside_set else label[2:]\n        if label_type not in type_set:\n            type_set[label_type] = len(type_set)\n        type_int_int_map[id] = type_set[label_type]\n    print(type_set)\n\n    # load embeddings, if given; initialize in range [-.01, .01]\n    embeddings_shape = (vocab_size-1, FLAGS.embed_dim)\n    embeddings = tf_utils.embedding_values(embeddings_shape, old=False)\n    embeddings_used = 0\n    if FLAGS.embeddings != \'\':\n        with open(FLAGS.embeddings, \'r\') as f:\n            for line in f.readlines():\n                split_line = line.strip().split("" "")\n                word = split_line[0]\n                embedding = split_line[1:]\n                if word in vocab_str_id_map:\n                    embeddings_used += 1\n                    # shift by -1 because we are going to add a 0 constant vector for the padding later\n                    embeddings[vocab_str_id_map[word]-1] = map(float, embedding)\n                elif word.lower() in vocab_str_id_map:\n                    embeddings_used += 1\n                    embeddings[vocab_str_id_map[word.lower()] - 1] = map(float, embedding)\n    print(""Loaded %d/%d embeddings (%2.2f%% coverage)"" % (embeddings_used, vocab_size, embeddings_used/vocab_size*100))\n\n    layers_map = sorted(json.loads(FLAGS.layers.replace(""\'"", \'""\')).items()) if FLAGS.model == \'cnn\' else None\n\n    pad_width = int(layers_map[0][1][\'width\']/2) if layers_map is not None else 1\n\n    with tf.Graph().as_default():\n        train_batcher = Batcher(train_dir, FLAGS.batch_size) if FLAGS.memmap_train else SeqBatcher(train_dir, FLAGS.batch_size)\n\n        dev_batch_size = FLAGS.batch_size # num_dev_examples\n        dev_batcher = SeqBatcher(dev_dir, dev_batch_size, num_buckets=0, num_epochs=1)\n        if FLAGS.ontonotes:\n            domain_dev_batchers = {domain: SeqBatcher(dev_dir.replace(\'*\', domain),\n                                                      dev_batch_size, num_buckets=0, num_epochs=1)\n                                   for domain in [\'bc\', \'nw\', \'bn\', \'wb\', \'mz\', \'tc\']}\n\n        train_eval_batch_size = FLAGS.batch_size\n        train_eval_batcher = SeqBatcher(train_dir, train_eval_batch_size, num_buckets=0, num_epochs=1)\n\n        char_embedding_model = BiLSTMChar(char_domain_size, FLAGS.char_dim, int(FLAGS.char_tok_dim/2)) \\\n            if FLAGS.char_dim > 0 and FLAGS.char_model == ""lstm"" else \\\n            (CNNChar(char_domain_size, FLAGS.char_dim, FLAGS.char_tok_dim, layers_map[0][1][\'width\'])\n                if FLAGS.char_dim > 0 and FLAGS.char_model == ""cnn"" else None)\n        char_embeddings = char_embedding_model.outputs if char_embedding_model is not None else None\n\n        if FLAGS.model == \'cnn\':\n            model = CNN(\n                    num_classes=labels_size,\n                    vocab_size=vocab_size,\n                    shape_domain_size=shape_domain_size,\n                    char_domain_size=char_domain_size,\n                    char_size=FLAGS.char_tok_dim,\n                    embedding_size=FLAGS.embed_dim,\n                    shape_size=FLAGS.shape_dim,\n                    nonlinearity=FLAGS.nonlinearity,\n                    layers_map=layers_map,\n                    viterbi=FLAGS.viterbi,\n                    projection=FLAGS.projection,\n                    loss=FLAGS.loss,\n                    margin=FLAGS.margin,\n                    repeats=FLAGS.block_repeats,\n                    share_repeats=FLAGS.share_repeats,\n                    char_embeddings=char_embeddings,\n                    embeddings=embeddings)\n        elif FLAGS.model == ""bilstm"":\n            model = BiLSTM(\n                    num_classes=labels_size,\n                    vocab_size=vocab_size,\n                    shape_domain_size=shape_domain_size,\n                    char_domain_size=char_domain_size,\n                    char_size=FLAGS.char_dim,\n                    embedding_size=FLAGS.embed_dim,\n                    shape_size=FLAGS.shape_dim,\n                    nonlinearity=FLAGS.nonlinearity,\n                    viterbi=FLAGS.viterbi,\n                    hidden_dim=FLAGS.lstm_dim,\n                    char_embeddings=char_embeddings,\n                    embeddings=embeddings)\n        else:\n            print(FLAGS.model + \' is not a valid model type\')\n            sys.exit(1)\n\n        # Define Training procedure\n        global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n        optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.lr, beta1=FLAGS.beta1, beta2=FLAGS.beta2, epsilon=FLAGS.epsilon, name=""optimizer"")\n\n        model_vars = tf.global_variables()\n\n        print(""model vars: %d"" % len(model_vars))\n        print(map(lambda v: v.name, model_vars))\n\n        # todo put in func\n        total_parameters = 0\n        for variable in tf.trainable_variables():\n            # shape is an array of tf.Dimension\n            shape = variable.get_shape()\n            variable_parametes = 1\n            for dim in shape:\n                variable_parametes *= dim.value\n            total_parameters += variable_parametes\n        print(""Total trainable parameters: %d"" % (total_parameters))\n\n        if FLAGS.clip_norm > 0:\n            grads, _ = tf.clip_by_global_norm(tf.gradients(model.loss, model_vars), FLAGS.clip_norm)\n            train_op = optimizer.apply_gradients(zip(grads, model_vars), global_step=global_step)\n        else:\n            train_op = optimizer.minimize(model.loss, global_step=global_step, var_list=model_vars)\n\n        tf.global_variables_initializer()\n\n        opt_vars = [optimizer.get_slot(s, n) for n in optimizer.get_slot_names() for s in model_vars if optimizer.get_slot(s, n) is not None]\n        model_vars += opt_vars\n\n        if FLAGS.load_dir:\n            reader = tf.train.NewCheckpointReader(FLAGS.load_dir + "".tf"")\n            saved_var_map = reader.get_variable_to_shape_map()\n            intersect_vars = [k for k in tf.global_variables() if k.name.split(\':\')[0] in saved_var_map and k.get_shape() == saved_var_map[k.name.split(\':\')[0]]]\n            leftovers = [k for k in tf.global_variables() if k.name.split(\':\')[0] not in saved_var_map or k.get_shape() != saved_var_map[k.name.split(\':\')[0]]]\n            print(""WARNING: Loading pretrained model, but not loading: "", map(lambda v: v.name, leftovers))\n            loader = tf.train.Saver(var_list=intersect_vars)\n\n        else:\n            loader = tf.train.Saver(var_list=model_vars)\n\n        saver = tf.train.Saver(var_list=model_vars)\n\n        sv = tf.train.Supervisor(logdir=FLAGS.model_dir if FLAGS.model_dir != \'\' else None,\n                                 global_step=global_step,\n                                 saver=None,\n                                 save_model_secs=0,\n                                 save_summaries_secs=0)\n\n        training_start_time = time.time()\n        with sv.managed_session(FLAGS.master, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n            def run_evaluation(eval_batches, extra_text=""""):\n                predictions = []\n                for b, (eval_label_batch, eval_token_batch, eval_shape_batch, eval_char_batch, eval_seq_len_batch, eval_tok_len_batch, eval_mask_batch) in enumerate(eval_batches):\n                    batch_size, batch_seq_len = eval_token_batch.shape\n\n                    char_lens = np.sum(eval_tok_len_batch, axis=1)\n                    max_char_len = np.max(eval_tok_len_batch)\n                    eval_padded_char_batch = np.zeros((batch_size, max_char_len * batch_seq_len))\n                    for b in range(batch_size):\n                        char_indices = [item for sublist in [range(i * max_char_len, i * max_char_len + d) for i, d in\n                                                             enumerate(eval_tok_len_batch[b])] for item in sublist]\n                        eval_padded_char_batch[b, char_indices] = eval_char_batch[b][:char_lens[b]]\n\n                    char_embedding_feeds = {} if FLAGS.char_dim == 0 else {\n                        char_embedding_model.input_chars: eval_padded_char_batch,\n                        char_embedding_model.batch_size: batch_size,\n                        char_embedding_model.max_seq_len: batch_seq_len,\n                        char_embedding_model.token_lengths: eval_tok_len_batch,\n                        char_embedding_model.max_tok_len: max_char_len\n                    }\n\n                    basic_feeds = {\n                        model.input_x1: eval_token_batch,\n                        model.input_x2: eval_shape_batch,\n                        model.input_y: eval_label_batch,\n                        model.input_mask: eval_mask_batch,\n                        model.max_seq_len: batch_seq_len,\n                        model.batch_size: batch_size,\n                        model.sequence_lengths: eval_seq_len_batch\n                    }\n\n                    basic_feeds.update(char_embedding_feeds)\n                    total_feeds = basic_feeds.copy()\n\n                    if FLAGS.viterbi:\n                        preds, transition_params = sess.run([model.predictions, model.transition_params], feed_dict=total_feeds)\n\n                        viterbi_repad = np.empty((batch_size, batch_seq_len))\n                        for batch_idx, (unary_scores, sequence_lens) in enumerate(zip(preds, eval_seq_len_batch)):\n                            viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(unary_scores, transition_params)\n                            viterbi_repad[batch_idx] = viterbi_sequence\n                        predictions.append(viterbi_repad)\n                    else:\n                        preds, scores = sess.run([model.predictions, model.unflat_scores], feed_dict=total_feeds)\n                        predictions.append(preds)\n\n                if FLAGS.print_preds != \'\':\n                    evaluation.print_conlleval_format(FLAGS.print_preds, eval_batches, predictions, labels_id_str_map, vocab_id_str_map, pad_width)\n\n                # print evaluation\n                f1_micro, precision = evaluation.segment_eval(eval_batches, predictions, type_set, type_int_int_map,\n                                                   labels_id_str_map, vocab_id_str_map,\n                                                   outside_idx=map(lambda t: type_set[t] if t in type_set else type_set[""O""], outside_set),\n                                                   pad_width=pad_width, start_end=FLAGS.start_end,\n                                                   extra_text=""Segment evaluation %s:"" % extra_text)\n\n                return f1_micro, precision\n\n            threads = tf.train.start_queue_runners(sess=sess)\n            log_every = int(max(100, num_train_examples / 5))\n\n            if FLAGS.load_dir != \'\':\n                print(""Deserializing model: "" + FLAGS.load_dir + "".tf"")\n                loader.restore(sess, FLAGS.load_dir + "".tf"")\n\n            def get_dev_batches(seq_batcher):\n                batches = []\n                # load all the dev batches into memory\n                done = False\n                while not done:\n                    try:\n                        dev_batch = sess.run(seq_batcher.next_batch_op)\n                        dev_label_batch, dev_token_batch, dev_shape_batch, dev_char_batch, dev_seq_len_batch, dev_tok_len_batch = dev_batch\n                        mask_batch = np.zeros(dev_token_batch.shape)\n                        actual_seq_lens = np.add(np.sum(dev_seq_len_batch, axis=1),\n                                                 (2 if FLAGS.start_end else 1) * pad_width * (\n                                                 (dev_seq_len_batch != 0).sum(axis=1) + (\n                                                 0 if FLAGS.start_end else 1)))\n                        for i, seq_len in enumerate(actual_seq_lens):\n                            mask_batch[i, :seq_len] = 1\n                        batches.append((dev_label_batch, dev_token_batch, dev_shape_batch, dev_char_batch,\n                                            dev_seq_len_batch, dev_tok_len_batch, mask_batch))\n                    except:\n                        done = True\n                return batches\n            dev_batches = get_dev_batches(dev_batcher)\n            if FLAGS.ontonotes:\n                domain_batches = {domain: get_dev_batches(domain_batcher)\n                                  for domain, domain_batcher in domain_dev_batchers.iteritems()}\n\n            train_batches = []\n            if FLAGS.train_eval:\n                # load all the train batches into memory\n                done = False\n                while not done:\n                    try:\n                        train_batch = sess.run(train_eval_batcher.next_batch_op)\n                        train_label_batch, train_token_batch, train_shape_batch, train_char_batch, train_seq_len_batch, train_tok_len_batch = train_batch\n                        mask_batch = np.zeros(train_token_batch.shape)\n                        actual_seq_lens = np.add(np.sum(train_seq_len_batch, axis=1), (2 if FLAGS.start_end else 1) * pad_width * ((train_seq_len_batch != 0).sum(axis=1) + (0 if FLAGS.start_end else 1)))\n                        for i, seq_len in enumerate(actual_seq_lens):\n                            mask_batch[i, :seq_len] = 1\n                        train_batches.append((train_label_batch, train_token_batch, train_shape_batch, train_char_batch, train_seq_len_batch, train_tok_len_batch, mask_batch))\n                    except Exception as e:\n                        done = True\n            if FLAGS.memmap_train:\n                train_batcher.load_and_bucket_data(sess)\n\n            def train(max_epochs, best_score, model_hidden_drop, model_input_drop, until_convergence, max_lower=6, min_iters=20):\n                print(""Training on %d sentences (%d examples)"" % (num_train_examples, num_train_examples))\n                start_time = time.time()\n                train_batcher._step = 1.0\n                converged = False\n                examples = 0\n                log_every_running = log_every\n                epoch_loss = 0.0\n                num_lower = 0\n                training_iteration = 0\n                speed_num = 0.0\n                speed_denom = 0.0\n                while not sv.should_stop() and training_iteration < max_epochs and not (until_convergence and converged):\n                    # evaluate\n                    if examples >= num_train_examples:\n                        training_iteration += 1\n\n                        if FLAGS.train_eval:\n                            run_evaluation(train_batches, ""TRAIN (iteration %d)"" % training_iteration)\n                        print()\n                        f1_micro, precision = run_evaluation(dev_batches, ""TEST (iteration %d)"" % training_iteration)\n                        print(""Avg training speed: %f examples/second"" % (speed_num/speed_denom))\n\n                        # keep track of running best / convergence heuristic\n                        if f1_micro > best_score:\n                            best_score = f1_micro\n                            num_lower = 0\n                            if FLAGS.model_dir != \'\' and best_score > FLAGS.save_min:\n                                    save_path = saver.save(sess, FLAGS.model_dir + "".tf"")\n                                    print(""Serialized model: %s"" % save_path)\n                        else:\n                            num_lower += 1\n                        if num_lower > max_lower and training_iteration > min_iters:\n                            converged = True\n\n                        # update per-epoch variables\n                        log_every_running = log_every\n                        examples = 0\n                        epoch_loss = 0.0\n                        start_time = time.time()\n\n                    if examples > log_every_running:\n                        speed_denom += time.time()-start_time\n                        speed_num += examples\n                        evaluation.print_training_error(examples, start_time, [epoch_loss], train_batcher._step)\n                        log_every_running += log_every\n\n                    # Training iteration\n\n                    label_batch, token_batch, shape_batch, char_batch, seq_len_batch, tok_lengths_batch = \\\n                        train_batcher.next_batch() if FLAGS.memmap_train else sess.run(train_batcher.next_batch_op)\n\n                    # make mask out of seq lens\n                    batch_size, batch_seq_len = token_batch.shape\n\n                    char_lens = np.sum(tok_lengths_batch, axis=1)\n                    max_char_len = np.max(tok_lengths_batch)\n                    padded_char_batch = np.zeros((batch_size, max_char_len * batch_seq_len))\n                    for b in range(batch_size):\n                        char_indices = [item for sublist in [range(i * max_char_len, i * max_char_len + d) for i, d in\n                                                             enumerate(tok_lengths_batch[b])] for item in sublist]\n                        padded_char_batch[b, char_indices] = char_batch[b][:char_lens[b]]\n\n                    max_sentences = max(map(len, seq_len_batch))\n                    new_seq_len_batch = np.zeros((batch_size, max_sentences))\n                    for i, seq_len_list in enumerate(seq_len_batch):\n                        new_seq_len_batch[i,:len(seq_len_list)] = seq_len_list\n                    seq_len_batch = new_seq_len_batch\n                    num_sentences_batch = np.sum(seq_len_batch != 0, axis=1)\n\n                    mask_batch = np.zeros((batch_size, batch_seq_len)).astype(""int"")\n                    actual_seq_lens = np.add(np.sum(seq_len_batch, axis=1), (2 if FLAGS.start_end else 1) * pad_width * (num_sentences_batch + (0 if FLAGS.start_end else 1))).astype(\'int\')\n                    for i, seq_len in enumerate(actual_seq_lens):\n                        mask_batch[i, :seq_len] = 1\n                    examples += batch_size\n\n                    # apply word dropout\n                    # create word dropout mask\n                    word_probs = np.random.random(token_batch.shape)\n                    drop_indices = np.where((word_probs > FLAGS.word_dropout) & (token_batch != vocab_str_id_map[""<PAD>""]))\n                    token_batch[drop_indices[0], drop_indices[1]] = vocab_str_id_map[""<OOV>""]\n\n                    char_embedding_feeds = {} if FLAGS.char_dim == 0 else {\n                        char_embedding_model.input_chars: padded_char_batch,\n                        char_embedding_model.batch_size: batch_size,\n                        char_embedding_model.max_seq_len: batch_seq_len,\n                        char_embedding_model.token_lengths: tok_lengths_batch,\n                        char_embedding_model.max_tok_len: max_char_len,\n                        char_embedding_model.input_dropout_keep_prob: FLAGS.char_input_dropout\n                    }\n\n                    if FLAGS.model == ""cnn"":\n                        cnn_feeds = {\n                                model.input_x1: token_batch,\n                                model.input_x2: shape_batch,\n                                model.input_y: label_batch,\n                                model.input_mask: mask_batch,\n                                model.max_seq_len: batch_seq_len,\n                                model.sequence_lengths: seq_len_batch,\n                                model.batch_size: batch_size,\n                                model.hidden_dropout_keep_prob: model_hidden_drop,\n                                model.input_dropout_keep_prob: model_input_drop,\n                                model.middle_dropout_keep_prob: FLAGS.middle_dropout,\n                                model.l2_penalty: FLAGS.l2,\n                                model.drop_penalty: FLAGS.regularize_drop_penalty,\n                            }\n                        cnn_feeds.update(char_embedding_feeds)\n                        _,  loss = sess.run([train_op, model.loss], feed_dict=cnn_feeds)\n                    elif FLAGS.model == ""bilstm"":\n                        lstm_feed = {\n                            model.input_x1: token_batch,\n                            model.input_x2: shape_batch,\n                            model.input_y: label_batch,\n                            model.input_mask: mask_batch,\n                            model.sequence_lengths: seq_len_batch,\n                            model.max_seq_len: batch_seq_len,\n                            model.batch_size: batch_size,\n                            model.hidden_dropout_keep_prob: FLAGS.hidden_dropout,\n                            model.middle_dropout_keep_prob: FLAGS.middle_dropout,\n                            model.input_dropout_keep_prob: FLAGS.input_dropout,\n                            model.l2_penalty: FLAGS.l2,\n                            model.drop_penalty: FLAGS.regularize_drop_penalty\n                        }\n                        lstm_feed.update(char_embedding_feeds)\n                        _, loss = sess.run([train_op, model.loss], feed_dict=lstm_feed)\n                    epoch_loss += loss\n                    train_batcher._step += 1\n                return best_score, training_iteration, speed_num/speed_denom\n\n            if FLAGS.evaluate_only:\n                if FLAGS.train_eval:\n                    run_evaluation(train_batches, ""(train)"")\n                print()\n                run_evaluation(dev_batches, ""(test)"")\n                if FLAGS.ontonotes:\n                    for domain, domain_batches in domain_batches.iteritems():\n                        print()\n                        run_evaluation(domain_batches, FLAGS.layers2 != \'\', ""(test - domain: %s)"" % domain)\n\n            else:\n                best_score, training_iteration, train_speed = train(FLAGS.max_epochs, 0.0,\n                                                       FLAGS.hidden_dropout, FLAGS.input_dropout,\n                                                       until_convergence=FLAGS.until_convergence)\n                if FLAGS.model_dir:\n                    print(""Deserializing model: "" + FLAGS.model_dir + "".tf"")\n                    saver.restore(sess, FLAGS.model_dir + "".tf"")\n\n            sv.coord.request_stop()\n            sv.coord.join(threads)\n            sess.close()\n\n            total_time = time.time()-training_start_time\n            if FLAGS.evaluate_only:\n                print(""Testing time: %d seconds"" % (total_time))\n            else:\n                print(""Training time: %d minutes, %d iterations (%3.2f minutes/iteration)"" % (total_time/60, training_iteration, total_time/(60*training_iteration)))\n                print(""Avg training speed: %f examples/second"" % (train_speed))\n                print(""Best dev F1: %2.2f"" % (best_score*100))\n\nif __name__ == \'__main__\':\n    tf.app.flags.DEFINE_string(\'train_dir\', \'\', \'directory containing preprocessed training data\')\n    tf.app.flags.DEFINE_string(\'dev_dir\', \'\', \'directory containing preprocessed dev data\')\n    tf.app.flags.DEFINE_string(\'test_dir\', \'\', \'directory containing preprocessed test data\')\n    tf.app.flags.DEFINE_string(\'maps_dir\', \'\', \'directory containing data intmaps\')\n\n    tf.app.flags.DEFINE_string(\'model_dir\', \'\', \'save model to this dir (if empty do not save)\')\n    tf.app.flags.DEFINE_string(\'load_dir\', \'\', \'load model from this dir (if empty do not load)\')\n\n    tf.app.flags.DEFINE_string(\'optimizer\', \'adam\', \'optimizer to use\')\n    tf.app.flags.DEFINE_string(\'master\', \'\', \'use for Supervisor\')\n    tf.app.flags.DEFINE_string(\'model\', \'cnn\', \'which model to use [cnn, seq2seq, lstm, bilstm]\')\n    tf.app.flags.DEFINE_integer(\'filter_size\', 3, ""filter size"")\n\n    tf.app.flags.DEFINE_float(\'lr\', 0.001, \'learning rate\')\n    tf.app.flags.DEFINE_float(\'l2\', 0.0, \'l2 penalty\')\n    tf.app.flags.DEFINE_float(\'beta1\', 0.9, \'beta1\')\n    tf.app.flags.DEFINE_float(\'beta2\', 0.999, \'beta2\')\n    tf.app.flags.DEFINE_float(\'epsilon\', 1e-8, \'epsilon\')\n\n    tf.app.flags.DEFINE_float(\'hidden_dropout\', .75, \'hidden layer dropout rate\')\n    tf.app.flags.DEFINE_float(\'input_dropout\', 1.0, \'input layer (word embedding) dropout rate\')\n    tf.app.flags.DEFINE_float(\'middle_dropout\', 1.0, \'middle layer dropout rate\')\n    tf.app.flags.DEFINE_float(\'word_dropout\', 1.0, \'whole-word (-> oov) dropout rate\')\n\n    tf.app.flags.DEFINE_float(\'clip_norm\', 0, \'clip gradients to have norm <= this\')\n    tf.app.flags.DEFINE_integer(\'batch_size\', 128, \'batch size\')\n    tf.app.flags.DEFINE_integer(\'lstm_dim\', 2048, \'lstm internal dimension\')\n    tf.app.flags.DEFINE_integer(\'embed_dim\', 50, \'word embedding dimension\')\n    tf.app.flags.DEFINE_integer(\'shape_dim\', 5, \'shape embedding dimension\')\n    tf.app.flags.DEFINE_integer(\'char_dim\', 0, \'character embedding dimension\')\n    tf.app.flags.DEFINE_integer(\'char_tok_dim\', 0, \'character token embedding dimension\')\n    tf.app.flags.DEFINE_string(\'char_model\', \'lstm\', \'character embedding model (lstm, cnn)\')\n\n    tf.app.flags.DEFINE_integer(\'max_finetune_epochs\', 100, \'train for this many epochs\')\n    tf.app.flags.DEFINE_integer(\'max_context_epochs\', 100, \'train for this many epochs\')\n\n    tf.app.flags.DEFINE_integer(\'max_epochs\', 100, \'train for this many epochs\')\n\n    tf.app.flags.DEFINE_integer(\'log_every\', 2, \'log status every k steps\')\n    tf.app.flags.DEFINE_string(\'embeddings\', \'\', \'file of pretrained embeddings to use\')\n    tf.app.flags.DEFINE_string(\'nonlinearity\', \'relu\', \'nonlinearity function to use (tanh, sigmoid, relu)\')\n    tf.app.flags.DEFINE_boolean(\'until_convergence\', False, \'whether to run until convergence\')\n    tf.app.flags.DEFINE_boolean(\'evaluate_only\', False, \'whether to only run evaluation\')\n    tf.app.flags.DEFINE_string(\'layers\', \'\', \'json definition of layers (dilation, filters, width)\')\n    tf.app.flags.DEFINE_string(\'print_preds\', \'\', \'print out predictions (for conll eval script) to given file (or do not if empty)\')\n    tf.app.flags.DEFINE_boolean(\'viterbi\', False, \'whether to use viberbi inference\')\n    tf.app.flags.DEFINE_boolean(\'train_eval\', False, \'whether to report train accuracy\')\n    tf.app.flags.DEFINE_boolean(\'memmap_train\', True, \'whether to load all training examples into memory\')\n    tf.app.flags.DEFINE_boolean(\'projection\', False, \'whether to do final halving projection (front end)\')\n\n    tf.app.flags.DEFINE_integer(\'block_repeats\', 1, \'number of times to repeat the stacked dilations block\')\n    tf.app.flags.DEFINE_boolean(\'share_repeats\', True, \'whether to share parameters between blocks\')\n\n    tf.app.flags.DEFINE_string(\'loss\', \'mean\', \'\')\n    tf.app.flags.DEFINE_float(\'margin\', 1.0, \'margin\')\n\n    tf.app.flags.DEFINE_float(\'char_input_dropout\', 1.0, \'dropout for character embeddings\')\n\n    tf.app.flags.DEFINE_float(\'save_min\', 0.0, \'min accuracy before saving\')\n\n    tf.app.flags.DEFINE_boolean(\'start_end\', False, \'whether using start/end or just pad between sentences\')\n    tf.app.flags.DEFINE_float(\'regularize_drop_penalty\', 0.0, \'penalty for dropout regularization\')\n\n    tf.app.flags.DEFINE_boolean(\'documents\', False, \'whether each example is a document (default: sentence)\')\n    tf.app.flags.DEFINE_boolean(\'ontonotes\', False, \'evaluate each domain of ontonotes seperately\')\n\n    tf.app.run()\n'"
src/tsv_to_tfrecords.py,21,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re, os\nimport sys\nimport tensorflow as tf\nimport numpy as np\nfrom glob import glob\n\ntf.app.flags.DEFINE_string(\'in_file\', \'naacl-data.tsv\', \'tsv file containing string data\')\ntf.app.flags.DEFINE_string(\'vocab\', \'\', \'file containing vocab (empty means make new vocab)\')\ntf.app.flags.DEFINE_string(\'labels\', \'\', \'file containing labels (but always add new labels)\')\ntf.app.flags.DEFINE_string(\'shapes\', \'\', \'file containing shapes (add new shapes only when adding new vocab)\')\ntf.app.flags.DEFINE_string(\'chars\', \'\', \'file containing characters\')\n\ntf.app.flags.DEFINE_string(\'embeddings\', \'\', \'pretrained embeddings\')\n\ntf.app.flags.DEFINE_string(\'out_dir\', \'\', \'export tf protos\')\n\ntf.app.flags.DEFINE_integer(\'window_size\', 3, \'window size (for computing padding)\')\ntf.app.flags.DEFINE_boolean(\'lowercase\', False, \'whether to lowercase\')\n\ntf.app.flags.DEFINE_boolean(\'debug\', False, \'print debugging output\')\n\ntf.app.flags.DEFINE_boolean(\'predict_pad\', False, \'whether to predict padding labels\')\n\ntf.app.flags.DEFINE_boolean(\'documents\', False, \'whether to grab documents rather than sentences\')\n\ntf.app.flags.DEFINE_boolean(\'update_maps\', False, \'whether to update maps\')\n\ntf.app.flags.DEFINE_string(\'update_vocab\', \'\', \'file to update vocab with tokens from training data\')\n\ntf.app.flags.DEFINE_string(\'dataset\', \'conll\', \'which dataset\')\n\n\nFLAGS = tf.app.flags.FLAGS\n\nZERO_STR = ""<ZERO>""\nPAD_STR = ""<PAD>""\nOOV_STR = ""<OOV>""\nNONE_STR = ""<NONE>""\nSENT_START = ""<S>""\nSENT_END = ""</S>""\n\npad_strs = [PAD_STR, SENT_START, SENT_END, ZERO_STR, NONE_STR]\n\nDOC_MARKER_CONLL = ""-DOCSTART-""\nDOC_MARKER_ONTONOTES = ""#begin document""\nDOC_MARKER = DOC_MARKER_CONLL if FLAGS.dataset == ""conll2003"" else DOC_MARKER_ONTONOTES\n\nlabel_int_str_map = {}\ntoken_int_str_map = {}\nchar_int_str_map = {}\n\npad_width = int(FLAGS.window_size/2)\nonto_genre = [""bn"", ""bc"", ""nw"", ""mz"", ""tc"", ""wb""]\n\ndef _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef shape(string):\n    if all(c.isupper() for c in string):\n        return ""AA""\n    if string[0].isupper():\n        return ""Aa""\n    if any(c for c in string if c.isupper()):\n        return ""aAa""\n    else:\n        return ""a""\n\n\ndef get_str_label_from_line_conll(line):\n    token_str, _, _, label_str = line.strip().split(\' \')\n    return token_str, label_str, \'\'\n\n\ndef get_str_label_from_line_ontonotes(line, current_tag):\n    parts = line.split()\n    try:\n        token_str = parts[3]\n        onto_label_str = parts[10]\n        if onto_label_str.startswith(\'(\'):\n            if onto_label_str.endswith(\')\'):\n                label_str = \'U-%s\' % onto_label_str[1:-1]\n                current_tag = \'\'\n            else:\n                current_tag = onto_label_str[1:-1]\n                label_str = \'B-%s\' % current_tag\n        elif onto_label_str.endswith(\')\'):\n            label_str = \'L-%s\' % current_tag\n            current_tag = \'\'\n        elif current_tag != \'\':\n            label_str = \'I-%s\' % current_tag\n        else:\n            label_str = \'O\'\n        return token_str, label_str, current_tag\n    except Exception as e:\n        print(""Caught exception: %s"" % e.message)\n        print(""Line: %s"" % line)\n        raise\n\n\ndef make_example(writer, lines, label_map, token_map, shape_map, char_map, update_vocab, update_chars):\n    # data format is:\n    # token pos phrase ner\n    # LONDON NNP I-NP I-LOC\n    # 1996-08-30 CD I-NP O\n\n    # West NNP I-NP I-MISC\n    # Indian NNP I-NP I-MISC\n    # all-rounder NN I-NP O\n    # ...\n\n    sent_len = len(lines)\n    num_breaks = sum([1 if line.strip() == """" else 0 for line in lines])\n    max_len_with_pad = pad_width * (num_breaks + 2) + (sent_len - num_breaks)\n    max_word_len = max(map(len, lines))\n\n    oov_count = 0\n    if sent_len == 0:\n        return 0, 0, 0\n\n    tokens = np.zeros(max_len_with_pad, dtype=np.int64)\n    shapes = np.zeros(max_len_with_pad, dtype=np.int64)\n    chars = np.zeros(max_len_with_pad*max_word_len, dtype=np.int64)\n    intmapped_labels = np.zeros(max_len_with_pad, dtype=np.int64)\n    sent_lens = []\n    tok_lens = []\n\n    # initial padding\n    tokens[:pad_width] = token_map[PAD_STR]\n    shapes[:pad_width] = shape_map[PAD_STR]\n    chars[:pad_width] = char_map[PAD_STR]\n    if FLAGS.predict_pad:\n        intmapped_labels[:pad_width] = label_map[PAD_STR]\n    tok_lens.extend([1]*pad_width)\n\n    last_label = ""O""\n    labels = []\n    current_sent_len = 0\n    char_start = pad_width\n    idx = pad_width\n    current_tag = \'\'\n    for i, line in enumerate(lines):\n        line = line.strip()\n        if line:\n            token_str, label_str, current_tag = get_str_label_from_line_conll(line) if FLAGS.dataset == \'conll2003\' else get_str_label_from_line_ontonotes(line, current_tag)\n\n            # skip docstart markers\n            if token_str == DOC_MARKER:\n                return 0, 0, 0\n\n            current_sent_len += 1\n\n            # process tokens to match Collobert embedding preprocessing:\n            # - normalize the digits to 0\n            # - lowercase\n            token_str_digits = re.sub(""\\d"", ""0"", token_str)\n\n            # get capitalization features\n            token_shape = shape(token_str_digits)\n\n            token_str_normalized = token_str_digits.lower() if FLAGS.lowercase else token_str_digits\n\n            if token_shape not in shape_map:\n                shape_map[token_shape] = len(shape_map)\n\n            # Don\'t use normalized token str -- want digits\n            for char in token_str:\n                if char not in char_map and update_chars:\n                    char_map[char] = len(char_map)\n                    char_int_str_map[char_map[char]] = char\n            tok_lens.append(len(token_str))\n\n            # convert label to BILOU encoding\n            label_bilou = label_str\n            # handle cases where we need to update the last token we processed\n            if label_str == ""O"" or label_str[0] == ""B"" or (last_label != ""O"" and label_str[2] != last_label[2]):\n                if last_label[0] == ""I"":\n                    labels[-1] = ""L"" + labels[-1][1:]\n                elif last_label[0] == ""B"":\n                    labels[-1] = ""U"" + labels[-1][1:]\n            if label_str[0] == ""I"":\n                if last_label == ""O"" or label_str[2] != last_label[2]:\n                    label_bilou = ""B-"" + label_str[2:]\n\n            if token_str_normalized not in token_map:\n                oov_count += 1\n                if update_vocab:\n                    token_map[token_str_normalized] = len(token_map)\n                    token_int_str_map[token_map[token_str_normalized]] = token_str_normalized\n\n            tokens[idx] = token_map.get(token_str_normalized, token_map[OOV_STR])\n            shapes[idx] = shape_map[token_shape]\n            chars[char_start:char_start+tok_lens[-1]] = [char_map.get(char, char_map[OOV_STR]) for char in token_str]\n            char_start += tok_lens[-1]\n            labels.append(label_bilou)\n            last_label = label_bilou\n            idx += 1\n        elif current_sent_len > 0:\n            sent_lens.append(current_sent_len)\n            current_sent_len = 0\n            tokens[idx:idx + pad_width] = token_map[PAD_STR]\n            shapes[idx:idx + pad_width] = shape_map[PAD_STR]\n            chars[char_start:char_start + pad_width] = char_map[PAD_STR]\n            char_start += pad_width\n            tok_lens.extend([1] * pad_width)\n            labels.extend([PAD_STR if FLAGS.predict_pad else ""O""] * pad_width)\n            idx += pad_width\n\n            last_label = ""O""\n\n    if last_label[0] == ""I"":\n        labels[-1] = ""L"" + labels[-1][1:]\n    elif last_label[0] == ""B"":\n        labels[-1] = ""U"" + labels[-1][1:]\n\n    if not FLAGS.documents:\n        sent_lens.append(sent_len)\n\n    # final padding\n    if not FLAGS.documents:\n        tokens[idx:idx+pad_width] = token_map[PAD_STR]\n        shapes[idx:idx+pad_width] = shape_map[PAD_STR]\n        chars[char_start:char_start+pad_width] = char_map[PAD_STR]\n        char_start += pad_width\n        tok_lens.extend([1] * pad_width)\n        if FLAGS.predict_pad:\n            intmapped_labels[idx:idx + pad_width] = label_map[PAD_STR]\n\n    for label in labels:\n        if label not in label_map:\n            label_map[label] = len(label_map)\n            label_int_str_map[label_map[label]] = label\n\n    intmapped_labels[pad_width:pad_width+len(labels)] = map(lambda s: label_map[s], labels)\n\n    # chars = chars.flatten()\n\n    # print(sent_lens)\n\n    padded_len = (len(sent_lens)+1)*pad_width+sum(sent_lens)\n    intmapped_labels = intmapped_labels[:padded_len]\n    tokens = tokens[:padded_len]\n    shapes = shapes[:padded_len]\n    chars = chars[:sum(tok_lens)]\n\n    if FLAGS.debug:\n        print(""sent lens: "", sent_lens)\n        print(""tok lens: "", tok_lens, len(tok_lens), sum(tok_lens))\n        print(""labels"", map(lambda t: label_int_str_map[t], intmapped_labels), len(intmapped_labels))\n        print(""tokens"", map(lambda t: token_int_str_map[t], tokens), len(tokens))\n        print(""chars"", map(lambda t: char_int_str_map[t], chars), len(chars))\n\n    example = tf.train.SequenceExample()\n\n    fl_labels = example.feature_lists.feature_list[""labels""]\n    for l in intmapped_labels:\n        fl_labels.feature.add().int64_list.value.append(l)\n\n    fl_tokens = example.feature_lists.feature_list[""tokens""]\n    for t in tokens:\n        fl_tokens.feature.add().int64_list.value.append(t)\n\n    fl_shapes = example.feature_lists.feature_list[""shapes""]\n    for s in shapes:\n        fl_shapes.feature.add().int64_list.value.append(s)\n\n    fl_chars = example.feature_lists.feature_list[""chars""]\n    for c in chars:\n        fl_chars.feature.add().int64_list.value.append(c)\n\n    fl_seq_len = example.feature_lists.feature_list[""seq_len""]\n    for seq_len in sent_lens:\n        fl_seq_len.feature.add().int64_list.value.append(seq_len)\n\n    fl_tok_len = example.feature_lists.feature_list[""tok_len""]\n    for tok_len in tok_lens:\n        fl_tok_len.feature.add().int64_list.value.append(tok_len)\n\n    writer.write(example.SerializeToString())\n    return sum(sent_lens), oov_count, 1\n\n\ndef tsv_to_examples():\n    label_map = {}\n    token_map = {}\n    shape_map = {}\n    char_map = {}\n\n    update_vocab = True\n    update_chars = True\n\n    token_map[PAD_STR] = len(token_map)\n    token_int_str_map[token_map[PAD_STR]] = PAD_STR\n    char_map[PAD_STR] = len(char_map)\n    char_int_str_map[char_map[PAD_STR]] = PAD_STR\n    shape_map[PAD_STR] = len(shape_map)\n    if FLAGS.predict_pad:\n        label_map[PAD_STR] = len(label_map)\n        label_int_str_map[label_map[PAD_STR]] = PAD_STR\n\n    token_map[OOV_STR] = len(token_map)\n    token_int_str_map[token_map[OOV_STR]] = OOV_STR\n    char_map[OOV_STR] = len(char_map)\n    char_int_str_map[char_map[OOV_STR]] = OOV_STR\n\n    # load vocab if we have one\n    if FLAGS.vocab != \'\':\n        update_vocab = False\n        with open(FLAGS.vocab, \'r\') as f:\n            for line in f.readlines():\n                word = line.strip().split("" "")[0]\n                if word not in token_map:\n                    # print(""adding word %s"" % word)\n                    token_map[word] = len(token_map)\n                    token_int_str_map[token_map[word]] = word\n    if FLAGS.update_vocab != \'\':\n        with open(FLAGS.update_vocab, \'r\') as f:\n            for line in f.readlines():\n                word = line.strip().split("" "")[0]\n                if word not in token_map:\n                    # print(""adding word %s"" % word)\n                    token_map[word] = len(token_map)\n                    token_int_str_map[token_map[word]] = word\n\n    # load labels if given\n    if FLAGS.labels != \'\':\n        with open(FLAGS.labels, \'r\') as f:\n            for line in f.readlines():\n                label, idx = line.strip().split(""\\t"")\n                label_map[label] = int(idx)\n                label_int_str_map[label_map[label]] = label\n\n    # load shapes if given\n    if FLAGS.shapes != \'\':\n        with open(FLAGS.shapes, \'r\') as f:\n            for line in f.readlines():\n                shape, idx = line.strip().split(""\\t"")\n                shape_map[shape] = int(idx)\n\n    # load chars if given\n    if FLAGS.chars != \'\':\n        update_chars = FLAGS.update_maps\n        with open(FLAGS.chars, \'r\') as f:\n            for line in f.readlines():\n                char, idx = line.strip().split(""\\t"")\n                char_map[char] = int(idx)\n                char_int_str_map[char_map[char]] = char\n\n    num_tokens = 0\n    num_sentences = 0\n    num_oov = 0\n    num_docs = 0\n\n    # TODO refactor this!!!\n    if FLAGS.dataset == ""conll2003"":\n        if not os.path.exists(FLAGS.out_dir):\n            print(""Output directory not found: %s"" % FLAGS.out_dir)\n        writer = tf.python_io.TFRecordWriter(FLAGS.out_dir + \'/examples.proto\')\n        with open(FLAGS.in_file) as f:\n            line_buf = []\n            line = f.readline()\n            line_idx = 1\n            while line:\n                line = line.strip()\n\n                if FLAGS.documents:\n                    if line.split("" "")[0] == DOC_MARKER:\n                        if line_buf:\n                            # reached the end of a document; process the lines\n                            toks, oov, sent = make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n                            num_tokens += toks\n                            num_oov += oov\n                            num_sentences += sent\n                            num_docs += 1\n                            line_buf = []\n                    else:\n                        # print(line)\n                        line_buf.append(line)\n                        line_idx += 1\n\n                else:\n                    # if the line is not empty, add it to the buffer\n                    if line:\n                        line_buf.append(line)\n                        line_idx += 1\n                    # otherwise, if there\'s stuff in the buffer, process it\n                    elif line_buf:\n                        # reached the end of a sentence; process the line\n                        toks, oov, sent = make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n                        num_tokens += toks\n                        num_oov += oov\n                        num_sentences += sent\n                        line_buf = []\n                # print(""reading line %d"" % line_idx)\n                line = f.readline()\n            if line_buf:\n                make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n        writer.close()\n\n        # export the string->int maps to file\n        for f_str, id_map in [(\'label\', label_map), (\'token\', token_map), (\'shape\', shape_map), (\'char\', char_map)]:\n            with open(FLAGS.out_dir + \'/\' + f_str + \'.txt\', \'w\') as f:\n                [f.write(s + \'\\t\' + str(i) + \'\\n\') for (s, i) in id_map.items()]\n\n        # export data sizes to file\n        with open(FLAGS.out_dir + ""/sizes.txt"", \'w\') as f:\n            print(num_sentences, file=f)\n            print(num_tokens, file=f)\n            print(num_docs, file=f)\n\n    else:\n        if not os.path.exists(FLAGS.out_dir):\n            print(""Output directory not found: %s"" % FLAGS.out_dir)\n        if not os.path.exists(FLAGS.out_dir + ""/protos""):\n            os.mkdir(FLAGS.out_dir + ""/protos"")             \n        for data_type in onto_genre:\n            num_tokens = 0\n            num_sentences = 0\n            num_oov = 0\n            num_docs = 0\n            writer = tf.python_io.TFRecordWriter(\'%s/protos/%s_examples.proto\' % (FLAGS.out_dir, data_type))\n            file_list = [y for x in os.walk(FLAGS.in_file) for y in glob(os.path.join(x[0], \'*_gold_conll\'))\\\n                         if ""/""+data_type+""/"" in y and ""/english/"" in y]\n            \n            for f_path in file_list:\n\n                with open(f_path) as f:\n                    line_buf = []\n                    line = f.readline()\n                    line_idx = 1\n                    while line:\n                        line = line.strip()\n    \n                        if FLAGS.documents:\n                            if line.startswith(""#""):\n                                if line_buf:\n                                    # reached the end of a document; process the lines\n                                    toks, oov, sent = make_example(writer, line_buf, label_map, token_map, shape_map,\n                                                                   char_map, update_vocab, update_chars)\n                                    num_tokens += toks\n                                    num_oov += oov\n                                    num_sentences += sent\n                                    num_docs += 1\n                                    line_buf = []\n                            else:\n                                # print(line)\n                                if line_buf or (not line_buf and line):\n                                    line_buf.append(line)\n                                    line_idx += 1\n                        else:\n                            # if the line is not empty, add it to the buffer\n                            if line:\n                                if not line.startswith(""#""):\n                                    line_buf.append(line)\n                                    line_idx += 1\n                            # otherwise, if there\'s stuff in the buffer, process it\n                            elif line_buf:\n                                # reached the end of a sentence; process the line\n                                toks, oov, sent = make_example(writer, line_buf, label_map, token_map, shape_map, char_map,\n                                                               update_vocab, update_chars)\n                                num_tokens += toks\n                                num_oov += oov\n                                num_sentences += sent\n                                line_buf = []\n                        # print(""reading line %d"" % line_idx)\n                        line = f.readline()\n                    if line_buf:\n                        make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab,\n                                     update_chars)\n                    # print(""Processed %d sentences"" % num_sentences)\n            writer.close()\n\n            # export the string->int maps to file\n            for f_str, id_map in [(\'label\', label_map), (\'token\', token_map), (\'shape\', shape_map), (\'char\', char_map)]:\n                with open(""%s/%s.txt"" % (FLAGS.out_dir, f_str), \'w\') as f:\n                    [f.write(s + \'\\t\' + str(i) + \'\\n\') for (s, i) in id_map.items()]\n    \n            # export data sizes to file\n            with open(""%s/%s_sizes.txt"" % (FLAGS.out_dir, data_type), \'w\') as f:\n                print(num_sentences, file=f)\n                print(num_tokens, file=f)\n                print(num_docs, file=f)\n            \n    print(""Embeddings coverage: %2.2f%%"" % ((1-(num_oov/num_tokens)) * 100))\n\n\ndef main(argv):\n    if FLAGS.out_dir == \'\':\n        print(\'Must supply out_dir\')\n        sys.exit(1)\n    tsv_to_examples()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
src/tsv_to_tfrecords_ontonotes.py,21,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport sys\nimport tensorflow as tf\nimport numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\n\ntf.app.flags.DEFINE_string(\'in_file\', \'naacl-data.tsv\', \'tsv file containing string data\')\ntf.app.flags.DEFINE_string(\'vocab\', \'\', \'file containing vocab (empty means make new vocab)\')\ntf.app.flags.DEFINE_string(\'labels\', \'\', \'file containing labels (but always add new labels)\')\ntf.app.flags.DEFINE_string(\'shapes\', \'\', \'file containing shapes (add new shapes only when adding new vocab)\')\ntf.app.flags.DEFINE_string(\'chars\', \'\', \'file containing characters\')\n\ntf.app.flags.DEFINE_string(\'embeddings\', \'\', \'pretrained embeddings\')\n\ntf.app.flags.DEFINE_string(\'out_dir\', \'\', \'export tf protos\')\n\ntf.app.flags.DEFINE_integer(\'window_size\', 3, \'window size (for computing padding)\')\ntf.app.flags.DEFINE_boolean(\'lowercase\', False, \'whether to lowercase\')\n\ntf.app.flags.DEFINE_boolean(\'start_end\', False, \'whether to use distinct start/end padding\')\ntf.app.flags.DEFINE_boolean(\'debug\', False, \'print debugging output\')\n\ntf.app.flags.DEFINE_boolean(\'predict_pad\', False, \'whether to predict padding labels\')\n\ntf.app.flags.DEFINE_boolean(\'documents\', False, \'whether to grab documents rather than sentences\')\n\ntf.app.flags.DEFINE_boolean(\'update_maps\', False, \'whether to update maps\')\n\ntf.app.flags.DEFINE_string(\'update_vocab\', \'\', \'file to update vocab with tokens from training data\')\n\nFLAGS = tf.app.flags.FLAGS\n\nZERO_STR = ""<ZERO>""\nPAD_STR = ""<PAD>""\nOOV_STR = ""<OOV>""\nNONE_STR = ""<NONE>""\nSENT_START = ""<S>""\nSENT_END = ""</S>""\n\npad_strs = [PAD_STR, SENT_START, SENT_END, ZERO_STR, NONE_STR]\n\nDOC_MARKER = ""#begin document""\n\n# indices of characters to grab: first and last 4\n# char_indices = [0, 1, 2, 3, -1, -2, -3, -4]\n\nlabel_int_str_map = {}\ntoken_int_str_map = {}\nchar_int_str_map = {}\n\npad_width = int(FLAGS.window_size/2)\n\n\ndef _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef shape(string):\n    if all(c.isupper() for c in string):\n        return ""AA""\n    if string[0].isupper():\n        return ""Aa""\n    if any(c for c in string if c.isupper()):\n        return ""aAa""\n    else:\n        return ""a""\n\n\ndef make_example(writer, lines, label_map, token_map, shape_map, char_map, update_vocab, update_chars):\n    # data format is:\n    # token pos phrase ner\n    # LONDON NNP I-NP I-LOC\n    # 1996-08-30 CD I-NP O\n\n    # West NNP I-NP I-MISC\n    # Indian NNP I-NP I-MISC\n    # all-rounder NN I-NP O\n    # ...\n\n    sent_len = len(lines)\n    num_breaks = sum([1 if line.strip() == """" else 0 for line in lines])\n    max_len_with_pad = pad_width * (num_breaks + (1 if FLAGS.start_end else 2)) * (2 if FLAGS.start_end else 1) + (sent_len - num_breaks)\n    max_word_len = max(map(len, lines))\n    # print(""Processing %s w/ %d lines %d breaks; max len w/ pad: %d"" % (""doc"" if FLAGS.documents else ""sent"", sent_len, num_breaks, max_len_with_pad))\n\n    # max_len_with_pad = FLAGS.max_len if FLAGS.documents else FLAGS.max_len + (FLAGS.window_size - 1) # assumes odd window size\n\n\n    oov_count = 0\n    if sent_len == 0:\n        return 0, 0, 0\n\n    tokens = np.zeros(max_len_with_pad, dtype=np.int64)\n    shapes = np.zeros(max_len_with_pad, dtype=np.int64)\n    chars = np.zeros(max_len_with_pad*max_word_len, dtype=np.int64)\n    intmapped_labels = np.zeros(max_len_with_pad, dtype=np.int64)\n    sent_lens = []\n    tok_lens = []\n\n    # initial padding\n    if FLAGS.start_end:\n        tokens[:pad_width] = token_map[SENT_START]\n        shapes[:pad_width] = shape_map[SENT_START]\n        chars[:pad_width] = char_map[SENT_START]\n        if FLAGS.predict_pad:\n            intmapped_labels[:pad_width] = label_map[SENT_START]\n    else:\n        tokens[:pad_width] = token_map[PAD_STR]\n        shapes[:pad_width] = shape_map[PAD_STR]\n        chars[:pad_width] = char_map[PAD_STR]\n        if FLAGS.predict_pad:\n            intmapped_labels[:pad_width] = label_map[PAD_STR]\n    tok_lens.extend([1]*pad_width)\n\n    last_label = ""O""\n    labels = []\n    current_sent_len = 0\n    char_start = pad_width\n    idx = pad_width\n    current_tag = \'\'\n    for i, line in enumerate(lines):\n        if line:\n            parts = line.strip().split()\n            if len(parts) >= 10:\n                token_str = parts[3]\n                stupid_label_str = parts[10]\n                if stupid_label_str.startswith(\'(\'):\n                    if stupid_label_str.endswith(\')\'):\n                        label_str = \'U-%s\' % stupid_label_str[1:-1]\n                        current_tag = \'\'\n                    else:\n                        current_tag = stupid_label_str[1:-1]\n                        label_str = \'B-%s\' % current_tag\n                elif stupid_label_str.endswith(\')\'):\n                    label_str = \'L-%s\' % current_tag\n                    current_tag = \'\'\n                elif current_tag != \'\':\n                    label_str = \'I-%s\' % current_tag\n                else:\n                    label_str = \'O\'\n\n                # skip docstart markers\n                if token_str == DOC_MARKER:\n                    return 0, 0, 0\n\n                current_sent_len += 1\n\n                # process tokens to match Collobert embedding preprocessing:\n                # - normalize the digits to 0\n                # - lowercase\n                token_str_digits = re.sub(""\\d"", ""0"", token_str)\n\n                # get capitalization features\n                token_shape = shape(token_str_digits)\n\n                # print(token_str_digits, token_shape)\n\n                token_str_normalized = token_str_digits.lower() if FLAGS.lowercase else token_str_digits\n\n                if token_shape not in shape_map:\n                    shape_map[token_shape] = len(shape_map)\n\n                # Don\'t use normalized token str -- want digits\n                for char in token_str:\n                    if char not in char_map and update_chars:\n                        char_map[char] = len(char_map)\n                        char_int_str_map[char_map[char]] = char\n                tok_lens.append(len(token_str))\n\n                # convert label to BILOU encoding\n                label_bilou = label_str\n                # handle cases where we need to update the last token we processed\n                if label_str == ""O"" or label_str[0] == ""B"" or (last_label != ""O"" and label_str[2] != last_label[2]):\n                    if last_label[0] == ""I"":\n                        labels[-1] = ""L"" + labels[-1][1:]\n                    elif last_label[0] == ""B"":\n                        labels[-1] = ""U"" + labels[-1][1:]\n                if label_str[0] == ""I"":\n                    if last_label == ""O"" or label_str[2] != last_label[2]:\n                        label_bilou = ""B-"" + label_str[2:]\n\n                if token_str_normalized not in token_map:\n                    oov_count += 1\n                    if update_vocab:\n                        token_map[token_str_normalized] = len(token_map)\n                        token_int_str_map[token_map[token_str_normalized]] = token_str_normalized\n\n                tokens[idx] = token_map.get(token_str_normalized, token_map[OOV_STR])\n                shapes[idx] = shape_map[token_shape] # if update_vocab else shape_map.get(token_shape, shape_map[token_shape[0]])\n                chars[char_start:char_start+tok_lens[-1]] = [char_map.get(char, char_map[OOV_STR]) for char in token_str]\n                char_start += tok_lens[-1]\n                labels.append(label_bilou)\n                last_label = label_bilou\n                # tmp.append(label_str)\n                # toks_tmp.append(token_str_normalized)\n                idx += 1\n            # doc info line\n            else:\n                return 0, 0, 0\n        elif current_sent_len > 0:\n            sent_lens.append(current_sent_len)\n            current_sent_len = 0\n            tokens[idx:idx + pad_width] = token_map[PAD_STR]\n            shapes[idx:idx + pad_width] = shape_map[PAD_STR]\n            chars[char_start:char_start + pad_width] = char_map[PAD_STR]\n            char_start += pad_width\n            tok_lens.extend([1] * pad_width)\n            labels.extend([PAD_STR if FLAGS.predict_pad else ""O""] * pad_width)\n            idx += pad_width\n\n            last_label = ""O""\n\n    if last_label[0] == ""I"":\n        labels[-1] = ""L"" + labels[-1][1:]\n    elif last_label[0] == ""B"":\n        labels[-1] = ""U"" + labels[-1][1:]\n\n    if not FLAGS.documents:\n        sent_lens.append(sent_len)\n\n    # final padding\n    if not FLAGS.documents and FLAGS.start_end:\n        tokens[idx:idx+pad_width] = token_map[SENT_END]\n        shapes[idx:idx+pad_width] = shape_map[SENT_END]\n        chars[char_start:char_start+pad_width] = char_map[SENT_END]\n        char_start += pad_width\n        tok_lens.extend([1] * pad_width)\n        if FLAGS.predict_pad:\n            intmapped_labels[idx:idx+pad_width] = label_map[SENT_END]\n    elif not FLAGS.documents:\n        tokens[idx:idx+pad_width] = token_map[PAD_STR]\n        shapes[idx:idx+pad_width] = shape_map[PAD_STR]\n        chars[char_start:char_start+pad_width] = char_map[PAD_STR]\n        char_start += pad_width\n        tok_lens.extend([1] * pad_width)\n        if FLAGS.predict_pad:\n            intmapped_labels[idx:idx + pad_width] = label_map[PAD_STR]\n\n    for label in labels:\n        if label not in label_map:\n            label_map[label] = len(label_map)\n            label_int_str_map[label_map[label]] = label\n\n    intmapped_labels[pad_width:pad_width+len(labels)] = map(lambda s: label_map[s], labels)\n\n    # chars = chars.flatten()\n\n    # print(sent_lens)\n\n    padded_len = (2 if FLAGS.start_end else 1)*(len(sent_lens)+(0 if FLAGS.start_end else 1))*pad_width+sum(sent_lens)\n    intmapped_labels = intmapped_labels[:padded_len]\n    tokens = tokens[:padded_len]\n    shapes = shapes[:padded_len]\n    chars = chars[:sum(tok_lens)]\n\n    if FLAGS.debug:\n        print(""sent lens: "", sent_lens)\n        print(""labels"", map(lambda t: label_int_str_map[t], intmapped_labels))\n        print(""tokens"", map(lambda t: token_int_str_map[t], tokens))\n        # print(""chars"", map(lambda t: char_int_str_map[t], chars))\n\n    example = tf.train.SequenceExample()\n\n    fl_labels = example.feature_lists.feature_list[""labels""]\n    for l in intmapped_labels:\n        fl_labels.feature.add().int64_list.value.append(l)\n\n    fl_tokens = example.feature_lists.feature_list[""tokens""]\n    for t in tokens:\n        fl_tokens.feature.add().int64_list.value.append(t)\n\n    fl_shapes = example.feature_lists.feature_list[""shapes""]\n    for s in shapes:\n        fl_shapes.feature.add().int64_list.value.append(s)\n\n    fl_chars = example.feature_lists.feature_list[""chars""]\n    for c in chars:\n        fl_chars.feature.add().int64_list.value.append(c)\n\n    fl_seq_len = example.feature_lists.feature_list[""seq_len""]\n    for seq_len in sent_lens:\n        fl_seq_len.feature.add().int64_list.value.append(seq_len)\n\n    fl_tok_len = example.feature_lists.feature_list[""tok_len""]\n    for tok_len in tok_lens:\n        fl_tok_len.feature.add().int64_list.value.append(tok_len)\n\n    # example = tf.train.Example(features=tf.train.Features(feature={\n    #     \'labels\': _int64_feature(intmapped_labels),\n    #     \'tokens\': _int64_feature(tokens),\n    #     \'shapes\': _int64_feature(shapes),\n    #     \'chars\': _int64_feature(chars),\n    #     \'seq_len\': _int64_feature(sent_lens if FLAGS.documents else [sent_len])\n    # }))\n\n    writer.write(example.SerializeToString())\n\n    # if len(tok_lens) != len(tokens):\n    #     print(\'%d %d\' % (len(tok_lens), len(tokens)))\n    #     print(\' \'.join(tok_strs))\n    #     print(tokens)\n    return sum(sent_lens), oov_count, 1\n\n\ndef tsv_to_examples():\n    # label_map = {ZERO_STR: 0}\n    # label_int_str_map[0] = ZERO_STR\n    # # embedding_map = {ZERO_STR: 0, PAD_STR: 1, OOV_STR: 2}\n    # token_map = {ZERO_STR: 0, OOV_STR: 1}\n    # token_int_str_map[0] = ZERO_STR\n    # token_int_str_map[1] = OOV_STR\n    # shape_map = {ZERO_STR: 0}\n    # char_map = {ZERO_STR: 0, NONE_STR: 1}\n    label_map = {}\n    token_map = {}\n    shape_map = {}\n    char_map = {}\n\n    update_vocab = True\n    update_chars = True\n\n    if FLAGS.start_end:\n        token_map[SENT_START] = len(token_map)\n        token_int_str_map[token_map[SENT_START]] = SENT_START\n        shape_map[SENT_START] = len(shape_map)\n        char_map[SENT_START] = len(char_map)\n        char_int_str_map[char_map[SENT_START]] = SENT_START\n        if FLAGS.predict_pad:\n            label_map[SENT_START] = len(label_map)\n            label_int_str_map[label_map[SENT_START]] = SENT_START\n        token_map[SENT_END] = len(token_map)\n        token_int_str_map[token_map[SENT_END]] = SENT_END\n        shape_map[SENT_END] = len(shape_map)\n        char_map[SENT_END] = len(char_map)\n        char_int_str_map[char_map[SENT_END]] = SENT_END\n        if FLAGS.predict_pad:\n            label_map[SENT_END] = len(label_map)\n            label_int_str_map[label_map[SENT_END]] = SENT_END\n\n    else:\n        token_map[PAD_STR] = len(token_map)\n        token_int_str_map[token_map[PAD_STR]] = PAD_STR\n        char_map[PAD_STR] = len(char_map)\n        char_int_str_map[char_map[PAD_STR]] = PAD_STR\n        shape_map[PAD_STR] = len(shape_map)\n        if FLAGS.predict_pad:\n            label_map[PAD_STR] = len(label_map)\n            label_int_str_map[label_map[PAD_STR]] = PAD_STR\n\n    token_map[OOV_STR] = len(token_map)\n    token_int_str_map[token_map[OOV_STR]] = OOV_STR\n    char_map[OOV_STR] = len(char_map)\n    char_int_str_map[char_map[OOV_STR]] = OOV_STR\n\n    # load embeddings if we have them\n    # if FLAGS.embeddings != \'\':\n    #     with open(FLAGS.embeddings, \'r\') as f:\n    #         for line in f.readlines():\n    #             # word, idx = line.strip().split(""\\t"")\n    #             # token_map[word] = int(idx)\n    #             word = line.strip().split("" "")[0]\n    #             if word not in token_map:\n    #                 # print(""adding word %s"" % word)\n    #                 embedding_map[word] = len(embedding_map)\n\n\n    # load vocab if we have one\n    if FLAGS.vocab != \'\':\n        update_vocab = False\n        with open(FLAGS.vocab, \'r\') as f:\n            for line in f.readlines():\n                word = line.strip().split("" "")[0]\n                # token_map[word] = int(idx)\n                # word = line.strip().split(""\\t"")[0]\n                if word not in token_map:\n                    # print(""adding word %s"" % word)\n                    token_map[word] = len(token_map)\n                    token_int_str_map[token_map[word]] = word\n    if FLAGS.update_vocab != \'\':\n        with open(FLAGS.update_vocab, \'r\') as f:\n            for line in f.readlines():\n                word = line.strip().split("" "")[0]\n                if word not in token_map:\n                    # print(""adding word %s"" % word)\n                    token_map[word] = len(token_map)\n                    token_int_str_map[token_map[word]] = word\n\n    # load labels if given\n    if FLAGS.labels != \'\':\n        with open(FLAGS.labels, \'r\') as f:\n            for line in f.readlines():\n                label, idx = line.strip().split(""\\t"")\n                label_map[label] = int(idx)\n                label_int_str_map[label_map[label]] = label\n\n    # load shapes if given\n    if FLAGS.shapes != \'\':\n        with open(FLAGS.shapes, \'r\') as f:\n            for line in f.readlines():\n                shape, idx = line.strip().split(""\\t"")\n                shape_map[shape] = int(idx)\n\n    # load chars if given\n    if FLAGS.chars != \'\':\n        update_chars = FLAGS.update_maps\n        with open(FLAGS.chars, \'r\') as f:\n            for line in f.readlines():\n                char, idx = line.strip().split(""\\t"")\n                char_map[char] = int(idx)\n                char_int_str_map[char_map[char]] = char\n\n    num_tokens = 0\n    num_sentences = 0\n    num_oov = 0\n    num_docs = 0\n    if not os.path.exists(FLAGS.out_dir + ""/protos""):\n        print(""Output directory not found: %s"" % FLAGS.out_dir)\n    data_type = FLAGS.in_file.strip().split(""/"")[-1]\n    writer = tf.python_io.TFRecordWriter(\'%s/protos/%s_examples.proto\' % (FLAGS.out_dir, data_type))\n    for fname in listdir(FLAGS.in_file):\n        with open(FLAGS.in_file + ""/"" + fname) as f:\n            # print(FLAGS.in_file + ""/"" + fname)\n            line_buf = []\n            line = f.readline()\n            line_idx = 1\n            while line:\n                line = line.strip()\n\n                if FLAGS.documents:\n                    if line.startswith(""#""):\n                        if line_buf:\n                            # reached the end of a document; process the lines\n                            toks, oov, sent = make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n                            num_tokens += toks\n                            num_oov += oov\n                            num_sentences += sent\n                            num_docs += 1\n                            line_buf = []\n                    else:\n                        # print(line)\n                        if line_buf or (not line_buf and line):\n                            line_buf.append(line)\n                            line_idx += 1\n\n\n                else:\n                    # if the line is not empty, add it to the buffer\n                    if line:\n                        if not line.startswith(""#""):\n                            line_buf.append(line)\n                            line_idx += 1\n                    # otherwise, if there\'s stuff in the buffer, process it\n                    elif line_buf:\n                        # reached the end of a sentence; process the line\n                        toks, oov, sent = make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n                        num_tokens += toks\n                        num_oov += oov\n                        num_sentences += sent\n                        line_buf = []\n                # print(""reading line %d"" % line_idx)\n                line = f.readline()\n            if line_buf:\n                make_example(writer, line_buf, label_map, token_map, shape_map, char_map, update_vocab, update_chars)\n        # print(""Processed %d sentences"" % num_sentences)\n    writer.close()\n\n    print(""Embeddings coverage: %2.2f%%"" % ((1-(num_oov/num_tokens)) * 100))\n\n    # remove padding from label domain\n    # filtered_label_map = label_map if FLAGS.predict_pad else \\\n    #     {label_str: label_map[label_str] for label_str in label_map.keys() if label_str not in pad_strs}\n\n    # export the string->int maps to file\n    for f_str, id_map in [(\'label\', label_map), (\'token\', token_map), (\'shape\', shape_map), (\'char\', char_map)]:\n        with open(""%s/%s.txt"" % (FLAGS.out_dir, f_str), \'w\') as f:\n            [f.write(s + \'\\t\' + str(i) + \'\\n\') for (s, i) in id_map.items()]\n\n    # export data sizes to file\n    with open(""%s/%s_sizes.txt"" % (FLAGS.out_dir, data_type), \'w\') as f:\n        print(num_sentences, file=f)\n        print(num_tokens, file=f)\n        print(num_docs, file=f)\n\n\ndef main(argv):\n    if FLAGS.out_dir == \'\':\n        print(\'Must supply out_dir\')\n        sys.exit(1)\n    tsv_to_examples()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
