file_path,api_count,code
run_dense_net.py,0,"b'import argparse\n\nfrom models.dense_net import DenseNet\nfrom data_providers.utils import get_data_provider_by_name\n\ntrain_params_cifar = {\n    \'batch_size\': 64,\n    \'n_epochs\': 300,\n    \'initial_learning_rate\': 0.1,\n    \'reduce_lr_epoch_1\': 150,  # epochs * 0.5\n    \'reduce_lr_epoch_2\': 225,  # epochs * 0.75\n    \'validation_set\': True,\n    \'validation_split\': None,  # None or float\n    \'shuffle\': \'every_epoch\',  # None, once_prior_train, every_epoch\n    \'normalization\': \'by_chanels\',  # None, divide_256, divide_255, by_chanels\n}\n\ntrain_params_svhn = {\n    \'batch_size\': 64,\n    \'n_epochs\': 40,\n    \'initial_learning_rate\': 0.1,\n    \'reduce_lr_epoch_1\': 20,\n    \'reduce_lr_epoch_2\': 30,\n    \'validation_set\': True,\n    \'validation_split\': None,  # you may set it 6000 as in the paper\n    \'shuffle\': True,  # shuffle dataset every epoch or not\n    \'normalization\': \'divide_255\',\n}\n\n\ndef get_train_params_by_name(name):\n    if name in [\'C10\', \'C10+\', \'C100\', \'C100+\']:\n        return train_params_cifar\n    if name == \'SVHN\':\n        return train_params_svhn\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--train\', action=\'store_true\',\n        help=\'Train the model\')\n    parser.add_argument(\n        \'--test\', action=\'store_true\',\n        help=\'Test model for required dataset if pretrained model exists.\'\n             \'If provided together with `--train` flag testing will be\'\n             \'performed right after training.\')\n    parser.add_argument(\n        \'--model_type\', \'-m\', type=str, choices=[\'DenseNet\', \'DenseNet-BC\'],\n        default=\'DenseNet\',\n        help=\'What type of model to use\')\n    parser.add_argument(\n        \'--growth_rate\', \'-k\', type=int, choices=[12, 24, 40],\n        default=12,\n        help=\'Grows rate for every layer, \'\n             \'choices were restricted to used in paper\')\n    parser.add_argument(\n        \'--depth\', \'-d\', type=int, choices=[40, 100, 190, 250],\n        default=40,\n        help=\'Depth of whole network, restricted to paper choices\')\n    parser.add_argument(\n        \'--dataset\', \'-ds\', type=str,\n        choices=[\'C10\', \'C10+\', \'C100\', \'C100+\', \'SVHN\'],\n        default=\'C10\',\n        help=\'What dataset should be used\')\n    parser.add_argument(\n        \'--total_blocks\', \'-tb\', type=int, default=3, metavar=\'\',\n        help=\'Total blocks of layers stack (default: %(default)s)\')\n    parser.add_argument(\n        \'--keep_prob\', \'-kp\', type=float, metavar=\'\',\n        help=""Keep probability for dropout."")\n    parser.add_argument(\n        \'--weight_decay\', \'-wd\', type=float, default=1e-4, metavar=\'\',\n        help=\'Weight decay for optimizer (default: %(default)s)\')\n    parser.add_argument(\n        \'--nesterov_momentum\', \'-nm\', type=float, default=0.9, metavar=\'\',\n        help=\'Nesterov momentum (default: %(default)s)\')\n    parser.add_argument(\n        \'--reduction\', \'-red\', type=float, default=0.5, metavar=\'\',\n        help=\'reduction Theta at transition layer for DenseNets-BC models\')\n\n    parser.add_argument(\n        \'--logs\', dest=\'should_save_logs\', action=\'store_true\',\n        help=\'Write tensorflow logs\')\n    parser.add_argument(\n        \'--no-logs\', dest=\'should_save_logs\', action=\'store_false\',\n        help=\'Do not write tensorflow logs\')\n    parser.set_defaults(should_save_logs=True)\n\n    parser.add_argument(\n        \'--saves\', dest=\'should_save_model\', action=\'store_true\',\n        help=\'Save model during training\')\n    parser.add_argument(\n        \'--no-saves\', dest=\'should_save_model\', action=\'store_false\',\n        help=\'Do not save model during training\')\n    parser.set_defaults(should_save_model=True)\n\n    parser.add_argument(\n        \'--renew-logs\', dest=\'renew_logs\', action=\'store_true\',\n        help=\'Erase previous logs for model if exists.\')\n    parser.add_argument(\n        \'--not-renew-logs\', dest=\'renew_logs\', action=\'store_false\',\n        help=\'Do not erase previous logs for model if exists.\')\n    \n    parser.add_argument(\n        \'--num_inter_threads\', \'-inter\', type=int, default=1, metavar=\'\',\n        help=\'number of inter threads for inference / test\')\n    parser.add_argument(\n        \'--num_intra_threads\', \'-intra\', type=int, default=128, metavar=\'\',\n        help=\'number of intra threads for inference / test\')\n    \n    \n    parser.set_defaults(renew_logs=True)\n\n    args = parser.parse_args()\n\n    if not args.keep_prob:\n        if args.dataset in [\'C10\', \'C100\', \'SVHN\']:\n            args.keep_prob = 0.8\n        else:\n            args.keep_prob = 1.0\n    if args.model_type == \'DenseNet\':\n        args.bc_mode = False\n        args.reduction = 1.0\n    elif args.model_type == \'DenseNet-BC\':\n        args.bc_mode = True\n\n    model_params = vars(args)\n\n    if not args.train and not args.test:\n        print(""You should train or test your network. Please check params."")\n        exit()\n\n    # some default params dataset/architecture related\n    train_params = get_train_params_by_name(args.dataset)\n    print(""Params:"")\n    for k, v in model_params.items():\n        print(""\\t%s: %s"" % (k, v))\n    print(""Train params:"")\n    for k, v in train_params.items():\n        print(""\\t%s: %s"" % (k, v))\n\n    print(""Prepare training data..."")\n    data_provider = get_data_provider_by_name(args.dataset, train_params)\n    print(""Initialize the model.."")\n    model = DenseNet(data_provider=data_provider, **model_params)\n    if args.train:\n        print(""Data provider train images: "", data_provider.train.num_examples)\n        model.train_all_epochs(train_params)\n    if args.test:\n        if not args.train:\n            model.load_model()\n        print(""Data provider test images: "", data_provider.test.num_examples)\n        print(""Testing..."")\n        loss, accuracy = model.test(data_provider.test, batch_size=200)\n        print(""mean cross_entropy: %f, mean accuracy: %f"" % (loss, accuracy))\n'"
data_providers/__init__.py,0,b''
data_providers/base_provider.py,0,"b'import numpy as np\n\n\nclass DataSet:\n    """"""Class to represent some dataset: train, validation, test""""""\n    @property\n    def num_examples(self):\n        """"""Return qtty of examples in dataset""""""\n        raise NotImplementedError\n\n    def next_batch(self, batch_size):\n        """"""Return batch of required size of data, labels""""""\n        raise NotImplementedError\n\n\nclass ImagesDataSet(DataSet):\n    """"""Dataset for images that provide some often used methods""""""\n\n    def _measure_mean_and_std(self):\n        # for every channel in image\n        means = []\n        stds = []\n        # for every channel in image(assume this is last dimension)\n        for ch in range(self.images.shape[-1]):\n            means.append(np.mean(self.images[:, :, :, ch]))\n            stds.append(np.std(self.images[:, :, :, ch]))\n        self._means = means\n        self._stds = stds\n\n    @property\n    def images_means(self):\n        if not hasattr(self, \'_means\'):\n            self._measure_mean_and_std()\n        return self._means\n\n    @property\n    def images_stds(self):\n        if not hasattr(self, \'_stds\'):\n            self._measure_mean_and_std()\n        return self._stds\n\n    def shuffle_images_and_labels(self, images, labels):\n        rand_indexes = np.random.permutation(images.shape[0])\n        shuffled_images = images[rand_indexes]\n        shuffled_labels = labels[rand_indexes]\n        return shuffled_images, shuffled_labels\n\n    def normalize_images(self, images, normalization_type):\n        """"""\n        Args:\n            images: numpy 4D array\n            normalization_type: `str`, available choices:\n                - divide_255\n                - divide_256\n                - by_chanels\n        """"""\n        if normalization_type == \'divide_255\':\n            images = images / 255\n        elif normalization_type == \'divide_256\':\n            images = images / 256\n        elif normalization_type == \'by_chanels\':\n            images = images.astype(\'float64\')\n            # for every channel in image(assume this is last dimension)\n            for i in range(images.shape[-1]):\n                images[:, :, :, i] = ((images[:, :, :, i] - self.images_means[i]) /\n                                       self.images_stds[i])\n        else:\n            raise Exception(""Unknown type of normalization"")\n        return images\n\n    def normalize_all_images_by_chanels(self, initial_images):\n        new_images = np.zeros(initial_images.shape)\n        for i in range(initial_images.shape[0]):\n            new_images[i] = self.normalize_image_by_chanel(initial_images[i])\n        return new_images\n\n    def normalize_image_by_chanel(self, image):\n        new_image = np.zeros(image.shape)\n        for chanel in range(3):\n            mean = np.mean(image[:, :, chanel])\n            std = np.std(image[:, :, chanel])\n            new_image[:, :, chanel] = (image[:, :, chanel] - mean) / std\n        return new_image\n\n\nclass DataProvider:\n    @property\n    def data_shape(self):\n        """"""Return shape as python list of one data entry""""""\n        raise NotImplementedError\n\n    @property\n    def n_classes(self):\n        """"""Return `int` of num classes""""""\n        raise NotImplementedError\n\n    def labels_to_one_hot(self, labels):\n        """"""Convert 1D array of labels to one hot representation\n        \n        Args:\n            labels: 1D numpy array\n        """"""\n        new_labels = np.zeros((labels.shape[0], self.n_classes))\n        new_labels[range(labels.shape[0]), labels] = np.ones(labels.shape)\n        return new_labels\n\n    def labels_from_one_hot(self, labels):\n        """"""Convert 2D array of labels to 1D class based representation\n        \n        Args:\n            labels: 2D numpy array\n        """"""\n        return np.argmax(labels, axis=1)\n'"
data_providers/cifar.py,0,"b'import tempfile\nimport os\nimport pickle\nimport random\n\nimport numpy as np\n\n\nfrom .base_provider import ImagesDataSet, DataProvider\nfrom .downloader import download_data_url\n\n\ndef augment_image(image, pad):\n    """"""Perform zero padding, randomly crop image to original size,\n    maybe mirror horizontally""""""\n    flip = random.getrandbits(1)\n    if flip:\n        image = image[:, ::-1, :]\n    init_shape = image.shape\n    new_shape = [init_shape[0] + pad * 2,\n                 init_shape[1] + pad * 2,\n                 init_shape[2]]\n    zeros_padded = np.zeros(new_shape)\n    zeros_padded[pad:init_shape[0] + pad, pad:init_shape[1] + pad, :] = image\n    # randomly crop to original size\n    init_x = np.random.randint(0, pad * 2)\n    init_y = np.random.randint(0, pad * 2)\n    cropped = zeros_padded[\n        init_x: init_x + init_shape[0],\n        init_y: init_y + init_shape[1],\n        :]\n    return cropped\n\n\ndef augment_all_images(initial_images, pad):\n    new_images = np.zeros(initial_images.shape)\n    for i in range(initial_images.shape[0]):\n        new_images[i] = augment_image(initial_images[i], pad=4)\n    return new_images\n\n\nclass CifarDataSet(ImagesDataSet):\n    def __init__(self, images, labels, n_classes, shuffle, normalization,\n                 augmentation):\n        """"""\n        Args:\n            images: 4D numpy array\n            labels: 2D or 1D numpy array\n            n_classes: `int`, number of cifar classes - 10 or 100\n            shuffle: `str` or None\n                None: no any shuffling\n                once_prior_train: shuffle train data only once prior train\n                every_epoch: shuffle train data prior every epoch\n            normalization: `str` or None\n                None: no any normalization\n                divide_255: divide all pixels by 255\n                divide_256: divide all pixels by 256\n                by_chanels: substract mean of every chanel and divide each\n                    chanel data by it\'s standart deviation\n            augmentation: `bool`\n        """"""\n        if shuffle is None:\n            self.shuffle_every_epoch = False\n        elif shuffle == \'once_prior_train\':\n            self.shuffle_every_epoch = False\n            images, labels = self.shuffle_images_and_labels(images, labels)\n        elif shuffle == \'every_epoch\':\n            self.shuffle_every_epoch = True\n        else:\n            raise Exception(""Unknown type of shuffling"")\n\n        self.images = images\n        self.labels = labels\n        self.n_classes = n_classes\n        self.augmentation = augmentation\n        self.normalization = normalization\n        self.images = self.normalize_images(images, self.normalization)\n        self.start_new_epoch()\n\n    def start_new_epoch(self):\n        self._batch_counter = 0\n        if self.shuffle_every_epoch:\n            images, labels = self.shuffle_images_and_labels(\n                self.images, self.labels)\n        else:\n            images, labels = self.images, self.labels\n        if self.augmentation:\n            images = augment_all_images(images, pad=4)\n        self.epoch_images = images\n        self.epoch_labels = labels\n\n    @property\n    def num_examples(self):\n        return self.labels.shape[0]\n\n    def next_batch(self, batch_size):\n        start = self._batch_counter * batch_size\n        end = (self._batch_counter + 1) * batch_size\n        self._batch_counter += 1\n        images_slice = self.epoch_images[start: end]\n        labels_slice = self.epoch_labels[start: end]\n        if images_slice.shape[0] != batch_size:\n            self.start_new_epoch()\n            return self.next_batch(batch_size)\n        else:\n            return images_slice, labels_slice\n\n\nclass CifarDataProvider(DataProvider):\n    """"""Abstract class for cifar readers""""""\n\n    def __init__(self, save_path=None, validation_set=None,\n                 validation_split=None, shuffle=None, normalization=None,\n                 one_hot=True, **kwargs):\n        """"""\n        Args:\n            save_path: `str`\n            validation_set: `bool`.\n            validation_split: `float` or None\n                float: chunk of `train set` will be marked as `validation set`.\n                None: if \'validation set\' == True, `validation set` will be\n                    copy of `test set`\n            shuffle: `str` or None\n                None: no any shuffling\n                once_prior_train: shuffle train data only once prior train\n                every_epoch: shuffle train data prior every epoch\n            normalization: `str` or None\n                None: no any normalization\n                divide_255: divide all pixels by 255\n                divide_256: divide all pixels by 256\n                by_chanels: substract mean of every chanel and divide each\n                    chanel data by it\'s standart deviation\n            one_hot: `bool`, return lasels one hot encoded\n        """"""\n        self._save_path = save_path\n        self.one_hot = one_hot\n        download_data_url(self.data_url, self.save_path)\n        train_fnames, test_fnames = self.get_filenames(self.save_path)\n\n        # add train and validations datasets\n        images, labels = self.read_cifar(train_fnames)\n        if validation_set is not None and validation_split is not None:\n            split_idx = int(images.shape[0] * (1 - validation_split))\n            self.train = CifarDataSet(\n                images=images[:split_idx], labels=labels[:split_idx],\n                n_classes=self.n_classes, shuffle=shuffle,\n                normalization=normalization,\n                augmentation=self.data_augmentation)\n            self.validation = CifarDataSet(\n                images=images[split_idx:], labels=labels[split_idx:],\n                n_classes=self.n_classes, shuffle=shuffle,\n                normalization=normalization,\n                augmentation=self.data_augmentation)\n        else:\n            self.train = CifarDataSet(\n                images=images, labels=labels,\n                n_classes=self.n_classes, shuffle=shuffle,\n                normalization=normalization,\n                augmentation=self.data_augmentation)\n\n        # add test set\n        images, labels = self.read_cifar(test_fnames)\n        self.test = CifarDataSet(\n            images=images, labels=labels,\n            shuffle=None, n_classes=self.n_classes,\n            normalization=normalization,\n            augmentation=False)\n\n        if validation_set and not validation_split:\n            self.validation = self.test\n\n    @property\n    def save_path(self):\n        if self._save_path is None:\n            self._save_path = os.path.join(\n                tempfile.gettempdir(), \'cifar%d\' % self.n_classes)\n        return self._save_path\n\n    @property\n    def data_url(self):\n        """"""Return url for downloaded data depends on cifar class""""""\n        data_url = (\'http://www.cs.toronto.edu/\'\n                    \'~kriz/cifar-%d-python.tar.gz\' % self.n_classes)\n        return data_url\n\n    @property\n    def data_shape(self):\n        return (32, 32, 3)\n\n    @property\n    def n_classes(self):\n        return self._n_classes\n\n    def get_filenames(self, save_path):\n        """"""Return two lists of train and test filenames for dataset""""""\n        raise NotImplementedError\n\n    def read_cifar(self, filenames):\n        if self.n_classes == 10:\n            labels_key = b\'labels\'\n        elif self.n_classes == 100:\n            labels_key = b\'fine_labels\'\n\n        images_res = []\n        labels_res = []\n        for fname in filenames:\n            with open(fname, \'rb\') as f:\n                images_and_labels = pickle.load(f, encoding=\'bytes\')\n            images = images_and_labels[b\'data\']\n            images = images.reshape(-1, 3, 32, 32)\n            images = images.swapaxes(1, 3).swapaxes(1, 2)\n            images_res.append(images)\n            labels_res.append(images_and_labels[labels_key])\n        images_res = np.vstack(images_res)\n        labels_res = np.hstack(labels_res)\n        if self.one_hot:\n            labels_res = self.labels_to_one_hot(labels_res)\n        return images_res, labels_res\n\n\nclass Cifar10DataProvider(CifarDataProvider):\n    _n_classes = 10\n    data_augmentation = False\n\n    def get_filenames(self, save_path):\n        sub_save_path = os.path.join(save_path, \'cifar-10-batches-py\')\n        train_filenames = [\n            os.path.join(\n                sub_save_path,\n                \'data_batch_%d\' % i) for i in range(1, 6)]\n        test_filenames = [os.path.join(sub_save_path, \'test_batch\')]\n        return train_filenames, test_filenames\n\n\nclass Cifar100DataProvider(CifarDataProvider):\n    _n_classes = 100\n    data_augmentation = False\n\n    def get_filenames(self, save_path):\n        sub_save_path = os.path.join(save_path, \'cifar-100-python\')\n        train_filenames = [os.path.join(sub_save_path, \'train\')]\n        test_filenames = [os.path.join(sub_save_path, \'test\')]\n        return train_filenames, test_filenames\n\n\nclass Cifar10AugmentedDataProvider(Cifar10DataProvider):\n    _n_classes = 10\n    data_augmentation = True\n\n\nclass Cifar100AugmentedDataProvider(Cifar100DataProvider):\n    _n_classes = 100\n    data_augmentation = True\n\n\nif __name__ == \'__main__\':\n    # some sanity checks for Cifar data providers\n    import matplotlib.pyplot as plt\n\n    # plot some CIFAR10 images with classes\n    def plot_images_labels(images, labels, axes, main_label, classes):\n        plt.text(0, 1.5, main_label, ha=\'center\', va=\'top\',\n                 transform=axes[len(axes) // 2].transAxes)\n        for image, label, axe in zip(images, labels, axes):\n            axe.imshow(image)\n            axe.set_title(classes[np.argmax(label)])\n            axe.set_axis_off()\n\n    cifar_10_idx_to_class = [\'airplane\', \'automobile\', \'bird\', \'cat\', \'deer\',\n                             \'dog\', \'frog\', \'horse\', \'ship\', \'truck\']\n    c10_provider = Cifar10DataProvider(\n        validation_set=True)\n    assert c10_provider._n_classes == 10\n    assert c10_provider.train.labels.shape[-1] == 10\n    assert len(c10_provider.train.labels.shape) == 2\n    assert np.all(c10_provider.validation.images == c10_provider.test.images)\n    assert c10_provider.train.images.shape[0] == 50000\n    assert c10_provider.test.images.shape[0] == 10000\n\n    # test split on validation dataset\n    c10_provider = Cifar10DataProvider(\n        one_hot=False, validation_set=True, validation_split=0.1)\n    assert len(c10_provider.train.labels.shape) == 1\n    assert not np.all(\n        c10_provider.validation.images == c10_provider.test.images)\n    assert c10_provider.train.images.shape[0] == 45000\n    assert c10_provider.validation.images.shape[0] == 5000\n    assert c10_provider.test.images.shape[0] == 10000\n\n    # test shuffling\n    c10_provider_not_shuffled = Cifar10DataProvider(shuffle=None)\n    c10_provider_shuffled = Cifar10DataProvider(shuffle=\'once_prior_train\')\n    assert not np.all(\n        c10_provider_not_shuffled.train.images != c10_provider_shuffled.train.images)\n    assert np.all(\n        c10_provider_not_shuffled.test.images == c10_provider_shuffled.test.images)\n\n    n_plots = 10\n    fig, axes = plt.subplots(nrows=4, ncols=n_plots)\n    plot_images_labels(\n        c10_provider_not_shuffled.train.images[:n_plots],\n        c10_provider_not_shuffled.train.labels[:n_plots],\n        axes[0],\n        \'Original dataset\',\n        cifar_10_idx_to_class)\n    dataset = Cifar10DataProvider(normalization=\'divide_256\')\n    plot_images_labels(\n        dataset.train.images[:n_plots],\n        dataset.train.labels[:n_plots],\n        axes[1],\n        \'Original dataset normalized dividing by 256\',\n        cifar_10_idx_to_class)\n    dataset = Cifar10DataProvider(normalization=\'by_chanels\')\n    plot_images_labels(\n        dataset.train.images[:n_plots],\n        dataset.train.labels[:n_plots],\n        axes[2],\n        \'Original dataset normalized by mean/std at every channel\',\n        cifar_10_idx_to_class)\n    plot_images_labels(\n        c10_provider_shuffled.train.images[:n_plots],\n        c10_provider_shuffled.train.labels[:n_plots],\n        axes[3],\n        \'Shuffled dataset\',\n        cifar_10_idx_to_class)\n    plt.show()\n\n    text_classes_file = os.path.join(\n        os.path.dirname(__file__), \'cifar_100_classes.txt\')\n    with open(\'/tmp/cifar100/cifar-100-python/meta\', \'rb\') as f:\n        cifar_100_meta = pickle.load(f, encoding=\'bytes\')\n    cifar_100_idx_to_class = cifar_100_meta[b\'fine_label_names\']\n\n    c100_provider_not_shuffled = Cifar100DataProvider(shuffle=None)\n    assert c100_provider_not_shuffled.train.labels.shape[-1] == 100\n    c100_provider_shuffled = Cifar100DataProvider(shuffle=\'once_prior_train\')\n\n    n_plots = 15\n    fig, axes = plt.subplots(nrows=2, ncols=n_plots)\n    plot_images_labels(\n        c100_provider_not_shuffled.train.images[:n_plots],\n        c100_provider_not_shuffled.train.labels[:n_plots],\n        axes[0],\n        \'Original dataset\',\n        cifar_100_idx_to_class)\n\n    plot_images_labels(\n        c100_provider_shuffled.train.images[:n_plots],\n        c100_provider_shuffled.train.labels[:n_plots],\n        axes[1],\n        \'Shuffled dataset\',\n        cifar_100_idx_to_class)\n    plt.show()\n'"
data_providers/downloader.py,0,"b'import sys\nimport os\nimport urllib.request\nimport tarfile\nimport zipfile\n\n\ndef report_download_progress(count, block_size, total_size):\n    pct_complete = float(count * block_size) / total_size\n    msg = ""\\r {0:.1%} already downloaded"".format(pct_complete)\n    sys.stdout.write(msg)\n    sys.stdout.flush()\n\n\ndef download_data_url(url, download_dir):\n    filename = url.split(\'/\')[-1]\n    file_path = os.path.join(download_dir, filename)\n\n    if not os.path.exists(file_path):\n        os.makedirs(download_dir, exist_ok=True)\n\n        print(""Download %s to %s"" % (url, file_path))\n        file_path, _ = urllib.request.urlretrieve(\n            url=url,\n            filename=file_path,\n            reporthook=report_download_progress)\n\n        print(""\\nExtracting files"")\n        if file_path.endswith("".zip""):\n            zipfile.ZipFile(file=file_path, mode=""r"").extractall(download_dir)\n        elif file_path.endswith(("".tar.gz"", "".tgz"")):\n            tarfile.open(name=file_path, mode=""r:gz"").extractall(download_dir)\n'"
data_providers/svhn.py,0,"b'import tempfile\nimport os\nimport scipy.io\n\nimport numpy as np\n\nfrom .base_provider import ImagesDataSet, DataProvider\nfrom .downloader import download_data_url\n\n\nclass SVHNDataSet(ImagesDataSet):\n    n_classes = 10\n\n    def __init__(self, images, labels, shuffle, normalization):\n        """"""\n        Args:\n            images: 4D numpy array\n            labels: 2D or 1D numpy array\n            shuffle: `bool`, should shuffle data or not\n            normalization: `str` or None\n                None: no any normalization\n                divide_255: divide all pixels by 255\n                divide_256: divide all pixels by 256\n                by_chanels: substract mean of every chanel and divide each\n                    chanel data by it\'s standart deviation\n        """"""\n        self.shuffle = shuffle\n        self.images = images\n        self.labels = labels\n        self.normalization = normalization\n        self.start_new_epoch()\n\n    def start_new_epoch(self):\n        self._batch_counter = 0\n        if self.shuffle:\n            self.images, self.labels = self.shuffle_images_and_labels(\n                self.images, self.labels)\n\n    @property\n    def num_examples(self):\n        return self.labels.shape[0]\n\n    def next_batch(self, batch_size):\n        start = self._batch_counter * batch_size\n        end = (self._batch_counter + 1) * batch_size\n        self._batch_counter += 1\n        images_slice = self.images[start: end]\n        labels_slice = self.labels[start: end]\n        # due to memory error it should be done inside batch\n        if self.normalization is not None:\n            images_slice = self.normalize_images(\n                images_slice, self.normalization)\n        if images_slice.shape[0] != batch_size:\n            self.start_new_epoch()\n            return self.next_batch(batch_size)\n        else:\n            return images_slice, labels_slice\n\n\nclass SVHNDataProvider(DataProvider):\n    def __init__(self, save_path=None, validation_set=None,\n                 validation_split=None, shuffle=False, normalization=None,\n                 one_hot=True, **kwargs):\n        """"""\n        Args:\n            save_path: `str`\n            validation_set: `bool`.\n            validation_split: `int` or None\n                float: chunk of `train set` will be marked as `validation set`.\n                None: if \'validation set\' == True, `validation set` will be\n                    copy of `test set`\n            shuffle: `bool`, should shuffle data or not\n            normalization: `str` or None\n                None: no any normalization\n                divide_255: divide all pixels by 255\n                divide_256: divide all pixels by 256\n                by_chanels: substract mean of every chanel and divide each\n                    chanel data by it\'s standart deviation\n            one_hot: `bool`, return lasels one hot encoded\n        """"""\n        self._save_path = save_path\n        train_images = []\n        train_labels = []\n        for part in [\'train\', \'extra\']:\n            images, labels = self.get_images_and_labels(part, one_hot)\n            train_images.append(images)\n            train_labels.append(labels)\n        train_images = np.vstack(train_images)\n        if one_hot:\n            train_labels = np.vstack(train_labels)\n        else:\n            train_labels = np.hstack(train_labels)\n        if validation_set and validation_split:\n            rand_indexes = np.random.permutation(train_images.shape[0])\n            valid_indexes = rand_indexes[:validation_split]\n            train_indexes = rand_indexes[:validation_split]\n            valid_images = train_images[valid_indexes]\n            valid_labels = train_labels[valid_indexes]\n            train_images = train_images[train_indexes]\n            train_labels = train_labels[train_indexes]\n            self.validation = SVHNDataSet(\n                valid_images, valid_labels, shuffle, normalization)\n\n        self.train = SVHNDataSet(\n            train_images, train_labels, shuffle, normalization)\n\n        test_images, test_labels = self.get_images_and_labels(\'test\', one_hot)\n        self.test = SVHNDataSet(test_images, test_labels, False, normalization)\n\n        if validation_set and not validation_split:\n            self.validation = self.test\n\n    def get_images_and_labels(self, name_part, one_hot=False):\n        url = self.data_url + name_part + \'_32x32.mat\'\n        download_data_url(url, self.save_path)\n        filename = os.path.join(self.save_path, name_part + \'_32x32.mat\')\n        data = scipy.io.loadmat(filename)\n        images = data[\'X\'].transpose(3, 0, 1, 2)\n        labels = data[\'y\'].reshape((-1))\n        labels[labels == 10] = 0\n        if one_hot:\n            labels = self.labels_to_one_hot(labels)\n        return images, labels\n\n    @property\n    def n_classes(self):\n        return 10\n\n    @property\n    def save_path(self):\n        if self._save_path is None:\n            self._save_path = os.path.join(tempfile.gettempdir(), \'svhn\')\n        return self._save_path\n\n    @property\n    def data_url(self):\n        return ""http://ufldl.stanford.edu/housenumbers/""\n\n    @property\n    def data_shape(self):\n        return (32, 32, 3)\n\n\nif __name__ == \'__main__\':\n    # WARNING: this test will require about 5 GB of RAM\n    import matplotlib.pyplot as plt\n\n    def plot_images_labels(images, labels, axes, main_label):\n        plt.text(0, 1.5, main_label, ha=\'center\', va=\'top\',\n                 transform=axes[len(axes) // 2].transAxes)\n        for image, label, axe in zip(images, labels, axes):\n            axe.imshow(image)\n            axe.set_title(np.argmax(label))\n            axe.set_axis_off()\n\n    n_plots = 10\n    fig, axes = plt.subplots(nrows=2, ncols=n_plots)\n\n    dataset = SVHNDataProvider()\n    plot_images_labels(\n        dataset.train.images[:n_plots],\n        dataset.train.labels[:n_plots],\n        axes[0],\n        \'Original dataset\')\n\n    dataset = SVHNDataProvider(shuffle=True)\n    plot_images_labels(\n        dataset.train.images[:n_plots],\n        dataset.train.labels[:n_plots],\n        axes[1],\n        \'Shuffled dataset\')\n\n    plt.show()\n'"
data_providers/utils.py,0,"b'from .cifar import Cifar10DataProvider, Cifar100DataProvider, \\\n    Cifar10AugmentedDataProvider, Cifar100AugmentedDataProvider\nfrom .svhn import SVHNDataProvider\n\n\ndef get_data_provider_by_name(name, train_params):\n    """"""Return required data provider class""""""\n    if name == \'C10\':\n        return Cifar10DataProvider(**train_params)\n    if name == \'C10+\':\n        return Cifar10AugmentedDataProvider(**train_params)\n    if name == \'C100\':\n        return Cifar100DataProvider(**train_params)\n    if name == \'C100+\':\n        return Cifar100AugmentedDataProvider(**train_params)\n    if name == \'SVHN\':\n        return SVHNDataProvider(**train_params)\n    else:\n        print(""Sorry, data provider for `%s` dataset ""\n              ""was not implemented yet"" % name)\n        exit()\n'"
models/__init__.py,0,b''
models/dense_net.py,54,"b'import os\nimport time\nimport shutil\nfrom datetime import timedelta\n\nimport numpy as np\nimport tensorflow as tf\n\n\nTF_VERSION = float(\'.\'.join(tf.__version__.split(\'.\')[:2]))\n\n\nclass DenseNet:\n    def __init__(self, data_provider, growth_rate, depth,\n                 total_blocks, keep_prob, num_inter_threads, num_intra_threads,\n                 weight_decay, nesterov_momentum, model_type, dataset,\n                 should_save_logs, should_save_model,\n                 renew_logs=False,\n                 reduction=1.0,\n                 bc_mode=False,\n                 **kwargs):\n        """"""\n        Class to implement networks from this paper\n        https://arxiv.org/pdf/1611.05552.pdf\n\n        Args:\n            data_provider: Class, that have all required data sets\n            growth_rate: `int`, variable from paper\n            depth: `int`, variable from paper\n            total_blocks: `int`, paper value == 3\n            keep_prob: `float`, keep probability for dropout. If keep_prob = 1\n                dropout will be disables\n            weight_decay: `float`, weight decay for L2 loss, paper = 1e-4\n            nesterov_momentum: `float`, momentum for Nesterov optimizer\n            model_type: `str`, \'DenseNet\' or \'DenseNet-BC\'. Should model use\n                bottle neck connections or not.\n            dataset: `str`, dataset name\n            should_save_logs: `bool`, should logs be saved or not\n            should_save_model: `bool`, should model be saved or not\n            renew_logs: `bool`, remove previous logs for current model\n            reduction: `float`, reduction Theta at transition layer for\n                DenseNets with bottleneck layers. See paragraph \'Compression\'\n                https://arxiv.org/pdf/1608.06993v3.pdf#4\n            bc_mode: `bool`, should we use bottleneck layers and features\n                reduction or not.\n        """"""\n        self.data_provider = data_provider\n        self.data_shape = data_provider.data_shape\n        self.n_classes = data_provider.n_classes\n        self.depth = depth\n        self.growth_rate = growth_rate\n        self.num_inter_threads = num_inter_threads\n        self.num_intra_threads = num_intra_threads\n        # how many features will be received after first convolution\n        # value the same as in the original Torch code\n        self.first_output_features = growth_rate * 2\n        self.total_blocks = total_blocks\n        self.layers_per_block = (depth - (total_blocks + 1)) // total_blocks\n        self.bc_mode = bc_mode\n        # compression rate at the transition layers\n        self.reduction = reduction\n        if not bc_mode:\n            print(""Build %s model with %d blocks, ""\n                  ""%d composite layers each."" % (\n                      model_type, self.total_blocks, self.layers_per_block))\n        if bc_mode:\n            self.layers_per_block = self.layers_per_block // 2\n            print(""Build %s model with %d blocks, ""\n                  ""%d bottleneck layers and %d composite layers each."" % (\n                      model_type, self.total_blocks, self.layers_per_block,\n                      self.layers_per_block))\n        print(""Reduction at transition layers: %.1f"" % self.reduction)\n\n        self.keep_prob = keep_prob\n        self.weight_decay = weight_decay\n        self.nesterov_momentum = nesterov_momentum\n        self.model_type = model_type\n        self.dataset_name = dataset\n        self.should_save_logs = should_save_logs\n        self.should_save_model = should_save_model\n        self.renew_logs = renew_logs\n        self.batches_step = 0\n\n        self._define_inputs()\n        self._build_graph()\n        self._initialize_session()\n        self._count_trainable_params()\n\n    def _initialize_session(self):\n        """"""Initialize session, variables, saver""""""\n        config = tf.ConfigProto()\n\n        # Specify the CPU inter and Intra threads used by MKL\n        config.intra_op_parallelism_threads = self.num_intra_threads\n        config.inter_op_parallelism_threads = self.num_inter_threads\n\n        # restrict model GPU memory utilization to min required\n        config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=config)\n        tf_ver = int(tf.__version__.split(\'.\')[1])\n        if TF_VERSION <= 0.10:\n            self.sess.run(tf.initialize_all_variables())\n            logswriter = tf.train.SummaryWriter\n        else:\n            self.sess.run(tf.global_variables_initializer())\n            logswriter = tf.summary.FileWriter\n        self.saver = tf.train.Saver()\n        self.summary_writer = logswriter(self.logs_path)\n\n    def _count_trainable_params(self):\n        total_parameters = 0\n        for variable in tf.trainable_variables():\n            shape = variable.get_shape()\n            variable_parametes = 1\n            for dim in shape:\n                variable_parametes *= dim.value\n            total_parameters += variable_parametes\n        print(""Total training params: %.1fM"" % (total_parameters / 1e6))\n\n    @property\n    def save_path(self):\n        try:\n            save_path = self._save_path\n        except AttributeError:\n            save_path = \'saves/%s\' % self.model_identifier\n            os.makedirs(save_path, exist_ok=True)\n            save_path = os.path.join(save_path, \'model.chkpt\')\n            self._save_path = save_path\n        return save_path\n\n    @property\n    def logs_path(self):\n        try:\n            logs_path = self._logs_path\n        except AttributeError:\n            logs_path = \'logs/%s\' % self.model_identifier\n            if self.renew_logs:\n                shutil.rmtree(logs_path, ignore_errors=True)\n            os.makedirs(logs_path, exist_ok=True)\n            self._logs_path = logs_path\n        return logs_path\n\n    @property\n    def model_identifier(self):\n        return ""{}_growth_rate={}_depth={}_dataset_{}"".format(\n            self.model_type, self.growth_rate, self.depth, self.dataset_name)\n\n    def save_model(self, global_step=None):\n        self.saver.save(self.sess, self.save_path, global_step=global_step)\n\n    def load_model(self):\n        try:\n            self.saver.restore(self.sess, self.save_path)\n        except Exception as e:\n            raise IOError(""Failed to to load model ""\n                          ""from save path: %s"" % self.save_path)\n        self.saver.restore(self.sess, self.save_path)\n        print(""Successfully load model from save path: %s"" % self.save_path)\n\n    def log_loss_accuracy(self, loss, accuracy, epoch, prefix,\n                          should_print=True):\n        if should_print:\n            print(""mean cross_entropy: %f, mean accuracy: %f"" % (\n                loss, accuracy))\n        summary = tf.Summary(value=[\n            tf.Summary.Value(\n                tag=\'loss_%s\' % prefix, simple_value=float(loss)),\n            tf.Summary.Value(\n                tag=\'accuracy_%s\' % prefix, simple_value=float(accuracy))\n        ])\n        self.summary_writer.add_summary(summary, epoch)\n\n    def _define_inputs(self):\n        shape = [None]\n        shape.extend(self.data_shape)\n        self.images = tf.placeholder(\n            tf.float32,\n            shape=shape,\n            name=\'input_images\')\n        self.labels = tf.placeholder(\n            tf.float32,\n            shape=[None, self.n_classes],\n            name=\'labels\')\n        self.learning_rate = tf.placeholder(\n            tf.float32,\n            shape=[],\n            name=\'learning_rate\')\n        self.is_training = tf.placeholder(tf.bool, shape=[])\n\n    def composite_function(self, _input, out_features, kernel_size=3):\n        """"""Function from paper H_l that performs:\n        - batch normalization\n        - ReLU nonlinearity\n        - convolution with required kernel\n        - dropout, if required\n        """"""\n        with tf.variable_scope(""composite_function""):\n            # BN\n            output = self.batch_norm(_input)\n            # ReLU\n            output = tf.nn.relu(output)\n            # convolution\n            output = self.conv2d(\n                output, out_features=out_features, kernel_size=kernel_size)\n            # dropout(in case of training and in case it is no 1.0)\n            output = self.dropout(output)\n        return output\n\n    def bottleneck(self, _input, out_features):\n        with tf.variable_scope(""bottleneck""):\n            output = self.batch_norm(_input)\n            output = tf.nn.relu(output)\n            inter_features = out_features * 4\n            output = self.conv2d(\n                output, out_features=inter_features, kernel_size=1,\n                padding=\'VALID\')\n            output = self.dropout(output)\n        return output\n\n    def add_internal_layer(self, _input, growth_rate):\n        """"""Perform H_l composite function for the layer and after concatenate\n        input with output from composite function.\n        """"""\n        # call composite function with 3x3 kernel\n        if not self.bc_mode:\n            comp_out = self.composite_function(\n                _input, out_features=growth_rate, kernel_size=3)\n        elif self.bc_mode:\n            bottleneck_out = self.bottleneck(_input, out_features=growth_rate)\n            comp_out = self.composite_function(\n                bottleneck_out, out_features=growth_rate, kernel_size=3)\n        # concatenate _input with out from composite function\n        if TF_VERSION >= 1.0:\n            output = tf.concat(axis=3, values=(_input, comp_out))\n        else:\n            output = tf.concat(3, (_input, comp_out))\n        return output\n\n    def add_block(self, _input, growth_rate, layers_per_block):\n        """"""Add N H_l internal layers""""""\n        output = _input\n        for layer in range(layers_per_block):\n            with tf.variable_scope(""layer_%d"" % layer):\n                output = self.add_internal_layer(output, growth_rate)\n        return output\n\n    def transition_layer(self, _input):\n        """"""Call H_l composite function with 1x1 kernel and after average\n        pooling\n        """"""\n        # call composite function with 1x1 kernel\n        out_features = int(int(_input.get_shape()[-1]) * self.reduction)\n        output = self.composite_function(\n            _input, out_features=out_features, kernel_size=1)\n        # run average pooling\n        output = self.avg_pool(output, k=2)\n        return output\n\n    def transition_layer_to_classes(self, _input):\n        """"""This is last transition to get probabilities by classes. It perform:\n        - batch normalization\n        - ReLU nonlinearity\n        - wide average pooling\n        - FC layer multiplication\n        """"""\n        # BN\n        output = self.batch_norm(_input)\n        # ReLU\n        output = tf.nn.relu(output)\n        # average pooling\n        last_pool_kernel = int(output.get_shape()[-2])\n        output = self.avg_pool(output, k=last_pool_kernel)\n        # FC\n        features_total = int(output.get_shape()[-1])\n        output = tf.reshape(output, [-1, features_total])\n        W = self.weight_variable_xavier(\n            [features_total, self.n_classes], name=\'W\')\n        bias = self.bias_variable([self.n_classes])\n        logits = tf.matmul(output, W) + bias\n        return logits\n\n    def conv2d(self, _input, out_features, kernel_size,\n               strides=[1, 1, 1, 1], padding=\'SAME\'):\n        in_features = int(_input.get_shape()[-1])\n        kernel = self.weight_variable_msra(\n            [kernel_size, kernel_size, in_features, out_features],\n            name=\'kernel\')\n        output = tf.nn.conv2d(_input, kernel, strides, padding)\n        return output\n\n    def avg_pool(self, _input, k):\n        ksize = [1, k, k, 1]\n        strides = [1, k, k, 1]\n        padding = \'VALID\'\n        output = tf.nn.avg_pool(_input, ksize, strides, padding)\n        return output\n\n    def batch_norm(self, _input):\n        output = tf.contrib.layers.batch_norm(\n            _input, scale=True, is_training=self.is_training,\n            updates_collections=None)\n        return output\n\n    def dropout(self, _input):\n        if self.keep_prob < 1:\n            output = tf.cond(\n                self.is_training,\n                lambda: tf.nn.dropout(_input, self.keep_prob),\n                lambda: _input\n            )\n        else:\n            output = _input\n        return output\n\n    def weight_variable_msra(self, shape, name):\n        return tf.get_variable(\n            name=name,\n            shape=shape,\n            initializer=tf.contrib.layers.variance_scaling_initializer())\n\n    def weight_variable_xavier(self, shape, name):\n        return tf.get_variable(\n            name,\n            shape=shape,\n            initializer=tf.contrib.layers.xavier_initializer())\n\n    def bias_variable(self, shape, name=\'bias\'):\n        initial = tf.constant(0.0, shape=shape)\n        return tf.get_variable(name, initializer=initial)\n\n    def _build_graph(self):\n        growth_rate = self.growth_rate\n        layers_per_block = self.layers_per_block\n        # first - initial 3 x 3 conv to first_output_features\n        with tf.variable_scope(""Initial_convolution""):\n            output = self.conv2d(\n                self.images,\n                out_features=self.first_output_features,\n                kernel_size=3)\n\n        # add N required blocks\n        for block in range(self.total_blocks):\n            with tf.variable_scope(""Block_%d"" % block):\n                output = self.add_block(output, growth_rate, layers_per_block)\n            # last block exist without transition layer\n            if block != self.total_blocks - 1:\n                with tf.variable_scope(""Transition_after_block_%d"" % block):\n                    output = self.transition_layer(output)\n\n        with tf.variable_scope(""Transition_to_classes""):\n            logits = self.transition_layer_to_classes(output)\n        prediction = tf.nn.softmax(logits)\n\n        # Losses\n        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n            logits=logits, labels=self.labels))\n        self.cross_entropy = cross_entropy\n        l2_loss = tf.add_n(\n            [tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n\n        # optimizer and train step\n        optimizer = tf.train.MomentumOptimizer(\n            self.learning_rate, self.nesterov_momentum, use_nesterov=True)\n        self.train_step = optimizer.minimize(\n            cross_entropy + l2_loss * self.weight_decay)\n\n        correct_prediction = tf.equal(\n            tf.argmax(prediction, 1),\n            tf.argmax(self.labels, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    def train_all_epochs(self, train_params):\n        n_epochs = train_params[\'n_epochs\']\n        learning_rate = train_params[\'initial_learning_rate\']\n        batch_size = train_params[\'batch_size\']\n        reduce_lr_epoch_1 = train_params[\'reduce_lr_epoch_1\']\n        reduce_lr_epoch_2 = train_params[\'reduce_lr_epoch_2\']\n        total_start_time = time.time()\n        for epoch in range(1, n_epochs + 1):\n            print(""\\n"", \'-\' * 30, ""Train epoch: %d"" % epoch, \'-\' * 30, \'\\n\')\n            start_time = time.time()\n            if epoch == reduce_lr_epoch_1 or epoch == reduce_lr_epoch_2:\n                learning_rate = learning_rate / 10\n                print(""Decrease learning rate, new lr = %f"" % learning_rate)\n\n            print(""Training..."")\n            loss, acc = self.train_one_epoch(\n                self.data_provider.train, batch_size, learning_rate)\n            if self.should_save_logs:\n                self.log_loss_accuracy(loss, acc, epoch, prefix=\'train\')\n\n            if train_params.get(\'validation_set\', False):\n                print(""Validation..."")\n                loss, acc = self.test(\n                    self.data_provider.validation, batch_size)\n                if self.should_save_logs:\n                    self.log_loss_accuracy(loss, acc, epoch, prefix=\'valid\')\n\n            time_per_epoch = time.time() - start_time\n            seconds_left = int((n_epochs - epoch) * time_per_epoch)\n            print(""Time per epoch: %s, Est. complete in: %s"" % (\n                str(timedelta(seconds=time_per_epoch)),\n                str(timedelta(seconds=seconds_left))))\n\n            if self.should_save_model:\n                self.save_model()\n\n        total_training_time = time.time() - total_start_time\n        print(""\\nTotal training time: %s"" % str(timedelta(\n            seconds=total_training_time)))\n\n    def train_one_epoch(self, data, batch_size, learning_rate):\n        num_examples = data.num_examples\n        total_loss = []\n        total_accuracy = []\n        for i in range(num_examples // batch_size):\n            batch = data.next_batch(batch_size)\n            images, labels = batch\n            feed_dict = {\n                self.images: images,\n                self.labels: labels,\n                self.learning_rate: learning_rate,\n                self.is_training: True,\n            }\n            fetches = [self.train_step, self.cross_entropy, self.accuracy]\n            result = self.sess.run(fetches, feed_dict=feed_dict)\n            _, loss, accuracy = result\n            total_loss.append(loss)\n            total_accuracy.append(accuracy)\n            if self.should_save_logs:\n                self.batches_step += 1\n                self.log_loss_accuracy(\n                    loss, accuracy, self.batches_step, prefix=\'per_batch\',\n                    should_print=False)\n        mean_loss = np.mean(total_loss)\n        mean_accuracy = np.mean(total_accuracy)\n        return mean_loss, mean_accuracy\n\n    def test(self, data, batch_size):\n        num_examples = data.num_examples\n        total_loss = []\n        total_accuracy = []\n        for i in range(num_examples // batch_size):\n            batch = data.next_batch(batch_size)\n            feed_dict = {\n                self.images: batch[0],\n                self.labels: batch[1],\n                self.is_training: False,\n            }\n            fetches = [self.cross_entropy, self.accuracy]\n            loss, accuracy = self.sess.run(fetches, feed_dict=feed_dict)\n            total_loss.append(loss)\n            total_accuracy.append(accuracy)\n        mean_loss = np.mean(total_loss)\n        mean_accuracy = np.mean(total_accuracy)\n        return mean_loss, mean_accuracy\n'"
