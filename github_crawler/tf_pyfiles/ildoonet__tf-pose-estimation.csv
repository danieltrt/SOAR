file_path,api_count,code
__init__.py,0,b'\n'
run.py,0,"b""import argparse\nimport logging\nimport sys\nimport time\n\nfrom tf_pose import common\nimport cv2\nimport numpy as np\nfrom tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import get_graph_path, model_wh\n\nlogger = logging.getLogger('TfPoseEstimatorRun')\nlogger.handlers.clear()\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='tf-pose-estimation run')\n    parser.add_argument('--image', type=str, default='./images/p1.jpg')\n    parser.add_argument('--model', type=str, default='cmu',\n                        help='cmu / mobilenet_thin / mobilenet_v2_large / mobilenet_v2_small')\n    parser.add_argument('--resize', type=str, default='0x0',\n                        help='if provided, resize images before they are processed. '\n                             'default=0x0, Recommends : 432x368 or 656x368 or 1312x736 ')\n    parser.add_argument('--resize-out-ratio', type=float, default=4.0,\n                        help='if provided, resize heatmaps before they are post-processed. default=1.0')\n\n    args = parser.parse_args()\n\n    w, h = model_wh(args.resize)\n    if w == 0 or h == 0:\n        e = TfPoseEstimator(get_graph_path(args.model), target_size=(432, 368))\n    else:\n        e = TfPoseEstimator(get_graph_path(args.model), target_size=(w, h))\n\n    # estimate human poses from a single image !\n    image = common.read_imgfile(args.image, None, None)\n    if image is None:\n        logger.error('Image can not be read, path=%s' % args.image)\n        sys.exit(-1)\n\n    t = time.time()\n    humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=args.resize_out_ratio)\n    elapsed = time.time() - t\n\n    logger.info('inference image: %s in %.4f seconds.' % (args.image, elapsed))\n\n    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n\n    try:\n        import matplotlib.pyplot as plt\n\n        fig = plt.figure()\n        a = fig.add_subplot(2, 2, 1)\n        a.set_title('Result')\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n        bgimg = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_BGR2RGB)\n        bgimg = cv2.resize(bgimg, (e.heatMat.shape[1], e.heatMat.shape[0]), interpolation=cv2.INTER_AREA)\n\n        # show network output\n        a = fig.add_subplot(2, 2, 2)\n        plt.imshow(bgimg, alpha=0.5)\n        tmp = np.amax(e.heatMat[:, :, :-1], axis=2)\n        plt.imshow(tmp, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n\n        tmp2 = e.pafMat.transpose((2, 0, 1))\n        tmp2_odd = np.amax(np.absolute(tmp2[::2, :, :]), axis=0)\n        tmp2_even = np.amax(np.absolute(tmp2[1::2, :, :]), axis=0)\n\n        a = fig.add_subplot(2, 2, 3)\n        a.set_title('Vectormap-x')\n        # plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\n        plt.imshow(tmp2_odd, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n\n        a = fig.add_subplot(2, 2, 4)\n        a.set_title('Vectormap-y')\n        # plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\n        plt.imshow(tmp2_even, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n        plt.show()\n    except Exception as e:\n        logger.warning('matplitlib error, %s' % e)\n        cv2.imshow('result', image)\n        cv2.waitKey()\n"""
run_checkpoint.py,11,"b'import argparse\nimport logging\nimport os\n\nimport tensorflow as tf\nfrom tf_pose.networks import get_network, model_wh, _get_base_path\n\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(levelname)s %(message)s\')\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allocator_type = \'BFC\'\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.95\nconfig.gpu_options.allow_growth = True\n\n\nif __name__ == \'__main__\':\n    """"""\n    Use this script to just save graph and checkpoint.\n    While training, checkpoints are saved. You can test them with this python code.\n    """"""\n    parser = argparse.ArgumentParser(description=\'Tensorflow Pose Estimation Graph Extractor\')\n    parser.add_argument(\'--model\', type=str, default=\'cmu\', help=\'cmu / mobilenet / mobilenet_thin / mobilenet_v2_large / mobilenet_v2_small\')\n    parser.add_argument(\'--resize\', type=str, default=\'0x0\')\n    parser.add_argument(\'--quantize\', action=\'store_true\')\n    args = parser.parse_args()\n\n    w, h = model_wh(args.resize)\n    if w <= 0 or h <= 0:\n        w = h = None\n    print(w, h)\n    input_node = tf.placeholder(tf.float32, shape=(None, h, w, 3), name=\'image\')\n\n    net, pretrain_path, last_layer = get_network(args.model, input_node, None, trainable=False)\n    if args.quantize:\n        g = tf.get_default_graph()\n        tf.contrib.quantize.create_eval_graph(input_graph=g)\n\n    with tf.Session(config=config) as sess:\n        loader = tf.train.Saver(net.restorable_variables())\n        loader.restore(sess, pretrain_path)\n\n        tf.train.write_graph(sess.graph_def, \'./tmp\', \'graph.pb\', as_text=True)\n\n        flops = tf.profiler.profile(None, cmd=\'graph\', options=tf.profiler.ProfileOptionBuilder.float_operation())\n        print(\'FLOP = \', flops.total_float_ops / float(1e6))\n\n        # graph = tf.get_default_graph()\n        # for n in tf.get_default_graph().as_graph_def().node:\n        #     if \'concat_stage\' not in n.name:\n        #         continue\n        #     print(n.name)\n\n        # saver = tf.train.Saver(max_to_keep=100)\n        # saver.save(sess, \'./tmp/chk\', global_step=1)\n'"
run_directory.py,0,"b""import argparse\nimport logging\nimport time\nimport glob\nimport ast\nimport os\nimport dill\n\nimport common\nimport cv2\nimport numpy as np\nfrom estimator import TfPoseEstimator\nfrom networks import get_graph_path, model_wh\n\nfrom lifting.prob_model import Prob3dPose\nfrom lifting.draw import plot_pose\n\nlogger = logging.getLogger('TfPoseEstimator')\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='tf-pose-estimation run by folder')\n    parser.add_argument('--folder', type=str, default='./images/')\n    parser.add_argument('--resolution', type=str, default='432x368', help='network input resolution. default=432x368')\n    parser.add_argument('--model', type=str, default='cmu', help='cmu / mobilenet_thin / mobilenet_v2_large / mobilenet_v2_small')\n    parser.add_argument('--scales', type=str, default='[None]', help='for multiple scales, eg. [1.0, (1.1, 0.05)]')\n    args = parser.parse_args()\n    scales = ast.literal_eval(args.scales)\n\n    w, h = model_wh(args.resolution)\n    e = TfPoseEstimator(get_graph_path(args.model), target_size=(w, h))\n\n    files_grabbed = glob.glob(os.path.join(args.folder, '*.jpg'))\n    all_humans = dict()\n    for i, file in enumerate(files_grabbed):\n        # estimate human poses from a single image !\n        image = common.read_imgfile(file, None, None)\n        t = time.time()\n        humans = e.inference(image, scales=scales)\n        elapsed = time.time() - t\n\n        logger.info('inference image #%d: %s in %.4f seconds.' % (i, file, elapsed))\n\n        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n        cv2.imshow('tf-pose-estimation result', image)\n        cv2.waitKey(5)\n\n        all_humans[file.replace(args.folder, '')] = humans\n\n    with open(os.path.join(args.folder, 'pose.dil'), 'wb') as f:\n        dill.dump(all_humans, f, protocol=dill.HIGHEST_PROTOCOL)\n"""
run_video.py,0,"b'import argparse\nimport logging\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import get_graph_path, model_wh\n\nlogger = logging.getLogger(\'TfPoseEstimator-Video\')\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\nfps_time = 0\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'tf-pose-estimation Video\')\n    parser.add_argument(\'--video\', type=str, default=\'\')\n    parser.add_argument(\'--resolution\', type=str, default=\'432x368\', help=\'network input resolution. default=432x368\')\n    parser.add_argument(\'--model\', type=str, default=\'mobilenet_thin\', help=\'cmu / mobilenet_thin / mobilenet_v2_large / mobilenet_v2_small\')\n    parser.add_argument(\'--show-process\', type=bool, default=False,\n                        help=\'for debug purpose, if enabled, speed for inference is dropped.\')\n    parser.add_argument(\'--showBG\', type=bool, default=True, help=\'False to show skeleton only.\')\n    args = parser.parse_args()\n\n    logger.debug(\'initialization %s : %s\' % (args.model, get_graph_path(args.model)))\n    w, h = model_wh(args.resolution)\n    e = TfPoseEstimator(get_graph_path(args.model), target_size=(w, h))\n    cap = cv2.VideoCapture(args.video)\n\n    if cap.isOpened() is False:\n        print(""Error opening video stream or file"")\n    while cap.isOpened():\n        ret_val, image = cap.read()\n\n        humans = e.inference(image)\n        if not args.showBG:\n            image = np.zeros(image.shape)\n        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n\n        cv2.putText(image, ""FPS: %f"" % (1.0 / (time.time() - fps_time)), (10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n        cv2.imshow(\'tf-pose-estimation result\', image)\n        fps_time = time.time()\n        if cv2.waitKey(1) == 27:\n            break\n\n    cv2.destroyAllWindows()\nlogger.debug(\'finished+\')\n'"
run_webcam.py,0,"b'import argparse\nimport logging\nimport time\n\nimport cv2\nimport numpy as np\n\nfrom tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import get_graph_path, model_wh\n\nlogger = logging.getLogger(\'TfPoseEstimator-WebCam\')\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\nfps_time = 0\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'tf-pose-estimation realtime webcam\')\n    parser.add_argument(\'--camera\', type=int, default=0)\n\n    parser.add_argument(\'--resize\', type=str, default=\'0x0\',\n                        help=\'if provided, resize images before they are processed. default=0x0, Recommends : 432x368 or 656x368 or 1312x736 \')\n    parser.add_argument(\'--resize-out-ratio\', type=float, default=4.0,\n                        help=\'if provided, resize heatmaps before they are post-processed. default=1.0\')\n\n    parser.add_argument(\'--model\', type=str, default=\'mobilenet_thin\', help=\'cmu / mobilenet_thin / mobilenet_v2_large / mobilenet_v2_small\')\n    parser.add_argument(\'--show-process\', type=bool, default=False,\n                        help=\'for debug purpose, if enabled, speed for inference is dropped.\')\n    \n    parser.add_argument(\'--tensorrt\', type=str, default=""False"",\n                        help=\'for tensorrt process.\')\n    args = parser.parse_args()\n\n    logger.debug(\'initialization %s : %s\' % (args.model, get_graph_path(args.model)))\n    w, h = model_wh(args.resize)\n    if w > 0 and h > 0:\n        e = TfPoseEstimator(get_graph_path(args.model), target_size=(w, h), trt_bool=str2bool(args.tensorrt))\n    else:\n        e = TfPoseEstimator(get_graph_path(args.model), target_size=(432, 368), trt_bool=str2bool(args.tensorrt))\n    logger.debug(\'cam read+\')\n    cam = cv2.VideoCapture(args.camera)\n    ret_val, image = cam.read()\n    logger.info(\'cam image=%dx%d\' % (image.shape[1], image.shape[0]))\n\n    while True:\n        ret_val, image = cam.read()\n\n        logger.debug(\'image process+\')\n        humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=args.resize_out_ratio)\n\n        logger.debug(\'postprocess+\')\n        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n\n        logger.debug(\'show+\')\n        cv2.putText(image,\n                    ""FPS: %f"" % (1.0 / (time.time() - fps_time)),\n                    (10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n                    (0, 255, 0), 2)\n        cv2.imshow(\'tf-pose-estimation result\', image)\n        fps_time = time.time()\n        if cv2.waitKey(1) == 27:\n            break\n        logger.debug(\'finished+\')\n\n    cv2.destroyAllWindows()\n'"
setup.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport subprocess\nimport setuptools\nfrom distutils.core import setup, Extension\n\nimport numpy as np\n\n_VERSION = \'0.1.1\'\n\ncwd = os.path.dirname(os.path.abspath(__file__))\nsubprocess.check_output([""bash"", ""models/graph/cmu/download.sh""], cwd=cwd)\n\nPOSE_DIR = os.path.realpath(os.path.dirname(__file__))\n\nREQUIRED_PACKAGES = [\n    \'argparse>=1.1\',\n    \'dill==0.2.7.1\',\n    \'fire >= 0.1.3\',\n    \'matplotlib >= 2.2.2\',\n    \'psutil >= 5.4.5\',\n    \'requests >= 2.18.4\',\n    \'scikit-image >= 0.13.1\',\n    \'scipy >= 1.1.0\',\n    \'slidingwindow >= 0.0.13\',\n    \'tqdm >= 4.23.4\',\n    \'tensorpack >= 0.8.5\',\n    \'pycocotools\'\n]\n\nDEPENDENCY_LINKS = [\n    \'git+https://github.com/ppwwyyxx/tensorpack.git#egg=tensorpack\',\n]\n\nEXT = Extension(\'_pafprocess\',\n                sources=[\n                    \'tf_pose/pafprocess/pafprocess_wrap.cpp\',\n                    \'tf_pose/pafprocess/pafprocess.cpp\',\n                ],\n                swig_opts=[\'-c++\'],\n                include_dirs=[np.get_include()])\n\nsetuptools.setup(\n    name=\'tf-pose\',\n    version=_VERSION,\n    description=\n    \'Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference.\',\n    install_requires=REQUIRED_PACKAGES,\n    dependency_links=DEPENDENCY_LINKS,\n    url=\'https://github.com/ildoonet/tf-pose-estimation/\',\n    author=\'Ildoo Kim\',\n    author_email=\'ildoo@ildoo.net\',\n    license=\'Apache License 2.0\',\n    package_dir={\'tf_pose_data\': \'models\'},\n    packages=[\'tf_pose_data\'] +\n             [pkg_name for pkg_name in setuptools.find_packages()  # main package\n              if \'tf_pose\' in pkg_name],\n    ext_modules=[EXT],\n    package_data={\'tf_pose_data\': [\'graph/cmu/graph_opt.pb\',\n                                   \'graph/mobilenet_thin/graph_opt.pb\']},\n    py_modules=[\n        ""pafprocess""\n    ],\n    zip_safe=False)\n'"
models/__init__.py,0,b''
scripts/broadcaster_ros.py,0,"b'#!/usr/bin/env python\nimport time\nimport os\nimport sys\nimport ast\n\nfrom threading import Lock\nimport rospy\nimport rospkg\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom tfpose_ros.msg import Persons, Person, BodyPartElm\n\nfrom tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import model_wh, get_graph_path\n\n\ndef humans_to_msg(humans):\n    persons = Persons()\n\n    for human in humans:\n        person = Person()\n\n        for k in human.body_parts:\n            body_part = human.body_parts[k]\n\n            body_part_msg = BodyPartElm()\n            body_part_msg.part_id = body_part.part_idx\n            body_part_msg.x = body_part.x\n            body_part_msg.y = body_part.y\n            body_part_msg.confidence = body_part.score\n            person.body_part.append(body_part_msg)\n        persons.persons.append(person)\n\n    return persons\n\n\ndef callback_image(data):\n    # et = time.time()\n    try:\n        cv_image = cv_bridge.imgmsg_to_cv2(data, ""bgr8"")\n    except CvBridgeError as e:\n        rospy.logerr(\'[tf-pose-estimation] Converting Image Error. \' + str(e))\n        return\n\n    acquired = tf_lock.acquire(False)\n    if not acquired:\n        return\n\n    try:\n        humans = pose_estimator.inference(cv_image, resize_to_default=True, upsample_size=resize_out_ratio)\n    finally:\n        tf_lock.release()\n\n    msg = humans_to_msg(humans)\n    msg.image_w = data.width\n    msg.image_h = data.height\n    msg.header = data.header\n\n    pub_pose.publish(msg)\n\n\nif __name__ == \'__main__\':\n    rospy.loginfo(\'initialization+\')\n    rospy.init_node(\'TfPoseEstimatorROS\', anonymous=True, log_level=rospy.INFO)\n\n    # parameters\n    image_topic = rospy.get_param(\'~camera\', \'\')\n    model = rospy.get_param(\'~model\', \'cmu\')\n\n    resolution = rospy.get_param(\'~resolution\', \'432x368\')\n    resize_out_ratio = float(rospy.get_param(\'~resize_out_ratio\', \'4.0\'))\n    tf_lock = Lock()\n\n    if not image_topic:\n        rospy.logerr(\'Parameter \\\'camera\\\' is not provided.\')\n        sys.exit(-1)\n\n    try:\n        w, h = model_wh(resolution)\n        graph_path = get_graph_path(model)\n\n        rospack = rospkg.RosPack()\n        graph_path = os.path.join(rospack.get_path(\'tfpose_ros\'), graph_path)\n    except Exception as e:\n        rospy.logerr(\'invalid model: %s, e=%s\' % (model, e))\n        sys.exit(-1)\n\n    pose_estimator = TfPoseEstimator(graph_path, target_size=(w, h))\n    cv_bridge = CvBridge()\n\n    rospy.Subscriber(image_topic, Image, callback_image, queue_size=1, buff_size=2**24)\n    pub_pose = rospy.Publisher(\'~pose\', Persons, queue_size=1)\n\n    rospy.loginfo(\'start+\')\n    rospy.spin()\n    rospy.loginfo(\'finished\')\n'"
scripts/visualization.py,0,"b'#!/usr/bin/env python\nimport time\nimport cv2\nimport rospy\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge, CvBridgeError\n\nfrom tfpose_ros.msg import Persons, Person, BodyPartElm\nfrom tf_pose.estimator import Human, BodyPart, TfPoseEstimator\n\n\nclass VideoFrames:\n    """"""\n    Reference : ros-video-recorder\n    https://github.com/ildoonet/ros-video-recorder/blob/master/scripts/recorder.py\n    """"""\n    def __init__(self, image_topic):\n        self.image_sub = rospy.Subscriber(image_topic, Image, self.callback_image, queue_size=1)\n        self.bridge = CvBridge()\n        self.frames = []\n\n    def callback_image(self, data):\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(data, ""bgr8"")\n        except CvBridgeError as e:\n            rospy.logerr(\'Converting Image Error. \' + str(e))\n            return\n\n        self.frames.append((data.header.stamp, cv_image))\n\n    def get_latest(self, at_time, remove_older=True):\n        fs = [x for x in self.frames if x[0] <= at_time]\n        if len(fs) == 0:\n            return None\n\n        f = fs[-1]\n        if remove_older:\n            self.frames = self.frames[len(fs) - 1:]\n\n        return f[1]\n\n\ndef cb_pose(data):\n    # get image with pose time\n    t = data.header.stamp\n    image = vf.get_latest(t, remove_older=True)\n    if image is None:\n        rospy.logwarn(\'No received images.\')\n        return\n\n    h, w = image.shape[:2]\n    if resize_ratio > 0:\n        image = cv2.resize(image, (int(resize_ratio*w), int(resize_ratio*h)), interpolation=cv2.INTER_LINEAR)\n\n    # ros topic to Person instance\n    humans = []\n    for p_idx, person in enumerate(data.persons):\n        human = Human([])\n        for body_part in person.body_part:\n            part = BodyPart(\'\', body_part.part_id, body_part.x, body_part.y, body_part.confidence)\n            human.body_parts[body_part.part_id] = part\n\n        humans.append(human)\n\n    # draw\n    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n    pub_img.publish(cv_bridge.cv2_to_imgmsg(image, \'bgr8\'))\n\n\nif __name__ == \'__main__\':\n    rospy.loginfo(\'initialization+\')\n    rospy.init_node(\'TfPoseEstimatorROS-Visualization\', anonymous=True)\n\n    # topics params\n    image_topic = rospy.get_param(\'~camera\', \'\')\n    pose_topic = rospy.get_param(\'~pose\', \'/pose_estimator/pose\')\n\n    resize_ratio = float(rospy.get_param(\'~resize_ratio\', \'-1\'))\n\n    # publishers\n    pub_img = rospy.Publisher(\'~output\', Image, queue_size=1)\n\n    # initialization\n    cv_bridge = CvBridge()\n    vf = VideoFrames(image_topic)\n    rospy.wait_for_message(image_topic, Image, timeout=30)\n\n    # subscribers\n    rospy.Subscriber(pose_topic, Persons, cb_pose, queue_size=1)\n\n    # run\n    rospy.spin()\n'"
tf_pose/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tf_pose.runner import infer, Estimator, get_estimator\n'"
tf_pose/common.py,1,"b""from enum import Enum\n\nimport tensorflow as tf\nimport cv2\n\n\nregularizer_conv = 0.004\nregularizer_dsconv = 0.0004\nbatchnorm_fused = True\nactivation_fn = tf.nn.relu\n\n\nclass CocoPart(Enum):\n    Nose = 0\n    Neck = 1\n    RShoulder = 2\n    RElbow = 3\n    RWrist = 4\n    LShoulder = 5\n    LElbow = 6\n    LWrist = 7\n    RHip = 8\n    RKnee = 9\n    RAnkle = 10\n    LHip = 11\n    LKnee = 12\n    LAnkle = 13\n    REye = 14\n    LEye = 15\n    REar = 16\n    LEar = 17\n    Background = 18\n\n\nclass MPIIPart(Enum):\n    RAnkle = 0\n    RKnee = 1\n    RHip = 2\n    LHip = 3\n    LKnee = 4\n    LAnkle = 5\n    RWrist = 6\n    RElbow = 7\n    RShoulder = 8\n    LShoulder = 9\n    LElbow = 10\n    LWrist = 11\n    Neck = 12\n    Head = 13\n\n    @staticmethod\n    def from_coco(human):\n        # t = {\n        #     MPIIPart.RAnkle: CocoPart.RAnkle,\n        #     MPIIPart.RKnee: CocoPart.RKnee,\n        #     MPIIPart.RHip: CocoPart.RHip,\n        #     MPIIPart.LHip: CocoPart.LHip,\n        #     MPIIPart.LKnee: CocoPart.LKnee,\n        #     MPIIPart.LAnkle: CocoPart.LAnkle,\n        #     MPIIPart.RWrist: CocoPart.RWrist,\n        #     MPIIPart.RElbow: CocoPart.RElbow,\n        #     MPIIPart.RShoulder: CocoPart.RShoulder,\n        #     MPIIPart.LShoulder: CocoPart.LShoulder,\n        #     MPIIPart.LElbow: CocoPart.LElbow,\n        #     MPIIPart.LWrist: CocoPart.LWrist,\n        #     MPIIPart.Neck: CocoPart.Neck,\n        #     MPIIPart.Nose: CocoPart.Nose,\n        # }\n\n        t = [\n            (MPIIPart.Head, CocoPart.Nose),\n            (MPIIPart.Neck, CocoPart.Neck),\n            (MPIIPart.RShoulder, CocoPart.RShoulder),\n            (MPIIPart.RElbow, CocoPart.RElbow),\n            (MPIIPart.RWrist, CocoPart.RWrist),\n            (MPIIPart.LShoulder, CocoPart.LShoulder),\n            (MPIIPart.LElbow, CocoPart.LElbow),\n            (MPIIPart.LWrist, CocoPart.LWrist),\n            (MPIIPart.RHip, CocoPart.RHip),\n            (MPIIPart.RKnee, CocoPart.RKnee),\n            (MPIIPart.RAnkle, CocoPart.RAnkle),\n            (MPIIPart.LHip, CocoPart.LHip),\n            (MPIIPart.LKnee, CocoPart.LKnee),\n            (MPIIPart.LAnkle, CocoPart.LAnkle),\n        ]\n\n        pose_2d_mpii = []\n        visibilty = []\n        for mpi, coco in t:\n            if coco.value not in human.body_parts.keys():\n                pose_2d_mpii.append((0, 0))\n                visibilty.append(False)\n                continue\n            pose_2d_mpii.append((human.body_parts[coco.value].x, human.body_parts[coco.value].y))\n            visibilty.append(True)\n        return pose_2d_mpii, visibilty\n\nCocoPairs = [\n    (1, 2), (1, 5), (2, 3), (3, 4), (5, 6), (6, 7), (1, 8), (8, 9), (9, 10), (1, 11),\n    (11, 12), (12, 13), (1, 0), (0, 14), (14, 16), (0, 15), (15, 17), (2, 16), (5, 17)\n]   # = 19\nCocoPairsRender = CocoPairs[:-2]\n# CocoPairsNetwork = [\n#     (12, 13), (20, 21), (14, 15), (16, 17), (22, 23), (24, 25), (0, 1), (2, 3), (4, 5),\n#     (6, 7), (8, 9), (10, 11), (28, 29), (30, 31), (34, 35), (32, 33), (36, 37), (18, 19), (26, 27)\n#  ]  # = 19\n\nCocoColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n\n\ndef read_imgfile(path, width=None, height=None):\n    val_image = cv2.imread(path, cv2.IMREAD_COLOR)\n    if width is not None and height is not None:\n        val_image = cv2.resize(val_image, (width, height))\n    return val_image\n\n\ndef get_sample_images(w, h):\n    val_image = [\n        read_imgfile('./images/p1.jpg', w, h),\n        read_imgfile('./images/p2.jpg', w, h),\n        read_imgfile('./images/p3.jpg', w, h),\n        read_imgfile('./images/golf.jpg', w, h),\n        read_imgfile('./images/hand1.jpg', w, h),\n        read_imgfile('./images/hand2.jpg', w, h),\n        read_imgfile('./images/apink1_crop.jpg', w, h),\n        read_imgfile('./images/ski.jpg', w, h),\n        read_imgfile('./images/apink2.jpg', w, h),\n        read_imgfile('./images/apink3.jpg', w, h),\n        read_imgfile('./images/handsup1.jpg', w, h),\n        read_imgfile('./images/p3_dance.png', w, h),\n    ]\n    return val_image\n\n\ndef to_str(s):\n    if not isinstance(s, str):\n        return s.decode('utf-8')\n    return s\n"""
tf_pose/datum_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: datum.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'datum.proto\',\n  package=\'\',\n  serialized_pb=_b(\'\\n\\x0b\\x64\\x61tum.proto\\""\\x81\\x01\\n\\x05\\x44\\x61tum\\x12\\x10\\n\\x08\\x63hannels\\x18\\x01 \\x01(\\x05\\x12\\x0e\\n\\x06height\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05width\\x18\\x03 \\x01(\\x05\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x04 \\x01(\\x0c\\x12\\r\\n\\x05label\\x18\\x05 \\x01(\\x05\\x12\\x12\\n\\nfloat_data\\x18\\x06 \\x03(\\x02\\x12\\x16\\n\\x07\\x65ncoded\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_DATUM = _descriptor.Descriptor(\n  name=\'Datum\',\n  full_name=\'Datum\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'Datum.channels\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'Datum.height\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'Datum.width\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'Datum.data\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'label\', full_name=\'Datum.label\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'float_data\', full_name=\'Datum.float_data\', index=5,\n      number=6, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'encoded\', full_name=\'Datum.encoded\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=16,\n  serialized_end=145,\n)\n\nDESCRIPTOR.message_types_by_name[\'Datum\'] = _DATUM\n\nDatum = _reflection.GeneratedProtocolMessageType(\'Datum\', (_message.Message,), dict(\n  DESCRIPTOR = _DATUM,\n  __module__ = \'datum_pb2\'\n  # @@protoc_insertion_point(class_scope:Datum)\n  ))\n_sym_db.RegisterMessage(Datum)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
tf_pose/estimator.py,18,"b'import logging\nimport math\n\nimport slidingwindow as sw\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nfrom tf_pose import common\nfrom tf_pose.common import CocoPart\nfrom tf_pose.tensblur.smoother import Smoother\nimport tensorflow.contrib.tensorrt as trt\n\ntry:\n    from tf_pose.pafprocess import pafprocess\nexcept ModuleNotFoundError as e:\n    print(e)\n    print(\'you need to build c++ library for pafprocess. See : https://github.com/ildoonet/tf-pose-estimation/tree/master/tf_pose/pafprocess\')\n    exit(-1)\n\nlogger = logging.getLogger(\'TfPoseEstimator\')\nlogger.handlers.clear()\nlogger.setLevel(logging.INFO)\nch = logging.StreamHandler()\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\nlogger.setLevel(logging.INFO)\n\n\ndef _round(v):\n    return int(round(v))\n\n\ndef _include_part(part_list, part_idx):\n    for part in part_list:\n        if part_idx == part.part_idx:\n            return True, part\n    return False, None\n\n\nclass Human:\n    """"""\n    body_parts: list of BodyPart\n    """"""\n    __slots__ = (\'body_parts\', \'pairs\', \'uidx_list\', \'score\')\n\n    def __init__(self, pairs):\n        self.pairs = []\n        self.uidx_list = set()\n        self.body_parts = {}\n        for pair in pairs:\n            self.add_pair(pair)\n        self.score = 0.0\n\n    @staticmethod\n    def _get_uidx(part_idx, idx):\n        return \'%d-%d\' % (part_idx, idx)\n\n    def add_pair(self, pair):\n        self.pairs.append(pair)\n        self.body_parts[pair.part_idx1] = BodyPart(Human._get_uidx(pair.part_idx1, pair.idx1),\n                                                   pair.part_idx1,\n                                                   pair.coord1[0], pair.coord1[1], pair.score)\n        self.body_parts[pair.part_idx2] = BodyPart(Human._get_uidx(pair.part_idx2, pair.idx2),\n                                                   pair.part_idx2,\n                                                   pair.coord2[0], pair.coord2[1], pair.score)\n        self.uidx_list.add(Human._get_uidx(pair.part_idx1, pair.idx1))\n        self.uidx_list.add(Human._get_uidx(pair.part_idx2, pair.idx2))\n\n    def is_connected(self, other):\n        return len(self.uidx_list & other.uidx_list) > 0\n\n    def merge(self, other):\n        for pair in other.pairs:\n            self.add_pair(pair)\n\n    def part_count(self):\n        return len(self.body_parts.keys())\n\n    def get_max_score(self):\n        return max([x.score for _, x in self.body_parts.items()])\n\n    def get_face_box(self, img_w, img_h, mode=0):\n        """"""\n        Get Face box compared to img size (w, h)\n        :param img_w:\n        :param img_h:\n        :param mode:\n        :return:\n        """"""\n        # SEE : https://github.com/ildoonet/tf-pose-estimation/blob/master/tf_pose/common.py#L13\n        _NOSE = CocoPart.Nose.value\n        _NECK = CocoPart.Neck.value\n        _REye = CocoPart.REye.value\n        _LEye = CocoPart.LEye.value\n        _REar = CocoPart.REar.value\n        _LEar = CocoPart.LEar.value\n\n        _THRESHOLD_PART_CONFIDENCE = 0.2\n        parts = [part for idx, part in self.body_parts.items() if part.score > _THRESHOLD_PART_CONFIDENCE]\n\n        is_nose, part_nose = _include_part(parts, _NOSE)\n        if not is_nose:\n            return None\n\n        size = 0\n        is_neck, part_neck = _include_part(parts, _NECK)\n        if is_neck:\n            size = max(size, img_h * (part_neck.y - part_nose.y) * 0.8)\n\n        is_reye, part_reye = _include_part(parts, _REye)\n        is_leye, part_leye = _include_part(parts, _LEye)\n        if is_reye and is_leye:\n            size = max(size, img_w * (part_reye.x - part_leye.x) * 2.0)\n            size = max(size,\n                       img_w * math.sqrt((part_reye.x - part_leye.x) ** 2 + (part_reye.y - part_leye.y) ** 2) * 2.0)\n\n        if mode == 1:\n            if not is_reye and not is_leye:\n                return None\n\n        is_rear, part_rear = _include_part(parts, _REar)\n        is_lear, part_lear = _include_part(parts, _LEar)\n        if is_rear and is_lear:\n            size = max(size, img_w * (part_rear.x - part_lear.x) * 1.6)\n\n        if size <= 0:\n            return None\n\n        if not is_reye and is_leye:\n            x = part_nose.x * img_w - (size // 3 * 2)\n        elif is_reye and not is_leye:\n            x = part_nose.x * img_w - (size // 3)\n        else:  # is_reye and is_leye:\n            x = part_nose.x * img_w - size // 2\n\n        x2 = x + size\n        if mode == 0:\n            y = part_nose.y * img_h - size // 3\n        else:\n            y = part_nose.y * img_h - _round(size / 2 * 1.2)\n        y2 = y + size\n\n        # fit into the image frame\n        x = max(0, x)\n        y = max(0, y)\n        x2 = min(img_w - x, x2 - x) + x\n        y2 = min(img_h - y, y2 - y) + y\n\n        if _round(x2 - x) == 0.0 or _round(y2 - y) == 0.0:\n            return None\n        if mode == 0:\n            return {""x"": _round((x + x2) / 2),\n                    ""y"": _round((y + y2) / 2),\n                    ""w"": _round(x2 - x),\n                    ""h"": _round(y2 - y)}\n        else:\n            return {""x"": _round(x),\n                    ""y"": _round(y),\n                    ""w"": _round(x2 - x),\n                    ""h"": _round(y2 - y)}\n\n    def get_upper_body_box(self, img_w, img_h):\n        """"""\n        Get Upper body box compared to img size (w, h)\n        :param img_w:\n        :param img_h:\n        :return:\n        """"""\n\n        if not (img_w > 0 and img_h > 0):\n            raise Exception(""img size should be positive"")\n\n        _NOSE = CocoPart.Nose.value\n        _NECK = CocoPart.Neck.value\n        _RSHOULDER = CocoPart.RShoulder.value\n        _LSHOULDER = CocoPart.LShoulder.value\n        _THRESHOLD_PART_CONFIDENCE = 0.3\n        parts = [part for idx, part in self.body_parts.items() if part.score > _THRESHOLD_PART_CONFIDENCE]\n        part_coords = [(img_w * part.x, img_h * part.y) for part in parts if\n                       part.part_idx in [0, 1, 2, 5, 8, 11, 14, 15, 16, 17]]\n\n        if len(part_coords) < 5:\n            return None\n\n        # Initial Bounding Box\n        x = min([part[0] for part in part_coords])\n        y = min([part[1] for part in part_coords])\n        x2 = max([part[0] for part in part_coords])\n        y2 = max([part[1] for part in part_coords])\n\n        # # ------ Adjust heuristically +\n        # if face points are detcted, adjust y value\n\n        is_nose, part_nose = _include_part(parts, _NOSE)\n        is_neck, part_neck = _include_part(parts, _NECK)\n        torso_height = 0\n        if is_nose and is_neck:\n            y -= (part_neck.y * img_h - y) * 0.8\n            torso_height = max(0, (part_neck.y - part_nose.y) * img_h * 2.5)\n        #\n        # # by using shoulder position, adjust width\n        is_rshoulder, part_rshoulder = _include_part(parts, _RSHOULDER)\n        is_lshoulder, part_lshoulder = _include_part(parts, _LSHOULDER)\n        if is_rshoulder and is_lshoulder:\n            half_w = x2 - x\n            dx = half_w * 0.15\n            x -= dx\n            x2 += dx\n        elif is_neck:\n            if is_lshoulder and not is_rshoulder:\n                half_w = abs(part_lshoulder.x - part_neck.x) * img_w * 1.15\n                x = min(part_neck.x * img_w - half_w, x)\n                x2 = max(part_neck.x * img_w + half_w, x2)\n            elif not is_lshoulder and is_rshoulder:\n                half_w = abs(part_rshoulder.x - part_neck.x) * img_w * 1.15\n                x = min(part_neck.x * img_w - half_w, x)\n                x2 = max(part_neck.x * img_w + half_w, x2)\n\n        # ------ Adjust heuristically -\n\n        # fit into the image frame\n        x = max(0, x)\n        y = max(0, y)\n        x2 = min(img_w - x, x2 - x) + x\n        y2 = min(img_h - y, y2 - y) + y\n\n        if _round(x2 - x) == 0.0 or _round(y2 - y) == 0.0:\n            return None\n        return {""x"": _round((x + x2) / 2),\n                ""y"": _round((y + y2) / 2),\n                ""w"": _round(x2 - x),\n                ""h"": _round(y2 - y)}\n\n    def __str__(self):\n        return \' \'.join([str(x) for x in self.body_parts.values()])\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass BodyPart:\n    """"""\n    part_idx : part index(eg. 0 for nose)\n    x, y: coordinate of body part\n    score : confidence score\n    """"""\n    __slots__ = (\'uidx\', \'part_idx\', \'x\', \'y\', \'score\')\n\n    def __init__(self, uidx, part_idx, x, y, score):\n        self.uidx = uidx\n        self.part_idx = part_idx\n        self.x, self.y = x, y\n        self.score = score\n\n    def get_part_name(self):\n        return CocoPart(self.part_idx)\n\n    def __str__(self):\n        return \'BodyPart:%d-(%.2f, %.2f) score=%.2f\' % (self.part_idx, self.x, self.y, self.score)\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass PoseEstimator:\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def estimate_paf(peaks, heat_mat, paf_mat):\n        pafprocess.process_paf(peaks, heat_mat, paf_mat)\n\n        humans = []\n        for human_id in range(pafprocess.get_num_humans()):\n            human = Human([])\n            is_added = False\n\n            for part_idx in range(18):\n                c_idx = int(pafprocess.get_part_cid(human_id, part_idx))\n                if c_idx < 0:\n                    continue\n\n                is_added = True\n                human.body_parts[part_idx] = BodyPart(\n                    \'%d-%d\' % (human_id, part_idx), part_idx,\n                    float(pafprocess.get_part_x(c_idx)) / heat_mat.shape[1],\n                    float(pafprocess.get_part_y(c_idx)) / heat_mat.shape[0],\n                    pafprocess.get_part_score(c_idx)\n                )\n\n            if is_added:\n                score = pafprocess.get_score(human_id)\n                human.score = score\n                humans.append(human)\n\n        return humans\n\n\nclass TfPoseEstimator:\n    # TODO : multi-scale\n\n    def __init__(self, graph_path, target_size=(320, 240), tf_config=None, trt_bool=False):\n        self.target_size = target_size\n\n        # load graph\n        logger.info(\'loading graph from %s(default size=%dx%d)\' % (graph_path, target_size[0], target_size[1]))\n        with tf.gfile.GFile(graph_path, \'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n\n        if trt_bool is True:\n            output_nodes = [""Openpose/concat_stage7""]\n            graph_def = trt.create_inference_graph(\n                graph_def,\n                output_nodes,\n                max_batch_size=1,\n                max_workspace_size_bytes=1 << 20,\n                precision_mode=""FP16"",\n                # precision_mode=""INT8"",\n                minimum_segment_size=3,\n                is_dynamic_op=True,\n                maximum_cached_engines=int(1e3),\n                use_calibration=True,\n            )\n\n        self.graph = tf.get_default_graph()\n        tf.import_graph_def(graph_def, name=\'TfPoseEstimator\')\n        self.persistent_sess = tf.Session(graph=self.graph, config=tf_config)\n\n        for ts in [n.name for n in tf.get_default_graph().as_graph_def().node]:\n            print(ts)\n\n        self.tensor_image = self.graph.get_tensor_by_name(\'TfPoseEstimator/image:0\')\n        self.tensor_output = self.graph.get_tensor_by_name(\'TfPoseEstimator/Openpose/concat_stage7:0\')\n        self.tensor_heatMat = self.tensor_output[:, :, :, :19]\n        self.tensor_pafMat = self.tensor_output[:, :, :, 19:]\n        self.upsample_size = tf.placeholder(dtype=tf.int32, shape=(2,), name=\'upsample_size\')\n        self.tensor_heatMat_up = tf.image.resize_area(self.tensor_output[:, :, :, :19], self.upsample_size,\n                                                      align_corners=False, name=\'upsample_heatmat\')\n        self.tensor_pafMat_up = tf.image.resize_area(self.tensor_output[:, :, :, 19:], self.upsample_size,\n                                                     align_corners=False, name=\'upsample_pafmat\')\n        if trt_bool is True:\n            smoother = Smoother({\'data\': self.tensor_heatMat_up}, 25, 3.0, 19)\n        else:\n            smoother = Smoother({\'data\': self.tensor_heatMat_up}, 25, 3.0)\n        gaussian_heatMat = smoother.get_output()\n\n        max_pooled_in_tensor = tf.nn.pool(gaussian_heatMat, window_shape=(3, 3), pooling_type=\'MAX\', padding=\'SAME\')\n        self.tensor_peaks = tf.where(tf.equal(gaussian_heatMat, max_pooled_in_tensor), gaussian_heatMat,\n                                     tf.zeros_like(gaussian_heatMat))\n\n        self.heatMat = self.pafMat = None\n\n        # warm-up\n        self.persistent_sess.run(tf.variables_initializer(\n            [v for v in tf.global_variables() if\n             v.name.split(\':\')[0] in [x.decode(\'utf-8\') for x in\n                                      self.persistent_sess.run(tf.report_uninitialized_variables())]\n             ])\n        )\n        self.persistent_sess.run(\n            [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up],\n            feed_dict={\n                self.tensor_image: [np.ndarray(shape=(target_size[1], target_size[0], 3), dtype=np.float32)],\n                self.upsample_size: [target_size[1], target_size[0]]\n            }\n        )\n        self.persistent_sess.run(\n            [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up],\n            feed_dict={\n                self.tensor_image: [np.ndarray(shape=(target_size[1], target_size[0], 3), dtype=np.float32)],\n                self.upsample_size: [target_size[1] // 2, target_size[0] // 2]\n            }\n        )\n        self.persistent_sess.run(\n            [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up],\n            feed_dict={\n                self.tensor_image: [np.ndarray(shape=(target_size[1], target_size[0], 3), dtype=np.float32)],\n                self.upsample_size: [target_size[1] // 4, target_size[0] // 4]\n            }\n        )\n\n        # logs\n        if self.tensor_image.dtype == tf.quint8:\n            logger.info(\'quantization mode enabled.\')\n\n    def __del__(self):\n        # self.persistent_sess.close()\n        pass\n\n    def get_flops(self):\n        flops = tf.profiler.profile(self.graph, options=tf.profiler.ProfileOptionBuilder.float_operation())\n        return flops.total_float_ops\n\n    @staticmethod\n    def _quantize_img(npimg):\n        npimg_q = npimg + 1.0\n        npimg_q /= (2.0 / 2 ** 8)\n        # npimg_q += 0.5\n        npimg_q = npimg_q.astype(np.uint8)\n        return npimg_q\n\n    @staticmethod\n    def draw_humans(npimg, humans, imgcopy=False):\n        if imgcopy:\n            npimg = np.copy(npimg)\n        image_h, image_w = npimg.shape[:2]\n        centers = {}\n        for human in humans:\n            # draw point\n            for i in range(common.CocoPart.Background.value):\n                if i not in human.body_parts.keys():\n                    continue\n\n                body_part = human.body_parts[i]\n                center = (int(body_part.x * image_w + 0.5), int(body_part.y * image_h + 0.5))\n                centers[i] = center\n                cv2.circle(npimg, center, 3, common.CocoColors[i], thickness=3, lineType=8, shift=0)\n\n            # draw line\n            for pair_order, pair in enumerate(common.CocoPairsRender):\n                if pair[0] not in human.body_parts.keys() or pair[1] not in human.body_parts.keys():\n                    continue\n\n                # npimg = cv2.line(npimg, centers[pair[0]], centers[pair[1]], common.CocoColors[pair_order], 3)\n                cv2.line(npimg, centers[pair[0]], centers[pair[1]], common.CocoColors[pair_order], 3)\n\n        return npimg\n\n    def _get_scaled_img(self, npimg, scale):\n        get_base_scale = lambda s, w, h: max(self.target_size[0] / float(h), self.target_size[1] / float(w)) * s\n        img_h, img_w = npimg.shape[:2]\n\n        if scale is None:\n            if npimg.shape[:2] != (self.target_size[1], self.target_size[0]):\n                # resize\n                npimg = cv2.resize(npimg, self.target_size, interpolation=cv2.INTER_CUBIC)\n            return [npimg], [(0.0, 0.0, 1.0, 1.0)]\n        elif isinstance(scale, float):\n            # scaling with center crop\n            base_scale = get_base_scale(scale, img_w, img_h)\n            npimg = cv2.resize(npimg, dsize=None, fx=base_scale, fy=base_scale, interpolation=cv2.INTER_CUBIC)\n\n            o_size_h, o_size_w = npimg.shape[:2]\n            if npimg.shape[0] < self.target_size[1] or npimg.shape[1] < self.target_size[0]:\n                newimg = np.zeros(\n                    (max(self.target_size[1], npimg.shape[0]), max(self.target_size[0], npimg.shape[1]), 3),\n                    dtype=np.uint8)\n                newimg[:npimg.shape[0], :npimg.shape[1], :] = npimg\n                npimg = newimg\n\n            windows = sw.generate(npimg, sw.DimOrder.HeightWidthChannel, self.target_size[0], self.target_size[1], 0.2)\n\n            rois = []\n            ratios = []\n            for window in windows:\n                indices = window.indices()\n                roi = npimg[indices]\n                rois.append(roi)\n                ratio_x, ratio_y = float(indices[1].start) / o_size_w, float(indices[0].start) / o_size_h\n                ratio_w, ratio_h = float(indices[1].stop - indices[1].start) / o_size_w, float(\n                    indices[0].stop - indices[0].start) / o_size_h\n                ratios.append((ratio_x, ratio_y, ratio_w, ratio_h))\n\n            return rois, ratios\n        elif isinstance(scale, tuple) and len(scale) == 2:\n            # scaling with sliding window : (scale, step)\n            base_scale = get_base_scale(scale[0], img_w, img_h)\n            npimg = cv2.resize(npimg, dsize=None, fx=base_scale, fy=base_scale, interpolation=cv2.INTER_CUBIC)\n            o_size_h, o_size_w = npimg.shape[:2]\n            if npimg.shape[0] < self.target_size[1] or npimg.shape[1] < self.target_size[0]:\n                newimg = np.zeros(\n                    (max(self.target_size[1], npimg.shape[0]), max(self.target_size[0], npimg.shape[1]), 3),\n                    dtype=np.uint8)\n                newimg[:npimg.shape[0], :npimg.shape[1], :] = npimg\n                npimg = newimg\n\n            window_step = scale[1]\n\n            windows = sw.generate(npimg, sw.DimOrder.HeightWidthChannel, self.target_size[0], self.target_size[1],\n                                  window_step)\n\n            rois = []\n            ratios = []\n            for window in windows:\n                indices = window.indices()\n                roi = npimg[indices]\n                rois.append(roi)\n                ratio_x, ratio_y = float(indices[1].start) / o_size_w, float(indices[0].start) / o_size_h\n                ratio_w, ratio_h = float(indices[1].stop - indices[1].start) / o_size_w, float(\n                    indices[0].stop - indices[0].start) / o_size_h\n                ratios.append((ratio_x, ratio_y, ratio_w, ratio_h))\n\n            return rois, ratios\n        elif isinstance(scale, tuple) and len(scale) == 3:\n            # scaling with ROI : (want_x, want_y, scale_ratio)\n            base_scale = get_base_scale(scale[2], img_w, img_h)\n            npimg = cv2.resize(npimg, dsize=None, fx=base_scale, fy=base_scale, interpolation=cv2.INTER_CUBIC)\n            ratio_w = self.target_size[0] / float(npimg.shape[1])\n            ratio_h = self.target_size[1] / float(npimg.shape[0])\n\n            want_x, want_y = scale[:2]\n            ratio_x = want_x - ratio_w / 2.\n            ratio_y = want_y - ratio_h / 2.\n            ratio_x = max(ratio_x, 0.0)\n            ratio_y = max(ratio_y, 0.0)\n            if ratio_x + ratio_w > 1.0:\n                ratio_x = 1. - ratio_w\n            if ratio_y + ratio_h > 1.0:\n                ratio_y = 1. - ratio_h\n\n            roi = self._crop_roi(npimg, ratio_x, ratio_y)\n            return [roi], [(ratio_x, ratio_y, ratio_w, ratio_h)]\n\n    def _crop_roi(self, npimg, ratio_x, ratio_y):\n        target_w, target_h = self.target_size\n        h, w = npimg.shape[:2]\n        x = max(int(w * ratio_x - .5), 0)\n        y = max(int(h * ratio_y - .5), 0)\n        cropped = npimg[y:y + target_h, x:x + target_w]\n\n        cropped_h, cropped_w = cropped.shape[:2]\n        if cropped_w < target_w or cropped_h < target_h:\n            npblank = np.zeros((self.target_size[1], self.target_size[0], 3), dtype=np.uint8)\n\n            copy_x, copy_y = (target_w - cropped_w) // 2, (target_h - cropped_h) // 2\n            npblank[copy_y:copy_y + cropped_h, copy_x:copy_x + cropped_w] = cropped\n        else:\n            return cropped\n\n    def inference(self, npimg, resize_to_default=True, upsample_size=1.0):\n        if npimg is None:\n            raise Exception(\'The image is not valid. Please check your image exists.\')\n\n        if resize_to_default:\n            upsample_size = [int(self.target_size[1] / 8 * upsample_size), int(self.target_size[0] / 8 * upsample_size)]\n        else:\n            upsample_size = [int(npimg.shape[0] / 8 * upsample_size), int(npimg.shape[1] / 8 * upsample_size)]\n\n        if self.tensor_image.dtype == tf.quint8:\n            # quantize input image\n            npimg = TfPoseEstimator._quantize_img(npimg)\n            pass\n\n        logger.debug(\'inference+ original shape=%dx%d\' % (npimg.shape[1], npimg.shape[0]))\n        img = npimg\n        if resize_to_default:\n            img = self._get_scaled_img(npimg, None)[0][0]\n        peaks, heatMat_up, pafMat_up = self.persistent_sess.run(\n            [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up], feed_dict={\n                self.tensor_image: [img], self.upsample_size: upsample_size\n            })\n        peaks = peaks[0]\n        self.heatMat = heatMat_up[0]\n        self.pafMat = pafMat_up[0]\n        logger.debug(\'inference- heatMat=%dx%d pafMat=%dx%d\' % (\n            self.heatMat.shape[1], self.heatMat.shape[0], self.pafMat.shape[1], self.pafMat.shape[0]))\n\n        t = time.time()\n        humans = PoseEstimator.estimate_paf(peaks, self.heatMat, self.pafMat)\n        logger.debug(\'estimate time=%.5f\' % (time.time() - t))\n        return humans\n\n\nif __name__ == \'__main__\':\n    import pickle\n\n    f = open(\'./etcs/heatpaf1.pkl\', \'rb\')\n    data = pickle.load(f)\n    logger.info(\'size={}\'.format(data[\'heatMat\'].shape))\n    f.close()\n\n    t = time.time()\n    humans = PoseEstimator.estimate_paf(data[\'peaks\'], data[\'heatMat\'], data[\'pafMat\'])\n    dt = time.time() - t;\n    t = time.time()\n    logger.info(\'elapsed #humans=%d time=%.8f\' % (len(humans), dt))\n'"
tf_pose/eval.py,0,"b'import sys\nimport os\nimport time\nfrom collections import OrderedDict\n\nimport numpy as np\nimport logging\nimport argparse\nimport json, re\nfrom tqdm import tqdm\n\nfrom tf_pose.common import read_imgfile\nfrom tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import model_wh, get_graph_path\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(levelname)s %(message)s\')\n\nlogger = logging.getLogger(\'TfPoseEstimator-Video\')\nlogger.setLevel(logging.INFO)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\neval_size = -1\n\n\ndef round_int(val):\n    return int(round(val))\n\n\ndef write_coco_json(human, image_w, image_h):\n    keypoints = []\n    coco_ids = [0, 15, 14, 17, 16, 5, 2, 6, 3, 7, 4, 11, 8, 12, 9, 13, 10]\n    for coco_id in coco_ids:\n        if coco_id not in human.body_parts.keys():\n            keypoints.extend([0, 0, 0])\n            continue\n        body_part = human.body_parts[coco_id]\n        keypoints.extend([round_int(body_part.x * image_w), round_int(body_part.y * image_h), 2])\n    return keypoints\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Tensorflow Openpose Inference\')\n    parser.add_argument(\'--resize\', type=str, default=\'0x0\', help=\'if provided, resize images before they are processed. default=0x0, Recommends : 432x368 or 656x368 or 1312x736 \')\n    parser.add_argument(\'--resize-out-ratio\', type=float, default=8.0, help=\'if provided, resize heatmaps before they are post-processed. default=8.0\')\n    parser.add_argument(\'--model\', type=str, default=\'cmu\', help=\'cmu / mobilenet_thin / mobilenet_v2_large\')\n    parser.add_argument(\'--cocoyear\', type=str, default=\'2014\')\n    parser.add_argument(\'--coco-dir\', type=str, default=\'/data/public/rw/coco/\')\n    parser.add_argument(\'--data-idx\', type=int, default=-1)\n    parser.add_argument(\'--multi-scale\', type=bool, default=False)\n    args = parser.parse_args()\n\n    cocoyear_list = [\'2014\', \'2017\']\n    if args.cocoyear not in cocoyear_list:\n        logger.error(\'cocoyear should be one of %s\' % str(cocoyear_list))\n        sys.exit(-1)\n\n    # TODO : Scales\n\n    image_dir = args.coco_dir + \'val%s/\' % args.cocoyear\n    coco_json_file = args.coco_dir + \'annotations/person_keypoints_val%s.json\' % args.cocoyear\n    cocoGt = COCO(coco_json_file)\n    catIds = cocoGt.getCatIds(catNms=[\'person\'])\n    keys = cocoGt.getImgIds(catIds=catIds)\n    if args.data_idx < 0:\n        if eval_size > 0:\n            keys = keys[:eval_size]  # only use the first #eval_size elements.\n        pass\n    else:\n        keys = [keys[args.data_idx]]\n    logger.info(\'validation %s set size=%d\' % (coco_json_file, len(keys)))\n    write_json = \'../etcs/%s_%s_%0.1f.json\' % (args.model, args.resize, args.resize_out_ratio)\n\n    logger.debug(\'initialization %s : %s\' % (args.model, get_graph_path(args.model)))\n    w, h = model_wh(args.resize)\n    if w == 0 or h == 0:\n        e = TfPoseEstimator(get_graph_path(args.model), target_size=(432, 368))\n    else:\n        e = TfPoseEstimator(get_graph_path(args.model), target_size=(w, h))\n\n    print(\'FLOPs: \', e.get_flops())\n\n    result = []\n    tqdm_keys = tqdm(keys)\n    for i, k in enumerate(tqdm_keys):\n        img_meta = cocoGt.loadImgs(k)[0]\n        img_idx = img_meta[\'id\']\n\n        img_name = os.path.join(image_dir, img_meta[\'file_name\'])\n        image = read_imgfile(img_name, None, None)\n        if image is None:\n            logger.error(\'image not found, path=%s\' % img_name)\n            sys.exit(-1)\n\n        # inference the image with the specified network\n        t = time.time()\n        humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=args.resize_out_ratio)\n        elapsed = time.time() - t\n\n        scores = 0\n        ann_idx = cocoGt.getAnnIds(imgIds=[img_idx], catIds=[1])\n        anns = cocoGt.loadAnns(ann_idx)\n        for human in humans:\n            item = {\n                \'image_id\': img_idx,\n                \'category_id\': 1,\n                \'keypoints\': write_coco_json(human, img_meta[\'width\'], img_meta[\'height\']),\n                \'score\': human.score\n            }\n            result.append(item)\n            scores += item[\'score\']\n\n        avg_score = scores / len(humans) if len(humans) > 0 else 0\n        tqdm_keys.set_postfix(OrderedDict({\'inference time\': elapsed, \'score\': avg_score}))\n        if args.data_idx >= 0:\n            logger.info(\'score:\', k, len(humans), len(anns), avg_score)\n\n            import matplotlib.pyplot as plt\n            fig = plt.figure()\n            a = fig.add_subplot(2, 3, 1)\n            plt.imshow(e.draw_humans(image, humans, True))\n\n            a = fig.add_subplot(2, 3, 2)\n            # plt.imshow(cv2.resize(image, (e.heatMat.shape[1], e.heatMat.shape[0])), alpha=0.5)\n            tmp = np.amax(e.heatMat[:, :, :-1], axis=2)\n            plt.imshow(tmp, cmap=plt.cm.gray, alpha=0.5)\n            plt.colorbar()\n\n            tmp2 = e.pafMat.transpose((2, 0, 1))\n            tmp2_odd = np.amax(np.absolute(tmp2[::2, :, :]), axis=0)\n            tmp2_even = np.amax(np.absolute(tmp2[1::2, :, :]), axis=0)\n\n            a = fig.add_subplot(2, 3, 4)\n            a.set_title(\'Vectormap-x\')\n            # plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\n            plt.imshow(tmp2_odd, cmap=plt.cm.gray, alpha=0.5)\n            plt.colorbar()\n\n            a = fig.add_subplot(2, 3, 5)\n            a.set_title(\'Vectormap-y\')\n            # plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\n            plt.imshow(tmp2_even, cmap=plt.cm.gray, alpha=0.5)\n            plt.colorbar()\n\n            plt.show()\n\n    fp = open(write_json, \'w\')\n    json.dump(result, fp)\n    fp.close()\n\n    cocoDt = cocoGt.loadRes(write_json)\n    cocoEval = COCOeval(cocoGt, cocoDt, \'keypoints\')\n    cocoEval.params.imgIds = keys\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n    print(\'\'.join([""%11.4f |"" % x for x in cocoEval.stats]))\n\n    pred = json.load(open(write_json, \'r\'))\n'"
tf_pose/network_base.py,52,"b'from __future__ import absolute_import\n\nimport sys\n\nimport abc\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom tf_pose.common import to_str\nfrom tf_pose import common\n\nDEFAULT_PADDING = \'SAME\'\n\n\n_init_xavier = tf.contrib.layers.xavier_initializer()\n_init_norm = tf.truncated_normal_initializer(stddev=0.01)\n_init_zero = slim.init_ops.zeros_initializer()\n_l2_regularizer_00004 = tf.contrib.layers.l2_regularizer(0.00004)\n_l2_regularizer_convb = tf.contrib.layers.l2_regularizer(common.regularizer_conv)\n\n\ndef layer(op):\n    \'\'\'\n    Decorator for composable network layers.\n    \'\'\'\n\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.terminals) == 0:\n            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n        elif len(self.terminals) == 1:\n            layer_input = self.terminals[0]\n        else:\n            layer_input = list(self.terminals)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n\n    return layer_decorated\n\n\nclass BaseNetwork(object):\n    def __init__(self, inputs, trainable=True):\n        # The input nodes for this network\n        self.inputs = inputs\n        # The current list of terminal nodes\n        self.terminals = []\n        # Mapping from layer names to layers\n        self.layers = dict(inputs)\n        # If true, the resulting variables are set as trainable\n        self.trainable = trainable\n        # Switch variable for dropout\n        self.use_dropout = tf.placeholder_with_default(tf.constant(1.0),\n                                                       shape=[],\n                                                       name=\'use_dropout\')\n        self.setup()\n\n    @abc.abstractmethod\n    def setup(self):\n        \'\'\'Construct the network. \'\'\'\n        raise NotImplementedError(\'Must be implemented by the subclass.\')\n\n    def load(self, data_path, session, ignore_missing=False):\n        \'\'\'\n        Load network weights.\n        data_path: The path to the numpy-serialized network weights\n        session: The current TensorFlow session\n        ignore_missing: If true, serialized weights for missing layers are ignored.\n        \'\'\'\n        data_dict = np.load(data_path, encoding=\'bytes\').item()\n        for op_name, param_dict in data_dict.items():\n            if isinstance(data_dict[op_name], np.ndarray):\n                if \'RMSProp\' in op_name:\n                    continue\n                with tf.variable_scope(\'\', reuse=True):\n                    var = tf.get_variable(op_name.replace(\':0\', \'\'))\n                    try:\n                        session.run(var.assign(data_dict[op_name]))\n                    except Exception as e:\n                        print(op_name)\n                        print(e)\n                        sys.exit(-1)\n            else:\n                op_name = to_str(op_name)\n                # if op_name > \'conv4\':\n                #     print(op_name, \'skipped\')\n                #     continue\n                # print(op_name, \'restored\')\n                with tf.variable_scope(op_name, reuse=True):\n                    for param_name, data in param_dict.items():\n                        try:\n                            var = tf.get_variable(to_str(param_name))\n                            session.run(var.assign(data))\n                        except ValueError as e:\n                            print(e)\n                            if not ignore_missing:\n                                raise\n\n    def feed(self, *args):\n        \'\'\'Set the input(s) for the next operation by replacing the terminal nodes.\n        The arguments can be either layer names or the actual layers.\n        \'\'\'\n        assert len(args) != 0\n        self.terminals = []\n        for fed_layer in args:\n            try:\n                is_str = isinstance(fed_layer, basestring)\n            except NameError:\n                is_str = isinstance(fed_layer, str)\n            if is_str:\n                try:\n                    fed_layer = self.layers[fed_layer]\n                except KeyError:\n                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n            self.terminals.append(fed_layer)\n        return self\n\n    def get_output(self, name=None):\n        \'\'\'Returns the current network output.\'\'\'\n        if not name:\n            return self.terminals[-1]\n        else:\n            return self.layers[name]\n\n    def get_tensor(self, name):\n        return self.get_output(name)\n\n    def get_unique_name(self, prefix):\n        \'\'\'Returns an index-suffixed unique name for the given prefix.\n        This is used for auto-generating layer names based on the type-prefix.\n        \'\'\'\n        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n        return \'%s_%d\' % (prefix, ident)\n\n    def make_var(self, name, shape, trainable=True):\n        \'\'\'Creates a new TensorFlow variable.\'\'\'\n        return tf.get_variable(name, shape, trainable=self.trainable & trainable, initializer=tf.contrib.layers.xavier_initializer())\n\n    def validate_padding(self, padding):\n        \'\'\'Verifies that the padding is one of the supported ones.\'\'\'\n        assert padding in (\'SAME\', \'VALID\')\n\n    @layer\n    def normalize_vgg(self, input, name):\n        # normalize input -0.5 ~ 0.5\n        input = tf.multiply(input, 1./ 256.0, name=name + \'_divide\')\n        input = tf.add(input, -0.5, name=name + \'_subtract\')\n        return input\n\n    @layer\n    def normalize_mobilenet(self, input, name):\n        input = tf.divide(input, 128.0, name=name + \'_divide\')\n        input = tf.subtract(input, 1.0, name=name + \'_subtract\')\n        return input\n\n    @layer\n    def normalize_nasnet(self, input, name):\n        input = tf.divide(input, 255.0, name=name + \'_divide\')\n        input = tf.subtract(input, 0.5, name=name + \'_subtract\')\n        input = tf.multiply(input, 2.0, name=name + \'_multiply\')\n        return input\n\n    @layer\n    def upsample(self, input, factor, name):\n        if isinstance(factor, str):\n            sh = tf.shape(self.get_tensor(factor))[1:3]\n        else:\n            sh = tf.shape(input)[1:3] * factor\n        return tf.image.resize_bilinear(input, sh, align_corners=False, name=name)\n\n    @layer\n    def separable_conv(self, input, k_h, k_w, c_o, stride, name, relu=True, set_bias=True):\n        with slim.arg_scope([slim.batch_norm], decay=0.999, fused=common.batchnorm_fused, is_training=self.trainable):\n            output = slim.separable_convolution2d(input,\n                                                  num_outputs=None,\n                                                  stride=stride,\n                                                  trainable=self.trainable,\n                                                  depth_multiplier=1.0,\n                                                  kernel_size=[k_h, k_w],\n                                                  # activation_fn=common.activation_fn if relu else None,\n                                                  activation_fn=None,\n                                                  # normalizer_fn=slim.batch_norm,\n                                                  weights_initializer=_init_xavier,\n                                                  # weights_initializer=_init_norm,\n                                                  weights_regularizer=_l2_regularizer_00004,\n                                                  biases_initializer=None,\n                                                  padding=DEFAULT_PADDING,\n                                                  scope=name + \'_depthwise\')\n\n            output = slim.convolution2d(output,\n                                        c_o,\n                                        stride=1,\n                                        kernel_size=[1, 1],\n                                        activation_fn=common.activation_fn if relu else None,\n                                        weights_initializer=_init_xavier,\n                                        # weights_initializer=_init_norm,\n                                        biases_initializer=_init_zero if set_bias else None,\n                                        normalizer_fn=slim.batch_norm,\n                                        trainable=self.trainable,\n                                        weights_regularizer=None,\n                                        scope=name + \'_pointwise\')\n\n        return output\n\n    @layer\n    def convb(self, input, k_h, k_w, c_o, stride, name, relu=True, set_bias=True, set_tanh=False):\n        with slim.arg_scope([slim.batch_norm], decay=0.999, fused=common.batchnorm_fused, is_training=self.trainable):\n            output = slim.convolution2d(input, c_o, kernel_size=[k_h, k_w],\n                                        stride=stride,\n                                        normalizer_fn=slim.batch_norm,\n                                        weights_regularizer=_l2_regularizer_convb,\n                                        weights_initializer=_init_xavier,\n                                        # weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n                                        biases_initializer=_init_zero if set_bias else None,\n                                        trainable=self.trainable,\n                                        activation_fn=common.activation_fn if relu else None,\n                                        scope=name)\n            if set_tanh:\n                output = tf.nn.sigmoid(output, name=name + \'_extra_acv\')\n        return output\n\n    @layer\n    def conv(self,\n             input,\n             k_h,\n             k_w,\n             c_o,\n             s_h,\n             s_w,\n             name,\n             relu=True,\n             padding=DEFAULT_PADDING,\n             group=1,\n             trainable=True,\n             biased=True):\n        # Verify that the padding is acceptable\n        self.validate_padding(padding)\n        # Get the number of channels in the input\n        c_i = int(input.get_shape()[-1])\n        # Verify that the grouping parameter is valid\n        assert c_i % group == 0\n        assert c_o % group == 0\n        # Convolution for a given input and kernel\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n            kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i / group, c_o], trainable=self.trainable & trainable)\n            if group == 1:\n                # This is the common-case. Convolve the input without any further complications.\n                output = convolve(input, kernel)\n            else:\n                # Split the input into groups and then convolve each of them independently\n                input_groups = tf.split(3, group, input)\n                kernel_groups = tf.split(3, group, kernel)\n                output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n                # Concatenate the groups\n                output = tf.concat(3, output_groups)\n            # Add the biases\n            if biased:\n                biases = self.make_var(\'biases\', [c_o], trainable=self.trainable & trainable)\n                output = tf.nn.bias_add(output, biases)\n\n            if relu:\n                # ReLU non-linearity\n                output = tf.nn.relu(output, name=scope.name)\n            return output\n\n    @layer\n    def relu(self, input, name):\n        return tf.nn.relu(input, name=name)\n\n    @layer\n    def max_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def avg_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.avg_pool(input,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def lrn(self, input, radius, alpha, beta, name, bias=1.0):\n        return tf.nn.local_response_normalization(input,\n                                                  depth_radius=radius,\n                                                  alpha=alpha,\n                                                  beta=beta,\n                                                  bias=bias,\n                                                  name=name)\n\n    @layer\n    def concat(self, inputs, axis, name):\n        return tf.concat(axis=axis, values=inputs, name=name)\n\n    @layer\n    def add(self, inputs, name):\n        return tf.add_n(inputs, name=name)\n\n    @layer\n    def fc(self, input, num_out, name, relu=True):\n        with tf.variable_scope(name) as scope:\n            input_shape = input.get_shape()\n            if input_shape.ndims == 4:\n                # The input is spatial. Vectorize it first.\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= d\n                feed_in = tf.reshape(input, [-1, dim])\n            else:\n                feed_in, dim = (input, input_shape[-1].value)\n            weights = self.make_var(\'weights\', shape=[dim, num_out])\n            biases = self.make_var(\'biases\', [num_out])\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=scope.name)\n            return fc\n\n    @layer\n    def softmax(self, input, name):\n        input_shape = map(lambda v: v.value, input.get_shape())\n        if len(input_shape) > 2:\n            # For certain models (like NiN), the singleton spatial dimensions\n            # need to be explicitly squeezed, since they\'re not broadcast-able\n            # in TensorFlow\'s NHWC ordering (unlike Caffe\'s NCHW).\n            if input_shape[1] == 1 and input_shape[2] == 1:\n                input = tf.squeeze(input, squeeze_dims=[1, 2])\n            else:\n                raise ValueError(\'Rank 2 tensor input expected for softmax!\')\n        return tf.nn.softmax(input, name=name)\n\n    @layer\n    def batch_normalization(self, input, name, scale_offset=True, relu=False):\n        # NOTE: Currently, only inference is supported\n        with tf.variable_scope(name) as scope:\n            shape = [input.get_shape()[-1]]\n            if scale_offset:\n                scale = self.make_var(\'scale\', shape=shape)\n                offset = self.make_var(\'offset\', shape=shape)\n            else:\n                scale, offset = (None, None)\n            output = tf.nn.batch_normalization(\n                input,\n                mean=self.make_var(\'mean\', shape=shape),\n                variance=self.make_var(\'variance\', shape=shape),\n                offset=offset,\n                scale=scale,\n                # TODO: This is the default Caffe batch norm eps\n                # Get the actual eps from parameters\n                variance_epsilon=1e-5,\n                name=name)\n            if relu:\n                output = tf.nn.relu(output)\n            return output\n\n    @layer\n    def dropout(self, input, keep_prob, name):\n        keep = 1 - self.use_dropout + (self.use_dropout * keep_prob)\n        return tf.nn.dropout(input, keep, name=name)\n\n    @layer\n    def se_block(self, input_feature, name, ratio=8):\n        """"""Contains the implementation of Squeeze-and-Excitation block.\n        As described in https://arxiv.org/abs/1709.01507.\n        ref : https://github.com/kobiso/SENet-tensorflow-slim/blob/master/nets/attention_module.py\n        """"""\n\n        kernel_initializer = tf.contrib.layers.variance_scaling_initializer()\n        bias_initializer = tf.constant_initializer(value=0.0)\n\n        with tf.variable_scope(name):\n            channel = input_feature.get_shape()[-1]\n            # Global average pooling\n            squeeze = tf.reduce_mean(input_feature, axis=[1, 2], keepdims=True)\n            excitation = tf.layers.dense(inputs=squeeze,\n                                         units=channel // ratio,\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=kernel_initializer,\n                                         bias_initializer=bias_initializer,\n                                         name=\'bottleneck_fc\')\n            excitation = tf.layers.dense(inputs=excitation,\n                                         units=channel,\n                                         activation=tf.nn.sigmoid,\n                                         kernel_initializer=kernel_initializer,\n                                         bias_initializer=bias_initializer,\n                                         name=\'recover_fc\')\n            scale = input_feature * excitation\n        return scale\n'"
tf_pose/network_cmu.py,1,"b""from __future__ import absolute_import\n\nfrom tf_pose import network_base\nimport tensorflow as tf\n\n\nclass CmuNetwork(network_base.BaseNetwork):\n    def setup(self):\n        (self.feed('image')\n             .normalize_vgg(name='preprocess')\n             .conv(3, 3, 64, 1, 1, name='conv1_1')\n             .conv(3, 3, 64, 1, 1, name='conv1_2')\n             .max_pool(2, 2, 2, 2, name='pool1_stage1', padding='VALID')\n             .conv(3, 3, 128, 1, 1, name='conv2_1')\n             .conv(3, 3, 128, 1, 1, name='conv2_2')\n             .max_pool(2, 2, 2, 2, name='pool2_stage1', padding='VALID')\n             .conv(3, 3, 256, 1, 1, name='conv3_1')\n             .conv(3, 3, 256, 1, 1, name='conv3_2')\n             .conv(3, 3, 256, 1, 1, name='conv3_3')\n             .conv(3, 3, 256, 1, 1, name='conv3_4')\n             .max_pool(2, 2, 2, 2, name='pool3_stage1', padding='VALID')\n             .conv(3, 3, 512, 1, 1, name='conv4_1')\n             .conv(3, 3, 512, 1, 1, name='conv4_2')\n             .conv(3, 3, 256, 1, 1, name='conv4_3_CPM')\n             .conv(3, 3, 128, 1, 1, name='conv4_4_CPM')          # *****\n\n             .conv(3, 3, 128, 1, 1, name='conv5_1_CPM_L1')\n             .conv(3, 3, 128, 1, 1, name='conv5_2_CPM_L1')\n             .conv(3, 3, 128, 1, 1, name='conv5_3_CPM_L1')\n             .conv(1, 1, 512, 1, 1, name='conv5_4_CPM_L1')\n             .conv(1, 1, 38, 1, 1, relu=False, name='conv5_5_CPM_L1'))\n\n        (self.feed('conv4_4_CPM')\n             .conv(3, 3, 128, 1, 1, name='conv5_1_CPM_L2')\n             .conv(3, 3, 128, 1, 1, name='conv5_2_CPM_L2')\n             .conv(3, 3, 128, 1, 1, name='conv5_3_CPM_L2')\n             .conv(1, 1, 512, 1, 1, name='conv5_4_CPM_L2')\n             .conv(1, 1, 19, 1, 1, relu=False, name='conv5_5_CPM_L2'))\n\n        (self.feed('conv5_5_CPM_L1',\n                   'conv5_5_CPM_L2',\n                   'conv4_4_CPM')\n             .concat(3, name='concat_stage2')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage2_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage2_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage2_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage2_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage2_L1')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage2_L1')\n             .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage2_L1'))\n\n        (self.feed('concat_stage2')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage2_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage2_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage2_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage2_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage2_L2')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage2_L2')\n             .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage2_L2'))\n\n        (self.feed('Mconv7_stage2_L1',\n                   'Mconv7_stage2_L2',\n                   'conv4_4_CPM')\n             .concat(3, name='concat_stage3')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage3_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage3_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage3_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage3_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage3_L1')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage3_L1')\n             .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage3_L1'))\n\n        (self.feed('concat_stage3')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage3_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage3_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage3_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage3_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage3_L2')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage3_L2')\n             .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage3_L2'))\n\n        (self.feed('Mconv7_stage3_L1',\n                   'Mconv7_stage3_L2',\n                   'conv4_4_CPM')\n             .concat(3, name='concat_stage4')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage4_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage4_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage4_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage4_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage4_L1')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage4_L1')\n             .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage4_L1'))\n\n        (self.feed('concat_stage4')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage4_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage4_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage4_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage4_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage4_L2')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage4_L2')\n             .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage4_L2'))\n\n        (self.feed('Mconv7_stage4_L1',\n                   'Mconv7_stage4_L2',\n                   'conv4_4_CPM')\n             .concat(3, name='concat_stage5')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage5_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage5_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage5_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage5_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage5_L1')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage5_L1')\n             .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage5_L1'))\n\n        (self.feed('concat_stage5')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage5_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage5_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage5_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage5_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage5_L2')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage5_L2')\n             .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage5_L2'))\n\n        (self.feed('Mconv7_stage5_L1',\n                   'Mconv7_stage5_L2',\n                   'conv4_4_CPM')\n             .concat(3, name='concat_stage6')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage6_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage6_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage6_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage6_L1')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage6_L1')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage6_L1')\n             .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage6_L1'))\n\n        (self.feed('concat_stage6')\n             .conv(7, 7, 128, 1, 1, name='Mconv1_stage6_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv2_stage6_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv3_stage6_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv4_stage6_L2')\n             .conv(7, 7, 128, 1, 1, name='Mconv5_stage6_L2')\n             .conv(1, 1, 128, 1, 1, name='Mconv6_stage6_L2')\n             .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage6_L2'))\n\n        with tf.variable_scope('Openpose'):\n            (self.feed('Mconv7_stage6_L2',\n                       'Mconv7_stage6_L1')\n                 .concat(3, name='concat_stage7'))\n\n    def loss_l1_l2(self):\n         l1s = []\n         l2s = []\n         for layer_name in self.layers.keys():\n              if 'Mconv7' in layer_name and '_L1' in layer_name:\n                   l1s.append(self.layers[layer_name])\n              if 'Mconv7' in layer_name and '_L2' in layer_name:\n                   l2s.append(self.layers[layer_name])\n\n         return l1s, l2s\n\n    def loss_last(self):\n         return self.get_output('Mconv7_stage6_L1'), self.get_output('Mconv7_stage6_L2')\n\n    def restorable_variables(self):\n         return None\n"""
tf_pose/network_dsconv.py,0,"b""from __future__ import absolute_import\n\nfrom tf_pose import network_base\n\n\nclass DSConvNetwork(network_base.BaseNetwork):\n    def __init__(self, inputs, trainable=True, conv_width=1.0):\n        self.conv_width = conv_width\n        network_base.BaseNetwork.__init__(self, inputs, trainable)\n\n    def setup(self):\n        (self.feed('image')\n         .conv(3, 3, 64, 1, 1, name='conv1_1', trainable=False)\n         # .conv(3, 3, 64, 1, 1, name='conv1_2', trainable=True)     # TODO\n         .separable_conv(3, 3, round(self.conv_width * 64), 2, name='conv1_2')\n         # .max_pool(2, 2, 2, 2, name='pool1_stage1')\n         .separable_conv(3, 3, round(self.conv_width * 128), 1, name='conv2_1')\n         .separable_conv(3, 3, round(self.conv_width * 128), 2, name='conv2_2')\n         # .max_pool(2, 2, 2, 2, name='pool2_stage1')\n         .separable_conv(3, 3, round(self.conv_width * 256), 1, name='conv3_1')\n         .separable_conv(3, 3, round(self.conv_width * 256), 1, name='conv3_2')\n         .separable_conv(3, 3, round(self.conv_width * 256), 1, name='conv3_3')\n         .separable_conv(3, 3, round(self.conv_width * 256), 2, name='conv3_4')\n         # .max_pool(2, 2, 2, 2, name='pool3_stage1')\n         .separable_conv(3, 3, round(self.conv_width * 512), 1, name='conv4_1')\n         .separable_conv(3, 3, round(self.conv_width * 512), 1, name='conv4_2')\n         .separable_conv(3, 3, round(self.conv_width * 256), 1, name='conv4_3_CPM')\n         .separable_conv(3, 3, 128, 1, name='conv4_4_CPM')\n         .separable_conv(3, 3, round(self.conv_width * 128), 1, name='conv5_1_CPM_L1')\n         .separable_conv(3, 3, round(self.conv_width * 128), 1, name='conv5_2_CPM_L1')\n         .separable_conv(3, 3, round(self.conv_width * 128), 1, name='conv5_3_CPM_L1')\n         .conv(1, 1, 512, 1, 1, name='conv5_4_CPM_L1')\n         .conv(1, 1, 38, 1, 1, relu=False, name='conv5_5_CPM_L1'))\n\n        (self.feed('conv4_4_CPM')\n         .separable_conv(3, 3, round(self.conv_width * 128), 1, name='conv5_1_CPM_L2')\n         .separable_conv(3, 3, round(self.conv_width * 128), 1, name='conv5_2_CPM_L2')\n         .separable_conv(3, 3, round(self.conv_width * 128), 1, name='conv5_3_CPM_L2')\n         .conv(1, 1, 512, 1, 1, name='conv5_4_CPM_L2')\n         .conv(1, 1, 19, 1, 1, relu=False, name='conv5_5_CPM_L2'))\n\n        (self.feed('conv5_5_CPM_L1',\n                   'conv5_5_CPM_L2',\n                   'conv4_4_CPM')\n         .concat(3, name='concat_stage2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage2_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage2_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage2_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage2_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage2_L1')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage2_L1')\n         .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage2_L1'))\n\n        (self.feed('concat_stage2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage2_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage2_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage2_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage2_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage2_L2')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage2_L2')\n         .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage2_L2'))\n\n        (self.feed('Mconv7_stage2_L1',\n                   'Mconv7_stage2_L2',\n                   'conv4_4_CPM')\n         .concat(3, name='concat_stage3')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage3_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage3_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage3_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage3_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage3_L1')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage3_L1')\n         .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage3_L1'))\n\n        (self.feed('concat_stage3')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage3_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage3_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage3_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage3_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage3_L2')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage3_L2')\n         .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage3_L2'))\n\n        (self.feed('Mconv7_stage3_L1',\n                   'Mconv7_stage3_L2',\n                   'conv4_4_CPM')\n         .concat(3, name='concat_stage4')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage4_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage4_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage4_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage4_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage4_L1')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage4_L1')\n         .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage4_L1'))\n\n        (self.feed('concat_stage4')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage4_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage4_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage4_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage4_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage4_L2')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage4_L2')\n         .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage4_L2'))\n\n        (self.feed('Mconv7_stage4_L1',\n                   'Mconv7_stage4_L2',\n                   'conv4_4_CPM')\n         .concat(3, name='concat_stage5')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage5_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage5_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage5_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage5_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage5_L1')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage5_L1')\n         .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage5_L1'))\n\n        (self.feed('concat_stage5')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage5_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage5_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage5_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage5_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage5_L2')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage5_L2')\n         .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage5_L2'))\n\n        (self.feed('Mconv7_stage5_L1',\n                   'Mconv7_stage5_L2',\n                   'conv4_4_CPM')\n         .concat(3, name='concat_stage6')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage6_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage6_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage6_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage6_L1')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage6_L1')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage6_L1')\n         .conv(1, 1, 38, 1, 1, relu=False, name='Mconv7_stage6_L1'))\n\n        (self.feed('concat_stage6')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv1_stage6_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv2_stage6_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv3_stage6_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv4_stage6_L2')\n         .separable_conv(7, 7, round(self.conv_width * 128), 1, name='Mconv5_stage6_L2')\n         .conv(1, 1, 128, 1, 1, name='Mconv6_stage6_L2')\n         .conv(1, 1, 19, 1, 1, relu=False, name='Mconv7_stage6_L2'))\n\n        (self.feed('Mconv7_stage6_L2',\n                   'Mconv7_stage6_L1')\n         .concat(3, name='concat_stage7'))\n"""
tf_pose/network_mobilenet.py,3,"b""from __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom tf_pose import network_base\n\n\nclass MobilenetNetwork(network_base.BaseNetwork):\n    def __init__(self, inputs, trainable=True, conv_width=1.0, conv_width2=None):\n        self.conv_width = conv_width\n        self.conv_width2 = conv_width2 if conv_width2 else conv_width\n        self.num_refine = 4\n        network_base.BaseNetwork.__init__(self, inputs, trainable)\n\n    def setup(self):\n        min_depth = 8\n        depth = lambda d: max(int(d * self.conv_width), min_depth)\n        depth2 = lambda d: max(int(d * self.conv_width2), min_depth)\n\n        with tf.variable_scope(None, 'MobilenetV1'):\n            (self.feed('image')\n             .convb(3, 3, depth(32), 2, name='Conv2d_0')\n             .separable_conv(3, 3, depth(64), 1, name='Conv2d_1')\n             .separable_conv(3, 3, depth(128), 2, name='Conv2d_2')\n             .separable_conv(3, 3, depth(128), 1, name='Conv2d_3')\n             .separable_conv(3, 3, depth(256), 2, name='Conv2d_4')\n             .separable_conv(3, 3, depth(256), 1, name='Conv2d_5')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_6')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_7')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_8')\n             # .separable_conv(3, 3, depth(512), 1, name='Conv2d_9')\n             # .separable_conv(3, 3, depth(512), 1, name='Conv2d_10')\n             # .separable_conv(3, 3, depth(512), 1, name='Conv2d_11')\n             # .separable_conv(3, 3, depth(1024), 2, name='Conv2d_12')\n             # .separable_conv(3, 3, depth(1024), 1, name='Conv2d_13')\n             )\n\n        (self.feed('Conv2d_1').max_pool(2, 2, 2, 2, name='Conv2d_1_pool'))\n        (self.feed('Conv2d_7').upsample(2, name='Conv2d_7_upsample'))\n\n        (self.feed('Conv2d_1_pool', 'Conv2d_3', 'Conv2d_7_upsample')\n            .concat(3, name='feat_concat'))\n\n        feature_lv = 'feat_concat'\n        with tf.variable_scope(None, 'Openpose'):\n            prefix = 'MConv_Stage1'\n            (self.feed(feature_lv)\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_1')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_2')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_3')\n             .separable_conv(1, 1, depth2(512), 1, name=prefix + '_L1_4')\n             .separable_conv(1, 1, 38, 1, relu=False, name=prefix + '_L1_5'))\n\n            (self.feed(feature_lv)\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_1')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_2')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_3')\n             .separable_conv(1, 1, depth2(512), 1, name=prefix + '_L2_4')\n             .separable_conv(1, 1, 19, 1, relu=False, name=prefix + '_L2_5'))\n\n            for stage_id in range(self.num_refine):\n                prefix_prev = 'MConv_Stage%d' % (stage_id + 1)\n                prefix = 'MConv_Stage%d' % (stage_id + 2)\n                (self.feed(prefix_prev + '_L1_5',\n                           prefix_prev + '_L2_5',\n                           feature_lv)\n                 .concat(3, name=prefix + '_concat')\n                 .separable_conv(7, 7, depth2(128), 1, name=prefix + '_L1_1')\n                 .separable_conv(7, 7, depth2(128), 1, name=prefix + '_L1_2')\n                 .separable_conv(7, 7, depth2(128), 1, name=prefix + '_L1_3')\n                 .separable_conv(1, 1, depth2(128), 1, name=prefix + '_L1_4')\n                 .separable_conv(1, 1, 38, 1, relu=False, name=prefix + '_L1_5'))\n\n                (self.feed(prefix + '_concat')\n                 .separable_conv(7, 7, depth2(128), 1, name=prefix + '_L2_1')\n                 .separable_conv(7, 7, depth2(128), 1, name=prefix + '_L2_2')\n                 .separable_conv(7, 7, depth2(128), 1, name=prefix + '_L2_3')\n                 .separable_conv(1, 1, depth2(128), 1, name=prefix + '_L2_4')\n                 .separable_conv(1, 1, 19, 1, relu=False, name=prefix + '_L2_5'))\n\n            # final result\n            (self.feed('MConv_Stage%d_L2_5' % self.get_refine_num(),\n                       'MConv_Stage%d_L1_5' % self.get_refine_num())\n             .concat(3, name='concat_stage7'))\n\n    def loss_l1_l2(self):\n        l1s = []\n        l2s = []\n        for layer_name in sorted(self.layers.keys()):\n            if '_L1_5' in layer_name:\n                l1s.append(self.layers[layer_name])\n            if '_L2_5' in layer_name:\n                l2s.append(self.layers[layer_name])\n\n        return l1s, l2s\n\n    def loss_last(self):\n        return self.get_output('MConv_Stage%d_L1_5' % self.get_refine_num()), \\\n               self.get_output('MConv_Stage%d_L2_5' % self.get_refine_num())\n\n    def restorable_variables(self):\n        vs = {v.op.name: v for v in tf.global_variables() if\n              'MobilenetV1/Conv2d' in v.op.name and\n              'RMSProp' not in v.op.name and 'Momentum' not in v.op.name and 'Ada' not in v.op.name\n              }\n        return vs\n\n    def get_refine_num(self):\n        return self.num_refine + 1\n"""
tf_pose/network_mobilenet_thin.py,3,"b""from __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom tf_pose import network_base\n\n\nclass MobilenetNetworkThin(network_base.BaseNetwork):\n    def __init__(self, inputs, trainable=True, conv_width=1.0, conv_width2=None):\n        self.conv_width = conv_width\n        self.conv_width2 = conv_width2 if conv_width2 else conv_width\n        network_base.BaseNetwork.__init__(self, inputs, trainable)\n\n    def setup(self):\n        min_depth = 8\n        depth = lambda d: max(int(d * self.conv_width), min_depth)\n        depth2 = lambda d: max(int(d * self.conv_width2), min_depth)\n\n        with tf.variable_scope(None, 'MobilenetV1'):\n            (self.feed('image')\n             .convb(3, 3, depth(32), 2, name='Conv2d_0')\n             .separable_conv(3, 3, depth(64), 1, name='Conv2d_1')\n             .separable_conv(3, 3, depth(128), 2, name='Conv2d_2')\n             .separable_conv(3, 3, depth(128), 1, name='Conv2d_3')\n             .separable_conv(3, 3, depth(256), 2, name='Conv2d_4')\n             .separable_conv(3, 3, depth(256), 1, name='Conv2d_5')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_6')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_7')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_8')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_9')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_10')\n             .separable_conv(3, 3, depth(512), 1, name='Conv2d_11')\n             # .separable_conv(3, 3, depth(1024), 2, name='Conv2d_12')\n             # .separable_conv(3, 3, depth(1024), 1, name='Conv2d_13')\n             )\n\n        (self.feed('Conv2d_3').max_pool(2, 2, 2, 2, name='Conv2d_3_pool'))\n\n        (self.feed('Conv2d_3_pool', 'Conv2d_7', 'Conv2d_11')\n         .concat(3, name='feat_concat'))\n\n        feature_lv = 'feat_concat'\n        with tf.variable_scope(None, 'Openpose'):\n            prefix = 'MConv_Stage1'\n            (self.feed(feature_lv)\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_1')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_2')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_3')\n             .separable_conv(1, 1, depth2(512), 1, name=prefix + '_L1_4')\n             .separable_conv(1, 1, 38, 1, relu=False, name=prefix + '_L1_5'))\n\n            (self.feed(feature_lv)\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_1')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_2')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_3')\n             .separable_conv(1, 1, depth2(512), 1, name=prefix + '_L2_4')\n             .separable_conv(1, 1, 19, 1, relu=False, name=prefix + '_L2_5'))\n\n            for stage_id in range(5):\n                prefix_prev = 'MConv_Stage%d' % (stage_id + 1)\n                prefix = 'MConv_Stage%d' % (stage_id + 2)\n                (self.feed(prefix_prev + '_L1_5',\n                           prefix_prev + '_L2_5',\n                           feature_lv)\n                 .concat(3, name=prefix + '_concat')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_1')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_2')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_3')\n                 .separable_conv(1, 1, depth2(128), 1, name=prefix + '_L1_4')\n                 .separable_conv(1, 1, 38, 1, relu=False, name=prefix + '_L1_5'))\n\n                (self.feed(prefix + '_concat')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_1')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_2')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_3')\n                 .separable_conv(1, 1, depth2(128), 1, name=prefix + '_L2_4')\n                 .separable_conv(1, 1, 19, 1, relu=False, name=prefix + '_L2_5'))\n\n            # final result\n            (self.feed('MConv_Stage6_L2_5',\n                       'MConv_Stage6_L1_5')\n             .concat(3, name='concat_stage7'))\n\n    def loss_l1_l2(self):\n        l1s = []\n        l2s = []\n        for layer_name in sorted(self.layers.keys()):\n            if '_L1_5' in layer_name:\n                l1s.append(self.layers[layer_name])\n            if '_L2_5' in layer_name:\n                l2s.append(self.layers[layer_name])\n\n        return l1s, l2s\n\n    def loss_last(self):\n        return self.get_output('MConv_Stage6_L1_5'), self.get_output('MConv_Stage6_L2_5')\n\n    def restorable_variables(self):\n        vs = {v.op.name: v for v in tf.global_variables() if\n              'MobilenetV1/Conv2d' in v.op.name and\n              # 'global_step' not in v.op.name and\n              # 'beta1_power' not in v.op.name and 'beta2_power' not in v.op.name and\n              'RMSProp' not in v.op.name and 'Momentum' not in v.op.name and\n              'Ada' not in v.op.name and 'Adam' not in v.op.name\n              }\n        return vs\n"""
tf_pose/network_mobilenet_v2.py,4,"b""from __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom tf_pose import network_base\nfrom tf_pose.mobilenet import mobilenet_v2\nfrom tf_pose.network_base import layer\n\n\nclass Mobilenetv2Network(network_base.BaseNetwork):\n    def __init__(self, inputs, trainable=True, conv_width=1.0, conv_width2=1.0):\n        self.conv_width = conv_width\n        self.refine_width = conv_width2\n        network_base.BaseNetwork.__init__(self, inputs, trainable)\n\n    @layer\n    def base(self, input, name):\n        with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n            net, endpoints = mobilenet_v2.mobilenet_base(input, self.conv_width, finegrain_classification_mode=(self.conv_width < 1.0))\n            for k, tensor in sorted(list(endpoints.items()), key=lambda x: x[0]):\n                self.layers['%s/%s' % (name, k)] = tensor\n                # print(k, tensor.shape)\n            return net\n\n    def setup(self):\n        depth2 = lambda x: int(x * self.refine_width)\n\n        self.feed('image').base(name='base')\n\n        # TODO : add more feature with downsample?\n        # self.feed('base/layer_4/output').max_pool(2, 2, 2, 2, name='base/layer_4/output/downsample')\n        # self.feed('base/layer_4/output').avg_pool(2, 2, 2, 2, name='base/layer_4/output/downsample')\n        self.feed('base/layer_14/output').upsample(factor='base/layer_7/output', name='base/layer_14/output/upsample')\n        (self.feed(\n            'base/layer_7/output',\n            'base/layer_14/output/upsample',\n            # 'base/layer_4/output/downsample'\n        ).concat(3, name='feat_concat'))\n\n        feature_lv = 'feat_concat'\n        with tf.variable_scope(None, 'Openpose'):\n            prefix = 'MConv_Stage1'\n            (self.feed(feature_lv)\n             # .se_block(name=prefix + '_L1_se', ratio=8)\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_1')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_2')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_3')\n             .separable_conv(1, 1, depth2(512), 1, name=prefix + '_L1_4')\n             .separable_conv(1, 1, 38, 1, relu=False, name=prefix + '_L1_5'))\n\n            (self.feed(feature_lv)\n             # .se_block(name=prefix + '_L2_se', ratio=8)\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_1')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_2')\n             .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_3')\n             .separable_conv(1, 1, depth2(512), 1, name=prefix + '_L2_4')\n             .separable_conv(1, 1, 19, 1, relu=False, name=prefix + '_L2_5'))\n\n            for stage_id in range(5):\n                prefix_prev = 'MConv_Stage%d' % (stage_id + 1)\n                prefix = 'MConv_Stage%d' % (stage_id + 2)\n                (self.feed(prefix_prev + '_L1_5',\n                           prefix_prev + '_L2_5',\n                           feature_lv)\n                 .concat(3, name=prefix + '_concat')\n                 # .se_block(name=prefix + '_L1_se', ratio=8)\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_1')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_2')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L1_3')\n                 .separable_conv(1, 1, depth2(128), 1, name=prefix + '_L1_4')\n                 .separable_conv(1, 1, 38, 1, relu=False, name=prefix + '_L1_5'))\n\n                (self.feed(prefix + '_concat')\n                 # .se_block(name=prefix + '_L2_se', ratio=8)\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_1')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_2')\n                 .separable_conv(3, 3, depth2(128), 1, name=prefix + '_L2_3')\n                 .separable_conv(1, 1, depth2(128), 1, name=prefix + '_L2_4')\n                 .separable_conv(1, 1, 19, 1, relu=False, name=prefix + '_L2_5'))\n\n            # final result\n            (self.feed('MConv_Stage6_L2_5',\n                       'MConv_Stage6_L1_5')\n             .concat(3, name='concat_stage7'))\n\n    def loss_l1_l2(self):\n        l1s = []\n        l2s = []\n        for layer_name in sorted(self.layers.keys()):\n            if '_L1_5' in layer_name:\n                l1s.append(self.layers[layer_name])\n            if '_L2_5' in layer_name:\n                l2s.append(self.layers[layer_name])\n\n        return l1s, l2s\n\n    def loss_last(self):\n        return self.get_output('MConv_Stage6_L1_5'), self.get_output('MConv_Stage6_L2_5')\n\n    def restorable_variables(self, only_backbone=True):\n        vs = {v.op.name: v for v in tf.global_variables() if\n              ('MobilenetV2' in v.op.name or (only_backbone is False and 'Openpose' in v.op.name)) and\n              # 'global_step' not in v.op.name and\n              # 'beta1_power' not in v.op.name and 'beta2_power' not in v.op.name and\n              'quant' not in v.op.name and\n              'RMSProp' not in v.op.name and 'Momentum' not in v.op.name and\n              'Ada' not in v.op.name and 'Adam' not in v.op.name\n              }\n        # print(set([v.op.name for v in tf.global_variables()]) - set(list(vs.keys())))\n        return vs\n"""
tf_pose/networks.py,1,"b""import os\nfrom os.path import dirname, abspath\n\nimport tensorflow as tf\n\nfrom tf_pose.network_mobilenet import MobilenetNetwork\nfrom tf_pose.network_mobilenet_thin import MobilenetNetworkThin\n\nfrom tf_pose.network_cmu import CmuNetwork\nfrom tf_pose.network_mobilenet_v2 import Mobilenetv2Network\n\n\ndef _get_base_path():\n    if not os.environ.get('OPENPOSE_MODEL', ''):\n        return './models'\n    return os.environ.get('OPENPOSE_MODEL')\n\n\ndef get_network(type, placeholder_input, sess_for_load=None, trainable=True):\n    if type == 'mobilenet':\n        net = MobilenetNetwork({'image': placeholder_input}, conv_width=0.75, conv_width2=1.00, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v1_0.75_224_2017_06_14/mobilenet_v1_0.75_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_fast':\n        net = MobilenetNetwork({'image': placeholder_input}, conv_width=0.5, conv_width2=0.5, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v1_0.75_224_2017_06_14/mobilenet_v1_0.75_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_accurate':\n        net = MobilenetNetwork({'image': placeholder_input}, conv_width=1.00, conv_width2=1.00, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v1_1.0_224_2017_06_14/mobilenet_v1_1.0_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n\n    elif type == 'mobilenet_thin':\n        net = MobilenetNetworkThin({'image': placeholder_input}, conv_width=0.75, conv_width2=0.50, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v1_0.75_224_2017_06_14/mobilenet_v1_0.75_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n\n    elif type in ['mobilenet_v2_w1.4_r1.0', 'mobilenet_v2_large', 'mobilenet_v2_large_quantize']:       # m_v2_large\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=1.4, conv_width2=1.0, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_1.4_224/mobilenet_v2_1.4_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_w1.4_r0.5':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=1.4, conv_width2=0.5, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_1.4_224/mobilenet_v2_1.4_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_w1.0_r1.0':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=1.0, conv_width2=1.0, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_w1.0_r0.75':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=1.0, conv_width2=0.75, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_w1.0_r0.5':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=1.0, conv_width2=0.5, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_w0.75_r0.75':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=0.75, conv_width2=0.75, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_0.75_224/mobilenet_v2_0.75_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_w0.5_r0.5' or type == 'mobilenet_v2_small':                                # m_v2_fast\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=0.5, conv_width2=0.5, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_0.5_224/mobilenet_v2_0.5_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n\n    elif type == 'mobilenet_v2_1.4':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=1.4, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_1.4_224/mobilenet_v2_1.4_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_1.0':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=1.0, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_0.75':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=0.75, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_0.75_224/mobilenet_v2_0.75_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n    elif type == 'mobilenet_v2_0.5':\n        net = Mobilenetv2Network({'image': placeholder_input}, conv_width=0.5, trainable=trainable)\n        pretrain_path = 'pretrained/mobilenet_v2_0.5_224/mobilenet_v2_0.5_224.ckpt'\n        last_layer = 'MConv_Stage6_L{aux}_5'\n\n    elif type in ['cmu', 'openpose']:\n        net = CmuNetwork({'image': placeholder_input}, trainable=trainable)\n        pretrain_path = 'numpy/openpose_coco.npy'\n        last_layer = 'Mconv7_stage6_L{aux}'\n    elif type in ['cmu_quantize', 'openpose_quantize']:\n        net = CmuNetwork({'image': placeholder_input}, trainable=trainable)\n        pretrain_path = 'train/cmu/bs8_lr0.0001_q_e80/model_latest-18000'\n        last_layer = 'Mconv7_stage6_L{aux}'\n    elif type == 'vgg':\n        net = CmuNetwork({'image': placeholder_input}, trainable=trainable)\n        pretrain_path = 'numpy/openpose_vgg16.npy'\n        last_layer = 'Mconv7_stage6_L{aux}'\n\n    else:\n        raise Exception('Invalid Model Name.')\n\n    pretrain_path_full = os.path.join(_get_base_path(), pretrain_path)\n    if sess_for_load is not None:\n        if type in ['cmu', 'vgg', 'openpose']:\n            if not os.path.isfile(pretrain_path_full):\n                raise Exception('Model file doesn\\'t exist, path=%s' % pretrain_path_full)\n            net.load(os.path.join(_get_base_path(), pretrain_path), sess_for_load)\n        else:\n            try:\n                s = '%dx%d' % (placeholder_input.shape[2], placeholder_input.shape[1])\n            except:\n                s = ''\n            ckpts = {\n                'mobilenet': 'trained/mobilenet_%s/model-246038' % s,\n                'mobilenet_thin': 'trained/mobilenet_thin_%s/model-449003' % s,\n                'mobilenet_fast': 'trained/mobilenet_fast_%s/model-189000' % s,\n                'mobilenet_accurate': 'trained/mobilenet_accurate/model-170000',\n                'mobilenet_v2_w1.4_r0.5': 'trained/mobilenet_v2_w1.4_r0.5/model_latest-380401',\n                'mobilenet_v2_large': 'trained/mobilenet_v2_w1.4_r1.0/model-570000',\n                'mobilenet_v2_small': 'trained/mobilenet_v2_w0.5_r0.5/model_latest-380401',\n            }\n            ckpt_path = os.path.join(_get_base_path(), ckpts[type])\n            loader = tf.train.Saver()\n            try:\n                loader.restore(sess_for_load, ckpt_path)\n            except Exception as e:\n                raise Exception('Fail to load model files. \\npath=%s\\nerr=%s' % (ckpt_path, str(e)))\n\n    return net, pretrain_path_full, last_layer\n\n\ndef get_graph_path(model_name):\n    dyn_graph_path = {\n        'cmu': 'graph/cmu/graph_opt.pb',\n        'openpose_quantize': 'graph/cmu/graph_opt_q.pb',\n        'mobilenet_thin': 'graph/mobilenet_thin/graph_opt.pb',\n        'mobilenet_v2_large': 'graph/mobilenet_v2_large/graph_opt.pb',\n        'mobilenet_v2_large_r0.5': 'graph/mobilenet_v2_large/graph_r0.5_opt.pb',\n        'mobilenet_v2_large_quantize': 'graph/mobilenet_v2_large/graph_opt_q.pb',\n        'mobilenet_v2_small': 'graph/mobilenet_v2_small/graph_opt.pb',\n    }\n\n    base_data_dir = dirname(dirname(abspath(__file__)))\n    if os.path.exists(os.path.join(base_data_dir, 'models')):\n        base_data_dir = os.path.join(base_data_dir, 'models')\n    else:\n        base_data_dir = os.path.join(base_data_dir, 'tf_pose_data')\n\n    graph_path = os.path.join(base_data_dir, dyn_graph_path[model_name])\n    if os.path.isfile(graph_path):\n        return graph_path\n\n    raise Exception('Graph file doesn\\'t exist, path=%s' % graph_path)\n\n\ndef model_wh(resolution_str):\n    width, height = map(int, resolution_str.split('x'))\n    if width % 16 != 0 or height % 16 != 0:\n        raise Exception('Width and height should be multiples of 16. w=%d, h=%d' % (width, height))\n    return int(width), int(height)\n"""
tf_pose/pose_augment.py,0,"b'import math\nimport random\n\nimport cv2\nimport numpy as np\nfrom tensorpack.dataflow.imgaug.geometry import RotationAndCropValid\n\nfrom tf_pose.common import CocoPart\n\n_network_w = 368\n_network_h = 368\n_scale = 2\n\n\ndef set_network_input_wh(w, h):\n    global _network_w, _network_h\n    _network_w, _network_h = w, h\n\n\ndef set_network_scale(scale):\n    global _scale\n    _scale = scale\n\n\ndef pose_random_scale(meta):\n    scalew = random.uniform(0.8, 1.2)\n    scaleh = random.uniform(0.8, 1.2)\n    neww = int(meta.width * scalew)\n    newh = int(meta.height * scaleh)\n    dst = cv2.resize(meta.img, (neww, newh), interpolation=cv2.INTER_AREA)\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0 or int(point[0] * scalew + 0.5) > neww or int(\n            #                         point[1] * scaleh + 0.5) > newh:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww, newh\n    meta.img = dst\n    return meta\n\n\ndef pose_resize_shortestedge_fixed(meta):\n    ratio_w = _network_w / meta.width\n    ratio_h = _network_h / meta.height\n    ratio = max(ratio_w, ratio_h)\n    return pose_resize_shortestedge(meta, int(min(meta.width * ratio + 0.5, meta.height * ratio + 0.5)))\n\n\ndef pose_resize_shortestedge_random(meta):\n    ratio_w = _network_w / meta.width\n    ratio_h = _network_h / meta.height\n    ratio = min(ratio_w, ratio_h)\n    target_size = int(min(meta.width * ratio + 0.5, meta.height * ratio + 0.5))\n    target_size = int(target_size * random.uniform(0.95, 1.6))\n    # target_size = int(min(_network_w, _network_h) * random.uniform(0.7, 1.5))\n    return pose_resize_shortestedge(meta, target_size)\n\n\ndef pose_resize_shortestedge(meta, target_size):\n    global _network_w, _network_h\n    img = meta.img\n\n    # adjust image\n    scale = target_size / min(meta.height, meta.width)\n    if meta.height < meta.width:\n        newh, neww = target_size, int(scale * meta.width + 0.5)\n    else:\n        newh, neww = int(scale * meta.height + 0.5), target_size\n\n    dst = cv2.resize(img, (neww, newh), interpolation=cv2.INTER_AREA)\n\n    pw = ph = 0\n    if neww < _network_w or newh < _network_h:\n        pw = max(0, (_network_w - neww) // 2)\n        ph = max(0, (_network_h - newh) // 2)\n        mw = (_network_w - neww) % 2\n        mh = (_network_h - newh) % 2\n        color = random.randint(0, 255)\n        dst = cv2.copyMakeBorder(dst, ph, ph+mh, pw, pw+mw, cv2.BORDER_CONSTANT, value=(color, 0, 0))\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0 or int(point[0]*scale+0.5) > neww or int(point[1]*scale+0.5) > newh:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((int(point[0]*scale+0.5) + pw, int(point[1]*scale+0.5) + ph))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww + pw * 2, newh + ph * 2\n    meta.img = dst\n    return meta\n\n\ndef pose_crop_center(meta):\n    global _network_w, _network_h\n    target_size = (_network_w, _network_h)\n    x = (meta.width - target_size[0]) // 2 if meta.width > target_size[0] else 0\n    y = (meta.height - target_size[1]) // 2 if meta.height > target_size[1] else 0\n\n    return pose_crop(meta, x, y, target_size[0], target_size[1])\n\n\ndef pose_crop_random(meta):\n    global _network_w, _network_h\n    target_size = (_network_w, _network_h)\n\n    for _ in range(50):\n        x = random.randrange(0, meta.width - target_size[0]) if meta.width > target_size[0] else 0\n        y = random.randrange(0, meta.height - target_size[1]) if meta.height > target_size[1] else 0\n\n        # check whether any face is inside the box to generate a reasonably-balanced datasets\n        for joint in meta.joint_list:\n            if x <= joint[CocoPart.Nose.value][0] < x + target_size[0] and y <= joint[CocoPart.Nose.value][1] < y + target_size[1]:\n                break\n\n    return pose_crop(meta, x, y, target_size[0], target_size[1])\n\n\ndef pose_crop(meta, x, y, w, h):\n    # adjust image\n    target_size = (w, h)\n\n    img = meta.img\n    resized = img[y:y+target_size[1], x:x+target_size[0], :]\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1000, -1000))\n            #     continue\n            new_x, new_y = point[0] - x, point[1] - y\n            # if new_x <= 0 or new_y <= 0 or new_x > target_size[0] or new_y > target_size[1]:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((new_x, new_y))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = target_size\n    meta.img = resized\n    return meta\n\n\ndef pose_flip(meta):\n    r = random.uniform(0, 1.0)\n    if r > 0.5:\n        return meta\n\n    img = meta.img\n    img = cv2.flip(img, 1)\n\n    # flip meta\n    flip_list = [CocoPart.Nose, CocoPart.Neck, CocoPart.LShoulder, CocoPart.LElbow, CocoPart.LWrist, CocoPart.RShoulder, CocoPart.RElbow, CocoPart.RWrist,\n                 CocoPart.LHip, CocoPart.LKnee, CocoPart.LAnkle, CocoPart.RHip, CocoPart.RKnee, CocoPart.RAnkle,\n                 CocoPart.LEye, CocoPart.REye, CocoPart.LEar, CocoPart.REar, CocoPart.Background]\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for cocopart in flip_list:\n            point = joint[cocopart.value]\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((meta.width - point[0], point[1]))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n\n    meta.img = img\n    return meta\n\n\ndef pose_rotation(meta):\n    deg = random.uniform(-15.0, 15.0)\n    img = meta.img\n\n    center = (img.shape[1] * 0.5, img.shape[0] * 0.5)       # x, y\n    rot_m = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), deg, 1)\n    ret = cv2.warpAffine(img, rot_m, img.shape[1::-1], flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT)\n    if img.ndim == 3 and ret.ndim == 2:\n        ret = ret[:, :, np.newaxis]\n    neww, newh = RotationAndCropValid.largest_rotated_rect(ret.shape[1], ret.shape[0], deg)\n    neww = min(neww, ret.shape[1])\n    newh = min(newh, ret.shape[0])\n    newx = int(center[0] - neww * 0.5)\n    newy = int(center[1] - newh * 0.5)\n    # print(ret.shape, deg, newx, newy, neww, newh)\n    img = ret[newy:newy + newh, newx:newx + neww]\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            x, y = _rotate_coord((meta.width, meta.height), (newx, newy), point, deg)\n            adjust_joint.append((x, y))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww, newh\n    meta.img = img\n\n    return meta\n\n\ndef _rotate_coord(shape, newxy, point, angle):\n    angle = -1 * angle / 180.0 * math.pi\n\n    ox, oy = shape\n    px, py = point\n\n    ox /= 2\n    oy /= 2\n\n    qx = math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n    qy = math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n\n    new_x, new_y = newxy\n\n    qx += ox - new_x\n    qy += oy - new_y\n\n    return int(qx + 0.5), int(qy + 0.5)\n\n\ndef pose_to_img(meta_l):\n    global _network_w, _network_h, _scale\n    return [\n        meta_l[0].img.astype(np.float16),\n        meta_l[0].get_heatmap(target_size=(_network_w // _scale, _network_h // _scale)),\n        meta_l[0].get_vectormap(target_size=(_network_w // _scale, _network_h // _scale))\n    ]\n'"
tf_pose/pose_dataset.py,4,"b'import logging\nimport math\nimport multiprocessing\nimport struct\nimport sys\nimport threading\n\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\nfrom contextlib import contextmanager\n\nimport os\nimport random\nimport requests\nimport cv2\nimport numpy as np\nimport time\n\nimport tensorflow as tf\n\nfrom tensorpack.dataflow import MultiThreadMapData\nfrom tensorpack.dataflow.image import MapDataComponent\nfrom tensorpack.dataflow.common import BatchData, MapData\nfrom tensorpack.dataflow.parallel import PrefetchData\nfrom tensorpack.dataflow.base import RNGDataFlow, DataFlowTerminated\n\nfrom pycocotools.coco import COCO\nfrom pose_augment import pose_flip, pose_rotation, pose_to_img, pose_crop_random, \\\n    pose_resize_shortestedge_random, pose_resize_shortestedge_fixed, pose_crop_center, pose_random_scale\nfrom numba import jit\n\nlogging.getLogger(""requests"").setLevel(logging.WARNING)\nlogger = logging.getLogger(\'pose_dataset\')\nlogger.setLevel(logging.INFO)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\nmplset = False\n\n\nclass CocoMetadata:\n    # __coco_parts = 57\n    __coco_parts = 19\n    __coco_vecs = list(zip(\n        [2, 9,  10, 2,  12, 13, 2, 3, 4, 3,  2, 6, 7, 6,  2, 1,  1,  15, 16],\n        [9, 10, 11, 12, 13, 14, 3, 4, 5, 17, 6, 7, 8, 18, 1, 15, 16, 17, 18]\n    ))\n\n    @staticmethod\n    def parse_float(four_np):\n        assert len(four_np) == 4\n        return struct.unpack(\'<f\', bytes(four_np))[0]\n\n    @staticmethod\n    def parse_floats(four_nps, adjust=0):\n        assert len(four_nps) % 4 == 0\n        return [(CocoMetadata.parse_float(four_nps[x*4:x*4+4]) + adjust) for x in range(len(four_nps) // 4)]\n\n    def __init__(self, idx, img_url, img_meta, annotations, sigma):\n        self.idx = idx\n        self.img_url = img_url\n        self.img = None\n        self.sigma = sigma\n\n        self.height = int(img_meta[\'height\'])\n        self.width = int(img_meta[\'width\'])\n\n        joint_list = []\n        for ann in annotations:\n            if ann.get(\'num_keypoints\', 0) == 0:\n                continue\n\n            kp = np.array(ann[\'keypoints\'])\n            xs = kp[0::3]\n            ys = kp[1::3]\n            vs = kp[2::3]\n\n            joint_list.append([(x, y) if v >= 1 else (-1000, -1000) for x, y, v in zip(xs, ys, vs)])\n\n        self.joint_list = []\n        transform = list(zip(\n            [1, 6, 7, 9, 11, 6, 8, 10, 13, 15, 17, 12, 14, 16, 3, 2, 5, 4],\n            [1, 7, 7, 9, 11, 6, 8, 10, 13, 15, 17, 12, 14, 16, 3, 2, 5, 4]\n        ))\n        for prev_joint in joint_list:\n            new_joint = []\n            for idx1, idx2 in transform:\n                j1 = prev_joint[idx1-1]\n                j2 = prev_joint[idx2-1]\n\n                if j1[0] <= 0 or j1[1] <= 0 or j2[0] <= 0 or j2[1] <= 0:\n                    new_joint.append((-1000, -1000))\n                else:\n                    new_joint.append(((j1[0] + j2[0]) / 2, (j1[1] + j2[1]) / 2))\n\n            new_joint.append((-1000, -1000))\n            self.joint_list.append(new_joint)\n\n        # logger.debug(\'joint size=%d\' % len(self.joint_list))\n\n    @jit\n    def get_heatmap(self, target_size):\n        heatmap = np.zeros((CocoMetadata.__coco_parts, self.height, self.width), dtype=np.float32)\n\n        for joints in self.joint_list:\n            for idx, point in enumerate(joints):\n                if point[0] < 0 or point[1] < 0:\n                    continue\n                CocoMetadata.put_heatmap(heatmap, idx, point, self.sigma)\n\n        heatmap = heatmap.transpose((1, 2, 0))\n\n        # background\n        heatmap[:, :, -1] = np.clip(1 - np.amax(heatmap, axis=2), 0.0, 1.0)\n\n        if target_size:\n            heatmap = cv2.resize(heatmap, target_size, interpolation=cv2.INTER_AREA)\n\n        return heatmap.astype(np.float16)\n\n    @staticmethod\n    @jit(nopython=True)\n    def put_heatmap(heatmap, plane_idx, center, sigma):\n        center_x, center_y = center\n        _, height, width = heatmap.shape[:3]\n\n        th = 4.6052\n        delta = math.sqrt(th * 2)\n\n        x0 = int(max(0, center_x - delta * sigma))\n        y0 = int(max(0, center_y - delta * sigma))\n\n        x1 = int(min(width, center_x + delta * sigma))\n        y1 = int(min(height, center_y + delta * sigma))\n\n        for y in range(y0, y1):\n            for x in range(x0, x1):\n                d = (x - center_x) ** 2 + (y - center_y) ** 2\n                exp = d / 2.0 / sigma / sigma\n                if exp > th:\n                    continue\n                heatmap[plane_idx][y][x] = max(heatmap[plane_idx][y][x], math.exp(-exp))\n                heatmap[plane_idx][y][x] = min(heatmap[plane_idx][y][x], 1.0)\n\n    @jit\n    def get_vectormap(self, target_size):\n        vectormap = np.zeros((CocoMetadata.__coco_parts*2, self.height, self.width), dtype=np.float32)\n        countmap = np.zeros((CocoMetadata.__coco_parts, self.height, self.width), dtype=np.int16)\n        for joints in self.joint_list:\n            for plane_idx, (j_idx1, j_idx2) in enumerate(CocoMetadata.__coco_vecs):\n                j_idx1 -= 1\n                j_idx2 -= 1\n\n                center_from = joints[j_idx1]\n                center_to = joints[j_idx2]\n\n                if center_from[0] < -100 or center_from[1] < -100 or center_to[0] < -100 or center_to[1] < -100:\n                    continue\n\n                CocoMetadata.put_vectormap(vectormap, countmap, plane_idx, center_from, center_to)\n\n        vectormap = vectormap.transpose((1, 2, 0))\n        nonzeros = np.nonzero(countmap)\n        for p, y, x in zip(nonzeros[0], nonzeros[1], nonzeros[2]):\n            if countmap[p][y][x] <= 0:\n                continue\n            vectormap[y][x][p*2+0] /= countmap[p][y][x]\n            vectormap[y][x][p*2+1] /= countmap[p][y][x]\n\n        if target_size:\n            vectormap = cv2.resize(vectormap, target_size, interpolation=cv2.INTER_AREA)\n\n        return vectormap.astype(np.float16)\n\n    @staticmethod\n    @jit(nopython=True)\n    def put_vectormap(vectormap, countmap, plane_idx, center_from, center_to, threshold=8):\n        _, height, width = vectormap.shape[:3]\n\n        vec_x = center_to[0] - center_from[0]\n        vec_y = center_to[1] - center_from[1]\n\n        min_x = max(0, int(min(center_from[0], center_to[0]) - threshold))\n        min_y = max(0, int(min(center_from[1], center_to[1]) - threshold))\n\n        max_x = min(width, int(max(center_from[0], center_to[0]) + threshold))\n        max_y = min(height, int(max(center_from[1], center_to[1]) + threshold))\n\n        norm = math.sqrt(vec_x ** 2 + vec_y ** 2)\n        if norm == 0:\n            return\n\n        vec_x /= norm\n        vec_y /= norm\n\n        for y in range(min_y, max_y):\n            for x in range(min_x, max_x):\n                bec_x = x - center_from[0]\n                bec_y = y - center_from[1]\n                dist = abs(bec_x * vec_y - bec_y * vec_x)\n\n                if dist > threshold:\n                    continue\n\n                countmap[plane_idx][y][x] += 1\n\n                vectormap[plane_idx*2+0][y][x] = vec_x\n                vectormap[plane_idx*2+1][y][x] = vec_y\n\n\nclass CocoPose(RNGDataFlow):\n    @staticmethod\n    def display_image(inp, heatmap, vectmap, as_numpy=False):\n        global mplset\n        # if as_numpy and not mplset:\n        #     import matplotlib as mpl\n        #     mpl.use(\'Agg\')\n        mplset = True\n        import matplotlib.pyplot as plt\n\n        fig = plt.figure()\n        a = fig.add_subplot(2, 2, 1)\n        a.set_title(\'Image\')\n        plt.imshow(CocoPose.get_bgimg(inp))\n\n        a = fig.add_subplot(2, 2, 2)\n        a.set_title(\'Heatmap\')\n        plt.imshow(CocoPose.get_bgimg(inp, target_size=(heatmap.shape[1], heatmap.shape[0])), alpha=0.5)\n        tmp = np.amax(heatmap, axis=2)\n        plt.imshow(tmp, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n\n        tmp2 = vectmap.transpose((2, 0, 1))\n        tmp2_odd = np.amax(np.absolute(tmp2[::2, :, :]), axis=0)\n        tmp2_even = np.amax(np.absolute(tmp2[1::2, :, :]), axis=0)\n\n        a = fig.add_subplot(2, 2, 3)\n        a.set_title(\'Vectormap-x\')\n        plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\n        plt.imshow(tmp2_odd, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n\n        a = fig.add_subplot(2, 2, 4)\n        a.set_title(\'Vectormap-y\')\n        plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\n        plt.imshow(tmp2_even, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n\n        if not as_numpy:\n            plt.show()\n        else:\n            fig.canvas.draw()\n            data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            fig.clear()\n            plt.close()\n            return data\n\n    @staticmethod\n    def get_bgimg(inp, target_size=None):\n        inp = cv2.cvtColor(inp.astype(np.uint8), cv2.COLOR_BGR2RGB)\n        if target_size:\n            inp = cv2.resize(inp, target_size, interpolation=cv2.INTER_AREA)\n        return inp\n\n    def __init__(self, path, img_path=None, is_train=True, decode_img=True, only_idx=-1):\n        self.is_train = is_train\n        self.decode_img = decode_img\n        self.only_idx = only_idx\n\n        if is_train:\n            whole_path = os.path.join(path, \'person_keypoints_train2017.json\')\n        else:\n            whole_path = os.path.join(path, \'person_keypoints_val2017.json\')\n        self.img_path = (img_path if img_path is not None else \'\') + (\'train2017/\' if is_train else \'val2017/\')\n        self.coco = COCO(whole_path)\n\n        logger.info(\'%s dataset %d\' % (path, self.size()))\n\n    def size(self):\n        return len(self.coco.imgs)\n\n    def get_data(self):\n        idxs = np.arange(self.size())\n        if self.is_train:\n            self.rng.shuffle(idxs)\n        else:\n            pass\n\n        keys = list(self.coco.imgs.keys())\n        for idx in idxs:\n            img_meta = self.coco.imgs[keys[idx]]\n            img_idx = img_meta[\'id\']\n            ann_idx = self.coco.getAnnIds(imgIds=img_idx)\n\n            if \'http://\' in self.img_path:\n                img_url = self.img_path + img_meta[\'file_name\']\n            else:\n                img_url = os.path.join(self.img_path, img_meta[\'file_name\'])\n\n            anns = self.coco.loadAnns(ann_idx)\n            meta = CocoMetadata(idx, img_url, img_meta, anns, sigma=8.0)\n\n            total_keypoints = sum([ann.get(\'num_keypoints\', 0) for ann in anns])\n            if total_keypoints == 0 and random.uniform(0, 1) > 0.2:\n                continue\n\n            yield [meta]\n\n\nclass MPIIPose(RNGDataFlow):\n    def __init__(self):\n        pass\n\n    def size(self):\n        pass\n\n    def get_data(self):\n        pass\n\n\ndef read_image_url(metas):\n    for meta in metas:\n        img_str = None\n        if \'http://\' in meta.img_url:\n            # print(meta.img_url)\n            for _ in range(10):\n                try:\n                    resp = requests.get(meta.img_url)\n                    if resp.status_code // 100 != 2:\n                        logger.warning(\'request failed code=%d url=%s\' % (resp.status_code, meta.img_url))\n                        time.sleep(1.0)\n                        continue\n                    img_str = resp.content\n                    break\n                except Exception as e:\n                    logger.warning(\'request failed url=%s, err=%s\' % (meta.img_url, str(e)))\n        else:\n            img_str = open(meta.img_url, \'rb\').read()\n\n        if not img_str:\n            logger.warning(\'image not read, path=%s\' % meta.img_url)\n            raise Exception()\n\n        nparr = np.fromstring(img_str, np.uint8)\n        meta.img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n    return metas\n\n\ndef get_dataflow(path, is_train, img_path=None):\n    ds = CocoPose(path, img_path, is_train)       # read data from lmdb\n    if is_train:\n        ds = MapData(ds, read_image_url)\n        ds = MapDataComponent(ds, pose_random_scale)\n        ds = MapDataComponent(ds, pose_rotation)\n        ds = MapDataComponent(ds, pose_flip)\n        ds = MapDataComponent(ds, pose_resize_shortestedge_random)\n        ds = MapDataComponent(ds, pose_crop_random)\n        ds = MapData(ds, pose_to_img)\n        # augs = [\n        #     imgaug.RandomApplyAug(imgaug.RandomChooseAug([\n        #         imgaug.GaussianBlur(max_size=3)\n        #     ]), 0.7)\n        # ]\n        # ds = AugmentImageComponent(ds, augs)\n        ds = PrefetchData(ds, 1000, multiprocessing.cpu_count() * 1)\n    else:\n        ds = MultiThreadMapData(ds, nr_thread=16, map_func=read_image_url, buffer_size=1000)\n        ds = MapDataComponent(ds, pose_resize_shortestedge_fixed)\n        ds = MapDataComponent(ds, pose_crop_center)\n        ds = MapData(ds, pose_to_img)\n        ds = PrefetchData(ds, 100, multiprocessing.cpu_count() // 4)\n\n    return ds\n\n\ndef _get_dataflow_onlyread(path, is_train, img_path=None):\n    ds = CocoPose(path, img_path, is_train)  # read data from lmdb\n    ds = MapData(ds, read_image_url)\n    ds = MapData(ds, pose_to_img)\n    # ds = PrefetchData(ds, 1000, multiprocessing.cpu_count() * 4)\n    return ds\n\n\ndef get_dataflow_batch(path, is_train, batchsize, img_path=None):\n    logger.info(\'dataflow img_path=%s\' % img_path)\n    ds = get_dataflow(path, is_train, img_path=img_path)\n    ds = BatchData(ds, batchsize)\n    # if is_train:\n    #     ds = PrefetchData(ds, 10, 2)\n    # else:\n    #     ds = PrefetchData(ds, 50, 2)\n\n    return ds\n\n\nclass DataFlowToQueue(threading.Thread):\n    def __init__(self, ds, placeholders, queue_size=5):\n        super().__init__()\n        self.daemon = True\n\n        self.ds = ds\n        self.placeholders = placeholders\n        self.queue = tf.FIFOQueue(queue_size, [ph.dtype for ph in placeholders], shapes=[ph.get_shape() for ph in placeholders])\n        self.op = self.queue.enqueue(placeholders)\n        self.close_op = self.queue.close(cancel_pending_enqueues=True)\n\n        self._coord = None\n        self._sess = None\n\n        self.last_dp = None\n\n    @contextmanager\n    def default_sess(self):\n        if self._sess:\n            with self._sess.as_default():\n                yield\n        else:\n            logger.warning(""DataFlowToQueue {} wasn\'t under a default session!"".format(self.name))\n            yield\n\n    def size(self):\n        return self.queue.size()\n\n    def start(self):\n        self._sess = tf.get_default_session()\n        super().start()\n\n    def set_coordinator(self, coord):\n        self._coord = coord\n\n    def run(self):\n        with self.default_sess():\n            try:\n                while not self._coord.should_stop():\n                    try:\n                        self.ds.reset_state()\n                        while True:\n                            for dp in self.ds.get_data():\n                                feed = dict(zip(self.placeholders, dp))\n                                self.op.run(feed_dict=feed)\n                                self.last_dp = dp\n                    except (tf.errors.CancelledError, tf.errors.OutOfRangeError, DataFlowTerminated):\n                        logger.error(\'err type1, placeholders={}\'.format(self.placeholders))\n                        sys.exit(-1)\n                    except Exception as e:\n                        logger.error(\'err type2, err={}, placeholders={}\'.format(str(e), self.placeholders))\n                        if isinstance(e, RuntimeError) and \'closed Session\' in str(e):\n                            pass\n                        else:\n                            logger.exception(""Exception in {}:{}"".format(self.name, str(e)))\n                        sys.exit(-1)\n            except Exception as e:\n                logger.exception(""Exception in {}:{}"".format(self.name, str(e)))\n            finally:\n                try:\n                    self.close_op.run()\n                except Exception:\n                    pass\n                logger.info(""{} Exited."".format(self.name))\n\n    def dequeue(self):\n        return self.queue.dequeue()\n\n\nif __name__ == \'__main__\':\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n\n    from pose_augment import set_network_input_wh, set_network_scale\n    # set_network_input_wh(368, 368)\n    set_network_input_wh(480, 320)\n    set_network_scale(8)\n\n    # df = get_dataflow(\'/data/public/rw/coco/annotations\', True, \'/data/public/rw/coco/\')\n    df = _get_dataflow_onlyread(\'/data/public/rw/coco/annotations\', True, \'/data/public/rw/coco/\')\n    # df = get_dataflow(\'/root/coco/annotations\', False, img_path=\'http://gpu-twg.kakaocdn.net/braincloud/COCO/\')\n\n    from tensorpack.dataflow.common import TestDataSpeed\n    TestDataSpeed(df).start()\n    sys.exit(0)\n\n    with tf.Session() as sess:\n        df.reset_state()\n        t1 = time.time()\n        for idx, dp in enumerate(df.get_data()):\n            if idx == 0:\n                for d in dp:\n                    logger.info(\'%d dp shape={}\'.format(d.shape))\n            print(time.time() - t1)\n            t1 = time.time()\n            CocoPose.display_image(dp[0], dp[1].astype(np.float32), dp[2].astype(np.float32))\n            print(dp[1].shape, dp[2].shape)\n            pass\n\n    logger.info(\'done\')\n'"
tf_pose/pystopwatch.py,0,"b""import time\nfrom collections import defaultdict\n\n\nclass StopWatchManager:\n    def __init__(self):\n        self.watches = defaultdict(StopWatch)\n\n    def get(self, name):\n        return self.watches[name]\n\n    def start(self, name):\n        self.get(name).start()\n\n    def stop(self, name):\n        self.get(name).stop()\n\n    def reset(self, name):\n        self.get(name).reset()\n\n    def get_elapsed(self, name):\n        return self.get(name).get_elapsed()\n\n    def __repr__(self):\n        return '\\n'.join(['%s: %.8f' % (k, v.elapsed_accumulated) for k, v in self.watches.items()])\n\n\nclass StopWatch:\n    def __init__(self):\n        self.elapsed_accumulated = 0.0\n        self.started_at = time.time()\n\n    def start(self):\n        self.started_at = time.time()\n\n    def stop(self):\n        self.elapsed_accumulated += time.time() - self.started_at\n\n    def reset(self):\n        self.elapsed_accumulated = 0.0\n\n    def get_elapsed(self):\n        return self.elapsed_accumulated"""
tf_pose/runner.py,0,"b'import base64\nimport os\n\nimport cv2\nfrom functools import lru_cache\n\nfrom tf_pose import common\nfrom tf_pose import eval\nfrom tf_pose.estimator import TfPoseEstimator\nfrom tf_pose.networks import get_graph_path, model_wh\n\nEstimator = TfPoseEstimator\n\n\n@lru_cache(maxsize=1)\ndef get_estimator(model=\'cmu\', resize=\'0x0\'):\n    w, h = model_wh(resize)\n    if w == 0 or h == 0:\n        e = TfPoseEstimator(get_graph_path(model), target_size=(432, 368))\n    else:\n        e = TfPoseEstimator(get_graph_path(model), target_size=(w, h))\n\n    return e\n\n\ndef infer(image, model=\'cmu\', resize=\'0x0\', resize_out_ratio=4.0):\n    """"""\n\n    :param image:\n    :param model:\n    :param resize:\n    :param resize_out_ratio:\n    :return: coco_style_keypoints array\n    """"""\n    w, h = model_wh(resize)\n    e = get_estimator(model, resize)\n\n    # estimate human poses from a single image !\n    image = common.read_imgfile(image, None, None)\n    if image is None:\n        raise Exception(\'Image can not be read, path=%s\' % image)\n    humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n    image_h, image_w = image.shape[:2]\n\n    if ""TERM_PROGRAM"" in os.environ and \'iTerm\' in os.environ[""TERM_PROGRAM""]:\n        image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n        image_str = cv2.imencode("".jpg"", image)[1].tostring()\n        print(""\\033]1337;File=name=;inline=1:"" + base64.b64encode(image_str).decode(""utf-8"") + ""\\a"")\n\n    return [(eval.write_coco_json(human, image_w, image_h), human.score) for human in humans]\n'"
tf_pose/train.py,56,"b'import matplotlib as mpl\nmpl.use(\'Agg\')      # training mode, no screen should be open. (It will block training loop)\n\nimport argparse\nimport logging\nimport os\nimport time\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom pose_dataset import get_dataflow_batch, DataFlowToQueue, CocoPose\nfrom pose_augment import set_network_input_wh, set_network_scale\nfrom common import get_sample_images\nfrom networks import get_network\n\nlogger = logging.getLogger(\'train\')\nlogger.handlers.clear()\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Training codes for Openpose using Tensorflow\')\n    parser.add_argument(\'--model\', default=\'mobilenet_v2_1.4\', help=\'model name\')\n    parser.add_argument(\'--datapath\', type=str, default=\'/data/public/rw/coco/annotations\')\n    parser.add_argument(\'--imgpath\', type=str, default=\'/data/public/rw/coco/\')\n    parser.add_argument(\'--batchsize\', type=int, default=64)\n    parser.add_argument(\'--gpus\', type=int, default=4)\n    parser.add_argument(\'--max-epoch\', type=int, default=600)\n    parser.add_argument(\'--lr\', type=str, default=\'0.001\')\n    parser.add_argument(\'--tag\', type=str, default=\'test\')\n    parser.add_argument(\'--checkpoint\', type=str, default=\'\')\n\n    parser.add_argument(\'--input-width\', type=int, default=432)\n    parser.add_argument(\'--input-height\', type=int, default=368)\n    parser.add_argument(\'--quant-delay\', type=int, default=-1)\n    args = parser.parse_args()\n\n    modelpath = logpath = \'./models/train/\'\n\n    if args.gpus <= 0:\n        raise Exception(\'gpus <= 0\')\n\n    # define input placeholder\n    set_network_input_wh(args.input_width, args.input_height)\n    scale = 4\n\n    if args.model in [\'cmu\', \'vgg\'] or \'mobilenet\' in args.model:\n        scale = 8\n\n    set_network_scale(scale)\n    output_w, output_h = args.input_width // scale, args.input_height // scale\n\n    logger.info(\'define model+\')\n    with tf.device(tf.DeviceSpec(device_type=""CPU"")):\n        input_node = tf.placeholder(tf.float32, shape=(args.batchsize, args.input_height, args.input_width, 3), name=\'image\')\n        vectmap_node = tf.placeholder(tf.float32, shape=(args.batchsize, output_h, output_w, 38), name=\'vectmap\')\n        heatmap_node = tf.placeholder(tf.float32, shape=(args.batchsize, output_h, output_w, 19), name=\'heatmap\')\n\n        # prepare data\n        df = get_dataflow_batch(args.datapath, True, args.batchsize, img_path=args.imgpath)\n        enqueuer = DataFlowToQueue(df, [input_node, heatmap_node, vectmap_node], queue_size=100)\n        q_inp, q_heat, q_vect = enqueuer.dequeue()\n\n    df_valid = get_dataflow_batch(args.datapath, False, args.batchsize, img_path=args.imgpath)\n    df_valid.reset_state()\n    validation_cache = []\n\n    val_image = get_sample_images(args.input_width, args.input_height)\n    logger.debug(\'tensorboard val image: %d\' % len(val_image))\n    logger.debug(q_inp)\n    logger.debug(q_heat)\n    logger.debug(q_vect)\n\n    # define model for multi-gpu\n    q_inp_split, q_heat_split, q_vect_split = tf.split(q_inp, args.gpus), tf.split(q_heat, args.gpus), tf.split(q_vect, args.gpus)\n\n    output_vectmap = []\n    output_heatmap = []\n    losses = []\n    last_losses_l1 = []\n    last_losses_l2 = []\n    outputs = []\n    for gpu_id in range(args.gpus):\n        with tf.device(tf.DeviceSpec(device_type=""GPU"", device_index=gpu_id)):\n            with tf.variable_scope(tf.get_variable_scope(), reuse=(gpu_id > 0)):\n                net, pretrain_path, last_layer = get_network(args.model, q_inp_split[gpu_id])\n                if args.checkpoint:\n                    pretrain_path = args.checkpoint\n                vect, heat = net.loss_last()\n                output_vectmap.append(vect)\n                output_heatmap.append(heat)\n                outputs.append(net.get_output())\n\n                l1s, l2s = net.loss_l1_l2()\n                for idx, (l1, l2) in enumerate(zip(l1s, l2s)):\n                    loss_l1 = tf.nn.l2_loss(tf.concat(l1, axis=0) - q_vect_split[gpu_id], name=\'loss_l1_stage%d_tower%d\' % (idx, gpu_id))\n                    loss_l2 = tf.nn.l2_loss(tf.concat(l2, axis=0) - q_heat_split[gpu_id], name=\'loss_l2_stage%d_tower%d\' % (idx, gpu_id))\n                    losses.append(tf.reduce_mean([loss_l1, loss_l2]))\n\n                last_losses_l1.append(loss_l1)\n                last_losses_l2.append(loss_l2)\n\n    outputs = tf.concat(outputs, axis=0)\n\n    with tf.device(tf.DeviceSpec(device_type=""GPU"")):\n        # define loss\n        total_loss = tf.reduce_sum(losses) / args.batchsize\n        total_loss_ll_paf = tf.reduce_sum(last_losses_l1) / args.batchsize\n        total_loss_ll_heat = tf.reduce_sum(last_losses_l2) / args.batchsize\n        total_loss_ll = tf.reduce_sum([total_loss_ll_paf, total_loss_ll_heat])\n\n        # define optimizer\n        step_per_epoch = 121745 // args.batchsize\n        global_step = tf.Variable(0, trainable=False)\n        if \',\' not in args.lr:\n            starter_learning_rate = float(args.lr)\n            # learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n            #                                            decay_steps=10000, decay_rate=0.33, staircase=True)\n            learning_rate = tf.train.cosine_decay(starter_learning_rate, global_step, args.max_epoch * step_per_epoch, alpha=0.0)\n        else:\n            lrs = [float(x) for x in args.lr.split(\',\')]\n            boundaries = [step_per_epoch * 5 * i for i, _ in range(len(lrs)) if i > 0]\n            learning_rate = tf.train.piecewise_constant(global_step, boundaries, lrs)\n\n    if args.quant_delay >= 0:\n        logger.info(\'train using quantized mode, delay=%d\' % args.quant_delay)\n        g = tf.get_default_graph()\n        tf.contrib.quantize.create_training_graph(input_graph=g, quant_delay=args.quant_delay)\n\n    # optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=0.0005, momentum=0.9, epsilon=1e-10)\n    optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=1e-8)\n    # optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.8, use_locking=True, use_nesterov=True)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = optimizer.minimize(total_loss, global_step, colocate_gradients_with_ops=True)\n    logger.info(\'define model-\')\n\n    # define summary\n    tf.summary.scalar(""loss"", total_loss)\n    tf.summary.scalar(""loss_lastlayer"", total_loss_ll)\n    tf.summary.scalar(""loss_lastlayer_paf"", total_loss_ll_paf)\n    tf.summary.scalar(""loss_lastlayer_heat"", total_loss_ll_heat)\n    tf.summary.scalar(""queue_size"", enqueuer.size())\n    tf.summary.scalar(""lr"", learning_rate)\n    merged_summary_op = tf.summary.merge_all()\n\n    valid_loss = tf.placeholder(tf.float32, shape=[])\n    valid_loss_ll = tf.placeholder(tf.float32, shape=[])\n    valid_loss_ll_paf = tf.placeholder(tf.float32, shape=[])\n    valid_loss_ll_heat = tf.placeholder(tf.float32, shape=[])\n    sample_train = tf.placeholder(tf.float32, shape=(4, 640, 640, 3))\n    sample_valid = tf.placeholder(tf.float32, shape=(12, 640, 640, 3))\n    train_img = tf.summary.image(\'training sample\', sample_train, 4)\n    valid_img = tf.summary.image(\'validation sample\', sample_valid, 12)\n    valid_loss_t = tf.summary.scalar(""loss_valid"", valid_loss)\n    valid_loss_ll_t = tf.summary.scalar(""loss_valid_lastlayer"", valid_loss_ll)\n    merged_validate_op = tf.summary.merge([train_img, valid_img, valid_loss_t, valid_loss_ll_t])\n\n    saver = tf.train.Saver(max_to_keep=1000)\n    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        logger.info(\'model weights initialization\')\n        sess.run(tf.global_variables_initializer())\n\n        if args.checkpoint and os.path.isdir(args.checkpoint):\n            logger.info(\'Restore from checkpoint...\')\n            # loader = tf.train.Saver(net.restorable_variables())\n            # loader.restore(sess, tf.train.latest_checkpoint(args.checkpoint))\n            saver.restore(sess, tf.train.latest_checkpoint(args.checkpoint))\n            logger.info(\'Restore from checkpoint...Done\')\n        elif pretrain_path:\n            logger.info(\'Restore pretrained weights... %s\' % pretrain_path)\n            if \'.npy\' in pretrain_path:\n                net.load(pretrain_path, sess, False)\n            else:\n                try:\n                    loader = tf.train.Saver(net.restorable_variables(only_backbone=False))\n                    loader.restore(sess, pretrain_path)\n                except:\n                    logger.info(\'Restore only weights in backbone layers.\')\n                    loader = tf.train.Saver(net.restorable_variables())\n                    loader.restore(sess, pretrain_path)\n            logger.info(\'Restore pretrained weights...Done\')\n\n        logger.info(\'prepare file writer\')\n        file_writer = tf.summary.FileWriter(os.path.join(logpath, args.tag), sess.graph)\n\n        logger.info(\'prepare coordinator\')\n        coord = tf.train.Coordinator()\n        enqueuer.set_coordinator(coord)\n        enqueuer.start()\n\n        logger.info(\'Training Started.\')\n        time_started = time.time()\n        last_gs_num = last_gs_num2 = 0\n        initial_gs_num = sess.run(global_step)\n\n        last_log_epoch1 = last_log_epoch2 = -1\n        while True:\n            _, gs_num = sess.run([train_op, global_step])\n            curr_epoch = float(gs_num) / step_per_epoch\n\n            if gs_num > step_per_epoch * args.max_epoch:\n                break\n\n            if gs_num - last_gs_num >= 500:\n                train_loss, train_loss_ll, train_loss_ll_paf, train_loss_ll_heat, lr_val, summary = sess.run([total_loss, total_loss_ll, total_loss_ll_paf, total_loss_ll_heat, learning_rate, merged_summary_op])\n\n                # log of training loss / accuracy\n                batch_per_sec = (gs_num - initial_gs_num) / (time.time() - time_started)\n                logger.info(\'epoch=%.2f step=%d, %0.4f examples/sec lr=%f, loss=%g, loss_ll=%g, loss_ll_paf=%g, loss_ll_heat=%g\' % (gs_num / step_per_epoch, gs_num, batch_per_sec * args.batchsize, lr_val, train_loss, train_loss_ll, train_loss_ll_paf, train_loss_ll_heat))\n                last_gs_num = gs_num\n\n                if last_log_epoch1 < curr_epoch:\n                    file_writer.add_summary(summary, curr_epoch)\n                    last_log_epoch1 = curr_epoch\n\n            if gs_num - last_gs_num2 >= 2000:\n                # save weights\n                saver.save(sess, os.path.join(modelpath, args.tag, \'model_latest\'), global_step=global_step)\n\n                average_loss = average_loss_ll = average_loss_ll_paf = average_loss_ll_heat = 0\n                total_cnt = 0\n\n                if len(validation_cache) == 0:\n                    for images_test, heatmaps, vectmaps in tqdm(df_valid.get_data()):\n                        validation_cache.append((images_test, heatmaps, vectmaps))\n                    df_valid.reset_state()\n                    del df_valid\n                    df_valid = None\n\n                # log of test accuracy\n                for images_test, heatmaps, vectmaps in validation_cache:\n                    lss, lss_ll, lss_ll_paf, lss_ll_heat, vectmap_sample, heatmap_sample = sess.run(\n                        [total_loss, total_loss_ll, total_loss_ll_paf, total_loss_ll_heat, output_vectmap, output_heatmap],\n                        feed_dict={q_inp: images_test, q_vect: vectmaps, q_heat: heatmaps}\n                    )\n                    average_loss += lss * len(images_test)\n                    average_loss_ll += lss_ll * len(images_test)\n                    average_loss_ll_paf += lss_ll_paf * len(images_test)\n                    average_loss_ll_heat += lss_ll_heat * len(images_test)\n                    total_cnt += len(images_test)\n\n                logger.info(\'validation(%d) %s loss=%f, loss_ll=%f, loss_ll_paf=%f, loss_ll_heat=%f\' % (total_cnt, args.tag, average_loss / total_cnt, average_loss_ll / total_cnt, average_loss_ll_paf / total_cnt, average_loss_ll_heat / total_cnt))\n                last_gs_num2 = gs_num\n\n                sample_image = [enqueuer.last_dp[0][i] for i in range(4)]\n                outputMat = sess.run(\n                    outputs,\n                    feed_dict={q_inp: np.array((sample_image + val_image) * max(1, (args.batchsize // 16)))}\n                )\n                pafMat, heatMat = outputMat[:, :, :, 19:], outputMat[:, :, :, :19]\n\n                sample_results = []\n                for i in range(len(sample_image)):\n                    test_result = CocoPose.display_image(sample_image[i], heatMat[i], pafMat[i], as_numpy=True)\n                    test_result = cv2.resize(test_result, (640, 640))\n                    test_result = test_result.reshape([640, 640, 3]).astype(float)\n                    sample_results.append(test_result)\n\n                test_results = []\n                for i in range(len(val_image)):\n                    test_result = CocoPose.display_image(val_image[i], heatMat[len(sample_image) + i], pafMat[len(sample_image) + i], as_numpy=True)\n                    test_result = cv2.resize(test_result, (640, 640))\n                    test_result = test_result.reshape([640, 640, 3]).astype(float)\n                    test_results.append(test_result)\n\n                # save summary\n                summary = sess.run(merged_validate_op, feed_dict={\n                    valid_loss: average_loss / total_cnt,\n                    valid_loss_ll: average_loss_ll / total_cnt,\n                    valid_loss_ll_paf: average_loss_ll_paf / total_cnt,\n                    valid_loss_ll_heat: average_loss_ll_heat / total_cnt,\n                    sample_valid: test_results,\n                    sample_train: sample_results\n                })\n                if last_log_epoch2 < curr_epoch:\n                    file_writer.add_summary(summary, curr_epoch)\n                    last_log_epoch2 = curr_epoch\n\n        saver.save(sess, os.path.join(modelpath, args.tag, \'model\'), global_step=global_step)\n    logger.info(\'optimization finished. %f\' % (time.time() - time_started))\n'"
tf_pose/mobilenet/__init__.py,0,b''
tf_pose/mobilenet/conv_blocks.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolution blocks for mobilenet.""""""\nimport contextlib\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\ndef _split_divisible(num, num_ways, divisible_by=8):\n  """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result\n\n\n@contextlib.contextmanager\ndef _v1_compatible_scope_naming(scope):\n  if scope is None:  # Create uniqified separable blocks.\n    with tf.variable_scope(None, default_name=\'separable\') as s, \\\n         tf.name_scope(s.original_name_scope):\n      yield \'\'\n  else:\n    # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.\n    # which provide numbered scopes.\n    scope += \'_\'\n    yield scope\n\n\n@slim.add_arg_scope\ndef split_separable_conv2d(input_tensor,\n                           num_outputs,\n                           scope=None,\n                           normalizer_fn=None,\n                           stride=1,\n                           rate=1,\n                           endpoints=None,\n                           use_explicit_padding=False):\n  """"""Separable mobilenet V1 style convolution.\n  Depthwise convolution, with default non-linearity,\n  followed by 1x1 depthwise convolution.  This is similar to\n  slim.separable_conv2d, but differs in tha it applies batch\n  normalization and non-linearity to depthwise. This  matches\n  the basic building of Mobilenet Paper\n  (https://arxiv.org/abs/1704.04861)\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs\n    scope: optional name of the scope. Note if provided it will use\n    scope_depthwise for deptwhise, and scope_pointwise for pointwise.\n    normalizer_fn: which normalizer function to use for depthwise/pointwise\n    stride: stride\n    rate: output rate (also known as dilation rate)\n    endpoints: optional, if provided, will export additional tensors to it.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n  Returns:\n    output tesnor\n  """"""\n\n  with _v1_compatible_scope_naming(scope) as scope:\n    dw_scope = scope + \'depthwise\'\n    endpoints = endpoints if endpoints is not None else {}\n    kernel_size = [3, 3]\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n      input_tensor = _fixed_padding(input_tensor, kernel_size, rate)\n    net = slim.separable_conv2d(\n        input_tensor,\n        None,\n        kernel_size,\n        depth_multiplier=1,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=dw_scope)\n\n    endpoints[dw_scope] = net\n\n    pw_scope = scope + \'pointwise\'\n    net = slim.conv2d(\n        net,\n        num_outputs, [1, 1],\n        stride=1,\n        normalizer_fn=normalizer_fn,\n        scope=pw_scope)\n    endpoints[pw_scope] = net\n  return net\n\n\ndef expand_input_by_factor(n, divisible_by=8):\n  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n\n\n@slim.add_arg_scope\ndef expanded_conv(input_tensor,\n                  num_outputs,\n                  expansion_size=expand_input_by_factor(6),\n                  stride=1,\n                  rate=1,\n                  kernel_size=(3, 3),\n                  residual=True,\n                  normalizer_fn=None,\n                  project_activation_fn=tf.identity,\n                  split_projection=1,\n                  split_expansion=1,\n                  expansion_transform=None,\n                  depthwise_location=\'expansion\',\n                  depthwise_channel_multiplier=1,\n                  endpoints=None,\n                  use_explicit_padding=False,\n                  padding=\'SAME\',\n                  scope=None):\n  """"""Depthwise Convolution Block with expansion.\n  Builds a composite convolution that has the following structure\n  expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs in the final layer.\n    expansion_size: the size of expansion, could be a constant or a callable.\n      If latter it will be provided \'num_inputs\' as an input. For forward\n      compatibility it should accept arbitrary keyword arguments.\n      Default will expand the input by factor of 6.\n    stride: depthwise stride\n    rate: depthwise rate\n    kernel_size: depthwise kernel\n    residual: whether to include residual connection between input\n      and output.\n    normalizer_fn: batchnorm or otherwise\n    project_activation_fn: activation function for the project layer\n    split_projection: how many ways to split projection operator\n      (that is conv expansion->bottleneck)\n    split_expansion: how many ways to split expansion op\n      (that is conv bottleneck->expansion) ops will keep depth divisible\n      by this value.\n    expansion_transform: Optional function that takes expansion\n      as a single input and returns output.\n    depthwise_location: where to put depthwise covnvolutions supported\n      values None, \'input\', \'output\', \'expansion\'\n    depthwise_channel_multiplier: depthwise channel multiplier:\n    each input will replicated (with different filters)\n    that many times. So if input had c channels,\n    output will have c x depthwise_channel_multpilier.\n    endpoints: An optional dictionary into which intermediate endpoints are\n      placed. The keys ""expansion_output"", ""depthwise_output"",\n      ""projection_output"" and ""expansion_transform"" are always populated, even\n      if the corresponding functions are not invoked.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    padding: Padding type to use if `use_explicit_padding` is not set.\n    scope: optional scope.\n  Returns:\n    Tensor of depth num_outputs\n  Raises:\n    TypeError: on inval\n  """"""\n  with tf.variable_scope(scope, default_name=\'expanded_conv\') as s, \\\n       tf.name_scope(s.original_name_scope):\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    if  depthwise_location not in [None, \'input\', \'output\', \'expansion\']:\n      raise TypeError(\'%r is unknown value for depthwise_location\' %\n                      depthwise_location)\n    if use_explicit_padding:\n      if padding != \'SAME\':\n        raise TypeError(\'`use_explicit_padding` should only be used with \'\n                        \'""SAME"" padding.\')\n      padding = \'VALID\'\n    depthwise_func = functools.partial(\n        slim.separable_conv2d,\n        num_outputs=None,\n        kernel_size=kernel_size,\n        depth_multiplier=depthwise_channel_multiplier,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=\'depthwise\')\n    # b1 -> b2 * r -> b2\n    #   i -> (o * r) (bottleneck) -> o\n    input_tensor = tf.identity(input_tensor, \'input\')\n    net = input_tensor\n\n    if depthwise_location == \'input\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(expansion_size):\n      inner_size = expansion_size(num_inputs=prev_depth)\n    else:\n      inner_size = expansion_size\n\n    if inner_size > net.shape[3]:\n      net = split_conv(\n          net,\n          inner_size,\n          num_ways=split_expansion,\n          scope=\'expand\',\n          stride=1,\n          normalizer_fn=normalizer_fn)\n      net = tf.identity(net, \'expansion_output\')\n    if endpoints is not None:\n      endpoints[\'expansion_output\'] = net\n\n    if depthwise_location == \'expansion\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net)\n\n    net = tf.identity(net, name=\'depthwise_output\')\n    if endpoints is not None:\n      endpoints[\'depthwise_output\'] = net\n    if expansion_transform:\n      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n    # Note in contrast with expansion, we always have\n    # projection to produce the desired output size.\n    net = split_conv(\n        net,\n        num_outputs,\n        num_ways=split_projection,\n        stride=1,\n        scope=\'project\',\n        normalizer_fn=normalizer_fn,\n        activation_fn=project_activation_fn)\n    if endpoints is not None:\n      endpoints[\'projection_output\'] = net\n    if depthwise_location == \'output\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(residual):  # custom residual\n      net = residual(input_tensor=input_tensor, output_tensor=net)\n    elif (residual and\n          # stride check enforces that we don\'t add residuals when spatial\n          # dimensions are None\n          stride == 1 and\n          # Depth matches\n          net.get_shape().as_list()[3] ==\n          input_tensor.get_shape().as_list()[3]):\n      net += input_tensor\n    return tf.identity(net, name=\'output\')\n\n\ndef split_conv(input_tensor,\n               num_outputs,\n               num_ways,\n               scope,\n               divisible_by=8,\n               **kwargs):\n  """"""Creates a split convolution.\n  Split convolution splits the input and output into\n  \'num_blocks\' blocks of approximately the same size each,\n  and only connects $i$-th input to $i$ output.\n  Args:\n    input_tensor: input tensor\n    num_outputs: number of output filters\n    num_ways: num blocks to split by.\n    scope: scope for all the operators.\n    divisible_by: make sure that every part is divisiable by this.\n    **kwargs: will be passed directly into conv2d operator\n  Returns:\n    tensor\n  """"""\n  b = input_tensor.get_shape().as_list()[3]\n\n  if num_ways == 1 or min(b // num_ways,\n                          num_outputs // num_ways) < divisible_by:\n    # Don\'t do any splitting if we end up with less than 8 filters\n    # on either side.\n    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n\n  outs = []\n  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n  output_splits = _split_divisible(\n      num_outputs, num_ways, divisible_by=divisible_by)\n  inputs = tf.split(input_tensor, input_splits, axis=3, name=\'split_\' + scope)\n  base = scope\n  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n    scope = base + \'_part_%d\' % (i,)\n    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n    n = tf.identity(n, scope + \'_output\')\n    outs.append(n)\n  return tf.concat(outs, 3, name=scope + \'_concat\')\n'"
tf_pose/mobilenet/mobilenet.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Mobilenet Base Class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport collections\nimport contextlib\nimport copy\nimport os\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef apply_activation(x, name=None, activation_fn=None):\n  return activation_fn(x, name=name) if activation_fn else x\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\n@contextlib.contextmanager\ndef _set_arg_scope_defaults(defaults):\n  """"""Sets arg scope defaults for all items present in defaults.\n  Args:\n    defaults: dictionary/list of pairs, containing a mapping from\n    function to a dictionary of default args.\n  Yields:\n    context manager where all defaults are set.\n  """"""\n  if hasattr(defaults, \'items\'):\n    items = list(defaults.items())\n  else:\n    items = defaults\n  if not items:\n    yield\n  else:\n    func, default_arg = items[0]\n    with slim.arg_scope(func, **default_arg):\n      with _set_arg_scope_defaults(items[1:]):\n        yield\n\n\n@slim.add_arg_scope\ndef depth_multiplier(output_params,\n                     multiplier,\n                     divisible_by=8,\n                     min_depth=8,\n                     **unused_kwargs):\n  if \'num_outputs\' not in output_params:\n    return\n  d = output_params[\'num_outputs\']\n  output_params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\n                                                 min_depth)\n\n\n_Op = collections.namedtuple(\'Op\', [\'op\', \'params\', \'multiplier_func\'])\n\n\ndef op(opfunc, **params):\n  multiplier = params.pop(\'multiplier_transorm\', depth_multiplier)\n  return _Op(opfunc, params=params, multiplier_func=multiplier)\n\n\nclass NoOpScope(object):\n  """"""No-op context manager.""""""\n\n  def __enter__(self):\n    return None\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    return False\n\n\ndef safe_arg_scope(funcs, **kwargs):\n  """"""Returns `slim.arg_scope` with all None arguments removed.\n  Arguments:\n    funcs: Functions to pass to `arg_scope`.\n    **kwargs: Arguments to pass to `arg_scope`.\n  Returns:\n    arg_scope or No-op context manager.\n  Note: can be useful if None value should be interpreted as ""do not overwrite\n    this parameter value"".\n  """"""\n  filtered_args = {name: value for name, value in kwargs.items()\n                   if value is not None}\n  if filtered_args:\n    return slim.arg_scope(funcs, **filtered_args)\n  else:\n    return NoOpScope()\n\n\n@slim.add_arg_scope\ndef mobilenet_base(  # pylint: disable=invalid-name\n    inputs,\n    conv_defs,\n    multiplier=1.0,\n    final_endpoint=None,\n    output_stride=None,\n    use_explicit_padding=False,\n    scope=None,\n    is_training=False):\n  """"""Mobilenet base network.\n  Constructs a network from inputs to the given final endpoint. By default\n  the network is constructed in inference mode. To create network\n  in training mode use:\n  with slim.arg_scope(mobilenet.training_scope()):\n     logits, endpoints = mobilenet_base(...)\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    conv_defs: A list of op(...) layers specifying the net architecture.\n    multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    final_endpoint: The name of last layer, for early termination for\n    for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 1 or any even number, excluding\n      zero. Typical values are 8 (accurate fully convolutional mode), 16\n      (fast fully convolutional mode), and 32 (classification mode).\n      NOTE- output_stride relies on all consequent operators to support dilated\n      operators via ""rate"" parameter. This might require wrapping non-conv\n      operators to operate properly.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional variable scope.\n    is_training: How to setup batch_norm and other ops. Note: most of the time\n      this does not need be set directly. Use mobilenet.training_scope() to set\n      up training instead. This parameter is here for backward compatibility\n      only. It is safe to set it to the value matching\n      training_scope(is_training=...). It is also safe to explicitly set\n      it to False, even if there is outer training_scope set to to training.\n      (The network will be built in inference mode). If this is set to None,\n      no arg_scope is added for slim.batch_norm\'s is_training parameter.\n  Returns:\n    tensor_out: output tensor.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n  Raises:\n    ValueError: depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  if multiplier <= 0:\n    raise ValueError(\'multiplier is not greater than zero.\')\n\n  # Set conv defs defaults and overrides.\n  conv_defs_defaults = conv_defs.get(\'defaults\', {})\n  conv_defs_overrides = conv_defs.get(\'overrides\', {})\n  if use_explicit_padding:\n    conv_defs_overrides = copy.deepcopy(conv_defs_overrides)\n    conv_defs_overrides[\n        (slim.conv2d, slim.separable_conv2d)] = {\'padding\': \'VALID\'}\n\n  if output_stride is not None:\n    if output_stride == 0 or (output_stride > 1 and output_stride % 2):\n      raise ValueError(\'Output stride must be None, 1 or a multiple of 2.\')\n\n  # a) Set the tensorflow scope\n  # b) set padding to default: note we might consider removing this\n  # since it is also set by mobilenet_scope\n  # c) set all defaults\n  # d) set all extra overrides.\n  with _scope_all(scope, default_scope=\'Mobilenet\'), \\\n      safe_arg_scope([slim.batch_norm], is_training=is_training), \\\n      _set_arg_scope_defaults(conv_defs_defaults), \\\n      _set_arg_scope_defaults(conv_defs_overrides):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    # Insert default parameters before the base scope which includes\n    # any custom overrides set in mobilenet.\n    end_points = {}\n    scopes = {}\n    for i, opdef in enumerate(conv_defs[\'spec\']):\n      params = dict(opdef.params)\n      opdef.multiplier_func(params, multiplier)\n      stride = params.get(\'stride\', 1)\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= stride\n      else:\n        layer_stride = stride\n        layer_rate = 1\n        current_stride *= stride\n      # Update params.\n      params[\'stride\'] = layer_stride\n      # Only insert rate to params if rate > 1.\n      if layer_rate > 1:\n        params[\'rate\'] = layer_rate\n      # Set padding\n      if use_explicit_padding:\n        if \'kernel_size\' in params:\n          net = _fixed_padding(net, params[\'kernel_size\'], layer_rate)\n        else:\n          params[\'use_explicit_padding\'] = True\n\n      end_point = \'layer_%d\' % (i + 1)\n      try:\n        net = opdef.op(net, **params)\n      except Exception:\n        print(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\n        raise\n      end_points[end_point] = net\n      scope = os.path.dirname(net.name)\n      scopes[scope] = end_point\n      if final_endpoint is not None and end_point == final_endpoint:\n        break\n\n    # Add all tensors that end with \'output\' to\n    # endpoints\n    for t in net.graph.get_operations():\n      scope = os.path.dirname(t.name)\n      bn = os.path.basename(t.name)\n      if scope in scopes and t.name.endswith(\'output\'):\n        end_points[scopes[scope] + \'/\' + bn] = t.outputs[0]\n    return net, end_points\n\n\n@contextlib.contextmanager\ndef _scope_all(scope, default_scope=None):\n  with tf.variable_scope(scope, default_name=default_scope) as s,\\\n       tf.name_scope(s.original_name_scope):\n    yield s\n\n\n@slim.add_arg_scope\ndef mobilenet(inputs,\n              num_classes=1001,\n              prediction_fn=slim.softmax,\n              reuse=None,\n              scope=\'Mobilenet\',\n              base_only=False,\n              **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_classes:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_classes\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n\ndef global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n  """"""Applies avg pool to produce 1x1 output.\n  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\n  baked in average pool which has better support across hardware.\n  Args:\n    input_tensor: input tensor\n    pool_op: pooling op (avg pool is default)\n  Returns:\n    a tensor batch_size x 1 x 1 x depth.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size = tf.convert_to_tensor(\n        [1, tf.shape(input_tensor)[1],\n         tf.shape(input_tensor)[2], 1])\n  else:\n    kernel_size = [1, shape[1], shape[2], 1]\n  output = pool_op(\n      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n  # Recover output shape, for unknown shape.\n  output.set_shape([None, 1, 1, None])\n  return output\n\n\ndef training_scope(is_training=True,\n                   weight_decay=0.00004,\n                   stddev=0.09,\n                   dropout_keep_prob=0.8,\n                   bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n     # the network created will be trainble with dropout/batch norm\n     # initialized appropriately.\n  Args:\n    is_training: if set to False this will ensure that all customizations are\n      set to non-training mode. This might be helpful for code that is reused\n      across both training/evaluation, but most of the time training_scope with\n      value False is not needed. If this is set to None, the parameters is not\n      added to the batch_norm arg_scope.\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: Standard deviation for initialization, if negative uses xavier.\n    dropout_keep_prob: dropout keep probability (not set if equals to None).\n    bn_decay: decay for the batch norm moving averages (not set if equals to\n      None).\n  Returns:\n    An argument scope to use via arg_scope.\n  """"""\n  # Note: do not introduce parameters that would change the inference\n  # model here (for example whether to use bias), modify conv_def instead.\n  batch_norm_params = {\n      \'decay\': bn_decay,\n      \'is_training\': is_training\n  }\n  if stddev < 0:\n    weight_intitializer = slim.initializers.xavier_initializer()\n  else:\n    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n      weights_initializer=weight_intitializer,\n      normalizer_fn=slim.batch_norm), \\\n      slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training),\\\n      safe_arg_scope([slim.batch_norm], **batch_norm_params), \\\n      safe_arg_scope([slim.dropout], is_training=is_training,\n                     keep_prob=dropout_keep_prob), \\\n      slim.arg_scope([slim.conv2d], \\\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\n      slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\n    return s\n'"
tf_pose/mobilenet/mobilenet_v2.py,5,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of Mobilenet V2.\nArchitecture: https://arxiv.org/abs/1801.04381\nThe base model gives 72.2% accuracy on ImageNet, with 300MMadds,\n3.4 M parameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport functools\n\nimport tensorflow as tf\n\nfrom tf_pose.mobilenet import conv_blocks as ops\nfrom tf_pose.mobilenet import mobilenet as lib\n\nslim = tf.contrib.slim\nop = lib.op\n\nexpand_input = ops.expand_input_by_factor\n\n# pyformat: disable\n# Architecture: https://arxiv.org/abs/1801.04381\nV2_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16),\n        op(ops.expanded_conv, stride=2, num_outputs=24),\n        op(ops.expanded_conv, stride=1, num_outputs=24),\n        op(ops.expanded_conv, stride=2, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=2, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=2, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=320),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)\n    ],\n)\n# pyformat: enable\n\n\n@slim.add_arg_scope\ndef mobilenet(input_tensor,\n              num_classes=1001,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              activation_fn=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n  Inference mode is created by default. To create training use training_scope\n  below.\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    activation_fn: Activation function to use, defaults to tf.nn.relu6 if not\n      specified.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediction_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  if \'multiplier\' in kwargs:\n    raise ValueError(\'mobilenetv2 doesn\\\'t support generic \'\n                     \'multiplier parameter use ""depth_multiplier"" instead.\')\n  if finegrain_classification_mode:\n    conv_defs = copy.deepcopy(conv_defs)\n    if depth_multiplier < 1:\n      conv_defs[\'spec\'][-1].params[\'num_outputs\'] /= depth_multiplier\n  if activation_fn:\n    conv_defs = copy.deepcopy(conv_defs)\n    defaults = conv_defs[\'defaults\']\n    conv_defaults = (\n        defaults[(slim.conv2d, slim.fully_connected, slim.separable_conv2d)])\n    conv_defaults[\'activation_fn\'] = activation_fn\n\n  depth_args = {}\n  # NB: do not set depth_args unless they are provided to avoid overriding\n  # whatever default depth_multiplier might have thanks to arg_scope.\n  if min_depth is not None:\n    depth_args[\'min_depth\'] = min_depth\n  if divisible_by is not None:\n    depth_args[\'divisible_by\'] = divisible_by\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet(\n        input_tensor,\n        num_classes=num_classes,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\nmobilenet.default_image_size = 224\n\n\ndef wrapped_partial(func, *args, **kwargs):\n  partial_func = functools.partial(func, *args, **kwargs)\n  functools.update_wrapper(partial_func, func)\n  return partial_func\n\n\n# Wrappers for mobilenet v2 with depth-multipliers. Be noticed that\n# \'finegrain_classification_mode\' is set to True, which means the embedding\n# layer will not be shrinked when given a depth-multiplier < 1.0.\nmobilenet_v2_140 = wrapped_partial(mobilenet, depth_multiplier=1.4)\nmobilenet_v2_050 = wrapped_partial(mobilenet, depth_multiplier=0.50,\n                                   finegrain_classification_mode=True)\nmobilenet_v2_035 = wrapped_partial(mobilenet, depth_multiplier=0.35,\n                                   finegrain_classification_mode=True)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):\n  """"""Creates base of the mobilenet (no pooling and no logits) .""""""\n  return mobilenet(input_tensor,\n                   depth_multiplier=depth_multiplier,\n                   base_only=True, **kwargs)\n\n\ndef training_scope(**kwargs):\n  """"""Defines MobilenetV2 training scope.\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n  with slim.\n  Args:\n    **kwargs: Passed to mobilenet.training_scope. The following parameters\n    are supported:\n      weight_decay- The weight decay to use for regularizing the model.\n      stddev-  Standard deviation for initialization, if negative uses xavier.\n      dropout_keep_prob- dropout keep probability\n      bn_decay- decay for the batch norm moving averages.\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  return lib.training_scope(**kwargs)\n\n\n__all__ = [\'training_scope\', \'mobilenet_base\', \'mobilenet\', \'V2_DEF\']\n'"
tf_pose/pafprocess/__init__.py,0,b''
tf_pose/pafprocess/pafprocess.py,0,"b'# This file was automatically generated by SWIG (http://www.swig.org).\n# Version 3.0.12\n#\n# Do not make changes to this file unless you know what you are doing--modify\n# the SWIG interface file instead.\n\nfrom sys import version_info as _swig_python_version_info\nif _swig_python_version_info >= (2, 7, 0):\n    def swig_import_helper():\n        import importlib\n        pkg = __name__.rpartition(\'.\')[0]\n        mname = \'.\'.join((pkg, \'_pafprocess\')).lstrip(\'.\')\n        try:\n            return importlib.import_module(mname)\n        except ImportError:\n            return importlib.import_module(\'_pafprocess\')\n    _pafprocess = swig_import_helper()\n    del swig_import_helper\nelif _swig_python_version_info >= (2, 6, 0):\n    def swig_import_helper():\n        from os.path import dirname\n        import imp\n        fp = None\n        try:\n            fp, pathname, description = imp.find_module(\'_pafprocess\', [dirname(__file__)])\n        except ImportError:\n            import _pafprocess\n            return _pafprocess\n        try:\n            _mod = imp.load_module(\'_pafprocess\', fp, pathname, description)\n        finally:\n            if fp is not None:\n                fp.close()\n        return _mod\n    _pafprocess = swig_import_helper()\n    del swig_import_helper\nelse:\n    import _pafprocess\ndel _swig_python_version_info\n\ntry:\n    _swig_property = property\nexcept NameError:\n    pass  # Python < 2.2 doesn\'t have \'property\'.\n\ntry:\n    import builtins as __builtin__\nexcept ImportError:\n    import __builtin__\n\ndef _swig_setattr_nondynamic(self, class_type, name, value, static=1):\n    if (name == ""thisown""):\n        return self.this.own(value)\n    if (name == ""this""):\n        if type(value).__name__ == \'SwigPyObject\':\n            self.__dict__[name] = value\n            return\n    method = class_type.__swig_setmethods__.get(name, None)\n    if method:\n        return method(self, value)\n    if (not static):\n        if _newclass:\n            object.__setattr__(self, name, value)\n        else:\n            self.__dict__[name] = value\n    else:\n        raise AttributeError(""You cannot add attributes to %s"" % self)\n\n\ndef _swig_setattr(self, class_type, name, value):\n    return _swig_setattr_nondynamic(self, class_type, name, value, 0)\n\n\ndef _swig_getattr(self, class_type, name):\n    if (name == ""thisown""):\n        return self.this.own()\n    method = class_type.__swig_getmethods__.get(name, None)\n    if method:\n        return method(self)\n    raise AttributeError(""\'%s\' object has no attribute \'%s\'"" % (class_type.__name__, name))\n\n\ndef _swig_repr(self):\n    try:\n        strthis = ""proxy of "" + self.this.__repr__()\n    except __builtin__.Exception:\n        strthis = """"\n    return ""<%s.%s; %s >"" % (self.__class__.__module__, self.__class__.__name__, strthis,)\n\ntry:\n    _object = object\n    _newclass = 1\nexcept __builtin__.Exception:\n    class _object:\n        pass\n    _newclass = 0\n\nclass Peak(_object):\n    __swig_setmethods__ = {}\n    __setattr__ = lambda self, name, value: _swig_setattr(self, Peak, name, value)\n    __swig_getmethods__ = {}\n    __getattr__ = lambda self, name: _swig_getattr(self, Peak, name)\n    __repr__ = _swig_repr\n    __swig_setmethods__[""x""] = _pafprocess.Peak_x_set\n    __swig_getmethods__[""x""] = _pafprocess.Peak_x_get\n    if _newclass:\n        x = _swig_property(_pafprocess.Peak_x_get, _pafprocess.Peak_x_set)\n    __swig_setmethods__[""y""] = _pafprocess.Peak_y_set\n    __swig_getmethods__[""y""] = _pafprocess.Peak_y_get\n    if _newclass:\n        y = _swig_property(_pafprocess.Peak_y_get, _pafprocess.Peak_y_set)\n    __swig_setmethods__[""score""] = _pafprocess.Peak_score_set\n    __swig_getmethods__[""score""] = _pafprocess.Peak_score_get\n    if _newclass:\n        score = _swig_property(_pafprocess.Peak_score_get, _pafprocess.Peak_score_set)\n    __swig_setmethods__[""id""] = _pafprocess.Peak_id_set\n    __swig_getmethods__[""id""] = _pafprocess.Peak_id_get\n    if _newclass:\n        id = _swig_property(_pafprocess.Peak_id_get, _pafprocess.Peak_id_set)\n\n    def __init__(self):\n        this = _pafprocess.new_Peak()\n        try:\n            self.this.append(this)\n        except __builtin__.Exception:\n            self.this = this\n    __swig_destroy__ = _pafprocess.delete_Peak\n    __del__ = lambda self: None\nPeak_swigregister = _pafprocess.Peak_swigregister\nPeak_swigregister(Peak)\ncvar = _pafprocess.cvar\nTHRESH_HEAT = cvar.THRESH_HEAT\nTHRESH_VECTOR_SCORE = cvar.THRESH_VECTOR_SCORE\nTHRESH_VECTOR_CNT1 = cvar.THRESH_VECTOR_CNT1\nTHRESH_PART_CNT = cvar.THRESH_PART_CNT\nTHRESH_HUMAN_SCORE = cvar.THRESH_HUMAN_SCORE\nNUM_PART = cvar.NUM_PART\nSTEP_PAF = cvar.STEP_PAF\nCOCOPAIRS_SIZE = cvar.COCOPAIRS_SIZE\nCOCOPAIRS_NET = cvar.COCOPAIRS_NET\nCOCOPAIRS = cvar.COCOPAIRS\n\nclass VectorXY(_object):\n    __swig_setmethods__ = {}\n    __setattr__ = lambda self, name, value: _swig_setattr(self, VectorXY, name, value)\n    __swig_getmethods__ = {}\n    __getattr__ = lambda self, name: _swig_getattr(self, VectorXY, name)\n    __repr__ = _swig_repr\n    __swig_setmethods__[""x""] = _pafprocess.VectorXY_x_set\n    __swig_getmethods__[""x""] = _pafprocess.VectorXY_x_get\n    if _newclass:\n        x = _swig_property(_pafprocess.VectorXY_x_get, _pafprocess.VectorXY_x_set)\n    __swig_setmethods__[""y""] = _pafprocess.VectorXY_y_set\n    __swig_getmethods__[""y""] = _pafprocess.VectorXY_y_get\n    if _newclass:\n        y = _swig_property(_pafprocess.VectorXY_y_get, _pafprocess.VectorXY_y_set)\n\n    def __init__(self):\n        this = _pafprocess.new_VectorXY()\n        try:\n            self.this.append(this)\n        except __builtin__.Exception:\n            self.this = this\n    __swig_destroy__ = _pafprocess.delete_VectorXY\n    __del__ = lambda self: None\nVectorXY_swigregister = _pafprocess.VectorXY_swigregister\nVectorXY_swigregister(VectorXY)\n\nclass ConnectionCandidate(_object):\n    __swig_setmethods__ = {}\n    __setattr__ = lambda self, name, value: _swig_setattr(self, ConnectionCandidate, name, value)\n    __swig_getmethods__ = {}\n    __getattr__ = lambda self, name: _swig_getattr(self, ConnectionCandidate, name)\n    __repr__ = _swig_repr\n    __swig_setmethods__[""idx1""] = _pafprocess.ConnectionCandidate_idx1_set\n    __swig_getmethods__[""idx1""] = _pafprocess.ConnectionCandidate_idx1_get\n    if _newclass:\n        idx1 = _swig_property(_pafprocess.ConnectionCandidate_idx1_get, _pafprocess.ConnectionCandidate_idx1_set)\n    __swig_setmethods__[""idx2""] = _pafprocess.ConnectionCandidate_idx2_set\n    __swig_getmethods__[""idx2""] = _pafprocess.ConnectionCandidate_idx2_get\n    if _newclass:\n        idx2 = _swig_property(_pafprocess.ConnectionCandidate_idx2_get, _pafprocess.ConnectionCandidate_idx2_set)\n    __swig_setmethods__[""score""] = _pafprocess.ConnectionCandidate_score_set\n    __swig_getmethods__[""score""] = _pafprocess.ConnectionCandidate_score_get\n    if _newclass:\n        score = _swig_property(_pafprocess.ConnectionCandidate_score_get, _pafprocess.ConnectionCandidate_score_set)\n    __swig_setmethods__[""etc""] = _pafprocess.ConnectionCandidate_etc_set\n    __swig_getmethods__[""etc""] = _pafprocess.ConnectionCandidate_etc_get\n    if _newclass:\n        etc = _swig_property(_pafprocess.ConnectionCandidate_etc_get, _pafprocess.ConnectionCandidate_etc_set)\n\n    def __init__(self):\n        this = _pafprocess.new_ConnectionCandidate()\n        try:\n            self.this.append(this)\n        except __builtin__.Exception:\n            self.this = this\n    __swig_destroy__ = _pafprocess.delete_ConnectionCandidate\n    __del__ = lambda self: None\nConnectionCandidate_swigregister = _pafprocess.ConnectionCandidate_swigregister\nConnectionCandidate_swigregister(ConnectionCandidate)\n\nclass Connection(_object):\n    __swig_setmethods__ = {}\n    __setattr__ = lambda self, name, value: _swig_setattr(self, Connection, name, value)\n    __swig_getmethods__ = {}\n    __getattr__ = lambda self, name: _swig_getattr(self, Connection, name)\n    __repr__ = _swig_repr\n    __swig_setmethods__[""cid1""] = _pafprocess.Connection_cid1_set\n    __swig_getmethods__[""cid1""] = _pafprocess.Connection_cid1_get\n    if _newclass:\n        cid1 = _swig_property(_pafprocess.Connection_cid1_get, _pafprocess.Connection_cid1_set)\n    __swig_setmethods__[""cid2""] = _pafprocess.Connection_cid2_set\n    __swig_getmethods__[""cid2""] = _pafprocess.Connection_cid2_get\n    if _newclass:\n        cid2 = _swig_property(_pafprocess.Connection_cid2_get, _pafprocess.Connection_cid2_set)\n    __swig_setmethods__[""score""] = _pafprocess.Connection_score_set\n    __swig_getmethods__[""score""] = _pafprocess.Connection_score_get\n    if _newclass:\n        score = _swig_property(_pafprocess.Connection_score_get, _pafprocess.Connection_score_set)\n    __swig_setmethods__[""peak_id1""] = _pafprocess.Connection_peak_id1_set\n    __swig_getmethods__[""peak_id1""] = _pafprocess.Connection_peak_id1_get\n    if _newclass:\n        peak_id1 = _swig_property(_pafprocess.Connection_peak_id1_get, _pafprocess.Connection_peak_id1_set)\n    __swig_setmethods__[""peak_id2""] = _pafprocess.Connection_peak_id2_set\n    __swig_getmethods__[""peak_id2""] = _pafprocess.Connection_peak_id2_get\n    if _newclass:\n        peak_id2 = _swig_property(_pafprocess.Connection_peak_id2_get, _pafprocess.Connection_peak_id2_set)\n\n    def __init__(self):\n        this = _pafprocess.new_Connection()\n        try:\n            self.this.append(this)\n        except __builtin__.Exception:\n            self.this = this\n    __swig_destroy__ = _pafprocess.delete_Connection\n    __del__ = lambda self: None\nConnection_swigregister = _pafprocess.Connection_swigregister\nConnection_swigregister(Connection)\n\n\ndef process_paf(p1, h1, f1):\n    return _pafprocess.process_paf(p1, h1, f1)\nprocess_paf = _pafprocess.process_paf\n\ndef get_num_humans():\n    return _pafprocess.get_num_humans()\nget_num_humans = _pafprocess.get_num_humans\n\ndef get_part_cid(human_id, part_id):\n    return _pafprocess.get_part_cid(human_id, part_id)\nget_part_cid = _pafprocess.get_part_cid\n\ndef get_score(human_id):\n    return _pafprocess.get_score(human_id)\nget_score = _pafprocess.get_score\n\ndef get_part_x(cid):\n    return _pafprocess.get_part_x(cid)\nget_part_x = _pafprocess.get_part_x\n\ndef get_part_y(cid):\n    return _pafprocess.get_part_y(cid)\nget_part_y = _pafprocess.get_part_y\n\ndef get_part_score(cid):\n    return _pafprocess.get_part_score(cid)\nget_part_score = _pafprocess.get_part_score\n# This file is compatible with both classic and new-style classes.\n\n\n'"
tf_pose/pafprocess/setup.py,0,"b'from distutils.core import setup, Extension\nimport numpy\nimport os\n\n# os.environ[\'CC\'] = \'g++\';\nsetup(name=\'pafprocess_ext\', version=\'1.0\',\n    ext_modules=[\n        Extension(\'_pafprocess\', [\'pafprocess.cpp\', \'pafprocess.i\'],\n                  swig_opts=[\'-c++\'],\n                  depends=[""pafprocess.h""],\n                  include_dirs=[numpy.get_include(), \'.\'])\n    ],\n    py_modules=[\n        ""pafprocess""\n    ]\n)\n'"
tf_pose/slidingwindow/ArrayUtils.py,0,"b'import math, mmap, tempfile\nimport numpy as np\nimport psutil\n\ndef _requiredSize(shape, dtype):\n\t""""""\n\tDetermines the number of bytes required to store a NumPy array with\n\tthe specified shape and datatype.\n\t""""""\n\treturn math.floor(np.prod(np.asarray(shape, dtype=np.uint64)) * np.dtype(dtype).itemsize)\n\n\nclass TempfileBackedArray(np.ndarray):\n\t""""""\n\tA NumPy ndarray that uses a memory-mapped temp file as its backing \n\t""""""\n\t\n\tdef __new__(subtype, shape, dtype=float, buffer=None, offset=0, strides=None, order=None, info=None):\n\t\t\n\t\t# Determine the size in bytes required to hold the array\n\t\tnumBytes = _requiredSize(shape, dtype)\n\t\t\n\t\t# Create the temporary file, resize it, and map it into memory\n\t\ttempFile = tempfile.TemporaryFile()\n\t\ttempFile.truncate(numBytes)\n\t\tbuf = mmap.mmap(tempFile.fileno(), numBytes, access=mmap.ACCESS_WRITE)\n\t\t\n\t\t# Create the ndarray with the memory map as the underlying buffer\n\t\tobj = super(TempfileBackedArray, subtype).__new__(subtype, shape, dtype, buf, 0, None, order)\n\t\t\n\t\t# Attach the file reference to the ndarray object\n\t\tobj._file = tempFile\n\t\treturn obj\n\t\n\tdef __array_finalize__(self, obj):\n\t\tif obj is None: return\n\t\tself._file = getattr(obj, \'_file\', None)\n\n\ndef arrayFactory(shape, dtype=float):\n\t""""""\n\tCreates a new ndarray of the specified shape and datatype, storing\n\tit in memory if there is sufficient available space or else using\n\ta memory-mapped temporary file to provide the underlying buffer.\n\t""""""\n\t\n\t# Determine the number of bytes required to store the array\n\trequiredBytes = _requiredSize(shape, dtype)\n\t\n\t# Determine if there is sufficient available memory\n\tvmem = psutil.virtual_memory()\n\tif vmem.available > requiredBytes:\n\t\treturn np.ndarray(shape=shape, dtype=dtype)\n\telse:\n\t\treturn TempfileBackedArray(shape=shape, dtype=dtype)\n\n\ndef zerosFactory(shape, dtype=float):\n\t""""""\n\tCreates a new NumPy array using `arrayFactory()` and fills it with zeros.\n\t""""""\n\tarr = arrayFactory(shape=shape, dtype=dtype)\n\tarr.fill(0)\n\treturn arr\n\n\ndef arrayCast(source, dtype):\n\t""""""\n\tCasts a NumPy array to the specified datatype, storing the copy\n\tin memory if there is sufficient available space or else using a\n\tmemory-mapped temporary file to provide the underlying buffer.\n\t""""""\n\t\n\t# Determine the number of bytes required to store the array\n\trequiredBytes = _requiredSize(source.shape, dtype)\n\t\n\t# Determine if there is sufficient available memory\n\tvmem = psutil.virtual_memory()\n\tif vmem.available > requiredBytes:\n\t\treturn source.astype(dtype, subok=False)\n\telse:\n\t\tdest = arrayFactory(source.shape, dtype)\n\t\tnp.copyto(dest, source, casting=\'unsafe\')\n\t\treturn dest\n\n\ndef determineMaxWindowSize(dtype, limit=None):\n\t""""""\n\tDetermines the largest square window size that can be used, based on\n\tthe specified datatype and amount of currently available system memory.\n\t\n\tIf `limit` is specified, then this value will be returned in the event\n\tthat it is smaller than the maximum computed size.\n\t""""""\n\tvmem = psutil.virtual_memory()\n\tmaxSize = math.floor(math.sqrt(vmem.available / np.dtype(dtype).itemsize))\n\tif limit is None or limit >= maxSize:\n\t\treturn maxSize\n\telse:\n\t\treturn limit\n'"
tf_pose/slidingwindow/Batching.py,0,"b'import numpy as np\n\ndef batchWindows(windows, batchSize):\n\t""""""\n\tSplits a list of windows into a series of batches.\n\t""""""\n\treturn np.array_split(np.array(windows), len(windows) // batchSize)\n'"
tf_pose/slidingwindow/Merging.py,0,"b'from .SlidingWindow import generate\nfrom .Batching import batchWindows\nimport numpy as np\n\ndef mergeWindows(data, dimOrder, maxWindowSize, overlapPercent, batchSize, transform, progressCallback = None):\n\t""""""\n\tGenerates sliding windows for the specified dataset and applies the specified\n\ttransformation function to each window. Where multiple overlapping windows\n\tinclude an element of the input dataset, the overlap is resolved by computing\n\tthe mean transform result value for that element.\n\t\n\tIrrespective of the order of the dimensions of the input dataset, the\n\ttransformation function should return a NumPy array with dimensions\n\t[batch, height, width, resultChannels].\n\t\n\tIf a progress callback is supplied, it will be called immediately before\n\tapplying the transformation function to each batch of windows. The callback\n\tshould accept the current batch index and number of batches as arguments.\n\t""""""\n\t\n\t# Determine the dimensions of the input data\n\tsourceWidth = data.shape[dimOrder.index(\'w\')]\n\tsourceHeight = data.shape[dimOrder.index(\'h\')]\n\t\n\t# Generate the sliding windows and group them into batches\n\twindows = generate(data, dimOrder, maxWindowSize, overlapPercent)\n\tbatches = batchWindows(windows, batchSize)\n\t\n\t# Apply the transform to the first batch of windows and determine the result dimensionality\n\texemplarResult = transform(data, batches[0])\n\tresultDimensions = exemplarResult.shape[ len(exemplarResult.shape) - 1 ]\n\t\n\t# Create the matrices to hold the sums and counts for the transform result values\n\tsums = np.zeros((sourceHeight, sourceWidth, resultDimensions), dtype=np.float)\n\tcounts = np.zeros((sourceHeight, sourceWidth), dtype=np.uint32)\n\t\n\t# Iterate over the batches and apply the transformation function to each batch\n\tfor batchNum, batch in enumerate(batches):\n\t\t\n\t\t# If a progress callback was supplied, call it\n\t\tif progressCallback != None:\n\t\t\tprogressCallback(batchNum, len(batches))\n\t\t\n\t\t# Apply the transformation function to the current batch\n\t\tbatchResult = transform(data, batch)\n\t\t\n\t\t# Iterate over the windows in the batch and update the sums matrix\n\t\tfor windowNum, window in enumerate(batch):\n\t\t\t\n\t\t\t# Create views into the larger matrices that correspond to the current window\n\t\t\twindowIndices = window.indices(False)\n\t\t\tsumsView = sums[windowIndices]\n\t\t\tcountsView = counts[windowIndices]\n\t\t\t\n\t\t\t# Update the result sums for each of the dataset elements in the window\n\t\t\tsumsView[:] += batchResult[windowNum]\n\t\t\tcountsView[:] += 1\n\t\n\t# Use the sums and the counts to compute the mean values\n\tfor dim in range(0, resultDimensions):\n\t\tsums[:,:,dim] /= counts\n\t\n\t# Return the mean values\n\treturn sums\n'"
tf_pose/slidingwindow/RectangleUtils.py,0,"b'import numpy as np\nimport math\n\ndef cropRect(rect, cropTop, cropBottom, cropLeft, cropRight):\n\t""""""\n\tCrops a rectangle by the specified number of pixels on each side.\n\t\n\tThe input rectangle and return value are both a tuple of (x,y,w,h).\n\t""""""\n\t\n\t# Unpack the rectangle\n\tx, y, w, h = rect\n\t\n\t# Crop by the specified value\n\tx += cropLeft\n\ty += cropTop\n\tw -= (cropLeft + cropRight)\n\th -= (cropTop + cropBottom)\n\t\n\t# Re-pack the padded rect\n\treturn (x,y,w,h)\n\n\ndef padRect(rect, padTop, padBottom, padLeft, padRight, bounds, clipExcess = True):\n\t""""""\n\tPads a rectangle by the specified values on each individual side,\n\tensuring the padded rectangle falls within the specified bounds.\n\t\n\tThe input rectangle, bounds, and return value are all a tuple of (x,y,w,h).\n\t""""""\n\t\n\t# Unpack the rectangle\n\tx, y, w, h = rect\n\t\n\t# Pad by the specified value\n\tx -= padLeft\n\ty -= padTop\n\tw += (padLeft + padRight)\n\th += (padTop + padBottom)\n\t\n\t# Determine if we are clipping overflows/underflows or\n\t# shifting the centre of the rectangle to compensate\n\tif clipExcess == True:\n\t\t\n\t\t# Clip any underflows\n\t\tx = max(0, x)\n\t\ty = max(0, y)\n\t\t\n\t\t# Clip any overflows\n\t\toverflowY = max(0, (y + h) - bounds[0])\n\t\toverflowX = max(0, (x + w) - bounds[1])\n\t\th -= overflowY\n\t\tw -= overflowX\n\t\t\n\telse:\n\t\t\n\t\t# Compensate for any underflows\n\t\tunderflowX = max(0, 0 - x)\n\t\tunderflowY = max(0, 0 - y)\n\t\tx += underflowX\n\t\ty += underflowY\n\t\t\n\t\t# Compensate for any overflows\n\t\toverflowY = max(0, (y + h) - bounds[0])\n\t\toverflowX = max(0, (x + w) - bounds[1])\n\t\tx -= overflowX\n\t\tw += overflowX\n\t\ty -= overflowY\n\t\th += overflowY\n\t\t\n\t\t# If there are still overflows or underflows after our\n\t\t# modifications, we have no choice but to clip them\n\t\tx, y, w, h = padRect((x,y,w,h), 0, 0, 0, 0, bounds, True)\n\t\n\t# Re-pack the padded rect\n\treturn (x,y,w,h)\n\n\ndef cropRectEqually(rect, cropping):\n\t""""""\n\tCrops a rectangle by the specified number of pixels on all sides.\n\t\n\tThe input rectangle and return value are both a tuple of (x,y,w,h).\n\t""""""\n\treturn cropRect(rect, cropping, cropping, cropping, cropping)\n\n\ndef padRectEqually(rect, padding, bounds, clipExcess = True):\n\t""""""\n\tApplies equal padding to all sides of a rectangle,\n\tensuring the padded rectangle falls within the specified bounds.\n\t\n\tThe input rectangle, bounds, and return value are all a tuple of (x,y,w,h).\n\t""""""\n\treturn padRect(rect, padding, padding, padding, padding, bounds, clipExcess)\n\n\ndef squareAspect(rect):\n\t""""""\n\tCrops either the width or height, as necessary, to make a rectangle into a square.\n\t\n\tThe input rectangle and return value are both a tuple of (x,y,w,h).\n\t""""""\n\t\n\t# Determine which dimension needs to be cropped\n\tx,y,w,h = rect\n\tif w > h:\n\t\tcropX = (w - h) // 2\n\t\treturn cropRect(rect, 0, 0, cropX, cropX)\n\telif w < h:\n\t\tcropY = (h - w) // 2\n\t\treturn cropRect(rect, cropY, cropY, 0, 0)\n\t\n\t# Already a square\n\treturn rect\n\n\ndef fitToSize(rect, targetWidth, targetHeight, bounds):\n\t""""""\n\tPads or crops a rectangle as necessary to achieve the target dimensions,\n\tensuring the modified rectangle falls within the specified bounds.\n\t\n\tThe input rectangle, bounds, and return value are all a tuple of (x,y,w,h).\n\t""""""\n\t\n\t# Determine the difference between the current size and target size\n\tx,y,w,h = rect\n\tdiffX = w - targetWidth\n\tdiffY = h - targetHeight\n\t\n\t# Determine if we are cropping or padding the width\n\tif diffX > 0:\n\t\tcropLeft  = math.floor(diffX / 2)\n\t\tcropRight = diffX - cropLeft\n\t\tx,y,w,h   = cropRect((x,y,w,h), 0, 0, cropLeft, cropRight)\n\telif diffX < 0:\n\t\tpadLeft  = math.floor(abs(diffX) / 2)\n\t\tpadRight = abs(diffX) - padLeft\n\t\tx,y,w,h  = padRect((x,y,w,h), 0, 0, padLeft, padRight, bounds, False)\n\t\n\t# Determine if we are cropping or padding the height\n\tif diffY > 0:\n\t\tcropTop    = math.floor(diffY / 2)\n\t\tcropBottom = diffY - cropTop\n\t\tx,y,w,h    = cropRect((x,y,w,h), cropTop, cropBottom, 0, 0)\n\telif diffY < 0:\n\t\tpadTop    = math.floor(abs(diffY) / 2)\n\t\tpadBottom = abs(diffY) - padTop\n\t\tx,y,w,h   = padRect((x,y,w,h), padTop, padBottom, 0, 0, bounds, False)\n\t\n\treturn (x,y,w,h)\n'"
tf_pose/slidingwindow/SlidingWindow.py,0,"b'import math\n\nclass DimOrder(object):\n\t""""""\n\tRepresents the order of the dimensions in a dataset\'s shape.\n\t""""""\n\tChannelHeightWidth = [\'c\', \'h\', \'w\']\n\tHeightWidthChannel = [\'h\', \'w\', \'c\']\n\n\nclass SlidingWindow(object):\n\t""""""\n\tRepresents a single window into a larger dataset.\n\t""""""\n\t\n\tdef __init__(self, x, y, w, h, dimOrder, transform = None):\n\t\t""""""\n\t\tCreates a new window with the specified dimensions and transform\n\t\t""""""\n\t\tself.x = x\n\t\tself.y = y\n\t\tself.w = w\n\t\tself.h = h\n\t\tself.dimOrder = dimOrder\n\t\tself.transform = transform\n\t\n\tdef apply(self, matrix):\n\t\t""""""\n\t\tSlices the supplied matrix and applies any transform bound to this window\n\t\t""""""\n\t\tview = matrix[ self.indices() ]\n\t\treturn self.transform(view) if self.transform != None else view\n\t\n\tdef getRect(self):\n\t\t""""""\n\t\tReturns the window bounds as a tuple of (x,y,w,h)\n\t\t""""""\n\t\treturn (self.x, self.y, self.w, self.h)\n\t\n\tdef setRect(self, rect):\n\t\t""""""\n\t\tSets the window bounds from a tuple of (x,y,w,h)\n\t\t""""""\n\t\tself.x, self.y, self.w, self.h = rect\n\t\n\tdef indices(self, includeChannel=True):\n\t\t""""""\n\t\tRetrieves the indices for this window as a tuple of slices\n\t\t""""""\n\t\tif self.dimOrder == DimOrder.HeightWidthChannel:\n\t\t\t\n\t\t\t# Equivalent to [self.y:self.y+self.h+1, self.x:self.x+self.w+1]\n\t\t\treturn (\n\t\t\t\tslice(self.y, self.y+self.h),\n\t\t\t\tslice(self.x, self.x+self.w)\n\t\t\t)\n\t\t\t\n\t\telif self.dimOrder == DimOrder.ChannelHeightWidth:\n\t\t\t\n\t\t\tif includeChannel is True:\n\t\t\t\t\n\t\t\t\t# Equivalent to [:, self.y:self.y+self.h+1, self.x:self.x+self.w+1]\n\t\t\t\treturn (\n\t\t\t\t\tslice(None, None),\n\t\t\t\t\tslice(self.y, self.y+self.h),\n\t\t\t\t\tslice(self.x, self.x+self.w)\n\t\t\t\t)\n\t\t\t\t\n\t\t\telse:\n\t\t\t\t\n\t\t\t\t# Equivalent to [self.y:self.y+self.h+1, self.x:self.x+self.w+1]\n\t\t\t\treturn (\n\t\t\t\t\tslice(self.y, self.y+self.h),\n\t\t\t\t\tslice(self.x, self.x+self.w)\n\t\t\t\t)\n\t\t\t\n\t\telse:\n\t\t\traise Error(\'Unsupported order of dimensions: \' + str(self.dimOrder))\n\t\t\n\tdef __str__(self):\n\t\treturn \'(\' + str(self.x) + \',\' + str(self.y) + \',\' + str(self.w) + \',\' + str(self.h) + \')\'\n\t\n\tdef __repr__(self):\n\t\treturn self.__str__()\n\n\ndef generate(data, dimOrder, maxWindowSizeW, maxWindowSizeH, overlapPercent, transforms = []):\n\t""""""\n\tGenerates a set of sliding windows for the specified dataset.\n\t""""""\n\t\n\t# Determine the dimensions of the input data\n\twidth = data.shape[dimOrder.index(\'w\')]\n\theight = data.shape[dimOrder.index(\'h\')]\n\t\n\t# Generate the windows\n\treturn generateForSize(width, height, dimOrder, maxWindowSizeW, maxWindowSizeH, overlapPercent, transforms)\n\n\ndef generateForSize(width, height, dimOrder, maxWindowSizeW, maxWindowSizeH, overlapPercent, transforms = []):\n\t""""""\n\tGenerates a set of sliding windows for a dataset with the specified dimensions and order.\n\t""""""\n\t\n\t# If the input data is smaller than the specified window size,\n\t# clip the window size to the input size on both dimensions\n\twindowSizeX = min(maxWindowSizeW, width)\n\twindowSizeY = min(maxWindowSizeH, height)\n\t\n\t# Compute the window overlap and step size\n\twindowOverlapX = int(math.floor(windowSizeX * overlapPercent))\n\twindowOverlapY = int(math.floor(windowSizeY * overlapPercent))\n\tstepSizeX = windowSizeX - windowOverlapX\n\tstepSizeY = windowSizeY - windowOverlapY\n\t\n\t# Determine how many windows we will need in order to cover the input data\n\tlastX = width - windowSizeX\n\tlastY = height - windowSizeY\n\txOffsets = list(range(0, lastX+1, stepSizeX))\n\tyOffsets = list(range(0, lastY+1, stepSizeY))\n\t\n\t# Unless the input data dimensions are exact multiples of the step size,\n\t# we will need one additional row and column of windows to get 100% coverage\n\tif len(xOffsets) == 0 or xOffsets[-1] != lastX:\n\t\txOffsets.append(lastX)\n\tif len(yOffsets) == 0 or yOffsets[-1] != lastY:\n\t\tyOffsets.append(lastY)\n\t\n\t# Generate the list of windows\n\twindows = []\n\tfor xOffset in xOffsets:\n\t\tfor yOffset in yOffsets:\n\t\t\tfor transform in [None] + transforms:\n\t\t\t\twindows.append(SlidingWindow(\n\t\t\t\t\tx=xOffset,\n\t\t\t\t\ty=yOffset,\n\t\t\t\t\tw=windowSizeX,\n\t\t\t\t\th=windowSizeY,\n\t\t\t\t\tdimOrder=dimOrder,\n\t\t\t\t\ttransform=transform\n\t\t\t\t))\n\t\n\treturn windows\n'"
tf_pose/slidingwindow/WindowDistance.py,0,"b'from .ArrayUtils import *\nimport numpy as np\nimport math\n\ndef generateDistanceMatrix(width, height):\n\t""""""\n\tGenerates a matrix specifying the distance of each point in a window to its centre.\n\t""""""\n\t\n\t# Determine the coordinates of the exact centre of the window\n\toriginX = width / 2\n\toriginY = height / 2\n\t\n\t# Generate the distance matrix\n\tdistances = zerosFactory((height,width), dtype=np.float)\n\tfor index, val in np.ndenumerate(distances):\n\t\ty,x = index\n\t\tdistances[(y,x)] = math.sqrt( math.pow(x - originX, 2) + math.pow(y - originY, 2) )\n\t\n\treturn distances\n'"
tf_pose/slidingwindow/__init__.py,0,"b'from .SlidingWindow import DimOrder, SlidingWindow, generate, generateForSize\nfrom .WindowDistance import generateDistanceMatrix\nfrom .RectangleUtils import *\nfrom .ArrayUtils import *\nfrom .Batching import *\nfrom .Merging import *\n'"
tf_pose/tensblur/__init__.py,0,b''
tf_pose/tensblur/smoother.py,4,"b'# vim: sta:et:sw=2:ts=2:sts=2\n# Written by Antonio Loquercio\n\nimport numpy as np\nimport scipy.stats as st\nimport pdb\n\nimport tensorflow as tf\n\n\ndef layer(op):\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.terminals) == 0:\n            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n        elif len(self.terminals) == 1:\n            layer_input = self.terminals[0]\n        else:\n            layer_input = list(self.terminals)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n\n    return layer_decorated\n\n\nclass Smoother(object):\n    def __init__(self, inputs, filter_size, sigma, heat_map_size=0):\n        self.inputs = inputs\n        self.terminals = []\n        self.layers = dict(inputs)\n        self.filter_size = filter_size\n        self.sigma = sigma\n        self.heat_map_size = heat_map_size\n        self.setup()\n\n    def setup(self):\n        self.feed(\'data\').conv(name=\'smoothing\')\n\n    def get_unique_name(self, prefix):\n        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n        return \'%s_%d\' % (prefix, ident)\n\n    def feed(self, *args):\n        assert len(args) != 0\n        self.terminals = []\n        for fed_layer in args:\n            if isinstance(fed_layer, str):\n                try:\n                    fed_layer = self.layers[fed_layer]\n                except KeyError:\n                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n            self.terminals.append(fed_layer)\n        return self\n\n    def gauss_kernel(self, kernlen=21, nsig=3, channels=1):\n        interval = (2*nsig+1.)/(kernlen)\n        x = np.linspace(-nsig-interval/2., nsig+interval/2., kernlen+1)\n        kern1d = np.diff(st.norm.cdf(x))\n        kernel_raw = np.sqrt(np.outer(kern1d, kern1d))\n        kernel = kernel_raw/kernel_raw.sum()\n        out_filter = np.array(kernel, dtype = np.float32)\n        out_filter = out_filter.reshape((kernlen, kernlen, 1, 1))\n        out_filter = np.repeat(out_filter, channels, axis = 2)\n        return out_filter\n\n    def make_gauss_var(self, name, size, sigma, c_i):\n        # with tf.device(""/cpu:0""):\n        kernel = self.gauss_kernel(size, sigma, c_i)\n        var = tf.Variable(tf.convert_to_tensor(kernel), name=name)\n        return var\n\n    def get_output(self):\n        \'\'\'Returns the smoother output.\'\'\'\n        return self.terminals[-1]\n\n    @layer\n    def conv(self,\n             input,\n             name,\n             padding=\'SAME\'):\n        # Get the number of channels in the input\n        if self.heat_map_size != 0:\n            c_i = self.heat_map_size \n        else:\n            c_i = input.get_shape().as_list()[3]\n        # Convolution for a given input and kernel\n        convolve = lambda i, k: tf.nn.depthwise_conv2d(i, k, [1, 1, 1, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n            kernel = self.make_gauss_var(\'gauss_weight\', self.filter_size, self.sigma, c_i)\n            output = convolve(input, kernel)\n        return output\n'"
models/graph/cmu/__init__.py,0,b''
models/graph/mobilenet_thin/__init__.py,0,b''
