file_path,api_count,code
attention.py,11,"b'import tensorflow as tf\n\n\ndef attention(inputs, attention_size, time_major=False, return_alphas=False):\n    """"""\n    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n\n    The idea was proposed in the article by Z. Yang et al., ""Hierarchical Attention Networks\n     for Document Classification"", 2016: http://www.aclweb.org/anthology/N16-1174.\n    Variables notation is also inherited from the article\n    \n    Args:\n        inputs: The Attention inputs.\n            Matches outputs of RNN/Bi-RNN layer (not final state):\n                In case of RNN, this must be RNN outputs `Tensor`:\n                    If time_major == False (default), this must be a tensor of shape:\n                        `[batch_size, max_time, cell.output_size]`.\n                    If time_major == True, this must be a tensor of shape:\n                        `[max_time, batch_size, cell.output_size]`.\n                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n                the backward RNN outputs `Tensor`.\n                    If time_major == False (default),\n                        outputs_fw is a `Tensor` shaped:\n                        `[batch_size, max_time, cell_fw.output_size]`\n                        and outputs_bw is a `Tensor` shaped:\n                        `[batch_size, max_time, cell_bw.output_size]`.\n                    If time_major == True,\n                        outputs_fw is a `Tensor` shaped:\n                        `[max_time, batch_size, cell_fw.output_size]`\n                        and outputs_bw is a `Tensor` shaped:\n                        `[max_time, batch_size, cell_bw.output_size]`.\n        attention_size: Linear size of the Attention weights.\n        time_major: The shape format of the `inputs` Tensors.\n            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n            Using `time_major = True` is a bit more efficient because it avoids\n            transposes at the beginning and end of the RNN calculation.  However,\n            most TensorFlow data is batch-major, so by default this function\n            accepts input and emits output in batch-major form.\n        return_alphas: Whether to return attention coefficients variable along with layer\'s output.\n            Used for visualization purpose.\n    Returns:\n        The Attention output `Tensor`.\n        In case of RNN, this will be a `Tensor` shaped:\n            `[batch_size, cell.output_size]`.\n        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n    """"""\n\n    if isinstance(inputs, tuple):\n        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n        inputs = tf.concat(inputs, 2)\n\n    if time_major:\n        # (T,B,D) => (B,T,D)\n        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n\n    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n\n    initializer = tf.random_normal_initializer(stddev=0.1)\n\n    # Trainable parameters\n    w_omega = tf.get_variable(name=""w_omega"", shape=[hidden_size, attention_size], initializer=initializer)\n    b_omega = tf.get_variable(name=""b_omega"", shape=[attention_size], initializer=initializer)\n    u_omega = tf.get_variable(name=""u_omega"", shape=[attention_size], initializer=initializer)\n\n    with tf.name_scope(\'v\'):\n        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n\n    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n    vu = tf.tensordot(v, u_omega, axes=1, name=\'vu\')  # (B,T) shape\n    alphas = tf.nn.softmax(vu, name=\'alphas\')         # (B,T) shape\n\n    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n\n    if not return_alphas:\n        return output\n    else:\n        return output, alphas\n'"
train.py,33,"b'#!/usr/bin/python\n""""""\nToy example of attention layer use\n\nTrain RNN (GRU) on IMDB dataset (binary classification)\nLearning and hyper-parameters were not tuned; script serves as an example \n""""""\nfrom __future__ import print_function, division\n\nimport numpy as np\nimport tensorflow as tf\nfrom keras.datasets import imdb\nfrom tensorflow.contrib.rnn import GRUCell\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\nfrom tqdm import tqdm\n\nfrom attention import attention\nfrom utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generator\n\nNUM_WORDS = 10000\nINDEX_FROM = 3\nSEQUENCE_LENGTH = 250\nEMBEDDING_DIM = 100\nHIDDEN_SIZE = 150\nATTENTION_SIZE = 50\nKEEP_PROB = 0.8\nBATCH_SIZE = 256\nNUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that\'s why train for a few epochs\nDELTA = 0.5\nMODEL_PATH = \'./model\'\n\n# Load the data set\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n\n# Sequences pre-processing\nvocabulary_size = get_vocabulary_size(X_train)\nX_test = fit_in_vocabulary(X_test, vocabulary_size)\nX_train = zero_pad(X_train, SEQUENCE_LENGTH)\nX_test = zero_pad(X_test, SEQUENCE_LENGTH)\n\n# Different placeholders\nwith tf.name_scope(\'Inputs\'):\n    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name=\'batch_ph\')\n    target_ph = tf.placeholder(tf.float32, [None], name=\'target_ph\')\n    seq_len_ph = tf.placeholder(tf.int32, [None], name=\'seq_len_ph\')\n    keep_prob_ph = tf.placeholder(tf.float32, name=\'keep_prob_ph\')\n\n# Embedding layer\nwith tf.name_scope(\'Embedding_layer\'):\n    embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n    tf.summary.histogram(\'embeddings_var\', embeddings_var)\n    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)\n\n# (Bi-)RNN layer(-s)\nrnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\ntf.summary.histogram(\'RNN_outputs\', rnn_outputs)\n\n# Attention layer\nwith tf.name_scope(\'Attention_layer\'):\n    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n    tf.summary.histogram(\'alphas\', alphas)\n\n# Dropout\ndrop = tf.nn.dropout(attention_output, keep_prob_ph)\n\n# Fully connected layer\nwith tf.name_scope(\'Fully_connected_layer\'):\n    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n    b = tf.Variable(tf.constant(0., shape=[1]))\n    y_hat = tf.nn.xw_plus_b(drop, W, b)\n    y_hat = tf.squeeze(y_hat)\n    tf.summary.histogram(\'W\', W)\n\nwith tf.name_scope(\'Metrics\'):\n    # Cross-entropy loss and optimizer initialization\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n    tf.summary.scalar(\'loss\', loss)\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n\n    # Accuracy metric\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n    tf.summary.scalar(\'accuracy\', accuracy)\n\nmerged = tf.summary.merge_all()\n\n# Batch generators\ntrain_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\ntest_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n\ntrain_writer = tf.summary.FileWriter(\'./logdir/train\', accuracy.graph)\ntest_writer = tf.summary.FileWriter(\'./logdir/test\', accuracy.graph)\n\nsession_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n\nsaver = tf.train.Saver()\n\nif __name__ == ""__main__"":\n    with tf.Session(config=session_conf) as sess:\n        sess.run(tf.global_variables_initializer())\n        print(""Start learning..."")\n        for epoch in range(NUM_EPOCHS):\n            loss_train = 0\n            loss_test = 0\n            accuracy_train = 0\n            accuracy_test = 0\n\n            print(""epoch: {}\\t"".format(epoch), end="""")\n\n            # Training\n            num_batches = X_train.shape[0] // BATCH_SIZE\n            for b in tqdm(range(num_batches)):\n                x_batch, y_batch = next(train_batch_generator)\n                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n                                                    feed_dict={batch_ph: x_batch,\n                                                               target_ph: y_batch,\n                                                               seq_len_ph: seq_len,\n                                                               keep_prob_ph: KEEP_PROB})\n                accuracy_train += acc\n                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n                train_writer.add_summary(summary, b + num_batches * epoch)\n            accuracy_train /= num_batches\n\n            # Testing\n            num_batches = X_test.shape[0] // BATCH_SIZE\n            for b in tqdm(range(num_batches)):\n                x_batch, y_batch = next(test_batch_generator)\n                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n                                                         feed_dict={batch_ph: x_batch,\n                                                                    target_ph: y_batch,\n                                                                    seq_len_ph: seq_len,\n                                                                    keep_prob_ph: 1.0})\n                accuracy_test += acc\n                loss_test += loss_test_batch\n                test_writer.add_summary(summary, b + num_batches * epoch)\n            accuracy_test /= num_batches\n            loss_test /= num_batches\n\n            print(""loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}"".format(\n                loss_train, loss_test, accuracy_train, accuracy_test\n            ))\n        train_writer.close()\n        test_writer.close()\n        saver.save(sess, MODEL_PATH)\n        print(""Run \'tensorboard --logdir=./logdir\' to checkout tensorboard logs."")\n'"
utils.py,0,"b'from __future__ import print_function\n\nimport numpy as np\n\n\ndef zero_pad(X, seq_len):\n    return np.array([x[:seq_len - 1] + [0] * max(seq_len - len(x), 1) for x in X])\n\n\ndef get_vocabulary_size(X):\n    return max([max(x) for x in X]) + 1  # plus the 0th word\n\n\ndef fit_in_vocabulary(X, voc_size):\n    return [[w for w in x if w < voc_size] for x in X]\n\n\ndef batch_generator(X, y, batch_size):\n    """"""Primitive batch generator \n    """"""\n    size = X.shape[0]\n    X_copy = X.copy()\n    y_copy = y.copy()\n    indices = np.arange(size)\n    np.random.shuffle(indices)\n    X_copy = X_copy[indices]\n    y_copy = y_copy[indices]\n    i = 0\n    while True:\n        if i + batch_size <= size:\n            yield X_copy[i:i + batch_size], y_copy[i:i + batch_size]\n            i += batch_size\n        else:\n            i = 0\n            indices = np.arange(size)\n            np.random.shuffle(indices)\n            X_copy = X_copy[indices]\n            y_copy = y_copy[indices]\n            continue\n\n\nif __name__ == ""__main__"":\n    # Test batch generator\n    gen = batch_generator(np.array([\'a\', \'b\', \'c\', \'d\']), np.array([1, 2, 3, 4]), 2)\n    for _ in range(8):\n        xx, yy = next(gen)\n        print(xx, yy)\n'"
visualize.py,2,"b'#!/usr/bin/python\n""""""\nExample of attention coefficients visualization\n\nUses saved model, so it should be executed after train.py\n""""""\nfrom train import *\n\nsaver = tf.train.Saver()\n\n# Calculate alpha coefficients for the first test example\nwith tf.Session() as sess:\n    saver.restore(sess, MODEL_PATH)\n\n    x_batch_test, y_batch_test = X_test[:1], y_test[:1]\n    seq_len_test = np.array([list(x).index(0) + 1 for x in x_batch_test])\n    alphas_test = sess.run([alphas], feed_dict={batch_ph: x_batch_test, target_ph: y_batch_test,\n                                                seq_len_ph: seq_len_test, keep_prob_ph: 1.0})\nalphas_values = alphas_test[0][0]\n\n# Build correct mapping from word to index and inverse\nword_index = imdb.get_word_index()\nword_index = {word: index + INDEX_FROM for word, index in word_index.items()}\nword_index["":PAD:""] = 0\nword_index["":START:""] = 1\nword_index["":UNK:""] = 2\nindex_word = {value: key for key, value in word_index.items()}\n# Represent the sample by words rather than indices\nwords = list(map(index_word.get, x_batch_test[0]))\n\n# Save visualization as HTML\nwith open(""visualization.html"", ""w"") as html_file:\n    for word, alpha in zip(words, alphas_values / alphas_values.max()):\n        if word == "":START:"":\n            continue\n        elif word == "":PAD:"":\n            break\n        html_file.write(\'<font style=""background: rgba(255, 255, 0, %f)"">%s</font>\\n\' % (alpha, word))\n\nprint(\'\\nOpen visualization.html to checkout the attention coefficients visualization.\')\n'"
