file_path,api_count,code
data_util.py,0,"b""import numpy as np\n\nclass BatchGenerator(object):\n    '''Generator for returning shuffled batches.\n\n    data_x -- list of input matrices\n    data_y -- list of output matrices\n    batch_size -- size of batch\n    input_size -- input width\n    output_size -- output width\n    mini -- create subsequences for truncating backprop\n    mini_len -- truncated backprop window'''\n\n    def __init__(self, data_x, data_y, batch_size, input_size, output_size, mini=True, mini_len=200):\n        self.input_size = input_size\n        self.output_size = output_size\n        self.data_x = data_x\n        self.data_y = data_y\n        self.batch_size = batch_size\n        self.batch_count = len(range(0, len(self.data_x), self.batch_size))\n        self.batch_length = None\n        self.mini = mini\n        self.mini_len = mini_len\n\n\n    def batch(self):\n        while True:\n            idxs = np.arange(0, len(self.data_x))\n            np.random.shuffle(idxs)\n            # np.random.shuffle(idxs)\n            shuff_x = []\n            shuff_y = []\n            for i in idxs:\n                shuff_x.append(self.data_x[i])\n                shuff_y.append(self.data_y[i])\n\n            for batch_idx in range(0, len(self.data_x), self.batch_size):\n                input_batch = []\n                output_batch = []\n                for j in xrange(batch_idx, min(batch_idx+self.batch_size,len(self.data_x)), 1):\n                    input_batch.append(shuff_x[j])\n                    output_batch.append(shuff_y[j])\n                input_batch, output_batch, seq_len = self.pad(input_batch, output_batch)\n                yield input_batch, output_batch, seq_len\n\n\n    def pad(self, sequence_X, sequence_Y):\n        current_batch = len(sequence_X)\n        padding_X = [0]*self.input_size\n        padding_Y = [0]*self.output_size\n\n        lens = [sequence_X[i].shape[0] for i in range(len(sequence_X))]\n        # lens2 = [sequence_Y[i].shape[0] for i in range(len(sequence_Y))]\n        #\n        max_lens = max(lens)\n        # max_lens2 = max(lens2)\n        #\n        # assert max_lens == max_lens2\n        # print(max_lens)\n        for i, x in enumerate(lens):\n            length = x\n            a = list(sequence_X[i])\n            b = list(sequence_Y[i])\n            while length < max_lens:\n                a.append(padding_X)\n                b.append(padding_Y)\n                length+=1\n\n            if self.mini:\n                while length % self.mini_len != 0:\n                    a.append(padding_X)\n                    b.append(padding_Y)\n                    length+=1\n\n            sequence_X[i] = np.array(a)\n            sequence_Y[i] = np.array(b)\n            # for x in minis:\n            #     mini_X.append(np.array(a[x:min(x+self.mini,x)]))\n            #     mini_Y.append(np.array(b[x:min(x+self.mini,x)]))\n            # print sequence_X[i].shape\n            # print sequence_Y[i].shape\n\n        # assert all(x.shape == (max_lens, self.input_size) for x in sequence_X)\n        # assert all(y.shape == (max_lens, self.output_size) for y in sequence_Y)\n\n        sequence_X = np.vstack([np.expand_dims(x, 1) for x in sequence_X])\n        sequence_Y = np.vstack([np.expand_dims(y, 1) for y in sequence_Y])\n\n        if not self.mini:\n            mini_batches = 1\n            max_lens = max(lens)\n        else:\n            mini_batches = length/self.mini_len\n            max_lens = self.mini_len\n\n        sequence_X = np.reshape(sequence_X, [current_batch*mini_batches, max_lens, self.input_size])\n        sequence_Y = np.reshape(sequence_Y, [current_batch*mini_batches, max_lens, self.output_size])\n\n        return sequence_X, sequence_Y, max_lens\n"""
file_util.py,0,"b'import os\nimport numpy as np\nfrom mido import MidiFile\nfrom midi_util import *\n\ndef validate_data(path, quant):\n    \'\'\'Creates a folder containing valid MIDI files.\n\n    Arguments:\n    path -- Original directory containing untouched midis.\n    quant -- Level of quantisation\'\'\'\n\n    path_prefix, path_suffix = os.path.split(path)\n\n    # Handle case where a trailing / requires two splits.\n    if len(path_suffix) == 0:\n        path_prefix, path_suffix = os.path.split(path_prefix)\n\n    total_file_count = 0\n    processed_count = 0\n\n    base_path_out = os.path.join(path_prefix, path_suffix+\'_valid\')\n\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if file.split(\'.\')[-1] == \'mid\' or file.split(\'.\')[-1] == \'MID\':\n                total_file_count += 1\n                print \'Processing \' + str(file)\n                midi_path = os.path.join(root,file)\n                try:\n                    midi_file = MidiFile(midi_path)\n                except (KeyError, IOError, TypeError, IndexError, EOFError, ValueError):\n                    print ""Bad MIDI.""\n                    continue\n                time_sig_msgs = [ msg for msg in midi_file.tracks[0] if msg.type == \'time_signature\' ]\n\n                if len(time_sig_msgs) == 1:\n                    time_sig = time_sig_msgs[0]\n                    if not (time_sig.numerator == 4 and time_sig.denominator == 4):\n                        print \'\\tTime signature not 4/4. Skipping ...\'\n                        continue\n                else:\n                    # print time_sig_msgs\n                    print \'\\tNo time signature. Skipping ...\'\n                    continue\n\n                mid = quantize(MidiFile(os.path.join(root,file)), quant)\n                if not mid:\n                    print \'Invalid MIDI. Skipping...\'\n                    continue\n\n                if not os.path.exists(base_path_out):\n                    os.makedirs(base_path_out)\n\n                out_file = os.path.join(base_path_out, file)\n\n                print \'\\tSaving\', out_file\n                midi_file.save(out_file)\n                processed_count += 1\n\n    print \'\\nProcessed {} files out of {}\'.format(processed_count, total_file_count)\n\ndef quantize_data(path, quant):\n    \'\'\'Creates a folder containing the quantised MIDI files.\n\n    Arguments:\n    path -- Validated directory containing midis.\n    quant -- Level of quantisation\'\'\'\n\n    path_prefix, path_suffix = os.path.split(path)\n\n    if len(path_suffix) == 0:\n        path_prefix, path_suffix = os.path.split(path_prefix)\n\n    total_file_count = 0\n    processed_count = 0\n\n    base_path_out = os.path.join(path_prefix, path_suffix+\'_quantized\')\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if file.split(\'.\')[-1] == \'mid\' or file.split(\'.\')[-1] == \'MID\':\n                total_file_count += 1\n                mid = quantize(MidiFile(os.path.join(root,file)),quant)\n                if not mid:\n                    print \'Invalid MIDI. Skipping...\'\n                    continue\n                suffix = root.split(path)[-1]\n                out_dir = base_path_out + \'/\' + suffix\n                if not os.path.exists(out_dir):\n                    os.makedirs(out_dir)\n                out_file = os.path.join(out_dir, file)\n\n                print \'Saving\', out_file\n                mid.save(out_file)\n\n                processed_count += 1\n\n    print \'Processed {} files out of {}\'.format(processed_count, total_file_count)\n\ndef save_data(path, quant, one_hot=True):\n        \'\'\'Creates a folder containing the quantised MIDI files.\n\n        Arguments:\n        path -- Quantised directory containing midis.\n        quant -- Level of quantisation\'\'\'\n\n    path_prefix, path_suffix = os.path.split(path)\n\n    # Handle case where a trailing / requires two splits.\n    if len(path_suffix) == 0:\n        path_prefix, path_suffix = os.path.split(path_prefix)\n\n    array_out = os.path.join(path_prefix, path_suffix+\'_inputs\')\n    velocity_out = os.path.join(path_prefix, path_suffix+\'_velocities\')\n\n    total_file_count = 0\n    processed_count = 0\n\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            # print os.path.join(root, file)\n            if file.split(\'.\')[-1] == \'mid\' or file.split(\'.\')[-1] == \'MID\':\n                total_file_count += 1\n\n\n                out_array = \'{}.npy\'.format(os.path.join(array_out, file))\n                out_velocity = \'{}.npy\'.format(os.path.join(velocity_out, file))\n                midi_path = os.path.join(root,file)\n                midi_file = MidiFile(midi_path)\n\n                print \'Processing \' + str(file)\n                mid = MidiFile(os.path.join(root,file))\n\n                # mid = quantize(midi_file,\n                #                quantization=quant)\n\n                if one_hot:\n                    try:\n                        array, velocity_array = midi_to_array_one_hot(mid, quant)\n                    except (KeyError, TypeError, IOError, IndexError, EOFError, ValueError):\n                        print ""Out of bounds""\n                        continue\n                else:\n                    array, velocity_array = midi_to_array(mid, quant)\n\n                if not os.path.exists(array_out):\n                    os.makedirs(array_out)\n\n                if not os.path.exists(velocity_out):\n                    os.makedirs(velocity_out)\n\n                # print out_dir\n\n                print \'Saving\', out_array\n\n                # print_array( mid, array)\n                # raw_input(""Press Enter to continue..."")\n\n                np.save(out_array, array)\n                np.save(out_velocity, velocity_array)\n\n                processed_count += 1\n    print \'\\nProcessed {} files out of {}\'.format(processed_count, total_file_count)\n\ndef load_data(path):\n    \'\'\'Returns lists of input and output numpy matrices.\n\n    Arguments:\n    path -- Quantised directory path.\n    quant -- Level of quantisation\'\'\'\n\n    names = []\n    X_list = []\n    Y_list = []\n    path_prefix, path_suffix = os.path.split(path)\n\n    # Handle case where a trailing / requires two splits.\n    if len(path_suffix) == 0:\n        path_prefix, path_suffix = os.path.split(path_prefix)\n\n    x_path = os.path.join(path_prefix, path_suffix+""_inputs"")\n    y_path = os.path.join(path_prefix, path_suffix+""_labels"")\n\n    for filename in os.listdir(x_path):\n        if filename.split(\'.\')[-1] == \'npy\':\n            abs_path = os.path.join(x_path,filename)\n            loaded = np.array(np.load(abs_path))\n\n\n            X_list.append(loaded)\n\n    for filename in os.listdir(y_path):\n        if filename.split(\'.\')[-1] == \'npy\':\n            abs_path = os.path.join(y_path,filename)\n            loaded = np.array(np.load(abs_path))\n            Y_list.append(loaded)\n\n    # X_list = np.array(X_list)\n    # Y_list = np.array(Y_list)\n\n\n    return X_list, Y_list\n'"
main.py,1,"b'import argparse\nimport tensorflow as tf\nimport os\nimport numpy as np\nfrom model import GenreLSTM\n\n\nparser = argparse.ArgumentParser(description=\'How to run this\')\n\nparser.add_argument(\n    ""-current_run"",\n    type=str,\n    help=""The name of the model which will also be the name of the session\'s folder.""\n)\n\nparser.add_argument(\n    ""-data_dir"",\n    type=str,\n    default=""./data"",\n    help=""Directory of datasets""\n)\n\nparser.add_argument(\n    ""-data_set"",\n    type=str,\n    default=""test"",\n    help=""The name of training dataset""\n)\n\nparser.add_argument(\n    ""-runs_dir"",\n    type=str,\n    default=""./runs"",\n    help=""The name of the model which will also be the name of the session folder""\n)\n\nparser.add_argument(\n    ""-bi"",\n    help=""True for bidirectional"",\n    action=\'store_true\'\n)\n\nparser.add_argument(\n    ""-forward_only"",\n    action=\'store_true\',\n    help=""True for forward only, False for training [False]""\n)\n\nparser.add_argument(\n    ""-load_model"",\n    type=str,\n    default=None,\n    help=""Folder name of model to load""\n)\n\nparser.add_argument(\n    ""-load_last"",\n    action=\'store_true\',\n    help=""Start from last epoch""\n)\n\nargs = parser.parse_args()\n\ndef setup_dir():\n\n    print(\'[*] Setting up directory...\')\n\n    main_path = args.runs_dir\n    current_run = os.path.join(main_path, args.current_run)\n\n    files_path = args.data_dir\n    files_path = os.path.join(files_path, args.data_set)\n\n    x_path = os.path.join(files_path, \'inputs\')\n    y_path = os.path.join(files_path, \'velocities\')\n    eval_path = os.path.join(files_path, \'eval\')\n\n    model_path = os.path.join(current_run, \'model\')\n    logs_path = os.path.join(current_run, \'tmp\')\n    png_path = os.path.join(current_run, \'png\')\n    pred_path = os.path.join(current_run, \'predictions\')\n\n    if not os.path.exists(current_run):\n        os.makedirs(current_run)\n    if not os.path.exists(logs_path):\n        os.makedirs(logs_path)\n    if not os.path.exists(model_path):\n        os.makedirs(model_path)\n    if not os.path.exists(png_path):\n        os.makedirs(png_path)\n    if not os.path.exists(pred_path):\n        os.makedirs(pred_path)\n\n    dirs = {\n            \'main_path\': main_path,\n            \'current_run\': current_run,\n            \'model_path\': model_path,\n            \'logs_path\': logs_path,\n            \'png_path\': png_path,\n            \'eval_path\': eval_path,\n            \'pred_path\': pred_path,\n            \'x_path\': x_path,\n            \'y_path\': y_path\n        }\n\n    # print main_path\n    # print current_run\n    # print model_path\n    # print logs_path\n    # print png_path\n    # print eval_path\n    # print x_path\n    # print y_path\n    return dirs\n\ndef load_training_data(x_path, y_path, genre):\n    X_data = []\n    Y_data = []\n    names = []\n    print(\'[*] Loading data...\')\n\n    x_path = os.path.join(x_path, genre)\n    y_path = os.path.join(y_path, genre)\n\n    for i, filename in enumerate(os.listdir(x_path)):\n        if filename.split(\'.\')[-1] == \'npy\':\n            names.append(filename)\n\n    for i, filename in enumerate(names):\n        abs_x_path = os.path.join(x_path,filename)\n        abs_y_path = os.path.join(y_path,filename)\n        loaded_x = np.load(abs_x_path)\n\n        X_data.append(loaded_x)\n\n        loaded_y = np.load(abs_y_path)\n        loaded_y = loaded_y/127\n        Y_data.append(loaded_y)\n        assert X_data[i].shape[0] == Y_data[i].shape[0]\n\n\n    return X_data, Y_data\n\ndef prepare_data():\n    dirs = setup_dir()\n    data = {}\n    data[""classical""] = {}\n    data[""jazz""] = {}\n\n    c_train_X , c_train_Y = load_training_data(dirs[\'x_path\'], dirs[\'y_path\'], ""classical"")\n\n    data[""classical""][""X""] = c_train_X\n    data[""classical""][""Y""] = c_train_Y\n\n    j_train_X , j_train_Y = load_training_data(dirs[\'x_path\'], dirs[\'y_path\'], ""jazz"")\n\n    data[""jazz""][""X""] = j_train_X\n    data[""jazz""][""Y""] = j_train_Y\n    return dirs, data\n\ndef main():\n    tf.logging.set_verbosity(tf.logging.ERROR)\n\n    dirs, data = prepare_data()\n\n    network  = GenreLSTM(dirs, input_size=176, mini=True, bi=args.bi)\n    network.prepare_model()\n\n    if not args.forward_only:\n        if args.load_model:\n            loaded_epoch = args.load_model.split(\'.\')[0]\n            loaded_epoch = loaded_epoch.split(\'-\')[-1]\n            loaded_epoch = loaded_epoch[1:]\n            print(""[*] Loading "" + args.load_model + "" and continuing from "" + loaded_epoch + ""."")\n            loaded_epoch = int(loaded_epoch)\n            network.train(data, model=args.load_model, starting_epoch=loaded_epoch+1)\n        elif args.load_last:\n            tree = os.listdir(dirs[""model_path""])\n            tree.remove(\'checkpoint\')\n            files = [(int(file.split(\'.\')[0].split(\'-\')[-1][1:]), file.split(\'.\')[0]) for file in tree]\n            files.sort(key = lambda t: t[0])\n            # print files\n            last = files[-1][1]\n            last = last + "".ckpt""\n            loaded_epoch = files[-1][0]\n            # loaded_epoch = last.split(\'-\')[-1]\n            # loaded_epoch = loaded_epoch[1:]\n            # last = last + "".ckpt""\n            print(""[*] Loading "" + last + "" and continuing from "" + str(loaded_epoch) + ""."")\n            network.train(data, model=last, starting_epoch=loaded_epoch+1)\n        else:\n            network.train(data)\n    else:\n        network.load(args.load_model)\n\nif __name__ == \'__main__\':\n    main()\n'"
midi_util.py,0,"b'from collections import defaultdict\nimport copy\nfrom math import log, floor, ceil\nimport pprint\nimport matplotlib.pyplot as plt\nimport pretty_midi\nimport mido\nfrom mido import MidiFile, MidiTrack, Message, MetaMessage\nimport numpy as np\nimport random\n\nDEBUG = False\n\n# The MIDI pitches we use.\nPITCHES = xrange(21,109,1)\nOFFSET = 109-21\nPITCHES_MAP = { p : i for i, p in enumerate(PITCHES) }\nprint len(PITCHES)\n\ndef nearest_pow2(x):\n    \'\'\'Normalize input to nearest power of 2, or midpoints between\n    consecutive powers of two. Round down when halfway between two\n    possibilities.\'\'\'\n\n    low = 2**int(floor(log(x, 2)))\n    high = 2**int(ceil(log(x, 2)))\n    mid = (low + high) / 2\n\n    if x < mid:\n        high = mid\n    else:\n        low = mid\n    if high - x < x - low:\n        nearest = high\n    else:\n        nearest = low\n    return nearest\n\ndef midi_to_array_one_hot(mid, quantization):\n    \'\'\'Return array representation of a 4/4 time signature, MIDI object.\n\n    Normalize the number of time steps in track to a power of 2. Then\n    construct a T x N*2 array A (T = number of time steps, N = number of\n    MIDI note numbers) where [A(t,n), A(t, n+1)] is the state of the note number\n    at time step t.\n\n    Arguments:\n    mid -- MIDI object with a 4/4 time signature\n    quantization -- The note duration, represented as 1/2**quantization.\'\'\'\n\n    time_sig_msgs = [ msg for msg in mid.tracks[0] if msg.type == \'time_signature\' ]\n    assert len(time_sig_msgs) == 1, \'No time signature found\'\n    time_sig = time_sig_msgs[0]\n    assert time_sig.numerator == 4 and time_sig.denominator == 4, \'Not 4/4 time.\'\n\n    # Quantize the notes to a grid of time steps.\n    mid = quantize(mid, quantization=quantization)\n\n    # Convert the note timing and velocity to an array.\n    _, track = get_note_track(mid)\n    ticks_per_quarter = mid.ticks_per_beat\n    time_msgs = [msg for msg in track if hasattr(msg, \'time\')]\n    cum_times = np.cumsum([msg.time for msg in time_msgs])\n\n    track_len_ticks = cum_times[-1]\n    if DEBUG:\n        print \'Track len in ticks:\', track_len_ticks\n    notes = [\n        (time * (2**quantization/4) / (ticks_per_quarter), msg.type, msg.note, msg.velocity)\n        for (time, msg) in zip(cum_times, time_msgs)\n        if msg.type == \'note_on\' or msg.type == \'note_off\']\n\n    num_steps = int(round(track_len_ticks / float(ticks_per_quarter)*2**quantization/4))\n    normalized_num_steps = nearest_pow2(num_steps)\n    notes.sort(key=lambda (position, note_type, note_num, velocity):(position,-velocity))\n\n    if DEBUG:\n        # pp = pprint.PrettyPrinter()\n        print num_steps\n        print normalized_num_steps\n        # pp.pprint(notes)\n\n    midi_array = np.zeros((normalized_num_steps, len(PITCHES)*2))\n    velocity_array = np.zeros((normalized_num_steps, len(PITCHES)))\n    open_msgs = defaultdict(list)\n\n    for (position, note_type, note_num, velocity) in notes:\n        if position == normalized_num_steps:\n            # print \'Warning: truncating from position {} to {}\'.format(position, normalized_num_steps - 1)\n            position = normalized_num_steps - 1\n            # continue\n\n        if position > normalized_num_steps:\n            # print \'Warning: skipping note at position {} (greater than {})\'.format(position, normalized_num_steps)\n            continue\n\n        if note_type == ""note_on"" and velocity > 0:\n            open_msgs[note_num].append((position, note_type, note_num, velocity))\n            midi_array[position, 2*PITCHES_MAP[note_num]] = 1\n            midi_array[position, 2*PITCHES_MAP[note_num]+1] = 1\n            velocity_array[position, PITCHES_MAP[note_num]] = velocity\n        elif note_type == \'note_off\' or (note_type == \'note_on\' and velocity == 0):\n\n            note_on_open_msgs = open_msgs[note_num]\n\n            if len(note_on_open_msgs) == 0:\n                print \'Bad MIDI, Note has no end time.\'\n                return\n\n            stack_pos, _, _, vel = note_on_open_msgs[0]\n            open_msgs[note_num] = note_on_open_msgs[1:]\n            current_pos = position\n            while current_pos > stack_pos:\n                # if midi_array[position, PITCHES_MAP[note_num]] != 1:\n                midi_array[current_pos, 2*PITCHES_MAP[note_num]] = 0\n                midi_array[current_pos, 2*PITCHES_MAP[note_num]+1] = 1\n                velocity_array[current_pos, PITCHES_MAP[note_num]] = vel\n                current_pos -= 1\n\n    for (position, note_type, note_num, velocity) in notes:\n        if position == normalized_num_steps:\n            print \'Warning: truncating from position {} to {}\'.format(position, normalized_num_steps - 1)\n            position = normalized_num_steps - 1\n            # continue\n\n        if position > normalized_num_steps:\n            # print \'Warning: skipping note at position {} (greater than {})\'.format(position, normalized_num_steps)\n            continue\n        if note_type == ""note_on"" and velocity > 0:\n            open_msgs[note_num].append((position, note_type, note_num, velocity))\n            midi_array[position, 2*PITCHES_MAP[note_num]] = 1\n            midi_array[position, 2*PITCHES_MAP[note_num]+1] = 1\n            velocity_array[position, PITCHES_MAP[note_num]] = velocity\n\n    assert len(midi_array) == len(velocity_array)\n    return midi_array, velocity_array\n\ndef print_array(mid, array, quantization=4):\n    \'\'\'Print a binary array representing midi notes.\'\'\'\n    bar = 1\n    ticks_per_beat = mid.ticks_per_beat\n    ticks_per_slice = ticks_per_beat/2**quantization\n\n    bars = [x*ticks_per_slice % ticks_per_beat for x in xrange(0,len(array))]\n\n    # print ticks_per_beat, ticks_per_slice\n    res = \'\'\n    for i, slice in enumerate(array):\n        for pitch in slice:\n            if pitch > 0:\n                res += str(int(pitch))\n            else:\n                res += \'-\'\n        if bars[i]== 0:\n            res += str(bar)\n            bar +=1\n        res += \'\\n\'\n    # Take out the last newline\n    print res[:-1]\n\ndef get_note_track(mid):\n    \'\'\'Given a MIDI object, return the first track with note events.\'\'\'\n\n    for i, track in enumerate(mid.tracks):\n        for msg in track:\n            if msg.type == \'note_on\':\n                return i, track\n    raise ValueError(\n        \'MIDI object does not contain any tracks with note messages.\')\n\ndef quantize_tick(tick, ticks_per_quarter, quantization):\n    \'\'\'Quantize the timestamp or tick.\n\n    Arguments:\n    tick -- An integer timestamp\n    ticks_per_quarter -- The number of ticks per quarter note\n    quantization -- The note duration, represented as 1/2**quantization\n    \'\'\'\n    assert (ticks_per_quarter * 4) % 2 ** quantization == 0, \\\n        \'Quantization too fine. Ticks per quantum must be an integer.\'\n    ticks_per_quantum = (ticks_per_quarter * 4) / float(2 ** quantization)\n    quantized_ticks = int(\n        round(tick / float(ticks_per_quantum)) * ticks_per_quantum)\n    return quantized_ticks\n\ndef unquantize(mid, style_mid):\n    unquantized_mid = copy.deepcopy(mid)\n    # By convention, Track 0 contains metadata and Track 1 contains\n    # the note on and note off events.\n    orig_note_track_idx, orig_note_track = get_note_track(mid)\n    style_note_track_idx, style_note_track = get_note_track(style_mid)\n\n    note_track = unquantize_track(orig_note_track, style_note_track)\n    unquantized_mid.tracks[orig_note_track_idx] = note_track\n\n    return unquantized_mid\n\ndef unquantize_track(orig_track, style_track):\n    \'\'\'Returns the unquantised orig_track with encoded velocities from the style_track.\n\n    Arguments:\n    orig_track -- Non-quantised MIDI object\n    style_track -- Quantised and stylised MIDI object \'\'\'\n\n    first_note_msg_idx = None\n\n    for i, msg in enumerate(orig_track):\n        if msg.type == \'note_on\':\n            orig_first_note_msg_idx = i\n            break\n\n    for i, msg in enumerate(style_track):\n        if msg.type == \'note_on\':\n            style_first_note_msg_idx = i\n            break\n\n    orig_cum_msgs = zip(\n        np.cumsum([msg.time for msg in orig_track[orig_first_note_msg_idx:]]),\n        [msg for msg in orig_track[orig_first_note_msg_idx:]])\n\n    style_cum_msgs = zip(\n        np.cumsum([msg.time for msg in style_track[style_first_note_msg_idx:]]),\n        [msg for msg in style_track[style_first_note_msg_idx:]])\n\n    orig_cum_msgs.sort(key=lambda (cum_time, msg): cum_time)\n    style_cum_msgs.sort(key=lambda (cum_time, msg): cum_time)\n\n    open_msgs = defaultdict(list)\n\n    for cum_time, msg in orig_cum_msgs:\n        if msg.type == \'note_on\' and msg.velocity > 0:\n            open_msgs[msg.note].append((cum_time,msg))\n\n    for i, (cum_time, msg) in enumerate(style_cum_msgs):\n         if msg.type == \'note_on\' and msg.velocity > 0:\n            note_on_open_msgs = open_msgs[msg.note]\n            note_on_cum_time, note_on_msg = note_on_open_msgs[0]\n            note_on_msg.velocity = msg.velocity\n            open_msgs[msg.note] = note_on_open_msgs[1:]\n\n    return orig_track\n\ndef quantize(mid, quantization=5):\n    \'\'\'Return a midi object whose notes are quantized to\n    1/2**quantization notes.\n\n    Arguments:\n    mid -- MIDI object\n    quantization -- The note duration, represented as\n      1/2**quantization.\'\'\'\n\n    quantized_mid = copy.deepcopy(mid)\n    # By convention, Track 0 contains metadata and Track 1 contains\n    # the note on and note off events.\n    note_track_idx, note_track = get_note_track(mid)\n    new_track = quantize_track( note_track, mid.ticks_per_beat, quantization)\n    if new_track == None:\n        return None\n    quantized_mid.tracks[note_track_idx] = new_track\n    return quantized_mid\n\ndef quantize_track(track, ticks_per_quarter, quantization):\n    \'\'\'Return the differential time stamps of the note_on, note_off, and\n    end_of_track events, in order of appearance, with the note_on events\n    quantized to the grid given by the quantization.\n\n    Arguments:\n    track -- MIDI track containing note event and other messages\n    ticks_per_quarter -- The number of ticks per quarter note\n    quantization -- The note duration, represented as\n      1/2**quantization.\'\'\'\n\n    pp = pprint.PrettyPrinter()\n\n    # Message timestamps are represented as differences between\n    # consecutive events. Annotate messages with cumulative timestamps.\n\n    # Assume the following structure:\n    # [header meta messages] [note messages] [end_of_track message]\n    first_note_msg_idx = None\n    for i, msg in enumerate(track):\n        if msg.type == \'note_on\':\n            first_note_msg_idx = i\n            break\n\n    cum_msgs = zip(\n        np.cumsum([msg.time for msg in track[first_note_msg_idx:]]),\n        [msg for msg in track[first_note_msg_idx:]])\n    end_of_track_cum_time = cum_msgs[-1][0]\n\n    quantized_track = MidiTrack()\n    quantized_track.extend(track[:first_note_msg_idx])\n    # Keep track of note_on events that have not had an off event yet.\n    # note number -> message\n    open_msgs = defaultdict(list)\n    quantized_msgs = []\n    for cum_time, msg in cum_msgs:\n        if DEBUG:\n            print \'Message:\', msg\n            print \'Open messages:\'\n            pp.pprint(open_msgs)\n        if msg.type == \'note_on\' and msg.velocity > 0:\n            # Store until note off event. Note that there can be\n            # several note events for the same note. Subsequent\n            # note_off events will be associated with these note_on\n            # events in FIFO fashion.\n            open_msgs[msg.note].append((cum_time, msg))\n        elif msg.type == \'note_off\' or (msg.type == \'note_on\' and msg.velocity == 0):\n            # assert msg.note in open_msgs, \\\n            #     \'Bad MIDI. Cannot have note off event before note on event\'\n\n            if msg.note not in open_msgs:\n                 print \'Bad MIDI. Cannot have note off event before note on event\'\n                 return\n\n            note_on_open_msgs = open_msgs[msg.note]\n\n            if len(note_on_open_msgs) == 0:\n                print \'Bad MIDI, Note has no end time.\'\n                return\n\n            # assert len(note_on_open_msgs) > 0, \'Bad MIDI, Note has no end time.\'\n\n            note_on_cum_time, note_on_msg = note_on_open_msgs[0]\n            open_msgs[msg.note] = note_on_open_msgs[1:]\n\n            # Quantized note_on time\n            quantized_note_on_cum_time = quantize_tick(\n                note_on_cum_time, ticks_per_quarter, quantization)\n\n            # The cumulative time of note_off is the quantized\n            # cumulative time of note_on plus the orginal difference\n            # of the unquantized cumulative times.\n            quantized_note_off_cum_time = quantized_note_on_cum_time + (cum_time - note_on_cum_time)\n            quantized_msgs.append((min(end_of_track_cum_time, quantized_note_on_cum_time), note_on_msg))\n            quantized_msgs.append((min(end_of_track_cum_time, quantized_note_off_cum_time), msg))\n\n            if DEBUG:\n                print \'Appended\', quantized_msgs[-2:]\n        elif msg.type == \'end_of_track\':\n            quantized_msgs.append((cum_time, msg))\n\n        if DEBUG:\n            print \'\\n\'\n\n    # Now, sort the quantized messages by (cumulative time,\n    # note_type), making sure that note_on events come before note_off\n    # events when two event have the same cumulative time. Compute\n    # differential times and construct the quantized track messages.\n    quantized_msgs.sort(\n        key=lambda (cum_time, msg): cum_time\n        if (msg.type==\'note_on\' and msg.velocity > 0) else cum_time + 0.5)\n\n    diff_times = [quantized_msgs[0][0]] + list(\n        np.diff([ msg[0] for msg in quantized_msgs ]))\n    for diff_time, (cum_time, msg) in zip(diff_times, quantized_msgs):\n        quantized_track.append(msg.copy(time=diff_time))\n    if DEBUG:\n        print \'Quantized messages:\'\n        pp.pprint(quantized_msgs)\n        pp.pprint(diff_times)\n    return quantized_track\n\ndef stylify(mid, velocity_array, quantization):\n    style_mid = copy.deepcopy(mid)\n    # By convention, Track 0 contains metadata and Track 1 contains\n    # the note on and note off events.\n    note_track_idx, note_track = get_note_track(mid)\n    new_track = stylify_track(mid, velocity_array, quantization)\n    style_mid.tracks[note_track_idx] = new_track\n    return style_mid\n\n# def midi_to_array(mid, quantization):\n#     \'\'\'Return array representation of a 4/4 time signature, MIDI object.\n#\n#     Normalize the number of time steps in track to a power of 2. Then\n#     construct a T x N array A (T = number of time steps, N = number of\n#     MIDI note numbers) where A(t,n) is the velocity of the note number\n#     n at time step t if the note is active, and 0 if it is not.\n#\n#     Arguments:\n#     mid -- MIDI object with a 4/4 time signature\n#     quantization -- The note duration, represented as 1/2**quantization.\'\'\'\n#\n#     time_sig_msgs = [ msg for msg in mid.tracks[0] if msg.type == \'time_signature\' ]\n#     assert len(time_sig_msgs) == 1, \'No time signature found\'\n#     time_sig = time_sig_msgs[0]\n#     assert time_sig.numerator == 4 and time_sig.denominator == 4, \'Not 4/4 time.\'\n#\n#     # Quantize the notes to a grid of time steps.\n#     mid = quantize(mid, quantization=quantization)\n#\n#     # Convert the note timing and velocity to an array.\n#     _, track = get_note_track(mid)\n#     ticks_per_quarter = mid.ticks_per_beat\n#     time_msgs = [msg for msg in track if hasattr(msg, \'time\')]\n#     cum_times = np.cumsum([msg.time for msg in time_msgs])\n#\n#     track_len_ticks = cum_times[-1]\n#     if DEBUG:\n#         print \'Track len in ticks:\', track_len_ticks\n#     notes = [\n#         (time * (2**quantization/4) / (ticks_per_quarter), msg.type, msg.note, msg.velocity)\n#         for (time, msg) in zip(cum_times, time_msgs)\n#         if msg.type == \'note_on\' or msg.type == \'note_off\']\n#\n#     num_steps = int(round(track_len_ticks / float(ticks_per_quarter)*2**quantization/4))\n#     normalized_num_steps = nearest_pow2(num_steps)\n#     notes.sort(key=lambda (position, note_type, note_num, velocity):(position,-velocity))\n#\n#     if DEBUG:\n#         # pp = pprint.PrettyPrinter()\n#         print num_steps\n#         print normalized_num_steps\n#         # pp.pprint(notes)\n#\n#     midi_array = np.zeros((normalized_num_steps, len(PITCHES)))\n#     velocity_array = np.zeros((normalized_num_steps, len(PITCHES)))\n#     open_msgs = defaultdict(list)\n#\n#     for (position, note_type, note_num, velocity) in notes:\n#         if position == normalized_num_steps:\n#             # print \'Warning: truncating from position {} to {}\'.format(position, normalized_num_steps - 1)\n#             position = normalized_num_steps - 1\n#             # continue\n#\n#         if position > normalized_num_steps:\n#             # print \'Warning: skipping note at position {} (greater than {})\'.format(position, normalized_num_steps)\n#             continue\n#\n#         if note_type == ""note_on"" and velocity > 0:\n#             open_msgs[note_num].append((position, note_type, note_num, velocity))\n#             midi_array[position, PITCHES_MAP[note_num]] = 1\n#             velocity_array[position, PITCHES_MAP[note_num]] = velocity\n#         elif note_type == \'note_off\' or (note_type == \'note_on\' and velocity == 0):\n#\n#             note_on_open_msgs = open_msgs[note_num]\n#\n#             if len(note_on_open_msgs) == 0:\n#                 print \'Bad MIDI, Note has no end time.\'\n#                 return\n#\n#             stack_pos, _, _, vel = note_on_open_msgs[0]\n#             open_msgs[note_num] = note_on_open_msgs[1:]\n#             current_pos = position\n#             while current_pos > stack_pos:\n#                 # if midi_array[position, PITCHES_MAP[note_num]] != 1:\n#                 midi_array[current_pos, PITCHES_MAP[note_num]] = 2\n#                 velocity_array[current_pos, PITCHES_MAP[note_num]] = vel\n#                 current_pos -= 1\n#\n#     for (position, note_type, note_num, velocity) in notes:\n#         if position == normalized_num_steps:\n#             print \'Warning: truncating from position {} to {}\'.format(position, normalized_num_steps - 1)\n#             position = normalized_num_steps - 1\n#             # continue\n#\n#         if position > normalized_num_steps:\n#             # print \'Warning: skipping note at position {} (greater than {})\'.format(position, normalized_num_steps)\n#             continue\n#         if note_type == ""note_on"" and velocity > 0:\n#             open_msgs[note_num].append((position, note_type, note_num, velocity))\n#             midi_array[position, PITCHES_MAP[note_num]] = 1\n#             velocity_array[position, PITCHES_MAP[note_num]] = velocity\n#\n#     return midi_array, velocity_array\n\ndef stylify_track(mid, velocity_array, quantization):\n\n    _, track = get_note_track(mid)\n    # first_note_msg_idx = None\n    #\n    # for i, msg in enumerate(track):\n    #     if msg.type == \'note_on\':\n    #         first_note_msg_idx = i\n    #         break\n\n    ticks_per_quarter = mid.ticks_per_beat\n\n    time_msgs = [msg for msg in track if hasattr(msg, \'time\')]\n\n    cum_times = np.cumsum([msg.time for msg in time_msgs])\n    track_len_ticks = cum_times[-1]\n\n    num_steps = int(round(track_len_ticks / float(ticks_per_quarter)*2**quantization/4))\n    normalized_num_steps = nearest_pow2(num_steps)\n    # notes.sort(key=lambda (position, note_type, note_num, velocity):(position,-velocity))\n\n    notes = [\n        (time * (2**quantization/4) / (ticks_per_quarter), msg.type, msg.note, msg.velocity)\n        for (time, msg) in zip(cum_times, time_msgs)\n        if msg.type == \'note_on\' or msg.type == \'note_off\']\n\n    cum_index = 0\n    for i, time_msg in enumerate(track):\n        if hasattr(time_msg, \'time\'):\n            if time_msg.type == \'note_on\' or time_msg.type == \'note_off\':\n                if time_msg.velocity > 0:\n                    pos = cum_times[cum_index] * (2**quantization/4) / (ticks_per_quarter)\n                    if pos == normalized_num_steps:\n                        pos = pos - 1\n                    if pos > normalized_num_steps:\n                        continue\n                    vel = velocity_array[pos, PITCHES_MAP[time_msg.note]]\n                    vel = vel*127\n                    # print vel\n                    vel = max(vel,1)\n                    track[i].velocity = int(round(vel))\n            cum_index += 1\n\n    return track\n\ndef scrub(mid, velocity=10, random=False):\n    \'\'\'Returns a midi object with one global velocity.\n\n    Sets all velocities to a contant.\n\n    Arguments:\n    mid -- MIDI object with a 4/4 time signature\n    velocity -- The global velocity\'\'\'\n    scrubbed_mid = copy.deepcopy(mid)\n    # By convention, Track 0 contains metadata and Track 1 contains\n    # the note on and note off events.\n    note_track_idx, note_track = get_note_track(mid)\n    if random:\n        new_track = scrub_track_random(note_track)\n    else:\n        new_track = scrub_track(note_track,velocity=10)\n    scrubbed_mid.tracks[note_track_idx] = new_track\n    return scrubbed_mid\n\ndef scrub_track_random(track):\n\n    first_note_msg_idx = None\n\n    for i, msg in enumerate(track):\n        if msg.type == \'note_on\':\n            first_note_msg_idx = i\n            break\n\n    note_msgs = track[first_note_msg_idx:]\n\n    for msg in note_msgs:\n         if msg.type == \'note_on\' and msg.velocity > 0:\n             msg.velocity = random.randint(0,127)\n\n    return track\n\ndef velocity_range(mid):\n    \'\'\'Returns a count of velocities.\n\n    Counts the range of velocities in a midi object.\n\n    Arguments:\n    mid -- MIDI object with a 4/4 time signature\'\'\'\n\n    _, track = get_note_track(mid)\n    first_note_msg_idx = None\n\n    for i, msg in enumerate(track):\n        if msg.type == \'note_on\':\n            first_note_msg_idx = i\n            break\n    velocities = defaultdict(lambda:0)\n    note_msgs = track[first_note_msg_idx:]\n    for msg in note_msgs:\n         if msg.type == \'note_on\' and msg.velocity > 0:\n             velocities[str(msg.velocity)] += 1\n    dynamics = len(velocities.keys())\n    # print velocities\n    if dynamics > 1:\n        return dynamics\n    else:\n        return 0\n\ndef scrub_track(track, velocity):\n    first_note_msg_idx = None\n\n    for i, msg in enumerate(track):\n        if msg.type == \'note_on\':\n            first_note_msg_idx = i\n            break\n\n    note_msgs = track[first_note_msg_idx:]\n\n    for msg in note_msgs:\n         if msg.type == \'note_on\' and msg.velocity > 0:\n             msg.velocity = 10\n\n    return track\n'"
model.py,89,"b'import tensorflow as tf\nfrom data_util import BatchGenerator\nimport os\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nmatplotlib.pyplot.ioff()\n\nclass GenreLSTM(object):\n    def __init__(self, dirs, mini=False, bi=False, one_hot=True, input_size=176, output_size=88, num_layers=3, batch_count=8):\n        self.input_size = int(input_size)\n        self.output_size = int(output_size)\n        self.num_layers = int(num_layers)\n        self.batch_count = int(batch_count)\n        self.dirs = dirs\n        self.bi = bi\n        self.mini = mini\n        self.one_hot = one_hot\n\n\n    def prepare_bidiretional(self, glorot=True):\n        print(""[*] Preparing bidirectional dynamic RNN..."")\n        self.input_cell = tf.contrib.rnn.LSTMCell(self.input_size, forget_bias=1.0)\n        self.input_cell = tf.contrib.rnn.DropoutWrapper(self.input_cell, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n        self.enc_outputs, self.enc_states = tf.nn.dynamic_rnn(self.input_cell, self.inputs, sequence_length=self.seq_len, dtype=tf.float32)\n\n\n        with tf.variable_scope(""encode"") as scope:\n\n            self.j_cell_fw = tf.contrib.rnn.LSTMBlockCell(self.input_size,forget_bias=1.0)\n            self.j_cell_fw = tf.contrib.rnn.DropoutWrapper(self.j_cell_fw, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n\n            self.j_cell_bw = tf.contrib.rnn.LSTMBlockCell(self.input_size,forget_bias=1.0)\n            self.j_cell_bw = tf.contrib.rnn.DropoutWrapper(self.j_cell_bw, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n\n            if self.num_layers > 1:\n                self.j_cell_fw = tf.contrib.rnn.MultiRNNCell([self.j_cell_fw]*self.num_layers)\n                self.j_cell_bw = tf.contrib.rnn.MultiRNNCell([self.j_cell_bw]*self.num_layers)\n\n\n\n            # self.j_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n            (self.j_fw, self.j_bw) , _ = tf.nn.bidirectional_dynamic_rnn(\n                                                        self.j_cell_fw,\n                                                        self.j_cell_bw,\n                                                        self.enc_outputs,\n                                                        sequence_length=self.seq_len,\n                                                        dtype=tf.float32)\n\n\n            self.jazz_outputs  = tf.concat([self.j_fw, self.j_bw],2)\n            # self.jazz_outputs = tf.add(self.j_outputs[0], self.j_outputs[1])\n\n            scope.reuse_variables()\n\n\n            self.c_cell_fw = tf.contrib.rnn.LSTMBlockCell(self.input_size,forget_bias=1.0)\n            self.c_cell_fw = tf.contrib.rnn.DropoutWrapper(self.c_cell_fw, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n\n            self.c_cell_bw = tf.contrib.rnn.LSTMBlockCell(self.input_size,forget_bias=1.0)\n            self.c_cell_bw = tf.contrib.rnn.DropoutWrapper(self.c_cell_bw, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n\n            if self.num_layers > 1:\n                self.c_cell_fw  = tf.contrib.rnn.MultiRNNCell([self.c_cell_fw ]*self.num_layers)\n                self.c_cell_bw  = tf.contrib.rnn.MultiRNNCell([self.c_cell_bw ]*self.num_layers)\n\n            (self.c_fw, self.c_bw), _ =  tf.nn.bidirectional_dynamic_rnn(\n            # self.c_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n                                                        self.c_cell_fw,\n                                                        self.c_cell_bw,\n                                                        self.enc_outputs,\n                                                        sequence_length=self.seq_len,\n                                                        dtype=tf.float32)\n\n\n            self.classical_outputs  = tf.concat([self.c_fw, self.c_bw],2)\n\n            # self.classical_outputs = tf.add(self.c_outputs[0], self.c_outputs[1])\n\n\n        self.jazz_B = tf.Variable(tf.random_normal([self.output_size], stddev=0.1))\n        self.classical_B = tf.Variable(tf.random_normal([self.output_size], stddev=0.1))\n\n        if glorot:\n            self.jazz_W = tf.get_variable(""jazz_W"", shape=[self.input_size*2, self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n            self.classical_W = tf.get_variable(""classical_W"", shape=[self.input_size*2, self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n        else:\n            self.jazz_W = tf.Variable(tf.random_normal([self.input_size*2,self.output_size], stddev=0.1))\n            self.classical_W = tf.Variable(tf.random_normal([self.input_size*2,self.output_size], stddev=0.1))\n\n        self.jazz_linear_out = tf.reshape(self.jazz_outputs, [tf.shape(self.true_jazz_outputs)[0]*self.seq_len[-1], 2*self.input_size])\n        self.jazz_linear_out = tf.matmul(self.jazz_linear_out, self.jazz_W) + self.jazz_B\n        self.jazz_linear_out = tf.reshape(self.jazz_linear_out,[tf.shape(self.true_jazz_outputs)[0],tf.shape(self.true_jazz_outputs)[1], tf.shape(self.true_jazz_outputs)[2]])\n\n        self.classical_linear_out = tf.reshape(self.classical_outputs, [tf.shape(self.true_classical_outputs)[0]*self.seq_len[-1], 2*self.input_size])\n        self.classical_linear_out = tf.matmul(self.classical_linear_out, self.classical_W) + self.classical_B\n        self.classical_linear_out = tf.reshape(self.classical_linear_out,[tf.shape(self.true_classical_outputs)[0],tf.shape(self.true_classical_outputs)[1], tf.shape(self.true_classical_outputs)[2]])\n\n    def prepare_unidiretional(self, glorot=True):\n        print(""[*] Preparing unidirectional dynamic RNN..."")\n        self.input_cell = tf.contrib.rnn.LSTMCell(self.input_size, forget_bias=1.0)\n        self.input_cell = tf.contrib.rnn.DropoutWrapper(self.input_cell, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n        self.enc_outputs, self.enc_states = tf.nn.dynamic_rnn(self.input_cell, self.inputs, sequence_length=self.seq_len, dtype=tf.float32)\n\n        with tf.variable_scope(""encode"") as scope:\n\n            self.jazz_cell = tf.contrib.rnn.LSTMCell(self.input_size, forget_bias=1.0)\n            self.jazz_cell = tf.contrib.rnn.DropoutWrapper(self.jazz_cell, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n\n            self.jazz_outputs, self.jazz_states = tf.nn.dynamic_rnn(self.jazz_cell, self.enc_outputs, sequence_length=self.seq_len, dtype=tf.float32)\n\n            scope.reuse_variables()\n\n            self.classical_cell = tf.contrib.rnn.LSTMCell(self.input_size, forget_bias=1.0)\n            self.classical_cell = tf.contrib.rnn.DropoutWrapper(self.classical_cell, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n            self.classical_outputs, self.classical_states = tf.nn.dynamic_rnn(self.classical_cell, self.enc_outputs, sequence_length=self.seq_len, dtype=tf.float32)\n\n        # self.cell = tf.contrib.rnn.DropoutWrapper(self.cell, input_keep_prob=self.input_keep_prob, output_keep_prob=self.output_keep_prob)\n        # self.stacked_cell = tf.contrib.rnn.MultiRNNCell([self.cell] * self.num_layers)\n\n        self.jazz_B = tf.Variable(tf.random_normal([self.output_size], stddev=0.1))\n        self.classical_B = tf.Variable(tf.random_normal([self.output_size], stddev=0.1))\n\n        if glorot:\n            self.jazz_W = tf.get_variable(""jazz_W"", shape=[self.input_size, self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n            self.classical_W = tf.get_variable(""classical_W"", shape=[self.input_size, self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n        else:\n            self.jazz_W = tf.Variable(tf.random_normal([self.input_size,self.output_size], stddev=0.1))\n            self.classical_W = tf.Variable(tf.random_normal([self.input_size,self.output_size], stddev=0.1))\n\n        self.jazz_linear_out = tf.reshape(self.jazz_outputs, [tf.shape(self.true_jazz_outputs)[0]*self.seq_len[-1], self.input_size])\n        self.jazz_linear_out = tf.matmul(self.jazz_linear_out, self.jazz_W) + self.jazz_B\n        self.jazz_linear_out = tf.reshape(self.jazz_linear_out,[tf.shape(self.true_jazz_outputs)[0],tf.shape(self.true_jazz_outputs)[1], tf.shape(self.true_jazz_outputs)[2]])\n\n        self.classical_linear_out = tf.reshape(self.classical_outputs, [tf.shape(self.true_classical_outputs)[0]*self.seq_len[-1], self.input_size])\n        self.classical_linear_out = tf.matmul(self.classical_linear_out, self.classical_W) + self.classical_B\n        self.classical_linear_out = tf.reshape(self.classical_linear_out,[tf.shape(self.true_classical_outputs)[0],tf.shape(self.true_classical_outputs)[1], tf.shape(self.true_classical_outputs)[2]])\n\n    def prepare_model(self, bi=False):\n\n        self.inputs = tf.placeholder(tf.float32, [None, None, self.input_size])\n\n        self.true_jazz_outputs = tf.placeholder(tf.float32, [None, None, self.output_size])\n        self.true_classical_outputs = tf.placeholder(tf.float32, [None, None, self.output_size])\n\n        self.seq_len = tf.placeholder(tf.int32, [None])\n\n        self.input_keep_prob = tf.placeholder(tf.float32, None)\n        self.output_keep_prob = tf.placeholder(tf.float32, None)\n\n        if self.bi:\n            self.prepare_bidiretional()\n        else:\n            self.prepare_unidiretional()\n\n        self.jazz_negation = tf.subtract(self.true_jazz_outputs, self.jazz_linear_out)\n        self.classical_negation = tf.subtract(self.true_classical_outputs, self.classical_linear_out)\n\n        self.jazz_loss =  tf.reduce_mean(tf.square(tf.subtract(self.jazz_linear_out, self.true_jazz_outputs)))\n        self.classical_loss =  tf.reduce_mean(tf.square(tf.subtract(self.classical_linear_out, self.true_classical_outputs)))\n\n        tf.summary.scalar(""Jazz error"", self.jazz_loss)\n        tf.summary.scalar(""Classical error"", self.classical_loss)\n        tf.summary.scalar(""Average error"", self.jazz_loss+self.classical_loss/2)\n\n        tf.summary.histogram(""Jazz negation"", self.jazz_negation)\n        tf.summary.histogram(""Classical negation"", self.classical_negation)\n\n    def clip_optimizer(self, learning_rate, loss):\n        opt = tf.train.AdamOptimizer(learning_rate)\n        gradients = opt.compute_gradients(loss)\n\n        for i, (grad, var) in enumerate(gradients):\n            if grad is not None:\n                gradients[i] = (tf.clip_by_norm(grad, 10), var)\n\n        return opt.apply_gradients(gradients)\n\n    def train(self, data, model=None, starting_epoch=0, clip_grad=True, epochs=1001, input_keep_prob=0.5, output_keep_prob=0.5, learning_rate=0.001 , eval_epoch=20,val_epoch=10, save_epoch=1):\n\n        self.data = data\n\n        if clip_grad:\n            jazz_optimizer = self.clip_optimizer(learning_rate,self.jazz_loss)\n            classical_optimizer = self.clip_optimizer(learning_rate,self.classical_loss)\n        else:\n            jazz_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.jazz_loss)\n            classical_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.classical_loss)\n\n\n\n        self.sess = tf.Session()\n\n        self.c_in_list, self.c_out_list,self.c_input_lens, self.c_files = self.eval_set(\'classical\')\n        self.j_in_list, self.j_out_list,self.j_input_lens, self.j_files = self.eval_set(\'jazz\')\n\n        if model:\n            self.load(model)\n        else:\n            self.sess.run(tf.global_variables_initializer())\n\n        self.summary_op = tf.summary.merge_all()\n\n        self.train_writer = tf.summary.FileWriter(os.path.join(self.dirs[\'logs_path\'], \'train\'), graph=self.sess.graph_def)\n        self.test_writer = tf.summary.FileWriter(os.path.join(self.dirs[\'logs_path\'], \'test\'), graph=self.sess.graph_def)\n\n        classical_batcher = BatchGenerator(self.data[""classical""][""X""], self.data[""classical""][""Y""], self.batch_count, self.input_size, self.output_size, mini=self.mini)\n        jazz_batcher = BatchGenerator(self.data[""jazz""][""X""], self.data[""jazz""][""Y""], self.batch_count, self.input_size, self.output_size, mini=self.mini)\n\n        self.v_classical_batcher = self.validate(""classical"")\n        self.v_classical_batcher = self.v_classical_batcher.batch()\n\n        self.v_jazz_batcher = self.validate(""jazz"")\n        self.v_jazz_batcher = self.v_jazz_batcher.batch()\n\n\n        classical_generator = classical_batcher.batch()\n        jazz_generator = jazz_batcher.batch()\n\n        print(""[*] Initiating training..."")\n\n        for epoch in xrange(starting_epoch, epochs):\n\n            classical_epoch_avg = 0\n            jazz_epoch_avg = 0\n\n            print(""[*] Epoch %d"" % epoch)\n            for batch in range(classical_batcher.batch_count):\n                batch_X, batch_Y, batch_len = classical_generator.next()\n                batch_len = [batch_len] * len(batch_X)\n                epoch_error, classical_summary, _  =  self.sess.run([self.classical_loss,\n                                                         self.summary_op,\n                                                         classical_optimizer,\n                                                         ], feed_dict={ self.inputs: batch_X,\n                                                                                  self.true_classical_outputs: batch_Y,\n                                                                                  self.true_jazz_outputs: batch_Y,\n                                                                                  self.seq_len: batch_len,\n                                                                                  self.input_keep_prob: input_keep_prob,\n                                                                                  self.output_keep_prob: output_keep_prob})\n                classical_epoch_avg += epoch_error\n                print(""\\tBatch %d/%d, Training MSE for Classical batch: %.9f"" % (batch+1, classical_batcher.batch_count, epoch_error))\n                self.train_writer.add_summary(classical_summary, epoch*classical_batcher.batch_count + epoch)\n\n            for batch in range(jazz_batcher.batch_count):\n                batch_X, batch_Y, batch_len = jazz_generator.next()\n                batch_len = [batch_len] * len(batch_X)\n                epoch_error, jazz_summary, _  =  self.sess.run([self.jazz_loss,\n                                                         self.summary_op,\n                                                         jazz_optimizer,\n                                                         ], feed_dict={ self.inputs: batch_X,\n                                                                                  self.true_jazz_outputs: batch_Y,\n                                                                                  self.true_classical_outputs: batch_Y,\n                                                                                  self.seq_len: batch_len,\n                                                                                  self.input_keep_prob: input_keep_prob,\n                                                                                  self.output_keep_prob: output_keep_prob})\n                jazz_epoch_avg += epoch_error\n                print(""\\tBatch %d/%d, Training MSE for Jazz batch: %.9f"" % (batch+1, jazz_batcher.batch_count, epoch_error))\n\n                self.train_writer.add_summary(jazz_summary, epoch*jazz_batcher.batch_count + epoch)\n                # self.validation(epoch)\n\n            print(""[*] Average Training MSE for Classical epoch %d: %.9f"" % (epoch, classical_epoch_avg/classical_batcher.batch_count))\n            print(""[*] Average Training MSE for Jazz epoch %d: %.9f"" % (epoch, jazz_epoch_avg/jazz_batcher.batch_count))\n\n            if epoch % val_epoch == 0 :\n                print(""[*] Validating model..."")\n                self.validation(epoch)\n\n            if epoch % save_epoch == 0 :\n                self.save(epoch)\n\n            if epoch % eval_epoch == 0 :\n                print(""[*] Evaluating model..."")\n                self.evaluate(epoch)\n\n        print(""[*] Training complete."")\n\n    def load(self, model_name, path=None) :\n        print("" [*] Loading checkpoint..."")\n        self.saver = tf.train.Saver(max_to_keep=0)\n        if not path:\n            self.saver.restore(self.sess, os.path.join(self.dirs[\'model_path\'], model_name))\n        else:\n            self.sess = tf.Session()\n            self.saver.restore(self.sess, path)\n\n    def save(self, epoch):\n        print(""[*] Saving checkpoint..."")\n        model_name =  ""model-e%d.ckpt"" % (epoch)\n        self.saver = tf.train.Saver(max_to_keep=0)\n        save_path = self.saver.save(self.sess, os.path.join(self.dirs[\'model_path\'], model_name))\n        print(""[*] Model saved in file: %s"" % save_path)\n\n    def predict(self, input_path, output_path):\n        in_list = []\n        out_list = []\n        filenames = []\n        input_lens = []\n\n        loaded = np.load(input_path)\n        true_vel = np.load(output_path)/127\n\n        in_list.append(loaded)\n        out_list.append(true_vel)\n\n        input_len = [len(loaded)]\n\n        c_error, c_out, j_out, e_out = self.sess.run([self.classical_loss, self.classical_linear_out, self.jazz_linear_out, self.enc_outputs], feed_dict={self.inputs:in_list,\n                                                                                                                                                            self.seq_len:input_len,\n                                                                                                                                                            self.input_keep_prob:1.0,\n                                                                                                                                                            self.output_keep_prob:1.0,\n                                                                                                                                                            self.true_classical_outputs:out_list,\n                                                                                                                                                            self.true_jazz_outputs:out_list})\n\n        return c_error, c_out, j_out, e_out, out_list\n\n    def validate(self, type):\n        \'\'\'Handles validation set data\'\'\'\n        input_eval_path = os.path.join(self.dirs[\'eval_path\'], ""inputs"")\n        vel_eval_path = os.path.join(self.dirs[\'eval_path\'], ""velocities"")\n\n        c_input_eval_path = os.path.join(input_eval_path, ""classical"")\n        c_vel_eval_path = os.path.join(vel_eval_path, ""classical"")\n\n        j_input_eval_path = os.path.join(input_eval_path, ""jazz"")\n        j_vel_eval_path = os.path.join(vel_eval_path, ""jazz"")\n\n        if type == ""classical"":\n            input_folder = os.listdir(c_input_eval_path)\n            file_count = len(input_folder)\n            vel_eval_path = c_vel_eval_path\n            input_eval_path = c_input_eval_path\n        else:\n            input_folder = os.listdir(j_input_eval_path)\n            file_count = len(input_folder)\n            vel_eval_path = j_vel_eval_path\n            input_eval_path = j_input_eval_path\n            #CLASSICS\n\n        in_list = []\n        out_list = []\n        filenames = []\n        for i, filename in enumerate(input_folder):\n            if filename.split(\'.\')[-1] == \'npy\':\n\n                vel_path = os.path.join(vel_eval_path, filename)\n                input_path = os.path.join(input_eval_path, filename)\n\n                true_vel = np.load(vel_path)/127\n                loaded = np.load(input_path)\n\n                if not self.one_hot:\n                    loaded = loaded/2\n\n                in_list.append(loaded)\n                out_list.append(true_vel)\n                filenames.append(filename)\n        valid_generator = BatchGenerator(in_list, out_list, self.batch_count, self.input_size, self.output_size, mini=False)\n        return valid_generator\n\n    def validation(self, epoch, pred_save=False):\n        \'\'\'Computes and logs loss of validation set\'\'\'\n        in_list, out_list, input_len = self.v_classical_batcher.next()\n        input_len = [input_len] * len(in_list)\n        c_error, c_out, j_out, e_out, c_summary = self.sess.run([self.classical_loss,\n                                          self.classical_linear_out,\n                                          self.jazz_linear_out,\n                                          self.enc_outputs,\n                                          self.summary_op],\n\n                                        feed_dict={self.inputs:in_list,\n                                                   self.seq_len:input_len,\n                                                   self.input_keep_prob:1.0,\n                                                   self.output_keep_prob:1.0,\n                                                   self.true_classical_outputs:out_list,\n                                                   self.true_jazz_outputs:out_list})\n\n\n        # for i, x in enumerate(c_out):\n            # self.plot_evaluation(epoch, c_files[i], c_out[i], j_out[i], e_out[i], out_list[i])\n\n        in_list, out_list, input_len = self.v_jazz_batcher.next()\n        input_len = [input_len] * len(in_list)\n\n        j_error, j_out, c_out, e_out, j_summary = self.sess.run([self.jazz_loss,\n                                          self.jazz_linear_out,\n                                          self.classical_linear_out,\n                                          self.enc_outputs,\n                                          self.summary_op],\n\n                                        feed_dict={self.inputs:in_list,\n                                                   self.seq_len:input_len,\n                                                   self.input_keep_prob:1.0,\n                                                   self.output_keep_prob:1.0,\n                                                   self.true_jazz_outputs:out_list,\n                                                   self.true_classical_outputs:out_list})\n\n\n        # for i, x in enumerate(c_out):\n            # self.plot_evaluation(epoch, j_files[i], c_out[i], j_out[i], e_out[i], out_list[i])\n\n        # print(""[*] Validating Model..."")\n\n        print(""[*] Average Test MSE for Classical epoch %d: %.9f"" % (epoch, c_error))\n        print(""[*] Average Test MSE for Jazz epoch %d: %.9f"" % (epoch, j_error))\n\n\n        self.test_writer.add_summary(j_summary, epoch)\n        self.test_writer.add_summary(c_summary, epoch)\n\n    def eval_set(self, type):\n        \'\'\'Loads validation set.\'\'\'\n        input_eval_path = os.path.join(self.dirs[\'eval_path\'], ""inputs"")\n        vel_eval_path = os.path.join(self.dirs[\'eval_path\'], ""velocities"")\n\n        c_input_eval_path = os.path.join(input_eval_path, ""classical"")\n        c_vel_eval_path = os.path.join(vel_eval_path, ""classical"")\n\n        j_input_eval_path = os.path.join(input_eval_path, ""jazz"")\n        j_vel_eval_path = os.path.join(vel_eval_path, ""jazz"")\n\n        if type == ""classical"":\n            input_folder = os.listdir(c_input_eval_path)\n            file_count = len(input_folder)\n            vel_eval_path = c_vel_eval_path\n            input_eval_path = c_input_eval_path\n        else:\n            input_folder = os.listdir(j_input_eval_path)\n            file_count = len(input_folder)\n            vel_eval_path = j_vel_eval_path\n            input_eval_path = j_input_eval_path\n            #CLASSICS\n\n        in_list = []\n        out_list = []\n        filenames = []\n        input_lens = []\n\n        for i, filename in enumerate(input_folder):\n            if filename.split(\'.\')[-1] == \'npy\':\n\n                vel_path = os.path.join(vel_eval_path, filename)\n                input_path = os.path.join(input_eval_path, filename)\n\n                true_vel = np.load(vel_path)/120\n                loaded = np.load(input_path)\n\n                if not self.one_hot:\n                    loaded = loaded/2\n\n                in_list.append([loaded])\n                out_list.append([true_vel])\n                filenames.append(filename)\n                input_len = [len(loaded)]\n                input_lens.append(input_len)\n\n        return in_list, out_list, input_lens, filenames\n\n    def evaluate(self, epoch, pred_save=False):\n        \'\'\'Performs prediciton and plots results on validation set.\'\'\'\n        for i, filename in enumerate(self.c_files):\n            c_error, c_out, j_out, e_out, summary = self.sess.run([self.classical_loss,\n                                              self.classical_linear_out,\n                                              self.jazz_linear_out,\n                                              self.enc_outputs,\n                                              self.summary_op],\n\n                                            feed_dict={self.inputs:self.c_in_list[i],\n                                                       self.seq_len:self.c_input_lens[i],\n                                                       self.input_keep_prob:1.0,\n                                                       self.output_keep_prob:1.0,\n                                                       self.true_classical_outputs:self.c_out_list[i],\n                                                       self.true_jazz_outputs:self.c_out_list[i]})\n\n\n            self.plot_evaluation(epoch, filename, c_out, j_out, e_out, self.c_out_list[i])\n                # if pred_save:\n                #     predicted = os.path.join(self.dirs[\'pred_path\'], filename.split(\'.\')[0] + ""-e%d"" % (epoch)+"".npy"")\n                #     np.save(predicted, linear[-1])\n\n        for i, filename in enumerate(self.j_files):\n            j_error, j_out, c_out, e_out, summary = self.sess.run([self.jazz_loss,\n                                              self.jazz_linear_out,\n                                              self.classical_linear_out,\n                                              self.enc_outputs,\n                                              self.summary_op],\n\n                                            feed_dict={self.inputs:self.j_in_list[i],\n                                                       self.seq_len:self.j_input_lens[i],\n                                                       self.input_keep_prob:1.0,\n                                                       self.output_keep_prob:1.0,\n                                                       self.true_classical_outputs:self.j_out_list[i],\n                                                       self.true_jazz_outputs:self.j_out_list[i]})\n\n            self.plot_evaluation(epoch, filename, c_out, j_out, e_out, self.j_out_list[i])\n                # if pred_save:\n                #     predicted = os.path.join(self.dirs[\'pred_path\'], filename.split(\'.\')[0] + ""-e%d"" % (epoch)+"".npy"")\n                #     np.save(predicted, linear[-1])\n\n\n    def plot_evaluation(self, epoch, filename, c_out, j_out, e_out, out_list, path=None):\n        \'\'\'Plotting/Saving training session graphs\n        epoch -- epoch number\n        c_out -- classical output\n        j_out -- jazz output\n        e_out -- interpretation layer output\n        out_list -- actual output\n        output_size -- output width\n        path -- Save path\'\'\'\n\n        fig = plt.figure(figsize=(14,11), dpi=120)\n        fig.suptitle(filename, fontsize=10, fontweight=\'bold\')\n\n        graph_items = [out_list[-1]*127, c_out[-1]*127, j_out[-1]*127,  (c_out[-1]-j_out[-1])*127 , e_out[-1]]\n        plots = len(graph_items)\n        cmap = [\'jet\', \'jet\', \'jet\', \'jet\', \'bwr\']\n        vmin = [0,0,0,-10,-1]\n        vmax = [127,127,127,10,1]\n        names = [""Actual"", ""Classical"", ""Jazz"", ""Difference"", ""Encoded""]\n\n\n        for i in xrange(0, plots):\n            fig.add_subplot(1,plots,i+1)\n            plt.imshow(graph_items[i], vmin=vmin[i], vmax=vmax[i], cmap=cmap[i], aspect=\'auto\')\n\n            a = plt.colorbar(aspect=80)\n            a.ax.tick_params(labelsize=7)\n            ax = plt.gca()\n            ax.xaxis.tick_top()\n\n            if i == 0:\n                ax.set_ylabel(\'Time Step\')\n            ax.xaxis.set_label_position(\'top\')\n            ax.tick_params(axis=\'both\', labelsize=7)\n            fig.subplots_adjust(top=0.85)\n            ax.set_title(names[i], y=1.09)\n            # plt.tight_layout()\n\n        if self.one_hot:\n            plt.xlim(0,88)\n        else:\n            plt.xlim(0,128)\n\n        #Don\'t show the figure and save it\n        if not path:\n            out_png = os.path.join(self.dirs[\'png_path\'], filename.split(\'.\')[0] + ""-e%d"" % (epoch)+"".png"")\n            plt.savefig(out_png, bbox_inches=\'tight\')\n            plt.close(fig)\n        else:\n            # out_png = os.path.join(self.dirs[\'png_path\'], filename.split(\'.\')[0] + ""-e%d"" % (epoch)+"".png"")\n            # plt.savefig(out_png, bbox_inches=\'tight\')\n            # plt.close(fig)\n            plt.show()\n            plt.close(fig)\n'"
