file_path,api_count,code
dataset/prepare_data.py,6,"b'""""""\nTurn a merged corpus into tfrecord files.\n\nNOTE: You will want to do this using several processes. I did this on an AWS machine with 72 CPUs using GNU parallel\nas that\'s where I had the deduplicated RealNews dataset.\n""""""\nimport argparse\nimport ujson as json\n# from sample.encoder import get_encoder, tokenize_for_grover_training, detokenize, sliding_window, create_int_feature\nimport random\nimport tensorflow.compat.v1 as tf\nimport collections\nimport os\nfrom tempfile import TemporaryDirectory\n\nfrom tokenization import tokenization\n\nparser = argparse.ArgumentParser(description=\'SCRAPE!\')\nparser.add_argument(\n    \'-fold\',\n    dest=\'fold\',\n    default=0,\n    type=int,\n    help=\'which fold we are on\'\n)\nparser.add_argument(\n    \'-num_folds\',\n    dest=\'num_folds\',\n    default=1,\n    type=int,\n    help=\'Number of folds (corresponding to both the number of training files and the number of testing files)\',\n)\nparser.add_argument(\n    \'-seed\',\n    dest=\'seed\',\n    default=1337,\n    type=int,\n    help=\'which seed to use\'\n)\nparser.add_argument(\n    \'-base_fn\',\n    dest=\'base_fn\',\n    default=\'news2016zh_\',\n    type=str,\n    help=\'We will output files that are like {base_fn}_{n}.tfrecord for n in 0, ..., 1023\'\n)\n\nparser.add_argument(\n    \'-input_fn\',\n    dest=\'input_fn\',\n    default=\'realnews.jsonl\',\n    type=str,\n    help=\'Base filename to use. THIS MUST BE A LOCAL FILE.\'\n)\nparser.add_argument(\n    \'-max_seq_length\',\n    dest=\'max_seq_length\',\n    default=1025,\n    type=int,\n    help=\'Max sequence length\',\n)\n\n\nargs = parser.parse_args()\nrandom.seed(args.seed + args.fold)\n\n#encoder = get_encoder()\ntokenizer = tokenization.FullTokenizer(\n    vocab_file=""bert-base-chinese-vocab.txt"", do_lower_case=True)\n\n\nclass TFRecordWriter(object):\n    def __init__(self, fn):\n        self.fn = fn\n        if fn.startswith(\'gs://\'):\n            from google.cloud import storage\n            self.s3client = None\n            self.gclient = storage.Client()\n            self.storage_dir = TemporaryDirectory()\n            self.writer = tf.python_io.TFRecordWriter(\n                os.path.join(self.storage_dir.name, \'temp.tfrecord\'))\n            self.bucket_name, self.file_name = self.fn.split(\n                \'gs://\', 1)[1].split(\'/\', 1)\n\n        else:\n            self.s3client = None\n            self.gclient = None\n            self.bucket_name = None\n            self.file_name = None\n            self.storage_dir = None\n            self.writer = tf.python_io.TFRecordWriter(fn)\n\n    def write(self, x):\n        self.writer.write(x)\n\n    def close(self):\n        self.writer.close()\n\n        if self.gclient is not None:\n            bucket = self.gclient.get_bucket(self.bucket_name)\n            blob = bucket.blob(self.file_name)\n            blob.upload_from_filename(os.path.join(\n                self.storage_dir.name, \'temp.tfrecord\'))\n            self.storage_dir.cleanup()\n\n    def __enter__(self):\n        # Called when entering ""with"" context.\n        return self\n\n    def __exit__(self, *_):\n        # Called when exiting ""with"" context.\n        # Upload shit\n        print(""CALLING CLOSE"")\n        self.close()\n\n\ndef article_iterator(tokenizer):\n    """""" Iterate through the provided filename + tokenize""""""\n    assert os.path.exists(args.input_fn)\n    for (dirpath, dirnames, filenames) in os.walk(args.input_fn):\n        for filename in filenames:\n            with open(os.path.join(dirpath, filename), \'r\') as f:\n                for l_no, l in enumerate(f):\n                    if l_no % args.num_folds == args.fold:\n                        article = json.loads(l)\n\n                        line = tokenization.convert_to_unicode(\n                            article[\'text\'])  # for news2016zh text body\n                        tokens = tokenizer.tokenize(line)\n                        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n                        article[\'input_ids\'] = input_ids\n\n                        article[\'inst_index\'] = (l_no // args.num_folds)\n                        if article[\'inst_index\'] < 100:\n                            print(\'---\\nINPUT{}. {}\\n---\\nTokens: {}\\n\'.format(article[\'inst_index\'],\n                                                                            tokens,\n                                                                            input_ids\n                                                                            ), flush=True)\n                        if len(article[\'input_ids\']) <= 64:  # min size of article\n                            continue\n                        yield article\n\n\ndef create_int_feature(values):\n    feature = tf.train.Feature(\n        int64_list=tf.train.Int64List(value=list(values)))\n    return feature\n\n\ndef buffered_and_sliding_window_article_iterator(tokenizer, final_desired_size=1025):\n    """""" We apply a sliding window to fix long sequences, and use a buffer that combines short sequences.""""""\n    for article in article_iterator(tokenizer):\n        if len(article[\'input_ids\']) >= final_desired_size:\n            article[\'input_ids\'] = article[\'input_ids\'][0:final_desired_size-1]\n        while len(article[\'input_ids\']) < final_desired_size:\n            article[\'input_ids\'].append(0)\n        yield article\n\n\n# OK now write the tfrecord file\ntotal_written = 0\ntrain_file = args.base_fn + \'train_wiki19_{:04d}.tfrecord\'.format(args.fold)\nwith TFRecordWriter(train_file) as train_writer:\n    for article in buffered_and_sliding_window_article_iterator(tokenizer,\n                                                                final_desired_size=args.max_seq_length + 1):\n        writer2use = train_writer\n        assert len(article[\'input_ids\']) == (args.max_seq_length + 1)\n\n        features = collections.OrderedDict()\n        features[""input_ids""] = create_int_feature(article[\'input_ids\'])\n        tf_example = tf.train.Example(\n            features=tf.train.Features(feature=features))\n\n        writer2use.write(tf_example.SerializeToString())\n        total_written += 1\n\n        # DEBUG\n        if article[\'inst_index\'] < 5:\n            print(""~~~\\nIndex {}. ARTICLE: {}\\n---\\nTokens: {}\\n\\n"".format(article[\'inst_index\'],\n                                                                           tokenizer.convert_ids_to_tokens(\n                                                                               article[\'input_ids\']),\n                                                                           article[\'input_ids\']\n                                                                           ), flush=True)\n        if article[\'inst_index\'] % 1000 == 0:\n            print(""{} articles, {} written"".format(\n                article[\'inst_index\'], total_written), flush=True)\nprint(""DONE UPLOADING"", flush=True)\n'"
scripts/demo.py,8,"b'import sys\nimport os\nimport argparse\nimport json\nimport re\n\nimport tensorflow.compat.v1 as tf\nimport numpy as np\n\nfrom train.modeling import GroverModel, GroverConfig, sample\nfrom tokenization import tokenization\n\n##### ignore tf deprecated warning temporarily\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\ntf.logging.set_verbosity(tf.logging.DEBUG)\nfrom tensorflow.python.util import deprecation\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\ntry:\n    from tensorflow.python.util import module_wrapper as deprecation\nexcept ImportError:\n    from tensorflow.python.util import deprecation_wrapper as deprecation\ndeprecation._PER_MODULE_WARNING_LIMIT = 0\n#####\n\nparser = argparse.ArgumentParser(description=\'Contextual generation (aka given some metadata we will generate articles\')\nparser.add_argument(\n    \'-metadata_fn\',\n    dest=\'metadata_fn\',\n    type=str,\n    help=\'Path to a JSONL containing metadata\',\n)\nparser.add_argument(\n    \'-out_fn\',\n    dest=\'out_fn\',\n    type=str,\n    help=\'Out jsonl, which will contain the completed jsons\',\n)\nparser.add_argument(\n    \'-input\',\n    dest=\'input\',\n    type=str,\n    help=\'Text to complete\',\n)\nparser.add_argument(\n    \'-config_fn\',\n    dest=\'config_fn\',\n    default=\'configs/mega.json\',\n    type=str,\n    help=\'Configuration JSON for the model\',\n)\nparser.add_argument(\n    \'-ckpt_fn\',\n    dest=\'ckpt_fn\',\n    default=\'../models/mega/model.ckpt\',\n    type=str,\n    help=\'checkpoint file for the model\',\n)\nparser.add_argument(\n    \'-target\',\n    dest=\'target\',\n    default=\'article\',\n    type=str,\n    help=\'What to generate for each item in metadata_fn. can be article (body), title, etc.\',\n)\nparser.add_argument(\n    \'-batch_size\',\n    dest=\'batch_size\',\n    default=1,\n    type=int,\n    help=\'How many things to generate per context. will split into chunks if need be\',\n)\nparser.add_argument(\n    \'-num_folds\',\n    dest=\'num_folds\',\n    default=1,\n    type=int,\n    help=\'Number of folds. useful if we want to split up a big file into multiple jobs.\',\n)\nparser.add_argument(\n    \'-fold\',\n    dest=\'fold\',\n    default=0,\n    type=int,\n    help=\'which fold we are on. useful if we want to split up a big file into multiple jobs.\'\n)\nparser.add_argument(\n    \'-max_batch_size\',\n    dest=\'max_batch_size\',\n    default=None,\n    type=int,\n    help=\'max batch size. You can leave this out and we will infer one based on the number of hidden layers\',\n)\nparser.add_argument(\n    \'-top_p\',\n    dest=\'top_p\',\n    default=0.95,\n    type=float,\n    help=\'p to use for top p sampling. if this isn\\\'t none, use this for everthing\'\n)\nparser.add_argument(\n    \'-min_len\',\n    dest=\'min_len\',\n    default=1024,\n    type=int,\n    help=\'min length of sample\',\n)\nparser.add_argument(\n    \'-eos_token\',\n    dest=\'eos_token\',\n    default=102,\n    type=int,\n    help=\'eos token id\',\n)\nparser.add_argument(\n    \'-samples\',\n    dest=\'samples\',\n    default=5,\n    type=int,\n    help=\'num_samples\',\n)\n\ndef extract_generated_target(output_tokens, tokenizer):\n    """"""\n    Given some tokens that were generated, extract the target\n    :param output_tokens: [num_tokens] thing that was generated\n    :param encoder: how they were encoded\n    :param target: the piece of metadata we wanted to generate!\n    :return:\n    """"""\n    # Filter out first instance of start token\n    assert output_tokens.ndim == 1\n\n    start_ind = 0\n    end_ind = output_tokens.shape[0]\n\n    return {\n        \'extraction\': tokenization.printable_text(\'\'.join(tokenizer.convert_ids_to_tokens(output_tokens))),\n        \'start_ind\': start_ind,\n        \'end_ind\': end_ind,\n    }\n\nargs = parser.parse_args()\nproj_root_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\nvocab_file_path = os.path.join(proj_root_path, ""tokenization/clue-vocab.txt"")\ntokenizer = tokenization.FullTokenizer(vocab_file=vocab_file_path , do_lower_case=True)\nnews_config = GroverConfig.from_json_file(args.config_fn)\n\n# We might have to split the batch into multiple chunks if the batch size is too large\ndefault_mbs = {12: 32, 24: 16, 48: 3}\nmax_batch_size = args.max_batch_size if args.max_batch_size is not None else default_mbs[news_config.num_hidden_layers]\n\n# factorize args.batch_size = (num_chunks * batch_size_per_chunk) s.t. batch_size_per_chunk < max_batch_size\nnum_chunks = int(np.ceil(args.batch_size / max_batch_size))\nbatch_size_per_chunk = int(np.ceil(args.batch_size / num_chunks))\n\n# This controls the top p for each generation.\ntop_p = np.ones((num_chunks, batch_size_per_chunk), dtype=np.float32) * args.top_p\n\ntf_config = tf.ConfigProto(allow_soft_placement=True)\n\nwith tf.Session(config=tf_config, graph=tf.Graph()) as sess:\n    initial_context = tf.placeholder(tf.int32, [batch_size_per_chunk, None])\n    p_for_topp = tf.placeholder(tf.float32, [batch_size_per_chunk])\n    eos_token = tf.placeholder(tf.int32, [])\n    min_len = tf.placeholder(tf.int32, [])\n    tokens, probs = sample(news_config=news_config, initial_context=initial_context,\n                           eos_token=eos_token, min_len=min_len, ignore_ids=None, p_for_topp=p_for_topp,\n                           do_topk=False)\n\n    saver = tf.train.Saver()\n    saver.restore(sess, args.ckpt_fn)\n    print(\'\xf0\x9f\x8d\xbaModel loaded. \\nInput something please:\xe2\xac\x87\xef\xb8\x8f\')\n    text = input()\n    while text != """":\n        for i in range(args.samples):\n            print(""Sample,"", i + 1, "" of "", args.samples)\n            line = tokenization.convert_to_unicode(text)\n            bert_tokens = tokenizer.tokenize(line)\n            encoded = tokenizer.convert_tokens_to_ids(bert_tokens)\n            context_formatted = []\n            context_formatted.extend(encoded)\n            # Format context end\n\n            gens = []\n            gens_raw = []\n            gen_probs = []\n\n            for chunk_i in range(num_chunks):\n                tokens_out, probs_out = sess.run([tokens, probs],\n                                                 feed_dict={initial_context: [context_formatted] * batch_size_per_chunk,\n                                                            eos_token: args.eos_token, min_len: args.min_len,\n                                                            p_for_topp: top_p[chunk_i]})\n\n                for t_i, p_i in zip(tokens_out, probs_out):\n                    extraction = extract_generated_target(output_tokens=t_i, tokenizer=tokenizer)\n                    gens.append(extraction[\'extraction\'])\n\n            l = re.findall(\'.{1,70}\', gens[0].replace(\'[UNK]\', \'\').replace(\'##\', \'\'))\n            print(""\\n"".join(l))\n        print(\'Next try:\xe2\xac\x87\xef\xb8\x8f\')\n        text = input()\n'"
scripts/down_gdrive_file.py,0,"b""import argparse\n\nfrom google.colab import auth\nfrom googleapiclient.discovery import build\nfrom apiclient.http import MediaIoBaseDownload\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser(description='Simple file download script for Google Drive')\nparser.add_argument(\n    '-file_id',\n    dest='file_id',\n    type=str,\n    help='File id in Google Drive URL',\n)\nparser.add_argument(\n    '-file_path',\n    dest='file_path',\n    type=str,\n    help='Output file path',\n)\n\nargs = parser.parse_args()\n\nauth.authenticate_user()\ndrive_service = build('drive', 'v3')\n\n# file_id, file_ext = ('1n_5-tgPpQ1gqbyLPbP1PwiFi2eo7SWw_', '.data-00000-of-00001')\n# filename = '%s/model.ckpt-%d%s' % (model_dir, 100000, file_ext)\nreq = drive_service.files().get_media(fileId=args.file_id)\nwith open(args.file_path, 'wb') as f:\n    downloader = MediaIoBaseDownload(f, req, chunksize=100*1024*1024)\n    done = False\n    pbar = tqdm(total=100, desc='%s' % args.file_path)\n    progress = 0\n    while done is False:\n        status, done = downloader.next_chunk()\n        new_progress = int(status.progress() * 100)\n        pbar.update(new_progress - progress)\n        progress = new_progress\n    pbar.close()\n"""
tokenization/__init__.py,0,b''
tokenization/tokenization.py,2,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow.compat.v1 as tf\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n  """"""Checks whether the casing config is consistent with the checkpoint name.""""""\n\n  # The casing has to be passed in by the user and there is no explicit check\n  # as to whether it matches the checkpoint. The casing information probably\n  # should have been stored in the bert_config.json file, but it\'s not, so\n  # we have to heuristically detect it to validate.\n\n  if not init_checkpoint:\n    return\n\n  m = re.match(""^.*?([A-Za-z0-9_-]+)/bert_model.ckpt"", init_checkpoint)\n  if m is None:\n    return\n\n  model_name = m.group(1)\n\n  lower_models = [\n      ""uncased_L-24_H-1024_A-16"", ""uncased_L-12_H-768_A-12"",\n      ""multilingual_L-12_H-768_A-12"", ""chinese_L-12_H-768_A-12""\n  ]\n\n  cased_models = [\n      ""cased_L-12_H-768_A-12"", ""cased_L-24_H-1024_A-16"",\n      ""multi_cased_L-12_H-768_A-12""\n  ]\n\n  is_bad_config = False\n  if model_name in lower_models and not do_lower_case:\n    is_bad_config = True\n    actual_flag = ""False""\n    case_name = ""lowercased""\n    opposite_flag = ""True""\n\n  if model_name in cased_models and do_lower_case:\n    is_bad_config = True\n    actual_flag = ""True""\n    case_name = ""cased""\n    opposite_flag = ""False""\n\n  if is_bad_config:\n    raise ValueError(\n        ""You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. ""\n        ""However, `%s` seems to be a %s model, so you ""\n        ""should pass in `--do_lower_case=%s` so that the fine-tuning matches ""\n        ""how the model was pre-training. If this error is wrong, please ""\n        ""just comment out this check."" % (actual_flag, init_checkpoint,\n                                          model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n  """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(""utf-8"", ""ignore"")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef printable_text(text):\n  """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n  # These functions want `str` for both Python2 and Python3, but in one case\n  # it\'s a Unicode string and in the other it\'s a byte string.\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, unicode):\n      return text.encode(""utf-8"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef load_vocab(vocab_file):\n  """"""Loads a vocabulary file into a dictionary.""""""\n  vocab = collections.OrderedDict()\n  index = 0\n  with tf.gfile.GFile(vocab_file, ""r"") as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab\n\n\ndef convert_by_vocab(vocab, items):\n  """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\n\nclass FullTokenizer(object):\n  """"""Runs end-to-end tokenziation.""""""\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n  def __init__(self, do_lower_case=True):\n    """"""Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    """"""\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text.""""""\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn\'t\n    # matter since the English models were not trained on any Chinese data\n    # and generally don\'t have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize("" "".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    """"""Strips accents from a piece of text.""""""\n    text = unicodedata.normalize(""NFD"", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == ""Mn"":\n        continue\n      output.append(char)\n    return """".join(output)\n\n  def _run_split_on_punc(self, text):\n    """"""Splits punctuation on a piece of text.""""""\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return ["""".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    """"""Adds whitespace around any CJK character.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append("" "")\n        output.append(char)\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n  def _is_chinese_char(self, cp):\n    """"""Checks whether CP is the codepoint of a CJK character.""""""\n    # This defines a ""chinese character"" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    """"""Performs invalid character removal and whitespace cleanup on text.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n  """"""Runs WordPiece tokenziation.""""""\n\n  def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = ""unaffable""\n      output = [""un"", ""##aff"", ""##able""]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    """"""\n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = """".join(chars[start:end])\n          if start > 0:\n            substr = ""##"" + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  """"""Checks whether `chars` is a whitespace character.""""""\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return True\n  cat = unicodedata.category(char)\n  if cat == ""Zs"":\n    return True\n  return False\n\n\ndef _is_control(char):\n  """"""Checks whether `chars` is a control character.""""""\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return False\n  cat = unicodedata.category(char)\n  if cat in (""Cc"", ""Cf""):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  """"""Checks whether `chars` is a punctuation character.""""""\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(""P""):\n    return True\n  return False\n'"
train/__init__.py,0,b''
train/dataloader.py,25,"b'# Original work Copyright 2018 The Google AI Language Team Authors.\n# Modified work Copyright 2019 Rowan Zellers\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nimport tensorflow.compat.v1 as tf\n\n\ndef _decode_record(record, name_to_features):\n    """"""Decodes a record to a TensorFlow example.""""""\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n\ndef input_fn_builder(input_files,\n                     seq_length,\n                     is_training,\n                     num_cpu_threads=4,\n                     evaluate_for_fixed_number_of_steps=True):\n    """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n\n    def input_fn(params):\n        """"""The actual input function.""""""\n        batch_size = params[""batch_size""]\n        name_to_features = {\n            ""input_ids"": tf.FixedLenFeature([seq_length + 1], tf.int64),\n        }\n\n        # For training, we want a lot of parallel reading and shuffling.\n        # For eval, we want no shuffling and parallel reading doesn\'t matter.\n        if is_training:\n            d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))\n            d = d.repeat()\n            d = d.shuffle(buffer_size=len(input_files))\n\n            # `cycle_length` is the number of parallel files that get read.\n            cycle_length = min(num_cpu_threads, len(input_files))\n\n            # `sloppy` mode means that the interleaving is not exact. This adds\n            # even more randomness to the training pipeline.\n            d = d.apply(\n                tf.data.experimental.parallel_interleave(\n                    tf.data.TFRecordDataset,\n                    sloppy=is_training,\n                    cycle_length=cycle_length))\n            d = d.shuffle(buffer_size=100)\n        else:\n            d = tf.data.TFRecordDataset(input_files)\n            # If we evaluate for a fixed number of steps we don\'t want to encounter\n            # out-of-range exceptions.\n            if evaluate_for_fixed_number_of_steps:\n                d = d.repeat()\n\n        # We must `drop_remainder` on training because the TPU requires fixed\n        # size dimensions. For eval, we assume we are evaluating on the CPU or GPU\n        # and we *don\'t* want to drop the remainder, otherwise we wont cover\n        # every sample.\n        d = d.apply(\n            tf.data.experimental.map_and_batch(\n                lambda record: _decode_record(record, name_to_features),\n                batch_size=batch_size,\n                num_parallel_batches=num_cpu_threads,\n                drop_remainder=True))\n        return d\n\n    return input_fn\n\n\n#  ~~~~~~~~~~~~~~ This is for classification / AF ~~~~~~~~~~~~~~~~~~\ndef classification_convert_examples_to_features(\n        examples, max_seq_length, batch_size, encoder, output_file, labels, pad_extra_examples=False,\n        chop_from_front_if_needed=True):\n    """"""Convert a set of `InputExample`s to a TFRecord file.""""""\n\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    label_map = {label: i for i, label in enumerate(labels)}\n\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))\n\n        # begin_summary is our [CLS] token\n        tokens = example[\'ids\'] + [encoder.begin_summary]\n\n        if len(tokens) > max_seq_length:\n            if chop_from_front_if_needed:\n                tokens = tokens[-max_seq_length:]\n            else:\n                tokens = example[\'ids\'][:(max_seq_length-1)] + [encoder.begin_summary]\n        elif len(tokens) < max_seq_length:\n            tokens.extend([encoder.padding] * (max_seq_length - len(tokens)))\n\n        features = collections.OrderedDict()\n        features[\'input_ids\'] = tf.train.Feature(int64_list=tf.train.Int64List(value=tokens))\n        features[\'label_ids\'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[label_map[example[\'label\']]]))\n        features[\'is_real_example\'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[1]))\n        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n        writer.write(tf_example.SerializeToString())\n\n    if pad_extra_examples:\n        for x in range(len(examples) % batch_size):\n            features = collections.OrderedDict()\n            features[\'input_ids\'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[0]*max_seq_length))\n            features[\'label_ids\'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[0]))\n            features[\'is_real_example\'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[0]))\n            tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n            writer.write(tf_example.SerializeToString())\n    writer.close()\n\n\ndef classification_input_fn_builder(input_file, seq_length, is_training,\n                                    drop_remainder,\n                                    buffer_size=100):\n    """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n\n    name_to_features = {\n        ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),\n        ""label_ids"": tf.FixedLenFeature([], tf.int64),\n        ""is_real_example"": tf.FixedLenFeature([], tf.int64),\n    }\n\n    def input_fn(params):\n        """"""The actual input function.""""""\n        batch_size = params[""batch_size""]\n\n        # For training, we want a lot of parallel reading and shuffling.\n        # For eval, we want no shuffling and parallel reading doesn\'t matter.\n        d = tf.data.TFRecordDataset(input_file)\n        if is_training:\n            d = d.repeat()\n            d = d.shuffle(buffer_size=buffer_size)\n\n        d = d.apply(\n            tf.data.experimental.map_and_batch(\n                lambda record: _decode_record(record, name_to_features),\n                batch_size=batch_size,\n                drop_remainder=drop_remainder))\n\n        return d\n\n    return input_fn\n'"
train/modeling.py,125,"b'# Original work Copyright 2018 The Google AI Language Team Authors.\n# Modified work Copyright 2019 Rowan Zellers\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport json\nimport math\n\nimport six\nimport tensorflow.compat.v1 as tf\n\nfrom train import optimization_adafactor\nfrom train.utils import get_assignment_map_from_checkpoint, get_shape_list, get_attention_mask, gelu, layer_norm, dropout, \\\n    construct_scalar_host_call\n\nclass GroverConfig(object):\n    """"""Configuration for `GroverModel`""""""\n\n    def __init__(self,\n                 vocab_size,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 initializer_range=0.02):\n        """"""Constructs NewsConfig.\n\n        Args:\n          vocab_size: Vocabulary size of `inputs_ids` in `GroverModel`.\n          hidden_size: Size of the layers\n          num_hidden_layers: Number of hidden layers in the Transformer encoder.\n          num_attention_heads: Number of attention heads for each attention layer in\n            the Transformer encoder.\n          intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n            layer in the Transformer encoder.\n          hidden_act: The non-linear activation function (function or string) in the\n            encoder and pooler.\n          hidden_dropout_prob: The dropout probability for all fully connected\n            layers in the embeddings, encoder, and pooler.\n          attention_probs_dropout_prob: The dropout ratio for the attention\n            probabilities.\n          max_position_embeddings: The maximum sequence length that this model might\n            ever be used with. Typically set this to something large just in case\n            (e.g., 512 or 1024 or 2048).\n          initializer_range: The stdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n        """"""\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.intermediate_size = intermediate_size\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.pad_token_id = 0\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `NewsConfig` from a Python dictionary of parameters.""""""\n        config = GroverConfig(vocab_size=None)\n        for (key, value) in six.iteritems(json_object):\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `NewsConfig` from a json file of parameters.""""""\n        with tf.gfile.GFile(json_file, ""r"") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\ndef mask_attention_for_ltr(attention_scores, attention_mask):\n    """"""\n    Mask attention so that we\'re only predicting going forward\n    :param attention_scores: [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n    :param attention_mask [query_length, key_length]\n    :return: masked attention\n    """"""\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    mask = attention_mask[None, None]\n    return attention_scores * mask - tf.cast(1e10, attention_scores.dtype) * (1 - mask)\n\n\ndef create_initializer(initializer_range=0.02):\n    """"""Creates a `truncated_normal_initializer` with the given range.""""""\n    return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef _attention_projection_and_transpose(x_flat, batch_size, seq_length, num_attention_heads, size_per_head,\n                                        name, initializer_range=0.02):\n    """"""\n    :param x_flat: [batch_size*seq_length, width]\n    :return: A fixed up tensor of size [batch_size, num_attention_heads, seq_length, size_per_head]\n    """"""\n    batch_size_seq_length, dim = get_shape_list(x_flat, expected_rank=2)\n\n    if dim != size_per_head * num_attention_heads:\n        raise ValueError(""passed in a tensor of shape {} when size_per_head={} and num_attention_heads={}"".format(\n            (batch_size_seq_length, dim), size_per_head, num_attention_heads\n        ))\n\n    projected = tf.layers.dense(\n        x_flat,\n        num_attention_heads * size_per_head,\n        name=name,\n        kernel_initializer=create_initializer(initializer_range))\n\n    projected = tf.reshape(\n        projected, [batch_size, seq_length, num_attention_heads, size_per_head])\n    output_tensor = tf.transpose(projected, [0, 2, 1, 3])\n    return output_tensor\n\n\ndef attention_layer(x_flat, attention_mask, batch_size, seq_length, size_per_head=512, num_attention_heads=1, *,\n                    cache=None,\n                    initializer_range=0.02, hidden_dropout_prob=0.1,\n                    attention_probs_dropout_prob=0.1, do_cache=False):\n    """"""\n\n    :param x_flat: Tensor input, should be [batch_size*seq_length, dim]\n    :param attention_mask: Attention mask to use of size [seq_length, seq_length+cached_length]\n    :param size_per_head: dim = size_per_head * num_attention_heads\n    :param num_attention_heads:  dim = size_per_head * num_attention_heads\n    :param cache: Optionally some past (cached) things of size\n                [batch, 2, heads, sequence, features], where 2 is [k, v]\n    :param do_cache: True if we should return cache\n    :return: A new tensor of shape [batch_size, seq_length, dim]\n    as well as a new cache ""cached_keys_and_values"" that will be of size\n                                   [batch_size, 2, num_attention_heads, seq_length, dim]\n    """"""\n    batch_size_seq_length, dim = get_shape_list(x_flat, expected_rank=2)\n\n    if dim != size_per_head * num_attention_heads:\n        raise ValueError(""passed in a tensor of shape {} when size_per_head={} and num_attention_heads={}"".format(\n            (batch_size_seq_length, dim), size_per_head, num_attention_heads\n        ))\n\n    query = _attention_projection_and_transpose(x_flat, batch_size=batch_size, seq_length=seq_length,\n                                                num_attention_heads=num_attention_heads, size_per_head=size_per_head,\n                                                name=\'query_layer\',\n                                                initializer_range=initializer_range)\n    key = _attention_projection_and_transpose(x_flat, batch_size=batch_size, seq_length=seq_length,\n                                              num_attention_heads=num_attention_heads, size_per_head=size_per_head,\n                                              name=\'key_layer\',\n                                              initializer_range=initializer_range)\n\n    value = _attention_projection_and_transpose(x_flat, batch_size=batch_size, seq_length=seq_length,\n                                                num_attention_heads=num_attention_heads, size_per_head=size_per_head,\n                                                name=\'value_layer\',\n                                                initializer_range=initializer_range)\n\n    # Add to cache\n    cached_keys_and_values = tf.stack([key, value], axis=1) if do_cache else None\n\n    # Things that were relevant from the cache\n    if cache is not None:\n        pk, pv = tf.unstack(cache, axis=1)\n        key = tf.concat([pk, key], axis=-2)\n        value = tf.concat([pv, value], axis=-2)\n\n    # Multiply [batch_size, num_attention_heads, seq_length, size_per_head] with\n    #          [batch_size, num_attention_heads, size_per_head, seq_length+cached_length] ->\n    #          [batch_size, num_attention_heads, seq_length, seq_length+cached_length]\n    attention_scores = tf.matmul(query, key, transpose_b=True)\n    attention_scores = tf.multiply(attention_scores,\n                                   1.0 / math.sqrt(float(size_per_head)))\n    attention_scores = mask_attention_for_ltr(attention_scores, attention_mask)\n    attention_probs = tf.nn.softmax(attention_scores)\n\n    # This is actually dropping out entire tokens to attend to, which might\n    # seem a bit unusual, but is taken from the original Transformer paper.\n    # NOPENOPENOPENOPE\n    # attention_probs = factoreddropout(attention_probs, attention_probs_dropout_prob)\n\n    # Multiply [batch_size, num_attention_heads, seq_length, seq_length+cached_length] with\n    #          [batch_size, num_attention_heads, seq_length+cached_length, size_per_head] ->\n    #          [batch_size, num_attention_heads, seq_length, size_per_head] ->\n    context_layer = tf.matmul(attention_probs, value)\n\n    # `context_layer` = [batch_size, seq_length, num_attention_heads, size_per_head]\n    context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n    context_layer = tf.reshape(context_layer, [batch_size * seq_length, num_attention_heads * size_per_head])\n\n    context_layer_projected = tf.layers.dense(\n        context_layer,\n        num_attention_heads * size_per_head,\n        kernel_initializer=create_initializer(initializer_range),\n        name=\'context_projection_layer\'\n    )\n    context_layer_projected = dropout(context_layer_projected, hidden_dropout_prob)\n\n    return context_layer_projected, cached_keys_and_values\n\n\ndef residual_mlp_layer(x_flat, intermediate_size, initializer_range=0.02, hidden_dropout_prob=0.1):\n    """"""\n    :param x: The attention output. It should be [batch_size*seq_length, dim]\n    :param intermediate_size: the hidden projection. By default this is the input_dim * 4.\n\n    in the original GPT we would return layer_norm(x_norm + h1) rather than layer_norm(x + h1)\n\n    :return:\n    """"""\n    batch_size_seq_length, hidden_size = get_shape_list(x_flat, expected_rank=2)\n    x_norm = layer_norm(x_flat, name=\'mlp_ln0\')\n\n    intermediate_output = tf.layers.dense(\n        x_norm,\n        intermediate_size,\n        activation=gelu,\n        kernel_initializer=create_initializer(initializer_range),\n        name=\'intermediate\',\n    )\n\n    output_for_residual = tf.layers.dense(\n        intermediate_output,\n        hidden_size,\n        name=\'output\',\n        kernel_initializer=create_initializer(initializer_range))\n    output_for_residual = dropout(output_for_residual, hidden_dropout_prob)\n\n    layer_output = layer_norm(x_flat + output_for_residual, name=\'mlp_ln1\')\n    return layer_output\n\n\ndef embed(input_ids,\n          vocab_size,\n          embedding_size,\n          position_offset=0,\n          initializer_range=0.02,\n          max_position_embeddings=512,\n          use_one_hot_embeddings=True):\n    """"""reur and position embeddings\n    :param input_ids: int Tensor of shape [batch_size, seq_length].\n    :param vocab_size: number of words in vocab\n    :param embedding_size: dimensionality of the embedding\n    :param position_offset: aka number of cached tokens.\n    :param initializer_range: float. Range of the weight initialization.\n    :param max_position_embeddings: int. Maximum sequence length.\n    :param use_one_hot_embeddings: probably want this to be true\n    :return: [batch_size, seq_length, embedding_size] embedded tensor\n    """"""\n    (batch_size, seq_length) = get_shape_list(input_ids, expected_rank=2)\n\n    embedding_table = tf.get_variable(\n        name=\'word_embed\',\n        shape=[vocab_size, embedding_size],\n        initializer=create_initializer(initializer_range),\n    )\n\n    assert_op = tf.assert_less_equal(tf.reduce_max(input_ids), vocab_size - 1)\n    with tf.control_dependencies([assert_op]):\n        if use_one_hot_embeddings:\n            flat_input_ids = tf.reshape(input_ids, [-1])\n            one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n            output_flat = tf.matmul(one_hot_input_ids, embedding_table)\n        else:\n            output_flat = tf.nn.embedding_lookup(embedding_table, input_ids)\n\n        embedded_input = tf.reshape(output_flat, [batch_size, seq_length, embedding_size])\n\n    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n\n    with tf.control_dependencies([assert_op]):\n        full_position_embeddings = tf.get_variable(\n            name=\'pos_embed\',\n            shape=[max_position_embeddings, embedding_size],\n            initializer=create_initializer(initializer_range),\n        )\n        # Since the position embedding table is a learned variable, we create it\n        # using a (long) sequence length `max_position_embeddings`. The actual\n        # sequence length might be shorter than this, for faster training of\n        # tasks that do not have long sequences.\n        #\n        # So `full_position_embeddings` is effectively an embedding table\n        # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n        # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n        # perform a slice.\n        if position_offset == 0:\n            embedded_input += tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])[None]\n        else:\n            # Tensorflow is too stupid to allow slicing\n            flat_pos_ids = (tf.range(seq_length, dtype=tf.int32) + position_offset)\n            one_hot_pos_ids = tf.one_hot(flat_pos_ids, depth=max_position_embeddings)\n\n            # [seq_length, full_position_embeddings], [full_position_embeddings, dim]\n            seq_embeds = tf.matmul(one_hot_pos_ids, full_position_embeddings)\n            embedded_input += seq_embeds[None]\n\n            # embedded_input += tf.slice(full_position_embeddings[position_offset:], [0, 0], [seq_length, -1])[None]\n\n    return layer_norm(embedded_input, name=\'embed_norm\'), embedding_table\n\n\ndef _top_p_sample(logits, ignore_ids=None, num_samples=1, p=0.9):\n    """"""\n    Does top-p sampling. if ignore_ids is on, then we will zero out those logits.\n    :param logits: [batch_size, vocab_size] tensor\n    :param ignore_ids: [vocab_size] one-hot representation of the indices we\'d like to ignore and never predict,\n                        like padding maybe\n    :param p: topp threshold to use, either a float or a [batch_size] vector\n    :return: [batch_size, num_samples] samples\n\n    # TODO FIGURE OUT HOW TO DO THIS ON TPUS. IT\'S HELLA SLOW RIGHT NOW, DUE TO ARGSORT I THINK\n    """"""\n    with tf.variable_scope(\'top_p_sample\'):\n        batch_size, vocab_size = get_shape_list(logits, expected_rank=2)\n\n        probs = tf.nn.softmax(logits if ignore_ids is None else logits - tf.cast(ignore_ids[None], tf.float32) * 1e10,\n                              axis=-1)\n\n        if isinstance(p, float) and p > 0.999999:\n            # Don\'t do top-p sampling in this case\n            print(""Top-p sampling DISABLED"", flush=True)\n            return {\n                \'probs\': probs,\n                \'sample\': tf.random.categorical(\n                    logits=logits if ignore_ids is None else logits - tf.cast(ignore_ids[None], tf.float32) * 1e10,\n                    num_samples=num_samples, dtype=tf.int32),\n            }\n\n        # [batch_size, vocab_perm]\n        indices = tf.argsort(probs, direction=\'DESCENDING\')\n        cumulative_probabilities = tf.math.cumsum(tf.batch_gather(probs, indices), axis=-1, exclusive=False)\n\n        # find the top pth index to cut off. careful we don\'t want to cutoff everything!\n        # result will be [batch_size, vocab_perm]\n        p_expanded = p if isinstance(p, float) else p[:, None]\n        exclude_mask = tf.logical_not(\n            tf.logical_or(cumulative_probabilities < p_expanded, tf.range(vocab_size)[None] < 1))\n\n        # OPTION A - sample in the sorted space, then unsort.\n        logits_to_use = tf.batch_gather(logits, indices) - tf.cast(exclude_mask, tf.float32) * 1e10\n        sample_perm = tf.random.categorical(logits=logits_to_use, num_samples=num_samples)\n        sample = tf.batch_gather(indices, sample_perm)\n\n        # OPTION B - unsort first - Indices need to go back to 0 -> N-1 -- then sample\n        # unperm_indices = tf.argsort(indices, direction=\'ASCENDING\')\n        # include_mask_unperm = tf.batch_gather(include_mask, unperm_indices)\n        # logits_to_use = logits - (1 - tf.cast(include_mask_unperm, tf.float32)) * 1e10\n        # sample = tf.random.categorical(logits=logits_to_use, num_samples=num_samples, dtype=tf.int32)\n\n    return {\n        \'probs\': probs,\n        \'sample\': sample,\n    }\n\n\ndef _top_k_sample(logits, ignore_ids=None, num_samples=1, k=10):\n    """"""\n    Does top-k sampling. if ignore_ids is on, then we will zero out those logits.\n    :param logits: [batch_size, vocab_size] tensor\n    :param ignore_ids: [vocab_size] one-hot representation of the indices we\'d like to ignore and never predict,\n                        like padding maybe\n    :param p: topp threshold to use, either a float or a [batch_size] vector\n    :return: [batch_size, num_samples] samples\n\n    # TODO FIGURE OUT HOW TO DO THIS ON TPUS. IT\'S HELLA SLOW RIGHT NOW, DUE TO ARGSORT I THINK\n    """"""\n    with tf.variable_scope(\'top_p_sample\'):\n        batch_size, vocab_size = get_shape_list(logits, expected_rank=2)\n\n        probs = tf.nn.softmax(logits if ignore_ids is None else logits - tf.cast(ignore_ids[None], tf.float32) * 1e10,\n                              axis=-1)\n        # [batch_size, vocab_perm]\n        indices = tf.argsort(probs, direction=\'DESCENDING\')\n\n        # find the top pth index to cut off. careful we don\'t want to cutoff everything!\n        # result will be [batch_size, vocab_perm]\n        k_expanded = k if isinstance(k, int) else k[:, None]\n        exclude_mask = tf.range(vocab_size)[None] >= k_expanded\n\n        # OPTION A - sample in the sorted space, then unsort.\n        logits_to_use = tf.batch_gather(logits, indices) - tf.cast(exclude_mask, tf.float32) * 1e10\n        sample_perm = tf.random.categorical(logits=logits_to_use, num_samples=num_samples)\n        sample = tf.batch_gather(indices, sample_perm)\n\n    return {\n        \'probs\': probs,\n        \'sample\': sample,\n    }\n\n\nclass GroverModel(object):\n    def __init__(self,\n                 config: GroverConfig,\n                 is_training,\n                 input_ids,\n                 cache=None,\n                 do_cache=False,\n                 pad_token_id=0,\n                 chop_off_last_token=True,\n                 scope=None,\n                 reuse=False):\n        """"""\n        :param config:\n        :param is_training:\n        :param input_ids: Tensor thats of size [batch_size, seq_length]\n        :param cache: Optionally, a tensor to use that will contain cached information of the size\n            [batch_size, num_layers, 2, num_heads, cache_length, features]\n        :param do_cache: Whether to cache again.\n        :param pad_token_id: Which token will be used for padding (probably 0.)\n        :param chop_off_last_token: True if we will end up using this for TRAINING only. False if we want to generate.\n                                    it means the last token in input_ids will not be processed by the model as input\n        :param scope: scope to run this on\n        """"""\n        self.config = copy.deepcopy(config)\n        self.is_training = is_training\n        self.pad_token_id = pad_token_id\n\n        if not is_training:\n            self.config.hidden_dropout_prob = 0.0\n            self.config.attention_probs_dropout_prob = 0.0\n\n        if chop_off_last_token:\n            self.target_ids = input_ids[:, 1:]\n            self.input_ids = input_ids[:, :-1]\n        else:\n            self.input_ids = input_ids\n            self.target_ids = tf.concat((input_ids[:, 1:],\n                                         tf.constant(self.pad_token_id, dtype=self.input_ids.dtype,\n                                                     shape=[get_shape_list(self.input_ids, 2)[0], 1])), 1)\n\n        self.batch_size, self.seq_length = get_shape_list(self.input_ids, 2)\n\n        if cache is None:\n            caches = [None] * config.num_hidden_layers\n            self.cache_length = 0\n        else:\n            batch_size_, num_layers_, two_, num_heads_, self.cache_length, features_ = get_shape_list(\n                cache, expected_rank=6)\n            assert batch_size_ == self.batch_size\n            assert num_layers_ == config.num_hidden_layers\n            assert two_ == 2\n            assert num_heads_ == config.num_attention_heads\n            assert features_ == (config.hidden_size // config.num_attention_heads)\n            caches = tf.unstack(cache, axis=1)\n\n        with tf.variable_scope(scope, default_name=\'newslm\', reuse=reuse):\n            with tf.variable_scope(""embeddings""):\n                embeddings, self.embedding_table = embed(self.input_ids, config.vocab_size,\n                                                         config.hidden_size,\n                                                         position_offset=self.cache_length,\n                                                         initializer_range=config.initializer_range,\n                                                         max_position_embeddings=config.max_position_embeddings,\n                                                         use_one_hot_embeddings=True)\n\n            mask = get_attention_mask(self.seq_length, self.seq_length + self.cache_length, dtype=embeddings.dtype)\n\n            # We keep the representation as a 2D tensor to avoid re-shaping it back and\n            # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n            # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n            # help the optimizer.\n            hidden_state = tf.reshape(embeddings, [self.batch_size * self.seq_length, self.config.hidden_size])\n            new_kvs = []\n            for layer_idx, layer_cache in enumerate(caches):\n                with tf.variable_scope(\'layer{:02d}\'.format(layer_idx)):\n                    # [batch_size * seq_length, hidden_size]\n                    attention_output, new_kv = attention_layer(\n                        hidden_state,\n                        mask,\n                        batch_size=self.batch_size,\n                        seq_length=self.seq_length,\n                        size_per_head=config.hidden_size // config.num_attention_heads,\n                        num_attention_heads=config.num_attention_heads,\n                        initializer_range=config.initializer_range,\n                        hidden_dropout_prob=self.config.hidden_dropout_prob,\n                        attention_probs_dropout_prob=self.config.attention_probs_dropout_prob,\n                        do_cache=do_cache,\n                        cache=layer_cache,\n                    )\n                    new_kvs.append(new_kv)\n\n                    # [batch_size * seq_length, hidden_size]\n                    hidden_state = residual_mlp_layer(hidden_state + attention_output,\n                                                      intermediate_size=config.intermediate_size,\n                                                      hidden_dropout_prob=self.config.hidden_dropout_prob)\n            self.hidden_state = hidden_state\n\n        self.new_kvs = tf.stack(new_kvs, axis=1) if do_cache else None\n\n        # Note that the hidden state is still flat (batch_size*hidden_size)\n        self.logits_flat = tf.matmul(self.hidden_state, self.embedding_table, transpose_b=True)\n\n        # THE OUTPUT BIAS DOES NOT SPARK JOY\n        # output_bias = tf.get_variable(\'output_bias\', shape=[config.vocab_size], initializer=tf.zeros_initializer())\n        # self.logits_flat = tf.nn.bias_add(self.logits_flat, output_bias)\n\n    @property\n    def log_probs(self):\n        logprobs_flat = tf.nn.log_softmax(self.logits_flat, axis=-1)\n        return tf.reshape(logprobs_flat, [self.batch_size, self.seq_length, -1])\n\n    def lm_loss(self):\n        """"""\n        :return: stuff\n        """"""\n        target_ids_flat = tf.reshape(self.target_ids, [-1])\n\n        # 1 if it\'s valid and 0 otherwise.\n        label_weights = tf.cast(tf.not_equal(target_ids_flat, self.pad_token_id), dtype=self.logits_flat.dtype)\n\n        # [batch_size * seq_length, vocab_size]\n        one_hot_labels = tf.one_hot(target_ids_flat,\n                                    depth=self.config.vocab_size,\n                                    dtype=self.logits_flat.dtype)\n\n        # [batch_size * seq_length, vocab_size]\n        logprobs_flat = tf.nn.log_softmax(self.logits_flat, axis=-1)\n\n        per_example_loss = -tf.reduce_sum(logprobs_flat * one_hot_labels, axis=[-1])\n\n        # per_example_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits_flat, labels=target_ids_flat)\n\n        numerator = tf.reduce_sum(label_weights * per_example_loss)\n        denominator = tf.reduce_sum(label_weights) + 1e-5\n        loss = numerator / denominator\n        return loss\n\n    def pooled_output(self, clf_token):\n        """"""\n        Extract pooled output given a token that says where we should look\n        :param clf_token:\n        :return:\n        """"""\n        pool_idx = tf.cast(tf.argmax(tf.cast(tf.equal(self.input_ids, clf_token), tf.float32), 1), tf.int32)\n        return tf.gather(self.hidden_state, tf.range(self.batch_size, dtype=tf.int32) * self.seq_length + pool_idx)\n\n\ndef model_fn_builder(config: GroverConfig, init_checkpoint, learning_rate,\n                     num_train_steps, num_warmup_steps, use_tpu):\n    """"""Returns `model_fn` closure for TPUEstimator.""""""\n\n    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n        """"""The `model_fn` for TPUEstimator.""""""\n\n        tf.logging.info(""*** Features ***"")\n        for name in sorted(features.keys()):\n            tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))\n\n        input_ids = features[""input_ids""]\n\n        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n        model = GroverModel(\n            config=config,\n            is_training=is_training,\n            input_ids=input_ids,\n            pad_token_id=config.pad_token_id,\n            chop_off_last_token=True,\n        )\n\n        total_loss = model.lm_loss()\n\n        if is_training:\n            train_op, train_metrics = optimization_adafactor.create_optimizer(\n                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n            tvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n        else:\n            train_op = None\n            train_metrics = {}\n            tvars = tf.trainable_variables()\n\n        initialized_variable_names = {}\n        scaffold_fn = None\n        if init_checkpoint:\n            (assignment_map, initialized_variable_names\n             ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n            if use_tpu:\n                def tpu_scaffold():\n                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n                    return tf.train.Scaffold()\n\n                scaffold_fn = tpu_scaffold\n            else:\n                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n        tf.logging.info(""**** Trainable Variables ****"")\n        for var in tvars:\n            init_string = """"\n            if var.name in initialized_variable_names:\n                init_string = "", *INIT_FROM_CKPT*""\n            tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,\n                            init_string)\n\n        output_spec = None\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            if use_tpu:\n                output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n                    mode=mode,\n                    loss=total_loss,\n                    train_op=train_op,\n                    host_call=construct_scalar_host_call(metric_dict=train_metrics, model_dir=params[\'model_dir\'],\n                                                         prefix=\'training/\'),\n                    scaffold_fn=scaffold_fn)\n            else:\n                output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n                    mode=mode,\n                    loss=total_loss,\n                    train_op=train_op,\n                    training_hooks=[\n                        tf.train.LoggingTensorHook({\'loss\': tf.metrics.mean(total_loss)[1]}, every_n_iter=100)],\n                    scaffold_fn=scaffold_fn)\n\n        elif mode == tf.estimator.ModeKeys.EVAL:\n            def metric_fn(total_loss):\n                loss = tf.metrics.mean(values=total_loss)\n                return {\n                    ""eval_loss"": loss,\n                }\n\n            eval_metrics = (metric_fn,\n                            [total_loss])\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n                mode=mode,\n                loss=total_loss,\n                eval_metrics=eval_metrics,\n                scaffold_fn=scaffold_fn)\n        else:\n            gt_logprobs = tf.squeeze(tf.batch_gather(model.log_probs, model.target_ids[:, :, None]), axis=2)\n\n            # Need top-p required under topp sampling!\n            better_than_gt = model.log_probs > gt_logprobs[:, :, None]\n            top_p_required = tf.reduce_sum(tf.cast(better_than_gt, tf.float32) * tf.exp(model.log_probs), axis=2)\n\n            # No top-p sampling for now, since this seems to be too slow on TPUs\n            if use_tpu:\n                predictions = tf.reshape(\n                    tf.random.categorical(logits=model.logits_flat, num_samples=1),\n                    get_shape_list(model.target_ids),\n                )\n            else:\n                # Argmax\n                # predictions = tf.math.argmax(model.log_probs, axis=-1, output_type=tf.int32)\n                predictions = tf.reshape(\n                    _top_p_sample(model.logits_flat, num_samples=1, p=0.99)[\'sample\'],\n                    get_shape_list(model.target_ids),\n                )\n            pred_logprobs = tf.squeeze(tf.batch_gather(model.log_probs, predictions[:, :, None]), axis=2)\n\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n                mode=mode,\n                predictions={\'gt_logprobs\': gt_logprobs,\n                             \'top_p_required\': top_p_required,\n                             \'predictions\': predictions,\n                             \'pred_logprobs\': pred_logprobs,\n                             \'labels\': input_ids},\n                scaffold_fn=scaffold_fn)\n        return output_spec\n\n    return model_fn\n\n\ndef sample_step(tokens, ignore_ids, news_config, batch_size=1, p_for_topp=0.95, cache=None, do_topk=False):\n    """"""\n    Helper function that samples from grover for a single step\n    :param tokens: [batch_size, n_ctx_b] tokens that we will predict from\n    :param ignore_ids: [n_vocab] mask of the tokens we don\'t want to predict\n    :param news_config: config for the GroverModel\n    :param batch_size: batch size to use\n    :param p_for_topp: top-p or top-k threshold\n    :param cache: [batch_size, news_config.num_hidden_layers, 2,\n                   news_config.num_attention_heads, n_ctx_a,\n                   news_config.hidden_size // news_config.num_attention_heads] OR, None\n    :return: new_tokens, size [batch_size]\n             new_probs, also size [batch_size]\n             new_cache, size [batch_size, news_config.num_hidden_layers, 2, n_ctx_b,\n                   news_config.num_attention_heads, news_config.hidden_size // news_config.num_attention_heads]\n    """"""\n    model = GroverModel(\n        config=news_config,\n        is_training=False,\n        input_ids=tokens,\n        reuse=tf.AUTO_REUSE,\n        scope=\'newslm\',\n        chop_off_last_token=False,\n        do_cache=True,\n        cache=cache,\n    )\n\n    # Extract the FINAL SEQ LENGTH\n    batch_size_times_seq_length, vocab_size = get_shape_list(model.logits_flat, expected_rank=2)\n    next_logits = tf.reshape(model.logits_flat, [batch_size, -1, vocab_size])[:, -1]\n\n    if do_topk:\n        sample_info = _top_k_sample(next_logits, num_samples=1, k=tf.cast(p_for_topp, dtype=tf.int32))\n    else:\n        sample_info = _top_p_sample(next_logits, ignore_ids=ignore_ids, num_samples=1, p=p_for_topp)\n\n    new_tokens = tf.squeeze(sample_info[\'sample\'], 1)\n    new_probs = tf.squeeze(tf.batch_gather(sample_info[\'probs\'], sample_info[\'sample\']), 1)\n    return {\n        \'new_tokens\': new_tokens,\n        \'new_probs\': new_probs,\n        \'new_cache\': model.new_kvs,\n    }\n\n\ndef initialize_from_context(initial_context, ignore_ids, news_config, p_for_topp=0.95, do_topk=False):\n    """""" same signature as sample_step""""""\n    batch_size, _ = get_shape_list(initial_context, expected_rank=2)\n\n    context_output = sample_step(tokens=initial_context, ignore_ids=ignore_ids, news_config=news_config,\n                                 batch_size=batch_size, p_for_topp=p_for_topp, cache=None, do_topk=do_topk)\n    return {\n        \'tokens\': tf.concat([initial_context, context_output[\'new_tokens\'][:, None]], 1),\n        \'cache\': context_output[\'new_cache\'],\n        \'probs\': context_output[\'new_probs\'][:, None]\n    }\n\n\ndef sample(news_config: GroverConfig, initial_context, eos_token, min_len, ignore_ids=None, p_for_topp=0.95,\n           do_topk=False):\n    """"""\n    V1 version of: sample outputs from a model, and do it all at once\n    :param news_config: Configuration used to construct the model\n    :param initial_context: [batch_size, seq_length] that we\'ll start generating with\n    :param eos_token: Stop generating if you see this (tf scalar)\n    :param min_len: min length of sample\n    :param ignore_ids: NEVER GENERATE THESE [vocab_size]\n    :return:\n    """"""\n    batch_size, _ = get_shape_list(initial_context, expected_rank=2)\n\n    if ignore_ids is None:\n        ignore_ids = tf.constant([x == 0 for x in range(news_config.vocab_size)], dtype=tf.bool)\n\n    with tf.name_scope(\'sample_sequence\'):\n        # Initial call to get cache\n        context_output = initialize_from_context(initial_context, ignore_ids=ignore_ids, news_config=news_config,\n                                                 p_for_topp=p_for_topp,\n                                                 do_topk=do_topk)\n        ctx = context_output[\'tokens\']\n        cache = context_output[\'cache\']\n        probs = context_output[\'probs\']\n\n        def body(ctx, cache, probs):\n            """""" for whatever reason this didn\'t work when I ran it on more than one at once... ugh.""""""\n            next_outputs = sample_step(ctx[:, -1][:, None], ignore_ids=ignore_ids, news_config=news_config,\n                                       batch_size=batch_size, p_for_topp=p_for_topp, cache=cache,\n                                       do_topk=do_topk)\n\n            # Update everything\n            new_cache = tf.concat([cache, next_outputs[\'new_cache\']], axis=-2)\n            new_ids = tf.concat([ctx, next_outputs[\'new_tokens\'][:, None]], axis=1)\n            new_probs = tf.concat([probs, next_outputs[\'new_probs\'][:, None]], axis=1)\n            return [new_ids, new_cache, new_probs]\n\n        def cond(ctx, cache, probs):\n            # ctx = tf.Print(ctx,[tf.shape(ctx)])\n            is_eos = tf.reduce_all(tf.reduce_any(tf.equal(ctx[:,-1:], eos_token), axis=1))\n            is_len = tf.greater(get_shape_list(ctx)[1], min_len)\n            return tf.logical_not(tf.logical_and(is_eos, is_len))\n\n        tokens, cache, probs = tf.while_loop(\n            cond=cond, body=body, maximum_iterations=1025 - get_shape_list(ctx)[1],\n            loop_vars=[ctx, cache, probs],\n            shape_invariants=[tf.TensorShape([batch_size, None]),\n                              tf.TensorShape(\n                                  [batch_size, news_config.num_hidden_layers, 2,\n                                   news_config.num_attention_heads,\n                                   None, news_config.hidden_size // news_config.num_attention_heads]),\n                              tf.TensorShape([batch_size, None]),\n                              ],\n            back_prop=False,\n        )\n    return tokens, probs\n'"
train/optimization_adafactor.py,39,"b'# Original work Copyright 2018 The Google AI Language Team Authors.\n# Modified work Copyright 2019 Rowan Zellers\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport re\nimport tensorflow as tf\nfrom train.utils import get_shape_list\n\n\ndef create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n    """"""Creates an optimizer training op.""""""\n    global_step = tf.train.get_or_create_global_step()\n\n    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n\n    # Implements linear decay of the learning rate.\n    learning_rate = tf.train.polynomial_decay(\n        learning_rate,\n        global_step,\n        num_train_steps,\n        end_learning_rate=0.0,\n        power=1.0,\n        cycle=False)\n\n    # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n    # learning rate will be `global_step/num_warmup_steps * init_lr`.\n    if num_warmup_steps:\n        global_steps_int = tf.cast(global_step, tf.int32)\n        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n\n        global_steps_float = tf.cast(global_steps_int, tf.float32)\n        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n        warmup_percent_done = global_steps_float / warmup_steps_float\n        warmup_learning_rate = init_lr * warmup_percent_done\n\n        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n        learning_rate = (\n                (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n\n    # It is recommended that you use this optimizer for fine tuning, since this\n    # is how the model was trained (note that the Adam m/v variables are NOT\n    # loaded from init_checkpoint.)\n    optimizer = AdaFactorOptimizer(\n        learning_rate=learning_rate,\n        weight_decay_rate=0.01,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-6,\n        exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])\n\n    if use_tpu:\n        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n\n    tvars = tf.trainable_variables()\n    grads = tf.gradients(loss, tvars)\n\n    # You could do this, but instead we don\'t because a) it\'s slow and b) we already did the \'update clipping\'\n    # (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n\n    train_op = optimizer.apply_gradients(\n        zip(grads, tvars), global_step=global_step)\n\n    # Normally the global step update is done inside of `apply_gradients`.\n    # However, `AdaFactorOptimizer` doesn\'t do this. But if you use\n    # a different optimizer, you should probably take this line out.\n    new_global_step = global_step + 1\n    train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n\n    train_metrics = {\n        \'learning_rate\': learning_rate,\n        \'minibatch_loss\': loss,\n        # \'minibatch_ppl\': tf.math.exp(loss),\n    }\n    return train_op, train_metrics\n\n\nclass AdaFactorOptimizer(tf.compat.v1.train.Optimizer):\n    """"""here\'s the optimizer we\'ll use""""""\n\n    def __init__(self,\n                 learning_rate,\n                 weight_decay_rate=0.0,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-6,\n                 exclude_from_weight_decay=None,\n                 clipping_rate=1.0,\n                 name=""AdaFactorOptimizer""):\n        """"""Constructs a AdaFactorOptimizer.""""""\n        super(AdaFactorOptimizer, self).__init__(False, name)\n\n        self.learning_rate = learning_rate\n        self.weight_decay_rate = weight_decay_rate\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n        self.epsilon1 = 1e-30\n        self.epsilon2 = 0.001\n        self.clipping_rate = clipping_rate\n        self.exclude_from_weight_decay = exclude_from_weight_decay\n        self.use_locking = False\n\n    def _use_factored(self, shape):\n        return len(shape) >= 2\n\n    def _parameter_scale(self, var):\n        """"""Estimate the scale of the parameters from the current values.\n        We include a minimum value of 0.001 to give it a chance to escape 0\n        if it was zero-initialized.\n        Instead of using the value, we could impute the scale from the shape,\n        as initializers do.\n        Args:\n          var: a variable or Tensor.\n        Returns:\n          a Scalar\n        """"""\n        return tf.maximum(reduce_rms(var), self.epsilon2)\n\n    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n        """"""See base class.""""""\n        assignments = []\n        for (grad, param) in grads_and_vars:\n            if grad is None or param is None:\n                continue\n\n            param_name = self._get_variable_name(param.name)\n            shape_list = get_shape_list(param, expected_rank=[1, 2])\n\n            # decay_rate = 1 - tf.pow(tf.cast(tf.train.get_or_create_global_step(), tf.float32) + 1.0, -0.8)\n            decay_rate = self.beta_2\n            grad_squared = tf.square(grad) + self.epsilon1\n\n            update_scale = self.learning_rate\n            # update_scale = self.learning_rate * tf.cast(self._parameter_scale(param), dtype=tf.float32)\n\n            # HACK: Make things dependent on grad.\n            # This confounds the XLA rewriter and keeps it from fusing computations\n            # across different variables.  This fusion is a bad for HBM usage, since\n            # it causes the gradients to persist in memory.\n            grad_squared_mean = tf.reduce_mean(grad_squared)\n            decay_rate += grad_squared_mean * 1e-30\n            update_scale += grad_squared_mean * 1e-30\n\n            # END HACK\n\n            if self._use_factored(shape_list):\n                num_rows, num_columns = shape_list\n\n                vr = tf.get_variable(\n                    name=param_name + ""/adafactor_vr"",\n                    shape=[num_rows],\n                    dtype=tf.float32,\n                    trainable=False,\n                    initializer=tf.zeros_initializer())\n                vc = tf.get_variable(\n                    name=param_name + ""/adafactor_vc"",\n                    shape=[num_columns],\n                    dtype=tf.float32,\n                    trainable=False,\n                    initializer=tf.zeros_initializer())\n\n                next_vr = decay_rate * vr + (1 - decay_rate) * tf.reduce_mean(grad_squared, 1)\n                next_vc = decay_rate * vc + (1 - decay_rate) * tf.reduce_mean(grad_squared, 0)\n\n                long_term_mean = tf.reduce_mean(next_vr, -1, keepdims=True)\n                r_factor = tf.rsqrt(next_vr / long_term_mean + self.epsilon1)\n                c_factor = tf.rsqrt(next_vc + self.epsilon1)\n                update = grad * tf.expand_dims(r_factor, -1) * tf.expand_dims(c_factor, -2)\n\n                assignments.append(vr.assign(next_vr, use_locking=self.use_locking))\n                assignments.append(vc.assign(next_vc, use_locking=self.use_locking))\n            else:\n                v = tf.get_variable(\n                    name=param_name + ""/adafactor_v"",\n                    shape=shape_list,\n                    dtype=tf.float32,\n                    trainable=False,\n                    initializer=tf.zeros_initializer())\n                next_v = decay_rate * v + (1 - decay_rate) * grad_squared\n\n                assignments.append(v.assign(next_v, use_locking=self.use_locking))\n                update = grad * tf.rsqrt(next_v + self.epsilon1)\n\n            clipping_denom = tf.maximum(1.0, reduce_rms(update) / self.clipping_rate)\n            update /= clipping_denom\n\n            # Do weight decay\n            # Just adding the square of the weights to the loss function is *not*\n            # the correct way of using L2 regularization/weight decay with Adam,\n            # since that will interact with the m and v parameters in strange ways.\n            #\n            # Instead we want ot decay the weights in a manner that doesn\'t interact\n            # with the m/v parameters. This is equivalent to adding the square\n            # # of the weights to the loss with plain (non-momentum) SGD.\n            if self._do_use_weight_decay(param_name):\n                update += self.weight_decay_rate * param\n\n            update_with_lr = update_scale * update\n            next_param = param - update_with_lr\n\n            assignments.append(param.assign(next_param, use_locking=self.use_locking))\n        return tf.group(*assignments, name=name)\n\n    def _do_use_weight_decay(self, param_name):\n        """"""Whether to use L2 weight decay for `param_name`.""""""\n        if not self.weight_decay_rate:\n            return False\n        if self.exclude_from_weight_decay:\n            for r in self.exclude_from_weight_decay:\n                if re.search(r, param_name) is not None:\n                    return False\n        return True\n\n    def _get_variable_name(self, param_name):\n        """"""Get the variable name from the tensor name.""""""\n        m = re.match(""^(.*):\\\\d+$"", param_name)\n        if m is not None:\n            param_name = m.group(1)\n        return param_name\n\n\ndef reduce_rms(x):\n    return tf.sqrt(tf.reduce_mean(tf.square(x)))\n'"
train/train_tpu.py,14,"b'# Original work Copyright 2018 The Google AI Language Team Authors.\n# Modified work Copyright 2019 Rowan Zellers\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n"""""" Training script! """"""\n\nimport tensorflow.compat.v1 as tf\n\nfrom train.dataloader import input_fn_builder\nfrom train.modeling import model_fn_builder, GroverConfig\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\n## Required parameters\nflags.DEFINE_string(\n    ""config_file"", \'configs/base.json\',\n    ""The config json file corresponding to the pre-trained news model. ""\n    ""This specifies the model architecture."")\n\nflags.DEFINE_string(\n    ""input_file"", None,\n    ""Input TF example files (can be a glob or comma separated)."")\n\nflags.DEFINE_string(\n    ""output_dir"", None,\n    ""The output directory where the model checkpoints will be written."")\n\n## Other parameters\nflags.DEFINE_string(\n    ""init_checkpoint"", None,\n    ""Initial checkpoint (usually from a pre-trained model)."")\n\nflags.DEFINE_integer(\n    ""max_seq_length"", 1024,\n    ""The maximum total input sequence length after BPE tokenization. ""\n    ""Sequences longer than this will be truncated, and sequences shorter ""\n    ""than this will be padded. Must match data generation."")\n\nflags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")\n\nflags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for adafactor."")\n\nflags.DEFINE_integer(""num_train_steps"", 100000, ""Number of training steps."")\n\nflags.DEFINE_integer(""num_warmup_steps"", 10000, ""Number of warmup steps."")\n\nflags.DEFINE_integer(""save_checkpoints_steps"", 1000,\n                     ""How often to save the model checkpoint."")\n\nflags.DEFINE_integer(""iterations_per_loop"", 1000,\n                     ""How many steps to make in each estimator call."")\n\nflags.DEFINE_integer(""max_eval_steps"", 100, ""Maximum number of eval steps."")\n\nflags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")\n\nflags.DEFINE_string(\n    ""tpu_name"", None,\n    ""The Cloud TPU to use for training. This should be either the name ""\n    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""\n    ""url."")\n\nflags.DEFINE_string(\n    ""tpu_zone"", None,\n    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""\n    ""specified, we will attempt to automatically detect the GCE project from ""\n    ""metadata."")\n\nflags.DEFINE_string(\n    ""gcp_project"", None,\n    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""\n    ""specified, we will attempt to automatically detect the GCE project from ""\n    ""metadata."")\n\nflags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")\n\nflags.DEFINE_integer(\n    ""num_tpu_cores"", 8,\n    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")\n\n\ndef main(_):\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    news_config = GroverConfig.from_json_file(FLAGS.config_file)\n\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n\n    input_files = []\n    for input_pattern in FLAGS.input_file.split("",""):\n        input_files.extend(tf.gfile.Glob(input_pattern))\n\n    tf.logging.info(""*** Input Files ***"")\n    for input_file in input_files:\n        tf.logging.info(""  %s"" % input_file)\n\n    tpu_cluster_resolver = None\n    if FLAGS.use_tpu and FLAGS.tpu_name:\n        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n            FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n    run_config = tf.contrib.tpu.RunConfig(\n        cluster=tpu_cluster_resolver,\n        master=FLAGS.master,\n        model_dir=FLAGS.output_dir,\n        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n        keep_checkpoint_max=None,\n        tpu_config=tf.contrib.tpu.TPUConfig(\n            iterations_per_loop=FLAGS.iterations_per_loop,\n            num_shards=FLAGS.num_tpu_cores,\n            per_host_input_for_training=is_per_host))\n\n    model_fn = model_fn_builder(news_config, init_checkpoint=FLAGS.init_checkpoint,\n                                learning_rate=FLAGS.learning_rate,\n                                num_train_steps=FLAGS.num_train_steps,\n                                num_warmup_steps=FLAGS.num_warmup_steps,\n                                use_tpu=FLAGS.use_tpu,\n                                )\n\n    # If TPU is not available, this will fall back to normal Estimator on CPU\n    # or GPU.\n    estimator = tf.contrib.tpu.TPUEstimator(\n        use_tpu=FLAGS.use_tpu,\n        model_fn=model_fn,\n        config=run_config,\n        train_batch_size=FLAGS.train_batch_size,\n        eval_batch_size=FLAGS.train_batch_size,\n        params={\'model_dir\': FLAGS.output_dir}\n    )\n\n    tf.logging.info(""***** Running training *****"")\n    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)\n    train_input_fn = input_fn_builder(\n        input_files=input_files,\n        seq_length=FLAGS.max_seq_length,\n        is_training=True)\n\n    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\n\nif __name__ == ""__main__"":\n    flags.mark_flag_as_required(""input_file"")\n    flags.mark_flag_as_required(""output_dir"")\n    tf.app.run()\n'"
train/utils.py,26,"b'# Original work Copyright 2018 The Google AI Language Team Authors.\n# Modified work Copyright 2019 Rowan Zellers\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nimport re\n\nimport six\nimport tensorflow.compat.v1 as tf\nimport numpy as np\nfrom tensorflow.python.lib.io import file_io\n\n\ndef _save_np(absolute_fn, array):\n    if absolute_fn.startswith(\'gs://\'):\n        with file_io.FileIO(absolute_fn, \'w\') as f:\n            np.save(f, array)\n    else:\n        np.save(absolute_fn, array)\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n    """"""Raises an exception if the tensor rank is not of the expected rank.\n\n    Args:\n      tensor: A tf.Tensor to check the rank of.\n      expected_rank: Python integer or list of integers, expected rank.\n      name: Optional name of the tensor for the error message.\n\n    Raises:\n      ValueError: If the expected shape doesn\'t match the actual shape.\n    """"""\n    if name is None:\n        name = tensor.name\n\n    expected_rank_dict = {}\n    if isinstance(expected_rank, six.integer_types):\n        expected_rank_dict[expected_rank] = True\n    else:\n        for x in expected_rank:\n            expected_rank_dict[x] = True\n\n    actual_rank = tensor.shape.ndims\n    if actual_rank not in expected_rank_dict:\n        scope_name = tf.get_variable_scope().name\n        raise ValueError(\n            ""For the tensor `%s` in scope `%s`, the actual rank ""\n            ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n            (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n    """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n    Args:\n      tensor: A tf.Tensor object to find the shape of.\n      expected_rank: (optional) int. The expected rank of `tensor`. If this is\n        specified and the `tensor` has a different rank, and exception will be\n        thrown.\n      name: Optional name of the tensor for the error message.\n\n    Returns:\n      A list of dimensions of the shape of tensor. All static dimensions will\n      be returned as python integers, and dynamic dimensions will be returned\n      as tf.Tensor scalars.\n    """"""\n    if name is None:\n        name = tensor.name\n\n    if expected_rank is not None:\n        assert_rank(tensor, expected_rank, name)\n\n    shape = tensor.shape.as_list()\n\n    non_static_indexes = []\n    for (index, dim) in enumerate(shape):\n        if dim is None:\n            non_static_indexes.append(index)\n\n    if not non_static_indexes:\n        return shape\n\n    dyn_shape = tf.shape(tensor)\n    for index in non_static_indexes:\n        shape[index] = dyn_shape[index]\n    return shape\n\n\ndef gelu(input_tensor):\n    """"""Gaussian Error Linear Unit.\n\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n\n    Args:\n      input_tensor: float Tensor to perform activation.\n\n    Returns:\n      `input_tensor` with the GELU activation applied.\n    """"""\n    cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n    return input_tensor * cdf\n\n\ndef layer_norm(input_tensor, name=None, epsilon=1e-5):\n    """"""Run layer normalization on the last dimension of the tensor.""""""\n    name2use = f\'LayerNorm_{name}\' if name is not None else name\n    with tf.variable_scope(name2use, default_name=\'LayerNorm\'):\n        dim = input_tensor.shape[-1].value\n        gamma = tf.get_variable(\'gamma\', [dim], initializer=tf.constant_initializer(1))\n        beta = tf.get_variable(\'beta\', [dim], initializer=tf.constant_initializer(0))\n        mean = tf.reduce_mean(input_tensor, axis=-1, keepdims=True)\n        std = tf.reduce_mean(tf.square(input_tensor - mean), axis=-1, keepdims=True)\n        input_tensor = (input_tensor - mean) * tf.rsqrt(std + epsilon)\n        input_tensor = input_tensor * gamma + beta\n    return input_tensor\n\n\ndef dropout(input_tensor, dropout_prob):\n    """"""Perform dropout.\n\n    Args:\n      input_tensor: float Tensor.\n      dropout_prob: Python float. The probability of dropping out a value (NOT of\n        *keeping* a dimension as in `tf.nn.dropout`).\n\n    Returns:\n      A version of `input_tensor` with dropout applied.\n    """"""\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, rate=dropout_prob)\n    return output\n\n\ndef get_attention_mask(nd, ns, *, dtype):\n    """"""\n    this is a TPU compatible version of tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd)\n    where the lower right triangle contains 1s\n    """"""\n    i = tf.range(nd)[:, None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n    """"""Compute the union of the current variables and checkpoint variables.""""""\n    assignment_map = {}\n    initialized_variable_names = {}\n\n    name_to_variable = collections.OrderedDict()\n    for var in tvars:\n        name = var.name\n        m = re.match(""^(.*):\\\\d+$"", name)\n        if m is not None:\n            name = m.group(1)\n        name_to_variable[name] = var\n\n    init_vars = tf.train.list_variables(init_checkpoint)\n\n    assignment_map = collections.OrderedDict()\n    for x in init_vars:\n        (name, var) = (x[0], x[1])\n        if name not in name_to_variable:\n            continue\n        assignment_map[name] = name\n        initialized_variable_names[name] = 1\n        initialized_variable_names[name + "":0""] = 1\n    return (assignment_map, initialized_variable_names)\n\n\ndef construct_scalar_host_call(metric_dict, model_dir, prefix=""""):\n    """"""Construct a host call to log scalars when training on TPU.\n\n    Args:\n      metric_dict: A dict of the tensors to be logged.\n      model_dir: The location to write the summary.\n      prefix: The prefix (if any) to prepend to the metric names.\n\n    Returns:\n      A tuple of (function, args_to_be_passed_to_said_function)\n    """"""\n    metric_names = list(metric_dict.keys())\n\n    def host_call_fn(global_step, *args):\n        """"""Training host call. Creates scalar summaries for training metrics.\n\n        This function is executed on the CPU and should not directly reference\n        any Tensors in the rest of the `model_fn`. To pass Tensors from the\n        model to the `metric_fn`, provide as part of the `host_call`. See\n        https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n        for more information.\n\n        Arguments should match the list of `Tensor` objects passed as the second\n        element in the tuple passed to `host_call`.\n\n        Args:\n          global_step: `Tensor with shape `[batch]` for the global_step\n          *args: Remaining tensors to log.\n\n        Returns:\n          List of summary ops to run on the CPU host.\n        """"""\n        step = global_step[0]\n        with tf.contrib.summary.create_file_writer(\n                logdir=model_dir, filename_suffix="".host_call"").as_default():\n            with tf.contrib.summary.always_record_summaries():\n                for i, name in enumerate(metric_names):\n                    tf.contrib.summary.scalar(prefix + name, args[i][0], step=step)\n\n                return tf.contrib.summary.all_summary_ops()\n\n    # To log the current learning rate, and gradient norm for Tensorboard, the\n    # summary op needs to be run on the host CPU via host_call. host_call\n    # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n    # dimension. These Tensors are implicitly concatenated to\n    # [params[\'batch_size\']].\n    global_step_tensor = tf.reshape(\n        tf.compat.v1.train.get_or_create_global_step(), [1])\n    other_tensors = [tf.reshape(metric_dict[key], [1]) for key in metric_names]\n\n    return host_call_fn, [global_step_tensor] + other_tensors\n'"
