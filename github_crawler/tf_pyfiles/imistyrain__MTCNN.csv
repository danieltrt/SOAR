file_path,api_count,code
python-caffe/MtcnnDetector.py,0,"b'import os\nimport sys\nimport cv2\nimport platform\nimport math\nimport numpy as np\nif platform.system()==""Windows"":\n    caffe_root = \'D:/CNN/ssd\'\nelse:\n    caffe_root = os.path.expanduser(\'~\') + ""/CNN/ssd""\nsys.path.insert(0, caffe_root + \'/python\')\nimport caffe\n\nclass MTCNNDetector(object):\n    \'\'\'\n        Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks\n        see https://github.com/kpzhang93/MTCNN_face_detection_alignment\n        this is a caffe version\n    \'\'\'\n    def __init__(self,\n                 minsize = 20,\n                 threshold = [0.6, 0.7, 0.7],\n                 factor = 0.709,\n                 fastresize = False,\n                 model_dir=""../model/caffe"",\n                 gpuid = 0):\n        \n        self.minsize = minsize\n        self.threshold = threshold\n        self.factor = factor\n        self.fastresize = fastresize\n        \n        model_P = model_dir+\'/det1.prototxt\'\n        weights_P = model_dir+\'/det1.caffemodel\'\n        model_R = model_dir+\'/det2.prototxt\'\n        weights_R = model_dir+\'/det2.caffemodel\'\n        model_O = model_dir+\'/det3.prototxt\'\n        weights_O = model_dir+\'/det3.caffemodel\'\n        \n        caffe.set_mode_gpu()\n        caffe.set_device(gpuid)\n        \n        self.PNet = caffe.Net(model_P, weights_P, caffe.TEST) \n        self.RNet = caffe.Net(model_R, weights_R, caffe.TEST)\n        self.ONet = caffe.Net(model_O, weights_O, caffe.TEST)     \n        \n\n    def bbreg(self,boundingbox,reg):\n    \n        \'\'\'Calibrate bounding boxes\'\'\'\n        \n        if reg.shape[1]==1:\n            reg = np.shape(reg,(reg.shape[2],reg.shape[3])).T\n        w = boundingbox[:,2]-boundingbox[:,0]+1\n        h = boundingbox[:,3]-boundingbox[:,1]+1\n        boundingbox[:,0:4] = np.reshape(np.hstack((boundingbox[:,0]+reg[:,0]*w, boundingbox[:,1]+reg[:,1]*h, boundingbox[:,2]+reg[:,2]*w, boundingbox[:,3]+reg[:,3]*h)),(4,w.shape[0])).T\n    \n        return boundingbox\n    \n    def nms(self,dets, thresh,type=\'Union\'):\n        \n        if dets.shape[0]==0:\n            keep = []\n            return keep\n\n        x1 = dets[:, 0]\n        y1 = dets[:, 1]\n        x2 = dets[:, 2]\n        y2 = dets[:, 3]\n        scores = dets[:, 4]\n    \n        areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n        order = scores.argsort()[::-1]\n    \n        keep = []\n        while order.size > 0:\n            i = order[0]\n            keep.append(i)\n            xx1 = np.maximum(x1[i], x1[order[1:]])\n            yy1 = np.maximum(y1[i], y1[order[1:]])\n            xx2 = np.minimum(x2[i], x2[order[1:]])\n            yy2 = np.minimum(y2[i], y2[order[1:]])\n    \n            w = np.maximum(0.0, xx2 - xx1 + 1)\n            h = np.maximum(0.0, yy2 - yy1 + 1)\n            inter = w * h\n            if type==\'Min\':\n                ovr = inter / np.minimum(areas[i], areas[order[1:]])  \n            else:\n                ovr = inter / (areas[i] + areas[order[1:]] - inter)\n            inds = np.where(ovr <= thresh)[0]\n            order = order[inds + 1]\n    \n        return keep\n        \n    def rerec(self,bboxA):\n        \n        \'\'\'Convert bboxA to square\'\'\'\n        \n        h = bboxA[:,3]-bboxA[:,1]\n        w = bboxA[:,2]-bboxA[:,0]\n        l = np.concatenate((w,h)).reshape((2,h.shape[0]))\n        l = np.amax(l, axis=0) \n        bboxA[:,0] = bboxA[:,0] + w*0.5 -l*0.5\n        bboxA[:,1] = bboxA[:,1] + h*0.5 -l*0.5\n        bboxA[:,2] = bboxA[:,0] + l\n        bboxA[:,3] = bboxA[:,1] + l\n    \n        return bboxA\n    \n    def sort_rows_by_icol1(self,inarray):\n\n        idex=np.lexsort([inarray[:,0],inarray[:,1]])\n        a_sort=inarray[idex,:]\n        return a_sort\n    \n    \n    def generateBoundingBox(self,map,reg,scale,threshold):\n    \n        \'\'\'Use heatmap to generate bounding boxes\'\'\'\n        \n        stride=2;\n        cellsize=12;\n        boundingbox=[];\n        \n        map = map.T\n        dx1=reg[:,:,0].T\n        dy1=reg[:,:,1].T\n        dx2=reg[:,:,2].T\n        dy2=reg[:,:,3].T\n  \n        [y,x]=np.where(map>=threshold)\n        y = np.reshape(y,(len(y),1))\n        x = np.reshape(x,(len(y),1))\n        a = np.where(map.flatten(1)>=threshold)\n\n        if y.shape[0]==1:\n            y=y.T\n            x=x.T\n            score=np.reshape(map.flatten(1)[a[0]],(1,1))\n            dx1=dx1.T\n            dy1=dy1.T\n            dx2=dx2.T\n            dy2=dy2.T\n        else:\n\n            score=map.flatten(1)[a[0]]\n            score=np.reshape(score, (a[0].shape[0],1))\n            \n        dx1N=np.reshape(dx1.flatten(1)[a[0]], (a[0].shape[0],1))\n        dy1N=np.reshape(dy1.flatten(1)[a[0]], (a[0].shape[0],1))\n        dx2N=np.reshape(dx2.flatten(1)[a[0]], (a[0].shape[0],1))\n        dy2N=np.reshape(dy2.flatten(1)[a[0]], (a[0].shape[0],1))  \n        \n        reg=np.hstack((dx1N,dy1N,dx2N,dy2N))\n        \n        if  reg.shape[0]==0:\n            reg = np.zeros(shape=(0,3))\n        \n        boundingbox=np.hstack((y,x))\n        boundingbox = self.sort_rows_by_icol1(boundingbox)\n        boundingbox=np.hstack((((stride*boundingbox+1)/scale-1).astype(int),(((stride*boundingbox+cellsize-1+1)/scale-1)).astype(int),score,reg))\n\n        return boundingbox\n    \n    def pad(self,total_boxes,w,h):\n    \n        \'\'\'Compute the padding coordinates (pad the bounding boxes to square)\'\'\'\n        \n        tmpw=total_boxes[:,2]-total_boxes[:,0]+1\n        tmph=total_boxes[:,3]-total_boxes[:,1]+1\n        numbox=total_boxes.shape[0]\n        \n        dx = np.ones((numbox,))\n        dy = np.ones((numbox,))\n        \n        edx = tmpw    \n        edy = tmph\n            \n        x = total_boxes[:,0]\n        y = total_boxes[:,1]\n        ex = total_boxes[:,2]\n        ey = total_boxes[:,3]\n        \n        tmp = np.where(ex>w)\n        edx[tmp] = -ex[tmp] + w + tmpw[tmp]\n        ex[tmp] = w\n        \n        tmp = np.where(ey>h)\n        edy[tmp]= -ey[tmp] + h + tmph[tmp]\n        ey[tmp] = h\n        \n        tmp = np.where(x < 1)\n        dx[tmp] = 2-x[tmp]\n        x[tmp] = 1\t\n        \n        tmp = np.where(y < 1)\n        dy[tmp] = 2-y[tmp]\n        y[tmp] = 1\n        \n        return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n    \n        \n    def LoadNet(self,model,weights):\n        caffe.set_mode_gpu()\n        caffe.set_device(0)\n        Net = caffe.Net(model, weights, caffe.TEST)\n        return Net\n    \n    def detectface(self,img):\n\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n\n        if self.fastresize:\n            im_data=(np.float32(img)-127.5)*0.0078125\n        \n        \n        factor_count=0\n        total_boxes=[]\n        points=[]\n        h=img.shape[0]\n        w=img.shape[1]\n\n        minl=min(w,h)\n        m=12.0/self.minsize\n        minl=minl*m\n        # creat scale pyramid\n        scales=[]\n        while (minl>=12.0):\n            scales.append(m*(math.pow(self.factor,factor_count)))\n            minl=minl*self.factor\n            factor_count=factor_count+1\n\n        total_boxes = np.zeros(shape=(0,9))\n\n        for scale in scales:\n        \n            hs=int(math.ceil(h*scale))\n            ws=int(math.ceil(w*scale))\n            if self.fastresize:\n                im_data_out = cv2.resize(im_data,(ws, hs),interpolation=cv2.INTER_NEAREST)\n\n            else:\n                im_data_out = (cv2.resize(img,(ws, hs),interpolation=cv2.INTER_NEAREST) - 127.5)*0.0078125\n            im_data_out = im_data_out[None,:] \n            im_data_out = im_data_out.transpose((0,3,2,1)) \n            self.PNet.blobs[\'data\'].reshape(1,3,ws,hs)\n            out = self.PNet.forward_all( data = im_data_out )\n            \n            \n            map = out[\'prob1\'][0].transpose((2,1,0))[:,:,1]\n            reg = out[\'conv4-2\'][0].transpose((2,1,0))\n            boxes = self.generateBoundingBox(map,reg,scale,self.threshold[0])\n            \n            pick = self.nms(boxes, 0.5)\n            boxes = boxes[pick,:]\n            if boxes.shape[0]!=0:\n                total_boxes = np.concatenate((total_boxes,boxes),axis=0)\n        \n\n        if total_boxes is not None:\n            pick = self.nms(total_boxes, 0.7)\n            total_boxes = total_boxes[pick,:]\n            regw=total_boxes[:,2]-total_boxes[:,0];\n            regh=total_boxes[:,3]-total_boxes[:,1];\n            total_boxes = np.concatenate((total_boxes[:,0]+total_boxes[:,5]*regw, total_boxes[:,1]+total_boxes[:,6]*regh, total_boxes[:,2]+total_boxes[:,7]*regw, total_boxes[:,3]+total_boxes[:,8]*regh, total_boxes[:,4])).reshape((5,regw.shape[0]))   \n            total_boxes = total_boxes.T\n            total_boxes=self.rerec(total_boxes)\n            total_boxes[:,0:4]=total_boxes[:,0:4].astype(int)\n            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = self.pad(total_boxes,w,h)\n            \n        numbox = total_boxes.shape[0]\n\n        \n        if  numbox > 0:    \n            #second stage\n            tempimg =  np.zeros((24,24,3,numbox))\n            for k in range(numbox):\n                tmp =  np.zeros(((int)(tmph[k]),(int)(tmpw[k]),3))\n                tmp[(int)(dy[k])-1:(int)(edy[k]),(int)(dx[k])-1:(int)(edx[k]),:]=img[(int)(y[k])-1:(int)(ey[k]),(int)(x[k])-1:(int)(ex[k]),:] \n                tempimg[:,:,:,k]= cv2.resize(tmp,(24, 24),interpolation=cv2.INTER_NEAREST)\n            tempimg = (tempimg-127.5)*0.0078125\n            tempimg = tempimg.transpose((3,2,1,0)) \n            self.RNet.blobs[\'data\'].reshape(numbox,3,24,24)\n            out = self.RNet.forward_all( data = tempimg )        \n\n            score=out[\'prob1\'][:,1]   ###why need to squeeze?\n            pas = np.where(score>self.threshold[1])            \n            total_boxes = np.hstack((total_boxes[pas[0],0:4], np.reshape(score[pas[0]],(len(pas[0]),1))))\n            mv = out[\'conv5-2\'][pas[0],:]\n\n            if total_boxes is not None:\n                pick = self.nms(total_boxes, 0.7)\n                total_boxes = total_boxes[pick,:]  \n                total_boxes=self.bbreg(total_boxes, mv[pick,:])\n                total_boxes=self.rerec(total_boxes)\n                \n            numbox = total_boxes.shape[0]\n        \n            if  numbox > 0: \n                # third stage\n                total_boxes = total_boxes.astype(int)\n                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = self.pad(total_boxes,w,h)\n                tempimg =  np.zeros((48,48,3,numbox))\n                for k in range(numbox):\n                    tmp =  np.zeros((tmph[k],tmpw[k],3))\n                    tmp[(int)(dy[k])-1:(int)(edy[k]),(int)(dx[k])-1:(int)(edx[k]),:]=img[y[k]-1:ey[k],x[k]-1:ex[k],:] \n                    tempimg[:,:,:,k]= cv2.resize(tmp,(48, 48),interpolation=cv2.INTER_NEAREST)    \n                tempimg = (tempimg-127.5)*0.0078125 \n                tempimg = tempimg.transpose((3,2,1,0)) \n                self.ONet.blobs[\'data\'].reshape(numbox,3,48,48)\n                out = self.ONet.forward_all( data = tempimg ) \n        \n                score = out[\'prob1\'][:,1]\n                points = out[\'conv6-3\']\n                pas = np.where(score>self.threshold[2])\n                points = points[pas[0],:].T\n                total_boxes = np.hstack((total_boxes[pas[0],0:4], np.reshape(score[pas[0]],(len(pas[0]),1))))\n                mv = out[\'conv6-2\'][pas[0],:]\n                w=total_boxes[:,2]-total_boxes[:,0]+1\n                h=total_boxes[:,3]-total_boxes[:,1]+1\n                points[0:5,:] = np.tile(np.reshape(w,(1,w.shape[0])),[5,1])*points[0:5,:]+np.tile(np.reshape(total_boxes[:,0],(1,total_boxes.shape[0])),[5,1])-1\n                points[5:10,:] = np.tile(np.reshape(h,(1,h.shape[0])),[5,1])*points[5:10,:]+np.tile(np.reshape(total_boxes[:,1],(1,total_boxes.shape[0])),[5,1])-1\n                if total_boxes is not None:\n                    total_boxes=self.bbreg(total_boxes, mv[:,:])\n                    pick = self.nms(total_boxes, 0.7, \'Min\')\n                    total_boxes = total_boxes[pick,:]\n                    points = points[:,pick]\n            numbox = total_boxes.shape[0]       \n        return total_boxes,points,numbox\n    \n\n'"
python-caffe/demo.py,0,"b'import cv2,os\nimport numpy as np\nfrom MtcnnDetector import MTCNNDetector\n\ndef test_dir(dir=""../imgs""):\n    files=os.listdir(dir)\n    for file in files:\n        imgpath=dir+""/""+file\n        img = cv2.imread(imgpath)\n        detector = FaceDetector(minsize = 20,fastresize = False) \n        total_boxes,points,numbox = detector.detectface(img)\n        for i in range(numbox):\n            cv2.rectangle(img,(int(total_boxes[i][0]),int(total_boxes[i][1])),(int(total_boxes[i][2]),int(total_boxes[i][3])),(0,255,0),2)        \n            for j in range(5):        \n                cv2.circle(img,(int(points[j,i]),int(points[j+5,i])),2,(0,0,255),2)\n        cv2.imshow( \'img\',img )\n        cv2.waitKey()\n\ndef test_camera(index=0):\n    cap=cv2.VideoCapture(index)\n    detector = MTCNNDetector(minsize = 20, fastresize = False) \n    while True:\n        ret,img=cap.read()\n        if not ret:\n            break\n        total_boxes,points,numbox = detector.detectface(img)\n        for i in range(numbox):\n            cv2.rectangle(img,(int(total_boxes[i][0]),int(total_boxes[i][1])),(int(total_boxes[i][2]),int(total_boxes[i][3])),(0,255,0),2)        \n            for j in range(5):        \n                cv2.circle(img,(int(points[j,i]),int(points[j+5,i])),2,(0,0,255),2)\n        cv2.imshow( \'img\',img )\n        cv2.waitKey(1)\n\nif __name__ == \'__main__\':\n    #test_dir()\n    test_camera()'"
python-caffe/pose.py,0,"b'#coding=utf-8\nimport cv2,os,sys\nimport numpy as np\nfrom MtcnnDetector import MTCNNDetector\nimport math\n\ncaffe_root = os.path.expanduser(\'~\') + ""/CNN/ssd""\nsys.path.insert(0, caffe_root+\'/python\')\nimport caffe\ninput_width = 64\ninput_height = 64\n\ndef preprocess(im, bbox):\n    pad = cal_padding(bbox, im)\n    im_pad = cv2.copyMakeBorder(im, pad, pad, pad, pad, borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0])\n    bbox = bbox + pad\n    bb_w = bbox[2] - bbox[0]\n    scale = bb_w * 1.0 / input_width\n    h, w, c = im_pad.shape\n    bbox = bbox / scale\n    bbox[0] = round(bbox[0])\n    bbox[1] = round(bbox[1])\n    bbox[2:] = bbox[0:2] + [input_width - 1, input_height - 1]\n    bbox = bbox.astype(np.int32)\n    im_pad = cv2.resize(im_pad, (int(w / scale), int(h / scale)))\n    cropImg = im_pad[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n    return cropImg,pad,bbox[0:2]+1,scale\n\ndef cal_padding(bbox, im):\n    x1,y1,x2,y2 = bbox\n    h,w,c = im.shape\n    pad = np.max([-x1, -y1, x2 - w, y2 - h, 0]) + 10\n    return int(pad)\n\ndef pad_bbox(bbox, pad_ratio):\n    # padding\n    pad_w = (bbox[2] - bbox[0]) * pad_ratio\n    pad_h = (bbox[3] - bbox[1]) * pad_ratio\n    bbox = np.array([bbox[0] - pad_w, bbox[1], bbox[2] + pad_w, bbox[3] + 2 * pad_h])\n    return np.array(bbox)\n\ndef obtain_bbox(bbox, w_h_ratio):\n\n    bbox = np.array(bbox).astype(np.float32)\n\n    w, h = bbox[2:] - bbox[0:2] + 1\n    if w*1.0/h >= w_h_ratio:\n        pad_h = (w/w_h_ratio -h)/2\n        pad_w = 0\n    elif w/h < w_h_ratio:\n        pad_h = 0\n        pad_w = (h*w_h_ratio -w)/2\n    bbox = bbox[0] - pad_w, bbox[1] - pad_h, bbox[2] + pad_w, bbox[3] + pad_h\n    return np.array(bbox)\n\ndef rotMatrixToEulerAngle(rotMat):\n    theta = cv2.norm(rotMat,cv2.NORM_L2)\n    w = np.cos(theta/2);\n    x = np.sin(theta/2) * rotMat[0] / theta\n    y = np.sin(theta/2) * rotMat[1] / theta\n    z = np.sin(theta/2) * rotMat[2] / theta\n\n    ysqr = y * y\n    # pitch (x-axis rotation)\n    t0 = 2.0 * (w * x * y * z)\n    t1 = 1.0 - 2.0 *(x * x + ysqr)\n    pitch = math.atan2(t0, t1)\n\n    # yaw (y-axis rotation)\n    t2 = 2.0 * (w * y - z * x)\n    if t2 > 1.0:\n        t2 = 1.0\n    elif t2 < -1.0:\n        t2 = -1.0\n    yaw = math.asin(t2)\n\n    # roll (z-axis rotation)\n    t3 = 2.0 * (w * z + x * y)\n    t4 = 1.0 - 2.0 * (ysqr + z * z)\n    roll = math.atan2(t3, t4)\n    return roll, yaw, pitch\n\nclass PoseEstimator:\n    def __init__(self,model_dir=""model""):\n        model_def = model_dir+\'/deploy.prototxt\'\n        model_weights= model_dir+\'/mobilenet-v2.caffemodel\'\n        self.net=caffe.Net(model_def,model_weights, caffe.TEST)\n        self.transformer = caffe.io.Transformer({\'data\': self.net.blobs[\'data\'].data.shape})\n        self.transformer.set_transpose(\'data\', (2, 0, 1))  # h,w,c-> c,h,w\n        self.detector = MTCNNDetector(minsize = 20,fastresize = False)\n\n    def predict(self,img):\n        total_boxes,points,numbox = self.detector.detectface(img)\n        if numbox==0:\n            return [],[]\n        bbox=total_boxes[0][0:4]\n        pt5=points[0]        \n        bbox = np.array(bbox, dtype=np.float32)\n        bbox = obtain_bbox(bbox, input_width*1.0/input_height)\n        bbox = pad_bbox(bbox, pad_ratio=0.05)\n        cropImg, pad, offset, scale = preprocess(img, bbox)\n        cropImg = (cropImg - 127.5)/128\n        cropImg = self.transformer.preprocess(\'data\', cropImg)\n        self.net.blobs[\'data\'].data[...] = cropImg\n        out = self.net.forward()[\'fc4\']\n        landmarks = out.reshape([2, 68])\n        landmarks = np.transpose(landmarks)\n        landmarks[:, 0] = landmarks[:, 0] * input_width - 1\n        landmarks[:, 1] = landmarks[:, 1] * input_height - 1\n        focal_length = input_width\n        center = [input_width/2, input_height/2]\n        camera_matrix = np.zeros([3,3], dtype=np.double)\n        camera_matrix[0,:] = [focal_length, 0, center[0]]\n        camera_matrix[1,:] = [ 0, focal_length, center[1]]\n        camera_matrix[2,:] = [0, 0, 1]\n        dist_coeffs = np.zeros([5,1], np.double)\n        objectPoints = np.zeros([6,3,1], dtype=np.double)\n        objectPoints[0,:,0] = [0,0,0]\n        objectPoints[1,:,0] = [0,-330,-65]\n        objectPoints[2,:,0] = [-225,170,-135]\n        objectPoints[3,:,0] = [225,170,-135]\n        objectPoints[4,:,0] = [-150,-150,-125]\n        objectPoints[5,:,0] = [150,-150,-125]\n        imagePoints = np.zeros([6,2])\n        imagePoints = landmarks[[30, 8, 36, 45, 48, 54],:]\n        ret, rotVects, transVects = cv2.solvePnP(objectPoints, imagePoints, camera_matrix, dist_coeffs)\n        roll, yaw, pitch = rotMatrixToEulerAngle(rotVects)\n        for j in range((int)(len(landmarks)/2)):\n            landmarks[j]=(landmarks[j]+ offset ) * scale - pad - 1\n        print(\'roll:%f,yaw:%f,pitch:%f\' %(roll,yaw,pitch))\n        return bbox,landmarks\n\n    def drawPose(self,img,bbox,landmarks):\n        h,w,c,=img.shape\n        if len(bbox)>0:\n            cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0,255,0))\n        for j in range((int)(len(landmarks)/2)):\n            cv2.circle(img,((int)(landmarks[j][0]),(int)(landmarks[j][1])),2,(0,0,255),2)\n\n\ndef test_dir(dir=""../imgs""):\n    ps=PoseEstimator()\n    files=os.listdir(dir)\n    for file in files:\n        imgpath=dir+""/""+file\n        img = cv2.imread(imgpath)\n        bbox,landmarks=ps.predict(img)\n        ps.drawPose(img,bbox,landmarks)\n        cv2.imshow(\'img\',img)\n        cv2.waitKey()\n\ndef test_camera():\n    cap=cv2.VideoCapture(0)\n    ps=PoseEstimator()\n    while True:\n        ret,img=cap.read()\n        if not ret:\n            break\n        bbox,landmarks=ps.predict(img)\n        ps.drawPose(img,bbox,landmarks)\n        cv2.imshow( \'img\',img )\n        cv2.waitKey(1)\n\nif __name__ == \'__main__\':   \n    #test_dir()\n    test_camera()'"
tensorflow/caffe2tf.py,83,"b'from __future__ import print_function\nimport os,sys,argparse,platform\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nif platform.system()==""Windows"":\n    caffe_root = \'D:/CNN/caffe\'\nelse:\n    caffe_root = \'/home/yanyu/CNN/caffe/\'\nsys.path.insert(0, caffe_root + \'/python\')\nimport caffe\n\n@slim.add_arg_scope\ndef prelu(inputs, data_format=\'NHWC\', scope=None):\n\n    with tf.variable_scope(scope, default_name=\'prelu\'):\n\n        channel_dim = 1 if data_format == \'NCHW\' else 3\n        inputs_shape = inputs.get_shape().as_list()\n        alpha_shape = [1 for i in range(len(inputs_shape))]\n        alpha_shape[channel_dim] = inputs_shape[channel_dim]\n        alpha = slim.model_variable(\n            \'weights\', alpha_shape,\n            initializer=tf.constant_initializer(0.25))\n\n        outputs = tf.where(inputs > 0, inputs, inputs * alpha)\n\n        return outputs\n\ndef fc2conv(var, v, *args):\n    w, h, in_, out_ = var.shape.as_list()\n    assert out_ == v.shape[0]\n    v = v.reshape(out_, in_, w, h)\n    # transpose wh to hw\n    v = v.transpose(3, 2, 1, 0)\n    \n    # make the reg format from [x1, y1, x2, y2] to [y1, x1, y2, x2]\n    if out_ == 4:\n        v_ = [v[..., 1], v[..., 0], v[..., 3], v[..., 2]]\n        v = np.stack(v_, axis=3)\n        print(var.name, v.shape)\n    \n    # make the landmark output format from\n    # [x1, x2, x3, x4, x5, y1, y2, y3, y4, y5] to\n    # [y1, y2, y3, y4, y5, x1, x2, x3, x4, x5]\n    if \'onet/conv6-3\' in var.name:\n        new_v = [v[..., 5:], v[..., :5]]\n        new_v = np.concatenate(new_v, axis=3)\n        print(var.name, v.shape)\n        return new_v\n    return v\n\ndef conv_t(var, v, *args):\n    w, h, in_, out_ = var.shape.as_list()\n    # transpose wh to hw\n    v = v.transpose(3, 2, 1, 0)\n\n    # change input image from rgb to bgr\n    if \'conv1\' in var.name:\n        v = v[:, :, ::-1, :]\n    \n    # make the reg format from [x1, y1, x2, y2] to [y1, x1, y2, x2]\n    if out_ == 4:\n        v_ = [v[..., 1], v[..., 0], v[..., 3], v[..., 2]]\n        v = np.stack(v_, axis=3)\n        print(var.name, v.shape)\n    return v\n\ndef conv_b_t(var, v, *args):\n    out_ = var.shape.as_list()[0]\n\n    # make the reg format from [x1, y1, x2, y2] to [y1, x1, y2, x2]\n    if out_ == 4:\n        v_ = [v[1], v[0], v[3], v[2]]\n        v = np.asarray(v_)\n        print(var.name, v.shape)\n    \n    # make the landmark output format from\n    # [x1, x2, x3, x4, x5, y1, y2, y3, y4, y5] to\n    # [y1, y2, y3, y4, y5, x1, x2, x3, x4, x5]\n    if \'onet/conv6-3\' in var.name:\n        new_v = [v[5:], v[:5]]\n        new_v = np.concatenate(new_v, axis=0)\n        print(var.name, v.shape)\n        v = new_v\n    return v\n\n\ndef det1(images, data_format):\n    net = slim.conv2d(images, 10, 3, stride=1, padding=\'VALID\', scope=\'conv1\')\n    net = prelu(net, scope=\'PReLU1\')\n    net = slim.max_pool2d(net, 2, stride=2, padding=\'VALID\', scope=\'pool1\')\n\n    net = slim.conv2d(net, 16, 3, stride=1, padding=\'VALID\', scope=\'conv2\')\n    net = prelu(net, scope=\'PReLU2\')\n\n    net = slim.conv2d(net, 32, 3, stride=1, padding=\'VALID\', scope=\'conv3\')\n    net = prelu(net, scope=\'PReLU3\')\n\n    prob = slim.conv2d(net, 2, 1, stride=1, padding=\'VALID\', scope=\'conv4-1\')\n    prob = tf.nn.softmax(prob, axis=3 if data_format == \'NHWC\' else 1, name=\'prob1\')\n\n    regress = slim.conv2d(net, 4, 1, stride=1, padding=\'VALID\', scope=\'conv4-2\')\n    return prob, regress\n\n\ndef det2(images):\n    net = slim.conv2d(images, 28, 3, stride=1, padding=\'VALID\', scope=\'conv1\')\n    net = prelu(net, scope=\'prelu1\')\n    net = slim.max_pool2d(net, 3, stride=2, padding=\'SAME\', scope=\'pool1\')\n\n    net = slim.conv2d(net, 48, 3, stride=1, padding=\'VALID\', scope=\'conv2\')\n    net = prelu(net, scope=\'prelu2\')\n    net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\', scope=\'pool2\')\n\n    net = slim.conv2d(net, 64, 2, stride=1, padding=\'VALID\', scope=\'conv3\')\n    net = prelu(net, scope=\'prelu3\')\n\n    net = slim.conv2d(net, 128, 3, stride=1, padding=\'VALID\', scope=\'conv4\')\n    net = prelu(net, scope=\'prelu4\')\n\n    prob = slim.conv2d(net, 2, 1, stride=1, padding=\'VALID\', scope=\'conv5-1\')\n    prob = slim.flatten(prob)\n    prob = tf.nn.softmax(prob, axis=1, name=\'prob1\')\n\n    regress = slim.conv2d(net, 4, 1, stride=1, padding=\'VALID\', scope=\'conv5-2\')\n    regress = slim.flatten(regress)\n    return prob, regress\n\n\ndef det3(images):\n    net = slim.conv2d(images, 32, 3, stride=1, padding=\'VALID\', scope=\'conv1\')\n    net = prelu(net, scope=\'prelu1\')\n    net = slim.max_pool2d(net, 3, stride=2, padding=\'SAME\', scope=\'pool1\')\n\n    net = slim.conv2d(net, 64, 3, stride=1, padding=\'VALID\', scope=\'conv2\')\n    net = prelu(net, scope=\'prelu2\')\n    net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\', scope=\'pool2\')\n\n    net = slim.conv2d(net, 64, 3, stride=1, padding=\'VALID\', scope=\'conv3\')\n    net = prelu(net, scope=\'prelu3\')\n    net = slim.max_pool2d(net, 2, stride=2, padding=\'VALID\', scope=\'pool3\')\n\n    net = slim.conv2d(net, 128, 2, stride=1, padding=\'VALID\', scope=\'conv4\')\n    net = prelu(net, scope=\'prelu4\')\n\n    net = slim.conv2d(net, 256, 3, stride=1, padding=\'VALID\', scope=\'conv5\')\n    net = prelu(net, scope=\'prelu5\')\n\n    prob = slim.conv2d(net, 2, 1, stride=1, padding=\'VALID\', scope=\'conv6-1\')\n    prob = slim.flatten(prob)\n    prob = tf.nn.softmax(prob, axis=1, name=\'prob1\')\n\n    regress = slim.conv2d(net, 4, 1, stride=1, padding=\'VALID\', scope=\'conv6-2\')\n    regress = slim.flatten(regress)\n\n    landmark = slim.conv2d(net, 10, 1, stride=1, padding=\'VALID\', scope=\'conv6-3\')\n    landmark = slim.flatten(landmark)\n    return prob, regress, landmark\n\n\ndef common_arg(data_format):\n    conv_scope = slim.arg_scope(\n        [slim.conv2d],\n        activation_fn=None)\n    fc_scope = slim.arg_scope(\n        [slim.fully_connected],\n        activation_fn=None)\n    data_format_scope = slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, prelu],\n        data_format=data_format)\n    with conv_scope, fc_scope, data_format_scope as scope:\n        return scope\n\n\ndef assign_from_caffe(proto, caffemodel, scope):\n    caffe.set_mode_cpu()\n    cnet = caffe.Net(proto, caffemodel, caffe.TEST)\n\n    assign_ops = []\n    for name, layer in cnet.layer_dict.items():\n        print(\'layer:\', name)\n        param_names = [\'weights\', \'biases\']\n        n_param = len(layer.blobs)\n        param_names = param_names[:n_param]\n\n        transform = [None for i in range(n_param)]\n        if layer.type == \'Convolution\':\n            transform = [(conv_t, None), (conv_b_t, None)]\n        if layer.type == \'InnerProduct\':\n            transform = [(fc2conv, None), (conv_b_t, None)]\n        elif layer.type == \'PReLU\':\n            transform = [(lambda var, v, args: v.reshape(var.get_shape().as_list()), None)]\n\n        assert len(param_names) == len(transform)\n\n        for i, (p, t) in enumerate(zip(param_names, transform)):\n            var, = slim.get_model_variables(scope + \'/\' + name, p)\n            print(var.name)\n            v = layer.blobs[i].data.copy()\n            if t:\n                v = t[0](var, v, t[1])\n            assign_ops.append(tf.assign(var, v))\n\n    assert len(slim.get_model_variables(scope)) == len(assign_ops)\n\n    return assign_ops\n\ndef regress_box(bbox, reg):\n    hw = bbox[:, 2:] - bbox[:, :2]\n    hw = tf.concat([hw, hw], axis=1)\n    bbox = bbox + hw * reg\n    return bbox\n\ndef square_box(bbox):\n    hw = bbox[:, 2:] - bbox[:, :2] + 1\n    max_side = tf.reduce_max(hw, axis=1, keepdims=True)\n    delta = tf.concat([(hw - max_side) * 0.5, (hw - max_side) * -0.5], axis=1)\n    bbox = bbox + delta\n    return bbox\n\ndef stage_one(images, min_size, factor, thresold, scope):\n    img_shape = tf.shape(images)\n    width, height = tf.to_float(img_shape[2]), tf.to_float(img_shape[1])\n    min_side = tf.to_float(tf.minimum(width, height))\n\n    with tf.device(\'/cpu:0\'):\n        prob_arr = tf.TensorArray(\n            tf.float32, size=0, clear_after_read=True,\n            dynamic_size=True, element_shape=[None], infer_shape=False)\n        reg_arr = tf.TensorArray(\n            tf.float32, size=0, clear_after_read=True,\n            dynamic_size=True, element_shape=[None, 4], infer_shape=False)\n        box_arr = tf.TensorArray(\n            tf.float32, size=0, clear_after_read=True,\n            dynamic_size=True, element_shape=[None, 4], infer_shape=False)\n\n    stride = 2\n    cell_size = 12\n\n    def body(i, scale, prob_arr, reg_arr, box_arr):\n\n        width_scaled = tf.to_int32(width * scale)\n        height_scaled = tf.to_int32(height * scale)\n        img = tf.image.resize_bilinear(images, [height_scaled, width_scaled])\n        prob, reg = det1(img, \'NHWC\')\n\n        with tf.device(\'/cpu:0\'):\n            prob, reg = prob[0], reg[0]\n\n            scope.reuse_variables()\n            mask = prob[:, :, 1] > thresold\n            indexes = tf.where(mask)\n\n            bbox = [\n                tf.to_float(indexes*stride + 1)/scale,\n                tf.to_float(indexes*stride + cell_size)/scale]\n            bbox = tf.concat(bbox, axis=1)\n            prob = tf.boolean_mask(prob[:, :, 1], mask)\n            reg = tf.boolean_mask(reg, mask)\n\n            idx = tf.image.non_max_suppression(bbox, prob, 1000, 0.5)\n\n            bbox = tf.gather(bbox, idx)\n            prob = tf.gather(prob, idx)\n            reg = tf.gather(reg, idx)\n\n            prob_arr = prob_arr.write(i, prob)\n            reg_arr = reg_arr.write(i, reg)\n            box_arr = box_arr.write(i, bbox)\n        return i+1, scale * factor, prob_arr, reg_arr, box_arr\n\n    _, _, prob_arr, reg_arr, box_arr = tf.while_loop(\n        lambda i, scale, prob_arr, reg_arr, box_arr: min_side * scale > 12.,\n        body,\n        [0, 12. / min_size, prob_arr, reg_arr, box_arr],\n        back_prop=False)\n\n    prob, reg, bbox = prob_arr.concat(), reg_arr.concat(), box_arr.concat()\n\n    idx = tf.image.non_max_suppression(bbox, prob, 1000, 0.7)\n    bbox, prob, reg = tf.gather(bbox, idx), tf.gather(prob, idx), tf.gather(reg, idx)\n\n    bbox = regress_box(bbox, reg)\n    bbox = square_box(bbox)\n    return bbox, prob\n\ndef stage_two(images, bbox, prob, threshold, scope):\n    img_shape = tf.shape(images)\n    width, height = tf.to_float(img_shape[2]), tf.to_float(img_shape[1])\n    bbox_norm = bbox / [height, width, height, width]\n    img_batch = tf.image.crop_and_resize(images, bbox_norm, tf.tile([0], tf.shape(prob)), [24, 24])\n    prob, reg = det2(img_batch)\n\n    with tf.device(\'/cpu:0\'):\n        mask = prob[:, 1] > threshold\n        prob, reg, bbox = (\n            tf.boolean_mask(prob[:, 1], mask),\n            tf.boolean_mask(reg, mask),\n            tf.boolean_mask(bbox, mask))\n        \n        idx = tf.image.non_max_suppression(bbox, prob, 1000, 0.7)\n        bbox, prob, reg = tf.gather(bbox, idx), tf.gather(prob, idx), tf.gather(reg, idx)\n\n        bbox = regress_box(bbox, reg)\n        bbox = square_box(bbox)\n    return bbox, prob\n\ndef stage_three(images, bbox, prob, threshold, scope):\n    img_shape = tf.shape(images)\n    width, height = tf.to_float(img_shape[2]), tf.to_float(img_shape[1])\n\n    bbox_norm = bbox / [height, width, height, width]\n    img_batch = tf.image.crop_and_resize(images, bbox_norm, tf.tile([0], tf.shape(prob)), [48, 48])\n\n    prob, reg, landmarks = det3(img_batch)\n\n    with tf.device(\'/cpu:0\'):\n        mask = prob[:, 1] > threshold\n        prob, reg, bbox, landmarks = (\n            tf.boolean_mask(prob[:, 1], mask),\n            tf.boolean_mask(reg, mask),\n            tf.boolean_mask(bbox, mask),\n            tf.boolean_mask(landmarks, mask))\n        \n        hw = bbox[:, 2:] - bbox[:, :2]\n        hw = tf.reshape(tf.tile(tf.expand_dims(hw, 2), [1, 1, 5]), [-1, 10])\n        top_left = tf.reshape(tf.tile(tf.expand_dims(bbox[:, :2], 2), [1, 1, 5]), [-1, 10])\n        landmarks = top_left + hw * landmarks\n\n        bbox = regress_box(bbox, reg)\n        idx = tf.image.non_max_suppression(bbox, prob, 1000, 0.6)\n        bbox, prob, reg, landmarks = (\n            tf.gather(bbox, idx), tf.gather(prob, idx),\n            tf.gather(reg, idx), tf.gather(landmarks, idx))\n    return bbox, prob, landmarks\n\ndef main(args):\n    data_format = \'NHWC\'\n    orig_graph = tf.Graph()\n    with orig_graph.as_default(), slim.arg_scope(common_arg(data_format)):\n\n        assign_ops = []\n        input_ops = []\n        output_ops = []\n\n        print(\'-\'*80)\n        print(\'parse det1...\')\n\n        if data_format == \'NHWC\':\n            img_shape = [None, None, 3]\n        else:\n            img_shape = [3, None, None]\n\n        images = tf.placeholder(\n            tf.float32, shape=img_shape, name=\'input\')\n        images = tf.expand_dims(images, 0)\n        images = (images - 127.5) / 128.\n\n        min_size = tf.placeholder(tf.float32, shape=[], name=\'min_size\')\n        thresholds = tf.placeholder(tf.float32, shape=[3], name=\'thresholds\')\n        factor = tf.placeholder(tf.float32, shape=[], name=\'factor\')\n\n        input_ops.extend([images, min_size, thresholds, factor])\n\n        # det1\n        with tf.variable_scope(\'pnet\') as scope:\n            bbox, prob = stage_one(images, min_size, factor, thresholds[0], scope)\n            assign_ops.extend(\n                assign_from_caffe(\n                    os.path.join(args.model_dir, \'det1.prototxt\'),\n                    os.path.join(args.model_dir, \'det1.caffemodel\'),\n                    scope.name)\n            )\n\n        print(\'-\'*80)\n        print(\'parse det2...\')\n        # det2\n        with tf.variable_scope(\'rnet\') as scope:\n            bbox, prob = tf.cond(\n                tf.shape(bbox)[0] > 0,\n                lambda : stage_two(images, bbox, prob, thresholds[1], scope),\n                lambda : (tf.constant(np.zeros([0, 4], dtype=\'float32\')),\n                          tf.constant(np.zeros([0], dtype=\'float32\')))\n            )\n            assign_ops.extend(\n                assign_from_caffe(\n                    os.path.join(args.model_dir, \'det2.prototxt\'),\n                    os.path.join(args.model_dir, \'det2.caffemodel\'),\n                    scope.name)\n            )\n\n        print(\'-\'*80)\n        print(\'parse det3...\')\n        with tf.variable_scope(\'onet\') as scope:\n            bbox, prob, landmarks = tf.cond(\n                tf.shape(bbox)[0] > 0,\n                lambda : stage_three(images, bbox, prob, thresholds[2], scope),\n                lambda : (tf.constant(np.zeros([0, 4], dtype=\'float32\')),\n                          tf.constant(np.zeros([0], dtype=\'float32\')),\n                          tf.constant(np.zeros([0, 10], dtype=\'float32\')))\n            )\n            assign_ops.extend(\n                assign_from_caffe(\n                    os.path.join(args.model_dir, \'det3.prototxt\'),\n                    os.path.join(args.model_dir, \'det3.caffemodel\'),\n                    scope.name)\n            )\n\n        output_ops.extend([tf.identity(prob, \'prob\'), tf.identity(landmarks, \'landmarks\'), tf.identity(bbox, \'box\')])\n        init_op = tf.global_variables_initializer()\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(graph=orig_graph, config=config)\n\n    sess.run(init_op)\n    sess.run(assign_ops)\n\n    orig_graph_def = orig_graph.as_graph_def()\n    dst_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess, orig_graph_def, [t.op.name for t in output_ops])\n\n    with open(args.dst, \'wb\') as f:\n        f.write(dst_graph_def.SerializeToString())\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\'parse mtcnn caffe model to tensorflow\')\n    parser.add_argument(\'--model_dir\',default=""../model/caffe"", help=\'directory contain mtcnn caffe model.\')\n    parser.add_argument(\'--dst\', default=""mtcnn.pb"",help=\'the dst tensorflow model\')\n    args = parser.parse_args()\n    main(args)\n'"
tensorflow/demo.py,0,"b""import argparse,cv2\nfrom mtcnn import MTCNN\n\ndef test_image(imgpath):\n    mtcnn = MTCNN('./mtcnn.pb')\n    img = cv2.imread(imgpath)\n    show=mtcnn.detectAndDraw(img)\n    cv2.imshow('img', show)\n    cv2.waitKey()\n\ndef test_camera(index=0):\n    mtcnn = MTCNN('./mtcnn.pb')\n    cap=cv2.VideoCapture(index)\n    while True:\n        ret,img=cap.read()\n        if not ret:\n            break\n        show=mtcnn.detectAndDraw(img)\n        cv2.imshow('img', show)\n        cv2.waitKey(1)\n\nif __name__ == '__main__':\n    #test_image()\n    test_camera()"""
tensorflow/mtcnn.py,5,"b""import tensorflow as tf\nimport cv2\n\nclass MTCNN:\n    def __init__(self, model_path, min_size=40, factor=0.709, thresholds=[0.6, 0.7, 0.7]):\n        self.min_size = min_size\n        self.factor = factor\n        self.thresholds = thresholds\n\n        graph = tf.Graph()\n        with graph.as_default():\n            with open(model_path, 'rb') as f:\n                graph_def = tf.GraphDef.FromString(f.read())\n                tf.import_graph_def(graph_def, name='')\n        self.graph = graph\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            intra_op_parallelism_threads=4,\n            inter_op_parallelism_threads=4)\n        config.gpu_options.allow_growth = True\n        self.sess = tf.Session(graph=graph, config=config)\n\n    def detect(self, img):\n        feeds = {\n            self.graph.get_operation_by_name('input').outputs[0]: img,\n            self.graph.get_operation_by_name('min_size').outputs[0]: self.min_size,\n            self.graph.get_operation_by_name('thresholds').outputs[0]: self.thresholds,\n            self.graph.get_operation_by_name('factor').outputs[0]: self.factor\n        }\n        fetches = [self.graph.get_operation_by_name('prob').outputs[0],\n                  self.graph.get_operation_by_name('landmarks').outputs[0],\n                  self.graph.get_operation_by_name('box').outputs[0]]\n        prob, landmarks, box = self.sess.run(fetches, feeds)\n        return box, prob, landmarks\n\n    def detectAndDraw(self,img):\n        bbox, scores, landmarks = self.detect(img)\n        print('total box:', len(bbox))\n        for box, pts in zip(bbox, landmarks):\n            box = box.astype('int32')\n            img = cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (255, 0, 0), 3)\n            pts = pts.astype('int32')\n            for i in range(5):\n                img = cv2.circle(img, (pts[i+5], pts[i]), 1, (0, 255, 0), 2)\n        return img"""
tensorflow/mtcnn_data.py,14,"b""import argparse\n\nimport tensorflow as tf\n\ndef preprocess(path):\n    # read file from disk\n    raw_str = tf.read_file(path)\n    \n    # decode image\n    img = tf.image.decode_image(raw_str, 3)\n    img.set_shape([None, None, 3])\n    \n    # convert image from RGB to BGR\n    img = tf.reverse(img, [2])\n    img = tf.to_float(img)\n\n    # import mtcnn model\n    with open('./mtcnn.pb', 'rb') as f:\n        gd = tf.GraphDef.FromString(f.read())\n        prob, landmarks, box = tf.import_graph_def(\n            gd,\n            {'input:0': img, 'min_size:0': tf.constant(40.),\n             'factor:0': tf.constant(0.709),\n             'thresholds:0': tf.constant([0.6,0.7,0.7])},\n            ['prob:0', 'landmarks:0', 'box:0'],\n            name='')\n    return path, prob, landmarks, box\n\n\ndef main(args):\n\n    with tf.device('/cpu:0'):\n        # build dataset\n        dataset = (tf.data\n            .TextLineDataset(args.imglist)\n            .map(preprocess, 8)             # processing data with multi-threads\n            .prefetch(1))                   # prefetch in other thread\n        iterator = dataset.make_one_shot_iterator()\n        path, prob, landmarks, box = iterator.get_next()\n    \n    # build session\n    config = tf.ConfigProto(\n        allow_soft_placement=True, log_device_placement=False,\n        intra_op_parallelism_threads=16, inter_op_parallelism_threads=16)\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    \n    # handle detection results\n    dst_list = open(args.dst + '.list', 'w')\n    dst_pts = open(args.dst + '.pts', 'w')\n    try:\n        while True:\n            p, lm = sess.run([path, landmarks])\n\n            if len(lm) == 0:\n                continue\n\n            dst_list.write('{}\\n'.format(p.decode()))\n            for i, l in enumerate(lm[0]):\n                dst_pts.write('{:.4f}'.format(l))\n                dst_pts.write(',' if i < 9 else '\\n')\n    except tf.errors.OutOfRangeError as e:\n        pass\n\n    dst_list.close()\n    dst_pts.close()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='tensorflow mtcnn')\n    parser.add_argument('imglist', help='image list')\n    parser.add_argument('dst', help='dst prefix')\n    args = parser.parse_args()\n    main(args)\n"""
