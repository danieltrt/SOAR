file_path,api_count,code
data_helpers.py,0,"b'import numpy as np\nimport re\nimport itertools\nimport codecs\nfrom collections import Counter\n\n\ndef clean_str(string):\n  """"""\n  Tokenization/string cleaning for all datasets except for SST.\n  Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n  """"""\n  string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n  string = re.sub(r""\\\'s"", "" \\\'s"", string)\n  string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n  string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n  string = re.sub(r""\\\'re"", "" \\\'re"", string)\n  string = re.sub(r""\\\'d"", "" \\\'d"", string)\n  string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n  string = re.sub(r"","", "" , "", string)\n  string = re.sub(r""!"", "" ! "", string)\n  string = re.sub(r""\\("", "" \\( "", string)\n  string = re.sub(r""\\)"", "" \\) "", string)\n  string = re.sub(r""\\?"", "" \\? "", string)\n  string = re.sub(r""\\s{2,}"", "" "", string)\n  return string.strip().lower()\n\n\ndef load_data_and_labels():\n  """"""\n  Loads MR polarity data from files, splits the data into words and generates labels.\n  Returns split sentences and labels.\n  """"""\n  # Load data from files\n  positive_examples = list(codecs.open(""./data/chinese/pos.txt"", ""r"", ""utf-8"").readlines())\n  positive_examples = [s.strip() for s in positive_examples]\n  negative_examples = list(codecs.open(""./data/chinese/neg.txt"", ""r"", ""utf-8"").readlines())\n  negative_examples = [s.strip() for s in negative_examples]\n  # Split by words\n  x_text = positive_examples + negative_examples\n  # x_text = [clean_str(sent) for sent in x_text]\n  x_text = [list(s) for s in x_text]\n\n  # Generate labels\n  positive_labels = [[0, 1] for _ in positive_examples]\n  negative_labels = [[1, 0] for _ in negative_examples]\n  y = np.concatenate([positive_labels, negative_labels], 0)\n  return [x_text, y]\n\n\ndef pad_sentences(sentences, padding_word=""<PAD/>""):\n  """"""\n  Pads all sentences to the same length. The length is defined by the longest sentence.\n  Returns padded sentences.\n  """"""\n  sequence_length = max(len(x) for x in sentences)\n  padded_sentences = []\n  for i in range(len(sentences)):\n    sentence = sentences[i]\n    num_padding = sequence_length - len(sentence)\n    new_sentence = sentence + [padding_word] * num_padding\n    padded_sentences.append(new_sentence)\n  return padded_sentences\n\n\ndef build_vocab(sentences):\n  """"""\n  Builds a vocabulary mapping from word to index based on the sentences.\n  Returns vocabulary mapping and inverse vocabulary mapping.\n  """"""\n  # Build vocabulary\n  word_counts = Counter(itertools.chain(*sentences))\n  # Mapping from index to word\n  vocabulary_inv = [x[0] for x in word_counts.most_common()]\n  # Mapping from word to index\n  vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n  return [vocabulary, vocabulary_inv]\n\n\ndef build_input_data(sentences, labels, vocabulary):\n  """"""\n  Maps sentencs and labels to vectors based on a vocabulary.\n  """"""\n  x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n  y = np.array(labels)\n  return [x, y]\n\n\ndef load_data():\n  """"""\n  Loads and preprocessed data for the MR dataset.\n  Returns input vectors, labels, vocabulary, and inverse vocabulary.\n  """"""\n  # Load and preprocess data\n  sentences, labels = load_data_and_labels()\n  sentences_padded = pad_sentences(sentences)\n  vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n  x, y = build_input_data(sentences_padded, labels, vocabulary)\n  return [x, y, vocabulary, vocabulary_inv]\n\n\ndef batch_iter(data, batch_size, num_epochs):\n  """"""\n  Generates a batch iterator for a dataset.\n  """"""\n  data = np.array(data)\n  data_size = len(data)\n  num_batches_per_epoch = int(len(data)/batch_size) + 1\n  for epoch in range(num_epochs):\n    # Shuffle the data at each epoch\n    shuffle_indices = np.random.permutation(np.arange(data_size))\n    shuffled_data = data[shuffle_indices]\n    for batch_num in range(num_batches_per_epoch):\n      start_index = batch_num * batch_size\n      end_index = min((batch_num + 1) * batch_size, data_size)\n      yield shuffled_data[start_index:end_index]\n'"
text_cnn.py,37,"b'import tensorflow as tf\n\n# highway layer that borrowed from https://github.com/carpedm20/lstm-char-cnn-tensorflow\ndef highway(input_, size, layer_size=1, bias=-2, f=tf.nn.relu):\n  """"""Highway Network (cf. http://arxiv.org/abs/1505.00387).\n\n  t = sigmoid(Wy + b)\n  z = t * g(Wy + b) + (1 - t) * y\n  where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n  """"""\n  output = input_\n  for idx in xrange(layer_size):\n    output = f(tf.nn.rnn_cell._linear(output, size, 0, scope=\'output_lin_%d\' % idx))\n\n    transform_gate = tf.sigmoid(\n      tf.nn.rnn_cell._linear(input_, size, 0, scope=\'transform_lin_%d\' % idx) + bias)\n    carry_gate = 1. - transform_gate\n\n    output = transform_gate * output + carry_gate * input_\n\n  return output\n\n\nclass TextCNN(object):\n  """"""\n  A CNN for text classification.\n  Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n  """"""\n  def __init__(\n    self, sequence_length, num_classes, vocab_size,\n    embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n      # Placeholders for input, output and dropout\n      self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n      self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n      self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n\n      # Keeping track of l2 regularization loss (optional)\n      l2_loss = tf.constant(0.0)\n\n      # Embedding layer\n      with tf.device(\'/cpu:0\'), tf.name_scope(""embedding""):\n        W = tf.Variable(\n            tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n            name=""W"")\n        self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n        self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n\n      # Create a convolution + maxpool layer for each filter size\n      pooled_outputs = []\n      for filter_size, num_filter in zip(filter_sizes, num_filters):\n        with tf.name_scope(""conv-maxpool-%s"" % filter_size):\n          # Convolution Layer\n          filter_shape = [filter_size, embedding_size, 1, num_filter]\n          W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")\n          b = tf.Variable(tf.constant(0.1, shape=[num_filter]), name=""b"")\n          conv = tf.nn.conv2d(\n            self.embedded_chars_expanded,\n            W,\n            strides=[1, 1, 1, 1],\n            padding=""VALID"",\n            name=""conv"")\n          # Apply nonlinearity\n          h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n          # Maxpooling over the outputs\n          pooled = tf.nn.max_pool(\n            h,\n            ksize=[1, sequence_length - filter_size + 1, 1, 1],\n            strides=[1, 1, 1, 1],\n            padding=\'VALID\',\n            name=""pool"")\n          pooled_outputs.append(pooled)\n\n      # Combine all the pooled features\n      num_filters_total = sum(num_filters)\n      self.h_pool = tf.concat(3, pooled_outputs)\n      self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n      # Add highway\n      with tf.name_scope(""highway""):\n        self.h_highway = highway(self.h_pool_flat, self.h_pool_flat.get_shape()[1], 1, 0)\n\n      # Add dropout\n      with tf.name_scope(""dropout""):\n        self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n\n      # Final (unnormalized) scores and predictions\n      with tf.name_scope(""output""):\n        W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=""W"")\n        b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=""b"")\n        l2_loss += tf.nn.l2_loss(W)\n        l2_loss += tf.nn.l2_loss(b)\n        self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores"")\n        self.predictions = tf.argmax(self.scores, 1, name=""predictions"")\n\n      # CalculateMean cross-entropy loss\n      with tf.name_scope(""loss""):\n        losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n        self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n      # Accuracy\n      with tf.name_scope(""accuracy""):\n        correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
train.py,31,"b'#! /usr/bin/env python\n\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport data_helpers\nfrom text_cnn import TextCNN\n\n# Parameters\n# ==================================================\n\n# Model Hyperparameters\ntf.flags.DEFINE_integer(""embedding_dim"", 128, ""Dimensionality of character embedding (default: 128)"")\ntf.flags.DEFINE_string(""filter_sizes"", ""1,2,3,4,5,6,8"", ""Comma-separated filter sizes (default: \'1,2,3,4,5,6,8\')"")\ntf.flags.DEFINE_string(""num_filters"", ""50,100,150,150,200,200,200"", ""Number of filters per filter size (default: 50,100,150,150,200,200,200)"")\ntf.flags.DEFINE_float(""dropout_keep_prob"", 0.5, ""Dropout keep probability (default: 0.5)"")\ntf.flags.DEFINE_float(""l2_reg_lambda"", 0.0, ""L2 regularizaion lambda (default: 0.0)"")\n\n# Training parameters\ntf.flags.DEFINE_integer(""batch_size"", 32, ""Batch Size (default: 32)"")\ntf.flags.DEFINE_integer(""num_epochs"", 200, ""Number of training epochs (default: 200)"")\ntf.flags.DEFINE_integer(""evaluate_every"", 100, ""Evaluate model on dev set after this many steps (default: 100)"")\ntf.flags.DEFINE_integer(""checkpoint_every"", 100, ""Save model after this many steps (default: 100)"")\n# Misc Parameters\ntf.flags.DEFINE_string(""checkpoint"", \'\', ""Resume checkpoint"")\ntf.flags.DEFINE_boolean(""allow_soft_placement"", True, ""Allow device soft device placement"")\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""Log placement of ops on devices"")\n\nFLAGS = tf.flags.FLAGS\nprint(""\\nParameters:"")\nfor attr, value in sorted(FLAGS.__flags.iteritems()):\n  print(""{}={}"".format(attr.upper(), value))\nprint("""")\n\n\n# Data Preparatopn\n# ==================================================\n\n# Load data\nprint(""Loading data..."")\nx, y, vocabulary, vocabulary_inv = data_helpers.load_data()\n# Randomly shuffle data\nnp.random.seed(10)\nshuffle_indices = np.random.permutation(np.arange(len(y)))\nx_shuffled = x[shuffle_indices]\ny_shuffled = y[shuffle_indices]\n# Split train/test set\n# TODO: This is very crude, should use cross-validation\nx_train, x_dev = x_shuffled[:-300], x_shuffled[-300:]\ny_train, y_dev = y_shuffled[:-300], y_shuffled[-300:]\nsequence_length = x_train.shape[1]\nprint(""Vocabulary Size: {:d}"".format(len(vocabulary)))\nprint(""Train/Dev split: {:d}/{:d}"".format(len(y_train), len(y_dev)))\nprint(""Sequnence Length: {:d}"".format(sequence_length))\n\n\n# Training\n# ==================================================\n\nwith tf.Graph().as_default():\n  session_conf = tf.ConfigProto(\n    allow_soft_placement=FLAGS.allow_soft_placement,\n    log_device_placement=FLAGS.log_device_placement)\n  sess = tf.Session(config=session_conf)\n  with sess.as_default():\n    cnn = TextCNN(\n      sequence_length=sequence_length,\n      num_classes=2,\n      vocab_size=len(vocabulary),\n      embedding_size=FLAGS.embedding_dim,\n      filter_sizes=map(int, FLAGS.filter_sizes.split("","")),\n      num_filters=map(int, FLAGS.num_filters.split("","")),\n      l2_reg_lambda=FLAGS.l2_reg_lambda)\n\n    # Define Training procedure\n    global_step = tf.Variable(0, name=""global_step"", trainable=False)\n    optimizer = tf.train.AdamOptimizer(1e-4)\n    grads_and_vars = optimizer.compute_gradients(cnn.loss, aggregation_method=2)\n    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n    # Keep track of gradient values and sparsity (optional)\n    grad_summaries = []\n    for g, v in grads_and_vars:\n      if g is not None:\n        grad_hist_summary = tf.histogram_summary(""{}/grad/hist"".format(v.name), g)\n        sparsity_summary = tf.scalar_summary(""{}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n        grad_summaries.append(grad_hist_summary)\n        grad_summaries.append(sparsity_summary)\n    grad_summaries_merged = tf.merge_summary(grad_summaries)\n\n    # Output directory for models and summaries\n    if FLAGS.checkpoint == """":\n      timestamp = str(int(time.time()))\n      out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", timestamp))\n      print(""Writing to {}\\n"".format(out_dir))\n    else:\n      out_dir = FLAGS.checkpoint\n\n    # Summaries for loss and accuracy\n    loss_summary = tf.scalar_summary(""loss"", cnn.loss)\n    acc_summary = tf.scalar_summary(""accuracy"", cnn.accuracy)\n\n    # Train Summaries\n    train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n    train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n    train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)\n\n    # Dev summaries\n    dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n    dev_summary_dir = os.path.join(out_dir, ""summaries"", ""dev"")\n    dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)\n\n    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n    checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n    if not os.path.exists(checkpoint_dir):\n      os.makedirs(checkpoint_dir)\n    saver = tf.train.Saver(tf.all_variables())\n\n    # Initialize all variables\n    sess.run(tf.initialize_all_variables())\n\n    ckpt = tf.train.get_checkpoint_state(os.path.join(FLAGS.checkpoint, \'checkpoints\'))\n    if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n      print ""Reading model parameters from %s"" % ckpt.model_checkpoint_path\n      saver.restore(sess, ckpt.model_checkpoint_path)\n\n    def train_step(x_batch, y_batch):\n      """"""\n      A single training step\n      """"""\n      feed_dict = {\n        cnn.input_x: x_batch,\n        cnn.input_y: y_batch,\n        cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n      }\n      _, step, summaries, loss, accuracy = sess.run(\n        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n        feed_dict)\n      time_str = datetime.datetime.now().strftime(""%d, %b %Y %H:%M:%S"")\n      print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n      train_summary_writer.add_summary(summaries, step)\n\n    def dev_step(x_batch, y_batch, writer=None):\n      """"""\n      Evaluates model on a dev set\n      """"""\n      feed_dict = {\n        cnn.input_x: x_batch,\n        cnn.input_y: y_batch,\n        cnn.dropout_keep_prob: 1.0\n      }\n      step, summaries, loss, accuracy = sess.run(\n        [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n        feed_dict)\n      time_str = datetime.datetime.now().strftime(""%d, %b %Y %H:%M:%S"")\n      print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n      if writer:\n        writer.add_summary(summaries, step)\n\n    # Generate batches\n    batches = data_helpers.batch_iter(\n      zip(x_train, y_train), FLAGS.batch_size, FLAGS.num_epochs)\n    # Training loop. For each batch...\n    for batch in batches:\n      x_batch, y_batch = zip(*batch)\n      train_step(x_batch, y_batch)\n      current_step = tf.train.global_step(sess, global_step)\n      if current_step % FLAGS.evaluate_every == 0:\n        print(""\\nEvaluation:"")\n        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n        print("""")\n      if current_step % FLAGS.checkpoint_every == 0:\n        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n        print(""Saved model checkpoint to {}\\n"".format(path))\n'"
