file_path,api_count,code
data_helpers.py,7,"b'from __future__ import print_function\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport _pickle as cPickle\nimport re\nimport itertools\nfrom collections import Counter\n\n\nPAD = ""_PAD""\nUNK = ""_UNK""\n\n\ndef Q2B(uchar):\n  """"""\xe5\x85\xa8\xe8\xa7\x92\xe8\xbd\xac\xe5\x8d\x8a\xe8\xa7\x92""""""\n  inside_code = ord(uchar)\n  if inside_code == 0x3000:\n    inside_code = 0x0020\n  else:\n    inside_code -= 0xfee0\n  #\xe8\xbd\xac\xe5\xae\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe4\xb8\x8d\xe6\x98\xaf\xe5\x8d\x8a\xe8\xa7\x92\xe5\xad\x97\xe7\xac\xa6\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\n  if inside_code < 0x0020 or inside_code > 0x7e:\n    return uchar\n  return chr(inside_code)\n\n\ndef replace_all(repls, text):\n  # return re.sub(\'|\'.join(repls.keys()), lambda k: repls[k.group(0)], text)\n  return re.sub(\'|\'.join(re.escape(key) for key in repls.keys()), lambda k: repls[k.group(0)], text)\n\n\ndef split_sentence(txt):\n  sents = re.split(r\'\\n|\\s|;|\xef\xbc\x9b|\xe3\x80\x82|\xef\xbc\x8c|\\.|,|\\?|\\!|\xef\xbd\x9c|[=]{2,}|[.]{3,}|[\xe2\x94\x80]{2,}|[\\-]{2,}|~|\xe3\x80\x81|\xe2\x95\xb1|\xe2\x88\xa5\', txt)\n  sents = [c for s in sents for c in re.split(r\'([^%]+[\\d,.]+%)\', s)]\n  sents = list(filter(None, sents))\n  return sents\n\n\ndef normalize_punctuation(text):\n  cpun = [[\'\t\'],\n          [\'\xef\xb9\x97\', \'\xef\xbc\x81\'],\n          [\'\xe2\x80\x9c\', \'\xe3\x82\x9b\', \'\xe3\x80\x83\', \'\xe2\x80\xb2\', \'\xef\xbc\x82\'],\n          [\'\xe2\x80\x9d\'],\n          [\'\xc2\xb4\', \'\xe2\x80\x98\', \'\xe2\x80\x99\'],\n          [\'\xef\xbc\x9b\', \'\xef\xb9\x94\'],\n          [\'\xe3\x80\x8a\', \'\xe3\x80\x88\', \'\xef\xbc\x9c\'],\n          [\'\xe3\x80\x8b\', \'\xe3\x80\x89\', \'\xef\xbc\x9e\'],\n          [\'\xef\xb9\x91\'],\n          [\'\xe3\x80\x90\', \'\xe3\x80\x8e\', \'\xe3\x80\x94\', \'\xef\xb9\x9d\', \'\xef\xbd\xa2\', \'\xef\xb9\x81\'],\n          [\'\xe3\x80\x91\', \'\xe3\x80\x8f\', \'\xe3\x80\x95\', \'\xef\xb9\x9e\', \'\xef\xbd\xa3\', \'\xef\xb9\x82\'],\n          [\'\xef\xbc\x88\', \'\xe3\x80\x8c\'],\n          [\'\xef\xbc\x89\', \'\xe3\x80\x8d\'],\n          [\'\xef\xb9\x96\', \'\xef\xbc\x9f\'],\n          [\'\xef\xb8\xb0\', \'\xef\xb9\x95\', \'\xef\xbc\x9a\'],\n          [\'\xe3\x83\xbb\', \'\xef\xbc\x8e\', \'\xc2\xb7\', \'\xe2\x80\xa7\', \'\xc2\xb0\'],\n          [\'\xe2\x97\x8f\', \'\xe2\x97\x8b\', \'\xe2\x96\xb2\', \'\xe2\x97\x8e\', \'\xe2\x97\x87\', \'\xe2\x96\xa0\', \'\xe2\x96\xa1\', \'\xe2\x80\xbb\', \'\xe2\x97\x86\'],\n          [\'\xe3\x80\x9c\', \'\xef\xbd\x9e\', \'\xe2\x88\xbc\'],\n          [\'\xef\xb8\xb1\', \'\xe2\x94\x82\', \'\xe2\x94\xbc\'],\n          [\'\xe2\x95\xb1\'],\n          [\'\xe2\x95\xb2\'],\n          [\'\xe2\x80\x94\', \'\xe3\x83\xbc\', \'\xe2\x80\x95\', \'\xe2\x80\x90\', \'\xe2\x88\x92\', \'\xe2\x94\x80\', \'\xef\xb9\xa3\', \'\xe2\x80\x93\', \'\xe3\x84\xa7\', \'\xef\xbc\x8d\']]\n  epun = [\' \', \'!\', \'""\', \'""\', \'\\\'\', \';\', \'<\', \'>\', \'\xe3\x80\x81\', \'[\', \']\', \'(\', \')\', \'?\', \':\', \'\xef\xbd\xa5\', \'\xe2\x80\xa2\', \'~\', \'|\', \'/\', \'\\\\\', \'-\']\n  repls = {}\n\n  for i in range(len(cpun)):\n    for j in range(len(cpun[i])):\n      repls[cpun[i][j]] = epun[i]\n\n  return replace_all(repls, text)\n\n\ndef clean_str(txt):\n  # txt = txt.replace(\'\xe8\xaa\xac\', \'\xe8\xaa\xaa\')\n  # txt = txt.replace(\'\xe9\x96\xb2\', \'\xe9\x96\xb1\')\n  # txt = txt.replace(\'\xe8\x84\xb1\', \'\xe8\x84\xab\')\n  # txt = txt.replace(\'\xe8\x9c\x95\', \'\xe8\x9b\xbb\')\n  # txt = txt.replace(\'\xe6\x88\xb7\', \'\xe6\x88\xb6\')\n  # \xe8\x87\xba\n  txt = txt.replace(\'\xe8\x87\xba\', \'\xe5\x8f\xb0\')\n  txt = txt.replace(\'\xe3\x80\x80\', \'\') # \\u3000\n  txt = normalize_punctuation(txt)\n  txt = \'\'.join([Q2B(c) for c in list(txt)])\n  return txt\n\n\ndef build_vocab(sentences):\n  """"""\n  Builds a vocabulary mapping from word to index based on the sentences.\n  Returns vocabulary mapping and inverse vocabulary mapping.\n  """"""\n  # Build vocabulary\n  word_counts = Counter(itertools.chain(*sentences))\n  # Mapping from index to word\n  vocabulary_inv = [x[0] for x in word_counts.most_common()]\n  # Mapping from word to index\n  vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n  return [vocabulary, vocabulary_inv]\n\n\ndef get_vocab(path=\'./data/vocab.pkl\'):\n  """"""Loads the vocab file, if present""""""\n  if not os.path.exists(path) or os.path.isdir(path):\n    raise ValueError(\'No file at {}\'.format(path))\n\n  char_list = cPickle.load(open(path, \'rb\'))\n  vocab = dict(zip(char_list, range(len(char_list))))\n\n  return vocab, char_list\n\n\ndef build_dataset(pos_path=\'chinese/pos_t.txt\', neg_path=\'chinese/neg_t.txt\',\n                  data_dir=\'./data\', max_doc_len=30, max_sent_len=50, ):\n  pos_docs = list(open(os.path.join(data_dir, pos_path)).readlines())\n  neg_docs = list(open(os.path.join(data_dir, neg_path)).readlines())\n  vocab, _ = get_vocab(\'./data/vocab.pkl\')\n  pos_size = len(pos_docs)\n  neg_size = len(neg_docs)\n  pos_train_size = int(pos_size * 0.9)\n  pos_valid_size = pos_size - pos_train_size\n  neg_train_size = int(neg_size * 0.9)\n  neg_valid_size = neg_size - neg_train_size\n  train_path = os.path.join(data_dir, \'train.tfrecords\')\n  valid_path = os.path.join(data_dir, \'valid.tfrecords\')\n\n  def upsampling(x, size):\n    if len(x) > size:\n      return x\n    diff_size = size - len(x)\n    return x + list(np.random.choice(x, diff_size, replace=False))\n\n\n  def write_data(doc, label, out_f):\n    doc = split_sentence(clean_str(doc))\n    document_length = len(doc)\n    sentence_lengths = np.zeros((max_doc_len,), dtype=np.int64)\n    data = np.ones((max_doc_len * max_sent_len,), dtype=np.int64)\n    doc_len = min(document_length, max_doc_len)\n\n    for j in range(doc_len):\n      sent = doc[j]\n      actual_len = len(sent)\n      pos = j * max_sent_len\n      sent_len = min(actual_len, max_sent_len)\n      # sentence_lengths\n      sentence_lengths[j] = sent_len\n      # dataset\n      data[pos:pos+sent_len] = [vocab.get(sent[k], 0) for k in range(sent_len)]\n\n    features = {\'sentence_lengths\': tf.train.Feature(int64_list=tf.train.Int64List(value=sentence_lengths)),\n                \'document_lengths\': tf.train.Feature(int64_list=tf.train.Int64List(value=[doc_len])),\n                \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                \'text\': tf.train.Feature(int64_list=tf.train.Int64List(value=data))}\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n    out_f.write(example.SerializeToString())\n\n  # oversampling\n  with tf.python_io.TFRecordWriter(train_path) as out_f:\n    train_size = max(pos_train_size, neg_train_size)\n    pos_train_docs = np.random.choice(upsampling(pos_docs[:pos_train_size], train_size), train_size, replace=False)\n    neg_train_docs = np.random.choice(upsampling(neg_docs[:neg_train_size], train_size), train_size, replace=False)\n\n    print(len(pos_train_docs), len(neg_train_docs))\n    for i in tqdm(range(train_size)):\n      pos_row = pos_train_docs[i]\n      neg_row = neg_train_docs[i]\n      write_data(pos_row, 1, out_f)\n      write_data(neg_row, 0, out_f)\n\n  with tf.python_io.TFRecordWriter(valid_path) as out_f:\n    valid_size = max(pos_valid_size, neg_valid_size)\n    pos_valid_docs = np.random.choice(upsampling(pos_docs[pos_train_size:], valid_size), valid_size, replace=False)\n    neg_valid_docs = np.random.choice(upsampling(neg_docs[neg_train_size:], valid_size), valid_size, replace=False)\n    for i in tqdm(range(valid_size)):\n      pos_row = pos_valid_docs[i]\n      neg_row = neg_valid_docs[i]\n      write_data(pos_row, 1, out_f)\n      write_data(neg_row, 0, out_f)\n\n  print(\'Done {} records, train {}, valid {}\'.format(pos_size + neg_size,\n                                                     pos_train_size + neg_train_size,\n                                                     pos_valid_size + neg_valid_size))\n\n\nif __name__ == \'__main__\':\n  build_dataset()\n'"
model.py,50,"b""from __future__ import print_function\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import DropoutWrapper, GRUCell\nimport tensorflow.contrib.layers as layers\nfrom tensorflow.python.ops.nn import bidirectional_dynamic_rnn\nfrom ran_cell import RANCell\n\n\nL2_REG = 1e-4\n\n\nclass Model(object):\n  def __init__(self, conf):\n    self.batch_size = conf.batch_size\n    self.vocab_size = conf.vocab_size\n    self.rnn_size = conf.rnn_size\n    self.document_size = conf.document_size\n    self.sentence_size = conf.sentence_size\n    self.word_attention_size = conf.word_attention_size\n    self.sent_attention_size = conf.sent_attention_size\n    self.char_embedding_size = conf.char_embedding_size\n    self.keep_prob = conf.keep_prob\n\n    self.is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n    self.inputs = tf.placeholder(shape=(self.batch_size, self.document_size, self.sentence_size), dtype=tf.int64, name='inputs')\n    self.labels = tf.placeholder(shape=(self.batch_size,), dtype=tf.int64, name='labels')\n    self.sentence_lengths = tf.placeholder(shape=(self.batch_size, self.document_size), dtype=tf.int64, name='sentence_lengths')\n    self.document_lengths = tf.placeholder(shape=(self.batch_size), dtype=tf.int64, name='document_lengths')\n\n    with tf.device('/cpu:0'):\n      self.embedding = tf.get_variable('embedding',\n                                       [self.vocab_size, self.char_embedding_size],\n                                       trainable=False)\n      inputs = tf.nn.embedding_lookup(self.embedding, self.inputs)\n\n    char_length = tf.reshape(self.sentence_lengths, [-1]) # [batch_size * document_size]\n    char_inputs = tf.reshape(inputs, [self.batch_size * self.document_size, self.sentence_size, self.char_embedding_size])\n\n    with tf.variable_scope('character_encoder') as scope:\n      char_outputs, _ = self.bi_gru_encode(char_inputs, char_length, scope)\n\n      with tf.variable_scope('attention') as scope:\n        char_attn_outputs = self.attention(char_outputs, self.word_attention_size, scope)\n        char_attn_outputs = tf.reshape(char_attn_outputs, [self.batch_size, self.document_size, -1])\n\n      with tf.variable_scope('dropout'):\n        char_attn_outputs = layers.dropout(char_attn_outputs,\n                                           keep_prob=self.keep_prob,\n                                           is_training=self.is_training)\n\n    with tf.variable_scope('sentence_encoder') as scope:\n      sent_outputs, _ = self.bi_gru_encode(char_attn_outputs, self.document_lengths, scope)\n\n      with tf.variable_scope('attention') as scope:\n        sent_attn_outputs = self.attention(sent_outputs, self.sent_attention_size, scope)\n\n      with tf.variable_scope('dropout'):\n        sent_attn_outputs = layers.dropout(sent_attn_outputs,\n                                           keep_prob=self.keep_prob,\n                                           is_training=self.is_training)\n\n    with tf.variable_scope('losses'):\n      logits = layers.fully_connected(inputs=sent_attn_outputs,\n                                      num_outputs=2,\n                                      activation_fn=None,\n                                      weights_regularizer=layers.l2_regularizer(scale=L2_REG))\n      pred = tf.argmax(logits, 1)\n      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n                                                                           labels=self.labels))\n      correct_pred = tf.equal(self.labels, pred)\n      correct_pred = tf.cast(correct_pred, tf.float32)\n      self.accuracy = tf.reduce_mean(correct_pred)\n\n      reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n      self.cost = tf.add_n([loss] + reg_losses)\n\n\n  def attention(self, inputs, size, scope):\n    with tf.variable_scope(scope or 'attention') as scope:\n      attention_context_vector = tf.get_variable(name='attention_context_vector',\n                                                 shape=[size],\n                                                 regularizer=layers.l2_regularizer(scale=L2_REG),\n                                                 dtype=tf.float32)\n      input_projection = layers.fully_connected(inputs, size,\n                                                activation_fn=tf.tanh,\n                                                weights_regularizer=layers.l2_regularizer(scale=L2_REG))\n      vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\n      attention_weights = tf.nn.softmax(vector_attn, dim=1)\n      weighted_projection = tf.multiply(inputs, attention_weights)\n      outputs = tf.reduce_sum(weighted_projection, axis=1)\n\n    return outputs\n\n\n  def bi_gru_encode(self, inputs, sentence_size, scope=None):\n    batch_size = inputs.get_shape()[0]\n\n    with tf.variable_scope(scope or 'bi_gru_encode'):\n      fw_cell = RANCell(self.rnn_size, keep_prob=self.keep_prob, normalize=True, is_training=self.is_training)\n      bw_cell = RANCell(self.rnn_size, keep_prob=self.keep_prob, normalize=True, is_training=self.is_training)\n      fw_cell_state = fw_cell.zero_state(batch_size, tf.float32)\n      bw_cell_state = bw_cell.zero_state(batch_size, tf.float32)\n\n      enc_out, (enc_state_fw, enc_state_bw) = bidirectional_dynamic_rnn(cell_fw=fw_cell,\n                                                                        cell_bw=bw_cell,\n                                                                        inputs=inputs,\n                                                                        sequence_length=sentence_size,\n                                                                        initial_state_fw=fw_cell_state,\n                                                                        initial_state_bw=bw_cell_state)\n\n      enc_state = tf.concat([enc_state_fw, enc_state_bw], 1)\n      enc_outputs = tf.concat(enc_out, 2)\n\n    return enc_outputs, enc_state\n\n\nif __name__ == '__main__':\n  tf.flags.DEFINE_integer('batch_size', 32, 'Batch Size')\n  tf.flags.DEFINE_integer('vocab_size', 1000, 'Vocabulary size')\n  tf.flags.DEFINE_integer('word_attention_size', 300, 'Word level attention unit size')\n  tf.flags.DEFINE_integer('sent_attention_size', 300, 'Sentence level attention unit size')\n  tf.flags.DEFINE_integer('document_size', 16, 'Document size')\n  tf.flags.DEFINE_integer('sentence_size', 25, 'Sentence size')\n  tf.flags.DEFINE_integer('attention_size', 300, 'Sentence size')\n  tf.flags.DEFINE_integer('rnn_size', 300, 'RNN unit size')\n  tf.flags.DEFINE_integer('char_embedding_size', 300, 'Embedding dimension')\n  tf.flags.DEFINE_float('keep_prob', 0.5, 'Dropout keep prob')\n  tf.flags.DEFINE_bool('is_training', True, 'training model')\n\n  FLAGS = tf.flags.FLAGS\n  FLAGS._parse_flags()\n\n  model = Model(FLAGS)\n"""
ran_cell.py,9,"b'import collections\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl\nfrom tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\nfrom tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl\nfrom tensorflow.python.ops.math_ops import tanh\nfrom tensorflow.python.ops import variable_scope as vs\n\n\n_checked_scope = core_rnn_cell_impl._checked_scope\n_BIAS_VARIABLE_NAME = ""bias""\n_WEIGHTS_VARIABLE_NAME = ""kernel""\n\n\ndef orthogonal(shape):\n  """"""Orthogonal initilaizer.""""""\n  flat_shape = (shape[0], np.prod(shape[1:]))\n  a = np.random.normal(0.0, 1.0, flat_shape)\n  u, _, v = np.linalg.svd(a, full_matrices=False)\n  q = u if u.shape == flat_shape else v\n  return q.reshape(shape)\n\n\ndef orthogonal_initializer(scale=1.0):\n  """"""Orthogonal initializer.""""""\n  def _initializer(shape, dtype=tf.float32, partition_info=None):  # pylint: disable=unused-argument\n    return tf.constant(orthogonal(shape) * scale, dtype)\n\n  return _initializer\n\n\ndef linear(args,\n           output_size,\n           bias,\n           bias_initializer=None,\n           kernel_initializer=None,\n           kernel_regularizer=None,\n           bias_regularizer=None,\n           normalize=False):\n  """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_initializer: starting value to initialize the bias\n      (default is all zeros).\n    kernel_initializer: starting value to initialize the weight.\n    kernel_regularizer: kernel regularizer\n    bias_regularizer: bias regularizer\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  """"""\n  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(""`args` must be specified"")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape() for a in args]\n  for shape in shapes:\n    if shape.ndims != 2:\n      raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)\n    if shape[1].value is None:\n      raise ValueError(""linear expects shape[1] to be provided for shape %s, ""\n                       ""but saw %s"" % (shape, shape[1]))\n    else:\n      total_arg_size += shape[1].value\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  scope = vs.get_variable_scope()\n  with vs.variable_scope(scope) as outer_scope:\n    weights = vs.get_variable(\n        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n        dtype=dtype,\n        initializer=kernel_initializer,\n        regularizer=kernel_regularizer)\n\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], weights)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n\n    if normalize:\n      res = tf.contrib.layers.layer_norm(res)\n\n    # remove the layer\xe2\x80\x99s bias if there is one (because it would be redundant)\n    if not bias or normalize:\n      return res\n\n    with vs.variable_scope(outer_scope) as inner_scope:\n      inner_scope.set_partitioner(None)\n      if bias_initializer is None:\n        bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n      biases = vs.get_variable(\n          _BIAS_VARIABLE_NAME, [output_size],\n          dtype=dtype,\n          initializer=bias_initializer,\n          regularizer=bias_regularizer)\n\n  return nn_ops.bias_add(res, biases)\n\n\nclass RANCell(RNNCell):\n  """"""Recurrent Additive Networks (cf. https://arxiv.org/abs/1705.07393).""""""\n\n  def __init__(self, num_units, input_size=None, activation=tanh, keep_prob=0.5,\n               normalize=False, reuse=None, is_training=tf.constant(False)):\n    if input_size is not None:\n      logging.warn(""%s: The input_size parameter is deprecated."", self)\n    self._num_units = num_units\n    self._activation = activation\n    self._normalize = normalize\n    self._keep_prob = keep_prob\n    self._reuse = reuse\n    self._is_training = is_training\n\n  @property\n  def state_size(self):\n    return tf.contrib.rnn.LSTMStateTuple(self._num_units, self.output_size)\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    with _checked_scope(self, scope or ""ran_cell"", reuse=self._reuse):\n      with vs.variable_scope(""gates""):\n        c, h = state\n        gates = tf.nn.sigmoid(linear([inputs, h], 2 * self._num_units, True,\n                                     normalize=self._normalize,\n                                     kernel_initializer=tf.orthogonal_initializer()))\n        i, f = array_ops.split(value=gates, num_or_size_splits=2, axis=1)\n\n      with vs.variable_scope(""candidate""):\n        content = linear([inputs], self._num_units, True, normalize=self._normalize)\n\n      new_c = i * content + f * c\n      new_h = self._activation(c)\n\n      new_h = tf.cond(self._is_training,\n                      lambda: nn_ops.dropout(new_h, self._keep_prob),\n                      lambda: new_h)\n\n      new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n      output = new_h\n    return output, new_state\n'"
train.py,34,"b""from __future__ import print_function\nimport os\nimport pprint\nimport time\nimport tensorflow as tf\nimport numpy as np\nfrom model import Model\nfrom data_helpers import get_vocab\n\npp = pprint.PrettyPrinter()\n\nflags = tf.app.flags\n\nflags.DEFINE_integer('batch_size', 64, 'Batch size')\nflags.DEFINE_integer('epochs', 100, 'epochs')\nflags.DEFINE_integer('rnn_size', 300, 'RNN unit size')\nflags.DEFINE_integer('word_attention_size', 300, 'Word level attention unit size')\nflags.DEFINE_integer('sent_attention_size', 300, 'Sentence level attention unit size')\nflags.DEFINE_integer('char_embedding_size', 300, 'Embedding dimension')\nflags.DEFINE_string('checkpoint_dir', 'checkpoint', 'Directory name to save the checkpoints [checkpoint]')\nflags.DEFINE_integer('vocab_size', 6790, 'vocabulary size')\nflags.DEFINE_float('keep_prob', 0.5, 'Dropout keep prob')\nflags.DEFINE_integer('document_size', 30, 'document size')\nflags.DEFINE_integer('sentence_size', 50, 'sentence size')\nflags.DEFINE_float('learning_rate', 1e-4, 'learning rate')\nflags.DEFINE_float('grad_clip', 5.0, 'grad clip')\n\nFLAGS = flags.FLAGS\n\ndef read_records(index=0):\n  train_queue = tf.train.string_input_producer(['./data/train.tfrecords'], num_epochs=FLAGS.epochs)\n  valid_queue = tf.train.string_input_producer(['./data/valid.tfrecords'], num_epochs=FLAGS.epochs)\n  queue = tf.QueueBase.from_list(index, [train_queue, valid_queue])\n  reader = tf.TFRecordReader()\n  _, serialized_example = reader.read(queue)\n  features = tf.parse_single_example(\n      serialized_example,\n      features={\n          'sentence_lengths': tf.FixedLenFeature([FLAGS.document_size], tf.int64),\n          'document_lengths': tf.FixedLenFeature([], tf.int64),\n          'label': tf.FixedLenFeature([], tf.int64),\n          'text': tf.FixedLenFeature([FLAGS.document_size * FLAGS.sentence_size], tf.int64),\n      })\n\n  sentence_lengths = features['sentence_lengths']\n  document_lengths = features['document_lengths']\n  label = features['label']\n  text = features['text']\n\n  sentence_lengths_batch, document_lengths_batch, label_batch, text_batch = tf.train.shuffle_batch(\n      [sentence_lengths, document_lengths, label, text],\n      batch_size=FLAGS.batch_size,\n      capacity=5000,\n      min_after_dequeue=1000)\n\n  return sentence_lengths_batch, document_lengths_batch, label_batch, text_batch\n\n\ndef main(_):\n  pp.pprint(FLAGS.__flags)\n\n  if not os.path.exists(FLAGS.checkpoint_dir):\n    print(' [*] Creating checkpoint directory...')\n    os.makedirs(FLAGS.checkpoint_dir)\n\n  checkpoint_path = os.path.join(FLAGS.checkpoint_dir, 'model.ckpt')\n\n  # load pre-trained char embedding\n  char_emb = np.load('./data/emb.npy')\n\n  sentence_lengths_batch, document_lengths_batch, label_batch, text_batch = read_records()\n  valid_sentence_lengths_batch, valid_document_lengths_batch, valid_label_batch, valid_text_batch = read_records(1)\n\n  text_batch = tf.reshape(text_batch, (-1, FLAGS.document_size, FLAGS.sentence_size))\n  valid_text_batch = tf.reshape(valid_text_batch, (-1, FLAGS.document_size, FLAGS.sentence_size))\n\n  with tf.variable_scope('model'):\n    train_model = Model(FLAGS)\n  with tf.variable_scope('model', reuse=True):\n    valid_model = Model(FLAGS)\n\n  # training operator\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  lr = tf.train.exponential_decay(FLAGS.learning_rate, global_step, 10000, 0.9)\n  tvars = tf.trainable_variables()\n  grads, _ = tf.clip_by_global_norm(tf.gradients(train_model.cost, tvars), FLAGS.grad_clip)\n  optimizer = tf.train.AdamOptimizer(lr)\n  train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n\n  tf.summary.scalar('train_loss', train_model.cost)\n  tf.summary.scalar('valid_loss', valid_model.cost)\n\n  saver = tf.train.Saver()\n\n  with tf.Session() as sess:\n    tf.local_variables_initializer().run()\n    tf.global_variables_initializer().run()\n\n    # assign char embedding\n    sess.run([], feed_dict={train_model.embedding: char_emb})\n    sess.run([], feed_dict={valid_model.embedding: char_emb})\n\n    # saver.restore(sess, checkpoint_path)\n\n    # stock_emb = train_model.label_embedding.eval()\n    #\n    # np.save('./data/stock_emb.npy', stock_emb)\n    # print('done')\n\n    # summary_op = tf.summary.merge_all()\n    # train_writer = tf.summary.FileWriter('./log/train', sess.graph)\n    # valid_writer = tf.summary.FileWriter('./log/test')\n\n    current_step = 0\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n    valid_cost = 0\n    valid_accuracy = 0\n    train_cost = 0\n    VALID_SIZE = 54\n\n    _, chars = get_vocab()\n\n    try:\n      while not coord.should_stop():\n        start = time.time()\n\n        if current_step % 500 == 0:\n          valid_cost = 0\n          for _ in range(VALID_SIZE):\n            valid_text, valid_label, valid_sentence_lengths, valid_document_lengths =\\\n              sess.run([valid_text_batch, valid_label_batch, valid_sentence_lengths_batch, valid_document_lengths_batch])\n\n            valid_outputs = sess.run([valid_model.cost, valid_model.accuracy], feed_dict={\n                valid_model.inputs: valid_text,\n                valid_model.labels: valid_label,\n                valid_model.sentence_lengths: valid_sentence_lengths,\n                valid_model.document_lengths: valid_document_lengths,\n                valid_model.is_training: False\n            })\n            valid_cost += valid_outputs[0]\n            valid_accuracy += valid_outputs[1]\n          valid_cost /= VALID_SIZE\n          valid_accuracy /= VALID_SIZE\n\n        inputs, labels, sentence_lengths, document_lengths =\\\n          sess.run([text_batch, label_batch, sentence_lengths_batch, document_lengths_batch])\n\n        # valid_writer.add_summary(summary, current_step)\n        train_cost, train_accuracy, _ = sess.run([train_model.cost, train_model.accuracy, train_op], feed_dict={\n            train_model.inputs: inputs,\n            train_model.labels: labels,\n            train_model.sentence_lengths: sentence_lengths,\n            train_model.document_lengths: document_lengths,\n            train_model.is_training: True\n        })\n        # train_writer.add_summary(summary, current_step)\n        end = time.time()\n\n        print('Cost at step %s: %s(%s), test cost: %s(%s), time: %s' %\n              (current_step, train_cost, train_accuracy, valid_cost, valid_accuracy, end - start))\n\n        current_step = tf.train.global_step(sess, global_step)\n\n        if current_step != 0 and current_step % 1000 == 0:\n          save_path = saver.save(sess, checkpoint_path)\n          print('Model saved in file:', save_path)\n\n    except tf.errors.OutOfRangeError:\n      print('Done training!')\n    finally:\n      coord.request_stop()\n\n    save_path = saver.save(sess, checkpoint_path)\n    print('Model saved in file:', save_path)\n\n    coord.join(threads)\n\nif __name__ == '__main__':\n  tf.app.run()\n"""
