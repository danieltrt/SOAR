file_path,api_count,code
setup.py,0,"b'import setuptools\n\nwith open(""README.md"", \'r\') as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name=\'reaver\',\n    version=\'2.1.9\',\n    author=\'Roman Ring\',\n    author_email=\'inoryy@gmail.com\',\n    description=\'Reaver: Modular Deep Reinforcement Learning Framework. Focused on StarCraft II. \'\n                \'Supports Gym, Atari, and MuJoCo. Matches reference results.\',\n    long_description=long_description,\n    keywords=\'reaver starcraft2 gym atari mujoco tensorflow reinforcement learning neural network\',\n    include_package_data=True,\n    packages=setuptools.find_packages(),\n    install_requires=[\n        \'PySC2 >= 3.0.0\',\n        \'gin-config >= 0.3.0\',\n        \'tensorflow >= 2.1.0\',\n        \'tensorflow-probability >= 0.9.0\'\n    ],\n    extras_require={\n        \'gym\': [\n            \'PyOpenGL\',\n            \'gym >= 0.9\',\n            \'opencv-python\'\n        ],\n        \'atari\': [\n            \'Pillow\',\n            \'PyOpenGL\',\n            \'gym >= 0.9\',\n            \'atari_py >= 0.1.4\',\n        ],\n        \'mujoco\': [\n            \'imageio\',\n            \'gym >= 0.9\',\n            \'mujoco_py >= 1.50\',\n        ]\n    },\n    license=\'MIT\',\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Environment :: Console\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Operating System :: POSIX :: Linux\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    ],\n    url=\'https://github.com/inoryy/reaver\',\n)\n'"
reaver/__init__.py,0,b'import reaver.envs\nimport reaver.models\nimport reaver.agents\nimport reaver.utils\n'
reaver/run.py,3,"b'import os\nimport gin\nimport tensorflow.compat.v1 as tf\nfrom absl import app, flags\n\nimport reaver as rvr\n\nflags.DEFINE_string(\'env\', None, \'Either Gym env id or PySC2 map name to run agent in.\')\nflags.DEFINE_string(\'agent\', \'a2c\', \'Name of the agent. Must be one of (a2c, ppo).\')\n\nflags.DEFINE_bool(\'render\', False, \'Whether to render first(!) env.\')\nflags.DEFINE_string(\'gpu\', \'0\', \'GPU(s) id(s) to use. If not set TensorFlow will use CPU.\')\n\nflags.DEFINE_integer(\'n_envs\', 4, \'Number of environments to execute in parallel.\')\nflags.DEFINE_integer(\'n_updates\', 1000000, \'Number of train updates (1 update has batch_sz * traj_len samples).\')\n\nflags.DEFINE_integer(\'ckpt_freq\', 500, \'Number of train updates per one checkpoint save.\')\nflags.DEFINE_integer(\'log_freq\', 100, \'Number of train updates per one console log.\')\nflags.DEFINE_integer(\'log_eps_avg\', 100, \'Number of episodes to average for performance stats.\')\nflags.DEFINE_integer(\'max_ep_len\', None, \'Max number of steps an agent can take in an episode.\')\n\nflags.DEFINE_string(\'results_dir\', \'results\', \'Directory for model weights, train logs, etc.\')\nflags.DEFINE_string(\'experiment\', None, \'Name of the experiment. Datetime by default.\')\n\nflags.DEFINE_multi_string(\'gin_files\', [], \'List of path(s) to gin config(s).\')\nflags.DEFINE_multi_string(\'gin_bindings\', [], \'Gin bindings to override config values.\')\n\nflags.DEFINE_bool(\'restore\', False,\n                  \'Restore & continue previously executed experiment. \'\n                  \'If experiment not specified then last modified is used.\')\nflags.DEFINE_bool(\'test\', False,\n                  \'Run an agent in test mode: restore flag is set to true and number of envs set to 1\'\n                  \'Loss is calculated, but gradients are not applied.\'\n                  \'Checkpoints, summaries, log files are not updated, but console logger is enabled.\')\n\nflags.DEFINE_alias(\'e\', \'env\')\nflags.DEFINE_alias(\'a\', \'agent\')\nflags.DEFINE_alias(\'p\', \'n_envs\')\nflags.DEFINE_alias(\'u\', \'n_updates\')\nflags.DEFINE_alias(\'lf\', \'log_freq\')\nflags.DEFINE_alias(\'cf\', \'ckpt_freq\')\nflags.DEFINE_alias(\'la\', \'log_eps_avg\')\nflags.DEFINE_alias(\'n\', \'experiment\')\nflags.DEFINE_alias(\'g\', \'gin_bindings\')\n\n\ndef main(argv):\n    tf.disable_eager_execution()\n    tf.disable_v2_behavior()\n\n    args = flags.FLAGS\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n    if args.env in rvr.utils.config.SC2_MINIGAMES_ALIASES:\n        args.env = rvr.utils.config.SC2_MINIGAMES_ALIASES[args.env]\n\n    if args.test:\n        args.n_envs = 1\n        args.log_freq = 1\n        args.restore = True\n\n    expt = rvr.utils.Experiment(args.results_dir, args.env, args.agent, args.experiment, args.restore)\n\n    gin_files = rvr.utils.find_configs(args.env, os.path.dirname(os.path.abspath(__file__)))\n    if args.restore:\n        gin_files += [expt.config_path]\n    gin_files += args.gin_files\n\n    if not args.gpu:\n        args.gin_bindings.append(""build_cnn_nature.data_format = \'channels_last\'"")\n        args.gin_bindings.append(""build_fully_conv.data_format = \'channels_last\'"")\n\n    gin.parse_config_files_and_bindings(gin_files, args.gin_bindings)\n    args.n_envs = min(args.n_envs, gin.query_parameter(\'ACAgent.batch_sz\'))\n\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    sess_mgr = rvr.utils.tensorflow.SessionManager(sess, expt.path, args.ckpt_freq, training_enabled=not args.test)\n\n    env_cls = rvr.envs.GymEnv if \'-v\' in args.env else rvr.envs.SC2Env\n    env = env_cls(args.env, args.render, max_ep_len=args.max_ep_len)\n\n    agent = rvr.agents.registry[args.agent](env.obs_spec(), env.act_spec(), sess_mgr=sess_mgr, n_envs=args.n_envs)\n    agent.logger = rvr.utils.StreamLogger(args.n_envs, args.log_freq, args.log_eps_avg, sess_mgr, expt.log_path)\n\n    if sess_mgr.training_enabled:\n        expt.save_gin_config()\n        expt.save_model_summary(agent.model)\n\n    agent.run(env, args.n_updates * agent.traj_len * agent.batch_sz // args.n_envs)\n\n\nif __name__ == \'__main__\':\n    flags.mark_flag_as_required(\'env\')\n    app.run(main)\n'"
tests/test_actor_critic.py,0,"b'import unittest\nimport numpy as np\nimport reaver as rvr\n\n\nclass TestActorCritic(unittest.TestCase):\n    def test_discounted_cumsum(self):\n        discount = 0.99\n        bootstrap = 5.0\n        dones = np.array([0, 0, 0])\n        rewards = np.array([1.0, 1.0, 1.0])\n\n        discounts = discount * (1-dones)\n        rewards = np.append(rewards, bootstrap)\n\n        result = rvr.agents.A2C.discounted_cumsum(rewards, discounts)\n\n        # 1.0 + 0.99*5.0 = 5.95\n        # 1.0 + 0.99*1.0 + 0.99^2*5.0 = 6.8905\n        # 1.0 + 0.99*1.0 + 0.99^2*1.0 + 0.99^3*5.0 = 7.821595\n        expected = [7.821595, 6.8905, 5.95, 5.0]\n\n        self.assertAlmostEqual(result.tolist(), expected)\n\n    def test_discounted_cumsum_terminals(self):\n        discount = 0.99\n        bootstrap = 5.0\n        dones = np.array([0, 1, 0])\n        rewards = np.array([1.0, 1.0, 1.0])\n\n        discounts = discount * (1-dones)\n        rewards = np.append(rewards, bootstrap)\n\n        result = rvr.agents.A2C.discounted_cumsum(rewards, discounts)\n\n        # 1.0 + 0.99*5.0 = 5.95\n        # 1.0 + 0 * (0.99*1.0 + 0.99^2*5.0) = 1.0\n        # 1.0 + 0.99*1.0 + 0 * (0.99^2*1.0 + 0.99^3*5.0) = 1.99\n        expected = [1.99, 1.0, 5.95, 5.0]\n\n        self.assertAlmostEqual(result.tolist(), expected)\n'"
tests/test_convergence.py,4,"b'import os\nimport warnings\nimport unittest\nimport numpy as np\nimport tensorflow as tf\nimport reaver as rvr\n\n\nclass TestConvergence(unittest.TestCase):\n    def setUp(self):\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        os.environ[""CUDA_VISIBLE_DEVICES""] = \'\'\n        # mute annoying deprecation warning spam by TensorFlow\n        # see https://github.com/tensorflow/tensorflow/issues/16152\n        warnings.filterwarnings(""ignore"", category=DeprecationWarning, module=""tensorflow"")\n\n        self.seed = 1234\n        self.env = rvr.envs.GymEnv(\'CartPole-v0\')\n\n    def test_a2c(self):\n        self._test_agent(rvr.agents.A2C)\n\n    def test_ppo(self):\n        self._test_agent(rvr.agents.PPO, n_epochs=3, minibatch_sz=128)\n\n    def _test_agent(self, agent_cls, **kwargs):\n        _kwargs = dict(optimizer=tf.train.AdamOptimizer(learning_rate=0.0007), entropy_coef=0.1, batch_sz=32,\n                       gae_lambda=0.0, clip_grads_norm=0.0, clip_rewards=1.0, normalize_advantages=False, **kwargs)\n\n        tf.reset_default_graph()\n        tf.set_random_seed(self.seed)\n\n        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, device_count={\'GPU\': 0}))\n        sess_mgr = rvr.utils.tensorflow.SessionManager(sess, base_path=\'/tmp/results/\', checkpoint_freq=None)\n\n        agent = agent_cls(self.env.obs_spec(), self.env.act_spec(), self._model_builder,\n                          rvr.models.MultiPolicy, sess_mgr, n_envs=4, **_kwargs)\n        agent.logger = rvr.utils.StreamLogger(n_envs=4, sess_mgr=sess_mgr)\n        agent.logger.streams = []\n\n        self.env._env.seed(self.seed)\n        agent.run(self.env, 100 * agent.traj_len * agent.batch_sz // agent.n_envs)\n\n        ep_rews = np.array(agent.logger.ep_rews_sum or [0])\n\n        self.assertGreaterEqual(ep_rews.max(), 200.0)\n        self.assertGreaterEqual(ep_rews.mean() + ep_rews.std(), 160.0)\n\n    @staticmethod\n    def _model_builder(obs_spec, act_spec):\n        return rvr.models.build_mlp(obs_spec, act_spec, value_separate=True, obs_shift=True, obs_scale=True)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
reaver/agents/__init__.py,0,"b""from .base import *\nfrom .random import RandomAgent\nfrom .a2c import AdvantageActorCriticAgent\nfrom .ppo import ProximalPolicyOptimizationAgent\n\nA2C = AdvantageActorCriticAgent\nPPO = ProximalPolicyOptimizationAgent\n\nregistry = {\n    'a2c': A2C,\n    'ppo': PPO\n}\n"""
reaver/agents/a2c.py,6,"b'import gin.tf\nimport tensorflow.compat.v1 as tf\n\nfrom reaver.envs.base import Spec\nfrom reaver.utils import StreamLogger\nfrom reaver.utils.tensorflow import SessionManager\nfrom reaver.utils.typing import ModelBuilder, PolicyType\nfrom reaver.agents.base import SyncRunningAgent, ActorCriticAgent, DEFAULTS\n\n\n@gin.configurable(\'A2CAgent\')\nclass AdvantageActorCriticAgent(SyncRunningAgent, ActorCriticAgent):\n    """"""\n    A2C: a synchronous version of Asynchronous Advantage Actor Critic (A3C)\n    See article for more details: https://arxiv.org/abs/1602.01783\n    """"""\n    def __init__(\n        self,\n        obs_spec: Spec,\n        act_spec: Spec,\n        model_fn: ModelBuilder=None,\n        policy_cls: PolicyType=None,\n        sess_mgr: SessionManager=None,\n        optimizer: tf.train.Optimizer=None,\n        n_envs=4,\n        value_coef=DEFAULTS[\'value_coef\'],\n        entropy_coef=DEFAULTS[\'entropy_coef\'],\n        traj_len=DEFAULTS[\'traj_len\'],\n        batch_sz=DEFAULTS[\'batch_sz\'],\n        discount=DEFAULTS[\'discount\'],\n        gae_lambda=DEFAULTS[\'gae_lambda\'],\n        clip_rewards=DEFAULTS[\'clip_rewards\'],\n        clip_grads_norm=DEFAULTS[\'clip_grads_norm\'],\n        normalize_returns=DEFAULTS[\'normalize_returns\'],\n        normalize_advantages=DEFAULTS[\'normalize_advantages\'],\n    ):\n        kwargs = {k: v for k, v in locals().items() if k in DEFAULTS and DEFAULTS[k] != v}\n\n        SyncRunningAgent.__init__(self, n_envs)\n        ActorCriticAgent.__init__(self, obs_spec, act_spec, sess_mgr=sess_mgr, **kwargs)\n        self.logger = StreamLogger(n_envs=n_envs, log_freq=10, sess_mgr=self.sess_mgr)\n\n    def loss_fn(self):\n        adv = tf.placeholder(tf.float32, [None], name=""advantages"")\n        returns = tf.placeholder(tf.float32, [None], name=""returns"")\n\n        policy_loss = -tf.reduce_mean(self.policy.logli * adv)\n        value_loss = tf.reduce_mean((self.value - returns)**2) * self.value_coef\n        entropy_loss = tf.reduce_mean(self.policy.entropy) * self.entropy_coef\n        # we want to reduce policy and value errors, and maximize entropy\n        # but since optimizer is minimizing the signs are opposite\n        full_loss = policy_loss + value_loss - entropy_loss\n\n        return full_loss, [policy_loss, value_loss, entropy_loss], [adv, returns]\n'"
reaver/agents/ppo.py,12,"b'import gin.tf\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom reaver.envs.base import Spec\nfrom reaver.utils import StreamLogger\nfrom reaver.utils.tensorflow import SessionManager\nfrom reaver.utils.typing import ModelBuilder, PolicyType\nfrom reaver.agents.base import SyncRunningAgent, ActorCriticAgent, DEFAULTS\n\n\n@gin.configurable(\'PPOAgent\')\nclass ProximalPolicyOptimizationAgent(SyncRunningAgent, ActorCriticAgent):\n    """"""\n    PPO: clipped version of the Proximal Policy Optimization algorithm\n\n    Here ""clipped"" refers to how trusted policy region is enforced.\n    While orig. PPO relied on KL divergence, this clips the pi / pi_old ratio.\n\n    See article for more details: https://arxiv.org/abs/1707.06347\n\n    PPO specific parameters:\n\n    :param n_epochs: number of times optimizer goes through full batch_sz*traj_len set\n    :param minibatch_sz: size of the randomly sampled minibatch\n    :param clip_ratio: max interval for pi / pi_old: [1-clip_ratio, 1+clip_ratio]\n    :param clip_value: max interval for new value error: [old_value-clip_value, old_value+clip_value]\n    """"""\n    def __init__(\n        self,\n        obs_spec: Spec,\n        act_spec: Spec,\n        model_fn: ModelBuilder=None,\n        policy_cls: PolicyType=None,\n        sess_mgr: SessionManager=None,\n        optimizer: tf.train.Optimizer=None,\n        n_envs=4,\n        n_epochs=3,\n        minibatch_sz=128,\n        clip_ratio=0.2,\n        clip_value=0.5,\n        value_coef=DEFAULTS[\'value_coef\'],\n        entropy_coef=DEFAULTS[\'entropy_coef\'],\n        traj_len=DEFAULTS[\'traj_len\'],\n        batch_sz=DEFAULTS[\'batch_sz\'],\n        discount=DEFAULTS[\'discount\'],\n        gae_lambda=DEFAULTS[\'gae_lambda\'],\n        clip_rewards=DEFAULTS[\'clip_rewards\'],\n        clip_grads_norm=DEFAULTS[\'clip_grads_norm\'],\n        normalize_returns=DEFAULTS[\'normalize_returns\'],\n        normalize_advantages=DEFAULTS[\'normalize_advantages\'],\n    ):\n        kwargs = {k: v for k, v in locals().items() if k in DEFAULTS and DEFAULTS[k] != v}\n\n        self.n_epochs = n_epochs\n        self.minibatch_sz = minibatch_sz\n        self.clip_ratio = clip_ratio\n        self.clip_value = clip_value\n\n        SyncRunningAgent.__init__(self, n_envs)\n        ActorCriticAgent.__init__(self, obs_spec, act_spec, sess_mgr=sess_mgr, **kwargs)\n        self.logger = StreamLogger(n_envs=n_envs, log_freq=10, sess_mgr=self.sess_mgr)\n\n        self.start_step = self.start_step // self.n_epochs\n\n    def minimize(self, advantages, returns):\n        inputs = [a.reshape(-1, *a.shape[2:]) for a in self.obs + self.acts]\n        tf_inputs = self.model.inputs + self.policy.inputs\n        logli_old = self.sess_mgr.run(self.policy.logli, tf_inputs, inputs)\n\n        inputs += [advantages.flatten(), returns.flatten(), logli_old, self.values.flatten()]\n        tf_inputs += self.loss_inputs\n\n        # TODO: rewrite this with persistent tensors to load data only once into the graph\n        loss_terms = grads_norm = None\n        n_samples = self.traj_len * self.batch_sz\n        indices = np.arange(n_samples)\n        for _ in range(self.n_epochs):\n            np.random.shuffle(indices)\n            for i in range(n_samples // self.minibatch_sz):\n                idxs, idxe = i*self.minibatch_sz, (i+1)*self.minibatch_sz\n                minibatch = [inpt[indices[idxs:idxe]] for inpt in inputs]\n                loss_terms, grads_norm, *_ = self.sess_mgr.run(self.minimize_ops, tf_inputs, minibatch)\n\n        return loss_terms, grads_norm\n\n    def loss_fn(self):\n        adv = tf.placeholder(tf.float32, [None], name=""advantages"")\n        returns = tf.placeholder(tf.float32, [None], name=""returns"")\n        logli_old = tf.placeholder(tf.float32, [None], name=""logli_old"")\n        value_old = tf.placeholder(tf.float32, [None], name=""value_old"")\n\n        ratio = tf.exp(self.policy.logli - logli_old)\n        clipped_ratio = tf.clip_by_value(ratio, 1-self.clip_ratio, 1+self.clip_ratio)\n\n        value_err = (self.value - returns)**2\n        if self.clip_value > 0.0:\n            clipped_value = tf.clip_by_value(self.value, value_old-self.clip_value, value_old+self.clip_value)\n            clipped_value_err = (clipped_value - returns)**2\n            value_err = tf.maximum(value_err, clipped_value_err)\n\n        policy_loss = -tf.reduce_mean(tf.minimum(adv * ratio, adv * clipped_ratio))\n        value_loss = tf.reduce_mean(value_err) * self.value_coef\n        entropy_loss = tf.reduce_mean(self.policy.entropy) * self.entropy_coef\n        # we want to reduce policy and value errors, and maximize entropy\n        # but since optimizer is minimizing the signs are opposite\n        full_loss = policy_loss + value_loss - entropy_loss\n\n        return full_loss, [policy_loss, value_loss, entropy_loss], [adv, returns, logli_old, value_old]\n'"
reaver/agents/random.py,0,"b'import numpy as np\nfrom reaver.agents.base import SyncRunningAgent\n\n\nclass RandomAgent(SyncRunningAgent):\n    def __init__(self, act_spec, n_envs):\n        super().__init__(n_envs)\n        self.act_spec = act_spec\n\n    def get_action(self, obs):\n        function_id = [np.random.choice(np.argwhere(obs[2][i] > 0).flatten()) for i in range(self.n_envs)]\n        args = [[[np.random.randint(0, size) for size in arg.shape] for _ in range(self.n_envs)]\n                for arg in self.act_spec.spaces[1:]]\n        return [function_id] + args\n'"
reaver/envs/__init__.py,0,"b'from .base import Env, Space, Spec\nfrom .sc2 import SC2Env\n\nimport importlib\nif importlib.util.find_spec(""gym"") is not None:\n    from .gym import GymEnv\n'"
reaver/envs/atari.py,0,"b'# ########################################################################################## #\n# Copied from https://github.com/google/dopamine/blob/master/dopamine/atari/preprocessing.py #\n# Changes (minimal): added proxy close method, reformatted, removed some unused pieces       #\n# ########################################################################################## #\n\n# coding=utf-8\n# Copyright 2018 The Dopamine Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A class implementing minimal Atari 2600 preprocessing.\n\nThis includes:\n  . Emitting a terminal signal when losing a life (optional).\n  . Frame skipping and color pooling.\n  . Resizing the image before it is provided to the agent.\n""""""\n\nimport cv2\nimport gin.tf\nimport numpy as np\nfrom gym.spaces.box import Box\n\n\n@gin.configurable\nclass AtariPreprocessing(object):\n    """"""A class implementing image preprocessing for Atari 2600 agents.\n\n    Specifically, this provides the following subset from the JAIR paper\n    (Bellemare et al., 2013) and Nature DQN paper (Mnih et al., 2015):\n\n      * Frame skipping (defaults to 4).\n      * Terminal signal when a life is lost (off by default).\n      * Grayscale and max-pooling of the last two frames.\n      * Downsample the screen to a square image (defaults to 84x84).\n\n    More generally, this class follows the preprocessing guidelines set down in\n    Machado et al. (2018), ""Revisiting the Arcade Learning Environment:\n    Evaluation Protocols and Open Problems for General Agents"".\n    """"""\n\n    def __init__(self, environment, frame_skip=4, terminal_on_life_loss=False,\n                 screen_size=84):\n        """"""Constructor for an Atari 2600 preprocessor.\n\n        Args:\n          environment: Gym environment whose observations are preprocessed.\n          frame_skip: int, the frequency at which the agent experiences the game.\n          terminal_on_life_loss: bool, If True, the step() method returns\n            is_terminal=True whenever a life is lost. See Mnih et al. 2015.\n          screen_size: int, size of a resized Atari 2600 frame.\n\n        Raises:\n          ValueError: if frame_skip or screen_size are not strictly positive.\n        """"""\n        if frame_skip <= 0:\n            raise ValueError(\'Frame skip should be strictly positive, got {}\'.\n                             format(frame_skip))\n        if screen_size <= 0:\n            raise ValueError(\'Target screen size should be strictly positive, got {}\'.\n                             format(screen_size))\n\n        self.environment = environment\n        self.terminal_on_life_loss = terminal_on_life_loss\n        self.frame_skip = frame_skip\n        self.screen_size = screen_size\n\n        obs_dims = self.environment.observation_space\n        # Stores temporary observations used for pooling over two successive\n        # frames.\n        self.screen_buffer = [\n            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),\n            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)\n        ]\n\n        self.game_over = False\n        self.lives = 0  # Will need to be set by reset().\n\n    @property\n    def observation_space(self):\n        # Return the observation space adjusted to match the shape of the processed\n        # observations.\n        return Box(low=0, high=255, shape=(self.screen_size, self.screen_size, 1),\n                   dtype=np.uint8)\n\n    @property\n    def action_space(self):\n        return self.environment.action_space\n\n    @property\n    def reward_range(self):\n        return self.environment.reward_range\n\n    @property\n    def metadata(self):\n        return self.environment.metadata\n\n    def reset(self):\n        """"""Resets the environment.\n\n        Returns:\n          observation: numpy array, the initial observation emitted by the\n            environment.\n        """"""\n        self.environment.reset()\n        self.lives = self.environment.ale.lives()\n        self._fetch_grayscale_observation(self.screen_buffer[0])\n        self.screen_buffer[1].fill(0)\n        return self._pool_and_resize()\n\n    def render(self, mode):\n        """"""Renders the current screen, before preprocessing.\n\n        This calls the Gym API\'s render() method.\n\n        Args:\n          mode: Mode argument for the environment\'s render() method.\n            Valid values (str) are:\n              \'rgb_array\': returns the raw ALE image.\n              \'human\': renders to display via the Gym renderer.\n\n        Returns:\n          if mode=\'rgb_array\': numpy array, the most recent screen.\n          if mode=\'human\': bool, whether the rendering was successful.\n        """"""\n        return self.environment.render(mode)\n\n    def step(self, action):\n        """"""Applies the given action in the environment.\n\n        Remarks:\n\n          * If a terminal state (from life loss or episode end) is reached, this may\n            execute fewer than self.frame_skip steps in the environment.\n          * Furthermore, in this case the returned observation may not contain valid\n            image data and should be ignored.\n\n        Args:\n          action: The action to be executed.\n\n        Returns:\n          observation: numpy array, the observation following the action.\n          reward: float, the reward following the action.\n          is_terminal: bool, whether the environment has reached a terminal state.\n            This is true when a life is lost and terminal_on_life_loss, or when the\n            episode is over.\n          info: Gym API\'s info data structure.\n        """"""\n        accumulated_reward = 0.\n\n        for time_step in range(self.frame_skip):\n            # We bypass the Gym observation altogether and directly fetch the\n            # grayscale image from the ALE. This is a little faster.\n            _, reward, game_over, info = self.environment.step(action)\n            accumulated_reward += reward\n\n            if self.terminal_on_life_loss:\n                new_lives = self.environment.ale.lives()\n                is_terminal = game_over or new_lives < self.lives\n                self.lives = new_lives\n            else:\n                is_terminal = game_over\n\n            if is_terminal:\n                break\n            # We max-pool over the last two frames, in grayscale.\n            elif time_step >= self.frame_skip - 2:\n                t = time_step - (self.frame_skip - 2)\n                self._fetch_grayscale_observation(self.screen_buffer[t])\n\n        # Pool the last two observations.\n        observation = self._pool_and_resize()\n\n        self.game_over = game_over\n        return observation, accumulated_reward, is_terminal, info\n\n    def close(self):\n        self.environment.close()\n\n    def _fetch_grayscale_observation(self, output):\n        """"""Returns the current observation in grayscale.\n\n        The returned observation is stored in \'output\'.\n\n        Args:\n          output: numpy array, screen buffer to hold the returned observation.\n\n        Returns:\n          observation: numpy array, the current observation in grayscale.\n        """"""\n        self.environment.ale.getScreenGrayscale(output)\n        return output\n\n    def _pool_and_resize(self):\n        """"""Transforms two frames into a Nature DQN observation.\n\n        For efficiency, the transformation is done in-place in self.screen_buffer.\n\n        Returns:\n          transformed_screen: numpy array, pooled, resized screen.\n        """"""\n        # Pool if there are enough screens to do so.\n        if self.frame_skip > 1:\n            np.maximum(self.screen_buffer[0], self.screen_buffer[1],\n                       out=self.screen_buffer[0])\n\n        transformed_image = cv2.resize(self.screen_buffer[0],\n                                       (self.screen_size, self.screen_size),\n                                       interpolation=cv2.INTER_AREA)\n        int_image = np.asarray(transformed_image, dtype=np.uint8)\n        return np.expand_dims(int_image, axis=2)\n'"
reaver/envs/gym.py,0,"b'import numpy as np\nfrom . import Env, Spec, Space\n\n\nclass GymEnv(Env):\n    def __init__(self, _id=\'CartPole-v0\', render=False, reset_done=True, max_ep_len=None):\n        super().__init__(_id, render, reset_done, max_ep_len)\n\n        self._env = None\n        self.specs = None\n        self.ep_step = 0\n\n    def start(self):\n        import gym  # lazy-loading\n        gym.logger.set_level(40)  # avoid annoying internal warn messages\n\n        self._env = gym.make(self.id)\n\n        try:\n            import atari_py\n            from reaver.envs.atari import AtariPreprocessing\n        except ImportError:\n            return\n\n        if any([env_name in self.id.lower() for env_name in atari_py.list_games()]):\n            self._env = AtariPreprocessing(self._env.env)\n\n        self.make_specs(running=True)\n\n    def step(self, action):\n        obs, reward, done, _ = self._env.step(self.wrap_act(action))\n\n        self.ep_step += 1\n        if self.ep_step >= self.max_ep_len:\n            done = 1\n\n        if done and self.reset_done:\n            obs = self.reset(wrap=False)\n\n        obs = self.wrap_obs(obs)\n\n        if self.render:\n            self._env.render()\n\n        # TODO what if reward is a float?\n        return obs, int(reward), int(done)\n\n    def reset(self, wrap=True):\n        obs = self._env.reset()\n\n        if wrap:\n            obs = self.wrap_obs(obs)\n\n        if self.render:\n            self._env.render()\n\n        self.ep_step = 0\n\n        return obs\n\n    def stop(self):\n        self._env.close()\n\n    def wrap_act(self, act):\n        if len(self.act_spec().spaces) == 1:\n            act = act[0]\n        return act\n\n    def wrap_obs(self, obs):\n        if len(self.obs_spec().spaces) == 1:\n            obs = [obs]\n        # can\'t trust gym space definitions it seems...\n        obs = [ob.astype(sp.dtype) for ob, sp in zip(obs, self.obs_spec().spaces)]\n        return obs\n\n    def obs_spec(self):\n        if not self.specs:\n            self.make_specs()\n        return self.specs[\'obs\']\n\n    def act_spec(self):\n        if not self.specs:\n            self.make_specs()\n        return self.specs[\'act\']\n\n    def make_specs(self, running=False):\n        render, self.render = self.render, False\n        if not running:\n            self.start()\n        self.specs = {\n            \'obs\': Spec(parse(self._env.observation_space), \'Observation\'),\n            \'act\': Spec(parse(self._env.action_space), \'Action\')\n        }\n        if not running:\n            self.stop()\n        self.render = render\n\n\ndef parse(gym_space, name=None):\n    cls_name = type(gym_space).__name__\n\n    if cls_name == \'Discrete\':\n        return [Space(dtype=gym_space.dtype, domain=(0, gym_space.n), categorical=True, name=name)]\n\n    if cls_name == \'Box\':\n        lo, hi = gym_space.low, gym_space.high\n        return [Space(shape=gym_space.shape, dtype=gym_space.dtype, domain=(lo, hi), name=name)]\n\n    if cls_name == \'Tuple\':\n        spaces = []\n        for space in gym_space.spaces:\n            spaces += parse(space)\n        return spaces\n\n    if cls_name == ""Dict"":\n        spaces = []\n        for name, space in gym_space.spaces.items():\n            spaces += parse(space, name)\n        return spaces\n'"
reaver/envs/sc2.py,0,"b'import sys\nimport gin\nimport numpy as np\nfrom absl import flags\nfrom pysc2.lib import actions\nfrom pysc2.lib import features\nfrom pysc2.lib import protocol\nfrom pysc2.env.environment import StepType\nfrom . import Env, Spec, Space\n\nACTIONS_MINIGAMES, ACTIONS_MINIGAMES_ALL, ACTIONS_ALL = [\'minigames\', \'minigames_all\', \'all\']\n\n\n@gin.configurable\nclass SC2Env(Env):\n    """"""\n    \'minigames\' action set is enough to solve all minigames listed in SC2LE\n    \'minigames_all\' expands that set with actions that may improve end results, but will drop performance\n    \'all\' is the full action set, only necessary for generic agent playing full game with all three races\n\n    You can also specify your own action set in the gin config file under SC2Env.action_ids\n    Full list of available actions https://github.com/deepmind/pysc2/blob/master/pysc2/lib/actions.py#L447-L1008\n    """"""\n    def __init__(\n        self,\n        map_name=\'MoveToBeacon\',\n        render=False,\n        reset_done=True,\n        max_ep_len=None,\n        spatial_dim=16,\n        step_mul=8,\n        obs_features=None,\n        action_ids=ACTIONS_MINIGAMES\n    ):\n        super().__init__(map_name, render, reset_done, max_ep_len)\n\n        self.step_mul = step_mul\n        self.spatial_dim = spatial_dim\n        self._env = None\n\n        # sensible action set for all minigames\n        if not action_ids or action_ids in [ACTIONS_MINIGAMES, ACTIONS_MINIGAMES_ALL]:\n            action_ids = [0, 1, 2, 3, 4, 6, 7, 12, 13, 42, 44, 50, 91, 183, 234, 309, 331, 332, 333, 334, 451, 452, 490]\n\n        # some additional actions for minigames (not necessary to solve)\n        if action_ids == ACTIONS_MINIGAMES_ALL:\n            action_ids += [11, 71, 72, 73, 74, 79, 140, 168, 239, 261, 264, 269, 274, 318, 335, 336, 453, 477]\n\n        # full action space, including outdated / unusable to current race / usable only in certain cases\n        if action_ids == ACTIONS_ALL:\n            action_ids = [f.id for f in actions.FUNCTIONS]\n\n        # by default use majority of obs features, except for some that are unnecessary for minigames\n        # e.g. race-specific like creep and shields or redundant like player_id\n        if not obs_features:\n            obs_features = {\n                \'screen\': [\'player_relative\', \'selected\', \'visibility_map\', \'unit_hit_points_ratio\', \'unit_density\'],\n                \'minimap\': [\'player_relative\', \'selected\', \'visibility_map\', \'camera\'],\n                # available actions should always be present and in first position\n                \'non-spatial\': [\'available_actions\', \'player\']}\n\n        self.act_wrapper = ActionWrapper(spatial_dim, action_ids)\n        self.obs_wrapper = ObservationWrapper(obs_features, action_ids)\n\n    def start(self):\n        # importing here to lazy-load\n        from pysc2.env import sc2_env\n\n        # fail-safe if executed not as absl app\n        if not flags.FLAGS.is_parsed():\n            flags.FLAGS(sys.argv)\n\n        self._env = sc2_env.SC2Env(\n            map_name=self.id,\n            visualize=self.render,\n            agent_interface_format=[features.parse_agent_interface_format(\n                feature_screen=self.spatial_dim,\n                feature_minimap=self.spatial_dim,\n                rgb_screen=None,\n                rgb_minimap=None\n            )],\n            step_mul=self.step_mul,\n            players=[sc2_env.Agent(sc2_env.Race.terran)])\n\n    def step(self, action):\n        try:\n            obs, reward, done = self.obs_wrapper(self._env.step(self.act_wrapper(action)))\n        except protocol.ConnectionError:\n            # hacky fix from websocket timeout issue...\n            # this results in faulty reward signals, but I guess it beats completely crashing...\n            self.restart()\n            return self.reset(), 0, 1\n\n        if done and self.reset_done:\n            obs = self.reset()\n\n        return obs, reward, done\n\n    def reset(self):\n        try:\n            obs, reward, done = self.obs_wrapper(self._env.reset())\n        except protocol.ConnectionError:\n            # hacky fix from websocket timeout issue...\n            # this results in faulty reward signals, but I guess it beats completely crashing...\n            self.restart()\n            return self.reset()\n\n        return obs\n\n    def stop(self):\n        self._env.close()\n\n    def restart(self):\n        self.stop()\n        self.start()\n\n    def obs_spec(self):\n        if not self.obs_wrapper.spec:\n            self.make_specs()\n        return self.obs_wrapper.spec\n\n    def act_spec(self):\n        if not self.act_wrapper.spec:\n            self.make_specs()\n        return self.act_wrapper.spec\n\n    def make_specs(self):\n        # importing here to lazy-load\n        from pysc2.env import mock_sc2_env\n        mock_env = mock_sc2_env.SC2TestEnv(map_name=self.id, agent_interface_format=[\n            features.parse_agent_interface_format(feature_screen=self.spatial_dim, feature_minimap=self.spatial_dim)])\n        self.act_wrapper.make_spec(mock_env.action_spec())\n        self.obs_wrapper.make_spec(mock_env.observation_spec())\n        mock_env.close()\n\n\nclass ObservationWrapper:\n    def __init__(self, _features=None, action_ids=None):\n        self.spec = None\n        self.features = _features\n        self.action_ids = action_ids\n\n        screen_feature_to_idx = {feat: idx for idx, feat in enumerate(features.SCREEN_FEATURES._fields)}\n        minimap_feature_to_idx = {feat: idx for idx, feat in enumerate(features.MINIMAP_FEATURES._fields)}\n\n        self.feature_masks = {\n            \'screen\': [screen_feature_to_idx[f] for f in _features[\'screen\']],\n            \'minimap\': [minimap_feature_to_idx[f] for f in _features[\'minimap\']]\n        }\n\n    def __call__(self, timestep):\n        ts = timestep[0]\n        obs, reward, done = ts.observation, ts.reward, ts.step_type == StepType.LAST\n\n        obs_wrapped = [\n            obs[\'feature_screen\'][self.feature_masks[\'screen\']],\n            obs[\'feature_minimap\'][self.feature_masks[\'minimap\']]\n        ]\n        for feat_name in self.features[\'non-spatial\']:\n            if feat_name == \'available_actions\':\n                fn_ids_idxs = [i for i, fn_id in enumerate(self.action_ids) if fn_id in obs[feat_name]]\n                mask = np.zeros((len(self.action_ids),), dtype=np.int32)\n                mask[fn_ids_idxs] = 1\n                obs[feat_name] = mask\n            obs_wrapped.append(obs[feat_name])\n\n        return obs_wrapped, reward, done\n\n    def make_spec(self, spec):\n        spec = spec[0]\n\n        default_dims = {\n            \'available_actions\': (len(self.action_ids), ),\n        }\n\n        screen_shape = (len(self.features[\'screen\']), *spec[\'feature_screen\'][1:])\n        minimap_shape = (len(self.features[\'minimap\']), *spec[\'feature_minimap\'][1:])\n        screen_dims = get_spatial_dims(self.features[\'screen\'], features.SCREEN_FEATURES)\n        minimap_dims = get_spatial_dims(self.features[\'minimap\'], features.MINIMAP_FEATURES)\n\n        spaces = [\n            SC2Space(screen_shape, \'screen\', self.features[\'screen\'], screen_dims),\n            SC2Space(minimap_shape, \'minimap\', self.features[\'minimap\'], minimap_dims),\n        ]\n\n        for feat in self.features[\'non-spatial\']:\n            if 0 in spec[feat]:\n                spec[feat] = default_dims[feat]\n            spaces.append(Space(spec[feat], name=feat))\n\n        self.spec = Spec(spaces, \'Observation\')\n\n\nclass ActionWrapper:\n    def __init__(self, spatial_dim, action_ids, args=None):\n        self.spec = None\n        if not args:\n            args = [\n                \'screen\',\n                \'minimap\',\n                \'screen2\',\n                \'queued\',\n                \'control_group_act\',\n                \'control_group_id\',\n                \'select_add\',\n                \'select_point_act\',\n                \'select_unit_act\',\n                # \'select_unit_id\'\n                \'select_worker\',\n                \'build_queue_id\',\n                # \'unload_id\'\n            ]\n        self.func_ids = action_ids\n        self.args, self.spatial_dim = args, spatial_dim\n\n    def __call__(self, action):\n        defaults = {\n            \'control_group_act\': 0,\n            \'control_group_id\': 0,\n            \'select_point_act\': 0,\n            \'select_unit_act\': 0,\n            \'select_unit_id\': 0,\n            \'build_queue_id\': 0,\n            \'unload_id\': 0,\n        }\n        fn_id_idx, args = action.pop(0), []\n        fn_id = self.func_ids[fn_id_idx]\n        for arg_type in actions.FUNCTIONS[fn_id].args:\n            arg_name = arg_type.name\n            if arg_name in self.args:\n                arg = action[self.args.index(arg_name)]\n                # pysc2 expects all args in their separate lists\n                if type(arg) not in [list, tuple]:\n                    arg = [arg]\n                # pysc2 expects spatial coords, but we have flattened => attempt to fix\n                if len(arg_type.sizes) > 1 and len(arg) == 1:\n                    arg = [arg[0] % self.spatial_dim, arg[0] // self.spatial_dim]\n                args.append(arg)\n            else:\n                args.append([defaults[arg_name]])\n\n        return [actions.FunctionCall(fn_id, args)]\n\n    def make_spec(self, spec):\n        spec = spec[0]\n\n        spaces = [SC2FuncIdSpace(self.func_ids, self.args)]\n        for arg_name in self.args:\n            arg = getattr(spec.types, arg_name)\n            if len(arg.sizes) > 1:\n                spaces.append(Space(domain=(0, arg.sizes), categorical=True, name=arg_name))\n            else:\n                spaces.append(Space(domain=(0, arg.sizes[0]), categorical=True, name=arg_name))\n\n        self.spec = Spec(spaces, ""Action"")\n\n\nclass SC2Space(Space):\n    def __init__(self, shape, name, spatial_feats=None, spatial_dims=None):\n        if spatial_feats:\n            name += ""{%s}"" % "", "".join(spatial_feats)\n        self.spatial_feats, self.spatial_dims = spatial_feats, spatial_dims\n\n        super().__init__(shape, name=name)\n\n\nclass SC2FuncIdSpace(Space):\n    def __init__(self, func_ids, args):\n        super().__init__(domain=(0, len(func_ids)), categorical=True, name=""function_id"")\n        self.args_mask = []\n        for fn_id in func_ids:\n            fn_id_args = [arg_type.name for arg_type in actions.FUNCTIONS[fn_id].args]\n            self.args_mask.append([arg in fn_id_args for arg in args])\n\n\ndef get_spatial_dims(feat_names, feats):\n    feats_dims = []\n    for feat_name in feat_names:\n        feat = getattr(feats, feat_name)\n        feats_dims.append(1)\n        if feat.type == features.FeatureType.CATEGORICAL:\n            feats_dims[-1] = feat.scale\n    return feats_dims\n'"
reaver/models/__init__.py,0,"b'from reaver.models.base import layers, build_mlp, build_cnn_nature, MultiPolicy\nfrom reaver.models.sc2 import build_fully_conv, SC2MultiPolicy\n'"
reaver/utils/__init__.py,0,"b'import reaver.utils.tensorflow\nfrom reaver.utils.config import find_configs\nfrom reaver.utils.experiment import Experiment\nfrom reaver.utils.logger import Logger, StreamLogger'"
reaver/utils/config.py,0,"b""import os\n\nSC2_MINIGAMES_ALIASES = {\n    'beacon': 'MoveToBeacon',\n    'shards': 'CollectMineralShards',\n    'roaches': 'DefeatRoaches',\n    'blings': 'DefeatZerglingsAndBanelings',\n    'lings': 'FindAndDefeatZerglings',\n    'minerals': 'CollectMineralsAndGas',\n    'marines': 'BuildMarines',\n}\n\nGYM_CONTINUOUS = [\n    'LunarLanderContinuous-v2',\n    'BipedalWalker-v2',\n    'CarRacing-v0',\n    'MountainCarContinuous-v0',\n    'Pendulum-v0',\n    'Acrobot-v1'\n]\n\nMUJOCO_ENVS = [\n    'Reacher-v2',\n    'Pusher-v2',\n    'Thrower-v2',\n    'Striker-v2',\n    'InvertedPendulum-v2',\n    'InvertedDoublePendulum-v2',\n    'HalfCheetah-v2',\n    'Hopper-v2',\n    'Swimmer-v2',\n    'Walker2d-v2',\n    'Ant-v2',\n    'Humanoid-v2',\n    'HumanoidStandup-v2'\n]\n\nATARI_GAMES = list(map(lambda name: ''.join([g.capitalize() for g in name.split('_')]), [\n    'air_raid', 'alien', 'amidar', 'assault', 'asterix', 'asteroids', 'atlantis',\n    'bank_heist', 'battle_zone', 'beam_rider', 'berzerk', 'bowling', 'boxing', 'breakout',\n    'carnival', 'centipede', 'chopper_command', 'crazy_climber', 'demon_attack', 'double_dunk',\n    'elevator_action', 'enduro', 'fishing_derby', 'freeway', 'frostbite', 'gopher', 'gravitar',\n    'hero', 'ice_hockey', 'jamesbond', 'journey_escape', 'kangaroo', 'krull', 'kung_fu_master',\n    'montezuma_revenge', 'ms_pacman', 'name_this_game', 'phoenix', 'pitfall', 'pong', 'pooyan',\n    'solaris', 'space_invaders', 'star_gunner', 'tennis', 'time_pilot', 'tutankham', 'up_n_down',\n    'private_eye', 'qbert', 'riverraid', 'road_runner', 'robotank', 'seaquest', 'skiing',\n    'venture', 'video_pinball', 'wizard_of_wor', 'yars_revenge', 'zaxxon']))\n\nATARI_ENVS = ['%s-v0' % name for name in ATARI_GAMES]\nATARI_ENVS += ['%s-v4' % name for name in ATARI_GAMES]\nATARI_ENVS += ['%sDeterministic-v0' % name for name in ATARI_GAMES]\nATARI_ENVS += ['%sDeterministic-v4' % name for name in ATARI_GAMES]\nATARI_ENVS += ['%sNoFrameskip-v0' % name for name in ATARI_GAMES]\nATARI_ENVS += ['%sNoFrameskip-v4' % name for name in ATARI_GAMES]\n\n\ndef find_configs(env_name, base_path=''):\n    if '-v' not in env_name:\n        return filter_exists(['sc2/base.gin', 'sc2/%s.gin' % env_name], base_path)\n\n    if env_name in GYM_CONTINUOUS:\n        return filter_exists(['gym/base.gin', 'gym/continuous.gin', 'gym/%s.gin' % env_name], base_path)\n\n    if env_name in MUJOCO_ENVS:\n        return filter_exists(['mujoco/base.gin', 'mujoco/%s.gin' % env_name], base_path)\n\n    if env_name in ATARI_ENVS:\n        return filter_exists(['atari/base.gin', 'atari/%s.gin' % env_name], base_path)\n\n    return filter_exists(['gym/base.gin', 'gym/%s.gin' % env_name], base_path)\n\n\ndef filter_exists(filenames, base_path):\n    full_paths = [os.path.join(base_path, 'configs', fl) for fl in filenames]\n    full_paths = [fl for fl in full_paths if os.path.exists(fl)]\n    return full_paths\n"""
reaver/utils/experiment.py,0,"b'import os\nimport gin\nfrom datetime import datetime as dt\n\n\nclass Experiment:\n    def __init__(self, results_dir, env_name, agent_name, name=None, restore=False):\n        if not name:\n            if restore:\n                experiments = [e for e in os.listdir(results_dir) if env_name in e and agent_name in e]\n                assert len(experiments) > 0, \'No experiment to restore\'\n                name = max(experiments, key=lambda p: os.path.getmtime(results_dir+\'/\'+p))\n                name = \'_\'.join(name.split(\'_\')[2:])\n            else:\n                name = dt.now().strftime(""%y-%m-%d_%H-%M-%S"")\n\n        self.name = name\n        self.restore = restore\n        self.env_name = env_name\n        self.agent_name = agent_name\n        self.results_dir = results_dir\n\n        os.makedirs(self.path, exist_ok=True)\n        os.makedirs(self.path + \'/summaries\', exist_ok=True)\n        os.makedirs(self.results_dir + \'/summaries\', exist_ok=True)\n        if not os.path.exists(self.summaries_path):\n            os.symlink(\'../%s/summaries\' % self.full_name, self.summaries_path)\n\n    @property\n    def full_name(self):\n        return \'%s_%s_%s\' % (self.env_name, self.agent_name, self.name)\n\n    @property\n    def path(self):\n        return \'%s/%s\' % (self.results_dir, self.full_name)\n\n    @property\n    def config_path(self):\n        return \'%s/%s\' % (self.path, \'config.gin\')\n\n    @property\n    def log_path(self):\n        return \'%s/%s\' % (self.path, \'train.log\')\n\n    @property\n    def checkpoints_path(self):\n        return self.path + \'/checkpoints\'\n\n    @property\n    def summaries_path(self):\n        return \'%s/summaries/%s\' % (self.results_dir, self.full_name)\n\n    def save_gin_config(self):\n        with open(self.config_path, \'w\') as cfg_file:\n            cfg_file.write(gin.operative_config_str())\n\n    def save_model_summary(self, model):\n        with open(self.path + \'/\' + \'model_summary.txt\', \'w\') as fl:\n            model.summary(print_fn=lambda line: print(line, file=fl))\n'"
reaver/utils/logger.py,0,"b'import os\nimport sys\nimport time\nimport numpy as np\nfrom collections import deque, namedtuple\n\n\nclass Logger:\n    def on_start(self): ...\n\n    def on_step(self, step, rewards, dones): ...\n\n    def on_update(self, step, loss_terms, grads_norm, returns, adv, next_value): ...\n\n    def on_finish(self): ...\n\n\nclass StreamLogger(Logger):\n    def __init__(self, n_envs, log_freq=100, rew_avg_eps=100, sess_mgr=None, log_file_path=None):\n        self.n_envs = n_envs\n        self.log_freq = log_freq\n        self.rew_avg_eps = rew_avg_eps\n\n        self.env_eps = [0]*n_envs\n        self.env_rews = [0]*n_envs\n        self.ep_rews_sum = deque([], maxlen=self.rew_avg_eps)\n\n        self.run_time = 0\n        self.start_time = None\n\n        self.sess_mgr = sess_mgr\n        self.streams = [sys.stdout]\n        self.log_file_path = None\n        if self.sess_mgr.training_enabled:\n            self.log_file_path = log_file_path\n\n        ColumnParams = namedtuple(""ColumnParams"", [""abbr"", ""width"", ""precision""])\n        self.col_params = dict(\n            runtime=ColumnParams(""T"", 6, 0),\n            frames=ColumnParams(""Fr"", 9, 0),\n            episodes=ColumnParams(""Ep"", 6, 0),\n            updates=ColumnParams(""Up"", 6, 0),\n            ep_rews_mean=ColumnParams(""RMe"", 7, 2),\n            ep_rews_std=ColumnParams(""RSd"", 7, 2),\n            ep_rews_max=ColumnParams(""RMa"", 7, 2),\n            ep_rews_min=ColumnParams(""RMi"", 7, 2),\n            policy_loss=ColumnParams(""Pl"", 8, 3),\n            value_loss=ColumnParams(""Vl"", 8, 3),\n            entropy_loss=ColumnParams(""El"", 6, 4),\n            grads_norm=ColumnParams(""Gr"", 8, 3),\n            frames_per_second=ColumnParams(""Fps"", 5, 0),\n        )\n\n        self.col_fmt = ""| {abbr} {value:{width}.{precision}f} ""\n\n    def on_step(self, step, rewards, dones):\n        self.env_rews += rewards\n        for i in range(self.n_envs):\n            if not dones[i]:\n                continue\n            self.ep_rews_sum.append(self.env_rews[i])\n            self.env_rews[i] = 0\n            self.env_eps[i] += 1\n\n    def on_update(self, step, loss_terms, grads_norm, returns, adv, next_value):\n        if step > 1 and step % self.log_freq:\n            return\n\n        frames = step * np.prod(returns.shape)\n        run_time = max(1, int(time.time() - self.start_time)) + self.run_time\n        ep_rews = np.array(self.ep_rews_sum or [0])\n\n        logs = dict(\n            runtime=run_time,\n            frames=frames,\n            updates=step,\n            episodes=int(np.sum(self.env_eps)),\n            frames_per_second=frames // run_time,\n            ep_rews_mean=ep_rews.mean(),\n            ep_rews_std=ep_rews.std(),\n            ep_rews_max=ep_rews.max(),\n            ep_rews_min=ep_rews.min(),\n            policy_loss=loss_terms[0],\n            value_loss=loss_terms[1],\n            entropy_loss=loss_terms[2],\n            grads_norm=grads_norm,\n        )\n\n        self.stream_logs(logs)\n        if self.sess_mgr:\n            self.summarize_logs(logs)\n\n    def stream_logs(self, logs):\n        log_str = """"\n        for key, params in self.col_params.items():\n            abbr, width, precision = params.abbr, params.width, params.precision\n            log_str += self.col_fmt.format(abbr=abbr, value=logs[key], width=width, precision=precision)\n        log_str += ""|""\n\n        for stream in self.streams:\n            print(log_str, file=stream)\n            stream.flush()\n\n    def summarize_logs(self, logs):\n        losses = [logs[\'policy_loss\'], logs[\'value_loss\'], logs[\'entropy_loss\']]\n        rews = [logs[\'ep_rews_mean\'], logs[\'ep_rews_std\'], logs[\'ep_rews_max\'], logs[\'ep_rews_min\']]\n\n        self.sess_mgr.add_summaries([\'Mean\', \'Std\', \'Max\', \'Min\'], rews, \'Rewards\', logs[\'updates\'])\n        self.sess_mgr.add_summaries([\'Policy\', \'Value\', \'Entropy\'], losses, \'Losses\', logs[\'updates\'])\n        self.sess_mgr.add_summary(\'Grads Norm\', logs[\'grads_norm\'], \'Losses\', logs[\'updates\'])\n\n    def on_start(self):\n        self.start_time = time.time()\n        if not self.log_file_path:\n            return\n\n        self.restore_logs()\n        self.streams.append(open(self.log_file_path, \'a+\'))\n\n    def on_finish(self):\n        if len(self.streams) > 1:\n            self.streams[1].close()\n\n    def restore_logs(self):\n        if not os.path.isfile(self.log_file_path):\n            return\n\n        with open(self.log_file_path, \'r\') as fl:\n            last_line = fl.readlines()[-1]\n        logs = last_line.split("" | "")\n        self.run_time = int(logs[0].split("" "")[-1])\n        self.env_eps.append(int(logs[2].split("" "")[-1]))\n\n\nclass AgentDebugLogger(Logger):\n    def __init__(self, agent, log_freq=100, debug_steps=10):\n        self.agent = agent\n        self.log_freq = log_freq\n        self.debug_steps = debug_steps\n\n    def on_update(self, step, loss_terms, grads_norm, returns, adv, next_value):\n        update_step = (step + 1) // self.agent.traj_len\n        if update_step > 1 and update_step % self.log_freq:\n            return\n\n        np.set_printoptions(suppress=True, precision=2)\n        n_steps = min(self.debug_steps, self.agent.traj_len)\n\n        print()\n        print(""First Env For Last %d Steps:"" % n_steps)\n        print(""Dones      "", self.agent.dones[-n_steps:, 0].flatten().astype(int))\n        print(""Rewards    "", self.agent.rewards[-n_steps:, 0].flatten())\n        print(""Values     "", self.agent.values[-n_steps:, 0].flatten(), round(next_value[0], 3))\n        print(""Returns    "", returns[-n_steps:, 0].flatten())\n        print(""Advs       "", adv[-n_steps:, 0].flatten())\n\n        sys.stdout.flush()\n'"
reaver/utils/plot.py,0,"b'import os\nimport math\nimport logging\nimport argparse\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom itertools import zip_longest\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n\n\ndef plot_from_summaries(summaries_path, title=None, samples_per_update=512, updates_per_log=100):\n    acc = EventAccumulator(summaries_path)\n    acc.Reload()\n\n    rews_mean = np.array([s[2] for s in acc.Scalars(\'Rewards/Mean\')])\n    rews_std = np.array([s[2] for s in acc.Scalars(\'Rewards/Std\')])\n    x = samples_per_update * updates_per_log * np.arange(0, len(rews_mean))\n\n    if not title:\n        title = summaries_path.split(\'/\')[-1].split(\'_\')[0]\n\n    plt.plot(x, rews_mean)\n    plt.fill_between(x, rews_mean - rews_std, rews_mean + rews_std, alpha=0.2)\n    plt.xlabel(\'Samples\')\n    plt.ylabel(\'Episode Rewards\')\n    plt.title(title)\n    plt.xlim([0, x[-1]+1])\n    plt.ticklabel_format(style=\'sci\', axis=\'x\', scilimits=(0, 0))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'logdir\', type=str)\n    parser.add_argument(\'--titles\', nargs=\'*\', default=[])\n    parser.add_argument(\'--samples_per_update\', type=int, default=512)\n    parser.add_argument(\'--updates_per_log\', type=int, default=100)\n    args = parser.parse_args()\n\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n    logging.getLogger(\'tensorflow\').setLevel(logging.ERROR)\n\n    plt.style.use(\'seaborn\')\n    mpl.rcParams[\'figure.figsize\'] = (10, 10)\n\n    paths = [os.path.join(args.logdir, p) for p in os.listdir(args.logdir)]\n\n    assert len(paths) >= len(args.titles), ""More titles than summaries""\n\n    n_plots = len(paths)\n    n_rows = math.ceil(n_plots / 2)\n    for idx, (path, title) in enumerate(zip_longest(paths, args.titles)):\n        if n_plots > 1:\n            plt.subplot(n_rows, 2, 1 + idx)\n        plot_from_summaries(path, title, args.samples_per_update, args.updates_per_log)\n    plt.tight_layout()\n    plt.show()\n'"
reaver/utils/tensorflow.py,15,"b""import gin\nimport tensorflow.compat.v1 as tf\ngin.external_configurable(tf.train.AdamOptimizer, module='tf.train')\ngin.external_configurable(tf.train.RMSPropOptimizer, module='tf.train')\ngin.external_configurable(tf.train.get_global_step, module='tf.train')\ngin.external_configurable(tf.train.piecewise_constant, module='tf.train')\ngin.external_configurable(tf.train.polynomial_decay, module='tf.train')\ngin.external_configurable(tf.initializers.orthogonal, 'tf.initializers.orthogonal')\n\n\nclass SessionManager:\n    def __init__(self, sess=None, base_path='results/', checkpoint_freq=100, training_enabled=True):\n        if not sess:\n            sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n        tf.keras.backend.set_session(sess)\n\n        self.sess = sess\n        self.saver = None\n        self.base_path = base_path\n        self.checkpoint_freq = checkpoint_freq\n        self.training_enabled = training_enabled\n        self.global_step = tf.train.get_or_create_global_step()\n        self.summary_writer = tf.summary.FileWriter(self.summaries_path)\n\n    def restore_or_init(self):\n        self.saver = tf.train.Saver()\n        checkpoint = tf.train.latest_checkpoint(self.checkpoints_path)\n        if checkpoint:\n            self.saver.restore(self.sess, checkpoint)\n\n            if self.training_enabled:\n                # merge with previous summary session\n                self.summary_writer.add_session_log(\n                    tf.SessionLog(status=tf.SessionLog.START), self.sess.run(self.global_step))\n        else:\n            self.sess.run(tf.global_variables_initializer())\n        # this call locks the computational graph into read-only state,\n        # as a safety measure against memory leaks caused by mistakingly adding new ops to it\n        self.sess.graph.finalize()\n\n    def run(self, tf_op, tf_inputs, inputs):\n        return self.sess.run(tf_op, feed_dict=dict(zip(tf_inputs, inputs)))\n\n    def on_update(self, step):\n        if not self.checkpoint_freq or not self.training_enabled or step % self.checkpoint_freq:\n            return\n\n        self.saver.save(self.sess, self.checkpoints_path + '/ckpt', global_step=step)\n\n    def add_summaries(self, tags, values, prefix='', step=None):\n        for tag, value in zip(tags, values):\n            self.add_summary(tag, value, prefix, step)\n\n    def add_summary(self, tag, value, prefix='', step=None):\n        if not self.training_enabled:\n            return\n        summary = self.create_summary(prefix + '/' + tag, value)\n        self.summary_writer.add_summary(summary, global_step=step)\n\n    @staticmethod\n    def create_summary(tag, value):\n        return tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n\n    @property\n    def start_step(self):\n        if self.training_enabled:\n            return self.global_step.eval(session=self.sess)\n        return 0\n\n    @property\n    def summaries_path(self):\n        return self.base_path + '/summaries'\n\n    @property\n    def checkpoints_path(self):\n        return self.base_path + '/checkpoints'\n"""
reaver/utils/typing.py,0,"b'from typing import Callable, List, Tuple, Any, Type\nfrom tensorflow.compat.v1.keras import Model\nfrom reaver.envs.base import Spec\nfrom reaver.models.base import MultiPolicy\n\nDone = bool\nReward = int\nAction = List[Any]\nObservation = List[Any]\n\nPolicyType = Type[MultiPolicy]\nModelBuilder = Callable[[Spec, Spec], Model]\n'"
reaver/agents/base/__init__.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass Agent(ABC):\n    @abstractmethod\n    def get_action(self, obs): ...\n\n\nfrom .memory import MemoryAgent\nfrom .running import SyncRunningAgent\nfrom .actor_critic import ActorCriticAgent, DEFAULTS\n'"
reaver/agents/base/actor_critic.py,4,"b'import gin.tf\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom abc import abstractmethod\n\nfrom reaver.envs.base import Spec\nfrom reaver.agents.base import MemoryAgent\nfrom reaver.utils import Logger\nfrom reaver.utils.tensorflow import SessionManager\nfrom reaver.utils.typing import ModelBuilder, PolicyType\n\nDEFAULTS = dict(\n    model_fn=None,\n    policy_cls=None,\n    optimizer=None,\n    learning_rate=0.0003,\n    value_coef=0.5,\n    entropy_coef=0.01,\n    traj_len=16,\n    batch_sz=16,\n    discount=0.99,\n    gae_lambda=0.95,\n    clip_rewards=0.0,\n    clip_grads_norm=0.0,\n    normalize_returns=False,\n    normalize_advantages=False,\n)\n\n\n@gin.configurable(\'ACAgent\')\nclass ActorCriticAgent(MemoryAgent):\n    """"""\n    Abstract class, unifies deep actor critic functionality\n    Handles on_step callbacks, either updating current batch\n    or executing one training step if the batch is ready\n\n    Extending classes only need to implement loss_fn method\n    """"""\n    def __init__(\n        self,\n        obs_spec: Spec,\n        act_spec: Spec,\n        model_fn: ModelBuilder=None,\n        policy_cls: PolicyType=None,\n        sess_mgr: SessionManager=None,\n        optimizer: tf.train.Optimizer=None,\n        value_coef=DEFAULTS[\'value_coef\'],\n        entropy_coef=DEFAULTS[\'entropy_coef\'],\n        traj_len=DEFAULTS[\'traj_len\'],\n        batch_sz=DEFAULTS[\'batch_sz\'],\n        discount=DEFAULTS[\'discount\'],\n        gae_lambda=DEFAULTS[\'gae_lambda\'],\n        clip_rewards=DEFAULTS[\'clip_rewards\'],\n        clip_grads_norm=DEFAULTS[\'clip_grads_norm\'],\n        normalize_returns=DEFAULTS[\'normalize_returns\'],\n        normalize_advantages=DEFAULTS[\'normalize_advantages\'],\n    ):\n        MemoryAgent.__init__(self, obs_spec, act_spec, traj_len, batch_sz)\n\n        if not sess_mgr:\n            sess_mgr = SessionManager()\n\n        if not optimizer:\n            optimizer = tf.train.AdamOptimizer(learning_rate=DEFAULTS[\'learning_rate\'])\n\n        self.sess_mgr = sess_mgr\n        self.value_coef = value_coef\n        self.entropy_coef = entropy_coef\n        self.discount = discount\n        self.gae_lambda = gae_lambda\n        self.clip_rewards = clip_rewards\n        self.normalize_returns = normalize_returns\n        self.normalize_advantages = normalize_advantages\n\n        self.model = model_fn(obs_spec, act_spec)\n        self.value = self.model.outputs[-1]\n        self.policy = policy_cls(act_spec, self.model.outputs[:-1])\n        self.loss_op, self.loss_terms, self.loss_inputs = self.loss_fn()\n\n        grads, vars = zip(*optimizer.compute_gradients(self.loss_op))\n        self.grads_norm = tf.global_norm(grads)\n        if clip_grads_norm > 0.:\n            grads, _ = tf.clip_by_global_norm(grads, clip_grads_norm, self.grads_norm)\n        self.train_op = optimizer.apply_gradients(zip(grads, vars), global_step=sess_mgr.global_step)\n        self.minimize_ops = self.make_minimize_ops()\n\n        sess_mgr.restore_or_init()\n        self.n_batches = sess_mgr.start_step\n        self.start_step = sess_mgr.start_step * traj_len\n\n        self.logger = Logger()\n\n    def get_action_and_value(self, obs):\n        return self.sess_mgr.run([self.policy.sample, self.value], self.model.inputs, obs)\n\n    def get_action(self, obs):\n        return self.sess_mgr.run(self.policy.sample, self.model.inputs, obs)\n\n    def on_step(self, step, obs, action, reward, done, value=None):\n        MemoryAgent.on_step(self, step, obs, action, reward, done, value)\n        self.logger.on_step(step, reward, done)\n\n        if not self.batch_ready():\n            return\n\n        next_values = self.sess_mgr.run(self.value, self.model.inputs, self.last_obs)\n        adv, returns = self.compute_advantages_and_returns(next_values)\n\n        loss_terms, grads_norm = self.minimize(adv, returns)\n\n        self.sess_mgr.on_update(self.n_batches)\n        self.logger.on_update(self.n_batches, loss_terms, grads_norm, returns, adv, next_values)\n\n    def minimize(self, advantages, returns):\n        inputs = self.obs + self.acts + [advantages, returns]\n        inputs = [a.reshape(-1, *a.shape[2:]) for a in inputs]\n        tf_inputs = self.model.inputs + self.policy.inputs + self.loss_inputs\n\n        loss_terms, grads_norm, *_ = self.sess_mgr.run(self.minimize_ops, tf_inputs, inputs)\n\n        return loss_terms, grads_norm\n\n    def compute_advantages_and_returns(self, bootstrap_value):\n        """"""\n        GAE can help with reducing variance of policy gradient estimates\n        """"""\n        if self.clip_rewards > 0.0:\n            np.clip(self.rewards, -self.clip_rewards, self.clip_rewards, out=self.rewards)\n\n        rewards = self.rewards.copy()\n        rewards[-1] += (1-self.dones[-1]) * self.discount * bootstrap_value\n\n        masked_discounts = self.discount * (1-self.dones)\n\n        returns = self.discounted_cumsum(rewards, masked_discounts)\n\n        if self.gae_lambda > 0.:\n            values = np.append(self.values, np.expand_dims(bootstrap_value, 0), axis=0)\n            # d_t = r_t + g * V(s_{t+1}) - V(s_t)\n            deltas = self.rewards + masked_discounts * values[1:] - values[:-1]\n            adv = self.discounted_cumsum(deltas, self.gae_lambda * masked_discounts)\n        else:\n            adv = returns - self.values\n\n        if self.normalize_advantages:\n            adv = (adv - adv.mean()) / (adv.std() + 1e-10)\n\n        if self.normalize_returns:\n            returns = (returns - returns.mean()) / (returns.std() + 1e-10)\n\n        return adv, returns\n\n    def on_start(self):\n        self.logger.on_start()\n\n    def on_finish(self):\n        self.logger.on_finish()\n\n    def make_minimize_ops(self):\n        ops = [self.loss_terms, self.grads_norm]\n        if self.sess_mgr.training_enabled:\n            ops.append(self.train_op)\n        # appending extra model update ops (e.g. running stats)\n        # note: this will most likely break if model.compile() is used\n        ops.extend(self.model.get_updates_for(None))\n        return ops\n\n    @staticmethod\n    def discounted_cumsum(x, discount):\n        y = np.zeros_like(x)\n        y[-1] = x[-1]\n        for t in range(x.shape[0]-2, -1, -1):\n            y[t] = x[t] + discount[t] * y[t+1]\n        return y\n\n    @abstractmethod\n    def loss_fn(self): ...\n'"
reaver/agents/base/memory.py,0,"b'import numpy as np\nfrom .running import RunningAgent\nfrom reaver.envs.base import Spec\n\n\nclass MemoryAgent(RunningAgent):\n    """"""\n    Handles experience memory storage - up to (B, T, ?),\n    where B is batch size, T is trajectory length\n    and ? is either 1 (None) for rewards/dones or act/obs shapes\n    """"""\n    def __init__(self, obs_spec: Spec, act_spec: Spec, traj_len, batch_sz):\n        RunningAgent.__init__(self)\n\n        self.traj_len = traj_len\n        self.batch_sz = batch_sz\n        self.shape = (traj_len, batch_sz)\n        self.batch_ptr = 0\n        self.n_batches = 0\n\n        self.dones = np.empty(self.shape, dtype=np.bool)\n        self.values = np.empty(self.shape, dtype=np.float32)\n        self.rewards = np.empty(self.shape, dtype=np.float32)\n        self.acts = [np.empty(self.shape + s.shape, dtype=s.dtype) for s in act_spec.spaces]\n        self.obs = [np.empty(self.shape + s.shape, dtype=s.dtype) for s in obs_spec.spaces]\n        self.last_obs = [np.empty((self.batch_sz, ) + s.shape, dtype=s.dtype) for s in obs_spec.spaces]\n\n    def on_step(self, step, obs, action, reward, done, value=None):\n        """"""\n        Used as a callback by extending agents.\n        Note that here ""step"" refers to update step, rather than agent timestep\n\n        NB! Agent will overwrite previous batch without warning\n        Keeping track of memory state is up to extending subclasses\n        """"""\n        step = step % self.traj_len\n        self.batch_ptr = self.batch_ptr % self.batch_sz\n        bs, be = self.batch_ptr, self.batch_ptr + reward.shape[0]\n\n        self.dones[step, bs:be] = done\n        self.rewards[step, bs:be] = reward\n\n        if value is not None:\n            self.values[step, bs:be] = value\n\n        for i in range(len(obs)):\n            self.obs[i][step, bs:be] = obs[i]\n\n        for i in range(len(action)):\n            self.acts[i][step, bs:be] = action[i]\n\n        if (step+1) % self.traj_len == 0:\n            # finished one trajectory\n            for i in range(len(obs)):\n                self.last_obs[i][bs:be] = self.next_obs[i]\n            self.batch_ptr += reward.shape[0]\n\n        if self.batch_ready():\n            self.n_batches += 1\n\n    def batch_ready(self):\n        """"""\n        Returns true if on_step was called batch_sz times\n        """"""\n        return self.batch_ptr == self.batch_sz\n'"
reaver/agents/base/running.py,0,"b'import copy\nfrom . import Agent\nfrom reaver.envs.base import Env, MultiProcEnv\n\n\nclass RunningAgent(Agent):\n    """"""\n    Generic abstract class, defines API for interacting with an environment\n    """"""\n    def __init__(self):\n        self.next_obs = None\n        self.start_step = 0\n\n    def run(self, env: Env, n_steps=1000000):\n        env = self.wrap_env(env)\n        env.start()\n        try:\n            self._run(env, n_steps)\n        except KeyboardInterrupt:\n            env.stop()\n            self.on_finish()\n\n    def _run(self, env, n_steps):\n        self.on_start()\n        obs, *_ = env.reset()\n        obs = [o.copy() for o in obs]\n        for step in range(self.start_step, self.start_step + n_steps):\n            action, value = self.get_action_and_value(obs)\n            self.next_obs, reward, done = env.step(action)\n            self.on_step(step, obs, action, reward, done, value)\n            obs = [o.copy() for o in self.next_obs]\n        env.stop()\n        self.on_finish()\n\n    def get_action_and_value(self, obs):\n        return self.get_action(obs), None\n\n    def on_start(self): ...\n\n    def on_step(self, step, obs, action, reward, done, value=None): ...\n\n    def on_finish(self): ...\n\n    def wrap_env(self, env: Env) -> Env:\n        return env\n\n\nclass SyncRunningAgent(RunningAgent):\n    """"""\n    Abstract class that handles synchronous multiprocessing via MultiProcEnv helper\n    Not meant to be used directly, extending classes automatically get the feature\n    """"""\n    def __init__(self, n_envs):\n        RunningAgent.__init__(self)\n        self.n_envs = n_envs\n\n    def wrap_env(self, env: Env) -> Env:\n        render, env.render = env.render, False\n        envs = [env] + [copy.deepcopy(env) for _ in range(self.n_envs-1)]\n        env.render = render\n\n        return MultiProcEnv(envs)\n'"
reaver/envs/base/__init__.py,0,"b""import platform\nfrom .spec import Space, Spec\nfrom .abc import Env\nfrom .shm_multiproc import ShmMultiProcEnv\nfrom .msg_multiproc import MsgMultiProcEnv\n\nMultiProcEnv = ShmMultiProcEnv\nif platform.system() == 'Windows':\n    MultiProcEnv = MsgMultiProcEnv\n"""
reaver/envs/base/abc.py,0,"b'from abc import ABC, abstractmethod\nfrom reaver.utils.typing import *\nfrom .spec import Spec\n\n\nclass Env(ABC):\n    """"""\n    Abstract Base Class for all environments supported by Reaver\n    Acts as a glue between the agents, models and envs modules\n    Implementing class can be a simple wrapper (e.g. over openAI Gym)\n\n    Note: observation / action specs contain a list of spaces,\n          this is implicitly assumed across all Reaver components\n    """"""\n    def __init__(self, _id: str, render=False, reset_done=True, max_ep_len=None):\n        self.id = _id\n        self.render = render\n        self.reset_done = reset_done\n        self.max_ep_len = max_ep_len if max_ep_len else float(\'inf\')\n\n    @abstractmethod\n    def start(self) -> None: ...\n\n    @abstractmethod\n    def step(self, action: Action) -> Tuple[Observation, Reward, Done]: ...\n\n    @abstractmethod\n    def reset(self) -> Observation: ...\n\n    @abstractmethod\n    def stop(self) -> None: ...\n\n    @abstractmethod\n    def obs_spec(self) -> Spec: ...\n\n    @abstractmethod\n    def act_spec(self) -> Spec: ...\n'"
reaver/envs/base/msg_multiproc.py,0,"b'import numpy as np\nfrom multiprocessing import Pipe, Process\nfrom . import Env\n\nSTART, STEP, RESET, STOP, DONE = range(5)\n\n\nclass MsgProcEnv(Env):\n    def __init__(self, env):\n        super().__init__(env.id)\n        self._env = env\n        self.conn = self.w_conn = self.proc = None\n\n    def start(self):\n        self.conn, self.w_conn = Pipe()\n        self.proc = Process(target=self._run)\n        self.proc.start()\n        self.conn.send((START, None))\n\n    def step(self, act):\n        self.conn.send((STEP, act))\n\n    def reset(self):\n        self.conn.send((RESET, None))\n\n    def stop(self):\n        self.conn.send((STOP, None))\n\n    def wait(self):\n        return self.conn.recv()\n\n    def obs_spec(self):\n        return self._env.obs_spec()\n\n    def act_spec(self):\n        return self._env.act_spec()\n\n    def _run(self):\n        while True:\n            msg, data = self.w_conn.recv()\n            if msg == START:\n                self._env.start()\n                self.w_conn.send(DONE)\n            elif msg == STEP:\n                obs, rew, done = self._env.step(data)\n                self.w_conn.send((obs, rew, done))\n            elif msg == RESET:\n                obs = self._env.reset()\n                self.w_conn.send((obs, -1, -1))\n            elif msg == STOP:\n                self._env.stop()\n                self.w_conn.close()\n                break\n\n\nclass MsgMultiProcEnv(Env):\n    """"""\n    Parallel environments via multiprocessing + pipes\n    """"""\n    def __init__(self, envs):\n        super().__init__(envs[0].id)\n        self.envs = [MsgProcEnv(env) for env in envs]\n\n    def start(self):\n        for env in self.envs:\n            env.start()\n        self.wait()\n\n    def step(self, actions):\n        for idx, env in enumerate(self.envs):\n            env.step([a[idx] for a in actions])\n        return self._observe()\n\n    def reset(self):\n        for e in self.envs:\n            e.reset()\n        return self._observe()\n\n    def _observe(self):\n        obs, reward, done = zip(*self.wait())\n        # n_envs x n_spaces -> n_spaces x n_envs\n        obs = list(map(np.array, zip(*obs)))\n\n        return obs, np.array(reward), np.array(done)\n\n    def stop(self):\n        for e in self.envs:\n            e.stop()\n        for e in self.envs:\n            e.proc.join()\n\n    def wait(self):\n        return [e.wait() for e in self.envs]\n\n    def obs_spec(self):\n        return self.envs[0].obs_spec()\n\n    def act_spec(self):\n        return self.envs[0].act_spec()\n'"
reaver/envs/base/shm_multiproc.py,0,"b'import ctypes\nimport numpy as np\nfrom multiprocessing import Pipe, Process\nfrom multiprocessing.sharedctypes import RawArray\nfrom . import Env, Space\n\nSTART, STEP, RESET, STOP, DONE = range(5)\n\n\nclass ShmProcEnv(Env):\n    def __init__(self, env, idx, shm):\n        super().__init__(env.id)\n        self._env, self.idx, self.shm = env, idx, shm\n        self.conn = self.w_conn = self.proc = None\n\n    def start(self):\n        self.conn, self.w_conn = Pipe()\n        self.proc = Process(target=self._run)\n        self.proc.start()\n        self.conn.send((START, None))\n\n    def step(self, act):\n        self.conn.send((STEP, act))\n\n    def reset(self):\n        self.conn.send((RESET, None))\n\n    def stop(self):\n        self.conn.send((STOP, None))\n\n    def wait(self):\n        return self.conn.recv()\n\n    def obs_spec(self):\n        return self._env.obs_spec()\n\n    def act_spec(self):\n        return self._env.act_spec()\n\n    def _run(self):\n        try:\n            while True:\n                msg, data = self.w_conn.recv()\n                if msg == START:\n                    self._env.start()\n                    self.w_conn.send(DONE)\n                elif msg == STEP:\n                    obs, rew, done = self._env.step(data)\n                    for shm, ob in zip(self.shm, obs + [rew, done]):\n                        np.copyto(dst=shm[self.idx], src=ob)\n                    self.w_conn.send(DONE)\n                elif msg == RESET:\n                    obs = self._env.reset()\n                    for shm, ob in zip(self.shm, obs + [0, 0]):\n                        np.copyto(dst=shm[self.idx], src=ob)\n                    self.w_conn.send(DONE)\n                elif msg == STOP:\n                    self._env.stop()\n                    self.w_conn.close()\n                    break\n        except KeyboardInterrupt:\n            self._env.stop()\n            self.w_conn.close()\n\n\nclass ShmMultiProcEnv(Env):\n    """"""\n    Parallel environments via multiprocessing + shared memory\n    """"""\n    def __init__(self, envs):\n        super().__init__(envs[0].id)\n        self.shm = [make_shared(len(envs), s) for s in envs[0].obs_spec().spaces]\n        self.shm.append(make_shared(len(envs), Space((1,), name=""reward"")))\n        self.shm.append(make_shared(len(envs), Space((1,), name=""done"")))\n        self.envs = [ShmProcEnv(env, idx, self.shm) for idx, env in enumerate(envs)]\n\n    def start(self):\n        for env in self.envs:\n            env.start()\n        self.wait()\n\n    def step(self, actions):\n        for idx, env in enumerate(self.envs):\n            env.step([a[idx] for a in actions])\n        return self._observe()\n\n    def reset(self):\n        for e in self.envs:\n            e.reset()\n        return self._observe()\n\n    def _observe(self):\n        self.wait()\n\n        obs = self.shm[:-2]\n        reward = np.squeeze(self.shm[-2], axis=-1)\n        done = np.squeeze(self.shm[-1], axis=-1)\n\n        return obs, reward, done\n\n    def stop(self):\n        for e in self.envs:\n            e.stop()\n        for e in self.envs:\n            e.proc.join()\n\n    def wait(self):\n        return [e.wait() for e in self.envs]\n\n    def obs_spec(self):\n        return self.envs[0].obs_spec()\n\n    def act_spec(self):\n        return self.envs[0].act_spec()\n\n\ndef make_shared(n_envs, obs_space):\n    shape = (n_envs, ) + obs_space.shape\n    raw = RawArray(to_ctype(obs_space.dtype), int(np.prod(shape)))\n    return np.frombuffer(raw, dtype=obs_space.dtype).reshape(shape)\n\n\ndef to_ctype(_type):\n    types = {\n        np.bool: ctypes.c_bool,\n        np.int8: ctypes.c_byte,\n        np.uint8: ctypes.c_ubyte,\n        np.int32: ctypes.c_int32,\n        np.int64: ctypes.c_longlong,\n        np.uint64: ctypes.c_ulonglong,\n        np.float32: ctypes.c_float,\n        np.float64: ctypes.c_double,\n    }\n    if isinstance(_type, np.dtype):\n        _type = _type.type\n    return types[_type]\n'"
reaver/envs/base/spec.py,0,"b'import numpy as np\nfrom typing import List\n\n\nclass Space:\n    """"""\n    Holds information about any generic space\n    In essence is a simplification of gym.spaces module into a single endpoint\n    """"""\n    def __init__(self, shape=(), dtype=np.int32, domain=(0, 1), categorical=False, name=None):\n        self.name = name\n        self.shape, self.dtype = shape, dtype\n        self.categorical, (self.lo, self.hi) = categorical, domain\n\n    def is_discrete(self) -> bool:\n        """"""\n        Space is considered continuous if its values are only ints\n        """"""\n        return np.issubdtype(self.dtype, np.integer)\n\n    def is_continuous(self) -> bool:\n        """"""\n        Space is considered continuous if its values can be floats\n        """"""\n        return np.issubdtype(self.dtype, np.floating)\n\n    def is_spatial(self) -> bool:\n        """"""\n        Space is considered spacial if it has three-dimensional shape HxWxC\n        """"""\n        return len(self.shape) > 1 or type(self.hi) in [list, tuple]\n\n    def size(self) -> int:\n        """"""\n        Number of labels if categorical\n        Number of intervals if discrete (can have multiple in one space)\n        Number of mean and log std.dev if continuous\n\n        Meant to be used to determine size of logit outputs in models\n        """"""\n        if self.is_discrete() and self.categorical:\n            if self.is_spatial():\n                return self.hi\n            return self.hi - self.lo\n\n        sz = 1\n        if len(self.shape) == 1:\n            sz = self.shape[0]\n\n        return sz\n\n    def sample(self, n=1):\n        """"""\n        Sample from this space. Useful for random agent, for example.\n        """"""\n        if self.is_discrete():\n            return np.random.randint(self.lo, self.hi+1, (n, ) + self.shape)\n\n        if self.is_continuous():\n            return np.random.uniform(self.lo, self.hi+1e-10, (n, ) + self.shape)\n\n    def __repr__(self):\n        mid = str(self.shape)\n        if self.categorical:\n            mid += "", cat: "" + str(self.hi)\n        return ""Space(%s, %s, %s)"" % (self.name, mid, str(self.dtype).strip(""<class>\' ""))\n\n\nclass Spec:\n    """"""\n    Convenience class to hold a list of spaces, can be used as an iterable\n    A typical environment is expected to have one observation spec and one action spec\n\n    Note: Every spec is expected to have a list of spaces, even if there is only one space\n    """"""\n    def __init__(self, spaces: List[Space], name=None):\n        self.name, self.spaces = name, spaces\n        for i, space in enumerate(self.spaces):\n            if not space.name:\n                space.name = str(i)\n\n    def sample(self, n=1):\n        return [space.sample(n) for space in self.spaces]\n\n    def __repr__(self):\n        return ""Spec: %s\\n%s"" % (self.name, ""\\n"".join(map(str, self.spaces)))\n\n    def __iter__(self):\n        return (space for space in self.spaces)\n\n    def __len__(self):\n        return len(self.spaces)\n'"
reaver/models/base/__init__.py,0,b'import reaver.models.base.layers\nfrom reaver.models.base.mlp import build_mlp\nfrom reaver.models.base.cnn import build_cnn_nature\nfrom reaver.models.base.policy import MultiPolicy\n'
reaver/models/base/cnn.py,0,"b'import gin\nfrom tensorflow.compat.v1.keras import Model\nfrom tensorflow.compat.v1.keras.layers import Input, Concatenate, Dense, Conv2D, Flatten\nfrom reaver.models.base.layers import Squeeze, Rescale, Transpose, RunningStatsNorm\n\n\n@gin.configurable\ndef build_cnn_nature(obs_spec, act_spec, data_format=\'channels_first\', value_separate=False, obs_shift=False, obs_scale=False):\n    conv_cfg = dict(padding=\'same\', data_format=data_format, activation=\'relu\')\n    conv_spec = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n\n    inputs = [Input(s.shape, name=""input_"" + s.name) for s in obs_spec]\n    inputs_concat = Concatenate()(inputs) if len(inputs) > 1 else inputs[0]\n\n    # expected NxCxHxW, but got NxHxWxC\n    if data_format == \'channels_first\' and inputs_concat.shape[1] > 3:\n        inputs_concat = Transpose([0, 3, 1, 2])(inputs_concat)\n\n    inputs_scaled = Rescale(1./255)(inputs_concat)\n    if obs_shift or obs_scale:\n        inputs_scaled = RunningStatsNorm(obs_shift, obs_scale)(inputs_scaled)\n\n    x = build_cnn(inputs_scaled, conv_spec, conv_cfg, dense=512, prefix=\'policy_\')\n    outputs = [Dense(s.size(), name=""logits_"" + s.name)(x) for s in act_spec]\n\n    if value_separate:\n        x = build_cnn(inputs_scaled, conv_spec, conv_cfg, dense=512, prefix=\'value_\')\n\n    value = Dense(1, name=""value_out"")(x)\n    value = Squeeze(axis=-1)(value)\n    outputs.append(value)\n\n    return Model(inputs=inputs, outputs=outputs)\n\n\ndef build_cnn(input_layer, layers, conv_cfg, dense=None, prefix=\'\'):\n    x = input_layer\n    for i, (n_filters, kernel_size, stride) in enumerate(layers):\n        x = Conv2D(n_filters, kernel_size, stride, name=\'%sconv%02d\' % (prefix, i+1), **conv_cfg)(x)\n\n    if dense:\n        x = Flatten()(x)\n        x = Dense(dense)(x)\n\n    return x\n'"
reaver/models/base/layers.py,14,"b'import tensorflow.compat.v1 as tf\nfrom tensorflow.compat.v1.keras.layers import Lambda, Layer\n\n\nclass RunningStatsNorm(Layer):\n    """"""\n    Normalizes inputs by running mean / std.dev statistics\n    """"""\n    def __init__(self, and_shift=True, and_scale=False, **kwargs):\n        self.and_shift, self.and_scale = and_shift, and_scale\n        self._ct = self._mu = self._var = None\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        shape = (*input_shape[1:],)\n        self._ct = self.add_weight(\'running_ct\', (), initializer=\'zeros\', trainable=False)\n        self._mu = self.add_weight(\'running_mu\', shape, initializer=\'zeros\', trainable=False)\n        self._var = self.add_weight(\'running_var\', shape, initializer=\'ones\', trainable=False)\n        super().build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        mu, var = self._update_stats(inputs)\n        if self.and_shift:\n            inputs -= mu\n        if self.and_scale:\n            inputs /= tf.sqrt(var)\n        return inputs\n\n    def _update_stats(self, x):\n        ct = tf.maximum(1e-10, self._ct)\n\n        ct_b = tf.to_float(tf.shape(x)[0])\n        mu_b, var_b = tf.nn.moments(x, axes=[0])\n\n        delta = mu_b - self._mu\n\n        new_ct = ct + ct_b\n        new_mu = self._mu + delta * ct_b / new_ct\n        new_var = (self._var * ct + var_b * ct_b + delta ** 2 * ct * ct_b / new_ct) / new_ct\n\n        self.add_update([\n            tf.assign(self._ct, new_ct),\n            tf.assign(self._mu, new_mu),\n            tf.assign(self._var, new_var)\n        ])\n\n        return new_mu, new_var\n\n\nclass Variable(Layer):\n    """"""\n    Concatenate an extra trainable variable to the dense layer\n    This variable is disconnected from the rest of the NN, including inputs\n    """"""\n    def __init__(self, **kwargs):\n        self._var = None\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        self._var = self.add_weight(\'var\', (1, input_shape[-1]), initializer=\'zeros\', trainable=True)\n        super().build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        """"""\n        repeat _var N times to match inputs dim and then concatenate them\n        """"""\n        return tf.concat([inputs, tf.tile(self._var, (tf.shape(inputs)[0], 1))], axis=-1)\n\n\nclass Squeeze(Lambda):\n    def __init__(self, axis=-1, name=None):\n        Lambda.__init__(self, lambda x: tf.squeeze(x, axis=axis), name=name)\n\n\nclass Split(Lambda):\n    def __init__(self, num_splits=2, axis=-1, name=None):\n        Lambda.__init__(self, lambda x: tf.split(x, num_splits, axis=axis), name=name)\n\n\nclass Transpose(Lambda):\n    def __init__(self, dims):\n        Lambda.__init__(self, lambda x: tf.transpose(x, dims))\n\n\nclass Log(Lambda):\n    def __init__(self):\n        Lambda.__init__(self, lambda x: tf.log(x + 1e-10))\n\n\nclass Rescale(Lambda):\n    def __init__(self, scale):\n        Lambda.__init__(self, lambda x: tf.cast(x, tf.float32) * scale)\n\n\nclass Broadcast2D(Lambda):\n    def __init__(self, size):\n        Lambda.__init__(self, lambda x: tf.tile(tf.expand_dims(tf.expand_dims(x, 2), 3), [1, 1, size, size]))\n'"
reaver/models/base/mlp.py,2,"b'import gin\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.compat.v1.keras.layers import Input, Concatenate, Dense\nfrom reaver.models.base.layers import Squeeze, Variable, RunningStatsNorm\nfrom reaver.envs.base import Spec\n\n\n@gin.configurable\ndef build_mlp(\n        obs_spec: Spec,\n        act_spec: Spec,\n        layer_sizes=(64, 64),\n        activation=\'relu\',\n        initializer=\'glorot_uniform\',\n        value_separate=False,\n        obs_shift=False,\n        obs_scale=False) -> tf.keras.Model:\n    """"""\n    Factory method for a simple fully connected neural network model used in e.g. MuJuCo environment\n\n    If value separate is set to true then a separate path is added for value fn, otherwise branches out of last layer\n    If obs shift is set to true then observations are normalized to mean zero with running mean estimate\n    If obs scale is set to true then observations are standardized to std.dev one with running std.dev estimate\n    """"""\n    inputs = inputs_ = [Input(s.shape, name=""input_"" + s.name) for s in obs_spec]\n    if obs_shift or obs_scale:\n        inputs_ = [RunningStatsNorm(obs_shift, obs_scale, name=""norm_"" + s.name)(x) for s, x in zip(obs_spec, inputs_)]\n    inputs_concat = Concatenate()(inputs_) if len(inputs_) > 1 else inputs_[0]\n\n    x = build_fc(inputs_concat, layer_sizes, activation, initializer)\n    outputs = [build_logits(space, x, initializer) for space in act_spec]\n\n    if value_separate:\n        x = build_fc(inputs_concat, layer_sizes, activation, initializer, \'value_\')\n\n    value = Dense(1, name=""value_out"", kernel_initializer=initializer)(x)\n    value = Squeeze(axis=-1)(value)\n    outputs.append(value)\n\n    return tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\ndef build_logits(space, prev_layer, initializer):\n    logits = Dense(space.size(), kernel_initializer=initializer, name=""logits_"" + space.name)(prev_layer)\n    if space.is_continuous():\n        logits = Variable(name=""logstd"")(logits)\n    return logits\n\n\ndef build_fc(input_layer, layer_sizes, activation, initializer, prefix=\'\'):\n    x = input_layer\n    for i, size in enumerate(layer_sizes):\n        x = Dense(size, activation=activation, kernel_initializer=initializer, name=\'%sfc%02d\' % (prefix, i+1))(x)\n    return x\n'"
reaver/models/base/policy.py,3,"b'import gin\nimport tensorflow.compat.v1 as tf\n\n\n@gin.configurable\nclass MultiPolicy:\n    def __init__(self, act_spec, logits):\n        self.logits = logits\n        self.inputs = [tf.placeholder(s.dtype, [None, *s.shape]) for s in act_spec]\n\n        self.dists = [self.make_dist(s, l) for s, l in zip(act_spec.spaces, logits)]\n\n        self.entropy = sum([dist.entropy() for dist in self.dists])\n        self.logli = sum([dist.log_prob(act) for dist, act in zip(self.dists, self.inputs)])\n\n        self.sample = [dist.sample() for dist in self.dists]\n\n    @staticmethod\n    def make_dist(space, logits):\n        # tfp is really heavy on init, better to lazy load\n        import tensorflow_probability as tfp\n\n        if space.is_continuous():\n            mu, logstd = tf.split(logits, 2, axis=-1)\n            return tfp.distributions.MultivariateNormalDiag(mu, tf.exp(logstd))\n        else:\n            return tfp.distributions.Categorical(logits)\n'"
reaver/models/sc2/__init__.py,0,b'from reaver.models.sc2.policy import SC2MultiPolicy\nfrom reaver.models.sc2.fully_conv import build_fully_conv\n'
reaver/models/sc2/fully_conv.py,10,"b'import gin\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.compat.v1.keras import Model\nfrom tensorflow.compat.v1.keras.initializers import VarianceScaling\nfrom tensorflow.compat.v1.keras.layers import Input, Concatenate, Dense, Embedding, Conv2D, Flatten, Lambda\nfrom reaver.models.base.layers import Squeeze, Split, Transpose, Log, Broadcast2D\n\n\n@gin.configurable\ndef build_fully_conv(obs_spec, act_spec, data_format=\'channels_first\', broadcast_non_spatial=False, fc_dim=256):\n    screen, screen_input = spatial_block(\'screen\', obs_spec.spaces[0], conv_cfg(data_format, \'relu\'))\n    minimap, minimap_input = spatial_block(\'minimap\', obs_spec.spaces[1], conv_cfg(data_format, \'relu\'))\n\n    non_spatial_inputs = [Input(s.shape) for s in obs_spec.spaces[2:]]\n\n    if broadcast_non_spatial:\n        non_spatial, spatial_dim = non_spatial_inputs[1], obs_spec.spaces[0].shape[1]\n        non_spatial = tf.log(non_spatial + 1e-5)\n        broadcasted_non_spatial = Broadcast2D(spatial_dim)(non_spatial)\n        state = tf.concat([screen, minimap, broadcasted_non_spatial], axis=1)\n    else:\n        state = tf.concat([screen, minimap], axis=1)\n\n    fc = Flatten(name=""state_flat"")(state)\n    fc = Dense(fc_dim, **dense_cfg(\'relu\'))(fc)\n\n    value = Dense(1, name=""value_out"", **dense_cfg(scale=0.1))(fc)\n    value = tf.squeeze(value, axis=-1)\n\n    logits = []\n    for space in act_spec:\n        if space.is_spatial():\n            logits.append(Conv2D(1, 1, **conv_cfg(data_format, scale=0.1))(state))\n            logits[-1] = Flatten()(logits[-1])\n        else:\n            logits.append(Dense(space.size(), **dense_cfg(scale=0.1))(fc))\n\n    mask_actions = Lambda(\n        lambda x: tf.where(non_spatial_inputs[0] > 0, x, -1000 * tf.ones_like(x)),\n        name=""mask_unavailable_action_ids""\n    )\n    logits[0] = mask_actions(logits[0])\n\n    return Model(\n        inputs=[screen_input, minimap_input] + non_spatial_inputs,\n        outputs=logits + [value]\n    )\n\n\ndef spatial_block(name, space, cfg):\n    inpt = Input(space.shape, name=name + \'_input\')\n    block = tf.split(inpt, space.shape[0], axis=1)\n\n    for i, (name, dim) in enumerate(zip(space.spatial_feats, space.spatial_dims)):\n        if dim > 1:\n            block[i] = tf.squeeze(block[i], axis=1)\n            # Embedding dim 10 as per https://arxiv.org/pdf/1806.01830.pdf\n            block[i] = Embedding(input_dim=dim, output_dim=10)(block[i])\n            # [N, H, W, C] -> [N, C, H, W]\n            block[i] = tf.transpose(block[i], perm=[0, 3, 1, 2])\n        else:\n            block[i] = tf.log(block[i] + 1e-5)\n\n    block = tf.concat(block, axis=1)\n    block = Conv2D(16, 5, **cfg)(block)\n    block = Conv2D(32, 3, **cfg)(block)\n\n    return block, inpt\n\n\ndef conv_cfg(data_format=\'channels_first\', activation=None, scale=1.0):\n    return dict(\n        padding=\'same\',\n        activation=activation,\n        data_format=data_format,\n        kernel_initializer=VarianceScaling(scale=2.0*scale)\n    )\n\n\ndef dense_cfg(activation=None, scale=1.0):\n    return dict(\n        activation=activation,\n        kernel_initializer=VarianceScaling(scale=2.0*scale)\n    )\n\n'"
reaver/models/sc2/policy.py,3,"b'import gin\nimport tensorflow.compat.v1 as tf\nfrom reaver.models.base import MultiPolicy\n\n\n@gin.configurable\nclass SC2MultiPolicy(MultiPolicy):\n    def __init__(self, act_spec, logits):\n        super().__init__(act_spec, logits)\n\n        args_mask = tf.constant(act_spec.spaces[0].args_mask, dtype=tf.float32)\n        act_args_mask = tf.gather(args_mask, self.inputs[0])\n        act_args_mask = tf.transpose(act_args_mask, [1, 0])\n\n        self.logli = self.dists[0].log_prob(self.inputs[0])\n        for i in range(1, len(self.dists)):\n            self.logli += act_args_mask[i-1] * self.dists[i].log_prob(self.inputs[i])\n'"
