file_path,api_count,code
example.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# MIT License\n#\n# Copyright (c) 2019 Iv\xc3\xa1n de Paz Centeno\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport cv2\nfrom mtcnn import MTCNN\n\ndetector = MTCNN()\n\nimage = cv2.cvtColor(cv2.imread(""ivan.jpg""), cv2.COLOR_BGR2RGB)\nresult = detector.detect_faces(image)\n\n# Result is an array with all the bounding boxes detected. We know that for \'ivan.jpg\' there is only one.\nbounding_box = result[0][\'box\']\nkeypoints = result[0][\'keypoints\']\n\ncv2.rectangle(image,\n              (bounding_box[0], bounding_box[1]),\n              (bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),\n              (0,155,255),\n              2)\n\ncv2.circle(image,(keypoints[\'left_eye\']), 2, (0,155,255), 2)\ncv2.circle(image,(keypoints[\'right_eye\']), 2, (0,155,255), 2)\ncv2.circle(image,(keypoints[\'nose\']), 2, (0,155,255), 2)\ncv2.circle(image,(keypoints[\'mouth_left\']), 2, (0,155,255), 2)\ncv2.circle(image,(keypoints[\'mouth_right\']), 2, (0,155,255), 2)\n\ncv2.imwrite(""ivan_drawn.jpg"", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n\nprint(result)\n'"
setup.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# MIT License\n#\n# Copyright (c) 2019 Iv\xc3\xa1n de Paz Centeno\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\nfrom setuptools import setup, setuptools\n\n\n__author__ = ""Iv\xc3\xa1n de Paz Centeno""\n__version__= ""0.1.0""\n\n\ndef readme():\n    with open(\'README.rst\', encoding=""UTF-8"") as f:\n        return f.read()\n\n\nif sys.version_info < (3, 4, 1):\n    sys.exit(\'Python < 3.4.1 is not supported!\')\n\n\nsetup(name=\'mtcnn\',\n      version=__version__,\n      description=\'Multi-task Cascaded Convolutional Neural Networks for Face Detection, based on TensorFlow\',\n      long_description=readme(),\n      url=\'http://github.com/ipazc/mtcnn\',\n      author=\'Iv\xc3\xa1n de Paz Centeno\',\n      author_email=\'ipazc@unileon.es\',\n      license=\'MIT\',\n      packages=setuptools.find_packages(exclude=[""tests.*"", ""tests""]),\n      install_requires=[\n          ""keras>=2.0.0"",\n          ""opencv-python>=4.1.0""\n      ],\n      classifiers=[\n          \'Environment :: Console\',\n          \'Intended Audience :: Developers\',\n          \'Intended Audience :: Education\',\n          \'Intended Audience :: Science/Research\',\n          \'Natural Language :: English\',\n          \'Programming Language :: Python :: 3.4\',\n          \'Programming Language :: Python :: 3.5\',\n          \'Programming Language :: Python :: 3.6\',\n          \'Programming Language :: Python :: 3.7\',\n      ],\n      test_suite=\'nose.collector\',\n      tests_require=[\'nose\'],\n      include_package_data=True,\n      keywords=""mtcnn face detection tensorflow pip package"",\n      zip_safe=False)\n'"
mtcnn/__init__.py,0,"b'#!/usr/bin/python3\r\n# -*- coding: utf-8 -*-\r\n\r\n# MIT License\r\n#\r\n# Copyright (c) 2019 Iv\xc3\xa1n de Paz Centeno\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the ""Software""), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n\r\nfrom mtcnn.mtcnn import MTCNN\r\n\r\n\r\n__author__ = ""Iv\xc3\xa1n de Paz Centeno""\r\n__version__= ""0.1.0""\r\n'"
mtcnn/layer_factory.py,20,"b'#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\n#MIT License\n#\n#Copyright (c) 2018 Iv\xc3\xa1n de Paz Centeno\n#\n#Permission is hereby granted, free of charge, to any person obtaining a copy\n#of this software and associated documentation files (the ""Software""), to deal\n#in the Software without restriction, including without limitation the rights\n#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n#copies of the Software, and to permit persons to whom the Software is\n#furnished to do so, subject to the following conditions:\n#\n#The above copyright notice and this permission notice shall be included in all\n#copies or substantial portions of the Software.\n#\n#THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n#SOFTWARE.\n\nimport tensorflow as tf\nfrom distutils.version import LooseVersion\n\n__author__ = ""Iv\xc3\xa1n de Paz Centeno""\n\n\nclass LayerFactory(object):\n    """"""\n    Allows to create stack layers for a given network.\n    """"""\n\n    AVAILABLE_PADDINGS = (\'SAME\', \'VALID\')\n\n    def __init__(self, network):\n        self.__network = network\n\n    @staticmethod\n    def __validate_padding(padding):\n        if padding not in LayerFactory.AVAILABLE_PADDINGS:\n            raise Exception(""Padding {} not valid"".format(padding))\n\n    @staticmethod\n    def __validate_grouping(channels_input: int, channels_output: int, group: int):\n        if channels_input % group != 0:\n            raise Exception(""The number of channels in the input does not match the group"")\n\n        if channels_output % group != 0:\n            raise Exception(""The number of channels in the output does not match the group"")\n\n    @staticmethod\n    def vectorize_input(input_layer):\n        input_shape = input_layer.get_shape()\n\n        if input_shape.ndims == 4:\n            # Spatial input, must be vectorized.\n            dim = 1\n            for x in input_shape[1:].as_list():\n                dim *= int(x)\n\n            #dim = operator.mul(*(input_shape[1:].as_list()))\n            vectorized_input = tf.reshape(input_layer, [-1, dim])\n        else:\n            vectorized_input, dim = (input_layer, input_shape[-1])\n\n        return vectorized_input, dim\n\n    def __make_var(self, name: str, shape: list):\n        """"""\n        Creates a tensorflow variable with the given name and shape.\n        :param name: name to set for the variable.\n        :param shape: list defining the shape of the variable.\n        :return: created TF variable.\n        """"""\n        return tf.compat.v1.get_variable(name, shape, trainable=self.__network.is_trainable(),\n                                         use_resource=False)\n\n    def new_feed(self, name: str, layer_shape: tuple):\n        """"""\n        Creates a feed layer. This is usually the first layer in the network.\n        :param name: name of the layer\n        :return:\n        """"""\n\n        feed_data = tf.compat.v1.placeholder(tf.float32, layer_shape, \'input\')\n        self.__network.add_layer(name, layer_output=feed_data)\n\n    def new_conv(self, name: str, kernel_size: tuple, channels_output: int,\n                 stride_size: tuple, padding: str=\'SAME\',\n                 group: int=1, biased: bool=True, relu: bool=True, input_layer_name: str=None):\n        """"""\n        Creates a convolution layer for the network.\n        :param name: name for the layer\n        :param kernel_size: tuple containing the size of the kernel (Width, Height)\n        :param channels_output: \xc2\xbf? Perhaps number of channels in the output? it is used as the bias size.\n        :param stride_size: tuple containing the size of the stride (Width, Height)\n        :param padding: Type of padding. Available values are: (\'SAME\', \'VALID\')\n        :param group: groups for the kernel operation. More info required.\n        :param biased: boolean flag to set if biased or not.\n        :param relu: boolean flag to set if ReLu should be applied at the end of the layer or not.\n        :param input_layer_name: name of the input layer for this layer. If None, it will take the last added layer of\n        the network.\n        """"""\n\n        # Verify that the padding is acceptable\n        self.__validate_padding(padding)\n\n        input_layer = self.__network.get_layer(input_layer_name)\n\n        # Get the number of channels in the input\n        channels_input = int(input_layer.get_shape()[-1])\n\n        # Verify that the grouping parameter is valid\n        self.__validate_grouping(channels_input, channels_output, group)\n\n        # Convolution for a given input and kernel\n        convolve = lambda input_val, kernel: tf.nn.conv2d(input=input_val,\n                filters=kernel, \n                strides=[1, stride_size[1], stride_size[0], 1],\n                padding=padding)\n\n        with tf.compat.v1.variable_scope(name) as scope:\n            kernel = self.__make_var(\'weights\', shape=[kernel_size[1], kernel_size[0], channels_input // group, channels_output])\n\n            output = convolve(input_layer, kernel)\n\n            # Add the biases, if required\n            if biased:\n                biases = self.__make_var(\'biases\', [channels_output])\n                output = tf.nn.bias_add(output, biases)\n\n            # Apply ReLU non-linearity, if required\n            if relu:\n                output = tf.nn.relu(output, name=scope.name)\n\n\n        self.__network.add_layer(name, layer_output=output)\n\n    def new_prelu(self, name: str, input_layer_name: str=None):\n        """"""\n        Creates a new prelu layer with the given name and input.\n        :param name: name for this layer.\n        :param input_layer_name: name of the layer that serves as input for this one.\n        """"""\n        input_layer = self.__network.get_layer(input_layer_name)\n\n        with tf.compat.v1.variable_scope(name):\n            channels_input = int(input_layer.get_shape()[-1])\n            alpha = self.__make_var(\'alpha\', shape=[channels_input])\n            output = tf.nn.relu(input_layer) + tf.multiply(alpha, -tf.nn.relu(-input_layer))\n\n        self.__network.add_layer(name, layer_output=output)\n\n    def new_max_pool(self, name:str, kernel_size: tuple, stride_size: tuple, padding=\'SAME\',\n                     input_layer_name: str=None):\n        """"""\n        Creates a new max pooling layer.\n        :param name: name for the layer.\n        :param kernel_size: tuple containing the size of the kernel (Width, Height)\n        :param stride_size: tuple containing the size of the stride (Width, Height)\n        :param padding: Type of padding. Available values are: (\'SAME\', \'VALID\')\n        :param input_layer_name: name of the input layer for this layer. If None, it will take the last added layer of\n        the network.\n        """"""\n\n        self.__validate_padding(padding)\n\n        input_layer = self.__network.get_layer(input_layer_name)\n\n        output = tf.nn.max_pool2d(input=input_layer,\n                                ksize=[1, kernel_size[1], kernel_size[0], 1],\n                                strides=[1, stride_size[1], stride_size[0], 1],\n                                padding=padding,\n                                name=name)\n\n        self.__network.add_layer(name, layer_output=output)\n\n    def new_fully_connected(self, name: str, output_count: int, relu=True, input_layer_name: str=None):\n        """"""\n        Creates a new fully connected layer.\n\n        :param name: name for the layer.\n        :param output_count: number of outputs of the fully connected layer.\n        :param relu: boolean flag to set if ReLu should be applied at the end of this layer.\n        :param input_layer_name: name of the input layer for this layer. If None, it will take the last added layer of\n        the network.\n        """"""\n\n        with tf.compat.v1.variable_scope(name):\n            input_layer = self.__network.get_layer(input_layer_name)\n            vectorized_input, dimension = self.vectorize_input(input_layer)\n\n            weights = self.__make_var(\'weights\', shape=[dimension, output_count])\n            biases = self.__make_var(\'biases\', shape=[output_count])\n            operation = tf.compat.v1.nn.relu_layer if relu else tf.compat.v1.nn.xw_plus_b\n\n            fc = operation(vectorized_input, weights, biases, name=name)\n\n        self.__network.add_layer(name, layer_output=fc)\n\n    def new_softmax(self, name, axis, input_layer_name: str=None):\n        """"""\n        Creates a new softmax layer\n        :param name: name to set for the layer\n        :param axis:\n        :param input_layer_name: name of the input layer for this layer. If None, it will take the last added layer of\n        the network.\n        """"""\n        input_layer = self.__network.get_layer(input_layer_name)\n\n        if LooseVersion(tf.__version__) < LooseVersion(""1.5.0""):\n            max_axis = tf.reduce_max(input_tensor=input_layer, axis=axis, keepdims=True)\n            target_exp = tf.exp(input_layer - max_axis)\n            normalize = tf.reduce_sum(input_tensor=target_exp, axis=axis, keepdims=True)\n        else:\n            max_axis = tf.reduce_max(input_tensor=input_layer, axis=axis, keepdims=True)\n            target_exp = tf.exp(input_layer - max_axis)\n            normalize = tf.reduce_sum(input_tensor=target_exp, axis=axis, keepdims=True)\n\n        softmax = tf.math.divide(target_exp, normalize, name)\n\n        self.__network.add_layer(name, layer_output=softmax)\n\n'"
mtcnn/mtcnn.py,0,"b'#!/usr/bin/python3\r\n# -*- coding: utf-8 -*-\r\n\r\n# MIT License\r\n#\r\n# Copyright (c) 2019 Iv\xc3\xa1n de Paz Centeno\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the ""Software""), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n\r\n#\r\n# This code is derived from the MTCNN implementation of David Sandberg for Facenet\r\n# (https://github.com/davidsandberg/facenet/)\r\n# It has been rebuilt from scratch, taking the David Sandberg\'s implementation as a reference.\r\n#\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport pkg_resources\r\n\r\nfrom mtcnn.exceptions import InvalidImage\r\nfrom mtcnn.network.factory import NetworkFactory\r\n\r\n__author__ = ""Iv\xc3\xa1n de Paz Centeno""\r\n\r\n\r\nclass StageStatus(object):\r\n    """"""\r\n    Keeps status between MTCNN stages\r\n    """"""\r\n\r\n    def __init__(self, pad_result: tuple = None, width=0, height=0):\r\n        self.width = width\r\n        self.height = height\r\n        self.dy = self.edy = self.dx = self.edx = self.y = self.ey = self.x = self.ex = self.tmpw = self.tmph = []\r\n\r\n        if pad_result is not None:\r\n            self.update(pad_result)\r\n\r\n    def update(self, pad_result: tuple):\r\n        s = self\r\n        s.dy, s.edy, s.dx, s.edx, s.y, s.ey, s.x, s.ex, s.tmpw, s.tmph = pad_result\r\n\r\n\r\nclass MTCNN(object):\r\n    """"""\r\n    Allows to perform MTCNN Detection ->\r\n        a) Detection of faces (with the confidence probability)\r\n        b) Detection of keypoints (left eye, right eye, nose, mouth_left, mouth_right)\r\n    """"""\r\n\r\n    def __init__(self, weights_file: str = None, min_face_size: int = 20, steps_threshold: list = None,\r\n                 scale_factor: float = 0.709):\r\n        """"""\r\n        Initializes the MTCNN.\r\n        :param weights_file: file uri with the weights of the P, R and O networks from MTCNN. By default it will load\r\n        the ones bundled with the package.\r\n        :param min_face_size: minimum size of the face to detect\r\n        :param steps_threshold: step\'s thresholds values\r\n        :param scale_factor: scale factor\r\n        """"""\r\n        if steps_threshold is None:\r\n            steps_threshold = [0.6, 0.7, 0.7]\r\n\r\n        if weights_file is None:\r\n            weights_file = pkg_resources.resource_stream(\'mtcnn\', \'data/mtcnn_weights.npy\')\r\n\r\n        self._min_face_size = min_face_size\r\n        self._steps_threshold = steps_threshold\r\n        self._scale_factor = scale_factor\r\n\r\n        self._pnet, self._rnet, self._onet = NetworkFactory().build_P_R_O_nets_from_file(weights_file)\r\n\r\n    @property\r\n    def min_face_size(self):\r\n        return self._min_face_size\r\n\r\n    @min_face_size.setter\r\n    def min_face_size(self, mfc=20):\r\n        try:\r\n            self._min_face_size = int(mfc)\r\n        except ValueError:\r\n            self._min_face_size = 20\r\n\r\n    def __compute_scale_pyramid(self, m, min_layer):\r\n        scales = []\r\n        factor_count = 0\r\n\r\n        while min_layer >= 12:\r\n            scales += [m * np.power(self._scale_factor, factor_count)]\r\n            min_layer = min_layer * self._scale_factor\r\n            factor_count += 1\r\n\r\n        return scales\r\n\r\n    @staticmethod\r\n    def __scale_image(image, scale: float):\r\n        """"""\r\n        Scales the image to a given scale.\r\n        :param image:\r\n        :param scale:\r\n        :return:\r\n        """"""\r\n        height, width, _ = image.shape\r\n\r\n        width_scaled = int(np.ceil(width * scale))\r\n        height_scaled = int(np.ceil(height * scale))\r\n\r\n        im_data = cv2.resize(image, (width_scaled, height_scaled), interpolation=cv2.INTER_AREA)\r\n\r\n        # Normalize the image\'s pixels\r\n        im_data_normalized = (im_data - 127.5) * 0.0078125\r\n\r\n        return im_data_normalized\r\n\r\n    @staticmethod\r\n    def __generate_bounding_box(imap, reg, scale, t):\r\n\r\n        # use heatmap to generate bounding boxes\r\n        stride = 2\r\n        cellsize = 12\r\n\r\n        imap = np.transpose(imap)\r\n        dx1 = np.transpose(reg[:, :, 0])\r\n        dy1 = np.transpose(reg[:, :, 1])\r\n        dx2 = np.transpose(reg[:, :, 2])\r\n        dy2 = np.transpose(reg[:, :, 3])\r\n\r\n        y, x = np.where(imap >= t)\r\n\r\n        if y.shape[0] == 1:\r\n            dx1 = np.flipud(dx1)\r\n            dy1 = np.flipud(dy1)\r\n            dx2 = np.flipud(dx2)\r\n            dy2 = np.flipud(dy2)\r\n\r\n        score = imap[(y, x)]\r\n        reg = np.transpose(np.vstack([dx1[(y, x)], dy1[(y, x)], dx2[(y, x)], dy2[(y, x)]]))\r\n\r\n        if reg.size == 0:\r\n            reg = np.empty(shape=(0, 3))\r\n\r\n        bb = np.transpose(np.vstack([y, x]))\r\n\r\n        q1 = np.fix((stride * bb + 1) / scale)\r\n        q2 = np.fix((stride * bb + cellsize) / scale)\r\n        boundingbox = np.hstack([q1, q2, np.expand_dims(score, 1), reg])\r\n\r\n        return boundingbox, reg\r\n\r\n    @staticmethod\r\n    def __nms(boxes, threshold, method):\r\n        """"""\r\n        Non Maximum Suppression.\r\n\r\n        :param boxes: np array with bounding boxes.\r\n        :param threshold:\r\n        :param method: NMS method to apply. Available values (\'Min\', \'Union\')\r\n        :return:\r\n        """"""\r\n        if boxes.size == 0:\r\n            return np.empty((0, 3))\r\n\r\n        x1 = boxes[:, 0]\r\n        y1 = boxes[:, 1]\r\n        x2 = boxes[:, 2]\r\n        y2 = boxes[:, 3]\r\n        s = boxes[:, 4]\r\n\r\n        area = (x2 - x1 + 1) * (y2 - y1 + 1)\r\n        sorted_s = np.argsort(s)\r\n\r\n        pick = np.zeros_like(s, dtype=np.int16)\r\n        counter = 0\r\n        while sorted_s.size > 0:\r\n            i = sorted_s[-1]\r\n            pick[counter] = i\r\n            counter += 1\r\n            idx = sorted_s[0:-1]\r\n\r\n            xx1 = np.maximum(x1[i], x1[idx])\r\n            yy1 = np.maximum(y1[i], y1[idx])\r\n            xx2 = np.minimum(x2[i], x2[idx])\r\n            yy2 = np.minimum(y2[i], y2[idx])\r\n\r\n            w = np.maximum(0.0, xx2 - xx1 + 1)\r\n            h = np.maximum(0.0, yy2 - yy1 + 1)\r\n\r\n            inter = w * h\r\n\r\n            if method is \'Min\':\r\n                o = inter / np.minimum(area[i], area[idx])\r\n            else:\r\n                o = inter / (area[i] + area[idx] - inter)\r\n\r\n            sorted_s = sorted_s[np.where(o <= threshold)]\r\n\r\n        pick = pick[0:counter]\r\n\r\n        return pick\r\n\r\n    @staticmethod\r\n    def __pad(total_boxes, w, h):\r\n        # compute the padding coordinates (pad the bounding boxes to square)\r\n        tmpw = (total_boxes[:, 2] - total_boxes[:, 0] + 1).astype(np.int32)\r\n        tmph = (total_boxes[:, 3] - total_boxes[:, 1] + 1).astype(np.int32)\r\n        numbox = total_boxes.shape[0]\r\n\r\n        dx = np.ones(numbox, dtype=np.int32)\r\n        dy = np.ones(numbox, dtype=np.int32)\r\n        edx = tmpw.copy().astype(np.int32)\r\n        edy = tmph.copy().astype(np.int32)\r\n\r\n        x = total_boxes[:, 0].copy().astype(np.int32)\r\n        y = total_boxes[:, 1].copy().astype(np.int32)\r\n        ex = total_boxes[:, 2].copy().astype(np.int32)\r\n        ey = total_boxes[:, 3].copy().astype(np.int32)\r\n\r\n        tmp = np.where(ex > w)\r\n        edx.flat[tmp] = np.expand_dims(-ex[tmp] + w + tmpw[tmp], 1)\r\n        ex[tmp] = w\r\n\r\n        tmp = np.where(ey > h)\r\n        edy.flat[tmp] = np.expand_dims(-ey[tmp] + h + tmph[tmp], 1)\r\n        ey[tmp] = h\r\n\r\n        tmp = np.where(x < 1)\r\n        dx.flat[tmp] = np.expand_dims(2 - x[tmp], 1)\r\n        x[tmp] = 1\r\n\r\n        tmp = np.where(y < 1)\r\n        dy.flat[tmp] = np.expand_dims(2 - y[tmp], 1)\r\n        y[tmp] = 1\r\n\r\n        return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\r\n\r\n    @staticmethod\r\n    def __rerec(bbox):\r\n        # convert bbox to square\r\n        height = bbox[:, 3] - bbox[:, 1]\r\n        width = bbox[:, 2] - bbox[:, 0]\r\n        max_side_length = np.maximum(width, height)\r\n        bbox[:, 0] = bbox[:, 0] + width * 0.5 - max_side_length * 0.5\r\n        bbox[:, 1] = bbox[:, 1] + height * 0.5 - max_side_length * 0.5\r\n        bbox[:, 2:4] = bbox[:, 0:2] + np.transpose(np.tile(max_side_length, (2, 1)))\r\n        return bbox\r\n\r\n    @staticmethod\r\n    def __bbreg(boundingbox, reg):\r\n        # calibrate bounding boxes\r\n        if reg.shape[1] == 1:\r\n            reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\r\n\r\n        w = boundingbox[:, 2] - boundingbox[:, 0] + 1\r\n        h = boundingbox[:, 3] - boundingbox[:, 1] + 1\r\n        b1 = boundingbox[:, 0] + reg[:, 0] * w\r\n        b2 = boundingbox[:, 1] + reg[:, 1] * h\r\n        b3 = boundingbox[:, 2] + reg[:, 2] * w\r\n        b4 = boundingbox[:, 3] + reg[:, 3] * h\r\n        boundingbox[:, 0:4] = np.transpose(np.vstack([b1, b2, b3, b4]))\r\n        return boundingbox\r\n\r\n    def detect_faces(self, img) -> list:\r\n        """"""\r\n        Detects bounding boxes from the specified image.\r\n        :param img: image to process\r\n        :return: list containing all the bounding boxes detected with their keypoints.\r\n        """"""\r\n        if img is None or not hasattr(img, ""shape""):\r\n            raise InvalidImage(""Image not valid."")\r\n\r\n        height, width, _ = img.shape\r\n        stage_status = StageStatus(width=width, height=height)\r\n\r\n        m = 12 / self._min_face_size\r\n        min_layer = np.amin([height, width]) * m\r\n\r\n        scales = self.__compute_scale_pyramid(m, min_layer)\r\n\r\n        stages = [self.__stage1, self.__stage2, self.__stage3]\r\n        result = [scales, stage_status]\r\n\r\n        # We pipe here each of the stages\r\n        for stage in stages:\r\n            result = stage(img, result[0], result[1])\r\n\r\n        [total_boxes, points] = result\r\n\r\n        bounding_boxes = []\r\n\r\n        for bounding_box, keypoints in zip(total_boxes, points.T):\r\n            x = max(0, int(bounding_box[0]))\r\n            y = max(0, int(bounding_box[1]))\r\n            width = int(bounding_box[2] - x)\r\n            height = int(bounding_box[3] - y)\r\n            bounding_boxes.append({\r\n                \'box\': [x, y, width, height],\r\n                \'confidence\': bounding_box[-1],\r\n                \'keypoints\': {\r\n                    \'left_eye\': (int(keypoints[0]), int(keypoints[5])),\r\n                    \'right_eye\': (int(keypoints[1]), int(keypoints[6])),\r\n                    \'nose\': (int(keypoints[2]), int(keypoints[7])),\r\n                    \'mouth_left\': (int(keypoints[3]), int(keypoints[8])),\r\n                    \'mouth_right\': (int(keypoints[4]), int(keypoints[9])),\r\n                }\r\n            })\r\n\r\n        return bounding_boxes\r\n\r\n    def __stage1(self, image, scales: list, stage_status: StageStatus):\r\n        """"""\r\n        First stage of the MTCNN.\r\n        :param image:\r\n        :param scales:\r\n        :param stage_status:\r\n        :return:\r\n        """"""\r\n        total_boxes = np.empty((0, 9))\r\n        status = stage_status\r\n\r\n        for scale in scales:\r\n            scaled_image = self.__scale_image(image, scale)\r\n\r\n            img_x = np.expand_dims(scaled_image, 0)\r\n            img_y = np.transpose(img_x, (0, 2, 1, 3))\r\n\r\n            out = self._pnet.predict(img_y)\r\n\r\n            out0 = np.transpose(out[0], (0, 2, 1, 3))\r\n            out1 = np.transpose(out[1], (0, 2, 1, 3))\r\n\r\n            boxes, _ = self.__generate_bounding_box(out1[0, :, :, 1].copy(),\r\n                                                    out0[0, :, :, :].copy(), scale, self._steps_threshold[0])\r\n\r\n            # inter-scale nms\r\n            pick = self.__nms(boxes.copy(), 0.5, \'Union\')\r\n            if boxes.size > 0 and pick.size > 0:\r\n                boxes = boxes[pick, :]\r\n                total_boxes = np.append(total_boxes, boxes, axis=0)\r\n\r\n        numboxes = total_boxes.shape[0]\r\n\r\n        if numboxes > 0:\r\n            pick = self.__nms(total_boxes.copy(), 0.7, \'Union\')\r\n            total_boxes = total_boxes[pick, :]\r\n\r\n            regw = total_boxes[:, 2] - total_boxes[:, 0]\r\n            regh = total_boxes[:, 3] - total_boxes[:, 1]\r\n\r\n            qq1 = total_boxes[:, 0] + total_boxes[:, 5] * regw\r\n            qq2 = total_boxes[:, 1] + total_boxes[:, 6] * regh\r\n            qq3 = total_boxes[:, 2] + total_boxes[:, 7] * regw\r\n            qq4 = total_boxes[:, 3] + total_boxes[:, 8] * regh\r\n\r\n            total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:, 4]]))\r\n            total_boxes = self.__rerec(total_boxes.copy())\r\n\r\n            total_boxes[:, 0:4] = np.fix(total_boxes[:, 0:4]).astype(np.int32)\r\n            status = StageStatus(self.__pad(total_boxes.copy(), stage_status.width, stage_status.height),\r\n                                 width=stage_status.width, height=stage_status.height)\r\n\r\n        return total_boxes, status\r\n\r\n    def __stage2(self, img, total_boxes, stage_status: StageStatus):\r\n        """"""\r\n        Second stage of the MTCNN.\r\n        :param img:\r\n        :param total_boxes:\r\n        :param stage_status:\r\n        :return:\r\n        """"""\r\n\r\n        num_boxes = total_boxes.shape[0]\r\n        if num_boxes == 0:\r\n            return total_boxes, stage_status\r\n\r\n        # second stage\r\n        tempimg = np.zeros(shape=(24, 24, 3, num_boxes))\r\n\r\n        for k in range(0, num_boxes):\r\n            tmp = np.zeros((int(stage_status.tmph[k]), int(stage_status.tmpw[k]), 3))\r\n\r\n            tmp[stage_status.dy[k] - 1:stage_status.edy[k], stage_status.dx[k] - 1:stage_status.edx[k], :] = \\\r\n                img[stage_status.y[k] - 1:stage_status.ey[k], stage_status.x[k] - 1:stage_status.ex[k], :]\r\n\r\n            if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\r\n                tempimg[:, :, :, k] = cv2.resize(tmp, (24, 24), interpolation=cv2.INTER_AREA)\r\n\r\n            else:\r\n                return np.empty(shape=(0,)), stage_status\r\n\r\n        tempimg = (tempimg - 127.5) * 0.0078125\r\n        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))\r\n\r\n        out = self._rnet.predict(tempimg1)\r\n\r\n        out0 = np.transpose(out[0])\r\n        out1 = np.transpose(out[1])\r\n\r\n        score = out1[1, :]\r\n\r\n        ipass = np.where(score > self._steps_threshold[1])\r\n\r\n        total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(), np.expand_dims(score[ipass].copy(), 1)])\r\n\r\n        mv = out0[:, ipass[0]]\r\n\r\n        if total_boxes.shape[0] > 0:\r\n            pick = self.__nms(total_boxes, 0.7, \'Union\')\r\n            total_boxes = total_boxes[pick, :]\r\n            total_boxes = self.__bbreg(total_boxes.copy(), np.transpose(mv[:, pick]))\r\n            total_boxes = self.__rerec(total_boxes.copy())\r\n\r\n        return total_boxes, stage_status\r\n\r\n    def __stage3(self, img, total_boxes, stage_status: StageStatus):\r\n        """"""\r\n        Third stage of the MTCNN.\r\n\r\n        :param img:\r\n        :param total_boxes:\r\n        :param stage_status:\r\n        :return:\r\n        """"""\r\n        num_boxes = total_boxes.shape[0]\r\n        if num_boxes == 0:\r\n            return total_boxes, np.empty(shape=(0,))\r\n\r\n        total_boxes = np.fix(total_boxes).astype(np.int32)\r\n\r\n        status = StageStatus(self.__pad(total_boxes.copy(), stage_status.width, stage_status.height),\r\n                             width=stage_status.width, height=stage_status.height)\r\n\r\n        tempimg = np.zeros((48, 48, 3, num_boxes))\r\n\r\n        for k in range(0, num_boxes):\r\n\r\n            tmp = np.zeros((int(status.tmph[k]), int(status.tmpw[k]), 3))\r\n\r\n            tmp[status.dy[k] - 1:status.edy[k], status.dx[k] - 1:status.edx[k], :] = \\\r\n                img[status.y[k] - 1:status.ey[k], status.x[k] - 1:status.ex[k], :]\r\n\r\n            if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\r\n                tempimg[:, :, :, k] = cv2.resize(tmp, (48, 48), interpolation=cv2.INTER_AREA)\r\n            else:\r\n                return np.empty(shape=(0,)), np.empty(shape=(0,))\r\n\r\n        tempimg = (tempimg - 127.5) * 0.0078125\r\n        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))\r\n\r\n        out = self._onet.predict(tempimg1)\r\n        out0 = np.transpose(out[0])\r\n        out1 = np.transpose(out[1])\r\n        out2 = np.transpose(out[2])\r\n\r\n        score = out2[1, :]\r\n\r\n        points = out1\r\n\r\n        ipass = np.where(score > self._steps_threshold[2])\r\n\r\n        points = points[:, ipass[0]]\r\n\r\n        total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(), np.expand_dims(score[ipass].copy(), 1)])\r\n\r\n        mv = out0[:, ipass[0]]\r\n\r\n        w = total_boxes[:, 2] - total_boxes[:, 0] + 1\r\n        h = total_boxes[:, 3] - total_boxes[:, 1] + 1\r\n\r\n        points[0:5, :] = np.tile(w, (5, 1)) * points[0:5, :] + np.tile(total_boxes[:, 0], (5, 1)) - 1\r\n        points[5:10, :] = np.tile(h, (5, 1)) * points[5:10, :] + np.tile(total_boxes[:, 1], (5, 1)) - 1\r\n\r\n        if total_boxes.shape[0] > 0:\r\n            total_boxes = self.__bbreg(total_boxes.copy(), np.transpose(mv))\r\n            pick = self.__nms(total_boxes.copy(), 0.7, \'Min\')\r\n            total_boxes = total_boxes[pick, :]\r\n            points = points[:, pick]\r\n\r\n        return total_boxes, points\r\n'"
mtcnn/network.py,5,"b'#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\n#MIT License\n#\n#Copyright (c) 2018 Iv\xc3\xa1n de Paz Centeno\n#\n#Permission is hereby granted, free of charge, to any person obtaining a copy\n#of this software and associated documentation files (the ""Software""), to deal\n#in the Software without restriction, including without limitation the rights\n#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n#copies of the Software, and to permit persons to whom the Software is\n#furnished to do so, subject to the following conditions:\n#\n#The above copyright notice and this permission notice shall be included in all\n#copies or substantial portions of the Software.\n#\n#THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n#SOFTWARE.\n\nimport tensorflow as tf\n\n__author__ = ""Iv\xc3\xa1n de Paz Centeno""\n\n\nclass Network(object):\n\n    def __init__(self, session, trainable: bool=True):\n        """"""\n        Initializes the network.\n        :param trainable: flag to determine if this network should be trainable or not.\n        """"""\n        self._session = session\n        self.__trainable = trainable\n        self.__layers = {}\n        self.__last_layer_name = None\n\n        with tf.compat.v1.variable_scope(self.__class__.__name__.lower()):\n            self._config()\n\n    def _config(self):\n        """"""\n        Configures the network layers.\n        It is usually done using the LayerFactory() class.\n        """"""\n        raise NotImplementedError(""This method must be implemented by the network."")\n\n    def add_layer(self, name: str, layer_output):\n        """"""\n        Adds a layer to the network.\n        :param name: name of the layer to add\n        :param layer_output: output layer.\n        """"""\n        self.__layers[name] = layer_output\n        self.__last_layer_name = name\n\n    def get_layer(self, name: str=None):\n        """"""\n        Retrieves the layer by its name.\n        :param name: name of the layer to retrieve. If name is None, it will retrieve the last added layer to the\n        network.\n        :return: layer output\n        """"""\n        if name is None:\n            name = self.__last_layer_name\n\n        return self.__layers[name]\n\n    def is_trainable(self):\n        """"""\n        Getter for the trainable flag.\n        """"""\n        return self.__trainable\n\n    def set_weights(self, weights_values: dict, ignore_missing=False):\n        """"""\n        Sets the weights values of the network.\n        :param weights_values: dictionary with weights for each layer\n        """"""\n        network_name = self.__class__.__name__.lower()\n\n        with tf.compat.v1.variable_scope(network_name):\n            for layer_name in weights_values:\n                with tf.compat.v1.variable_scope(layer_name, reuse=True):\n                    for param_name, data in weights_values[layer_name].items():\n                        try:\n                            var = tf.compat.v1.get_variable(param_name, use_resource=False)\n                            self._session.run(var.assign(data))\n\n                        except ValueError:\n                            if not ignore_missing:\n                                raise\n\n    def feed(self, image):\n        """"""\n        Feeds the network with an image\n        :param image: image (perhaps loaded with CV2)\n        :return: network result\n        """"""\n        network_name = self.__class__.__name__.lower()\n\n        with tf.compat.v1.variable_scope(network_name):\n            return self._feed(image)\n\n    def _feed(self, image):\n        raise NotImplementedError(""Method not implemented."")'"
tests/__init__.py,0,b''
tests/test_mtcnn.py,0,"b'import unittest\nimport cv2\n\nfrom mtcnn.exceptions import InvalidImage\nfrom mtcnn import MTCNN\n\nmtcnn = None\n\n\nclass TestMTCNN(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        global mtcnn\n        mtcnn = MTCNN()\n\n    def test_detect_faces(self):\n        """"""\n        MTCNN is able to detect faces and landmarks on an image\n        :return:\n        """"""\n        ivan = cv2.imread(""ivan.jpg"")\n\n        result = mtcnn.detect_faces(ivan)  # type: list\n\n        self.assertEqual(len(result), 1)\n\n        first = result[0]\n\n        self.assertIn(\'box\', first)\n        self.assertIn(\'keypoints\', first)\n        self.assertTrue(len(first[\'box\']), 1)\n        self.assertTrue(len(first[\'keypoints\']), 5)\n\n        keypoints = first[\'keypoints\']  # type: dict\n        self.assertIn(\'nose\', keypoints)\n        self.assertIn(\'mouth_left\', keypoints)\n        self.assertIn(\'mouth_right\', keypoints)\n        self.assertIn(\'left_eye\', keypoints)\n        self.assertIn(\'right_eye\', keypoints)\n\n        self.assertEqual(len(keypoints[\'nose\']), 2)\n        self.assertEqual(len(keypoints[\'mouth_left\']), 2)\n        self.assertEqual(len(keypoints[\'mouth_right\']), 2)\n        self.assertEqual(len(keypoints[\'left_eye\']), 2)\n        self.assertEqual(len(keypoints[\'right_eye\']), 2)\n\n    def test_detect_faces_invalid_content(self):\n        """"""\n        MTCNN detects invalid images\n        :return:\n        """"""\n        ivan = cv2.imread(""example.py"")\n\n        with self.assertRaises(InvalidImage):\n            result = mtcnn.detect_faces(ivan)  # type: list\n\n    def test_detect_no_faces_on_no_faces_content(self):\n        """"""\n        MTCNN successfully reports an empty list when no faces are detected.\n        :return:\n        """"""\n        ivan = cv2.imread(""no-faces.jpg"")\n\n        result = mtcnn.detect_faces(ivan)  # type: list\n        self.assertEqual(len(result), 0)\n\n\n    def test_mtcnn_multiple_instances(self):\n        """"""\n        Multiple instances of MTCNN can be created in the same thread.\n        :return:\n        """"""\n        detector_1 = MTCNN(steps_threshold=[.2, .7, .7])\n        detector_2 = MTCNN(steps_threshold=[.1, .1, .1])\n\n        ivan = cv2.imread(""ivan.jpg"")\n\n        faces_1 = detector_1.detect_faces(ivan)\n        faces_2 = detector_2.detect_faces(ivan)\n\n        self.assertEqual(len(faces_1), 1)\n        self.assertGreater(len(faces_2), 1)\n\n    @classmethod\n    def tearDownClass(cls):\n        global mtcnn\n        del mtcnn\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
mtcnn/exceptions/__init__.py,0,"b'#!/usr/bin/python3\r\n# -*- coding: utf-8 -*-\r\n\r\n# MIT License\r\n#\r\n# Copyright (c) 2019 Iv\xc3\xa1n de Paz Centeno\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the ""Software""), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n\r\nfrom mtcnn.exceptions.invalid_image import InvalidImage\r\n'"
mtcnn/exceptions/invalid_image.py,0,"b'#!/usr/bin/python3\r\n# -*- coding: utf-8 -*-\r\n\r\n# MIT License\r\n#\r\n# Copyright (c) 2019 Iv\xc3\xa1n de Paz Centeno\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the ""Software""), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n\r\n\r\n__author__ = ""Iv\xc3\xa1n de Paz Centeno""\r\n\r\nclass InvalidImage(Exception):\r\n    pass\r\n'"
mtcnn/network/__init__.py,0,"b'#!/usr/bin/python3\r\n# -*- coding: utf-8 -*-\r\n\r\n# MIT License\r\n#\r\n# Copyright (c) 2019 Iv\xc3\xa1n de Paz Centeno\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the ""Software""), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n'"
mtcnn/network/factory.py,0,"b'#!/usr/bin/python3\r\n# -*- coding: utf-8 -*-\r\n\r\n# MIT License\r\n#\r\n# Copyright (c) 2019 Iv\xc3\xa1n de Paz Centeno\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a copy\r\n# of this software and associated documentation files (the ""Software""), to deal\r\n# in the Software without restriction, including without limitation the rights\r\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n# copies of the Software, and to permit persons to whom the Software is\r\n# furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in all\r\n# copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n# SOFTWARE.\r\n\r\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, PReLU, Flatten, Softmax\r\nfrom tensorflow.keras.models import Model\r\n\r\nimport numpy as np\r\n\r\n\r\nclass NetworkFactory:\r\n\r\n    def build_pnet(self, input_shape=None):\r\n        if input_shape is None:\r\n            input_shape = (None, None, 3)\r\n\r\n        p_inp = Input(input_shape)\r\n\r\n        p_layer = Conv2D(10, kernel_size=(3, 3), strides=(1, 1), padding=""valid"")(p_inp)\r\n        p_layer = PReLU(shared_axes=[1, 2])(p_layer)\r\n        p_layer = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=""same"")(p_layer)\r\n\r\n        p_layer = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding=""valid"")(p_layer)\r\n        p_layer = PReLU(shared_axes=[1, 2])(p_layer)\r\n\r\n        p_layer = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding=""valid"")(p_layer)\r\n        p_layer = PReLU(shared_axes=[1, 2])(p_layer)\r\n\r\n        p_layer_out1 = Conv2D(2, kernel_size=(1, 1), strides=(1, 1))(p_layer)\r\n        p_layer_out1 = Softmax(axis=3)(p_layer_out1)\r\n\r\n        p_layer_out2 = Conv2D(4, kernel_size=(1, 1), strides=(1, 1))(p_layer)\r\n\r\n        p_net = Model(p_inp, [p_layer_out2, p_layer_out1])\r\n\r\n        return p_net\r\n\r\n    def build_rnet(self, input_shape=None):\r\n        if input_shape is None:\r\n            input_shape = (24, 24, 3)\r\n\r\n        r_inp = Input(input_shape)\r\n\r\n        r_layer = Conv2D(28, kernel_size=(3, 3), strides=(1, 1), padding=""valid"")(r_inp)\r\n        r_layer = PReLU(shared_axes=[1, 2])(r_layer)\r\n        r_layer = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=""same"")(r_layer)\r\n\r\n        r_layer = Conv2D(48, kernel_size=(3, 3), strides=(1, 1), padding=""valid"")(r_layer)\r\n        r_layer = PReLU(shared_axes=[1, 2])(r_layer)\r\n        r_layer = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=""valid"")(r_layer)\r\n\r\n        r_layer = Conv2D(64, kernel_size=(2, 2), strides=(1, 1), padding=""valid"")(r_layer)\r\n        r_layer = PReLU(shared_axes=[1, 2])(r_layer)\r\n        r_layer = Flatten()(r_layer)\r\n        r_layer = Dense(128)(r_layer)\r\n        r_layer = PReLU()(r_layer)\r\n\r\n        r_layer_out1 = Dense(2)(r_layer)\r\n        r_layer_out1 = Softmax(axis=1)(r_layer_out1)\r\n\r\n        r_layer_out2 = Dense(4)(r_layer)\r\n\r\n        r_net = Model(r_inp, [r_layer_out2, r_layer_out1])\r\n\r\n        return r_net\r\n\r\n    def build_onet(self, input_shape=None):\r\n        if input_shape is None:\r\n            input_shape = (48, 48, 3)\r\n\r\n        o_inp = Input(input_shape)\r\n        o_layer = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding=""valid"")(o_inp)\r\n        o_layer = PReLU(shared_axes=[1, 2])(o_layer)\r\n        o_layer = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=""same"")(o_layer)\r\n\r\n        o_layer = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding=""valid"")(o_layer)\r\n        o_layer = PReLU(shared_axes=[1, 2])(o_layer)\r\n        o_layer = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=""valid"")(o_layer)\r\n\r\n        o_layer = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding=""valid"")(o_layer)\r\n        o_layer = PReLU(shared_axes=[1, 2])(o_layer)\r\n        o_layer = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=""same"")(o_layer)\r\n\r\n        o_layer = Conv2D(128, kernel_size=(2, 2), strides=(1, 1), padding=""valid"")(o_layer)\r\n        o_layer = PReLU(shared_axes=[1, 2])(o_layer)\r\n\r\n        o_layer = Flatten()(o_layer)\r\n        o_layer = Dense(256)(o_layer)\r\n        o_layer = PReLU()(o_layer)\r\n\r\n        o_layer_out1 = Dense(2)(o_layer)\r\n        o_layer_out1 = Softmax(axis=1)(o_layer_out1)\r\n        o_layer_out2 = Dense(4)(o_layer)\r\n        o_layer_out3 = Dense(10)(o_layer)\r\n\r\n        o_net = Model(o_inp, [o_layer_out2, o_layer_out3, o_layer_out1])\r\n        return o_net\r\n\r\n    def build_P_R_O_nets_from_file(self, weights_file):\r\n        weights = np.load(weights_file, allow_pickle=True).tolist()\r\n\r\n        p_net = self.build_pnet()\r\n        r_net = self.build_rnet()\r\n        o_net = self.build_onet()\r\n\r\n        p_net.set_weights(weights[\'pnet\'])\r\n        r_net.set_weights(weights[\'rnet\'])\r\n        o_net.set_weights(weights[\'onet\'])\r\n\r\n        return p_net, r_net, o_net\r\n'"
