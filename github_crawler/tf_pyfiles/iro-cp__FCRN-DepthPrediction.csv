file_path,api_count,code
tensorflow/predict.py,3,"b""import argparse\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\nimport models\n\ndef predict(model_data_path, image_path):\n\n    \n    # Default input size\n    height = 228\n    width = 304\n    channels = 3\n    batch_size = 1\n   \n    # Read image\n    img = Image.open(image_path)\n    img = img.resize([width,height], Image.ANTIALIAS)\n    img = np.array(img).astype('float32')\n    img = np.expand_dims(np.asarray(img), axis = 0)\n   \n    # Create a placeholder for the input image\n    input_node = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n\n    # Construct the network\n    net = models.ResNet50UpProj({'data': input_node}, batch_size, 1, False)\n        \n    with tf.Session() as sess:\n\n        # Load the converted parameters\n        print('Loading the model')\n\n        # Use to load from ckpt file\n        saver = tf.train.Saver()     \n        saver.restore(sess, model_data_path)\n\n        # Use to load from npy file\n        #net.load(model_data_path, sess) \n\n        # Evalute the network for the given image\n        pred = sess.run(net.get_output(), feed_dict={input_node: img})\n        \n        # Plot result\n        fig = plt.figure()\n        ii = plt.imshow(pred[0,:,:,0], interpolation='nearest')\n        fig.colorbar(ii)\n        plt.show()\n        \n        return pred\n        \n                \ndef main():\n    # Parse arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument('model_path', help='Converted parameters for the model')\n    parser.add_argument('image_paths', help='Directory of images to predict')\n    args = parser.parse_args()\n\n    # Predict the image\n    pred = predict(args.model_path, args.image_paths)\n    \n    os._exit(0)\n\nif __name__ == '__main__':\n    main()\n\n        \n\n\n\n"""
tensorflow/models/__init__.py,0,b'from .fcrn import ResNet50UpProj\n'
tensorflow/models/fcrn.py,0,"b""from .network import Network\n\nclass ResNet50UpProj(Network):\n    def setup(self):\n        (self.feed('data')\n             .conv(7, 7, 64, 2, 2, relu=False, name='conv1')\n             .batch_normalization(relu=True, name='bn_conv1')\n             .max_pool(3, 3, 2, 2, name='pool1')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch1')\n             .batch_normalization(name='bn2a_branch1'))\n\n        (self.feed('pool1')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2a_branch2a')\n             .batch_normalization(relu=True, name='bn2a_branch2a')\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2a_branch2b')\n             .batch_normalization(relu=True, name='bn2a_branch2b')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2a_branch2c')\n             .batch_normalization(name='bn2a_branch2c'))\n\n        (self.feed('bn2a_branch1',\n                   'bn2a_branch2c')\n             .add(name='res2a')\n             .relu(name='res2a_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2b_branch2a')\n             .batch_normalization(relu=True, name='bn2b_branch2a')\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2b_branch2b')\n             .batch_normalization(relu=True, name='bn2b_branch2b')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2b_branch2c')\n             .batch_normalization(name='bn2b_branch2c'))\n\n        (self.feed('res2a_relu',\n                   'bn2b_branch2c')\n             .add(name='res2b')\n             .relu(name='res2b_relu')\n             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2c_branch2a')\n             .batch_normalization(relu=True, name='bn2c_branch2a')\n             .conv(3, 3, 64, 1, 1, biased=False, relu=False, name='res2c_branch2b')\n             .batch_normalization(relu=True, name='bn2c_branch2b')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res2c_branch2c')\n             .batch_normalization(name='bn2c_branch2c'))\n\n        (self.feed('res2b_relu',\n                   'bn2c_branch2c')\n             .add(name='res2c')\n             .relu(name='res2c_relu')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res3a_branch1')\n             .batch_normalization(name='bn3a_branch1'))\n\n        (self.feed('res2c_relu')\n             .conv(1, 1, 128, 2, 2, biased=False, relu=False, name='res3a_branch2a')\n             .batch_normalization(relu=True, name='bn3a_branch2a')\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3a_branch2b')\n             .batch_normalization(relu=True, name='bn3a_branch2b')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3a_branch2c')\n             .batch_normalization(name='bn3a_branch2c'))\n\n        (self.feed('bn3a_branch1',\n                   'bn3a_branch2c')\n             .add(name='res3a')\n             .relu(name='res3a_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b_branch2a')\n             .batch_normalization(relu=True, name='bn3b_branch2a')\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3b_branch2b')\n             .batch_normalization(relu=True, name='bn3b_branch2b')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3b_branch2c')\n             .batch_normalization(name='bn3b_branch2c'))\n\n        (self.feed('res3a_relu',\n                   'bn3b_branch2c')\n             .add(name='res3b')\n             .relu(name='res3b_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3c_branch2a')\n             .batch_normalization(relu=True, name='bn3c_branch2a')\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3c_branch2b')\n             .batch_normalization(relu=True, name='bn3c_branch2b')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3c_branch2c')\n             .batch_normalization(name='bn3c_branch2c'))\n\n        (self.feed('res3b_relu',\n                   'bn3c_branch2c')\n             .add(name='res3c')\n             .relu(name='res3c_relu')\n             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3d_branch2a')\n             .batch_normalization(relu=True, name='bn3d_branch2a')\n             .conv(3, 3, 128, 1, 1, biased=False, relu=False, name='res3d_branch2b')\n             .batch_normalization(relu=True, name='bn3d_branch2b')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res3d_branch2c')\n             .batch_normalization(name='bn3d_branch2c'))\n\n        (self.feed('res3c_relu',\n                   'bn3d_branch2c')\n             .add(name='res3d')\n             .relu(name='res3d_relu')\n             .conv(1, 1, 1024, 2, 2, biased=False, relu=False, name='res4a_branch1')\n             .batch_normalization(name='bn4a_branch1'))\n\n        (self.feed('res3d_relu')\n             .conv(1, 1, 256, 2, 2, biased=False, relu=False, name='res4a_branch2a')\n             .batch_normalization(relu=True, name='bn4a_branch2a')\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4a_branch2b')\n             .batch_normalization(relu=True, name='bn4a_branch2b')\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4a_branch2c')\n             .batch_normalization(name='bn4a_branch2c'))\n\n        (self.feed('bn4a_branch1',\n                   'bn4a_branch2c')\n             .add(name='res4a')\n             .relu(name='res4a_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b_branch2a')\n             .batch_normalization(relu=True, name='bn4b_branch2a')\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4b_branch2b')\n             .batch_normalization(relu=True, name='bn4b_branch2b')\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4b_branch2c')\n             .batch_normalization(name='bn4b_branch2c'))\n\n        (self.feed('res4a_relu',\n                   'bn4b_branch2c')\n             .add(name='res4b')\n             .relu(name='res4b_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4c_branch2a')\n             .batch_normalization(relu=True, name='bn4c_branch2a')\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4c_branch2b')\n             .batch_normalization(relu=True, name='bn4c_branch2b')\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4c_branch2c')\n             .batch_normalization(name='bn4c_branch2c'))\n\n        (self.feed('res4b_relu',\n                   'bn4c_branch2c')\n             .add(name='res4c')\n             .relu(name='res4c_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4d_branch2a')\n             .batch_normalization(relu=True, name='bn4d_branch2a')\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4d_branch2b')\n             .batch_normalization(relu=True, name='bn4d_branch2b')\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4d_branch2c')\n             .batch_normalization(name='bn4d_branch2c'))\n\n        (self.feed('res4c_relu',\n                   'bn4d_branch2c')\n             .add(name='res4d')\n             .relu(name='res4d_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4e_branch2a')\n             .batch_normalization(relu=True, name='bn4e_branch2a')\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4e_branch2b')\n             .batch_normalization(relu=True, name='bn4e_branch2b')\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4e_branch2c')\n             .batch_normalization(name='bn4e_branch2c'))\n\n        (self.feed('res4d_relu',\n                   'bn4e_branch2c')\n             .add(name='res4e')\n             .relu(name='res4e_relu')\n             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4f_branch2a')\n             .batch_normalization(relu=True, name='bn4f_branch2a')\n             .conv(3, 3, 256, 1, 1, biased=False, relu=False, name='res4f_branch2b')\n             .batch_normalization(relu=True, name='bn4f_branch2b')\n             .conv(1, 1, 1024, 1, 1, biased=False, relu=False, name='res4f_branch2c')\n             .batch_normalization(name='bn4f_branch2c'))\n\n        (self.feed('res4e_relu',\n                   'bn4f_branch2c')\n             .add(name='res4f')\n             .relu(name='res4f_relu')\n             .conv(1, 1, 2048, 2, 2, biased=False, relu=False, name='res5a_branch1')\n             .batch_normalization(name='bn5a_branch1'))\n\n        (self.feed('res4f_relu')\n             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res5a_branch2a')\n             .batch_normalization(relu=True, name='bn5a_branch2a')\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5a_branch2b')\n             .batch_normalization(relu=True, name='bn5a_branch2b')\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5a_branch2c')\n             .batch_normalization(name='bn5a_branch2c'))\n\n        (self.feed('bn5a_branch1',\n                   'bn5a_branch2c')\n             .add(name='res5a')\n             .relu(name='res5a_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5b_branch2a')\n             .batch_normalization(relu=True, name='bn5b_branch2a')\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5b_branch2b')\n             .batch_normalization(relu=True, name='bn5b_branch2b')\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5b_branch2c')\n             .batch_normalization(name='bn5b_branch2c'))\n\n        (self.feed('res5a_relu',\n                   'bn5b_branch2c')\n             .add(name='res5b')\n             .relu(name='res5b_relu')\n             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5c_branch2a')\n             .batch_normalization(relu=True, name='bn5c_branch2a')\n             .conv(3, 3, 512, 1, 1, biased=False, relu=False, name='res5c_branch2b')\n             .batch_normalization(relu=True, name='bn5c_branch2b')\n             .conv(1, 1, 2048, 1, 1, biased=False, relu=False, name='res5c_branch2c')\n             .batch_normalization(name='bn5c_branch2c'))\n\n        (self.feed('res5b_relu',\n                   'bn5c_branch2c')\n             .add(name='res5c')\n             .relu(name='res5c_relu')\n             .conv(1, 1, 1024, 1, 1, biased=True, relu=False, name='layer1')\n             .batch_normalization(relu=False, name='layer1_BN')\n             .up_project([3, 3, 1024, 512], id = '2x', stride = 1, BN=True)\n             .up_project([3, 3, 512, 256], id = '4x', stride = 1, BN=True)\n             .up_project([3, 3, 256, 128], id = '8x', stride = 1, BN=True)\n             .up_project([3, 3, 128, 64], id = '16x', stride = 1, BN=True)\n             .dropout(name = 'drop', keep_prob = 1.)\n             .conv(3, 3, 1, 1, 1, name = 'ConvPred'))\n"""
tensorflow/models/network.py,43,"b'import numpy as np\nimport tensorflow as tf\n\n# ----------------------------------------------------------------------------------\n# Commonly used layers and operations based on ethereon\'s implementation \n# https://github.com/ethereon/caffe-tensorflow\n# Slight modifications may apply. FCRN-specific operations have also been appended. \n# ----------------------------------------------------------------------------------\n# Thanks to *Helisa Dhamo* for the model conversion and integration into TensorFlow.\n# ----------------------------------------------------------------------------------\n\nDEFAULT_PADDING = \'SAME\'\n\n\ndef get_incoming_shape(incoming):\n    """""" Returns the incoming data shape """"""\n    if isinstance(incoming, tf.Tensor):\n        return incoming.get_shape().as_list()\n    elif type(incoming) in [np.array, list, tuple]:\n        return np.shape(incoming)\n    else:\n        raise Exception(""Invalid incoming layer."")\n\n\ndef interleave(tensors, axis):\n    old_shape = get_incoming_shape(tensors[0])[1:]\n    new_shape = [-1] + old_shape\n    new_shape[axis] *= len(tensors)\n    return tf.reshape(tf.stack(tensors, axis + 1), new_shape)\n\ndef layer(op):\n    \'\'\'Decorator for composable network layers.\'\'\'\n\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n\n        # Figure out the layer inputs.\n        if len(self.terminals) == 0:\n            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n        elif len(self.terminals) == 1:\n            layer_input = self.terminals[0]\n        else:\n            layer_input = list(self.terminals)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n\n    return layer_decorated\n\n\nclass Network(object):\n\n    def __init__(self, inputs, batch, keep_prob, is_training, trainable = True):\n        # The input nodes for this network\n        self.inputs = inputs\n        # The current list of terminal nodes\n        self.terminals = []\n        # Mapping from layer names to layers\n        self.layers = dict(inputs)\n        # If true, the resulting variables are set as trainable\n        self.trainable = trainable\n        self.batch_size = batch\n        self.keep_prob = keep_prob\n        self.is_training = is_training\n        self.setup()\n\n\n    def setup(self):\n        \'\'\'Construct the network. \'\'\'\n        raise NotImplementedError(\'Must be implemented by the subclass.\')\n\n    def load(self, data_path, session, ignore_missing=False):\n        \'\'\'Load network weights.\n        data_path: The path to the numpy-serialized network weights\n        session: The current TensorFlow session\n        ignore_missing: If true, serialized weights for missing layers are ignored.\n        \'\'\'\n        data_dict = np.load(data_path, encoding=\'latin1\').item()\n        for op_name in data_dict: \n            with tf.variable_scope(op_name, reuse=True):\n                for param_name, data in iter(data_dict[op_name].items()):      \n                    try:\n                        var = tf.get_variable(param_name)\n                        session.run(var.assign(data))\n\n                    except ValueError:\n                        if not ignore_missing:\n                            raise\n\n    def feed(self, *args):\n        \'\'\'Set the input(s) for the next operation by replacing the terminal nodes.\n        The arguments can be either layer names or the actual layers.\n        \'\'\'\n        assert len(args) != 0\n        self.terminals = []\n        for fed_layer in args:\n            if isinstance(fed_layer, str):\n                try:\n                    fed_layer = self.layers[fed_layer]\n                except KeyError:\n                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n            self.terminals.append(fed_layer)\n        return self\n\n    def get_output(self):\n        \'\'\'Returns the current network output.\'\'\'\n        return self.terminals[-1]\n\n    def get_layer_output(self, name):\n        return self.layers[name]\n\n    def get_unique_name(self, prefix):\n        \'\'\'Returns an index-suffixed unique name for the given prefix.\n        This is used for auto-generating layer names based on the type-prefix.\n        \'\'\'\n        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n        return \'%s_%d\' % (prefix, ident)\n\n    def make_var(self, name, shape):\n        \'\'\'Creates a new TensorFlow variable.\'\'\'\n        return tf.get_variable(name, shape, dtype = \'float32\', trainable=self.trainable)\n\n    def validate_padding(self, padding):\n        \'\'\'Verifies that the padding is one of the supported ones.\'\'\'\n        assert padding in (\'SAME\', \'VALID\')\n\n    @layer\n    def conv(self,\n             input_data,\n             k_h,\n             k_w,\n             c_o,\n             s_h,\n             s_w,\n             name,\n             relu=True,\n             padding=DEFAULT_PADDING,\n             group=1,\n             biased=True):\n\n        # Verify that the padding is acceptable\n        self.validate_padding(padding)\n        # Get the number of channels in the input\n        c_i = input_data.get_shape()[-1]\n\n        if (padding == \'SAME\'):\n            input_data = tf.pad(input_data, [[0, 0], [(k_h - 1)//2, (k_h - 1)//2], [(k_w - 1)//2, (k_w - 1)//2], [0, 0]], ""CONSTANT"")\n        \n        # Verify that the grouping parameter is valid\n        assert c_i % group == 0\n        assert c_o % group == 0\n        # Convolution for a given input and kernel\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=\'VALID\')\n        \n        with tf.variable_scope(name) as scope:\n            kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n\n            if group == 1:\n                # This is the common-case. Convolve the input without any further complications.\n                output = convolve(input_data, kernel)\n            else:\n                # Split the input into groups and then convolve each of them independently\n\n                input_groups = tf.split(3, group, input_data)\n                kernel_groups = tf.split(3, group, kernel)\n                output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n                # Concatenate the groups\n                output = tf.concat(3, output_groups)\n\n            # Add the biases\n            if biased:\n                biases = self.make_var(\'biases\', [c_o])\n                output = tf.nn.bias_add(output, biases)\n            if relu:\n                # ReLU non-linearity\n                output = tf.nn.relu(output, name=scope.name)\n\n            return output\n\n    @layer\n    def relu(self, input_data, name):\n        return tf.nn.relu(input_data, name=name)\n\n    @layer\n    def max_pool(self, input_data, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(input_data,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def avg_pool(self, input_data, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n        self.validate_padding(padding)\n        return tf.nn.avg_pool(input_data,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def lrn(self, input_data, radius, alpha, beta, name, bias=1.0):\n        return tf.nn.local_response_normalization(input_data,\n                                                  depth_radius=radius,\n                                                  alpha=alpha,\n                                                  beta=beta,\n                                                  bias=bias,\n                                                  name=name)\n\n    @layer\n    def concat(self, inputs, axis, name):\n        return tf.concat(concat_dim=axis, values=inputs, name=name)\n\n    @layer\n    def add(self, inputs, name):\n        return tf.add_n(inputs, name=name)\n\n    @layer\n    def fc(self, input_data, num_out, name, relu=True):\n        with tf.variable_scope(name) as scope:\n            input_shape = input_data.get_shape()\n            if input_shape.ndims == 4:\n                # The input is spatial. Vectorize it first.\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= d\n                feed_in = tf.reshape(input_data, [-1, dim])\n            else:\n                feed_in, dim = (input_data, input_shape[-1].value)\n            weights = self.make_var(\'weights\', shape=[dim, num_out])\n            biases = self.make_var(\'biases\', [num_out])\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=scope.name)\n            return fc\n\n    @layer\n    def softmax(self, input_data, name):\n        input_shape = map(lambda v: v.value, input_data.get_shape())\n        if len(input_shape) > 2:\n            # For certain models (like NiN), the singleton spatial dimensions\n            # need to be explicitly squeezed, since they\'re not broadcast-able\n            # in TensorFlow\'s NHWC ordering (unlike Caffe\'s NCHW).\n            if input_shape[1] == 1 and input_shape[2] == 1:\n                input_data = tf.squeeze(input_data, squeeze_dims=[1, 2])\n            else:\n                raise ValueError(\'Rank 2 tensor input expected for softmax!\')\n        return tf.nn.softmax(input_data, name)\n\n    @layer\n    def batch_normalization(self, input_data, name, scale_offset=True, relu=False):\n\n        with tf.variable_scope(name) as scope:\n            shape = [input_data.get_shape()[-1]]\n            pop_mean = tf.get_variable(""mean"", shape, initializer = tf.constant_initializer(0.0), trainable=False)\n            pop_var = tf.get_variable(""variance"", shape, initializer = tf.constant_initializer(1.0), trainable=False)\n            epsilon = 1e-4\n            decay = 0.999\n            if scale_offset:\n                scale = tf.get_variable(""scale"", shape, initializer = tf.constant_initializer(1.0))\n                offset = tf.get_variable(""offset"", shape, initializer = tf.constant_initializer(0.0))\n            else:\n                scale, offset = (None, None)\n            if self.is_training:\n                batch_mean, batch_var = tf.nn.moments(input_data, [0, 1, 2])\n\n                train_mean = tf.assign(pop_mean,\n                               pop_mean * decay + batch_mean * (1 - decay))\n                train_var = tf.assign(pop_var,\n                              pop_var * decay + batch_var * (1 - decay))\n                with tf.control_dependencies([train_mean, train_var]):\n                    output = tf.nn.batch_normalization(input_data,\n                    batch_mean, batch_var, offset, scale, epsilon, name = name)\n            else:\n                output = tf.nn.batch_normalization(input_data,\n                pop_mean, pop_var, offset, scale, epsilon, name = name)\n\n            if relu:\n                output = tf.nn.relu(output)\n\n            return output\n\n    @layer\n    def dropout(self, input_data, keep_prob, name):\n        return tf.nn.dropout(input_data, keep_prob, name=name)\n    \n\n    def unpool_as_conv(self, size, input_data, id, stride = 1, ReLU = False, BN = True):\n\n\t\t# Model upconvolutions (unpooling + convolution) as interleaving feature\n\t\t# maps of four convolutions (A,B,C,D). Building block for up-projections. \n\n\n        # Convolution A (3x3)\n        # --------------------------------------------------\n        layerName = ""layer%s_ConvA"" % (id)\n        self.feed(input_data)\n        self.conv( 3, 3, size[3], stride, stride, name = layerName, padding = \'SAME\', relu = False)\n        outputA = self.get_output()\n\n        # Convolution B (2x3)\n        # --------------------------------------------------\n        layerName = ""layer%s_ConvB"" % (id)\n        padded_input_B = tf.pad(input_data, [[0, 0], [1, 0], [1, 1], [0, 0]], ""CONSTANT"")\n        self.feed(padded_input_B)\n        self.conv(2, 3, size[3], stride, stride, name = layerName, padding = \'VALID\', relu = False)\n        outputB = self.get_output()\n\n        # Convolution C (3x2)\n        # --------------------------------------------------\n        layerName = ""layer%s_ConvC"" % (id)\n        padded_input_C = tf.pad(input_data, [[0, 0], [1, 1], [1, 0], [0, 0]], ""CONSTANT"")\n        self.feed(padded_input_C)\n        self.conv(3, 2, size[3], stride, stride, name = layerName, padding = \'VALID\', relu = False)\n        outputC = self.get_output()\n\n        # Convolution D (2x2)\n        # --------------------------------------------------\n        layerName = ""layer%s_ConvD"" % (id)\n        padded_input_D = tf.pad(input_data, [[0, 0], [1, 0], [1, 0], [0, 0]], ""CONSTANT"")\n        self.feed(padded_input_D)\n        self.conv(2, 2, size[3], stride, stride, name = layerName, padding = \'VALID\', relu = False)\n        outputD = self.get_output()\n\n        # Interleaving elements of the four feature maps\n        # --------------------------------------------------\n        left = interleave([outputA, outputB], axis=1)  # columns\n        right = interleave([outputC, outputD], axis=1)  # columns\n        Y = interleave([left, right], axis=2) # rows\n        \n        if BN:\n            layerName = ""layer%s_BN"" % (id)\n            self.feed(Y)\n            self.batch_normalization(name = layerName, scale_offset = True, relu = False)\n            Y = self.get_output()\n\n        if ReLU:\n            Y = tf.nn.relu(Y, name = layerName)\n        \n        return Y\n\n\n    def up_project(self, size, id, stride = 1, BN = True):\n        \n        # Create residual upsampling layer (UpProjection)\n\n        input_data = self.get_output()\n\n        # Branch 1\n        id_br1 = ""%s_br1"" % (id)\n\n        # Interleaving Convs of 1st branch\n        out = self.unpool_as_conv(size, input_data, id_br1, stride, ReLU=True, BN=True)\n\n        # Convolution following the upProjection on the 1st branch\n        layerName = ""layer%s_Conv"" % (id)\n        self.feed(out)\n        self.conv(size[0], size[1], size[3], stride, stride, name = layerName, relu = False)\n\n        if BN:\n            layerName = ""layer%s_BN"" % (id)\n            self.batch_normalization(name = layerName, scale_offset=True, relu = False)\n\n        # Output of 1st branch\n        branch1_output = self.get_output()\n\n            \n        # Branch 2\n        id_br2 = ""%s_br2"" % (id)\n        # Interleaving convolutions and output of 2nd branch\n        branch2_output = self.unpool_as_conv(size, input_data, id_br2, stride, ReLU=False)\n\n        \n        # sum branches\n        layerName = ""layer%s_Sum"" % (id)\n        output = tf.add_n([branch1_output, branch2_output], name = layerName)\n        # ReLU\n        layerName = ""layer%s_ReLU"" % (id)\n        output = tf.nn.relu(output, name=layerName)\n\n        self.feed(output)\n        return self\n'"
