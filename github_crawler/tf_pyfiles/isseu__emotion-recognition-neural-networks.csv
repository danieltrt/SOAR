file_path,api_count,code
constants.py,0,"b""#                               __                    __\n#                              /\\ \\__                /\\ \\__\n#   ___    ___     ___     ____\\ \\ ,_\\    __      ___\\ \\ ,_\\   ____\n#  /'___\\ / __`\\ /' _ `\\  /',__\\\\ \\ \\/  /'__`\\  /' _ `\\ \\ \\/  /',__\\\n# /\\ \\__//\\ \\L\\ \\/\\ \\/\\ \\/\\__, `\\\\ \\ \\_/\\ \\L\\.\\_/\\ \\/\\ \\ \\ \\_/\\__, `\\\n# \\ \\____\\ \\____/\\ \\_\\ \\_\\/\\____/ \\ \\__\\ \\__/.\\_\\ \\_\\ \\_\\ \\__\\/\\____/\n#  \\/____/\\/___/  \\/_/\\/_/\\/___/   \\/__/\\/__/\\/_/\\/_/\\/_/\\/__/\\/___/  .txt\n#\n#\n\nCASC_PATH = './haarcascade_files/haarcascade_frontalface_default.xml'\nSIZE_FACE = 48\nEMOTIONS = ['angry', 'disgusted', 'fearful',\n            'happy', 'sad', 'surprised', 'neutral']\nSAVE_DIRECTORY = './data/'\nSAVE_MODEL_FILENAME = 'Gudi_model_100_epochs_20000_faces'\nDATASET_CSV_FILENAME = 'fer2013.csv'\nSAVE_DATASET_IMAGES_FILENAME = 'data_images.npy'\nSAVE_DATASET_LABELS_FILENAME = 'data_labels.npy'\n"""
cvs_to_numpy.py,0,"b'from constants import CASC_PATH, SIZE_FACE, EMOTIONS, SAVE_DATASET_IMAGES_FILENAME, SAVE_DATASET_LABELS_FILENAME, SAVE_DIRECTORY, DATASET_CSV_FILENAME\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom os.path import join\n\n\ncascade_classifier = cv2.CascadeClassifier(CASC_PATH)\n\n\ndef format_image(image):\n    if len(image.shape) > 2 and image.shape[2] == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        image = cv2.imdecode(image, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n    gray_border = np.zeros((150, 150), np.uint8)\n    gray_border[:, :] = 200\n    gray_border[\n        int((150 / 2) - (SIZE_FACE / 2)): int((150 / 2) + (SIZE_FACE / 2)),\n        int((150 / 2) - (SIZE_FACE / 2)): int((150 / 2) + (SIZE_FACE / 2))\n    ] = image\n    image = gray_border\n    faces = cascade_classifier.detectMultiScale(\n        image,\n        scaleFactor=1.3,\n        minNeighbors=5\n    )\n    #  None is we don\'t found an image\n    if not len(faces) > 0:\n        return None\n    max_area_face = faces[0]\n    for face in faces:\n        if face[2] * face[3] > max_area_face[2] * max_area_face[3]:\n            max_area_face = face\n    # Chop image to face\n    face = max_area_face\n    image = image[face[1]:(face[1] + face[2]), face[0]:(face[0] + face[3])]\n    # Resize image to network size\n\n    try:\n        image = cv2.resize(image, (SIZE_FACE, SIZE_FACE),\n                           interpolation=cv2.INTER_CUBIC) / 255.\n    except Exception:\n        print(""[+] Problem during resize"")\n        return None\n    return image\n\n\ndef emotion_to_vec(x):\n    d = np.zeros(len(EMOTIONS))\n    d[x] = 1.0\n    return d\n\n\ndef flip_image(image):\n    return cv2.flip(image, 1)\n\n\ndef data_to_image(data):\n    data_image = np.fromstring(\n        str(data), dtype=np.uint8, sep=\' \').reshape((SIZE_FACE, SIZE_FACE))\n    data_image = Image.fromarray(data_image).convert(\'RGB\')\n    data_image = np.array(data_image)[:, :, ::-1].copy()\n    data_image = format_image(data_image)\n    return data_image\n\n\ndata = pd.read_csv(join(SAVE_DIRECTORY, DATASET_CSV_FILENAME))\n\nlabels = []\nimages = []\nindex = 1\ntotal = data.shape[0]\nfor index, row in data.iterrows():\n    emotion = emotion_to_vec(row[\'emotion\'])\n    image = data_to_image(row[\'pixels\'])\n    if image is not None:\n        labels.append(emotion)\n        images.append(image)\n        # images.append(flip_image(image))\n    index += 1\n    print(""Progress: {}/{} {:.2f}%"".format(index, total, index * 100.0 / total))\n\nprint(""Total: "" + str(len(images)))\nnp.save(join(SAVE_DIRECTORY, SAVE_DATASET_IMAGES_FILENAME), images)\nnp.save(join(SAVE_DIRECTORY, SAVE_DATASET_LABELS_FILENAME), labels)\n'"
dataset_loader.py,0,"b'from os.path import join\nimport numpy as np\nfrom constants import *\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\n\nclass DatasetLoader:\n    def load_from_save(self):\n        images = np.load(join(SAVE_DIRECTORY, SAVE_DATASET_IMAGES_FILENAME))\n        images = images.reshape([-1, SIZE_FACE, SIZE_FACE, 1])\n        labels = np.load(join(SAVE_DIRECTORY, SAVE_DATASET_LABELS_FILENAME)).reshape([-1, len(EMOTIONS)])\n        self._images, self._images_test, self._labels, self._labels_test = train_test_split(images, labels, test_size=0.20, random_state=42)\n    \n    @property\n    def images(self):\n        return self._images\n    \n    @property\n    def labels(self):\n        return self._labels\n\n    @property\n    def images_test(self):\n        return self._images_test\n\n    @property\n    def labels_test(self):\n        return self._labels_test\n'"
emotion_recognition.py,0,"b'from __future__ import division, absolute_import\nimport re\nimport numpy as np\nfrom dataset_loader import DatasetLoader\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected, flatten\nfrom tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.layers.normalization import local_response_normalization\nfrom tflearn.layers.estimator import regression\nfrom constants import *\nfrom os.path import isfile, join\nimport random\nimport sys\n\n\nclass EmotionRecognition:\n\n    def __init__(self):\n        self.dataset = DatasetLoader()\n\n    def build_network(self):\n        # Smaller \'AlexNet\'\n        # https://github.com/tflearn/tflearn/blob/master/examples/images/alexnet.py\n        print(\'[+] Building CNN\')\n        self.network = input_data(shape=[None, SIZE_FACE, SIZE_FACE, 1])\n        self.network = conv_2d(self.network, 64, 5, activation=\'relu\')\n        #self.network = local_response_normalization(self.network)\n        self.network = max_pool_2d(self.network, 3, strides=2)\n        self.network = conv_2d(self.network, 64, 5, activation=\'relu\')\n        self.network = max_pool_2d(self.network, 3, strides=2)\n        self.network = conv_2d(self.network, 128, 4, activation=\'relu\')\n        self.network = dropout(self.network, 0.3)\n        self.network = fully_connected(self.network, 3072, activation=\'relu\')\n        self.network = fully_connected(\n            self.network, len(EMOTIONS), activation=\'softmax\')\n        self.network = regression(\n            self.network,\n            optimizer=\'momentum\',\n            loss=\'categorical_crossentropy\'\n        )\n        self.model = tflearn.DNN(\n            self.network,\n            checkpoint_path=SAVE_DIRECTORY + \'/emotion_recognition\',\n            max_checkpoints=1,\n            tensorboard_verbose=2\n        )\n        self.load_model()\n\n    def load_saved_dataset(self):\n        self.dataset.load_from_save()\n        print(\'[+] Dataset found and loaded\')\n\n    def start_training(self):\n        self.load_saved_dataset()\n        self.build_network()\n        if self.dataset is None:\n            self.load_saved_dataset()\n        # Training\n        print(\'[+] Training network\')\n        self.model.fit(\n            self.dataset.images, self.dataset.labels,\n            validation_set=(self.dataset.images_test,\n                            self.dataset.labels_test),\n            n_epoch=100,\n            batch_size=50,\n            shuffle=True,\n            show_metric=True,\n            snapshot_step=200,\n            snapshot_epoch=True,\n            run_id=\'emotion_recognition\'\n        )\n\n    def predict(self, image):\n        if image is None:\n            return None\n        image = image.reshape([-1, SIZE_FACE, SIZE_FACE, 1])\n        return self.model.predict(image)\n\n    def save_model(self):\n        self.model.save(join(SAVE_DIRECTORY, SAVE_MODEL_FILENAME))\n        print(\'[+] Model trained and saved at \' + SAVE_MODEL_FILENAME)\n\n    def load_model(self):\n        if isfile(join(SAVE_DIRECTORY, SAVE_MODEL_FILENAME)):\n            self.model.load(join(SAVE_DIRECTORY, SAVE_MODEL_FILENAME))\n            print(\'[+] Model loaded from \' + SAVE_MODEL_FILENAME)\n\n\ndef show_usage():\n    # I din\'t want to have more dependecies\n    print(\'[!] Usage: python emotion_recognition.py\')\n    print(\'\\t emotion_recognition.py train \\t Trains and saves model with saved dataset\')\n    print(\'\\t emotion_recognition.py poc \\t Launch the proof of concept\')\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) <= 1:\n        show_usage()\n        exit()\n    network = EmotionRecognition()\n    if sys.argv[1] == \'train\':\n        network.start_training()\n        network.save_model()\n    elif sys.argv[1] == \'poc\':\n        import poc\n    else:\n        show_usage()\n'"
manual_poc.py,0,"b'# Proof-of-concept\nimport cv2\nimport sys\nimport os\nfrom constants import *\nfrom emotion_recognition import EmotionRecognition\nimport numpy as np\n\n\ndef format_image(image):\n    if len(image.shape) > 2 and image.shape[2] == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        image = cv2.imdecode(image, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n    faces = cv2.CascadeClassifier(CASC_PATH).detectMultiScale(\n        image,\n        scaleFactor=1.3,\n        minNeighbors=5\n    )\n    # None is we don\'t found an image\n    if not len(faces) > 0:\n        return None\n    max_area_face = faces[0]\n    for face in faces:\n        if face[2] * face[3] > max_area_face[2] * max_area_face[3]:\n            max_area_face = face\n    # Chop image to face\n    face = max_area_face\n    image = image[face[1]:(face[1] + face[2]), face[0]:(face[0] + face[3])]\n\n    # Resize image to network size\n    try:\n        image = cv2.resize(image, (SIZE_FACE, SIZE_FACE),\n                           interpolation=cv2.INTER_CUBIC) / 255.\n        while True:\n            cv2.imshow(""frame"", image)\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n    except Exception:\n        print(""[+] Problem during resize"")\n        return None\n    # cv2.imshow(""Lol"", image)\n    # cv2.waitKey(0)\n    return image\n\n\n# Load Model\nnetwork = EmotionRecognition()\nnetwork.build_network()\n\nfiles = []\n\nfor f in os.listdir(""./""):\n    ext = os.path.splitext(f)[1]\n    if ext.lower() in ["".jpg""]:\n        files.append(f)\n\nfor f in files:\n    frame = cv2.imread(f)\n    # Predict result with network\n    result = network.predict(format_image(frame))\n\n    if result is not None:\n        for index, emotion in enumerate(EMOTIONS):\n            print(emotion, \': \', result[0][index])\n\n    print(""Emotion: of "", f, ""-"", EMOTIONS[np.argmax(result[0])])\n'"
plot_emotion_matrix.py,0,"b'# -*- coding: utf-8 -*-\nimport cv2\nimport sys\nfrom constants import *\nfrom emotion_recognition import EmotionRecognition\nfrom os.path import join\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load Model\nnetwork = EmotionRecognition()\nnetwork.build_network()\n\nimages = np.load(join(SAVE_DIRECTORY, SAVE_DATASET_IMAGES_FILENAME))\nlabels = np.load(join(SAVE_DIRECTORY, SAVE_DATASET_LABELS_FILENAME))\nimages = images.reshape([-1, SIZE_FACE, SIZE_FACE, 1])\nlabels = labels.reshape([-1, len(EMOTIONS)])\n\nprint(\'[+] Loading Data\')\ndata = np.zeros((len(EMOTIONS), len(EMOTIONS)))\nfor i in xrange(images.shape[0]):\n    result = network.predict(images[i])\n    data[np.argmax(labels[i]), result[0].index(max(result[0]))] += 1\n    # print x[i], \' vs \', y[i]\n\n# Take % by column\nfor i in range(len(data)):\n    total = np.sum(data[i])\n    for x in range(len(data[0])):\n        data[i][x] = data[i][x] / total\nprint(data)\n\nprint(\'[+] Generating graph\')\nc = plt.pcolor(data, edgecolors=\'k\', linewidths=4,\n               cmap=\'Blues\', vmin=0.0, vmax=1.0)\n\n\ndef show_values(pc, fmt=""%.2f"", **kw):\n    pc.update_scalarmappable()\n    ax = pc.get_axes()\n    ax.set_yticks(np.arange(len(EMOTIONS)) + 0.5, minor=False)\n    ax.set_xticks(np.arange(len(EMOTIONS)) + 0.5, minor=False)\n    ax.set_xticklabels(EMOTIONS, minor=False)\n    ax.set_yticklabels(EMOTIONS, minor=False)\n    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n        x, y = p.vertices[:-2, :].mean(0)\n        if np.all(color[:3] > 0.5):\n            color = (0.0, 0.0, 0.0)\n        else:\n            color = (1.0, 1.0, 1.0)\n        ax.text(x, y, fmt % value, ha=""center"", va=""center"", color=color, **kw)\n\n\nshow_values(c)\nplt.xlabel(\'Predicted Emotion\')\nplt.ylabel(\'Real Emotion\')\nplt.show()\n'"
poc.py,0,"b'# Proof-of-concept\nimport cv2\nimport sys\nfrom constants import *\nfrom emotion_recognition import EmotionRecognition\nimport numpy as np\n\ncascade_classifier = cv2.CascadeClassifier(CASC_PATH)\n\n\ndef brighten(data, b):\n    datab = data * b\n    return datab\n\n\ndef format_image(image):\n    if len(image.shape) > 2 and image.shape[2] == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        image = cv2.imdecode(image, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n    faces = cascade_classifier.detectMultiScale(\n        image,\n        scaleFactor=1.3,\n        minNeighbors=5\n    )\n    # None is we don\'t found an image\n    if not len(faces) > 0:\n        return None\n    max_area_face = faces[0]\n    for face in faces:\n        if face[2] * face[3] > max_area_face[2] * max_area_face[3]:\n            max_area_face = face\n    # Chop image to face\n    face = max_area_face\n    image = image[face[1]:(face[1] + face[2]), face[0]:(face[0] + face[3])]\n    # Resize image to network size\n    try:\n        image = cv2.resize(image, (SIZE_FACE, SIZE_FACE),\n                           interpolation=cv2.INTER_CUBIC) / 255.\n    except Exception:\n        print(""[+] Problem during resize"")\n        return None\n    # cv2.imshow(""Lol"", image)\n    # cv2.waitKey(0)\n    return image\n\n\n# Load Model\nnetwork = EmotionRecognition()\nnetwork.build_network()\n\nvideo_capture = cv2.VideoCapture(0)\nfont = cv2.FONT_HERSHEY_SIMPLEX\n\nfeelings_faces = []\nfor index, emotion in enumerate(EMOTIONS):\n    feelings_faces.append(cv2.imread(\'./emojis/\' + emotion + \'.png\', -1))\n\nwhile True:\n    # Capture frame-by-frame\n    ret, frame = video_capture.read()\n\n    # Predict result with network\n    result = network.predict(format_image(frame))\n\n    # Draw face in frame\n    # for (x,y,w,h) in faces:\n    #   cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 2)\n\n    # Write results in frame\n    if result is not None:\n        for index, emotion in enumerate(EMOTIONS):\n            cv2.putText(frame, emotion, (10, index * 20 + 20),\n                        cv2.FONT_HERSHEY_PLAIN, 0.5, (0, 255, 0), 1)\n            cv2.rectangle(frame, (130, index * 20 + 10), (130 +\n                                                          int(result[0][index] * 100), (index + 1) * 20 + 4), (255, 0, 0), -1)\n\n        face_image = feelings_faces[np.argmax(result[0])]\n\n        # Ugly transparent fix\n        for c in range(0, 3):\n            frame[200:320, 10:130, c] = face_image[:, :, c] * \\\n                (face_image[:, :, 3] / 255.0) + frame[200:320,\n                                                      10:130, c] * (1.0 - face_image[:, :, 3] / 255.0)\n\n    # Display the resulting frame\n    cv2.imshow(\'Video\', frame)\n\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\n\n# When everything is done, release the capture\nvideo_capture.release()\ncv2.destroyAllWindows()\n'"
