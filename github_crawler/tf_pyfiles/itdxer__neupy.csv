file_path,api_count,code
setup.py,0,"b'import os\nimport warnings\n\nfrom setuptools import setup, find_packages\n\nimport neupy\n\n\nCURRENT_DIR = os.path.abspath(os.path.dirname(__file__))\n\n\ndef get_requirements():\n    try:\n        requirements_file = os.path.join(CURRENT_DIR, \'requirements\',\n                                         \'main.txt\')\n\n        with open(requirements_file) as f:\n            requirements = f.read()\n            return requirements.splitlines()\n\n    except IOError as e:\n        warnings.warn(""error"", e)\n        # Simple hack for `tox` test.\n        return []\n\n\nsetup(\n    # Info\n    name=\'neupy\',\n    version=neupy.__version__,\n    license=\'MIT\',\n    url=\'http://neupy.com\',\n    description=neupy.__doc__.strip(),\n\n    # Author\n    author=\'Yurii Shevhcuk\',\n    author_email=\'mail@itdxer.com\',\n\n    # Package\n    packages=find_packages(),\n    install_requires=get_requirements(),\n    include_package_data=True,\n    zip_safe=False,\n    keywords=[\'neural networks\', \'deep learning\'],\n\n    # Other\n    classifiers=[\n        \'Intended Audience :: Developers\',\n        \'Natural Language :: English\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n)\n'"
examples/__init__.py,0,b''
neupy/__init__.py,0,"b'""""""\nNeuPy is a Python library for Artificial Neural Networks and Deep Learning.\n""""""\n\n__version__ = \'0.8.2\'\n'"
neupy/exceptions.py,0,"b'__all__ = (\n    \'LayerConnectionError\', \'InvalidConnection\', \'NotTrained\',\n    \'StopTraining\', \'WeightInitializationError\', \'PropagationError\',\n)\n\n\nclass LayerConnectionError(Exception):\n    """"""\n    Error class that triggers in case of connection\n    issues within layers.\n    """"""\n\n\nclass InvalidConnection(Exception):\n    """"""\n    Connection is not suitable for the specified algorithm\n    """"""\n\n\nclass NotTrained(Exception):\n    """"""\n    Algorithms hasn\'t been trained yet.\n    """"""\n\n\nclass StopTraining(Exception):\n    """"""\n    Interrupt training procedure.\n    """"""\n\n\nclass WeightInitializationError(Exception):\n    """"""\n    In case if there is some problem with weight initialization\n    """"""\n\n\nclass PropagationError(Exception):\n    """"""\n    Error propagation triggers when input cannot be propagated\n    through the layer.\n    """"""\n'"
neupy/init.py,6,"b'import abc\n\nimport six\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy.core.docs import SharedDocsABCMeta\n\n\n__all__ = (\'Initializer\', \'Constant\', \'Normal\', \'Uniform\', \'Orthogonal\',\n           \'HeNormal\', \'HeUniform\', \'XavierNormal\', \'XavierUniform\')\n\n\ndef identify_fans(shape):\n    """"""\n    Identify fans from shape.\n\n    Parameters\n    ----------\n    shape : tuple or list\n\n    Returns\n    -------\n    tuple\n        Tuple that contains :math:`fan_{in}` and :math:`fan_{out}`.\n    """"""\n    n_dimensions = len(shape)\n\n    if n_dimensions == 0:\n        raise ValueError(""Cannot apply initializer when shape is unknown"")\n\n    elif n_dimensions == 1:\n        fan_in, fan_out = shape[0], 1\n\n    elif n_dimensions == 2:\n        fan_in, fan_out = shape\n\n    else:\n        # By default we assume that weights with more than 2 dimensions\n        # are generated for convolutional layers.\n        receptive_field = np.prod(shape[:-2]).item(0)\n        fan_in = shape[-2] * receptive_field\n        fan_out = shape[-1] * receptive_field\n\n    return fan_in, fan_out\n\n\ndef classname(instance):\n    return instance.__class__.__name__\n\n\ndef set_numpy_seed(seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n\nclass Initializer(six.with_metaclass(SharedDocsABCMeta)):\n    """"""\n    Base class for parameter initialization.\n\n    Methods\n    -------\n    sample(shape, return_array=False)\n        Returns tensorflow\'s tensor or numpy array with specified\n        shape. Type of the object depends on the ``return_array`` value.\n        Numpy array will be returned when ``return_array=True`` and\n        tensor otherwise.\n    """"""\n    inherit_method_docs = True\n\n    @abc.abstractmethod\n    def sample(self, shape, return_array=False):\n        """"""\n        Returns tensorflow\'s tensor with specified shape.\n\n        Parameters\n        ----------\n        shape : tuple\n            Parameter shape.\n\n        return_array : bool\n            Returns numpy\'s array when equal to ``True``\n            and tensorflow\'s tensor when equal to ``False``.\n            Defaults to ``False``.\n\n        Returns\n        -------\n        array-like or Tensor\n        """"""\n        raise NotImplementedError\n\n\nclass Constant(Initializer):\n    """"""\n    Initialize parameter that has constant values.\n\n    Parameters\n    ----------\n    value : float, int\n        All parameters in the tensor will be equal to\n        this value. Defaults to ``0``.\n\n    Methods\n    -------\n    {Initializer.Methods}\n    """"""\n    def __init__(self, value=0):\n        self.value = value\n\n    def sample(self, shape, return_array=False):\n        if return_array:\n            return np.ones(shape) * self.value\n\n        elif self.value == 0:\n            return tf.zeros(shape)\n\n        return tf.ones(shape) * self.value\n\n    def __repr__(self):\n        return \'{}({})\'.format(classname(self), self.value)\n\n\nclass Normal(Initializer):\n    """"""\n    Initialize parameter sampling from the normal distribution.\n\n    Parameters\n    ----------\n    mean : int, float\n        Mean of the normal distribution.\n\n    std : int, float\n        Standard deviation of the normal distribution.\n\n    seed : None or int\n        Random seed. Integer value will make results reproducible.\n        Defaults to ``None``.\n\n    Methods\n    -------\n    {Initializer.Methods}\n    """"""\n    def __init__(self, mean=0, std=0.01, seed=None):\n        self.mean = mean\n        self.std = std\n        self.seed = seed\n\n    def sample(self, shape, return_array=False):\n        if return_array:\n            set_numpy_seed(self.seed)\n            return np.random.normal(loc=self.mean, scale=self.std, size=shape)\n\n        return tf.random_normal(\n            mean=self.mean, stddev=self.std,\n            shape=shape, seed=self.seed)\n\n    def __repr__(self):\n        return \'{}(mean={}, std={})\'.format(\n            classname(self), self.mean, self.std)\n\n\nclass Uniform(Initializer):\n    """"""\n    Initialize parameter sampling from the uniform\n    distribution.\n\n    Parameters\n    ----------\n    minval : int, float\n        Minimum possible value.\n\n    maxval : int, float\n        Maximum possible value.\n\n    seed : None or int\n        Random seed. Integer value will make results reproducible.\n        Defaults to ``None``.\n\n    Methods\n    -------\n    {Initializer.Methods}\n    """"""\n    def __init__(self, minval=0, maxval=1, seed=None):\n        self.minval = minval\n        self.maxval = maxval\n        self.seed = seed\n\n    def sample(self, shape, return_array=False):\n        minval, maxval = self.minval, self.maxval\n\n        if return_array:\n            set_numpy_seed(self.seed)\n            return np.random.random(shape) * (maxval - minval) + minval\n\n        return tf.random_uniform(shape, minval, maxval, seed=self.seed)\n\n    def __repr__(self):\n        return \'{}({}, {})\'.format(\n            classname(self), self.minval, self.maxval)\n\n\nclass Orthogonal(Initializer):\n    """"""\n    Initialize matrix with orthogonal basis.\n\n    Parameters\n    ----------\n    scale : int, float\n        Scales output matrix by a specified factor.\n        Defaults to ``1``.\n\n    seed : None or int\n        Random seed. Integer value will make results reproducible.\n        Defaults to ``None``.\n\n    Raises\n    ------\n    ValueError\n        In case if tensor shape has more than 2\n        dimensions.\n\n    Methods\n    -------\n    {Initializer.Methods}\n    """"""\n    def __init__(self, scale=1.0, seed=None):\n        self.scale = scale\n        self.seed = seed\n\n    def sample(self, shape, return_array=False):\n        ndim = len(shape)\n\n        if ndim not in (1, 2):\n            raise ValueError(\n                ""Shape attribute must have 1 or 2 dimensions. ""\n                ""Found {} dimensions"".format(ndim))\n\n        normal = Normal(seed=self.seed)\n        rand_matrix = normal.sample(shape, return_array)\n\n        if ndim == 1:\n            return rand_matrix\n\n        if return_array:\n            u, _, v = np.linalg.svd(rand_matrix, full_matrices=False)\n        else:\n            _, u, v = tf.linalg.svd(rand_matrix, full_matrices=False)\n            v = tf.transpose(v)\n\n        nrows, ncols = shape\n        ortho_base = u if nrows > ncols else v\n        return self.scale * ortho_base[:nrows, :ncols]\n\n    def __repr__(self):\n        return \'{}(scale={})\'.format(classname(self), self.scale)\n\n\nclass InitializerWithGain(Initializer):\n    """"""\n    Initialization class that has gain property\n\n    Parameters\n    ----------\n    gain : float\n        Scales variance of the distribution by this factor. Value ``2``\n        is a suitable choice for layers that have Relu non-linearity.\n        Defaults to ``1``.\n\n    seed : None or int\n        Random seed. Integer value will make results reproducible.\n        Defaults to ``None``.\n    """"""\n    def __init__(self, gain=1.0, seed=None):\n        self.gain = gain\n        self.seed = seed\n        super(InitializerWithGain, self).__init__()\n\n    def __repr__(self):\n        return \'{}(gain={})\'.format(classname(self), self.gain)\n\n\nclass HeNormal(InitializerWithGain):\n    """"""\n    Kaiming He parameter initialization method based on the\n    normal distribution.\n\n    Parameters\n    ----------\n    {InitializerWithGain.Parameters}\n\n    Methods\n    -------\n    {Initializer.Methods}\n\n    References\n    ----------\n    [1] Kaiming He, Xiangyu Zhan, Shaoqing Ren, Jian Sun.\n        Delving Deep into Rectifiers: Surpassing Human-Level\n        Performance on ImageNet Classification, 2015.\n    """"""\n    def sample(self, shape, return_array=False):\n        fan_in, _ = identify_fans(shape)\n        variance = 1. / fan_in\n        std = np.sqrt(self.gain * variance)\n\n        normal = Normal(0, std, seed=self.seed)\n        return normal.sample(shape, return_array)\n\n\nclass HeUniform(InitializerWithGain):\n    """"""\n    Kaiming He parameter initialization method based on the\n    uniformal distribution.\n\n    Parameters\n    ----------\n    {InitializerWithGain.Parameters}\n\n    Methods\n    -------\n    {Initializer.Methods}\n\n    References\n    ----------\n    [1] Kaiming He, Xiangyu Zhan, Shaoqing Ren, Jian Sun.\n        Delving Deep into Rectifiers: Surpassing Human-Level\n        Performance on ImageNet Classification, 2015.\n    """"""\n    def sample(self, shape, return_array=False):\n        fan_in, _ = identify_fans(shape)\n        variance = 3. / fan_in\n        abs_max_value = np.sqrt(self.gain * variance)\n\n        uniform = Uniform(\n            minval=-abs_max_value,\n            maxval=abs_max_value,\n            seed=self.seed,\n        )\n        return uniform.sample(shape, return_array)\n\n\nclass XavierNormal(InitializerWithGain):\n    """"""\n    Xavier Glorot parameter initialization method based on\n    normal distribution.\n\n    Parameters\n    ----------\n    {InitializerWithGain.Parameters}\n\n    Methods\n    -------\n    {Initializer.Methods}\n\n    References\n    ----------\n    [1] Xavier Glorot, Y Bengio. Understanding the difficulty\n        of training deep feedforward neural networks, 2010.\n    """"""\n    def sample(self, shape, return_array=False):\n        fan_in, fan_out = identify_fans(shape)\n        variance = 1. / (fan_in + fan_out)\n        std = np.sqrt(self.gain * variance)\n\n        normal = Normal(0, std, seed=self.seed)\n        return normal.sample(shape, return_array)\n\n\nclass XavierUniform(InitializerWithGain):\n    """"""\n    Xavier Glorot parameter initialization method based\n    on uniform distribution.\n\n    Parameters\n    ----------\n    {InitializerWithGain.Parameters}\n\n    Methods\n    -------\n    {Initializer.Methods}\n\n    References\n    ----------\n    [1] Xavier Glorot, Y Bengio. Understanding the difficulty\n        of training deep feedforward neural networks, 2010.\n    """"""\n    def sample(self, shape, return_array=False):\n        fan_in, fan_out = identify_fans(shape)\n        variance = 3. / (fan_in + fan_out)\n        abs_max_value = np.sqrt(self.gain * variance)\n\n        uniform = Uniform(\n            minval=-abs_max_value,\n            maxval=abs_max_value,\n            seed=self.seed,\n        )\n        return uniform.sample(shape, return_array)\n'"
neupy/storage.py,2,"b'import json\nfrom time import gmtime, strftime\n\nimport six\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import cPickle as pickle\n\nimport neupy\nfrom neupy.core.docs import shared_docs\nfrom neupy.layers.graph import LayerGraph\nfrom neupy.algorithms.base import BaseNetwork\nfrom neupy.utils import asfloat, tf_utils\n\n\n__all__ = (\n    \'save\', \'load\',  # aliases to hdf5\n    \'save_pickle\', \'load_pickle\',\n    \'save_json\', \'load_json\',\n    \'save_hdf5\', \'load_hdf5\',\n    \'load_dict\', \'save_dict\',\n)\n\n\nclass ParameterLoaderError(Exception):\n    """"""\n    Exception triggers in case if there are some issues\n    during the parameter loading procedure.\n    """"""\n\n\nclass InvalidFormat(Exception):\n    """"""\n    Exception triggers when there are some issue with\n    data format that stores network data.\n    """"""\n\n\ndef extract_network(instance):\n    if isinstance(instance, BaseNetwork):\n        return instance.network\n\n    if isinstance(instance, LayerGraph):\n        return instance\n\n    raise TypeError(\n        ""Invalid input type. Input should be network or optimizer ""\n        ""with network, got `{}` instance instead"".format(type(instance)))\n\n\ndef load_layer_parameter(layer, layer_data):\n    """"""\n    Set layer parameters to the values specified in the\n    stored data\n    """"""\n    session = tf_utils.tensorflow_session()\n\n    for param_name, param_data in layer_data[\'parameters\'].items():\n        parameter = getattr(layer, param_name)\n\n        if not isinstance(parameter, tf.Variable):\n            raise ParameterLoaderError(\n                ""The `{}` parameter from the `{}` layer expected to be ""\n                ""instance of the tf.Variable, but current value equal to {}. ""\n                ""Layer: {}"".format(param_name, layer.name, parameter, layer))\n\n        parameter.load(asfloat(param_data[\'value\']), session)\n\n\ndef load_dict_by_names(layers_conn, layers_data, ignore_missing=False):\n    """"""""\n    Load parameters in to layer using layer names as the reference.\n\n    Raises\n    ------\n    ParameterLoaderError\n        In case if it\'s impossible to load parameters from data\n\n    Returns\n    -------\n    bool\n        Returns `True` in case if data was loaded successfully\n        and `False` when parameters wasn\'t loaded\n    """"""\n    layers_data = {l[\'name\']: l for l in layers_data}\n    layers_conn = {l.name: l for l in layers_conn}\n\n    if not ignore_missing and layers_data.keys() != layers_conn.keys():\n        raise ParameterLoaderError(\n            ""Cannot match layers by name. \\n""\n            ""  Layer names in network: {}\\n""\n            ""  Layer names in stored data: {}""\n            """".format(layers_conn.keys(), layers_data.keys()))\n\n    elif ignore_missing and all(l not in layers_data for l in layers_conn):\n        raise ParameterLoaderError(""Non of the layers can be matched by name"")\n\n    for layer_name, layer in layers_conn.items():\n        if layer_name in layers_data:\n            load_layer_parameter(layer, layers_data[layer_name])\n\n\ndef load_dict_sequentially(layers_conn, layers_data):\n    """"""""\n    Load parameters in to layer using sequential order of\n    layer in network and stored data\n    """"""\n    for layer, layer_data in zip(layers_conn, layers_data):\n        load_layer_parameter(layer, layer_data)\n\n\ndef validate_data_structure(data):\n    """"""\n    Validates structure of the stored data\n\n    Parameters\n    ----------\n    data : dict\n\n    Raises\n    ------\n    InvalidFormat\n        When format is invalid\n    """"""\n    if not isinstance(data, dict):\n        raise InvalidFormat(""Stored data should be in dictionary format"")\n\n    if \'layers\' not in data:\n        raise InvalidFormat(""Stored data has no key `layers`"")\n\n    if not isinstance(data[\'layers\'], list):\n        raise InvalidFormat(""Layers stored not in the list format"")\n\n    if not data[\'layers\']:\n        raise InvalidFormat(""Stored data don\'t have any layer stored"")\n\n    for layer_index, layer in enumerate(data[\'layers\']):\n        if not isinstance(layer, dict):\n            raise InvalidFormat(\n                ""Layer in the {} position (0-based indices) is not a ""\n                ""dictionary (it is {})"".format(layer_index, type(layer)))\n\n        for attr in (\'parameters\', \'name\'):\n            if attr not in layer:\n                raise InvalidFormat(\n                    ""Layer in the {} position (0-based indices) don\'t ""\n                    ""have key `{}` specified"".format(layer_index, attr))\n\n        if not isinstance(layer[\'parameters\'], dict):\n            raise InvalidFormat(\n                ""Layer in the {} position (0-based indices) parameters ""\n                ""specified as `{}`, but dictionary expected""\n                """".format(layer_index, type(layer[\'parameters\'])))\n\n        for param_name, param in layer[\'parameters\'].items():\n            if not isinstance(param, dict):\n                raise InvalidFormat(\n                    ""Layer in the {} position (0-based indices) has ""\n                    ""incorrect value for parameter named `{}`. It has ""\n                    ""to be a dictionary, but got {}""\n                    """".format(layer_index, param_name, type(param)))\n\n            if \'value\' not in param:\n                raise InvalidFormat(\n                    ""Layer in the {} position (0-based indices) has ""\n                    ""incorrect value for parameter named `{}`. Parameter ""\n                    ""doesn\'t have key named `value`""\n                    """".format(layer_index, param_name))\n\n\ndef load_dict(network, data, ignore_missing=False,\n              load_by=\'names_or_order\', skip_validation=True):\n    """"""\n    Load network networks from dictionary.\n\n    Parameters\n    ----------\n    network : network, list of layer or network\n\n    data : dict\n        Dictionary that stores network parameters.\n\n    ignore_missing : bool\n        ``False`` means that error will be triggered in case\n        if some of the layers doesn\'t have storage parameters\n        in the specified source. Defaults to ``False``.\n\n    load_by : {``names``, ``order``, ``names_or_order``}\n        Defines strategy that will be used during parameter loading\n\n        - ``names`` - Matches layers using their names.\n\n        - ``order`` - Matches layers using their relative position.\n\n        - ``names_or_order`` - First, method tries to match layers using\n          their names. If that doesn\'t work, then layer\'s relative position\n          will be used.\n\n        Defaults to ``names_or_order``.\n\n    skip_validation : bool\n        When set to ``False`` validation will be applied per each layer in\n        order to make sure that there were no changes between created\n        and stored models. Defaults to ``True``\n\n    Raises\n    ------\n    ValueError\n        Happens in case if `ignore_missing=False` and there is no\n        parameters for some of the layers.\n    """"""\n    if load_by not in (\'names\', \'order\', \'names_or_order\'):\n        raise ValueError(\n            ""Invalid value for the `load_by` argument: {}. Should be ""\n            ""one of the following values: names, order, names_or_order.""\n            """".format(load_by))\n\n    if not skip_validation:\n        validate_data_structure(data)\n\n    network = extract_network(network)\n\n    # We need to initialize network, to make sure\n    # that each layer will generate shared variables\n    network.create_variables()\n\n    # We are only interested in layers that has parameters\n    layers = data[\'layers\']\n    layers_data = [l for l in layers if l[\'parameters\']]\n    layers_conn = [l for l in network if l.variables]\n\n    if not ignore_missing and len(layers_data) != len(layers_conn):\n        raise ParameterLoaderError(\n            ""Couldn\'t load parameters from the dictionary. Connection ""\n            ""has {} layers with parameters whether stored data has {}""\n            """".format(len(layers_data), len(layers_conn)))\n\n    if load_by == \'names\':\n        load_dict_by_names(\n            layers_conn, layers_data, ignore_missing)\n\n    elif load_by == \'order\':\n        load_dict_sequentially(layers_conn, layers_data)\n\n    else:\n        try:\n            # First we try to load parameters using there names as\n            # identifiers. Names are more reliable identifiers than\n            # order of layers in the network\n            load_dict_by_names(layers_conn, layers_data, ignore_missing)\n\n        except ParameterLoaderError:\n            # If we couldn\'t load data using layer names we will try to\n            # compare layers in sequence one by one. Even if names are\n            # different networks can be the same and order of parameters\n            # should also be the same\n            load_dict_sequentially(layers_conn, layers_data)\n\n\ndef save_dict(network):\n    """"""\n    Save network into the dictionary.\n\n    Parameters\n    ----------\n    network : network, list of layer or network\n\n    Returns\n    -------\n    dict\n        Saved parameters and information about network in dictionary\n        using specific format. Learn more about the NeuPy\'s storage\n        format in the official documentation.\n\n    Examples\n    --------\n    >>> from neupy import layers, storage\n    >>>\n    >>> network = layers.Input(10) >> layers.Softmax(3)\n    >>> layers_data = storage.save_dict(network)\n    >>>\n    >>> layers_data.keys()\n    [\'layers\', \'graph\', \'metadata\']\n    """"""\n    network = extract_network(network)\n    network.create_variables()\n\n    session = tf_utils.tensorflow_session()\n    tf_utils.initialize_uninitialized_variables()\n\n    data = {\n        \'metadata\': {\n            \'language\': \'python\',\n            \'library\': \'neupy\',\n            \'version\': neupy.__version__,\n            \'created\': strftime(""%a, %d %b %Y %H:%M:%S %Z"", gmtime()),\n        },\n        # Make it as a list in order to save the right order\n        # of paramters, otherwise it can be convert to the dictionary.\n        \'graph\': network.layer_names_only(),\n        \'layers\': [],\n    }\n\n    for layer in network:\n        parameters = {}\n        configs = {}\n\n        for attrname, parameter in layer.variables.items():\n            parameters[attrname] = {\n                \'value\': asfloat(session.run(parameter)),\n                \'trainable\': parameter.trainable,\n            }\n\n        for option_name in layer.options:\n            if option_name not in parameters:\n                configs[option_name] = getattr(layer, option_name)\n\n        data[\'layers\'].append({\n            \'class_name\': layer.__class__.__name__,\n            \'name\': layer.name,\n            \'parameters\': parameters,\n            \'configs\': configs,\n        })\n\n    return data\n\n\n@shared_docs(save_dict)\ndef save_pickle(network, filepath, python_compatible=False):\n    """"""\n    Save layer parameters in pickle file.\n\n    Parameters\n    ----------\n    {save_dict.network}\n\n    filepath : str\n        Path to the pickle file that stores network parameters.\n\n    python_compatible : bool\n        If `True` pickled object would be compatible with\n        Python 2 and 3 (pickle protocol equal to `2`).\n        If `False` then value would be pickled as highest\n        protocol (`pickle.HIGHEST_PROTOCOL`).\n        Defaults to `False`.\n\n    Examples\n    --------\n    >>> from neupy import layers, storage\n    >>>\n    >>> network = layers.Input(10) > layers.Softmax(3)\n    >>> storage.save_pickle(network, \'/path/to/parameters.pickle\')\n    """"""\n    network = extract_network(network)\n    data = save_dict(network)\n\n    with open(filepath, \'wb+\') as f:\n        # Protocol 2 is compatible for both python versions\n        protocol = pickle.HIGHEST_PROTOCOL if not python_compatible else 2\n        pickle.dump(data, f, protocol)\n\n\n@shared_docs(load_dict)\ndef load_pickle(network, filepath, ignore_missing=False,\n                load_by=\'names_or_order\', skip_validation=True):\n    """"""\n    Load and set parameters for layers from the\n    specified filepath.\n\n    Parameters\n    ----------\n    {load_dict.network}\n\n    filepath : str\n        Path to pickle file that will store network parameters.\n\n    {load_dict.ignore_missing}\n\n    {load_dict.load_by}\n\n    {load_dict.skip_validation}\n\n    Raises\n    ------\n    {load_dict.Raises}\n\n    Examples\n    --------\n    >>> from neupy import layers, storage\n    >>>\n    >>> network = layers.Input(10) > layers.Softmax(3)\n    >>> storage.load_pickle(network, \'/path/to/parameters.pickle\')\n    """"""\n    network = extract_network(network)\n\n    with open(filepath, \'rb\') as f:\n        # Specify encoding for python 3 in order to be able to\n        # read files that has been created in python 2\n        options = {\'encoding\': \'latin1\'} if six.PY3 else {}\n        data = pickle.load(f, **options)\n\n    load_dict(network, data, ignore_missing, load_by)\n\n\n@shared_docs(save_dict)\ndef save_hdf5(network, filepath):\n    """"""\n    Save network parameters in HDF5 format.\n\n    Parameters\n    ----------\n    {save_dict.network}\n\n    filepath : str\n        Path to the HDF5 file that stores network parameters.\n\n    Examples\n    --------\n    >>> from neupy import layers, storage\n    >>>\n    >>> network = layers.Input(10) > layers.Softmax(3)\n    >>> storage.save_hdf5(network, \'/path/to/parameters.hdf5\')\n    """"""\n    network = extract_network(network)\n    data = save_dict(network)\n\n    with h5py.File(filepath, mode=\'w\') as f:\n        layer_names = []\n\n        for layer in data[\'layers\']:\n            layer_name = layer[\'name\']\n            layer_group = f.create_group(layer_name)\n\n            for attrname, attrvalue in layer.items():\n                if attrname != \'parameters\':\n                    layer_group.attrs[attrname] = json.dumps(\n                        attrvalue, default=repr)\n\n            for param_name, param in layer[\'parameters\'].items():\n                dataset = layer_group.create_dataset(\n                    param_name, data=param[\'value\'])\n\n                dataset.attrs[\'trainable\'] = param[\'trainable\']\n\n            layer_names.append(layer_name)\n\n        f.attrs[\'metadata\'] = json.dumps(data[\'metadata\'])\n        f.attrs[\'graph\'] = json.dumps(data[\'graph\'])\n        f.attrs[\'layer_names\'] = json.dumps(layer_names)\n\n\n@shared_docs(load_dict)\ndef load_hdf5(network, filepath, ignore_missing=False,\n              load_by=\'names_or_order\', skip_validation=True):\n    """"""\n    Load network parameters from HDF5 file.\n\n    Parameters\n    ----------\n    {load_dict.network}\n\n    filepath : str\n        Path to HDF5 file that will store network parameters.\n\n    {load_dict.ignore_missing}\n\n    {load_dict.load_by}\n\n    {load_dict.skip_validation}\n\n    Raises\n    ------\n    {load_dict.Raises}\n\n    Examples\n    --------\n    >>> from neupy import layers, storage\n    >>>\n    >>> network = layers.Input(10) > layers.Softmax(3)\n    >>> storage.load_hdf5(network, \'/path/to/parameters.hdf5\')\n    """"""\n    network = extract_network(network)\n    data = {}\n\n    with h5py.File(filepath, mode=\'r\') as f:\n        data[\'metadata\'] = json.loads(f.attrs[\'metadata\'])\n        data[\'graph\'] = json.loads(f.attrs[\'graph\'])\n        data[\'layers\'] = []\n\n        layer_names = json.loads(f.attrs[\'layer_names\'])\n\n        for layer_name in layer_names:\n            layer_group = f[layer_name]\n            layer = {\'name\': layer_name}\n\n            for attrname, attrvalue in layer_group.attrs.items():\n                try:\n                    layer[attrname] = json.loads(attrvalue)\n                except ValueError:\n                    layer[attrname] = attrvalue\n\n            layer[\'parameters\'] = {}\n            for param_name, parameter in layer_group.items():\n                layer[\'parameters\'][param_name] = {\n                    \'value\': parameter.value,\n                    \'trainable\': parameter.attrs[\'trainable\'],\n                }\n\n            data[\'layers\'].append(layer)\n\n    load_dict(network, data, ignore_missing, load_by)\n\n\ndef convert_numpy_array_to_list_recursively(data):\n    for key, value in data.items():\n        if isinstance(value, dict):\n            convert_numpy_array_to_list_recursively(value)\n\n        elif isinstance(value, np.ndarray):\n            data[key] = value.tolist()\n\n        elif isinstance(value, list):\n            for entity in value:\n                if isinstance(entity, dict):\n                    convert_numpy_array_to_list_recursively(entity)\n\n\n@shared_docs(save_dict)\ndef save_json(network, filepath, indent=None):\n    """"""\n    Save network parameters in JSON format.\n\n    Parameters\n    ----------\n    {save_dict.network}\n\n    filepath : str\n        Path to the JSON file that stores network parameters.\n\n    indent : int or None\n        Indentation that would be specified for the output JSON.\n        Intentation equal to `2` or `4` makes it easy to read raw\n        text files. The `None` value disables indentation which means\n        that everything will be stored compactly. Defaults to `None`.\n\n    Examples\n    --------\n    >>> from neupy import layers, storage\n    >>>\n    >>> network = layers.Input(10) > layers.Softmax(3)\n    >>> storage.save_json(network, \'/path/to/parameters.json\')\n    """"""\n    network = extract_network(network)\n    data = save_dict(network)\n\n    with open(filepath, \'w\') as f:\n        # Without extra data processor we won\'t be able to dump\n        # numpy array into json without raising an error.\n        # `json` will have issues with numpy array encoding\n        convert_numpy_array_to_list_recursively(data)\n        return json.dump(data, f, indent=indent, default=repr)\n\n\n@shared_docs(load_dict)\ndef load_json(network, filepath, ignore_missing=False,\n              load_by=\'names_or_order\', skip_validation=True):\n    """"""\n    Load network parameters from JSON file.\n\n    Parameters\n    ----------\n    {load_dict.network}\n\n    filepath : str\n        Path to JSON file that will store network parameters.\n\n    {load_dict.ignore_missing}\n\n    {load_dict.load_by}\n\n    {load_dict.skip_validation}\n\n    Raises\n    ------\n    {load_dict.Raises}\n\n    Examples\n    --------\n    >>> from neupy import layers, storage\n    >>>\n    >>> network = layers.Input(10) > layers.Softmax(3)\n    >>> storage.load_json(network, \'/path/to/parameters.json\')\n    """"""\n    network = extract_network(network)\n    data = save_dict(network)\n\n    with open(filepath, \'r\') as f:\n        data = json.load(f)\n\n    load_dict(network, data, ignore_missing, load_by)\n\n\n# Convenient aliases\nsave = save_hdf5\nload = load_hdf5\n'"
site/conf.py,0,"b'# -*- coding: utf-8 -*-\n\n# flake8: noqa\n\nimport os\nimport re\nimport sys\nimport importlib\n\nimport bs4\nimport tinkerer\nimport tinkerer.paths\n\n# Change this to the name of your blog\nproject = \'NeuPy\'\n\n# Change this to the tagline of your blog\ntagline = \'Neural Networks in Python\'\n\n# Change this to the description of your blog\ndescription = (\'NeuPy is a Python library for Artificial Neural Networks. \'\n               \'NeuPy supports many different types of Neural Networks \'\n               \'from a simple perceptron to deep learning models.\')\n\n# Change this to your name\nauthor = \'Yurii Shevchuk\'\n\n# Change this to your copyright string\ncopyright = \'2015 - 2019, \' + author\n\n# Change this to your blog root URL (required for RSS feed)\nwebsite = \'http://neupy.com\'\n\n# **************************************************************\n# More tweaks you can do\n# **************************************************************\n\n# Add your Disqus shortname to enable comments powered by Disqus\ndisqus_shortname = ""neupy""\n\n# Change your favicon (new favicon goes in _static directory)\nhtml_favicon = \'_static/favicon.ico\'\n\n# Pick another Tinkerer theme or use your own\nhtml_theme = \'flat\'\n\n# Theme-specific options, see docs\nhtml_theme_options = {}\n\n# Link to RSS service like FeedBurner if any, otherwise feed is\n# linked directly\nrss_service = None\n\n# Generate full posts for RSS feed even when using ""read more""\nrss_generate_full_posts = False\n\n# Number of blog posts per page\nposts_per_page = 10\n\n# Character use to replace non-alphanumeric characters in slug\nslug_word_separator = \'_\'\n\n# Set to page under /pages (eg. ""about"" for ""pages/about.html"")\nlanding_page = \'home\'\n\n# Set to override the default name of the first page (\'Home\')\nfirst_page_title = \'Articles\'\n\n# **************************************************************\n# Edit lines below to further customize Sphinx build\n# **************************************************************\n\n# Add other Sphinx extensions here\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'numpydoc\',\n    \'sphinx.ext.linkcode\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.mathjax\',\n\n    \'tinkerer.ext.blog\',\n    \'tinkerer.ext.disqus\',\n]\n\nautosummary_generate = True\nautodoc_default_flags = [\'members\', \'undoc-members\']\n\n# Add other template paths here\ntemplates_path = [\'_templates\']\n\n# Add other static paths here\nhtml_static_path = [\'_static\', tinkerer.paths.static]\n\n# Add other theme paths here\nhtml_theme_path = [\'_themes\', tinkerer.paths.themes]\n\n# Add file patterns to exclude from build\nexclude_patterns = [\'drafts/*\', \'_templates/*\']\n\n# Add templates to be rendered in sidebar here\nhtml_sidebars = {\n    \'**\': [\n        # \'recent.html\',\n        \'searchbox.html\',\n        \'installation.html\',\n        \'issues.html\',\n    ],\n}\n\n# Add an index to the HTML documents.\nhtml_use_index = False\n\n# **************************************************************\n# Autodoc settigs\n# **************************************************************\n\n# Add module folder in path to make it visible for autodoc extention\nmodule_path = os.path.abspath(os.path.join(\'..\', \'neupy\'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nautoclass_content = ""class""\n\n# **************************************************************\n# NumPyDoc\n# **************************************************************\n\nnumpydoc_show_class_members = False\nnumpydoc_show_inherited_class_members = False\nnumpydoc_class_members_toctree = True\n\n# **************************************************************\n# Customizations\n# **************************************************************\n\nGITHUB_REPO = \'https://github.com/itdxer/neupy/tree/master\'\n\n\ndef linkcode_resolve(domain, info):\n    if domain == \'py\' and info[\'module\']:\n        module_name = info[\'module\']\n\n        if \'.\' not in info[\'fullname\']:\n            module = importlib.import_module(module_name)\n            value = getattr(module, info[\'fullname\'])\n            module_name = value.__module__\n\n        filename = module_name.replace(\'.\', \'/\')\n        return ""{}/{}.py"".format(GITHUB_REPO, filename)\n\n\ndef get_module_for_class(classname, moduletype):\n    """"""\n    Return model for class just using its name and module type.\n    """"""\n    available_module_types = {\n        \'architecture\': \'neupy.architectures\',\n        \'network\': \'neupy.algorithms\',\n        \'layer\': \'neupy.layers\',\n        \'plot\': \'neupy.plots\',\n    }\n\n    if moduletype not in available_module_types:\n        raise ValueError(""Invalid module type `{}`"".format(moduletype))\n\n    modulepath = available_module_types[moduletype]\n    algorithms_module = importlib.import_module(modulepath)\n    network_class = getattr(algorithms_module, classname)\n\n    if network_class is None:\n        raise ImportError(""Can\'t import network class {}"".format(classname))\n\n    return network_class.__module__\n\n\ndef process_docstring(app, what, name, obj, options, lines):\n    """"""\n    Function replaces labeled class names to real links in\n    the documentation.\n\n    Available types:\n    - :network:`NetworkClassName`\n    - :layer:`LayerClassName`\n    - :plot:`function_name`\n    - :architecture:`function_name`\n    """"""\n    labels = [\'network\', \'layer\', \'plot\', \'architecture\']\n    labels_regexp = \'|\'.join(labels)\n\n    regexp = re.compile(\n        r\'\\:({})+?\\:\'\n        r\'\\`(.+?)(|\\s<.+?>)\\`\'.format(labels_regexp)\n    )\n\n    if options is not None:\n        # Do not show information about parent classes\n        options[\'show-inheritance\'] = False\n\n    for i, line in enumerate(lines):\n        # Replace values one by one to make sure that there is no overlaps.\n        replacement = regexp.search(line)\n        while replacement:\n            moduletype, display_name, classname = replacement.groups()\n\n            if not classname:\n                classname = display_name\n            else:\n                # Remove first second and last symbols.\n                # They must be: \\s, < and > respectively.\n                classname = classname[2:-1]\n\n            module = get_module_for_class(classname, moduletype)\n\n            if not classname:\n                newline_pattern = r\':class:`\\2 <{}\\.\\2>`\'.format(module)\n            else:\n                newline_pattern = r\':class:`\\2 <{}\\.{}>`\'.format(\n                    module, classname)\n\n            line = regexp.sub(newline_pattern, line, count=1)\n            line = line.replace(\'\\.\', \'.\')\n            lines[i] = line\n            replacement = regexp.search(line)\n\n\ndef preprocess_texts(app, docname, source):\n    """"""\n    Call the same behaviour for all texts, not only for autodoc.\n    """"""\n    process_docstring(app, None, None, None, None, source)\n\n\ndef extend_html_page_context(app, pagename, templatename, context, doctree):\n    env = app.builder.env\n\n    if pagename in env.blog_metadata:\n        metadata = env.blog_metadata[pagename]\n\n        if metadata.is_post:\n            body = bs4.BeautifulSoup(metadata.body)\n\n            short_description = body.select_one(\'.short-description\')\n\n            if short_description is None:\n                print(""Article {} doesn\'t have short description""\n                      """".format(pagename))\n\n                metadata.short_description = """"\n                return\n\n            short_description_html = str(short_description)\n            short_description.decompose()\n\n            metadata.short_description = short_description_html\n            metadata.body = str(body)\n\n\ndef process_arguments(app, what, name, obj, options, signature,\n                      return_annotation):\n    """"""\n    Exclude arguments for classes in the documentation.\n    """"""\n    if what == \'class\':\n        return (None, None)\n\n\ndef setup(app):\n    """"""\n    Function with reserved name that would be trigger by Sphinx\n    if it will find such function in configuration file.\n    """"""\n    app.connect(\'autodoc-process-docstring\', process_docstring)\n    app.connect(\'autodoc-process-signature\', process_arguments)\n    app.connect(\'source-read\', preprocess_texts)\n    app.connect(""html-page-context"", extend_html_page_context)\n\n# **************************************************************\n# Do not modify below lines as the values are required by\n# Tinkerer to play nice with Sphinx\n# **************************************************************\n\nsource_suffix = tinkerer.source_suffix\nmaster_doc = tinkerer.master_doc\nversion = tinkerer.__version__\nrelease = tinkerer.__version__\n\nhtml_title = project\nhtml_show_sourcelink = False\nhtml_add_permalinks = None\n'"
tests/base.py,3,"b'import pickle\nimport inspect\nimport logging\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy import utils, layers, init\nfrom neupy.utils import tensorflow_eval, tensorflow_session, shape_to_tuple\nfrom neupy.layers.base import format_name_if_specified_as_pattern\n\nfrom helpers import vectors_for_testing\n\n\nclass BaseTestCase(unittest.TestCase):\n    single_thread = False\n    verbose = False\n    random_seed = 0\n\n    def eval(self, value):\n        return tensorflow_eval(value)\n\n    def setUp(self):\n        tf.reset_default_graph()\n\n        if self.single_thread:\n            sess = tensorflow_session()\n            sess.close()\n\n            config = tf.ConfigProto(\n                allow_soft_placement=True,\n                intra_op_parallelism_threads=1,\n                inter_op_parallelism_threads=1,\n            )\n            tensorflow_session.cache = tf.Session(config=config)\n\n        if not self.verbose:\n            logging.disable(logging.CRITICAL)\n\n        # Clean identifiers map for each test\n        if hasattr(format_name_if_specified_as_pattern, \'counters\'):\n            del format_name_if_specified_as_pattern.counters\n\n        utils.reproducible(seed=self.random_seed)\n\n    def tearDown(self):\n        sess = tensorflow_session()\n        sess.close()\n\n    def assertItemsEqual(self, list1, list2):\n        self.assertEqual(sorted(list1), sorted(list2))\n\n    def assertInvalidVectorTrain(self, network, input_vector, target=None,\n                                 decimal=5, is_feature1d=True, **train_kwargs):\n        """"""\n        Method helps test network prediction training using different\n        types of row or column vector.\n        """"""\n        input_vectors = vectors_for_testing(input_vector, is_feature1d)\n\n        if target is not None:\n            target_vectors = vectors_for_testing(target, is_feature1d)\n            input_vectors = zip(input_vectors, target_vectors)\n\n        train_args = inspect.getargspec(network.train).args\n\n        if \'epochs\' in train_args and \'epochs\' not in train_kwargs:\n            train_kwargs[\'epochs\'] = 5\n\n        elif \'epsilon\' in train_args and \'epsilon\' not in train_kwargs:\n            train_kwargs[\'epsilon\'] = 0.1\n\n        for i, X in enumerate(input_vectors, start=1):\n            if target is None:\n                network.train(X, **train_kwargs)\n            else:\n                network.train(*X, **train_kwargs)\n\n    def assertInvalidVectorPred(self, network, input_vector, target,\n                                decimal=5, is_feature1d=True):\n        """"""\n        Method helps test network prediction procedure using different\n        types of row or column vector.\n        """"""\n        test_vectors = vectors_for_testing(input_vector, is_feature1d)\n\n        for i, test_vector in enumerate(test_vectors, start=1):\n            predicted_vector = network.predict(test_vector)\n            np.testing.assert_array_almost_equal(\n                predicted_vector, target, decimal=decimal)\n\n    def assertPickledNetwork(self, network, X):\n        stored_network = pickle.dumps(network)\n        loaded_network = pickle.loads(stored_network)\n\n        network_prediction = network.predict(X)\n        loaded_network_prediction = loaded_network.predict(X)\n\n        np.testing.assert_array_almost_equal(\n            loaded_network_prediction,\n            network_prediction)\n\n    def assertCanNetworkOverfit(self, network_class, epochs=100,\n                                min_accepted_loss=0.001):\n\n        x_train = 2 * np.random.random((10, 2)) - 1  # zero centered\n        y_train = np.random.random((10, 1))\n\n        relu_xavier_normal = init.XavierNormal(gain=4)\n        relu_weight = relu_xavier_normal.sample((2, 20), return_array=True)\n\n        xavier_normal = init.XavierNormal(gain=2)\n        sigmoid_weight = xavier_normal.sample((20, 1), return_array=True)\n\n        optimizer = network_class([\n            layers.Input(2),\n            layers.Relu(20, weight=relu_weight),\n            layers.Sigmoid(1, weight=sigmoid_weight),\n        ])\n\n        optimizer.train(x_train, y_train, epochs=epochs)\n        self.assertLess(optimizer.errors.train[-1], min_accepted_loss)\n\n    def assertShapesEqual(self, shape1, shape2, *args, **kwargs):\n        shape1 = shape_to_tuple(shape1)\n        shape2 = shape_to_tuple(shape2)\n        self.assertEqual(shape1, shape2, *args, **kwargs)\n'"
tests/helpers.py,0,"b'import sys\nfrom contextlib import contextmanager\n\nimport six\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom matplotlib import pyplot as plt\n\nfrom neupy import algorithms, layers, utils, init\nfrom neupy.storage import save_dict, load_dict\n\n\ndef simple_classification(n_samples=100, n_features=10, random_state=33):\n    """"""\n    Generate simple classification task for training.\n\n    Parameters\n    ----------\n    n_samples : int\n        Number of samples in dataset.\n    n_features : int\n        Number of features for each sample.\n    random_state : int\n        Random state to make results reproducible.\n\n    Returns\n    -------\n    tuple\n        Returns tuple that contains 4 variables. There are input train,\n        input test, target train, target test respectevly.\n    """"""\n    X, y = datasets.make_classification(\n        n_samples=n_samples,\n        n_features=n_features,\n        random_state=random_state,\n    )\n    shuffle_split = StratifiedShuffleSplit(\n        n_splits=1,\n        train_size=0.6,\n        test_size=0.1,\n        random_state=random_state,\n    )\n\n    train_index, test_index = next(shuffle_split.split(X, y))\n    x_train, x_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    return x_train, x_test, y_train, y_test\n\n\n@contextmanager\ndef catch_stdout():\n    """"""\n    Context manager that catches output in terminal and returns\n    StringIO instance.\n\n    Yields\n    ------\n    StringIO instance\n\n    Examples\n    --------\n    >>> with catch_stdout() as out:\n    ...     print(""Unittest"")\n    ...     terminal_output = out.getvalue()\n    ...     self.assertIn(""test"", terminal_output)\n    """"""\n    old_out = sys.stdout\n    old_err = sys.stderr\n\n    out = six.StringIO()\n\n    sys.stdout = out\n    sys.stderr = out\n\n    yield out\n\n    sys.stdout = old_out\n    sys.stderr = old_err\n\n\ndef compare_networks(default_class, tested_class, data, **kwargs):\n    """"""\n    Compare two network arcitectures.\n\n    Parameters\n    ----------\n    default_class : BaseNetwork instance\n    tested_class : BaseNetwork instance\n    data : tuple\n    **kwargs :\n\n    Raises\n    ------\n    AssertionError\n        Raise exception when first network have better prediction\n        accuracy then the second one.\n    """"""\n    epochs = kwargs.pop(\'epochs\', 100)\n    show_comparison_plot = kwargs.pop(\'show_comparison_plot\', False)\n\n    # Compute result for default network (which must be slower)\n    network = default_class(**kwargs)\n\n    if hasattr(network, \'connection\'):\n        initial_parameters = save_dict(network.network)\n\n    network.train(*data, epochs=epochs)\n\n    network_default_error = network.errors.train[-1]\n    errors1 = network.errors.train\n\n    # Compute result for test network (which must be faster)\n    if hasattr(network, \'connection\'):\n        load_dict(network.network, initial_parameters)\n\n    network = tested_class(**kwargs)\n\n    network.train(*data, epochs=epochs)\n    network_tested_error = network.errors.train[-1]\n    errors2 = network.errors.train\n\n    if show_comparison_plot:\n        error_range = np.arange(max(len(errors1), len(errors2)))\n        plt.plot(error_range[:len(errors1)], errors1)\n        plt.plot(error_range[:len(errors2)], errors2)\n        plt.show()\n\n    if network_default_error <= network_tested_error:\n        raise AssertionError(\n            ""First network has smaller error ({}) that the second one ({}).""\n            """".format(network_default_error, network_tested_error))\n\n\ndef reproducible_network_train(seed=0, epochs=500, **additional_params):\n    """"""\n    Make a reproducible train for Gradient Descent based neural\n    network with a XOR problem and return trained network.\n\n    Parameters\n    ----------\n    seed : int\n        Random State seed number for reproducibility. Defaults to ``0``.\n    epochs : int\n        Number of epochs for training. Defaults to ``500``.\n    **additional_params\n        Aditional parameters for Neural Network.\n\n    Returns\n    -------\n    GradientDescent instance\n        Returns trained network.\n    """"""\n    utils.reproducible(seed)\n\n    xor_x_train = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n    xor_y_train = np.array([[1, -1, -1, 1]]).T\n\n    xavier_normal = init.XavierNormal()\n    tanh_weight1 = xavier_normal.sample((2, 5), return_array=True)\n    tanh_weight2 = xavier_normal.sample((5, 1), return_array=True)\n\n    network = algorithms.GradientDescent(\n        network=[\n            layers.Input(2),\n            layers.Tanh(5, weight=tanh_weight1),\n            layers.Tanh(1, weight=tanh_weight2),\n        ],\n        batch_size=None,\n        **additional_params\n    )\n    network.train(xor_x_train, xor_y_train, epochs=epochs)\n    return network\n\n\ndef vectors_for_testing(vector, is_feature1d=True):\n    """"""\n    Function generate different possible variations of one vector.\n    That feature useful for testing algorithms input data.\n\n    Parameters\n    ----------\n    vector : ndarray\n        Vector that would be transformed in different data types.\n    is_feature1d : bool\n        Parameter explain the vector type. Parameter equal to ``True`` mean\n        that input data a banch of samples that contains one feature each.\n        Defaults to ``True``.\n\n    Raises\n    ------\n    ValueError\n        If input is not a vector\n\n    Returns\n    -------\n    list\n        List that contains the same vectors in different data types like\n        numpy 2D vector or pandas Data Frame\n    """"""\n\n    if vector.ndim != 1 and min(vector.shape) != 1:\n        raise ValueError(""Input should be a vector"")\n\n    shape2d = (vector.size, 1) if is_feature1d else (1, vector.size)\n\n    vectors_list = []\n    if vector.ndim == 1:\n        vectors_list.extend([vector, pd.Series(vector)])\n\n    vectors_list.extend([\n        vector.reshape(shape2d),\n        pd.DataFrame(vector.reshape(shape2d))])\n\n    return vectors_list\n'"
examples/autoencoder/__init__.py,0,b''
examples/autoencoder/conv_autoencoder.py,0,"b""import numpy as np\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\nfrom neupy.layers import *\nfrom neupy import algorithms\n\n\ndef load_data():\n    X, _ = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)\n    X = (X / 255.).astype(np.float32)\n\n    np.random.shuffle(X)\n    x_train_2d, x_test_2d = X[:60000], X[60000:]\n    x_train_4d = x_train_2d.reshape((60000, 28, 28, 1))\n    x_test_4d = x_test_2d.reshape((10000, 28, 28, 1))\n\n    return x_train_4d, x_test_4d\n\n\ndef visualize_reconstructions(x_test_4d, n_samples=6):\n    x_test = x_test_4d.reshape(x_test_4d.shape[0], -1)\n    images = x_test[:n_samples] * 255.\n    predicted_images = optimizer.predict(x_test_4d[:n_samples])\n    predicted_images = predicted_images * 255.\n\n    # Compare real and reconstructed images\n    fig, axes = plt.subplots(n_samples, 2, figsize=(12, 8))\n    iterator = zip(axes, images, predicted_images)\n\n    for (left_ax, right_ax), real_image, predicted_image in iterator:\n        real_image = real_image.reshape((28, 28))\n        predicted_image = predicted_image.reshape((28, 28))\n\n        left_ax.imshow(real_image, cmap=plt.cm.binary)\n        right_ax.imshow(predicted_image, cmap=plt.cm.binary)\n\n    plt.show()\n\n\nif __name__ == '__main__':\n    network = join(\n        Input((28, 28, 1)),\n\n        Convolution((3, 3, 16)) >> Relu(),\n        Convolution((3, 3, 16)) >> Relu(),\n        MaxPooling((2, 2)),\n\n        Convolution((3, 3, 32)) >> Relu(),\n        MaxPooling((2, 2)),\n\n        Reshape(),\n\n        Relu(128),\n        Relu(16),\n\n        # Notice that in the decoder every operation reverts back\n        # changes from the encoder layer.\n        Relu(128),\n\n        # 800 is a shape that we got after we reshaped our image in the\n        # Reshape layer\n        Relu(800),\n\n        Reshape((5, 5, 32)),\n\n        # Upscaling layer reverts changes from the max pooling layer\n        Upscale((2, 2)),\n\n        # Deconvolution (a.k.a Transposed Convolution) reverts\n        # changes done by Convolution\n        Deconvolution((3, 3, 16)) >> Relu(),\n\n        Upscale((2, 2)),\n        Deconvolution((3, 3, 16)) >> Relu(),\n        Deconvolution((3, 3, 1)) >> Sigmoid()\n    )\n    optimizer = algorithms.Momentum(\n        network,\n        step=0.02,\n        momentum=0.9,\n        batch_size=128,\n        loss='rmse',\n\n        shuffle_data=True,\n        verbose=True,\n\n        regularizer=algorithms.l2(0.01),\n    )\n\n    x_train_4d, x_test_4d = load_data()\n    optimizer.train(x_train_4d, x_train_4d, x_test_4d, x_test_4d, epochs=1)\n    visualize_reconstructions(x_test_4d, n_samples=6)\n"""
examples/autoencoder/denoising_autoencoder.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom neupy import algorithms, layers, utils\n\n\ndef load_data():\n    X, _ = datasets.fetch_openml(\'mnist_784\', version=1, return_X_y=True)\n    X = (X / 255.).astype(np.float32)\n\n    np.random.shuffle(X)\n    x_train, x_test = X[:60000], X[60000:]\n\n    return x_train, x_test\n\n\ndef visualize_reconstructions(autoencoder, x_test):\n    n_samples = 4\n    image_vectors = x_test[:n_samples, :]\n    images = image_vectors * 255.\n    predicted_images = autoencoder.predict(image_vectors)\n    predicted_images = predicted_images * 255.\n\n    # Compare real and reconstructed images\n    fig, axes = plt.subplots(4, 2, figsize=(12, 8))\n    iterator = zip(axes, images, predicted_images)\n\n    for (left_ax, right_ax), real_image, predicted_image in iterator:\n        real_image = real_image.reshape((28, 28))\n        predicted_image = predicted_image.reshape((28, 28))\n\n        left_ax.imshow(real_image, cmap=plt.cm.binary)\n        right_ax.imshow(predicted_image, cmap=plt.cm.binary)\n\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    autoencoder = algorithms.Momentum(\n        [\n            layers.Input(784),\n            layers.GaussianNoise(mean=0.5, std=0.1),\n            layers.Sigmoid(100),\n            layers.Sigmoid(784),\n        ],\n        step=0.1,\n        verbose=True,\n        momentum=0.9,\n        nesterov=True,\n        loss=\'rmse\',\n    )\n\n    print(""Preparing data..."")\n    x_train, x_test = load_data()\n\n    print(""Training autoencoder..."")\n    autoencoder.train(x_train, x_train, x_test, x_test, epochs=40)\n\n    visualize_reconstructions(autoencoder, x_test)\n'"
examples/autoencoder/stacked_conv_autoencoders.py,0,"b'from __future__ import division\n\nimport numpy as np\nfrom sklearn import datasets, metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom neupy.layers import *\nfrom neupy import algorithms, layers\n\n\nX, y = datasets.fetch_openml(\'mnist_784\', version=1, return_X_y=True)\nX = X / 255.\n\ntarget_scaler = OneHotEncoder(categories=\'auto\', sparse=False)\ny = target_scaler.fit_transform(y.reshape(-1, 1))\n\n# Originaly we should have 70000 images from the MNIST dataset, but\n# we will use only 1000 training example, All data that doesn\'t have\n# labels we use to train features in the convolutional autoencoder.\nn_labeled = 1000\nn_samples = len(X)\nn_unlabeled = n_samples - n_labeled\n\nx_labeled, x_unlabeled, y_labeled, y_unlabeled = train_test_split(\n    X.astype(np.float32),\n    y.astype(np.float32),\n    test_size=(1 - n_labeled / n_samples)\n)\n\nx_labeled_4d = x_labeled.reshape((n_labeled, 28, 28, 1))\nx_unlabeled_4d = x_unlabeled.reshape((n_unlabeled, 28, 28, 1))\n\n# We will features trained in the encoder and the first part for the future\n# classifier. At first we pre-train them with unlabeled data, since we have\n# a lot of it and we hope to learn some common features from it.\nencoder = join(\n    Input((28, 28, 1)),\n\n    Convolution((3, 3, 16)) >> Relu(),\n    Convolution((3, 3, 16)) >> Relu(),\n    MaxPooling((2, 2)),\n\n    Convolution((3, 3, 32)) >> Relu(),\n    MaxPooling((2, 2)),\n\n    Reshape(),\n\n    Relu(256),\n    Relu(128),\n)\n\n# Notice that in the decoder every operation reverts\n# back changes from the encoder layer.\ndecoder = join(\n    Relu(256),\n    Relu(32 * 5 * 5),\n\n    Reshape((5, 5, 32)),\n\n    Upscale((2, 2)),\n    Deconvolution((3, 3, 16)) >> Relu(),\n\n    Upscale((2, 2)),\n    Deconvolution((3, 3, 16)) >> Relu(),\n    Deconvolution((3, 3, 1)) >> Sigmoid(),\n)\n\nconv_autoencoder = algorithms.Momentum(\n    network=(encoder >> decoder),\n\n    loss=\'rmse\',\n    step=0.02,\n    batch_size=128,\n    regularizer=algorithms.l2(0.001),\n\n    shuffle_data=True,\n    verbose=True,\n)\nconv_autoencoder.train(\n    x_unlabeled_4d, x_unlabeled_4d,\n    x_labeled_4d, x_labeled_4d,\n    epochs=1,\n)\n\n# In order to speed up training for the upper layers we generate\n# output from the encoder. In this way we won\'t need to regenerate\n# encoded inputs for every epoch.\nx_labeled_encoded = encoder.predict(x_labeled_4d)\nx_unlabeled_encoded = encoder.predict(x_unlabeled_4d)\n\nclassifier_network = join(\n    PRelu(512),\n    Dropout(0.25),\n    Softmax(10),\n)\n\nencoder_classifier = algorithms.Adadelta(\n    Input(encoder.output_shape[1:]) >> classifier_network,\n    verbose=True,\n    step=0.05,\n    shuffle_data=True,\n    batch_size=64,\n    loss=\'categorical_crossentropy\',\n)\nencoder_classifier.train(\n    x_labeled_encoded, y_labeled,\n    x_unlabeled_encoded, y_unlabeled,\n    epochs=400,\n)\n\n# The final part of training is to put encoder and final classifier layers\n# in order to fine tune network parameters before finilizing it\'s prediction\nclassifier = algorithms.GradientDescent(\n    network=(encoder >> classifier_network),\n    verbose=True,\n    step=0.005,\n    shuffle_data=True,\n    batch_size=64,\n    loss=\'categorical_crossentropy\',\n    regularizer=algorithms.l2(0.02),\n)\nclassifier.train(x_labeled_4d, y_labeled, epochs=100)\nclassifier.train(\n    x_labeled_4d, y_labeled,\n    x_unlabeled_4d, y_unlabeled,\n    epochs=1,\n)\n\nunlabeled_predicted = classifier.predict(x_unlabeled_4d).argmax(axis=1)\ny_unlabeled_classes = np.asarray(y_unlabeled).argmax(axis=1)\n\nprint(metrics.classification_report(y_unlabeled_classes, unlabeled_predicted))\nscore = metrics.accuracy_score(y_unlabeled_classes, unlabeled_predicted)\nprint(""Validation accuracy: {:.2%}"".format(score))\n'"
examples/autoencoder/variational_autoencoder.py,12,"b""import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn import model_selection, datasets\n\nfrom neupy import algorithms\nfrom neupy.layers import *\nfrom neupy.exceptions import LayerConnectionError\n\n\ndef load_data():\n    X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)\n\n    X = X / 255.\n    X -= X.mean(axis=0)\n\n    x_train, x_test = model_selection.train_test_split(\n        X.astype(np.float32),\n        test_size=(1 / 7.)\n    )\n    return x_train, x_test\n\n\ndef generate_and_plot_sampels(network, nx, ny):\n    space_x = np.linspace(-3, 3, nx)\n    space_y = np.linspace(-3, 3, ny)\n\n    plt.figure(figsize=(12, 12))\n\n    grid = gridspec.GridSpec(nx, ny)\n    grid.update(wspace=0, hspace=0)\n\n    for i, x in enumerate(space_x):\n        for j, y in enumerate(space_y):\n            sample = np.array([[x, y]])\n            digit = generator.predict(sample)\n            image = digit[0].reshape((28, 28))\n\n            plt.subplot(grid[nx * i + j])\n            plt.imshow(image, cmap='Greys_r')\n            plt.axis('off')\n\n    plt.show()\n\n\nclass GaussianSample(BaseLayer):\n    def get_output_shape(self, mu_shape, sigma_shape):\n        return mu_shape\n\n    def output(self, mu, sigma, **kwargs):\n        return mu + tf.exp(sigma) * tf.random_normal(tf.shape(mu))\n\n\nclass Collect(Identity):\n    def output(self, input, **kwargs):\n        tf.add_to_collection(self.name, input)\n        return input\n\n\ndef binary_crossentropy(expected, predicted):\n    epsilon = 1e-7  # smallest positive 32-bit float number\n    predicted = tf.clip_by_value(predicted, epsilon, 1.0 - epsilon)\n\n    return tf.reduce_sum(\n        expected * tf.log(predicted) +\n        (1 - expected) * tf.log(1 - predicted),\n        axis=1\n    )\n\n\ndef vae_loss(expected, predicted):\n    mean = tf.get_collection('mu')[-1]\n    log_var = tf.get_collection('sigma')[-1]\n\n    epsilon = 1e-7\n    predicted = tf.clip_by_value(predicted, epsilon, 1.0 - epsilon)\n\n    crossentropy_loss = binary_crossentropy(expected, predicted)\n    kl_loss = tf.reduce_sum(\n        1 + log_var - tf.square(mean) - tf.exp(log_var),\n        axis=1\n    )\n    return tf.reduce_mean(-crossentropy_loss - 0.5 * kl_loss)\n\n\nif __name__ == '__main__':\n    # Construct Variational Autoencoder\n    network = join(\n        # Encoder\n        Input(784, name='input'),\n        Tanh(256),\n\n        # Sampler\n        parallel(\n            # Two is the maximum number of dimensions that we can visualize\n            Linear(2, name='mu') >> Collect('mu'),\n            Linear(2, name='sigma') >> Collect('sigma'),\n        ),\n        GaussianSample(),\n\n        # Decoder\n        # Note: Identity layer acts as a reference. Using it\n        # we can easily cut the decoder from the network\n        Identity('decoder'),\n        Tanh(256),\n        Sigmoid(784),\n    )\n\n    # Train network\n    optimizer = algorithms.RMSProp(\n        network,\n        loss=vae_loss,\n        regularizer=algorithms.l2(0.001),\n\n        batch_size=128,\n        shuffle_data=True,\n        step=0.001,\n        verbose=True,\n    )\n\n    x_train, x_test = load_data()\n    optimizer.train(x_train, x_train, x_test, x_test, epochs=50)\n\n    # Sample digits from the obtained distribution\n    generator = Input(2) >> network.start('decoder')\n    generate_and_plot_sampels(generator, 15, 15)\n"""
examples/boltzmann_machine/rbm_faces_sampling.py,0,"b'import itertools\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom skimage.filters import threshold_local\nfrom neupy import algorithms, utils\nfrom neupy.utils import asfloat\n\n\ndef show_image(ax, image):\n    image_shape = (62, 47)\n    ax.imshow(image.reshape(image_shape), cmap=plt.cm.gray)\n\n\ndef disable_ticks(ax):\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n\ndef plot_rbm_sampled_images(rbm_network, data, training_data):\n    n_samples = len(training_data)\n    iterations = [0, 0, 1, 10, 100, 1000]\n\n    nrows, ncols = (6, 6)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 16))\n\n    titles = [\n        \'Raw image\',\n        \'Binarized image\',\n        \'Sample after\\n1 iteration\',\n        \'Sample after\\n10 iterations\',\n        \'Sample after\\n100 iterations\',\n        \'Sample after\\n1000 iterations\',\n    ]\n\n    for i, title in enumerate(titles):\n        axes[0][i].set_title(title)\n\n    for i, row_axes in enumerate(axes):\n        image_index = np.random.randint(0, n_samples)\n        binary_image = training_data[image_index]\n\n        show_image(row_axes[0], data[image_index])\n        show_image(row_axes[1], binary_image)\n\n        for j in range(2, ncols):\n            n_iter = iterations[j]\n            sampled_image = rbm_network.gibbs_sampling(binary_image, n_iter)\n            show_image(row_axes[j], sampled_image)\n\n    for ax in itertools.chain(*axes):\n        disable_ticks(ax)\n\n\ndef binarize_images(data):\n    binarized_data = []\n    for image in data:\n        image = image.reshape((62, 47))\n        image_threshold = threshold_local(image, block_size=15)\n        binary_adaptive_image = image > image_threshold\n        binarized_data.append(binary_adaptive_image.ravel())\n    return asfloat(binarized_data)\n\n\nutils.reproducible()\n\nprint(""Reading images..."")\npeople_dataset = datasets.fetch_lfw_people()\ndata = people_dataset.data\nnp.random.shuffle(data)\n\nprint(""Binarizing images..."")\nbinarized_data = binarize_images(data)\n\nx_train, x_test, binarized_x_train, binarized_x_test = train_test_split(\n    data, binarized_data, test_size=0.1\n)\n\nrbm = algorithms.RBM(\n    n_visible=2914,\n    n_hidden=1200,\n    step=0.01,\n    batch_size=10,\n\n    verbose=True,\n    shuffle_data=True,\n)\nrbm.train(binarized_x_train, binarized_x_test, epochs=40)\n\nplot_rbm_sampled_images(rbm, x_test, binarized_x_test)\nplt.show()\n'"
examples/boltzmann_machine/rbm_mnist.py,0,"b""import matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom neupy import algorithms, utils\nfrom neupy.utils import asfloat, tensorflow_session\n\n\ndef plot_rbm_components(rbm_network):\n    session = tensorflow_session()\n    weight = session.run(rbm_network.weight)\n\n    plt.figure(figsize=(10, 10))\n    plt.suptitle('RBM componenets', size=16)\n\n    for index, image in enumerate(weight.T, start=1):\n        plt.subplot(10, 10, index)\n        plt.imshow(image.reshape((28, 28)), cmap=plt.cm.gray)\n\n        plt.xticks([])\n        plt.yticks([])\n\n    plt.show()\n\n\nutils.reproducible()\n\nX, _ = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)\nX = asfloat(X > 130)\n\nrbm = algorithms.RBM(\n    n_visible=784,\n    n_hidden=100,\n    step=0.01,\n    batch_size=20,\n\n    verbose=True,\n    shuffle_data=True,\n)\nrbm.train(X, X, epochs=10)\nplot_rbm_components(rbm)\n"""
examples/cnn/alexnet.py,2,"b'import tensorflow as tf\n\nfrom neupy.layers import *\n\n\nclass SliceChannels(BaseLayer):\n    """"""\n    Layer expects image as an input with second dimension\n    sepcified as a channel. Image will be sliced over the channel.\n    The ``from`` and ``to`` indices can be specified as the parameters.\n\n    Parameters\n    ----------\n    from_channel : int\n        From which channel we will start slicing.\n\n    to_channel : int\n        To which channel we will be slicing. This layer won\'t be\n        included in the output.\n\n    {BaseLayer.name}\n    """"""\n    def __init__(self, from_channel, to_channel, name=None):\n        self.from_channel = from_channel\n        self.to_channel = to_channel\n        super(SliceChannels, self).__init__(name=name)\n\n    def get_output_shape(self, input_shape):\n        n_channels = self.to_channel - self.from_channel\n\n        if input_shape.ndims is None:\n            return tf.TensorShape((None, None, None, n_channels))\n\n        if input_shape.ndims != 4:\n            raise ValueError(\n                ""Layer {} expects 4 dimensional inputs, got {} instead.""\n                """".format(self.name, input_shape.ndims))\n\n        n_samples, height, width, _ = input_shape\n        return tf.TensorShape((n_samples, height, width, n_channels))\n\n    def output(self, input_value):\n        return input_value[:, :, :, self.from_channel:self.to_channel]\n\n    def __repr__(self):\n        return ""{}({}, {})"".format(\n            self.__class__.__name__,\n            self.from_channel,\n            self.to_channel)\n\n\nalexnet = join(\n    Input((227, 227, 3)),\n\n    Convolution((11, 11, 96), stride=(4, 4), name=\'conv_1\') >> Relu(),\n    MaxPooling((3, 3), stride=(2, 2)),\n    LocalResponseNorm(),\n\n    parallel([\n        SliceChannels(0, 48),\n        Convolution((5, 5, 128), padding=\'SAME\', name=\'conv_2_1\') >> Relu(),\n    ], [\n        SliceChannels(48, 96),\n        Convolution((5, 5, 128), padding=\'SAME\', name=\'conv_2_2\') >> Relu(),\n    ]),\n    Concatenate(),\n\n    MaxPooling((3, 3), stride=(2, 2)),\n    LocalResponseNorm(),\n    Convolution((3, 3, 384), padding=\'SAME\', name=\'conv_3\') >> Relu(),\n\n    parallel([\n        SliceChannels(0, 192),\n        Convolution((3, 3, 192), padding=\'SAME\', name=\'conv_4_1\') >> Relu(),\n    ], [\n        SliceChannels(192, 384),\n        Convolution((3, 3, 192), padding=\'SAME\', name=\'conv_4_2\') >> Relu(),\n    ]),\n    Concatenate(),\n\n    parallel([\n        SliceChannels(0, 192),\n        Convolution((3, 3, 128), padding=\'SAME\', name=\'conv_5_1\') >> Relu(),\n    ], [\n        SliceChannels(192, 384),\n        Convolution((3, 3, 128), padding=\'SAME\', name=\'conv_5_2\') >> Relu(),\n    ]),\n    Concatenate(),\n    MaxPooling((3, 3), stride=(2, 2)),\n\n    Reshape(),\n    Relu(4096, name=\'dense_1\') >> Dropout(0.5),\n    Relu(4096, name=\'dense_2\') >> Dropout(0.5),\n    Softmax(1000, name=\'dense_3\'),\n)\nalexnet.show()\n'"
examples/cnn/cifar10_cnn.py,0,"b'import numpy as np\nfrom sklearn import metrics\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom neupy.layers import *\nfrom neupy import algorithms\nfrom neupy.utils import asfloat\nfrom load_cifar10 import read_cifar10\n\n\ndef process_cifar10_data(x_train, x_test):\n    x_train, x_test = asfloat(x_train), asfloat(x_test)\n\n    mean = x_train.mean(axis=(0, 1, 2)).reshape(1, 1, 1, -1)\n    std = x_train.std(axis=(0, 1, 2)).reshape(1, 1, 1, -1)\n\n    x_train -= mean\n    x_train /= std\n    x_test -= mean\n    x_test /= std\n\n    return x_train, x_test\n\n\ndef one_hot_encoder(y_train, y_test):\n    y_train, y_test = asfloat(y_train), asfloat(y_test)\n\n    target_scaler = OneHotEncoder(categories=\'auto\', sparse=False)\n    y_train = target_scaler.fit_transform(y_train.reshape(-1, 1))\n    y_test = target_scaler.transform(y_test.reshape(-1, 1))\n\n    return y_train, y_test\n\n\nif __name__ == \'__main__\':\n    x_train, x_test, y_train, y_test = read_cifar10()\n\n    x_train, x_test = process_cifar10_data(x_train, x_test)\n    y_train, y_test = one_hot_encoder(y_train, y_test)\n\n    network = algorithms.Adam(\n        [\n            Input((32, 32, 3)),\n\n            Convolution((3, 3, 32)) >> Relu(),\n            Convolution((3, 3, 32)) >> Relu(),\n            MaxPooling((2, 2)),\n\n            Convolution((3, 3, 64)) >> Relu(),\n            Convolution((3, 3, 64)) >> Relu(),\n            MaxPooling((2, 2)),\n\n            Reshape(),\n            Relu(256) >> Dropout(0.5),\n            Softmax(10),\n        ],\n\n        step=algorithms.step_decay(\n            initial_value=0.001,\n            # Parameter controls step redution frequency. The larger\n            # the value the slower step parameter decreases. Step will\n            # be reduced after every mini-batch update. In the training\n            # data we have 500 mini-batches.\n            reduction_freq=5 * 500,\n        ),\n        regularizer=algorithms.l2(0.01),\n\n        loss=\'categorical_crossentropy\',\n        batch_size=100,\n        shuffle_data=True,\n        verbose=True,\n    )\n    network.train(x_train, y_train, x_test, y_test, epochs=30)\n\n    y_predicted = network.predict(x_test).argmax(axis=1)\n    y_test_labels = np.asarray(y_test.argmax(axis=1)).reshape(len(y_test))\n\n    print(metrics.classification_report(y_test_labels, y_predicted))\n    score = metrics.accuracy_score(y_test_labels, y_predicted)\n    print(""Validation accuracy: {:.2%}"".format(score))\n    print(metrics.confusion_matrix(y_predicted, y_test_labels))\n'"
examples/cnn/googlenet.py,0,"b""import tensorflow as tf\n\nfrom neupy.layers import *\n\n\ndef Inception(nfilters):\n    return join(\n        parallel([\n            MaxPooling((3, 3), stride=1, padding='SAME'),\n            Convolution((1, 1, nfilters[0])) >> Relu(),\n        ], [\n            Convolution((1, 1, nfilters[1])) >> Relu(),\n        ], [\n            Convolution((1, 1, nfilters[2])) >> Relu(),\n            Convolution((3, 3, nfilters[3]), padding='SAME') >> Relu(),\n        ], [\n            Convolution((1, 1, nfilters[4])) >> Relu(),\n            Convolution((5, 5, nfilters[5]), padding='SAME') >> Relu(),\n        ]),\n        Concatenate(),\n    )\n\n\nUNKNOWN = None\ngooglenet = join(\n    Input((UNKNOWN, UNKNOWN, 3)),\n\n    Convolution((7, 7, 64), padding='SAME', stride=2),\n    Relu(),\n    MaxPooling((3, 3), stride=2),\n    LocalResponseNorm(alpha=0.00002, k=1),\n\n    Convolution((1, 1, 64)) >> Relu(),\n    Convolution((3, 3, 192), padding='SAME') >> Relu(),\n    LocalResponseNorm(alpha=0.00002, k=1),\n    MaxPooling((3, 3), stride=2),\n\n    Inception((32, 64, 96, 128, 16, 32)),\n    Inception((64, 128, 128, 192, 32, 96)),\n    MaxPooling((3, 3), stride=2),\n\n    Inception((64, 192, 96, 208, 16, 48)),\n    Inception((64, 160, 112, 224, 24, 64)),\n    Inception((64, 128, 128, 256, 24, 64)),\n    Inception((64, 112, 144, 288, 32, 64)),\n    Inception((128, 256, 160, 320, 32, 128)),\n    MaxPooling((3, 3), stride=2),\n\n    Inception((128, 256, 160, 320, 32, 128)),\n    Inception((128, 384, 192, 384, 48, 128)),\n    GlobalPooling('avg'),\n\n    Softmax(1000),\n)\ngooglenet.show()\n"""
examples/cnn/imagenet_tools.py,0,"b'from __future__ import division\n\nimport os\n\nimport requests\nfrom tqdm import tqdm\nimport numpy as np\nfrom imageio import imread\nfrom skimage import transform\n\nfrom neupy.utils import asfloat\n\n\nCURRENT_DIR = os.path.abspath(os.path.dirname(__file__))\nFILES_DIR = os.path.join(CURRENT_DIR, \'files\')\nIMAGENET_CLASSES_FILE = os.path.join(FILES_DIR, \'imagenet_classes.txt\')\n\n\ndef download_file(url, filepath, description=\'\'):\n    head_response = requests.head(url)\n    filesize = int(head_response.headers[\'content-length\'])\n\n    response = requests.get(url, stream=True)\n    chunk_size = int(1e7)\n\n    n_iter = (filesize // chunk_size) + 1\n\n    print(description)\n    print(\'URL: {}\'.format(url))\n    with open(filepath, ""wb"") as handle:\n        for data in tqdm(response.iter_content(chunk_size), total=n_iter):\n            handle.write(data)\n\n    print(\'Downloaded sucessfully\')\n\n\ndef read_image(image_name, image_size=None, crop_size=None):\n    image = imread(image_name, pilmode=\'RGB\')\n\n    if image_size is not None:\n        height, width, _ = image.shape\n        new_height, new_width = image_size\n\n        if height < width:\n            # Since width is bigger than height, this scaler\n            # factor will say by how much it bigger\n            # New width dimension will be scaled in the way\n            # that output image will have proportional width and\n            # height compae to it\'s original size\n            proportion_scaler = width / height\n            image_size = (new_height, int(new_width * proportion_scaler))\n        else:\n            proportion_scaler = height / width\n            image_size = (int(new_height * proportion_scaler), new_width)\n\n        image = transform.resize(\n            image, image_size,\n            preserve_range=True,\n            mode=\'constant\')\n\n    if crop_size is not None:\n        height, width, _ = image.shape\n        height_slice = slice(\n            (height - crop_size[0]) // 2,\n            (height + crop_size[0]) // 2)\n\n        width_slice = slice(\n            (width - crop_size[1]) // 2,\n            (width + crop_size[1]) // 2)\n\n        image = image[height_slice, width_slice, :]\n\n    # (height, width, channel) -> (1, height, width, channel)\n    image = np.expand_dims(image, axis=0)\n    return asfloat(image)\n\n\ndef process(image, use_bgr):\n    # Per channel normalization\n    image[:, :, :, 0] -= 123.68\n    image[:, :, :, 1] -= 116.78\n    image[:, :, :, 2] -= 103.94\n\n    if use_bgr:\n        # RGB -> BGR\n        image[:, :, :, (0, 1, 2)] = image[:, :, :, (2, 1, 0)]\n\n    return image\n\n\ndef load_image(image_name, image_size=None, crop_size=None, use_bgr=True):\n    image = read_image(image_name, image_size, crop_size)\n    return process(image, use_bgr)\n\n\ndef deprocess(image):\n    image = image.copy()\n\n    # BGR -> RGB\n    image[:, :, (0, 1, 2)] = image[:, :, (2, 1, 0)]\n\n    image[:, :, 0] += 123.68\n    image[:, :, 1] += 116.78\n    image[:, :, 2] += 103.94\n\n    return image.astype(int)\n\n\ndef top_n(probs, n=5):\n    if probs.ndim == 2:\n        probs = probs[0]  # take probabilities for first image\n\n    with open(IMAGENET_CLASSES_FILE, \'r\') as f:\n        class_names = f.read().splitlines()\n        class_names = np.array(class_names)\n\n    max_probs_indices = probs.argsort()[-n:][::-1]\n    class_probs = probs[max_probs_indices]\n    top_classes = class_names[max_probs_indices]\n\n    return top_classes, class_probs\n\n\ndef print_top_n(probs, n=5):\n    top_classes, class_probs = top_n(probs, n)\n\n    print(\'-----------------------\')\n    print(\'Top-{} predicted classes\'.format(n))\n    print(\'-----------------------\')\n\n    for top_class, class_prob in zip(top_classes, class_probs):\n        print(""{:<80s}: {:.2%}"".format(top_class, class_prob))\n\n    print(\'-----------------------\')\n'"
examples/cnn/inception_v3.py,0,"b'from neupy.layers import *\nfrom neupy import layers, plots\n\n\ndef ConvReluBN(*conv_args, **conv_kwargs):\n    return join(\n        Convolution(*conv_args, **conv_kwargs),\n        Relu(),\n        BatchNorm(epsilon=0.001),\n    )\n\n\ndef Inception_1(conv_filters):\n    return join(\n        parallel([\n            ConvReluBN((1, 1, conv_filters[0][0])),\n        ], [\n            ConvReluBN((1, 1, conv_filters[1][0])),\n            ConvReluBN((5, 5, conv_filters[1][1]), padding=2),\n        ], [\n            ConvReluBN((1, 1, conv_filters[2][0])),\n            ConvReluBN((3, 3, conv_filters[2][1]), padding=1),\n            ConvReluBN((3, 3, conv_filters[2][2]), padding=1),\n        ], [\n            AveragePooling((3, 3), stride=(1, 1), padding=\'SAME\'),\n            ConvReluBN((1, 1, conv_filters[3][0])),\n        ]),\n        Concatenate(),\n    )\n\n\ndef Inception_2(conv_filters):\n    return join(\n        parallel([\n            ConvReluBN((1, 1, conv_filters[0][0])),\n        ], [\n            ConvReluBN((1, 1, conv_filters[1][0])),\n            ConvReluBN((1, 7, conv_filters[1][1]), padding=(0, 3)),\n            ConvReluBN((7, 1, conv_filters[1][2]), padding=(3, 0)),\n        ], [\n            ConvReluBN((1, 1, conv_filters[2][0])),\n            ConvReluBN((7, 1, conv_filters[2][1]), padding=(3, 0)),\n            ConvReluBN((1, 7, conv_filters[2][2]), padding=(0, 3)),\n            ConvReluBN((7, 1, conv_filters[2][3]), padding=(3, 0)),\n            ConvReluBN((1, 7, conv_filters[2][4]), padding=(0, 3)),\n        ], [\n            AveragePooling((3, 3), stride=(1, 1), padding=\'SAME\'),\n            ConvReluBN((1, 1, conv_filters[3][0])),\n        ]),\n        Concatenate(),\n    )\n\n\ndef Inception_3(pooling):\n    pooling_layers = {\'max\': MaxPooling, \'avg\': AveragePooling}\n\n    if pooling not in pooling_layers:\n        raise ValueError(""Invalid pooling option: {}"".format(pooling))\n\n    Pooling = pooling_layers[pooling]\n\n    return join(\n        parallel([\n            ConvReluBN((1, 1, 320)),\n        ], [\n            ConvReluBN((1, 1, 384)),\n            parallel(\n                ConvReluBN((1, 3, 384), padding=(0, 1)),\n                ConvReluBN((3, 1, 384), padding=(1, 0)),\n            ),\n        ], [\n            ConvReluBN((1, 1, 448)),\n            ConvReluBN((3, 3, 384), padding=1),\n            parallel(\n                ConvReluBN((1, 3, 384), padding=(0, 1)),\n                ConvReluBN((3, 1, 384), padding=(1, 0)),\n            ),\n        ], [\n            Pooling((3, 3), stride=(1, 1), padding=\'SAME\'),\n            ConvReluBN((1, 1, 192)),\n        ]),\n        Concatenate(),\n    )\n\n\ninception_v3 = join(\n    Input((299, 299, 3)),\n\n    ConvReluBN((3, 3, 32), stride=2),\n    ConvReluBN((3, 3, 32)),\n    ConvReluBN((3, 3, 64), padding=1),\n    MaxPooling((3, 3), stride=(2, 2)),\n\n    ConvReluBN((1, 1, 80)),\n    ConvReluBN((3, 3, 192)),\n    MaxPooling((3, 3), stride=(2, 2)),\n\n    Inception_1([[64], [48, 64], [64, 96, 96], [32]]),\n    Inception_1([[64], [48, 64], [64, 96, 96], [64]]),\n    Inception_1([[64], [48, 64], [64, 96, 96], [64]]),\n\n    parallel([\n        ConvReluBN((3, 3, 384), stride=2),\n    ], [\n        ConvReluBN((1, 1, 64)),\n        ConvReluBN((3, 3, 96), padding=1),\n        ConvReluBN((3, 3, 96), stride=2),\n    ], [\n        MaxPooling((3, 3), stride=(2, 2))\n    ]),\n    Concatenate(),\n\n    Inception_2([[192], [128, 128, 192], [128, 128, 128, 128, 192], [192]]),\n    Inception_2([[192], [160, 160, 192], [160, 160, 160, 160, 192], [192]]),\n    Inception_2([[192], [160, 160, 192], [160, 160, 160, 160, 192], [192]]),\n    Inception_2([[192], [192, 192, 192], [192, 192, 192, 192, 192], [192]]),\n\n    parallel([\n        ConvReluBN((1, 1, 192)),\n        ConvReluBN((3, 3, 320), stride=2),\n    ], [\n        ConvReluBN((1, 1, 192)),\n        ConvReluBN((1, 7, 192), padding=(0, 3)),\n        ConvReluBN((7, 1, 192), padding=(3, 0)),\n        ConvReluBN((3, 3, 192), stride=2),\n    ], [\n        MaxPooling((3, 3), stride=(2, 2))\n    ]),\n    Concatenate(),\n\n    Inception_3(pooling=\'avg\'),\n    Inception_3(pooling=\'max\'),\n\n    GlobalPooling(\'avg\'),\n    Softmax(1000),\n)\ninception_v3.show()\n'"
examples/cnn/load_cifar10.py,0,"b'import os\nimport tarfile\n\nimport numpy as np\nimport six\nfrom six.moves.urllib.request import urlretrieve\nfrom six.moves import cPickle as pickle\n\nfrom imagenet_tools import FILES_DIR\n\n\nCIFAR10_URL = ""https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz""\nCIFAR10_TAR_PATH = os.path.join(FILES_DIR, ""cifar-10-python.tar.gz"")\nCIFAR10_PATH = os.path.join(FILES_DIR, ""cifar-10-batches-py"")\n\nTEST_FILE = \'test_batch\'\nTRAINING_FILES = [\n    \'data_batch_1\', \'data_batch_2\',\n    \'data_batch_3\', \'data_batch_4\',\n    \'data_batch_5\',\n]\n\n\ndef download_cifar10_if_not_found():\n    if os.path.exists(CIFAR10_PATH):\n        print(""CIFAR10 was already downloaded and extracted."")\n        print(""  Output path: {}"".format(CIFAR10_PATH))\n        print("""")\n        return\n\n    print(""Downloading CIFAR 10 dataset..."")\n    print(""  From URL:    {}"".format(CIFAR10_URL))\n    print(""  Output path: {}"".format(CIFAR10_TAR_PATH))\n    urlretrieve(CIFAR10_URL, CIFAR10_TAR_PATH)\n    print(""Downloading finished\\n"")\n\n    print(""Extracting CIFAR 10 dataset..."")\n    with tarfile.open(CIFAR10_TAR_PATH) as tar:\n        tar.extractall(path=FILES_DIR)\n\n    print(""Extracting finished\\n"")\n\n\ndef read_cifar10_file(filename):\n    path = os.path.join(CIFAR10_PATH, filename)\n\n    with open(path, \'rb\') as f:\n        # Specify encoding for python 3 in order to be able to\n        # read files that has been created in python 2\n        options = {\'encoding\': \'latin1\'} if six.PY3 else {}\n        batch = pickle.load(f, **options)\n\n    return batch[\'data\'], batch[\'labels\']\n\n\ndef read_cifar10():\n    download_cifar10_if_not_found()\n\n    print(""Reading CIFAR10 data..."")\n    x_train, y_train = [], []\n\n    for training_filename in TRAINING_FILES:\n        data, labels = read_cifar10_file(training_filename)\n        x_train.append(data)\n        y_train.append(labels)\n\n    x_test, y_test = read_cifar10_file(TEST_FILE)\n\n    x_train = np.concatenate(x_train)\n    y_train = np.concatenate(y_train)\n\n    x_train = x_train.reshape((x_train.shape[0], 3, 32, 32))\n    x_train = np.transpose(x_train, (0, 2, 3, 1))\n\n    x_test = x_test.reshape((x_test.shape[0], 3, 32, 32))\n    x_test = np.transpose(x_test, (0, 2, 3, 1))\n\n    print(""Finished reading CIFAR10 data\\n"")\n    return x_train, x_test, y_train, y_test\n'"
examples/cnn/mnist_cnn.py,0,"b'import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics, datasets\n\nfrom neupy.layers import *\nfrom neupy import algorithms\n\n\ndef load_data():\n    X, y = datasets.fetch_openml(\'mnist_784\', version=1, return_X_y=True)\n    X = X.reshape(-1, 28, 28, 1)\n    X /= 255.\n\n    target_scaler = OneHotEncoder(sparse=False, categories=\'auto\')\n    y = target_scaler.fit_transform(y.reshape(-1, 1))\n\n    return train_test_split(\n        X.astype(np.float32),\n        y.astype(np.float32),\n        test_size=(1 / 7.)\n    )\n\n\noptimizer = algorithms.Momentum(\n    [\n        Input((28, 28, 1)),\n\n        Convolution((3, 3, 32)) >> BatchNorm() >> Relu(),\n        Convolution((3, 3, 48)) >> BatchNorm() >> Relu(),\n        MaxPooling((2, 2)),\n\n        Convolution((3, 3, 64)) >> BatchNorm() >> Relu(),\n        MaxPooling((2, 2)),\n\n        Reshape(),\n        Linear(1024) >> BatchNorm() >> Relu(),\n        Softmax(10),\n    ],\n\n    # Using categorical cross-entropy as a loss function.\n    # It\'s suitable for classification with 3 and more classes.\n    loss=\'categorical_crossentropy\',\n\n    # Mini-batch size. It defines how many samples will be propagated\n    # through the network at once. During the training, weights will\n    # be updated after every mini-batch propagation.\n    # Note: When number of training samples is not divisible by 128\n    # the last mini-batch will have less than 128 samples.\n    batch_size=128,\n\n    # Step == Learning rate\n    # Step decay algorithm minimizes learning step\n    # monotonically after each weight update.\n    step=algorithms.step_decay(\n        initial_value=0.05,\n        # Parameter controls step redution frequency. The higher\n        # the value the slower step parameter decreases.\n        reduction_freq=500,\n    ),\n\n    # Shows information about algorithm and\n    # training progress in terminal\n    verbose=True,\n\n    # Randomly shuffles training dataset before every epoch\n    shuffle_data=True,\n)\nprint(""Preparing data..."")\nx_train, x_test, y_train, y_test = load_data()\n\n# Training network for 4 epochs\nprint(""Training..."")\noptimizer.train(x_train, y_train, x_test, y_test, epochs=4)\n\n# Make prediction on the test dataset\ny_predicted = optimizer.predict(x_test).argmax(axis=1)\ny_test_labels = np.asarray(y_test.argmax(axis=1)).reshape(len(y_test))\n\n# Compare network\'s predictions to the actual label values\n# and build simple classification report.\nprint(metrics.classification_report(y_test_labels, y_predicted))\nscore = metrics.accuracy_score(y_test_labels, y_predicted)\nprint(""Validation accuracy: {:.2%}"".format(score))\n'"
examples/cnn/resnet50.py,0,"b'import os\n\nfrom neupy import storage, architectures\n\nfrom imagenet_tools import (CURRENT_DIR, FILES_DIR, load_image,\n                            print_top_n, download_file)\n\n\nRESNET50_WEIGHTS_FILE = os.path.join(FILES_DIR, \'resnet50.hdf5\')\nDOG_IMAGE_PATH = os.path.join(CURRENT_DIR, \'images\', \'german-shepherd.jpg\')\n\n\ndef download_resnet50_weights():\n    if not os.path.exists(RESNET50_WEIGHTS_FILE):\n        download_file(\n            url=""http://neupy.s3.amazonaws.com/tensorflow/imagenet-models/resnet50.hdf5"",\n            filepath=RESNET50_WEIGHTS_FILE,\n            description=\'Downloading weights\')\n\n    print(""File with ResNet-50 weights: {}"".format(RESNET50_WEIGHTS_FILE))\n    return RESNET50_WEIGHTS_FILE\n\n\nif __name__ == \'__main__\':\n    resnet50_weights_filename = download_resnet50_weights()\n    resnet50 = architectures.resnet50()\n\n    print(""Recovering ResNet-50 parameters..."")\n    storage.load(resnet50, resnet50_weights_filename)\n\n    print(""Making prediction..."")\n    dog_image = load_image(\n        DOG_IMAGE_PATH,\n        image_size=(256, 256),\n        crop_size=(224, 224))\n\n    output = resnet50.predict(dog_image)\n    print_top_n(output, n=5)\n'"
examples/cnn/squeezenet.py,0,"b'import os\n\nfrom neupy import storage, architectures\n\nfrom imagenet_tools import (CURRENT_DIR, FILES_DIR, load_image,\n                            print_top_n, download_file)\n\n\nSQUEEZENET_WEIGHTS_FILE = os.path.join(FILES_DIR, \'squeezenet.hdf5\')\n\n# Networks weight ~4.8 Mb\nsqueezenet = architectures.squeezenet()\n\nif not os.path.exists(SQUEEZENET_WEIGHTS_FILE):\n    download_file(\n        url=""http://neupy.s3.amazonaws.com/tensorflow/imagenet-models/squeezenet.hdf5"",\n        filepath=SQUEEZENET_WEIGHTS_FILE,\n        description=\'Downloading weights\')\n\nstorage.load(squeezenet, SQUEEZENET_WEIGHTS_FILE)\n\nmonkey_image = load_image(\n    os.path.join(CURRENT_DIR, \'images\', \'titi-monkey.jpg\'),\n    image_size=(227, 227),\n    crop_size=(227, 227),\n    use_bgr=True)\n\noutput = squeezenet.predict(monkey_image)\nprint_top_n(output, n=5)\n'"
examples/cnn/vgg16.py,0,"b'import os\n\nfrom neupy import storage, architectures\n\nfrom imagenet_tools import (CURRENT_DIR, FILES_DIR, load_image,\n                            print_top_n, download_file)\n\n\nVGG16_WEIGHTS_FILE = os.path.join(FILES_DIR, \'vgg16.hdf5\')\nvgg16 = architectures.vgg16()\n\nif not os.path.exists(VGG16_WEIGHTS_FILE):\n    download_file(\n        url=""http://neupy.s3.amazonaws.com/tensorflow/imagenet-models/vgg16.hdf5"",\n        filepath=VGG16_WEIGHTS_FILE,\n        description=\'Downloading weights\')\n\nstorage.load(vgg16, VGG16_WEIGHTS_FILE)\n\ndog_image = load_image(\n    os.path.join(CURRENT_DIR, \'images\', \'dog.jpg\'),\n    image_size=(256, 256),\n    crop_size=(224, 224))\n\noutput = vgg16.predict(dog_image)\nprint_top_n(output, n=5)\n'"
examples/cnn/vgg19.py,0,"b'import os\n\nfrom neupy import storage, architectures\n\nfrom imagenet_tools import (CURRENT_DIR, FILES_DIR, load_image,\n                            print_top_n, download_file)\n\n\nVGG19_WEIGHTS_FILE = os.path.join(FILES_DIR, \'vgg19.hdf5\')\nDOG_IMAGE_PATH = os.path.join(CURRENT_DIR, \'images\', \'german-shepherd.jpg\')\nvgg19 = architectures.vgg19()\n\nif not os.path.exists(VGG19_WEIGHTS_FILE):\n    download_file(\n        url=""http://neupy.s3.amazonaws.com/tensorflow/imagenet-models/vgg19.hdf5"",\n        filepath=VGG19_WEIGHTS_FILE,\n        description=\'Downloading weights\')\n\nstorage.load(vgg19, VGG19_WEIGHTS_FILE)\n\ndog_image = load_image(\n    DOG_IMAGE_PATH,\n    image_size=(256, 256),\n    crop_size=(224, 224))\n\noutput = vgg19.predict(dog_image)\nprint_top_n(output, n=5)\n'"
examples/competitive/__init__.py,0,b''
examples/competitive/reduce_iris_sample_size_lvq.py,0,"b""import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom neupy import algorithms, utils\n\n\nutils.reproducible()\n\n\ndef plot_scattermatrix(data, target):\n    df = pd.DataFrame(data)\n    df['target'] = target\n    return sns.pairplot(df, hue='target', diag_kind='hist')\n\n\nif __name__ == '__main__':\n    dataset = datasets.load_iris()\n    data, target = dataset.data, dataset.target\n\n    lvqnet = algorithms.LVQ3(\n        # number of features\n        n_inputs=4,\n\n        # number of data points that we want\n        # to have at the end\n        n_subclasses=30,\n\n        # number of classes\n        n_classes=3,\n\n        verbose=True,\n        show_epoch=20,\n\n        step=0.001,\n        n_updates_to_stepdrop=150 * 100,\n    )\n    lvqnet.train(data, target, epochs=100)\n\n    plot_scattermatrix(data, target)\n    plot_scattermatrix(data=lvqnet.weight, target=lvqnet.subclass_to_class)\n    plt.show()\n"""
examples/competitive/sofm_basic.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom neupy import algorithms, utils\n\n\nutils.reproducible()\nplt.style.use(\'ggplot\')\n\n\nX = np.array([\n    [0.1961, 0.9806],\n    [-0.1961, 0.9806],\n    [0.9806, 0.1961],\n    [0.9806, -0.1961],\n    [-0.5812, -0.8137],\n    [-0.8137, -0.5812],\n])\n\nsofmnet = algorithms.SOFM(\n    n_inputs=2,\n    n_outputs=3,\n\n    step=0.5,\n    show_epoch=20,\n    shuffle_data=True,\n    verbose=True,\n\n    learning_radius=0,\n    features_grid=(3, 1),\n)\n\nplt.plot(X.T[0:1, :], X.T[1:2, :], \'ko\')\nsofmnet.train(X, epochs=100)\n\nprint(""> Start plotting"")\nplt.xlim(-1, 1.2)\nplt.ylim(-1, 1.2)\n\nplt.plot(sofmnet.weight[0:1, :], sofmnet.weight[1:2, :], \'bx\')\nplt.show()\n\nfor data in X:\n    print(sofmnet.predict(np.reshape(data, (2, 1)).T))\n'"
examples/competitive/sofm_compare_grid_types.py,0,"b""import matplotlib.pyplot as plt\nfrom neupy import algorithms, utils\n\nfrom utils import plot_2d_grid, make_circle\n\n\nplt.style.use('ggplot')\nutils.reproducible()\n\n\nif __name__ == '__main__':\n    GRID_WIDTH = 10\n    GRID_HEIGHT = 10\n\n    configurations = [{\n        'grid_type': 'hexagon',\n        'use_hexagon_grid': True,\n        'title': 'Using hexagon grid',\n    }, {\n        'grid_type': 'rect',\n        'use_hexagon_grid': False,\n        'title': 'Using regcangular grid',\n    }]\n\n    data = make_circle()\n\n    red, blue = ('#E24A33', '#348ABD')\n    n_columns = len(configurations)\n\n    plt.figure(figsize=(12, 5))\n\n    for index, conf in enumerate(configurations, start=1):\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            features_grid=(GRID_HEIGHT, GRID_WIDTH),\n\n            verbose=True,\n            shuffle_data=True,\n            grid_type=conf['grid_type'],\n\n            learning_radius=8,\n            reduce_radius_after=5,\n\n            std=2,\n            reduce_std_after=5,\n\n            step=0.3,\n            reduce_step_after=5,\n        )\n        sofm.train(data, epochs=40)\n\n        plt.subplot(1, n_columns, index)\n\n        plt.title(conf['title'])\n        plt.scatter(*data.T, color=blue, alpha=0.05)\n        plt.scatter(*sofm.weight, color=red)\n\n        weights = sofm.weight.reshape((2, GRID_HEIGHT, GRID_WIDTH))\n        plot_2d_grid(weights, color=red, hexagon=conf['use_hexagon_grid'])\n\n    plt.show()\n"""
examples/competitive/sofm_compare_weight_init.py,0,"b'from itertools import product\n\nimport matplotlib.pyplot as plt\nfrom neupy import algorithms, utils, init\n\nfrom utils import plot_2d_grid, make_circle, make_elipse, make_square\n\n\nplt.style.use(\'ggplot\')\nutils.reproducible()\n\n\nif __name__ == \'__main__\':\n    GRID_WIDTH = 4\n    GRID_HEIGHT = 4\n\n    datasets = [\n        make_square(),\n        make_circle(),\n        make_elipse(corr=0.7),\n    ]\n    configurations = [{\n        \'weight_init\': init.Uniform(0, 1),\n        \'title\': \'Random uniform initialization\',\n    }, {\n        \'weight_init\': \'sample_from_data\',\n        \'title\': \'Sampled from the data\',\n    }, {\n        \'weight_init\': \'init_pca\',\n        \'title\': \'Initialize with PCA\',\n    }]\n\n    plt.figure(figsize=(15, 15))\n    plt.title(""Compare weight initialization methods for SOFM"")\n\n    red, blue = (\'#E24A33\', \'#348ABD\')\n    n_columns = len(configurations)\n    n_rows = len(datasets)\n    index = 1\n\n    for data, conf in product(datasets, configurations):\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            features_grid=(GRID_HEIGHT, GRID_WIDTH),\n\n            verbose=True,\n            shuffle_data=True,\n            weight=conf[\'weight_init\'],\n\n            learning_radius=8,\n            reduce_radius_after=5,\n\n            std=2,\n            reduce_std_after=5,\n\n            step=0.3,\n            reduce_step_after=5,\n        )\n\n        if not sofm.initialized:\n            sofm.init_weights(data)\n\n        plt.subplot(n_rows, n_columns, index)\n\n        plt.title(conf[\'title\'])\n        plt.scatter(*data.T, color=blue, alpha=0.05)\n        plt.scatter(*sofm.weight, color=red)\n\n        weights = sofm.weight.reshape((2, GRID_HEIGHT, GRID_WIDTH))\n        plot_2d_grid(weights, color=red)\n\n        index += 1\n\n    plt.show()\n'"
examples/competitive/sofm_digits.py,0,"b'from __future__ import division\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn import datasets\nfrom neupy import algorithms, utils\n\n\nutils.reproducible()\n\nGRID_HEIGHT = 40\nGRID_WIDTH = 40\n\ndigits = datasets.load_digits()\ndata = digits.data\n\nsofm = algorithms.SOFM(\n    n_inputs=64,\n    features_grid=(GRID_HEIGHT, GRID_WIDTH),\n\n    # Learning radius defines area within which we find\n    # winning neuron neighbours. The higher the value\n    # the more values we will be updated after each iteration.\n    learning_radius=5,\n    # Every 20 epochs learning radius will be reduced by 1.\n    reduce_radius_after=20,\n\n    step=0.5,\n    std=1,\n\n    shuffle_data=True,\n    verbose=True,\n)\n\nsofm.train(data, epochs=100)\nclusters = sofm.predict(data).argmax(axis=1)\n\nprint(""Building visualization..."")\nplt.figure(figsize=(12, 11))\n\ngrid = gridspec.GridSpec(GRID_HEIGHT, GRID_WIDTH)\ngrid.update(wspace=0, hspace=0)\n\nfor row_id in range(GRID_HEIGHT):\n    print(""Progress: {:.2%}"".format(row_id / GRID_HEIGHT))\n\n    for col_id in range(GRID_WIDTH):\n        index = row_id * GRID_HEIGHT + col_id\n        clustered_samples = data[clusters == index]\n\n        if len(clustered_samples) > 0:\n            # We take the first sample, but it can be any\n            # sample from this cluster\n            sample = clustered_samples[0]\n\n        else:\n            # If we don\'t have samples in cluster then\n            # it means that there is a gap in space\n            sample = np.zeros(64)\n\n        plt.subplot(grid[index])\n        plt.imshow(sample.reshape((8, 8)), cmap=\'Greys\')\n        plt.axis(\'off\')\n\nprint(""Visualization has been built succesfully"")\nplt.show()\n'"
examples/competitive/sofm_heatmap_visualization.py,0,"b'import argparse\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, preprocessing\nfrom neupy import algorithms, utils\n\nfrom utils import iter_neighbours\n\n\nplt.style.use(\'ggplot\')\nutils.reproducible()\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--expanded-heatmap\', action=\'store_true\')\n\nclass_parameters = [\n    dict(\n        marker=\'o\',\n        markeredgecolor=\'#E24A33\',\n        markersize=11,\n        markeredgewidth=2,\n        markerfacecolor=\'None\',\n    ),\n    dict(\n        marker=\'s\',\n        markeredgecolor=\'#348ABD\',\n        markersize=14,\n        markeredgewidth=2,\n        markerfacecolor=\'None\',\n    ),\n]\n\n\ndef load_data():\n    data, target = datasets.load_breast_cancer(return_X_y=True)\n\n    scaler = preprocessing.MinMaxScaler()\n    data = scaler.fit_transform(data)\n\n    return data, target\n\n\ndef compute_heatmap(weight):\n    heatmap = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n    for (neuron_x, neuron_y), neighbours in iter_neighbours(weight):\n        total_distance = 0\n\n        for (neigbour_x, neigbour_y) in neighbours:\n            neuron_vec = weight[:, neuron_x, neuron_y]\n            neigbour_vec = weight[:, neigbour_x, neigbour_y]\n\n            distance = np.linalg.norm(neuron_vec - neigbour_vec)\n            total_distance += distance\n\n        avg_distance = total_distance / len(neighbours)\n        heatmap[neuron_x, neuron_y] = avg_distance\n\n    return heatmap\n\n\ndef compute_heatmap_expanded(weight):\n    heatmap = np.zeros((2 * GRID_HEIGHT - 1, 2 * GRID_WIDTH - 1))\n    for (neuron_x, neuron_y), neighbours in iter_neighbours(weight):\n        for (neigbour_x, neigbour_y) in neighbours:\n            neuron_vec = weight[:, neuron_x, neuron_y]\n            neigbour_vec = weight[:, neigbour_x, neigbour_y]\n\n            distance = np.linalg.norm(neuron_vec - neigbour_vec)\n\n            if neuron_x == neigbour_x and (neigbour_y - neuron_y) == 1:\n                heatmap[2 * neuron_x, 2 * neuron_y + 1] = distance\n\n            elif (neigbour_x - neuron_x) == 1 and neigbour_y == neuron_y:\n                heatmap[2 * neuron_x + 1, 2 * neuron_y] = distance\n\n    return heatmap\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    GRID_HEIGHT = 20\n    GRID_WIDTH = 20\n\n    sofm = algorithms.SOFM(\n        n_inputs=30,\n        features_grid=(GRID_HEIGHT, GRID_WIDTH),\n\n        learning_radius=4,\n        reduce_radius_after=50,\n\n        step=0.5,\n        std=1,\n\n        shuffle_data=True,\n        verbose=True,\n    )\n\n    data, target = load_data()\n    sofm.train(data, epochs=300)\n    clusters = sofm.predict(data).argmax(axis=1)\n\n    plt.figure(figsize=(13, 13))\n    plt.title(""Embedded 30-dimensional dataset using SOFM"")\n\n    for actual_class, cluster_index in zip(target, clusters):\n        cluster_x, cluster_y = divmod(cluster_index, GRID_HEIGHT)\n        parameters = class_parameters[actual_class]\n\n        if args.expanded_heatmap:\n            plt.plot(2 * cluster_x, 2 * cluster_y, **parameters)\n        else:\n            plt.plot(cluster_x, cluster_y, **parameters)\n\n    weight = sofm.weight.reshape((sofm.n_inputs, GRID_HEIGHT, GRID_WIDTH))\n\n    if args.expanded_heatmap:\n        heatmap = compute_heatmap_expanded(weight)\n    else:\n        heatmap = compute_heatmap(weight)\n\n    plt.imshow(heatmap, cmap=\'Greys_r\', interpolation=\'nearest\')\n\n    plt.axis(\'off\')\n    plt.colorbar()\n    plt.show()\n'"
examples/competitive/sofm_iris_clustering.py,0,"b""import numpy as np\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom neupy import algorithms, utils\n\n\nplt.style.use('ggplot')\nutils.reproducible()\n\n\nif __name__ == '__main__':\n    ggplot_colors = plt.rcParams['axes.prop_cycle']\n    colors = np.array([c['color'] for c in ggplot_colors])\n\n    dataset = datasets.load_iris()\n    # use only two features in order\n    # to make visualization simpler\n    data = dataset.data[:, [2, 3]]\n    target = dataset.target\n\n    sofm = algorithms.SOFM(\n        # Use only two features for the input\n        n_inputs=2,\n\n        # Number of outputs defines number of features\n        # in the SOFM or in terms of clustering - number\n        # of clusters\n        n_outputs=20,\n\n        # In clustering application we will prefer that\n        # clusters will be updated independently from each\n        # other. For this reason we set up learning radius\n        # equal to zero\n        learning_radius=0,\n\n        # Training step size or learning rate\n        step=0.25,\n\n        # Shuffles dataset before every training epoch.\n        shuffle_data=True,\n\n        # Instead of generating random weights\n        # (features / cluster centers) SOFM will sample\n        # them from the data. Which means that after\n        # initialization step 3 random data samples will\n        # become cluster centers\n        weight='sample_from_data',\n\n        # Shows training progress in terminal\n        verbose=True,\n    )\n    sofm.train(data, epochs=200)\n\n    plt.title('Clustering iris dataset with SOFM')\n    plt.xlabel('Feature #3')\n    plt.ylabel('Feature #4')\n\n    plt.scatter(*data.T, c=colors[target], s=100, alpha=1)\n    cluster_centers = plt.scatter(*sofm.weight, s=300, c=colors[3])\n\n    plt.legend([cluster_centers], ['Cluster center'], loc='upper left')\n    plt.show()\n"""
examples/competitive/sofm_moon_topology.py,0,"b""import matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom neupy import algorithms, utils\n\nfrom utils import plot_2d_grid\n\n\nplt.style.use('ggplot')\nutils.reproducible()\n\n\nif __name__ == '__main__':\n    GRID_WIDTH = 20\n    GRID_HEIGHT = 1\n\n    data, targets = datasets.make_moons(n_samples=400, noise=0.1)\n    data = data[targets == 1]\n\n    sofm = algorithms.SOFM(\n        n_inputs=2,\n        features_grid=(GRID_HEIGHT, GRID_WIDTH),\n\n        verbose=True,\n        shuffle_data=True,\n\n        # The winning neuron will be selected based on the\n        # Euclidean distance. For this task it's important\n        # that distance is Euclidean. Other distances will\n        # not give us the same results.\n        distance='euclid',\n\n        learning_radius=2,\n        # Reduce learning radius by 1 after every 20 epochs.\n        # Learning radius will be equal to 2 during first\n        # 20 epochs and on the 21st epoch it will be equal to 1.\n        reduce_radius_after=20,\n\n        # 2 Means that neighbour neurons will have high learning\n        # rates during the first iterations\n        std=2,\n        # Defines a rate at which parameter `std` will be reduced.\n        # Reduction is monotonic and reduces after each epoch.\n        # In 50 epochs std = 2 / 2 = 1 and after 100 epochs\n        # std = 2 / 3 and so on.\n        reduce_std_after=50,\n\n        # Step (or learning rate)\n        step=0.3,\n        # Defines a rate at which parameter `step` will reduced.\n        # Reduction is monotonic and reduces after each epoch.\n        # In 50 epochs step = 0.3 / 2 = 0.15 and after 100 epochs\n        # std = 0.3 / 3 = 0.1 and so on.\n        reduce_step_after=50,\n    )\n    sofm.train(data, epochs=20)\n\n    red, blue = ('#E24A33', '#348ABD')\n\n    plt.scatter(*data.T, color=blue)\n    plt.scatter(*sofm.weight, color=red)\n\n    weights = sofm.weight.reshape((2, GRID_HEIGHT, GRID_WIDTH))\n    plot_2d_grid(weights, color=red)\n\n    plt.show()\n"""
examples/competitive/utils.py,0,"b'from itertools import product\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef iter_neighbours(weights, hexagon=False):\n    _, grid_height, grid_width = weights.shape\n\n    hexagon_even_actions = ((-1, 0), (0, -1), (1, 0), (0, 1), (1, 1), (-1, 1))\n    hexagon_odd_actions = ((-1, 0), (0, -1), (1, 0), (0, 1), (-1, -1), (1, -1))\n    rectangle_actions = ((-1, 0), (0, -1), (1, 0), (0, 1))\n\n    for neuron_x, neuron_y in product(range(grid_height), range(grid_width)):\n        neighbours = []\n\n        if hexagon and neuron_x % 2 == 1:\n            actions = hexagon_even_actions\n        elif hexagon:\n            actions = hexagon_odd_actions\n        else:\n            actions = rectangle_actions\n\n        for shift_x, shift_y in actions:\n            neigbour_x = neuron_x + shift_x\n            neigbour_y = neuron_y + shift_y\n\n            if 0 <= neigbour_x < grid_height and 0 <= neigbour_y < grid_width:\n                neighbours.append((neigbour_x, neigbour_y))\n\n        yield (neuron_x, neuron_y), neighbours\n\n\ndef plot_2d_grid(weights, ax=None, color=\'green\', hexagon=False):\n    if weights.ndim != 3:\n        raise ValueError(""Number of dimensions should be equal to 3 ""\n                         ""(shape: (2, height, width)), got {} instead""\n                         """".format(weights.ndim))\n\n    n_features = weights.shape[0]\n\n    if n_features != 2:\n        raise ValueError(""First dimension should be equal to 2"")\n\n    if ax is None:\n        ax = plt.gca()\n\n    for (neuron_x, neuron_y), neighbours in iter_neighbours(weights, hexagon):\n        for (neigbour_x, neigbour_y) in neighbours:\n            neurons_x_coords = (neuron_x, neigbour_x)\n            neurons_y_coords = (neuron_y, neigbour_y)\n\n            neurons = weights[:, neurons_x_coords, neurons_y_coords]\n            ax.plot(*neurons, color=color)\n\n\ndef make_square():\n    return np.random.random((10000, 2))\n\n\ndef make_circle():\n    data = make_square()\n    x, y = data[:, 0], data[:, 1]\n\n    distance_from_center = ((x - 0.5) ** 2 + (y - 0.5) ** 2)\n    return data[distance_from_center <= 0.5 ** 2]\n\n\ndef make_elipse(corr=0.8):\n    projection = np.array([\n        [corr, 1 - corr],\n        [1 - corr, corr]])\n\n    data = make_circle()\n    return data.dot(projection)\n'"
examples/memory/cmac_basic.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom neupy import algorithms, utils\n\n\nutils.reproducible()\nplt.style.use('ggplot')\n\nX_train = np.reshape(np.linspace(0, 2 * np.pi, 100), (100, 1))\nX_test = np.reshape(np.sort(2 * np.pi * np.random.random(50)), (50, 1))\n\ny_train = np.sin(X_train)\ny_test = np.sin(X_test)\n\ncmac = algorithms.CMAC(\n    quantization=100,\n    associative_unit_size=10,\n    step=0.2,\n    verbose=True,\n    show_epoch=100,\n)\ncmac.train(X_train, y_train, epochs=100)\npredicted_test = cmac.predict(X_test)\n\nplt.plot(X_train, y_train)\nplt.plot(X_test, predicted_test)\nplt.show()\n"""
examples/memory/dhn_energy_func.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\ndef energy(input_vector):\n    input_vector = np.array(input_vector)\n    X = np.array([[1, -1], [-1, 1]])\n    weight = X.T.dot(X) - 2 * np.eye(2)\n    return -0.5 * input_vector.dot(weight).dot(input_vector)\n\n\nfig = plt.figure(figsize=(9, 9))\nax = fig.add_subplot(111, projection='3d')\n\nx = y = np.arange(-1.0, 1.0, 0.01)\nX, Y = np.meshgrid(x, y)\nenergies = map(energy, zip(np.ravel(X), np.ravel(Y)))\nzs = np.array(list(energies))\nZ = zs.reshape(X.shape)\n\nax.view_init(elev=6, azim=-72)\nax.plot_surface(X, Y, Z, cmap='Reds')\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel('Energy')\n\nplt.show()\n"""
examples/memory/password_recovery.py,0,"b'from __future__ import division, print_function\n\nimport random\nimport pprint\nimport string\nfrom collections import OrderedDict\nfrom operator import itemgetter\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom neupy import algorithms\n\n\ndef str2bin(text, max_length=30):\n    if len(text) > max_length:\n        raise ValueError(""Text can\'t contains more ""\n                         ""than {} symbols"".format(max_length))\n\n    text = text.rjust(max_length)\n\n    bits_list = []\n    for symbol in text:\n        bits = bin(ord(symbol))\n        # Cut `0b` from the beggining and fill with zeros if they\n        # are missed\n        bits = bits[2:].zfill(8)\n        bits_list.extend(map(int, bits))\n\n    return list(bits_list)\n\n\ndef chunker(sequence, size):\n    for position in range(0, len(sequence), size):\n        yield sequence[position:position + size]\n\n\ndef bin2str(array):\n    characters = []\n    for binary_symbol_code in chunker(array, size=8):\n        binary_symbol_str = \'\'.join(map(str, binary_symbol_code))\n        character = chr(int(binary_symbol_str, base=2))\n        characters.append(character)\n    return \'\'.join(characters).lstrip()\n\n\ndef generate_password(min_length=5, max_length=30):\n    symbols = list(\n        string.ascii_letters +\n        string.digits +\n        string.punctuation\n    )\n    password_len = random.randrange(min_length, max_length + 1)\n    password = [np.random.choice(symbols) for _ in range(password_len)]\n    return \'\'.join(password)\n\n\ndef save_password(real_password, noise_level=5):\n    if noise_level < 1:\n        raise ValueError(""`noise_level` must be equal or greater than one"")\n\n    binary_password = str2bin(real_password)\n    bin_password_len = len(binary_password)\n\n    data = [binary_password]\n\n    for _ in range(noise_level):\n        # The farther from the 0.5 value the less likely\n        # password recovery\n        noise = np.random.binomial(1, 0.55, bin_password_len)\n        data.append(noise)\n\n    dhnet = algorithms.DiscreteHopfieldNetwork(mode=\'sync\')\n    dhnet.train(np.array(data))\n\n    return dhnet\n\n\ndef recover_password(dhnet, broken_password):\n    test = np.array(str2bin(broken_password))\n    recovered_password = dhnet.predict(test)\n\n    if recovered_password.ndim == 2:\n        recovered_password = recovered_password[0, :]\n\n    return bin2str(recovered_password)\n\n\ndef cutword(word, k, fromleft=False):\n    if fromleft:\n        return (word[-k:] if k != 0 else \'\').rjust(len(word))\n    return (word[:k] if k != 0 else \'\').ljust(len(word))\n\n\ndef cripple_password(word, k):\n    crippled_password = random.sample(list(enumerate(word)), k=k)\n    word_letters_list = map(itemgetter(1), sorted(crippled_password))\n    return \'\'.join(word_letters_list)\n\n\ndef loss_of_chars(word, k):\n    word_with_missed_chars = cripple_password(word, k)\n    broken_word = []\n\n    for symbol in word:\n        if word_with_missed_chars.startswith(symbol):\n            word_with_missed_chars = word_with_missed_chars[1:]\n            broken_word.append(symbol)\n        else:\n            broken_word.append(\' \')\n\n    return \'\'.join(broken_word)\n\n\nif __name__ == \'__main__\':\n    n_times = 10000\n    cases = OrderedDict([\n        (\'exclude-one\', (lambda x: x - 1)),\n        (\'exclude-quarter\', (lambda x: 3 * x // 4)),\n        (\'exclude-half\', (lambda x: x // 2)),\n        (\'just-one-symbol\', (lambda x: 1)),\n        (\'empty-string\', (lambda x: 0)),\n    ])\n    results = OrderedDict.fromkeys(cases.keys(), 0)\n\n    for _ in tqdm(range(n_times)):\n        real_password = generate_password(min_length=25, max_length=25)\n\n        for casename, func in cases.items():\n            n_letters = func(len(real_password))\n            broken_password = cutword(real_password, k=n_letters,\n                                      fromleft=True)\n\n            dhnet = save_password(real_password, noise_level=11)\n            recovered_password = recover_password(dhnet, broken_password)\n\n            if recovered_password != real_password:\n                results[casename] += 1\n\n    print(""Number of fails for each test case:"")\n    pprint.pprint(results)\n'"
examples/mlp/boston_price_prediction.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom neupy import algorithms, layers\n\n\nplt.style.use(\'ggplot\')\n\n\ndef rmsle(expected, predicted):\n    log_expected = np.log1p(expected + 1)\n    log_predicted = np.log1p(predicted + 1)\n    squared_log_error = np.square(log_expected - log_predicted)\n    return np.sqrt(np.mean(squared_log_error))\n\n\ndef load_data():\n    dataset = datasets.load_boston()\n    data = dataset.data\n    target = dataset.target.reshape((-1, 1))\n\n    data_scaler = preprocessing.MinMaxScaler((-3, 3))\n    target_scaler = preprocessing.MinMaxScaler()\n\n    data = data_scaler.fit_transform(data)\n    target = target_scaler.fit_transform(target)\n\n    return target_scaler, train_test_split(data, target, test_size=0.15)\n\n\nif __name__ == \'__main__\':\n    optimizer = algorithms.Hessian(\n        network=[\n            layers.Input(13),\n            layers.Sigmoid(50),\n            layers.Sigmoid(10),\n            layers.Sigmoid(1),\n        ],\n        verbose=True,\n    )\n\n    target_scaler, (x_train, x_test, y_train, y_test) = load_data()\n    optimizer.train(x_train, y_train, x_test, y_test, epochs=10)\n    y_predict = optimizer.predict(x_test)\n\n    y_test = target_scaler.inverse_transform(y_test.reshape((-1, 1)))\n    y_predict = target_scaler.inverse_transform(y_predict).T.round(1)\n    error = rmsle(y_predict, y_test)\n    print(""RMSLE = {}"".format(error))\n'"
examples/mlp/gd_algorithms_visualization.py,0,"b'from functools import partial\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\nfrom neupy.utils import asfloat, tensorflow_session\nfrom neupy import algorithms, layers, utils\n\n\nutils.reproducible()\n\nX_train = np.array([\n    [0.9, 0.3],\n    [0.5, 0.3],\n    [0.2, 0.1],\n    [0.7, 0.5],\n    [0.1, 0.8],\n    [0.1, 0.9],\n])\ny_train = np.array([\n    [1],\n    [1],\n    [1],\n    [0],\n    [0],\n    [0],\n])\ndefault_weight = np.array([[-4.], [-4.]])\n\nweights = None\ncurrent_epoch = 0\n\n\ndef draw_countour(xgrid, ygrid, target_function):\n    output = np.zeros((xgrid.shape[0], ygrid.shape[0]))\n\n    for i, x in enumerate(xgrid):\n        for j, y in enumerate(ygrid):\n            output[j, i] = target_function(x, y)\n\n    X, Y = np.meshgrid(xgrid, ygrid)\n\n    plt.contourf(X, Y, output, 20, alpha=1, cmap=\'Blues\')\n    plt.colorbar()\n\n\ndef weight_quiver(weights, color=\'c\'):\n    plt.quiver(weights[0, :-1],\n               weights[1, :-1],\n               weights[0, 1:] - weights[0, :-1],\n               weights[1, 1:] - weights[1, :-1],\n               scale_units=\'xy\', angles=\'xy\', scale=1,\n               color=color)\n\n\ndef save_epoch_weight(optimizer):\n    """"""\n    Signal processor which save weight update for every\n    epoch.\n    """"""\n    global weights\n    global current_epoch\n\n    session = tensorflow_session()\n    input_layer_weight = session.run(optimizer.network.layers[1].weight)\n    weights[:, current_epoch + 1:current_epoch + 2] = input_layer_weight\n\n\ndef create_network():\n    """"""\n    Generate new network every time when we call it.\n    """"""\n    return layers.join(\n        layers.Input(2),\n        layers.Sigmoid(1, weight=default_weight.copy(), bias=None)\n    )\n\n\ndef draw_quiver(network_class, name, color=\'r\'):\n    """"""\n    Train algorithm and draw quiver for every epoch update\n    for this algorithm.\n    """"""\n    global weights\n    global current_epoch\n\n    bpn = network_class(create_network(), signals=save_epoch_weight)\n\n    # We don\'t know in advance number of epochs that network\n    # need to reach the goal. For this reason we use 1000 as\n    # an upper limit for all network epochs, later we\n    # need to fix\n    weights = np.zeros((2, 1000))\n    weights[:, 0:1] = default_weight.copy()\n\n    current_epoch = 0\n    while bpn.score(X_train, y_train) > 0.125:\n        bpn.train(X_train, y_train, epochs=1)\n        current_epoch += 1\n\n    weights = weights[:, :current_epoch + 1]\n    weight_quiver(weights, color=color)\n\n    label = ""{name} ({n} steps)"".format(name=name, n=current_epoch)\n    return mpatches.Patch(color=color, label=label)\n\n\ndef target_function(optimizer, x, y):\n    weight = optimizer.network.layers[1].weight\n    new_weight = np.array([[x], [y]])\n\n    session = tensorflow_session()\n    weight.load(asfloat(new_weight), session)\n\n    return optimizer.score(X_train, y_train)\n\n\n# Get data for countour plot\nbp_network = algorithms.GradientDescent(\n    create_network(),\n    step=0.3,\n    batch_size=None,\n    signals=save_epoch_weight\n)\nnetwork_target_function = partial(target_function, bp_network)\n\nplt.figure()\nplt.title(""Approximation function contour plot"")\nplt.xlabel(""First weight"")\nplt.ylabel(""Second weight"")\n\ndraw_countour(\n    np.linspace(-5, 5, 50),\n    np.linspace(-5, 5, 50),\n    network_target_function\n)\n\nalgorithms = (\n    (partial(algorithms.GradientDescent, step=0.3), \'Gradient Descent\', \'k\'),\n    (partial(algorithms.Momentum, batch_size=None, step=0.3), \'Momentum\', \'g\'),\n    (partial(algorithms.RPROP, step=0.3), \'RPROP\', \'m\'),\n    (partial(algorithms.IRPROPPlus, step=0.3), \'iRPROP+\', \'r\'),\n    (partial(algorithms.Hessian, penalty_const=0.01), ""Newton\'s method"", \'y\'),\n)\n\npatches = []\nfor algorithm, algorithm_name, color in algorithms:\n    print(""The \'{}\' network training"".format(algorithm_name))\n    quiver_patch = draw_quiver(algorithm, algorithm_name, color)\n    patches.append(quiver_patch)\n\nprint(""Plot training results"")\nplt.legend(handles=patches)\nplt.show()\n'"
examples/mlp/mix_categorical_numerical_inputs.py,0,"b'import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\nfrom neupy import layers, algorithms, utils\n\n\nutils.reproducible()\n\n\ndef make_dataset():\n    data, target = make_classification(n_samples=10000, n_features=20)\n\n    # number of categorical columns\n    n_categorical = 3\n\n    # if value x in [0, 0.2) then category 0\n    # if value x in [0.2, 0.5) then category 1\n    # if value x in [0.5, 0.8) then category 2\n    # if value x in [0.8, 0.1] then category 3\n    bins = np.array([0, 0.2, 0.5, 0.8, 1])\n\n    # Create categorical features\n    for i in range(n_categorical):\n        data[:, i] = np.digitize(data[:, i], bins=bins)\n\n    return train_test_split(data, target, test_size=0.1)\n\n\ndef only_numerical(data):\n    return data[:, 3:]\n\n\nclass ConvertCategorical(object):\n    def fit_transform(self, data):\n        n_categories = np.max(data, axis=0) + 1\n        self.index_shifts = np.cumsum(n_categories) - n_categories[0]\n        return self.transform(data)\n\n    def transform(self, data):\n        return data + self.index_shifts\n\n\nconvert_categorical = ConvertCategorical()\nx_train, x_test, y_train, y_test = make_dataset()\n\nx_train_cat = convert_categorical.fit_transform(x_train[:, :3])\nx_train_num = only_numerical(x_train)\n\n# We use .max(), because each category has a unique identifier.\n# Maximum value defines last category ID. Since first category has\n# 0 identifier, we need to +1 to obtain total result\nn_unique_categories = int(x_train_cat.max() + 1)\n\nx_test_cat = convert_categorical.transform(x_test[:, :3])\nx_test_num = only_numerical(x_test)\n\nnetwork = layers.join(\n    layers.parallel([\n        # 3 categorical inputs\n        layers.Input(3),\n\n        # Train embedding matrix for categorical inputs.\n        # It has 18 different unique categories (6 categories\n        # per each of the 3 columns). Next layer projects each\n        # category into 4 dimensional space. Output shape from\n        # the layer should be: (batch_size, 3, 4)\n        layers.Embedding(n_unique_categories, 4),\n\n        # Reshape (batch_size, 3, 4) to (batch_size, 12)\n        layers.Reshape(),\n    ], [\n        # 17 numerical inputs\n        layers.Input(17),\n    ]),\n\n    # Concatenate (batch_size, 12) and (batch_size, 17)\n    # into one matrix with shape (batch_size, 29)\n    layers.Concatenate(),\n\n    layers.Relu(128),\n    layers.Relu(32) >> layers.Dropout(0.5),\n\n    layers.Sigmoid(1),\n)\n\noptimizer = algorithms.Momentum(\n    network,\n\n    step=0.05,\n    verbose=True,\n    loss=\'binary_crossentropy\',\n\n    momentum=0.9,\n    nesterov=True,\n\n    # Apply L2 (Weight Decay) regularziation in\n    # order to prevent overfitting\n    regularizer=algorithms.l2(0.01),\n)\n\n# Categorical input should be first, because input layer\n# for categorical matrices was defined first.\noptimizer.train([x_train_cat, x_train_num], y_train,\n                [x_test_cat, x_test_num], y_test,\n                epochs=50)\n\ny_predicted = optimizer.predict(x_test_cat, x_test_num)\naccuracy = accuracy_score(y_test, y_predicted.round())\nprint(""Accuracy: {:.2%}"".format(accuracy))\n'"
examples/mlp/mnist_mlp.py,0,"b'import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import model_selection, metrics, datasets\nfrom neupy import algorithms, layers\n\n\ndef load_data():\n    X, y = datasets.fetch_openml(\'mnist_784\', version=1, return_X_y=True)\n    X /= 255.\n    X -= X.mean(axis=0)\n\n    target_scaler = OneHotEncoder(sparse=False, categories=\'auto\')\n    y = target_scaler.fit_transform(y.reshape(-1, 1))\n\n    return model_selection.train_test_split(\n        X.astype(np.float32),\n        y.astype(np.float32),\n        test_size=(1 / 7.))\n\n\noptimizer = algorithms.Momentum(\n    [\n        layers.Input(784),\n        layers.Relu(500),\n        layers.Relu(300),\n        layers.Softmax(10),\n    ],\n\n    # Using categorical cross-entropy as a loss function.\n    # It\'s suitable for classification with 3 and more classes.\n    loss=\'categorical_crossentropy\',\n\n    # Learning rate\n    step=0.01,\n\n    # Shows information about algorithm and\n    # training progress in terminal\n    verbose=True,\n\n    # Randomly shuffles training dataset before every epoch\n    shuffle_data=True,\n\n    momentum=0.99,\n    # Activates Nesterov momentum\n    nesterov=True,\n)\n\nprint(""Preparing data..."")\nx_train, x_test, y_train, y_test = load_data()\n\nprint(""Training..."")\noptimizer.train(x_train, y_train, x_test, y_test, epochs=20)\n\ny_predicted = optimizer.predict(x_test).argmax(axis=1)\ny_test = np.asarray(y_test.argmax(axis=1)).reshape(len(y_test))\n\nprint(metrics.classification_report(y_test, y_predicted))\nscore = metrics.accuracy_score(y_test, y_predicted)\nprint(""Validation accuracy: {:.2%}"".format(score))\n'"
examples/rbfn/grnn_params_selection.py,0,"b'import heapq\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom neupy import algorithms\n\n\ndef rmsle(expected, predicted):\n    log_expected = np.log1p(expected + 1)\n    log_predicted = np.log1p(predicted + 1)\n    squared_log_error = np.square(log_expected - log_predicted)\n    return np.sqrt(np.mean(squared_log_error))\n\n\ndef scorer(network, X, y):\n    result = network.predict(X)\n    return rmsle(result, y)\n\n\ndef report(results, n_top=3):\n    ranks = heapq.nlargest(n_top, results[\'rank_test_score\'])\n\n    for i in ranks:\n        candidates = np.flatnonzero(results[\'rank_test_score\'] == i)\n        for candidate in candidates:\n            print(""Mean validation score: {0:.3f} (std: {1:.3f})"".format(\n                  results[\'mean_test_score\'][candidate],\n                  results[\'std_test_score\'][candidate]))\n            print(""Parameters: {0}"".format(results[\'params\'][candidate]))\n            print("""")\n\n\nprint(""Run Random Search CV"")\ndataset = datasets.load_diabetes()\nrandom_search = RandomizedSearchCV(\n    algorithms.GRNN(std=0.1, verbose=False),\n    param_distributions={\'std\': np.arange(1e-2, 1, 1e-3)},\n    n_iter=100,\n    cv=3,\n    scoring=scorer,\n)\nrandom_search.fit(dataset.data, dataset.target)\nreport(random_search.cv_results_)\n'"
examples/rbfn/pnn_iris.py,0,"b'import numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom neupy.algorithms import PNN\n\n\ndataset = datasets.load_iris()\ndata, target = dataset.data, dataset.target\n\nprint(""> Start classify iris dataset"")\nskfold = StratifiedKFold(n_splits=10)\n\nfor i, (train, test) in enumerate(skfold.split(data, target), start=1):\n    x_train, x_test = data[train], data[test]\n    y_train, y_test = target[train], target[test]\n\n    pnn_network = PNN(std=0.1, verbose=False)\n    pnn_network.train(x_train, y_train)\n    result = pnn_network.predict(x_test)\n\n    n_predicted_correctly = np.sum(result == y_test)\n    n_test_samples = test.size\n\n    print(""Test #{:<2}: Guessed {} out of {}"".format(\n        i, n_predicted_correctly, n_test_samples))\n'"
examples/reinforcement_learning/rl_cartpole.py,0,"b'import os\nimport random\nimport argparse\nfrom collections import deque\n\nimport gym\nimport numpy as np\nfrom neupy import layers, algorithms, utils, storage\n\n\nutils.reproducible()\n\nCURRENT_DIR = os.path.abspath(os.path.dirname(__file__))\nFILES_DIR = os.path.join(CURRENT_DIR, \'files\')\nCARTPOLE_WEIGHTS = os.path.join(FILES_DIR, \'cartpole-weights.pickle\')\n\n\ndef training_samples(network, memory, gamma=0.9):\n    data = np.array(memory, dtype=[\n        (\'state\', np.ndarray),\n        (\'action\', np.int),\n        (\'reward\', np.int),\n        (\'done\', np.bool),\n        (\'new_state\', np.ndarray),\n    ])\n\n    state = np.array(data[\'state\'].tolist())\n    new_state = np.array(data[\'new_state\'].tolist())\n\n    # Note: Calculating Q for all states at once is much faster\n    # that do it per each sample separately\n    Q = network.predict(state)\n    new_Q = network.predict(new_state)\n    max_Q = np.max(new_Q, axis=1)\n\n    n_samples = len(memory)\n    row_index = np.arange(n_samples)\n    column_index = data[\'action\']\n\n    Q[(row_index, column_index)] = np.where(\n        data[\'done\'],\n        data[\'reward\'],\n        data[\'reward\'] + gamma * max_Q\n    )\n\n    return state, Q\n\n\ndef play_game(env, network, n_steps=1000):\n    state = env.reset()\n\n    for _ in range(n_steps):\n        env.render()\n\n        q = network.predict(state)\n        action = int(np.argmax(q[0]))\n\n        state, _, done, _ = env.step(action)\n\n        if done:\n            break\n\n\ndef train_network(env, network, memory, n_games=200, max_score=200,\n                  epsilon=0.1, gamma=0.9):\n    for episode in range(1, n_games + 1):\n        state = env.reset()\n\n        for t in range(max_score):\n            if random.random() <= epsilon:\n                # Select random action with probability\n                # equal to the `epsilon`\n                action = random.randint(0, 1)\n            else:\n                # Use action selected by the network\n                q = network.predict(state)\n                action = int(np.argmax(q[0]))\n\n            new_state, reward, done, info = env.step(action)\n            memory.append((state, action, reward, done, new_state))\n\n            if done:\n                # We done when network lost the game.\n                # Low reward will penalyze network.\n                reward = -10\n\n            if len(memory) == memory_size:\n                # Train only when we collected enough samples\n                x_train, y_train = training_samples(network, memory, gamma)\n                network.train(x_train, y_train, epochs=1)\n                loss = network.errors.train[-1]\n\n            state = new_state\n\n            if done:\n                break\n\n        if len(memory) == memory_size:\n            print(""Game #{:<3} | Lost after {:<3} iterations | loss: {:.4}""\n                  """".format(episode, t + 1, loss))\n        else:\n            print(""Game #{:<3} | Lost after {:<3} iterations""\n                  """".format(episode, t + 1))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'Trains network to play CartPole game\',\n    )\n    parser.add_argument(\n        \'-p\', \'--pretrained\', dest=\'use_pretrained\', action=\'store_true\',\n        help=\'load pretrained network from file and play without training\',\n    )\n    args = parser.parse_args()\n    network = algorithms.RMSProp(\n        [\n            layers.Input(4),\n\n            layers.Relu(64),\n            layers.Relu(48),\n            layers.Relu(32),\n            layers.Relu(64) >> layers.Dropout(0.2),\n\n            # Expecting two different actions:\n            # 1. Move left\n            # 2. Move right\n            layers.Linear(2),\n        ],\n\n        step=0.0005,\n        loss=\'rmse\',\n        batch_size=None,\n        verbose=False\n    )\n\n    env = gym.make(\'CartPole-v0\')\n    env.seed(0)  # To make results reproducible for the gym\n\n    memory_size = 1000  # Number of samples stored in the memory\n    memory = deque(maxlen=memory_size)\n\n    if args.use_pretrained:\n        if not os.path.exists(CARTPOLE_WEIGHTS):\n            raise OSError(\n                ""Cannot find file with pretrained weights ""\n                ""(File name: {})"".format(CARTPOLE_WEIGHTS))\n\n        print(""Loading pretrained weights"")\n        storage.load(network, CARTPOLE_WEIGHTS)\n\n    else:\n        print(""Start training"")\n        train_network(\n            env, network, memory,\n            n_games=150,  # Number of games that networks is going to play,\n            max_score=200,  # Maximum score that network can achive in the game\n            epsilon=0.2,  # Probability to select random action during the game\n            gamma=0.99,\n        )\n\n        if not os.path.exists(FILES_DIR):\n            os.mkdir(FILES_DIR)\n\n        print(""Saving parameters"")\n        storage.save(network, CARTPOLE_WEIGHTS)\n\n    # After the training we can check how network solves the problem\n    print(""Start playing game"")\n    play_game(env, network, n_steps=100000)\n'"
examples/rnn/reber_gru.py,0,"b'import numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom neupy import algorithms, layers\nfrom neupy.datasets import reber\n\n\ndef add_padding(data):\n    n_sampels = len(data)\n    max_seq_length = max(map(len, data))\n\n    data_matrix = np.zeros((n_sampels, max_seq_length))\n    for i, sample in enumerate(data):\n        data_matrix[i, -len(sample):] = sample\n\n    return data_matrix\n\n\n# An example of possible values for the `data` and `labels`\n# variables\n#\n# >>> data\n# array([array([1, 3, 1, 4]),\n#        array([0, 3, 0, 3, 0, 4, 3, 0, 4, 4]),\n#        array([0, 3, 0, 0, 3, 0, 4, 2, 4, 1, 0, 4, 0])], dtype=object)\n# >>>\n# >>> labels\n# array([1, 0, 0])\ndata, labels = reber.make_reber_classification(\n    n_samples=10000, return_indices=True)\n\n# Shift all indices by 1. In the next row we will add zero\n# paddings, so we need to make sure that we will not confuse\n# paddings with zero indices.\ndata = data + 1\n\n# Add paddings at the beggining of each vector to make sure\n# that all samples has the same length. This trick allows to\n# train network with multiple independent samples.\ndata = add_padding(data)\n\nx_train, x_test, y_train, y_test = train_test_split(\n    data, labels, test_size=0.2)\n\nn_categories = len(reber.avaliable_letters) + 1  # +1 for zero paddings\nn_time_steps = x_train.shape[1]\n\noptimizer = algorithms.RMSProp(\n    [\n        layers.Input(n_time_steps),\n        # shape: (n_samples, n_time_steps)\n\n        layers.Embedding(n_categories, 10),\n        # shape: (n_samples, n_time_steps, 10)\n\n        # unroll_scan - speed up calculation for short sequences\n        layers.GRU(20, unroll_scan=True),\n        # shape: (n_samples, 20)\n\n        layers.Sigmoid(1),\n        # shape: (n_samples, 1)\n    ],\n    step=0.01,\n    verbose=True,\n    batch_size=64,\n    loss=\'binary_crossentropy\',\n)\noptimizer.train(x_train, y_train, x_test, y_test, epochs=20)\n\ny_predicted = optimizer.predict(x_test).round()\naccuracy = (y_predicted.T == y_test).mean()\nprint(""Test accuracy: {:.2%}"".format(accuracy))\n'"
examples/rnn/shakespeare_lstm.py,0,"b'import os\n\nimport numpy as np\nfrom tqdm import trange\nfrom sklearn.model_selection import train_test_split\nfrom neupy import layers, algorithms\n\n\nCURRENT_DIR = os.path.abspath(os.path.dirname(__file__))\nFILES_DIR = os.path.join(CURRENT_DIR, \'files\')\nMODELS_DIR = os.path.join(CURRENT_DIR, \'models\')\nTEXT_FILE = os.path.join(FILES_DIR, \'shakespeare.txt\')\n\n\nclass TextPreprocessing(object):\n    def __init__(self, filepath):\n        with open(filepath, \'r\') as f:\n            text = f.read()\n\n        self.text = text\n        self.characters = sorted(list(set(text)))\n        self.n_characters = len(self.characters)\n\n    def load_samples(self, window_size=50, stride=5):\n        text = self.text\n        characters = self.characters\n        n_characters = self.n_characters\n        n_samples = 1 + ((len(text) - window_size) // stride)\n\n        samples = np.zeros((n_samples, window_size, n_characters))\n        targets = np.zeros((n_samples, n_characters))\n\n        for i in trange(0, len(text) - window_size, stride, total=n_samples):\n            sample_id = i // stride\n            input_sequence = text[i:i + window_size]\n\n            for j, char in enumerate(input_sequence):\n                samples[sample_id, j, characters.index(char)] = 1\n\n            target_char = text[i + window_size]\n            targets[sample_id, characters.index(target_char)] = 1\n\n        return train_test_split(samples, targets, test_size=0.1)\n\n    def encode(self, sequence):\n        sequence_size = len(sequence)\n\n        data = np.zeros((sequence_size, self.n_characters))\n        data[(np.arange(sequence_size), sequence)] = 1\n\n        return np.expand_dims(data, axis=0)\n\n    def sample(self, predictions, temperature=1.0):\n        log_predictions = np.log(predictions) / temperature\n        predictions = np.exp(log_predictions)\n\n        predictions = predictions.astype(\'float64\')\n        normalized_predictions = predictions / np.sum(predictions)\n        probabilities = np.random.multinomial(\n            n=1, pvals=normalized_predictions, size=1)\n\n        return np.argmax(probabilities)\n\n    def int_to_string(self, int_sequence):\n        char_sequence = [self.characters[i] for i in int_sequence]\n        return \'\'.join(char_sequence)\n\n\nif __name__ == \'__main__\':\n    window_size = 40\n\n    print(""Loading Shakespeare\'s text ..."")\n    preprocessor = TextPreprocessing(filepath=TEXT_FILE)\n    n_characters = preprocessor.n_characters\n\n    x_train, x_test, y_train, y_test = preprocessor.load_samples(\n        window_size, stride=10)\n\n    network = algorithms.RMSProp(\n        [\n            layers.Input((window_size, n_characters)),\n            layers.LSTM(128),\n            layers.Softmax(n_characters),\n        ],\n        step=0.01,\n        verbose=True,\n        batch_size=128,\n        loss=\'categorical_crossentropy\',\n    )\n    network.train(x_train, y_train, x_test, y_test, epochs=10)\n\n    # Number of symbols that will be generated\n    n_new_symbols = 1000\n    # Which samples to use from the test data\n    test_sample_id = 0\n\n    test_sample = x_test[test_sample_id]\n    int_sequence = list(test_sample.argmax(axis=1))\n\n    print(\'\\nGenerating new text using pretrained RNN ...\')\n    for i in trange(n_new_symbols, total=n_new_symbols):\n        last_characters = int_sequence[-window_size:]\n        data = preprocessor.encode(last_characters)\n        output = network.predict(data)\n        output = preprocessor.sample(output[0], temperature=0.5)\n        int_sequence.append(output)\n\n    print(preprocessor.int_to_string(int_sequence))\n'"
neupy/algorithms/__init__.py,0,b'from .gd.base import *\nfrom .gd.lev_marq import *\nfrom .gd.quasi_newton import *\nfrom .gd.conjgrad import *\nfrom .gd.hessian import *\nfrom .gd.hessdiag import *\nfrom .gd.rprop import *\nfrom .gd.momentum import *\nfrom .gd.adadelta import *\nfrom .gd.adagrad import *\nfrom .gd.rmsprop import *\nfrom .gd.adam import *\nfrom .gd.adamax import *\n\nfrom .gd.step_updates import *\nfrom .gd.regularizers import *\n\nfrom .memory.discrete_hopfield_network import *\nfrom .memory.bam import *\nfrom .memory.cmac import *\n\nfrom .associative.oja import *\nfrom .associative.hebb import *\nfrom .associative.instar import *\nfrom .associative.kohonen import *\n\nfrom .competitive.sofm import *\nfrom .competitive.art import *\nfrom .competitive.lvq import *\nfrom .competitive.growing_neural_gas import *\n\nfrom .rbfn.pnn import *\nfrom .rbfn.grnn import *\n\nfrom .rbm import *\n'
neupy/algorithms/base.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, absolute_import, unicode_literals\n\nimport time\nimport inspect\nfrom abc import abstractmethod\n\nimport numpy as np\n\nfrom neupy.exceptions import StopTraining\nfrom neupy.core.logs import Verbose\nfrom neupy.core.config import ConfigurableABC\nfrom neupy.core.properties import Property, NumberProperty, IntProperty\nfrom neupy.algorithms import signals as base_signals\nfrom neupy.algorithms.plots import plot_optimizer_errors\nfrom neupy.utils import iters, as_tuple\n\n\n__all__ = (\'BaseSkeleton\', \'BaseNetwork\')\n\n\ndef preformat_value(value):\n    if inspect.isfunction(value) or inspect.isclass(value):\n        return value.__name__\n\n    elif isinstance(value, (list, tuple, set)):\n        return [preformat_value(v) for v in value]\n\n    elif isinstance(value, (np.ndarray, np.matrix)):\n        return value.shape\n\n    return value\n\n\nclass BaseSkeleton(ConfigurableABC, Verbose):\n    """"""\n    Base class for neural network algorithms.\n\n    Methods\n    -------\n    fit(\\*args, \\*\\*kwargs)\n        Alias to the ``train`` method.\n\n    predict(X)\n        Predicts output for the specified input.\n    """"""\n\n    def __init__(self, *args, **options):\n        super(BaseSkeleton, self).__init__(*args, **options)\n\n        self.logs.title(""Main information"")\n        self.logs.message(""ALGORITHM"", self.__class__.__name__)\n        self.logs.newline()\n\n        for key, data in sorted(self.options.items()):\n            formated_value = preformat_value(getattr(self, key))\n            msg_text = ""{} = {}"".format(key, formated_value)\n            self.logs.message(""OPTION"", msg_text, color=\'green\')\n\n        self.logs.newline()\n\n    @abstractmethod\n    def train(self, X, y):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def predict(self, X):\n        raise NotImplementedError()\n\n    def transform(self, X):\n        return self.predict(X)\n\n    def fit(self, X, y=None, *args, **kwargs):\n        self.train(X, y, *args, **kwargs)\n        return self\n\n    def repr_options(self):\n        options = []\n        for option_name in self.options:\n            option_value = getattr(self, option_name)\n            option_value = preformat_value(option_value)\n\n            option_repr = ""{}={}"".format(option_name, option_value)\n            options.append(option_repr)\n\n        return \', \'.join(options)\n\n    def __repr__(self):\n        class_name = self.__class__.__name__\n        available_options = self.repr_options()\n        return ""{}({})"".format(class_name, available_options)\n\n    def __getstate__(self):\n        return self.__dict__\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n\n\nclass Events(object):\n    def __init__(self, network, signals):\n        self.network = network\n        self.signals = signals\n        self.logs = []\n\n    def trigger(self, name, store_data=False, **data):\n        if store_data and data:\n            self.logs.append(dict(data, name=name))\n\n        for signal in self.signals:\n            if hasattr(signal, name):\n                signal_method = getattr(signal, name)\n                signal_method(self.network, **data)\n\n\nclass BaseNetwork(BaseSkeleton):\n    """"""\n    Base class for Neural Network algorithms.\n\n    Parameters\n    ----------\n    step : float\n        Learning rate, defaults to ``0.1``.\n\n    show_epoch : int\n        This property controls how often the network will display\n        information about training. It has to be defined as positive\n        integer. For instance, number ``100`` mean that network shows\n        summary at 1st, 100th, 200th, 300th ... and last epochs.\n\n        Defaults to ``1``.\n\n    shuffle_data : bool\n        If it\'s ``True`` than training data will be shuffled before\n        the training. Defaults to ``True``.\n\n    signals : dict, list or function\n        Function that will be triggered after certain events during\n        the training.\n\n    {Verbose.Parameters}\n\n    Methods\n    -------\n    {BaseSkeleton.fit}\n\n    predict(X)\n        Propagates input ``X`` through the network and\n        returns produced output.\n\n    plot_errors(logx=False, show=True, **figkwargs)\n        Using errors collected during the training this method\n        generates plot that can give additional insight into the\n        performance reached during the training.\n\n    Attributes\n    ----------\n    errors : list\n        Information about errors. It has two main attributes, namely\n        ``train`` and ``valid``. These attributes provide access to\n        the training and validation errors respectively.\n\n    last_epoch : int\n        Value equals to the last trained epoch. After initialization\n        it is equal to ``0``.\n\n    n_updates_made : int\n        Number of training updates applied to the network.\n    """"""\n    step = NumberProperty(default=0.1, minval=0)\n    show_epoch = IntProperty(minval=1, default=1)\n    shuffle_data = Property(default=False, expected_type=bool)\n    signals = Property(expected_type=object)\n\n    def __init__(self, *args, **options):\n        super(BaseNetwork, self).__init__(*args, **options)\n\n        self.last_epoch = 0\n        self.n_updates_made = 0\n        self.errors = base_signals.ErrorCollector()\n\n        signals = list(as_tuple(\n            base_signals.ProgressbarSignal(),\n            base_signals.PrintLastErrorSignal(),\n            self.errors,\n            self.signals,\n        ))\n\n        for i, signal in enumerate(signals):\n            if inspect.isfunction(signal):\n                signals[i] = base_signals.EpochEndSignal(signal)\n\n            elif inspect.isclass(signal):\n                signals[i] = signal()\n\n        self.events = Events(network=self, signals=signals)\n\n    def one_training_update(self, X_train, y_train=None):\n        """"""\n        Function would be trigger before run all training procedure\n        related to the current epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Current epoch number.\n        """"""\n        raise NotImplementedError()\n\n    def score(self, X, y):\n        raise NotImplementedError()\n\n    def plot_errors(self, logx=False, show=True, **figkwargs):\n        return plot_optimizer_errors(\n            optimizer=self,\n            logx=logx,\n            show=show,\n            **figkwargs\n        )\n\n    def train(self, X_train, y_train=None, X_test=None, y_test=None,\n              epochs=100, batch_size=None):\n        """"""\n        Method train neural network.\n\n        Parameters\n        ----------\n        X_train : array-like\n        y_train : array-like or None\n        X_test : array-like or None\n        y_test : array-like or None\n\n        epochs : int\n            Defaults to ``100``.\n\n        epsilon : float or None\n            Defaults to ``None``.\n        """"""\n        if epochs <= 0:\n            raise ValueError(""Number of epochs needs to be a positive number"")\n\n        epochs = int(epochs)\n        first_epoch = self.last_epoch + 1\n        batch_size = batch_size or getattr(self, \'batch_size\', None)\n\n        self.events.trigger(\n            name=\'train_start\',\n            X_train=X_train,\n            y_train=y_train,\n            epochs=epochs,\n            batch_size=batch_size,\n            store_data=False,\n        )\n\n        try:\n            for epoch in range(first_epoch, first_epoch + epochs):\n                self.events.trigger(\'epoch_start\')\n\n                self.last_epoch = epoch\n                iterator = iters.minibatches(\n                    (X_train, y_train),\n                    batch_size,\n                    self.shuffle_data,\n                )\n\n                for X_batch, y_batch in iterator:\n                    self.events.trigger(\'update_start\')\n                    update_start_time = time.time()\n\n                    train_error = self.one_training_update(X_batch, y_batch)\n                    self.n_updates_made += 1\n\n                    self.events.trigger(\n                        name=\'train_error\',\n                        value=train_error,\n                        eta=time.time() - update_start_time,\n                        epoch=epoch,\n                        n_updates=self.n_updates_made,\n                        n_samples=iters.count_samples(X_batch),\n                        store_data=True,\n                    )\n                    self.events.trigger(\'update_end\')\n\n                if X_test is not None:\n                    test_start_time = time.time()\n                    validation_error = self.score(X_test, y_test)\n                    self.events.trigger(\n                        name=\'valid_error\',\n                        value=validation_error,\n                        eta=time.time() - test_start_time,\n                        epoch=epoch,\n                        n_updates=self.n_updates_made,\n                        n_samples=iters.count_samples(X_test),\n                        store_data=True,\n                    )\n\n                self.events.trigger(\'epoch_end\')\n\n        except StopTraining as err:\n            self.logs.message(\n                ""TRAIN"",\n                ""Epoch #{} was stopped. Message: {}"".format(epoch, str(err)))\n\n        self.events.trigger(\'train_end\')\n'"
neupy/algorithms/plots.py,0,"b'import pkgutil\nimport warnings\nimport importlib\n\nimport matplotlib.pyplot as plt\n\n\ndef load_pandas_module():\n    if not pkgutil.find_loader(\'pandas\'):\n        raise ImportError(\n            ""The `pandas` library is not installed. Try to ""\n            ""install it with pip: \\n    pip install pandas"")\n\n    return importlib.import_module(\'pandas\')\n\n\ndef plot(x_train, y_train, x_valid, y_valid, ax, logx=False):\n    plot_function = ax.semilogx if logx else ax.plot\n\n    # With large number of samples there will be to many\n    # points in the plot and it will be hard to read it\n    style = \'-\' if len(x_train) > 50 or len(x_valid) > 50 else \'o-\'\n    line_train, = plot_function(x_train, y_train, style, label=\'train\')\n\n    if len(x_valid) > 0:\n        line_valid_, = plot_function(x_valid, y_valid, style, label=\'valid\')\n        ax.legend([line_train, line_valid_], [\'Training\', \'Validation\'])\n\n\ndef plot_errors_per_update(train, valid, ax, logx=False):\n    plot(\n        # Loss that we\'ve obtain per each training update is calculated before\n        # the update. Which means that loss represents state of the network\n        # before the update.\n        x_train=train.n_updates - 1,\n        y_train=train.value,\n\n        x_valid=valid.n_updates,\n        y_valid=valid.value,\n\n        logx=logx,\n        ax=ax,\n    )\n\n    ax.set_title(\'Training perfomance\')\n    ax.set_ylabel(\'Loss\')\n    ax.set_xlabel(\'Number of updates passed\')\n\n\ndef plot_error_per_epoch(train, valid, ax, logx=False):\n    valid = valid[[\'epoch\', \'value\']]\n    train = train[[\'epoch\', \'value\']]\n\n    train_epochs = train.groupby(\'epoch\').agg({\'value\': \'first\'})\n    valid_epochs = valid.groupby(\'epoch\').agg({\'value\': \'first\'})\n\n    plot(\n        # Loss that we\'ve obtain per each training update is calculated before\n        # the update. Which means that loss represents state of the network\n        # before the update.\n        x_train=train_epochs.index - 1,\n        y_train=train_epochs.value,\n\n        x_valid=valid_epochs.index,\n        y_valid=valid_epochs.value,\n\n        logx=logx,\n        ax=ax,\n    )\n\n    ax.set_ylabel(\'Loss\')\n    ax.set_xlabel(\'Number of training epochs passed\')\n\n\ndef plot_optimizer_errors(optimizer, logx=False, show=True, **figkwargs):\n    if \'figsize\' not in figkwargs:\n        figkwargs[\'figsize\'] = (12, 8)\n\n    if not optimizer.events.logs:\n        warnings.warn(""There is no data to plot"")\n        return\n\n    pd = load_pandas_module()\n    history = pd.DataFrame(optimizer.events.logs)\n\n    train = history[history.name == \'train_error\']\n    valid = history[history.name == \'valid_error\']\n    train_max = train.max()\n\n    if train_max.epoch == train_max.n_updates:\n        fig, ax = plt.subplots(1, 1, **figkwargs)\n        # When number of epochs exactly the same as number of updates\n        # plots per number of updates and epochs will be exactly the same\n        plot_error_per_epoch(train, valid, logx=logx, ax=ax)\n    else:\n        fig, axes = plt.subplots(2, 1, **figkwargs)\n        plot_errors_per_update(train, valid, logx=logx, ax=axes[0])\n        plot_error_per_epoch(train, valid, logx=logx, ax=axes[1])\n\n    if show:\n        plt.show()\n'"
neupy/algorithms/rbm.py,49,"b'import numpy as np\nimport tensorflow as tf\n\nfrom neupy.core.config import DumpableObject\nfrom neupy.core.properties import IntProperty, ParameterProperty\nfrom neupy.algorithms.base import BaseNetwork\nfrom neupy.utils import asfloat, format_data, dot, iters, tf_utils\nfrom neupy import init\n\n\n__all__ = (\'RBM\',)\n\n\ndef random_binomial(p):\n    with tf.name_scope(\'random-binomial\'):\n        samples = tf.random_uniform(tf.shape(p), dtype=tf.float32) <= p\n        return tf.cast(samples, tf.float32)\n\n\ndef random_sample(data, n_samples):\n    with tf.name_scope(\'random-sample\'):\n        data_shape = tf.shape(data)\n        max_index = data_shape[0]\n        sample_indices = tf.random_uniform(\n            (n_samples,), minval=0, maxval=max_index,\n            dtype=tf.int32\n        )\n        return tf.gather(data, sample_indices)\n\n\nclass RBM(BaseNetwork, DumpableObject):\n    """"""\n    Boolean/Bernoulli Restricted Boltzmann Machine (RBM).\n    Algorithm assumes that inputs are either binary\n    values or values between 0 and 1.\n\n    Parameters\n    ----------\n    n_visible : int\n        Number of visible units. Number of features (columns)\n        in the input data.\n\n    n_hidden : int\n        Number of hidden units. The large the number the more\n        information network can capture from the data, but it\n        also mean that network is more likely to overfit.\n\n    batch_size : int\n        Size of the mini-batch. Defaults to ``10``.\n\n    weight : array-like, Tensorfow variable, Initializer or scalar\n        Default initialization methods\n        you can find :ref:`here <init-methods>`.\n        Defaults to :class:`Normal <neupy.init.Normal>`.\n\n    hidden_bias : array-like, Tensorfow variable, Initializer or scalar\n        Default initialization methods\n        you can find :ref:`here <init-methods>`.\n        Defaults to :class:`Constant(value=0) <neupy.init.Constant>`.\n\n    visible_bias : array-like, Tensorfow variable, Initializer or scalar\n        Default initialization methods\n        you can find :ref:`here <init-methods>`.\n        Defaults to :class:`Constant(value=0) <neupy.init.Constant>`.\n\n    {BaseNetwork.Parameters}\n\n    Methods\n    -------\n    train(X_train, epochs=100)\n        Trains network.\n\n    predict(X)\n        Alias to the ``visible_to_hidden`` method.\n\n    visible_to_hidden(visible_input)\n        Propagates data through the network and returns output\n        from the hidden layer.\n\n    hidden_to_visible(hidden_input)\n        Propagates output from the hidden layer backward\n        to the visible.\n\n    gibbs_sampling(visible_input, n_iter=1)\n        Makes Gibbs sampling ``n`` times using visible input.\n\n    {BaseSkeleton.fit}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> data = np.array([\n    ...     [1, 0, 1, 0],\n    ...     [1, 0, 1, 0],\n    ...     [1, 0, 0, 0],  # incomplete sample\n    ...     [1, 0, 1, 0],\n    ...\n    ...     [0, 1, 0, 1],\n    ...     [0, 0, 0, 1],  # incomplete sample\n    ...     [0, 1, 0, 1],\n    ...     [0, 1, 0, 1],\n    ...     [0, 1, 0, 1],\n    ...     [0, 1, 0, 1],\n    ... ])\n    >>>\n    >>> rbm = algorithms.RBM(n_visible=4, n_hidden=1)\n    >>> rbm.train(data, epochs=100)\n    >>>\n    >>> hidden_states = rbm.visible_to_hidden(data)\n    >>> hidden_states.round(2)\n    array([[ 0.99],\n           [ 0.99],\n           [ 0.95],\n           [ 0.99],\n           [ 0.  ],\n           [ 0.01],\n           [ 0.  ],\n           [ 0.  ],\n           [ 0.  ],\n           [ 0.  ]])\n\n    References\n    ----------\n    [1] G. Hinton, A Practical Guide to Training Restricted\n        Boltzmann Machines, 2010.\n        http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf\n    """"""\n    n_visible = IntProperty(minval=1)\n    n_hidden = IntProperty(minval=1)\n    batch_size = IntProperty(minval=1, default=10)\n\n    weight = ParameterProperty(default=init.Normal())\n    hidden_bias = ParameterProperty(default=init.Constant(value=0))\n    visible_bias = ParameterProperty(default=init.Constant(value=0))\n\n    def __init__(self, n_visible, n_hidden, **options):\n        options.update({\'n_visible\': n_visible, \'n_hidden\': n_hidden})\n        super(RBM, self).__init__(**options)\n\n        self.init_functions()\n        self.init_methods()\n\n    def init_functions(self):\n        with tf.variable_scope(\'rbm\'):\n            self.weight = tf_utils.create_variable(\n                value=self.weight,\n                name=\'weight\',\n                shape=(self.n_visible, self.n_hidden),\n                trainable=False,\n            )\n            self.hidden_bias = tf_utils.create_variable(\n                value=self.hidden_bias,\n                name=\'hidden-bias\',\n                shape=(self.n_hidden,),\n                trainable=False,\n            )\n            self.visible_bias = tf_utils.create_variable(\n                value=self.visible_bias,\n                name=\'visible-bias\',\n                shape=(self.n_visible,),\n                trainable=False,\n            )\n            self.h_samples = tf.Variable(\n                tf.zeros([self.batch_size, self.n_hidden]),\n                name=""hidden-samples"",\n                dtype=tf.float32,\n                trainable=False,\n            )\n\n            self.network_input = tf.placeholder(\n                tf.float32,\n                (None, self.n_visible),\n                name=""network-input"",\n            )\n            self.network_hidden_input = tf.placeholder(\n                tf.float32,\n                (None, self.n_hidden),\n                name=""network-hidden-input"",\n            )\n\n    def init_methods(self):\n        def free_energy(visible_sample):\n            with tf.name_scope(\'free-energy\'):\n                wx = tf.matmul(visible_sample, self.weight)\n                wx_b = wx + self.hidden_bias\n\n                visible_bias_term = dot(visible_sample, self.visible_bias)\n\n                # We can get infinity when wx_b is a relatively large number\n                # (maybe 100). Taking exponent makes it even larger and\n                # for with float32 it can convert it to infinity. But because\n                # number is so large we don\'t care about +1 value before taking\n                # logarithms and therefore we can just pick value as it is\n                # since our operation won\'t change anything.\n                hidden_terms = tf.where(\n                    # exp(30) is such a big number that +1 won\'t\n                    # make any difference in the outcome.\n                    tf.greater(wx_b, 30),\n                    wx_b,\n                    tf.log1p(tf.exp(wx_b)),\n                )\n\n                hidden_term = tf.reduce_sum(hidden_terms, axis=1)\n                return -(visible_bias_term + hidden_term)\n\n        def visible_to_hidden(visible_sample):\n            with tf.name_scope(\'visible-to-hidden\'):\n                wx = tf.matmul(visible_sample, self.weight)\n                wx_b = wx + self.hidden_bias\n                return tf.nn.sigmoid(wx_b)\n\n        def hidden_to_visible(hidden_sample):\n            with tf.name_scope(\'hidden-to-visible\'):\n                wx = tf.matmul(hidden_sample, self.weight, transpose_b=True)\n                wx_b = wx + self.visible_bias\n                return tf.nn.sigmoid(wx_b)\n\n        def sample_hidden_from_visible(visible_sample):\n            with tf.name_scope(\'sample-hidden-to-visible\'):\n                hidden_prob = visible_to_hidden(visible_sample)\n                hidden_sample = random_binomial(hidden_prob)\n                return hidden_sample\n\n        def sample_visible_from_hidden(hidden_sample):\n            with tf.name_scope(\'sample-visible-to-hidden\'):\n                visible_prob = hidden_to_visible(hidden_sample)\n                visible_sample = random_binomial(visible_prob)\n                return visible_sample\n\n        network_input = self.network_input\n        network_hidden_input = self.network_hidden_input\n        input_shape = tf.shape(network_input)\n        n_samples = input_shape[0]\n\n        weight = self.weight\n        h_bias = self.hidden_bias\n        v_bias = self.visible_bias\n        h_samples = self.h_samples\n        step = asfloat(self.step)\n\n        with tf.name_scope(\'positive-values\'):\n            # We have to use `cond` instead of `where`, because\n            # different if-else cases might have different shapes\n            # and it triggers exception in tensorflow.\n            v_pos = tf.cond(\n                tf.equal(n_samples, self.batch_size),\n                lambda: network_input,\n                lambda: random_sample(network_input, self.batch_size)\n            )\n            h_pos = visible_to_hidden(v_pos)\n\n        with tf.name_scope(\'negative-values\'):\n            v_neg = sample_visible_from_hidden(h_samples)\n            h_neg = visible_to_hidden(v_neg)\n\n        with tf.name_scope(\'weight-update\'):\n            weight_update = (\n                tf.matmul(v_pos, h_pos, transpose_a=True) -\n                tf.matmul(v_neg, h_neg, transpose_a=True)\n            ) / asfloat(n_samples)\n\n        with tf.name_scope(\'hidden-bias-update\'):\n            h_bias_update = tf.reduce_mean(h_pos - h_neg, axis=0)\n\n        with tf.name_scope(\'visible-bias-update\'):\n            v_bias_update = tf.reduce_mean(v_pos - v_neg, axis=0)\n\n        with tf.name_scope(\'flipped-input-features\'):\n            # Each row will have random feature marked with number 1\n            # Other values will be equal to 0\n            possible_feature_corruptions = tf.eye(self.n_visible)\n            corrupted_features = random_sample(\n                possible_feature_corruptions, n_samples)\n\n            rounded_input = tf.round(network_input)\n            # If we scale input values from [0, 1] range to [-1, 1]\n            # than it will be easier to flip feature values with simple\n            # multiplication.\n            scaled_rounded_input = 2 * rounded_input - 1\n            scaled_flipped_rounded_input = (\n                # for corrupted_features we convert 0 to 1 and 1 to -1\n                # in this way after multiplication we will flip all\n                # signs where -1 in the transformed corrupted_features\n                (-2 * corrupted_features + 1) * scaled_rounded_input\n            )\n            # Scale it back to the [0, 1] range\n            flipped_rounded_input = (scaled_flipped_rounded_input + 1) / 2\n\n        with tf.name_scope(\'pseudo-likelihood-loss\'):\n            # Stochastic pseudo-likelihood\n            error = tf.reduce_mean(\n                self.n_visible * tf.log_sigmoid(\n                    free_energy(flipped_rounded_input) -\n                    free_energy(rounded_input)\n                )\n            )\n\n        with tf.name_scope(\'gibbs-sampling\'):\n            gibbs_sampling = sample_visible_from_hidden(\n                sample_hidden_from_visible(network_input))\n\n        tf_utils.initialize_uninitialized_variables()\n        self.weight_update_one_step = tf_utils.function(\n            [network_input],\n            error,\n            name=\'rbm/train-epoch\',\n            updates=[\n                (weight, weight + step * weight_update),\n                (h_bias, h_bias + step * h_bias_update),\n                (v_bias, v_bias + step * v_bias_update),\n                (h_samples, random_binomial(p=h_neg)),\n            ]\n        )\n        self.score_func = tf_utils.function(\n            [network_input],\n            error,\n            name=\'rbm/prediction-error\',\n        )\n        self.visible_to_hidden_one_step = tf_utils.function(\n            [network_input],\n            visible_to_hidden(network_input),\n            name=\'rbm/visible-to-hidden\',\n        )\n        self.hidden_to_visible_one_step = tf_utils.function(\n            [network_hidden_input],\n            hidden_to_visible(network_hidden_input),\n            name=\'rbm/hidden-to-visible\',\n        )\n        self.gibbs_sampling_one_step = tf_utils.function(\n            [network_input],\n            gibbs_sampling,\n            name=\'rbm/gibbs-sampling\',\n        )\n\n    def train(self, X_train, X_test=None, epochs=100):\n        """"""\n        Train RBM.\n\n        Parameters\n        ----------\n        X_train : 1D or 2D array-like\n        X_test : 1D or 2D array-like or None\n            Defaults to ``None``.\n        epochs : int\n            Number of training epochs. Defaults to ``100``.\n        """"""\n        return super(RBM, self).train(\n            X_train=X_train, y_train=None,\n            X_test=X_test, y_test=None,\n            epochs=epochs)\n\n    def one_training_update(self, X_train, y_train=None):\n        """"""\n        Train one epoch.\n\n        Parameters\n        ----------\n        X_train : array-like (n_samples, n_features)\n\n        Returns\n        -------\n        float\n        """"""\n        return self.weight_update_one_step(X_train)\n\n    def predict(self, X):\n        return self.visible_to_hidden(X)\n\n    def visible_to_hidden(self, visible_input):\n        """"""\n        Propagates data through the network and returns output\n        from the hidden layer.\n\n        Parameters\n        ----------\n        visible_input : array-like (n_samples, n_visible_features)\n\n        Returns\n        -------\n        array-like\n        """"""\n        is_input_feature1d = (self.n_visible == 1)\n        visible_input = format_data(visible_input, is_input_feature1d)\n\n        outputs = iters.apply_batches(\n            function=self.visible_to_hidden_one_step,\n            inputs=visible_input,\n            batch_size=self.batch_size,\n            show_progressbar=self.logs.enable,\n        )\n        return np.concatenate(outputs, axis=0)\n\n    def hidden_to_visible(self, hidden_input):\n        """"""\n        Propagates output from the hidden layer backward\n        to the visible.\n\n        Parameters\n        ----------\n        hidden_input : array-like (n_samples, n_hidden_features)\n\n        Returns\n        -------\n        array-like\n        """"""\n        is_input_feature1d = (self.n_hidden == 1)\n        hidden_input = format_data(hidden_input, is_input_feature1d)\n\n        outputs = iters.apply_batches(\n            function=self.hidden_to_visible_one_step,\n            inputs=hidden_input,\n            batch_size=self.batch_size,\n            show_progressbar=self.logs.enable,\n        )\n        return np.concatenate(outputs, axis=0)\n\n    def score(self, X, y=None):\n        """"""\n        Compute the pseudo-likelihood of input samples.\n\n        Parameters\n        ----------\n        X : array-like\n            Values of the visible layer\n\n        Returns\n        -------\n        float\n            Value of the pseudo-likelihood.\n        """"""\n        return iters.apply_batches(\n            function=self.score_func,\n            inputs=format_data(X, is_feature1d=(self.n_visible == 1)),\n            batch_size=self.batch_size,\n\n            show_output=True,\n            show_progressbar=self.logs.enable,\n            average_outputs=True,\n        )\n\n    def gibbs_sampling(self, visible_input, n_iter=1):\n        """"""\n        Makes Gibbs sampling n times using visible input.\n\n        Parameters\n        ----------\n        visible_input : 1d or 2d array\n        n_iter : int\n            Number of Gibbs sampling iterations. Defaults to ``1``.\n\n        Returns\n        -------\n        array-like\n            Output from the visible units after perfoming n\n            Gibbs samples. Array will contain only binary\n            units (0 and 1).\n        """"""\n        is_input_feature1d = (self.n_visible == 1)\n        visible_input = format_data(visible_input, is_input_feature1d)\n\n        input_ = visible_input\n        for iteration in range(n_iter):\n            input_ = self.gibbs_sampling_one_step(input_)\n\n        return input_\n'"
neupy/algorithms/signals.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nimport time\n\nimport progressbar\n\nfrom neupy.utils import iters\n\n\n__all__ = (\'ProgressbarSignal\', \'PrintLastErrorSignal\', \'EpochEndSignal\',\n           \'ErrorCollector\')\n\n\ndef format_time(time):\n    """"""\n    Format seconds into human readable format.\n\n    Parameters\n    ----------\n    time : float\n        Time specified in seconds\n\n    Returns\n    -------\n    str\n        Formated time.\n    """"""\n    mins, seconds = divmod(int(time), 60)\n    hours, minutes = divmod(mins, 60)\n\n    if hours > 0:\n        return \'{:0>2d}:{:0>2d}:{:0>2d}\'.format(hours, minutes, seconds)\n\n    elif minutes > 0:\n        return \'{:0>2d}:{:0>2d}\'.format(minutes, seconds)\n\n    elif seconds > 0:\n        return \'{:.0f} sec\'.format(seconds)\n\n    elif time >= 1e-3:\n        return \'{:.0f} ms\'.format(time * 1e3)\n\n    elif time >= 1e-6:\n        return \'{:.0f} \xce\xbcs\'.format(time * 1e6)  # microseconds\n\n    return \'{:.0f} ns\'.format(time * 1e9)  # nanoseconds or smaller\n\n\nclass EpochEndSignal(object):\n    def __init__(self, function):\n        self.function = function\n\n    def epoch_end(self, network):\n        self.function(network)\n\n\nclass ProgressbarSignal(object):\n    def train_start(self, network, **kwargs):\n        if kwargs[\'batch_size\'] is None:\n            self.n_batches = 1\n            return\n\n        self.n_batches = iters.count_minibatches(\n            kwargs[\'X_train\'],\n            kwargs[\'batch_size\'])\n\n    def epoch_start(self, network):\n        self.index = 0\n        self.last_train_error = None\n        self.bar = progressbar.NullBar()\n\n        if network.verbose and self.n_batches >= 2:\n            self.bar = iters.make_progressbar(self.n_batches, show_output=True)\n            self.bar.update(0)\n\n    def train_error(self, network, **data):\n        self.last_train_error = data[\'value\']\n\n    def update_end(self, network):\n        self.index += 1\n        self.bar.update(self.index, loss=self.last_train_error)\n\n    def epoch_end(self, network):\n        self.bar.finish(end=\'\\r\')\n        self.bar.fd.write(\'\\r\' + \' \' * self.bar.term_width + \'\\r\')\n\n    def __reduce__(self):\n        return self.__class__, tuple()\n\n\nclass PrintLastErrorSignal(object):\n    def print_last_error(self, network):\n        messages = []\n        base_message = ""#{} : [{}] "".format(\n            network.last_epoch,\n            format_time(self.last_epoch_time))\n\n        if self.last_train_error is not None:\n            messages.append(""train: {:.6f}"".format(self.last_train_error))\n\n        if self.last_valid_error is not None:\n            messages.append(""valid: {:.6f}"".format(self.last_valid_error))\n\n        network.logs.write(base_message + \', \'.join(messages))\n\n    def train_start(self, network, **kwargs):\n        self.first_epoch = network.last_epoch + 1\n        self.last_epoch_shown = 0\n\n    def epoch_start(self, network):\n        self.epoch_start_time = time.time()\n        self.last_train_error = None\n        self.last_valid_error = None\n\n    def train_error(self, network, value, **kwargs):\n        self.last_train_error = value\n\n    def valid_error(self, network, value, **kwargs):\n        self.last_valid_error = value\n\n    def epoch_end(self, network):\n        epoch = network.last_epoch\n        self.last_epoch_time = time.time() - self.epoch_start_time\n\n        if epoch % network.show_epoch == 0 or epoch == self.first_epoch:\n            self.print_last_error(network)\n            self.last_epoch_shown = network.last_epoch\n\n    def train_end(self, network):\n        if network.last_epoch != self.last_epoch_shown:\n            self.print_last_error(network)\n\n\nclass ErrorCollector(object):\n    def __init__(self):\n        self.valid = []\n        self.train = []\n\n    def train_error(self, network, value, **kwargs):\n        self.train.append(value)\n\n    def valid_error(self, network, value, **kwargs):\n        self.valid.append(value)\n'"
neupy/architectures/__init__.py,0,b'from .vgg16 import *\nfrom .vgg19 import *\nfrom .squeezenet import *\nfrom .resnet import *\n\nfrom .mixture_of_experts import *\n'
neupy/architectures/mixture_of_experts.py,1,"b'import tensorflow as tf\n\nfrom neupy import layers\nfrom neupy.utils import tf_utils, as_tuple\nfrom neupy.layers.base import BaseGraph\n\n\n__all__ = (\'mixture_of_experts\',)\n\n\ndef check_if_network_is_valid(network, index):\n    if not isinstance(network, BaseGraph):\n        raise TypeError(\n            ""Invalid input, Mixture of experts expects networks/layers""\n            ""in the list of networks, got `{}` instance instead""\n            """".format(type(network)))\n\n    if len(network.input_layers) > 1:\n        raise ValueError(\n            ""Each network from the mixture of experts has to process single ""\n            ""input tensor. Network #{} (0-based indices) has more than one ""\n            ""input layer. Input layers: {}""\n            """".format(index, network.output_layers))\n\n    if len(network.output_layers) > 1:\n        raise ValueError(\n            ""Each network from the mixture of experts has to output single ""\n            ""tensor. Network #{} (0-based indices) has more than one output ""\n            ""layer. Output layers: {}"".format(index, network.output_layers))\n\n    if network.input_shape.ndims != 2:\n        raise ValueError(\n            ""Each network from the mixture of experts has to process ""\n            ""only 2-dimensional inputs. Network #{} (0-based indices) ""\n            ""processes only {}-dimensional inputs. Input layer\'s shape: {}""\n            """".format(index, network.input_shape.ndims, network.input_shape))\n\n\ndef check_if_networks_compatible(networks):\n    input_shapes = []\n    output_shapes = []\n\n    for i, network in enumerate(networks):\n        input_shapes.append(network.input_shape)\n        output_shapes.append(network.output_shape)\n\n    for shape in input_shapes:\n        if not shape.is_compatible_with(input_shapes[0]):\n            raise ValueError(\n                ""Networks have incompatible input shapes. Shapes: {}""\n                """".format(tf_utils.shape_to_tuple(input_shapes)))\n\n    for shape in output_shapes:\n        if not shape.is_compatible_with(output_shapes[0]):\n            raise ValueError(\n                ""Networks have incompatible output shapes. Shapes: {}""\n                """".format(tf_utils.shape_to_tuple(output_shapes)))\n\n\ndef mixture_of_experts(networks, gating_layer=None):\n    """"""\n    Generates mixture of experts architecture from the set of\n    networks that has the same input and output shapes.\n\n    Mixture of experts learns to how to mix results from different\n    networks in order to get better performances. It adds gating layer\n    that using input data tries to figure out which of the networks\n    will make better contribution to the final result. The final result\n    mixes from all networks using different weights. The higher the weight\n    the larger contribution from the individual layer.\n\n    Parameters\n    ----------\n    networks : list of networks/layers\n\n    gating_layer : None or layer\n        In case if value equal to `None` that the following layer\n        will be created.\n\n        .. code-block:: python\n\n            gating_layer = layers.Softmax(len(networks))\n\n        Output from the gating layer should be 1D and equal to\n        the number of networks.\n\n    Raises\n    ------\n    ValueError\n        In case if there is some problem with input networks\n        or custom gating layer.\n\n    Returns\n    -------\n    network\n        Mixture of experts network that combine all networks into\n        single one and adds gating layer to it.\n\n    Examples\n    --------\n    >>> from neupy import algorithms, architectures\n    >>> from neupy.layers import *\n    >>>\n    >>> network = architectures.mixture_of_experts([\n    ...     join(\n    ...         Input(10),\n    ...         Relu(5),\n    ...     ),\n    ...     join(\n    ...         Input(10),\n    ...         Relu(33),\n    ...         Relu(5),\n    ...     ),\n    ...     join(\n    ...         Input(10),\n    ...         Relu(12),\n    ...         Relu(25),\n    ...         Relu(5),\n    ...     ),\n    ... ])\n    >>> network\n    (?, 10) -> [... 12 layers ...] -> (?, 5)\n    >>>\n    >>> optimizer = algorithms.Momentum(network, step=0.1)\n    """"""\n    if not isinstance(networks, (list, tuple)):\n        raise ValueError(""Networks should be specified as a list"")\n\n    for index, network in enumerate(networks):\n        check_if_network_is_valid(network, index)\n\n    check_if_networks_compatible(networks)\n    input_shape = tf.TensorShape(None)\n\n    for network in networks:\n        input_shape = input_shape.merge_with(network.input_shape)\n\n    n_layers_to_combine = len(networks)\n    n_features = input_shape[1].value\n\n    if n_features is None:\n        raise ValueError(\n            ""Cannot create mixture of experts model, because ""\n            ""number of input features is unknown"")\n\n    if gating_layer is None:\n        gating_layer = layers.Softmax(n_layers_to_combine)\n\n    if not isinstance(gating_layer, layers.BaseLayer):\n        raise ValueError(\n            ""Invalid type for gating layer. Type: {}""\n            """".format(type(gating_layer)))\n\n    return layers.join(\n        layers.Input(n_features),\n        # Note: Gating network should be specified\n        # as a first parameter.\n        layers.parallel(*as_tuple(gating_layer, networks)),\n        layers.GatedAverage(),\n    )\n'"
neupy/architectures/resnet.py,0,"b'from neupy import layers\nfrom neupy.utils import function_name_scope\n\n\n__all__ = (\'resnet50\',)\n\n\n@function_name_scope\ndef ResidualUnit(n_input_filters, stride=1, rate=1, has_branch=False,\n                 name=None):\n\n    def bn_name(index):\n        return \'bn\' + name + \'_branch\' + index\n\n    def conv_name(index):\n        return \'res\' + name + \'_branch\' + index\n\n    n_output_filters = 4 * n_input_filters\n    main_branch = layers.join(\n        # The main purpose of this 1x1 convolution layer is to\n        # reduce number of filters. For instance, for the tensor with\n        # 256 filters it can be reduced to 64. This trick allows to\n        # reduce computation by factor of 4.\n        layers.Convolution(\n            size=(1, 1, n_input_filters),\n            stride=stride,\n            bias=None,\n            name=conv_name(\'2a\'),\n        ),\n        layers.BatchNorm(name=bn_name(\'2a\')),\n        layers.Relu(),\n\n        # This convolution layer applies 3x3 filter in order to\n        # extract features.\n        layers.Convolution(\n            (3, 3, n_input_filters),\n            padding=\'same\',\n            dilation=rate,\n            bias=None,\n            name=conv_name(\'2b\'),\n        ),\n        layers.BatchNorm(name=bn_name(\'2b\')),\n        layers.Relu(),\n\n        # Last layer reverses operations of the first layer. In this\n        # case we increase number of filters. For instance, from previously\n        # obtained 64 filters we can increase it back to the 256 filters\n        layers.Convolution(\n            (1, 1, n_output_filters),\n            bias=None,\n            name=conv_name(\'2c\')\n        ),\n        layers.BatchNorm(name=bn_name(\'2c\')),\n    )\n\n    if has_branch:\n        residual_branch = layers.join(\n            layers.Convolution(\n                (1, 1, n_output_filters),\n                stride=stride,\n                bias=None,\n                name=conv_name(\'1\'),\n            ),\n            layers.BatchNorm(name=bn_name(\'1\')),\n        )\n    else:\n        # Empty list defines residual connection, meaning that\n        # output from this branch would be equal to its input\n        residual_branch = layers.Identity(\'residual-\' + name)\n\n    return layers.join(\n        # For the output from two branches we just combine results\n        # with simple elementwise sum operation. The main purpose of\n        # the residual connection is to build shortcuts for the\n        # gradient during backpropagation.\n        (main_branch | residual_branch),\n        layers.Elementwise(),\n        layers.Relu(),\n    )\n\n\ndef resnet50(input_shape=(224, 224, 3), include_global_pool=True,\n             in_out_ratio=32):\n    """"""\n    ResNet50 network architecture with random parameters. Parameters\n    can be loaded using ``neupy.storage`` module.\n\n    ResNet50 has roughly 25.5 million parameters.\n\n    Parameters\n    ----------\n    input_shape : tuple\n        Network\'s input shape. Defaults to ``(224, 224, 3)``.\n\n    include_global_pool : bool\n        Specifies if returned output should include global pooling\n        layer. Defaults to ``True``.\n\n    in_out_ratio : {4, 8, 16, 32}\n        Every layer that applies strides reduces height and width per every\n        image. There are 5 of these layers in Resnet and at the end each\n        dimensions gets reduced by ``32``. For example, 224x224 image\n        will be reduced to 7x7 image patches. This parameter specifies\n        what level of reduction we want to obtain after we\'ve propagated\n        network through all the convolution layers.\n\n    Notes\n    -----\n    Because of the global pooling layer, ResNet50 can be applied to\n    the images with variable sizes. The only limitation is that image\n    size should be bigger than 32x32, otherwise network won\'t be able\n    to apply all transformations to the image.\n\n    Examples\n    --------\n    ResNet-50 for ImageNet classification\n\n    >>> from neupy import architectures, algorithms\n    >>>\n    >>> resnet = architectures.resnet50()\n    >>> resnet\n    (?, 224, 224, 3) -> [... 187 layers ...] -> (?, 1000)\n    >>>\n    >>> optimizer = algorithms.Momentum(resnet50)\n\n    ResNet-50 for custom classification task\n\n    >>> from neupy import architectures\n    >>> resnet = architectures.resnet50(include_global_pool=False)\n    >>> resnet\n    (?, 224, 224, 3) -> [... 185 layers ...] -> (?, 7, 7, 2048)\n    >>>\n    >>> from neupy.layers import *\n    >>> resnet = resnet >> GlobalPooling(\'avg\') >> Softmax(21)\n    (?, 224, 224, 3) -> [... 187 layers ...] -> (?, 21)\n\n    ResNet-50 for image segmentation\n\n    >>> from neupy import architectures\n    >>> resnet = architectures.resnet50(\n    ...     include_global_pool=False,\n    ...     in_out_ratio=8,\n    ... )\n    >>> resnet\n    (?, 224, 224, 3) -> [... 185 layers ...] -> (?, 28, 28, 2048)\n\n    See Also\n    --------\n    :architecture:`vgg16` : VGG16 network\n    :architecture:`squeezenet` : SqueezeNet network\n    :architecture:`resnet50` : ResNet-50 network\n\n    References\n    ----------\n    Deep Residual Learning for Image Recognition.\n    https://arxiv.org/abs/1512.03385\n    """"""\n    in_out_configs = {\n        4: {\'strides\': [1, 1, 1], \'rates\': [2, 4, 8]},\n        8: {\'strides\': [2, 1, 1], \'rates\': [1, 2, 4]},\n        16: {\'strides\': [2, 2, 1], \'rates\': [1, 1, 2]},\n        32: {\'strides\': [2, 2, 2], \'rates\': [1, 1, 1]},\n    }\n\n    if in_out_ratio not in in_out_configs:\n        raise ValueError(\n            ""Expected one of the folowing in_out_ratio values: {}, got ""\n            ""{} instead."".format(in_out_configs.keys(), in_out_ratio))\n\n    strides = in_out_configs[in_out_ratio][\'strides\']\n    rates = in_out_configs[in_out_ratio][\'rates\']\n\n    resnet = layers.join(\n        layers.Input(input_shape),\n\n        # Convolutional layer reduces image\'s height and width by a factor\n        # of 2 (because of the stride)\n        # from (3, 224, 224) to (64, 112, 112)\n        layers.Convolution(\n            (7, 7, 64), stride=2, bias=None,\n            padding=\'same\', name=\'conv1\'\n        ),\n        layers.BatchNorm(name=\'bn_conv1\'),\n        layers.Relu(),\n\n        # Stride equal two 2 reduces image size by a factor of two\n        # from (64, 112, 112) to (64, 56, 56)\n        layers.MaxPooling((3, 3), stride=2, padding=""same""),\n\n        # The branch option applies extra convolution x+ batch\n        # normalization transformations to the residual\n        ResidualUnit(64, name=\'2a\', has_branch=True),\n        ResidualUnit(64, name=\'2b\'),\n        ResidualUnit(64, name=\'2c\'),\n\n        # When stride=2 reduces width and hight by factor of 2\n        ResidualUnit(128, stride=strides[0], name=\'3a\', has_branch=True),\n        ResidualUnit(128, rate=rates[0], name=\'3b\'),\n        ResidualUnit(128, rate=rates[0], name=\'3c\'),\n        ResidualUnit(128, rate=rates[0], name=\'3d\'),\n\n        # When stride=2 reduces width and hight by factor of 2\n        ResidualUnit(256, rate=rates[0], name=\'4a\',\n                     stride=strides[1], has_branch=True),\n        ResidualUnit(256, rate=rates[1], name=\'4b\'),\n        ResidualUnit(256, rate=rates[1], name=\'4c\'),\n        ResidualUnit(256, rate=rates[1], name=\'4d\'),\n        ResidualUnit(256, rate=rates[1], name=\'4e\'),\n        ResidualUnit(256, rate=rates[1], name=\'4f\'),\n\n        # When stride=2 reduces width and hight by factor of 2\n        ResidualUnit(512, rate=rates[1], name=\'5a\',\n                     stride=strides[2], has_branch=True),\n        ResidualUnit(512, rate=rates[2], name=\'5b\'),\n        ResidualUnit(512, rate=rates[2], name=\'5c\'),\n    )\n\n    if include_global_pool:\n        resnet = layers.join(\n            resnet,\n            # Since the final residual unit has 2048 output filters, global\n            # pooling will replace every output image with single average\n            # value. Despite input image size, output from this layer always\n            # will be a vector with 2048 values.\n            layers.GlobalPooling(\'avg\'),\n            layers.Softmax(1000, name=\'fc1000\'),\n        )\n\n    return resnet\n'"
neupy/architectures/squeezenet.py,0,"b'from neupy import layers\nfrom neupy.utils import function_name_scope\n\n\n__all__ = (\'squeezenet\',)\n\n\n@function_name_scope\ndef Fire(s_1x1, e_1x1, e_3x3, name):\n    return layers.join(\n        layers.Convolution(\n            (1, 1, s_1x1),\n            padding=\'SAME\',\n            name=name + \'/squeeze1x1\'\n        ),\n        layers.Relu(),\n        layers.parallel([\n            layers.Convolution(\n                (1, 1, e_1x1),\n                padding=\'SAME\',\n                name=name + \'/expand1x1\'\n            ),\n            layers.Relu(),\n        ], [\n            layers.Convolution(\n                (3, 3, e_3x3),\n                padding=\'SAME\',\n                name=name + \'/expand3x3\'\n            ),\n            layers.Relu(),\n        ]),\n        layers.Concatenate(),\n    )\n\n\ndef squeezenet():\n    """"""\n    SqueezeNet network architecture with random parameters.\n    Parameters can be loaded using ``neupy.storage`` module.\n\n    SqueezeNet has roughly 1.2 million parameters. It is almost\n    50 times less than in AlexNet. Parameters can be stored as 5Mb\n    file.\n\n    Examples\n    --------\n    >>> from neupy import architectures\n    >>> squeezenet = architectures.squeezenet()\n    >>> squeezenet\n    (?, 227, 227, 3) -> [... 67 layers ...] -> (?, 1000)\n    >>>\n    >>> from neupy import algorithms\n    >>> optimizer = algorithms.Momentum(squeezenet)\n\n    See Also\n    --------\n    :architecture:`vgg16` : VGG16 network\n    :architecture:`vgg19` : VGG19 network\n    :architecture:`resnet50` : ResNet50 network\n\n    References\n    ----------\n    SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n    and <0.5MB model size\n    https://arxiv.org/abs/1602.07360\n    """"""\n    return layers.join(\n        layers.Input((227, 227, 3)),\n\n        layers.Convolution((7, 7, 96), stride=(2, 2),\n                           padding=\'VALID\', name=\'conv1\'),\n        layers.Relu(),\n        layers.MaxPooling((3, 3), stride=(2, 2)),\n\n        Fire(16, 64, 64, name=\'fire2\'),\n        Fire(16, 64, 64, name=\'fire3\'),\n        Fire(32, 128, 128, name=\'fire4\'),\n        layers.MaxPooling((2, 2)),\n\n        Fire(32, 128, 128, name=\'fire5\'),\n        Fire(48, 192, 192, name=\'fire6\'),\n        Fire(48, 192, 192, name=\'fire7\'),\n        Fire(64, 256, 256, name=\'fire8\'),\n        layers.MaxPooling((2, 2)),\n\n        Fire(64, 256, 256, name=\'fire9\'),\n        layers.Dropout(0.5),\n\n        layers.Convolution((1, 1, 1000), name=\'conv10\'),\n        layers.GlobalPooling(\'avg\'),\n        layers.Reshape(),\n        layers.Softmax(),\n    )\n'"
neupy/architectures/vgg16.py,0,"b'from neupy import layers\n\n\n__all__ = (\'vgg16\',)\n\n\ndef vgg16():\n    """"""\n    VGG16 network architecture with random parameters. Parameters\n    can be loaded using ``neupy.storage`` module.\n\n    Originally VGG16 was built in order to solve image classification\n    problem. It was used in the ImageNet competition. The goal of the\n    competition is to build a model that classifies image into one of\n    the 1,000 categories. Categories include animals, objects, transports\n    and so on.\n\n    VGG16 has roughly 138 million parameters.\n\n    Examples\n    --------\n    >>> from neupy import architectures\n    >>> vgg16 = architectures.vgg16()\n    >>> vgg16\n    (?, 224, 224, 3) -> [... 41 layers ...] -> (?, 1000)\n\n    >>>\n    >>> from neupy import algorithms\n    >>> optimizer = algorithms.Momentum(vgg16, verbose=True)\n\n    See Also\n    --------\n    :architecture:`vgg19` : VGG19 network\n    :architecture:`squeezenet` : SqueezeNet network\n    :architecture:`resnet50` : ResNet50 network\n\n    References\n    ----------\n    Very Deep Convolutional Networks for Large-Scale Image Recognition.\n    https://arxiv.org/abs/1409.1556\n    """"""\n    SamePadConv = layers.Convolution.define(padding=\'SAME\')\n\n    return layers.join(\n        layers.Input((224, 224, 3)),\n\n        SamePadConv((3, 3, 64), name=\'conv1_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 64), name=\'conv1_2\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        SamePadConv((3, 3, 128), name=\'conv2_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 128), name=\'conv2_2\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        SamePadConv((3, 3, 256), name=\'conv3_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 256), name=\'conv3_2\') >> layers.Relu(),\n        SamePadConv((3, 3, 256), name=\'conv3_3\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        SamePadConv((3, 3, 512), name=\'conv4_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv4_2\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv4_3\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        SamePadConv((3, 3, 512), name=\'conv5_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv5_2\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv5_3\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        layers.Reshape(),\n\n        layers.Linear(4096, name=\'dense_1\') >> layers.Relu(),\n        layers.Dropout(0.5),\n\n        layers.Linear(4096, name=\'dense_2\') >> layers.Relu(),\n        layers.Dropout(0.5),\n\n        layers.Linear(1000, name=\'dense_3\') >> layers.Softmax(),\n    )\n'"
neupy/architectures/vgg19.py,0,"b'from neupy import layers\n\n\n__all__ = (\'vgg19\',)\n\n\ndef vgg19():\n    """"""\n    VGG19 network architecture with random parameters. Parameters\n    can be loaded using ``neupy.storage`` module.\n\n    Originally VGG19 was built in order to solve image classification\n    problem. It was used in the ImageNet competition. The goal of the\n    competition is to build a model that classifies image into one of\n    the 1,000 categories. Categories include animals, objects, transports\n    and so on.\n\n    VGG19 has roughly 143 million parameters.\n\n    Examples\n    --------\n    >>> from neupy import architectures\n    >>> vgg19 = architectures.vgg19()\n    >>> vgg19\n    (?, 224, 224, 3) -> [... 47 layers ...] -> (?, 1000)\n    >>>\n    >>> from neupy import algorithms\n    >>> optimizer = algorithms.Momentum(vgg19)\n\n    See Also\n    --------\n    :architecture:`vgg16` : VGG16 network\n    :architecture:`squeezenet` : SqueezeNet network\n    :architecture:`resnet50` : ResNet50 network\n\n    References\n    ----------\n    Very Deep Convolutional Networks for Large-Scale Image Recognition.\n    https://arxiv.org/abs/1409.1556\n    """"""\n    SamePadConv = layers.Convolution.define(padding=\'SAME\')\n\n    return layers.join(\n        layers.Input((224, 224, 3)),\n\n        SamePadConv((3, 3, 64), name=\'conv1_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 64), name=\'conv1_2\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        SamePadConv((3, 3, 128), name=\'conv2_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 128), name=\'conv2_2\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        SamePadConv((3, 3, 256), name=\'conv3_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 256), name=\'conv3_2\') >> layers.Relu(),\n        SamePadConv((3, 3, 256), name=\'conv3_3\') >> layers.Relu(),\n        SamePadConv((3, 3, 256), name=\'conv3_4\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        SamePadConv((3, 3, 512), name=\'conv4_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv4_2\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv4_3\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv4_4\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        SamePadConv((3, 3, 512), name=\'conv5_1\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv5_2\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv5_3\') >> layers.Relu(),\n        SamePadConv((3, 3, 512), name=\'conv5_4\') >> layers.Relu(),\n        layers.MaxPooling((2, 2)),\n\n        layers.Reshape(),\n\n        layers.Linear(4096, name=\'dense_1\') >> layers.Relu(),\n        layers.Dropout(0.5),\n\n        layers.Linear(4096, name=\'dense_2\') >> layers.Relu(),\n        layers.Dropout(0.5),\n\n        layers.Linear(1000, name=\'dense_3\') >> layers.Softmax(),\n    )\n'"
neupy/core/__init__.py,0,b''
neupy/core/config.py,1,"b'from abc import ABCMeta\nfrom collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom six import with_metaclass\n\nfrom neupy.utils import tensorflow_eval\nfrom .properties import BaseProperty, WithdrawProperty\nfrom .docs import SharedDocsMeta\n\n\n__all__ = (\'ConfigMeta\', \'ConfigABCMeta\', \'Configurable\', \'ConfigurableABC\',\n           \'ExtractParameters\', \'DumpableObject\')\n\n\nOption = namedtuple(\'Option\', \'class_name value\')\n\n\nclass ExtractParameters(object):\n    def get_params(self, deep=False):\n        options = {}\n\n        for property_name, option in self.options.items():\n            value = getattr(self, property_name)\n\n            if isinstance(value, tf.Variable):\n                value = tensorflow_eval(value)\n\n            property_ = option.value\n            is_numpy_array = isinstance(value, np.ndarray)\n\n            if hasattr(option.value, \'choices\'):\n                choices = property_.choices\n\n                if not is_numpy_array and value in choices.values():\n                    choices = {v: k for k, v in choices.items()}\n                    value = choices[value]\n\n            options[property_name] = value\n\n        return options\n\n    def set_params(self, **params):\n        self.__dict__.update(params)\n        return self\n\n\ndef initialize_with_kwargs(class_, kwargs):\n    return class_(**kwargs)\n\n\nclass DumpableObject(ExtractParameters):\n    def __reduce__(self):\n        return initialize_with_kwargs, (self.__class__, self.get_params())\n\n\nclass ConfigMeta(SharedDocsMeta):\n    """"""\n    Meta-class that configure initialized properties. Also it helps\n    inheit properties from parent classes and use them.\n    """"""\n    def __new__(cls, clsname, bases, attrs):\n        new_class = super(ConfigMeta, cls).__new__(cls, clsname, bases, attrs)\n        parents = [kls for kls in bases if isinstance(kls, ConfigMeta)]\n\n        if not hasattr(new_class, \'options\'):\n            new_class.options = {}\n\n        for base_class in parents:\n            new_class.options = dict(base_class.options, **new_class.options)\n\n        options = new_class.options\n\n        # Set properties names and save options for different classes\n        for key, value in attrs.items():\n            if isinstance(value, BaseProperty):\n                value.name = key\n                options[key] = Option(class_name=clsname, value=value)\n\n            if isinstance(value, WithdrawProperty) and key in options:\n                del options[key]\n\n        return new_class\n\n\nclass BaseConfigurable(ExtractParameters):\n    """"""\n    Base configuration class. It help set up and validate\n    initialized property values.\n\n    Parameters\n    ----------\n    **options\n        Available properties.\n    """"""\n    def __init__(self, **options):\n        available_options = set(self.options.keys())\n        invalid_options = set(options) - available_options\n\n        if invalid_options:\n            clsname = self.__class__.__name__\n            raise ValueError(\n                ""The `{}` object contains invalid properties: {}""\n                """".format(clsname, \', \'.join(invalid_options)))\n\n        for key, value in options.items():\n            setattr(self, key, value)\n\n        for option_name, option in self.options.items():\n            if option.value.required and option_name not in options:\n                raise ValueError(\n                    ""Option `{}` is required."".format(option_name))\n\n\nclass Configurable(with_metaclass(ConfigMeta, BaseConfigurable)):\n    """"""\n    Class that combine ``BaseConfigurable`` class functionality and\n    ``ConfigMeta`` meta-class.\n    """"""\n\n\nclass ConfigABCMeta(ABCMeta, ConfigMeta):\n    """"""\n    Meta-class that combines ``ConfigMeta`` and ``abc.ABCMeta``\n    meta-classes.\n    """"""\n\n\nclass ConfigurableABC(with_metaclass(ConfigABCMeta, BaseConfigurable)):\n    """"""\n    Class that combine ``BaseConfigurable`` class functionality,\n    ``ConfigMeta`` and ``abc.ABCMeta`` meta-classes.\n    """"""\n'"
neupy/core/docs.py,0,"b'import re\nfrom inspect import isfunction\nfrom abc import ABCMeta\n\nfrom six import with_metaclass\n\nfrom neupy.utils import AttributeKeyDict\n\n\n__all__ = (""SharedDocsMeta"", ""SharedDocs"", ""SharedDocsException"",\n           ""SharedDocsABCMeta"", ""shared_docs"")\n\n\ndef find_numpy_doc_indent(docs):\n    """"""\n    Find indent for Numpy styled documentation and return\n    number of shifts inside of it.\n\n    Parameters\n    ----------\n    docs : str\n\n    Returns\n    -------\n    int or None\n        Returns number of indentations in documentation. If\n        it doesn\'t identify indentation function output will\n        be ``None`` value.\n    """"""\n\n    indent_detector = re.compile(r""(?P<indent>\\ *)(?P<dashes>-{3,})"")\n    indent_info = indent_detector.findall(docs)\n\n    if not indent_info:\n        return None\n\n    indent, _ = indent_info[0]\n    return len(indent)\n\n\ndef iter_doc_parameters(docs):\n    """"""\n    Find parameters defined in the documentation.\n\n    Parameters\n    ----------\n    docs : str\n\n    Yields\n    ------\n    tuple\n        Yields tuple that contain 2 values, namely parameter\n        name and full parameter description\n    """"""\n    n_indents = find_numpy_doc_indent(docs)\n    doc_indent = \' \' * n_indents if n_indents else \'\'\n    parser = re.compile(\n        r""(?P<name>\\*?\\*?\\w+)(?P<type>\\ *\\:\\ *[^\\n]+)?""\n        r""((?P<description>(\\n{indent}\\ +[^\\n]+)|(\\n))*)""\n        """".format(indent=doc_indent)\n    )\n\n    for name, type_, desc, _, _, _ in parser.findall(docs):\n        if type_ or name.startswith(\'*\'):\n            # In case of *args nad **kwargs we need to clean\n            # starts from the beggining\n            parameter_name = name.lstrip(\'*\')\n            parameter_description = \'\'.join([name, type_, desc])\n\n            yield parameter_name, parameter_description.rstrip()\n\n\ndef iter_doc_methods(docs):\n    """"""\n    Find methods defined in the documentation.\n\n    Parameters\n    ----------\n    docs : str\n\n    Yields\n    ------\n    tuple\n        Yields tuple that contain 2 values, namely method\n        name and full method description\n    """"""\n\n    n_indents = find_numpy_doc_indent(docs)\n    doc_indent = \' \' * n_indents if n_indents else \'\'\n    parser = re.compile(\n        r""(?P<name>\\w+?)(\\((.+?)?\\))""\n        r""((?P<description>\\n{indent}\\ +[^\\n]+)*)""\n        """".format(indent=doc_indent)\n    )\n\n    for name, func_params, _, desc, _ in parser.findall(docs):\n        method_description = \'\'.join([name, func_params, desc])\n        yield name, method_description\n\n\ndef parse_full_section(section_name, docs):\n    """"""\n    Find warning defined in the documentation.\n\n    Parameters\n    ----------\n    docs : str\n\n    Returns\n    -------\n    str or None\n        Returns warnings from documentation or ``None`` if\n        function didn\'t find it.\n    """"""\n    parser = re.compile(r""{}\\s+-+\\s+(?P<section_text>(.*\\n)+?)\\s+""\n                        # Here we try to find next section title or\n                        # the end of the documentation\n                        r""([\\w\\ ]+\\n\\s+-+\\s+|$)""\n                        r"""".format(section_name))\n    parsed_doc_parts = parser.findall(docs)\n\n    if not parsed_doc_parts:\n        return None\n\n    section_text_block = parsed_doc_parts[0]\n    full_section_text = section_text_block[0]\n\n    # Regexp can catch multiple `\\n` symbols at the and of\n    # the section. For this reason we need to get rid of them.\n    return full_section_text.rstrip()\n\n\ndef parse_variables_from_docs(instances):\n    """"""\n    Parse documentation with NumPy style and returns all\n    extracted information.\n\n    Parameters\n    ----------\n    instances : list\n        List of objects that has documentations.\n\n    Returns\n    -------\n    dict\n        Variables parsed from the documentations.\n    """"""\n    variables = {}\n    # Note: We do not include \'Examples\' section because it\n    # includes class/function name which will be useless when\n    # we inherit documentation for the new object.\n    doc_sections = [\'Warns\', \'Returns\', \'Yields\', \'Raises\', \'See Also\',\n                    \'Parameters\', \'Attributes\', \'Methods\', \'Notes\']\n\n    if not instances:\n        return variables\n\n    for instance in instances:\n        parent_docs = instance.__doc__\n\n        if parent_docs is None:\n            continue\n\n        parent_variables = AttributeKeyDict()\n        parent_variables.update(iter_doc_parameters(parent_docs))\n        parent_variables.update(iter_doc_methods(parent_docs))\n\n        for section_name in doc_sections:\n            full_section = parse_full_section(section_name, parent_docs)\n\n            if full_section is not None:\n                parent_variables[section_name] = full_section\n\n        parent_name = instance.__name__\n        variables[parent_name] = parent_variables\n\n    return variables\n\n\ndef format_docs(instance, parent_instances):\n    """"""\n    Format instance\'s documentation.\n\n    Parameters\n    ----------\n    instance : object\n        Any object that has documentation.\n    parent_instances : list\n        List of object that has documentations. Function will\n        extract all information from theirs documentations and\n        it will use them to format main instance documentation.\n\n    Returns\n    -------\n    str\n        Formatted documentation.\n\n    Raises\n    ------\n    SharedDocsException\n        If function cannot format documentation properly.\n    """"""\n    try:\n        instance_docs = instance.__doc__\n        variables = parse_variables_from_docs(parent_instances)\n        return instance_docs.format(**variables)\n\n    except Exception as exception:\n        exception_classname = exception.__class__.__name__\n        raise SharedDocsException(\n            ""Can\'t format documentation for `{}` object. ""\n            ""Catched `{}` exception with message: {}"".format(\n                instance.__name__,\n                exception_classname,\n                exception\n            )\n        )\n\n\nclass SharedDocsException(Exception):\n    """"""\n    Exception that help identify problems related to shared\n    documentation.\n    """"""\n\n\ndef has_docs(value):\n    """""" Checks whether object has documentation.\n\n    Parameters\n    ----------\n    value : object\n\n    Returns\n    -------\n    bool\n        Function returns ``True`` if object has a documentation\n        and ``False`` otherwise.\n    """"""\n    return value.__doc__ is not None\n\n\ndef inherit_docs_for_methods(class_, attrs):\n    """"""\n    Class methods inherit documentation from the parent\n    classes in case if methods doesn\'t have it.\n\n    Parameters\n    ----------\n    class_ : object\n    attrs : dict\n        Class attributes.\n    """"""\n    for attrname, attrvalue in attrs.items():\n        if not isfunction(attrvalue) or has_docs(attrvalue):\n            continue\n\n        for parent_class in class_.__mro__:\n            if not hasattr(parent_class, attrname):\n                continue\n\n            parent_attrvalue = getattr(parent_class, attrname)\n            if has_docs(parent_attrvalue):\n                attrvalue.__doc__ = parent_attrvalue.__doc__\n                break\n\n\nclass SharedDocsMeta(type):\n    """"""\n    Meta-class for shared documentation. This class contains\n    main functionality that help inherit parameters and methods\n    descriptions from parent classes. This class automatically\n    format class documentation using basic python format syntax\n    for objects.\n    """"""\n    def __new__(cls, clsname, bases, attrs):\n        new_class = super(SharedDocsMeta, cls).__new__(\n            cls, clsname, bases, attrs)\n\n        if attrs.get(\'inherit_method_docs\', True):\n            inherit_docs_for_methods(new_class, attrs)\n\n        if new_class.__doc__ is None:\n            return new_class\n\n        class_docs = new_class.__doc__\n        n_indents = find_numpy_doc_indent(class_docs)\n\n        if n_indents is not None:\n            new_class.__doc__ = format_docs(new_class, new_class.__mro__)\n\n        return new_class\n\n\nclass SharedDocsABCMeta(SharedDocsMeta, ABCMeta):\n    """"""\n    Meta-class that combine ``SharedDocsMeta`` and ``ABCMeta``\n    meta-classes.\n    """"""\n\n\nclass SharedDocs(with_metaclass(SharedDocsMeta)):\n    """"""\n    Main class that provide with shared documentation\n    functionality.\n\n    Attributes\n    ----------\n    inherit_method_docs : bool\n        ``True`` means that methods that doesn\'t have\n        documentation will be inherited from the parent\n        methods. ``False`` will disable this option for\n        the specified class. Defaults to ``True``.\n    """"""\n    inherit_method_docs = True\n\n\ndef shared_docs(parent_function):\n    """"""\n    Decorator shares documentation between functions.\n\n    Parameters\n    ----------\n    parent_function : object\n        Any object that has documentation.\n    """"""\n    def decorator(function):\n        function.__doc__ = format_docs(function, [parent_function])\n        return function\n    return decorator\n'"
neupy/core/logs.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function, unicode_literals\n\nimport os\nimport sys\n\nfrom .config import Configurable\nfrom .properties import BaseProperty\n\n\n__all__ = (\'Verbose\',)\n\n\ndef is_color_supported():\n    """"""\n    Returns ``True`` if the running system\'s terminal supports\n    color, and ``False`` otherwise.\n\n    Notes\n    -----\n    Code from Djano: https://github.com/django/django/blob/\\\n    master/django/core/management/color.py\n\n    Returns\n    -------\n    bool\n    """"""\n    supported_platform = (\n        sys.platform != \'Pocket PC\' and\n        (sys.platform != \'win32\' or \'ANSICON\' in os.environ)\n    )\n\n    # isatty is not always implemented\n    is_a_tty = hasattr(sys.stdout, \'isatty\') and sys.stdout.isatty()\n    is_support = (supported_platform and is_a_tty)\n    return is_support\n\n\ndef create_style(ansi_code, use_bright_mode=False):\n    """"""\n    Create style based on ANSI code number.\n\n    Parameters\n    ----------\n    ansi_code : int\n        ANSI style code.\n\n    Returns\n    -------\n    function\n        Function that takes string argument and add ANDI styles\n        if its possible.\n    """"""\n    def style(text):\n        if is_color_supported():\n            mode = int(use_bright_mode)\n            return ""\\033[{};{}m{}\\033[0m"".format(mode, ansi_code, text)\n        return text\n    return style\n\n\nclass TerminalLogger(object):\n    """"""\n    Customized logging class that replace standard logging\n    functionality.\n\n    Attributes\n    ----------\n    enable : bool\n        Enable/disable logging output. Defaults to ``True``.\n\n    template : str\n        Terminal output message template. Defaults\n        to ``""[{name}] {text}""``.\n\n    stdout : object\n        Writes output in terminal. Defaults to ``sys.stdout``.\n    """"""\n    colors = {\n        \'gray\': create_style(ansi_code=37),\n        \'green\': create_style(ansi_code=32),\n        \'red\': create_style(ansi_code=31),\n        \'white\': create_style(ansi_code=37),\n    }\n    styles = {\n        \'bold\': create_style(ansi_code=1, use_bright_mode=True),\n        \'underline\': create_style(ansi_code=4, use_bright_mode=True),\n    }\n\n    def __init__(self, enable=True):\n        self.enable = enable\n        self.template = ""[{tag}] {text}""\n        self.stdout = sys.stdout\n\n    def write(self, text):\n        """"""\n        Method writes text in terminal if logging is enable.\n\n        Parameters\n        ----------\n        text : str\n        """"""\n        if self.enable:\n            self.stdout.write(text)\n            self.stdout.write(\'\\n\')\n\n    def newline(self):\n        self.write(\'\\r\')\n\n    def message(self, tag, text, color=\'green\'):\n        """"""\n        Methods writes message in terminal using specific template.\n        Each row should have tag and text. Tag identifies message\n        category and text information related to this category.\n\n        Parameters\n        ----------\n        name : str\n        text : str\n        color : {{\'green\', \'gray\', \'red\', \'white\'}}\n            Property that color text defined as ``tag`` parameter.\n            Defaults to ``green``.\n        """"""\n        if color not in self.colors:\n            available_colors = \', \'.join(self.colors.keys())\n            raise ValueError(\n                ""Invalid color `{}`. Available colors: {}""\n                """".format(color, available_colors))\n\n        colorizer = self.colors[color]\n        formated_tag = colorizer(tag.upper())\n        message = self.template.format(tag=formated_tag, text=text)\n        self.write(message)\n\n    def title(self, text):\n        """"""\n        Method write text as a title message. Text will be displayed\n        using bold and underline text styles. Also there will be empty\n        lines before and after the message.\n\n        Parameters\n        ----------\n        text : str\n        """"""\n        bold = self.styles[\'bold\']\n        underline = self.styles[\'underline\']\n        self.write(""\\n{text}\\n"".format(text=underline(bold(text))))\n\n    def __reduce__(self):\n        return (self.__class__, (self.enable,))\n\n\nclass VerboseProperty(BaseProperty):\n    """"""\n    Property that synchronize updates with ``enable`` attribute in\n    logging instance.\n\n    Parameters\n    ----------\n    {BaseProperty.default}\n    {BaseProperty.required}\n    """"""\n    expected_type = bool\n\n    def __set__(self, instance, value):\n        instance.logs.enable = value\n        return super(VerboseProperty, self).__set__(instance, value)\n\n\nclass Verbose(Configurable):\n    """"""\n    Class that controls NeuPy logging.\n\n    Parameters\n    ----------\n    verbose : bool\n        Property controls verbose output in terminal. The ``True`` value\n        enables informative output in the terminal and ``False`` - disable\n        it. Defaults to ``False``.\n\n    Attributes\n    ----------\n    logs : TerminalLogger\n        ``TerminalLogger`` instance.\n    """"""\n    verbose = VerboseProperty(default=False)\n\n    def __init__(self, **options):\n        self.logs = TerminalLogger()\n        self.logs.enable = self.verbose\n        super(Verbose, self).__init__(**options)\n'"
neupy/core/properties.py,3,"b'import numbers\nimport inspect\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy import init\nfrom neupy.utils import as_tuple\nfrom neupy.core.docs import SharedDocs\n\n\n__all__ = (\n    \'BaseProperty\', \'Property\', \'ArrayProperty\', \'BoundedProperty\',\n    \'ProperFractionProperty\', \'NumberProperty\', \'IntProperty\',\n    \'TypedListProperty\', \'ChoiceProperty\', \'WithdrawProperty\',\n    \'ParameterProperty\', \'FunctionWithOptionsProperty\',\n)\n\n\nnumber_type = (int, float, np.floating, np.integer)\n\n\nclass BaseProperty(SharedDocs):\n    """"""\n    Base class for properties.\n\n    Parameters\n    ----------\n    default : object\n        Default property value. Defaults to ``None``.\n\n    required : bool\n        If parameter equal to ``True`` and value is not defined\n        after initialization then exception will be triggered.\n        Defaults to ``False``.\n\n    allow_none : bool\n        When value is equal to ``True`` than ``None`` is a valid\n        value for the parameter. Defaults to ``False``.\n\n    Attributes\n    ----------\n    name : str or None\n        Name of the property. ``None`` in case if name\n        wasn\'t specified.\n\n    expected_type : tuple or object\n        Expected data types of the property.\n    """"""\n    expected_type = object\n\n    def __init__(self, default=None, required=False, allow_none=False):\n        self.name = None\n        self.default = default\n        self.required = required\n        self.allow_none = allow_none\n\n        if allow_none:\n            self.expected_type = as_tuple(self.expected_type, type(None))\n\n    def __set__(self, instance, value):\n        if not self.allow_none or value is not None:\n            self.validate(value)\n\n        instance.__dict__[self.name] = value\n\n    def __get__(self, instance, owner):\n        if instance is None:\n            return\n\n        if self.default is not None and self.name not in instance.__dict__:\n            self.__set__(instance, self.default)\n\n        return instance.__dict__.get(self.name, None)\n\n    def validate(self, value):\n        """"""\n        Validate properties value\n\n        Parameters\n        ----------\n        value : object\n        """"""\n        if not isinstance(value, self.expected_type):\n            availabe_types = as_tuple(self.expected_type)\n            availabe_types = \', \'.join(t.__name__ for t in availabe_types)\n            dtype = value.__class__.__name__\n\n            raise TypeError(\n                ""Invalid data type `{0}` for `{1}` property. ""\n                ""Expected types: {2}"".format(dtype, self.name, availabe_types))\n\n    def __repr__(self):\n        classname = self.__class__.__name__\n\n        if self.name is None:\n            return \'{}()\'.format(classname)\n\n        return \'{}(name=""{}"")\'.format(classname, self.name)\n\n\nclass WithdrawProperty(object):\n    """"""\n    Defines inherited property that needs to be withdrawn.\n\n    Attributes\n    ----------\n    name : str or None\n        Name of the property. ``None`` in case if name\n        wasn\'t specified.\n    """"""\n    def __get__(self, instance, owner):\n        # Remove itself, to make sure that instance doesn\'t\n        # have reference to this property. Instead user should\n        # be able to see default value from the parent classes,\n        # but not allowed to assign different value in __init__\n        # method.\n        #\n        # Other part of functionality defined in the\n        # ``ConfigMeta`` class.\n        del self\n\n\nclass Property(BaseProperty):\n    """"""\n    Simple and flexible class that helps identity properties with\n    specified type.\n\n    Parameters\n    ----------\n    expected_type : object\n        Valid data types.\n\n    {BaseProperty.Parameters}\n    """"""\n    def __init__(self, expected_type=object, *args, **kwargs):\n        self.expected_type = expected_type\n        super(Property, self).__init__(*args, **kwargs)\n\n\nclass ArrayProperty(BaseProperty):\n    """"""\n    Numpy array or matrix property.\n\n    Parameters\n    ----------\n    {BaseProperty.Parameters}\n    """"""\n    expected_type = (np.ndarray, np.matrix)\n\n\nclass TypedListProperty(BaseProperty):\n    """"""\n    List property that contains specified element types.\n\n    Parameters\n    ----------\n    n_elements : int\n        Number of elements in the list. The ``None``\n        value mean that list can contains any number of\n        elements. Defaults to ``None``.\n\n    element_type : object or tuple\n        Type of the elements within the list.\n\n    {BaseProperty.Parameters}\n    """"""\n    expected_type = (list, tuple)\n\n    def __init__(self, n_elements=None, element_type=int, *args, **kwargs):\n        self.n_elements = n_elements\n        self.element_type = element_type\n        super(TypedListProperty, self).__init__(*args, **kwargs)\n\n    def validate(self, value):\n        super(TypedListProperty, self).validate(value)\n\n        if self.n_elements is not None and len(value) != self.n_elements:\n            raise ValueError(\n                ""Expected list with {} variables"".format(self.n_elements))\n\n        if not all(isinstance(v, self.element_type) for v in value):\n            element_type = as_tuple(self.element_type)\n            type_names = (type_.__name__ for type_ in element_type)\n            element_type_name = \', \'.join(type_names)\n\n            raise TypeError(\n                ""The `{}` parameter received invalid element types ""\n                ""in list/tuple. Expected element types: {}, Value: {}""\n                """".format(self.name, element_type_name, value))\n\n\nclass ChoiceProperty(BaseProperty):\n    """"""\n    Property that can have discrete number of properties.\n\n    Parameters\n    ----------\n    choices : list, tuple or dict\n        Stores all possible choices. Defines list of possible\n        choices. If value specified as a dictionary than key\n        would be just an alias to the expected value.\n\n    {BaseProperty.Parameters}\n    """"""\n    choices = {}\n\n    def __init__(self, choices, *args, **kwargs):\n        super(ChoiceProperty, self).__init__(*args, **kwargs)\n        self.choices = choices\n\n        if isinstance(choices, (list, tuple, set)):\n            self.choices = dict(zip(choices, choices))\n\n        if not isinstance(self.choices, dict):\n            class_name = self.choices.__class__.__name__\n\n            raise ValueError(\n                ""Choice properties can be only a `dict`, got ""\n                ""`{0}`"".format(class_name))\n\n        if not self.choices:\n            raise ValueError(\n                ""Must be at least one choice in property ""\n                ""`{0}`"".format(self.name))\n\n    def __set__(self, instance, value):\n        if value in self.choices:\n            return super(ChoiceProperty, self).__set__(instance, value)\n\n        possible_choices = "", "".join(self.choices.keys())\n        raise ValueError(\n            ""Wrong value `{0}` for property `{1}`. Available values: ""\n            ""{2}"".format(value, self.name, possible_choices)\n        )\n\n    def __get__(self, instance, owner):\n        if instance is None:\n            return\n\n        choice_key = super(ChoiceProperty, self).__get__(instance, owner)\n        return self.choices[choice_key]\n\n\nclass BoundedProperty(BaseProperty):\n    """"""\n    Number property that have specified numerical bounds.\n\n    Parameters\n    ----------\n    minval : float\n        Minimum possible value for the property.\n\n    maxval : float\n        Maximum possible value for the property.\n\n    {BaseProperty.Parameters}\n    """"""\n\n    def __init__(self, minval=-np.inf, maxval=np.inf, *args, **kwargs):\n        self.minval = minval\n        self.maxval = maxval\n        super(BoundedProperty, self).__init__(*args, **kwargs)\n\n    def validate(self, value):\n        super(BoundedProperty, self).validate(value)\n\n        if not (self.minval <= value <= self.maxval):\n            raise ValueError(\n                ""Value `{}` should be between {} and {}""\n                """".format(self.name, self.minval, self.maxval))\n\n\nclass ProperFractionProperty(BoundedProperty):\n    """"""\n    Proper fraction property. Identify all possible numbers\n    between zero and one.\n\n    Parameters\n    ----------\n    {BaseProperty.Parameters}\n    """"""\n    expected_type = (float, int)\n\n    def __init__(self, *args, **kwargs):\n        super(ProperFractionProperty, self).__init__(\n            minval=0, maxval=1, *args, **kwargs)\n\n\nclass NumberProperty(BoundedProperty):\n    """"""\n    Float or integer number property.\n\n    Parameters\n    ----------\n    {BoundedProperty.Parameters}\n    """"""\n    expected_type = number_type\n\n\nclass IntProperty(BoundedProperty):\n    """"""\n    Integer property.\n\n    Parameters\n    ----------\n    {BoundedProperty.Parameters}\n    """"""\n    expected_type = (numbers.Integral, np.integer)\n\n    def __set__(self, instance, value):\n        if isinstance(value, float) and value.is_integer():\n            value = int(value)\n        super(IntProperty, self).__set__(instance, value)\n\n\nclass ParameterProperty(ArrayProperty):\n    """"""\n    In addition to Numpy arrays and matrix property support also\n    Tensorfow variables and NeuPy Initializers.\n\n    Parameters\n    ----------\n    {ArrayProperty.Parameters}\n    """"""\n    expected_type = as_tuple(\n        np.ndarray,\n        number_type,\n        init.Initializer,\n        tf.Variable,\n        tf.Tensor,\n    )\n\n    def __set__(self, instance, value):\n        if isinstance(value, number_type):\n            value = init.Constant(value)\n        super(ParameterProperty, self).__set__(instance, value)\n\n\nclass FunctionWithOptionsProperty(ChoiceProperty):\n    """"""\n    Property that helps select error function from\n    available or define a new one.\n\n    Parameters\n    ----------\n    {ChoiceProperty.Parameters}\n    """"""\n    def __set__(self, instance, value):\n        if inspect.isfunction(value):\n            return super(ChoiceProperty, self).__set__(instance, value)\n\n        return super(FunctionWithOptionsProperty, self).__set__(\n            instance, value)\n\n    def __get__(self, instance, value):\n        founded_value = super(ChoiceProperty, self).__get__(instance, value)\n\n        if inspect.isfunction(founded_value):\n            return founded_value\n\n        return super(FunctionWithOptionsProperty, self).__get__(\n            instance, founded_value)\n\n\nclass ScalarVariableProperty(BaseProperty):\n    expected_type = as_tuple(tf.Variable, number_type)\n'"
neupy/datasets/__init__.py,0,b'from .reber import *\nfrom .digits import *\n'
neupy/datasets/digits.py,0,"b'import numpy as np\n\n\n__all__ = (\'make_digits\', \'load_digits\')\n\n\ndigits_data = np.array([\n    [\n        # zero\n        0, 1, 1, 0,\n        1, 0, 0, 1,\n        1, 0, 0, 1,\n        1, 0, 0, 1,\n        1, 0, 0, 1,\n        0, 1, 1, 0,\n    ], [\n        # one\n        0, 1, 1, 0,\n        0, 0, 1, 0,\n        0, 0, 1, 0,\n        0, 0, 1, 0,\n        0, 0, 1, 0,\n        0, 0, 1, 0,\n    ], [\n        # two\n        1, 1, 1, 0,\n        0, 0, 0, 1,\n        0, 0, 0, 1,\n        0, 1, 1, 0,\n        1, 0, 0, 0,\n        1, 1, 1, 1,\n    ], [\n        # three\n        1, 1, 1, 0,\n        0, 0, 0, 1,\n        0, 1, 1, 0,\n        0, 0, 0, 1,\n        0, 0, 0, 1,\n        1, 1, 1, 0,\n    ], [\n        # four\n        0, 0, 1, 1,\n        0, 1, 0, 1,\n        1, 0, 0, 1,\n        1, 1, 1, 1,\n        0, 0, 0, 1,\n        0, 0, 0, 1,\n    ], [\n        # five\n        0, 1, 1, 1,\n        1, 0, 0, 0,\n        0, 1, 1, 0,\n        0, 0, 0, 1,\n        0, 0, 0, 1,\n        1, 1, 1, 0,\n    ], [\n        # six\n        0, 1, 1, 0,\n        1, 0, 0, 0,\n        1, 1, 1, 0,\n        1, 0, 0, 1,\n        1, 0, 0, 1,\n        0, 1, 1, 0,\n    ], [\n        # seven\n        0, 1, 1, 1,\n        0, 0, 0, 1,\n        0, 0, 0, 1,\n        0, 0, 1, 0,\n        0, 1, 0, 0,\n        0, 1, 0, 0,\n    ], [\n        # eight\n        0, 1, 1, 0,\n        1, 0, 0, 1,\n        0, 1, 1, 0,\n        1, 0, 0, 1,\n        1, 0, 0, 1,\n        0, 1, 1, 0,\n    ], [\n        # nine\n        0, 1, 1, 0,\n        1, 0, 0, 1,\n        1, 0, 0, 1,\n        0, 1, 1, 1,\n        0, 0, 0, 1,\n        0, 1, 1, 0,\n    ]\n], dtype=np.float32)\ndigits_labels = np.arange(10)\n\n\ndef load_digits():\n    """"""\n    Returns dataset that contains discrete digits.\n\n    Returns\n    -------\n    tuple\n        Tuple contains two values. First one is a matrix\n        with shape (10, 24). Second one is a vector that contains\n        labels for each row. Each digit can be transformed\n        into (6, 4) binary image.\n    """"""\n    return digits_data, digits_labels\n\n\ndef make_digits(n_samples=100, noise_level=0.1, mode=\'flip\'):\n    """"""\n    Returns discrete digits dataset.\n\n    Parameters\n    ----------\n    n_samples : int\n        Number of samples. Defaults to ``100``.\n\n    noise_level : float\n        Defines level of a discrete noise added to the images.\n        Noise level defines probability for the pixel\n        to be removed. Value should be in [0, 1) range.\n        Defaults to ``0.1``.\n\n    mode : {``remove``, ``flip``}\n        This option allow to specify how additional noise will\n        modify each image.\n\n        - ``flip`` - Per every randomly selected pixel function flips\n          binary value. ``1 -> 0`` and ``0 -> 1``.\n\n        - ``remove`` - Per every randomly selected pixel function checks\n          if value equal to ``1`` if it\'s true that it gets replaced\n          with ``0``.\n\n    Returns\n    -------\n    tuple\n        Tuple contains two values. First one is a matrix\n        with shape (n_samples, 24). Second one is a vector\n        that contains labels for each row. Each digit can\n        be transformed into (6, 4) binary image.\n\n    Examples\n    --------\n    >>> from neupy import datasets, utils\n    >>>\n    >>> utils.reproducible()\n    >>>\n    >>> digits, labels = datasets.make_digits(noise_level=0.15)\n    >>> digit, label = digits[0], labels[0]\n    >>>\n    >>> label\n    5\n    >>>\n    >>> digit.reshape((6, 4))\n    array([[0, 0, 1, 1],\n           [1, 0, 0, 0],\n           [0, 1, 1, 0],\n           [0, 0, 0, 0],\n           [0, 0, 0, 1],\n           [1, 1, 1, 0]], dtype=uint8)\n    """"""\n    if mode not in {\'remove\', \'flip\'}:\n        raise ValueError(""Unknown mode: {}"".format(mode))\n\n    if not 0 <= noise_level < 1:\n        raise ValueError(""noise_level should be float number ""\n                         ""from [0, 1) range, got {}"".format(noise_level))\n\n    if n_samples < 1:\n        raise ValueError(""Number of samples should be an integer greater ""\n                         ""or equal to 1, got {}"".format(n_samples))\n\n    digit_indices = np.random.randint(10, size=n_samples)\n    digit_images = digits_data[digit_indices]\n    digit_labels = digits_labels[digit_indices]\n\n    mask = np.random.binomial(\n        n=1, p=noise_level, size=digit_images.shape).astype(bool)\n\n    if mode == \'flip\':\n        digit_images[mask] = np.where(digit_images[mask] == 1, 0, 1)\n\n    elif mode == \'remove\':\n        digit_images[mask] = 0\n\n    return digit_images, digit_labels\n'"
neupy/datasets/reber.py,0,"b'# -*- coding: utf-8 -*-\nimport math\nfrom random import choice, randint\n\nimport numpy as np\n\n\n__all__ = (\'make_reber\', \'is_valid_by_reber\', \'make_reber_classification\')\n\n\navaliable_letters = \'TVPXS\'\nreber_rules = {\n    0: [(\'T\', 1), (\'V\', 2)],\n    1: [(\'P\', 1), (\'T\', 3)],\n    2: [(\'X\', 2), (\'V\', 4)],\n    3: [(\'X\', 2), (\'S\', None)],\n    4: [(\'P\', 3), (\'S\', None)],\n}\n\n\ndef is_valid_by_reber(word):\n    """"""\n    \xd0\xa1hecks whether a word belongs to grammar Reber.\n\n    Parameters\n    ----------\n    word : str or list of letters\n        The word that you want to test.\n\n    Returns\n    -------\n    bool\n        ``True`` if word valid by Reber grammar and\n        ``False`` otherwise.\n\n    Examples\n    --------\n    >>> from neupy.datasets import is_valid_by_reber\n    >>>\n    >>> is_valid_by_reber(\'TTS\')\n    True\n    >>> is_valid_by_reber(\'STS\')\n    False\n    >>>\n    >>> is_valid_by_reber([\'T\', \'T\', \'S\'])\n    True\n    >>> is_valid_by_reber([\'S\', \'T\', \'S\'])\n    False\n    """"""\n    if not word.endswith(""S""):\n        return False\n\n    position = 0\n    for letter in word:\n        possible_letters = reber_rules[position]\n        letters = [step[0] for step in possible_letters]\n        if letter not in letters:\n            return False\n        _, position = possible_letters[letters.index(letter)]\n    return True\n\n\ndef make_reber(n_words=100):\n    """"""\n    Generate list of words valid by Reber grammar.\n\n    Parameters\n    ----------\n    n_words : int\n        Number of reber words, defaults to ``100``.\n\n    Returns\n    -------\n    list\n        List of Reber words.\n\n    Examples\n    --------\n    >>> from neupy.datasets import make_reber\n    >>> make_reber(4)\n    [\'TPTXVS\', \'VXXVS\', \'TPPTS\', \'TTXVPXXVS\']\n    """"""\n    if n_words < 1:\n        raise ValueError(""Must be at least one word"")\n\n    words = []\n    for i in range(n_words):\n        position = 0\n        word = []\n\n        while position is not None:\n            possible_letters = reber_rules[position]\n            letter, position = choice(possible_letters)\n            word.append(letter)\n\n        words.append(\'\'.join(word))\n    return words\n\n\ndef convert_letters_to_indices(samples):\n    """"""\n    Convert Reber Grammar words to the list of indices where\n    each index referes to specific letter.\n\n    Parameters\n    ----------\n    samples : list of str\n        List of words.\n\n    Examples\n    --------\n    >>> convert_letters_to_indices([\'XXXXVTTSSV\', \'VXXVS\'])\n    array([array([3, 3, 3, 3, 1, 0, 0, 4, 4, 1]),\n           array([1, 3, 3, 1, 4])], dtype=object)\n    """"""\n    index_samples = []\n    for sample in samples:\n        word = [avaliable_letters.index(letter) for letter in sample]\n        index_samples.append(np.array(word))\n    return np.array(index_samples)\n\n\ndef make_reber_classification(n_samples, invalid_size=0.5, lenrange=(3, 14),\n                              return_indices=False):\n    """"""\n    Generate random dataset for Reber grammar classification.\n    Invalid words contains the same letters as at Reber grammar, but\n    they are build without grammar rules.\n\n    Parameters\n    ----------\n    n_samples : int\n        Number of samples in dataset.\n\n    invalid_size : float\n        Proportion of invalid words in dataset, defaults to ``0.5``.\n        Value must be between ``0`` and ``1``.\n\n    lenrange : tuple\n        Length of each word will be bounded by the two numbers\n        specified in this range. Defaults to ``(3, 14)``.\n\n    return_indices : bool\n        If ``True``, each word will be converted to array where each\n        letter converted to the index. Defaults to ``False``.\n\n    Returns\n    -------\n    tuple\n        Return two lists. First contains words and second - labels for them.\n\n    Examples\n    --------\n    >>> from neupy.datasets import make_reber_classification\n    >>>\n    >>> data, labels = make_reber_classification(10, invalid_size=0.5)\n    >>> data\n    array([\'SXSXVSXXVX\', \'VVPS\', \'VVPSXTTS\', \'VVS\', \'VXVS\', \'VVS\',\n           \'PPTTTXPSPTV\', \'VTTSXVPTXVXT\', \'VSSXSTX\', \'TTXVS\'],\n          dtype=\'<U12\')\n    >>> labels\n    array([0, 1, 0, 1, 1, 1, 0, 0, 0, 1])\n    >>>\n    >>> data, labels = make_reber_classification(\n    ...     4, invalid_size=0.5, return_indices=True)\n    >>> data\n    array([array([1, 3, 1, 4]),\n           array([0, 3, 0, 3, 0, 4, 3, 0, 4, 4]),\n           array([1, 3, 1, 2, 3, 1, 2, 4]),\n           array([0, 3, 0, 0, 3, 0, 4, 2, 4, 1, 0, 4, 0])], dtype=object)\n    """"""\n    if n_samples < 2:\n        raise ValueError(""There are must be at least 2 samples"")\n\n    if not 0 < invalid_size < 1:\n        raise ValueError(""`invalid_size` argument value must be between ""\n                         ""zero and one, got {}"".format(invalid_size))\n\n    n_valid_words = int(math.ceil(n_samples * invalid_size))\n    n_invalid_words = n_samples - n_valid_words\n\n    valid_words = make_reber(n_valid_words)\n    valid_labels = [1] * n_valid_words\n\n    invalid_words = []\n    invalid_labels = [0] * n_valid_words\n\n    for i in range(n_invalid_words):\n        word_length = randint(*lenrange)\n        word = [choice(avaliable_letters) for _ in range(word_length)]\n        invalid_words.append(\'\'.join(word))\n\n    samples = np.array(valid_words + invalid_words)\n    labels = np.array(valid_labels + invalid_labels)\n\n    indices = np.arange(len(samples))\n    np.random.shuffle(indices)\n\n    samples, labels = samples[indices], labels[indices]\n\n    if return_indices:\n        samples = convert_letters_to_indices(samples)\n\n    return samples, labels\n'"
neupy/layers/__init__.py,0,b'from .base import *\nfrom .graph import *\nfrom .activations import *\nfrom .convolutions import *\nfrom .pooling import *\nfrom .stochastic import *\nfrom .normalization import *\nfrom .merge import *\nfrom .reshape import *\nfrom .embedding import *\nfrom .recurrent import *\n\n\n# Extra aliases for the layers\nMaxPool = MaxPooling\nAvgPool = AveragePooling\nGlobalPool = GlobalPooling\n\nConv = Convolution\nBN = BatchNorm\nGN = GroupNorm\n'
neupy/layers/activations.py,18,"b'import numpy as np\nimport tensorflow as tf\n\nfrom neupy import init\nfrom neupy.utils import asfloat, as_tuple, tf_utils\nfrom neupy.exceptions import LayerConnectionError, WeightInitializationError\nfrom neupy.core.properties import (\n    NumberProperty, TypedListProperty,\n    ParameterProperty, IntProperty,\n)\nfrom .base import BaseLayer\n\n\n__all__ = (\n    \'Linear\', \'Sigmoid\', \'Tanh\', \'Softmax\',\n    \'Relu\', \'LeakyRelu\', \'Elu\', \'PRelu\',\n    \'Softplus\', \'HardSigmoid\',\n)\n\n\nclass Linear(BaseLayer):\n    """"""\n    Layer with linear activation function. It applies linear transformation\n    when the ``n_units`` parameter specified and acts as an identity\n    when it\'s not specified.\n\n    Parameters\n    ----------\n    n_units : int or None\n        Number of units in the layers. It also corresponds to the number of\n        output features that will be produced per sample after passing it\n        through this layer. The ``None`` value means that layer will not have\n        parameters and it will only apply activation function to the input\n        without linear transformation output for the specified input value.\n        Defaults to ``None``.\n\n    weight : array-like, Tensorfow variable, scalar or Initializer\n        Defines layer\'s weights. Default initialization methods\n        you can find :ref:`here <init-methods>`.\n        Defaults to :class:`HeNormal() <neupy.init.HeNormal>`.\n\n    bias : 1D array-like, Tensorfow variable, scalar, Initializer or None\n        Defines layer\'s bias. Default initialization methods you can find\n        :ref:`here <init-methods>`. Defaults to\n        :class:`Constant(0) <neupy.init.Constant>`.\n        The ``None`` value excludes bias from the calculations and\n        do not add it into parameters list.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    activation_function(input)\n        Applies activation function to the input.\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n    Linear Regression\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Linear(5)\n    """"""\n    n_units = IntProperty(minval=1, allow_none=True)\n    weight = ParameterProperty()\n    bias = ParameterProperty(allow_none=True)\n\n    def __init__(self, n_units=None, weight=init.HeNormal(), bias=0,\n                 name=None):\n\n        super(Linear, self).__init__(name=name)\n\n        self.n_units = n_units\n        self.weight = weight\n        self.bias = bias\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n\n        if self.n_units is None:\n            return input_shape\n\n        if input_shape and input_shape.ndims != 2:\n            raise LayerConnectionError(\n                ""Input shape expected to have 2 dimensions, got {} instead. ""\n                ""Shape: {}"".format(input_shape.ndims, input_shape))\n\n        n_samples = input_shape[0]\n        return tf.TensorShape((n_samples, self.n_units))\n\n    def create_variables(self, input_shape):\n        if self.n_units is None:\n            return\n\n        input_shape = tf.TensorShape(input_shape)\n        self.input_shape = input_shape\n        _, n_input_features = input_shape\n\n        if n_input_features.value is None:\n            raise WeightInitializationError(\n                ""Cannot create variables for the layer `{}`, because ""\n                ""number of input features is unknown. Input shape: {}""\n                ""Layer: {}"".format(self.name, input_shape, self))\n\n        self.weight = self.variable(\n            value=self.weight, name=\'weight\',\n            shape=as_tuple(n_input_features, self.n_units))\n\n        if self.bias is not None:\n            self.bias = self.variable(\n                value=self.bias, name=\'bias\',\n                shape=as_tuple(self.n_units))\n\n    def output(self, input, **kwargs):\n        input = tf.convert_to_tensor(input, dtype=tf.float32)\n\n        if self.n_units is None:\n            return self.activation_function(input)\n\n        if self.bias is None:\n            output = tf.matmul(input, self.weight)\n            return self.activation_function(output)\n\n        output = tf.matmul(input, self.weight) + self.bias\n        return self.activation_function(output)\n\n    def activation_function(self, input_value):\n        return input_value\n\n    def __repr__(self):\n        if self.n_units is None:\n            return self._repr_arguments(name=self.name)\n\n        return self._repr_arguments(\n            self.n_units,\n            name=self.name,\n            weight=self.weight,\n            bias=self.bias,\n        )\n\n\nclass Sigmoid(Linear):\n    """"""\n    Layer with the sigmoid used as an activation function. It applies\n    linear transformation when the ``n_units`` parameter specified and\n    sigmoid function after the transformation. When ``n_units`` is not\n    specified, only sigmoid function will be applied to the input.\n\n    Parameters\n    ----------\n    {Linear.Parameters}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Examples\n    --------\n    Logistic Regression (LR)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Sigmoid(1)\n\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Sigmoid(5) >> Sigmoid(1)\n\n    Convolutional Neural Networks (CNN) for Semantic Segmentation\n\n    Sigmoid layer can be used in order to normalize probabilities\n    per pixel in semantic classification task with two classes.\n    In the example below, we have as input 32x32 image that predicts\n    one of the two classes. Sigmoid normalizes raw predictions per pixel\n    to the valid probabilities.\n\n    >>> from neupy.layers import *\n    >>> network = Input((32, 32, 1)) >> Sigmoid()\n    """"""\n    def activation_function(self, input_value):\n        return tf.nn.sigmoid(input_value)\n\n\nclass HardSigmoid(Linear):\n    """"""\n    Layer with the hard sigmoid used as an activation function. It applies\n    linear transformation when the ``n_units`` parameter specified and\n    hard sigmoid function after the transformation. When ``n_units`` is\n    not specified, only hard sigmoid function will be applied to the input.\n\n    Parameters\n    ----------\n    {Linear.Parameters}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Examples\n    --------\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> HardSigmoid(5)\n    """"""\n    def activation_function(self, input_value):\n        input_value = (0.2 * input_value) + 0.5\n        return tf.clip_by_value(input_value, 0., 1.)\n\n\nclass Tanh(Linear):\n    """"""\n    Layer with the hyperbolic tangent used as an activation function.\n    It applies linear transformation when the ``n_units`` parameter\n    specified and ``tanh`` function after the transformation. When\n    ``n_units`` is not specified, only ``tanh`` function will be applied\n    to the input.\n\n    Parameters\n    ----------\n    {Linear.Parameters}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Examples\n    --------\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Tanh(5)\n    """"""\n    def activation_function(self, input_value):\n        return tf.nn.tanh(input_value)\n\n\nclass Relu(Linear):\n    """"""\n    Layer with the rectifier (ReLu) used as an activation function.\n    It applies linear transformation when the ``n_units`` parameter\n    specified and ``relu`` function after the transformation. When\n    ``n_units`` is not specified, only ``relu`` function will be applied\n    to the input.\n\n    Parameters\n    ----------\n    {Linear.n_units}\n\n    alpha : float\n        Alpha parameter defines the decreasing rate\n        for the negative values. If ``alpha``\n        is non-zero value then layer behave like a\n        leaky ReLu. Defaults to ``0``.\n\n    weight : array-like, Tensorfow variable, scalar or Initializer\n        Defines layer\'s weights. Default initialization methods\n        you can find :ref:`here <init-methods>`.\n        Defaults to :class:`HeNormal(gain=2) <neupy.init.HeNormal>`.\n\n    {Linear.bias}\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Examples\n    --------\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Relu(20) >> Relu(1)\n\n    Convolutional Neural Networks (CNN)\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((32, 32, 3)),\n    ...     Convolution((3, 3, 16)) >> Relu(),\n    ...     Convolution((3, 3, 32)) >> Relu(),\n    ...     Reshape(),\n    ...     Softmax(10),\n    ... )\n    """"""\n    alpha = NumberProperty(minval=0)\n\n    def __init__(self, n_units=None, alpha=0, weight=init.HeNormal(gain=2),\n                 bias=init.Constant(value=0), name=None):\n\n        self.alpha = alpha\n        super(Relu, self).__init__(\n            n_units=n_units, weight=weight, bias=bias, name=name)\n\n    def activation_function(self, input_value):\n        if self.alpha == 0:\n            return tf.nn.relu(input_value)\n        return tf.nn.leaky_relu(input_value, asfloat(self.alpha))\n\n    def __repr__(self):\n        if self.n_units is None:\n            return self._repr_arguments(name=self.name, alpha=self.alpha)\n\n        return self._repr_arguments(\n            self.n_units,\n            name=self.name,\n            alpha=self.alpha,\n            weight=self.weight,\n            bias=self.bias,\n        )\n\n\nclass LeakyRelu(Linear):\n    """"""\n    Layer with the leaky rectifier (Leaky ReLu) used as an activation\n    function. It applies linear transformation when the ``n_units``\n    parameter specified and leaky relu function after the transformation.\n    When ``n_units`` is not specified, only leaky relu function will be\n    applied to the input.\n\n    Parameters\n    ----------\n    {Linear.Parameters}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Notes\n    -----\n    Do the same as ``Relu(input_size, alpha=0.01)``.\n\n    Examples\n    --------\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> LeakyRelu(20) >> LeakyRelu(1)\n    """"""\n    def activation_function(self, input_value):\n        return tf.nn.leaky_relu(input_value, alpha=asfloat(0.01))\n\n\nclass Softplus(Linear):\n    """"""\n    Layer with the softplus used as an activation function. It applies linear\n    transformation when the ``n_units`` parameter specified and softplus\n    function after the transformation. When ``n_units`` is not specified,\n    only softplus function will be applied to the input.\n\n    Parameters\n    ----------\n    {Linear.Parameters}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Examples\n    --------\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Softplus(4)\n    """"""\n    def activation_function(self, input_value):\n        return tf.nn.softplus(input_value)\n\n\nclass Softmax(Linear):\n    """"""\n    Layer with the softmax activation function. It applies linear\n    transformation when the ``n_units`` parameter specified and softmax\n    function after the transformation. When ``n_units`` is not specified,\n    only softmax function will be applied to the input.\n\n    Parameters\n    ----------\n    {Linear.Parameters}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Examples\n    --------\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Relu(20) >> Softmax(10)\n\n    Convolutional Neural Networks (CNN) for Semantic Segmentation\n\n    Softmax layer can be used in order to normalize probabilities\n    per pixel. In the example below, we have as input 32x32 image\n    with raw prediction per each pixel for 10 different classes.\n    Softmax normalizes raw predictions per pixel to the probability\n    distribution.\n\n    >>> from neupy.layers import *\n    >>> network = Input((32, 32, 10)) >> Softmax()\n    """"""\n    def activation_function(self, input_value):\n        return tf.nn.softmax(input_value)\n\n\nclass Elu(Linear):\n    """"""\n    Layer with the exponential linear unit (ELU) used as an activation\n    function. It applies linear transformation when the ``n_units``\n    parameter specified and elu function after the transformation.\n    When ``n_units`` is not specified, only elu function will be\n    applied to the input.\n\n    Parameters\n    ----------\n    {Linear.Parameters}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Examples\n    --------\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Elu(5) >> Elu(1)\n\n    References\n    ----------\n    .. [1] http://arxiv.org/pdf/1511.07289v3.pdf\n    """"""\n    def activation_function(self, input_value):\n        return tf.nn.elu(input_value)\n\n\nclass PRelu(Linear):\n    """"""\n    Layer with the parametrized ReLu used as an activation function.\n    Layer learns additional parameter ``alpha`` during the training.\n\n    It applies linear transformation when the ``n_units`` parameter\n    specified and parametrized relu function after the transformation.\n    When ``n_units`` is not specified, only parametrized relu function\n    will be applied to the input.\n\n    Parameters\n    ----------\n    alpha_axes : int or tuple\n        Axes that will not include unique alpha parameter.\n        Single integer value defines the same as a tuple with one value.\n        Defaults to ``-1``.\n\n    alpha : array-like, Tensorfow variable, scalar or Initializer\n        Separate alpha parameter per each non-shared axis for the ReLu.\n        Scalar value means that each element in the tensor will be\n        equal to the specified value. Default initialization methods you\n        can find :ref:`here <init-methods>`.\n        Defaults to ``Constant(value=0.25)``.\n\n    {Linear.Parameters}\n\n    Methods\n    -------\n    {Linear.Methods}\n\n    Attributes\n    ----------\n    {Linear.Attributes}\n\n    Examples\n    --------\n    Feedforward Neural Networks (FNN)\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> PRelu(20) >> PRelu(1)\n\n    Convolutional Neural Networks (CNN)\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((32, 32, 3)),\n    ...     Convolution((3, 3, 16)) >> PRelu(),\n    ...     Convolution((3, 3, 32)) >> PRelu(),\n    ...     Reshape(),\n    ...     Softmax(10),\n    ... )\n\n    References\n    ----------\n    .. [1] Delving Deep into Rectifiers: Surpassing Human-Level\n           Performance on ImageNet Classification.\n           https://arxiv.org/pdf/1502.01852v1.pdf\n    """"""\n    alpha_axes = TypedListProperty()\n    alpha = ParameterProperty()\n\n    def __init__(self, n_units=None, alpha_axes=-1, alpha=0.25,\n                 weight=init.HeNormal(gain=2), bias=0, name=None):\n\n        self.alpha = alpha\n        self.alpha_axes = as_tuple(alpha_axes)\n\n        if 0 in self.alpha_axes:\n            raise ValueError(""Cannot specify alpha for 0-axis"")\n\n        super(PRelu, self).__init__(\n            n_units=n_units, weight=weight, bias=bias, name=name)\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n\n        if input_shape and max(self.alpha_axes) >= input_shape.ndims:\n            max_axis_index = input_shape.ndims - 1\n\n            raise LayerConnectionError(\n                ""Cannot specify alpha for the axis #{}. Maximum ""\n                ""available axis is {} (0-based indices).""\n                """".format(max(self.alpha_axes), max_axis_index))\n\n        return super(PRelu, self).get_output_shape(input_shape)\n\n    def create_variables(self, input_shape):\n        super(PRelu, self).create_variables(input_shape)\n        output_shape = self.get_output_shape(input_shape)\n\n        self.alpha = self.variable(\n            value=self.alpha, name=\'alpha\',\n            shape=[output_shape[axis] for axis in self.alpha_axes])\n\n    def activation_function(self, input):\n        input = tf.convert_to_tensor(input, dtype=tf.float32)\n        ndim = input.shape.ndims\n\n        dimensions = np.arange(ndim)\n        alpha_axes = dimensions[list(self.alpha_axes)]\n\n        alpha = tf_utils.dimshuffle(self.alpha, ndim, alpha_axes)\n        return tf.maximum(0.0, input) + alpha * tf.minimum(0.0, input)\n\n    def __repr__(self):\n        if self.n_units is None:\n            return self._repr_arguments(\n                name=self.name,\n                alpha_axes=self.alpha_axes,\n                alpha=self.alpha)\n\n        return self._repr_arguments(\n            self.n_units,\n            name=self.name,\n            alpha_axes=self.alpha_axes,\n            alpha=self.alpha,\n            weight=self.weight,\n            bias=self.bias)\n'"
neupy/layers/base.py,8,"b'import re\nimport copy\nimport types\nimport string\nimport inspect\nfrom functools import partial\nfrom collections import OrderedDict, defaultdict\n\nimport six\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.core.properties import Property, TypedListProperty\nfrom neupy.utils import as_tuple, tf_utils\nfrom neupy.layers.graph import BaseGraph, make_one_if_possible\n\n\n__all__ = (\'BaseLayer\', \'Identity\', \'Input\')\n\n\ndef create_name_pattern_from_layer_name(layer):\n    classname = layer.__class__.__name__\n    layer_name = re.sub(r\'(?<!^)(?=[A-Z][a-z_])\', \'-\', classname)\n    return layer_name.lower() + ""-{}""\n\n\ndef format_name_if_specified_as_pattern(name):\n    if not hasattr(format_name_if_specified_as_pattern, \'counters\'):\n        format_name_if_specified_as_pattern.counters = defaultdict(lambda: 1)\n\n    formatter = string.Formatter()\n    variables = [val[1] for val in formatter.parse(name) if val[1] is not None]\n\n    if len(variables) == 0:\n        return name\n\n    if len(variables) >= 2:\n        raise ValueError(\n            ""Provided pattern has more than one field specified. ""\n            ""Pattern: {}"".format(name))\n\n    layer_id = format_name_if_specified_as_pattern.counters[name]\n    formatted_name = name.format(layer_id)\n\n    # Increment only after string was successfully formatted\n    format_name_if_specified_as_pattern.counters[name] += 1\n    return formatted_name\n\n\nclass BaseLayer(BaseGraph):\n    """"""\n    Base class for the layers.\n\n    Parameters\n    ----------\n    name : str or None\n        Layer\'s name. Can be used as a reference to specific layer. Name\n        Can be specified as:\n\n            - String: Specified name will be used as a direct reference to\n              the layer. For example, `name=""fc""`\n\n            - Format string: Name pattern could be defined as a format string\n              and specified field will be replaced with an index. For example,\n              `name=""fc{{}}""` will be replaced with `fc1`, `fc2` and so on.\n              A bit more complex formatting methods are acceptable, for\n              example, `name=""fc-{{:<03d}}""` will be converted to `fc-001`,\n              `fc-002`, `fc-003` and so on.\n\n            - ``None``: When value specified as ``None`` than name will be\n              generated from the class name.\n\n        Defaults to ``None``.\n\n    Methods\n    -------\n    variable(value, name, shape=None, trainable=True)\n        Initializes variable with specified values.\n\n    get_output_shape(input_shape)\n        Computes expected output shape from the layer based on the\n        specified input shape.\n\n    output(*inputs, **kwargs)\n        Propagates input through the layer. The ``kwargs``  variable\n        might contain additional information that propagates through the\n        network.\n\n    Attributes\n    ----------\n    variables : dict\n        Variable names and their values. Dictionary can be empty in case\n        if variables hasn\'t been created yet.\n    """"""\n    name = Property(expected_type=six.string_types)\n\n    def __init__(self, name=None):\n        # Layer by default gets intialized as a graph with single node in it\n        super(BaseLayer, self).__init__(forward_graph=[(self, [])])\n        self.original_name = name\n\n        if name is None:\n            name = create_name_pattern_from_layer_name(layer=self)\n\n        name = format_name_if_specified_as_pattern(name)\n\n        self.variables = OrderedDict()\n        self.name = name\n\n        self._input_shape = tf.TensorShape(None)\n        self.frozen = False\n\n        # This decorator ensures that result produced by the\n        # `output` method will be marked under layer\'s name scope.\n        self.output = types.MethodType(\n            tf_utils.class_method_name_scope(self.output), self)\n\n    @classmethod\n    def define(cls, *args, **kwargs):\n        return partial(cls, *args, **kwargs)\n\n    @property\n    def input_shape(self):\n        # Explicit TensorShape transformation not only ensures\n        # that we have right type in the output, but also copies\n        # value stored in the `_input_shape` in order to make sure\n        # that no in-place update can effect original value\n        return tf.TensorShape(self._input_shape)\n\n    @input_shape.setter\n    def input_shape(self, shape):\n        if not self._input_shape.is_compatible_with(shape):\n            raise ValueError(\n                ""Cannot update input shape of the layer, because it\'s ""\n                ""incompatible with current input shape. Current shape: {}, ""\n                ""New shape: {}, Layer: {}"".format(\n                    self._input_shape, shape, self))\n\n        self._input_shape = tf.TensorShape(shape)\n\n    @property\n    def output_shape(self):\n        return self.get_output_shape(self.input_shape)\n\n    def get_output_shape(self, input_shape):\n        return tf.TensorShape(None)\n\n    def create_variables(self, *input_shapes):\n        return NotImplemented\n\n    def variable(self, value, name, shape=None, trainable=True):\n        layer_name = \'layer/{layer_name}/{parameter_name}\'.format(\n            layer_name=self.name,\n            parameter_name=name.replace(\'_\', \'-\'))\n\n        self.variables[name] = tf_utils.create_variable(\n            value, layer_name, shape, trainable)\n\n        return self.variables[name]\n\n    def _repr_arguments(self, *args, **kwargs):\n        def format_value(value):\n            references = {\n                \'Variable\': tf.Variable,\n                \'Array\': np.ndarray,\n                \'Matrix\': np.matrix,\n            }\n\n            for name, datatype in references.items():\n                if isinstance(value, datatype):\n                    return \'<{} shape={}>\'.format(name, value.shape)\n\n            return repr(value)\n\n        formatted_args = [str(arg) for arg in args]\n        argspec = inspect.getargspec(self.__class__.__init__)\n\n        def kwargs_priority(value):\n            if value in argspec.args:\n                return argspec.args.index(value)\n            return float(\'inf\')\n\n        # Kwargs will have destroyed order of the arguments, and order in\n        # the __init__ method allows to use proper order and validate names\n        for name in sorted(kwargs.keys(), key=kwargs_priority):\n            value = format_value(kwargs[name])\n            formatted_args.append(\'{}={}\'.format(name, value))\n\n        return \'{clsname}({formatted_args})\'.format(\n            clsname=self.__class__.__name__,\n            formatted_args=\', \'.join(formatted_args))\n\n    def __copy__(self):\n        params = self.get_params()\n        # We make sure that new name will be created if user\n        # specified name=None\n        params[\'name\'] = self.original_name\n\n        copied_layer = self.__class__(**params)\n        # Input shape can change and it cannot be controlled from\n        # the __init__ method\n        copied_layer.input_shape = self.input_shape\n\n        return copied_layer\n\n    def __deepcopy__(self, memo):\n        memo[id(self)] = copied_layer = self.__copy__()\n\n        for key, new_value in copied_layer.get_params().items():\n            old_value = getattr(self, key)\n\n            # Some of the objects could be already copied\n            # (for example, tensorflow\'s variables)\n            if old_value is new_value:\n                copied_new_value = copy.deepcopy(new_value, memo)\n                setattr(copied_layer, key, copied_new_value)\n\n        return copied_layer\n\n    def __repr__(self):\n        kwargs = {}\n\n        for name in self.options:\n            value = getattr(self, name)\n            kwargs[name] = value\n\n        return self._repr_arguments(**kwargs)\n\n\nclass Identity(BaseLayer):\n    """"""\n    Passes input through the layer without changes. Can be\n    useful while defining residual networks in the network.\n\n    Parameters\n    ----------\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n    """"""\n    def get_output_shape(self, input_shape):\n        return tf.TensorShape(input_shape)\n\n    def output(self, input, **kwargs):\n        return input\n\n\nclass Input(BaseLayer):\n    """"""\n    Layer defines network\'s input.\n\n    Parameters\n    ----------\n    shape : int or tuple\n        Shape of the input features per sample. Batch\n        dimension has to be excluded from the shape.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n    Feedforward Neural Network (FNN)\n\n    In the example, input layer defines network that expects\n    2D inputs (matrices). In other words, input to the network\n    should be set of samples combined into matrix where each sample\n    has 10 dimensional vector associated with it.\n\n    >>> from neupy.layers import *\n    >>> network = Input(10) >> Relu(5) >> Softmax(3)\n\n    Convolutional Neural Network (CNN)\n\n    In the example, input layer specified that we expect multiple\n    28x28 image as an input and each image should have single\n    channel (images with no color).\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((28, 28, 1)),\n    ...     Convolution((3, 3, 16)) >> Relu(),\n    ...     Convolution((3, 3, 16)) >> Relu(),\n    ...     Reshape()\n    ...     Softmax(10),\n    ... )\n    """"""\n    shape = TypedListProperty(element_type=(int, type(None)))\n\n    def __init__(self, shape, name=None):\n        super(Input, self).__init__(name=name)\n\n        if isinstance(shape, tf.TensorShape):\n            shape = tf_utils.shape_to_tuple(shape)\n\n        self.shape = as_tuple(shape)\n\n    @BaseLayer.input_shape.getter\n    def input_shape(self):\n        batch_shape = tf.TensorShape([None])\n        return batch_shape.concatenate(self.shape)\n\n    def output(self, input, **kwargs):\n        return input\n\n    def get_output_shape(self, input_shape):\n        if not self.input_shape.is_compatible_with(input_shape):\n            raise LayerConnectionError(\n                ""Input layer got unexpected input shape. ""\n                ""Received shape: {}, Expected shape: {}""\n                """".format(input_shape, self.input_shape)\n            )\n        return self.input_shape.merge_with(input_shape)\n\n    def __repr__(self):\n        return self._repr_arguments(\n            make_one_if_possible(self.shape),\n            name=self.name)\n'"
neupy/layers/convolutions.py,14,"b'from __future__ import division\n\nimport math\nimport collections\n\nimport six\nimport tensorflow as tf\n\nfrom neupy import init\nfrom neupy.utils import as_tuple\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.core.properties import (\n    TypedListProperty, Property,\n    ParameterProperty,\n)\nfrom .base import BaseLayer\n\n\n__all__ = (\'Convolution\', \'Deconvolution\')\n\n\nclass Spatial2DProperty(TypedListProperty):\n    expected_type = (list, tuple, int)\n\n    def __init__(self, *args, **kwargs):\n        kwargs[\'element_type\'] = int\n        super(Spatial2DProperty, self).__init__(*args, **kwargs)\n\n    def __set__(self, instance, value):\n        if isinstance(value, collections.Iterable) and len(value) == 1:\n            value = (value[0], 1)\n\n        if isinstance(value, int):\n            value = (value, value)\n\n        super(Spatial2DProperty, self).__set__(instance, value)\n\n    def validate(self, value):\n        super(Spatial2DProperty, self).validate(value)\n\n        if len(value) > 2:\n            raise ValueError(\n                ""Stride can have only one or two elements ""\n                ""in the list. Got {}"".format(len(value)))\n\n        if any(element <= 0 for element in value):\n            raise ValueError(\n                ""Stride size should contain only values greater than zero"")\n\n\ndef deconv_output_shape(dimension_size, filter_size, padding, stride,\n                        dilation=1):\n    """"""\n    Computes deconvolution\'s output shape for one spatial dimension.\n\n    Parameters\n    ----------\n    dimension_size : int or None\n        Size of the dimension. Typically it\'s image\'s weight or height.\n        It might be equal to ``None`` when we input might have variable\n        dimension.\n\n    filter_size : int\n        Size of the convolution filter.\n\n    padding : {``valid``, ``same``} or int\n        Type or size of the zero-padding.\n\n    stride : int\n        Stride size.\n\n    dilation : int\n        Dilation rate. Only ``dilation=1`` is supported for the\n        deconvolution.\n\n    Returns\n    -------\n    int\n        Dimension size after applying deconvolution\n        operation with specified configurations.\n    """"""\n    if isinstance(dimension_size, tf.Dimension):\n        dimension_size = dimension_size.value\n\n    if dimension_size is None:\n        return None\n\n    if dilation != 1:\n        raise ValueError(""Deconvolution layer doesn\'t support dilation"")\n\n    if padding in (\'VALID\', \'valid\'):\n        return dimension_size * stride + max(filter_size - stride, 0)\n\n    elif padding in (\'SAME\', \'same\'):\n        return dimension_size * stride\n\n    elif isinstance(padding, int):\n        return dimension_size * stride - 2 * padding + filter_size - 1\n\n    raise ValueError(\n        ""`{!r}` is unknown deconvolution\'s padding value"".format(padding))\n\n\ndef conv_output_shape(dimension_size, filter_size, padding, stride,\n                      dilation=1):\n    """"""\n    Computes convolution\'s output shape for one spatial dimension.\n\n    Parameters\n    ----------\n    dimension_size : int or None\n        Size of the dimension. Typically it\'s image\'s weight or height.\n        It might be equal to ``None`` when we input might have variable\n        dimension.\n\n    filter_size : int\n        Size of the convolution filter.\n\n    padding : {``valid``, ``same``} or int\n        Type or size of the zero-padding.\n\n    stride : int\n        Stride size.\n\n    dilation : int\n        Dilation rate. Defaults to ``1``.\n\n    Returns\n    -------\n    int\n        Dimension size after applying convolution\n        operation with specified configurations.\n    """"""\n    if isinstance(dimension_size, tf.Dimension):\n        dimension_size = dimension_size.value\n\n    if dimension_size is None:\n        return None\n\n    if not isinstance(stride, int):\n        raise ValueError(\n            ""Stride needs to be an integer, got {} (value {!r})""\n            """".format(type(stride), stride))\n\n    if not isinstance(filter_size, int):\n        raise ValueError(\n            ""Filter size needs to be an integer, got {} ""\n            ""(value {!r})"".format(type(filter_size), filter_size))\n\n    # We can think of the dilation as very sparse convolutional filter\n    # filter=3 and dilation=2 the same as filter=5 and dilation=1\n    filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n\n    if padding in (\'VALID\', \'valid\'):\n        return int(math.ceil((dimension_size - filter_size + 1) / stride))\n\n    elif padding in (\'SAME\', \'same\'):\n        return int(math.ceil(dimension_size / stride))\n\n    elif isinstance(padding, int):\n        return int(math.ceil(\n            (dimension_size + 2 * padding - filter_size + 1) / stride))\n\n    raise ValueError(\n        ""`{!r}` is unknown convolution\'s padding value"".format(padding))\n\n\nclass PaddingProperty(Property):\n    expected_type = (six.string_types, int, tuple)\n    valid_string_choices = (\'VALID\', \'SAME\', \'same\', \'valid\')\n\n    def __set__(self, instance, value):\n        if isinstance(value, int):\n            if value < 0:\n                raise ValueError(\n                    ""Integer border mode value needs to be ""\n                    ""greater or equal to zero, got {}"".format(value))\n\n            value = (value, value)\n\n        if isinstance(value, six.string_types):\n            value = value.upper()\n\n        super(PaddingProperty, self).__set__(instance, value)\n\n    def validate(self, value):\n        super(PaddingProperty, self).validate(value)\n\n        if isinstance(value, tuple) and len(value) != 2:\n            raise ValueError(\n                ""Border mode property suppose to get a tuple that ""\n                ""contains two elements, got {} elements""\n                """".format(len(value)))\n\n        is_invalid_string = (\n            isinstance(value, six.string_types) and\n            value not in self.valid_string_choices\n        )\n\n        if is_invalid_string:\n            valid_choices = \', \'.join(self.valid_string_choices)\n            raise ValueError(\n                ""`{}` is invalid string value. Available choices: {}""\n                """".format(value, valid_choices))\n\n        if isinstance(value, tuple) and any(element < 0 for element in value):\n            raise ValueError(\n                ""Tuple border mode value needs to contain only elements ""\n                ""that greater or equal to zero, got {}"".format(value))\n\n\nclass Convolution(BaseLayer):\n    """"""\n    Convolutional layer.\n\n    Parameters\n    ----------\n    size : tuple of int\n        Filter shape. In should be defined as a tuple with three\n        integers ``(filter rows, filter columns, output channels)``.\n\n    padding : {{``same``, ``valid``}}, int, tuple\n        Zero padding for the input tensor.\n\n        - ``valid`` - Padding won\'t be added to the tensor. Result will be\n          the same as for ``padding=0``\n\n        - ``same`` - Padding will depend on the number of rows and columns\n          in the filter. This padding makes sure that image with the\n          ``stride=1`` won\'t change its width and height. It\'s the same as\n          ``padding=(filter rows // 2, filter columns // 2)``.\n\n        - Custom value for the padding can be specified as an integer, like\n          ``padding=1`` or it can be specified as a tuple when different\n          dimensions have different padding values, for example\n          ``padding=(2, 3)``.\n\n        Defaults to ``valid``.\n\n    stride : tuple with ints, int.\n        Stride size. Defaults to ``(1, 1)``\n\n    dilation : int, tuple\n        Rate for the filter upsampling. When ``dilation > 1`` layer will\n        become dilated convolution (or atrous convolution). Defaults to ``1``.\n\n    weight : array-like, Tensorfow variable, scalar or Initializer\n        Defines layer\'s weights. Shape of the weight will be equal to\n        ``(filter rows, filter columns, input channels, output channels)``.\n        Default initialization methods you can find\n        :ref:`here <init-methods>`. Defaults to\n        :class:`HeNormal(gain=2) <neupy.init.HeNormal>`.\n\n    bias : 1D array-like, Tensorfow variable, scalar, Initializer or None\n        Defines layer\'s bias. Default initialization methods you can find\n        :ref:`here <init-methods>`. Defaults to\n        :class:`Constant(0) <neupy.init.Constant>`.\n        The ``None`` value excludes bias from the calculations and\n        do not add it into parameters list.\n\n    {BaseLayer.name}\n\n    Examples\n    --------\n    2D Convolution\n\n    >>> from neupy import layers\n    >>>\n    >>> layers.join(\n    ...     layers.Input((28, 28, 3)),\n    ...     layers.Convolution((3, 3, 16)),\n    ... )\n\n    1D Convolution\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((30, 10)),\n    ...     Reshape((30, 1, 10)),  # convert 3D to 4D\n    ...     Convolution((3, 1, 16)),\n    ...     Reshape((-1, 16))  # convert 4D back to 3D\n    ... )\n    >>> network\n    (?, 30, 10) -> [... 4 layers ...] -> (?, 28, 16)\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n    """"""\n    size = TypedListProperty(element_type=int, n_elements=3)\n    weight = ParameterProperty()\n    bias = ParameterProperty(allow_none=True)\n\n    padding = PaddingProperty()\n    stride = Spatial2DProperty()\n    dilation = Spatial2DProperty()\n\n    # We use gain=2 because it\'s suitable choice for relu non-linearity\n    # and relu is the most common non-linearity used for CNN.\n    def __init__(self, size, padding=\'valid\', stride=1, dilation=1,\n                 weight=init.HeNormal(gain=2), bias=0, name=None):\n\n        super(Convolution, self).__init__(name=name)\n\n        self.size = size\n        self.padding = padding\n        self.stride = stride\n        self.dilation = dilation\n        self.weight = weight\n        self.bias = bias\n\n    def fail_if_shape_invalid(self, input_shape):\n        if input_shape and input_shape.ndims != 4:\n            raise LayerConnectionError(\n                ""Convolutional layer expects an input with 4 ""\n                ""dimensions, got {} with shape {}""\n                """".format(len(input_shape), input_shape))\n\n    def output_shape_per_dim(self, *args, **kwargs):\n        return conv_output_shape(*args, **kwargs)\n\n    def expected_output_shape(self, input_shape):\n        n_samples = input_shape[0]\n        row_filter_size, col_filter_size, n_kernels = self.size\n        row_stride, col_stride = self.stride\n        row_dilation, col_dilation = self.dilation\n\n        if isinstance(self.padding, (list, tuple)):\n            row_padding, col_padding = self.padding\n        else:\n            row_padding, col_padding = self.padding, self.padding\n\n        return (\n            n_samples,\n            self.output_shape_per_dim(\n                input_shape[1], row_filter_size,\n                row_padding, row_stride, row_dilation\n            ),\n            self.output_shape_per_dim(\n                input_shape[2], col_filter_size,\n                col_padding, col_stride, col_dilation\n            ),\n            n_kernels,\n        )\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n        self.fail_if_shape_invalid(input_shape)\n\n        if input_shape.ndims is None:\n            n_samples = input_shape[0]\n            n_kernels = self.size[-1]\n            return tf.TensorShape((n_samples, None, None, n_kernels))\n\n        return tf.TensorShape(self.expected_output_shape(input_shape))\n\n    def create_variables(self, input_shape):\n        self.input_shape = input_shape\n        n_channels = input_shape[-1]\n        n_rows, n_cols, n_filters = self.size\n\n        # Compare to the regular convolution weights,\n        # transposed one has switched input and output channels.\n        self.weight = self.variable(\n            value=self.weight, name=\'weight\',\n            shape=(n_rows, n_cols, n_channels, n_filters))\n\n        if self.bias is not None:\n            self.bias = self.variable(\n                value=self.bias, name=\'bias\',\n                shape=as_tuple(n_filters))\n\n    def output(self, input, **kwargs):\n        input = tf.convert_to_tensor(input, tf.float32)\n        self.fail_if_shape_invalid(input.shape)\n        padding = self.padding\n\n        if not isinstance(padding, six.string_types):\n            height_pad, width_pad = padding\n            input = tf.pad(input, [\n                [0, 0],\n                [height_pad, height_pad],\n                [width_pad, width_pad],\n                [0, 0],\n            ])\n            # VALID option will make sure that\n            # convolution won\'t use any padding.\n            padding = \'VALID\'\n\n        output = tf.nn.convolution(\n            input,\n            self.weight,\n            padding=padding,\n            strides=self.stride,\n            dilation_rate=self.dilation,\n            data_format=""NHWC"",\n        )\n\n        if self.bias is not None:\n            bias = tf.reshape(self.bias, (1, 1, 1, -1))\n            output += bias\n\n        return output\n\n    def __repr__(self):\n        return self._repr_arguments(\n            self.size,\n            padding=self.padding,\n            stride=self.stride,\n            dilation=self.dilation,\n            weight=self.weight,\n            bias=self.bias,\n            name=self.name,\n        )\n\n\nclass Deconvolution(Convolution):\n    """"""\n    Deconvolution layer (also known as Transposed Convolution.).\n\n    Parameters\n    ----------\n    {Convolution.size}\n\n    {Convolution.padding}\n\n    {Convolution.stride}\n\n    {Convolution.dilation}\n\n    weight : array-like, Tensorfow variable, scalar or Initializer\n        Defines layer\'s weights. Shape of the weight will be equal to\n        ``(filter rows, filter columns, output channels, input channels)``.\n        Default initialization methods you can find\n        :ref:`here <init-methods>`. Defaults to\n        :class:`HeNormal(gain=2) <neupy.init.HeNormal>`.\n\n    {Convolution.bias}\n\n    {Convolution.name}\n\n    Methods\n    -------\n    {Convolution.Methods}\n\n    Attributes\n    ----------\n    {Convolution.Attributes}\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((28, 28, 3)),\n    ...     Convolution((3, 3, 16)),\n    ...     Deconvolution((3, 3, 1)),\n    ... )\n    >>> network\n    (?, 28, 28, 3) -> [... 3 layers ...] -> (?, 28, 28, 1)\n    """"""\n    def __init__(self, size, padding=\'valid\', stride=1,\n                 weight=init.HeNormal(gain=2), bias=0, name=None):\n\n        super(Deconvolution, self).__init__(\n            size=size, padding=padding, stride=stride,\n            dilation=1, weight=weight, bias=bias, name=name)\n\n    def output_shape_per_dim(self, *args, **kwargs):\n        return deconv_output_shape(*args, **kwargs)\n\n    def create_variables(self, input_shape):\n        self.input_shape = input_shape\n        n_channels = input_shape[-1]\n        n_rows, n_cols, n_filters = self.size\n\n        # Compare to the regular convolution weights,\n        # transposed one has switched input and output channels.\n        self.weight = self.variable(\n            value=self.weight, name=\'weight\',\n            shape=(n_rows, n_cols, n_filters, n_channels))\n\n        if self.bias is not None:\n            self.bias = self.variable(\n                value=self.bias, name=\'bias\',\n                shape=as_tuple(n_filters))\n\n    def output(self, input, **kwargs):\n        input = tf.convert_to_tensor(input, tf.float32)\n        # We need to get information about output shape from the input\n        # tensor\'s shape, because for some inputs we might have\n        # height and width specified as None and shape value won\'t be\n        # computed for these dimensions.\n        padding = self.padding\n\n        # It\'s important that expected output shape gets computed on then\n        # Tensor (produced by tf.shape) rather than on TensorShape object.\n        # Tensorflow cannot convert TensorShape object into Tensor and\n        # it will cause an exception in the conv2d_transpose layer.\n        output_shape = self.expected_output_shape(tf.shape(input))\n\n        if isinstance(self.padding, (list, tuple)):\n            height_pad, width_pad = self.padding\n\n            # VALID option will make sure that\n            # deconvolution won\'t use any padding.\n            padding = \'VALID\'\n\n            # conv2d_transpose doesn\'t know about extra paddings that we added\n            # in the convolution. For this reason, we have to expand our\n            # expected output shape and later we will remove these paddings\n            # manually after transpose convolution.\n            output_shape = (\n                output_shape[0],\n                output_shape[1] + 2 * height_pad,\n                output_shape[2] + 2 * width_pad,\n                output_shape[3],\n            )\n\n        output = tf.nn.conv2d_transpose(\n            value=input,\n            filter=self.weight,\n            output_shape=list(output_shape),\n            strides=as_tuple(1, self.stride, 1),\n            padding=padding,\n            data_format=""NHWC""\n        )\n\n        if isinstance(self.padding, (list, tuple)):\n            h_pad, w_pad = self.padding\n\n            if h_pad > 0:\n                output = output[:, h_pad:-h_pad, :, :]\n\n            if w_pad > 0:\n                output = output[:, :, w_pad:-w_pad, :]\n\n        if self.bias is not None:\n            bias = tf.reshape(self.bias, (1, 1, 1, -1))\n            output += bias\n\n        return output\n\n    def __repr__(self):\n        return self._repr_arguments(\n            self.size,\n            padding=self.padding,\n            stride=self.stride,\n            weight=self.weight,\n            bias=self.bias,\n            name=self.name,\n        )\n'"
neupy/layers/embedding.py,3,"b'import tensorflow as tf\n\nfrom neupy import init\nfrom neupy.utils import as_tuple\nfrom neupy.core.properties import IntProperty, ParameterProperty\nfrom .base import BaseLayer\n\n\n__all__ = (\'Embedding\',)\n\n\nclass Embedding(BaseLayer):\n    """"""\n    Embedding layer accepts indices as an input and returns\n    rows from the weight matrix associated with these indices.\n    It\'s useful when inputs are categorical features or for the\n    word embedding tasks.\n\n    Parameters\n    ----------\n    input_size : int\n        Layer\'s input vector dimension. It\'s, typically, associated with\n        number of categories or number of unique words that input vector has.\n\n    output_size : int\n        Layer\'s output vector dimension.\n\n    weight : array-like, Tensorfow variable, scalar or Initializer\n        Defines layer\'s weights. Default initialization methods\n        you can find :ref:`here <init-methods>`.\n        Defaults to :class:`HeNormal() <neupy.init.HeNormal>`.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n\n    This example converts dataset that has only categorical\n    variables into format that suitable for Embedding layer.\n\n    >>> import numpy as np\n    >>> from neupy.layers import *\n    >>>\n    >>> dataset = np.array([\n    ...     [\'cold\', \'high\'],\n    ...     [\'hot\',  \'low\'],\n    ...     [\'cold\', \'low\'],\n    ...     [\'hot\',  \'low\'],\n    ... ])\n    >>>\n    >>> unique_value, dataset_indices = np.unique(\n    ...     dataset, return_inverse=True\n    ... )\n    >>> dataset_indices = dataset_indices.reshape((4, 2))\n    >>> dataset_indices\n    array([[0, 1],\n           [2, 3],\n           [0, 3],\n           [2, 3]])\n    >>>\n    >>> n_features = dataset.shape[1]\n    >>> n_unique_categories = len(unique_value)\n    >>> embedded_size = 1\n    >>>\n    >>> network = join(\n    ...     Input(n_features),\n    ...     Embedding(n_unique_categories, embedded_size),\n    ...     # Output from the embedding layer is 3D\n    ...     # To make output 2D we need to reshape dimensions\n    ...     Reshape(),\n    ... )\n    """"""\n    input_size = IntProperty(minval=1)\n    output_size = IntProperty(minval=1)\n    weight = ParameterProperty()\n\n    def __init__(self, input_size, output_size,\n                 weight=init.HeNormal(), name=None):\n\n        super(Embedding, self).__init__(name=name)\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.weight = weight\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n        return input_shape.concatenate(self.output_size)\n\n    def create_variables(self, input_shape):\n        self.input_shape = input_shape\n        self.weight = self.variable(\n            value=self.weight, name=\'weight\',\n            shape=as_tuple(self.input_size, self.output_size))\n\n    def output(self, input_value, **kwargs):\n        input_value = tf.cast(input_value, tf.int32)\n        return tf.gather(self.weight, input_value)\n\n    def __repr__(self):\n        return self._repr_arguments(\n            self.input_size,\n            self.output_size,\n            name=self.name,\n            weight=self.weight,\n        )\n'"
neupy/layers/graph.py,6,"b'import sys\nimport copy\nimport tempfile\nfrom itertools import chain\nfrom functools import wraps\nfrom abc import abstractmethod\nfrom collections import OrderedDict\n\nimport six\nimport graphviz\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy.core.config import ConfigurableABC, DumpableObject\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.utils import as_tuple, tf_utils, iters\n\n\n__all__ = (\n    \'BaseGraph\', \'LayerGraph\',\n    \'join\', \'parallel\', \'merge\', \'repeat\',\n)\n\n\ndef make_one_if_possible(shape):\n    """"""\n    Format layer\'s input or output shape.\n\n    Parameters\n    ----------\n    shape : int or tuple\n\n    Returns\n    -------\n    int or tuple\n    """"""\n    if isinstance(shape, (tuple, list)) and len(shape) == 1:\n        return shape[0]\n    return shape\n\n\ndef filter_graph(dictionary, include_keys):\n    """"""\n    Create new list that contains only values\n    specified in the ``include_keys`` attribute.\n\n    Parameters\n    ----------\n    dictionary : dict\n        Original dictionary\n\n    include_keys : list or tuple\n        Keys that will copied from original dictionary\n        into a new one.\n\n    Returns\n    -------\n    dict\n    """"""\n    filtered_dict = OrderedDict()\n\n    for key, value in dictionary.items():\n        if key in include_keys:\n            filtered_dict[key] = [v for v in value if v in include_keys]\n\n    return filtered_dict\n\n\ndef is_cyclic(graph):\n    """"""\n    Check if graph has cycles.\n\n    Parameters\n    ----------\n    graph : dict\n\n    Returns\n    -------\n    bool\n        Return ``True`` if the directed graph has a cycle.\n\n    Examples\n    --------\n    >>> is_cyclic({1: [2], 2: [3], 3: [1]})\n    True\n    >>> is_cyclic({1: [2], 2: [3], 3: [4]})\n    False\n    """"""\n    path = set()\n    visited = set()\n\n    def visit(vertex):\n        if vertex in visited:\n            return False\n\n        visited.add(vertex)\n        path.add(vertex)\n\n        for neighbour in graph.get(vertex, ()):\n            if neighbour in path or visit(neighbour):\n                return True\n\n        path.remove(vertex)\n        return False\n\n    return any(visit(vertex) for vertex in graph)\n\n\ndef find_outputs_in_graph(graph):\n    outputs = []\n\n    for from_node, to_nodes in graph.items():\n        if not to_nodes:\n            outputs.append(from_node)\n\n    return outputs\n\n\ndef topological_sort(graph):\n    """"""\n    Repeatedly go through all of the nodes in the graph, moving each of\n    the nodes that has all its edges resolved, onto a sequence that\n    forms our sorted graph. A node has all of its edges resolved and\n    can be moved once all the nodes its edges point to, have been moved\n    from the unsorted graph onto the sorted one.\n\n    Parameters\n    ----------\n    graph : dict\n        Dictionary that has graph structure.\n\n    Raises\n    ------\n    RuntimeError\n        If graph has cycles.\n\n    Returns\n    -------\n    list\n        List of nodes sorted in topological order.\n    """"""\n    if not graph:\n        return []\n\n    if is_cyclic(graph):\n        raise RuntimeError(\n            ""Cannot apply topological sort to the graphs with cycles"")\n\n    sorted_nodes = []\n    graph_unsorted = graph.copy()\n\n    while graph_unsorted:\n        for node, edges in list(graph_unsorted.items()):\n            if all(edge not in graph_unsorted for edge in edges):\n                del graph_unsorted[node]\n                sorted_nodes.append(node)\n\n    return sorted_nodes\n\n\ndef lazy_property(function):\n    attr = \'_lazy__\' + function.__name__\n\n    @property\n    @wraps(function)\n    def wrapper(self):\n        if not hasattr(self, attr):\n            setattr(self, attr, function(self))\n        return getattr(self, attr)\n\n    return wrapper\n\n\nclass BaseGraph(ConfigurableABC, DumpableObject):\n    events = []\n\n    def __init__(self, forward_graph=None):\n        self.forward_graph = OrderedDict(forward_graph or [])\n\n    @lazy_property\n    def backward_graph(self):\n        # First we copy all the nodes in order to\n        # make sure that order stays the same\n        backward = OrderedDict([(node, []) for node in self.forward_graph])\n\n        for to_node, from_nodes in self.forward_graph.items():\n            for from_node in from_nodes:\n                backward[from_node].append(to_node)\n\n        return backward\n\n    @lazy_property\n    def input_layers(self):\n        return find_outputs_in_graph(self.backward_graph)\n\n    @lazy_property\n    def output_layers(self):\n        return find_outputs_in_graph(self.forward_graph)\n\n    @lazy_property\n    def inputs(self):\n        placeholders = []\n\n        for layer in self.input_layers:\n            placeholder = tf.placeholder(\n                tf.float32,\n                shape=tf_utils.shape_to_tuple(layer.input_shape),\n                name=""placeholder/input/{}"".format(layer.name),\n            )\n            placeholders.append(placeholder)\n\n        return make_one_if_possible(placeholders)\n\n    @lazy_property\n    def targets(self):\n        placeholders = []\n\n        for layer in self.output_layers:\n            placeholder = tf.placeholder(\n                tf.float32,\n                shape=tf_utils.shape_to_tuple(layer.output_shape),\n                name=""placeholder/target/{}"".format(layer.name),\n            )\n            placeholders.append(placeholder)\n\n        return make_one_if_possible(placeholders)\n\n    @lazy_property\n    def outputs(self):\n        networks_output = self.output(*as_tuple(self.inputs))\n        tf_utils.initialize_uninitialized_variables()\n        return networks_output\n\n    @lazy_property\n    def training_outputs(self):\n        networks_output = self.output(*as_tuple(self.inputs), training=True)\n        tf_utils.initialize_uninitialized_variables()\n        return networks_output\n\n    def __gt__(self, other):\n        left, right = self, other\n        self.events.append((\'__gt__\', join(left, right)))\n\n        graph = LayerGraph()\n        previous_operator = None\n\n        for operator, value in reversed(self.events):\n            if operator == previous_operator:\n                break\n\n            if operator == \'__gt__\':\n                # It\'s important to put `value` before graph, because\n                # we merge in reverse order and we need to make sure\n                # that every new value has higher priority.\n                graph = merge(value, graph)\n\n            previous_operator = operator\n\n        return graph\n\n    def __bool__(self):\n        self.events.append((\'__bool__\', self))\n        return True\n\n    def __nonzero__(self):\n        return self.__bool__()  # Hack for python 2\n\n    def __rshift__(self, other):\n        return join(self, other)\n\n    def __irshift__(self, other):\n        return self.__rshift__(other)\n\n    def __or__(self, other):\n        return parallel(self, other)\n\n    def __ior__(self, other):\n        return self.__or__(other)\n\n    @abstractmethod\n    def output(self, inputs):\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def output_shape(self):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_output_shape(self, input_shape):\n        raise NotImplementedError()\n\n\nclass LayerGraph(BaseGraph):\n    def __init__(self, forward_graph=None):\n        super(LayerGraph, self).__init__(forward_graph)\n\n        # This allows to run simple check that ensures that\n        # created graph have defined layer shape\n        self.output_shape\n\n    def clean_layer_references(self, layer_references):\n        layers = []\n\n        for layer_reference in layer_references:\n            if isinstance(layer_reference, six.string_types):\n                layer_reference = self.layer(layer_reference)\n            layers.append(layer_reference)\n\n        return layers\n\n    def slice(self, directed_graph, layers):\n        layers = self.clean_layer_references(layers)\n        forward_graph = self.forward_graph\n\n        if all(layer not in forward_graph for layer in layers):\n            unused_layer = next(l for l in layers if l not in forward_graph)\n            raise ValueError(\n                ""Layer `{}` is not used in the graph. Graph: {}, ""\n                ""Layer: {}"".format(unused_layer.name, self, unused_layer))\n\n        observed_layers = []\n        layers = copy.copy(layers)\n\n        while layers:\n            current_layer = layers.pop()\n            observed_layers.append(current_layer)\n\n            for next_layer in directed_graph[current_layer]:\n                if next_layer not in observed_layers:\n                    layers.append(next_layer)\n\n        forward_subgraph = filter_graph(forward_graph, observed_layers)\n        return self.__class__(forward_subgraph)\n\n    def end(self, *output_layers):\n        return self.slice(self.backward_graph, output_layers)\n\n    def start(self, *input_layers):\n        return self.slice(self.forward_graph, input_layers)\n\n    @lazy_property\n    def layers(self):\n        return list(self)\n\n    def layer(self, layer_name):\n        if not isinstance(layer_name, six.string_types):\n            raise ValueError(\n                ""Layer name expected to be a string, ""\n                ""got value {}"".format(layer_name))\n\n        layers = []\n\n        for layer in self.forward_graph:\n            if layer.name == layer_name:\n                layers.append(layer)\n\n        if not layers:\n            raise NameError(\n                ""Cannot find layer with name {!r}"".format(layer_name))\n\n        if len(layers) >= 2:\n            raise NameError(\n                ""Ambiguous layer name `{}`. Network has {} ""\n                ""layers with the same name. Layers: {}"".format(\n                    layer_name, len(layers), layers))\n\n        return layers[0]\n\n    @lazy_property\n    def input_shapes(self):\n        return [tf.TensorShape(l.input_shape) for l in self.input_layers]\n\n    @lazy_property\n    def input_shape(self):\n        return make_one_if_possible(self.input_shapes)\n\n    @lazy_property\n    def output_shape(self):\n        return self.get_output_shape(*self.input_shapes)\n\n    @lazy_property\n    def output_shapes_per_layer(self):\n        return self.propagate_forward(\n            copy.deepcopy(self.input_shapes),\n            method=\'get_output_shape\')\n\n    def get_output_shape(self, *inputs):\n        outputs = self.propagate_forward(\n            copy.deepcopy(inputs),\n            method=\'get_output_shape\',\n        )\n        return make_one_if_possible(\n            [outputs[l] for l in self.output_layers])\n\n    def create_variables(self):\n        output_shapes = self.output_shapes_per_layer\n        backward_graph = self.backward_graph\n\n        for layer in self:\n            input_shapes = [layer.input_shape]\n            from_layers = backward_graph[layer]\n\n            if layer.frozen:\n                continue\n\n            if from_layers:\n                input_shapes = [output_shapes[l] for l in from_layers]\n\n            layer.create_variables(*input_shapes)\n            layer.frozen = True\n\n    def output(self, *inputs, **kwargs):\n        self.create_variables()\n        outputs = self.propagate_forward(inputs, method=\'output\', **kwargs)\n        return make_one_if_possible([outputs[l] for l in self.output_layers])\n\n    def preformat_inputs(self, inputs):\n        if len(inputs) == 1 and isinstance(inputs[0], dict):\n            inputs = inputs[0]\n\n        if not isinstance(inputs, dict):\n            n_input_layers = len(self.input_layers)\n            n_input_vars = len(inputs)\n\n            if n_input_vars != n_input_layers:\n                raise ValueError(\n                    ""Connection has {} input layer(s), but {} inputs was ""\n                    ""provided"".format(n_input_layers, n_input_vars))\n\n            inputs = dict(zip(self.input_layers, inputs))\n\n        prepared_inputs = {}\n        for layer, input_variable in inputs.items():\n            if isinstance(layer, six.string_types):\n                layer = self.layer(layer)\n\n            if layer not in self.forward_graph:\n                raise ValueError(\n                    ""The `{}` layer doesn\'t appear in the network""\n                    """".format(layer.name))\n\n            if layer not in self.input_layers:\n                raise ValueError(\n                    ""`{}` is not an input layer in the network""\n                    """".format(layer.name))\n\n            prepared_inputs[layer] = input_variable\n\n        return prepared_inputs\n\n    def pass_through_the_layer(self, layer, method, *args, **kwargs):\n        layer_method = getattr(layer, method)\n\n        try:\n            return layer_method(*args, **kwargs)\n        except Exception as exception:\n            modified_exception = exception.__class__(\n                ""{original_message}. Exception occurred while propagating ""\n                ""data through the method `{method}`. Layer: {layer!r}"".format(\n                    original_message=str(exception).strip(\'.\'),\n                    method=method, layer=layer\n                )\n            )\n\n            if hasattr(sys, \'last_traceback\') and six.PY3:\n                modified_exception = modified_exception.with_traceback(\n                    sys.last_traceback)\n\n            raise modified_exception\n\n    def propagate_forward(self, inputs, method, **kwargs):\n        backward_graph = self.backward_graph\n        inputs = self.preformat_inputs(inputs)\n        outputs = copy.copy(inputs)\n\n        for layer, layer_input in inputs.items():\n            outputs[layer] = self.pass_through_the_layer(\n                layer, method, layer_input, **kwargs)\n\n        for layer in (l for l in self if l not in inputs):\n            layer_inputs = [outputs[l] for l in backward_graph[layer]]\n            outputs[layer] = self.pass_through_the_layer(\n                layer, method, *layer_inputs, **kwargs)\n\n        return outputs\n\n    @property\n    def variables(self):\n        self.create_variables()\n\n        variables = OrderedDict()\n        observed_variables = []\n\n        for layer in self:\n            for name, value in layer.variables.items():\n                if value not in observed_variables:\n                    observed_variables.append(value)\n                    variables[(layer, name)] = value\n\n        return variables\n\n    @property\n    def n_parameters(self):\n        n_parameters = 0\n\n        for variable in self.variables.values():\n            n_parameters += variable.shape.num_elements()\n\n        return n_parameters\n\n    def predict(self, *inputs, **kwargs):\n        session = tf_utils.tensorflow_session()\n\n        batch_size = kwargs.pop(\'batch_size\', None)\n        verbose = kwargs.pop(\'verbose\', True)\n\n        # We require do to this check for python 2 compatibility\n        if kwargs:\n            raise TypeError(""Unknown arguments: {}"".format(kwargs))\n\n        def single_batch_predict(*inputs):\n            feed_dict = dict(zip(as_tuple(self.inputs), inputs))\n            return session.run(self.outputs, feed_dict=feed_dict)\n\n        outputs = iters.apply_batches(\n            function=single_batch_predict,\n            inputs=inputs,\n            batch_size=batch_size,\n            show_progressbar=verbose,\n        )\n        return np.concatenate(outputs, axis=0)\n\n    def is_sequential(self):\n        if len(self.input_layers) > 1 or len(self.output_layers) > 1:\n            return False\n\n        forward_graph_layers = self.forward_graph.values()\n        backward_graph_layers = self.backward_graph.values()\n\n        for layers in chain(forward_graph_layers, backward_graph_layers):\n            if len(layers) >= 2:\n                # One of the layers has multiple input\n                # or output networks\n                return False\n\n        return True\n\n    def layer_names_only(self):\n        prepared_graph = OrderedDict()\n\n        for from_layer, to_layers in self.forward_graph.items():\n            prepared_graph[from_layer.name] = [l.name for l in to_layers]\n\n        return list(prepared_graph.items())\n\n    def show(self, filepath=None):\n        """"""\n        Generates visual representation of the network. Method will create\n        PDF that will be rendered and opened automatically. Graph will\n        be stored in the ``filepath`` when it\'s specified.\n        """"""\n        if filepath is None:\n            filepath = tempfile.mktemp()\n\n        def layer_uid(layer):\n            return str(id(layer))\n\n        digraph = graphviz.Digraph()\n        shapes_per_layer = self.output_shapes_per_layer\n\n        for layer in self.forward_graph.keys():\n            digraph.node(layer_uid(layer), str(layer.name))\n\n        output_id = 1\n        for from_layer, to_layers in self.forward_graph.items():\n            for to_layer in to_layers:\n                digraph.edge(\n                    layer_uid(from_layer),\n                    layer_uid(to_layer),\n                    label="" {}"".format(shapes_per_layer[from_layer]))\n\n            if not to_layers:\n                output = \'output-{}\'.format(output_id)\n\n                digraph.node(output, \'Output #{}\'.format(output_id))\n                digraph.edge(\n                    layer_uid(from_layer), output,\n                    label="" {}"".format(shapes_per_layer[from_layer]))\n\n                output_id += 1\n\n        digraph.render(filepath, view=True)\n\n    def get_params(self):\n        return {\'forward_graph\': self.forward_graph}\n\n    def __contains__(self, entity):\n        return entity in self.forward_graph\n\n    def __len__(self):\n        return len(self.forward_graph)\n\n    def __iter__(self):\n        for layer in topological_sort(self.backward_graph):\n            yield layer\n\n    def __repr__(self):\n        if not self.forward_graph:\n            return ""[empty graph]""\n\n        def format_shapes(shape):\n            if isinstance(shape, tf.TensorShape):\n                return str(shape)\n\n            shapes = \', \'.join([format_shapes(s) for s in shape])\n            return \'[\' + shapes + \']\'\n\n        return \'{} -> [... {} layers ...] -> {}\'.format(\n            format_shapes(self.input_shape),\n            len(self),\n            format_shapes(self.output_shape))\n\n\ndef validate_graphs_before_combining(left_graph, right_graph):\n    left_out_layers = left_graph.output_layers\n    right_in_layers = right_graph.input_layers\n\n    if len(left_out_layers) > 1 and len(right_in_layers) > 1:\n        raise LayerConnectionError(\n            ""Cannot make many to many connection between graphs. One graph ""\n            ""has {n_left_outputs} outputs (layer names: {left_names}) and ""\n            ""the other one has {n_right_inputs} inputs (layer names: ""\n            ""{right_names}). First graph: {left_graph}, Second graph: ""\n            ""{right_graph}"".format(\n                left_graph=left_graph,\n                n_left_outputs=len(left_out_layers),\n                left_names=[layer.name for layer in left_out_layers],\n\n                right_graph=right_graph,\n                n_right_inputs=len(right_in_layers),\n                right_names=[layer.name for layer in right_in_layers],\n            )\n        )\n\n    left_out_shapes = as_tuple(left_graph.output_shape)\n    right_in_shapes = as_tuple(right_graph.input_shape)\n\n    for left_layer, left_out_shape in zip(left_out_layers, left_out_shapes):\n        right = zip(right_in_layers, right_in_shapes)\n\n        for right_layer, right_in_shape in right:\n            if left_out_shape.is_compatible_with(right_in_shape):\n                continue\n\n            raise LayerConnectionError(\n                ""Cannot connect layer `{left_name}` to layer `{right_name}`, ""\n                ""because output shape ({left_out_shape}) of the first layer ""\n                ""is incompatible with the input shape ({right_in_shape}) ""\n                ""of the second layer. First layer: {left_layer}, Second ""\n                ""layer: {right_layer}"".format(\n                    left_layer=left_layer,\n                    left_name=left_layer.name,\n                    left_out_shape=left_out_shape,\n\n                    right_layer=right_layer,\n                    right_name=right_layer.name,\n                    right_in_shape=right_in_shape,\n                )\n            )\n\n\ndef merge(left_graph, right_graph, combine=False):\n    """"""\n    Merges two graphs into single one. When ``combine=False`` new\n    connection won\'t be created. And when ``combine=True`` input layers\n    from the ``left_graph`` will be combined to the output layers from\n    ``right_graph``.\n\n    Parameters\n    ----------\n    left_graph : layer, network\n    right_graph : layer, network\n\n    combine : bool\n        Defaults to ``False``.\n    """"""\n    if combine:\n        validate_graphs_before_combining(left_graph, right_graph)\n\n    forward_graph = OrderedDict()\n\n    for key, value in left_graph.forward_graph.items():\n        # To make sure that we copied lists inside of the\n        # dictionary, but didn\'t copied values inside of the list\n        forward_graph[key] = copy.copy(value)\n\n    for key, values in right_graph.forward_graph.items():\n        if key in forward_graph:\n            for value in values:\n                if value not in forward_graph[key]:\n                    forward_graph[key].append(value)\n        else:\n            forward_graph[key] = copy.copy(values)\n\n    if combine:\n        for left_out_layer in left_graph.output_layers:\n            for right_in_layer in right_graph.input_layers:\n                forward_graph[left_out_layer].append(right_in_layer)\n\n    if is_cyclic(forward_graph):\n        raise LayerConnectionError(\n            ""Cannot define connection between layers, because it creates ""\n            ""cycle in the graph. Left graph: {}, Right graph: {}""\n            """".format(left_graph, right_graph))\n\n    return LayerGraph(forward_graph)\n\n\ndef parallel(*networks):\n    """"""\n    Merges all networks/layers into single network without joining\n    input and output layers together.\n\n    Parameters\n    ----------\n    *networks\n        Layers or networks. Each network can be specified as a list or\n        tuple. In this case, this input will be passed to the ``join``\n        function in order to create full network from the specified\n        list of the connections.\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = parallel(\n    ...     Input(10),\n    ...     Input(9) >> Relu(5),\n    ...     Relu(5),\n    ... )\n    [(?, 10), (?, 9), <unknown>] -> [... 4 layers ...] -> \\\n    [(?, 10), (?, 5), (?, 5)]\n\n    Networks can be specified as a list of layers.\n\n    >>> from neupy.layers import *\n    >>> network = parallel([\n    ...     Input((28, 28, 1)),\n    ...     Convolution((3, 3, 16)),\n    ... ], [\n    ...     Input((28, 28, 1)),\n    ...     Convolution((3, 3, 12)),\n    ... ])\n    >>>\n    >>> network\n    [(?, 28, 28, 1), (?, 28, 28, 1)] -> [... 4 layers ...] -> \\\n    [(?, 26, 26, 16), (?, 26, 26, 12)]\n    """"""\n    graph = LayerGraph()\n\n    for network in networks:\n        if isinstance(network, (list, tuple)):\n            network = join(*network)\n        graph = merge(graph, network)\n\n    return graph\n\n\ndef join(*networks):\n    """"""\n    Sequentially combines layers and networks into single network.\n    Function will go sequentially over layers/networks and\n    combine output layers from the one network that it picks\n    to the input layers from the next network in the sequence.\n\n    Parameters\n    ----------\n    *networks\n        Layers or networks.\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((28, 28, 1)),\n    ...     Convolution((3, 3, 16)) >> Relu(),\n    ...     Convolution((3, 3, 16)) >> Relu(),\n    ...     Reshape(),\n    ...     Softmax(10),\n    ... )\n    >>> network\n    (?, 28, 28, 1) -> [... 7 layers ...] -> (?, 10)\n    """"""\n    graph = LayerGraph()\n\n    for network in networks:\n        graph = merge(graph, network, combine=True)\n\n    return graph\n\n\ndef repeat(network_or_layer, n):\n    """"""\n    Function copies input `n - 1` times and connects everything in sequential\n    order.\n\n    Parameters\n    ----------\n    network_or_layer : network or layer\n        Layer or network (connection of layers).\n\n    n : int\n        Number of times input should be replicated.\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>>\n    >>> block = Conv((3, 3, 32)) >> Relu() >> BN()\n    >>> block\n    <unknown> -> [... 3 layers ...] -> (?, ?, ?, 32)\n    >>>\n    >>> repeat(block, n=5)\n    <unknown> -> [... 15 layers ...] -> (?, ?, ?, 32)\n    """"""\n    if n <= 0 or not isinstance(n, int):\n        raise ValueError(\n            ""The `n` parameter should be a positive integer, ""\n            ""got {} instead"".format(n))\n\n    if n == 1:\n        return network_or_layer\n\n    input_shape = network_or_layer.input_shape\n    output_shape = network_or_layer.output_shape\n\n    if not input_shape.is_compatible_with(output_shape):\n        raise LayerConnectionError(\n            ""Cannot connect network/layer to its copy, because input ""\n            ""shape is incompatible with the output shape. Input shape: {}, ""\n            ""Output shape: {}"".format(input_shape, output_shape))\n\n    new_networks = [copy.deepcopy(network_or_layer) for _ in range(n - 1)]\n    return join(network_or_layer, *new_networks)\n'"
neupy/layers/merge.py,14,"b'import copy\nfrom functools import reduce\n\nimport tensorflow as tf\n\nfrom neupy.core.properties import FunctionWithOptionsProperty, IntProperty\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.utils import tf_utils\nfrom .base import BaseLayer\n\n\n__all__ = (\'Elementwise\', \'Concatenate\', \'GatedAverage\')\n\n\nclass Elementwise(BaseLayer):\n    """"""\n    Layer merges multiple input with elementwise function and generate\n    single output. Each input to this layer should have exactly the same\n    shape, otherwise it won\'t be possible to apply elementwise operation.\n\n    Parameters\n    ----------\n    merge_function : callable or {{``add``, ``multiply``}}\n        Callable object that accepts two inputs and\n        combines them in value using elementwise operation.\n\n        - ``add`` - Sum all the inputs. Alias to ``tf.add``.\n\n        - ``multiply`` - Multiplies all the inputs. Alias to ``tf.multiply``.\n\n        - Custom function requires to have two input arguments.\n\n        .. code-block:: python\n\n            def subtraction(x, y):\n                return x - y\n\n        Defaults to ``add``.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n    >>> from neupy import layers\n    >>> network = (Input(10) | Input(10)) >> Elementwise(\'add\')\n    [(?, 10), (?, 10)] -> [... 3 layers ...] -> (?, 10)\n    """"""\n    merge_function = FunctionWithOptionsProperty(choices={\n        \'add\': tf.add,\n        \'multiply\': tf.multiply,\n    })\n\n    def __init__(self, merge_function=\'add\', name=None):\n        super(Elementwise, self).__init__(name=name)\n        self.original_function = merge_function\n        self.merge_function = merge_function\n\n    def get_output_shape(self, *input_shapes):\n        input_shapes = [tf.TensorShape(shape) for shape in input_shapes]\n        first_shape = input_shapes[0]\n\n        if len(input_shapes) < 2:\n            raise LayerConnectionError(\n                ""Layer `{}` expected multiple inputs. Input shapes: {}""\n                """".format(self.name, tf_utils.shape_to_tuple(input_shapes)))\n\n        if any(shape.ndims is None for shape in input_shapes):\n            return tf.TensorShape(None)\n\n        for shape in input_shapes:\n            if not shape.is_compatible_with(first_shape):\n                formatted_shapes = tf_utils.shape_to_tuple(input_shapes)\n                raise LayerConnectionError(\n                    ""Input shapes to the `{}` layer have incompatible shapes. ""\n                    ""Input shapes: {}, Layer: {}""\n                    """".format(self.name, formatted_shapes, self))\n\n        return first_shape\n\n    def output(self, *inputs, **kwargs):\n        return reduce(self.merge_function, inputs)\n\n    def __repr__(self):\n        return self._repr_arguments(\n            repr(self.original_function), name=self.name)\n\n\nclass Concatenate(BaseLayer):\n    """"""\n    Concatenate multiple inputs into one. Inputs will be concatenated over\n    the specified axis (controlled with parameter ``axis``).\n\n    Parameters\n    ----------\n    axis : int\n        The axis along which the inputs will be concatenated.\n        Default is ``-1``.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = (Input(10) | Input(20)) >> Concatenate()\n    [(?, 10), (?, 20)] -> [... 3 layers ...] -> (?, 30)\n    """"""\n    axis = IntProperty()\n\n    def __init__(self, axis=-1, name=None):\n        super(Concatenate, self).__init__(name=name)\n        self.axis = axis\n\n    def get_output_shape(self, *input_shapes):\n        input_shapes = [tf.TensorShape(shape) for shape in input_shapes]\n        # The axis value has 0-based indices where 0s index points\n        # to the batch dimension of the input. Shapes in the neupy\n        # do not store information about the batch and we need to\n        # put None value on the 0s position.\n        valid_shape = input_shapes[0]\n\n        if any(shape.ndims is None for shape in input_shapes):\n            return tf.TensorShape(None)\n\n        # Avoid using negative indices\n        possible_axes = list(range(len(valid_shape)))\n        concat_axis = possible_axes[self.axis]\n\n        for input_shape in input_shapes[1:]:\n            if len(valid_shape) != len(input_shape):\n                raise LayerConnectionError(\n                    ""Cannot concatenate layers, because inputs have ""\n                    ""different number of dimensions. Shapes: {} and {}""\n                    """".format(valid_shape, input_shape))\n\n            for axis, axis_size in enumerate(input_shape):\n                if axis != concat_axis and valid_shape[axis] != axis_size:\n                    raise LayerConnectionError(\n                        ""Cannot concatenate layers, because some of them ""\n                        ""don\'t match over dimension #{} (0-based indices). ""\n                        ""Shapes: {} and {}""\n                        """".format(axis, valid_shape, input_shape))\n\n        output_shape = input_shapes.pop(0)\n        output_shape = [dim.value for dim in output_shape.dims]\n\n        for input_shape in input_shapes:\n            output_shape[self.axis] += input_shape[self.axis]\n\n        return tf.TensorShape(output_shape)\n\n    def output(self, *inputs, **kwargs):\n        return tf.concat(inputs, axis=self.axis)\n\n\ndef exclude_index(array, index):\n    """"""\n    Copies array and exclude single element in specific\n    index position.\n\n    Parameters\n    ----------\n    array : list or tuple\n\n    index : int\n        Index of the value that has to be excluded from the array\n\n    Returns\n    -------\n    list\n    """"""\n    array = list(array)\n    copied_array = copy.copy(array)\n    copied_array.pop(index)\n    return copied_array\n\n\nclass GatedAverage(BaseLayer):\n    """"""\n    Layer uses applies weighted elementwise addition to multiple outputs.\n    Weight can be control using separate input known as **gate**. Number\n    of outputs from the gate has to be equal to the number of networks,\n    since each value from the weight will be a weight per each network.\n\n    Layer expects gate as a first input, but it can be controlled with\n    the ``gate_index`` parameter.\n\n    Parameters\n    ----------\n    gate_index : int\n        Input layers passed as a list and current variable specifies\n        index in which it can find gating network. Defaults to ``0``,\n        which means that it expects to see gating layer in first position.\n\n    {BaseLayer.Parameters}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>>\n    >>> gate = Input(10) >> Softmax(2)\n    >>> net1 = Input(20) >> Relu(10)\n    >>> net2 = Input(20) >> Relu(20) >> Relu(10)\n    >>>\n    >>> network = (gate | net1 | net2) >> GatedAverage()\n    >>> network\n    [(10,), (20,), (20,)] -> [... 8 layers ...] -> 10\n    """"""\n    gate_index = IntProperty(default=0)\n\n    def __init__(self, gate_index=0, name=None):\n        super(GatedAverage, self).__init__(name=name)\n        self.gate_index = gate_index\n\n    def fail_if_shape_invalid(self, input_shapes):\n        n_input_layers = len(input_shapes)\n\n        try:\n            gate_shape = input_shapes[self.gate_index]\n        except IndexError:\n            raise LayerConnectionError(\n                ""Invalid index for gating layer. Number of input ""\n                ""layers: {}. Gating layer index: {}""\n                """".format(n_input_layers, self.gate_index))\n\n        other_shapes = exclude_index(input_shapes, self.gate_index)\n        if gate_shape and len(gate_shape) != 2:\n            raise LayerConnectionError(\n                ""Output from the gating network should be 2-dimensional. ""\n                ""Output shape from gating layer: {!r}""\n                """".format(gate_shape))\n\n        n_expected_networks = gate_shape[-1]\n        # Note: -1 from all layers in order to exclude gating layer\n        if n_expected_networks != (n_input_layers - 1):\n            raise LayerConnectionError(\n                ""Gating layer can work only for combining only {} networks, ""\n                ""got {} networks instead.""\n                """".format(n_expected_networks, (n_input_layers - 1)))\n\n        for shape in other_shapes:\n            if not shape.is_compatible_with(other_shapes[0]):\n                raise LayerConnectionError(\n                    ""Output layer that has to be merged expect to ""\n                    ""have the same shapes. Shapes: {!r}""\n                    """".format(tf_utils.shape_to_tuple(other_shapes)))\n\n    def get_output_shape(self, *input_shapes):\n        input_shapes = [tf.TensorShape(shape) for shape in input_shapes]\n\n        if any(shape.ndims is None for shape in input_shapes):\n            return tf.TensorShape(None)\n\n        self.fail_if_shape_invalid(input_shapes)\n\n        if self.gate_index >= 0:\n            # Take layer from the left side from the gating layer.\n            # In case if gating layer at the zeros position then\n            # it will take the last layer (-1 index).\n            return input_shapes[self.gate_index - 1]\n\n        # In case if it negative index, we take layer from the right side\n        return input_shapes[self.gate_index + 1]\n\n    def output(self, *inputs, **kwargs):\n        gating_value = inputs[self.gate_index]\n        other_values = exclude_index(inputs, self.gate_index)\n        output_values = []\n\n        for i, other_value in enumerate(other_values):\n            n_feature_dim = other_value.shape.ndims - 1\n            gate = tf.reshape(gating_value[:, i], [-1] + [1] * n_feature_dim)\n            output_value = tf.multiply(other_value, gate)\n            output_values.append(output_value)\n\n        return sum(output_values)\n'"
neupy/layers/normalization.py,18,"b'import tensorflow as tf\n\nfrom neupy.core.properties import (\n    ProperFractionProperty,\n    ParameterProperty,\n    TypedListProperty,\n    NumberProperty,\n    IntProperty,\n)\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import (\n    WeightInitializationError,\n    LayerConnectionError,\n)\nfrom .base import Identity\n\n\n__all__ = (\'BatchNorm\', \'LocalResponseNorm\', \'GroupNorm\')\n\n\nclass BatchNorm(Identity):\n    """"""\n    Batch normalization layer.\n\n    Parameters\n    ----------\n    axes : tuple with ints or None\n        Axes along which normalization will be applied. The ``None``\n        value means that normalization will be applied over all axes\n        except the last one. In case of 4D tensor it will\n        be equal to ``(0, 1, 2)``. Defaults to ``None``.\n\n    epsilon : float\n        Epsilon is a positive constant that adds to the standard\n        deviation to prevent the division by zero.\n        Defaults to ``1e-5``.\n\n    alpha : float\n        Coefficient for the exponential moving average of\n        batch-wise means and standard deviations computed during\n        training; the closer to one, the more it will depend on\n        the last batches seen. Value needs to be between ``0`` and ``1``.\n        Defaults to ``0.1``.\n\n    gamma : array-like, Tensorfow variable, scalar or Initializer\n        Scale. Default initialization methods you can\n        find :ref:`here <init-methods>`.\n        Defaults to ``Constant(value=1)``.\n\n    beta : array-like, Tensorfow variable, scalar or Initializer\n        Offset. Default initialization methods you can\n        find :ref:`here <init-methods>`.\n        Defaults to ``Constant(value=0)``.\n\n    running_mean : array-like, Tensorfow variable, scalar or Initializer\n        Default initialization methods you can\n        find :ref:`here <init-methods>`.\n        Defaults to ``Constant(value=0)``.\n\n    running_inv_std : array-like, Tensorfow variable, scalar or Initializer\n        Default initialization methods you can\n        find :ref:`here <init-methods>`.\n        Defaults to ``Constant(value=1)``.\n\n    {Identity.name}\n\n    Methods\n    -------\n    {Identity.Methods}\n\n    Attributes\n    ----------\n    {Identity.Attributes}\n\n    Examples\n    --------\n\n    Feedforward Neural Networks (FNN) with batch normalization after\n    activation function was applied.\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input(10),\n    ...     Relu(5) >> BatchNorm(),\n    ...     Relu(5) >> BatchNorm(),\n    ...     Sigmoid(1),\n    ... )\n\n    Feedforward Neural Networks (FNN) with batch normalization before\n    activation function was applied.\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input(10),\n    ...     Linear(5) >> BatchNorm() >> Relu(),\n    ...     Linear(5) >> BatchNorm() >> Relu(),\n    ...     Sigmoid(1),\n    ... )\n\n    Convolutional Neural Networks (CNN)\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((28, 28, 1)),\n    ...     Convolution((3, 3, 16)) >> BatchNorm() >> Relu(),\n    ...     Convolution((3, 3, 16)) >> BatchNorm() >> Relu(),\n    ...     Reshape(),\n    ...     Softmax(10),\n    ... )\n\n    References\n    ----------\n    .. [1] Batch Normalization: Accelerating Deep Network Training\n           by Reducing Internal Covariate Shift,\n           http://arxiv.org/pdf/1502.03167v3.pdf\n    """"""\n    axes = TypedListProperty(allow_none=True)\n    epsilon = NumberProperty(minval=0)\n    alpha = ProperFractionProperty()\n    beta = ParameterProperty()\n    gamma = ParameterProperty()\n\n    running_mean = ParameterProperty()\n    running_inv_std = ParameterProperty()\n\n    def __init__(self, axes=None, alpha=0.1, beta=0, gamma=1, epsilon=1e-5,\n                 running_mean=0, running_inv_std=1, name=None):\n\n        super(BatchNorm, self).__init__(name=name)\n\n        self.axes = axes\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.running_mean = running_mean\n        self.running_inv_std = running_inv_std\n\n        if axes is not None and len(set(axes)) != len(axes):\n            raise ValueError(\n                ""Specified axes have to contain only unique values"")\n\n    def create_variables(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n\n        if input_shape.ndims is None:\n            raise WeightInitializationError(\n                ""Cannot initialize variables for the batch normalization ""\n                ""layer, because input shape is undefined. Layer: {}""\n                """".format(self))\n\n        if self.axes is None:\n            # If ndims == 4 then axes = (0, 1, 2)\n            # If ndims == 2 then axes = (0,)\n            self.axes = tuple(range(input_shape.ndims - 1))\n\n        if any(axis >= input_shape.ndims for axis in self.axes):\n            raise LayerConnectionError(\n                ""Batch normalization cannot be applied over one of ""\n                ""the axis, because input has only {} dimensions. Layer: {}""\n                """".format(input_shape.ndims, self))\n\n        parameter_shape = tuple([\n            input_shape[axis].value if axis not in self.axes else 1\n            for axis in range(input_shape.ndims)\n        ])\n\n        if any(parameter is None for parameter in parameter_shape):\n            unknown_dim_index = parameter_shape.index(None)\n\n            raise WeightInitializationError(\n                ""Cannot create variables for batch normalization, because ""\n                ""input has unknown dimension #{} (0-based indices). ""\n                ""Input shape: {}, Layer: {}"".format(\n                    unknown_dim_index, input_shape, self))\n\n        self.input_shape = input_shape\n        self.running_mean = self.variable(\n            value=self.running_mean, shape=parameter_shape,\n            name=\'running_mean\', trainable=False)\n\n        self.running_inv_std = self.variable(\n            value=self.running_inv_std, shape=parameter_shape,\n            name=\'running_inv_std\', trainable=False)\n\n        self.gamma = self.variable(\n            value=self.gamma, name=\'gamma\',\n            shape=parameter_shape)\n\n        self.beta = self.variable(\n            value=self.beta, name=\'beta\',\n            shape=parameter_shape)\n\n    def output(self, input, training=False):\n        input = tf.convert_to_tensor(input, dtype=tf.float32)\n\n        if not training:\n            mean = self.running_mean\n            inv_std = self.running_inv_std\n        else:\n            alpha = asfloat(self.alpha)\n            mean = tf.reduce_mean(\n                input, self.axes,\n                keepdims=True, name=""mean"",\n            )\n            variance = tf.reduce_mean(\n                tf.squared_difference(input, tf.stop_gradient(mean)),\n                self.axes,\n                keepdims=True,\n                name=""variance"",\n            )\n            inv_std = tf.rsqrt(variance + asfloat(self.epsilon))\n\n            tf.add_to_collection(\n                tf.GraphKeys.UPDATE_OPS,\n                self.running_inv_std.assign(\n                    asfloat(1 - alpha) * self.running_inv_std + alpha * inv_std\n                )\n            )\n            tf.add_to_collection(\n                tf.GraphKeys.UPDATE_OPS,\n                self.running_mean.assign(\n                    asfloat(1 - alpha) * self.running_mean + alpha * mean\n                )\n            )\n\n        normalized_value = (input - mean) * inv_std\n        return self.gamma * normalized_value + self.beta\n\n\nclass LocalResponseNorm(Identity):\n    """"""\n    Local Response Normalization Layer.\n\n    Aggregation is purely across channels, not within channels,\n    and performed ""pixelwise"".\n\n    If the value of the :math:`i` th channel is :math:`x_i`, the output is\n\n    .. math::\n        x_i = \\\\frac{{x_i}}{{ (k + ( \\\\alpha \\\\sum_j x_j^2 ))^\\\\beta }}\n\n    where the summation is performed over this position on :math:`n`\n    neighboring channels.\n\n    Parameters\n    ----------\n    alpha : float\n        Coefficient, see equation above. Defaults to ``1e-4``.\n\n    beta : float\n        Offset, see equation above. Defaults to ``0.75``.\n\n    k : float\n        Exponent, see equation above. Defaults to ``2``.\n\n    depth_radius : int\n        Number of adjacent channels to normalize over, must be odd.\n        Defaults to ``5``.\n\n    {Identity.name}\n\n    Methods\n    -------\n    {Identity.Methods}\n\n    Attributes\n    ----------\n    {Identity.Attributes}\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = Input((10, 10, 12)) >> LocalResponseNorm()\n    """"""\n    alpha = NumberProperty()\n    beta = NumberProperty()\n    k = NumberProperty()\n    depth_radius = IntProperty()\n\n    def __init__(self, alpha=1e-4, beta=0.75, k=2, depth_radius=5, name=None):\n        super(LocalResponseNorm, self).__init__(name=name)\n\n        if depth_radius % 2 == 0:\n            raise ValueError(""Only works with odd `depth_radius` values"")\n\n        self.alpha = alpha\n        self.beta = beta\n        self.k = k\n        self.depth_radius = depth_radius\n\n    def get_output_shape(self, input_shape):\n        if input_shape and input_shape.ndims != 4:\n            raise LayerConnectionError(\n                ""Layer `{}` expected input with 4 dimensions, got {} instead. ""\n                ""Shape: {}"".format(self.name, input_shape.ndims, input_shape))\n\n        return super(LocalResponseNorm, self).get_output_shape(input_shape)\n\n    def output(self, input, **kwargs):\n        return tf.nn.local_response_normalization(\n            input,\n            depth_radius=self.depth_radius,\n            bias=self.k,\n            alpha=self.alpha,\n            beta=self.beta)\n\n\nclass GroupNorm(Identity):\n    """"""\n    Group Normalization layer. This layer is a simple alternative to the\n    Batch Normalization layer for cases when batch size is small.\n\n    Parameters\n    ----------\n    n_groups : int\n        During normalization all the channels will be break down into\n        separate groups and mean and variance will be estimated per group.\n        This parameter controls number of groups.\n\n    gamma : array-like, Tensorfow variable, scalar or Initializer\n        Scale. Default initialization methods you can\n        find :ref:`here <init-methods>`.\n        Defaults to ``Constant(value=1)``.\n\n    beta : array-like, Tensorfow variable, scalar or Initializer\n        Offset. Default initialization methods you can\n        find :ref:`here <init-methods>`.\n        Defaults to ``Constant(value=0)``.\n\n    epsilon : float\n        Epsilon ensures that input rescaling procedure that uses estimated\n        variance will never cause division by zero. Defaults to ``1e-5``.\n\n    {Identity.name}\n\n    Methods\n    -------\n    {Identity.Methods}\n\n    Attributes\n    ----------\n    {Identity.Attributes}\n\n    Examples\n    --------\n    Convolutional Neural Networks (CNN)\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((28, 28, 1)),\n    ...     Convolution((3, 3, 16)) >> GroupNorm(4) >> Relu(),\n    ...     Convolution((3, 3, 16)) >> GroupNorm(4) >> Relu(),\n    ...     Reshape(),\n    ...     Softmax(10),\n    ... )\n\n    References\n    ----------\n    .. [1] Group Normalization, Yuxin Wu, Kaiming He,\n           https://arxiv.org/pdf/1803.08494.pdf\n    """"""\n    n_groups = IntProperty(minval=1)\n    beta = ParameterProperty()\n    gamma = ParameterProperty()\n    epsilon = NumberProperty(minval=0)\n\n    def __init__(self, n_groups, beta=0, gamma=1, epsilon=1e-5, name=None):\n        super(GroupNorm, self).__init__(name=name)\n\n        self.n_groups = n_groups\n        self.beta = beta\n        self.gamma = gamma\n        self.epsilon = epsilon\n\n    def create_variables(self, input_shape):\n        n_channels = input_shape[3]\n\n        if n_channels.value is None:\n            raise WeightInitializationError(\n                ""Cannot initialize variables when number of ""\n                ""channels is unknown. Input shape: {}, Layer: {}""\n                """".format(input_shape, self))\n\n        parameter_shape = (1, 1, 1, n_channels)\n\n        self.gamma = self.variable(\n            value=self.gamma, name=\'gamma\',\n            shape=parameter_shape)\n\n        self.beta = self.variable(\n            value=self.beta, name=\'beta\',\n            shape=parameter_shape)\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n\n        if input_shape and input_shape.ndims != 4:\n            raise LayerConnectionError(\n                ""Group normalization layer expects 4 dimensional input, ""\n                ""got {} instead. Input shape: {}, Layer: {}""\n                """".format(input_shape.ndims, input_shape, self))\n\n        n_channels = input_shape[3]\n\n        if n_channels.value and n_channels % self.n_groups != 0:\n            raise LayerConnectionError(\n                ""Cannot divide {} input channels into {} groups. ""\n                ""Input shape: {}, Layer: {}"".format(\n                    n_channels, self.n_groups, input_shape, self))\n\n        return super(GroupNorm, self).get_output_shape(input_shape)\n\n    def output(self, input):\n        input = tf.convert_to_tensor(input, dtype=tf.float32)\n        input_shape = tf.shape(input)\n        n_groups = self.n_groups\n\n        # We access dimensional information in form of tensors in case\n        # if some of the dimensions are undefined. In this way we make\n        # sure that reshape will work even if part of the input shape\n        # is undefined.\n        dims = [input_shape[i] for i in range(4)]\n        n_samples, height, width, n_channels = dims\n\n        input = tf.reshape(input, [\n            n_samples, height, width, n_groups, n_channels // n_groups])\n\n        mean, variance = tf.nn.moments(input, [1, 2, 4], keep_dims=True)\n        input = (input - mean) / tf.sqrt(variance + self.epsilon)\n        input = tf.reshape(input, input_shape)\n\n        return input * self.gamma + self.beta\n'"
neupy/layers/pooling.py,17,"b'from __future__ import division\n\nimport math\n\nimport tensorflow as tf\n\nfrom neupy.utils import as_tuple, tf_utils\nfrom neupy.core.properties import (TypedListProperty, ChoiceProperty,\n                                   FunctionWithOptionsProperty)\nfrom neupy.exceptions import LayerConnectionError\nfrom .base import BaseLayer\nfrom .convolutions import Spatial2DProperty\n\n\n__all__ = (\'MaxPooling\', \'AveragePooling\', \'Upscale\', \'GlobalPooling\')\n\n\ndef pooling_output_shape(dimension_size, pool_size, padding, stride):\n    """"""\n    Computes output shape for pooling operation.\n\n    Parameters\n    ----------\n    dimension_size : int\n        Size of the dimension. Typically it\'s image\'s\n        weight or height.\n\n    filter_size : int\n        Size of the pooling filter.\n\n    padding : int\n        Size of the zero-padding.\n\n    stride : int\n        Stride size.\n\n    Returns\n    -------\n    int\n    """"""\n    if isinstance(dimension_size, tf.Dimension):\n        dimension_size = dimension_size.value\n\n    if dimension_size is None:\n        return None\n\n    if padding in (\'SAME\', \'same\'):\n        return int(math.ceil(dimension_size / stride))\n\n    elif padding in (\'VALID\', \'valid\'):\n        return int(math.ceil((dimension_size - pool_size + 1) / stride))\n\n    raise ValueError(\n        ""{!r} is unknown padding value for pooling"".format(padding))\n\n\nclass BasePooling(BaseLayer):\n    """"""\n    Base class for the pooling layers.\n\n    Parameters\n    ----------\n    size : tuple with 2 integers\n        Factor by which to downscale ``(vertical, horizontal)``.\n        ``(2, 2)`` will halve the image in each dimension.\n\n    stride : tuple or int.\n        Stride size, which is the number of shifts over\n        rows/cols to get the next pool region. If stride is\n        ``None``, it is considered equal to ``size`` (no overlap on\n        pooling regions).\n\n    padding : {{``valid``, ``same``}}\n        ``(pad_h, pad_w)``, pad zeros to extend beyond four borders of\n        the images, pad_h is the size of the top and bottom margins,\n        and pad_w is the size of the left and right margins.\n\n    {BaseLayer.Parameters}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n    """"""\n    size = TypedListProperty(required=True, element_type=int)\n    stride = Spatial2DProperty(allow_none=True)\n    padding = ChoiceProperty(choices=(\'SAME\', \'VALID\', \'same\', \'valid\'))\n    pooling_type = None\n\n    def __init__(self, size, stride=None, padding=\'valid\', name=None):\n        super(BasePooling, self).__init__(name=name)\n\n        self.size = size\n        self.stride = stride\n        self.padding = padding\n\n    def fail_if_shape_invalid(self, input_shape):\n        if input_shape and input_shape.ndims != 4:\n            raise LayerConnectionError(\n                ""Pooling layer expects an input with 4 ""\n                ""dimensions, got {} with shape {}. Layer: {}""\n                """".format(len(input_shape), input_shape, self))\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n\n        if input_shape.ndims is None:\n            return tf.TensorShape((None, None, None, None))\n\n        self.fail_if_shape_invalid(input_shape)\n\n        n_samples, rows, cols, n_kernels = input_shape\n        row_filter_size, col_filter_size = self.size\n\n        stride = self.size if self.stride is None else self.stride\n        row_stride, col_stride = stride\n\n        output_rows = pooling_output_shape(\n            rows, row_filter_size, self.padding, row_stride)\n\n        output_cols = pooling_output_shape(\n            cols, col_filter_size, self.padding, col_stride)\n\n        # In python 2, we can get float number after rounding procedure\n        # and it might break processing in the subsequent layers.\n        return tf.TensorShape((n_samples, output_rows, output_cols, n_kernels))\n\n    def output(self, input_value, **kwargs):\n        return tf.nn.pool(\n            input_value,\n            self.size,\n            pooling_type=self.pooling_type,\n            padding=self.padding.upper(),\n            strides=self.stride or self.size,\n            data_format=""NHWC"")\n\n    def __repr__(self):\n        return self._repr_arguments(\n            self.size,\n            name=self.name,\n            stride=self.stride,\n            padding=self.padding,\n        )\n\n\nclass MaxPooling(BasePooling):\n    """"""\n    Maximum pooling layer.\n\n    Parameters\n    ----------\n    {BasePooling.Parameters}\n\n    Methods\n    -------\n    {BasePooling.Methods}\n\n    Attributes\n    ----------\n    {BasePooling.Attributes}\n\n    Examples\n    --------\n    2D pooling\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((10, 10, 3)),\n    ...     MaxPooling((2, 2)),\n    ... )\n    >>> network\n    (?, 10, 10, 3) -> [... 2 layers ...] -> (?, 5, 5, 3)\n\n    1D pooling\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((30, 10)),\n    ...     Reshape((30, 1, 10)),\n    ...     MaxPooling((2, 1)),\n    ...     Reshape((-1, 10))\n    ... )\n    >>> network\n    (?, 30, 10) -> [... 4 layers ...] -> (?, 15, 10)\n    """"""\n    pooling_type = \'MAX\'\n\n\nclass AveragePooling(BasePooling):\n    """"""\n    Average pooling layer.\n\n    Parameters\n    ----------\n    {BasePooling.Parameters}\n\n    Methods\n    -------\n    {BasePooling.Methods}\n\n    Attributes\n    ----------\n    {BasePooling.Attributes}\n\n    Examples\n    --------\n    2D pooling\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((10, 10, 3)),\n    ...     AveragePooling((2, 2)),\n    ... )\n    >>> network\n    (?, 10, 10, 3) -> [... 2 layers ...] -> (?, 5, 5, 3)\n\n    1D pooling\n\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((30, 10)),\n    ...     Reshape((30, 1, 10)),\n    ...     AveragePooling((2, 1)),\n    ...     Reshape((-1, 10))\n    ... )\n    >>> network\n    (?, 30, 10) -> [... 4 layers ...] -> (?, 15, 10)\n    """"""\n    pooling_type = \'AVG\'\n\n\nclass Upscale(BaseLayer):\n    """"""\n    Upscales input over two axis (height and width).\n\n    Parameters\n    ----------\n    scale : int or tuple with two int\n        Scaling factor for the input value. In the tuple first\n        parameter identifies scale of the height and the second\n        one of the width.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = Input((10, 10, 3)) >> Upscale((2, 2))\n    (?, 10, 10, 3) -> [... 2 layers ...] -> (?, 20, 20, 3)\n    """"""\n    scale = TypedListProperty(n_elements=2)\n\n    def __init__(self, scale, name=None):\n        super(Upscale, self).__init__(name=name)\n\n        if isinstance(scale, int):\n            scale = as_tuple(scale, scale)\n\n        if any(element <= 0 for element in scale):\n            raise ValueError(\n                ""Only positive integers are allowed for scale"")\n\n        self.scale = scale\n\n    def fail_if_shape_invalid(self, input_shape):\n        if input_shape and input_shape.ndims != 4:\n            raise LayerConnectionError(\n                ""Upscale layer should have an input value with 4 dimensions ""\n                ""(batch, height, width, channel), got input with {} ""\n                ""dimensions instead. Shape: {}""\n                """".format(input_shape.ndims, input_shape))\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n        self.fail_if_shape_invalid(input_shape)\n\n        if input_shape.ndims is None:\n            return tf.TensorShape((None, None, None, None))\n\n        n_samples, height, width, channel = input_shape\n        height_scale, width_scale = self.scale\n\n        return tf.TensorShape([\n            n_samples,\n            height_scale * height,\n            width_scale * width,\n            channel,\n        ])\n\n    def output(self, input_value, **kwargs):\n        input_value = tf.convert_to_tensor(input_value, dtype=tf.float32)\n        self.fail_if_shape_invalid(input_value.shape)\n        return tf_utils.repeat(input_value, as_tuple(1, self.scale, 1))\n\n    def __repr__(self):\n        return self._repr_arguments(self.scale, name=self.name)\n\n\nclass GlobalPooling(BaseLayer):\n    """"""\n    Global pooling layer.\n\n    Parameters\n    ----------\n    function : {{``avg``, ``max``, ``sum``}} or callable\n        Common functions has been predefined for the user.\n        These options are available:\n\n        - ``avg`` - For average global pooling. The same as\n          ``tf.reduce_mean``.\n\n        - ``max`` - For max global pooling. The same as\n          ``tf.reduce_max``.\n\n        - ``sum`` - For sum global pooling. The same as\n          ``tf.reduce_sum``.\n\n        Parameter also excepts custom functions that have\n        following format.\n\n        .. code-block:: python\n\n            def agg_func(x, axis=None):\n                pass\n\n        Defaults to ``avg``.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = Input((4, 4, 16)) >> GlobalPooling(\'avg\')\n    (?, 4, 4, 16) -> [... 2 layers ...] -> (?, 16)\n    """"""\n    function = FunctionWithOptionsProperty(choices={\n        \'avg\': tf.reduce_mean,\n        \'max\': tf.reduce_max,\n        \'sum\': tf.reduce_sum,\n    })\n\n    def __init__(self, function, name=None):\n        super(GlobalPooling, self).__init__(name=name)\n        self.original_function = function\n        self.function = function\n\n    def get_output_shape(self, input_shape):\n        return tf.TensorShape([input_shape[0], input_shape[-1]])\n\n    def output(self, input_value, **kwargs):\n        input_value = tf.convert_to_tensor(input_value, dtype=tf.float32)\n        ndims = len(input_value.shape)\n\n        if ndims == 2:\n            return input_value\n\n        # All dimensions except first and last\n        agg_axis = range(1, ndims - 1)\n        return self.function(input_value, axis=list(agg_axis))\n\n    def __repr__(self):\n        return self._repr_arguments(\n            repr(self.original_function), name=self.name)\n'"
neupy/layers/recurrent.py,46,"b'import types\n\nimport tensorflow as tf\n\nfrom neupy import init\nfrom neupy.utils import tensorflow_session\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.core.properties import (\n    IntProperty, Property,\n    NumberProperty, ParameterProperty,\n)\nfrom .base import BaseLayer\n\n\n__all__ = (\'LSTM\', \'GRU\')\n\n\ndef clip_gradient(value, clip_value):\n    if not hasattr(clip_gradient, \'added_gradients\'):\n        clip_gradient.added_gradients = set()\n\n    session = tensorflow_session()\n    graph = session.graph\n    operation_name = ""ClipGradient-"" + str(clip_value)\n\n    if operation_name not in clip_gradient.added_gradients:\n        # Make sure that we won\'t create the same operation twice.\n        # Otherwise tensorflow will trigger an exception.\n        @tf.RegisterGradient(operation_name)\n        def clip_gradient_grad(op, grad):\n            return tf.clip_by_value(grad, -clip_value, clip_value)\n\n        clip_gradient.added_gradients.add(operation_name)\n\n    with graph.gradient_override_map({""Identity"": operation_name}):\n        return tf.identity(value)\n\n\ndef unroll_scan(fn, sequence, outputs_info):\n    """"""\n    Helper function to unroll for loops. Can be used to unroll\n    ``tensorflow.scan``.\n\n    Parameters\n    ----------\n    fn : function\n        Function that defines calculations at each step.\n\n    sequences : TensorVariable or list of TensorVariables\n        List of TensorVariable with sequence data. The function iterates\n        over the first dimension of each TensorVariable.\n\n    outputs_info : list of TensorVariables\n        List of tensors specifying the initial values for each recurrent\n        value.\n\n    Returns\n    -------\n    List of TensorVariables. Each element in the list gives\n    the recurrent values at each time step.\n    """"""\n    with tf.name_scope(\'unroll-scan\'):\n        outputs = []\n        prev_vals = outputs_info\n\n        for entity in tf.unstack(sequence):\n            output = fn(prev_vals, entity)\n            outputs.append(output[-1])\n            prev_vals = output\n\n        return tf.stack(outputs)\n\n\nclass BaseRNNLayer(BaseLayer):\n    """"""\n    Base class for the recurrent layers\n\n    Parameters\n    ----------\n    n_units : int\n        Number of hidden units in the layer.\n\n    only_return_final : bool\n        If ``True``, only return the final sequential output\n        (e.g. for tasks where a single target value for the entire\n        sequence is desired). In this case, Tensorfow makes an\n        optimization which saves memory. Defaults to ``True``.\n\n    {BaseLayer.name}\n    """"""\n    n_units = IntProperty(minval=1)\n    only_return_final = Property(expected_type=bool)\n\n    def __init__(self, n_units, only_return_final=True, name=None):\n        super(BaseRNNLayer, self).__init__(name=name)\n        self.only_return_final = only_return_final\n        self.n_units = n_units\n\n    def fail_if_shape_invalid(self, input_shape):\n        if input_shape and input_shape.ndims != 3:\n            clsname = self.__class__.__name__\n            raise LayerConnectionError(\n                ""{} layer was expected input with three dimensions, ""\n                ""but got input with {} dimensions instead. Layer: {}""\n                """".format(clsname, input_shape.ndims, self))\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n        n_samples = input_shape[0]\n\n        self.fail_if_shape_invalid(input_shape)\n\n        if self.only_return_final:\n            return tf.TensorShape((n_samples, self.n_units))\n\n        n_time_steps = input_shape[1]\n        return tf.TensorShape((n_samples, n_time_steps, self.n_units))\n\n\nclass LSTM(BaseRNNLayer):\n    """"""\n    Long Short Term Memory (LSTM) Layer.\n\n    Parameters\n    ----------\n    {BaseRNNLayer.n_units}\n\n    {BaseRNNLayer.only_return_final}\n\n    input_weights : Initializer, ndarray\n        Weight parameters for input connection.\n        Defaults to :class:`HeNormal() <neupy.init.HeNormal>`.\n\n    hidden_weights : Initializer, ndarray\n        Weight parameters for hidden connection.\n        Defaults to :class:`HeNormal() <neupy.init.HeNormal>`.\n\n    cell_weights : Initializer, ndarray\n        Weight parameters for cell connection. Require only when\n        ``peepholes=True`` otherwise it will be ignored.\n        Defaults to :class:`HeNormal() <neupy.init.HeNormal>`.\n\n    bias : Initializer, ndarray\n        Bias parameters for all gates.\n        Defaults to :class:`Constant(0) <neupy.init.Constant>`.\n\n    ingate : function\n        Activation function for the input gate.\n        Defaults to ``tf.nn.sigmoid``.\n\n    forgetgate : function\n        Activation function for the forget gate.\n        Defaults to ``tf.nn.sigmoid``.\n\n    outgate : function\n        Activation function for the output gate.\n        Defaults to ``tf.nn.sigmoid``.\n\n    cell : function\n        Activation function for the cell.\n        Defaults to ``tf.tanh``.\n\n    learn_init : bool\n        If ``True``, make ``cell_init`` and ``hidden_init`` trainable\n        variables. Defaults to ``False``.\n\n    cell_init : array-like, Tensorfow variable, scalar or Initializer\n        Initializer for initial cell state (:math:`c_0`).\n        Defaults to :class:`Constant(0) <neupy.init.Constant>`.\n\n    hidden_init : array-like, Tensorfow variable, scalar or Initializer\n        Initializer for initial hidden state (:math:`h_0`).\n        Defaults to :class:`Constant(0) <neupy.init.Constant>`.\n\n    backwards : bool\n        If ``True``, process the sequence backwards and then reverse the\n        output again such that the output from the layer is always\n        from :math:`x_1` to :math:`x_n`. Defaults to ``False``\n\n    peepholes : bool\n        If ``True``, the LSTM uses peephole connections.\n        When ``False``, cell parameters  are ignored.\n        Defaults to ``False``.\n\n    unroll_scan : bool\n        If ``True`` the recursion is unrolled instead of using scan.\n        For some graphs this gives a significant speed up but it\n        might also consume more memory. When ``unroll_scan=True``,\n        backpropagation always includes the full sequence, so\n        ``n_gradient_steps`` must be set to ``-1`` and the input\n        sequence length must be known at compile time (i.e.,\n        cannot be given as ``None``). Defaults to ``False``.\n\n    gradient_clipping : float or int\n        If nonzero, the gradient messages are clipped to the\n        given value during the backward pass. Defaults to ``0``.\n\n    {BaseLayer.name}\n\n    Notes\n    -----\n    Code was adapted from the\n    `Lasagne <https://github.com/Lasagne/Lasagne>`_ library.\n\n    Examples\n    --------\n    Sequence classification\n\n    >>> from neupy.layers import *\n    >>>\n    >>> n_time_steps = 40\n    >>> n_categories = 20\n    >>> embedded_size = 10\n    >>>\n    >>> network = join(\n    ...     Input(n_time_steps),\n    ...     Embedding(n_categories, embedded_size),\n    ...     LSTM(20),\n    ...     Sigmoid(1),\n    ... )\n    >>> network\n    (?, 40) -> [... 4 layers ...] -> (?, 1)\n    """"""\n    input_weights = ParameterProperty()\n    hidden_weights = ParameterProperty()\n    cell_weights = ParameterProperty()\n    biases = ParameterProperty()\n\n    ingate = Property(expected_type=types.FunctionType)\n    forgetgate = Property(expected_type=types.FunctionType)\n    outgate = Property(expected_type=types.FunctionType)\n    cell = Property(expected_type=types.FunctionType)\n\n    learn_init = Property(expected_type=bool)\n    cell_init = ParameterProperty()\n    hidden_init = ParameterProperty()\n\n    unroll_scan = Property(expected_type=bool)\n    backwards = Property(expected_type=bool)\n    peepholes = Property(expected_type=bool)\n    gradient_clipping = NumberProperty(minval=0)\n\n    def __init__(self, n_units, only_return_final=True,\n                 # Trainable parameters\n                 input_weights=init.HeNormal(),\n                 hidden_weights=init.HeNormal(),\n                 cell_weights=init.HeNormal(), biases=0,\n                 # Activation functions\n                 ingate=tf.nn.sigmoid, forgetgate=tf.nn.sigmoid,\n                 outgate=tf.nn.sigmoid, cell=tf.tanh,\n                 # Cell states\n                 cell_init=0, hidden_init=0, learn_init=False,\n                 # Misc\n                 unroll_scan=False, backwards=False, peepholes=False,\n                 gradient_clipping=0, name=None):\n\n        super(LSTM, self).__init__(\n            n_units=n_units,\n            only_return_final=only_return_final,\n            name=name,\n        )\n\n        self.input_weights = input_weights\n        self.hidden_weights = hidden_weights\n        self.cell_weights = cell_weights\n        self.biases = biases\n\n        self.ingate = ingate\n        self.forgetgate = forgetgate\n        self.outgate = outgate\n        self.cell = cell\n\n        self.learn_init = learn_init\n        self.cell_init = cell_init\n        self.hidden_init = hidden_init\n\n        self.unroll_scan = unroll_scan\n        self.backwards = backwards\n        self.peepholes = peepholes\n        self.gradient_clipping = gradient_clipping\n\n    def create_variables(self, input_shape):\n        self.input_shape = input_shape\n        self.input_weights = self.variable(\n            value=self.input_weights,\n            name=\'input_weights\',\n            shape=(input_shape[-1], 4 * self.n_units),\n        )\n        self.hidden_weights = self.variable(\n            value=self.hidden_weights, name=\'hidden_weights\',\n            shape=(self.n_units, 4 * self.n_units),\n        )\n        self.biases = self.variable(\n            value=self.biases, name=\'biases\',\n            shape=(4 * self.n_units,),\n        )\n        self.cell_init = self.variable(\n            value=self.cell_init,\n            shape=(1, self.n_units),\n            name=""cell_init"",\n            trainable=self.learn_init,\n        )\n        self.hidden_init = self.variable(\n            value=self.hidden_init,\n            shape=(1, self.n_units),\n            name=""hidden_init"",\n            trainable=self.learn_init,\n        )\n\n        # If peephole (cell to gate) connections were enabled, initialize\n        # peephole connections.  These are elementwise products with the cell\n        # state, so they are represented as vectors.\n        if self.peepholes:\n            self.weight_cell_to_ingate = self.variable(\n                value=self.cell_weights,\n                name=\'weight_cell_to_ingate\',\n                shape=(self.n_units,))\n            self.weight_cell_to_forgetgate = self.variable(\n                value=self.cell_weights,\n                name=\'weight_cell_to_forgetgate\',\n                shape=(self.n_units,))\n            self.weight_cell_to_outgate = self.variable(\n                value=self.cell_weights,\n                name=\'weight_cell_to_outgate\',\n                shape=(self.n_units,))\n\n    def output(self, input, **kwargs):\n        # Because scan iterates over the first dimension we\n        # dimshuffle to (n_time_steps, n_samples, n_features)\n        input = tf.transpose(input, [1, 0, 2])\n\n        def one_lstm_step(states, input_n):\n            with tf.name_scope(\'lstm-cell\'):\n                cell_previous, hid_previous = states\n                input_n = tf.matmul(input_n, self.input_weights) + self.biases\n\n                # Calculate gates pre-activations and slice\n                gates = input_n + tf.matmul(hid_previous, self.hidden_weights)\n\n                # Clip gradients\n                if self.gradient_clipping != 0:\n                    gates = clip_gradient(gates, self.gradient_clipping)\n\n                # Extract the pre-activation gate values\n                ingate, forgetgate, cell_input, outgate = tf.split(\n                    gates, 4, axis=1)\n\n                if self.peepholes:\n                    # Compute peephole connections\n                    ingate += cell_previous * self.weight_cell_to_ingate\n                    forgetgate += (\n                        cell_previous * self.weight_cell_to_forgetgate)\n\n                # Apply nonlinearities\n                ingate = self.ingate(ingate)\n                forgetgate = self.forgetgate(forgetgate)\n                cell_input = self.cell(cell_input)\n\n                # Compute new cell value\n                cell = forgetgate * cell_previous + ingate * cell_input\n\n                if self.peepholes:\n                    outgate += cell * self.weight_cell_to_outgate\n\n                outgate = self.outgate(outgate)\n\n                # Compute new hidden unit activation\n                hid = outgate * tf.tanh(cell)\n                return [cell, hid]\n\n        input_shape = tf.shape(input)\n        n_samples = input_shape[1]  # batch dim has been moved\n        cell_init = tf.tile(self.cell_init, (n_samples, 1))\n        hidden_init = tf.tile(self.hidden_init, (n_samples, 1))\n        sequence = input\n\n        if self.backwards:\n            sequence = tf.reverse(sequence, axis=[0])\n\n        if self.unroll_scan:\n            # Explicitly unroll the recurrence instead of using scan\n            hid_out = unroll_scan(\n                fn=one_lstm_step,\n                sequence=sequence,\n                outputs_info=[cell_init, hidden_init],\n            )\n        else:\n            _, hid_out = tf.scan(\n                fn=one_lstm_step,\n                elems=sequence,\n                initializer=[cell_init, hidden_init],\n                name=\'lstm-scan\',\n            )\n\n        # When it is requested that we only return the final sequence step,\n        # we need to slice it out immediately after scan is applied\n        if self.only_return_final:\n            return hid_out[-1]\n\n        # if scan is backward reverse the output\n        if self.backwards:\n            hid_out = tf.reverse(hid_out, axis=[0])\n\n        # dimshuffle back to (n_samples, n_time_steps, n_features))\n        hid_out = tf.transpose(hid_out, [1, 0, 2])\n\n        return hid_out\n\n\nclass GRU(BaseRNNLayer):\n    """"""\n    Gated Recurrent Unit (GRU) Layer.\n\n    Parameters\n    ----------\n    {BaseRNNLayer.n_units}\n\n    {BaseRNNLayer.only_return_final}\n\n    input_weights : Initializer, ndarray\n        Weight parameters for input connection.\n        Defaults to :class:`HeNormal() <neupy.init.HeNormal>`.\n\n    hidden_weights : Initializer, ndarray\n        Weight parameters for hidden connection.\n        Defaults to :class:`HeNormal() <neupy.init.HeNormal>`.\n\n    biases : Initializer, ndarray\n        Bias parameters for all gates.\n        Defaults to :class:`Constant(0) <neupy.init.Constant>`.\n\n    resetgate : function\n        Activation function for the reset gate.\n        Defaults to ``tf.nn.sigmoid``.\n\n    updategate : function\n        Activation function for the update gate.\n        Defaults to ``tf.nn.sigmoid``.\n\n    hidden_update : function\n        Activation function for the hidden state update.\n        Defaults to ``tf.tanh``.\n\n    learn_init : bool\n        If ``True``, make ``hidden_init`` trainable variable.\n        Defaults to ``False``.\n\n    hidden_init : array-like, Tensorfow variable, scalar or Initializer\n        Initializer for initial hidden state (:math:`h_0`).\n        Defaults to :class:`Constant(0) <neupy.init.Constant>`.\n\n    backwards : bool\n        If ``True``, process the sequence backwards and then reverse the\n        output again such that the output from the layer is always\n        from :math:`x_1` to :math:`x_n`. Defaults to ``False``.\n\n    unroll_scan : bool\n        If ``True`` the recursion is unrolled instead of using scan.\n        For some graphs this gives a significant speed up but it\n        might also consume more memory. When ``unroll_scan=True``,\n        backpropagation always includes the full sequence, so\n        ``n_gradient_steps`` must be set to ``-1`` and the input\n        sequence length must be known at compile time (i.e.,\n        cannot be given as ``None``). Defaults to ``False``.\n\n    {BaseLayer.name}\n\n    Notes\n    -----\n    Code was adapted from the\n    `Lasagne <https://github.com/Lasagne/Lasagne>`_ library.\n\n    Examples\n    --------\n    Sequence classification\n\n    >>> from neupy.layers import *\n    >>>\n    >>> n_time_steps = 40\n    >>> n_categories = 20\n    >>> embedded_size = 10\n    >>>\n    >>> network = join(\n    ...     Input(n_time_steps),\n    ...     Embedding(n_categories, embedded_size),\n    ...     GRU(20),\n    ...     Sigmoid(1),\n    ... )\n    >>> network\n    (?, 40) -> [... 4 layers ...] -> (?, 1)\n    """"""\n    input_weights = ParameterProperty()\n    hidden_weights = ParameterProperty()\n    biases = ParameterProperty()\n\n    resetgate = Property(expected_type=types.FunctionType)\n    updategate = Property(expected_type=types.FunctionType)\n    hidden_update = Property(expected_type=types.FunctionType)\n\n    hidden_init = ParameterProperty()\n    learn_init = Property(default=False, expected_type=bool)\n\n    backwards = Property(expected_type=bool)\n    unroll_scan = Property(expected_type=bool)\n    gradient_clipping = NumberProperty(default=0, minval=0)\n\n    def __init__(self, n_units, only_return_final=True,\n                 # Trainable parameters\n                 input_weights=init.HeNormal(),\n                 hidden_weights=init.HeNormal(),\n                 biases=0,\n                 # Activation functions\n                 resetgate=tf.nn.sigmoid,\n                 updategate=tf.nn.sigmoid,\n                 hidden_update=tf.tanh,\n                 # Cell states\n                 hidden_init=0, learn_init=False,\n                 # Misc\n                 unroll_scan=False, backwards=False,\n                 gradient_clipping=0, name=None):\n\n        super(GRU, self).__init__(\n            n_units=n_units,\n            only_return_final=only_return_final,\n            name=name,\n        )\n\n        self.input_weights = input_weights\n        self.hidden_weights = hidden_weights\n        self.biases = biases\n\n        self.resetgate = resetgate\n        self.updategate = updategate\n        self.hidden_update = hidden_update\n\n        self.hidden_init = hidden_init\n        self.learn_init = learn_init\n\n        self.unroll_scan = unroll_scan\n        self.backwards = backwards\n        self.gradient_clipping = gradient_clipping\n\n    def create_variables(self, input_shape):\n        self.input_weights = self.variable(\n            value=self.input_weights,\n            name=\'input_weights\',\n            shape=(input_shape[-1], 3 * self.n_units),\n        )\n        self.hidden_weights = self.variable(\n            value=self.hidden_weights,\n            name=\'hidden_weights\',\n            shape=(self.n_units, 3 * self.n_units),\n        )\n        self.biases = self.variable(\n            value=self.biases, name=\'biases\',\n            shape=(3 * self.n_units,),\n        )\n        self.hidden_init = self.variable(\n            value=self.hidden_init,\n            shape=(1, self.n_units),\n            name=""hidden_init"",\n            trainable=self.learn_init\n        )\n\n    def output(self, input, **kwargs):\n        # Because scan iterates over the first dimension we\n        # dimshuffle to (n_time_steps, n_samples, n_features)\n        input = tf.transpose(input, [1, 0, 2])\n\n        # Create single recurrent computation step function\n        # input_n is the n\'th vector of the input\n        def one_gru_step(states, input_n):\n            with tf.name_scope(\'gru-cell\'):\n                hid_previous, = states\n                input_n = tf.matmul(input_n, self.input_weights) + self.biases\n\n                # Compute W_{hr} h_{t - 1}, W_{hu} h_{t - 1},\n                # and W_{hc} h_{t - 1}\n                hid_input = tf.matmul(hid_previous, self.hidden_weights)\n\n                if self.gradient_clipping != 0:\n                    input_n = clip_gradient(input_n, self.gradient_clipping)\n                    hid_input = clip_gradient(\n                        hid_input, self.gradient_clipping)\n\n                hid_resetgate, hid_updategate, hid_hidden = tf.split(\n                    hid_input, 3, axis=1)\n\n                in_resetgate, in_updategate, in_hidden = tf.split(\n                    input_n, 3, axis=1)\n\n                # Reset and update gates\n                resetgate = self.resetgate(hid_resetgate + in_resetgate)\n                updategate = self.updategate(hid_updategate + in_updategate)\n\n                # Compute W_{xc}x_t + r_t \\odot (W_{hc} h_{t - 1})\n                hidden_update = in_hidden + resetgate * hid_hidden\n\n                if self.gradient_clipping != 0:\n                    hidden_update = clip_gradient(\n                        hidden_update, self.gradient_clipping)\n\n                hidden_update = self.hidden_update(hidden_update)\n\n                # Compute (1 - u_t)h_{t - 1} + u_t c_t\n                return [\n                    hid_previous - updategate * (hid_previous - hidden_update)\n                ]\n\n        input_shape = tf.shape(input)\n        n_samples = input_shape[1]  # batch dim has been moved\n        hidden_init = tf.tile(self.hidden_init, (n_samples, 1))\n        sequence = input\n\n        if self.backwards:\n            sequence = tf.reverse(sequence, axis=[0])\n\n        if self.unroll_scan:\n            # Explicitly unroll the recurrence instead of using scan\n            hid_out = unroll_scan(\n                fn=one_gru_step,\n                sequence=sequence,\n                outputs_info=[hidden_init]\n            )\n        else:\n            hid_out, = tf.scan(\n                fn=one_gru_step,\n                elems=sequence,\n                initializer=[hidden_init],\n                name=\'gru-scan\',\n            )\n\n        # When it is requested that we only return the final sequence step,\n        # we need to slice it out immediately after scan is applied\n        if self.only_return_final:\n            return hid_out[-1]\n\n        # if scan is backward reverse the output\n        if self.backwards:\n            hid_out = tf.reverse(hid_out, axis=[0])\n\n        # dimshuffle back to (n_samples, n_time_steps, n_features))\n        hid_out = tf.transpose(hid_out, [1, 0, 2])\n        return hid_out\n'"
neupy/layers/reshape.py,9,"b'import numpy as np\nimport tensorflow as tf\n\nfrom neupy.utils import as_tuple\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.core.properties import TypedListProperty\nfrom .base import BaseLayer\n\n\n__all__ = (\'Reshape\', \'Transpose\')\n\n\nclass Reshape(BaseLayer):\n    """"""\n    Layer reshapes input tensor.\n\n    Parameters\n    ----------\n    shape : tuple\n        New feature shape. If one dimension specified with the ``-1`` value\n        that this dimension will be computed from the total size that remains.\n        Defaults to ``-1``.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n\n    Covert 4D input to 2D\n\n    >>> from neupy.layers import *\n    >>> network = Input((2, 5, 5)) >> Reshape()\n    (?, 2, 5, 5) -> [... 2 layers ...] -> (?, 50)\n\n    Convert 3D to 4D\n\n    >>> from neupy.layers import *\n    >>> network = Input((5, 4)) >> Reshape((5, 2, 2))\n    (?, 5, 4) -> [... 2 layers ...] -> (?, 5, 2, 2)\n    """"""\n    shape = TypedListProperty()\n\n    def __init__(self, shape=-1, name=None):\n        super(Reshape, self).__init__(name=name)\n        self.shape = as_tuple(shape)\n\n        if self.shape.count(-1) >= 2:\n            raise ValueError(""Only single -1 value can be specified"")\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n        feature_shape = input_shape[1:]\n        missing_value = None\n\n        if -1 in self.shape and feature_shape.is_fully_defined():\n            known_shape_values = [val for val in self.shape if val != -1]\n\n            n_feature_values = np.prod(feature_shape.dims)\n            n_expected_values = np.prod(known_shape_values)\n\n            if n_feature_values % n_expected_values != 0:\n                raise ValueError(\n                    ""Input shape and specified shape are incompatible Shape: ""\n                    ""{}, Input shape: {}"".format(self.shape, input_shape))\n\n            missing_value = int(n_feature_values // n_expected_values)\n\n        n_samples = input_shape[0]\n        new_feature_shape = [\n            missing_value if val == -1 else val for val in self.shape]\n\n        return tf.TensorShape([n_samples] + new_feature_shape)\n\n    def output(self, input, **kwargs):\n        """"""\n        Reshape the feature space for the input value.\n\n        Parameters\n        ----------\n        input : array-like or Tensorfow variable\n        """"""\n        input = tf.convert_to_tensor(input, dtype=tf.float32)\n        input_shape = tf.shape(input)\n\n        n_samples = input_shape[0]\n        expected_shape = self.get_output_shape(input.shape)\n        feature_shape = expected_shape[1:]\n\n        if feature_shape.is_fully_defined():\n            # For cases when we have -1 in the shape and feature shape\n            # can be precomputed from the input we want to be explicit about\n            # expected output shape. Because of the unknown batch dimension\n            # it won\'t be possible for tensorflow to derive exact output\n            # shape from the -1\n            output_shape = as_tuple(n_samples, feature_shape.dims)\n        else:\n            output_shape = as_tuple(n_samples, self.shape)\n\n        return tf.reshape(input, output_shape)\n\n    def __repr__(self):\n        return self._repr_arguments(self.shape, name=self.name)\n\n\nclass Transpose(BaseLayer):\n    """"""\n    Layer transposes input tensor. Permutes the dimensions according\n    to the ``perm`` parameter.\n\n    Parameters\n    ----------\n    perm : tuple or list\n        A permutation of the dimensions of the input tensor.\n\n    {BaseLayer.name}\n\n    Methods\n    -------\n    {BaseLayer.Methods}\n\n    Attributes\n    ----------\n    {BaseLayer.Attributes}\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = Input((7, 11)) >> Transpose((0, 2, 1))\n    (?, 7, 11) -> [... 2 layers ...] -> (?, 11, 7)\n    """"""\n    perm = TypedListProperty()\n\n    def __init__(self, perm, name=None):\n        super(Transpose, self).__init__(name=name)\n        self.perm = perm\n\n    def fail_if_shape_invalid(self, input_shape):\n        if input_shape and max(self.perm) >= input_shape.ndims:\n            raise LayerConnectionError(\n                ""Cannot apply transpose operation to the ""\n                ""input. Permutation: {}, Input shape: {}""\n                """".format(self.perm, input_shape))\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n        self.fail_if_shape_invalid(input_shape)\n\n        if input_shape.ndims is None:\n            n_dims_expected = len(self.perm)\n            return tf.TensorShape([None] * n_dims_expected)\n\n        input_shape = np.array(input_shape.dims)\n        perm = list(self.perm)\n\n        return tf.TensorShape(input_shape[perm])\n\n    def output(self, input, **kwargs):\n        # Input value has batch dimension, but perm will never have it\n        # specified as (zero index), so we need to add it in order to\n        # fix batch dimension in place.\n        return tf.transpose(input, list(self.perm))\n\n    def __repr__(self):\n        return self._repr_arguments(self.perm, name=self.name)\n'"
neupy/layers/stochastic.py,13,"b'import tensorflow as tf\n\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.core.properties import (\n    ProperFractionProperty,\n    NumberProperty,\n    TypedListProperty,\n)\nfrom .base import Identity\n\n\n__all__ = (\'Dropout\', \'GaussianNoise\', \'DropBlock\')\n\n\ndef bernoulli_sample(mean, shape):\n    samples = tf.random_uniform(shape, minval=0, maxval=1, dtype=tf.float32)\n    sign_samples = tf.sign(mean - samples)\n    return (sign_samples + 1) / 2\n\n\nclass Dropout(Identity):\n    """"""\n    Dropout layer. It randomly switches of (multiplies by zero)\n    input values, where probability to be switched per each value\n    can be controlled with the ``proba`` parameter. For example,\n    ``proba=0.2`` will mean that only 20% of the input values will\n    be multiplied by 0 and 80% of the will be unchanged.\n\n    It\'s important to note that output from the dropout is controlled by\n    the ``training`` parameter in the ``output`` method. Dropout\n    will be applied only in cases when ``training=True`` propagated\n    through the network, otherwise it will act as an identity.\n\n    Parameters\n    ----------\n    proba : float\n        Fraction of the input units to drop. Value needs to be\n        between ``0`` and ``1``.\n\n    {Identity.name}\n\n    Methods\n    -------\n    {Identity.Methods}\n\n    Attributes\n    ----------\n    {Identity.Attributes}\n\n    See Also\n    --------\n    :layer:`DropBlock` : DropBlock layer.\n\n    References\n    ----------\n    [1]  Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,\n         Ruslan Salakhutdinov, Dropout: a simple way to prevent neural\n         networks from overfitting, 2014.\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input(10),\n    ...     Relu(5) >> Dropout(0.5),\n    ...     Relu(5) >> Dropout(0.5),\n    ...     Sigmoid(1),\n    ... )\n    >>> network\n    (?, 10) -> [... 6 layers ...] -> (?, 1)\n    """"""\n    proba = ProperFractionProperty()\n\n    def __init__(self, proba, name=None):\n        super(Dropout, self).__init__(name=name)\n        self.proba = proba\n\n    def output(self, input_value, training=False):\n        if not training:\n            return input_value\n        return tf.nn.dropout(input_value, keep_prob=(1.0 - self.proba))\n\n\nclass GaussianNoise(Identity):\n    """"""\n    Add gaussian noise to the input value. Mean and standard deviation\n    of the noise can be controlled from the layers parameters.\n\n    It\'s important to note that output from the layer is controled by\n    the ``training`` parameter in the ``output`` method. Layer\n    will be applied only in cases when ``training=True`` propagated\n    through the network, otherwise it will act as an identity.\n\n    Parameters\n    ----------\n    std : float\n        Standard deviation of the gaussian noise. Values needs to\n        be greater than zero. Defaults to ``1``.\n\n    mean : float\n        Mean of the gaussian noise. Defaults to ``0``.\n\n    {Identity.name}\n\n    Methods\n    -------\n    {Identity.Methods}\n\n    Attributes\n    ----------\n    {Identity.Attributes}\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input(10),\n    ...     Relu(5) >> GaussianNoise(std=0.1),\n    ...     Relu(5) >> GaussianNoise(std=0.1),\n    ...     Sigmoid(1),\n    ... )\n    >>> network\n    (?, 10) -> [... 6 layers ...] -> (?, 1)\n    """"""\n    mean = NumberProperty()\n    std = NumberProperty(minval=0)\n\n    def __init__(self, mean=1, std=0, name=None):\n        super(GaussianNoise, self).__init__(name=name)\n        self.mean = mean\n        self.std = std\n\n    def output(self, input_value, training=False):\n        if not training:\n            return input_value\n\n        noise = tf.random_normal(\n            shape=tf.shape(input_value),\n            mean=self.mean,\n            stddev=self.std)\n\n        return input_value + noise\n\n\nclass DropBlock(Identity):\n    """"""\n    DropBlock, a form of structured dropout, where units in a contiguous\n    region of a feature map are dropped together.\n\n    Parameters\n    ----------\n    keep_proba : float\n        Fraction of the input units to keep. Value needs to be\n        between ``0`` and ``1``.\n\n    block_size : int or tuple\n        Size of the block to be dropped. Blocks that have squared shape can\n        be specified with a single integer value. For example, `block_size=5`\n        the same as `block_size=(5, 5)`.\n\n    {Identity.name}\n\n    Methods\n    -------\n    {Identity.Methods}\n\n    Attributes\n    ----------\n    {Identity.Attributes}\n\n    See Also\n    --------\n    :layer:`Dropout` : Dropout layer.\n\n    References\n    ----------\n    [1] Golnaz Ghiasi, Tsung-Yi Lin, Quoc V. Le. DropBlock: A regularization\n        method for convolutional networks, 2018.\n\n    Examples\n    --------\n    >>> from neupy.layers import *\n    >>> network = join(\n    ...     Input((28, 28, 1)),\n    ...\n    ...     Convolution((3, 3, 16)) >> Relu(),\n    ...     DropBlock(keep_proba=0.1, block_size=5),\n    ...\n    ...     Convolution((3, 3, 32)) >> Relu(),\n    ...     DropBlock(keep_proba=0.1, block_size=5),\n    ... )\n    """"""\n    keep_proba = ProperFractionProperty()\n    block_size = TypedListProperty(n_elements=2)\n\n    def __init__(self, keep_proba, block_size, name=None):\n        super(DropBlock, self).__init__(name=name)\n\n        if isinstance(block_size, int):\n            block_size = (block_size, block_size)\n\n        self.keep_proba = keep_proba\n        self.block_size = block_size\n\n    def get_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape)\n\n        if input_shape and input_shape.ndims != 4:\n            raise LayerConnectionError(\n                ""DropBlock layer expects input with 4 dimensions, got {} ""\n                ""with shape {}"".format(len(input_shape), input_shape))\n\n        return input_shape\n\n    def output(self, input, training=False):\n        if not training:\n            return input\n\n        input = tf.convert_to_tensor(input, tf.float32)\n        input_shape = tf.shape(input)\n\n        block_height, block_width = self.block_size\n        height, width = input_shape[1], input_shape[2]\n\n        input_area = asfloat(width * height)\n        block_area = asfloat(block_width * block_height)\n        area = asfloat((width - block_width + 1) * (height - block_height + 1))\n\n        mask = bernoulli_sample(\n            mean=(1. - self.keep_proba) * input_area / (block_area * area),\n            shape=[\n                input_shape[0],\n                height - block_height + 1,\n                width - block_width + 1,\n                input_shape[3],\n            ],\n        )\n\n        br_height = (block_height - 1) // 2\n        tl_height = (block_height - 1) - br_height\n\n        br_width = (block_width - 1) // 2\n        tl_width = (block_width - 1) - br_width\n\n        mask = tf.pad(mask, [\n            [0, 0],\n            [tl_height, br_height],\n            [tl_width, br_width],\n            [0, 0],\n        ])\n        mask = tf.nn.max_pool(\n            mask,\n            [1, block_height, block_width, 1],\n            strides=[1, 1, 1, 1],\n            padding=\'SAME\',\n        )\n        mask = tf.cast(1 - mask, tf.float32)\n\n        feature_normalizer = asfloat(tf.size(mask)) / tf.reduce_sum(mask)\n        return tf.multiply(input, mask) * feature_normalizer\n'"
neupy/plots/__init__.py,0,b'from .hinton import *\nfrom .saliency_map import *\n'
neupy/plots/hinton.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\nfrom neupy.utils import format_data\n\n\n__all__ = (\'hinton\',)\n\n\ndef hinton(matrix, max_weight=None, ax=None, add_legend=True):\n    """"""\n    Draw Hinton diagram for visualizing a weight matrix.\n\n    Parameters\n    ----------\n    matrix: array-like\n        Matrix that you want to visualize using Hinton diagram.\n\n    max_weight : float\n        Maximum value of the matrix. If it\'s equal to ``None``\n        than value would be calculated using the maximum from\n        the matrix. Defaults to ``None``.\n\n    ax : object\n        Matplotlib Axes instance. If value equal to ``None``\n        then function generate the new Axes instance. Defaults\n        to ``None``.\n\n    Returns\n    -------\n    object\n        Matplotlib Axes instance.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from neupy import plots\n    >>>\n    >>> weight = np.random.randn(20, 20)\n    >>>\n    >>> plt.style.use(\'ggplot\')\n    >>> plt.title(""Hinton diagram"")\n    >>> plt.figure(figsize=(16, 12))\n    >>>\n    >>> plots.hinton(weight)\n    >>> plt.show()\n\n    References\n    ----------\n    [1] http://matplotlib.org/examples/specialty_plots/hinton_demo.html\n    """"""\n    if ax is None:\n        ax = plt.gca()\n\n    matrix = format_data(matrix, is_feature1d=True)\n\n    if max_weight is None:\n        max_value = np.abs(matrix).max()\n        max_value_log2_base = np.log(max_value) / np.log(2)\n        max_weight = 2 ** np.ceil(max_value_log2_base)\n\n    ax.patch.set_facecolor(\'gray\')\n    ax.set_aspect(\'equal\', \'box\')\n    ax.xaxis.set_major_locator(plt.NullLocator())\n    ax.yaxis.set_major_locator(plt.NullLocator())\n\n    for (y, x), weight in np.ndenumerate(matrix):\n        color = (\'white\' if weight > 0 else \'black\')\n        size = min(np.sqrt(np.abs(weight / max_weight)), 1.)\n        rect = plt.Rectangle([x - size / 2., y - size / 2.], size, size,\n                             facecolor=color, edgecolor=color)\n        ax.add_patch(rect)\n\n    ax.autoscale_view()\n    ax.invert_yaxis()\n\n    if add_legend:\n        max_value = matrix.max().round(2)\n        min_value = matrix.min().round(2)\n\n        white = Rectangle(xy=(0, 0), width=1., height=1., linewidth=1.,\n                          linestyle=\'solid\', facecolor=\'#ffffff\')\n        black = Rectangle(xy=(0, 0), width=1., height=1., color=\'#000000\')\n\n        if min_value < 0 and max_value > 0:\n            rectangles = [white, black]\n            rect_description = [\n                \'Positive value\\n\'\n                \'Max: {}\'.format(max_value),\n                \'Negative value\\n\'\n                \'Min: {}\'.format(min_value),\n            ]\n\n        elif min_value >= 0:\n            rectangles = [white]\n            rect_description = [\n                \'Positive value\\n\'\n                \'Min: {}\\n\'\n                \'Max: {}\'.format(min_value, max_value),\n            ]\n\n        else:\n            rectangles = [black]\n            rect_description = [\n                \'Negative value\\n\'\n                \'Min: {}\\n\'\n                \'Max: {}\'.format(min_value, max_value),\n            ]\n\n        ax.legend(rectangles, rect_description, loc=\'center left\',\n                  bbox_to_anchor=(1., 0.5))\n\n    return ax\n'"
neupy/plots/saliency_map.py,2,"b'import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import gaussian_filter\n\nfrom neupy.utils import tensorflow_session\nfrom neupy.exceptions import InvalidConnection\nfrom neupy.algorithms.gd.base import BaseOptimizer\n\n\n__all__ = (\'saliency_map\', \'saliency_map_graph\')\n\n\ndef saliency_map_graph(network):\n    """"""\n    Returns tensorflow variables for saliency map.\n\n    Parameters\n    ----------\n    network : network\n    image : ndarray\n    """"""\n    session = tensorflow_session()\n\n    if not hasattr(saliency_map_graph, \'cache\'):\n        saliency_map_graph.cache = {}\n\n    if session in saliency_map_graph.cache:\n        return saliency_map_graph.cache[session]\n\n    inputs = network.inputs\n    prediction = network.outputs\n\n    output_class = tf.argmax(prediction[0])\n    saliency, = tf.gradients(tf.reduce_max(prediction), inputs)\n\n    # Caching will ensure that we won\'t build\n    # tensorflow graph every time we generate\n    saliency_map_graph.cache[session] = inputs, saliency, output_class\n    return inputs, saliency, output_class\n\n\ndef saliency_map(network, image, mode=\'heatmap\', sigma=8,\n                 ax=None, show=True, **kwargs):\n    """"""\n    Saliency Map plot.\n\n    Parameters\n    ----------\n    network : network\n        Network based on which will be computed saliency map.\n\n    image : 3D array-like tensor\n        Image based on which will be computed saliency map.\n\n    mode : {``raw``, ``heatmap``}\n        - ``raw``\n          Visualize raw gradient. White color on the plot\n          defines high gradient values.\n\n        - ``heatmap``\n          Applies gaussian filter to the gradient and visualize\n          as a heatmap plot.\n\n        Defaults to ``heatmap``.\n\n    sigma : float\n        Standard deviation for kernel in Gaussian filter.\n        It is used only when ``mode=\'heatmap\'``. Defaults to ``8``.\n\n    ax : object or None\n        Matplotlib axis object. ``None`` values means that axis equal\n        to the current axes instance (the same as ``ax = plt.gca()``).\n        Defaults to ``None``.\n\n    show : bool\n        If parameter is equal to ``True`` then plot will be\n        displayed. Defaults to ``True``.\n\n    **kwargs\n        Arguments for ``plt.imshow`` function.\n\n    Returns\n    -------\n    object\n        Matplotlib axis instance.\n\n    Examples\n    --------\n    >>> from neupy import layers, plots\n    >>>\n    >>> network = layers.join(\n    ...     layers.Input((3, 28, 28)),\n    ...     layers.Convolution((32, 3, 3)) >> layers.Relu(),\n    ...     layers.Reshape(),\n    ...     layers.Softmax(10),\n    ... )\n    >>>\n    >>> dog_image = load_dog_image()\n    >>> plots.saliency_map(network, dog_image)\n    """"""\n    if image.ndim == 3:\n        image = np.expand_dims(image, axis=0)\n\n    if image.ndim != 4:\n        raise ValueError(\n            ""Invalid image shape. Image expected to be 3D, ""\n            ""got {}D image"".format(image.ndim))\n\n    valid_modes = (\'raw\', \'heatmap\')\n    if mode not in valid_modes:\n        raise ValueError(\n            ""{!r} is invalid value for mode argument. Valid ""\n            ""mode values are: {!r}"".format(mode, valid_modes))\n\n    if isinstance(network, BaseOptimizer):\n        network = network.network\n\n    if len(network.output_layers) != 1:\n        raise InvalidConnection(\n            ""Cannot build saliency map for the network that ""\n            ""has more than one output layer."")\n\n    if len(network.input_layers) != 1:\n        raise InvalidConnection(\n            ""Cannot build saliency map for the network that ""\n            ""has more than one input layer."")\n\n    if len(network.input_shape) != 4:\n        raise InvalidConnection(\n            ""Input layer has to be 4 dimensions, but network expects ""\n            ""{} dimensional input"".format(len(network.input_shape)))\n\n    if ax is None:\n        ax = plt.gca()\n\n    x, saliency, output_class = saliency_map_graph(network)\n\n    session = tensorflow_session()\n    saliency, output = session.run(\n        [saliency, output_class], feed_dict={x: image})\n\n    saliency = saliency[0].max(axis=-1)\n\n    if mode == \'heatmap\':\n        saliency = gaussian_filter(saliency, sigma=sigma)\n\n    elif mode == \'raw\':\n        kwargs.setdefault(\'cmap\', \'gray\')\n\n    ax.set_title(\'Predicted output #{} (0-based indices)\'.format(output))\n    ax.imshow(saliency, **kwargs)\n\n    if show:\n        plt.show()\n\n    return ax\n'"
neupy/utils/__init__.py,0,b'from .tf_utils import *\nfrom .processing import *\nfrom .misc import *\nfrom .iters import *\n'
neupy/utils/iters.py,0,"b'from __future__ import division\n\nimport math\n\nimport numpy as np\nimport progressbar\n\nfrom neupy.utils.misc import as_tuple\n\n\n__all__ = (\n    \'apply_batches\', \'minibatches\',\n    \'count_minibatches\', \'count_samples\',\n)\n\n\ndef count_samples(inputs):\n    if isinstance(inputs, (list, tuple)):\n        return count_samples(inputs[0])\n    return len(inputs)\n\n\ndef count_minibatches(inputs, batch_size):\n    return int(math.ceil(count_samples(inputs) / batch_size))\n\n\ndef apply_slices(inputs, indices):\n    if inputs is None:\n        return inputs\n\n    if isinstance(inputs, (list, tuple)):\n        return [apply_slices(input_, indices) for input_ in inputs]\n\n    return inputs[indices]\n\n\ndef minibatches(inputs, batch_size=None, shuffle=False):\n    """"""\n    Iterates batch slices.\n\n    Parameters\n    ----------\n    inputs : array-like, list\n\n    batch_size : int\n        Mini-batch size. Number should be greater than ``0``.\n\n    shuffle : bool\n        Defaults to ``True``.\n\n    Yields\n    ------\n    object\n        Batch slices.\n    """"""\n    n_samples = count_samples(inputs)\n    batch_size = n_samples if batch_size is None else batch_size\n    n_batches = count_minibatches(inputs, batch_size)\n\n    if shuffle:\n        indices = np.arange(n_samples)\n        np.random.shuffle(indices)\n\n        for index in range(n_batches):\n            batch_slice = slice(index * batch_size, (index + 1) * batch_size)\n            yield apply_slices(inputs, indices[batch_slice])\n\n    elif n_batches != 1:\n        for index in range(n_batches):\n            batch_slice = slice(index * batch_size, (index + 1) * batch_size)\n            yield apply_slices(inputs, batch_slice)\n\n    else:\n        yield inputs\n\n\ndef average_batch_errors(errors, n_samples, batch_size):\n    """"""\n    Computes average error per sample. Function assumes that error from\n    each batch was just an average loss of each individual sample.\n\n    Parameters\n    ----------\n    errors : list\n        List of average errors calculated per batch.\n\n    n_samples : int\n        Number of input samples..\n\n    batch_size : int\n        Mini-batch size.\n\n    Returns\n    -------\n    float\n        Average error per sample.\n    """"""\n    errors = np.atleast_1d(errors)\n    batches = np.ones_like(errors) * batch_size\n\n    if len(errors) == 1:\n        return errors.item(0)\n\n    if n_samples % batch_size != 0:\n        # Last batch can be smaller than the usual one, because we\n        # won\'t have enough samples for the full one.\n        batches[-1] = n_samples % batch_size\n\n    return np.dot(errors, batches) / n_samples\n\n\ndef make_progressbar(max_value, show_output):\n    widgets = [\n        progressbar.Timer(format=\'Time: %(elapsed)s\'),\n        \' |\',\n        progressbar.Percentage(),\n        progressbar.Bar(),\n        \' \',\n        progressbar.ETA(),\n    ]\n\n    if show_output:\n        widgets.extend([\' | \', progressbar.DynamicMessage(\'loss\')])\n\n    return progressbar.ProgressBar(max_value=max_value, widgets=widgets)\n\n\ndef apply_batches(function, inputs, batch_size, show_progressbar=False,\n                  show_output=False, average_outputs=False):\n    """"""\n    Splits inputs into mini-batches and passes them to the function.\n    Function returns list of outputs or average loss in case\n    if ``average_outputs=True``.\n\n    Parameters\n    ----------\n    function : func\n        Function that accepts one or more positional inputs.\n        Each of them should be an array-like variable that\n        have exactly the same number of rows.\n\n    inputs : tuple, list\n        The arguments that will be provided to the function specified\n        in the ``function`` argument.\n\n    batch_size : int\n        Mini-batch size. Defines maximum number of samples that will be\n        used as an input to the ``function``.\n\n    show_progressbar : bool\n        When ``True`` than progress bar will be shown in the terminal.\n        Defaults to ``False``.\n\n    show_output : bool\n        Assumes that outputs from the function errors. The ``True`` value\n        will show information in the progressbar. Error will be related to\n        the last epoch. Defaults to ``False``.\n\n    average_outputs : bool\n        Output from each batch will be combined into single average. This\n        option assumes that loss per batch was calculated from.\n        Defaults to ``False``.\n\n    Returns\n    -------\n    list\n        List of function outputs.\n    """"""\n    n_samples = count_samples(inputs)\n    batch_size = n_samples if batch_size is None else batch_size\n\n    n_batches = count_minibatches(inputs, batch_size)\n    bar = progressbar.NullBar()\n\n    if show_progressbar and n_batches >= 2:\n        bar = make_progressbar(n_batches, show_output)\n        bar.update(0)  # triggers empty progressbar\n\n    outputs = []\n    iterator = minibatches(inputs, batch_size, shuffle=False)\n\n    for i, sliced_inputs in enumerate(iterator):\n        output = function(*as_tuple(sliced_inputs))\n        outputs.append(output)\n\n        kwargs = dict(loss=output) if show_output else {}\n        bar.update(i, **kwargs)\n\n    # Clean progressbar from the screen\n    bar.fd.write(\'\\r\' + \' \' * bar.term_width + \'\\r\')\n\n    if average_outputs:\n        # When loss calculated per batch separately it might be\n        # necessary to combine error into single value\n        return average_batch_errors(outputs, n_samples, batch_size)\n\n    return outputs\n'"
neupy/utils/misc.py,1,"b'import os\nimport random\n\nimport tensorflow as tf\nimport numpy as np\n\n\n__all__ = (\'as_tuple\', \'AttributeKeyDict\', \'reproducible\')\n\n\ndef as_tuple(*values):\n    """"""\n    Convert sequence of values in one big tuple.\n\n    Parameters\n    ----------\n    *values\n        Values that needs to be combined in one big tuple.\n\n    Returns\n    -------\n    tuple\n        All input values combined in one tuple\n\n    Examples\n    --------\n    >>> as_tuple(None, (1, 2, 3), None)\n    (None, 1, 2, 3, None)\n    >>>\n    >>> as_tuple((1, 2, 3), (4, 5, 6))\n    (1, 2, 3, 4, 5, 6)\n    """"""\n    cleaned_values = []\n    for value in values:\n        if isinstance(value, (tuple, list)):\n            cleaned_values.extend(value)\n        else:\n            cleaned_values.append(value)\n    return tuple(cleaned_values)\n\n\ndef reproducible(seed=0):\n    """"""\n    Set up the same seed value for the NumPy and\n    python random module to make your code reproducible.\n\n    Parameters\n    ----------\n    seed : int\n        Defaults to ``0``.\n    """"""\n    os.environ[\'PYTHONHASHSEED\'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.set_random_seed(seed)\n\n\nclass AttributeKeyDict(dict):\n    """"""\n    Modified built-in Python ``dict`` class. That modification\n    helps get and set values like attributes.\n\n    Examples\n    --------\n    >>> attrdict = AttributeKeyDict()\n    >>> attrdict\n    {}\n    >>> attrdict.test_key = \'test_value\'\n    >>> attrdict\n    {\'test_key\': \'test_value\'}\n    >>> attrdict.test_key\n    \'test_value\'\n    """"""\n    def __getattr__(self, attrname):\n        return self[attrname]\n\n    def __setattr__(self, attrname, value):\n        self[attrname] = value\n\n    def __delattr__(self, attrname):\n        del self[attrname]\n\n    def __reduce__(self):\n        return (self.__class__, (dict(self),))\n'"
neupy/utils/processing.py,2,"b'import numpy as np\nimport tensorflow as tf\nfrom scipy.sparse import issparse\n\n\n__all__ = (\'format_data\', \'asfloat\')\n\n\ndef format_data(data, is_feature1d=True, copy=False, make_float=True):\n    """"""\n    Transform data in a standardized format.\n\n    Notes\n    -----\n    It should be applied to the input data prior to use in\n    learning algorithms.\n\n    Parameters\n    ----------\n    data : array-like\n        Data that should be formatted. That could be, matrix, vector or\n        Pandas DataFrame instance.\n\n    is_feature1d : bool\n        Should be equal to ``True`` if input data is a vector that\n        contains N samples with 1 feature each. Defaults to ``True``.\n\n    copy : bool\n        Defaults to ``False``.\n\n    make_float : bool\n        If `True` then input will be converted to float.\n        Defaults to ``False``.\n\n    Returns\n    -------\n    ndarray\n        The same input data but transformed to a standardized format\n        for further use.\n    """"""\n    if data is None or issparse(data):\n        return data\n\n    if make_float:\n        data = asfloat(data)\n\n    if not isinstance(data, np.ndarray) or copy:\n        data = np.array(data, copy=copy)\n\n    # Valid number of features for one or two dimensions\n    n_features = data.shape[-1]\n\n    if data.ndim == 1:\n        data_shape = (n_features, 1) if is_feature1d else (1, n_features)\n        data = data.reshape(data_shape)\n\n    return data\n\n\ndef asfloat(value):\n    """"""\n    Convert variable to 32 bit float number.\n\n    Parameters\n    ----------\n    value : matrix, ndarray, Tensorfow variable or scalar\n        Value that could be converted to float type.\n\n    Returns\n    -------\n    matrix, ndarray, Tensorfow variable or scalar\n        Output would be input value converted to 32 bit float.\n    """"""\n    float_type = \'float32\'\n\n    if isinstance(value, (np.matrix, np.ndarray)):\n        if value.dtype != np.dtype(float_type):\n            return value.astype(float_type)\n\n        return value\n\n    elif isinstance(value, (tf.Tensor, tf.SparseTensor)):\n        return tf.cast(value, tf.float32)\n\n    elif issparse(value):\n        return value\n\n    float_x_type = np.cast[float_type]\n    return float_x_type(value)\n'"
neupy/utils/tf_utils.py,30,"b'from functools import wraps\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy.utils.misc import as_tuple\nfrom neupy.utils.processing import asfloat\n\n\n__all__ = (\n    # Main tensorflow functions\n    \'tensorflow_session\', \'tensorflow_eval\', \'create_variable\',\n    \'initialize_uninitialized_variables\', \'function\',\n\n    # Functions that help to deal with tensorflow name scope\n    \'class_method_name_scope\', \'function_name_scope\',\n\n    # Misc utils for tensorflow\n    \'flatten\', \'outer\', \'repeat\', \'dimshuffle\', \'shape_to_tuple\',\n    \'dot\', \'make_single_vector\', \'setup_parameter_updates\',\n)\n\n\ndef function(inputs, outputs, updates=None, name=None):\n    """"""\n    Function simulates behaviour of the Theano\'s functions.\n\n    Parameters\n    ----------\n    inputs : list\n        List of input placeholders\n\n    outputs : list, Tensor\n        Output that has to be computed by the function\n\n    updates : list or None\n        List of the updates that has to be performed on the variables.\n        The ``None`` value means that no updates will be applied at the\n        end of the computation. Defaults to ``None``.\n\n    name : str or None\n        Defaults to ``None``.\n\n    Returns\n    -------\n    function\n    """"""\n    if updates is None:\n        updates = []\n\n    session = tensorflow_session()\n    tensorflow_updates = []\n\n    # Ensure that all new values has been computed. Absence of these\n    # checks might lead to the non-deterministic update behaviour.\n    new_values = [val[1] for val in updates if isinstance(val, (list, tuple))]\n\n    # Make sure that all outputs has been computed\n    with tf.control_dependencies(as_tuple(outputs, new_values)):\n        for update in updates:\n            if isinstance(update, (list, tuple)):\n                old_value, new_value = update\n                update = old_value.assign(new_value)\n            tensorflow_updates.append(update)\n\n        # Group variables in order to avoid output for the updates\n        tensorflow_updates = tf.group(*tensorflow_updates)\n\n    @wraps(function)\n    def wrapper(*input_values):\n        feed_dict = dict(zip(inputs, input_values))\n        result, _ = session.run(\n            [outputs, tensorflow_updates],\n            feed_dict=feed_dict,\n        )\n        return result\n    return wrapper\n\n\ndef tensorflow_session():\n    if hasattr(tensorflow_session, \'cache\'):\n        session = tensorflow_session.cache\n\n        if not session._closed:\n            return session\n\n    config = tf.ConfigProto(\n        allow_soft_placement=True,\n        inter_op_parallelism_threads=0,\n        intra_op_parallelism_threads=0,\n    )\n    session = tf.Session(config=config)\n\n    tensorflow_session.cache = session\n    return session\n\n\ndef initialize_uninitialized_variables(variables=None):\n    if variables is None:\n        variables = tf.global_variables()\n\n    if not variables:\n        return\n\n    session = tensorflow_session()\n    is_not_initialized = session.run([\n        tf.is_variable_initialized(var) for var in variables])\n\n    not_initialized_vars = [\n        v for (v, f) in zip(variables, is_not_initialized) if not f]\n\n    if len(not_initialized_vars):\n        session.run(tf.variables_initializer(not_initialized_vars))\n\n\ndef function_name_scope(function):\n    """"""\n    Decorator that wraps any function with the name score that has the\n    same name as a function.\n    """"""\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        with tf.name_scope(function.__name__):\n            return function(*args, **kwargs)\n    return wrapper\n\n\ndef class_method_name_scope(method):\n    """"""\n    Decorator that wraps any method with the name score that has the\n    same name as a method.\n    """"""\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        with tf.name_scope(self.__class__.__name__):\n            if hasattr(method, \'__self__\'):  # check if method bounded\n                return method(*args, **kwargs)\n            return method(self, *args, **kwargs)\n\n    wrapper.original_method = method\n    return wrapper\n\n\ndef tensorflow_eval(value):\n    session = tensorflow_session()\n    initialize_uninitialized_variables()\n    return session.run(value)\n\n\n@function_name_scope\ndef flatten(value):\n    return tf.reshape(value, [-1])\n\n\n@function_name_scope\ndef outer(a, b):\n    a = tf.expand_dims(a, 1)  # column vector\n    b = tf.expand_dims(b, 0)  # row vector\n    return tf.matmul(a, b)\n\n\n@function_name_scope\ndef dot(a, b):\n    return tf.tensordot(a, b, 1)\n\n\ndef repeat(tensor, repeats):\n    """"""\n    Repeat elements of an tensor. The same as ``numpy.repeat``.\n\n    Parameters\n    ----------\n    input : tensor\n    repeats: list, tuple\n        Number of repeat for each dimension, length must be the\n        same as the number of dimensions in input.\n\n    Returns\n    -------\n    tensor\n        Has the same type as input. Has the shape\n        of ``tensor.shape * repeats``.\n    """"""\n    with tf.variable_scope(""repeat""):\n        expanded_tensor = tf.expand_dims(tensor, -1)\n\n        multiples = as_tuple(1, repeats)\n        tiled_tensor = tf.tile(expanded_tensor, multiples)\n\n        repeats = tf.convert_to_tensor(repeats)\n        return tf.reshape(tiled_tensor, tf.shape(tensor) * repeats)\n\n\ndef make_single_vector(parameters):\n    with tf.name_scope(\'parameters-vector\'):\n        return tf.concat([flatten(param) for param in parameters], axis=0)\n\n\ndef setup_parameter_updates(parameters, parameter_update_vector):\n    """"""\n    Creates update rules for list of parameters from one vector.\n    Function is useful in Conjugate Gradient or\n    Levenberg-Marquardt optimization algorithms\n\n    Parameters\n    ----------\n    parameters : list\n        List of parameters.\n\n    parameter_update_vector : Tensorfow varible\n        Vector that contains updates for all parameters.\n\n    Returns\n    -------\n    list\n        List of updates separeted for each parameter.\n    """"""\n    updates = []\n    start_position = 0\n\n    for parameter in parameters:\n        end_position = start_position + tf.size(parameter)\n        new_parameter = tf.reshape(\n            parameter_update_vector[start_position:end_position],\n            parameter.shape\n        )\n        updates.append((parameter, new_parameter))\n        start_position = end_position\n\n    return updates\n\n\ndef dimshuffle(value, ndim, axes):\n    """"""\n    Shuffle dimension based on the specified number of\n    dimensions and axes.\n\n    Parameters\n    ----------\n    value : Tensorfow variable\n    ndim : int\n    axes : tuple, list\n\n    Returns\n    -------\n    Tensorfow variable\n    """"""\n    for dim in range(ndim):\n        if dim not in axes:\n            value = tf.expand_dims(value, dim)\n    return value\n\n\ndef shape_to_tuple(shape):\n    if isinstance(shape, tf.TensorShape):\n        if shape.ndims is not None:\n            return tuple([dim.value for dim in shape.dims])\n        return None\n\n    if isinstance(shape, tf.Dimension):\n        return shape.value\n\n    if isinstance(shape, list):\n        return [shape_to_tuple(s) for s in shape]\n\n    if isinstance(shape, tuple):\n        return tuple([shape_to_tuple(s) for s in shape])\n\n    return shape\n\n\ndef create_variable(value, name, shape, trainable=True):\n    """"""\n    Creates NN parameter as Tensorfow variable.\n\n    Parameters\n    ----------\n    value : array-like, Tensorfow variable, scalar or Initializer\n        Default value for the parameter.\n\n    name : str\n        Shared variable name.\n\n    shape : tuple\n        Parameter\'s shape.\n\n    trainable : bool\n        Whether parameter trainable by backpropagation.\n\n    Returns\n    -------\n    Tensorfow variable.\n    """"""\n    from neupy import init\n\n    if shape is not None:\n        shape = shape_to_tuple(shape)\n\n    if isinstance(value, (tf.Variable, tf.Tensor, np.ndarray, np.matrix)):\n        variable_shape = shape_to_tuple(value.shape)\n\n        if as_tuple(variable_shape) != as_tuple(shape):\n            raise ValueError(\n                ""Cannot create variable with name `{}`. Provided variable ""\n                ""with shape {} is incompatible with expected shape {}""\n                """".format(name, variable_shape, shape))\n\n    if isinstance(value, (tf.Variable, tf.Tensor)):\n        return value\n\n    if isinstance(value, (int, float)):\n        value = init.Constant(value)\n\n    if isinstance(value, init.Initializer):\n        value = value.sample(shape)\n\n    return tf.Variable(\n        asfloat(value),\n        name=name,\n        dtype=tf.float32,\n        trainable=trainable,\n    )\n'"
site/search-index/__init__.py,0,b''
site/search-index/build.py,5,"b'import os\nimport re\nimport json\nimport pickle\nimport logging\nimport argparse\nfrom textwrap import dedent\nfrom collections import defaultdict, namedtuple\n\nimport nltk\nimport numpy as np\nimport scipy.sparse as sp\nfrom six.moves.urllib.parse import urljoin, urlparse\n\nfrom pagerank import pagerank\nfrom webgraph import WebPageGraph, Link\nfrom htmltools import iter_html_files, ParseHTML\n\n\nlogging.basicConfig(format=\'[%(levelname)-5s] %(message)s\',\n                    level=logging.DEBUG)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""-d"", ""--for-deploy"", action=""store_true"",\n                    help=(""save output in a javascript file that will ""\n                          ""be used for deployment""))\n\nCURRENT_DIR = os.path.abspath(os.path.dirname(__file__))\n\nPYTHON_INDEX_DIR = os.path.join(CURRENT_DIR, \'index-files\')\nPYTHON_INDEX_FILE = os.path.join(PYTHON_INDEX_DIR, \'index.pickle\')\n\nJS_INDEX_DIR = os.path.join(CURRENT_DIR, \'..\', \'blog\', \'html\', \'_static\', \'js\')\nJS_INDEX_FILE = os.path.join(JS_INDEX_DIR, \'searchindex.js\')\n\nSITE_DIR = os.path.abspath(os.path.join(CURRENT_DIR, \'..\', \'blog\', \'html\'))\nSITE_ROOT = \'http://neupy.com\'\n\n\ndef make_url_from_file(filepath):\n    _, url = filepath.split(SITE_DIR)\n    return urljoin(SITE_ROOT, url)\n\n\ndef ignore_link(link):\n    patterns = [\n        # Base pages\n        \'/rss.html\',\n        \'/index.html\',\n        \'/master.html\',\n        \'/search.html\',\n        \'/archive.html\',\n        \'/py-modindex.html\',\n\n        # Other pages\n        \'/pages/home.html\',\n        \'/apidocs/modules.html\',\n        \'/apidocs/neupy.html\',\n        \'/modules/generated/.+\',\n\n        # Pages that has collected information\n        r\'/page\\d{1,}.html\',\n        r\'.*tags/.+\\.html\',\n        r\'.*cheatsheet\\.html\',\n\n        # Static files\n        r\'.+(css|js|jpg|png)$\',\n        r\'/_images/.+\',\n        r\'.*\\.tar\\.gz\',\n    ]\n    uri = urlparse(link)\n\n    for pattern in patterns:\n        if re.match(pattern, uri.path):\n            return True\n\n        if uri.fragment in (\'subpackages\', \'submodules\'):\n            return True\n\n        if uri.fragment.endswith(\'-package\'):\n            return True\n\n    return False\n\n\ndef url_filter(links):\n    filtered_links = []\n\n    for link in links:\n        if not ignore_link(link.uri):\n            filtered_links.append(link)\n\n    return filtered_links\n\n\ndef save_python_index(data):\n    if not os.path.exists(PYTHON_INDEX_DIR):\n        os.mkdir(PYTHON_INDEX_DIR)\n\n    with open(PYTHON_INDEX_FILE, \'wb\') as f:\n        pickle.dump(data, f)\n\n\ndef remove_useless_keys(documents):\n    useless_keys = (\'filepath\', \'filename\', \'links\', \'html\', \'text\')\n\n    for document in documents:\n        for useless_key in useless_keys:\n            if useless_key in document:\n                del document[useless_key]\n\n\ndef save_js_index(documents, vocabulary, tf, idf, rank):\n    if not os.path.exists(JS_INDEX_DIR):\n        os.mkdir(JS_INDEX_DIR)\n\n    output_template = dedent(""""""\n    var searchIndex = {{\n        documents: {documents},\n        idf: {idf},\n        tf: {{\n            col: {tf_col},\n            row: {tf_row},\n            data: {tf_value}\n        }},\n        vocabulary: {vocabulary},\n        rank: {rank}\n    }}\n    """""")\n\n    remove_useless_keys(documents)\n    tf = tf.tocoo()\n\n    with open(JS_INDEX_FILE, \'w\') as f:\n        f.write(\n            output_template.format(\n                documents=json.dumps(documents, indent=4),\n                idf=idf.tolist(),\n                tf_col=tf.col.tolist(),\n                tf_row=tf.row.tolist(),\n                tf_value=tf.data.tolist(),\n                vocabulary=json.dumps(vocabulary),\n                rank=rank.tolist(),\n            ))\n\n\ndef page_tagging(url):\n    parsed_uri = urlparse(url)\n\n    tagging_rules = {\n        \'algorithm\': r\'^/apidocs/neupy.algorithms\',\n        \'layer\': r\'^/apidocs/neupy.layers\',\n        \'plot\': r\'^/apidocs/neupy.plots\',\n        \'documentation\': r\'^/docs/\',\n        \'article\': r\'^/\\d{4}/\\d{2}/\\d{2}/\',\n        \'tutorial\': r\'^/\\d{4}/\\d{2}/\\d{2}/\',\n    }\n\n    for tag, tagging_rule in tagging_rules.items():\n        tagging_rule_regex = re.compile(tagging_rule)\n\n        if tagging_rule_regex.match(parsed_uri.path):\n            return tag\n\n    return None\n\n\ndef collect_documents(directory):\n    logging.info(""Collecting documents from the directory (%s)"", directory)\n    Document = namedtuple(""Document"", ""filename filepath uri links ""\n                                      ""html text title tag snippet"")\n\n    documents = []\n    for filepath in iter_html_files(directory):\n        current_page_url = make_url_from_file(filepath)\n        filename = os.path.basename(filepath)\n\n        if ignore_link(current_page_url):\n            logging.debug(\'Skip ""%s"", because file is defined in the \'\n                          \'ignore list\', filename)\n            continue\n\n        html = ParseHTML.fromfile(filepath, current_page_url)\n        tag = page_tagging(current_page_url)\n        text = html.text()\n\n        if not text:\n            logging.debug(\'Skip ""%s"", because text is missed\', filename)\n            continue\n\n        for subdocument in html.subdocuments():\n            if ignore_link(subdocument.uri):\n                logging.debug(\'Skip ""%s"", because URL is defined in the \'\n                              \'ignore list\', subdocument.uri)\n\n            else:\n                doc = Document(filename, filepath, subdocument.uri,\n                               url_filter(subdocument.links),\n                               subdocument.html, subdocument.text,\n                               subdocument.title, tag, subdocument.snippet)\n                documents.append(doc)\n\n    return documents\n\n\nif __name__ == \'__main__\':\n    logging.info(""Started building index"")\n    args = parser.parse_args()\n\n    documents = []\n    vocabulary = {}\n    term_frequency = defaultdict(int)\n\n    index_pointers = [0]\n    indices = []\n    data = []\n\n    logging.info(""Collecting documents"")\n    all_documents = collect_documents(SITE_DIR)\n\n    logging.info(""Define relations between documents"")\n    webgraph = WebPageGraph.create_from_documents(all_documents)\n\n    for document in all_documents:\n        logging.debug(\'Processing ""%s""\', document.uri)\n\n        text = document.text\n        text = text.lower().replace(\'.\', \' \').replace(\'=\', \' \')\n\n        anchor_texts = []\n        for _, link in webgraph.page_linked_by(Link(document.uri)):\n            if link.text:\n                anchor_texts.append(link.text)\n\n        text = \' \'.join([text] + anchor_texts)\n\n        for term in nltk.word_tokenize(text):\n            if term not in vocabulary:\n                vocabulary[term] = len(vocabulary)\n\n            termid = vocabulary[term]\n            term_frequency[termid] += 1\n\n            indices.append(termid)\n            data.append(1)\n\n        index_pointers.append(len(indices))\n        documents.append(document._asdict())\n\n    n_documents = len(documents)\n    n_terms = len(vocabulary)\n\n    if n_documents == 0:\n        raise OSError(""Cannot find site documents. Probably site ""\n                      ""hasn\'t been build yet."")\n\n    logging.info(""Found {} documents"".format(n_documents))\n    logging.info(""Found {} terms"".format(n_terms))\n\n    logging.info(""Calculation TF and IDF"")\n    frequencies = sp.csr_matrix((data, indices, index_pointers),\n                                shape=(n_documents, n_terms))\n    df = (frequencies >= 1).sum(axis=0)\n    idf = np.log((n_documents / df) + 1)\n    idf = np.asarray(idf)[0]\n\n    tf = np.log1p(frequencies)\n    tf.data += 1\n\n    logging.info(""Applying PageRank"")\n    rank = webgraph.pagerank()\n\n    logging.info(""Saving index"")\n\n    if args.for_deploy:\n        save_js_index(documents, vocabulary, tf, idf, rank)\n    else:\n        save_python_index([documents, vocabulary, tf, idf, rank])\n\n    logging.info(""Index build was finished succesfuly"")\n'"
site/search-index/htmltools.py,0,"b'import os\nfrom collections import namedtuple\n\nfrom bs4 import BeautifulSoup\nfrom six.moves.urllib.parse import urljoin, urlparse\n\nfrom webgraph import Link\n\n\ndef iter_html_files(directory):\n    for path, directories, filenames in os.walk(directory):\n        for filename in filenames:\n            if filename.endswith(\'.html\'):\n                yield os.path.join(path, filename)\n\n\ndef extract_title(html):\n    headers = html.select(""h1,h2,h3,h4,h5"")\n    first_header = headers[0]\n    return first_header.text\n\n\ndef extrat_object_snippet(html):\n    # We take only first paragraph from the description and ignore other\n    # even if they continue class\'/function\'s description.\n    paragraphs = html.select(\'dd:nth-of-type(1) > p:nth-of-type(1)\')\n\n    if not paragraphs:\n        return \'\'\n\n    snippet = paragraphs[0]\n    return snippet.text\n\n\nclass ParseHTML(object):\n    def __init__(self, html, url):\n        self.raw_html = html\n        self.html = BeautifulSoup(html, ""html.parser"")\n        self.url = url\n\n    @classmethod\n    def fromfile(cls, filepath, url):\n        with open(filepath) as html_file:\n            html = html_file.read()\n            return cls(html, url)\n\n    def text(self):\n        articles = self.html.select(\'div.main-container .body article\')\n\n        if not articles:\n            return \'\'\n\n        content = articles[0]\n\n        for tag in content.findAll([\'script\', \'style\', \'noscript\']):\n            tag.extract()\n\n        return content.text\n\n    def extract_links(self, html):\n        for anchor in html.findAll(\'a\'):\n            uri = urlparse(anchor[\'href\'])\n\n            if uri.netloc in (\'neupy.com\', \'\'):\n                url = urljoin(self.url, uri.path)\n                if uri.fragment:\n                    url = url + ""#"" + uri.fragment\n\n                yield Link(uri=url, text=anchor.text)\n\n    def links(self):\n        return self.extract_links(self.html)\n\n    def subdocuments(self):\n        Subdocument = namedtuple(""Subdocument"",\n                                 ""uri links html text title snippet"")\n\n        subdocuments = []\n        apidocs = self.html.select(\'dl.function,dl.class,dl.exception\')\n\n        if apidocs:\n            for subdoc in apidocs:\n                first_child = subdoc.findChild()\n                object_name = first_child.select(\'.descname\')\n                title = object_name[0].text\n\n                subdocuments.append(\n                    Subdocument(\n                        uri=self.url + ""#"" + first_child[\'id\'],\n                        links=list(self.extract_links(subdoc)),\n                        html=str(subdoc),\n                        text=subdoc.text,\n                        title=title,\n                        snippet=extrat_object_snippet(subdoc)))\n\n        else:\n            main_container = self.html.select(\'.main-container\')\n            suptitle = extract_title(main_container[0])\n\n            for subdoc in self.html.select(\'div.section\'):\n                for section in subdoc.select(\'div.section,div#contents\'):\n                    section.extract()\n\n                title = extract_title(subdoc)\n\n                if suptitle and suptitle != title:\n                    title = suptitle + "" / "" + title\n\n                subdocuments.append(\n                    Subdocument(\n                        uri=self.url + ""#"" + subdoc[\'id\'],\n                        links=list(self.extract_links(subdoc)),\n                        html=str(subdoc),\n                        text=subdoc.text,\n                        title=title,\n                        snippet=\'\'))\n\n        if not subdocuments or (len(subdocuments) == 1 and not apidocs):\n            subdocumets = [\n                Subdocument(\n                    uri=self.url,\n                    links=self.links(),\n                    html=self.raw_html,\n                    text=self.text(),\n                    title=extract_title(subdoc),\n                    snippet=\'\')]\n\n        return subdocuments\n\n    def __reduce__(self):\n        arguments = (self.raw_html, self.url)\n        return (self.__class__, arguments)\n'"
site/search-index/pagerank.py,0,"b'import scipy\nimport numpy as np\nimport scipy.sparse as sp\n\n\ndef pagerank(graph_matrix, n_iter=100, alpha=0.9, tol=1e-6):\n    n_nodes = graph_matrix.shape[0]\n    n_edges_per_node = graph_matrix.sum(axis=1)\n    n_edges_per_node = np.array(n_edges_per_node).flatten()\n\n    np.seterr(divide=\'ignore\')\n    normilize_vector = np.where((n_edges_per_node != 0),\n                                1. / n_edges_per_node, 0)\n    np.seterr(divide=\'warn\')\n\n    normilize_matrix = sp.spdiags(normilize_vector, 0,\n                                  *graph_matrix.shape, format=\'csr\')\n\n    graph_proba_matrix = normilize_matrix * graph_matrix\n\n    teleport_proba = np.repeat(1. / n_nodes, n_nodes)\n    is_dangling, = scipy.where(normilize_vector == 0)\n\n    x_current = teleport_proba\n    for _ in range(n_iter):\n        x_previous = x_current.copy()\n\n        dangling_total_proba = sum(x_current[is_dangling])\n        x_current = (\n            x_current * graph_proba_matrix +\n            dangling_total_proba * teleport_proba\n        )\n        x_current = alpha * x_current + (1 - alpha) * teleport_proba\n\n        error = np.abs(x_current - x_previous).mean()\n        if error < tol:\n            break\n\n    else:\n        print(""PageRank didn\'t converge"")\n\n    return x_current\n'"
site/search-index/search.py,0,"b'import os\nimport pickle\nimport argparse\n\nimport nltk\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom build import INDEX_FILE\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""-q"", ""--query"", help=""search query"", default=None)\nparser.add_argument(""-n"", ""--number-of-results"", default=10, type=int,\n                    dest=""n_results"", help=""maximum number of top results"")\n\n\ndef load_index():\n    if not os.path.exists(INDEX_FILE):\n        raise IndexError(""Cannot find index file. Build a new file with ""\n                         ""command:\\n    python search-index/build.py"")\n\n    with open(INDEX_FILE, \'rb\') as f:\n        return pickle.load(f)\n\n\ndef answer_query(query, index, n_results=10):\n    documents, vocabulary, tf, idf, pagerank = index\n    # document_norm = sp.linalg.norm(tf, axis=1)\n\n    query_tokens = nltk.word_tokenize(query.lower())\n    indices = [vocabulary[t] for t in query_tokens if t in vocabulary]\n\n    if not indices:\n        print(\'\\nCannot find documents relevant to the specified query\')\n        return\n\n    document_vector = tf[:, indices]\n    query_vector = idf[indices]\n    rank = document_vector.dot(query_vector)  # / document_norm\n\n    document_ids, = rank.nonzero()\n\n    similarity_score = rank[document_ids].T\n    pagerank_score = pagerank[document_ids]\n    rank = similarity_score + pagerank_score\n    order = np.asarray(rank.argsort())\n\n    print(""Found {} relevant documents"".format(len(document_ids)))\n\n    order = order[:n_results]\n    for i, index in enumerate(reversed(order), start=1):\n        document_id = document_ids[index]\n        document = documents[document_id]\n        score = rank[index]\n\n        print("""")\n        print(""{}) {}"".format(i, document[\'uri\']))\n        print(""   Total Score: {}"".format(score))\n        print(""   PageRank Score: {}"".format(pagerank_score[index]))\n        print(""   Similarity Score: {}"".format(similarity_score[index]))\n\n\nif __name__ == \'__main__\':\n    print(""Search results on NeuPy website"")\n\n    index = load_index()\n    args = parser.parse_args()\n\n    if args.query is not None:\n        answer_query(args.query, index, args.n_results)\n\n    else:\n        while True:\n            print(\'\\n\' + \'-\' * 60)\n            query = input(\'Query: \')\n            answer_query(query, index, args.n_results)\n'"
site/search-index/webgraph.py,0,"b'import pprint\nfrom collections import OrderedDict, namedtuple\n\nimport six\nimport numpy as np\nimport scipy.sparse as sp\nfrom six.moves.urllib.parse import urlparse\n\nfrom pagerank import pagerank\n\n\nclass Link(object):\n    def __init__(self, uri, text=\'\'):\n        self.uri = uri\n        self.text = text\n\n    def __eq__(self, link):\n        if isinstance(link, six.string_types):\n            return self.uri == link\n        return self.uri == link.uri\n\n    def __hash__(self):\n        return hash(self.uri)\n\n    def __reduce__(self):\n        arguments = (self.uri, self.text)\n        return (self.__class__, arguments)\n\n    def __repr__(self):\n        if not self.text:\n            return ""<Link uri:{}>"".format(self.uri)\n        return ""<Link uri:{} text:{}>"".format(self.uri, self.text)\n\n\nclass WebPageGraph(object):\n    def __init__(self):\n        self.graph = OrderedDict()\n\n    @classmethod\n    def create_from_documents(cls, documents):\n        webgraph = cls()\n\n        for document in documents:\n            webgraph.add_page(Link(document.uri))\n\n        for document in documents:\n            for link in document.links:\n                if webgraph.has_page(link):\n                    webgraph.connect_pages(Link(document.uri), link)\n                    continue\n\n                link_uri = urlparse(link.uri)\n                link_url = link_uri.path\n\n                for existed_link, _ in webgraph:\n                    existed_link_uri = urlparse(existed_link.uri)\n                    existed_link_url = existed_link_uri.path\n\n                    if link_url == existed_link_url:\n                        webgraph.connect_pages(\n                            Link(document.uri),\n                            Link(existed_link.uri, link.text))\n        return webgraph\n\n    def add_page(self, page):\n        if page not in self.graph:\n            self.graph[page] = []\n\n    def has_page(self, page):\n        return page in self.graph\n\n    def connect_pages(self, page_1, page_2):\n        self.add_page(page_1)\n        self.add_page(page_2)\n\n        self.graph[page_1].append(page_2)\n\n    def page_linked_by(self, page):\n        for from_page, to_pages in self.graph.items():\n            for to_page in to_pages:\n                if to_page == page:\n                    yield from_page, to_page\n\n    def pagerank(self):\n        return pagerank(self.tomatrix())\n\n    def tomatrix(self):\n        links = {}\n\n        for node in self.graph:\n            links[node] = len(links)\n\n        rows, cols = [], []\n\n        for from_node, to_nodes in self.graph.items():\n            from_node_id = links[from_node]\n\n            for to_node in to_nodes:\n                to_node_id = links[to_node]\n\n                rows.append(from_node_id)\n                cols.append(to_node_id)\n\n        n_links = len(self)\n        data = np.ones(len(rows))\n        matrix = sp.coo_matrix((data, (rows, cols)),\n                               shape=(n_links, n_links))\n\n        return matrix\n\n    @property\n    def pages(self):\n        return list(self.graph.keys())\n\n    def __iter__(self):\n        for from_page, to_pages in self.graph.items():\n            yield from_page, to_pages\n\n    def __len__(self):\n        return len(self.graph)\n\n    def __repr__(self):\n        graph = list(self.graph.items())\n        return pprint.pformat(graph)\n'"
tests/algorithms/__init__.py,0,b''
tests/algorithms/test_network_features.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nimport numpy as np\nfrom sklearn import datasets\nfrom neupy import algorithms, layers\nfrom neupy.algorithms.base import preformat_value\n\nfrom helpers import catch_stdout\nfrom base import BaseTestCase\n\n\nclass NetworkMainTestCase(BaseTestCase):\n    def test_training_epoch_accumulation(self):\n        data, target = datasets.make_classification(\n            30, n_features=10, n_classes=2)\n\n        network = algorithms.GradientDescent([\n            layers.Input(10),\n            layers.Sigmoid(3),\n            layers.Sigmoid(1),\n        ])\n        self.assertEqual(network.last_epoch, 0)\n\n        network.train(data, target, epochs=10)\n        self.assertEqual(network.last_epoch, 10)\n\n        network.train(data, target, epochs=5)\n        self.assertEqual(network.last_epoch, 15)\n\n    def test_train_and_test_dataset_training(self):\n        data, target = datasets.make_classification(\n            30, n_features=10, n_classes=2)\n\n        network = algorithms.GradientDescent([\n                layers.Input(10),\n                layers.Sigmoid(3),\n                layers.Sigmoid(1),\n            ],\n            batch_size=None,\n        )\n\n        # Should work fine without exceptions\n        network.train(data, target, epochs=2)\n        network.train(data, target, data, target, epochs=2)\n\n        with self.assertRaises(ValueError):\n            network.train(data, target, data, epochs=2)\n\n        with self.assertRaises(ValueError):\n            network.train(data, target, y_test=target, epochs=2)\n\n    def test_wrong_number_of_training_epochs(self):\n        network = algorithms.GradientDescent(\n            layers.Input(2) > layers.Sigmoid(1),\n            verbose=False,\n            batch_size=None,\n        )\n\n        with self.assertRaisesRegexp(ValueError, ""a positive number""):\n            network.train(np.zeros((4, 2)), np.zeros((4, 1)), epochs=0)\n\n        with self.assertRaisesRegexp(ValueError, ""a positive number""):\n            network.train(np.zeros((4, 2)), np.zeros((4, 1)), epochs=-1)\n\n    def test_preformat_value(self):\n        def my_func():\n            pass\n\n        class MyClass(object):\n            pass\n\n        self.assertEqual(\'my_func\', preformat_value(my_func))\n        self.assertEqual(\'MyClass\', preformat_value(MyClass))\n\n        expected = [\'my_func\', \'MyClass\', 1]\n        actual = preformat_value((my_func, MyClass, 1))\n        np.testing.assert_array_equal(expected, actual)\n\n        expected = [\'my_func\', \'MyClass\', 1]\n        actual = preformat_value([my_func, MyClass, 1])\n        np.testing.assert_array_equal(expected, actual)\n\n        expected = sorted([\'my_func\', \'MyClass\', \'x\'])\n        actual = sorted(preformat_value({my_func, MyClass, \'x\'}))\n        np.testing.assert_array_equal(expected, actual)\n\n        self.assertEqual(1, preformat_value(1))\n\n        expected = (3, 2)\n        actual = preformat_value(np.ones((3, 2)))\n        np.testing.assert_array_equal(expected, actual)\n\n        expected = (1, 2)\n        actual = preformat_value(np.matrix([[1, 1]]))\n        np.testing.assert_array_equal(expected, actual)\n\n    def test_init_logging(self):\n        with catch_stdout() as out:\n            algorithms.GradientDescent(\n                layers.Input(2) > layers.Sigmoid(3) > layers.Sigmoid(1),\n                verbose=False,\n            )\n            terminal_output = out.getvalue()\n            self.assertEqual("""", terminal_output.strip())\n\n        with catch_stdout() as out:\n            algorithms.GradientDescent(\n                layers.Input(2) > layers.Sigmoid(3) > layers.Sigmoid(1),\n                verbose=True,\n            )\n            terminal_output = out.getvalue()\n\n            self.assertNotEqual("""", terminal_output.strip())\n            self.assertIn(""verbose = True"", terminal_output)\n'"
tests/algorithms/test_plots.py,0,"b'import warnings\n\nimport mock\nimport pytest\nimport matplotlib.pyplot as plt\n\nfrom neupy import algorithms, layers\nfrom neupy.algorithms.plots import load_pandas_module\n\nfrom helpers import simple_classification\nfrom base import BaseTestCase\n\n\nclass PlotsTestCase(BaseTestCase):\n    def setUp(self):\n        super(PlotsTestCase, self).setUp()\n        self.network = layers.join(\n            layers.Input(10),\n            layers.Sigmoid(20),\n            layers.Sigmoid(1),\n        )\n\n    def test_failed_pandas_import(self):\n        with mock.patch(\'pkgutil.find_loader\') as mock_find_loader:\n            mock_find_loader.return_value = None\n\n            message = ""The `pandas` library is not installed""\n            with self.assertRaisesRegexp(ImportError, message):\n                load_pandas_module()\n\n    def test_not_tarining_data_to_plot(self):\n        optimizer = algorithms.Adadelta(self.network)\n\n        with warnings.catch_warnings(record=True) as warns:\n            optimizer.plot_errors()\n            self.assertEqual(1, len(warns))\n            self.assertEqual(\n                str(warns[-1].message),\n                ""There is no data to plot"")\n\n    @pytest.mark.mpl_image_compare\n    def test_plot_errors_no_batch(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        optimizer = algorithms.Adadelta(self.network, batch_size=None)\n        optimizer.train(x_train, y_train, x_test, y_test, epochs=10)\n        optimizer.plot_errors(show=False)\n\n        return plt.gcf()\n\n    @pytest.mark.mpl_image_compare\n    def test_plot_errors_batch(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        optimizer = algorithms.Adadelta(\n            self.network,\n            shuffle_data=True,\n            batch_size=10,\n        )\n        optimizer.train(x_train, y_train, x_test, y_test, epochs=100)\n        optimizer.plot_errors(show=False)\n\n        return plt.gcf()\n\n    @pytest.mark.mpl_image_compare\n    def test_plot_errors_show_triggered_automatically(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        optimizer = algorithms.Adadelta(\n            self.network,\n            shuffle_data=True,\n            batch_size=10,\n        )\n        optimizer.train(x_train, y_train, epochs=100)\n        events = []\n\n        def mocked_show(*args, **kwargs):\n            events.append(\'show\')\n\n        with mock.patch(\'matplotlib.pyplot.show\', side_effect=mocked_show):\n            optimizer.plot_errors(show=True)\n            self.assertSequenceEqual(events, [\'show\'])\n\n        return plt.gcf()\n'"
tests/algorithms/test_regularizers.py,0,"b'import numpy as np\n\nfrom neupy import algorithms, layers\n\nfrom base import BaseTestCase\nfrom helpers import simple_classification\n\n\nclass L2RegularizationTestCase(BaseTestCase):\n    def test_l2_regularization(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5, weight=2, bias=2),\n        )\n        regularizer = algorithms.l2(0.01, exclude=[\'bias\'])\n\n        regularization_cost = self.eval(regularizer(network))\n        self.assertAlmostEqual(regularization_cost, 2.0)\n\n    def test_l2_regularization_with_bias(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5, weight=2, bias=2),\n        )\n        regularizer = algorithms.l2(0.01, exclude=[])\n\n        regularization_cost = self.eval(regularizer(network))\n        self.assertAlmostEqual(regularization_cost, 2.2)\n\n    def test_l2_repr(self):\n        l2_repr = repr(algorithms.l2(0.01, exclude=[\'bias\']))\n        self.assertEqual(l2_repr, ""l2(0.01, exclude=[\'bias\'])"")\n\n        l2_repr = repr(algorithms.l2(decay_rate=0.01, exclude=[\'bias\']))\n        self.assertEqual(l2_repr, ""l2(decay_rate=0.01, exclude=[\'bias\'])"")\n\n    def test_training_with_l2_regularization(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        mnet = algorithms.Momentum(\n            [\n                layers.Input(10),\n                layers.Sigmoid(20),\n                layers.Sigmoid(1)\n            ],\n            step=0.35,\n            momentum=0.99,\n            batch_size=None,\n            verbose=False,\n            nesterov=True,\n            regularizer=algorithms.l2(0.001),\n        )\n        mnet.train(x_train, y_train, x_test, y_test, epochs=40)\n        self.assertGreater(0.15, mnet.errors.valid[-1])\n\n\nclass L1RegularizationTestCase(BaseTestCase):\n    def test_l1_regularization(self):\n        weight = 2 * np.sign(np.random.random((10, 5)) - 0.5)\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5, weight=weight, bias=2),\n        )\n        regularizer = algorithms.l1(0.01)\n\n        regularization_cost = self.eval(regularizer(network))\n        self.assertAlmostEqual(regularization_cost, 1.0)\n\n\nclass MaxNormRegularizationTestCase(BaseTestCase):\n    def test_max_norm_regularization(self):\n        weight = np.arange(20).reshape(4, 5)\n        network = layers.join(\n            layers.Input(4),\n            layers.Relu(5, weight=weight, bias=100),\n        )\n        regularizer = algorithms.maxnorm(0.01)\n\n        regularization_cost = self.eval(regularizer(network))\n        self.assertAlmostEqual(regularization_cost, 0.19)\n'"
tests/algorithms/test_signals.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\n\nimport numpy as np\nimport progressbar\nfrom sklearn.datasets import make_classification\n\nfrom neupy import algorithms, layers\nfrom neupy.exceptions import StopTraining\nfrom neupy.algorithms.signals import format_time\n\nfrom base import BaseTestCase\nfrom helpers import catch_stdout\n\n\ndef train_network(epochs=2, **kwargs):\n    network = algorithms.GradientDescent(\n        [\n            layers.Input(10),\n            layers.Sigmoid(3),\n            layers.Sigmoid(1),\n        ],\n        **kwargs\n    )\n\n    data, target = make_classification(30, n_features=10, n_classes=2)\n    network.train(data, target, data, target, epochs=epochs)\n\n    return network\n\n\nclass SignalsTestCase(BaseTestCase):\n    def test_signal_func_one_training_update_end(self):\n        global triggered_times\n        triggered_times = 0\n\n        def print_message(network):\n            global triggered_times\n            triggered_times += 1\n\n        train_network(\n            epochs=4,\n            signals=print_message,\n            batch_size=None)\n\n        self.assertEqual(triggered_times, 4)\n\n    def test_signal_func_stop_iteration(self):\n        def stop_training_after_the_5th_epoch(network):\n            if network.last_epoch == 5:\n                raise StopTraining(""Stopped training"")\n\n        network = train_network(\n            epochs=10,\n            batch_size=None,\n            signals=stop_training_after_the_5th_epoch)\n\n        self.assertEqual(network.last_epoch, 5)\n\n    def test_custom_signal_class(self):\n        class SimpleSignal(object):\n            triggers = []\n\n            n_train_start = 0\n            n_train_end = 0\n\n            n_epoch_start = 0\n            n_epoch_end = 0\n\n            n_update_start = 0\n            n_update_end = 0\n\n            n_train_error = 0\n            n_valid_error = 0\n\n            def train_start(self, network, **data):\n                self.n_train_start += 1\n                self.triggers.append(\'train_start\')\n\n            def train_end(self, network):\n                self.n_train_end += 1\n                self.triggers.append(\'train_end\')\n\n            def epoch_start(self, network):\n                self.n_epoch_start += 1\n                self.triggers.append(\'epoch_start\')\n\n            def epoch_end(self, network):\n                self.n_epoch_end += 1\n                self.triggers.append(\'epoch_end\')\n\n            def update_start(self, network):\n                self.n_update_start += 1\n                self.triggers.append(\'update_start\')\n\n            def update_end(self, network):\n                self.n_update_end += 1\n                self.triggers.append(\'update_end\')\n\n            def train_error(self, network, **data):\n                self.n_train_error += 1\n                self.triggers.append(\'train_error\')\n\n            def valid_error(self, network, **data):\n                self.n_valid_error += 1\n                self.triggers.append(\'valid_error\')\n\n        simple_signal = SimpleSignal()\n        train_network(signals=[simple_signal], batch_size=10)\n\n        self.assertEqual(simple_signal.n_train_start, 1)\n        self.assertEqual(simple_signal.n_train_end, 1)\n\n        self.assertEqual(simple_signal.n_epoch_start, 2)\n        self.assertEqual(simple_signal.n_epoch_end, 2)\n\n        self.assertEqual(simple_signal.n_update_start, 6)\n        self.assertEqual(simple_signal.n_update_end, 6)\n\n        expected_triggers = [\n            \'train_start\',\n\n            \'epoch_start\',\n            \'update_start\', \'train_error\', \'update_end\',\n            \'update_start\', \'train_error\', \'update_end\',\n            \'update_start\', \'train_error\', \'update_end\',\n            \'valid_error\',\n            \'epoch_end\',\n\n            \'epoch_start\',\n            \'update_start\', \'train_error\', \'update_end\',\n            \'update_start\', \'train_error\', \'update_end\',\n            \'update_start\', \'train_error\', \'update_end\',\n            \'valid_error\',\n            \'epoch_end\',\n\n            \'train_end\',\n        ]\n        self.assertSequenceEqual(expected_triggers, simple_signal.triggers)\n\n    def test_multiple_signals(self):\n        class SimpleSignal(object):\n            events = []\n\n            def __init__(self, name):\n                self.name = name\n\n            def update_end(self, network):\n                self.events.append(self.name)\n\n        train_network(\n            batch_size=10,\n            signals=[SimpleSignal(\'a\'), SimpleSignal(\'b\')])\n\n        self.assertEqual(SimpleSignal.events, [\'a\', \'b\'] * 6)\n\n    def test_print_training_progress_signal(self):\n        x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n        y_train = np.array([[1, 0, 0, 1]]).T\n\n        Case = namedtuple(""Case"", ""show_epoch should_be_n_times n_epochs"")\n        cases = (\n            Case(show_epoch=5, should_be_n_times=3, n_epochs=10),\n            Case(show_epoch=7, should_be_n_times=3, n_epochs=10),\n            Case(show_epoch=100, should_be_n_times=2, n_epochs=10),\n        )\n\n        for case in cases:\n            with catch_stdout() as out:\n                bpnet = algorithms.GradientDescent(\n                    layers.join(\n                        layers.Input(2),\n                        layers.Sigmoid(3),\n                        layers.Sigmoid(1),\n                    ),\n                    step=0.1,\n                    verbose=True,\n                    batch_size=None,\n                    show_epoch=case.show_epoch,\n                )\n                bpnet.train(x_train, y_train, epochs=case.n_epochs)\n                terminal_output = out.getvalue()\n\n            # One of the choices has to be true whether other\n            # choices should give count equal to zero.\n            time_counts = (\n                terminal_output.count("" \xce\xbcs]"") +\n                terminal_output.count("" ms]"") +\n                terminal_output.count("" ns]"")\n            )\n            self.assertEqual(case.should_be_n_times, time_counts)\n\n    def test_network_training_summary(self):\n        with catch_stdout() as out:\n            network = algorithms.GradientDescent(\n                layers.join(\n                    layers.Input(2),\n                    layers.Sigmoid(3),\n                    layers.Sigmoid(1),\n                ),\n                verbose=False,\n                batch_size=None,\n            )\n\n            x = np.zeros((5, 2))\n            y = np.zeros((5, 1))\n\n            network.verbose = True\n            n_epochs = 10\n            network.train(x, y, epochs=n_epochs)\n\n            terminal_output = out.getvalue().strip()\n\n        # `n_epochs - 1` because \\n appears only between\n        # inline summary lines.\n        # Also network prints 5 additional lines at the beggining\n        self.assertEqual(terminal_output.count(\'\\n\'), n_epochs - 1)\n        self.assertEqual(terminal_output.count(\'train: \'), n_epochs)\n\n    def test_format_time(self):\n        self.assertEqual(""01:06:40"", format_time(4000))\n        self.assertEqual(""02:05"", format_time(125))\n        self.assertEqual(""45 sec"", format_time(45))\n        self.assertEqual(""100 ms"", format_time(0.1))\n        self.assertEqual(""10 \xce\xbcs"", format_time(1e-5))\n        self.assertEqual(""200 ns"", format_time(2e-7))\n\n    def test_progressbar_signal(self):\n        x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n        y_train = np.array([[1, 0, 0, 1]]).T\n\n        class BatchStartSignal(object):\n            def update_end(self, network):\n                progressbar_signal = network.events.signals[0]\n                bar = progressbar_signal.bar\n\n                if isinstance(bar, progressbar.NullBar):\n                    network.nullbar += 1\n                else:\n                    network.otherbar += 1\n\n        network = algorithms.GradientDescent(\n            layers.join(\n                layers.Input(2),\n                layers.Sigmoid(1),\n            ),\n            verbose=False,\n            batch_size=2,\n            signals=BatchStartSignal\n        )\n\n        network.nullbar = 0\n        network.otherbar = 0\n\n        network.verbose = True\n        network.train(x_train, y_train, epochs=10)\n\n        self.assertEqual(network.nullbar, 0)\n        self.assertEqual(network.otherbar, 20)\n\n        network.verbose = True\n        network.batch_size = 10\n        network.train(x_train, y_train, epochs=10)\n\n        self.assertEqual(network.nullbar, 10)\n        self.assertEqual(network.otherbar, 20)\n\n        network.verbose = False\n        network.batch_size = 1\n        network.train(x_train, y_train, epochs=10)\n\n        self.assertEqual(network.nullbar, 50)\n        self.assertEqual(network.otherbar, 20)\n'"
tests/algorithms/test_step_updates.py,0,"b'from neupy import algorithms, layers\n\nfrom base import BaseTestCase\nfrom helpers import simple_classification\n\n\nclass StepUpdateTestCase(BaseTestCase):\n    def assert_invalid_step_values(self, step, initial_value,\n                                   final_value, epochs):\n\n        x_train, x_test, y_train, y_test = simple_classification()\n        optimizer = algorithms.Momentum(\n            [\n                layers.Input(10),\n                layers.Sigmoid(5),\n                layers.Sigmoid(1),\n            ],\n            step=step,\n            momentum=0.99,\n            batch_size=None,\n            verbose=False,\n            nesterov=True,\n        )\n\n        step = self.eval(optimizer.step)\n        self.assertAlmostEqual(step, initial_value)\n\n        optimizer.train(x_train, y_train, x_test, y_test, epochs=epochs)\n\n        step = self.eval(optimizer.step)\n        self.assertAlmostEqual(step, final_value)\n\n    def test_step_decay(self):\n        self.assert_invalid_step_values(\n            algorithms.step_decay(\n                initial_value=0.1,\n                reduction_freq=10,\n            ),\n            initial_value=0.1,\n            final_value=0.1 / 4,\n            epochs=31,\n        )\n\n    def test_exponential_decay(self):\n        self.assert_invalid_step_values(\n            algorithms.exponential_decay(\n                initial_value=0.1,\n                reduction_freq=10,\n                reduction_rate=0.9,\n            ),\n            initial_value=0.1,\n            final_value=0.1 * 0.9 ** 3,\n            epochs=31,\n        )\n\n    def test_polynomial_decay_limit_reached(self):\n        self.assert_invalid_step_values(\n            algorithms.polynomial_decay(\n                initial_value=0.1,\n                decay_iter=20,\n                minstep=0.02,\n            ),\n            initial_value=0.1,\n            final_value=0.02,\n            epochs=31,\n        )\n\n    def test_polynomial_decay_limit_not_reached(self):\n        self.assert_invalid_step_values(\n            algorithms.polynomial_decay(\n                initial_value=0.1,\n                decay_iter=40,\n                minstep=0.02,\n            ),\n            initial_value=0.1,\n            final_value=0.04,\n            epochs=31,\n        )\n'"
tests/architectures/__init__.py,0,b''
tests/architectures/test_mixtures_of_experts.py,0,"b'import copy\n\nimport numpy as np\nfrom sklearn import datasets, preprocessing, model_selection\n\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.algorithms.gd import objectives\nfrom neupy import algorithms, layers, architectures\n\nfrom base import BaseTestCase\n\n\nclass MixtureOfExpertsTestCase(BaseTestCase):\n    def setUp(self):\n        super(MixtureOfExpertsTestCase, self).setUp()\n        self.networks = [\n            layers.Input(1),\n            layers.join(\n                layers.Input(1),\n                layers.Sigmoid(20),\n                layers.Sigmoid(1)\n            ),\n        ]\n\n    def test_mixture_of_experts_problem_with_specific_network(self):\n        with self.assertRaisesRegexp(ValueError, ""specified as a list""):\n            architectures.mixture_of_experts(*self.networks)\n\n        with self.assertRaisesRegexp(ValueError, ""has more than one input""):\n            last_network = layers.join(\n                layers.parallel(\n                    layers.Input(1),\n                    layers.Input(2),\n                ),\n                layers.Concatenate(),\n            )\n            architectures.mixture_of_experts(\n                networks=self.networks + [last_network])\n\n        with self.assertRaisesRegexp(ValueError, ""has more than one output""):\n            last_network = layers.join(\n                layers.Input(1),\n                layers.parallel(\n                    layers.Softmax(1),\n                    layers.Softmax(1),\n                ),\n            )\n            architectures.mixture_of_experts(\n                networks=self.networks + [last_network])\n\n        error_message = (\n            ""Each network from the mixture of experts has to ""\n            ""process only 2-dimensional inputs. Network #2.+""\n            ""Input layer\'s shape: \\(\\?, 1, 1, 1\\)""\n        )\n        with self.assertRaisesRegexp(ValueError, error_message):\n            last_network = layers.Input((1, 1, 1))\n            architectures.mixture_of_experts(\n                networks=self.networks + [last_network])\n\n    def test_mixture_of_experts_problem_with_incompatible_networks(self):\n        error_message = ""Networks have incompatible input shapes.""\n        with self.assertRaisesRegexp(ValueError, error_message):\n            architectures.mixture_of_experts(\n                networks=self.networks + [layers.Input(10)])\n\n        error_message = ""Networks have incompatible output shapes.""\n        with self.assertRaisesRegexp(ValueError, error_message):\n            architectures.mixture_of_experts(\n                networks=self.networks + [\n                    layers.Input(1) >> layers.Relu(10)\n                ])\n\n    def test_mixture_of_experts_init_gating_network_exceptions(self):\n        with self.assertRaisesRegexp(ValueError, ""Invalid type""):\n            architectures.mixture_of_experts(\n                networks=self.networks,\n                gating_layer=(layers.Input(1) >> layers.Softmax(2)))\n\n        error_message = (\n            ""Gating layer can work only for combining only ""\n            ""10 networks, got 2 networks instead.""\n        )\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            architectures.mixture_of_experts(\n                networks=self.networks,\n                gating_layer=layers.Softmax(10))\n\n    def test_mixture_of_experts_undefined_features(self):\n        error_message = (\n            ""Cannot create mixture of experts model, because ""\n            ""number of input features is unknown""\n        )\n        with self.assertRaisesRegexp(ValueError, error_message):\n            architectures.mixture_of_experts([\n                layers.Input(None) >> layers.Relu(10),\n                layers.Input(None) >> layers.Relu(10),\n            ])\n\n    def test_mixture_of_experts_non_network_inputs(self):\n        error_message = (\n            ""Invalid input, Mixture of experts expects networks/layers""\n        )\n        with self.assertRaisesRegexp(TypeError, error_message):\n            architectures.mixture_of_experts([\n                layers.Input(5) >> layers.Relu(10),\n                [layers.Input(5), layers.Relu(10)]\n            ])\n\n    def test_mixture_of_experts_multi_class_classification(self):\n        insize, outsize = (10, 3)\n        n_epochs = 10\n\n        default_configs = dict(\n            step=0.1,\n            batch_size=10,\n            loss=\'categorical_crossentropy\',\n            verbose=False)\n\n        architecture = layers.join(\n            layers.Input(insize),\n            layers.Relu(20),\n            layers.Softmax(outsize))\n\n        data, target = datasets.make_classification(\n            n_samples=200,\n            n_features=insize,\n            n_classes=outsize,\n            n_clusters_per_class=2,\n            n_informative=5)\n\n        input_scaler = preprocessing.MinMaxScaler((-1, 1))\n        one_hot = preprocessing.OneHotEncoder(categories=\'auto\')\n\n        target = target.reshape((-1, 1))\n        encoded_target = one_hot.fit_transform(target)\n        x_train, x_test, y_train, y_test = model_selection.train_test_split(\n            input_scaler.fit_transform(data),\n            np.asarray(encoded_target.todense()),\n            test_size=0.2)\n\n        # -------------- Train single GradientDescent -------------- #\n\n        bpnet = algorithms.Momentum(\n            copy.deepcopy(architecture),\n            **default_configs\n        )\n\n        bpnet.train(x_train, y_train, epochs=n_epochs)\n        network_output = bpnet.predict(x_test)\n\n        network_error = self.eval(\n            objectives.categorical_crossentropy(y_test, network_output))\n\n        # -------------- Train ensemlbe -------------- #\n\n        moe = algorithms.Momentum(\n            architectures.mixture_of_experts([\n                copy.deepcopy(architecture),\n                copy.deepcopy(architecture),\n                copy.deepcopy(architecture),\n            ]),\n            **default_configs\n        )\n        moe.train(x_train, y_train, epochs=n_epochs)\n        ensemble_output = moe.predict(x_test)\n\n        ensemlbe_error = self.eval(\n            objectives.categorical_crossentropy(y_test, ensemble_output))\n\n        self.assertGreater(network_error, ensemlbe_error)\n\n    def test_mixture_of_experts_architecture(self):\n        network = architectures.mixture_of_experts([\n            layers.join(\n                layers.Input(10),\n                layers.Relu(5),\n            ),\n            layers.join(\n                layers.Input(10),\n                layers.Relu(20),\n                layers.Relu(5),\n            ),\n            layers.join(\n                layers.Input(10),\n                layers.Relu(30),\n                layers.Relu(40),\n                layers.Relu(5),\n            ),\n        ])\n\n        self.assertEqual(len(network), 12)\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, (None, 5))\n\n        random_input = asfloat(np.random.random((3, 10)))\n        prediction = self.eval(network.output(random_input))\n\n        self.assertEqual(prediction.shape, (3, 5))\n'"
tests/architectures/test_resnet.py,0,"b""import numpy as np\n\nfrom neupy.utils import asfloat\nfrom neupy import architectures\n\nfrom base import BaseTestCase\n\n\nclass Resnet50TestCase(BaseTestCase):\n    def test_resnet50_architecture(self):\n        resnet50 = architectures.resnet50()\n        self.assertShapesEqual(resnet50.input_shape, (None, 224, 224, 3))\n        self.assertShapesEqual(resnet50.output_shape, (None, 1000))\n\n        random_input = asfloat(np.random.random((7, 224, 224, 3)))\n        prediction = self.eval(resnet50.output(random_input))\n        self.assertEqual(prediction.shape, (7, 1000))\n\n    def test_resnet50_exceptions(self):\n        with self.assertRaises(ValueError):\n            architectures.resnet50(in_out_ratio=2)\n\n    def test_resnet50_no_global_pooling(self):\n        resnet50 = architectures.resnet50(include_global_pool=False)\n\n        self.assertShapesEqual(resnet50.input_shape, (None, 224, 224, 3))\n        self.assertShapesEqual(resnet50.output_shape, (None, 7, 7, 2048))\n\n    def test_resnet50_spatial(self):\n        resnet50 = architectures.resnet50(\n            include_global_pool=False,\n            in_out_ratio=8,\n        )\n        self.assertShapesEqual(resnet50.input_shape, (None, 224, 224, 3))\n        self.assertShapesEqual(resnet50.output_shape, (None, 28, 28, 2048))\n\n        random_input = asfloat(np.random.random((7, 224, 224, 3)))\n        prediction = self.eval(resnet50.output(random_input))\n        self.assertEqual(prediction.shape, (7, 28, 28, 2048))\n\n    def test_resnet50_dilation_rates(self):\n        resnet50 = architectures.resnet50(\n            include_global_pool=False,\n            in_out_ratio=8,\n        )\n\n        layer = resnet50.layer('res4d_branch2b')\n        self.assertEqual(layer.dilation, (2, 2))\n\n        layer = resnet50.layer('res5a_branch2b')\n        self.assertEqual(layer.dilation, (2, 2))\n\n        layer = resnet50.layer('res5b_branch2b')\n        self.assertEqual(layer.dilation, (4, 4))\n"""
tests/architectures/test_squeezenet.py,0,"b'import numpy as np\n\nfrom neupy.utils import asfloat\nfrom neupy import architectures\n\nfrom base import BaseTestCase\n\n\nclass SqueezenetTestCase(BaseTestCase):\n    def test_squeezenet_architecture(self):\n        squeezenet = architectures.squeezenet()\n        self.assertShapesEqual(squeezenet.input_shape, (None, 227, 227, 3))\n        self.assertShapesEqual(squeezenet.output_shape, (None, 1000))\n\n        random_input = asfloat(np.random.random((7, 227, 227, 3)))\n        prediction = self.eval(squeezenet.output(random_input))\n        self.assertEqual(prediction.shape, (7, 1000))\n'"
tests/architectures/test_vgg16.py,0,"b'import numpy as np\n\nfrom neupy.utils import asfloat\nfrom neupy import architectures\n\nfrom base import BaseTestCase\n\n\nclass VGG16TestCase(BaseTestCase):\n    def test_vgg16_architecture(self):\n        vgg16 = architectures.vgg16()\n        self.assertShapesEqual(vgg16.input_shape, (None, 224, 224, 3))\n        self.assertShapesEqual(vgg16.output_shape, (None, 1000))\n\n        random_input = asfloat(np.random.random((2, 224, 224, 3)))\n        prediction = self.eval(vgg16.output(random_input))\n        self.assertEqual(prediction.shape, (2, 1000))\n'"
tests/architectures/test_vgg19.py,0,"b'import numpy as np\n\nfrom neupy.utils import asfloat\nfrom neupy import architectures\n\nfrom base import BaseTestCase\n\n\nclass VGG19TestCase(BaseTestCase):\n    def test_vgg19_architecture(self):\n        vgg19 = architectures.vgg19()\n        self.assertShapesEqual(vgg19.input_shape, (None, 224, 224, 3))\n        self.assertShapesEqual(vgg19.output_shape, (None, 1000))\n\n        random_input = asfloat(np.random.random((7, 224, 224, 3)))\n        prediction = self.eval(vgg19.output(random_input))\n        self.assertEqual(prediction.shape, (7, 1000))\n'"
tests/compatibilities/__init__.py,0,b''
tests/compatibilities/test_pandas.py,0,"b""import pandas as pd\nfrom sklearn import datasets, preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom neupy import algorithms, layers\nfrom neupy.utils import asfloat\nfrom neupy.algorithms.gd import objectives\n\nfrom base import BaseTestCase\n\n\nclass PandasCompatibilityTestCase(BaseTestCase):\n    def test_pandas_for_bp(self):\n        dataset = datasets.load_diabetes()\n        target = dataset.target.reshape(-1, 1)\n\n        input_scaler = preprocessing.MinMaxScaler()\n        target_scaler = preprocessing.MinMaxScaler()\n\n        n_features = dataset.data.shape[1]\n        input_columns = ['column_' + str(i) for i in range(n_features)]\n\n        pandas_data = pd.DataFrame(dataset.data, columns=input_columns)\n        pandas_data['target'] = target_scaler.fit_transform(target)\n        pandas_data[input_columns] = input_scaler.fit_transform(\n            pandas_data[input_columns]\n        )\n\n        x_train, x_test, y_train, y_test = train_test_split(\n            asfloat(pandas_data[input_columns]),\n            asfloat(pandas_data['target']),\n            test_size=0.15\n        )\n\n        bpnet = algorithms.GradientDescent(\n            [\n                layers.Input(10),\n                layers.Sigmoid(30),\n                layers.Sigmoid(1),\n            ],\n            batch_size=None,\n        )\n        bpnet.train(x_train, y_train, epochs=50)\n        y_predict = bpnet.predict(x_test).reshape(-1, 1)\n        y_test = y_test.reshape(-1, 1)\n\n        error = objectives.rmsle(\n            target_scaler.inverse_transform(y_test),\n            target_scaler.inverse_transform(y_predict).round()\n        )\n        error = self.eval(error)\n        self.assertGreater(0.5, error)\n"""
tests/compatibilities/test_sklearn_compatibility.py,0,"b""import numpy as np\nfrom sklearn import datasets, preprocessing, model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\nfrom neupy import algorithms, layers\nfrom neupy.utils import asfloat\nfrom neupy.algorithms.gd import objectives\n\nfrom base import BaseTestCase\n\n\nclass SklearnCompatibilityTestCase(BaseTestCase):\n    def test_pipeline(self):\n        dataset = datasets.load_diabetes()\n        target_scaler = preprocessing.MinMaxScaler()\n        target = dataset.target.reshape(-1, 1)\n\n        x_train, x_test, y_train, y_test = train_test_split(\n            asfloat(dataset.data),\n            asfloat(target_scaler.fit_transform(target)),\n            test_size=0.15\n        )\n\n        network = algorithms.GradientDescent(\n            network=[\n                layers.Input(10),\n                layers.Sigmoid(25),\n                layers.Sigmoid(1),\n            ],\n            batch_size=None,\n            show_epoch=100,\n            verbose=False,\n        )\n        pipeline = Pipeline([\n            ('min_max_scaler', preprocessing.MinMaxScaler()),\n            ('gd', network),\n        ])\n        pipeline.fit(x_train, y_train, gd__epochs=50)\n        y_predict = pipeline.predict(x_test)\n\n        error = objectives.rmsle(\n            target_scaler.inverse_transform(y_test),\n            target_scaler.inverse_transform(y_predict).round()\n        )\n        error = self.eval(error)\n        self.assertGreater(0.5, error)\n\n    def test_grid_search(self):\n        def scorer(network, X, y):\n            y = asfloat(y)\n            result = asfloat(network.predict(X))\n            return self.eval(objectives.rmsle(result[:, 0], y))\n\n        dataset = datasets.load_diabetes()\n        x_train, x_test, y_train, y_test = train_test_split(\n            dataset.data, dataset.target, test_size=0.3\n        )\n\n        grnnet = algorithms.GRNN(std=0.5, verbose=False)\n        grnnet.train(x_train, y_train)\n        error = scorer(grnnet, x_test, y_test)\n\n        self.assertAlmostEqual(0.513, error, places=3)\n\n        random_search = model_selection.RandomizedSearchCV(\n            grnnet,\n            param_distributions={'std': np.arange(1e-2, 0.1, 1e-4)},\n            n_iter=10,\n            scoring=scorer,\n            random_state=self.random_seed,\n            cv=3,\n        )\n        random_search.fit(dataset.data, dataset.target)\n        scores = random_search.cv_results_\n\n        best_score = min(scores['mean_test_score'])\n        self.assertAlmostEqual(0.4266, best_score, places=3)\n\n    def test_transfrom_method(self):\n        dataset = datasets.load_diabetes()\n\n        grnnet = algorithms.GRNN(std=0.5, verbose=False)\n        grnnet.train(dataset.data, dataset.target)\n\n        y_predicted = grnnet.predict(dataset.data)\n        y_transformed = grnnet.transform(dataset.data)\n\n        np.testing.assert_array_almost_equal(y_predicted, y_transformed)\n"""
tests/core/__init__.py,0,b''
tests/core/test_configs.py,0,"b""from abc import abstractmethod\n\nfrom neupy.core.config import Configurable, ConfigurableABC\nfrom neupy.core.properties import Property\n\nfrom base import BaseTestCase\n\n\nclass ConfigsTestCase(BaseTestCase):\n    def test_configuration_inheritance(self):\n        class A(Configurable):\n            property_a = Property()\n\n        class B(A):\n            property_b = Property()\n\n        class C(B):\n            property_c = Property()\n\n        class D(A):\n            property_d = Property()\n\n        self.assertEqual(sorted(A.options.keys()), ['property_a'])\n        self.assertEqual(sorted(B.options.keys()),\n                         ['property_a', 'property_b'])\n        self.assertEqual(sorted(D.options.keys()),\n                         ['property_a', 'property_d'])\n        self.assertEqual(sorted(C.options.keys()),\n                         ['property_a', 'property_b', 'property_c'])\n\n    def test_invalid_configs_setup(self):\n        class A(Configurable):\n            correct_property = Property()\n\n        A(correct_property=3)\n        with self.assertRaises(ValueError):\n            A(invalid_property=3)\n\n    def test_abc_config(self):\n        class A(ConfigurableABC):\n            @abstractmethod\n            def important_method(self):\n                pass\n\n        class InheritA(A):\n            pass\n\n        with self.assertRaises(TypeError):\n            InheritA()\n\n    def test_config_with_parent_without_options(self):\n        class A(object):\n            # Doesn't have Configurable as a parent class\n            property_a = Property()\n\n        class B(Configurable):\n            property_b = Property()\n\n        class C(A, B):\n            pass\n\n        self.assertIn('property_b', C.options)\n        self.assertNotIn('property_a', C.options)\n"""
tests/core/test_logging.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\n\nfrom neupy.core import logs\nfrom neupy.core.logs import Verbose, TerminalLogger\n\nfrom base import BaseTestCase\nfrom helpers import catch_stdout\n\n\nclass LoggingTestCase(BaseTestCase):\n    def test_logging_exceptions(self):\n        with self.assertRaises(ValueError):\n            logs = TerminalLogger()\n            logs.message(""tag"", ""text"", color=""unknown-color"")\n\n    def test_logging_switcher(self):\n        class A(Verbose):\n            def callme(self):\n                self.logs.message(""TEST"", ""output"")\n\n        with catch_stdout() as out:\n            a = A(verbose=True)\n            a.callme()\n            terminal_output = out.getvalue()\n\n            self.assertIn(""TEST"", terminal_output)\n            self.assertIn(""output"", terminal_output)\n\n        a.verbose = False\n        with catch_stdout() as out:\n            a.callme()\n            terminal_output = out.getvalue()\n\n            self.assertNotIn(""TEST"", terminal_output)\n            self.assertNotIn(""output"", terminal_output)\n\n    def test_logging_methods(self):\n        with catch_stdout() as out:\n            logs = TerminalLogger()\n\n            Case = namedtuple(""Case"", ""method msg_args expectation"")\n            test_cases = (\n                Case(\n                    logs.write,\n                    msg_args=[""Simple text""],\n                    expectation=""Simple text""\n                ),\n                Case(\n                    logs.message,\n                    msg_args=[""TEST"", ""Message""],\n                    expectation=r""\\[.*TEST.*\\] Message"",\n                ),\n                Case(\n                    logs.title,\n                    msg_args=[""Title""],\n                    expectation=r""\\n.*Title.*\\n"",\n                ),\n            )\n\n            for test_case in test_cases:\n                test_case.method(*test_case.msg_args)\n                terminal_output = out.getvalue()\n                self.assertRegexpMatches(\n                    terminal_output, test_case.expectation)\n\n    def test_terminal_colors(self):\n        logger = TerminalLogger()\n        real_is_color_supported = logs.is_color_supported\n\n        logs.is_color_supported = lambda: False\n        red_color = logger.colors[\'red\']\n        self.assertEqual(\'test\', red_color(\'test\'))\n\n        logs.is_color_supported = lambda: True\n        self.assertNotEqual(\'test\', red_color(\'test\'))\n        self.assertIn(\'test\', red_color(\'test\'))\n\n        logs.is_color_supported = real_is_color_supported\n'"
tests/core/test_properties.py,0,"b'from collections import namedtuple\n\nimport numpy as np\n\nfrom neupy.core.config import Configurable\nfrom neupy.core.properties import (\n    Property, ArrayProperty, TypedListProperty,\n    BoundedProperty, ProperFractionProperty,\n    NumberProperty, IntProperty, ChoiceProperty,\n    WithdrawProperty,\n)\n\nfrom base import BaseTestCase\n\n\nclass PropertiesBasicsTestCase(BaseTestCase):\n    def test_basic_properties(self):\n        class A(Configurable):\n            int_property = Property(expected_type=int)\n\n        a = A()\n        a.int_property = 1\n        a.int_property = -10\n        with self.assertRaises(TypeError):\n            a.int_property = \'1\'\n\n        class B(Configurable):\n            int_property = Property(expected_type=(str, set))\n\n        b = B()\n        b.int_property = {1, 2, 3}\n        b.int_property = \'hello\'\n        with self.assertRaises(TypeError):\n            b.int_property = [5, 4]\n\n    def test_required_properties(self):\n        class A(Configurable):\n            required_prop = Property(required=True)\n\n        A(required_prop=\'defined\')\n        with self.assertRaises(ValueError):\n            A()\n\n    def test_properties_with_specified_typse(self):\n        Case = namedtuple(""Case"", ""property_class valid invalid"")\n        test_cases = (\n            Case(property_class=IntProperty,\n                 valid=[1, 2, 0, -10, np.int32(23), np.int64(11), 1.0],\n                 invalid=[1.00001, 12.34, \'invalid\']),\n            Case(property_class=NumberProperty,\n                 valid=[1, 1.0, 12.34, -345],\n                 invalid=[\'invalid\', [4, 5]]),\n            Case(property_class=ArrayProperty,\n                 valid=[np.array([]), np.ones((5, 3)), np.matrix([1, 2])],\n                 invalid=[[1, 2, 3], \'invalid\']),\n        )\n\n        for test_case in test_cases:\n            class A(Configurable):\n                value = test_case.property_class()\n\n            a = A()\n            for valid_value in test_case.valid:\n                a.value = valid_value\n\n            for invalid_value in test_case.invalid:\n                with self.assertRaises(TypeError):\n                    a.value = invalid_value\n\n    def test_property_get_method(self):\n        prop = Property(default=3)\n        self.assertEqual(None, prop.__get__(None, None))\n\n    def test_property_repr(self):\n        prop = Property(default=3)\n        self.assertEqual(\'Property()\', repr(prop))\n\n    def test_property_repr_with_name(self):\n        prop = Property(default=3)\n        prop.name = \'test\'\n\n        self.assertEqual(\'Property(name=""test"")\', repr(prop))\n\n\nclass BoundedPropertiesTestCase(BaseTestCase):\n    def test_bounded_properties(self):\n        class A(Configurable):\n            bounded_property = BoundedProperty(minval=-1, maxval=1)\n\n        a = A()\n        a.bounded_property = 0\n        with self.assertRaises(ValueError):\n            a.bounded_property = -2\n\n    def test_propert_raciton_property(self):\n        class A(Configurable):\n            fraction = ProperFractionProperty()\n\n        a = A()\n\n        valid_values = [0, 0.5, 1 / 3., 1]\n        for valid_value in valid_values:\n            a.fraction = valid_value\n\n        invalid_values = [10, 1 + 1e-10, 0 - 1e-10, \'test\']\n        for invalid_value in invalid_values:\n            msg = ""Specified value: {}"".format(invalid_value)\n            with self.assertRaises((ValueError, TypeError), msg=msg):\n                a.fraction = invalid_value\n\n\nclass TypedListPropertiesTestCase(BaseTestCase):\n    def test_typed_list_properties(self):\n        class A(Configurable):\n            list_of_properties = TypedListProperty(element_type=str,\n                                                   n_elements=3)\n\n        a = A()\n        a.list_of_properties = (\'1\', \'2\', \'3\')\n\n        with self.assertRaises(TypeError):\n            # Invalid types\n            a.list_of_properties = (1, \'2\', \'3\')\n\n        with self.assertRaises(ValueError):\n            # Invalid number of elements\n            a.list_of_properties = (\'1\', \'2\', \'3\', \'4\')\n\n    def test_multiple_types(self):\n        class A(Configurable):\n            list_of_properties = TypedListProperty(element_type=(int, float),\n                                                   n_elements=3)\n\n        a = A(list_of_properties=[1, -2.0, 3.4])\n\n        with self.assertRaises(TypeError):\n            a.list_of_properties = (1, 1.23, \'x\')\n\n\nclass ChoicesPropertiesTestCase(BaseTestCase):\n    def test_choice_property_from_dict(self):\n        class A(Configurable):\n            choice = ChoiceProperty(choices={\'one\': 1, \'two\': 2, \'three\': 3})\n\n        a = A(choice=\'three\')\n        self.assertEqual(a.choice, 3)\n        a.choice = \'one\'\n        self.assertEqual(a.choice, 1)\n\n    def test_choice_property_invalid_values(self):\n        class A(Configurable):\n            choice = ChoiceProperty(choices={\'one\': 1, \'two\': 2, \'three\': 3})\n\n        a = A(choice=\'three\')\n        invalid_values = [2, None, ""2""]\n        for invalid_value in invalid_values:\n            with self.assertRaises(ValueError):\n                a.choice = invalid_value\n\n    def test_choice_property_from_list(self):\n        class A(Configurable):\n            choice = ChoiceProperty(choices=[\'one\', \'two\', \'three\'],\n                                    default=\'two\')\n\n        a = A()\n        self.assertEqual(a.choice, \'two\')\n        a.choice = \'three\'\n        self.assertEqual(a.choice, \'three\')\n\n        with self.assertRaises(ValueError):\n            a.choice = 1\n\n    def test_choice_property_exceptions(self):\n        with self.assertRaises(ValueError):\n            class A(Configurable):\n                choice = ChoiceProperty(choices=\'test\')\n\n        with self.assertRaises(ValueError):\n            class B(Configurable):\n                choice = ChoiceProperty(choices=[])\n\n    def test_choice_property_on_unknown_instance(self):\n        prop = ChoiceProperty(choices=[1, 2, 3])\n        self.assertEqual(None, prop.__get__(None, None))\n\n\nclass WithdrawPropertyTestCase(BaseTestCase):\n    def test_withdraw_property(self):\n        class A(Configurable):\n            prop = Property(default=3)\n\n        class B(A):\n            prop = WithdrawProperty()\n\n        a = A()\n        self.assertIn(\'prop\', a.options)\n        self.assertEqual(a.prop, 3)\n\n        b = B()\n        self.assertNotIn(\'prop\', b.options)\n'"
tests/core/test_shared_docs.py,0,"b'from neupy.core.docs import SharedDocs, shared_docs, parse_variables_from_docs\n\nfrom base import BaseTestCase\n\n\nclass SharedDocsTestCase(BaseTestCase):\n    def test_simple_case(self):\n        class A(SharedDocs):\n            """"""\n            Class A documentation.\n\n            Parameters\n            ----------\n            var1 : int\n                Var1 description.\n                Defaults to ``2``.\n            var2 : str\n                Var2 description.\n            test : complex or float\n            """"""\n\n        class B(A):\n            """"""\n            Class B documentation.\n\n            Parameters\n            ----------\n            {A.var1}\n            {A.var2}\n            {A.test}\n            """"""\n\n        self.assertIn(""Class B documentation"", B.__doc__)\n\n        self.assertIn(""var1 : int"", B.__doc__)\n        self.assertIn(""var2 : str"", B.__doc__)\n        self.assertIn(""test : complex or float"", B.__doc__)\n        self.assertIn(""Defaults to ``2``."", B.__doc__)\n\n    def test_shared_methods(self):\n        class A(SharedDocs):\n            """"""\n            Class A documentation.\n\n            Methods\n            -------\n            foo()\n                Foo description.\n                Even more\n            bar(params=True)\n            double_row(param1=True, param2=True,\\\n            param3=True)\n                Additional description for ``double_row``.\n\n            Examples\n            --------\n            one-two-three\n            """"""\n\n        class B(A):\n            """"""\n            Class B documentation.\n\n            Methods\n            -------\n            {A.foo}\n            {A.bar}\n            {A.double_row}\n            """"""\n\n        self.assertIn(""Class B documentation"", B.__doc__)\n\n        self.assertIn(""foo()"", B.__doc__)\n        self.assertIn(""Foo description."", B.__doc__)\n        self.assertIn(""Even more"", B.__doc__)\n        self.assertIn(""bar(params=True)"", B.__doc__)\n\n        # Check multi-row method\n        self.assertIn(""double_row(param1=True, param2=True,"", B.__doc__)\n        self.assertIn(""param3=True)"", B.__doc__)\n        self.assertIn(""Additional description for ``double_row``."", B.__doc__)\n\n    def test_complex_class_inheritance(self):\n        class A(SharedDocs):\n            """"""\n            Class A documentation.\n\n            Parameters\n            ----------\n            var_a : int\n            var_x : str\n            """"""\n\n        class B(SharedDocs):\n            """"""\n            Class B documentation\n\n            Parameters\n            ----------\n            var_b : int\n            var_x : float\n            """"""\n\n        class C(A, B):\n            """"""\n            Class C documentation.\n\n            Parameters\n            ----------\n            {A.var_a}\n            {B.var_b}\n            {A.var_x}\n            """"""\n\n        self.assertIn(""Class C documentation"", C.__doc__)\n\n        self.assertIn(""var_a : int"", C.__doc__)\n        self.assertIn(""var_b : int"", C.__doc__)\n        self.assertIn(""var_x : str"", C.__doc__)\n\n    def test_shared_docs_between_functions(self):\n        def function_a(x, y):\n            """"""\n            Function A documentation.\n\n            Parameters\n            ----------\n            x : int\n                First input varaible x.\n            y : int\n                Second input variable y.\n\n            Returns\n            -------\n            int\n                Output is equal to x + y.\n            """"""\n\n        @shared_docs(function_a)\n        def function_b(x, y):\n            """"""\n            Function B documentation.\n\n            Parameters\n            ----------\n            {function_a.x}\n            {function_a.y}\n\n            Returns\n            -------\n            int\n                Output is equal to x * y.\n            """"""\n\n        docs = function_b.__doc__\n        self.assertIn(""Function B documentation"", docs)\n\n        self.assertIn(""x : int"", docs)\n        self.assertIn(""First input varaible x."", docs)\n        self.assertIn(""y : int"", docs)\n        self.assertIn(""Second input variable y."", docs)\n\n    def test_inherit_method_docs(self):\n        class A(SharedDocs):\n            def method(self, x, y):\n                """"""\n                Class A method.\n\n                Parameters\n                ----------\n                x : int\n                    X variable.\n                y : int\n                    Y varaible.\n\n                Returns\n                -------\n                int\n                    x + y\n                """"""\n                return x + y\n\n            def method2(self):\n                pass\n\n        class B(A):\n            def method(self, x, y):\n                # It\'s suppose to inherit documentation\n                # from the A class\n                return x + y\n\n            def method2(self):\n                pass\n\n        self.assertEqual(B.method.__doc__, A.method.__doc__)\n        self.assertIsNone(B.method2.__doc__)\n\n        class C(A):\n            def method(self, x, y):\n                """"""\n                Updated documentation for the method.\n                """"""\n                return x + y\n\n        self.assertNotEqual(C.method.__doc__, A.method.__doc__)\n        self.assertIn(""Updated documentation for the method."",\n                      C.method.__doc__)\n\n    def test_disabled_inherit_method_docs(self):\n        class A(SharedDocs):\n            def method(self, x, y):\n                """"""\n                Method 1.\n                """"""\n\n        class B(A):\n            inherit_method_docs = False\n\n            def method(self, x, y):\n                pass\n\n        self.assertIsNone(B.method.__doc__)\n\n        class C(A):\n            inherit_method_docs = True\n\n            def method(self, x, y):\n                pass\n\n        self.assertIsNotNone(C.method.__doc__)\n\n    def test_args_and_kwargs_parameters(self):\n        class A(SharedDocs):\n            """"""\n            Class A\n\n            Parameters\n            ----------\n            *args\n                Arguments.\n            **kwargs\n                Keyword Arguments.\n            """"""\n\n        class B(A):\n            """"""\n            Class B\n\n            Parameters\n            ----------\n            {A.args}\n            {A.kwargs}\n            """"""\n\n        docs = B.__doc__\n        self.assertIn(""Class B"", docs)\n\n        self.assertIn(""*args"", docs)\n        self.assertNotIn(""*args :"", docs)\n        self.assertIn(""Arguments."", docs)\n\n        self.assertIn(""**kwargs"", docs)\n        self.assertNotIn(""**kwargs :"", docs)\n        self.assertIn(""Keyword Arguments."", docs)\n\n\nclass SharedDocsParseSectionsTestCase(BaseTestCase):\n    def test_shared_warns_section(self):\n        class A(SharedDocs):\n            """"""\n            Class A documentation.\n\n            Warns\n            -----\n            Important warning.\n            Additional information related to warning message.\n\n            Examples\n            --------\n            Some section before `Warns`\n            """"""\n\n        class B(A):\n            """"""\n            Class B documentation.\n\n            Warns\n            -----\n            {A.Warns}\n            """"""\n\n        docs = B.__doc__\n        self.assertIn(""Class B documentation"", docs)\n\n        self.assertIn(""Important warning"", docs)\n        self.assertIn(""related to warning message"", docs)\n\n        self.assertNotIn(""Some section before"", docs)\n        self.assertNotIn(""Examples"", docs)\n\n    def test_shared_returns_section(self):\n        class A(SharedDocs):\n            """"""\n            A class\n\n            Returns\n            -------\n            int\n                Add two number together\n            """"""\n\n        class B(A):\n            """"""\n            B class\n\n            Returns\n            -------\n            {A.Returns}\n            """"""\n\n            expected_doc = """"""\n            B class\n\n            Returns\n            -------\n            int\n                Add two number together\n            """"""\n\n        self.assertEqual(B.__doc__, B.expected_doc)\n\n    def test_shared_yields_section(self):\n        def foo(x, y):\n            """"""\n            foo function\n\n            Yields\n            ------\n            int\n                Integer values.\n            """"""\n\n        @shared_docs(foo)\n        def bar(x, y):\n            """"""\n            bar function\n\n            Yields\n            ------\n            {foo.Yields}\n            """"""\n\n        def expected(x, y):\n            """"""\n            bar function\n\n            Yields\n            ------\n            int\n                Integer values.\n            """"""\n\n        self.assertEqual(bar.__doc__, expected.__doc__)\n\n    def test_shared_raises_section(self):\n        def foo(x, y):\n            """"""\n            foo function\n\n            Raises\n            ------\n            ValueError\n                Just raise it all the time\n            """"""\n\n        @shared_docs(foo)\n        def bar(x, y):\n            """"""\n            bar function\n\n            Raises\n            ------\n            {foo.Raises}\n            """"""\n\n        def expected(x, y):\n            """"""\n            bar function\n\n            Raises\n            ------\n            ValueError\n                Just raise it all the time\n            """"""\n\n        self.assertEqual(bar.__doc__, expected.__doc__)\n\n    def test_shared_see_also(self):\n        def foo(x, y):\n            """"""\n            foo function\n\n            See Also\n            --------\n            foo1, foo2, foo3\n            foo4\n            """"""\n\n        @shared_docs(foo)\n        def bar(x, y):\n            """"""\n            bar function\n\n            See Also\n            --------\n            {foo.See Also}\n            """"""\n\n        def expected(x, y):\n            """"""\n            bar function\n\n            See Also\n            --------\n            foo1, foo2, foo3\n            foo4\n            """"""\n\n        self.assertEqual(bar.__doc__, expected.__doc__)\n\n    def test_share_all_parameters_section(self):\n        class A(SharedDocs):\n            """"""\n            Class A documentation.\n\n            Parameters\n            ----------\n            var_a : int\n                Variable a\n\n                * List element 1.\n\n                * List element 2.\n            var_x : str\n                Variable x string\n            """"""\n\n        class B(A):\n            """"""\n            Class B documentation\n\n            Parameters\n            ----------\n            {A.var_a}\n            var_b : int\n                Variable b\n            var_x : float\n                Variable x float\n            """"""\n\n        class C(B):\n            """"""\n            Class C documentation.\n\n            Parameters\n            ----------\n            {B.Parameters}\n            """"""\n\n        class ExpectedDoc(object):\n            """"""\n            Class C documentation.\n\n            Parameters\n            ----------\n            var_a : int\n                Variable a\n\n                * List element 1.\n\n                * List element 2.\n            var_b : int\n                Variable b\n            var_x : float\n                Variable x float\n            """"""\n\n        self.assertEqual(C.__doc__, ExpectedDoc.__doc__)\n\n    def test_shared_attributes_section(self):\n        class A(SharedDocs):\n            """"""\n            Class A.\n\n            Attributes\n            ----------\n            x : int\n            y : str\n                Short description.\n\n            Examples\n            --------\n            >>> a = A()\n            >>> a.x\n            0\n            """"""\n\n        class B(A):\n            """"""\n            Class B.\n\n            Attributes\n            ----------\n            {A.Attributes}\n\n            Examples\n            --------\n            >>> b = B()\n            >>> b.x\n            1\n            """"""\n\n        class ExpectedDoc(object):\n            """"""\n            Class B.\n\n            Attributes\n            ----------\n            x : int\n            y : str\n                Short description.\n\n            Examples\n            --------\n            >>> b = B()\n            >>> b.x\n            1\n            """"""\n\n        self.assertEqual(B.__doc__, ExpectedDoc.__doc__)\n\n    def test_shared_methods_section(self):\n        class A(SharedDocs):\n            """"""\n            Class A.\n\n            Methods\n            -------\n            add(x, y)\n                Returns x + y\n            mul(x, y)\n                Returns x * y\n\n            Warns\n            -----\n            Some warnings\n            """"""\n\n        class B(A):\n            """"""\n            Class B.\n\n            Methods\n            -------\n            {A.Methods}\n            """"""\n\n        class ExpectedDoc(object):\n            """"""\n            Class B.\n\n            Methods\n            -------\n            add(x, y)\n                Returns x + y\n            mul(x, y)\n                Returns x * y\n            """"""\n\n        self.assertEqual(B.__doc__, ExpectedDoc.__doc__)\n\n    def test_parse_variables_from_docs(self):\n        self.assertEqual({}, parse_variables_from_docs([]))\n\n        class A(object):\n            pass\n\n        self.assertEqual({}, parse_variables_from_docs([A]))\n'"
tests/datasets/__init__.py,0,b''
tests/datasets/test_digits.py,0,"b'import numpy as np\n\nfrom neupy import datasets\n\nfrom base import BaseTestCase\n\n\nclass DiscreteDigitsDatasetTestCase(BaseTestCase):\n    def test_load_digits(self):\n        data, labels = datasets.load_digits()\n\n        self.assertEqual(data.shape, (10, 24))\n        self.assertEqual(labels.shape, (10,))\n\n    def test_make_digits_exceptions(self):\n        with self.assertRaisesRegexp(ValueError, ""from \\[0\\, 1\\) range""):\n            datasets.make_digits(noise_level=-1)\n\n        with self.assertRaisesRegexp(ValueError, ""from \\[0\\, 1\\) range""):\n            datasets.make_digits(noise_level=1)\n\n        with self.assertRaisesRegexp(ValueError, ""greater or equal to 1""):\n            datasets.make_digits(n_samples=0)\n\n        with self.assertRaisesRegexp(ValueError, ""Unknown mode""):\n            datasets.make_digits(n_samples=0, mode=\'unknown\')\n\n    def test_make_digits_remove_mode(self):\n        data, labels = datasets.load_digits()\n        noisy_data, noisy_labels = datasets.make_digits(\n            noise_level=0.3, n_samples=100, mode=\'remove\')\n\n        diff = data[noisy_labels] - noisy_data\n        diff_signs = np.sign(diff).flatten()\n\n        self.assertNotEqual(np.abs(diff).sum(), 0)\n        self.assertEqual({0, 1}, set(diff_signs))\n\n    def test_make_digits_flip_mode(self):\n        data, labels = datasets.load_digits()\n        noisy_data, noisy_labels = datasets.make_digits(\n            noise_level=0.5, n_samples=100, mode=\'flip\')\n\n        diff_signs = np.sign(data[noisy_labels] - noisy_data).flatten()\n        self.assertEqual({-1, 0, 1}, set(diff_signs))\n'"
tests/datasets/test_reber.py,0,"b'import math\n\nimport numpy as np\n\nfrom neupy.datasets.reber import avaliable_letters\nfrom neupy.datasets import (make_reber, is_valid_by_reber,\n                            make_reber_classification)\n\nfrom base import BaseTestCase\n\n\nclass ReberTestCase(BaseTestCase):\n    def test_reber_word_generation(self):\n        words = make_reber(50)\n        self.assertEqual(50, len(words))\n\n        for word in words:\n            self.assertTrue(is_valid_by_reber(word))\n\n    def test_reber_expcetions(self):\n        with self.assertRaises(ValueError):\n            make_reber(n_words=0)\n\n        with self.assertRaises(ValueError):\n            make_reber(n_words=-1)\n\n    def test_reber_classification_data(self):\n        invalid_data_ratio = 0.5\n        n_words = 100\n\n        words, labels = make_reber_classification(\n            n_words, invalid_size=invalid_data_ratio\n        )\n\n        self.assertEqual(n_words, len(labels))\n        self.assertEqual(math.ceil(n_words * invalid_data_ratio), sum(labels))\n\n        for word, label in zip(words, labels):\n            self.assertEqual(bool(label), is_valid_by_reber(word))\n\n    def test_reber_classification_exceptions(self):\n        with self.assertRaisesRegexp(ValueError, ""at least 2 samples""):\n            make_reber_classification(n_samples=1)\n\n        with self.assertRaises(ValueError):\n            make_reber_classification(n_samples=10, invalid_size=-1)\n\n        with self.assertRaises(ValueError):\n            make_reber_classification(n_samples=10, invalid_size=2)\n\n    def test_return_indices_for_reber_classification(self):\n        words, _ = make_reber_classification(100, return_indices=True)\n\n        min_index = np.min([np.min(word) for word in words])\n        max_index = np.max([np.max(word) for word in words])\n\n        self.assertEqual(min_index, 0)\n        self.assertEqual(max_index, len(avaliable_letters) - 1)\n'"
tests/layers/__init__.py,0,b''
tests/layers/test_activations.py,1,"b'import math\n\nimport six\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import (\n    LayerConnectionError,\n    WeightInitializationError,\n)\nfrom neupy import layers, algorithms, init\n\nfrom base import BaseTestCase\nfrom helpers import simple_classification\n\n\nclass ActivationLayerMainTestCase(BaseTestCase):\n    def test_linear_layer_withuot_bias(self):\n        input_layer = layers.Input(10)\n        output_layer = layers.Linear(2, weight=0.1, bias=None)\n        network = layers.join(input_layer, output_layer)\n\n        input_value = asfloat(np.ones((1, 10)))\n        actual_output = self.eval(network.output(input_value))\n        expected_output = np.ones((1, 2))\n\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n    def test_exception(self):\n        with self.assertRaises(TypeError):\n            layers.Linear(2, weight=None)\n\n    def test_repr_without_size(self):\n        layer = layers.Sigmoid()\n        self.assertEqual(""Sigmoid(name=\'sigmoid-1\')"", str(layer))\n\n    def test_repr_with_size(self):\n        layer1 = layers.Sigmoid(13)\n        self.assertEqual(\n            str(layer1),\n            (\n                ""Sigmoid(13, weight=HeNormal(gain=1.0), ""\n                ""bias=Constant(0), name=\'sigmoid-1\')""\n            )\n        )\n\n    def test_variables(self):\n        network = layers.join(\n            layers.Input(2),\n            layers.Sigmoid(3, name=\'sigmoid\'),\n        )\n        self.assertDictEqual(network.layer(\'sigmoid\').variables, {})\n\n        network.outputs\n        variables = network.layer(\'sigmoid\').variables\n        self.assertSequenceEqual(\n            sorted(variables.keys()),\n            [\'bias\', \'weight\'])\n\n        self.assertShapesEqual(variables[\'bias\'].shape, (3,))\n        self.assertShapesEqual(variables[\'weight\'].shape, (2, 3))\n\n    def test_failed_propagation_for_multiple_inputs(self):\n        inputs = layers.parallel(\n            layers.Input(1),\n            layers.Input(2),\n        )\n        if six.PY3:\n            expected_message = ""2 positional arguments but 3 were given.""\n        else:\n            expected_message = (\n                ""get_output_shape\\(\\) takes exactly 2 arguments \\(3 given\\)""\n            )\n        with self.assertRaisesRegexp(TypeError, expected_message):\n            layers.join(inputs, layers.Relu(3, name=\'relu\'))\n\n    def test_fail_rejoining_to_new_input(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5, name=\'relu\'),\n        )\n        network.create_variables()\n\n        error_message = ""Cannot connect layer `in` to layer `relu`""\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(layers.Input(7, name=\'in\'), network.layer(\'relu\'))\n\n    def test_invalid_input_shape(self):\n        error_message = (\n            ""Input shape expected to have 2 ""\n            ""dimensions, got 3 instead. Shape: \\(\\?, 10, 3\\)""\n        )\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(\n                layers.Input((10, 3)),\n                layers.Linear(10),\n            )\n\n    def test_unknwown_feature_during_weight_init(self):\n        network = layers.join(\n            layers.Input(None),\n            layers.Linear(10, name=\'linear\'),\n        )\n\n        message = (\n            ""Cannot create variables for the layer `linear`, ""\n            ""because number of input features is unknown. ""\n            ""Input shape: \\(\\?, \\?\\)""\n        )\n        with self.assertRaisesRegexp(WeightInitializationError, message):\n            network.create_variables()\n\n        with self.assertRaisesRegexp(WeightInitializationError, message):\n            network.outputs\n\n    def test_invalid_weight_shape(self):\n        network = layers.join(\n            layers.Input(5),\n            layers.Linear(4, weight=np.ones((3, 3))),\n        )\n        with self.assertRaisesRegexp(ValueError, ""Cannot create variable""):\n            network.create_variables()\n\n        variable = tf.Variable(np.ones((3, 3)), dtype=tf.float32)\n        network = layers.join(\n            layers.Input(5),\n            layers.Linear(4, weight=variable),\n        )\n        with self.assertRaisesRegexp(ValueError, ""Cannot create variable""):\n            network.create_variables()\n\n\nclass ActivationLayersTestCase(BaseTestCase):\n    def test_activation_layers_without_size(self):\n        X = np.array([1, 2, -1, 10])\n        expected_output = np.array([1, 2, 0, 10])\n\n        layer = layers.Relu()\n\n        actual_output = self.eval(layer.output(X))\n        np.testing.assert_array_equal(actual_output, expected_output)\n\n    def test_hard_sigmoid_layer(self):\n        layer = layers.HardSigmoid(6)\n\n        input_value = asfloat(np.array([[-3, -2, -1, 0, 1, 2]]))\n        expected = np.array([[0, 0.1, 0.3, 0.5, 0.7, 0.9]])\n\n        output = self.eval(layer.activation_function(input_value))\n        np.testing.assert_array_almost_equal(output, expected)\n\n    def test_linear_layer(self):\n        layer = layers.Linear(1)\n        self.assertEqual(layer.activation_function(1), 1)\n\n    def test_tanh_layer(self):\n        layer1 = layers.Tanh(1)\n        self.assertGreater(1, self.eval(layer1.activation_function(1.)))\n\n    def test_leaky_relu(self):\n        X = asfloat(np.array([[10, 1, 0.1, 0, -0.1, -1]]).T)\n        expected_output = asfloat(np.array([[10, 1, 0.1, 0, -0.001, -0.01]]).T)\n        layer = layers.LeakyRelu(1)\n\n        actual_output = self.eval(layer.activation_function(X))\n        np.testing.assert_array_almost_equal(\n            expected_output, actual_output)\n\n    def test_softplus_layer(self):\n        layer = layers.Softplus(1)\n        self.assertAlmostEqual(\n            math.log(2),\n            self.eval(layer.activation_function(0.)))\n\n    def test_elu_layer(self):\n        test_input = asfloat(np.array([[10, 1, 0.1, 0, -1]]).T)\n        expected_output = np.array([\n            [10, 1, 0.1, 0, -0.6321205588285577]]).T\n\n        layer = layers.Elu()\n        actual_output = self.eval(layer.activation_function(test_input))\n\n        np.testing.assert_array_almost_equal(\n            expected_output, actual_output)\n\n\nclass SigmoidTestCase(BaseTestCase):\n    def test_sigmoid_layer(self):\n        layer1 = layers.Sigmoid(1)\n        self.assertGreater(1, self.eval(layer1.activation_function(1.)))\n\n    def test_sigmoid_semantic_segmentation(self):\n        network = layers.join(\n            layers.Input((10, 10, 1)),\n            layers.Sigmoid(),\n        )\n\n        input = 10 * np.random.random((2, 10, 10, 1)) - 5\n        actual_output = self.eval(network.output(input))\n\n        self.assertTrue(np.all(actual_output >= 0))\n        self.assertTrue(np.all(actual_output <= 1))\n\n\nclass SoftmaxTestCase(BaseTestCase):\n    def test_softmax_layer(self):\n        test_input = asfloat(np.array([[0.5, 0.5, 0.1]]))\n        softmax_layer = layers.Softmax(3)\n        correct_result = np.array([[0.37448695, 0.37448695, 0.25102611]])\n        np.testing.assert_array_almost_equal(\n            correct_result,\n            self.eval(softmax_layer.activation_function(test_input)))\n\n    def test_softmax_semantic_segmentation(self):\n        network = layers.join(\n            layers.Input((10, 10, 6)),\n            layers.Softmax(),\n        )\n\n        input = np.random.random((2, 10, 10, 6))\n        actual_output = self.eval(network.output(input))\n\n        np.testing.assert_array_almost_equal(\n            actual_output.sum(axis=-1),\n            np.ones((2, 10, 10)))\n\n\nclass ReluTestCase(BaseTestCase):\n    def test_relu_activation(self):\n        layer = layers.Relu()\n        self.assertEqual(0, self.eval(layer.activation_function(-10)))\n        self.assertEqual(0, self.eval(layer.activation_function(0)))\n        self.assertEqual(10, self.eval(layer.activation_function(10)))\n\n        layer = layers.Relu(alpha=0.1)\n        self.assertAlmostEqual(-1, self.eval(layer.activation_function(-10)))\n        self.assertAlmostEqual(-0.2, self.eval(layer.activation_function(-2)))\n\n    def test_relu(self):\n        # Test alpha parameter\n        X = asfloat(np.array([[10, 1, 0.1, 0, -0.1, -1]]).T)\n        expected_output = asfloat(np.array([[10, 1, 0.1, 0, -0.01, -0.1]]).T)\n        layer = layers.Relu(1, alpha=0.1)\n\n        actual_output = self.eval(layer.activation_function(X))\n        np.testing.assert_array_almost_equal(\n            expected_output, actual_output)\n\n    def test_repr_without_size(self):\n        self.assertEqual(""Relu(alpha=0, name=\'relu-1\')"", str(layers.Relu()))\n\n    def test_repr_with_size(self):\n        self.assertEqual(\n            str(layers.Relu(10)),\n            (\n                ""Relu(10, alpha=0, weight=HeNormal(gain=2), ""\n                ""bias=Constant(0), name=\'relu-1\')""\n            )\n        )\n\n\nclass PReluTestCase(BaseTestCase):\n    def test_invalid_alpha_axes_parameter(self):\n        network = layers.join(\n            layers.PRelu(10, alpha_axes=2),\n            layers.Relu(),\n        )\n        with self.assertRaises(LayerConnectionError):\n            # cannot specify 2-axis, because we only\n            # have 0 and 1 axes (2D input)\n            layers.join(layers.Input(10), network)\n\n        with self.assertRaises(ValueError):\n            # 0-axis is not allowed\n            layers.PRelu(10, alpha_axes=0)\n\n    def test_prelu_alpha_init_random_params(self):\n        prelu_layer = layers.PRelu(10, alpha=init.XavierNormal())\n        prelu_layer.create_variables((None, 5))\n\n        alpha = self.eval(prelu_layer.alpha)\n        self.assertEqual(10, np.unique(alpha).size)\n\n    def test_prelu_alpha_init_constant_value(self):\n        prelu_layer = layers.PRelu(10, alpha=0.25)\n        prelu_layer.create_variables((None, 5))\n\n        alpha = self.eval(prelu_layer.alpha)\n        self.assertEqual(alpha.shape, (10,))\n        np.testing.assert_array_almost_equal(alpha, np.ones(10) * 0.25)\n\n    def test_prelu_layer_param_conv(self):\n        network = layers.join(\n            layers.Input((10, 10, 3)),\n            layers.Convolution((3, 3, 5)),\n            layers.PRelu(alpha=0.25, alpha_axes=(1, 3), name=\'prelu\'),\n        )\n        network.create_variables()\n\n        alpha = self.eval(network.layer(\'prelu\').alpha)\n        expected_alpha = np.ones((8, 5)) * 0.25\n\n        self.assertEqual(alpha.shape, (8, 5))\n        np.testing.assert_array_almost_equal(alpha, expected_alpha)\n\n    def test_prelu_output_by_dense_input(self):\n        prelu_layer = layers.PRelu(alpha=0.25)\n        prelu_layer.create_variables((None, 1))\n\n        X = np.array([[10, 1, 0.1, 0, -0.1, -1]]).T\n        expected_output = np.array([[10, 1, 0.1, 0, -0.025, -0.25]]).T\n        actual_output = self.eval(prelu_layer.activation_function(X))\n\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n    def test_prelu_output_by_spatial_input(self):\n        network = layers.join(\n            layers.Input((10, 10, 3)),\n            layers.Convolution((3, 3, 5)),\n            layers.PRelu(alpha=0.25, alpha_axes=(1, 3)),\n        )\n\n        X = asfloat(np.random.random((1, 10, 10, 3)))\n        actual_output = self.eval(network.output(X))\n        self.assertEqual(actual_output.shape, (1, 8, 8, 5))\n\n    def test_prelu_param_updates(self):\n        x_train, _, y_train, _ = simple_classification()\n        prelu_layer1 = layers.PRelu(20, alpha=0.25)\n        prelu_layer2 = layers.PRelu(1, alpha=0.25)\n\n        gdnet = algorithms.GradientDescent(\n            [\n                layers.Input(10),\n                prelu_layer1,\n                prelu_layer2,\n            ],\n            batch_size=None,\n        )\n\n        prelu1_alpha_before_training = self.eval(prelu_layer1.alpha)\n        prelu2_alpha_before_training = self.eval(prelu_layer2.alpha)\n\n        gdnet.train(x_train, y_train, epochs=10)\n\n        prelu1_alpha_after_training = self.eval(prelu_layer1.alpha)\n        prelu2_alpha_after_training = self.eval(prelu_layer2.alpha)\n\n        self.assertTrue(all(np.not_equal(\n            prelu1_alpha_before_training,\n            prelu1_alpha_after_training,\n        )))\n        self.assertTrue(all(np.not_equal(\n            prelu2_alpha_before_training,\n            prelu2_alpha_after_training,\n        )))\n\n    def test_repr_without_size(self):\n        self.assertEqual(\n            ""PRelu(alpha_axes=(-1,), alpha=Constant(0.25), name=\'p-relu-1\')"",\n            str(layers.PRelu()))\n\n    def test_repr_with_size(self):\n        self.assertEqual(\n            str(layers.PRelu(10)),\n            (\n                ""PRelu(10, alpha_axes=(-1,), alpha=Constant(0.25), ""\n                ""weight=HeNormal(gain=2), bias=Constant(0), ""\n                ""name=\'p-relu-1\')""\n            )\n        )\n\n    def test_prelu_variables(self):\n        network = layers.join(\n            layers.Input(2),\n            layers.PRelu(3, name=\'prelu\'),\n        )\n        self.assertDictEqual(network.layer(\'prelu\').variables, {})\n\n        network.create_variables()\n        variables = network.layer(\'prelu\').variables\n        self.assertSequenceEqual(\n            sorted(variables.keys()),\n            [\'alpha\', \'bias\', \'weight\'])\n\n        self.assertShapesEqual(variables[\'bias\'].shape, (3,))\n        self.assertShapesEqual(variables[\'weight\'].shape, (2, 3))\n        self.assertShapesEqual(variables[\'alpha\'].shape, (3,))\n'"
tests/layers/test_basic.py,2,"b'import copy\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy import layers, algorithms, init\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import LayerConnectionError\n\nfrom base import BaseTestCase\n\n\nclass LayersBasicsTestCase(BaseTestCase):\n    def test_shared_parameters_between_layers(self):\n        weight = tf.Variable(\n            np.random.random((10, 10)),\n            name=\'weight\',\n            dtype=tf.float32)\n\n        inp = layers.Input(10)\n        hid1 = layers.Relu(10, weight=weight)\n        hid2 = layers.Relu(10, weight=weight)\n\n        network = inp >> hid1 >> hid2\n        self.assertIs(hid1.weight, hid2.weight)\n\n        # Check that it is able to train network without errors\n        x_train = y_train = asfloat(np.random.random((15, 10)))\n        gdnet = algorithms.GradientDescent(network)\n        gdnet.train(x_train, y_train, epochs=5)\n\n        np.testing.assert_array_almost_equal(\n            self.eval(hid1.weight),\n            self.eval(hid2.weight),\n        )\n\n    def test_predict_new_model(self):\n        input_value = np.random.random((7, 4))\n\n        network = layers.Input(4) >> layers.Relu(5)\n        output_value = network.predict(input_value)\n\n        self.assertEqual(output_value.shape, (7, 5))\n\n    def test_layer_definitions(self):\n        Conv = layers.Convolution.define(\n            padding=\'SAME\',\n            weight=init.Constant(1),\n            bias=None,\n        )\n        network = layers.join(\n            layers.Input((28, 28, 1)),\n            Conv((3, 3, 16)),\n            Conv((3, 3, 32)),\n        )\n        network.create_variables()\n\n        self.assertShapesEqual(network.output_shape, (None, 28, 28, 32))\n\n        weight_1 = self.eval(network.layers[1].weight)\n        self.assertEqual(weight_1.sum(), 1 * 3 * 3 * 16)\n        self.assertIsNone(network.layers[1].bias)\n\n        weight_2 = self.eval(network.layers[2].weight)\n        self.assertEqual(weight_2.sum(), 16 * 3 * 3 * 32)\n        self.assertIsNone(network.layers[2].bias)\n\n\nclass LayerNameTestCase(BaseTestCase):\n    def test_layer_defined_name(self):\n        input_layer = layers.Input(10, name=\'input\')\n        self.assertEqual(input_layer.name, \'input\')\n\n    def test_layer_default_name(self):\n        input_layer = layers.Input(10)\n        output_layer = layers.Sigmoid(1)\n\n        self.assertEqual(output_layer.name, \'sigmoid-1\')\n        self.assertEqual(input_layer.name, \'input-1\')\n\n    def test_layer_name_for_network(self):\n        input_layer = layers.Input(1)\n        hidden_layer = layers.Sigmoid(5)\n        output_layer = layers.Sigmoid(10)\n\n        network = input_layer >> hidden_layer >> output_layer\n        network.outputs\n\n        self.assertEqual(hidden_layer.name, \'sigmoid-1\')\n        self.assertIn(\'layer/sigmoid-1/weight\', hidden_layer.weight.name)\n        self.assertIn(\'layer/sigmoid-1/bias\', hidden_layer.bias.name)\n\n        self.assertEqual(output_layer.name, \'sigmoid-2\')\n        self.assertIn(\'layer/sigmoid-2/weight\', output_layer.weight.name)\n        self.assertIn(\'layer/sigmoid-2/bias\', output_layer.bias.name)\n\n    def test_layer_name_with_repeated_layer_type(self):\n        input_layer = layers.Input(1)\n        hidden1_layer = layers.Relu(2)\n        hidden2_layer = layers.Relu(3)\n        output_layer = layers.Relu(4)\n\n        self.assertEqual(input_layer.name, \'input-1\')\n        self.assertEqual(hidden1_layer.name, \'relu-1\')\n        self.assertEqual(hidden2_layer.name, \'relu-2\')\n        self.assertEqual(output_layer.name, \'relu-3\')\n\n    def test_layer_name_with_capital_letters(self):\n        class ABCD(layers.BaseLayer):\n            def output(self, input):\n                return input\n\n        layer = ABCD()\n        self.assertEqual(layer.name, \'abcd-1\')\n\n    def test_new_layer_init_exception(self):\n        with self.assertRaisesRegexp(TypeError, ""abstract methods output""):\n            class NewLayer(layers.BaseLayer):\n                pass\n\n            NewLayer()\n\n    def test_layer_name_with_first_few_capital_letters(self):\n        class ABCDef(layers.BaseLayer):\n            def output(self, input):\n                return input\n\n        layer_1 = ABCDef()\n        self.assertEqual(layer_1.name, \'abc-def-1\')\n\n        class abcDEF(layers.BaseLayer):\n            def output(self, input):\n                return input\n\n        layer_2 = abcDEF()\n        self.assertEqual(layer_2.name, \'abcdef-1\')\n\n        class abcDef(layers.BaseLayer):\n            def output(self, input):\n                return input\n\n        layer_3 = abcDef()\n        self.assertEqual(layer_3.name, \'abc-def-2\')\n\n    def test_layer_name_using_pattern(self):\n        layer1 = layers.Relu(name=\'rl{}\')\n        self.assertEqual(layer1.name, \'rl1\')\n\n        layer2 = layers.Relu(name=\'rl{}\')\n        self.assertEqual(layer2.name, \'rl2\')\n\n        layer3 = layers.Relu(name=\'relu-{}a\')\n        self.assertEqual(layer3.name, \'relu-1a\')\n\n        layer4 = layers.Relu(name=\'rl{}\')\n        self.assertEqual(layer4.name, \'rl3\')\n\n    def test_layer_name_using_complex_pattern(self):\n        layer = layers.Relu(name=\'rl{:>04d}\')\n        self.assertEqual(layer.name, \'rl0001\')\n\n    def test_layer_name_using_invalid_pattern(self):\n        error_message = ""Provided pattern has more than one field specified""\n\n        with self.assertRaisesRegexp(ValueError, error_message):\n            layers.Relu(name=\'relu-{}-{}\')\n\n\nclass LayerCopyTestCase(BaseTestCase):\n    def test_layer_copy(self):\n        relu = layers.Relu(10, weight=init.Normal(), bias=None)\n        copied_relu = copy.copy(relu)\n\n        self.assertEqual(relu.name, \'relu-1\')\n        self.assertEqual(copied_relu.name, \'relu-2\')\n\n        self.assertIsInstance(relu.weight, init.Normal)\n        self.assertIsNone(relu.bias)\n\n    def test_initialized_layer_copy(self):\n        network = layers.Input(10) >> layers.Relu(5)\n        network.create_variables()\n\n        relu = network.layers[1]\n        copied_relu = copy.copy(relu)\n\n        self.assertEqual(relu.name, \'relu-1\')\n        self.assertEqual(copied_relu.name, \'relu-2\')\n        self.assertIsInstance(copied_relu.weight, np.ndarray)\n\n        error_message = (\n            ""Cannot connect layer `relu-1` to layer `relu-2`, because output ""\n            ""shape \\(\\(\\?, 5\\)\\) of the first layer is incompatible with the ""\n            ""input shape \\(\\(\\?, 10\\)\\) of the second layer.""\n        )\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            # copied relu expects 10 input features, but network outputs 5\n            layers.join(network, copied_relu)\n\n    def test_layer_deep_copy(self):\n        relu = layers.Relu(10, weight=np.zeros((5, 10)))\n\n        copied_relu = copy.copy(relu)\n        self.assertIs(relu.weight, copied_relu.weight)\n\n        deepcopied_relu = copy.deepcopy(relu)\n        self.assertIsNot(relu.weight, deepcopied_relu.weight)\n\n    def test_copy_layer_name_with_pattern_name(self):\n        relu = layers.Relu(10, name=\'rl{}\')\n        self.assertEqual(relu.name, \'rl1\')\n\n        copied_relu = copy.copy(relu)\n        self.assertEqual(copied_relu.name, \'rl2\')\n'"
tests/layers/test_convolution.py,1,"b'import random\nfrom itertools import product\nfrom collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy import layers\nfrom neupy.utils import asfloat, shape_to_tuple\nfrom neupy.layers.convolutions import conv_output_shape, deconv_output_shape\nfrom neupy.exceptions import LayerConnectionError\n\nfrom base import BaseTestCase\n\n\nclass ConvLayersTestCase(BaseTestCase):\n    def get_shape(self, value):\n        shape = self.eval(tf.shape(value))\n        return tuple(shape)\n\n    def test_convolution_params(self):\n        inp = layers.Input((5, 5, 1))\n        conv = layers.Convolution((2, 2, 6))\n\n        # Propagate data through the network in\n        # order to trigger initialization\n        (inp >> conv).outputs\n\n        self.assertEqual((2, 2, 1, 6), self.get_shape(conv.weight))\n        self.assertEqual((6,), self.get_shape(conv.bias))\n\n    def test_conv_shapes(self):\n        paddings = [\'valid\', \'same\']\n        strides = [(1, 1), (2, 1), (2, 2)]\n        x = asfloat(np.random.random((20, 12, 11, 2)))\n\n        for stride, padding in product(strides, paddings):\n            network = layers.join(\n                layers.Input((12, 11, 2)),\n                layers.Convolution((3, 4, 5), padding=padding, stride=stride),\n            )\n            y = self.eval(network.output(x))\n\n            self.assertShapesEqual(\n                y.shape[1:],\n                network.output_shape[1:],\n                msg=\'padding={} and stride={}\'.format(padding, stride),\n            )\n\n    def test_valid_strides(self):\n        Case = namedtuple(""Case"", ""stride expected_output"")\n        testcases = (\n            Case(stride=(4, 4), expected_output=(4, 4)),\n            Case(stride=(4,), expected_output=(4, 1)),\n            Case(stride=4, expected_output=(4, 4)),\n        )\n\n        for testcase in testcases:\n            conv = layers.Convolution(\n                (2, 3, 1), stride=testcase.stride)\n\n            msg = ""Input stride size: {}"".format(testcase.stride)\n            self.assertEqual(\n                testcase.expected_output, conv.stride, msg=msg)\n\n    def test_conv_invalid_strides(self):\n        invalid_strides = (\n            (4, 4, 4),\n            -10,\n            (-5, -5),\n            (-5, 5),\n            (-5, 0),\n        )\n\n        for stride in invalid_strides:\n            msg = ""Input stride size: {}"".format(stride)\n            with self.assertRaises(ValueError, msg=msg):\n                layers.Convolution((2, 3, 1), stride=stride)\n\n    def test_valid_padding(self):\n        valid_paddings = (\'VALID\', \'SAME\', \'same\', \'valid\', 10, 1, (7, 1))\n        for padding in valid_paddings:\n            layers.Convolution((2, 3, 1), padding=padding)\n\n    def test_invalid_padding(self):\n        invalid_paddings = (\'invalid mode\', -10, (10, -5))\n\n        for padding in invalid_paddings:\n            msg = ""Padding: {}"".format(padding)\n\n            with self.assertRaises(ValueError, msg=msg):\n                layers.Convolution((2, 3, 1), padding=padding)\n\n    def test_conv_output_shape_func_exceptions(self):\n        with self.assertRaises(ValueError):\n            # Wrong stride value\n            conv_output_shape(\n                dimension_size=5, filter_size=5,\n                padding=\'VALID\', stride=\'not int\')\n\n        with self.assertRaises(ValueError):\n            # Wrong filter size value\n            conv_output_shape(\n                dimension_size=5, filter_size=\'not int\',\n                padding=\'SAME\', stride=5)\n\n        with self.assertRaisesRegexp(ValueError, ""unknown \\S+ padding value""):\n            # Wrong padding value\n            conv_output_shape(\n                dimension_size=5, filter_size=5,\n                padding=1.5, stride=5,\n            )\n\n    def test_conv_output_shape_int_padding(self):\n        output_shape = conv_output_shape(\n            dimension_size=10,\n            padding=3,\n            filter_size=5,\n            stride=5,\n        )\n        self.assertEqual(output_shape, 3)\n\n    def test_conv_unknown_dim_size(self):\n        shape = conv_output_shape(\n            dimension_size=None, filter_size=5,\n            padding=\'VALID\', stride=5,\n        )\n        self.assertEqual(shape, None)\n\n    def test_conv_invalid_padding_exception(self):\n        error_msg = ""greater or equal to zero""\n        with self.assertRaisesRegexp(ValueError, error_msg):\n            layers.Convolution((1, 3, 3), padding=-1)\n\n        error_msg = ""Tuple .+ greater or equal to zero""\n        with self.assertRaisesRegexp(ValueError, error_msg):\n            layers.Convolution((1, 3, 3), padding=(2, -1))\n\n        with self.assertRaisesRegexp(ValueError, ""invalid string value""):\n            layers.Convolution((1, 3, 3), padding=\'NOT_SAME\')\n\n        with self.assertRaisesRegexp(ValueError, ""contains two elements""):\n            layers.Convolution((1, 3, 3), padding=(3, 3, 3))\n\n    def test_conv_invalid_input_shape(self):\n        with self.assertRaises(LayerConnectionError):\n            layers.join(\n                layers.Input(10),\n                layers.Convolution((1, 3, 3)),\n            )\n\n    def test_conv_with_custom_int_padding(self):\n        network = layers.join(\n            layers.Input((5, 5, 1)),\n            layers.Convolution((3, 3, 1), bias=0, weight=1, padding=2),\n        )\n\n        x = asfloat(np.ones((1, 5, 5, 1)))\n        expected_output = np.array([\n            [1, 2, 3, 3, 3, 2, 1],\n            [2, 4, 6, 6, 6, 4, 2],\n            [3, 6, 9, 9, 9, 6, 3],\n            [3, 6, 9, 9, 9, 6, 3],\n            [3, 6, 9, 9, 9, 6, 3],\n            [2, 4, 6, 6, 6, 4, 2],\n            [1, 2, 3, 3, 3, 2, 1],\n        ]).reshape((1, 7, 7, 1))\n\n        actual_output = self.eval(network.output(x))\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n    def test_conv_with_custom_tuple_padding(self):\n        inp = layers.Input((5, 5, 1))\n        conv = layers.Convolution((3, 3, 1), bias=0, weight=1, padding=(0, 2))\n\n        network = (inp >> conv)\n        network.outputs\n\n        x = asfloat(np.ones((1, 5, 5, 1)))\n        expected_output = np.array([\n            [3, 6, 9, 9, 9, 6, 3],\n            [3, 6, 9, 9, 9, 6, 3],\n            [3, 6, 9, 9, 9, 6, 3],\n        ]).reshape((1, 3, 7, 1))\n        actual_output = self.eval(network.output(x))\n\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n        self.assertShapesEqual(network.output_shape, (None, 3, 7, 1))\n\n    def test_conv_without_bias(self):\n        inp = layers.Input((5, 5, 1))\n        conv = layers.Convolution((3, 3, 1), bias=None, weight=1)\n\n        network = inp >> conv\n        network.outputs\n\n        x = asfloat(np.ones((1, 5, 5, 1)))\n        expected_output = 9 * np.ones((1, 3, 3, 1))\n        actual_output = self.eval(network.output(x))\n\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n    def test_conv_unknown_input_width_and_height(self):\n        network = layers.join(\n            layers.Input((None, None, 3)),\n            layers.Convolution((3, 3, 5)),\n        )\n        self.assertShapesEqual(network.output_shape, (None, None, None, 5))\n\n        input_value = asfloat(np.ones((1, 12, 12, 3)))\n        actual_output = self.eval(network.output(input_value))\n        self.assertEqual(actual_output.shape, (1, 10, 10, 5))\n\n        input_value = asfloat(np.ones((1, 21, 21, 3)))\n        actual_output = self.eval(network.output(input_value))\n        self.assertEqual(actual_output.shape, (1, 19, 19, 5))\n\n    def test_dilated_convolution(self):\n        network = layers.join(\n            layers.Input((6, 6, 1)),\n            layers.Convolution((3, 3, 1), dilation=2, weight=1, bias=None),\n        )\n\n        input_value = asfloat(np.arange(36).reshape(1, 6, 6, 1))\n        actual_output = self.eval(network.output(input_value))\n\n        self.assertShapesEqual(actual_output.shape, (1, 2, 2, 1))\n        self.assertShapesEqual(\n            actual_output.shape[1:],\n            network.output_shape[1:])\n\n        actual_output = actual_output[0, :, :, 0]\n        expected_output = np.array([\n            [126, 135],  # every row value adds +1 per filter value (+9)\n            [180, 189],  # every col value adds +6 per filter value (+54)\n        ])\n        np.testing.assert_array_almost_equal(actual_output, expected_output)\n\n    def test_convolution_repr(self):\n        layer = layers.Convolution((3, 3, 10), name=\'conv\')\n        self.assertEqual(\n            str(layer),\n            (\n                ""Convolution((3, 3, 10), padding=\'VALID\', stride=(1, 1), ""\n                ""dilation=(1, 1), weight=HeNormal(gain=2), bias=Constant(0), ""\n                ""name=\'conv\')""\n            )\n        )\n\n    def test_conv_output_shape_when_input_unknown(self):\n        block = layers.join(\n            layers.Convolution((3, 3, 32)),\n            layers.Relu(),\n            layers.BatchNorm(),\n        )\n        self.assertShapesEqual(block.input_shape, None)\n        self.assertShapesEqual(block.output_shape, (None, None, None, 32))\n\n\nclass DeconvolutionTestCase(BaseTestCase):\n    def test_deconvolution(self):\n        network = layers.join(\n            layers.Input((10, 10, 3)),\n            layers.Convolution((3, 3, 7)),\n            layers.Deconvolution((3, 3, 4)),\n        )\n\n        shapes = network.output_shapes_per_layer\n        shapes = {l: shape_to_tuple(s) for l, s in shapes.items()}\n        self.assertDictEqual(\n            shapes, {\n                network.layers[0]: (None, 10, 10, 3),\n                network.layers[1]: (None, 8, 8, 7),\n                network.layers[2]: (None, 10, 10, 4),\n            }\n        )\n\n        input_value = asfloat(np.random.random((1, 10, 10, 3)))\n        actual_output = self.eval(network.output(input_value))\n\n        self.assertEqual(actual_output.shape, (1, 10, 10, 4))\n\n    def test_deconvolution_same_padding(self):\n        network = layers.join(\n            layers.Input((10, 10, 3)),\n            layers.Convolution((3, 3, 7), padding=\'same\'),\n            layers.Deconvolution((3, 3, 4), padding=\'same\'),\n        )\n\n        shapes = network.output_shapes_per_layer\n        shapes = {l: shape_to_tuple(s) for l, s in shapes.items()}\n\n        self.assertDictEqual(\n            shapes, {\n                network.layers[0]: (None, 10, 10, 3),\n                network.layers[1]: (None, 10, 10, 7),\n                network.layers[2]: (None, 10, 10, 4),\n            }\n        )\n\n        input_value = asfloat(np.random.random((1, 10, 10, 3)))\n        actual_output = self.eval(network.output(input_value))\n\n        self.assertEqual(actual_output.shape, (1, 10, 10, 4))\n\n    def test_deconvolution_int_padding(self):\n        network = layers.join(\n            layers.Input((10, 10, 3)),\n            layers.Convolution((3, 3, 7), padding=9),\n            layers.Deconvolution((3, 3, 4), padding=9),\n        )\n\n        shapes = network.output_shapes_per_layer\n        shapes = {l: shape_to_tuple(s) for l, s in shapes.items()}\n        self.assertDictEqual(\n            shapes, {\n                network.layers[0]: (None, 10, 10, 3),\n                network.layers[1]: (None, 26, 26, 7),\n                network.layers[2]: (None, 10, 10, 4),\n            }\n        )\n\n        input_value = asfloat(np.random.random((1, 10, 10, 3)))\n        actual_output = self.eval(network.output(input_value))\n\n        self.assertEqual(actual_output.shape, (1, 10, 10, 4))\n\n    def test_deconvolution_tuple_padding(self):\n        network = layers.join(\n            layers.Input((10, 10, 3)),\n            layers.Convolution((3, 3, 7), padding=(9, 3)),\n            layers.Deconvolution((3, 3, 4), padding=(9, 3)),\n        )\n\n        shapes = network.output_shapes_per_layer\n        shapes = {l: shape_to_tuple(s) for l, s in shapes.items()}\n        self.assertSequenceEqual(\n            shapes, {\n                network.layers[0]: (None, 10, 10, 3),\n                network.layers[1]: (None, 26, 14, 7),\n                network.layers[2]: (None, 10, 10, 4),\n            }\n        )\n\n        input_value = asfloat(np.random.random((1, 10, 10, 3)))\n        actual_output = self.eval(network.output(input_value))\n\n        self.assertEqual(actual_output.shape, (1, 10, 10, 4))\n\n    def test_deconv_unknown_input_width_and_height(self):\n        network = layers.join(\n            layers.Input((None, None, 3)),\n            layers.Convolution((3, 3, 7)),\n            layers.Deconvolution((3, 3, 4)),\n        )\n\n        shapes = network.output_shapes_per_layer\n        shapes = {l: shape_to_tuple(s) for l, s in shapes.items()}\n        self.assertDictEqual(\n            shapes, {\n                network.layers[0]: (None, None, None, 3),\n                network.layers[1]: (None, None, None, 7),\n                network.layers[2]: (None, None, None, 4),\n            }\n        )\n\n        input_value = asfloat(np.random.random((1, 10, 10, 3)))\n        actual_output = self.eval(network.output(input_value))\n        self.assertEqual(actual_output.shape, (1, 10, 10, 4))\n\n        input_value = asfloat(np.random.random((1, 7, 7, 3)))\n        actual_output = self.eval(network.output(input_value))\n        self.assertEqual(actual_output.shape, (1, 7, 7, 4))\n\n    def test_deconv_output_shape(self):\n        self.assertEqual(None, deconv_output_shape(None, 3, \'same\', 1))\n\n        self.assertEqual(12, deconv_output_shape(10, 3, \'valid\', 1))\n        self.assertEqual(16, deconv_output_shape(10, 7, \'valid\', 1))\n        self.assertEqual(10, deconv_output_shape(10, 3, \'same\', 1))\n\n        self.assertEqual(14, deconv_output_shape(4, 5, \'valid\', 3))\n        self.assertEqual(12, deconv_output_shape(4, 3, \'same\', 3))\n        self.assertEqual(12, deconv_output_shape(4, 7, \'same\', 3))\n\n    def test_deconv_output_shape_exception(self):\n        with self.assertRaisesRegexp(ValueError, ""unknown \\S+ padding""):\n            deconv_output_shape(10, 3, padding=\'xxx\', stride=1)\n\n        with self.assertRaisesRegexp(ValueError, ""doesn\'t support dilation""):\n            deconv_output_shape(10, 3, padding=\'valid\', stride=1, dilation=2)\n\n    def test_deconvolution_for_random_cases(self):\n        # A few random cases will check if output shape computed from\n        # the network is the same as the shape that we get after we\n        # propagated input through the network.\n        for test_id in range(30):\n            width = random.randint(7, 20)\n            height = random.randint(7, 20)\n\n            fh = random.randint(1, 7)\n            fw = random.randint(1, 7)\n\n            pad = random.choice([\n                \'valid\',\n                \'same\',\n                random.randint(0, 10),\n                (\n                    random.randint(0, 10),\n                    random.randint(0, 10),\n                ),\n            ])\n            stride = random.choice([\n                random.randint(1, 4),\n                (\n                    random.randint(1, 4),\n                    random.randint(1, 4),\n                ),\n            ])\n\n            print(\'\\n------------\')\n            print(""Test case #{}"".format(test_id))\n            print(\'------------\')\n            print(""Image shape: {}x{}"".format(height, width))\n            print(""Filter shape: {}x{}"".format(fh, fw))\n            print(""Padding: {}"".format(pad))\n            print(""Stride: {}"".format(stride))\n\n            network = layers.join(\n                layers.Input((height, width, 1)),\n                layers.Convolution((fh, fw, 2), padding=pad, stride=stride),\n                layers.Deconvolution((fh, fw, 1), padding=pad, stride=stride),\n            )\n\n            input_value = asfloat(np.random.random((1, height, width, 1)))\n            actual_output = self.eval(network.output(input_value))\n            self.assertEqual(actual_output.shape[1:], network.output_shape[1:])\n\n    def test_deconvolution_repr(self):\n        layer = layers.Deconvolution((3, 3, 10), name=\'deconv\')\n        self.assertEqual(\n            str(layer),\n            (\n                ""Deconvolution((3, 3, 10), padding=\'VALID\', stride=(1, 1), ""\n                ""weight=HeNormal(gain=2), bias=Constant(0), name=\'deconv\')""\n            )\n        )\n'"
tests/layers/test_create_custom_layers.py,0,"b'from neupy import layers\n\nfrom base import BaseTestCase\n\n\nclass CreateCustomLayersTestCase(BaseTestCase):\n    def test_custom_layer(self):\n        class CustomLayer(layers.BaseLayer):\n            def output(self, input):\n                return 2 * input\n\n        custom_layer = CustomLayer()\n        network = layers.join(layers.Input(10), custom_layer)\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, None)\n\n        custom_layer.input_shape = (None, 10)\n        err_message = ""Cannot update input shape of the layer""\n        with self.assertRaisesRegexp(ValueError, err_message):\n            custom_layer.input_shape = (None, 20)\n\n    def test_custom_layer_repr_priority(self):\n        class CustomLayer(layers.Identity):\n            def __init__(self, value, name=None):\n                self.value = value\n                super(CustomLayer, self).__init__(name=name)\n\n            def __repr__(self):\n                return self._repr_arguments(self.value, a=3, name=self.name)\n\n        custom_layer = CustomLayer(33)\n        self.assertEqual(\n            repr(custom_layer),\n            ""CustomLayer(33, name=\'custom-layer-1\', a=3)"")\n'"
tests/layers/test_embeddings.py,0,"b'import numpy as np\n\nfrom neupy import layers\nfrom neupy.utils import asfloat\n\nfrom base import BaseTestCase\n\n\nclass EmbeddingLayerTestCase(BaseTestCase):\n    def test_embedding_layer(self):\n        weight = np.arange(10).reshape((5, 2))\n\n        network = layers.join(\n            layers.Input(1),\n            layers.Embedding(5, 2, weight=weight),\n        )\n\n        input_vector = asfloat(np.array([[0, 1, 4]]).T)\n        expected_output = np.array([\n            [[0, 1]],\n            [[2, 3]],\n            [[8, 9]],\n        ])\n        actual_output = self.eval(network.output(input_vector))\n\n        self.assertShapesEqual(network.output_shape, (None, 1, 2))\n        np.testing.assert_array_equal(expected_output, actual_output)\n\n    def test_embedding_layer_repr(self):\n        self.assertEqual(\n            str(layers.Embedding(5, 2)),\n            (\n                ""Embedding(5, 2, weight=HeNormal(gain=1.0), ""\n                ""name=\'embedding-1\')""\n            )\n        )\n\n    def test_embedding_variables(self):\n        network = layers.join(\n            layers.Input(2),\n            layers.Embedding(3, 5, name=\'embed\'),\n        )\n        self.assertDictEqual(network.layer(\'embed\').variables, {})\n\n        network.outputs\n        variables = network.layer(\'embed\').variables\n        self.assertSequenceEqual(list(variables.keys()), [\'weight\'])\n        self.assertShapesEqual(variables[\'weight\'].shape, (3, 5))\n'"
tests/layers/test_gated_average.py,0,"b'import numpy as np\n\nfrom neupy import layers\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import LayerConnectionError\n\nfrom base import BaseTestCase\n\n\nclass GatedAverageTestCase(BaseTestCase):\n    def test_gated_average_layer_output_shape(self):\n        network = layers.join(\n            layers.parallel(\n                layers.Input(10) >> layers.Softmax(2),\n                layers.Input(20) >> layers.Relu(8),\n                layers.Input(20) >> layers.Relu(8),\n            ),\n            layers.GatedAverage()\n        )\n        self.assertShapesEqual(network.output_shape, (None, 8))\n\n    def test_gated_average_layer_negative_index(self):\n        network = layers.join(\n            layers.parallel(\n                layers.Input(20) >> layers.Relu(8),\n                layers.Input(20) >> layers.Relu(8),\n                layers.Input(10) >> layers.Softmax(2),\n            ),\n            layers.GatedAverage(gate_index=-1, name=\'gate\')\n        )\n        self.assertShapesEqual(network.output_shape, (None, 8))\n\n        network = layers.join(\n            layers.parallel(\n                layers.Input(10) >> layers.Softmax(2),\n                layers.Input(20) >> layers.Relu(8),\n                layers.Input(20) >> layers.Relu(8),\n            ),\n            layers.GatedAverage(gate_index=-3, name=\'gate\')\n        )\n        self.assertShapesEqual(network.output_shape, (None, 8))\n\n    def test_gated_average_layer_exceptions_index_position(self):\n        networks = layers.parallel(\n            layers.Input(10) >> layers.Softmax(2),\n            layers.Input(20) >> layers.Relu(8),\n            layers.Input(20) >> layers.Relu(8),\n        )\n        with self.assertRaisesRegexp(LayerConnectionError, ""Invalid index""):\n            layers.join(networks, layers.GatedAverage(gate_index=3))\n\n        with self.assertRaisesRegexp(LayerConnectionError, ""Invalid index""):\n            layers.join(networks, layers.GatedAverage(gate_index=-4))\n\n    def test_gated_average_layer_exceptions(self):\n        networks = layers.parallel(\n            layers.Input((10, 3, 3)),\n            layers.Input(20) >> layers.Relu(8),\n            layers.Input(20) >> layers.Relu(8),\n        )\n        error_message = ""should be 2-dimensional""\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(networks, layers.GatedAverage())\n\n        networks = layers.parallel(\n            layers.Input(10) >> layers.Softmax(3),\n            layers.Input(20) >> layers.Relu(8),\n            layers.Input(20) >> layers.Relu(8),\n        )\n        error_message = ""only 3 networks, got 2 networks""\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(networks, layers.GatedAverage())\n\n        networks = layers.parallel(\n            layers.Input(10) >> layers.Softmax(2),\n            layers.Input(20) >> layers.Relu(8),\n            layers.Input(20) >> layers.Relu(10),\n        )\n        error_message = ""expect to have the same shapes""\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(networks, layers.GatedAverage())\n\n    def test_gated_average_layer_non_default_index(self):\n        network = layers.join(\n            layers.parallel(\n                layers.Input(20) >> layers.Relu(8),\n                layers.Input(10) >> layers.Softmax(2),\n                layers.Input(20) >> layers.Relu(8),\n            ),\n            layers.GatedAverage(gate_index=1),\n        )\n        self.assertShapesEqual(network.output_shape, (None, 8))\n\n    def test_gated_average_layer_output(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.parallel(\n                layers.Softmax(2),\n                layers.Relu(8),\n                layers.Relu(8),\n            ),\n            layers.GatedAverage(),\n        )\n\n        random_input = asfloat(np.random.random((20, 10)))\n        actual_output = self.eval(network.output(random_input))\n        self.assertShapesEqual(actual_output.shape, (20, 8))\n\n    def test_gated_average_layer_multi_dimensional_inputs(self):\n        network = layers.join(\n            layers.Input((5, 5, 1)),\n            layers.parallel(\n                layers.Reshape() >> layers.Softmax(2),\n                layers.Convolution((2, 2, 3)),\n                layers.Convolution((2, 2, 3)),\n            ),\n            layers.GatedAverage(),\n        )\n\n        self.assertShapesEqual(network.input_shape, (None, 5, 5, 1))\n        self.assertShapesEqual(network.output_shape, (None, 4, 4, 3))\n\n        random_input = asfloat(np.random.random((8, 5, 5, 1)))\n        actual_output = self.eval(network.output(random_input))\n\n        self.assertEqual(actual_output.shape, (8, 4, 4, 3))\n'"
tests/layers/test_init_methods.py,0,"b'import math\n\nfrom scipy import stats\nimport numpy as np\n\nfrom neupy import init\nfrom neupy.init import identify_fans\n\nfrom base import BaseTestCase\n\n\nclass BaseInitializerTestCase(BaseTestCase):\n    def assertUniformlyDistributed(self, value):\n        self.assertTrue(stats.kstest(value.ravel(), \'uniform\'),\n                        msg=""Sampled distribution is not uniformal"")\n\n    def assertNormalyDistributed(self, value):\n        self.assertTrue(stats.mstats.normaltest(value.ravel()),\n                        msg=""Sampled distribution is not normal"")\n\n\nclass FanIdentifierTestCase(BaseInitializerTestCase):\n    def test_identify_fans_1d(self):\n        self.assertEqual((10, 1), identify_fans((10,)))\n        self.assertEqual((1, 1), identify_fans((1,)))\n\n    def test_identify_fans_2d(self):\n        self.assertEqual((10, 20), identify_fans((10, 20)))\n        self.assertEqual((20, 10), identify_fans((20, 10)))\n\n    def test_identify_fans_exceptions(self):\n        with self.assertRaisesRegexp(ValueError, ""shape is unknown""):\n            identify_fans(tuple())\n\n    def test_identify_fans_conv(self):\n        self.assertEqual((9, 90), identify_fans((3, 3, 1, 10)))\n        self.assertEqual((250, 150), identify_fans((5, 5, 10, 6)))\n\n    def test_identify_fans_other_dim(self):\n        self.assertEqual((18, 180), identify_fans((3, 3, 2, 1, 10)))\n        self.assertEqual((48, 80), identify_fans((8, 6, 10)))\n\n\nclass ConstantInitializationTestCase(BaseInitializerTestCase):\n    def test_constant_initializer(self):\n        const = init.Constant(value=0)\n        np.testing.assert_array_almost_equal(\n            self.eval(const.sample(shape=(2, 3))),\n            np.zeros((2, 3))\n        )\n\n        const = init.Constant(value=1.5)\n        np.testing.assert_array_almost_equal(\n            self.eval(const.sample(shape=(2, 3))),\n            np.ones((2, 3)) * 1.5\n        )\n\n        const = init.Constant(value=1.5)\n        np.testing.assert_array_almost_equal(\n            const.sample(shape=(2, 3), return_array=True),\n            np.ones((2, 3)) * 1.5\n        )\n\n    def test_constant_initialize_repr(self):\n        const_initializer = init.Constant(value=3)\n        self.assertEqual(""Constant(3)"", str(const_initializer))\n\n\nclass NormalInitializeTestCase(BaseInitializerTestCase):\n    def test_normal_initializer(self):\n        norm = init.Normal(mean=0, std=0.01)\n        weight = self.eval(norm.sample((30, 30)))\n        self.assertNormalyDistributed(weight)\n\n        weight = norm.sample((30, 30), return_array=True)\n        self.assertNormalyDistributed(weight)\n\n    def test_normal_reprodusible_with_outside_seed(self):\n        norm = init.Normal(mean=0, std=0.01)\n\n        np.random.seed(0)\n        weight1 = norm.sample((10, 4), return_array=True)\n\n        np.random.seed(0)\n        weight2 = norm.sample((10, 4), return_array=True)\n\n        np.testing.assert_array_almost_equal(weight1, weight2)\n\n    def test_normal_initialize_repr(self):\n        hormal_initializer = init.Normal(mean=0, std=0.01)\n        self.assertEqual(""Normal(mean=0, std=0.01)"", str(hormal_initializer))\n\n    def test_reproducibility(self):\n        normal = init.Normal(mean=0, std=0.01, seed=0)\n\n        weight1 = normal.sample((10, 20), return_array=True)\n        weight2 = normal.sample((10, 20), return_array=True)\n\n        np.testing.assert_array_almost_equal(weight1, weight2)\n\n\nclass UniformInitializeTestCase(BaseInitializerTestCase):\n    def test_uniformal_initializer(self):\n        uniform = init.Uniform(minval=-10, maxval=10)\n        weight = self.eval(uniform.sample((30, 30)))\n\n        self.assertUniformlyDistributed(weight)\n        self.assertAlmostEqual(-10, np.min(weight), places=1)\n        self.assertAlmostEqual(10, np.max(weight), places=1)\n\n        weight = uniform.sample((30, 30), return_array=True)\n        self.assertUniformlyDistributed(weight)\n        self.assertAlmostEqual(-10, np.min(weight), places=1)\n        self.assertAlmostEqual(10, np.max(weight), places=1)\n\n    def test_uniform_reprodusible_with_outside_seed(self):\n        uniform = init.Uniform(minval=-10, maxval=10)\n\n        np.random.seed(0)\n        weight1 = uniform.sample((10, 4), return_array=True)\n\n        np.random.seed(0)\n        weight2 = uniform.sample((10, 4), return_array=True)\n\n        np.testing.assert_array_almost_equal(weight1, weight2)\n\n    def test_uniform_initializer_repr(self):\n        uniform_initializer = init.Uniform(minval=0, maxval=1)\n        self.assertEqual(""Uniform(0, 1)"", str(uniform_initializer))\n\n\nclass InitializerWithGainTestCase(BaseInitializerTestCase):\n    def test_gain_relu_he_normal_scale(self):\n        he_initializer = init.HeNormal(gain=1, seed=0)\n        sample_1 = self.eval(he_initializer.sample((4, 4)))\n\n        he_initializer = init.HeNormal(gain=2, seed=0)\n        sample_2 = self.eval(he_initializer.sample((4, 4)))\n\n        self.assertAlmostEqual(\n            np.mean(sample_2 / sample_1),\n            math.sqrt(2),\n            places=5\n        )\n\n\nclass HeInitializeTestCase(BaseInitializerTestCase):\n    def test_he_normal(self):\n        he_normal = init.HeNormal()\n        weight = self.eval(he_normal.sample((40, 40)))\n\n        self.assertNormalyDistributed(weight)\n        self.assertAlmostEqual(weight.mean(), 0, places=1)\n        self.assertAlmostEqual(weight.std(), math.sqrt(1. / 40),\n                               places=2)\n\n    def test_he_uniform(self):\n        n_inputs = 30\n        bound = math.sqrt(3. / n_inputs)\n\n        he_uniform = init.HeUniform()\n        weight = self.eval(he_uniform.sample((n_inputs, 30)))\n\n        self.assertUniformlyDistributed(weight)\n        self.assertAlmostEqual(weight.mean(), 0, places=1)\n        self.assertGreaterEqual(weight.min(), -bound)\n        self.assertLessEqual(weight.max(), bound)\n\n    def test_he_initializer_repr(self):\n        he_initializer = init.HeNormal()\n        self.assertEqual(""HeNormal(gain=1.0)"", str(he_initializer))\n\n\nclass XavierInitializeTestCase(BaseInitializerTestCase):\n    def test_xavier_normal(self):\n        n_inputs, n_outputs = 30, 30\n\n        xavier_normal = init.XavierNormal()\n        weight = self.eval(xavier_normal.sample((n_inputs, n_outputs)))\n\n        self.assertNormalyDistributed(weight)\n        self.assertAlmostEqual(weight.mean(), 0, places=1)\n        self.assertAlmostEqual(\n            weight.std(),\n            math.sqrt(1. / (n_inputs + n_outputs)),\n            places=2)\n\n    def test_xavier_uniform(self):\n        n_inputs, n_outputs = 10, 30\n\n        xavier_uniform = init.XavierUniform()\n        weight = self.eval(xavier_uniform.sample((n_inputs, n_outputs)))\n\n        bound = math.sqrt(3. / (n_inputs + n_outputs))\n\n        self.assertUniformlyDistributed(weight)\n        self.assertAlmostEqual(weight.mean(), 0, places=1)\n        self.assertGreaterEqual(weight.min(), -bound)\n        self.assertLessEqual(weight.max(), bound)\n\n\nclass OrthogonalInitializeTestCase(BaseInitializerTestCase):\n    def test_orthogonal_matrix_initializer_errors(self):\n        with self.assertRaises(ValueError):\n            ortho = init.Orthogonal()\n            # More than 2 dimensions\n            ortho.sample((5, 5, 5))\n\n    def test_orthogonal_matrix_initializer(self):\n        # Note: Matrix can\'t be orthogonal for row and column space\n        # at the same time in case if matrix rectangular\n\n        ortho = init.Orthogonal(scale=1)\n        # Matrix that have more rows than columns\n        weight = self.eval(ortho.sample((30, 10)))\n        np.testing.assert_array_almost_equal(\n            np.eye(10),\n            weight.T.dot(weight),\n            decimal=5\n        )\n\n        ortho = init.Orthogonal(scale=1)\n        # Matrix that have more columns than rows\n        weight = self.eval(ortho.sample((10, 30)))\n        np.testing.assert_array_almost_equal(\n            np.eye(10),\n            weight.dot(weight.T),\n            decimal=5\n        )\n\n    def test_orthogonal_init_repr(self):\n        ortho_initializer = init.Orthogonal(scale=1)\n        self.assertEqual(""Orthogonal(scale=1)"", str(ortho_initializer))\n\n    def test_orthogonal_1d_shape(self):\n        ortho = init.Orthogonal(scale=1)\n        sampled_data = self.eval(ortho.sample(shape=(1,)))\n        self.assertEqual((1,), sampled_data.shape)\n\n    def test_reproducibility(self):\n        ortho = init.Orthogonal(seed=0)\n\n        weight1 = ortho.sample((10, 20), return_array=True)\n        weight2 = ortho.sample((10, 20), return_array=True)\n\n        np.testing.assert_array_almost_equal(weight1, weight2)\n'"
tests/layers/test_input.py,0,"b'from neupy import layers\nfrom neupy.exceptions import LayerConnectionError\n\nfrom base import BaseTestCase\n\n\nclass InputTestCase(BaseTestCase):\n    def test_input_exceptions(self):\n        layer = layers.Input(10)\n        error_message = ""Input layer got unexpected input""\n\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layer.get_output_shape((10, 15))\n\n    def test_output_shape_merging(self):\n        layer = layers.Input(10)\n        self.assertShapesEqual(layer.get_output_shape((None, 10)), (None, 10))\n        self.assertShapesEqual(layer.get_output_shape((5, 10)), (5, 10))\n\n        layer = layers.Input((None, None, 3))\n        self.assertShapesEqual(\n            layer.get_output_shape((None, 28, 28, 3)),\n            (None, 28, 28, 3),\n        )\n        self.assertShapesEqual(\n            layer.get_output_shape((None, None, 28, 3)),\n            (None, None, 28, 3),\n        )\n        self.assertShapesEqual(\n            layer.get_output_shape((10, 28, 28, None)),\n            (10, 28, 28, 3),\n        )\n\n    def test_merged_inputs(self):\n        network = layers.join(\n            layers.Input((10, 2)),\n            layers.Input((None, 2)),\n        )\n        self.assertShapesEqual(network.input_shape, (None, 10, 2))\n        self.assertShapesEqual(network.output_shape, (None, 10, 2))\n\n    def test_input_layers_connected(self):\n        network = layers.join(layers.Input(1), layers.Input(1))\n        self.assertShapesEqual(network.input_shape, (None, 1))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n\n    def test_input_repr(self):\n        self.assertEqual(\n            str(layers.Input(10)),\n            ""Input(10, name=\'input-1\')"",\n        )\n        self.assertEqual(\n            str(layers.Input((10, 3))),\n            ""Input((10, 3), name=\'input-2\')"",\n        )\n        self.assertEqual(\n            str(layers.Input((None, None, 3))),\n            ""Input((None, None, 3), name=\'input-3\')"",\n        )\n        self.assertEqual(\n            str(layers.Input(None)),\n            ""Input(None, name=\'input-4\')"",\n        )\n\n    def test_input_with_tensor_shape(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5),\n        )\n        network_2 = layers.join(\n            layers.Input(network.output_shape[1:]),\n            layers.Relu(3),\n        )\n        self.assertEqual(network_2.layers[0].shape, (5,))\n        self.assertShapesEqual(network_2.input_shape, (None, 5))\n        self.assertShapesEqual(network_2.output_shape, (None, 3))\n'"
tests/layers/test_merge.py,1,"b'import tensorflow as tf\nimport numpy as np\n\nfrom neupy import layers\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import LayerConnectionError\n\nfrom base import BaseTestCase\n\n\nclass ElementwiseTestCase(BaseTestCase):\n    def test_elementwise_basic(self):\n        elem_layer = layers.Elementwise(merge_function=tf.add)\n\n        x1_matrix = asfloat(np.random.random((10, 2)))\n        x2_matrix = asfloat(np.random.random((10, 2)))\n\n        expected_output = x1_matrix + x2_matrix\n        actual_output = self.eval(elem_layer.output(x1_matrix, x2_matrix))\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n    def test_elementwise_exceptions(self):\n        with self.assertRaises(ValueError):\n            not_callable_object = (1, 2, 3)\n            layers.Elementwise(merge_function=not_callable_object)\n\n        with self.assertRaises(ValueError):\n            layers.Elementwise(merge_function=\'wrong-func-name\')\n\n        message = ""expected multiple inputs""\n        with self.assertRaisesRegexp(LayerConnectionError, message):\n            layers.join(layers.Input(5), layers.Elementwise(\'multiply\'))\n\n        inputs = layers.parallel(\n            layers.Input(2),\n            layers.Input(1),\n        )\n        error_message = ""layer have incompatible shapes""\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(inputs, layers.Elementwise(\'add\'))\n\n    def test_elementwise_in_network(self):\n        network = layers.join(\n            layers.Input(2),\n            layers.parallel(\n                layers.Relu(1, weight=1, bias=0),\n                layers.Relu(1, weight=2, bias=0),\n            ),\n            layers.Elementwise(\'add\'),\n        )\n        self.assertShapesEqual(network.input_shape, (None, 2))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n\n        test_input = asfloat(np.array([[0, 1], [-1, -1]]))\n        actual_output = self.eval(network.output(test_input))\n        expected_output = np.array([[3, 0]]).T\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n    def test_elementwise_custom_function(self):\n        def weighted_sum(a, b):\n            return 0.2 * a + 0.8 * b\n\n        network = layers.join(\n            layers.Input(2),\n            layers.parallel(\n                layers.Relu(1, weight=1, bias=0),\n                layers.Relu(1, weight=2, bias=0),\n            ),\n            layers.Elementwise(weighted_sum),\n        )\n        self.assertShapesEqual(network.input_shape, (None, 2))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n\n        test_input = asfloat(np.array([[0, 1], [-1, -1]]))\n        actual_output = self.eval(network.output(test_input))\n        expected_output = np.array([[1.8, 0]]).T\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n\nclass ConcatenateTestCase(BaseTestCase):\n    def test_concatenate_basic(self):\n        concat_layer = layers.Concatenate(axis=-1)\n\n        x1_tensor4 = asfloat(np.random.random((1, 3, 4, 2)))\n        x2_tensor4 = asfloat(np.random.random((1, 3, 4, 8)))\n        output = self.eval(concat_layer.output(x1_tensor4, x2_tensor4))\n\n        self.assertEqual((1, 3, 4, 10), output.shape)\n\n    def test_concatenate_different_dim_number(self):\n        inputs = layers.parallel(\n            layers.Input((28, 28)),\n            layers.Input((28, 28, 1)),\n        )\n\n        expected_msg = ""different number of dimensions""\n        with self.assertRaisesRegexp(LayerConnectionError, expected_msg):\n            layers.join(inputs, layers.Concatenate(axis=1))\n\n    def test_concatenate_init_error(self):\n        inputs = layers.parallel(\n            layers.Input((28, 28, 3)),\n            layers.Input((28, 28, 1)),\n        )\n\n        expected_message = ""don\'t match over dimension #3""\n        with self.assertRaisesRegexp(LayerConnectionError, expected_message):\n            layers.join(inputs, layers.Concatenate(axis=2))\n\n    def test_concatenate_conv_layers(self):\n        network = layers.join(\n            layers.Input((28, 28, 3)),\n            layers.parallel(\n                layers.Convolution((5, 5, 7)),\n                layers.join(\n                    layers.Convolution((3, 3, 1)),\n                    layers.Convolution((3, 3, 4)),\n                ),\n            ),\n            layers.Concatenate(axis=-1)\n        )\n\n        self.assertShapesEqual((None, 24, 24, 11), network.output_shape)\n\n        x_tensor4 = asfloat(np.random.random((5, 28, 28, 3)))\n        actual_output = self.eval(network.output(x_tensor4))\n\n        self.assertEqual((5, 24, 24, 11), actual_output.shape)\n\n    def test_concat_with_late_inputs(self):\n        network = layers.join(\n            layers.parallel(\n                layers.Relu(),\n                layers.Relu(),\n            ),\n            layers.Concatenate(),\n        )\n\n        self.assertShapesEqual(network.input_shape, [None, None])\n        self.assertShapesEqual(network.output_shape, None)\n\n        network = layers.Input((10, 10, 3)) >> network\n\n        self.assertShapesEqual(network.input_shape, (None, 10, 10, 3))\n        self.assertShapesEqual(network.output_shape, (None, 10, 10, 6))\n'"
tests/layers/test_normalization.py,15,"b'import tempfile\n\nimport numpy as np\nimport tensorflow as tf\nfrom scipy import stats\n\nfrom neupy import layers, algorithms, storage\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import (\n    LayerConnectionError,\n    WeightInitializationError,\n)\n\nfrom base import BaseTestCase\nfrom helpers import simple_classification\n\n\nclass BatchNormTestCase(BaseTestCase):\n    def test_batch_norm_as_shared_variable(self):\n        gamma = tf.Variable(\n            asfloat(np.ones((1, 2))),\n            name=\'gamma\',\n            dtype=tf.float32,\n        )\n        beta = tf.Variable(\n            asfloat(2 * np.ones((1, 2))),\n            name=\'beta\',\n            dtype=tf.float32,\n        )\n\n        batch_norm = layers.BatchNorm(gamma=gamma, beta=beta)\n        network = layers.join(layers.Input(2), batch_norm)\n        network.outputs\n\n        self.assertIs(gamma, batch_norm.gamma)\n        self.assertIs(beta, batch_norm.beta)\n\n    def test_simple_batch_norm(self):\n        network = layers.Input(10) > layers.BatchNorm()\n\n        input_value = tf.Variable(\n            asfloat(np.random.random((30, 10))),\n            name=\'input_value\',\n            dtype=tf.float32,\n        )\n        output_value = self.eval(network.output(input_value, training=True))\n\n        self.assertTrue(stats.mstats.normaltest(output_value))\n        self.assertAlmostEqual(output_value.mean(), 0, places=3)\n        self.assertAlmostEqual(output_value.std(), 1, places=3)\n\n    def test_batch_norm_gamma_beta_params(self):\n        default_beta = -3.14\n        default_gamma = 4.3\n        network = layers.join(\n            layers.Input(10),\n            layers.BatchNorm(gamma=default_gamma, beta=default_beta)\n        )\n\n        input_value = tf.Variable(\n            asfloat(np.random.random((30, 10))),\n            name=\'input_value\',\n            dtype=tf.float32,\n        )\n        output_value = self.eval(network.output(input_value, training=True))\n\n        self.assertAlmostEqual(output_value.mean(), default_beta, places=3)\n        self.assertAlmostEqual(output_value.std(), default_gamma, places=3)\n\n    def test_batch_norm_between_layers(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(40),\n            layers.BatchNorm(),\n            layers.Relu(1),\n        )\n\n        input_value = tf.Variable(\n            asfloat(np.random.random((30, 10))),\n            name=\'input_value\',\n            dtype=tf.float32,\n        )\n        outpu_value = self.eval(network.output(input_value, training=True))\n        self.assertEqual(outpu_value.shape, (30, 1))\n\n    def test_batch_norm_in_non_training_state(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.BatchNorm(),\n        )\n        input_value = tf.Variable(\n            asfloat(np.random.random((30, 10))),\n            name=\'input_value\',\n            dtype=tf.float32,\n        )\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        self.assertEqual(len(update_ops), 0)\n\n        output_value = network.output(input_value)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        self.assertEqual(len(update_ops), 0)\n\n        network.output(input_value, training=True)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        self.assertEqual(len(update_ops), 2)\n\n        # Without training your running mean and std suppose to be\n        # equal to 0 and 1 respectavely.\n        output_value = self.eval(output_value)\n        np.testing.assert_array_almost_equal(\n            self.eval(input_value),\n            output_value,\n            decimal=4)\n\n    def test_batch_norm_storage(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        batch_norm = layers.BatchNorm()\n        gdnet = algorithms.GradientDescent(\n            [\n                layers.Input(10),\n                layers.Relu(5),\n                batch_norm,\n                layers.Sigmoid(1),\n            ],\n            batch_size=10,\n            verbose=True,  # keep it as `True`\n        )\n        gdnet.train(x_train, y_train, epochs=5)\n\n        error_before_save = gdnet.score(x_test, y_test)\n        mean_before_save = self.eval(batch_norm.running_mean)\n        variance_before_save = self.eval(batch_norm.running_inv_std)\n\n        with tempfile.NamedTemporaryFile() as temp:\n            storage.save(gdnet, temp.name)\n            storage.load(gdnet, temp.name)\n\n            error_after_load = gdnet.score(x_test, y_test)\n            mean_after_load = self.eval(batch_norm.running_mean)\n            variance_after_load = self.eval(batch_norm.running_inv_std)\n\n            self.assertAlmostEqual(error_before_save, error_after_load)\n            np.testing.assert_array_almost_equal(\n                mean_before_save, mean_after_load)\n\n            np.testing.assert_array_almost_equal(\n                variance_before_save,\n                variance_after_load)\n\n    def test_batchnorm_wrong_axes(self):\n        message = ""Specified axes have to contain only unique values""\n        with self.assertRaisesRegexp(ValueError, message):\n            layers.BatchNorm(axes=(0, 1, 1))\n\n    def test_batchnorm_wrong_axes_values(self):\n        network = layers.join(\n            layers.Relu(),\n            layers.BatchNorm(),\n        )\n        message = (\n            ""Cannot initialize variables for the batch normalization ""\n            ""layer, because input shape is undefined""\n        )\n        with self.assertRaisesRegexp(WeightInitializationError, message):\n            network.create_variables()\n\n    def test_batchnorm_unsuitable_axes_values(self):\n        network = layers.join(\n            layers.Input((10, 3)),\n            layers.BatchNorm(axes=(0, 2, 3)),\n        )\n        message = (\n            ""Batch normalization cannot be applied over one of ""\n            ""the axis, because input has only 3 dimensions""\n        )\n        with self.assertRaisesRegexp(LayerConnectionError, message):\n            network.create_variables()\n\n    def test_batchnorm_unknown_dimension(self):\n        network = layers.join(\n            layers.Input((10, 10, None)),\n            layers.BatchNorm(),\n        )\n        message = (\n            ""Cannot create variables for batch normalization, because ""\n            ""input has unknown dimension #3 \\(0-based indices\\). ""\n            ""Input shape: \\(\\?, 10, 10, \\?\\)""\n        )\n        with self.assertRaisesRegexp(WeightInitializationError, message):\n            network.create_variables()\n\n\nclass LocalResponseNormTestCase(BaseTestCase):\n    def test_local_response_norm_exceptions(self):\n        with self.assertRaisesRegexp(ValueError, ""Only works with odd""):\n            layers.LocalResponseNorm(depth_radius=2)\n\n        with self.assertRaises(LayerConnectionError):\n            layers.join(layers.Input(10), layers.LocalResponseNorm())\n\n    def test_local_response_normalization_layer(self):\n        network = layers.join(\n            layers.Input((1, 1, 1)),\n            layers.LocalResponseNorm(),\n        )\n\n        x_tensor = asfloat(np.ones((1, 1, 1, 1)))\n        actual_output = self.eval(network.output(x_tensor, training=True))\n        expected_output = np.array([0.59458]).reshape((-1, 1, 1, 1))\n\n        np.testing.assert_array_almost_equal(\n            expected_output, actual_output, decimal=5)\n\n\nclass GroupNormTestCase(BaseTestCase):\n    def test_group_norm_repr(self):\n        layer = layers.GroupNorm(4)\n        self.assertEqual(\n            str(layer),\n            (\n                ""GroupNorm(n_groups=4, beta=Constant(0), ""\n                ""gamma=Constant(1), epsilon=1e-05, name=\'group-norm-1\')""\n            )\n        )\n\n    def test_group_norm_connection_exception(self):\n        message = ""Cannot divide 11 input channels into 4 groups""\n        with self.assertRaisesRegexp(LayerConnectionError, message):\n            layers.join(\n                layers.Input((10, 10, 11)),\n                layers.GroupNorm(4),\n            )\n\n        message = (\n            ""Group normalization layer expects 4 ""\n            ""dimensional input, got 3 instead.""\n        )\n        with self.assertRaisesRegexp(LayerConnectionError, message):\n            layers.join(\n                layers.Input((10, 12)),\n                layers.GroupNorm(4),\n            )\n\n    def test_group_norm_weight_init_exception(self):\n        network = layers.join(\n            layers.Input((10, 10, None)),\n            layers.GroupNorm(4),\n        )\n\n        message = (\n            ""Cannot initialize variables when number of ""\n            ""channels is unknown.""\n        )\n        with self.assertRaisesRegexp(WeightInitializationError, message):\n            network.create_variables()\n\n        with self.assertRaisesRegexp(WeightInitializationError, message):\n            network.outputs\n\n    def test_group_norm(self):\n        network = layers.join(\n            layers.Input((10, 10, 12)),\n            layers.GroupNorm(4),\n        )\n        self.assertShapesEqual(network.input_shape, (None, 10, 10, 12))\n        self.assertShapesEqual(network.output_shape, (None, 10, 10, 12))\n\n        input = np.random.random((7, 10, 10, 12))\n        actual_output = self.eval(network.output(input))\n        self.assertEqual(actual_output.shape, (7, 10, 10, 12))\n\n    def test_group_norm_unknown_shape(self):\n        network = layers.join(\n            layers.Input((None, None, 16)),\n            layers.GroupNorm(4),\n        )\n\n        input = np.random.random((7, 6, 6, 16))\n        actual_output = self.eval(network.output(input))\n        self.assertEqual(actual_output.shape, (7, 6, 6, 16))\n'"
tests/layers/test_pooling.py,1,"b'from collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy import layers\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.layers.pooling import pooling_output_shape\n\nfrom base import BaseTestCase\n\n\nclass PoolingLayersTestCase(BaseTestCase):\n    def test_pooling_output_shape_exception(self):\n        expected_msg = r""1 is unknown padding value for pooling""\n        with self.assertRaisesRegexp(ValueError, expected_msg):\n            pooling_output_shape(\n                dimension_size=5,\n                pool_size=2,\n                padding=1,\n                stride=2)\n\n    def test_pooling_output_shape(self):\n        otuput_shape = pooling_output_shape(None, None, None, None)\n        self.assertEqual(otuput_shape, None)\n\n        otuput_shape = pooling_output_shape(\n            dimension_size=5, pool_size=2,\n            padding=\'VALID\', stride=2)\n\n        self.assertEqual(otuput_shape, 2)\n\n        otuput_shape = pooling_output_shape(\n            dimension_size=5, pool_size=2,\n            padding=\'VALID\', stride=1)\n\n        self.assertEqual(otuput_shape, 4)\n\n        otuput_shape = pooling_output_shape(\n            dimension_size=5, pool_size=2,\n            padding=\'VALID\', stride=4)\n\n        self.assertEqual(otuput_shape, 1)\n\n    def test_pooling_undefined_output_shape(self):\n        max_pool_layer = layers.MaxPooling((2, 2))\n        self.assertShapesEqual(\n            max_pool_layer.output_shape,\n            (None, None, None, None))\n\n    def test_pooling_layer_output_shape(self):\n        network = layers.join(\n            layers.Input((10, 10, 3)),\n            layers.MaxPooling((2, 2)),\n        )\n        self.assertShapesEqual(network.output_shape, (None, 5, 5, 3))\n\n    def test_pooling_stride_int(self):\n        network = layers.join(\n            layers.Input((28, 28, 1)),\n            layers.MaxPooling((2, 2), stride=1, padding=\'same\'),\n        )\n        self.assertShapesEqual(\n            network.input_shape,\n            network.output_shape)\n\n    def test_pooling_exceptions(self):\n        with self.assertRaises(ValueError):\n            layers.MaxPooling((2, 2), padding=\'TEST\')\n\n        with self.assertRaises(ValueError):\n            layers.MaxPooling((2, 2), padding=1)\n\n    def test_pooling_repr(self):\n        layer = layers.MaxPooling((2, 2))\n        self.assertEqual(\n            str(layer),\n            (\n                ""MaxPooling((2, 2), stride=None, padding=\'valid\', ""\n                ""name=\'max-pooling-1\')""\n            ),\n        )\n        layer = layers.MaxPooling((2, 2), stride=(3, 3))\n        self.assertEqual(\n            str(layer),\n            (\n                ""MaxPooling((2, 2), stride=(3, 3), padding=\'valid\', ""\n                ""name=\'max-pooling-2\')""\n            ),\n        )\n\n    def test_max_pooling(self):\n        X = asfloat(np.array([\n            [1, 2, 3, -1],\n            [4, -6, 3, 1],\n            [0, 0, 1, 0],\n            [0, -1, 0, 0],\n        ])).reshape(1, 4, 4, 1)\n        expected_output = asfloat(np.array([\n            [4, 3],\n            [0, 1],\n        ])).reshape(1, 2, 2, 1)\n\n        network = layers.join(\n            layers.Input((4, 4, 1)),\n            layers.MaxPooling((2, 2)),\n        )\n        actual_output = self.eval(network.output(X))\n        np.testing.assert_array_almost_equal(actual_output, expected_output)\n\n    def test_average_pooling(self):\n        X = asfloat(np.array([\n            [1, 2, 3, -1],\n            [4, -6, 3, 1],\n            [0, 0, 1, 0],\n            [0, -1, 0, 0],\n        ])).reshape(1, 4, 4, 1)\n        expected_output = asfloat(np.array([\n            [1 / 4., 6 / 4.],\n            [-1 / 4., 1 / 4.],\n        ])).reshape(1, 2, 2, 1)\n\n        network = layers.join(\n            layers.Input((4, 4, 1)),\n            layers.AveragePooling((2, 2)),\n        )\n        actual_output = self.eval(network.output(X))\n        np.testing.assert_array_almost_equal(actual_output, expected_output)\n\n    def test_pooling_late_shape_init(self):\n        network = layers.join(\n            layers.AveragePooling((2, 2)),\n            layers.Softmax(),\n        )\n        self.assertShapesEqual(network.output_shape, (None, None, None, None))\n\n        network = layers.join(layers.Input((10, 10, 1)), network)\n        self.assertShapesEqual(network.output_shape, (None, 5, 5, 1))\n\n    def test_failed_pooling_connection(self):\n        network = layers.join(\n            layers.Relu(),\n            layers.AveragePooling((2, 2)),\n            layers.Reshape(),\n        )\n        error_message = (\n            ""Pooling layer expects an input with 4 ""\n            ""dimensions, got 2 with shape \\(\\?, 10\\)""\n        )\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(layers.Input(10), network)\n\n\nclass UpscaleLayersTestCase(BaseTestCase):\n    def test_upscale_layer_exceptions(self):\n        with self.assertRaises(LayerConnectionError):\n            # Input shape should be 4 dimensional\n            layers.join(layers.Input(10), layers.Upscale((2, 2)))\n\n        invalid_scales = [-1, (2, 0), (-4, 1), (3, 3, 3)]\n        for invalid_scale in invalid_scales:\n            with self.assertRaises(ValueError):\n                layers.Upscale(invalid_scale)\n\n    def test_upscale_layer_shape(self):\n        Case = namedtuple(""Case"", ""scale expected_shape"")\n        testcases = (\n            Case(scale=(2, 2), expected_shape=(None, 28, 28, 1)),\n            Case(scale=(2, 1), expected_shape=(None, 28, 14, 1)),\n            Case(scale=(1, 2), expected_shape=(None, 14, 28, 1)),\n            Case(scale=(1, 1), expected_shape=(None, 14, 14, 1)),\n            Case(scale=(1, 10), expected_shape=(None, 14, 140, 1)),\n        )\n\n        for testcase in testcases:\n            network = layers.join(\n                layers.Input((14, 14, 1)),\n                layers.Upscale(testcase.scale),\n            )\n\n            self.assertShapesEqual(\n                network.output_shape,\n                testcase.expected_shape,\n                msg=""scale: {}"".format(testcase.scale))\n\n    def test_upscale_layer(self):\n        input_value = np.array([\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n        ]).reshape((1, 2, 4, 1))\n        expected_output = np.array([\n            [1, 1, 2, 2, 3, 3, 4, 4],\n            [1, 1, 2, 2, 3, 3, 4, 4],\n            [1, 1, 2, 2, 3, 3, 4, 4],\n            [5, 5, 6, 6, 7, 7, 8, 8],\n            [5, 5, 6, 6, 7, 7, 8, 8],\n            [5, 5, 6, 6, 7, 7, 8, 8],\n        ]).reshape((1, 6, 8, 1))\n\n        upscale_layer = layers.Upscale((3, 2))\n        network = layers.join(layers.Input((2, 4, 1)), upscale_layer)\n        self.assertShapesEqual(network.output_shape, (None, 6, 8, 1))\n\n        actual_output = self.eval(network.output(asfloat(input_value)))\n        np.testing.assert_array_almost_equal(\n            asfloat(expected_output), actual_output)\n\n    def test_upscale_late_shape_init(self):\n        network = layers.join(\n            layers.Upscale((2, 2)),\n            layers.Softmax(),\n        )\n        self.assertShapesEqual(network.output_shape, (None, None, None, None))\n\n        network = layers.join(layers.Input((10, 10, 5)), network)\n        self.assertShapesEqual(network.output_shape, (None, 20, 20, 5))\n\n    def test_global_pooling_repr(self):\n        self.assertEqual(\n            ""Upscale((2, 3), name=\'upscale-1\')"",\n            str(layers.Upscale((2, 3))))\n\n\nclass GlobalPoolingLayersTestCase(BaseTestCase):\n    def test_global_pooling_output_shape(self):\n        input_layer = layers.Input((8, 8, 3))\n        global_pooling_layer = layers.GlobalPooling(\'avg\')\n        network = layers.join(\n            input_layer,\n            global_pooling_layer\n        )\n        self.assertShapesEqual(network.input_shape, (None, 8, 8, 3))\n        self.assertShapesEqual(network.output_shape, (None, 3))\n\n    def test_global_pooling(self):\n        x = asfloat(np.ones((2, 4, 5, 3)))\n        expected_outputs = np.ones((2, 3))\n\n        global_mena_pooling_layer = layers.GlobalPooling(\'avg\')\n        actual_output = self.eval(global_mena_pooling_layer.output(x))\n\n        self.assertEqual(actual_output.shape, (2, 3))\n        np.testing.assert_array_equal(expected_outputs, actual_output)\n\n    def test_global_pooling_other_function(self):\n        x = asfloat(np.ones((2, 4, 5, 3)))\n        expected_outputs = 20 * np.ones((2, 3))\n\n        global_sum_pooling_layer = layers.GlobalPooling(function=tf.reduce_sum)\n        actual_output = self.eval(global_sum_pooling_layer.output(x))\n\n        self.assertShapesEqual(actual_output.shape, (2, 3))\n        np.testing.assert_array_equal(expected_outputs, actual_output)\n\n    def test_global_pooling_unknown_option(self):\n        with self.assertRaises(ValueError):\n            layers.GlobalPooling(\'unknown\')\n\n    def test_global_pooling_for_lower_dimensions(self):\n        layer = layers.GlobalPooling(\'max\')\n        x = np.random.random((4, 5))\n        y = layer.output(x)\n        self.assertShapesEqual(y.shape, (4, 5))\n\n    def test_global_pooling_late_shape_init(self):\n        network = layers.join(\n            layers.Relu(),\n            layers.GlobalPooling(\'max\'),\n        )\n        self.assertShapesEqual(network.output_shape, (None, None))\n\n        network = layers.join(layers.Input((10, 10, 3)), network)\n        self.assertShapesEqual(network.output_shape, (None, 3))\n\n    def test_global_pooling_repr(self):\n        layer = layers.GlobalPooling(\'max\')\n        self.assertEqual(\n            ""GlobalPooling(\'max\', name=\'global-pooling-1\')"",\n            str(layer))\n\n        layer = layers.GlobalPooling(lambda x: x)\n        self.assertRegexpMatches(\n            str(layer),\n            r""GlobalPooling\\(<function .+>, name=\'global-pooling-2\'\\)"",\n        )\n'"
tests/layers/test_recurrent.py,2,"b""import numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nfrom neupy.datasets import reber\nfrom neupy.utils import asfloat, tensorflow_session\nfrom neupy.exceptions import LayerConnectionError\nfrom neupy.layers.recurrent import clip_gradient\nfrom neupy import layers, algorithms, init\n\nfrom base import BaseTestCase\n\n\ndef add_sequence_padding(data):\n    n_sampels = len(data)\n    max_seq_length = max(map(len, data))\n\n    data_matrix = np.zeros((n_sampels, max_seq_length))\n    for i, sample in enumerate(data):\n        data_matrix[i, -len(sample):] = sample\n\n    return data_matrix\n\n\nclass GradientClippingTestCase(BaseTestCase):\n    def test_clip_gradient(self):\n        session = tensorflow_session()\n\n        x = tf.Variable(asfloat(1), dtype=tf.float32)\n        x_clipped = clip_gradient(x, 1.5)\n        y = x_clipped ** 2\n        gradient, = tf.gradients(y, x)\n        self.assertAlmostEqual(self.eval(gradient), 1.5)\n\n        x.load(asfloat(0.1), session)\n        self.assertAlmostEqual(self.eval(gradient), 0.2)\n\n        x.load(asfloat(-0.1), session)\n        self.assertAlmostEqual(self.eval(gradient), -0.2)\n\n        x.load(asfloat(-2), session)\n        self.assertAlmostEqual(self.eval(gradient), -1.5)\n\n\nclass LSTMTestCase(BaseTestCase):\n    single_thread = True\n    random_seed = 44\n\n    def setUp(self):\n        super(LSTMTestCase, self).setUp()\n\n        data, labels = reber.make_reber_classification(\n            n_samples=100,\n            return_indices=True,\n        )\n        data = add_sequence_padding(data + 1)  # +1 to shift indices\n\n        self.data = train_test_split(data, labels, test_size=0.2)\n        self.n_categories = len(reber.avaliable_letters) + 1\n        self.n_time_steps = self.data[0].shape[1]\n\n    def train_lstm(self, data, verbose=False, **lstm_options):\n        x_train, x_test, y_train, y_test = data\n        network = algorithms.RMSProp(\n            [\n                layers.Input(self.n_time_steps),\n                layers.Embedding(self.n_categories, 10),\n                layers.LSTM(20, **lstm_options),\n                layers.Sigmoid(1),\n            ],\n\n            step=0.05,\n            verbose=verbose,\n            batch_size=16,\n            loss='binary_crossentropy',\n        )\n        network.train(x_train, y_train, x_test, y_test, epochs=20)\n\n        y_predicted = network.predict(x_test).round()\n        accuracy = (y_predicted.T == y_test).mean()\n        return accuracy\n\n    def test_lstm_with_gradient_clipping(self):\n        accuracy = self.train_lstm(self.data, gradient_clipping=1)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_simple_lstm_sequence_classification(self):\n        accuracy = self.train_lstm(self.data)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_lstm_with_enabled_peepholes_option(self):\n        accuracy = self.train_lstm(self.data, peepholes=True)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_lstm_with_enabled_unroll_scan_option(self):\n        accuracy = self.train_lstm(self.data, unroll_scan=True)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_lstm_with_enabled_backwards_option(self):\n        x_train, x_test, y_train, y_test = self.data\n        x_train = x_train[:, ::-1]\n        x_test = x_test[:, ::-1]\n\n        data = x_train, x_test, y_train, y_test\n        accuracy = self.train_lstm(data, backwards=True, unroll_scan=False)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n        accuracy = self.train_lstm(data, backwards=True, unroll_scan=True)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_lstm_output_shapes(self):\n        network_1 = layers.join(\n            layers.Input((10, 2)),\n            layers.LSTM(20, only_return_final=True),\n        )\n        self.assertShapesEqual(network_1.output_shape, (None, 20))\n\n        network_2 = layers.join(\n            layers.Input((10, 2)),\n            layers.LSTM(20, only_return_final=False),\n        )\n        self.assertShapesEqual(network_2.output_shape, (None, 10, 20))\n\n    def test_stacked_lstm(self):\n        x_train, x_test, y_train, y_test = self.data\n        network = algorithms.RMSProp(\n            [\n                layers.Input(self.n_time_steps),\n                layers.Embedding(self.n_categories, 10),\n                layers.LSTM(\n                    n_units=10,\n                    only_return_final=False,\n                    input_weights=init.Normal(0.1),\n                    hidden_weights=init.Normal(0.1),\n                ),\n                layers.LSTM(\n                    n_units=2,\n                    input_weights=init.Normal(0.1),\n                    hidden_weights=init.Normal(0.1),\n                ),\n                layers.Sigmoid(1),\n            ],\n            step=0.05,\n            verbose=False,\n            batch_size=1,\n            loss='binary_crossentropy',\n        )\n        network.train(x_train, y_train, x_test, y_test, epochs=20)\n\n        y_predicted = network.predict(x_test).round()\n        accuracy = (y_predicted.T == y_test).mean()\n\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_stacked_lstm_with_enabled_backwards_option(self):\n        x_train, x_test, y_train, y_test = self.data\n        x_train = x_train[:, ::-1]\n        x_test = x_test[:, ::-1]\n\n        network = algorithms.RMSProp(\n            [\n                layers.Input(self.n_time_steps),\n                layers.Embedding(self.n_categories, 10),\n                layers.LSTM(10, only_return_final=False, backwards=True),\n                layers.LSTM(2, backwards=True),\n                layers.Sigmoid(1),\n            ],\n\n            step=0.02,\n            verbose=False,\n            batch_size=1,\n            loss='binary_crossentropy',\n        )\n        network.train(x_train, y_train, x_test, y_test, epochs=20)\n\n        y_predicted = network.predict(x_test).round()\n        accuracy = (y_predicted.T == y_test).mean()\n\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_lstm_connection_exceptions(self):\n        network = layers.join(layers.LSTM(10), layers.Reshape())\n\n        with self.assertRaises(LayerConnectionError):\n            layers.join(layers.Input(1), network)\n\n\nclass GRUTestCase(BaseTestCase):\n    def setUp(self):\n        super(GRUTestCase, self).setUp()\n\n        data, labels = reber.make_reber_classification(\n            n_samples=100,\n            return_indices=True,\n        )\n        data = add_sequence_padding(data + 1)  # +1 to shift indices\n\n        self.data = train_test_split(data, labels, test_size=0.2)\n        self.n_categories = len(reber.avaliable_letters) + 1\n        self.n_time_steps = self.data[0].shape[1]\n\n    def train_gru(self, data, **gru_options):\n        x_train, x_test, y_train, y_test = data\n        network = algorithms.RMSProp(\n            [\n                layers.Input(self.n_time_steps),\n                layers.Embedding(self.n_categories, 10),\n                layers.GRU(20, **gru_options),\n                layers.Sigmoid(1),\n            ],\n            step=0.05,\n            verbose=False,\n            batch_size=16,\n            loss='binary_crossentropy',\n        )\n        network.train(x_train, y_train, x_test, y_test, epochs=20)\n\n        y_predicted = network.predict(x_test).round()\n        accuracy = (y_predicted.T == y_test).mean()\n        return accuracy\n\n    def test_simple_gru_sequence_classification(self):\n        accuracy = self.train_gru(self.data)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_simple_gru_without_precomputed_input(self):\n        accuracy = self.train_gru(self.data)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_gru_with_gradient_clipping(self):\n        accuracy = self.train_gru(self.data, gradient_clipping=1)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_gru_with_enabled_unroll_scan_option(self):\n        accuracy = self.train_gru(self.data, unroll_scan=True)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_gru_with_enabled_backwards_option(self):\n        x_train, x_test, y_train, y_test = self.data\n        x_train = x_train[:, ::-1]\n        x_test = x_test[:, ::-1]\n\n        data = x_train, x_test, y_train, y_test\n        accuracy = self.train_gru(data, backwards=True, unroll_scan=False)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n        accuracy = self.train_gru(data, backwards=True, unroll_scan=True)\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_gru_output_shapes(self):\n        network_1 = layers.join(\n            layers.Input((10, 2)),\n            layers.GRU(20, only_return_final=True),\n        )\n        self.assertShapesEqual(network_1.output_shape, (None, 20))\n\n        network_2 = layers.join(\n            layers.Input((10, 2)),\n            layers.GRU(20, only_return_final=False),\n        )\n        self.assertShapesEqual(network_2.output_shape, (None, 10, 20))\n\n    def test_stacked_gru(self):\n        x_train, x_test, y_train, y_test = self.data\n        network = algorithms.RMSProp(\n            [\n                layers.Input(self.n_time_steps),\n                layers.Embedding(self.n_categories, 10),\n                layers.GRU(10, only_return_final=False),\n                layers.GRU(1),\n                layers.Sigmoid(1),\n            ],\n\n            step=0.01,\n            verbose=False,\n            batch_size=1,\n            loss='binary_crossentropy',\n        )\n        network.train(x_train, y_train, x_test, y_test, epochs=10)\n\n        y_predicted = network.predict(x_test).round()\n        accuracy = (y_predicted.T == y_test).mean()\n\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_stacked_gru_with_enabled_backwards_option(self):\n        x_train, x_test, y_train, y_test = self.data\n        x_train = x_train[:, ::-1]\n        x_test = x_test[:, ::-1]\n\n        network = algorithms.RMSProp(\n            [\n                layers.Input(self.n_time_steps),\n                layers.Embedding(self.n_categories, 10),\n                layers.GRU(10, only_return_final=False, backwards=True),\n                layers.GRU(2, backwards=True),\n                layers.Sigmoid(1),\n            ],\n\n            step=0.01,\n            verbose=False,\n            batch_size=10,\n            loss='binary_crossentropy',\n        )\n        network.train(x_train, y_train, x_test, y_test, epochs=20)\n\n        y_predicted = network.predict(x_test).round()\n        accuracy = (y_predicted.T == y_test).mean()\n\n        self.assertGreaterEqual(accuracy, 0.8)\n\n    def test_gru_connection_exceptions(self):\n        network = layers.join(layers.GRU(10), layers.Reshape())\n\n        with self.assertRaises(LayerConnectionError):\n            layers.join(layers.Input(1), network)\n"""
tests/layers/test_reshape.py,0,"b'import numpy as np\n\nfrom neupy import layers\nfrom neupy.utils import asfloat, tf_utils\nfrom neupy.exceptions import LayerConnectionError\n\nfrom base import BaseTestCase\n\n\nclass ReshapeLayerTestCase(BaseTestCase):\n    def test_reshape_layer_1d_shape(self):\n        x = np.random.random((5, 4, 3, 2, 1))\n        network = layers.Input((4, 3, 2, 1)) >> layers.Reshape()\n\n        y = self.eval(network.output(x))\n        self.assertEqual(y.shape, (5, 4 * 3 * 2 * 1))\n\n    def test_reshape_layer_2d_shape(self):\n        x = np.random.random((5, 20))\n\n        input_layer = layers.Input(20)\n        reshape_layer = layers.Reshape((4, 5))\n        input_layer > reshape_layer\n\n        y = self.eval(reshape_layer.output(x))\n        self.assertEqual(y.shape, (5, 4, 5))\n        self.assertShapesEqual(reshape_layer.output_shape, (None, 4, 5))\n\n    def test_reshape_unknown_shape(self):\n        network = layers.join(\n            layers.Input((None, 20)),\n            layers.Reshape(),\n        )\n        self.assertShapesEqual(network.output_shape, (None, None))\n\n        x = np.random.random((7, 12, 20))\n        y = self.eval(network.output(x))\n        self.assertEqual(y.shape, (7, 12 * 20))\n\n    def test_reshape_with_negative_value(self):\n        network = layers.join(\n            layers.Input((7, 20)),\n            layers.Reshape((5, -1)),\n        )\n        self.assertShapesEqual(network.output_shape, (None, 5, 28))\n\n        x = np.random.random((11, 7, 20))\n        y = self.eval(network.output(x))\n        self.assertEqual(y.shape, (11, 5, 28))\n\n    def test_reshape_with_negative_value_unknown_in_shape(self):\n        network = layers.join(\n            layers.Input((7, None)),\n            layers.Reshape([5, -1]),\n        )\n        self.assertShapesEqual(network.output_shape, (None, 5, None))\n\n        x = np.random.random((11, 7, 10))\n        y = self.eval(network.output(x))\n        self.assertEqual(y.shape, (11, 5, 14))\n\n    def test_reshape_exceptions(self):\n        with self.assertRaisesRegexp(ValueError, ""Only single""):\n            layers.Reshape([-1, -1])\n\n        with self.assertRaisesRegexp(ValueError, ""are incompatible""):\n            layers.join(\n                layers.Input(20),\n                layers.Reshape((-1, 6)),\n            )\n\n    def test_reshape_repr(self):\n        layer = layers.Reshape()\n        self.assertEqual(\n            ""Reshape((-1,), name=\'reshape-1\')"",\n            str(layer))\n\n        layer = layers.Reshape((5, 2), name=\'reshape-layer\')\n        self.assertEqual(\n            ""Reshape((5, 2), name=\'reshape-layer\')"",\n            str(layer))\n\n    def test_partially_defined_input_shape(self):\n        network = layers.join(\n            layers.Input((None, None, 3)),\n            layers.Convolution((3, 3, 5)),\n            layers.Reshape((-1, 5)),\n        )\n\n        self.assertShapesEqual(network.input_shape, (None, None, None, 3))\n        self.assertShapesEqual(network.output_shape, (None, None, 5))\n\n        x = network.inputs\n        y = network.outputs\n        session = tf_utils.tensorflow_session()\n\n        images = np.random.random((2, 10, 10, 3))\n        output = session.run(y, feed_dict={x: images})\n\n        self.assertEqual(output.shape, (2, 64, 5))\n\n\nclass TransposeTestCase(BaseTestCase):\n    def test_simple_transpose(self):\n        network = layers.join(\n            layers.Input((7, 11)),\n            layers.Transpose((0, 2, 1)),\n        )\n        self.assertShapesEqual(network.output_shape, (None, 11, 7))\n\n    def test_transpose_unknown_input_dim(self):\n        network = layers.join(\n            layers.Input((None, 10, 20)),\n            layers.Transpose((0, 2, 1, 3)),\n        )\n        self.assertShapesEqual(network.output_shape, (None, 10, None, 20))\n\n        value = asfloat(np.random.random((12, 100, 10, 20)))\n        output_value = self.eval(network.output(value))\n        self.assertEqual(output_value.shape, (12, 10, 100, 20))\n\n        value = asfloat(np.random.random((12, 33, 10, 20)))\n        output_value = self.eval(network.output(value))\n        self.assertEqual(output_value.shape, (12, 10, 33, 20))\n\n    def test_transpose_exceptions(self):\n        error_message = ""Cannot apply transpose operation to the input""\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(\n                layers.Input(20),\n                layers.Transpose((0, 2, 1)),\n            )\n\n    def test_transpose_repr(self):\n        layer = layers.Transpose((0, 2, 1))\n        self.assertEqual(\n            ""Transpose((0, 2, 1), name=\'transpose-1\')"",\n            str(layer))\n\n        layer = layers.Transpose((0, 2, 1), name=\'test\')\n        self.assertEqual(\n            ""Transpose((0, 2, 1), name=\'test\')"",\n            str(layer))\n\n    def test_transpose_undefined_input_shape(self):\n        network = layers.Transpose((1, 0, 2))\n        self.assertShapesEqual(network.input_shape, None)\n        self.assertShapesEqual(network.output_shape, (None, None, None))\n\n        network = layers.Transpose((1, 0))\n        self.assertShapesEqual(network.input_shape, None)\n        self.assertShapesEqual(network.output_shape, (None, None))\n'"
tests/layers/test_stochastic.py,0,"b'from scipy import stats\nimport numpy as np\n\nfrom neupy import layers\nfrom neupy.exceptions import LayerConnectionError\n\nfrom base import BaseTestCase\n\n\nclass DropoutLayerTestCase(BaseTestCase):\n    def test_dropout_layer(self):\n        test_input = np.ones((50, 20))\n        dropout_layer = layers.Dropout(proba=0.5)\n\n        y = dropout_layer.output(test_input, training=True)\n        layer_output = self.eval(y)\n\n        self.assertGreater(layer_output.sum(), 900)\n        self.assertLess(layer_output.sum(), 1100)\n\n        self.assertTrue(np.all(\n            np.bitwise_or(layer_output == 0, layer_output == 2)\n        ))\n\n    def test_dropout_disable_training_state(self):\n        test_input = np.ones((50, 20))\n        dropout_layer = layers.Dropout(proba=0.5)\n        layer_output = dropout_layer.output(test_input)\n        np.testing.assert_array_equal(layer_output, test_input)\n\n    def test_dropout_repr(self):\n        layer = layers.Dropout(0.5)\n        self.assertEqual(\n            ""Dropout(proba=0.5, name=\'dropout-1\')"",\n            str(layer))\n\n\nclass GaussianNoiseLayerTestCase(BaseTestCase):\n    def test_gaussian_noise_layer(self):\n        test_input = np.zeros((50, 20))\n        gauss_noise = layers.GaussianNoise(std=0.5)\n\n        layer_output = self.eval(gauss_noise.output(test_input, training=True))\n        self.assertTrue(stats.mstats.normaltest(layer_output))\n\n    def test_gaussian_noise_disable_training_state(self):\n        test_input = np.ones((50, 20))\n        gauss_noise = layers.GaussianNoise(std=1)\n        layer_output = gauss_noise.output(test_input)\n        np.testing.assert_array_equal(layer_output, test_input)\n\n    def test_gaussian_noise_repr(self):\n        layer = layers.GaussianNoise(0, 1)\n        self.assertEqual(\n            ""GaussianNoise(mean=0, std=1, name=\'gaussian-noise-1\')"",\n            str(layer))\n\n\nclass DropBlockTestCase(BaseTestCase):\n    def test_drop_block(self):\n        test_input = np.ones((2, 100, 100, 1))\n        dropblock_layer = layers.DropBlock(keep_proba=0.9, block_size=(2, 10))\n        output = dropblock_layer.output(test_input, training=True)\n        actual_output = self.eval(output)\n\n        fraction_masked = np.mean(actual_output != 0)\n\n        self.assertTrue(0.88 <= fraction_masked <= 0.92)\n        self.assertGreater(actual_output.max(), 1)\n        self.assertAlmostEqual(\n            actual_output.max(),\n            1 / fraction_masked,\n        )\n\n    def test_drop_block_during_inference(self):\n        test_input = np.ones((1, 20, 20, 1))\n        dropblock_layer = layers.DropBlock(keep_proba=0.9, block_size=5)\n        actual_output = dropblock_layer.output(test_input, training=False)\n\n        self.assertIs(test_input, actual_output)\n        self.assertEqual(dropblock_layer.block_size, (5, 5))\n\n    def test_drop_block_shape(self):\n        network = layers.join(\n            layers.Input((10, 10, 3)),\n            layers.DropBlock(0.9, block_size=5),\n        )\n        self.assertShapesEqual(network.output_shape, (None, 10, 10, 3))\n\n    def test_drop_block_invalid_connection(self):\n        err_message = ""DropBlock layer expects input with 4 dimensions""\n        with self.assertRaisesRegexp(LayerConnectionError, err_message):\n            layers.join(\n                layers.Input((10, 10)),\n                layers.DropBlock(0.9, block_size=5),\n            )\n'"
tests/plots/__init__.py,0,b''
tests/plots/test_hinton.py,0,"b'import pytest\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom neupy import plots\n\nfrom base import BaseTestCase\n\n\nclass HintonDiagramTestCase(BaseTestCase):\n    single_thread = True\n\n    @pytest.mark.mpl_image_compare\n    def test_simple_hinton(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n        plt.sca(ax)  # To test the case when ax=None\n\n        weight = np.random.randn(20, 20)\n        plots.hinton(weight, add_legend=True)\n\n        return fig\n\n    @pytest.mark.mpl_image_compare\n    def test_max_weight(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n\n        weight = 100 * np.random.randn(20, 20)\n        plots.hinton(weight, ax=ax, max_weight=10, add_legend=True)\n\n        return fig\n\n    @pytest.mark.mpl_image_compare\n    def test_hinton_without_legend(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n\n        weight = np.random.randn(20, 20)\n        plots.hinton(weight, ax=ax, add_legend=False)\n\n        return fig\n\n    @pytest.mark.mpl_image_compare\n    def test_hinton_only_positive(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n\n        weight = np.random.random((20, 20))\n        plots.hinton(weight, ax=ax)\n\n        return fig\n\n    @pytest.mark.mpl_image_compare\n    def test_hinton_only_negative(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n\n        weight = -np.random.random((20, 20))\n        plots.hinton(weight, ax=ax)\n\n        return fig\n\n    @pytest.mark.mpl_image_compare\n    def test_hinton_1darray(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n\n        weight = -np.random.randn(20)\n        plots.hinton(weight, ax=ax)\n\n        return fig\n'"
tests/plots/test_saliency_map.py,0,"b'import mock\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom neupy import plots, layers, algorithms\nfrom neupy.exceptions import InvalidConnection\n\nfrom base import BaseTestCase\n\n\nclass SaliencyMapTestCase(BaseTestCase):\n    single_thread = True\n\n    def setUp(self):\n        super(SaliencyMapTestCase, self).setUp()\n        self.network = layers.join(\n            layers.Input((28, 28, 3)),\n            layers.Convolution((3, 3, 8), name=\'conv\') >> layers.Relu(),\n            layers.Reshape(),\n            layers.Softmax(10),\n        )\n        self.image = np.ones((28, 28, 3))\n\n    def test_saliency_map_invalid_mode(self):\n        message = ""\'invalid-mode\' is invalid value for mode argument""\n\n        with self.assertRaisesRegexp(ValueError, message):\n            plots.saliency_map(self.network, self.image, mode=\'invalid-mode\')\n\n    def test_saliency_map_invalid_n_outputs(self):\n        new_network = layers.join(\n            self.network,\n            layers.parallel(\n                layers.Sigmoid(1),\n                layers.Sigmoid(2),\n            )\n        )\n        message = (\n            ""Cannot build saliency map for the network that ""\n            ""has more than one output layer.""\n        )\n        with self.assertRaisesRegexp(InvalidConnection, message):\n            plots.saliency_map(new_network, self.image)\n\n    def test_saliency_map_invalid_n_inputs(self):\n        new_network = layers.join(\n            layers.parallel(\n                layers.Input((28, 28, 3)),\n                layers.Input((28, 28, 3)),\n            ),\n            layers.Concatenate(),\n            self.network.start(\'conv\'),\n        )\n        message = (\n            ""Cannot build saliency map for the network that ""\n            ""has more than one input layer.""\n        )\n        with self.assertRaisesRegexp(InvalidConnection, message):\n            plots.saliency_map(new_network, self.image)\n\n    def test_saliency_map_invalid_input_image(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(),\n        )\n        message = (\n            ""Input layer has to be 4 dimensions, but network expects ""\n            ""2 dimensional input""\n        )\n        with self.assertRaisesRegexp(InvalidConnection, message):\n            plots.saliency_map(network, self.image)\n\n        message = (\n            ""Invalid image shape. Image expected to be 3D, got 2D image""\n        )\n        with self.assertRaisesRegexp(ValueError, message):\n            plots.saliency_map(self.network, np.ones((28, 28)))\n\n    def test_saliency_maps(self):\n        events = []\n        original_gca = plt.gca\n\n        def mocked_imshow(array, *args, **kwargs):\n            events.append(\'imshow\')\n            self.assertEqual(array.shape, (28, 28))\n\n        def mocked_show(*args, **kwargs):\n            events.append(\'show\')\n\n        def mocked_gca(*args, **kwargs):\n            events.append(\'gca\')\n            return original_gca()\n\n        imshow_path = \'matplotlib.axes.Axes.imshow\'\n        with mock.patch(imshow_path, side_effect=mocked_imshow):\n            plots.saliency_map(\n                self.network, self.image,\n                mode=\'heatmap\', show=False,\n            )\n            self.assertSequenceEqual(events, [\'imshow\'])\n\n            plots.saliency_map(\n                self.network, self.image,\n                mode=\'raw\', show=False,\n            )\n            self.assertSequenceEqual(events, [\'imshow\', \'imshow\'])\n\n            with mock.patch(\'matplotlib.pyplot.show\', side_effect=mocked_show):\n                plots.saliency_map(self.network, self.image, show=True)\n                self.assertSequenceEqual(\n                    events, [\'imshow\', \'imshow\', \'imshow\', \'show\'])\n\n            with mock.patch(\'matplotlib.pyplot.gca\', side_effect=mocked_gca):\n                plots.saliency_map(self.network, self.image, show=False)\n                self.assertSequenceEqual(\n                    events,\n                    [\'imshow\', \'imshow\', \'imshow\', \'show\', \'gca\', \'imshow\'])\n\n            optimizer = algorithms.GradientDescent(self.network)\n            plots.saliency_map(optimizer, self.image, show=False)\n'"
tests/storage/__init__.py,0,b''
tests/storage/test_dict_storage.py,0,"b'import copy\n\nimport numpy as np\n\nfrom neupy.utils import asfloat\nfrom neupy import layers, storage\nfrom neupy.storage import (\n    validate_data_structure, InvalidFormat,\n    ParameterLoaderError, load_layer_parameter,\n)\n\nfrom base import BaseTestCase\n\n\nclass DictStorageTestCase(BaseTestCase):\n    maxDiff = 10000\n\n    def test_storage_invalid_input_type(self):\n        network = [\n            layers.Input(10),\n            layers.Relu(5),\n            layers.Relu(2),\n        ]\n        message = (\n            ""Invalid input type. Input should be ""\n            ""network or optimizer with network""\n        )\n        with self.assertRaisesRegexp(TypeError, message):\n            storage.save_dict(network)\n\n    def test_storage_save_dict(self):\n        network = layers.join(\n            layers.parallel([\n                layers.Input(2, name=\'input-1\'),\n                layers.PRelu(1, name=\'prelu\')\n            ], [\n                layers.Input(1, name=\'input-2\'),\n                layers.Sigmoid(4, name=\'sigmoid\'),\n                layers.BatchNorm(name=\'batch-norm\'),\n            ]),\n            layers.Concatenate(name=\'concatenate\'),\n            layers.Softmax(3, name=\'softmax\'),\n        )\n        dict_network = storage.save_dict(network)\n\n        expected_keys = (\'metadata\', \'layers\', \'graph\')\n        self.assertItemsEqual(expected_keys, dict_network.keys())\n\n        expected_metadata_keys = (\'created\', \'language\', \'library\', \'version\')\n        actual_metadata_keys = dict_network[\'metadata\'].keys()\n        self.assertItemsEqual(expected_metadata_keys, actual_metadata_keys)\n\n        self.assertEqual(len(dict_network[\'layers\']), 7)\n\n        expected_layers = [{\n            \'class_name\': \'Input\',\n            \'configs\': {\'name\': \'input-1\', \'shape\': (2,)},\n            \'name\': \'input-1\',\n        }, {\n            \'class_name\': \'PRelu\',\n            \'configs\': {\'alpha_axes\': (-1,), \'name\': \'prelu\', \'n_units\': 1},\n            \'name\': \'prelu\',\n        }, {\n            \'class_name\': \'Input\',\n            \'configs\': {\'name\': \'input-2\', \'shape\': (1,)},\n            \'name\': \'input-2\',\n        }, {\n            \'class_name\': \'Sigmoid\',\n            \'configs\': {\'name\': \'sigmoid\', \'n_units\': 4},\n            \'name\': \'sigmoid\',\n        }, {\n            \'class_name\': \'BatchNorm\',\n            \'configs\': {\n                \'alpha\': 0.1,\n                \'axes\': (0,),\n                \'epsilon\': 1e-05,\n                \'name\': \'batch-norm\'\n            },\n            \'name\': \'batch-norm\',\n        }, {\n            \'class_name\': \'Concatenate\',\n            \'configs\': {\'axis\': -1, \'name\': \'concatenate\'},\n            \'name\': \'concatenate\',\n        }, {\n            \'class_name\': \'Softmax\',\n            \'configs\': {\'name\': \'softmax\', \'n_units\': 3},\n            \'name\': \'softmax\',\n        }]\n        actual_layers = []\n        for i, layer in enumerate(dict_network[\'layers\']):\n            self.assertIn(\'parameters\', layer, msg=""Layer #"" + str(i))\n\n            layer = copy.deepcopy(layer)\n            del layer[\'parameters\']\n            actual_layers.append(layer)\n\n        self.assertEqual(actual_layers, expected_layers)\n\n    def test_storage_load_dict_using_names(self):\n        relu = layers.Relu(2, name=\'relu\')\n        network = layers.join(layers.Input(10), relu)\n\n        weight = np.ones((10, 2))\n        bias = np.ones((2,))\n\n        storage.load_dict(network, {\n            \'metadata\': {},  # avoided for simplicity\n            \'graph\': {},  # avoided for simplicity\n            # Input layer was avoided on purpose\n            \'layers\': [{\n                \'name\': \'relu\',\n                \'class_name\': \'Relu\',\n                \'configs\': {},\n                \'parameters\': {\n                    \'weight\': {\'trainable\': True, \'value\': weight},\n                    \'bias\': {\'trainable\': True, \'value\': bias},\n                }\n            }]\n        })\n\n        np.testing.assert_array_almost_equal(weight, self.eval(relu.weight))\n        np.testing.assert_array_almost_equal(bias, self.eval(relu.bias))\n\n    def test_storage_load_dict_using_wrong_names(self):\n        network = layers.join(\n            layers.Input(3),\n            layers.Relu(4, name=\'relu\'),\n            layers.Linear(5, name=\'linear\') >> layers.Relu(),\n            layers.Softmax(6, name=\'softmax\'),\n        )\n\n        storage.load_dict(network, {\n            \'metadata\': {},  # avoided for simplicity\n            \'graph\': {},  # avoided for simplicity\n            # Input layer was avoided on purpose\n            \'layers\': [{\n                \'name\': \'name-1\',\n                \'class_name\': \'Relu\',\n                \'configs\': {},\n                \'parameters\': {\n                    \'weight\': {\'trainable\': True, \'value\': np.ones((3, 4))},\n                    \'bias\': {\'trainable\': True, \'value\': np.ones((4,))},\n                }\n            }, {\n                \'name\': \'name-2\',\n                \'class_name\': \'Relu\',\n                \'configs\': {},\n                \'parameters\': {\n                    \'weight\': {\'trainable\': True, \'value\': np.ones((4, 5))},\n                    \'bias\': {\'trainable\': True, \'value\': np.ones((5,))},\n                }\n            }, {\n                \'name\': \'name-3\',\n                \'class_name\': \'Softmax\',\n                \'configs\': {},\n                \'parameters\': {\n                    \'weight\': {\'trainable\': True, \'value\': np.ones((5, 6))},\n                    \'bias\': {\'trainable\': True, \'value\': np.ones((6,))},\n                }\n            }]\n        }, load_by=\'order\', skip_validation=False)\n\n        relu = network.layer(\'relu\')\n        self.assertEqual(12, np.sum(self.eval(relu.weight)))\n        self.assertEqual(4, np.sum(self.eval(relu.bias)))\n\n        linear = network.layer(\'linear\')\n        self.assertEqual(20, np.sum(self.eval(linear.weight)))\n        self.assertEqual(5, np.sum(self.eval(linear.bias)))\n\n        softmax = network.layer(\'softmax\')\n        self.assertEqual(30, np.sum(self.eval(softmax.weight)))\n        self.assertEqual(6, np.sum(self.eval(softmax.bias)))\n\n    def test_storage_load_dict_invalid_number_of_paramters(self):\n        network = layers.join(\n            layers.Input(3),\n            layers.Relu(4, name=\'relu\'),\n            layers.Linear(5, name=\'linear\') > layers.Relu(),\n            layers.Softmax(6, name=\'softmax\'),\n        )\n        data = {\n            \'metadata\': {},  # avoided for simplicity\n            \'graph\': {},  # avoided for simplicity\n            # Input layer was avoided on purpose\n            \'layers\': [{\n                \'name\': \'name-1\',\n                \'class_name\': \'Relu\',\n                \'configs\': {},\n                \'parameters\': {\n                    \'weight\': {\n                        \'trainable\': True,\n                        \'value\': np.ones((3, 4))\n                    },\n                    \'bias\': {\'trainable\': True, \'value\': np.ones((4,))},\n                }\n            }]\n        }\n\n        with self.assertRaises(ParameterLoaderError):\n            storage.load_dict(network, data, ignore_missing=False)\n\n    def test_failed_loading_mode_for_storage(self):\n        network = layers.Input(2) >> layers.Sigmoid(1)\n\n        with self.assertRaisesRegexp(ValueError, ""Invalid value""):\n            storage.load_dict(network, {}, load_by=\'unknown\')\n\n    def test_failed_load_parameter_invalid_type(self):\n        sigmoid = layers.Sigmoid(1, bias=None)\n        network = layers.join(layers.Input(2), sigmoid)\n        network.create_variables()\n\n        with self.assertRaisesRegexp(ParameterLoaderError, ""equal to None""):\n            load_layer_parameter(sigmoid, {\n                \'parameters\': {\n                    \'bias\': {\n                        \'value\': np.array([[0]]),\n                        \'trainable\': True,\n                    },\n                },\n            })\n\n\nclass StoredDataValidationTestCase(BaseTestCase):\n    def test_stored_data_dict_format_basics(self):\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure([])\n\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({})\n\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\'layers\': {}})\n\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\'layers\': []})\n\n    def test_stored_data_layers_format(self):\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\'layers\': [[]]})\n\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\'layers\': [{\n                \'parameters\': {},\n            }]})\n\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\'layers\': [{\n                \'parameters\': {},\n            }]})\n\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\'layers\': [{\n                \'parameters\': {},\n            }]})\n\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\n                \'layers\': [{\n                    \'parameters\': [],  # wrong type\n                    \'name\': \'name\',\n                }]\n            })\n\n        result = validate_data_structure({\n            \'layers\': [{\n                \'parameters\': {},\n                \'name\': \'name\',\n            }]\n        })\n        self.assertIsNone(result)\n\n    def test_stored_data_parameters_format(self):\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\'layers\': [{\n                \'name\': \'name\',\n                \'parameters\': {\n                    \'weight\': np.ones((2, 3)),\n                }\n            }]})\n\n        with self.assertRaises(InvalidFormat):\n            validate_data_structure({\'layers\': [{\n                \'name\': \'name\',\n                \'parameters\': {\n                    \'weight\': {\n                        \'data\': np.ones((2, 3)),\n                    },\n                }\n            }]})\n\n        result = validate_data_structure({\'layers\': [{\n            \'name\': \'name\',\n            \'parameters\': {\n                \'weight\': {\n                    \'value\': np.ones((2, 3)),\n                    \'trainable\': True,\n                },\n            }\n        }]})\n        self.assertIsNone(result)\n\n    def test_basic_skip_validation(self):\n        network = layers.Input(10) >> layers.Relu(1)\n\n        with self.assertRaises(InvalidFormat):\n            storage.load_dict(network, {}, skip_validation=False)\n\n\nclass TransferLearningTestCase(BaseTestCase):\n    def test_transfer_learning_using_position(self):\n        network_pretrained = layers.join(\n            layers.Input(10),\n            layers.Elu(5),\n            layers.Elu(2, name=\'elu\'),\n            layers.Sigmoid(1),\n        )\n        network_new = layers.join(\n            layers.Input(10),\n            layers.Elu(5),\n            layers.Elu(2),\n        )\n        pretrained_layers_stored = storage.save_dict(network_pretrained)\n\n        with self.assertRaises(ParameterLoaderError):\n            storage.load_dict(\n                network_new,\n                pretrained_layers_stored,\n                load_by=\'names_or_order\',\n                ignore_missing=False)\n\n        storage.load_dict(\n            network_new,\n            pretrained_layers_stored,\n            load_by=\'names_or_order\',\n            ignore_missing=True)\n\n        random_input = asfloat(np.random.random((12, 10)))\n        new_network_output = self.eval(network_new.output(random_input))\n        pretrained_output = self.eval(\n            network_pretrained.end(\'elu\').output(random_input))\n\n        np.testing.assert_array_almost_equal(\n            pretrained_output, new_network_output)\n\n    def test_transfer_learning_using_names(self):\n        network_pretrained = layers.join(\n            layers.Input(10),\n            layers.Elu(5, name=\'elu-a\'),\n            layers.Elu(2, name=\'elu-b\'),\n            layers.Sigmoid(1),\n        )\n        network_new = layers.join(\n            layers.Input(10),\n            layers.Elu(5, name=\'elu-a\'),\n            layers.Elu(2, name=\'elu-b\'),\n            layers.Elu(8, name=\'elu-c\'),  # new layer\n        )\n        pretrained_layers_stored = storage.save_dict(network_pretrained)\n\n        storage.load_dict(\n            network_new,\n            pretrained_layers_stored,\n            load_by=\'names\',\n            skip_validation=False,\n            ignore_missing=True)\n\n        random_input = asfloat(np.random.random((12, 10)))\n\n        pretrained_output = self.eval(\n            network_pretrained.end(\'elu-b\').output(random_input))\n        new_network_output = self.eval(\n            network_new.end(\'elu-b\').output(random_input))\n\n        np.testing.assert_array_almost_equal(\n            pretrained_output, new_network_output)\n\n        pred = self.eval(network_new.output(random_input))\n        self.assertEqual(pred.shape, (12, 8))\n'"
tests/storage/test_hdf5_storage.py,0,"b'import json\nimport tempfile\n\nimport numpy as np\nfrom mock import patch\n\nfrom neupy import storage, layers\nfrom neupy.utils import asfloat\n\nfrom base import BaseTestCase\n\n\nclass HDF5StorageTestCase(BaseTestCase):\n    def test_simple_storage_hdf5(self):\n        network_1 = layers.join(\n            layers.Input(10),\n            layers.parallel(\n                layers.Sigmoid(5),\n                layers.Relu(5),\n            ),\n            layers.Elementwise(),\n        )\n        network_2 = layers.join(\n            layers.Input(10),\n            layers.parallel(\n                layers.Sigmoid(5),\n                layers.Relu(5),\n            ),\n            layers.Elementwise(),\n        )\n\n        random_input = asfloat(np.random.random((13, 10)))\n        random_output_1 = self.eval(network_1.output(random_input))\n        random_output_2_1 = self.eval(network_2.output(random_input))\n\n        # Outputs has to be different\n        self.assertFalse(np.any(random_output_1 == random_output_2_1))\n\n        with tempfile.NamedTemporaryFile() as temp:\n            storage.save_hdf5(network_1, temp.name)\n            storage.load_hdf5(network_2, temp.name)\n\n            random_output_2_2 = self.eval(\n                network_2.output(random_input))\n\n            np.testing.assert_array_almost_equal(\n                random_output_1, random_output_2_2)\n\n    def test_hdf5_storage_broken_attributes(self):\n        network = layers.Input(1) >> layers.Relu(2, name=\'relu\')\n        json_loads = json.loads\n\n        def break_json(value):\n            if value == \'""relu""\':\n                return json_loads(""{"")\n            return json_loads(value)\n\n        with tempfile.NamedTemporaryFile() as temp:\n            storage.save_hdf5(network, temp.name)\n\n            with patch(\'json.loads\', side_effect=break_json):\n                storage.load_hdf5(network, temp.name)\n'"
tests/storage/test_json_storage.py,0,"b'import tempfile\n\nimport numpy as np\n\nfrom neupy import storage, layers\nfrom neupy.utils import asfloat\n\nfrom base import BaseTestCase\n\n\nclass JSONStorageTestCase(BaseTestCase):\n    def test_json_storage(self):\n        connection_1 = layers.join(\n            layers.Input(10),\n            layers.parallel(\n                layers.Sigmoid(5),\n                layers.Relu(5),\n            ),\n            layers.Elementwise(),\n        )\n        connection_2 = layers.join(\n            layers.Input(10),\n            layers.parallel(\n                layers.Sigmoid(5),\n                layers.Relu(5),\n            ),\n            layers.Elementwise(),\n        )\n\n        random_input = asfloat(np.random.random((13, 10)))\n        random_output_1 = self.eval(connection_1.output(random_input))\n        random_output_2_1 = self.eval(connection_2.output(random_input))\n\n        # Outputs have to be different\n        self.assertFalse(np.any(random_output_1 == random_output_2_1))\n\n        with tempfile.NamedTemporaryFile() as temp:\n            storage.save_json(connection_1, temp.name)\n            storage.load_json(connection_2, temp.name)\n\n            random_output_2_2 = self.eval(\n                connection_2.output(random_input))\n\n            np.testing.assert_array_almost_equal(\n                random_output_1, random_output_2_2)\n'"
tests/storage/test_object_storage.py,0,"b'import tempfile\n\nimport dill\nimport numpy as np\nfrom sklearn import datasets, preprocessing\nfrom six.moves import cPickle as pickle\n\nfrom neupy import algorithms, layers, init\n\nfrom base import BaseTestCase\nfrom helpers import catch_stdout\n\n\nclass BasicStorageTestCase(BaseTestCase):\n    def test_simple_dill_storage(self):\n        bpnet = algorithms.GradientDescent(\n            [\n                layers.Input(2),\n                layers.Sigmoid(3),\n                layers.Sigmoid(1),\n            ],\n            step=0.25,\n            batch_size=None,\n        )\n        data, target = datasets.make_regression(n_features=2, n_targets=1)\n\n        data = preprocessing.MinMaxScaler().fit_transform(data)\n        target_scaler = preprocessing.MinMaxScaler()\n        target = target_scaler.fit_transform(target.reshape(-1, 1))\n\n        with tempfile.NamedTemporaryFile() as temp:\n            original_layers = bpnet.network.layers\n            test_layer_weights = self.eval(original_layers[1].weight)\n\n            dill.dump(bpnet, temp)\n            temp.file.seek(0)\n\n            restored_bpnet = dill.load(temp)\n            temp.file.seek(0)\n\n            restored_layers = restored_bpnet.network.layers\n\n            self.assertEqual(0.25, restored_bpnet.step)\n            self.assertEqual(3, len(restored_layers))\n            np.testing.assert_array_equal(\n                test_layer_weights,\n                self.eval(restored_layers[1].weight)\n            )\n\n            # Check if it\'s possible to recover training state\n            bpnet.train(data, target, epochs=5)\n            real_bpnet_error = bpnet.score(data, target)\n            updated_input_weight = self.eval(original_layers[1].weight)\n\n            dill.dump(bpnet, temp)\n            temp.file.seek(0)\n\n            restored_bpnet_2 = dill.load(temp)\n            temp.file.seek(0)\n\n            restored_layers_2 = restored_bpnet_2.network.layers\n            np.testing.assert_array_equal(\n                updated_input_weight,\n                self.eval(restored_layers_2[1].weight)\n            )\n\n            restored_bpnet_error = restored_bpnet_2.score(data, target)\n            self.assertEqual(real_bpnet_error, restored_bpnet_error)\n\n    def test_non_initialized_graph_storage(self):\n        network = layers.Relu(10) >> layers.Relu(2)  # no input layer\n\n        with tempfile.NamedTemporaryFile() as temp:\n            pickle.dump(network, temp)\n            temp.file.seek(0)\n\n            network_restored = pickle.load(temp)\n\n            self.assertFalse(network_restored.layers[0].frozen)\n            self.assertIsInstance(\n                network_restored.layers[0].weight,\n                init.Initializer,\n            )\n\n            # Loaded parmeters are not variables and we\n            # expect layer not to be frozen\n            self.assertFalse(network_restored.layers[1].frozen)\n            self.assertIsInstance(\n                network_restored.layers[1].weight,\n                init.Initializer,\n            )\n\n    def test_basic_storage(self):\n        X = np.random.random((100, 2))\n        y = np.random.random(100) > 0.5\n\n        # We keep verbose=True in order to see if value will\n        # be True when we restore it from the pickle object.\n        pnn = algorithms.PNN(std=0.123, verbose=True)\n        pnn.train(X, y)\n\n        stored_pnn = pickle.dumps(pnn)\n        loaded_pnn = pickle.loads(stored_pnn)\n\n        testcases = [\n            (\'pnn\', pnn),\n            (\'loaded_pnn\', loaded_pnn),\n        ]\n\n        for name, network in testcases:\n            print(""Test case name: {}"".format(name))\n\n            self.assertAlmostEqual(network.std, 0.123)\n            self.assertAlmostEqual(network.verbose, True)\n\n            with catch_stdout() as out:\n                network.logs.stdout = out\n                network.logs.write(""Test message"")\n                terminal_output = out.getvalue()\n                self.assertIn(""Test message"", terminal_output)\n\n        pnn_prediction = pnn.predict(X)\n        loaded_pnn_prediction = loaded_pnn.predict(X)\n\n        np.testing.assert_array_almost_equal(\n            loaded_pnn_prediction, pnn_prediction)\n'"
tests/storage/test_pickle_storage.py,0,"b""import os\nimport tempfile\n\nimport numpy as np\n\nfrom neupy import algorithms, layers, storage\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import StopTraining\n\nfrom base import BaseTestCase\nfrom helpers import simple_classification\n\n\nclass LayerStoragePickleTestCase(BaseTestCase):\n    def test_storage_pickle_save_conection_from_network(self):\n        network = algorithms.GradientDescent([\n            layers.Input(10),\n            layers.Sigmoid(5),\n            layers.Sigmoid(2),\n        ])\n\n        with tempfile.NamedTemporaryFile() as temp:\n            storage.save_pickle(network, temp.name)\n            temp.file.seek(0)\n\n            filesize_after = os.path.getsize(temp.name)\n            self.assertGreater(filesize_after, 0)\n\n    def test_simple_storage_pickle(self):\n        network_1 = layers.join(\n            layers.Input(10),\n            layers.Sigmoid(5),\n            layers.Sigmoid(2),\n        )\n        network_2 = layers.join(\n            layers.Input(10),\n            layers.Sigmoid(5),\n            layers.Sigmoid(2),\n        )\n\n        random_input = asfloat(np.random.random((13, 10)))\n        random_output_1 = self.eval(network_1.output(random_input))\n        random_output_2_1 = self.eval(network_2.output(random_input))\n\n        self.assertFalse(np.any(random_output_1 == random_output_2_1))\n\n        with tempfile.NamedTemporaryFile() as temp:\n            storage.save_pickle(network_1, temp.name)\n            storage.load_pickle(network_2, temp.name)\n            random_output_2_2 = self.eval(network_2.output(random_input))\n\n            np.testing.assert_array_almost_equal(\n                random_output_1, random_output_2_2)\n\n    def test_storage_pickle_save_load_save(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Sigmoid(5),\n            layers.Sigmoid(2),\n        )\n\n        with tempfile.NamedTemporaryFile() as temp:\n            storage.save_pickle(network, temp.name)\n            temp.file.seek(0)\n\n            filesize_first = os.path.getsize(temp.name)\n\n            storage.load_pickle(network, temp.name)\n\n        with tempfile.NamedTemporaryFile() as temp:\n            storage.save_pickle(network, temp.name)\n            temp.file.seek(0)\n\n            filesize_second = os.path.getsize(temp.name)\n\n        self.assertEqual(filesize_first, filesize_second)\n\n    def test_storage_pickle_save_and_load_during_the_training(self):\n        tempdir = tempfile.mkdtemp()\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        errors = {}\n\n        def on_epoch_end(network):\n            epoch = network.last_epoch\n            errors[epoch] = network.score(x_test, y_test)\n\n            if epoch == 4:\n                storage.load_pickle(\n                    network.network,\n                    os.path.join(tempdir, 'training-epoch-2'))\n                raise StopTraining('Stop training process after 4th epoch')\n            else:\n                storage.save_pickle(\n                    network.network,\n                    os.path.join(tempdir, 'training-epoch-{}'.format(epoch)))\n\n        gdnet = algorithms.GradientDescent(\n            network=[\n                layers.Input(10),\n                layers.Sigmoid(4),\n                layers.Sigmoid(1)\n            ],\n            signals=on_epoch_end,\n            batch_size=None,\n            step=0.5\n        )\n        gdnet.train(x_train, y_train)\n\n        validation_error = gdnet.score(x_test, y_test)\n\n        self.assertGreater(errors[2], errors[4])\n        self.assertAlmostEqual(validation_error, errors[2])\n        self.assertNotAlmostEqual(validation_error, errors[4])\n"""
tests/utils/__init__.py,0,b''
tests/utils/test_iters.py,0,"b""import numpy as np\n\nfrom neupy.utils import iters\nfrom neupy.utils.iters import (\n    average_batch_errors,\n    count_samples,\n    count_minibatches,\n)\n\nfrom base import BaseTestCase\n\n\nclass ItersUtilsTestCase(BaseTestCase):\n    def test_minibatches(self):\n        n_samples = 50\n        batch_size = 20\n        expected_shapes = [(20, 2), (20, 2), (10, 2)]\n\n        data = np.random.random((n_samples, 2))\n        batch_slices = list(iters.minibatches(data, batch_size))\n\n        for batch, expected_shape in zip(batch_slices, expected_shapes):\n            self.assertEqual(batch.shape, expected_shape)\n\n    def test_minibatches_unknown_batch_size(self):\n        data = np.random.random((24, 2))\n        iterbatches = iters.minibatches(data, batch_size=None, shuffle=False)\n\n        for index, batch in enumerate(iterbatches, start=1):\n            self.assertEqual(batch.shape, (24, 2))\n            self.assertIs(batch, data)\n\n        self.assertEqual(index, 1)\n\n    def test_minibatches_with_shuffle(self):\n        data = np.arange(24)\n        iterbatches = iters.minibatches(data, batch_size=12, shuffle=True)\n\n        collected_samples = []\n        for batch in iterbatches:\n            collected_samples.append(batch)\n\n        collected_samples = np.concatenate(collected_samples)\n        np.testing.assert_array_equal(data, sorted(collected_samples))\n        self.assertFalse(np.allclose(data == collected_samples, b=1e-7))\n\n    def test_minibatches_nested_inputs(self):\n        data = [np.arange(24)], np.arange(24)\n        iterbatches = iters.minibatches(data, batch_size=12, shuffle=False)\n\n        collected_samples = []\n        for batch in iterbatches:\n            collected_samples.append(batch)\n\n        batch_1 = np.arange(12)\n        batch_2 = np.arange(12, 24)\n\n        self.assertEqual(len(collected_samples), 2)\n        np.testing.assert_array_equal(collected_samples[0][0], [batch_1])\n        np.testing.assert_array_equal(collected_samples[0][1], batch_1)\n\n        np.testing.assert_array_equal(collected_samples[1][0], [batch_2])\n        np.testing.assert_array_equal(collected_samples[1][1], batch_2)\n\n    def test_minibatches_nested_inputs_with_nones(self):\n        data = [np.arange(24)], None\n        iterbatches = iters.minibatches(data, batch_size=12, shuffle=False)\n\n        collected_samples = []\n        for batch in iterbatches:\n            collected_samples.append(batch)\n\n        batch_1 = np.arange(12)\n        batch_2 = np.arange(12, 24)\n\n        self.assertEqual(len(collected_samples), 2)\n        np.testing.assert_array_equal(collected_samples[0][0], [batch_1])\n        np.testing.assert_array_equal(collected_samples[0][1], None)\n\n        np.testing.assert_array_equal(collected_samples[1][0], [batch_2])\n        np.testing.assert_array_equal(collected_samples[1][1], None)\n\n    def test_apply_batches_with_progressbar(self):\n        # So far we just make sure that test didn't trigger any error\n        # In the future, we need to check content of the terminal output\n        outputs = iters.apply_batches(\n            function=lambda x: x * 2,\n            inputs=np.arange(20),\n            batch_size=8,\n            show_progressbar=True,\n        )\n        self.assertEqual(len(outputs), 3)\n\n    def test_apply_batches(self):\n        def mse(y_actual, y_predicted):\n            return np.mean((y_actual - y_predicted) ** 2)\n\n        y_actual = np.arange(20)\n        y_predicted = np.ones(20) * 10\n\n        outputs = iters.apply_batches(\n            function=mse,\n            inputs=[y_actual, y_predicted],\n            batch_size=7,\n        )\n        np.testing.assert_array_almost_equal(\n            outputs, np.array([53, 4, 45.1666666]))\n\n        avg_loss = iters.apply_batches(\n            function=mse,\n            inputs=[y_actual, y_predicted],\n            batch_size=7,\n            average_outputs=True,\n        )\n        self.assertEqual(avg_loss, mse(y_actual, y_predicted))\n\n    def test_batch_average(self):\n        expected_error = 0.9  # or 225 / 250\n        actual_error = average_batch_errors([1, 1, 0.5], 250, 100)\n        self.assertAlmostEqual(expected_error, actual_error)\n\n        expected_error = 0.8  # or 240 / 300\n        actual_error = average_batch_errors([1, 1, 0.4], 300, 100)\n        self.assertAlmostEqual(expected_error, actual_error)\n\n    def test_count_samples_function(self):\n        x = np.random.random((10, 5))\n\n        self.assertEqual(count_samples(x), 10)\n        self.assertEqual(count_samples([x, x]), 10)\n        self.assertEqual(count_samples([[x], None]), 10)\n\n    def test_count_minibatches(self):\n        x = np.random.random((10, 5))\n\n        self.assertEqual(count_minibatches(x, batch_size=2), 5)\n        self.assertEqual(count_minibatches(x, batch_size=3), 4)\n        self.assertEqual(count_minibatches([x], batch_size=5), 2)\n        self.assertEqual(count_minibatches([[x], None], batch_size=4), 3)\n"""
tests/utils/test_misc.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nimport random\nimport pickle\nfrom collections import namedtuple\n\nimport numpy as np\n\nfrom neupy.utils.misc import as_tuple, AttributeKeyDict, reproducible\n\nfrom base import BaseTestCase\n\n\nclass MiscUtilsTestCase(BaseTestCase):\n    def test_attribute_key_dict(self):\n        attrdict = AttributeKeyDict(val1=\'hello\', val2=\'world\')\n\n        # Get\n        self.assertEqual(attrdict.val1, \'hello\')\n        self.assertEqual(attrdict.val2, \'world\')\n\n        with self.assertRaises(KeyError):\n            attrdict.unknown_variable\n\n        # Set\n        attrdict.new_value = \'test\'\n        self.assertEqual(attrdict.new_value, \'test\')\n\n        # Delete\n        del attrdict.val1\n        with self.assertRaises(KeyError):\n            attrdict.val1\n\n        # Storage\n        recovered = pickle.loads(pickle.dumps(attrdict))\n        self.assertDictEqual(dict(recovered), {\n            \'new_value\': \'test\', \'val2\': \'world\'})\n\n    def test_as_tuple(self):\n        Case = namedtuple(""Case"", ""input_args expected_output"")\n        testcases = (\n            Case(\n                input_args=(1, 2, 3),\n                expected_output=(1, 2, 3),\n            ),\n            Case(\n                input_args=(None, (1, 2, 3), None),\n                expected_output=(None, 1, 2, 3, None),\n            ),\n            Case(\n                input_args=((1, 2, 3), tuple()),\n                expected_output=(1, 2, 3),\n            ),\n            Case(\n                input_args=((1, 2, 3), [], (4, 5)),\n                expected_output=(1, 2, 3, 4, 5),\n            ),\n            Case(\n                input_args=((1, 2, 3), (4, 5, 3)),\n                expected_output=(1, 2, 3, 4, 5, 3),\n            ),\n        )\n\n        for testcase in testcases:\n            actual_output = as_tuple(*testcase.input_args)\n            self.assertEqual(\n                actual_output, testcase.expected_output,\n                msg=""Input args: {}"".format(testcase.input_args))\n\n    def test_reproducible_utils_math_library(self):\n        reproducible(seed=0)\n        x1 = random.random()\n\n        reproducible(seed=0)\n        x2 = random.random()\n\n        self.assertAlmostEqual(x1, x2)\n\n    def test_reproducible_utils_numpy_library(self):\n        reproducible(seed=0)\n        x1 = np.random.random((10, 10))\n\n        reproducible(seed=0)\n        x2 = np.random.random((10, 10))\n\n        np.testing.assert_array_almost_equal(x1, x2)\n'"
tests/utils/test_processing.py,3,"b'import numpy as np\nimport tensorflow as tf\nfrom scipy.sparse import csr_matrix\n\nfrom neupy.utils.processing import format_data, asfloat\n\nfrom base import BaseTestCase\n\n\nclass ProcessingUtilsTestCase(BaseTestCase):\n    def test_format_data(self):\n        # None input\n        self.assertEqual(format_data(None), None)\n\n        # Sparse data\n        sparse_matrix = csr_matrix((3, 4), dtype=np.int8)\n        formated_sparce_matrix = format_data(sparse_matrix)\n        self.assertIs(formated_sparce_matrix, sparse_matrix)\n        self.assertEqual(formated_sparce_matrix.dtype, sparse_matrix.dtype)\n\n        # Vector input\n        x = np.random.random(10)\n        formated_x = format_data(x, is_feature1d=True)\n        self.assertEqual(formated_x.shape, (10, 1))\n\n        x = np.random.random(10)\n        formated_x = format_data(x, is_feature1d=False)\n        self.assertEqual(formated_x.shape, (1, 10))\n\n    def test_asfloat(self):\n        # Sparse matrix\n        sparse_matrix = csr_matrix((3, 4), dtype=np.int8)\n        self.assertIs(sparse_matrix, asfloat(sparse_matrix))\n\n        # Numpy array-like elements\n        x = np.array([1, 2, 3], dtype=np.float32)\n        self.assertIs(x, asfloat(x))\n\n        x = np.array([1, 2, 3], dtype=np.int8)\n        self.assertIsNot(x, asfloat(x))\n\n        # Python list\n        x = [1, 2, 3]\n        self.assertEqual(asfloat(x).shape, (3,))\n\n        # Tensorfow variables\n        x = tf.placeholder(dtype=tf.int32)\n        self.assertNotEqual(x.dtype, tf.float32)\n        self.assertEqual(asfloat(x).dtype, tf.float32)\n'"
tests/utils/test_tf_utils.py,22,"b'import pytest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.errors import FailedPreconditionError\n\nfrom neupy import init\nfrom neupy.utils import tf_utils, asfloat\n\nfrom base import BaseTestCase\n\n\n@pytest.mark.parametrize(""in_shape,out_shape"", [\n    ((10,), (10,)),\n    ((10, 2), (20,)),\n    ((10, 2, 4), (80,)),\n])\ndef test_flatten(in_shape, out_shape):\n    X = np.random.random(in_shape)\n    Y = tf_utils.tensorflow_eval(tf_utils.flatten(X))\n    assert Y.shape == out_shape\n\n\nclass TFUtilsTestCase(BaseTestCase):\n    def test_outer(self):\n        actual = self.eval(tf_utils.outer(np.ones(10), np.ones(10)))\n        np.testing.assert_array_almost_equal(actual, np.ones((10, 10)))\n\n    def test_dot(self):\n        actual = self.eval(tf_utils.dot(\n            np.arange(10).astype(np.float32),\n            2 * np.arange(10).astype(np.float32),\n        ))\n        self.assertEqual(actual, 570)\n\n    def test_repeat(self):\n        matrix = np.array([\n            [1, 2],\n            [3, 4],\n        ])\n        actual = self.eval(tf_utils.repeat(matrix, (2, 3)))\n        expected = np.array([\n            [1, 1, 1, 2, 2, 2],\n            [1, 1, 1, 2, 2, 2],\n            [3, 3, 3, 4, 4, 4],\n            [3, 3, 3, 4, 4, 4],\n        ])\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_make_single_vector(self):\n        w1 = tf.Variable(np.ones((4, 3)))\n        b1 = tf.Variable(np.zeros((3,)))\n        w2 = tf.Variable(np.ones((3, 2)))\n\n        actual = self.eval(tf_utils.make_single_vector([w1, b1, w2]))\n        expected = np.array([1] * 12 + [0] * 3 + [1] * 6)\n\n        np.testing.assert_array_equal(actual, expected)\n\n    def test_setup_parameter_updates(self):\n        w1 = tf.Variable(np.ones((4, 3)))\n        b1 = tf.Variable(np.zeros((3,)))\n        w2 = tf.Variable(np.ones((3, 2)))\n\n        tf_utils.initialize_uninitialized_variables([w1, b1, w2])\n\n        updates = 2 * tf_utils.make_single_vector([w1, b1, w2]) + 1\n        updates = tf_utils.setup_parameter_updates([w1, b1, w2], updates)\n\n        sess = tf_utils.tensorflow_session()\n        for parameter, new_value in updates:\n            sess.run(parameter.assign(new_value))\n\n        np.testing.assert_array_almost_equal(\n            self.eval(w1),\n            3 * np.ones((4, 3)),\n        )\n        np.testing.assert_array_almost_equal(\n            self.eval(b1),\n            np.ones(3),\n        )\n        np.testing.assert_array_almost_equal(\n            self.eval(w2),\n            3 * np.ones((3, 2)),\n        )\n\n    def test_function_name_scope(self):\n        @tf_utils.function_name_scope\n        def new_variable():\n            return tf.Variable(0, name=\'myvar\')\n\n        variable = new_variable()\n        self.assertEqual(variable.name, \'new_variable/myvar:0\')\n\n    def test_class_method_name_scope(self):\n        class MyRelu(object):\n            @tf_utils.class_method_name_scope\n            def output(self):\n                return tf.Variable(0, name=\'weights\')\n\n        variable = MyRelu().output()\n        self.assertEqual(variable.name, \'MyRelu/weights:0\')\n\n    def test_function_without_updates(self):\n        x = tf.placeholder(name=\'x\', dtype=tf.float32)\n        w = tf.Variable(asfloat(np.random.random((4, 3))), name=\'w\')\n        b = tf.Variable(asfloat(np.random.random((3,))), name=\'b\')\n        y = tf.matmul(x, w) + b\n\n        prediction = tf_utils.function([x], y)\n        tf_utils.initialize_uninitialized_variables()\n\n        actual = prediction(np.random.random((7, 4)))\n        self.assertEqual(actual.shape, (7, 3))\n\n    def test_function_with_updates(self):\n        x = tf.placeholder(name=\'x\', dtype=tf.float32)\n        w = tf.Variable(asfloat(np.ones((4, 3))), name=\'w\')\n        b = tf.Variable(asfloat(np.ones((3,))), name=\'b\')\n        y = tf.matmul(x, w) + b\n\n        prediction = tf_utils.function([x], y, updates=[\n            (b, b - 0.5),\n            w.assign(w + 0.5),\n        ])\n        tf_utils.initialize_uninitialized_variables()\n\n        actual = prediction(np.random.random((7, 4)))\n        self.assertEqual(actual.shape, (7, 3))\n\n        np.testing.assert_array_almost_equal(\n            self.eval(w),\n            1.5 * np.ones((4, 3)),\n        )\n        np.testing.assert_array_almost_equal(\n            self.eval(b),\n            0.5 * np.ones((3,)),\n        )\n\n    def test_tensorflow_session_function(self):\n        sess_a = tf_utils.tensorflow_session()\n        sess_b = tf_utils.tensorflow_session()\n        self.assertIs(sess_a, sess_b)\n\n        sess_b.close()\n        sess_c = tf_utils.tensorflow_session()\n        self.assertIsNot(sess_b, sess_c)\n\n    def test_initialize_uninitialized_variables(self):\n        sess = tf_utils.tensorflow_session()\n\n        a = tf.Variable(np.ones((4, 3)), name=\'a\')\n        b = tf.Variable(np.ones((4, 3)), name=\'b\')\n        tf_utils.initialize_uninitialized_variables()\n        actual = sess.run(a + b)\n        np.testing.assert_array_almost_equal(actual, 2 * np.ones((4, 3)))\n\n        c = tf.Variable(np.ones((2, 3)), name=\'c\')\n        d = tf.Variable(np.ones((2, 3)), name=\'dx\')\n        tf_utils.initialize_uninitialized_variables([c])\n\n        with self.assertRaisesRegexp(FailedPreconditionError, ""value dx""):\n            sess.run(c + d)\n\n    def test_variable_creation(self):\n        weight = np.ones((3, 3))\n        var1 = tf_utils.create_variable(weight, name=\'var1\', shape=(3, 3))\n        self.assertShapesEqual(var1.shape, (3, 3))\n\n        var2 = tf_utils.create_variable(5, name=\'var2\', shape=(4, 3))\n        self.assertShapesEqual(var2.shape, (4, 3))\n        np.testing.assert_array_almost_equal(\n            self.eval(var2), 5 * np.ones((4, 3)))\n\n        initializer = init.Normal()\n        var3 = tf_utils.create_variable(initializer, name=\'var3\', shape=(4, 7))\n        self.assertShapesEqual(var3.shape, (4, 7))\n\n        weight = tf.Variable(np.ones((3, 3)), dtype=tf.float32)\n        var4 = tf_utils.create_variable(weight, name=\'var4\', shape=(3, 3))\n        self.assertShapesEqual(var4.shape, (3, 3))\n        self.assertIs(var4, weight)\n\n        weight = np.ones((3, 4))\n        with self.assertRaisesRegexp(ValueError, ""Cannot create variable""):\n            tf_utils.create_variable(weight, name=\'var5\', shape=(3, 3))\n\n        weight = tf.Variable(np.ones((4, 3)), dtype=tf.float32)\n        with self.assertRaisesRegexp(ValueError, ""Cannot create variable""):\n            tf_utils.create_variable(weight, name=\'var6\', shape=(3, 3))\n'"
examples/mlp/imdb_review_classification/loaddata.py,0,"b'import os\nimport re\nimport shutil\nimport tarfile\nfrom itertools import product\n\nimport requests\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom src.utils import create_logger, DATA_DIR, REVIEWS_FILE\n\n\nlogger = create_logger(__name__)\n\nARCHIVE_URL = (\'http://ai.stanford.edu/~amaas/data/\'\n               \'sentiment/aclImdb_v1.tar.gz\')\nEXTRACTED_DIRECTORY = os.path.join(DATA_DIR, \'aclImdb\')\nREVIEW_DATA_PATH = os.path.join(DATA_DIR, \'reviews\')\n\n\ndef download_file(url, filepath):\n    response = requests.get(url, stream=True)\n    logger.info(""Downloading {}"".format(url))\n\n    with open(filepath, ""wb"") as local_file:\n        local_file.write(response.content)\n\n    logger.info(\'File downloaded and saved succesfully\')\n\n\ndef remove_tags_from_text(text):\n    tags_regexp = re.compile(r\'<[^>]+>\')\n    return tags_regexp.sub(\'\', text)\n\n\nif __name__ == \'__main__\':\n    if not os.path.exists(DATA_DIR):\n        os.makedirs(DATA_DIR)\n\n    archive_name = os.path.basename(ARCHIVE_URL)\n    path_to_archive = os.path.join(DATA_DIR, archive_name)\n\n    if not os.path.exists(path_to_archive):\n        download_file(ARCHIVE_URL, path_to_archive)\n    else:\n        logger.info(""Archive {} has already downloaded"".format(archive_name))\n\n    if not os.path.exists(REVIEW_DATA_PATH):\n        logger.info(""Extracting files from the archive"")\n\n        with tarfile.open(path_to_archive) as tar_archive_file:\n            tar_archive_file.extractall(path=DATA_DIR)\n\n        shutil.move(EXTRACTED_DIRECTORY, REVIEW_DATA_PATH)\n    else:\n        logger.info(""Files have been already extracted."")\n\n    dataset_types = [\'train\', \'test\']\n    sentiment_types = [\'pos\', \'neg\']\n    data = []\n\n    for dataset_type, review in product(dataset_types, sentiment_types):\n        datapath = os.path.join(REVIEW_DATA_PATH, dataset_type, review)\n\n        desc = \'read {:<5s} and {}\'.format(dataset_type, review)\n        for filename in tqdm(os.listdir(datapath), leave=True, desc=desc):\n            filepath = os.path.join(datapath, filename)\n\n            with open(filepath, \'r\') as f:\n                raw_text = f.read()\n                text = remove_tags_from_text(raw_text)\n                data.append((text, dataset_type, review))\n\n    dataset = pd.DataFrame(data, columns=[\'text\', \'type\', \'sentiment\'])\n    logger.info(""Saving data in CSV file"")\n    dataset.to_csv(REVIEWS_FILE, sep=\'\\t\', index=False)\n'"
examples/mlp/imdb_review_classification/train_classifier.py,0,"b'import os\n\nimport dill\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom neupy import algorithms, layers, utils\n\nfrom src.word_embedding_nn import WordEmbeddingNN\nfrom src.preprocessing import TokenizeText, IgnoreUnknownWords\nfrom src.utils import (WORD_EMBEDDING_NN, NN_CLASSIFIER_MODEL, REVIEWS_FILE,\n                       create_logger)\n\n\nlogger = create_logger(__name__)\nutils.reproducible()\n\nif not os.path.exists(WORD_EMBEDDING_NN):\n    raise EnvironmentError(\n        ""Can\'t find NN model. File {} doesn\'t exist. Probably you ""\n        ""haven\'t train it yet. Run `train_word_embedding_nn.py` script.""\n        """".format(WORD_EMBEDDING_NN))\n\nlogger.info(""Reading data"")\ndata = pd.read_csv(REVIEWS_FILE, sep=\'\\t\')\n\nlogger.info(""Loading word embedding NN"")\nword2vec = WordEmbeddingNN.load(WORD_EMBEDDING_NN)\n\nprepare_data_pipeline = Pipeline([\n    (\'tokenize_texts\', TokenizeText(ignore_stopwords=False)),\n    (\'ignore_unknown_words\', IgnoreUnknownWords(dictionary=word2vec.vocab)),\n    (\'word_embedding\', word2vec),\n])\n\nclassifier = algorithms.RPROP(\n    [\n        layers.Input(100),\n        layers.Relu(200),\n        layers.Relu(50),\n        layers.Sigmoid(1),\n    ],\n    loss=\'binary_crossentropy\',\n    verbose=True,\n    shuffle_data=True,\n\n    maxstep=1,\n    minstep=1e-7,\n)\n\nlogger.info(""Preparing train data"")\ntrain_data = data[data.type == \'train\']\ntexts = train_data.text.values\nx_train = prepare_data_pipeline.transform(texts)\ny_train = (train_data.sentiment.values == \'pos\')\n\nlogger.info(""Preparing test data"")\ntest_data = data[data.type == \'test\']\ntexts = test_data.text.values\nx_test = prepare_data_pipeline.transform(texts)\ny_test = (test_data.sentiment.values == \'pos\')\n\nclassifier.train(x_train, y_train, x_test, y_test, epochs=35)\ny_test_predicted = classifier.predict(x_test).round()\n\nprint(metrics.classification_report(y_test_predicted, y_test))\nprint(metrics.confusion_matrix(y_test_predicted, y_test))\n\nwith open(NN_CLASSIFIER_MODEL, \'wb\') as f:\n    dill.dump(classifier, file=f)\n'"
examples/mlp/imdb_review_classification/train_word_embedding_nn.py,0,"b'import os\n\nimport pandas as pd\nfrom neupy import utils\n\nfrom src.word_embedding_nn import WordEmbeddingNN\nfrom src.preprocessing import TokenizeText\nfrom src.utils import create_logger, REVIEWS_FILE, WORD_EMBEDDING_NN\n\n\nlogger = create_logger(__name__)\nutils.reproducible()\n\nif not os.path.exists(REVIEWS_FILE):\n    raise EnvironmentError(""Cannot find reviews.csv file. Probably you ""\n                           ""haven\'t run `loadata.py` script yet."")\n\ndata = pd.read_csv(REVIEWS_FILE, sep=\'\\t\')\ntrain_data = data[data.type == \'train\']\ndocuments = train_data.text.values\n\nlogger.info(""Tokenizing train data"")\ntext_tokenizer = TokenizeText(ignore_stopwords=False)\nword2vec = WordEmbeddingNN(size=100, workers=4, min_count=5, window=10)\n\ntext = text_tokenizer.transform(documents)\n\nlogger.info(""Building vocabulary"")\nword2vec.build_vocab(text)\nword2vec.train(text, n_epochs=10)\n\nlogger.info(""Saving model into the {} file"".format(WORD_EMBEDDING_NN))\nword2vec.save(WORD_EMBEDDING_NN)\n'"
examples/rbfn/music_speech/__init__.py,0,b''
examples/rbfn/music_speech/getdata.py,0,"b'import os\nimport math\nimport random\nimport argparse\n\nimport numpy as np\nfrom scipy.io import wavfile\n\n\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\nmusic_dir = os.path.join(data_dir, \'music_wav\')\nspeech_dir = os.path.join(data_dir, \'speech_wav\')\n\nsplited_data_file = os.path.join(data_dir, \'splited_data.npz\')\n\ntrain_size = 0.85\n\nSPEECH = 0\nMUSIC = 1\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--seed\', \'-s\', default=None, dest=\'seed\',\n                    help=""This parameter makes results reproduceble"",\n                    type=int)\n\n\ndef train_test_data():\n    data = np.load(splited_data_file)\n    return data[\'x_train\'], data[\'x_test\'], data[\'y_train\'], data[\'y_test\']\n\n\nif __name__ == \'__main__\':\n    print(""Start read data"")\n    args = parser.parse_args()\n\n    if args.seed is not None:\n        np.random.seed(args.seed)\n        random.seed(args.seed)\n\n    x_train, x_test = [], []\n    y_train, y_test = [], []\n\n    for class_code, directory in enumerate([music_dir, speech_dir]):\n        filenames = os.listdir(directory)\n        n_train_samples = math.floor(len(filenames) * train_size)\n        train_filenames = random.sample(filenames, k=n_train_samples)\n\n        for filename in filenames:\n            full_filepath = os.path.join(directory, filename)\n            _, wav_vector = wavfile.read(full_filepath)\n\n            if filename in train_filenames:\n                x_train.append(wav_vector)\n            else:\n                x_test.append(wav_vector)\n\n        classes = np.repeat(class_code, len(filenames))\n        y_train = np.concatenate([y_train, classes[:n_train_samples]])\n        y_test = np.concatenate([y_test, classes[n_train_samples:]])\n\n    x_train = np.array(x_train)\n    x_test = np.array(x_test)\n\n    print(""Train data shape: {}"".format(x_train.shape))\n    print(""Test data shape: {}"".format(x_test.shape))\n\n    print(""Save data in file"")\n    np.savez(splited_data_file, x_train=x_train, x_test=x_test,\n             y_train=y_train, y_test=y_test)\n'"
examples/rbfn/music_speech/train.py,0,"b'""""""\nMusic/Speech classification using PNN\n-------------------------------------\n\nA similar dataset which was collected for the purposes of\nmusic/speech discrimination. The dataset consists of 120 tracks,\neach 30 seconds long. Each class (music/speech) has 60 examples.\nThe tracks are all 22050Hz Mono 16-bit audio files in .wav format.\n\nDataset page: http://marsyasweb.appspot.com/download/data_sets/\nDataset file: http://opihi.cs.uvic.ca/sound/music_speech.tar.gz\n""""""\nimport numpy as np\nfrom neupy import algorithms\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing, model_selection, metrics, decomposition\nimport matplotlib.pyplot as plt\nfrom librosa.feature import mfcc\n\nfrom getdata import train_test_data, parser\n\n\nplt.style.use(\'ggplot\')\nparser.add_argument(\'--pca\', \'-p\', dest=\'apply_pca\', default=False,\n                    action=\'store_true\',\n                    help=""Apply PCA for the train data set visualization"")\n\nx_train, x_test, y_train, y_test = train_test_data()\n\n\ndef extract_features(data, n_fft=2048):\n    data = data.astype(np.float32)\n\n    res = []\n    for row in data:\n        centroid = mfcc(row, n_fft=n_fft, sr=22050)\n        res.append([\n            np.min(centroid),\n            np.max(centroid),\n            np.median(centroid),\n        ])\n\n    return np.array(res)\n\n\nprint(""> Data preprocessing procedure"")\n\nargs = parser.parse_args()\n\nif args.seed is not None:\n    np.random.seed(args.seed)\n\nstd = 0.2\nn_fft = 128\n\nprint(""STD = {}"".format(std))\nprint(""#FFT = {}"".format(n_fft))\n\nscaler = preprocessing.MinMaxScaler()\nx_train = scaler.fit_transform(extract_features(x_train, n_fft=n_fft))\nx_test = scaler.transform(extract_features(x_test, n_fft=n_fft))\nx_train, y_train = shuffle(x_train, y_train)\n\nif args.apply_pca:\n    pca = decomposition.PCA(2)\n    plt.scatter(*pca.fit_transform(x_train).T, c=y_train, s=100)\n    plt.show()\n    print(""PCA explain {:.2%}"".format(pca.explained_variance_ratio_.sum()))\n\nprint(""\\n> Train prediction"")\n\nskf = model_selection.StratifiedKFold(n_splits=5)\nskf_iterator = skf.split(x_train, y_train)\nscores = []\n\nfor i, (train_index, test_index) in enumerate(skf_iterator, start=1):\n    print(""\\nK-fold #{}"".format(i))\n    pnnet = algorithms.PNN(std=std, verbose=False)\n\n    x_fold_train, x_fold_test = x_train[train_index], x_train[test_index]\n    y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n\n    pnnet.fit(x_fold_train, y_fold_train)\n    y_predicted = pnnet.predict(x_fold_test)\n    score = metrics.roc_auc_score(y_predicted, y_fold_test)\n    accurucy = metrics.accuracy_score(y_predicted, y_fold_test)\n    scores.append(score)\n\n    print(""ROC AUC score: {:.4f}"".format(score))\n    print(""Accurucy: {:.2%}"".format(accurucy))\n    print(metrics.confusion_matrix(y_predicted, y_fold_test))\n\nprint(""Average ROC AUC score: {:.4f}"".format(np.mean(scores)))\n\nprint(""\\n> Test prediction"")\npnnet = algorithms.PNN(std=std, verbose=False)\npnnet.fit(x_train, y_train)\ny_predicted = pnnet.predict(x_test)\ntest_accurucy = metrics.roc_auc_score(y_predicted, y_test)\nprint(""Test data accurucy: {:.4f}"".format(test_accurucy))\n'"
examples/reinforcement_learning/vin/__init__.py,0,b''
examples/reinforcement_learning/vin/evaluation.py,0,"b'from __future__ import division\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom neupy.utils import asfloat\n\n\nactions = [\n    lambda x, y: (x - 1, y),  # north\n    lambda x, y: (x + 1, y),  # south\n    lambda x, y: (x, y + 1),  # east\n    lambda x, y: (x, y - 1),  # west\n    lambda x, y: (x - 1, y + 1),  # north-east\n    lambda x, y: (x - 1, y - 1),  # north-west\n    lambda x, y: (x + 1, y + 1),  # south-east\n    lambda x, y: (x + 1, y - 1),  # south-west\n]\n\n\ndef invalid_coordinate(grid, position):\n    height, width = grid.shape\n    x_coord, y_coord = position\n    return not (0 <= x_coord < height) or not (0 <= y_coord < width)\n\n\ndef shortest_path_length(grid, start, finish):\n    if start == finish:\n        return 0\n\n    path_grid = np.zeros(grid.shape, dtype=int)\n    path_grid[start] = 1\n    positions = [start]\n\n    while positions:\n        current = positions.pop(0)\n        current_value = path_grid[current]\n\n        for action in actions:\n            next_coord = action(*current)\n\n            if invalid_coordinate(grid, next_coord):\n                continue\n\n            not_obstacle = grid[next_coord] != 1\n            not_observed = path_grid[next_coord] == 0\n\n            if not_obstacle and not_observed:\n                if next_coord == finish:\n                    return current_value\n\n                path_grid[next_coord] = current_value + 1\n                positions.append(next_coord)\n\n    # Impossible to generate path between two points\n    return None\n\n\ndef detect_trajectory(f_next_step, grid, coords, max_iter=30,\n                      return_none_if_failed=False):\n\n    target_coords = get_target_coords(grid)\n    input_grid = asfloat(np.expand_dims(grid, axis=0))\n    grid = grid[:, :, 0]\n\n    trajectory = [coords]\n    coord_x, coord_y = coords\n\n    for i in range(max_iter):\n        current_position = (coord_x, coord_y)\n\n        if target_coords == current_position:\n            # Network successfully reached desired destination.\n            break\n\n        if grid[current_position] == 1:\n            # Interrupt trajectory detection procedure, because\n            # current position is located on the obstacle.\n            break\n\n        step = f_next_step([\n            input_grid,\n            int_as_2d_array(coord_x),\n            int_as_2d_array(coord_y),\n        ])\n        step = np.argmax(step, axis=1)\n        action = actions[step[0]]\n\n        coord_x, coord_y = action(coord_x, coord_y)\n        trajectory.append((coord_x, coord_y))\n\n    if return_none_if_failed and trajectory[-1] != target_coords:\n        return None\n\n    return np.array(trajectory)\n\n\ndef int_as_2d_array(number):\n    return asfloat(np.array([[number]]))\n\n\ndef get_target_coords(gridworld_image):\n    goal_channel = gridworld_image[:, :, 1]\n    return np.unravel_index(goal_channel.argmax(), goal_channel.shape)\n\n\ndef evaluate_accuracy(predict, x_test, s1_test, s2_test):\n    n_grids = x_test.shape[0]\n    state_batch_size = s1_test.shape[1]\n\n    total = n_grids * state_batch_size\n    correct = 0\n    differences = 0\n    n_failed_cases = 0\n\n    data = tqdm(zip(x_test, s1_test, s2_test),\n                total=n_grids, desc=\'Evaluating accuracy\')\n\n    for grid, x_coords, y_coords in data:\n        finish = get_target_coords(grid)\n\n        for start in zip(x_coords, y_coords):\n            len_shortest = shortest_path_length(grid[:, :, 0], start, finish)\n\n            max_iter = 2 * len_shortest\n            trajectory = detect_trajectory(\n                predict, grid, start, max_iter,\n                return_none_if_failed=True,\n            )\n\n            if trajectory is not None:\n                # Trajectory includes starting position and -1 will make\n                # sure that we do not count it when we measure length of\n                # the trajectory\n                len_predicted = len(trajectory) - 1\n            else:\n                len_predicted = max_iter\n                n_failed_cases += 1\n\n            differences += len_predicted - len_shortest\n\n            if len_predicted == len_shortest:\n                correct += 1\n\n    accuracy = correct / total\n    loss = differences / total\n\n    print(""Number of failed cases: {:,}"".format(n_failed_cases))\n    print(""Success rate: {:.2%}"".format(accuracy))\n    print(""Prediction loss: {:.4f}"".format(loss))\n'"
examples/reinforcement_learning/vin/loaddata.py,0,"b'import pickle\nimport argparse\n\nimport scipy.io\nimport numpy as np\nfrom sklearn.utils import shuffle\nfrom neupy.utils import asfloat\n\nfrom settings import environments\n\n\ndef save_data(data, filepath):\n    with open(filepath, \'wb\') as f:\n        # Use protocol 2, for python 2 and 3 compatibility\n        return pickle.dump(data, f, protocol=2)\n\n\ndef load_data(filepath):\n    with open(filepath, \'rb\') as f:\n        return pickle.load(f)\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--seed\', \'-s\', type=int, default=0)\nparser.add_argument(\'--imsize\', \'-i\', choices=[8, 16, 28],\n                    type=int, required=True)\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    env = environments[args.imsize]\n    np.random.seed(args.seed)\n\n    matlab_data = scipy.io.loadmat(env[\'mat_file\'])\n    image_data = matlab_data[""batch_im_data""]\n    image_data = (image_data - 1) / 255.  # obstacles = 1, free zone = 0\n\n    value_data = matlab_data[""batch_value_data""]\n    s1_data = matlab_data[""state_x_data""].astype(\'int8\')\n    s2_data = matlab_data[""state_y_data""].astype(\'int8\')\n    y_data = matlab_data[""batch_label_data""].astype(\'int8\')\n\n    image_data = asfloat(image_data.reshape(-1, 1, *env[\'image_size\']))\n    value_data = asfloat(value_data.reshape(-1, 1, *env[\'image_size\']))\n\n    x_data = np.append(image_data, value_data, axis=1)\n    x_data = np.transpose(x_data, (0, 2, 3, 1))\n    n_samples = x_data.shape[0]\n\n    training_samples = int(6 / 7.0 * n_samples)\n\n    x_train, x_test = np.split(x_data, [training_samples])\n    s1_train, s1_test = np.split(s1_data, [training_samples])\n    s2_train, s2_test = np.split(s2_data, [training_samples])\n    y_train, y_test = np.split(y_data, [training_samples])\n\n    x_train, s1_train, s2_train, y_train = shuffle(\n        x_train, s1_train, s2_train, y_train)\n\n    save_data((x_train, s1_train, s2_train, y_train), env[\'train_data_file\'])\n    save_data((x_test, s1_test, s2_test, y_test), env[\'test_data_file\'])\n'"
examples/reinforcement_learning/vin/settings.py,0,"b""import os\nfrom neupy import algorithms\n\nCURRENT_DIR = os.path.abspath(os.path.dirname(__file__))\nDATA_DIR = os.path.join(CURRENT_DIR, 'data')\nMODELS_DIR = os.path.join(CURRENT_DIR, 'models')\n\nenvironments = {\n    8: dict(\n        mat_file=os.path.join(DATA_DIR, 'gridworld_8.mat'),\n        train_data_file=os.path.join(DATA_DIR, 'gridworld-8-train.pickle'),\n        test_data_file=os.path.join(DATA_DIR, 'gridworld-8-test.pickle'),\n        pretrained_network_file=os.path.join(\n            MODELS_DIR, 'pretrained-VIN-8.hdf5'),\n\n        input_image_shape=(8, 8, 2),\n        image_size=(8, 8),\n        k=10,\n        epochs=120,\n        steps={\n            30: 0.005,\n            60: 0.002,\n            90: 0.001,\n        },\n        training_options=dict(\n            step=0.01,\n            batch_size=36,\n        ),\n    ),\n    16: dict(\n        mat_file=os.path.join(DATA_DIR, 'gridworld_16.mat'),\n        train_data_file=os.path.join(DATA_DIR, 'gridworld-16-train.pickle'),\n        test_data_file=os.path.join(DATA_DIR, 'gridworld-16-test.pickle'),\n        pretrained_network_file=os.path.join(\n            MODELS_DIR, 'pretrained-VIN-16.hdf5'),\n\n        input_image_shape=(16, 16, 2),\n        image_size=(16, 16),\n        k=20,\n        epochs=120,\n        steps={\n            30: 0.005,\n            60: 0.002,\n            90: 0.001,\n        },\n        training_options=dict(\n            step=0.01,\n            batch_size=36,\n        ),\n    ),\n    28: dict(\n        mat_file=os.path.join(DATA_DIR, 'gridworld_28.mat'),\n        train_data_file=os.path.join(DATA_DIR, 'gridworld-28-train.pickle'),\n        test_data_file=os.path.join(DATA_DIR, 'gridworld-28-test.pickle'),\n        pretrained_network_file=os.path.join(\n            MODELS_DIR, 'pretrained-VIN-28.hdf5'),\n\n        input_image_shape=(28, 28, 2),\n        image_size=(28, 28),\n        k=36,\n        epochs=120,\n        steps={\n            30: 0.005,\n            60: 0.002,\n            90: 0.001,\n        },\n        training_options=dict(\n            decay=0.9,\n            epsilon=1e-6,\n            step=0.01,\n            batch_size=12,\n        )\n    ),\n}\n"""
examples/reinforcement_learning/vin/train_vin.py,18,"b'import os\nimport argparse\nfrom functools import partial\n\nimport tensorflow as tf\n\nfrom neupy import init, algorithms, storage\nfrom neupy.layers import *\nfrom neupy.utils import as_tuple, asfloat, tf_utils, tensorflow_session\n\nfrom loaddata import load_data\nfrom settings import MODELS_DIR, environments\nfrom evaluation import evaluate_accuracy\n\n\nUNKNOWN = None\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--imsize\', \'-i\', choices=[8, 16, 28],\n                    type=int, required=True)\n\n\ndef create_random_weight(shape):\n    initializer = init.Normal()\n    weight = initializer.sample(shape)\n    return tf.Variable(asfloat(weight), dtype=tf.float32)\n\n\nclass ChannelGlobalMaxPooling(BaseLayer):\n    def get_output_shape(self, input_shape):\n        return input_shape[:-1].concatenate(1)\n\n    def output(self, input_value, **kwargs):\n        return tf.reduce_max(input_value, axis=-1, keepdims=True)\n\n\nclass SelectValueAtStatePosition(BaseLayer):\n    def get_output_shape(self, Q_input_shape, state_shape_1, state_shape_2):\n        n_samples = Q_input_shape[0]\n        n_states = state_shape_1[1]\n        n_filters = Q_input_shape[-1]\n        return tf.TensorShape((n_samples * n_states, n_filters))\n\n    def output(self, Q, input_state_1, input_state_2, **kwargs):\n        with tf.name_scope(""Q-output""):\n            # Number of samples depend on the state\'s batch size.\n            # Each iteration we can try to predict direction from\n            # multiple different starting points at the same time.\n            input_shape = tf.shape(input_state_1)\n            n_states = input_shape[1]\n            Q_shape = tf.shape(Q)\n\n            indices = tf.stack([\n                # Number of repetitions depends on the size of\n                # the state batch\n                tf_utils.repeat(tf.range(Q_shape[0]), n_states),\n\n                # Each state is a coordinate (x and y)\n                # that point to some place on a grid.\n                tf.cast(tf_utils.flatten(input_state_1), tf.int32),\n                tf.cast(tf_utils.flatten(input_state_2), tf.int32),\n            ], axis=1)\n\n            # Output is a matrix that has n_samples * n_states rows\n            # and n_filters (which is Q.shape[1]) columns.\n            return tf.gather_nd(Q, indices)\n\n\ndef create_VIN(input_image_shape=(8, 8, 2), n_hidden_filters=150,\n               n_state_filters=10, k=10):\n\n    # Default initialization method\n    normal = init.Normal()\n\n    # Create shared weights\n    q_weight = create_random_weight((3, 3, 1, n_state_filters))\n    fb_weight = create_random_weight((3, 3, 1, n_state_filters))\n\n    # Define basic layers\n    SamePadConv = partial(Convolution, padding=\'SAME\', bias=None)\n\n    R = join(\n        Input(input_image_shape, name=\'grid-input\'),\n        SamePadConv((3, 3, n_hidden_filters), weight=normal, bias=normal),\n        SamePadConv((1, 1, 1), weight=normal),\n    )\n    Q = R >> SamePadConv((3, 3, n_state_filters), weight=q_weight)\n\n    for i in range(k):\n        V = Q >> ChannelGlobalMaxPooling()\n        Q = join(\n            # Convolve R and V separately and then add outputs together with\n            # the Elementwise layer. This part of the code looks different\n            # from the one that was used in the original VIN repo, but\n            # it does the same operation.\n            #\n            # conv(x, w) == (conv(x1, w1) + conv(x2, w2))\n            # where, x = concat(x1, x2)\n            #        w = concat(w1, w2)\n            #\n            # See code sample from Github Gist: https://bit.ly/2zm3ntN\n            parallel(\n                R >> SamePadConv((3, 3, n_state_filters), weight=q_weight),\n                V >> SamePadConv((3, 3, n_state_filters), weight=fb_weight),\n            ),\n            Elementwise(\'add\'),\n        )\n\n    input_state_1 = Input(UNKNOWN, name=\'state-1-input\')\n    input_state_2 = Input(UNKNOWN, name=\'state-2-input\')\n\n    # Select the conv-net channels at the state position (S1, S2)\n    VIN = (Q | input_state_1 | input_state_2) >> SelectValueAtStatePosition()\n\n    # Set up softmax layer that predicts actions base on (S1, S2)\n    # position. Each action encodes specific direction:\n    # N, S, E, W, NE, NW, SE, SW (in the same order)\n    VIN = VIN >> Softmax(8, bias=None, weight=normal)\n\n    return VIN\n\n\ndef loss_function(expected, predicted):\n    epsilon = 1e-7  # for 32-bit float\n\n    predicted = tf.clip_by_value(predicted, epsilon, 1.0 - epsilon)\n    expected = tf.cast(tf_utils.flatten(expected), tf.int32)\n\n    log_predicted = tf.log(predicted)\n    indices = tf.stack([tf.range(tf.size(expected)), expected], axis=1)\n    errors = tf.gather_nd(log_predicted, indices)\n\n    return -tf.reduce_mean(errors)\n\n\ndef on_epoch_end_from_steps(steps):\n    def on_epoch_end(network):\n        if network.last_epoch in steps:\n            print(""Saving pre-trained VIN model..."")\n            storage.save(network, env[\'pretrained_network_file\'])\n\n            new_step = steps[network.last_epoch]\n            session = tensorflow_session()\n            network.step.load(new_step, session)\n\n    return on_epoch_end\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    env = environments[args.imsize]\n\n    print(""Loading train and test data..."")\n    x_train, s1_train, s2_train, y_train = load_data(env[\'train_data_file\'])\n    x_test, s1_test, s2_test, y_test = load_data(env[\'test_data_file\'])\n\n    print(""Initializing VIN..."")\n    network = algorithms.RMSProp(\n        create_VIN(\n            env[\'input_image_shape\'],\n            n_hidden_filters=150,\n            n_state_filters=10,\n            k=env[\'k\'],\n        ),\n        verbose=True,\n\n        # Loss function applies categorical cross entropy\n        # in a bit more efficient way.\n        loss=loss_function,\n\n        # Shape of the target value might be different compare to the\n        # expected shape of the output. Without this change network will\n        # assume that target shape will be the same as network\'s output\n        # shape, which is (None, 8)\n        target=tf.placeholder(tf.float32, shape=(None, None)),\n\n        # Signal will ensure that step (learning rate) will be reduced\n        # after certain number of iterations\n        signals=on_epoch_end_from_steps(env[\'steps\']),\n        **env[\'training_options\']\n    )\n\n    print(""Training VIN..."")\n    network.train(\n        (x_train, s1_train, s2_train), y_train,\n        (x_test, s1_test, s2_test), y_test,\n        epochs=env[\'epochs\'],\n    )\n\n    if not os.path.exists(MODELS_DIR):\n        os.mkdir(MODELS_DIR)\n\n    print(""Saving pre-trained VIN model..."")\n    storage.save(network, env[\'pretrained_network_file\'])\n\n    print(""Evaluating accuracy on test set..."")\n    evaluate_accuracy(network.predict, x_test, s1_test, s2_test)\n'"
examples/reinforcement_learning/vin/visualize.py,0,"b'from itertools import product\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\nfrom neupy import storage\n\nfrom loaddata import load_data\nfrom train_vin import parser, create_VIN\nfrom settings import environments\nfrom evaluation import detect_trajectory\n\n\ndef plot_grid_and_trajectory(f_next_step, grid, coords):\n    image_shape = grid.shape[1:-1]\n    trajectory = detect_trajectory(f_next_step, grid[0], coords)\n\n    trajectory_grid = np.zeros(image_shape)\n    trajectory_grid[trajectory[:, 0], trajectory[:, 1]] = 1\n\n    start_position = np.zeros(image_shape)\n    start_position[coords[0], coords[1]] = 1\n\n    # Grid world map\n    plt.imshow(grid[0, :, :, 0], interpolation=\'none\', cmap=\'binary\')\n\n    # Trajectory\n    cmap = plt.cm.Reds\n    cmap.set_under(alpha=0)\n    plt.imshow(trajectory_grid, interpolation=\'none\',\n               cmap=cmap, clim=[0.1, 1.6])\n\n    # Start position\n    cmap = plt.cm.Reds\n    cmap.set_under(alpha=0)\n    plt.imshow(start_position, interpolation=\'none\',\n               cmap=cmap, clim=[0.1, 1.1])\n\n    # Goal position\n    cmap = plt.cm.Greens\n    cmap.set_under(alpha=0)\n    plt.imshow(grid[0, :, :, 1] / 10., interpolation=\'none\',\n               cmap=cmap, clim=[0.1, 1.1])\n\n    # Intercections between trajectories and obstacles\n    cmap = plt.cm.Blues\n    cmap.set_under(alpha=0)\n    plt.imshow(np.bitwise_and(trajectory_grid == 1, grid[0, :, :, 0] == 1),\n               interpolation=\'none\', cmap=cmap, clim=[0.1, 1.1])\n\n\ndef sample_random_position(grid):\n    obstacles_grid = grid[0, :, :, 0]\n    x_coords, y_coords = np.argwhere(obstacles_grid == 0).T\n    position = np.random.randint(x_coords.size)\n    return (x_coords[position], y_coords[position])\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    env = environments[args.imsize]\n\n    print(""Loading data..."")\n    x_test, _, _, _ = load_data(env[\'test_data_file\'])\n\n    print(""Initializing VIN..."")\n    VIN = create_VIN(\n        env[\'input_image_shape\'],\n        n_hidden_filters=150,\n        n_state_filters=10,\n        k=env[\'k\'],\n    )\n    print(""Loading pre-trained VIN parameterss..."")\n    storage.load(VIN, env[\'pretrained_network_file\'])\n\n    plt.figure(figsize=(8, 8))\n    gridspec = gridspec.GridSpec(5, 4, height_ratios=[0, 2, 2, 2, 2])\n    gridspec.update(wspace=0.1, hspace=0.1)\n\n    plt.suptitle(\'Trajectories between two points predicted by VIN \')\n\n    plt.subplot(gridspec[0, :])\n    plt.legend(\n        handles=[\n            mpatches.Patch(color=\'#A71C1B\', label=\'Start\'),\n            mpatches.Patch(color=\'#F35D47\', label=\'Trajectory\'),\n            mpatches.Patch(color=\'#007035\', label=\'Goal\'),\n        ],\n        loc=3,\n        ncol=3,\n        mode=""expand"",\n        borderaxespad=0.,\n        bbox_to_anchor=(0., 1.02, 1., .102),\n    )\n    plt.axis(\'off\')\n\n    print(""Visualizing random images..."")\n    for row, col in product(range(1, 5), range(4)):\n        example_id = np.random.randint(x_test.shape[0])\n\n        grid = np.expand_dims(x_test[example_id], axis=0)\n        coords = sample_random_position(grid)\n\n        plt.subplot(gridspec[row, col])\n        plot_grid_and_trajectory(VIN.predict, grid, coords)\n        plt.axis(\'off\')\n\n    print(""Images showed"")\n    plt.show()\n'"
neupy/algorithms/associative/__init__.py,0,b''
neupy/algorithms/associative/base.py,0,"b'import numpy as np\n\nfrom neupy import init\nfrom neupy.utils import format_data\nfrom neupy.core.properties import IntProperty, ParameterProperty, ArrayProperty\nfrom neupy.algorithms.base import BaseNetwork\n\n\n__all__ = (\'BaseStepAssociative\',)\n\n\nclass BaseAssociative(BaseNetwork):\n    """"""\n    Base class for associative learning.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of features (columns) in the input data.\n\n    n_outputs : int\n        Number of outputs in the  network.\n\n    weight : array-like, Initializer\n        Neural network weights.\n        Value defined manually should have shape ``(n_inputs, n_outputs)``.\n        Defaults to :class:`Normal() <neupy.init.Normal>`.\n\n    {BaseNetwork.Parameters}\n\n    Methods\n    -------\n    {BaseSkeleton.predict}\n\n    train(X_train, epochs=100)\n        Train neural network.\n\n    {BaseSkeleton.fit}\n    """"""\n    n_inputs = IntProperty(minval=1, required=True)\n    n_outputs = IntProperty(minval=1, required=True)\n    weight = ParameterProperty(default=init.Normal())\n\n    def __init__(self, **options):\n        super(BaseAssociative, self).__init__(**options)\n        self.init_weights()\n\n    def init_weights(self):\n        valid_weight_shape = (self.n_inputs, self.n_outputs)\n\n        if isinstance(self.weight, init.Initializer):\n            self.weight = self.weight.sample(\n                valid_weight_shape, return_array=True)\n\n        if self.weight.shape != valid_weight_shape:\n            raise ValueError(\n                ""Weight matrix has invalid shape. Got {}, expected {}""\n                """".format(self.weight.shape, valid_weight_shape))\n\n        self.weight = self.weight.astype(float)\n\n    def format_input_data(self, X):\n        X = format_data(X, is_feature1d=(self.n_inputs == 1))\n\n        if X.ndim != 2:\n            raise ValueError(\n                ""Cannot make prediction, because input ""\n                ""data has more than 2 dimensions"")\n\n        if X.shape[1] != self.n_inputs:\n            raise ValueError(\n                ""Input data expected to have {} features, ""\n                ""but got {}"".format(self.n_inputs, X.shape[1]))\n\n        return X\n\n    def train(self, X_train, epochs=100):\n        X_train = self.format_input_data(X_train)\n        return super(BaseAssociative, self).train(\n            X_train=X_train, epochs=epochs)\n\n\nclass BaseStepAssociative(BaseAssociative):\n    """"""\n    Base class for associative algorithms which have 2 layers and first\n    one is has step function as activation.\n\n    Parameters\n    ----------\n    {BaseAssociative.n_inputs}\n\n    {BaseAssociative.n_outputs}\n\n    n_unconditioned : int\n        Number of unconditioned units in neural networks. All these\n        units wouldn\'t update during the training procedure.\n        Unconditioned should be the first feature in the dataset.\n\n    weight : array-like\n        Neural network weights.\n        Value defined manually should have shape ``(n_inputs, n_outputs)``.\n        Defaults to ``None`` which means that all unconditional\n        weights will be equal to ``1``. Other weights equal to ``0``.\n\n    bias : array-like, Initializer\n        Neural network bias units.\n        Defaults to :class:`Constant(-0.5) <neupy.init.Constant>`.\n\n    {BaseNetwork.Parameters}\n\n    Methods\n    -------\n    {BaseAssociative.Methods}\n    """"""\n    n_inputs = IntProperty(minval=2, required=True)\n    n_unconditioned = IntProperty(minval=1, required=True)\n\n    weight = ArrayProperty()\n    bias = ParameterProperty(default=init.Constant(-0.5))\n\n    def init_weights(self):\n        if self.n_inputs <= self.n_unconditioned:\n            raise ValueError(\n                ""Number of unconditioned features should be less than total ""\n                ""number of features. `n_inputs`={} and `n_unconditioned`={}""\n                """".format(self.n_inputs, self.n_unconditioned))\n\n        valid_weight_shape = (self.n_inputs, self.n_outputs)\n        valid_bias_shape = (self.n_outputs,)\n\n        if self.weight is None:\n            self.weight = np.zeros(valid_weight_shape)\n            self.weight[:self.n_unconditioned, :] = 1\n\n        if isinstance(self.bias, init.Initializer):\n            self.bias = self.bias.sample(valid_bias_shape, return_array=True)\n\n        super(BaseStepAssociative, self).init_weights()\n\n        if self.bias.shape != valid_bias_shape:\n            raise ValueError(\n                ""Bias vector has invalid shape. Got {}, expected {}""\n                """".format(self.bias.shape, valid_bias_shape))\n\n        self.bias = self.bias.astype(float)\n\n    def predict(self, X):\n        X = format_data(X, is_feature1d=False)\n        raw_output = X.dot(self.weight) + self.bias\n        return np.where(raw_output > 0, 1, 0)\n\n    def train(self, X_train, *args, **kwargs):\n        X_train = format_data(X_train, is_feature1d=False)\n        return super(BaseStepAssociative, self).train(X_train, *args, **kwargs)\n\n    def one_training_update(self, X_train, y_train):\n        weight = self.weight\n        n_unconditioned = self.n_unconditioned\n        predict = self.predict\n        weight_delta = self.weight_delta\n\n        error = 0\n\n        for x_row in X_train:\n            x_row = np.expand_dims(x_row, axis=0)\n            layer_output = predict(x_row)\n\n            delta = weight_delta(x_row, layer_output)\n            weight[n_unconditioned:, :] += delta\n\n            # This error can tell us whether network has converged\n            # to some value of weights. Low errors will mean that weights\n            # hasn\'t been updated much during the training epoch.\n            error += np.linalg.norm(delta)\n\n        return error\n'"
neupy/algorithms/associative/hebb.py,0,"b'from neupy.core.properties import BoundedProperty\nfrom .base import BaseStepAssociative\n\n\n__all__ = (\'HebbRule\',)\n\n\nclass HebbRule(BaseStepAssociative):\n    """"""\n    Neural Network with Hebbian Learning. It\'s an unsupervised algorithm.\n    Network can learn associations from the data.\n\n    Notes\n    -----\n    - Network always generates weights that contains ``0``\n      weight for the conditioned stimulus and ``1`` for the other.\n      Such initialization helps to control your default state\n      for the feature learning.\n\n    Parameters\n    ----------\n    decay_rate : float\n        Decay rate controls network\'s weights. It helps network to\n        \'forget\' information and control weight\'s size. Without this\n        parameter network\'s weights will increase fast.\n        Defaults to ``0.2``.\n\n    {BaseStepAssociative.Parameters}\n\n    Methods\n    -------\n    {BaseStepAssociative.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> pavlov_dog_data = np.array([\n    ...     [1, 0],  # food, no bell\n    ...     [1, 1],  # food, bell\n    ... ])\n    >>> dog_test_cases = np.array([\n    ...     [0, 0],  # no food, no bell\n    ...     [0, 1],  # no food, bell\n    ...     [1, 0],  # food, no bell\n    ...     [1, 1],  # food, bell\n    ... ])\n    >>>\n    >>> hebbnet = algorithms.HebbRule(\n    ...     n_inputs=2,\n    ...     n_outputs=1,\n    ...     n_unconditioned=1,\n    ...     step=0.1,\n    ...     decay_rate=0.8,\n    ...     verbose=False\n    ... )\n    >>> hebbnet.train(pavlov_dog_data, epochs=2)\n    >>> hebbnet.predict(dog_test_cases)\n    array([[0],\n           [1],\n           [1],\n           [1]])\n    """"""\n    decay_rate = BoundedProperty(default=0.2, minval=0)\n\n    def weight_delta(self, input_row, layer_output):\n        n_unconditioned = self.n_unconditioned\n        weight = self.weight[n_unconditioned:, :]\n        delta = input_row[:, n_unconditioned:].T.dot(layer_output)\n        return -self.decay_rate * weight + self.step * delta\n'"
neupy/algorithms/associative/instar.py,0,"b'import numpy as np\n\nfrom .base import BaseStepAssociative\n\n\n__all__ = (\'Instar\',)\n\n\nclass Instar(BaseStepAssociative):\n    """"""\n    Instar is a simple unsupervised Neural Network algorithm\n    which detects associations.\n\n    Parameters\n    ----------\n    {BaseAssociative.Parameters}\n\n    Methods\n    -------\n    {BaseAssociative.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> train_data = np.array([\n    ...     [0, 1, 0, 0],\n    ...     [1, 1, 0, 0],\n    ... ])\n    >>> test_cases = np.array([\n    ...     [0, 1, 0, 0],\n    ...     [0, 0, 0, 0],\n    ...     [0, 0, 1, 1],\n    ... ])\n    >>>\n    >>> instnet = algorithms.Instar(\n    ...     n_inputs=4,\n    ...     n_outputs=1,\n    ...     n_unconditioned=1,\n    ...     step=1,\n    ...     verbose=False,\n    ... )\n    >>>\n    >>> instnet.train(train_data, epochs=2)\n    >>> instnet.predict(test_cases)\n    array([[1],\n           [0],\n           [0]])\n    """"""\n    def weight_delta(self, input_row, layer_output):\n        n_unconditioned = self.n_unconditioned\n        weight = self.weight[n_unconditioned:, :]\n\n        return self.step * np.dot(\n            input_row[:, n_unconditioned:].T - weight,\n            layer_output.T)\n'"
neupy/algorithms/associative/kohonen.py,0,"b'import numpy as np\n\nfrom neupy.utils import format_data\nfrom .base import BaseAssociative\n\n\n__all__ = (\'Kohonen\',)\n\n\nclass Kohonen(BaseAssociative):\n    """"""\n    Kohonen Neural Network used for unsupervised learning.\n\n    Parameters\n    ----------\n    {BaseAssociative.Parameters}\n\n    Methods\n    -------\n    {BaseAssociative.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms, layers, utils\n    >>>\n    >>> utils.reproducible()\n    >>>\n    >>> X = np.array([\n    ...     [0.1961,  0.9806],\n    ...     [-0.1961,  0.9806],\n    ...     [0.9806,  0.1961],\n    ...     [0.9806, -0.1961],\n    ...     [-0.5812, -0.8137],\n    ...     [-0.8137, -0.5812],\n    ... ])\n    >>>\n    >>> kohonet = algorithms.Kohonen(\n    ...     n_inputs=2,\n    ...     n_outputs=3,\n    ...     step=0.5,\n    ...     verbose=False\n    ... )\n    >>> kohonet.train(X, epochs=100)\n    >>> kohonet.predict(X)\n    array([[ 0.,  1.,  0.],\n           [ 0.,  1.,  0.],\n           [ 1.,  0.,  0.],\n           [ 1.,  0.,  0.],\n           [ 0.,  0.,  1.],\n           [ 0.,  0.,  1.]])\n    """"""\n    def predict_raw(self, X):\n        X = format_data(X)\n        return X.dot(self.weight)\n\n    def predict(self, X):\n        raw_output = self.predict_raw(X)\n        output = np.zeros(raw_output.shape, dtype=np.int0)\n        max_args = raw_output.argmax(axis=1)\n        output[range(raw_output.shape[0]), max_args] = 1\n        return output\n\n    def one_training_update(self, X_train, y_train):\n        step = self.step\n        predict = self.predict\n\n        error = 0\n        for input_row in X_train:\n            input_row = np.reshape(input_row, (1, input_row.size))\n            layer_output = predict(input_row)\n\n            _, index_y = np.nonzero(layer_output)\n            distance = input_row.T - self.weight[:, index_y]\n            self.weight[:, index_y] += step * distance\n\n            error += np.abs(distance).mean()\n\n        return error / len(X_train)\n'"
neupy/algorithms/associative/oja.py,0,"b'import numpy as np\n\nfrom neupy.utils import format_data\nfrom neupy.exceptions import NotTrained\nfrom neupy.core.properties import IntProperty, ParameterProperty\nfrom neupy.algorithms.base import BaseNetwork\nfrom neupy import init\n\n\n__all__ = (\'Oja\',)\n\n\nclass Oja(BaseNetwork):\n    """"""\n    Oja is an unsupervised technique used for the\n    dimensionality reduction tasks.\n\n    Notes\n    -----\n    - In practice use step as very small value.\n      For instance, value ``1e-7`` can be a good choice.\n\n    - Normalize the input data before use Oja algorithm.\n      Input data shouldn\'t contains large values.\n\n    - Set up smaller values for weight if error for a few\n      first iterations is big compare to the input values scale.\n      For instance, if your input data have values between\n      ``0`` and ``1`` error value equal to ``100`` is big.\n\n    - During the training network report mean absolute error (MAE)\n\n    Parameters\n    ----------\n    minimized_data_size : int\n        Expected number of features after minimization,\n        defaults to ``1``.\n\n    weight : array-like or ``None``\n        Defines networks weights.\n        Defaults to :class:`XavierNormal() <neupy.init.XavierNormal>`.\n\n    {BaseNetwork.Parameters}\n\n    Methods\n    -------\n    reconstruct(X)\n        Reconstruct original dataset from the minimized input.\n\n    train(X, epochs=100)\n        Trains the network to the data X. Network trains until maximum\n        number of ``epochs`` was reached.\n\n    predict(X)\n        Returns hidden representation of the input data ``X``. Basically,\n        it applies dimensionality reduction.\n\n    {BaseSkeleton.fit}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> data = np.array([[2, 2], [1, 1], [4, 4], [5, 5]])\n    >>>\n    >>> ojanet = algorithms.Oja(\n    ...     minimized_data_size=1,\n    ...     step=0.01,\n    ...     verbose=False\n    ... )\n    >>>\n    >>> ojanet.train(data, epochs=100)\n    >>> minimized = ojanet.predict(data)\n    >>> minimized\n    array([[-2.82843122],\n           [-1.41421561],\n           [-5.65686243],\n           [-7.07107804]])\n    >>> ojanet.reconstruct(minimized)\n    array([[ 2.00000046,  2.00000046],\n           [ 1.00000023,  1.00000023],\n           [ 4.00000093,  4.00000093],\n           [ 5.00000116,  5.00000116]])\n    """"""\n    minimized_data_size = IntProperty(minval=1)\n    weight = ParameterProperty(default=init.XavierNormal())\n\n    def one_training_update(self, X, y_train):\n        weight = self.weight\n\n        minimized = np.dot(X, weight)\n        reconstruct = np.dot(minimized, weight.T)\n        error = X - reconstruct\n\n        weight += self.step * np.dot(error.T, minimized)\n        mae = np.sum(np.abs(error)) / X.size\n\n        # Clean objects from the memory\n        del minimized\n        del reconstruct\n        del error\n\n        return mae\n\n    def train(self, X, epochs=100):\n        X = format_data(X)\n        n_input_features = X.shape[1]\n\n        if isinstance(self.weight, init.Initializer):\n            weight_shape = (n_input_features, self.minimized_data_size)\n            self.weight = self.weight.sample(weight_shape, return_array=True)\n\n        if n_input_features != self.weight.shape[0]:\n            raise ValueError(\n                ""Invalid number of features. Expected {}, got {}""\n                """".format(self.weight.shape[0], n_input_features))\n\n        super(Oja, self).train(X, epochs=epochs)\n\n    def reconstruct(self, X):\n        if not isinstance(self.weight, np.ndarray):\n            raise NotTrained(""Network hasn\'t been trained yet"")\n\n        X = format_data(X)\n        if X.shape[1] != self.minimized_data_size:\n            raise ValueError(\n                ""Invalid input data feature space, expected ""\n                ""{}, got {}."".format(X.shape[1], self.minimized_data_size))\n\n        return np.dot(X, self.weight.T)\n\n    def predict(self, X):\n        if not isinstance(self.weight, np.ndarray):\n            raise NotTrained(""Network hasn\'t been trained yet"")\n\n        X = format_data(X)\n        return np.dot(X, self.weight)\n'"
neupy/algorithms/competitive/__init__.py,0,b''
neupy/algorithms/competitive/art.py,0,"b'from __future__ import division\n\nimport numpy as np\n\nfrom neupy.utils import format_data\nfrom neupy.core.properties import (ProperFractionProperty,\n                                   IntProperty)\nfrom neupy.algorithms.base import BaseNetwork\n\n\n__all__ = (\'ART1\',)\n\n\nclass ART1(BaseNetwork):\n    """"""\n    Adaptive Resonance Theory (ART1) Network for binary\n    data clustering.\n\n    Notes\n    -----\n    - Weights are not random, so the result will be\n      always reproduceble.\n\n    Parameters\n    ----------\n    rho : float\n        Control reset action in training process. Value must be\n        between ``0`` and ``1``, defaults to ``0.5``.\n\n    n_clusters : int\n        Number of clusters, defaults to ``2``. Min value is also ``2``.\n\n    {BaseNetwork.Parameters}\n\n    Methods\n    -------\n    train(X)\n        ART trains until all clusters are found.\n\n    predict(X)\n        Each prediction trains a new network. It\'s an alias to\n        the ``train`` method.\n\n    {BaseSkeleton.fit}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> data = np.array([\n    ...     [0, 1, 0],\n    ...     [1, 0, 0],\n    ...     [1, 1, 0],\n    ... ])\n    >>>>\n    >>> artnet = algorithms.ART1(\n    ...     step=2,\n    ...     rho=0.7,\n    ...     n_clusters=2,\n    ...     verbose=False\n    ... )\n    >>> artnet.predict(data)\n    array([ 0.,  1.,  1.])\n    """"""\n    rho = ProperFractionProperty(default=0.5)\n    n_clusters = IntProperty(default=2, minval=2)\n\n    def train(self, X):\n        X = format_data(X)\n\n        if X.ndim != 2:\n            raise ValueError(""Input value must be 2 dimensional, got ""\n                             ""{}"".format(X.ndim))\n\n        n_samples, n_features = X.shape\n        n_clusters = self.n_clusters\n        step = self.step\n        rho = self.rho\n\n        if np.any((X != 0) & (X != 1)):\n            raise ValueError(""ART1 Network works only with binary matrices"")\n\n        if not hasattr(self, \'weight_21\'):\n            self.weight_21 = np.ones((n_features, n_clusters))\n\n        if not hasattr(self, \'weight_12\'):\n            scaler = step / (step + n_clusters - 1)\n            self.weight_12 = scaler * self.weight_21.T\n\n        weight_21 = self.weight_21\n        weight_12 = self.weight_12\n\n        if n_features != weight_21.shape[0]:\n            raise ValueError(""Input data has invalid number of features. ""\n                             ""Got {} instead of {}""\n                             """".format(n_features, weight_21.shape[0]))\n\n        classes = np.zeros(n_samples)\n\n        # Train network\n        for i, p in enumerate(X):\n            disabled_neurons = []\n            reseted_values = []\n            reset = True\n\n            while reset:\n                output1 = p\n                input2 = np.dot(weight_12, output1.T)\n\n                output2 = np.zeros(input2.size)\n                input2[disabled_neurons] = -np.inf\n                winner_index = input2.argmax()\n                output2[winner_index] = 1\n\n                expectation = np.dot(weight_21, output2)\n                output1 = np.logical_and(p, expectation).astype(int)\n\n                reset_value = np.dot(output1.T, output1) / np.dot(p.T, p)\n                reset = reset_value < rho\n\n                if reset:\n                    disabled_neurons.append(winner_index)\n                    reseted_values.append((reset_value, winner_index))\n\n                if len(disabled_neurons) >= n_clusters:\n                    # Got this case only if we test all possible clusters\n                    reset = False\n                    winner_index = None\n\n                if not reset:\n                    if winner_index is not None:\n                        weight_12[winner_index, :] = (step * output1) / (\n                            step + np.dot(output1.T, output1) - 1\n                        )\n                        weight_21[:, winner_index] = output1\n                    else:\n                        # Get result with the best `rho`\n                        winner_index = max(reseted_values)[1]\n\n                    classes[i] = winner_index\n\n        return classes\n\n    def predict(self, X):\n        return self.train(X)\n'"
neupy/algorithms/competitive/growing_neural_gas.py,0,"b'from operator import attrgetter\n\nimport numpy as np\n\nfrom neupy.utils import format_data\nfrom neupy.exceptions import StopTraining\nfrom neupy.algorithms.base import BaseNetwork\nfrom neupy.core.properties import (NumberProperty, ProperFractionProperty,\n                                   IntProperty)\n\n\n__all__ = (\'GrowingNeuralGas\', \'NeuralGasGraph\', \'NeuronNode\')\n\n\ndef sample_data_point(data, n=1):\n    indices = np.random.choice(len(data), n)\n    return data[indices, :]\n\n\nclass NeuralGasGraph(object):\n    """"""\n    Undirected graph structure that stores neural gas network\'s\n    neurons and connections between them.\n\n    Attributes\n    ----------\n    edges_per_node : dict\n        Dictionary that where key is a unique node and value is a list\n        of nodes that connection to this edge.\n\n    edges : dict\n        Dictonary that stores age per each connection. Ech key will have\n        the following format: ``(node_1, node_2)``.\n\n    nodes : list\n        List of all nodes in the graph (read-only attribute).\n\n    n_nodes : int\n        Number of nodes in the network (read-only attribute).\n\n    n_edges : int\n        Number of edges in the network (read-only attribute).\n    """"""\n    def __init__(self):\n        self.edges_per_node = {}\n        self.edges = {}\n\n    @property\n    def nodes(self):\n        return list(self.edges_per_node.keys())\n\n    @property\n    def n_nodes(self):\n        return len(self.edges_per_node)\n\n    @property\n    def n_edges(self):\n        return len(self.edges)\n\n    def add_node(self, node):\n        self.edges_per_node[node] = set()\n\n    def remove_node(self, node):\n        if self.edges_per_node[node]:\n            raise ValueError(\n                ""Cannot remove node, because it\'s connected to ""\n                ""{} other node(s)"".format(len(self.edges_per_node[node])))\n\n        del self.edges_per_node[node]\n\n    def add_edge(self, node_1, node_2):\n        if node_2 in self.edges_per_node[node_1]:\n            return self.reset_edge(node_1, node_2)\n\n        self.edges_per_node[node_1].add(node_2)\n        self.edges_per_node[node_2].add(node_1)\n        self.edges[(node_1, node_2)] = 0\n\n    def reset_edge(self, node_1, node_2):\n        edge_id = self.find_edge_id(node_1, node_2)\n        self.edges[edge_id] = 0\n\n    def remove_edge(self, node_1, node_2):\n        edge_id = self.find_edge_id(node_1, node_2)\n\n        self.edges_per_node[node_1].remove(node_2)\n        self.edges_per_node[node_2].remove(node_1)\n\n        del self.edges[edge_id]\n\n    def find_edge_id(self, node_1, node_2):\n        if (node_1, node_2) in self.edges:\n            return (node_1, node_2)\n\n        if (node_2, node_1) in self.edges:\n            return (node_2, node_1)\n\n        raise ValueError(""Edge between specified nodes doesn\'t exist"")\n\n    def __repr__(self):\n        return ""<{} n_nodes={}, n_edges={}>"".format(\n            self.__class__.__name__,\n            self.n_nodes, self.n_edges)\n\n\nclass NeuronNode(object):\n    """"""\n    Structure representes neuron in the Neural Gas algorithm.\n\n    Attributes\n    ----------\n    weight : 2d-array\n        Neuron\'s position in the space.\n\n    error : float\n        Error accumulated during the training.\n    """"""\n    def __init__(self, weight):\n        self.weight = weight\n        self.error = 0\n\n    def __repr__(self):\n        return ""<{} error={}>"".format(\n            self.__class__.__name__,\n            round(float(self.error), 6))\n\n\nclass GrowingNeuralGas(BaseNetwork):\n    """"""\n    Growing Neural Gas (GNG) algorithm.\n\n    Current algorithm has two modifications that hasn\'t been mentioned\n    in the paper, but they help to speed up training.\n\n    - The ``n_start_nodes`` parameter provides possibility to increase\n      number of nodes during initialization step. It\'s useful when\n      algorithm takes a lot of time building up large amount of neurons.\n\n    - The ``min_distance_for_update`` parameter allows to speed up\n      training when some data samples has neurons very close to them. The\n      ``min_distance_for_update`` parameter controls threshold for the\n      minimum distance for which we will want to update weights.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of features in each sample.\n\n    n_start_nodes : int\n        Number of nodes that algorithm generates from the data during\n        the initialization step. Defaults to ``2``.\n\n    step : float\n        Step (learning rate) for the neuron winner. Defaults to ``0.2``.\n\n    neighbour_step : float\n        Step (learning rate) for the neurons that connected via edges\n        with neuron winner. This value typically has to be smaller than\n        ``step`` value. Defaults to ``0.05``.\n\n    max_edge_age : int\n        It means that if edge won\'t be updated for ``max_edge_age`` iterations\n        than it would be removed. The larger the value the more updates we\n        allow to do before removing edge. Defaults to ``100``.\n\n    n_iter_before_neuron_added : int\n        Each ``n_iter_before_neuron_added`` weight update algorithm add new\n        neuron. The smaller the value the more frequently algorithm adds\n        new neurons to the network. Defaults to ``1000``.\n\n    error_decay_rate : float\n        This error decay rate would be applied to every neuron in the\n        graph after each training iteration. It ensures that old errors\n        will be reduced over time. Defaults to ``0.995``.\n\n    after_split_error_decay_rate : float\n        This decay rate reduces error for neurons with largest errors\n        after algorithm added new neuron. This value typically lower than\n        ``error_decay_rate``. Defaults to ``0.5``.\n\n    max_nodes : int\n        Maximum number of nodes that would be generated during the training.\n        This parameter won\'t stop training when maximum number of nodes\n        will be exceeded. Defaults to ``1000``.\n\n    min_distance_for_update : float\n        Parameter controls for which neurons we want to apply updates.\n        In case if euclidean distance between data sample and closest\n        neurons will be less than the ``min_distance_for_update`` value than\n        update would be skipped for this data sample. Setting value to zero\n        will disable effect provided by this parameter. Defaults to ``0``.\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    {Verbose.verbose}\n\n    Methods\n    -------\n    train(X_train, epochs=100)\n        Network learns topological structure of the data. Learned\n        structure will be stored in the ``graph`` attribute.\n\n    {BaseSkeleton.fit}\n\n    initialize_nodes(data)\n        Network initializes nodes randomly sampling ``n_start_nodes``\n        from the data. It would be applied automatically before\n        the training in case if graph is empty.\n\n        Note: Node re-initialization can reset network.\n\n    Notes\n    -----\n    - Unlike other algorithms this network doesn\'t make predictions.\n      Instead, it learns topological structure of the data in form of\n      the graph. After that training, structure of the network can be\n      extracted from the ``graph`` attribute.\n\n    - In order to speed up training, it might be useful to increase\n      the ``n_start_nodes`` parameter.\n\n    - During the training it happens that nodes learn topological\n      structure of one part of the data better than the other, mostly\n      because of the different data sample density in different places.\n      Increasing the ``min_distance_for_update`` can speed up training\n      ignoring updates for the neurons that very close to the data sample.\n      (below specified ``min_distance_for_update`` value). Training can be\n      stopped in case if none of the neurons has been updated during\n      the training epoch.\n\n    Attributes\n    ----------\n    graph : NeuralGasGraph instance\n        This attribute stores all neurons and connections between them\n        in the form of undirected graph.\n\n    {BaseNetwork.Attributes}\n\n    Examples\n    --------\n    >>> from neupy import algorithms\n    >>> from sklearn.datasets import make_blobs\n    >>>\n    >>> data, _ = make_blobs(\n    ...     n_samples=1000,\n    ...     n_features=2,\n    ...     centers=2,\n    ...     cluster_std=0.4,\n    ... )\n    >>>\n    >>> neural_gas = algorithms.GrowingNeuralGas(\n    ...     n_inputs=2,\n    ...     shuffle_data=True,\n    ...     verbose=True,\n    ...     max_edge_age=10,\n    ...     n_iter_before_neuron_added=50,\n    ...     max_nodes=100,\n    ... )\n    >>> neural_gas.graph.n_nodes\n    100\n    >>> len(neural_gas.graph.edges)\n    175\n    >>> edges = list(neural_gas.graph.edges.keys())\n    >>> neuron_1, neuron_2 = edges[0]\n    >>>\n    >>> neuron_1.weight\n    array([[-6.77166299,  2.4121606 ]])\n    >>> neuron_2.weight\n    array([[-6.829309  ,  2.27839633]])\n\n    References\n    ----------\n    [1] A Growing Neural Gas Network Learns Topologies, Bernd Fritzke\n    """"""\n    n_inputs = IntProperty(minval=1, required=True)\n    n_start_nodes = IntProperty(minval=2, default=2)\n\n    step = NumberProperty(default=0.2, minval=0)\n    neighbour_step = NumberProperty(default=0.05, minval=0)\n    max_edge_age = IntProperty(default=100, minval=1)\n    max_nodes = IntProperty(default=1000, minval=1)\n\n    n_iter_before_neuron_added = IntProperty(default=1000, minval=1)\n    after_split_error_decay_rate = ProperFractionProperty(default=0.5)\n    error_decay_rate = ProperFractionProperty(default=0.995)\n    min_distance_for_update = NumberProperty(default=0.0, minval=0)\n\n    def __init__(self, *args, **kwargs):\n        super(GrowingNeuralGas, self).__init__(*args, **kwargs)\n        self.n_updates = 0\n        self.graph = NeuralGasGraph()\n\n    def format_input_data(self, X):\n        is_feature1d = self.n_inputs == 1\n        X = format_data(X, is_feature1d)\n\n        if X.ndim != 2:\n            raise ValueError(""Cannot make prediction, because input ""\n                             ""data has more than 2 dimensions"")\n\n        n_samples, n_features = X.shape\n\n        if n_features != self.n_inputs:\n            raise ValueError(""Input data expected to have {} features, ""\n                             ""but got {}"".format(self.n_inputs, n_features))\n\n        return X\n\n    def initialize_nodes(self, data):\n        self.graph = NeuralGasGraph()\n\n        for sample in sample_data_point(data, n=self.n_start_nodes):\n            self.graph.add_node(NeuronNode(sample.reshape(1, -1)))\n\n    def train(self, X_train, epochs=100):\n        X_train = self.format_input_data(X_train)\n\n        if not self.graph.nodes:\n            self.initialize_nodes(X_train)\n\n        return super(GrowingNeuralGas, self).train(\n            X_train=X_train, y_train=None,\n            X_test=None, y_test=None,\n            epochs=epochs)\n\n    def one_training_update(self, X_train, y_train=None):\n        graph = self.graph\n        step = self.step\n        neighbour_step = self.neighbour_step\n\n        max_nodes = self.max_nodes\n        max_edge_age = self.max_edge_age\n\n        error_decay_rate = self.error_decay_rate\n        after_split_error_decay_rate = self.after_split_error_decay_rate\n        n_iter_before_neuron_added = self.n_iter_before_neuron_added\n\n        # We square this value, because we deal with\n        # squared distances during the training.\n        min_distance_for_update = np.square(self.min_distance_for_update)\n\n        n_samples = len(X_train)\n        total_error = 0\n        did_update = False\n\n        for sample in X_train:\n            nodes = graph.nodes\n            weights = np.concatenate([node.weight for node in nodes])\n\n            distance = np.linalg.norm(weights - sample, axis=1)\n            neuron_ids = np.argsort(distance)\n\n            closest_neuron_id, second_closest_id = neuron_ids[:2]\n            closest_neuron = nodes[closest_neuron_id]\n            second_closest = nodes[second_closest_id]\n            total_error += distance[closest_neuron_id]\n\n            if distance[closest_neuron_id] < min_distance_for_update:\n                continue\n\n            self.n_updates += 1\n            did_update = True\n\n            closest_neuron.error += distance[closest_neuron_id]\n            closest_neuron.weight += step * (sample - closest_neuron.weight)\n\n            graph.add_edge(closest_neuron, second_closest)\n\n            for to_neuron in list(graph.edges_per_node[closest_neuron]):\n                edge_id = graph.find_edge_id(to_neuron, closest_neuron)\n                age = graph.edges[edge_id]\n\n                if age >= max_edge_age:\n                    graph.remove_edge(to_neuron, closest_neuron)\n\n                    if not graph.edges_per_node[to_neuron]:\n                        graph.remove_node(to_neuron)\n\n                else:\n                    graph.edges[edge_id] += 1\n                    to_neuron.weight += neighbour_step * (\n                        sample - to_neuron.weight)\n\n            time_to_add_new_neuron = (\n                self.n_updates % n_iter_before_neuron_added == 0 and\n                graph.n_nodes < max_nodes)\n\n            if time_to_add_new_neuron:\n                nodes = graph.nodes\n                largest_error_neuron = max(nodes, key=attrgetter(\'error\'))\n                neighbour_neuron = max(\n                    graph.edges_per_node[largest_error_neuron],\n                    key=attrgetter(\'error\'))\n\n                largest_error_neuron.error *= after_split_error_decay_rate\n                neighbour_neuron.error *= after_split_error_decay_rate\n\n                new_weight = 0.5 * (\n                    largest_error_neuron.weight + neighbour_neuron.weight\n                )\n                new_neuron = NeuronNode(weight=new_weight.reshape(1, -1))\n\n                graph.remove_edge(neighbour_neuron, largest_error_neuron)\n                graph.add_node(new_neuron)\n                graph.add_edge(largest_error_neuron, new_neuron)\n                graph.add_edge(neighbour_neuron, new_neuron)\n\n            for node in graph.nodes:\n                node.error *= error_decay_rate\n\n        if not did_update and min_distance_for_update != 0 and n_samples > 1:\n            raise StopTraining(\n                ""Distance between every data sample and neurons, closest ""\n                ""to them, is less then {}"".format(min_distance_for_update))\n\n        return total_error / n_samples\n\n    def predict(self, *args, **kwargs):\n        raise NotImplementedError(\n            ""Growing Neural Gas algorithm doesn\'t make prediction. ""\n            ""It only learns graph structure from the data ""\n            ""(class has `graph` attribute). "")\n'"
neupy/algorithms/competitive/lvq.py,0,"b'from __future__ import division\n\nimport numpy as np\n\nfrom neupy import init\nfrom neupy.utils import format_data\nfrom neupy.exceptions import NotTrained\nfrom neupy.algorithms.base import BaseNetwork\nfrom neupy.core.properties import (\n    IntProperty, Property,\n    TypedListProperty, NumberProperty,\n)\n\n\n__all__ = (\'LVQ\', \'LVQ2\', \'LVQ21\', \'LVQ3\')\n\n\ndef euclid_distance(X, weight):\n    X = np.expand_dims(X, axis=0)\n    euclid_dist = np.linalg.norm(X - weight, axis=1)\n    return np.expand_dims(euclid_dist, axis=0)\n\n\ndef n_argmin(array, n, axis=0):\n    sorted_argumets = array.argsort(axis=axis).ravel()\n    return sorted_argumets[:n]\n\n\nclass LVQ(BaseNetwork):\n    """"""\n    Learning Vector Quantization (LVQ) algorithm.\n\n    Notes\n    -----\n    - Input data needs to be normalized, because LVQ uses\n      Euclidean distance to find clusters.\n\n    - Training error is just a ratio of misclassified\n      samples\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input units. It should be equal to the\n        number of features in the input data set.\n\n    n_subclasses : int, None\n        Defines total number of subclasses. Values should be greater\n        or equal to the number of classes. ``None`` will set up number\n        of subclasses equal to the number of classes. Defaults to ``None``\n        (or the same as ``n_classes``).\n\n    n_classes : int\n        Number of classes in the data set.\n\n    prototypes_per_class : list, None\n        Defines number of prototypes per each class. For instance,\n        if ``n_classes=3`` and ``n_subclasses=8`` then there are\n        can be 3 subclasses for the first class, 3 for the second one\n        and 2 for the third one (3 + 3 + 2 == 8). The following example\n        can be specified as ``prototypes_per_class=[3, 3, 2]``.\n\n        There are two rules that apply to this parameter:\n\n        1. ``sum(prototypes_per_class) == n_subclasses``\n\n        2. ``len(prototypes_per_class) == n_classes``\n\n        The ``None`` value will distribute approximately equal\n        number of subclasses per each class. It\'s approximately,\n        because, for cases, when ``n_subclasses % n_classes != 0``\n        there is no way to distribute equal number of subclasses\n        per each class.\n\n        Defaults to ``None``.\n\n    {BaseNetwork.step}\n\n    n_updates_to_stepdrop : int or None\n        If this options is not equal to ``None`` then after every\n        update LVQ reduces step size and do it until number of\n        applied updates would reach the ``n_updates_to_stepdrop``\n        value. The minimum possible step size defined in the\n        ``minstep`` parameter.\n\n        Be aware that number of updates is not the same as number\n        of epochs. LVQ applies update after each propagated sample\n        through the network. Relations between this parameter and\n        maximum number of epochs is following\n\n        .. code-block:: python\n\n            n_updates_to_stepdrop = n_samples * n_max_epochs\n\n        If parameter equal to ``None`` then step size wouldn\'t be\n        reduced after each update.\n\n        Defaults to ``None``.\n\n    minstep : float\n        Step size would never be lower than this value. This\n        property useful only in case if ``n_updates_to_stepdrop``\n        is not ``None``. Defaults to ``1e-5``.\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    {Verbose.verbose}\n\n    Methods\n    -------\n    {BaseSkeleton.predict}\n\n    {BaseSkeleton.fit}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [2, 2], [1, 2]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>>\n    >>> lvqnet = algorithms.LVQ(n_inputs=2, n_classes=2)\n    >>> lvqnet.train(X, y, epochs=100)\n    >>> lvqnet.predict([[2, 1], [-1, -1]])\n    array([1, 0])\n    """"""\n    n_inputs = IntProperty(minval=1)\n    n_subclasses = IntProperty(minval=2, default=None, allow_none=True)\n    n_classes = IntProperty(minval=2)\n\n    prototypes_per_class = TypedListProperty(allow_none=True, default=None)\n    weight = Property(\n        expected_type=(np.ndarray, init.Initializer),\n        allow_none=True,\n        default=None,\n    )\n    n_updates_to_stepdrop = IntProperty(\n        default=None,\n        allow_none=True,\n        minval=1,\n    )\n    minstep = NumberProperty(minval=0, default=1e-5)\n\n    def __init__(self, **options):\n        self.initialized = False\n        super(LVQ, self).__init__(**options)\n\n        self.n_updates = 0\n\n        if self.n_subclasses is None:\n            self.n_subclasses = self.n_classes\n\n        if isinstance(self.weight, init.Initializer):\n            weight_shape = (self.n_inputs, self.n_subclasses)\n            self.weight = self.weight.sample(weight_shape, return_array=True)\n\n        if self.weight is not None:\n            self.initialized = True\n\n        if self.n_subclasses < self.n_classes:\n            raise ValueError(""Number of subclasses should be greater ""\n                             ""or equal to the number of classes. Network ""\n                             ""was defined with {} subclasses and {} classes""\n                             """".format(self.n_subclasses, self.n_classes))\n\n        if self.prototypes_per_class is None:\n            whole, reminder = divmod(self.n_subclasses, self.n_classes)\n            self.prototypes_per_class = [whole] * self.n_classes\n\n            if reminder:\n                # Since we have reminder left, it means that we cannot\n                # have an equal number of subclasses per each class,\n                # therefor we will add +1 to randomly selected class.\n                class_indices = np.random.choice(self.n_classes, reminder,\n                                                 replace=False)\n\n                for class_index in class_indices:\n                    self.prototypes_per_class[class_index] += 1\n\n        if len(self.prototypes_per_class) != self.n_classes:\n            raise ValueError(""LVQ defined for classification problem that has ""\n                             ""{} classes, but the `prototypes_per_class` ""\n                             ""variable has defined data for {} classes.""\n                             """".format(self.n_classes,\n                                       len(self.prototypes_per_class)))\n\n        if sum(self.prototypes_per_class) != self.n_subclasses:\n            raise ValueError(""Invalid distribution of subclasses for the ""\n                             ""`prototypes_per_class` variable. Got total ""\n                             ""of {} subclasses ({}) instead of {} expected""\n                             """".format(sum(self.prototypes_per_class),\n                                       self.prototypes_per_class,\n                                       self.n_subclasses))\n\n        self.subclass_to_class = []\n        for class_id, n_prototypes in enumerate(self.prototypes_per_class):\n            self.subclass_to_class.extend([class_id] * n_prototypes)\n\n    @property\n    def training_step(self):\n        if self.n_updates_to_stepdrop is None:\n            return self.step\n\n        updates_ratio = (1 - self.n_updates / self.n_updates_to_stepdrop)\n        return self.minstep + (self.step - self.minstep) * updates_ratio\n\n    def predict(self, X):\n        if not self.initialized:\n            raise NotTrained(""LVQ network hasn\'t been trained yet"")\n\n        X = format_data(X)\n        subclass_to_class = self.subclass_to_class\n        weight = self.weight\n\n        predictions = []\n        for input_row in X:\n            output = euclid_distance(input_row, weight)\n            winner_subclass = int(output.argmin(axis=1))\n\n            predicted_class = subclass_to_class[winner_subclass]\n            predictions.append(predicted_class)\n\n        return np.array(predictions)\n\n    def train(self, X_train, y_train, *args, **kwargs):\n        X_train = format_data(X_train)\n        y_train = format_data(y_train)\n\n        n_input_samples = len(X_train)\n\n        if n_input_samples <= self.n_subclasses:\n            raise ValueError(""Number of training input samples should be ""\n                             ""greater than number of subclasses. Training ""\n                             ""method received {} input samples.""\n                             """".format(n_input_samples))\n\n        if not self.initialized:\n            target_classes = sorted(np.unique(y_train).astype(np.int))\n            expected_classes = list(range(self.n_classes))\n\n            if target_classes != expected_classes:\n                raise ValueError(\n                    ""All classes should be integers from the range [0, {}], ""\n                    ""but got the following classes instead {}""\n                    """".format(self.n_classes - 1, target_classes))\n\n            weights = []\n            iterator = zip(target_classes, self.prototypes_per_class)\n            for target_class, n_prototypes in iterator:\n                is_valid_class = (y_train[:, 0] == target_class)\n                is_valid_class = is_valid_class.astype(\'float64\')\n                n_samples_per_class = sum(is_valid_class)\n                is_valid_class /= n_samples_per_class\n\n                if n_samples_per_class <= n_prototypes:\n                    raise ValueError(\n                        ""Input data has {0} samples for class-{1}. Number ""\n                        ""of samples per specified class-{1} should be ""\n                        ""greater than {2}."".format(\n                            n_samples_per_class, target_class, n_prototypes))\n\n                class_weight_indices = np.random.choice(\n                    np.arange(n_input_samples), n_prototypes,\n                    replace=False, p=is_valid_class)\n\n                class_weight = X_train[class_weight_indices]\n                weights.extend(class_weight)\n\n            self.weight = np.array(weights)\n            self.initialized = True\n\n        super(LVQ, self).train(X_train, y_train, *args, **kwargs)\n\n    def one_training_update(self, X_train, y_train):\n        weight = self.weight\n        subclass_to_class = self.subclass_to_class\n\n        n_correct_predictions = 0\n        for input_row, target in zip(X_train, y_train):\n            step = self.training_step\n            output = euclid_distance(input_row, weight)\n            winner_subclass = int(output.argmin())\n            predicted_class = subclass_to_class[winner_subclass]\n\n            weight_update = input_row - weight[winner_subclass, :]\n            is_correct_prediction = (predicted_class == target).item(0)\n\n            if is_correct_prediction:\n                weight[winner_subclass, :] += step * weight_update\n            else:\n                weight[winner_subclass, :] -= step * weight_update\n\n            n_correct_predictions += is_correct_prediction\n            self.n_updates += 1\n\n        n_samples = len(X_train)\n        return 1 - n_correct_predictions / n_samples\n\n\nclass LVQ2(LVQ):\n    """"""\n    Learning Vector Quantization 2 (LVQ2) algorithm.\n    Improved version for the LVQ algorithm.\n\n    Parameters\n    ----------\n    epsilon : float\n        Ration between to closest subclasses that\n        triggers double weight update. Defaults to ``0.1``.\n\n    {LVQ.Parameters}\n\n    Notes\n    -----\n    {LVQ.Notes}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [2, 2], [1, 2]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>>\n    >>> lvqnet = algorithms.LVQ2(n_inputs=2, n_classes=2)\n    >>> lvqnet.train(X, y, epochs=100)\n    >>> lvqnet.predict([[2, 1], [-1, -1]])\n    array([1, 0])\n    """"""\n    epsilon = NumberProperty(default=0.1)\n\n    def one_training_update(self, X_train, y_train):\n        weight = self.weight\n        epsilon = self.epsilon\n        subclass_to_class = self.subclass_to_class\n\n        n_correct_predictions = 0\n        for input_row, target in zip(X_train, y_train):\n            step = self.training_step\n            output = euclid_distance(input_row, weight)\n            winner_subclasses = n_argmin(output, n=2, axis=1)\n\n            top1_subclass, top2_subclass = winner_subclasses\n            top1_class = subclass_to_class[top1_subclass]\n            top2_class = subclass_to_class[top2_subclass]\n\n            top1_weight_update = input_row - weight[top1_subclass, :]\n            is_correct_prediction = (top1_class == target).item(0)\n\n            closest_dist, runner_up_dist = output[0, winner_subclasses]\n            double_update_condition_satisfied = (\n                not is_correct_prediction and\n                (top2_class == target) and\n                closest_dist > ((1 - epsilon) * runner_up_dist) and\n                runner_up_dist < ((1 + epsilon) * closest_dist)\n            )\n\n            if double_update_condition_satisfied:\n                top2_weight_update = input_row - weight[top2_class, :]\n                weight[top1_subclass, :] -= step * top1_weight_update\n                weight[top2_subclass, :] += step * top2_weight_update\n\n            elif is_correct_prediction:\n                weight[top1_subclass, :] += step * top1_weight_update\n\n            else:\n                weight[top1_subclass, :] -= step * top1_weight_update\n\n            n_correct_predictions += is_correct_prediction\n\n        n_samples = len(X_train)\n        return 1 - n_correct_predictions / n_samples\n\n\nclass LVQ21(LVQ2):\n    """"""\n    Learning Vector Quantization 2.1 (LVQ2.1) algorithm.\n    Improved version for the LVQ2 algorithm.\n\n    Parameters\n    ----------\n    {LVQ2.Parameters}\n\n    Notes\n    -----\n    {LVQ2.Notes}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [2, 2], [1, 2]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>>\n    >>> lvqnet = algorithms.LVQ21(n_inputs=2, n_classes=2)\n    >>> lvqnet.train(X, y, epochs=100)\n    >>> lvqnet.predict([[2, 1], [-1, -1]])\n    array([1, 0])\n    """"""\n    def one_training_update(self, X_train, y_train):\n        weight = self.weight\n        epsilon = self.epsilon\n        subclass_to_class = self.subclass_to_class\n\n        n_correct_predictions = 0\n        for input_row, target in zip(X_train, y_train):\n            step = self.training_step\n            output = euclid_distance(input_row, weight)\n            winner_subclasses = n_argmin(output, n=2, axis=1)\n\n            top1_subclass, top2_subclass = winner_subclasses\n            top1_class = subclass_to_class[top1_subclass]\n            top2_class = subclass_to_class[top2_subclass]\n\n            top1_weight_update = input_row - weight[top1_subclass, :]\n            is_correct_prediction = (top1_class == target).item(0)\n\n            closest_dist, runner_up_dist = output[0, winner_subclasses]\n            double_update_condition_satisfied = (\n                (\n                    (top1_class == target and top2_class != target) or\n                    (top1_class != target and top2_class == target)\n                ) and\n                closest_dist > ((1 - epsilon) * runner_up_dist) and\n                runner_up_dist < ((1 + epsilon) * closest_dist)\n            )\n\n            if double_update_condition_satisfied:\n                top2_weight_update = input_row - weight[top2_class, :]\n\n                if is_correct_prediction:\n                    weight[top2_subclass, :] -= step * top2_weight_update\n                    weight[top1_subclass, :] += step * top1_weight_update\n                else:\n                    weight[top1_subclass, :] -= step * top1_weight_update\n                    weight[top2_subclass, :] += step * top2_weight_update\n\n            elif is_correct_prediction:\n                weight[top1_subclass, :] += step * top1_weight_update\n\n            else:\n                weight[top1_subclass, :] -= step * top1_weight_update\n\n            n_correct_predictions += is_correct_prediction\n            self.n_updates += 1\n\n        n_samples = len(X_train)\n        return 1 - n_correct_predictions / n_samples\n\n\nclass LVQ3(LVQ21):\n    """"""\n    Learning Vector Quantization 3 (LVQ3) algorithm.\n    Improved version for the LVQ2.1 algorithm.\n\n    Parameters\n    ----------\n    {LVQ.n_inputs}\n\n    {LVQ.n_subclasses}\n\n    {LVQ.n_classes}\n\n    {LVQ.prototypes_per_class}\n\n    {LVQ2.epsilon}\n\n    slowdown_rate : float\n        Paremeter scales learning step in order to decrease it\n        in case if the two closest subclasses predict target\n        value correctly. Defaults to ``0.4``.\n\n    step : float\n        Learning rate, defaults to ``0.01``.\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    {Verbose.verbose}\n\n    Notes\n    -----\n    {LVQ21.Notes}\n    - Decreasing step and increasing number of training epochs\n      can improve the performance.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [2, 2], [1, 2]])\n    >>> y = np.array([0, 0, 0, 1, 1, 1])\n    >>>\n    >>> lvqnet = algorithms.LVQ3(n_inputs=2, n_classes=2)\n    >>> lvqnet.train(X, y, epochs=100)\n    >>> lvqnet.predict([[2, 1], [-1, -1]])\n    array([1, 0])\n    """"""\n    step = NumberProperty(minval=0, default=0.01)\n    slowdown_rate = NumberProperty(minval=0, default=0.4)\n\n    def one_training_update(self, X_train, y_train):\n        weight = self.weight\n        epsilon = self.epsilon\n        slowdown_rate = self.slowdown_rate\n        subclass_to_class = self.subclass_to_class\n\n        n_correct_predictions = 0\n        for input_row, target in zip(X_train, y_train):\n            step = self.training_step\n            output = euclid_distance(input_row, weight)\n            winner_subclasses = n_argmin(output, n=2, axis=1)\n\n            top1_subclass, top2_subclass = winner_subclasses\n            top1_class = subclass_to_class[top1_subclass]\n            top2_class = subclass_to_class[top2_subclass]\n\n            top1_weight_update = input_row - weight[top1_subclass, :]\n            is_first_correct = (top1_class == target).item(0)\n            is_second_correct = (top2_class == target).item()\n\n            closest_dist, runner_up_dist = output[0, winner_subclasses]\n            double_update_condition_satisfied = (\n                (\n                    (is_first_correct and not is_second_correct) or\n                    (is_second_correct and not is_first_correct)\n                ) and\n                closest_dist > ((1 - epsilon) * runner_up_dist) and\n                runner_up_dist < ((1 + epsilon) * closest_dist)\n            )\n            two_closest_correct_condition_satisfied = (\n                is_first_correct and is_second_correct and\n                closest_dist > ((1 - epsilon) * (1 + epsilon) * runner_up_dist)\n            )\n\n            if double_update_condition_satisfied:\n                top2_weight_update = input_row - weight[top2_class, :]\n\n                if is_first_correct:\n                    weight[top1_subclass, :] += step * top1_weight_update\n                    weight[top2_subclass, :] -= step * top2_weight_update\n                else:\n                    weight[top1_subclass, :] -= step * top1_weight_update\n                    weight[top2_subclass, :] += step * top2_weight_update\n\n            elif two_closest_correct_condition_satisfied:\n                beta = step * slowdown_rate\n                top2_weight_update = input_row - weight[top2_class, :]\n\n                weight[top1_subclass, :] += beta * top1_weight_update\n                weight[top2_subclass, :] += beta * top2_weight_update\n\n            else:\n                weight[top1_subclass, :] -= step * top1_weight_update\n\n            n_correct_predictions += is_first_correct\n            self.n_updates += 1\n\n        n_samples = len(X_train)\n        return 1 - n_correct_predictions / n_samples\n'"
neupy/algorithms/competitive/neighbours.py,0,"b'from __future__ import division\n\nimport numpy as np\n\nfrom neupy.core.docs import shared_docs\n\n\ndef gaussian_df(data, mean=0, std=1):\n    """"""\n    Returns gaussian density for each data sample.\n    Gaussian specified by the mean and standard deviation.\n\n    Parameters\n    ----------\n    data : array-like\n\n    mean : float\n        Gaussian mean.\n\n    std : float\n        Gaussian standard deviation.\n    """"""\n    if std == 0:\n        return np.where(data == mean, 1, 0)\n\n    normalizer = 2 * np.pi * std ** 2\n    return np.exp(-np.square(data - mean) / normalizer)\n\n\ndef find_neighbour_distance(grid, center):\n    """"""\n    Returns distance from the center into different directions\n    per each dimension separately.\n\n    Parameters\n    ----------\n    grid : array-like\n       Array that contains grid of n-dimensional vectors.\n\n    center : tuple\n        Index of the main neuron for which function returns\n        distance to neuron\'s neighbours.\n\n    Returns\n    -------\n    list of n-dimensional vectors\n    """"""\n    if len(center) != grid.ndim:\n        raise ValueError(\n            ""Cannot find center, because grid of neurons has {} dimensions ""\n            ""and center has specified coordinates for {} dimensional grid""\n            """".format(grid.ndim, len(center)))\n\n    slices = []\n    for dim_length, center_coord in zip(grid.shape, center):\n        slices.append(slice(-center_coord, dim_length - center_coord))\n\n    return np.ogrid[slices]\n\n\n@shared_docs(find_neighbour_distance)\ndef find_step_scaler_on_rect_grid(grid, center, std=1):\n    """"""\n    Function returns multivariate gaussian around the center\n    with specified standard deviation.\n\n    Parameters\n    ----------\n    {find_neighbour_distance.grid}\n\n    {find_neighbour_distance.center}\n\n    std : int, float\n        Gaussian standard deviation. Defaults to ``1``.\n    """"""\n    distances = find_neighbour_distance(grid, center)\n    gaussian_array = sum(gaussian_df(dist, std=std) for dist in distances)\n    return gaussian_array / grid.ndim\n\n\n@shared_docs(find_neighbour_distance)\ndef find_neighbours_on_rect_grid(grid, center, radius):\n    """"""\n    Function find all neuron\'s neighbours around specified\n    center within a certain radius.\n\n    Parameters\n    ----------\n    {find_neighbour_distance.grid}\n\n    {find_neighbour_distance.center}\n\n    radius : int\n        Radius specifies what neurons, around the center, are neighbours.\n\n    Returns\n    -------\n    array-like\n        Return matrix with the same dimension as ``grid``\n        where center element and it neighbours positions\n        filled with value ``1`` and other as a ``0`` value.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy.algorithms.competitive import sofm\n    >>>\n    >>> sofm.find_neighbours_on_rect_grid(\n    ...     grid=np.zeros((3, 3)),\n    ...     center=(0, 0),\n    ...     radius=1)\n    ...\n    array([[ 1.,  1.,  0.],\n           [ 1.,  0.,  0.],\n           [ 0.,  0.,  0.]])\n    >>>\n    >>> sofm.find_neighbours_on_rect_grid(\n    ...     grid=np.zeros((5, 5)),\n    ...     center=(2, 2),\n    ...     radius=2)\n    ...\n    array([[ 0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  1.,  0.],\n           [ 1.,  1.,  1.,  1.,  1.],\n           [ 0.,  1.,  1.,  1.,  0.],\n           [ 0.,  0.,  1.,  0.,  0.]])\n    """"""\n    distances = find_neighbour_distance(grid, center)\n    mask = sum(dist ** 2 for dist in distances) <= radius ** 2\n    grid[mask] = 1\n    return grid\n\n\ndef generate_neighbours_pattern(radius):\n    """"""\n    Generates pattern that defines neighbours\n    within specified radius on the hexagon grid.\n\n    Parameters\n    ----------\n    radius : int\n\n    Returns\n    -------\n    array-like\n        2d-array that has number of rows equal to number\n        of columns equal to ``2 * radius + 1``\n    """"""\n    cache = generate_neighbours_pattern.cache\n\n    if radius in cache:\n        return cache[radius]\n\n    size = 2 * radius + 1\n    max_distance = radius + 1\n    pattern = np.zeros((size, size))\n\n    for i, width in enumerate(range(size - radius, size + 1)):\n        start_point = radius - int(np.floor(width / 2))\n        neighbour_range = slice(start_point, start_point + width)\n\n        neighbour_distance = np.pad(\n            array=(max_distance - i) * [max_distance - i],\n            pad_width=i,\n            mode=\'linear_ramp\',\n            end_values=max_distance)\n\n        pattern[i, neighbour_range] = neighbour_distance\n        # every row in the pattern is mirror symmetric\n        pattern[size - i - 1, neighbour_range] = neighbour_distance\n\n    cache[radius] = pattern\n    return pattern\n\n\ngenerate_neighbours_pattern.cache = {}\n\n\ndef find_neighbours_on_hexagon_grid(grid, center, radius):\n    """"""\n    Marks neighbors on the hexagon grid around specified\n    center within specified radius.\n\n    Parameters\n    ----------\n    grid : 2d-arry\n        Hexagon grid.\n\n    center : tuple\n        Coordinates of the center neuron on the grid.\n        Should be a tuple with two integers: ``(x, y)``.\n\n    radius : int\n        Radius specifies what neurons, around the center, are neighbours.\n\n    Returns\n    -------\n    2d-array\n    """"""\n    if radius == 0:\n        grid[center] = 1\n        return grid\n\n    center_x, center_y = center\n    pattern = generate_neighbours_pattern(radius)\n\n    # We add zero padding in order to be able\n    # insert full pattern into the matrix. It\'s\n    # useful in case if we are not able to mark all\n    # neighbours from the pattern for specific neuron.\n    # For instance, in case when center = (0, 0) we\n    # are not able to mark neurons on the left and top\n    # sides of the pattern\'s center.\n    grid = np.pad(grid, radius, mode=\'constant\')\n\n    if center_x % 2 == 1:\n        # Since every even row shifted to the right\n        # compare to the odd rows then odd rows shifted\n        # to the left compare to the even rows.\n        # It means that our pattern has to be reversed\n        # for the even rows.\n        pattern = pattern[:, ::-1]\n\n    pattern_size = 2 * radius + 1\n    x_range = slice(center_x, center_x + pattern_size)\n    y_range = slice(center_y, center_y + pattern_size)\n\n    grid[x_range, y_range] = pattern\n\n    # removing padding\n    return grid[radius:-radius, radius:-radius]\n\n\n@shared_docs(find_step_scaler_on_rect_grid)\ndef find_step_scaler_on_hexagon_grid(grid, center=None, std=1):\n    """"""\n    Find step scale for neighbouring neurons. The further\n    neuron from the center the lower step scale it has.\n\n    Notes\n    -----\n    Non-neighbor neurons will have high step size\n    We do not set up it to zero, since later we will\n    ignore these values anyway\n\n    Parameters\n    ----------\n    grid : array-like\n\n    center : object\n        Variable is going to be ignored. It\'s defined\n        only for compatibility with other functions\n\n    {find_step_scaler_on_rect_grid.std}\n    """"""\n    return gaussian_df(grid, mean=1, std=std)\n'"
neupy/algorithms/competitive/randomized_pca.py,0,"b'import numpy as np\nfrom scipy import linalg\n\n\ndef svd_flip(u, v):\n    """"""\n    Sign correction to ensure deterministic output from SVD.\n    Adjusts the columns of u and the rows of v such that the\n    loadings in the columns in u that are largest in absolute\n    value are always positive.\n\n    Parameters\n    ----------\n    u, v : ndarray\n        u and v are the output of `linalg.svd` or\n        `sklearn.utils.extmath.randomized_svd`, with matching inner\n        dimensions so one can compute `np.dot(u * s, v)`.\n\n    Notes\n    -----\n    scikit-learn implementation\n\n    Returns\n    -------\n    u_adjusted, v_adjusted : arrays with the same dimensions\n    as the input.\n    """"""\n    max_abs_rows = np.argmax(np.abs(v), axis=1)\n    signs = np.sign(v[range(v.shape[0]), max_abs_rows])\n\n    u *= signs\n    v *= signs[:, np.newaxis]\n\n    return u, v\n\n\ndef randomized_range_finder(A, size, n_iter):\n    """"""\n    Computes an orthonormal matrix whose range\n    approximates the range of A.\n\n    Parameters\n    ----------\n    A: 2D array\n        The input data matrix\n\n    size: integer\n        Size of the return array\n\n    n_iter: integer\n        Number of power iterations used to stabilize the result\n\n    Returns\n    -------\n    Q: 2D array\n        A (size x size) projection matrix, the range of which\n        approximates well the range of the input matrix A.\n\n    Notes\n    -----\n    scikit-learn implementation\n    """"""\n    # Generating normal random vectors with shape: (A.shape[1], size)\n    Q = np.random.normal(size=(A.shape[1], size))\n\n    # Perform power iterations with Q to further \'imprint\' the top\n    # singular vectors of A in Q\n    for i in range(n_iter):\n        Q, _ = linalg.lu(np.dot(A, Q), permute_l=True)\n        Q, _ = linalg.lu(np.dot(A.T, Q), permute_l=True)\n\n    # Sample the range of A using by linear projection of Q\n    # Extract an orthonormal basis\n    Q, _ = linalg.qr(np.dot(A, Q), mode=\'economic\')\n    return Q\n\n\ndef randomized_svd(M, n_components, n_oversamples=10):\n    """"""\n    Computes a truncated randomized SVD.\n\n    Parameters\n    ----------\n    M: ndarray or sparse matrix\n        Matrix to decompose\n\n    n_components: int\n        Number of singular values and vectors to extract.\n\n    n_oversamples: int (default is 10)\n        Additional number of random vectors to sample the range of M so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of M is n_components + n_oversamples. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of singular vectors and singular values.\n\n    Notes\n    -----\n    scikit-learn implementation\n    """"""\n    n_random = n_components + n_oversamples\n    n_samples, n_features = M.shape\n    n_iter = 7 if n_components < .1 * min(M.shape) else 4\n\n    Q = randomized_range_finder(M, n_random, n_iter)\n    # project M to the (k + p) dimensional space\n    # using the basis vectors\n    B = np.dot(Q.T, M)\n\n    # compute the SVD on the thin matrix: (k + p) wide\n    Uhat, s, V = linalg.svd(B, full_matrices=False)\n    del B\n\n    U = np.dot(Q, Uhat)\n    U, V = svd_flip(U, V)\n\n    return U[:, :n_components], s[:n_components], V[:n_components, :]\n\n\ndef randomized_pca(data, n_componets):\n    """"""\n    Randomized PCA based on the scikit-learn\n    implementation.\n\n    Parameters\n    ----------\n    data : 2d array-like\n\n    n_components : int\n        Number of PCA components\n\n    Returns\n    -------\n    (eigenvectors, eigenvalues)\n    """"""\n    n_samples, n_features = data.shape\n\n    U, S, V = randomized_svd(data, n_componets)\n    eigenvalues = (S ** 2) / n_samples\n    eigenvectors = V / S[:, np.newaxis] * np.sqrt(n_samples)\n\n    return eigenvectors, eigenvalues\n'"
neupy/algorithms/competitive/sofm.py,0,"b'from __future__ import division\n\nfrom collections import namedtuple\n\nimport six\nimport numpy as np\nfrom numpy.linalg import norm\n\nfrom neupy import init\nfrom neupy.utils import as_tuple, format_data\nfrom neupy.algorithms import Kohonen\nfrom neupy.exceptions import WeightInitializationError\nfrom neupy.algorithms.associative.base import BaseAssociative\nfrom neupy.core.properties import (BaseProperty, TypedListProperty,\n                                   ChoiceProperty, NumberProperty,\n                                   ParameterProperty, IntProperty)\nfrom .randomized_pca import randomized_pca\nfrom .neighbours import (find_step_scaler_on_rect_grid,\n                         find_neighbours_on_rect_grid,\n                         find_neighbours_on_hexagon_grid,\n                         find_step_scaler_on_hexagon_grid)\n\n\n__all__ = (\'SOFM\',)\n\n\ndef neg_euclid_distance(X, weight):\n    """"""\n    Negative Euclidean distance between input\n    data and weight.\n    """"""\n    euclid_dist = norm(X.T - weight, axis=0)\n    return -np.expand_dims(euclid_dist, axis=0)\n\n\ndef cosine_similarity(X, weight):\n    """"""\n    Cosine similarity between input data and weight.\n    """"""\n    norm_prod = norm(X) * norm(weight, axis=0)\n    summated_data = np.dot(X, weight)\n    cosine_dist = summated_data / norm_prod\n    return np.reshape(cosine_dist, (1, weight.shape[1]))\n\n\ndef decay_function(value, epoch, reduction_rate):\n    """"""\n    Applies to the input value monophonic decay.\n\n    Parameters\n    ----------\n    value : int, float\n\n    epoch : int\n        Current training iteration (epoch).\n\n    reduction_rate : int\n        The larger the value the slower decay]\n    """"""\n    return value / (1 + epoch / reduction_rate)\n\n\nclass SOFMWeightParameter(ChoiceProperty):\n    expected_type = as_tuple(ParameterProperty.expected_type, six.string_types)\n\n    def __set__(self, instance, value):\n        if isinstance(value, six.string_types):\n            return super(ChoiceProperty, self).__set__(instance, value)\n        return BaseProperty.__set__(self, instance, value)\n\n    def __get__(self, instance, owner):\n        choice_key = super(ChoiceProperty, self).__get__(instance, owner)\n\n        if isinstance(choice_key, six.string_types):\n            return self.choices[choice_key]\n\n        return choice_key\n\n\ndef sample_data(data, features_grid):\n    """"""\n    Samples from the data number of rows specified in\n    the ``n_outputs`` argument. In case if ``n_outputs > n_samples``\n    then sample will be with replacement.\n\n    Parameters\n    ----------\n    data : matrix ``(n_samples, n_features)``\n        Matrix where each row is a data sample.\n\n    features_grid : tuple\n        Tuple that defines shape of the feature grid.\n\n    Returns\n    -------\n    matrix ``(n_features, n_outputs)``\n    """"""\n    n_outputs = np.prod(features_grid)\n    n_samples, n_features = data.shape\n\n    with_replacement = n_samples < n_outputs\n    indices = np.random.choice(n_samples, n_outputs,\n                               replace=with_replacement)\n\n    return data[indices].T\n\n\ndef linear_initialization(data, features_grid):\n    """"""\n    Linear weight initialization base on the randomized PCA.\n\n    Parameters\n    ----------\n    data : 2d array-like\n\n    features_grid : tuple\n        Tuple that defines shape of the feature grid.\n\n    Returns\n    -------\n    2d array-like\n        Initialized weights\n    """"""\n    cols = features_grid[1]\n    n_nodes = np.prod(features_grid)\n\n    n_pca_components = 2\n    coord = np.zeros((n_nodes, n_pca_components))\n\n    for i in range(n_nodes):\n        coord[i, 0] = int(i / cols)\n        coord[i, 1] = int(i % cols)\n\n    maximum = np.max(coord, axis=0)\n    coord = 2 * (coord / maximum - 0.5)\n\n    data_mean = np.mean(data, axis=0)\n    data_std = np.std(data, axis=0)\n    data = (data - data_mean) / data_std\n\n    eigenvectors, eigenvalues = randomized_pca(data, n_pca_components)\n\n    norms = np.sqrt(np.einsum(\'ij,ij->i\', eigenvectors, eigenvectors))\n    eigenvectors = ((eigenvectors.T / norms) * eigenvalues).T\n    weight = data_mean + coord.dot(eigenvectors) * data_std\n\n    return weight.T\n\n\nclass SOFM(Kohonen):\n    """"""\n    Self-Organizing Feature Map (SOFM or SOM).\n\n    Notes\n    -----\n    - Training data samples should have normalized features.\n\n    Parameters\n    ----------\n    {BaseAssociative.n_inputs}\n\n    n_outputs : int or None\n        Number of outputs. Parameter is optional in case if\n        ``feature_grid`` was specified.\n\n        .. code-block:: python\n\n            if n_outputs is None:\n                n_outputs = np.prod(feature_grid)\n\n    learning_radius : int\n        Parameter defines radius within which we consider all\n        neurons as neighbours to the winning neuron. The bigger\n        the value the more neurons will be updated after each\n        iteration.\n\n        The ``0`` values means that we don\'t update\n        neighbour neurons.\n\n        Defaults to ``0``.\n\n    std : int, float\n        Parameters controls learning rate for each neighbour.\n        The further neighbour  neuron from the winning neuron\n        the smaller that learning rate for it. Learning rate\n        scales based on the factors produced by the normal\n        distribution with center in the place of a winning\n        neuron and standard deviation specified as a parameter.\n        The learning rate for the winning neuron is always equal\n        to the value specified in the ``step`` parameter and for\n        neighbour neurons it\'s always lower.\n\n        The bigger the value for this parameter the bigger\n        learning rate for the neighbour neurons.\n\n        Defaults to ``1``.\n\n    features_grid : list, tuple, None\n        Feature grid defines shape of the output neurons.\n        The new shape should be compatible with the number\n        of outputs. It means that the following condition\n        should be true:\n\n        .. code-block:: python\n\n            np.prod(features_grid) == n_outputs\n\n        SOFM implementation supports n-dimensional grids.\n        For instance, in order to specify grid as cube instead of\n        the regular rectangular shape we can set up options as\n        the following:\n\n        .. code-block:: python\n\n            SOFM(\n                ...\n                features_grid=(5, 5, 5),\n                ...\n            )\n\n        Defaults to ``(n_outputs, 1)``.\n\n    grid_type : {{``rect``, ``hexagon``}}\n        Defines connection type in feature grid. Type defines\n        which neurons we will consider as closest to the winning\n        neuron during the training.\n\n        - ``rect`` - Connections between neurons will be organized\n          in hexagonal grid.\n\n        - ``hexagon`` - Connections between neurons will be organized\n          in hexagonal grid. It works only for 1d or 2d grids.\n\n        Defaults to ``rect``.\n\n    distance : {{``euclid``, ``dot_product``, ``cos``}}\n        Defines function that will be used to compute\n        closest weight to the input sample.\n\n        - ``dot_product``: Just a regular dot product between\n          data sample and network\'s weights\n\n        - ``euclid``: Euclidean distance between data sample\n          and network\'s weights\n\n        - ``cos``: Cosine distance between data sample and\n          network\'s weights\n\n        Defaults to ``euclid``.\n\n    reduce_radius_after : int or None\n        Every specified number of epochs ``learning_radius``\n        parameter will be reduced by ``1``. Process continues\n        until ``learning_radius`` equal to ``0``.\n\n        The ``None`` value disables parameter reduction\n        during the training.\n\n        Defaults to ``100``.\n\n    reduce_step_after : int or None\n        Defines reduction rate at which parameter ``step`` will\n        be reduced using the following formula:\n\n        .. code-block:: python\n\n            step = step / (1 + current_epoch / reduce_step_after)\n\n        The ``None`` value disables parameter reduction\n        during the training.\n\n        Defaults to ``100``.\n\n    reduce_std_after : int or None\n        Defines reduction rate at which parameter ``std`` will\n        be reduced using the following formula:\n\n        .. code-block:: python\n\n            std = std / (1 + current_epoch / reduce_std_after)\n\n        The ``None`` value disables parameter reduction\n        during the training.\n\n        Defaults to ``100``.\n\n    weight : array-like, Initializer or {{``init_pca``, ``sample_from_data``}}\n        Neural network weights.\n        Value defined manualy should have shape ``(n_inputs, n_outputs)``.\n\n        Also, it\'s possible to initialized weights base on the\n        training data. There are two options:\n\n        - ``sample_from_data`` - Before starting the training will\n          randomly take number of training samples equal to number\n          of expected outputs.\n\n        - ``init_pca`` - Before training starts SOFM will applies PCA\n          on a covariance matrix build from the training samples.\n          Weights will be generated based on the two eigenvectors\n          associated with the largest eigenvalues.\n\n        Defaults to :class:`Normal() <neupy.init.Normal>`.\n\n    {BaseNetwork.step}\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    {Verbose.verbose}\n\n    Methods\n    -------\n    init_weights(train_data)\n        Initialized weights based on the input data. It works only\n        for the `init_pca` and `sample_from_data` options. For other\n        cases it will throw an error.\n\n    {BaseSkeleton.predict}\n\n    {BaseAssociative.train}\n\n    {BaseSkeleton.fit}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms, utils\n    >>>\n    >>> utils.reproducible()\n    >>>\n    >>> data = np.array([\n    ...     [0.1961, 0.9806],\n    ...     [-0.1961, 0.9806],\n    ...     [-0.5812, -0.8137],\n    ...     [-0.8137, -0.5812],\n    ... ])\n    >>>\n    >>> sofm = algorithms.SOFM(\n    ...     n_inputs=2,\n    ...     n_outputs=2,\n    ...     step=0.1,\n    ...     learning_radius=0\n    ... )\n    >>> sofm.train(data, epochs=100)\n    >>> sofm.predict(data)\n    array([[0, 1],\n           [0, 1],\n           [1, 0],\n           [1, 0]])\n    """"""\n    n_outputs = IntProperty(minval=1, allow_none=True, default=None)\n    weight = SOFMWeightParameter(\n        default=init.Normal(),\n        choices={\n            \'init_pca\': linear_initialization,\n            \'sample_from_data\': sample_data,\n        }\n    )\n    features_grid = TypedListProperty(allow_none=True, default=None)\n\n    DistanceParameter = namedtuple(\'DistanceParameter\', \'name func\')\n    distance = ChoiceProperty(\n        default=\'euclid\',\n        choices={\n            \'dot_product\': DistanceParameter(\n                name=\'dot_product\',\n                func=np.dot),\n            \'euclid\': DistanceParameter(\n                name=\'euclid\',\n                func=neg_euclid_distance),\n            \'cos\': DistanceParameter(\n                name=\'cosine\',\n                func=cosine_similarity),\n        })\n\n    GridTypeMethods = namedtuple(\n        \'GridTypeMethods\', \'name find_neighbours find_step_scaler\')\n\n    grid_type = ChoiceProperty(\n        default=\'rect\',\n        choices={\n            \'rect\': GridTypeMethods(\n                name=\'rectangle\',\n                find_neighbours=find_neighbours_on_rect_grid,\n                find_step_scaler=find_step_scaler_on_rect_grid),\n            \'hexagon\': GridTypeMethods(\n                name=\'hexagon\',\n                find_neighbours=find_neighbours_on_hexagon_grid,\n                find_step_scaler=find_step_scaler_on_hexagon_grid)\n        }\n    )\n\n    learning_radius = IntProperty(default=0, minval=0)\n    std = NumberProperty(minval=0, default=1)\n\n    reduce_radius_after = IntProperty(default=100, minval=1, allow_none=True)\n    reduce_std_after = IntProperty(default=100, minval=1, allow_none=True)\n    reduce_step_after = IntProperty(default=100, minval=1, allow_none=True)\n\n    def __init__(self, **options):\n        super(BaseAssociative, self).__init__(**options)\n\n        if self.n_outputs is None and self.features_grid is None:\n            raise ValueError(""One of the following parameters has to be ""\n                             ""specified: n_outputs, features_grid"")\n\n        elif self.n_outputs is None:\n            self.n_outputs = np.prod(self.features_grid)\n\n        n_grid_elements = np.prod(self.features_grid)\n        invalid_feature_grid = (\n            self.features_grid is not None and\n            n_grid_elements != self.n_outputs)\n\n        if invalid_feature_grid:\n            raise ValueError(\n                ""Feature grid should contain the same number of elements ""\n                ""as in the output layer: {0}, but found: {1} (shape: {2})""\n                """".format(\n                    self.n_outputs,\n                    n_grid_elements,\n                    self.features_grid))\n\n        if self.features_grid is None:\n            self.features_grid = (self.n_outputs, 1)\n\n        if len(self.features_grid) > 2 and self.grid_type.name == \'hexagon\':\n            raise ValueError(""SOFM with hexagon grid type should have ""\n                             ""one or two dimensional feature grid, but got ""\n                             ""{}d instead (shape: {!r})"".format(\n                                len(self.features_grid),\n                                self.features_grid))\n\n        is_pca_init = (\n            isinstance(options.get(\'weight\'), six.string_types) and\n            options.get(\'weight\') == \'init_pca\'\n        )\n\n        self.initialized = False\n        if not callable(self.weight):\n            super(Kohonen, self).init_weights()\n            self.initialized = True\n\n            if self.distance.name == \'cosine\':\n                self.weight /= np.linalg.norm(self.weight, axis=0)\n\n        elif is_pca_init and self.grid_type.name != \'rectangle\':\n            raise WeightInitializationError(\n                ""Cannot apply PCA weight initialization for non-rectangular ""\n                ""grid. Grid type: {}"".format(self.grid_type.name))\n\n    def predict_raw(self, X):\n        X = format_data(X, is_feature1d=(self.n_inputs == 1))\n\n        if X.ndim != 2:\n            raise ValueError(""Only 2D inputs are allowed"")\n\n        n_samples = X.shape[0]\n        output = np.zeros((n_samples, self.n_outputs))\n\n        for i, input_row in enumerate(X):\n            output[i, :] = self.distance.func(\n                input_row.reshape(1, -1), self.weight)\n\n        return output\n\n    def update_indexes(self, layer_output):\n        neuron_winner = layer_output.argmax(axis=1).item(0)\n        winner_neuron_coords = np.unravel_index(\n            neuron_winner, self.features_grid)\n\n        learning_radius = self.learning_radius\n        step = self.step\n        std = self.std\n\n        if self.reduce_radius_after is not None:\n            learning_radius -= self.last_epoch // self.reduce_radius_after\n            learning_radius = max(0, learning_radius)\n\n        if self.reduce_step_after is not None:\n            step = decay_function(step, self.last_epoch,\n                                  self.reduce_step_after)\n\n        if self.reduce_std_after is not None:\n            std = decay_function(std, self.last_epoch,\n                                 self.reduce_std_after)\n\n        methods = self.grid_type\n        output_grid = np.reshape(layer_output, self.features_grid)\n\n        output_with_neighbours = methods.find_neighbours(\n            grid=output_grid,\n            center=winner_neuron_coords,\n            radius=learning_radius)\n\n        step_scaler = methods.find_step_scaler(\n            grid=output_grid,\n            center=winner_neuron_coords,\n            std=std)\n\n        index_y, = np.nonzero(\n            output_with_neighbours.reshape(self.n_outputs))\n\n        step_scaler = step_scaler.reshape(self.n_outputs)\n        return index_y, step * step_scaler[index_y]\n\n    def init_weights(self, X_train):\n        if self.initialized:\n            raise WeightInitializationError(\n                ""Weights have been already initialized"")\n\n        weight_initializer = self.weight\n        self.weight = weight_initializer(X_train, self.features_grid)\n        self.initialized = True\n\n        if self.distance.name == \'cosine\':\n            self.weight /= np.linalg.norm(self.weight, axis=0)\n\n    def train(self, X_train, epochs=100):\n        if not self.initialized:\n            self.init_weights(X_train)\n        super(SOFM, self).train(X_train, epochs=epochs)\n\n    def one_training_update(self, X_train, y_train=None):\n        step = self.step\n        predict = self.predict\n        update_indexes = self.update_indexes\n\n        error = 0\n        for input_row in X_train:\n            input_row = np.reshape(input_row, (1, input_row.size))\n            layer_output = predict(input_row)\n\n            index_y, step = update_indexes(layer_output)\n            distance = input_row.T - self.weight[:, index_y]\n            updated_weights = (self.weight[:, index_y] + step * distance)\n\n            if self.distance.name == \'cosine\':\n                updated_weights /= np.linalg.norm(updated_weights, axis=0)\n\n            self.weight[:, index_y] = updated_weights\n            error += np.abs(distance).mean()\n\n        return error / len(X_train)\n'"
neupy/algorithms/gd/__init__.py,0,b''
neupy/algorithms/gd/adadelta.py,1,"b'import tensorflow as tf\n\nfrom neupy.core.properties import (\n    ProperFractionProperty,\n    ScalarVariableProperty,\n    NumberProperty,\n)\nfrom .base import GradientDescent\n\n\n__all__ = (\'Adadelta\',)\n\n\nclass Adadelta(GradientDescent):\n    """"""\n    Adadelta algorithm.\n\n    Parameters\n    ----------\n    rho : float\n        Decay rate. Value need to be between ``0``\n        and ``1``. Defaults to ``0.95``.\n\n    epsilon : float\n        Value need to be greater than ``0``. Defaults to ``1e-7``.\n\n    step : float\n        Learning rate, defaults to ``1.0``. Original paper doesn\'t have\n        learning rate specified in the paper. Step value equal to ``1.0``\n        allow to achieve the same effect, since multiplication by one won\'t\n        have any effect on the update.\n\n    {GradientDescent.batch_size}\n\n    {BaseOptimizer.regularizer}\n\n    {BaseOptimizer.network}\n\n    {BaseOptimizer.loss}\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    Attributes\n    ----------\n    {GradientDescent.Attributes}\n\n    Methods\n    -------\n    {GradientDescent.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.Adadelta(network)\n    >>> optimizer.train(x_train, y_train)\n\n    References\n    ----------\n    [1] Matthew D. Zeiler,\n        ADADELTA: An Adaptive Learning Rate Method\n        https://arxiv.org/pdf/1212.5701.pdf\n    """"""\n    step = ScalarVariableProperty(default=1.0)\n    rho = ProperFractionProperty(default=0.95)\n    epsilon = NumberProperty(default=1e-7, minval=0)\n\n    def init_train_updates(self):\n        optimizer = tf.train.AdadeltaOptimizer(\n            rho=self.rho,\n            epsilon=self.epsilon,\n            learning_rate=self.step,\n        )\n        self.functions.optimizer = optimizer\n        return [optimizer.minimize(self.variables.loss)]\n'"
neupy/algorithms/gd/adagrad.py,1,"b'import tensorflow as tf\n\nfrom .base import GradientDescent\n\n\n__all__ = (\'Adagrad\',)\n\n\nclass Adagrad(GradientDescent):\n    """"""\n    Adagrad algorithm.\n\n    Parameters\n    ----------\n    {GradientDescent.Parameters}\n\n    Attributes\n    ----------\n    {GradientDescent.Attributes}\n\n    Methods\n    -------\n    {GradientDescent.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.Adagrad(network)\n    >>> optimizer.train(x_train, y_train)\n\n    References\n    ----------\n    [1] John Duchi, Elad Hazan, Yoram Singer,\n        Adaptive Subgradient Methods for Online Learning and Stochastic\n        Optimization\n        http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n    """"""\n    def init_train_updates(self):\n        optimizer = tf.train.AdagradOptimizer(\n            learning_rate=self.step,\n        )\n        self.functions.optimizer = optimizer\n        return [optimizer.minimize(self.variables.loss)]\n'"
neupy/algorithms/gd/adam.py,1,"b'import tensorflow as tf\n\nfrom neupy.core.properties import (\n    ProperFractionProperty,\n    ScalarVariableProperty,\n    NumberProperty,\n)\nfrom .base import GradientDescent\n\n\n__all__ = (\'Adam\',)\n\n\nclass Adam(GradientDescent):\n    """"""\n    Adam algorithm.\n\n    Parameters\n    ----------\n    beta1 : float\n        Decay rate. Value need to be between ``0`` and ``1``.\n        Defaults to ``0.95``.\n\n    beta2 : float\n        Decay rate. Value need to be between ``0`` and ``1``.\n        Defaults to ``0.95``.\n\n    epsilon : float\n        Value need to be greater than ``0``. Defaults to ``1e-5``.\n\n    step : float\n        Learning rate, defaults to ``0.001``.\n\n    {GradientDescent.batch_size}\n\n    {BaseOptimizer.regularizer}\n\n    {BaseOptimizer.network}\n\n    {BaseOptimizer.loss}\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    {Verbose.verbose}\n\n    Attributes\n    ----------\n    {GradientDescent.Attributes}\n\n    Methods\n    -------\n    {GradientDescent.Methods}\n\n    References\n    ----------\n    [1] Diederik P. Kingma, Jimmy Lei Ba\n        Adam: a Method for Stochastic Optimization.\n        https://arxiv.org/pdf/1412.6980.pdf\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.Adam(network)\n    >>> optimizer.train(x_train, y_train)\n    """"""\n    step = ScalarVariableProperty(default=0.001)\n    beta1 = ProperFractionProperty(default=0.9)\n    beta2 = ProperFractionProperty(default=0.999)\n    epsilon = NumberProperty(default=1e-7, minval=0)\n\n    def init_train_updates(self):\n        optimizer = tf.train.AdamOptimizer(\n            beta1=self.beta1,\n            beta2=self.beta2,\n            epsilon=self.epsilon,\n            learning_rate=self.step,\n        )\n        self.functions.optimizer = optimizer\n        return [optimizer.minimize(self.variables.loss)]\n'"
neupy/algorithms/gd/adamax.py,11,"b'import tensorflow as tf\n\nfrom neupy.utils import asfloat\nfrom neupy.core.properties import (\n    ProperFractionProperty,\n    ScalarVariableProperty,\n    NumberProperty,\n)\nfrom .base import GradientDescent\n\n\n__all__ = (\'Adamax\',)\n\n\nclass Adamax(GradientDescent):\n    """"""\n    AdaMax algorithm.\n\n    Parameters\n    ----------\n    beta1 : float\n        Decay rate. Value need to be between ``0`` and ``1``.\n        Defaults to ``0.9``.\n\n    beta2 : float\n        Decay rate. Value need to be between ``0`` and ``1``.\n        Defaults to ``0.999``.\n\n    epsilon : float\n        Value need to be greater than ``0``. Defaults to ``1e-7``.\n\n    step : float\n        Learning rate, defaults to ``0.002``.\n\n    {GradientDescent.batch_size}\n\n    {BaseOptimizer.regularizer}\n\n    {BaseOptimizer.network}\n\n    {BaseOptimizer.loss}\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    {Verbose.verbose}\n\n    Attributes\n    ----------\n    {GradientDescent.Attributes}\n\n    Methods\n    -------\n    {GradientDescent.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> mnet = algorithms.Adamax(network)\n    >>> mnet.train(x_train, y_train)\n\n    References\n    ----------\n    [1] Diederik P. Kingma, Jimmy Lei Ba\n        Adam: a Method for Stochastic Optimization.\n        https://arxiv.org/pdf/1412.6980.pdf\n    """"""\n    step = ScalarVariableProperty(default=0.002)\n    beta1 = ProperFractionProperty(default=0.9)\n    beta2 = ProperFractionProperty(default=0.999)\n    epsilon = NumberProperty(default=1e-7, minval=0)\n\n    def init_functions(self):\n        self.variables.iteration = tf.Variable(\n            asfloat(1),\n            name=\'iteration\',\n            dtype=tf.float32,\n        )\n        super(Adamax, self).init_functions()\n\n    def init_train_updates(self):\n        iteration = self.variables.iteration\n        beta1 = self.beta1\n        beta2 = self.beta2\n\n        updates = []\n        variables = []\n\n        for (_, _), variable in self.network.variables.items():\n            if variable.trainable:\n                variables.append(variable)\n\n        gradients = tf.gradients(self.variables.loss, variables)\n        scale = self.step / (1. - beta1 ** iteration)\n\n        for parameter, gradient in zip(variables, gradients):\n            prev_first_moment = tf.Variable(\n                tf.zeros(parameter.shape),\n                name=""{}/prev-first-moment"".format(parameter.op.name),\n                dtype=tf.float32,\n            )\n            prev_weighted_inf_norm = tf.Variable(\n                tf.zeros(parameter.shape),\n                name=""{}/prev-weighted-inf-norm"".format(parameter.op.name),\n                dtype=tf.float32,\n            )\n\n            first_moment = beta1 * prev_first_moment + (1. - beta1) * gradient\n            weighted_inf_norm = tf.maximum(\n                beta2 * prev_weighted_inf_norm,\n                tf.abs(gradient),\n            )\n\n            parameter_delta = (\n                scale * (first_moment / (weighted_inf_norm + self.epsilon)))\n\n            updates.extend([\n                (prev_first_moment, first_moment),\n                (prev_weighted_inf_norm, weighted_inf_norm),\n                (parameter, parameter - parameter_delta),\n            ])\n\n        updates.append((iteration, iteration + 1))\n        return updates\n'"
neupy/algorithms/gd/base.py,8,"b'from __future__ import division\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy import layers\nfrom neupy.core.properties import (\n    FunctionWithOptionsProperty,\n    ScalarVariableProperty,\n    IntProperty, Property,\n)\nfrom neupy.utils import (\n    AttributeKeyDict, format_data,\n    as_tuple, iters, tf_utils,\n)\nfrom neupy.algorithms.gd import objectives\nfrom neupy.exceptions import InvalidConnection\nfrom neupy.algorithms.base import BaseNetwork\n\n\n__all__ = (\'BaseOptimizer\', \'GradientDescent\')\n\n\nclass BaseOptimizer(BaseNetwork):\n    """"""\n    Gradient descent algorithm.\n\n    Parameters\n    ----------\n    network : list, tuple or LayerConnection instance\n        Network\'s architecture. There are a few ways\n        to define it.\n\n        - List of layers.\n          For instance, ``[Input(2), Tanh(4), Relu(1)]``.\n\n        - Constructed layers.\n          For instance, ``Input(2) >> Tanh(4) >> Relu(1)``.\n\n    regularizer : function or None\n        Network\'s regularizer.\n\n    loss : str or function\n        Error/loss function. Defaults to ``mse``.\n\n        - ``mae`` - Mean Absolute Error.\n\n        - ``mse`` - Mean Squared Error.\n\n        - ``rmse`` - Root Mean Squared Error.\n\n        - ``msle`` - Mean Squared Logarithmic Error.\n\n        - ``rmsle`` - Root Mean Squared Logarithmic Error.\n\n        - ``categorical_crossentropy`` - Categorical cross entropy.\n\n        - ``binary_crossentropy`` - Binary cross entropy.\n\n        - ``binary_hinge`` - Binary hinge entropy.\n\n        - ``categorical_hinge`` - Categorical hinge entropy.\n\n        - Custom function which accepts two mandatory arguments.\n          The first one is expected value and the second one is\n          predicted value. Example:\n\n        .. code-block:: python\n\n            def custom_func(expected, predicted):\n                return expected - predicted\n\n    step : float, Variable\n        Learning rate, defaults to ``0.1``.\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    {BaseNetwork.verbose}\n\n    Attributes\n    ----------\n    {BaseNetwork.Attributes}\n\n    Methods\n    -------\n    {BaseSkeleton.predict}\n\n    train(X_train, y_train, X_test=None, y_test=None, epochs=100)\n        Train network. You can control network\'s training procedure\n        with ``epochs`` parameter. The ``X_test`` and ``y_test`` should\n        be presented both in case network\'s validation required\n        after each training epoch.\n\n    {BaseSkeleton.fit}\n    """"""\n    step = ScalarVariableProperty(default=0.1)\n    target = Property(default=None, allow_none=True)\n    regularizer = Property(default=None, allow_none=True)\n    loss = FunctionWithOptionsProperty(default=\'mse\', choices={\n        \'mae\': objectives.mae,\n        \'mse\': objectives.mse,\n        \'rmse\': objectives.rmse,\n        \'msle\': objectives.msle,\n        \'rmsle\': objectives.rmsle,\n\n        \'binary_crossentropy\': objectives.binary_crossentropy,\n        \'categorical_crossentropy\': objectives.categorical_crossentropy,\n\n        \'binary_hinge\': objectives.binary_hinge,\n        \'categorical_hinge\': objectives.categorical_hinge,\n    })\n\n    def __init__(self, network, options=None, **kwargs):\n        options = options or kwargs\n\n        if isinstance(network, (list, tuple)):\n            network = layers.join(*network)\n\n        self.network = network\n\n        if len(self.network.output_layers) != 1:\n            n_outputs = len(network.output_layers)\n\n            raise InvalidConnection(\n                ""Connection should have one output ""\n                ""layer, got {}"".format(n_outputs))\n\n        target = options.get(\'target\')\n        if target is not None and isinstance(target, (list, tuple)):\n            options[\'target\'] = tf.placeholder(tf.float32, shape=target)\n\n        self.target = self.network.targets\n        super(BaseOptimizer, self).__init__(**options)\n\n        start_init_time = time.time()\n        self.logs.message(\n            ""TENSORFLOW"",\n            ""Initializing Tensorflow variables and functions."")\n\n        self.variables = AttributeKeyDict()\n        self.functions = AttributeKeyDict()\n        self.network.outputs\n        self.init_functions()\n\n        self.logs.message(\n            ""TENSORFLOW"",\n            ""Initialization finished successfully. It took {:.2f} seconds""\n            """".format(time.time() - start_init_time))\n\n    def init_train_updates(self):\n        raise NotImplementedError()\n\n    def init_functions(self):\n        loss = self.loss(self.target, self.network.outputs)\n        val_loss = self.loss(self.target, self.network.training_outputs)\n\n        if self.regularizer is not None:\n            loss += self.regularizer(self.network)\n\n        self.variables.update(\n            step=self.step,\n            loss=loss,\n            val_loss=val_loss,\n        )\n\n        with tf.name_scope(\'training-updates\'):\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n            with tf.control_dependencies(update_ops):\n                training_updates = self.init_train_updates()\n                training_updates.extend(update_ops)\n\n        tf_utils.initialize_uninitialized_variables()\n\n        with tf.name_scope(\'optimizer\'):\n            self.functions.update(\n                predict=tf_utils.function(\n                    inputs=as_tuple(self.network.inputs),\n                    outputs=self.network.outputs,\n                    name=\'predict\'\n                ),\n                one_training_update=tf_utils.function(\n                    inputs=as_tuple(self.network.inputs, self.target),\n                    outputs=loss,\n                    updates=training_updates,\n                    name=\'one-update-step\'\n                ),\n                score=tf_utils.function(\n                    inputs=as_tuple(self.network.inputs, self.target),\n                    outputs=val_loss,\n                    name=\'score\'\n                ),\n            )\n\n    def format_input(self, X):\n        X = as_tuple(X)\n        X_formatted = []\n\n        if len(X) != len(self.network.input_layers):\n            raise ValueError(\n                ""Number of inputs doesn\'t match number ""\n                ""of input layers in the network."")\n\n        for input, input_layer in zip(X, self.network.input_layers):\n            input_shape = tf.TensorShape(input_layer.input_shape)\n            is_feature1d = (input_shape.ndims == 2 and input_shape[1] == 1)\n            formatted_input = format_data(input, is_feature1d=is_feature1d)\n\n            if (formatted_input.ndim + 1) == input_shape.ndims:\n                # We assume that when one dimension was missed than user\n                # wants to propagate single sample through the network\n                formatted_input = np.expand_dims(formatted_input, axis=0)\n\n            X_formatted.append(formatted_input)\n\n        return X_formatted\n\n    def format_target(self, y):\n        output_shape = tf.TensorShape(self.network.output_shape)\n        is_feature1d = (output_shape.ndims == 2 and output_shape[1] == 1)\n        formatted_target = format_data(y, is_feature1d=is_feature1d)\n\n        if (formatted_target.ndim + 1) == len(output_shape):\n            # We assume that when one dimension was missed than user\n            # wants to propagate single sample through the network\n            formatted_target = np.expand_dims(formatted_target, axis=0)\n\n        return formatted_target\n\n    def score(self, X, y):\n        """"""\n        Calculate prediction accuracy for input data.\n\n        Parameters\n        ----------\n        X : array-like\n        y : array-like\n\n        Returns\n        -------\n        float\n            Prediction error.\n        """"""\n        X = self.format_input(X)\n        y = self.format_target(y)\n        return self.functions.score(*as_tuple(X, y))\n\n    def predict(self, *X, **kwargs):\n        """"""\n        Makes a raw prediction.\n\n        Parameters\n        ----------\n        X : array-like\n\n        Returns\n        -------\n        array-like\n        """"""\n        default_batch_size = getattr(self, \'batch_size\', None)\n        predict_kwargs = dict(\n            batch_size=kwargs.pop(\'batch_size\', default_batch_size),\n            verbose=self.verbose,\n        )\n\n        # We require do to this check for python 2 compatibility\n        if kwargs:\n            raise TypeError(""Unknown arguments: {}"".format(kwargs))\n\n        return self.network.predict(*self.format_input(X), **predict_kwargs)\n\n    def train(self, X_train, y_train, X_test=None, y_test=None,\n              *args, **kwargs):\n\n        is_test_data_partialy_missing = (\n            (X_test is None and y_test is not None) or\n            (X_test is not None and y_test is None)\n        )\n\n        if is_test_data_partialy_missing:\n            raise ValueError(\n                ""Input or target test samples are missed. They ""\n                ""must be defined together or none of them."")\n\n        X_train = self.format_input(X_train)\n        y_train = self.format_target(y_train)\n\n        if X_test is not None:\n            X_test = self.format_input(X_test)\n            y_test = self.format_target(y_test)\n\n        return super(BaseOptimizer, self).train(\n            X_train=X_train, y_train=y_train,\n            X_test=X_test, y_test=y_test,\n            *args, **kwargs)\n\n    def one_training_update(self, X_train, y_train):\n        return self.functions.one_training_update(\n            *as_tuple(X_train, y_train))\n\n    def get_params(self, deep=False, with_network=True):\n        params = super(BaseOptimizer, self).get_params()\n        if with_network:\n            params[\'network\'] = self.network\n        return params\n\n    def __reduce__(self):\n        parameters = self.get_params(with_network=False)\n\n        # We only need to know placeholders shape\n        # in order to be able to reconstruct it\n        parameters[\'target\'] = tf_utils.shape_to_tuple(\n            parameters[\'target\'].shape)\n\n        args = (self.network, parameters)\n        return (self.__class__, args)\n\n    def __repr__(self):\n        return ""{}({}, {})"".format(\n            self.__class__.__name__,\n            self.network,\n            self.repr_options())\n\n\nclass GradientDescent(BaseOptimizer):\n    """"""\n    Mini-batch Gradient Descent algorithm.\n\n    Parameters\n    ----------\n    batch_size : int or None\n        Set up min-batch size. The ``None`` value will ensure that all data\n        samples will be propagated through the network at once.\n        Defaults to ``128``.\n\n    {BaseOptimizer.Parameters}\n\n    Attributes\n    ----------\n    {BaseOptimizer.Attributes}\n\n    Methods\n    -------\n    {BaseOptimizer.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.algorithms import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.GradientDescent(network, batch_size=1)\n    >>> optimizer.train(x_train, y_train)\n    """"""\n    batch_size = IntProperty(default=128, minval=0, allow_none=True)\n\n    def init_train_updates(self):\n        optimizer = tf.train.GradientDescentOptimizer(\n            learning_rate=self.step,\n        )\n        self.functions.optimizer = optimizer\n        return [optimizer.minimize(self.variables.loss)]\n\n    def one_training_update(self, X_train, y_train):\n        """"""\n        Train one epoch.\n\n        Parameters\n        ----------\n        X_train : array-like\n            Training input dataset.\n\n        y_train : array-like\n            Training target dataset.\n\n        Returns\n        -------\n        float\n            Training error.\n        """"""\n        return self.functions.one_training_update(\n            *as_tuple(X_train, y_train))\n\n    def score(self, X, y):\n        """"""\n        Check the prediction error for the specified input samples\n        and their targets.\n\n        Parameters\n        ----------\n        X : array-like\n        y : array-like\n\n        Returns\n        -------\n        float\n            Prediction error.\n        """"""\n        X = self.format_input(X)\n        y = self.format_target(y)\n\n        return iters.apply_batches(\n            function=self.functions.score,\n            inputs=as_tuple(X, y),\n            batch_size=self.batch_size,\n            show_output=True,\n            show_progressbar=self.logs.enable,\n            average_outputs=True,\n        )\n'"
neupy/algorithms/gd/conjgrad.py,12,"b'import tensorflow as tf\n\nfrom neupy.utils import dot, function_name_scope, asfloat, make_single_vector\nfrom neupy.core.properties import (ChoiceProperty, NumberProperty,\n                                   WithdrawProperty)\nfrom neupy.utils.tf_utils import setup_parameter_updates\nfrom .base import BaseOptimizer\nfrom .quasi_newton import safe_division, WolfeLineSearchForStep\n\n\n__all__ = (\'ConjugateGradient\',)\n\n\n@function_name_scope\ndef fletcher_reeves(old_g, new_g, delta_w, epsilon=1e-7):\n    return safe_division(\n        dot(new_g, new_g),\n        dot(old_g, old_g),\n        epsilon,\n    )\n\n\n@function_name_scope\ndef polak_ribiere(old_g, new_g, delta_w, epsilon=1e-7):\n    return safe_division(\n        dot(new_g, new_g - old_g),\n        dot(old_g, old_g),\n        epsilon,\n    )\n\n\n@function_name_scope\ndef hentenes_stiefel(old_g, new_g, delta_w, epsilon=1e-7):\n    gradient_delta = new_g - old_g\n    return safe_division(\n        dot(gradient_delta, new_g),\n        dot(delta_w, gradient_delta),\n        epsilon,\n    )\n\n\n@function_name_scope\ndef liu_storey(old_g, new_g, delta_w, epsilon=1e-7):\n    return -safe_division(\n        dot(new_g, new_g - old_g),\n        dot(delta_w, old_g),\n        epsilon,\n    )\n\n\n@function_name_scope\ndef dai_yuan(old_g, new_g, delta_w, epsilon=1e-7):\n    return safe_division(\n        dot(new_g, new_g),\n        dot(new_g - old_g, delta_w),\n        epsilon,\n    )\n\n\nclass ConjugateGradient(WolfeLineSearchForStep, BaseOptimizer):\n\n    """"""\n    Conjugate Gradient algorithm.\n\n    Parameters\n    ----------\n    update_function : ``fletcher_reeves``, ``polak_ribiere``,\\\n    ``hentenes_stiefel``, ``dai_yuan``, ``liu_storey``\n        Update function. Defaults to ``fletcher_reeves``.\n\n    epsilon : float\n        Ensures computational stability during the division in\n        ``update_function`` when denominator is very small number.\n        Defaults to ``1e-7``.\n\n    {WolfeLineSearchForStep.Parameters}\n\n    {BaseOptimizer.network}\n\n    {BaseOptimizer.loss}\n\n    {BaseOptimizer.show_epoch}\n\n    {BaseOptimizer.shuffle_data}\n\n    {BaseOptimizer.signals}\n\n    {BaseOptimizer.verbose}\n\n    {BaseOptimizer.regularizer}\n\n    Attributes\n    ----------\n    {BaseOptimizer.Attributes}\n\n    Methods\n    -------\n    {BaseOptimizer.Methods}\n\n    Examples\n    --------\n    >>> from sklearn import datasets, preprocessing\n    >>> from sklearn.model_selection import train_test_split\n    >>> from neupy import algorithms, layers\n    >>>\n    >>> dataset = datasets.load_boston()\n    >>> data, target = dataset.data, dataset.target\n    >>>\n    >>> data_scaler = preprocessing.MinMaxScaler()\n    >>> target_scaler = preprocessing.MinMaxScaler()\n    >>>\n    >>> x_train, x_test, y_train, y_test = train_test_split(\n    ...     data_scaler.fit_transform(data),\n    ...     target_scaler.fit_transform(target),\n    ...     test_size=0.15\n    ... )\n    >>>\n    >>> cgnet = algorithms.ConjugateGradient(\n    ...     network=[\n    ...         layers.Input(13),\n    ...         layers.Sigmoid(50),\n    ...         layers.Sigmoid(1),\n    ...     ],\n    ...     update_function=\'fletcher_reeves\',\n    ...     verbose=False\n    ... )\n    >>>\n    >>> cgnet.train(x_train, y_train, epochs=100)\n    >>> y_predict = cgnet.predict(x_test).round(1)\n    >>>\n    >>> real = target_scaler.inverse_transform(y_test)\n    >>> predicted = target_scaler.inverse_transform(y_predict)\n\n    References\n    ----------\n    [1] Jorge Nocedal, Stephen J. Wright, Numerical Optimization.\n        Chapter 5, Conjugate Gradient Methods, p. 101-133\n    """"""\n    epsilon = NumberProperty(default=1e-7, minval=0)\n    update_function = ChoiceProperty(\n        default=\'fletcher_reeves\',\n        choices={\n            \'fletcher_reeves\': fletcher_reeves,\n            \'polak_ribiere\': polak_ribiere,\n            \'hentenes_stiefel\': hentenes_stiefel,\n            \'liu_storey\': liu_storey,\n            \'dai_yuan\': dai_yuan,\n        }\n    )\n    step = WithdrawProperty()\n\n    def init_functions(self):\n        n_parameters = self.network.n_parameters\n        self.variables.update(\n            prev_delta=tf.Variable(\n                tf.zeros([n_parameters]),\n                name=""conj-grad/prev-delta"",\n                dtype=tf.float32,\n            ),\n            prev_gradient=tf.Variable(\n                tf.zeros([n_parameters]),\n                name=""conj-grad/prev-gradient"",\n                dtype=tf.float32,\n            ),\n            iteration=tf.Variable(\n                asfloat(self.last_epoch),\n                name=\'conj-grad/current-iteration\',\n                dtype=tf.float32\n            ),\n        )\n        super(ConjugateGradient, self).init_functions()\n\n    def init_train_updates(self):\n        iteration = self.variables.iteration\n        previous_delta = self.variables.prev_delta\n        previous_gradient = self.variables.prev_gradient\n\n        n_parameters = self.network.n_parameters\n        variables = self.network.variables\n        parameters = [var for var in variables.values() if var.trainable]\n        param_vector = make_single_vector(parameters)\n\n        gradients = tf.gradients(self.variables.loss, parameters)\n        full_gradient = make_single_vector(gradients)\n\n        beta = self.update_function(\n            previous_gradient, full_gradient, previous_delta, self.epsilon)\n\n        parameter_delta = tf.where(\n            tf.equal(tf.mod(iteration, n_parameters), 0),\n            -full_gradient,\n            -full_gradient + beta * previous_delta\n        )\n\n        step = self.find_optimal_step(param_vector, parameter_delta)\n        updated_parameters = param_vector + step * parameter_delta\n        updates = setup_parameter_updates(parameters, updated_parameters)\n\n        # We have to compute these values first, otherwise\n        # parallelization, in tensorflow, can mix update order\n        # and, for example, previous gradient can be equal to\n        # current gradient value. It happens because tensorflow\n        # try to execute operations in parallel.\n        with tf.control_dependencies([full_gradient, parameter_delta]):\n            updates.extend([\n                previous_gradient.assign(full_gradient),\n                previous_delta.assign(parameter_delta),\n                iteration.assign(iteration + 1),\n            ])\n\n        return updates\n'"
neupy/algorithms/gd/hessdiag.py,4,"b'from __future__ import division\n\nimport tensorflow as tf\n\nfrom neupy.core.properties import ProperFractionProperty\nfrom neupy.utils import flatten, make_single_vector\nfrom neupy.utils.tf_utils import setup_parameter_updates\n\nfrom .base import BaseOptimizer\n\n\n__all__ = (\'HessianDiagonal\',)\n\n\nclass HessianDiagonal(BaseOptimizer):\n    """"""\n    Algorithm that uses calculates only diagonal values from the Hessian matrix\n    and uses it instead of the Hessian matrix.\n\n    Parameters\n    ----------\n    min_eigval : float\n        Set up minimum eigenvalue for Hessian diagonal matrix. After a few\n        iteration elements will be extremely small and matrix inverse\n        produce huge number in hessian diagonal elements. This\n        parameter control diagonal elements size. Defaults to ``1e-2``.\n\n    {BaseOptimizer.Parameters}\n\n    Attributes\n    ----------\n    {BaseOptimizer.Attributes}\n\n    Methods\n    -------\n    {BaseOptimizer.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.HessianDiagonal(network)\n    >>> optimizer.train(x_train, y_train)\n\n    Notes\n    -----\n    - Method requires all training data during propagation, which means\n      it cannot be trained with mini-batches.\n\n    See Also\n    --------\n    :network:`BaseOptimizer` : BaseOptimizer algorithm.\n    :network:`Hessian` : Newton\'s method.\n    """"""\n    min_eigval = ProperFractionProperty(default=1e-2)\n\n    def init_train_updates(self):\n        step = self.step\n        inv_min_eigval = 1 / self.min_eigval\n        variables = self.network.variables\n        parameters = [var for var in variables.values() if var.trainable]\n        param_vector = make_single_vector(parameters)\n\n        gradients = tf.gradients(self.variables.loss, parameters)\n        full_gradient = make_single_vector(gradients)\n\n        second_derivatives = []\n        for parameter, gradient in zip(parameters, gradients):\n            second_derivative, = tf.gradients(gradient, parameter)\n            second_derivatives.append(flatten(second_derivative))\n\n        hessian_diag = tf.concat(second_derivatives, axis=0)\n\n        # it\'s easier to clip inverse hessian rather than the hessian,.\n        inv_hessian_diag = tf.clip_by_value(\n            # inverse for diagonal matrix easy to compute with\n            # elementwise inverse operation.\n            1 / hessian_diag,\n            -inv_min_eigval,\n            inv_min_eigval,\n        )\n        updates = setup_parameter_updates(\n            parameters,\n            param_vector - step * full_gradient * inv_hessian_diag\n        )\n        return updates\n'"
neupy/algorithms/gd/hessian.py,9,"b'import tensorflow as tf\n\nfrom neupy.core.properties import BoundedProperty, WithdrawProperty\nfrom neupy.utils import (\n    asfloat, flatten, function_name_scope,\n    make_single_vector,\n)\nfrom neupy.utils.tf_utils import setup_parameter_updates\nfrom .base import BaseOptimizer\n\n\n__all__ = (\'Hessian\',)\n\n\n@function_name_scope\ndef find_hessian_and_gradient(error_function, parameters):\n    """"""\n    Compute hessian matrix and gradient vector.\n\n    Parameters\n    ----------\n    error_function : Tensor\n\n    parameters : list of Tensorfow variable\n        Neural network parameters (e.g. weights, biases).\n\n    Returns\n    -------\n    Tensorfow variable\n    """"""\n    gradients = tf.gradients(error_function, parameters)\n    full_gradient = make_single_vector(gradients)\n\n    full_gradient_shape = tf.shape(full_gradient)\n    n_samples = full_gradient_shape[0]\n\n    def compute_gradient_per_value(index, result):\n        gradients = tf.gradients(full_gradient[index], parameters)\n        hessian = make_single_vector(gradients)\n        return (index + 1, result.write(index, hessian))\n\n    _, hessian = tf.while_loop(\n        lambda index, _: index < n_samples,\n        compute_gradient_per_value,\n        [\n            tf.constant(0, tf.int32),\n            tf.TensorArray(tf.float32, size=n_samples),\n        ]\n    )\n\n    return hessian.stack(), full_gradient\n\n\nclass Hessian(BaseOptimizer):\n    """"""\n    Hessian gradient decent optimization, also known as Newton\'s method. This\n    algorithm uses second-order derivative (hessian matrix) in order to\n    choose correct step during the training iteration. Because of this,\n    method doesn\'t have ``step`` parameter.\n\n    Parameters\n    ----------\n    penalty_const : float\n        Inverse hessian could be singular matrix. For this reason\n        algorithm include penalty that add to hessian matrix identity\n        multiplied by defined constant. Defaults to ``1``.\n\n    {BaseOptimizer.network}\n\n    {BaseOptimizer.loss}\n\n    {BaseOptimizer.regularizer}\n\n    {BaseOptimizer.show_epoch}\n\n    {BaseOptimizer.shuffle_data}\n\n    {BaseOptimizer.signals}\n\n    {BaseOptimizer.verbose}\n\n    Attributes\n    ----------\n    {BaseOptimizer.Attributes}\n\n    Methods\n    -------\n    {BaseOptimizer.Methods}\n\n    Notes\n    -----\n    - Method requires all training data during propagation, which means\n      it cannot be trained with mini-batches.\n\n    - This method calculates full hessian matrix which means it will compute\n      matrix with NxN parameters, where N = number of parameters in the\n      network.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.Hessian(network)\n    >>> optimizer.train(x_train, y_train)\n\n    See Also\n    --------\n    :network:`HessianDiagonal` : Hessian diagonal approximation.\n    """"""\n    penalty_const = BoundedProperty(default=1, minval=0)\n    step = WithdrawProperty()\n\n    def init_train_updates(self):\n        penalty_const = asfloat(self.penalty_const)\n\n        n_parameters = self.network.n_parameters\n        variables = self.network.variables\n        parameters = [var for var in variables.values() if var.trainable]\n        param_vector = make_single_vector(parameters)\n\n        hessian_matrix, full_gradient = find_hessian_and_gradient(\n            self.variables.loss, parameters\n        )\n        parameter_update = tf.matrix_solve(\n            hessian_matrix + penalty_const * tf.eye(n_parameters),\n            tf.reshape(full_gradient, [-1, 1])\n        )\n        updated_parameters = param_vector - flatten(parameter_update)\n        updates = setup_parameter_updates(parameters, updated_parameters)\n\n        return updates\n'"
neupy/algorithms/gd/lev_marq.py,13,"b'import numpy as np\nimport tensorflow as tf\n\nfrom neupy.utils import (\n    tensorflow_session, flatten,\n    function_name_scope, make_single_vector,\n)\nfrom neupy.core.properties import (BoundedProperty, ChoiceProperty,\n                                   WithdrawProperty)\nfrom neupy.algorithms import BaseOptimizer\nfrom neupy.algorithms.gd import objectives\nfrom neupy.utils.tf_utils import setup_parameter_updates\n\n\n__all__ = (\'LevenbergMarquardt\',)\n\n\n@function_name_scope\ndef compute_jacobian(values, parameters):\n    """"""\n    Compute Jacobian matrix.\n\n    Parameters\n    ----------\n    values : Tensorfow variable\n        Computed MSE for each sample separately.\n\n    parameters : list of Tensorfow variable\n        Neural network parameters (e.g. weights, biases).\n\n    Returns\n    -------\n    Tensorfow variable\n    """"""\n    values_shape = tf.shape(values)\n    n_samples = values_shape[0]\n\n    def compute_gradient_per_value(index, result):\n        gradients = tf.gradients(values[index], parameters)\n        full_gradient = make_single_vector(gradients)\n        return index + 1, result.write(index, full_gradient)\n\n    _, jacobian = tf.while_loop(\n        lambda index, _: index < n_samples,\n        compute_gradient_per_value,\n        [\n            tf.constant(0, tf.int32),\n            tf.TensorArray(tf.float32, size=n_samples),\n        ]\n    )\n\n    return jacobian.stack()\n\n\nclass LevenbergMarquardt(BaseOptimizer):\n    """"""\n    Levenberg-Marquardt algorithm is a variation of the Newton\'s method.\n    It minimizes MSE error. The algorithm approximates Hessian matrix using\n    dot product between two jacobian matrices.\n\n    Notes\n    -----\n    - Method requires all training data during propagation, which means\n      it cannot be trained with mini-batches.\n\n    - Network minimizes only Mean Squared Error (MSE) loss function.\n\n    - Efficient for small training datasets, because it\n      computes gradient per each sample separately.\n\n    - Efficient for small-sized networks.\n\n    Parameters\n    ----------\n    {BaseOptimizer.network}\n\n    mu : float\n        Control inversion for J.T * J matrix, defaults to ``0.1``.\n\n    mu_update_factor : float\n        Factor to decrease the mu if error was reduced after last update,\n        otherwise increase mu by the same factor. Defaults to ``1.2``\n\n    error : {{``mse``}}\n        Levenberg-Marquardt works only for quadratic functions.\n        Defaults to ``mse``.\n\n    {BaseOptimizer.show_epoch}\n\n    {BaseOptimizer.shuffle_data}\n\n    {BaseOptimizer.signals}\n\n    {BaseOptimizer.verbose}\n\n    Attributes\n    ----------\n    {BaseOptimizer.Attributes}\n\n    Methods\n    -------\n    {BaseOptimizer.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.LevenbergMarquardt(network)\n    >>> optimizer.train(x_train, y_train)\n\n    See Also\n    --------\n    :network:`BaseOptimizer` : BaseOptimizer algorithm.\n    """"""\n    mu = BoundedProperty(default=0.01, minval=0)\n    mu_update_factor = BoundedProperty(default=1.2, minval=1)\n    loss = ChoiceProperty(default=\'mse\', choices={\'mse\': objectives.mse})\n\n    step = WithdrawProperty()\n    regularizer = WithdrawProperty()\n\n    def init_functions(self):\n        self.variables.update(\n            mu=tf.Variable(self.mu, name=\'lev-marq/mu\'),\n            last_error=tf.Variable(np.nan, name=\'lev-marq/last-error\'),\n        )\n        super(LevenbergMarquardt, self).init_functions()\n\n    def init_train_updates(self):\n        training_outputs = self.network.training_outputs\n        last_error = self.variables.last_error\n        error_func = self.variables.loss\n        mu = self.variables.mu\n\n        new_mu = tf.where(\n            tf.less(last_error, error_func),\n            mu * self.mu_update_factor,\n            mu / self.mu_update_factor,\n        )\n\n        err_for_each_sample = flatten((self.target - training_outputs) ** 2)\n\n        variables = self.network.variables\n        params = [var for var in variables.values() if var.trainable]\n        param_vector = make_single_vector(params)\n\n        J = compute_jacobian(err_for_each_sample, params)\n        J_T = tf.transpose(J)\n        n_params = J.shape[1]\n\n        parameter_update = tf.matrix_solve(\n            tf.matmul(J_T, J) + new_mu * tf.eye(n_params.value),\n            tf.matmul(J_T, tf.expand_dims(err_for_each_sample, 1))\n        )\n        updated_params = param_vector - flatten(parameter_update)\n\n        updates = [(mu, new_mu)]\n        parameter_updates = setup_parameter_updates(params, updated_params)\n        updates.extend(parameter_updates)\n\n        return updates\n\n    def one_training_update(self, X_train, y_train):\n        if self.errors.train:\n            last_error = self.errors.train[-1]\n            self.variables.last_error.load(last_error, tensorflow_session())\n\n        return super(LevenbergMarquardt, self).one_training_update(\n            X_train, y_train)\n'"
neupy/algorithms/gd/momentum.py,1,"b'import tensorflow as tf\n\nfrom neupy.core.properties import ProperFractionProperty, Property\nfrom .base import GradientDescent\n\n\n__all__ = (\'Momentum\',)\n\n\nclass Momentum(GradientDescent):\n    """"""\n    Momentum algorithm.\n\n    Parameters\n    ----------\n    momentum : float\n        Control previous gradient ratio. Defaults to ``0.9``.\n\n    nesterov : bool\n        Instead of classic momentum computes Nesterov momentum.\n        Defaults to ``False``.\n\n    {GradientDescent.Parameters}\n\n    Attributes\n    ----------\n    {GradientDescent.Attributes}\n\n    Methods\n    -------\n    {GradientDescent.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.Momentum(network)\n    >>> optimizer.train(x_train, y_train)\n\n    See Also\n    --------\n    :network:`GradientDescent` : GradientDescent algorithm.\n    """"""\n    momentum = ProperFractionProperty(default=0.9)\n    nesterov = Property(default=False, expected_type=bool)\n\n    def init_train_updates(self):\n        optimizer = tf.train.MomentumOptimizer(\n            use_nesterov=self.nesterov,\n            momentum=self.momentum,\n            learning_rate=self.step,\n        )\n        self.functions.optimizer = optimizer\n        return [optimizer.minimize(self.variables.loss)]\n'"
neupy/algorithms/gd/objectives.py,19,"b'from __future__ import division\n\nimport tensorflow as tf\n\nfrom neupy.core.docs import shared_docs\nfrom neupy.utils import asfloat, function_name_scope\n\n\n__all__ = (\n    \'mse\', \'rmse\', \'mae\', \'msle\', \'rmsle\',\n    \'binary_crossentropy\', \'categorical_crossentropy\',\n    \'binary_hinge\', \'categorical_hinge\',\n)\n\n\nsmallest_positive_number = 1e-7  # for 32-bit float numbers\n\n\ndef error_function(expected, predicted):\n    """"""\n    Parameters\n    ----------\n    expected : array-like, tensorflow variable\n    predicted : array-like, tensorflow variable\n\n    Returns\n    -------\n    array-like, tensorflow variable\n    """"""\n    raise NotImplementedError\n\n\n@function_name_scope\n@shared_docs(error_function)\ndef mse(expected, predicted):\n    """"""\n    Mean squared error.\n\n    .. math::\n        mse(t, o) = mean((t - o) ^ 2)\n\n    where :math:`t=expected` and :math:`o=predicted`\n\n    Parameters\n    ----------\n    {error_function.expected}\n    {error_function.predicted}\n\n    Returns\n    -------\n    {error_function.Returns}\n    """"""\n    return tf.reduce_mean(tf.square(predicted - expected))\n\n\n@function_name_scope\n@shared_docs(error_function)\ndef rmse(expected, predicted):\n    """"""\n    Root mean squared error.\n\n    .. math::\n        rmse(t, o) = \\\\sqrt{{mean((t - o) ^ 2)}} = \\\\sqrt{{mse(t, 0)}}\n\n    where :math:`t=expected` and :math:`o=predicted`\n\n    Parameters\n    ----------\n    {error_function.expected}\n    {error_function.predicted}\n\n    Returns\n    -------\n    {error_function.Returns}\n    """"""\n    return tf.sqrt(mse(expected, predicted))\n\n\n@function_name_scope\n@shared_docs(error_function)\ndef mae(expected, predicted):\n    """"""\n    Mean absolute error.\n\n    .. math::\n        mae(t, o) = mean(\\\\left| t - o \\\\right|)\n\n    where :math:`t=expected` and :math:`o=predicted`\n\n    Parameters\n    ----------\n    {error_function.expected}\n    {error_function.predicted}\n\n    Returns\n    -------\n    {error_function.Returns}\n    """"""\n    return tf.reduce_mean(tf.abs(expected - predicted))\n\n\n@function_name_scope\n@shared_docs(error_function)\ndef msle(expected, predicted):\n    """"""\n    Mean squared logarithmic error.\n\n    .. math::\n        msle(t, o) = mean((\\\\log(t + 1) - \\\\log(o + 1)) ^ 2)\n\n    where :math:`t=expected` and :math:`o=predicted`\n\n    Parameters\n    ----------\n    {error_function.expected}\n    {error_function.predicted}\n\n    Returns\n    -------\n    {error_function.Returns}\n    """"""\n    squared_log = tf.square(tf.log(predicted + 1) - tf.log(expected + 1))\n    return tf.reduce_mean(squared_log)\n\n\n@function_name_scope\n@shared_docs(error_function)\ndef rmsle(expected, predicted):\n    """"""\n    Root mean squared logarithmic error.\n\n    .. math::\n        rmsle(t, o) = \\\\sqrt{{\n            mean((\\\\log(t + 1) - \\\\log(o + 1)) ^ 2)\n        }} = \\\\sqrt{{msle(t, o)}}\n\n    where :math:`t=expected` and :math:`o=predicted`\n\n    Parameters\n    ----------\n    {error_function.expected}\n    {error_function.predicted}\n\n    Returns\n    -------\n    {error_function.Returns}\n    """"""\n    return tf.sqrt(msle(expected, predicted))\n\n\n@function_name_scope\n@shared_docs(error_function)\ndef binary_crossentropy(expected, predicted):\n    """"""\n    Binary cross-entropy error.\n\n    .. math::\n        crossentropy(t, o) = -(t\\\\cdot log(o) + (1 - t) \\\\cdot log(1 - o))\n\n    where :math:`t=expected` and :math:`o=predicted`\n\n    Parameters\n    ----------\n    {error_function.expected}\n    {error_function.predicted}\n\n    Returns\n    -------\n    {error_function.Returns}\n    """"""\n    epsilon = smallest_positive_number\n\n    predicted = tf.clip_by_value(predicted, epsilon, 1.0 - epsilon)\n    crossentropy = (\n        -expected * tf.log(predicted) - (1 - expected) * tf.log(1 - predicted))\n\n    return tf.reduce_mean(crossentropy)\n\n\n@function_name_scope\n@shared_docs(error_function)\ndef categorical_crossentropy(expected, predicted):\n    """"""\n    Categorical cross-entropy error.\n\n    Parameters\n    ----------\n    {error_function.expected}\n    {error_function.predicted}\n\n    Returns\n    -------\n    {error_function.Returns}\n    """"""\n    epsilon = smallest_positive_number\n\n    predicted = tf.clip_by_value(predicted, epsilon, 1.0 - epsilon)\n    errors = tf.reduce_sum(expected * tf.log(predicted), axis=-1)\n\n    return -tf.reduce_mean(errors)\n\n\n@function_name_scope\ndef binary_hinge(expected, predicted, delta=1):\n    """"""\n    Computes the binary hinge loss between predictions\n    and targets.\n\n    .. math::\n        hinge(t, o) = \\\\max(0, \\\\delta - t o)\n\n    where :math:`t=expected` and :math:`o=predicted`\n\n    Parameters\n    ----------\n    expected : Tensorfow tensor\n        Targets in {-1, 1} such as ground truth labels.\n\n    predicted : Tensorfow tensor\n        Predictions in (-1, 1), such as hyperbolic tangent\n        output of a neural network.\n\n    delta : scalar\n        The hinge loss margin. Defaults to ``1``.\n\n    Returns\n    -------\n    Tensorfow tensor\n        An expression for the average binary hinge loss.\n\n    Notes\n    -----\n    This is an alternative to the binary cross-entropy\n    loss for binary classification problems.\n    """"""\n    error = tf.nn.relu(delta - predicted * expected)\n    return tf.reduce_mean(error)\n\n\n@function_name_scope\ndef categorical_hinge(expected, predicted, delta=1):\n    """"""\n    Computes the multi-class hinge loss between\n    predictions and targets.\n\n    .. math::\n        hinge_{i}(t, o) = \\\\max_{j \\\\not = o_i} (0, t_j - t_{o_i} + \\\\delta)\n\n    Parameters\n    ----------\n    expected : Tensorfow 2D tensor or 1D tensor\n        Either a vector of int giving the correct class index\n        per data point or a 2D tensor of one-hot encoding of\n        the correct class in the same layout as predictions\n        (non-binary targets in [0, 1] do not work!).\n\n    predicted : Tensorfow 2D tensor\n        Predictions in (0, 1), such as softmax output of\n        a neural network, with data points in rows and class\n        probabilities in columns.\n\n    delta : scalar\n        The hinge loss margin. Defaults to ``1``.\n\n    Returns\n    -------\n    Tensorfow 1D tensor\n        An expression for the average multi-class hinge loss.\n\n    Notes\n    -----\n    This is an alternative to the categorical cross-entropy\n    loss for multi-class classification problems.\n    """"""\n    shape = tf.shape(expected)\n    n_samples = asfloat(shape[0])\n\n    positive = tf.reduce_sum(expected * predicted, axis=-1)\n    negative = tf.reduce_max((asfloat(1) - expected) * predicted, axis=-1)\n    errors = tf.nn.relu(negative - positive + asfloat(1))\n    return tf.reduce_sum(errors) / n_samples\n'"
neupy/algorithms/gd/quasi_newton.py,28,"b'import tensorflow as tf\n\nfrom neupy.core.config import Configurable\nfrom neupy.core.properties import (\n    ChoiceProperty, NumberProperty,\n    WithdrawProperty, IntProperty,\n)\nfrom neupy.utils.tf_utils import setup_parameter_updates\nfrom neupy.algorithms.minsearch.wolfe import line_search\nfrom neupy.utils import (\n    asfloat, dot, outer, as_tuple,\n    function_name_scope, make_single_vector,\n)\nfrom .base import BaseOptimizer\n\n\n__all__ = (\'QuasiNewton\',)\n\n\nclass WolfeLineSearchForStep(Configurable):\n    """"""\n    Class that has all functions required in order to apply line search over\n    step parameter that used during the network training.\n\n    Parameters\n    ----------\n    wolfe_maxiter : int\n        Controls maximum number of iteration during the line search that\n        identifies optimal step size during the weight update stage.\n        Defaults to ``20``.\n\n    wolfe_c1 : float\n        Parameter for Armijo condition rule. It\'s used during the line search\n        that identifies optimal step size during the weight update stage.\n        Defaults ``1e-4``.\n\n    wolfe_c2 : float\n        Parameter for curvature condition rule. It\'s used during the line\n        search that identifies optimal step size during the weight update\n        stage. Defaults ``0.9``.\n    """"""\n    wolfe_maxiter = IntProperty(default=20, minval=0)\n    wolfe_c1 = NumberProperty(default=1e-4, minval=0)\n    wolfe_c2 = NumberProperty(default=0.9, minval=0)\n\n    def find_optimal_step(self, parameter_vector, parameter_update):\n        network_inputs = as_tuple(self.network.inputs)\n        layers_and_parameters = list(self.network.variables.items())\n\n        def prediction(step):\n            step = asfloat(step)\n            updated_params = parameter_vector + step * parameter_update\n\n            try:\n                # This trick allow us to replace shared variables\n                # with tensorflow variables and get output from the network\n                start_pos = 0\n                for (layer, varname), variable in layers_and_parameters:\n                    n_param_values = int(variable.shape.num_elements())\n                    end_pos = start_pos + n_param_values\n\n                    updated_param_value = tf.reshape(\n                        updated_params[start_pos:end_pos],\n                        variable.shape)\n\n                    setattr(layer, varname, updated_param_value)\n                    start_pos = end_pos\n\n                output = self.network.output(*network_inputs, training=True)\n\n            finally:\n                # Restore previous parameters\n                for (layer, varname), param in layers_and_parameters:\n                    setattr(layer, varname, param)\n\n            return output\n\n        def phi(step):\n            return self.loss(self.target, prediction(step))\n\n        def derphi(step):\n            error_func = self.loss(self.target, prediction(step))\n            gradient, = tf.gradients(error_func, step)\n            return gradient\n\n        return line_search(\n            phi, derphi, self.wolfe_maxiter,\n            self.wolfe_c1, self.wolfe_c2)\n\n\n@function_name_scope\ndef safe_reciprocal(value, epsilon):\n    """"""\n    The same as regular function in the tensorflow accept that it ensures\n    that non of the input values have magnitude smaller than epsilon.\n    Otherwise small values will be capped to the epsilon.\n    """"""\n    inv_epsilon = 1. / epsilon\n    return tf.clip_by_value(\n        tf.reciprocal(value),\n        -inv_epsilon,\n        inv_epsilon\n    )\n\n\n@function_name_scope\ndef safe_division(numerator, denominator, epsilon):\n    """"""\n    The same as regular function in the tensorflow accept that it ensures\n    that non of the denominator values have magnutide smaller than epsilon.\n    Otherwise small values will be capped to the epsilon.\n    """"""\n    inv_denominator = safe_reciprocal(denominator, epsilon)\n    return numerator * inv_denominator\n\n\n@function_name_scope\ndef bfgs(inv_H, delta_w, delta_grad, epsilon=1e-7):\n    """"""\n    It can suffer from round-off error and inaccurate line searches.\n    """"""\n    n_parameters = int(inv_H.shape[0])\n\n    I = tf.eye(n_parameters)\n    rho = safe_reciprocal(dot(delta_grad, delta_w), epsilon)\n\n    X = I - outer(delta_w, delta_grad) * rho\n    X_T = tf.transpose(X)\n    Z = rho * outer(delta_w, delta_w)\n\n    return tf.matmul(X, tf.matmul(inv_H, X_T)) + Z\n\n\n@function_name_scope\ndef dfp(inv_H, delta_w, delta_grad, epsilon=1e-7):\n    """"""\n    DFP is a method very similar to BFGS. It\'s rank 2 formula update.\n    It can suffer from round-off error and inaccurate line searches.\n    """"""\n    inv_H_dot_grad = dot(inv_H, delta_grad)\n\n    x = safe_division(\n        outer(delta_w, delta_w),\n        dot(delta_grad, delta_w),\n        epsilon\n    )\n    y = safe_division(\n        tf.matmul(outer(inv_H_dot_grad, delta_grad), inv_H),\n        dot(delta_grad, inv_H_dot_grad),\n        epsilon\n    )\n\n    return inv_H - y + x\n\n\n@function_name_scope\ndef sr1(inv_H, delta_w, delta_grad, epsilon=1e-7):\n    """"""\n    Symmetric rank 1 (SR1). Generates update for the inverse hessian\n    matrix adding symmetric rank-1 matrix. It\'s possible that there is no\n    rank 1 updates for the matrix and in this case update won\'t be applied\n    and original inverse hessian will be returned.\n    """"""\n    param = delta_w - dot(inv_H, delta_grad)\n    denominator = dot(param, delta_grad)\n\n    return tf.where(\n        # This check protects from the cases when update\n        # doesn\'t exist. It\'s possible that during certain\n        # iteration there is no rank-1 update for the matrix.\n        tf.less(\n            tf.abs(denominator),\n            epsilon * tf.norm(param) * tf.norm(delta_grad)\n        ),\n        inv_H,\n        inv_H + outer(param, param) / denominator\n    )\n\n\nclass QuasiNewton(WolfeLineSearchForStep, BaseOptimizer):\n    """"""\n    Quasi-Newton algorithm. Every iteration quasi-Network method approximates\n    inverse Hessian matrix with iterative updates. It doesn\'t have ``step``\n    parameter. Instead, algorithm applies line search for the step parameter\n    that satisfies strong Wolfe condition. Parameters that control wolfe\n    search start with the ``wolfe_`` prefix.\n\n    Parameters\n    ----------\n    update_function : ``bfgs``, ``dfp``, ``sr1``\n        Update function for the iterative inverse hessian matrix\n        approximation. Defaults to ``bfgs``.\n\n        - ``bfgs`` -  It\'s rank 2 formula update. It can suffer from\n          round-off error and inaccurate line searches.\n\n        - ``dfp`` - DFP is a method very similar to BFGS. It\'s rank 2 formula\n          update. It can suffer from round-off error and inaccurate line\n          searches.\n\n        - ``sr1`` - Symmetric rank 1 (SR1). Generates update for the\n          inverse hessian matrix adding symmetric rank-1 matrix. It\'s\n          possible that there is no rank 1 updates for the matrix and in\n          this case update won\'t be applied and original inverse hessian\n          will be returned.\n\n    h0_scale : float\n        Default Hessian matrix is an identity matrix. The\n        ``h0_scale`` parameter scales identity matrix.\n        Defaults to ``1``.\n\n    epsilon : float\n        Controls numerical stability for the ``update_function`` parameter.\n        Defaults to ``1e-7``.\n\n    {WolfeLineSearchForStep.Parameters}\n\n    {BaseOptimizer.network}\n\n    {BaseOptimizer.loss}\n\n    {BaseOptimizer.show_epoch}\n\n    {BaseOptimizer.shuffle_data}\n\n    {BaseOptimizer.signals}\n\n    {BaseOptimizer.verbose}\n\n    {BaseOptimizer.regularizer}\n\n    Notes\n    -----\n    - Method requires all training data during propagation, which means\n      it cannot be trained with mini-batches.\n\n    Attributes\n    ----------\n    {BaseOptimizer.Attributes}\n\n    Methods\n    -------\n    {BaseOptimizer.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> optimizer = algorithms.QuasiNewton(\n    ...     Input(2) >> Sigmoid(3) >> Sigmoid(1),\n    ...     update_function=\'bfgs\'\n    ... )\n    >>> optimizer.train(x_train, y_train, epochs=10)\n\n    References\n    ----------\n    [1] Yang Ding, Enkeleida Lushi, Qingguo Li,\n        Investigation of quasi-Newton methods for unconstrained optimization.\n        http://people.math.sfu.ca/~elushi/project_833.pdf\n\n    [2] Jorge Nocedal, Stephen J. Wright, Numerical Optimization.\n        Chapter 6, Quasi-Newton Methods, p. 135-163\n    """"""\n    update_function = ChoiceProperty(\n        default=\'bfgs\',\n        choices={\n            \'bfgs\': bfgs,\n            \'dfp\': dfp,\n            \'sr1\': sr1,\n        }\n    )\n    epsilon = NumberProperty(default=1e-7, minval=0)\n    h0_scale = NumberProperty(default=1, minval=0)\n    step = WithdrawProperty()\n\n    def init_variables(self):\n        with tf.name_scope(\'quasi-newton\'):\n            n_parameters = self.network.n_parameters\n            self.variables.update(\n                inv_hessian=tf.Variable(\n                    asfloat(self.h0_scale) * tf.eye(n_parameters),\n                    name=""inv-hessian"",\n                    dtype=tf.float32,\n                ),\n                prev_params=tf.Variable(\n                    tf.zeros([n_parameters]),\n                    name=""prev-params"",\n                    dtype=tf.float32,\n                ),\n                prev_full_gradient=tf.Variable(\n                    tf.zeros([n_parameters]),\n                    name=""prev-full-gradient"",\n                    dtype=tf.float32,\n                ),\n                iteration=tf.Variable(\n                    asfloat(self.last_epoch),\n                    name=\'current-iteration\',\n                    dtype=tf.float32\n                ),\n            )\n\n    def init_train_updates(self):\n        self.init_variables()\n\n        iteration = self.variables.iteration\n        inv_hessian = self.variables.inv_hessian\n        prev_params = self.variables.prev_params\n        prev_full_gradient = self.variables.prev_full_gradient\n\n        variables = self.network.variables\n        params = [var for var in variables.values() if var.trainable]\n        param_vector = make_single_vector(params)\n\n        gradients = tf.gradients(self.variables.loss, params)\n        full_gradient = make_single_vector(gradients)\n\n        new_inv_hessian = tf.where(\n            tf.equal(iteration, 0),\n            inv_hessian,\n            self.update_function(\n                inv_H=inv_hessian,\n                delta_w=param_vector - prev_params,\n                delta_grad=full_gradient - prev_full_gradient,\n                epsilon=self.epsilon\n            )\n        )\n        param_delta = -dot(new_inv_hessian, full_gradient)\n        step = self.find_optimal_step(param_vector, param_delta)\n        updated_params = param_vector + step * param_delta\n        updates = setup_parameter_updates(params, updated_params)\n\n        # We have to compute these values first, otherwise\n        # parallelization, in tensorflow, can mix update order\n        # and, for example, previous gradient can be equal to\n        # current gradient value. It happens because tensorflow\n        # try to execute operations in parallel.\n        required_variables = [new_inv_hessian, param_vector, full_gradient]\n        with tf.control_dependencies(required_variables):\n            updates.extend([\n                inv_hessian.assign(new_inv_hessian),\n                prev_params.assign(param_vector),\n                prev_full_gradient.assign(full_gradient),\n                iteration.assign(iteration + 1),\n            ])\n\n        return updates\n'"
neupy/algorithms/gd/regularizers.py,4,"b'from __future__ import division\n\nfrom functools import wraps\n\nimport tensorflow as tf\n\nfrom neupy.utils import asfloat\n\n\n__all__ = (\'define_regularizer\', \'l1\', \'l2\', \'maxnorm\')\n\n\nclass Regularizer(object):\n    def __init__(self, function, *args, **kwargs):\n        self.function = function\n        self.exclude = kwargs.pop(\'exclude\', [\'bias\'])\n\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self, network):\n        cost = asfloat(0)\n\n        for (layer, varname), variable in network.variables.items():\n            if varname not in self.exclude and variable.trainable:\n                cost += self.function(variable, *self.args, **self.kwargs)\n\n        return cost\n\n    def __repr__(self):\n        kwargs_repr = [repr(arg) for arg in self.args]\n        kwargs_repr += [""{}={}"".format(k, v) for k, v in self.kwargs.items()]\n        kwargs_repr += [""exclude={}"".format(self.exclude)]\n        return ""{}({})"".format(self.function.__name__, \', \'.join(kwargs_repr))\n\n\ndef define_regularizer(function):\n    """"""\n    Wraps regularization function and uses it in order to apply\n    regularization per each parameter in the network.\n\n    Examples\n    --------\n    >>> import tensorflow as tf\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> @algorithms.define_regularizer\n    ... def l2(weight, decay_rate=0.01):\n    ...     return decay_rate * tf.reduce_sum(tf.pow(weight, 2))\n    ...\n    >>> l2_regularizer = l2(dacay_rate=0.01)\n    >>> network = Input(5) >> Relu(10) >> Sigmoid(1)\n    >>> reguarization_cost = l2_regularizer(network)\n    """"""\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        return Regularizer(function, *args, **kwargs)\n    return wrapper\n\n\n@define_regularizer\ndef l1(weight, decay_rate=0.01):\n    """"""\n    Applies l1 regularization to the trainable parameters in the network.\n\n    Regularization cost per weight parameter in the layer can be computed\n    in the following way (pseudocode).\n\n    .. code-block:: python\n\n        cost = decay_rate * sum(abs(weight))\n\n    Parameters\n    ----------\n    decay_rate : float\n        Controls training penalties during the parameter updates.\n        The larger the value the stronger effect regularization\n        has during the training. Defaults to ``0.01``.\n\n    exclude : list\n        List of parameter names that has to be excluded from the\n        regularization. Defaults to ``[\'bias\']``.\n\n    Examples\n    --------\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.l1(decay_rate=0.01)\n    ... )\n\n    With included regularization for bias\n\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.l1(decay_rate=0.01, exclude=[])\n    ... )\n    """"""\n    return tf.multiply(decay_rate, tf.reduce_sum(tf.abs(weight)))\n\n\n@define_regularizer\ndef l2(weight, decay_rate=0.01):\n    """"""\n    Applies l2 regularization to the trainable parameters in the network.\n\n    Regularization cost per weight parameter in the layer can be computed\n    in the following way (pseudocode).\n\n    .. code-block:: python\n\n        cost = decay_rate * sum(weight ** 2)\n\n    Parameters\n    ----------\n    decay_rate : float\n        Controls training penalties during the parameter updates.\n        The larger the value the stronger effect regularization\n        has during the training. Defaults to ``0.01``.\n\n    exclude : list\n        List of parameter names that has to be excluded from the\n        regularization. Defaults to ``[\'bias\']``.\n\n    Examples\n    --------\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.l2(decay_rate=0.01)\n    ... )\n\n    With included regularization for bias\n\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.l2(decay_rate=0.01, exclude=[])\n    ... )\n    """"""\n    return tf.multiply(decay_rate, tf.reduce_sum(tf.pow(weight, 2)))\n\n\n@define_regularizer\ndef maxnorm(weight, decay_rate=0.01):\n    """"""\n    Applies max-norm regularization to the trainable parameters in the\n    network. Also known and l-inf regularization.\n\n    Regularization cost per weight parameter in the layer can be computed\n    in the following way (pseudocode).\n\n    .. code-block:: python\n\n        cost = decay_rate * max(abs(weight))\n\n    Parameters\n    ----------\n    decay_rate : float\n        Controls training penalties during the parameter updates.\n        The larger the value the stronger effect regularization\n        has during the training. Defaults to ``0.01``.\n\n    exclude : list\n        List of parameter names that has to be excluded from the\n        regularization. Defaults to ``[\'bias\']``.\n\n    Examples\n    --------\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.maxnorm(decay_rate=0.01)\n    ... )\n\n    With included regularization for bias\n\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.maxnorm(decay_rate=0.01, exclude=[])\n    ... )\n    """"""\n    return tf.multiply(decay_rate, tf.reduce_max(tf.abs(weight)))\n'"
neupy/algorithms/gd/rmsprop.py,1,"b'import tensorflow as tf\n\nfrom neupy.core.properties import (\n    ProperFractionProperty,\n    NumberProperty, Property,\n)\nfrom .base import GradientDescent\n\n\n__all__ = (\'RMSProp\',)\n\n\nclass RMSProp(GradientDescent):\n    """"""\n    RMSProp algorithm.\n\n    Parameters\n    ----------\n    decay : float\n        Decay rate. Value need to be between ``0`` and ``1``.\n        Defaults to ``0.95``.\n\n    momentum : float\n        Defaults to ``0``.\n\n    epsilon : float\n        Value need to be greater than ``0``. Defaults to ``1e-7``.\n\n    centered : bool\n        (from Tensorflow documentation) If ``True``, gradients are\n        normalized by the estimated variance of the gradient; if ``False``,\n        by the uncentered second moment. Setting this to ``True`` may\n        help with training, but is slightly more expensive in terms\n        of computation and memory. Defaults to ``False``.\n\n    {GradientDescent.Parameters}\n\n    Attributes\n    ----------\n    {GradientDescent.Attributes}\n\n    Methods\n    -------\n    {GradientDescent.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.RMSProp(network)\n    >>> optimizer.train(x_train, y_train)\n    """"""\n    decay = ProperFractionProperty(default=0.95)\n    momentum = NumberProperty(default=0, minval=0)\n    epsilon = NumberProperty(default=1e-7, minval=0)\n    centered = Property(default=False, expected_type=bool)\n\n    def init_train_updates(self):\n        optimizer = tf.train.RMSPropOptimizer(\n            decay=self.decay,\n            momentum=self.momentum,\n            centered=self.centered,\n            epsilon=self.epsilon,\n            learning_rate=self.step,\n        )\n        self.functions.optimizer = optimizer\n        return [optimizer.minimize(self.variables.loss)]\n'"
neupy/algorithms/gd/rprop.py,24,"b'import tensorflow as tf\nimport numpy as np\n\nfrom neupy.utils import tensorflow_session\nfrom neupy.core.properties import BoundedProperty, ProperFractionProperty\nfrom .base import BaseOptimizer\n\n\n__all__ = (\'RPROP\', \'IRPROPPlus\')\n\n\nclass RPROP(BaseOptimizer):\n    """"""\n    Resilient backpropagation (RPROP) is an optimization\n    algorithm for supervised learning.\n\n    RPROP algorithm takes into account only direction of the gradient\n    and completely ignores its magnitude. Every weight values has a unique\n    step size associated with it (by default all of the are equal to ``step``).\n\n    The rule is following, when gradient direction changes (sign of the\n    gradient) we decrease step size for specific weight multiplying it by\n    ``decrease_factor`` and if sign stays the same than we increase step\n    size for this specific weight multiplying it by ``increase_factor``.\n\n    The step size is always bounded by ``minstep`` and ``maxstep``.\n\n    Notes\n    -----\n    Algorithm doesn\'t work with mini-batches.\n\n    Parameters\n    ----------\n    minstep : float\n        Minimum possible value for step. Defaults to ``0.001``.\n\n    maxstep : float\n        Maximum possible value for step. Defaults to ``10``.\n\n    increase_factor : float\n        Increase factor for step in case when gradient doesn\'t change\n        sign compare to previous epoch.\n\n    decrease_factor : float\n        Decrease factor for step in case when gradient changes sign\n        compare to previous epoch.\n\n    {BaseOptimizer.Parameters}\n\n    Attributes\n    ----------\n    {BaseOptimizer.Attributes}\n\n    Methods\n    -------\n    {BaseOptimizer.Methods}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.RPROP(network)\n    >>> optimizer.train(x_train, y_train)\n\n    See Also\n    --------\n    :network:`IRPROPPlus` : iRPROP+ algorithm.\n    :network:`GradientDescent` : GradientDescent algorithm.\n    """"""\n\n    # This properties correct upper and lower bounds for steps.\n    minstep = BoundedProperty(default=0.001, minval=0)\n    maxstep = BoundedProperty(default=10, minval=0)\n\n    # This properties increase/decrease step by dividing it to\n    # some coefficient.\n    increase_factor = BoundedProperty(minval=1, default=1.2)\n    decrease_factor = ProperFractionProperty(default=0.5)\n\n    def update_prev_delta(self, prev_delta):\n        return prev_delta\n\n    def init_train_updates(self):\n        updates = []\n\n        variables = []\n        for (_, _), variable in self.network.variables.items():\n            if variable.trainable:\n                variables.append(variable)\n\n        gradients = tf.gradients(self.variables.loss, variables)\n\n        for parameter, gradient in zip(variables, gradients):\n            with tf.variable_scope(parameter.op.name):\n                steps = tf.Variable(\n                    # Steps will be decreased after the first iteration,\n                    # because all previous gradients are equal to zero.\n                    # In order to make sure that network will use the same\n                    # step per every weight we re-scale step and after the\n                    # first iteration it will be multiplied by\n                    # ``decrease_factor`` and scaled back to the default\n                    # step value.\n                    tf.ones_like(parameter) * self.step,\n                    name=""steps"",\n                    dtype=tf.float32,\n                )\n                prev_delta = tf.Variable(\n                    tf.zeros(parameter.shape),\n                    name=""prev-delta"",\n                    dtype=tf.float32,\n                )\n                # We collect only signs since it ensures numerical stability\n                # after multiplication when we deal with small numbers.\n                prev_gradient_sign = tf.Variable(\n                    tf.zeros(parameter.shape),\n                    name=""prev-grad-sign"",\n                    dtype=tf.float32,\n                )\n\n            updated_prev_delta = self.update_prev_delta(prev_delta)\n            gradient_sign = tf.sign(gradient)\n\n            grad_sign_product = gradient_sign * prev_gradient_sign\n            gradient_changed_sign = tf.equal(grad_sign_product, -1)\n\n            updated_steps = tf.clip_by_value(\n                tf.where(\n                    tf.equal(grad_sign_product, 1),\n                    steps * self.increase_factor,\n                    tf.where(\n                        gradient_changed_sign,\n                        steps * self.decrease_factor,\n                        steps,\n                    )\n                ),\n                self.minstep,\n                self.maxstep,\n            )\n            parameter_delta = tf.where(\n                gradient_changed_sign,\n                # If we subtract previous negative weight update it means\n                # that we will revert weight update that has been  applied\n                # in the previous iteration.\n                -updated_prev_delta,\n                updated_steps * gradient_sign,\n            )\n            # Making sure that during the next iteration sign, after\n            # we multiplied by the new gradient, won\'t be negative.\n            # Otherwise, the same roll back using previous delta\n            # won\'t make much sense.\n            clipped_gradient_sign = tf.where(\n                gradient_changed_sign,\n                tf.zeros_like(gradient_sign),\n                gradient_sign,\n            )\n\n            updates.extend([\n                (parameter, parameter - parameter_delta),\n                (steps, updated_steps),\n                (prev_gradient_sign, clipped_gradient_sign),\n                (prev_delta, parameter_delta),\n            ])\n\n        return updates\n\n\nclass IRPROPPlus(RPROP):\n    """"""\n    iRPROP+ is an optimization algorithm for supervised learning.\n    This is a variation of the :network:`RPROP` algorithm.\n\n    Parameters\n    ----------\n    {RPROP.minstep}\n\n    {RPROP.maxstep}\n\n    {RPROP.increase_factor}\n\n    {RPROP.decrease_factor}\n\n    {BaseOptimizer.regularizer}\n\n    {BaseOptimizer.network}\n\n    {BaseOptimizer.loss}\n\n    {BaseNetwork.show_epoch}\n\n    {BaseNetwork.shuffle_data}\n\n    {BaseNetwork.signals}\n\n    {Verbose.verbose}\n\n    Methods\n    -------\n    {BaseSkeleton.predict}\n\n    {BaseOptimizer.train}\n\n    {BaseSkeleton.fit}\n\n    Notes\n    -----\n    {RPROP.Notes}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> x_train = np.array([[1, 2], [3, 4]])\n    >>> y_train = np.array([[1], [0]])\n    >>>\n    >>> network = Input(2) >> Sigmoid(3) >> Sigmoid(1)\n    >>> optimizer = algorithms.IRPROPPlus(network)\n    >>> optimizer.train(x_train, y_train)\n\n    References\n    ----------\n    [1] Christian Igel, Michael Huesken (2000)\n        Improving the Rprop Learning Algorithm\n\n    See Also\n    --------\n    :network:`RPROP` : RPROP algorithm.\n    :network:`GradientDescent` : GradientDescent algorithm.\n    """"""\n    def init_functions(self):\n        self.variables.update(\n            last_error=tf.Variable(np.nan, name=\'irprop-plus/last-error\'),\n            previous_error=tf.Variable(\n                np.nan, name=\'irprop-plus/previous-error\'),\n        )\n        super(IRPROPPlus, self).init_functions()\n\n    def one_training_update(self, X_train, y_train):\n        if len(self.errors.train) >= 2:\n            previous_error, last_error = self.errors.train[-2:]\n            session = tensorflow_session()\n\n            self.variables.last_error.load(last_error, session)\n            self.variables.previous_error.load(previous_error, session)\n\n        return super(IRPROPPlus, self).one_training_update(X_train, y_train)\n\n    def update_prev_delta(self, prev_delta):\n        last_error = self.variables.last_error\n        prev_error = self.variables.previous_error\n\n        return tf.where(\n            # We revert weight when gradient changed the sign only in\n            # cases when error increased. Otherwise we don\'t apply any\n            # update for this weight.\n            last_error > prev_error,\n            prev_delta,\n            tf.zeros_like(prev_delta),\n        )\n'"
neupy/algorithms/gd/step_updates.py,15,"b'import tensorflow as tf\n\nfrom neupy.utils import asfloat, function_name_scope\n\n\n__all__ = (\'step_decay\', \'exponential_decay\', \'polynomial_decay\')\n\n\ndef init_variables(initial_value, iteration=0, name=\'step\'):\n    iteration = tf.Variable(\n        asfloat(iteration),\n        dtype=tf.float32,\n        name=\'iteration\',\n    )\n    step = tf.Variable(\n        asfloat(initial_value),\n        dtype=tf.float32,\n        name=name,\n    )\n    return step, iteration\n\n\n@function_name_scope\ndef step_decay(initial_value, reduction_freq, start_iter=0, name=\'step\'):\n    """"""\n    Algorithm minimizes learning step monotonically after\n    each iteration.\n\n    .. math::\n        \\\\alpha_{t + 1} = \\\\frac{\\\\alpha_{0}}{1 + \\\\frac{t}{m}}\n\n    where :math:`\\\\alpha` is a step, :math:`t` is an iteration number\n    and :math:`m` is a ``reduction_freq`` parameter.\n\n    .. code-block:: python\n\n        step = initial_value / (1 + current_iteration / reduction_freq)\n\n    Notes\n    -----\n    Step will be reduced faster when you have smaller training batches.\n\n    Parameters\n    ----------\n    initial_value : float\n        Initial value for the learning rate. It\'s the learning rate\n        returned during the first iteration.\n\n    reduction_freq : int\n        Parameter controls step reduction frequency. The larger the\n        value the slower step parameter decreases.\n\n        For instance, if ``reduction_freq=100``\n        and ``step=0.12`` then after ``100`` iterations ``step`` is\n        going to be equal to ``0.06`` (which is ``0.12 / 2``),\n        after ``200`` iterations ``step`` is going to be equal to\n        ``0.04`` (which is ``0.12 / 3``) and so on.\n\n    start_iter : int\n        Start iteration. At has to be equal to ``0`` when network just\n        started the training. Defaults to ``0``.\n\n    name : str\n        Learning rate\'s variable name. Defaults to ``step``.\n\n    Examples\n    --------\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.step_decay(\n    ...         initial_value=0.1,\n    ...         reduction_freq=100,\n    ...     )\n    ... )\n    """"""\n    step, iteration = init_variables(initial_value, start_iter, name)\n    reduction_freq = asfloat(reduction_freq)\n\n    step_update = initial_value / (1 + iteration / reduction_freq)\n    updated_step = step.assign(step_update)\n    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, updated_step)\n\n    with tf.control_dependencies([updated_step]):\n        next_iteration = iteration.assign(iteration + 1)\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, next_iteration)\n\n    return step\n\n\n@function_name_scope\ndef exponential_decay(initial_value, reduction_freq, reduction_rate,\n                      staircase=False, start_iter=0, name=\'step\'):\n    """"""\n    Applies exponential decay to the learning rate. This function is a\n    wrapper for the tensorflow\'s ``exponential_decay`` function.\n\n    .. math::\n        \\\\alpha_{t + 1} = \\\\alpha_{0} \\\\cdot d^{\\\\frac{t}{r}}\n\n    where :math:`\\\\alpha` is a step, :math:`t` is an iteration number,\n    :math:`d` is a ``reduction_freq`` and :math:`r` is a ``reduction_rate``.\n\n    .. code-block:: python\n\n        step = initial_value * reduction_rate ^ (\n            current_iteration / reduction_freq)\n\n    When ``staircase=True`` the :math:`\\\\frac{t}{r}` value will be\n    rounded.\n\n    .. code-block:: python\n\n        step = initial_value * reduction_rate ^ floor(\n            current_iteration / reduction_freq)\n\n    Notes\n    -----\n    Step will be reduced faster when you have smaller training batches.\n\n    Parameters\n    ----------\n    initial_value : float\n        Initial value for the learning rate.\n\n    reduction_freq : int\n        Parameter controls step reduction frequency. The larger the\n        value the slower step parameter decreases.\n\n    reduction_rate : float\n        Parameter controls step reduction rate. The larger the\n        value the slower step parameter decreases.\n\n    staircase : bool\n         If ``True`` decay the learning rate at discrete intervals.\n         Defaults to ``False``.\n\n    start_iter : int\n        Start iteration. At has to be equal to ``0`` when network just\n        started the training. Defaults to ``0``.\n\n    name : str\n        Learning rate\'s variable name. Defaults to ``step``.\n\n    Examples\n    --------\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.exponential_decay(\n    ...         initial_value=0.1,\n    ...         reduction_freq=1000,\n    ...         reduction_rate=0.95,\n    ...     )\n    ... )\n    """"""\n    step, iteration = init_variables(initial_value, start_iter, name)\n    step_update = tf.train.exponential_decay(\n        learning_rate=initial_value,\n        global_step=iteration,\n        decay_steps=reduction_freq,\n        decay_rate=reduction_rate,\n        staircase=staircase,\n    )\n\n    updated_step = step.assign(step_update)\n    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, updated_step)\n\n    with tf.control_dependencies([updated_step]):\n        next_iteration = iteration.assign(iteration + 1)\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, next_iteration)\n\n    return step\n\n\n@function_name_scope\ndef polynomial_decay(initial_value, decay_iter, minstep=0.001, power=1.0,\n                     cycle=False, start_iter=0, name=\'step\'):\n    """"""\n    Applies polynomial decay to the learning rate. This function is a\n    wrapper for the tensorflow\'s ``polynomial_decay`` function.\n\n    .. code-block:: python\n\n        iteration = min(current_iteration, decay_iter)\n        step = minstep + (\n            (initial_value - minstep) *\n            (1 - iteration / decay_iter) ^ power\n        )\n\n    If cycle is ``True`` then a multiple of ``decay_iter`` is used,\n    the first one that is bigger than ``current_iterations``.\n\n    .. code-block:: python\n\n        decay_iter = decay_iter * ceil(current_iteration / decay_iter)\n        step = minstep + (\n            (initial_value - minstep) *\n            (1 - current_iteration / decay_iter) ^ power\n        )\n\n    Notes\n    -----\n    Step will be reduced faster when you have smaller training batches.\n\n    Parameters\n    ----------\n    initial_value : float\n       Initial value for the learning rate.\n\n    decay_iter : int\n        When ``cycle=False`` parameter identifies number of iterations\n        when ``minstep`` will be reached. When ``cycle=True`` than\n        the ``decay_iter`` value will be increased. See code above.\n\n    minstep : float\n        Step will never be lower than that minimum possible step,\n        specified by this parameter. Defaults to ``0.001``.\n\n    power : float\n        The power of the polynomial. Defaults to ``1``.\n\n    cycle : bool\n        When value equal to ``True`` than step will be further reduced\n        when ``current_iteration > decay_iter``. Defaults to ``False``.\n\n    start_iter : int\n        Start iteration. At has to be equal to ``0`` when network just\n        started the training. Defaults to ``0``.\n\n    name : str\n       Learning rate\'s variable name. Defaults to ``step``.\n\n    Examples\n    --------\n    >>> from neupy import algorithms\n    >>> from neupy.layers import *\n    >>>\n    >>> optimizer = algorithms.Momentum(\n    ...     Input(5) >> Relu(10) >> Sigmoid(1),\n    ...     step=algorithms.polynomial_decay(\n    ...         initial_value=0.1,\n    ...         decay_iter=1000,\n    ...         minstep=0.01,\n    ...     )\n    ... )\n    """"""\n    step, iteration = init_variables(initial_value, start_iter, name)\n    step_update = tf.train.polynomial_decay(\n        learning_rate=initial_value,\n        global_step=iteration,\n        decay_steps=decay_iter,\n        end_learning_rate=minstep,\n        power=power,\n        cycle=cycle,\n    )\n\n    updated_step = step.assign(step_update)\n    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, updated_step)\n\n    with tf.control_dependencies([updated_step]):\n        next_iteration = iteration.assign(iteration + 1)\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, next_iteration)\n\n    return step\n'"
neupy/algorithms/memory/__init__.py,0,b''
neupy/algorithms/memory/bam.py,0,"b'import random\n\nimport numpy as np\nfrom numpy.core.umath_tests import inner1d\n\nfrom neupy.utils import format_data\nfrom neupy.exceptions import NotTrained\nfrom .base import DiscreteMemory\n\n\n__all__ = (\'DiscreteBAM\',)\n\n\ndef bin2sign(matrix):\n    return np.where(matrix == 0, -1, 1)\n\n\ndef sign2bin(matrix):\n    return np.where(matrix > 0, 1, 0).astype(int)\n\n\ndef hopfield_energy(weight, X, y):\n    return -0.5 * inner1d(X.dot(weight), y)\n\n\nclass DiscreteBAM(DiscreteMemory):\n    """"""\n    Discrete BAM Network with associations.\n    Network associate every input with some target value.\n\n    Parameters\n    ----------\n    {DiscreteMemory.Parameters}\n\n    Methods\n    -------\n    train(X, y)\n        Train network and update network weights.\n\n    predict_output(X, n_times=None)\n        Using input data recover output data. Returns two arguments.\n        First is an input data, second is an output data.\n\n    predict(X, n_times=None)\n        Alias to the ``predict_output`` method.\n\n    predict_input(y, n_times=None)\n        Using output data recover input data. Returns two arguments.\n        First is input data, second is output data.\n\n    energy(X, y)\n        Calculate Hopfield Energy for the input and output data.\n\n    Notes\n    -----\n    - Input and output vectors should contain only binary values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> zero = np.matrix([\n    ...     0, 1, 1, 1, 0,\n    ...     1, 0, 0, 0, 1,\n    ...     1, 0, 0, 0, 1,\n    ...     1, 0, 0, 0, 1,\n    ...     1, 0, 0, 0, 1,\n    ...     0, 1, 1, 1, 0\n    ... ])\n    >>>\n    >>> one = np.matrix([\n    ...     0, 1, 1, 0, 0,\n    ...     0, 0, 1, 0, 0,\n    ...     0, 0, 1, 0, 0,\n    ...     0, 0, 1, 0, 0,\n    ...     0, 0, 1, 0, 0,\n    ...     0, 0, 1, 0, 0\n    ... ])\n    >>> zero.reshape((5, 6))\n    >>>\n    >>> half_zero = np.matrix([\n    ...     0, 1, 1, 1, 0,\n    ...     1, 0, 0, 0, 1,\n    ...     1, 0, 0, 0, 1,\n    ...     0, 0, 0, 0, 0,\n    ...     0, 0, 0, 0, 0,\n    ...     0, 0, 0, 0, 0,\n    ... ])\n    >>>\n    >>> zero_hint = np.matrix([[0, 1, 0, 0]])\n    >>> one_hint = np.matrix([[1, 0, 0, 0]])\n    >>>\n    >>> data = np.concatenate([zero, one], axis=0)\n    >>> hints = np.concatenate([zero_hint, one_hint], axis=0)\n    >>>\n    >>> bamnet = algorithms.DiscreteBAM(mode=\'sync\')\n    >>> bamnet.train(data, hints)\n    >>>\n    >>> recovered_zero, recovered_hint = bamnet.predict(half_zero)\n    >>> recovered_hint\n    matrix([[0, 1, 0, 0]])\n    >>>\n    >>> zero_hint\n    matrix([[0, 1, 0, 0]])\n    """"""\n    def apply_async_process(self, X_sign, y_sign, n_times=None):\n        if n_times is None:\n            n_times = self.n_times\n\n        n_input_features = X_sign.shape[-1]\n        n_output_features = y_sign.shape[-1]\n\n        for _ in range(n_times):\n            i = random.randrange(n_input_features)\n            j = random.randrange(n_output_features)\n\n            X_sign[:, i] = np.sign(y_sign.dot(self.weight[i, :]))\n            y_sign[:, j] = np.sign(X_sign.dot(self.weight[:, j]))\n\n        return sign2bin(X_sign), sign2bin(y_sign)\n\n    def predict_input(self, y_bin, n_times=None):\n        if self.weight is None:\n            raise NotTrained(""Network hasn\'t been trained yet"")\n\n        self.discrete_validation(y_bin)\n\n        y_bin = format_data(y_bin, is_feature1d=False)\n        y_sign = bin2sign(y_bin)\n        X_sign = np.sign(y_sign.dot(self.weight.T))\n\n        if self.mode == \'sync\':\n            return sign2bin(X_sign), y_bin\n\n        return self.apply_async_process(X_sign, y_sign, n_times)\n\n    def predict_output(self, X_bin, n_times=None):\n        if self.weight is None:\n            raise NotTrained(""Network hasn\'t been trained yet"")\n\n        self.discrete_validation(X_bin)\n\n        X_bin = format_data(X_bin, is_feature1d=False)\n        X_sign = bin2sign(X_bin)\n        y_sign = np.sign(X_sign.dot(self.weight))\n\n        if self.mode == \'sync\':\n            return X_bin, sign2bin(y_sign)\n\n        return self.apply_async_process(X_sign, y_sign, n_times)\n\n    def predict(self, X_bin, n_times=None):\n        return self.predict_output(X_bin, n_times)\n\n    def train(self, X_bin, y_bin):\n        self.discrete_validation(X_bin)\n        self.discrete_validation(y_bin)\n\n        X_sign = bin2sign(format_data(X_bin, is_feature1d=False))\n        y_sign = bin2sign(format_data(y_bin, is_feature1d=False))\n\n        _, weight_nrows = X_sign.shape\n        _, weight_ncols = y_sign.shape\n        weight_shape = (weight_nrows, weight_ncols)\n\n        if self.weight is None:\n            self.weight = np.zeros(weight_shape)\n\n        if self.weight.shape != weight_shape:\n            raise ValueError(\n                ""Invalid input shapes. Number of input ""\n                ""features must be equal to {} and {} output ""\n                ""features"".format(weight_nrows, weight_ncols))\n\n        self.weight += X_sign.T.dot(y_sign)\n\n    def energy(self, X_bin, y_bin):\n        self.discrete_validation(X_bin)\n        self.discrete_validation(y_bin)\n\n        X_sign, y_sign = bin2sign(X_bin), bin2sign(y_bin)\n        X_sign = format_data(X_sign, is_feature1d=False)\n        y_sign = format_data(y_sign, is_feature1d=False)\n        nrows, n_features = X_sign.shape\n\n        if nrows == 1:\n            return hopfield_energy(self.weight, X_sign, y_sign)\n\n        output = np.zeros(nrows)\n        for i, rows in enumerate(zip(X_sign, y_sign)):\n            output[i] = hopfield_energy(self.weight, *rows)\n\n        return output\n'"
neupy/algorithms/memory/base.py,0,"b'import numpy as np\n\nfrom neupy.core.properties import ChoiceProperty, IntProperty\nfrom neupy.core.config import Configurable\nfrom neupy.algorithms.base import BaseSkeleton\n\n\n__all__ = (\'DiscreteMemory\',)\n\n\nclass DiscreteMemory(BaseSkeleton, Configurable):\n    """"""\n    Base class for discrete memory networks.\n\n    Notes\n    -----\n    - Input and output vectors should contain only binary values.\n\n    Parameters\n    ----------\n    mode : {{``sync``, ``async``}}\n        Specifies pattern recovery mode.\n\n        - ``sync`` mode tries to recover pattern using all\n          values from the input vector.\n\n        - ``async`` mode choose randomly some values from the\n          input vector and iteratively repeat this procedure.\n          Number of iterations defines by the ``n_times``\n          parameter.\n\n        Defaults to ``sync``.\n\n    n_times : int\n        Available only in ``async`` mode. Identify number\n        of random trials. Defaults to ``100``.\n\n    {Verbose.verbose}\n    """"""\n    mode = ChoiceProperty(choices=[\'async\', \'sync\'])\n    n_times = IntProperty(minval=1)\n\n    def __init__(self, mode=\'sync\', n_times=100, verbose=False):\n        self.mode = mode\n        self.n_times = n_times\n        self.weight = None\n        super(DiscreteMemory, self).__init__(verbose=verbose)\n\n    def discrete_validation(self, matrix):\n        """"""\n        Validate discrete matrix.\n\n        Parameters\n        ----------\n        matrix : array-like\n            Matrix for validation.\n        """"""\n        if np.any(~np.isin(matrix, [0, 1])):\n            raise ValueError(\n                ""This network expects only discrete inputs. It mean that ""\n                ""it\'s possible to can use only matrices with binary values ""\n                ""(0 and 1)."")\n'"
neupy/algorithms/memory/cmac.py,0,"b'import numpy as np\n\nfrom neupy.utils import format_data\nfrom neupy.core.properties import IntProperty\nfrom neupy.algorithms.base import BaseNetwork\n\n\n__all__ = (\'CMAC\',)\n\n\nclass CMAC(BaseNetwork):\n    """"""\n    Cerebellar Model Articulation Controller (CMAC) Network based on memory.\n\n    Notes\n    -----\n    - Network always use Mean Absolute Error (MAE).\n    - Network works for multi dimensional target values.\n\n    Parameters\n    ----------\n    quantization : int\n        Network transforms every input to discrete value.\n        Quantization value controls number of total number of\n        categories after quantization, defaults to ``10``.\n\n    associative_unit_size : int\n        Number of associative blocks in memory, defaults to ``2``.\n\n    {BaseNetwork.Parameters}\n\n    Attributes\n    ----------\n    weight : dict\n        Network\'s weight that contains memorized patterns.\n\n    Methods\n    -------\n    {BaseSkeleton.predict}\n\n    train(X_train, y_train, X_test=None, y_test=None, epochs=100)\n        Trains the network to the data X. Network trains until maximum\n        number of ``epochs`` was reached.\n\n    {BaseSkeleton.fit}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy.algorithms import CMAC\n    >>>\n    >>> train_space = np.linspace(0, 2 * np.pi, 100)\n    >>> test_space = np.linspace(np.pi, 2 * np.pi, 50)\n    >>>\n    >>> X_train = np.reshape(train_space, (100, 1))\n    >>> X_test = np.reshape(test_space, (50, 1))\n    >>>\n    >>> y_train = np.sin(X_train)\n    >>> y_test = np.sin(X_test)\n    >>>\n    >>> cmac = CMAC(\n    ...     quantization=100,\n    ...     associative_unit_size=32,\n    ...     step=0.2,\n    ... )\n    ...\n    >>> cmac.train(X_train, y_train, epochs=100)\n    >>>\n    >>> predicted_test = cmac.predict(X_test)\n    >>> cmac.score(y_test, predicted_test)\n    0.0023639417543036569\n    """"""\n    quantization = IntProperty(default=10, minval=1)\n    associative_unit_size = IntProperty(default=2, minval=2)\n\n    def __init__(self, **options):\n        self.weight = {}\n        super(CMAC, self).__init__(**options)\n\n    def predict(self, X):\n        X = format_data(X)\n\n        get_memory_coords = self.get_memory_coords\n        get_result_by_coords = self.get_result_by_coords\n        predicted = []\n\n        for input_sample in self.quantize(X):\n            coords = get_memory_coords(input_sample)\n            predicted.append(get_result_by_coords(coords))\n\n        return np.array(predicted)\n\n    def get_result_by_coords(self, coords):\n        return sum(\n            self.weight.setdefault(coord, 0) for coord in coords\n        ) / self.associative_unit_size\n\n    def get_memory_coords(self, quantized_value):\n        assoc_unit_size = self.associative_unit_size\n\n        for i in range(assoc_unit_size):\n            point = ((quantized_value + i) / assoc_unit_size).astype(int)\n            yield tuple(np.concatenate([point, [i]]))\n\n    def quantize(self, X):\n        return (X * self.quantization).astype(int)\n\n    def one_training_update(self, X_train, y_train):\n        get_memory_coords = self.get_memory_coords\n        get_result_by_coords = self.get_result_by_coords\n        weight = self.weight\n        step = self.step\n\n        n_samples = X_train.shape[0]\n        quantized_input = self.quantize(X_train)\n        errors = 0\n\n        for input_sample, target_sample in zip(quantized_input, y_train):\n            coords = list(get_memory_coords(input_sample))\n            predicted = get_result_by_coords(coords)\n\n            error = target_sample - predicted\n            for coord in coords:\n                weight[coord] += step * error\n\n            errors += sum(abs(error))\n\n        return errors / n_samples\n\n    def score(self, X, y):\n        predicted = self.predict(X)\n        return np.mean(np.abs(predicted - y))\n\n    def train(self, X_train, y_train, X_test=None, y_test=None, epochs=100):\n        is_test_data_partialy_missed = (\n            (X_test is None and y_test is not None) or\n            (X_test is not None and y_test is None)\n        )\n\n        if is_test_data_partialy_missed:\n            raise ValueError(\n                ""Input and target test samples are missed. ""\n                ""They must be defined together or none of them."")\n\n        X_train = format_data(X_train)\n        y_train = format_data(y_train)\n\n        if X_test is not None:\n            X_test = format_data(X_test)\n            y_test = format_data(y_test)\n\n        return super(CMAC, self).train(\n            X_train, y_train, X_test, y_test, epochs=epochs)\n'"
neupy/algorithms/memory/discrete_hopfield_network.py,0,"b'import math\nimport random\n\nimport numpy as np\nfrom numpy.core.umath_tests import inner1d\n\nfrom neupy.utils import format_data\nfrom neupy.core.properties import Property\nfrom .base import DiscreteMemory\n\n\n__all__ = (\'DiscreteHopfieldNetwork\',)\n\n\ndef bin2sign(matrix):\n    return np.where(matrix == 0, -1, 1)\n\n\ndef hopfield_energy(weight, X, output_data):\n    return -0.5 * inner1d(X.dot(weight), output_data)\n\n\nclass DiscreteHopfieldNetwork(DiscreteMemory):\n    """"""\n    Discrete Hopfield Network. It can memorize binary samples\n    and reconstruct them from corrupted samples.\n\n    Notes\n    -----\n    - Works only with binary data. Input matrix should\n      contain only zeros and ones.\n\n    Parameters\n    ----------\n    {DiscreteMemory.Parameters}\n\n    check_limit : bool\n        Option enable a limit of patterns control for the\n        network using logarithmically proportion rule.\n        Defaults to ``True``.\n\n        .. math::\n\n            \\\\frac{{n_{{features}}}}{{2 \\\\cdot log_{{e}}(n_{{features}})}}\n\n    Methods\n    -------\n    energy(X)\n        Computes Discrete Hopfield Energy.\n\n    train(X)\n        Save input data pattern into the network\'s memory. Each call will\n        make partial fit for the network.\n\n    predict(X, n_times=None)\n        Recover data from the memory using input pattern.\n        For the prediction procedure you can control number\n        of iterations. If you set up this value equal to ``None``\n        then the value would be equal to the value that you\n        set up for the property with the same name - ``n_times``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from neupy import algorithms\n    >>>\n    >>> def draw_bin_image(image_matrix):\n    ...     for row in image_matrix.tolist():\n    ...         print(\'| \' + \' \'.join(\' *\'[val] for val in row))\n    ...\n    >>> zero = np.matrix([\n    ...     0, 1, 1, 1, 0,\n    ...     1, 0, 0, 0, 1,\n    ...     1, 0, 0, 0, 1,\n    ...     1, 0, 0, 0, 1,\n    ...     1, 0, 0, 0, 1,\n    ...     0, 1, 1, 1, 0\n    ... ])\n    >>>\n    >>> one = np.matrix([\n    ...     0, 1, 1, 0, 0,\n    ...     0, 0, 1, 0, 0,\n    ...     0, 0, 1, 0, 0,\n    ...     0, 0, 1, 0, 0,\n    ...     0, 0, 1, 0, 0,\n    ...     0, 0, 1, 0, 0\n    ... ])\n    >>>\n    >>> two = np.matrix([\n    ...     1, 1, 1, 0, 0,\n    ...     0, 0, 0, 1, 0,\n    ...     0, 0, 0, 1, 0,\n    ...     0, 1, 1, 0, 0,\n    ...     1, 0, 0, 0, 0,\n    ...     1, 1, 1, 1, 1,\n    ... ])\n    >>>\n    >>> half_zero = np.matrix([\n    ...     0, 1, 1, 1, 0,\n    ...     1, 0, 0, 0, 1,\n    ...     1, 0, 0, 0, 1,\n    ...     0, 0, 0, 0, 0,\n    ...     0, 0, 0, 0, 0,\n    ...     0, 0, 0, 0, 0,\n    ... ])\n    >>>\n    >>> draw_bin_image(zero.reshape((6, 5)))\n    |   * * *\n    | *       *\n    | *       *\n    | *       *\n    | *       *\n    |   * * *\n    >>> draw_bin_image(half_zero.reshape((6, 5)))\n    |   * * *\n    | *       *\n    | *       *\n    |\n    |\n    |\n    >>> data = np.concatenate([zero, one, two], axis=0)\n    >>>\n    >>> dhnet = algorithms.DiscreteHopfieldNetwork()\n    >>> dhnet.train(data)\n    >>>\n    >>> result = dhnet.predict(half_zero)\n    >>> draw_bin_image(result.reshape((6, 5)))\n    |   * * *\n    | *       *\n    | *       *\n    | *       *\n    | *       *\n    |   * * *\n\n    See Also\n    --------\n    :ref:`password-recovery`: Password recovery with Discrete Hopfield Network.\n    :ref:`discrete-hopfield-network`: Discrete Hopfield Network article.\n    """"""\n    check_limit = Property(expected_type=bool)\n\n    def __init__(self, mode=\'sync\', n_times=100, verbose=False,\n                 check_limit=True):\n\n        self.n_memorized_samples = 0\n        self.check_limit = check_limit\n\n        super(DiscreteHopfieldNetwork, self).__init__(\n            mode=mode, n_times=n_times, verbose=verbose)\n\n    def train(self, X_bin):\n        self.discrete_validation(X_bin)\n\n        X_sign = bin2sign(X_bin)\n        X_sign = format_data(X_sign, is_feature1d=False, make_float=False)\n\n        n_rows, n_features = X_sign.shape\n        n_rows_after_update = self.n_memorized_samples + n_rows\n\n        if self.check_limit:\n            memory_limit = math.ceil(n_features / (2 * math.log(n_features)))\n\n            if n_rows_after_update > memory_limit:\n                raise ValueError(""You can\'t memorize more than {0} ""\n                                 ""samples"".format(memory_limit))\n\n        weight_shape = (n_features, n_features)\n\n        if self.weight is None:\n            self.weight = np.zeros(weight_shape, dtype=int)\n\n        if self.weight.shape != weight_shape:\n            n_features_expected = self.weight.shape[1]\n            raise ValueError(""Input data has invalid number of features. ""\n                             ""Got {} features instead of {}.""\n                             """".format(n_features, n_features_expected))\n\n        self.weight += X_sign.T.dot(X_sign)\n        np.fill_diagonal(self.weight, np.zeros(len(self.weight)))\n        self.n_memorized_samples = n_rows_after_update\n\n    def predict(self, X_bin, n_times=None):\n        self.discrete_validation(X_bin)\n\n        X_sign = bin2sign(X_bin)\n        X_sign = format_data(X_sign, is_feature1d=False, make_float=False)\n\n        if self.mode == \'async\':\n            _, n_features = X_sign.shape\n\n            if n_times is None:\n                n_times = self.n_times\n\n            for _ in range(n_times):\n                position = random.randrange(n_features)\n                raw_new_value = X_sign.dot(self.weight[:, position])\n                X_sign[:, position] = np.sign(raw_new_value)\n        else:\n            X_sign = X_sign.dot(self.weight)\n\n        return np.where(X_sign > 0, 1, 0).astype(int)\n\n    def energy(self, X_bin):\n        self.discrete_validation(X_bin)\n\n        X_sign = bin2sign(X_bin)\n        X_sign = format_data(X_sign, is_feature1d=False, make_float=False)\n\n        n_rows, n_features = X_sign.shape\n\n        if n_rows == 1:\n            return hopfield_energy(self.weight, X_sign, X_sign)\n\n        output = np.zeros(n_rows)\n        for i, row in enumerate(X_sign):\n            output[i] = hopfield_energy(self.weight, row, row)\n\n        return output\n'"
neupy/algorithms/minsearch/__init__.py,0,b''
neupy/algorithms/minsearch/golden_search.py,18,"b'import math\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy.utils import asfloat\n\n\n__all__ = (\'fmin_golden_search\',)\n\n\ndef interval_location(f, minstep=1e-5, maxstep=50., maxiter=1024):\n    """"""\n    Identify interval where, potentially, optimal step could be.\n\n    Parameters\n    ----------\n    f : func\n\n    minstep : float\n        Defaults to ``1e-5``.\n\n    maxstep : float\n        Defaults to ``50``.\n\n    maxiter : int\n        Defaults to ``1024``.\n\n    tol : float\n        Defaults to ``1e-5``.\n\n    Returns\n    -------\n    float\n        Right bound of interval where could be optimal step in\n        specified direction. In case if there is no such direction\n        function return ``maxstep`` instead.\n    """"""\n    with tf.name_scope(\'interval-location\'):\n        def find_right_bound(_, prev_func_output, step):\n            func_output = f(step)\n            step = tf.where(\n                tf.greater(prev_func_output, func_output),\n                tf.minimum(2. * step, maxstep),\n                step\n            )\n            continue_searching = tf.logical_and(\n                tf.greater_equal(prev_func_output, func_output),\n                step < maxstep,\n            )\n            return [continue_searching, func_output, step]\n\n        _, _, step = tf.while_loop(\n            cond=lambda continue_searching, *args: continue_searching,\n            body=find_right_bound,\n            loop_vars=[\n                True,\n                tf.constant(asfloat(np.inf)),\n                tf.constant(asfloat(minstep)),\n            ],\n            back_prop=False,\n            maximum_iterations=maxiter,\n        )\n        return step\n\n\ndef golden_search(f, maxstep=50, maxiter=1024, tol=1e-5):\n    """"""\n    Identify best step for function in specific direction.\n\n    Parameters\n    ----------\n    f : func\n\n    maxstep : float\n        Defaults to ``50``.\n\n    maxiter : int\n        Defaults to ``1024``.\n\n    tol : float\n        Defaults to ``1e-5``.\n\n    Returns\n    -------\n    float\n        Identified optimal step.\n    """"""\n    with tf.name_scope(\'golden-search\'):\n        golden_ratio = asfloat((math.sqrt(5) - 1) / 2)\n        tol = asfloat(tol)\n\n        def interval_reduction(a, b, c, d):\n            fc = f(c)\n            fd = f(d)\n\n            is_c_smaller_than_d = tf.less(fc, fd)\n\n            new_a = tf.where(is_c_smaller_than_d, a, c)\n            new_b = tf.where(is_c_smaller_than_d, d, b)\n            new_c = tf.where(\n                is_c_smaller_than_d, d - golden_ratio * (d - a), d)\n            new_d = tf.where(\n                is_c_smaller_than_d, c, c + golden_ratio * (b - c))\n\n            return [new_a, new_b, new_c, new_d]\n\n        a = tf.constant(asfloat(0))\n        b = maxstep\n        c = b - golden_ratio * (b - a)\n        d = a + golden_ratio * (b - a)\n\n        a, b, c, d = tf.while_loop(\n            cond=lambda a, b, c, d: tf.greater(tf.abs(c - d), tol),\n            body=interval_reduction,\n            loop_vars=[a, b, c, d],\n            back_prop=False,\n            maximum_iterations=maxiter,\n        )\n        return (a + b) / 2\n\n\ndef fmin_golden_search(f, minstep=1e-5, maxstep=50., maxiter=1024, tol=1e-5):\n    """"""\n    Minimize scalar function using Golden Search.\n\n    Parameters\n    ----------\n    f : func\n        Function that needs to be minimized. Function has\n        to return the scalar.\n\n        .. code-block:: python\n\n            def f(x):\n                return x ** 2\n\n    minstep : float\n        Minimum step value. Defaults to ``1e-5``.\n\n    maxstep : float\n        Maximum step size. Defaults to ``50``.\n\n    maxiter : int\n        Maximum number of itrations. Defaults to ``1024``.\n\n    tol : float\n        Defaults to ``1e-5``.\n\n    Returns\n    -------\n    object\n        Returns the Tensorfow instance that finally should produce\n        best possible step for specified function.\n    """"""\n    params = (\n        (\'maxiter\', maxiter),\n        (\'minstep\', minstep),\n        (\'maxstep\', maxstep),\n        (\'tol\', tol),\n    )\n\n    for param_name, param_value in params:\n        if param_value <= 0:\n            raise ValueError(""Parameter `{}` should be greater than zero.""\n                             """".format(param_name))\n\n    if minstep >= maxstep:\n        raise ValueError(""`minstep` should be smaller than `maxstep`"")\n\n    maxstep = interval_location(f, minstep, maxstep, maxiter)\n    best_step = golden_search(f, maxstep, maxiter, tol)\n\n    return best_step\n'"
neupy/algorithms/minsearch/wolfe.py,46,"b'""""""\nMain source code from Pylearn2 library:\nhttps://github.com/lisa-lab/pylearn2/blob/master/pylearn2/\\\noptimization/linesearch.py\n""""""\nimport tensorflow as tf\n\nfrom neupy.utils import asfloat\n\n\ndef sequential_or(*conditions):\n    """"""\n    Use ``or`` operator between all conditions. Function is just\n    a syntax sugar that make long Tensorfow logical conditions looks\n    less ugly.\n\n    Parameters\n    ----------\n    *conditions\n        Conditions that returns ``True`` or ``False``\n    """"""\n    first_condition, other_conditions = conditions[0], conditions[1:]\n    if not other_conditions:\n        return first_condition\n    return tf.logical_or(first_condition, sequential_or(*other_conditions))\n\n\ndef sequential_and(*conditions):\n    """"""\n    Use ``and`` operator between all conditions. Function is just\n    a syntax sugar that make long Tensorfow logical conditions looks\n    less ugly.\n\n    Parameters\n    ----------\n    *conditions\n        Conditions that returns ``True`` or ``False``\n    """"""\n    first_condition, other_conditions = conditions[0], conditions[1:]\n    if not other_conditions:\n        return first_condition\n    return tf.logical_and(first_condition, sequential_and(*other_conditions))\n\n\ndef line_search(f, f_deriv, maxiter=20, c1=1e-4, c2=0.9):\n    """"""\n    Find ``x`` that satisfies strong Wolfe conditions.\n    ``x > 0`` is assumed to be a descent direction.\n\n    Parameters\n    ----------\n    f : callable f(x)\n        Objective scalar function.\n\n    f_deriv : callable f\'(x)\n        Objective function derivative.\n\n    maxiter : int\n        Maximum number of iterations. Defaults ``20``.\n\n    c1 : float\n        Parameter for Armijo condition rule. Defaults ``1e-4``.\n\n    c2 : float\n        Parameter for curvature condition rule. Defaults ``0.9``.\n\n    Returns\n    -------\n    Variable\n        Value ``x`` that satisfies strong Wolfe conditions and\n        minimize function ``f``.\n\n    Notes\n    -----\n    Uses the line search algorithm to enforce strong Wolfe\n    conditions.  See Wright and Nocedal, \'Numerical Optimization\',\n    1999, pg. 59-60.\n    For the zoom phase it uses an algorithm by [...].\n    """"""\n    if not 0 < c1 < 1:\n        raise ValueError(""c1 should be a float between 0 and 1"")\n\n    if not 0 < c2 < 1:\n        raise ValueError(""c2 should be a float between 0 and 1"")\n\n    if c2 < c1:\n        raise ValueError(""c2 needs to be greater than c1"")\n\n    if maxiter <= 0:\n        raise ValueError(""maxiter needs to be greater than 0"")\n\n    c1, c2 = asfloat(c1), asfloat(c2)\n\n    def search_iteration_step(condition, x_previous, x_current, y_previous,\n                              y_current, y_deriv_previous, iteration, x_star):\n\n        y_deriv_current = f_deriv(x_current)\n        x_new = x_current * asfloat(2)\n        y_new = f(x_new)\n\n        condition1 = tf.logical_or(\n            y_current > (y0 + c1 * x_current * y_deriv_0),\n            tf.logical_and(\n                y_current >= y_previous,\n                tf.not_equal(iteration, 1),\n            )\n        )\n        condition2 = tf.abs(y_deriv_current) <= -c2 * y_deriv_0\n        condition3 = y_deriv_current >= 0\n\n        x_star = tf.where(\n            condition1,\n            zoom(\n                x_previous, x_current, y_previous,\n                y_current, y_deriv_previous,\n                f, f_deriv, y0, y_deriv_0, c1, c2\n            ),\n            tf.where(\n                condition2,\n                x_current,\n                tf.where(\n                    condition3,\n                    zoom(\n                        x_current, x_previous, y_current,\n                        y_previous, y_deriv_current,\n                        f, f_deriv, y0, y_deriv_0, c1, c2\n                    ),\n                    x_new,\n                ),\n            ),\n        )\n        y_deriv_previous_new = tf.where(\n            condition1,\n            y_deriv_previous,\n            y_deriv_current\n        )\n\n        is_any_condition_satisfied = sequential_or(\n            condition1, condition2, condition3)\n\n        y_current_new = tf.where(\n            is_any_condition_satisfied,\n            y_current,\n            y_new\n        )\n        continue_searching_condition = tf.logical_and(\n            tf.not_equal(x_new, 0),\n            tf.logical_not(is_any_condition_satisfied),\n        )\n\n        return [\n            continue_searching_condition,\n            x_current, x_new, y_current, y_current_new,\n            y_deriv_previous_new, iteration + 1, x_star\n        ]\n\n    one = tf.constant(asfloat(1))\n    zero = tf.constant(asfloat(0))\n\n    x0, x1 = zero, one\n    y0, y1 = f(x0), f(x1)\n    y_deriv_0 = f_deriv(x0)\n\n    outs = tf.while_loop(\n        cond=lambda condition, *args: condition,\n        body=search_iteration_step,\n        loop_vars=[True, x0, x1, y0, y1, y_deriv_0, 1, zero],\n        back_prop=False,\n        maximum_iterations=maxiter,\n    )\n    return outs[-1]\n\n\ndef quadratic_minimizer(x_a, y_a, y_prime_a, x_b, y_b, bound_size_ratio=0.1):\n    """"""\n    Finds the minimizer for a quadratic polynomial that\n    goes through the points (x_a, y_a), (x_b, y_b) with derivative\n    at x_a of y_prime_a.\n\n    Parameters\n    ----------\n    x_a : float or tensorflow variable\n        Left point ``a`` in the ``x`` axis.\n\n    y_a : float or tensorflow variable\n        Output from function ``y`` at point ``a``.\n\n    y_prime_a : float or tensorflow variable\n        Output from function ``y\'`` (``y`` derivative) at point ``a``.\n\n    x_b : float or tensorflow variable\n        Right point ``a`` in the ``x`` axis.\n\n    y_b : float or tensorflow variable\n        Output from function ``y`` at point ``b``.\n\n    bound_size_ratio : float\n        Value control acceptable bounds for interpolation. If value\n        close to one of the points interpolation result will be ignored.\n        The bigger ratio, the more likely to reject interpolation.\n        Value needs to be between ``0`` and ``1``. Defaults to ``0.1``.\n\n    Returns\n    -------\n    object\n        Tensorfow variable that after evaluation is equal to\n        point ``x`` which is minimizer for quadratic function.\n    """"""\n\n    if not 0 <= bound_size_ratio < 1:\n        raise ValueError(""Value ``bound_size_ratio`` need to be a float ""\n                         ""between 0 and 1, got {}"".format(bound_size_ratio))\n\n    # The main formula works for the region [0, a] we need to\n    # shift function to the left side and put point ``a``\n    # at ``0`` position.\n    x_range = x_b - x_a\n    coef = (y_b - y_a - y_prime_a * x_range) / (x_range ** asfloat(2))\n    minimizer = -y_prime_a / (asfloat(2) * coef) + x_a\n    bound_size_ratio = asfloat(bound_size_ratio)\n\n    return tf.where(\n        sequential_or(\n            # Handle bad cases\n            tf.equal(x_range, 0),\n            coef <= 0,\n\n            tf.is_nan(minimizer),\n            tf.greater(minimizer, x_b - bound_size_ratio * x_range),\n            tf.less(minimizer, x_a + bound_size_ratio * x_range),\n        ),\n        x_a + asfloat(0.5) * x_range,\n        # Since we shifted function\'s output to the left, we need to shift\n        # the result to the right to make it correct for\n        # the specified region. That\'s why we are adding ``x_a``\n        # at the end.\n        -y_prime_a / (asfloat(2) * coef) + x_a\n    )\n\n\ndef cubic_minimizer(x_a, y_a, y_prime_a, x_b, y_b, x_c, y_c,\n                    bound_size_ratio=0.2):\n    """"""\n    Finds the minimizer for a cubic polynomial that goes\n    through the points (x_a, y_a), (x_b, y_b), and (x_c, y_c)\n    with derivative at ``x_a`` of y_prime_a.\n\n    Parameters\n    ----------\n    x_a : float or tensorflow variable\n        First point ``a`` in the ``x`` axis.\n\n    y_a : float or tensorflow variable\n        Output from function ``y`` at point ``a``.\n\n    y_prime_a : float or tensorflow variable\n        Output from function ``y\'`` (``y`` derivative) at point ``a``.\n\n    x_b : float or tensorflow variable\n        Second point ``b`` in the ``x`` axis.\n\n    y_b : float or tensorflow variable\n        Output from function ``y`` at point ``b``.\n\n    x_c : float or tensorflow variable\n        Third point ``c`` in the ``x`` axis.\n\n    y_c : float or tensorflow variable\n        Output from function ``y`` at point ``c``.\n\n    bound_size_ratio : float\n        Value control acceptable bounds for interpolation. If\n        value is close to one of the points than interpolation\n        result will be ignored. The bigger the ratio, the more\n        likely it\'s going to reject interpolation. Value needs\n        to be between ``0`` and ``1``. Defaults to ``0.1``.\n\n    Returns\n    -------\n    object\n        Tensorfow variable that after evaluation is equal to\n        the point ``x`` which is a minimizer for the cubic function.\n    """"""\n\n    if not 0 <= bound_size_ratio < 1:\n        raise ValueError(""The `bound_size_ratio` value should be a float ""\n                         ""number between 0 and 1, got {}""\n                         """".format(bound_size_ratio))\n\n    bound_size_ratio = asfloat(bound_size_ratio)\n\n    from_a2b_dist = x_b - x_a\n    from_a2c_dist = x_c - x_a\n\n    denominator = (\n        (from_a2b_dist * from_a2c_dist) ** asfloat(2) *\n        (from_a2b_dist - from_a2c_dist)\n    )\n    tau_ab = y_b - y_a - y_prime_a * from_a2b_dist\n    tau_ac = y_c - y_a - y_prime_a * from_a2c_dist\n\n    alpha = (\n        from_a2c_dist ** asfloat(2) * tau_ab -\n        from_a2b_dist ** asfloat(2) * tau_ac\n    ) / denominator\n    beta = (\n        from_a2b_dist ** asfloat(3) * tau_ac -\n        from_a2c_dist ** asfloat(3) * tau_ab\n    ) / denominator\n    radical = beta ** asfloat(2) - asfloat(3) * alpha * y_prime_a\n\n    minimizer = x_a + (-beta + tf.sqrt(radical)) / (asfloat(3) * alpha)\n\n    return tf.where(\n        sequential_or(\n            # Handle bad cases\n            radical < 0,\n\n            tf.equal(x_a, x_b),\n            tf.equal(x_a, x_c),\n            tf.equal(x_b, x_c),\n            tf.equal(alpha, 0),\n\n            tf.is_nan(minimizer),\n            tf.greater(minimizer, x_b - bound_size_ratio * from_a2b_dist),\n            tf.less(minimizer, x_a + bound_size_ratio * from_a2b_dist),\n        ),\n        quadratic_minimizer(x_a, y_a, y_prime_a, x_b, y_b),\n        minimizer,\n    )\n\n\ndef zoom(x_low, x_high, y_low, y_high, y_deriv_low,\n         f, f_deriv, y0, y_deriv_0, c1, c2, maxiter=10):\n    """"""\n    Notes\n    -----\n    Part of the optimization algorithm in `scalar_search_wolfe2`.\n\n    Parameters\n    ----------\n    x_low : float\n        Step size\n\n    x_high : float\n        Step size\n\n    y_low : float\n        Value of f at x_low\n\n    y_high : float\n        Value of f at x_high\n\n    y_deriv_low : float\n        Value of derivative at x_low\n\n    f : callable f(x)\n        Generates computational graph\n\n    f_deriv : callable f\'(x)\n        Generates computational graph\n\n    y0 : float\n        Value of f for ``x = 0``\n\n    y_deriv_0 : float\n        Value of the derivative for ``x = 0``\n\n    c1 : float\n        Parameter for Armijo condition rule.\n\n    c2 : float\n        Parameter for curvature condition rule.\n\n    maxiter : int\n        Maximum number of iterations. Defaults to ``10``.\n    """"""\n\n    def zoom_itertion_step(_, x_low, y_low, y_deriv_low, x_high, y_high,\n                           x_recent, y_recent, x_star):\n\n        x_new = cubic_minimizer(\n            x_low, y_low, y_deriv_low,\n            x_high, y_high,\n            x_recent, y_recent)\n\n        y_new = f(x_new)\n        y_deriv_new = f_deriv(x_new)\n\n        continue_searching_condition = sequential_or(\n            y_new > (y0 + c1 * x_new * y_deriv_0),\n            y_new >= y_low,\n            tf.abs(y_deriv_new) > (-c2 * y_deriv_0),\n        )\n\n        condition1 = tf.logical_or(\n            y_new > (y0 + c1 * x_new * y_deriv_0),\n            y_new >= y_low\n        )\n        condition2 = y_deriv_new * (x_high - x_low) >= 0\n\n        x_recent = tf.where(\n            tf.logical_or(condition1, condition2), x_high, x_low)\n        y_recent = tf.where(\n            tf.logical_or(condition1, condition2), y_high, y_low)\n        x_high = tf.where(\n            condition1, x_new, tf.where(condition2, x_low, x_high))\n        y_high = tf.where(\n            condition1, y_new, tf.where(condition2, y_low, y_high))\n\n        x_low = tf.where(condition1, x_low, x_new)\n        y_low = tf.where(condition1, y_low, y_new)\n        y_deriv_low = tf.where(condition1, y_deriv_low, y_deriv_new)\n\n        x_star = x_new\n\n        return [\n            continue_searching_condition,\n            x_low, y_low, y_deriv_low,\n            x_high, y_high,\n            y_recent, x_recent,\n            x_star\n        ]\n\n    zero = tf.constant(asfloat(0))\n    x_recent = zero\n    y_recent = y0\n\n    outs = tf.while_loop(\n        cond=lambda condition, *args: condition,\n        body=zoom_itertion_step,\n        loop_vars=[\n            True,\n            x_low, y_low, y_deriv_low,\n            x_high, y_high,\n            x_recent, y_recent,\n            zero,\n        ],\n        back_prop=False,\n        maximum_iterations=maxiter,\n    )\n    return outs[-1]\n'"
neupy/algorithms/rbfn/__init__.py,0,b''
neupy/algorithms/rbfn/grnn.py,0,"b'from numpy import dot\n\nfrom neupy.utils import format_data\nfrom neupy.exceptions import NotTrained\nfrom neupy.core.properties import BoundedProperty\nfrom neupy.algorithms.base import BaseSkeleton\nfrom .utils import pdf_between_data\n\n\n__all__ = (\'GRNN\',)\n\n\nclass GRNN(BaseSkeleton):\n    """"""\n    Generalized Regression Neural Network (GRNN). Network applies\n    only to the regression problems.\n\n    Parameters\n    ----------\n    std : float\n        Standard deviation for PDF function.\n        If your input features have high values than standard\n        deviation should also be high. For instance, if input features\n        from range ``[0, 20]`` that standard deviation should be\n        also a big value like ``10`` or ``15``. Small values will\n        lead to bad prediction.\n\n    {Verbose.verbose}\n\n    Notes\n    -----\n    - GRNN Network is sensitive for cases when one input feature\n      has higher values than the other one. Input data has to be\n      normalized before training.\n\n    - Standard deviation has to match the range of the input features\n      Check ``std`` parameter description for more information.\n\n    - The bigger training dataset the slower prediction.\n      Algorithm is much more efficient for small datasets.\n\n    - Network uses lazy learning which mean that network doesn\'t\n      need iterative training. It just stores parameters\n      and use them to make a predictions.\n\n    Methods\n    -------\n    train(X_train, y_train, copy=True)\n        Network just stores all the information about the data and use\n        it for the prediction. Parameter ``copy`` copies input data\n        before saving it inside the network.\n\n    predict(X)\n        Return prediction per each sample in the ``X``.\n\n    {BaseSkeleton.fit}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn import datasets, preprocessing\n    >>> from sklearn.model_selection import train_test_split\n    >>> from neupy import algorithms\n    >>>\n    >>> dataset = datasets.load_diabetes()\n    >>> x_train, x_test, y_train, y_test = train_test_split(\n    ...     preprocessing.minmax_scale(dataset.data),\n    ...     preprocessing.minmax_scale(dataset.target.reshape(-1, 1)),\n    ...     test_size=0.3,\n    ... )\n    >>>\n    >>> nw = algorithms.GRNN(std=0.1, verbose=False)\n    >>> nw.train(x_train, y_train)\n    >>>\n    >>> y_predicted = nw.predict(x_test)\n    >>> mse = np.mean((y_predicted - y_test) ** 2)\n    >>> mse\n    0.05280970704568171\n    """"""\n    std = BoundedProperty(minval=0)\n\n    def __init__(self, std, verbose=False):\n        self.std = std\n        self.X_train = None\n        self.y_train = None\n        super(GRNN, self).__init__(verbose=verbose)\n\n    def train(self, X_train, y_train, copy=True):\n        """"""\n        Trains network. PNN doesn\'t actually train, it just stores\n        input data and use it for prediction.\n\n        Parameters\n        ----------\n        X_train : array-like (n_samples, n_features)\n\n        y_train : array-like (n_samples,)\n            Target variable should be vector or matrix\n            with one feature column.\n\n        copy : bool\n            If value equal to ``True`` than input matrices will\n            be copied. Defaults to ``True``.\n\n        Raises\n        ------\n        ValueError\n            In case if something is wrong with input data.\n        """"""\n        X_train = format_data(X_train, copy=copy)\n        y_train = format_data(y_train, copy=copy)\n\n        if y_train.shape[1] != 1:\n            raise ValueError(""Target value must be one dimensional array"")\n\n        self.X_train = X_train\n        self.y_train = y_train\n\n        if X_train.shape[0] != y_train.shape[0]:\n            raise ValueError(""Number of samples in the input and target ""\n                             ""datasets are different"")\n\n    def predict(self, X):\n        """"""\n        Make a prediction from the input data.\n\n        Parameters\n        ----------\n        X : array-like (n_samples, n_features)\n\n        Raises\n        ------\n        ValueError\n            In case if something is wrong with input data.\n\n        Returns\n        -------\n        array-like (n_samples,)\n        """"""\n        if self.X_train is None:\n            raise NotTrained(\n                ""Cannot make a prediction. Network hasn\'t been trained yet"")\n\n        X = format_data(X)\n\n        if X.shape[1] != self.X_train.shape[1]:\n            raise ValueError(\n                ""Input data must contain {0} features, got {1}""\n                """".format(self.X_train.shape[1], X.shape[1]))\n\n        ratios = pdf_between_data(self.X_train, X, self.std)\n        return (dot(self.y_train.T, ratios) / ratios.sum(axis=0)).T\n'"
neupy/algorithms/rbfn/pnn.py,0,"b'import numpy as np\n\nfrom neupy.utils import format_data, iters\nfrom neupy.core.properties import BoundedProperty, IntProperty\nfrom neupy.algorithms.base import BaseSkeleton\nfrom neupy.exceptions import NotTrained\nfrom .utils import pdf_between_data\n\n\n__all__ = (\'PNN\',)\n\n\nclass PNN(BaseSkeleton):\n    """"""\n    Probabilistic Neural Network (PNN). Network applies only to\n    the classification problems.\n\n    Notes\n    -----\n    - PNN Network is sensitive for cases when one input feature\n      has higher values than the other one. Input data has to be\n      normalized before training.\n\n    - Standard deviation has to match the range of the input features\n      Check ``std`` parameter description for more information.\n\n    - The bigger training dataset the slower prediction.\n      Algorithm is much more efficient for small datasets.\n\n    - Network uses lazy learning which mean that network doesn\'t\n      need iterative training. It just stores parameters\n      and use them to make a predictions.\n\n    Parameters\n    ----------\n    std : float\n        Standard deviation for the Probability Density Function (PDF).\n        If your input features have high values than standard deviation\n        should also be high. For instance, if input features from range\n        ``[0, 20]`` that standard deviation should be also a big value\n        like ``10`` or ``15``. Small values will lead to bad prediction.\n\n    batch_size : int or None\n        Set up min-batch size. The ``None`` value will ensure that all data\n        samples will be propagated through the network at once.\n        Defaults to ``128``.\n\n    {Verbose.verbose}\n\n    Methods\n    -------\n    train(X_train, y_train, copy=True)\n        Network just stores all the information about the data and use\n        it for the prediction. Parameter ``copy`` copies input data\n        before saving it inside the network.\n\n        The ``y_train`` argument should be a vector or\n        matrix with one feature column.\n\n    predict(X)\n        Return classes associated with each sample in the ``X``.\n\n    predict_proba(X)\n        Predict probabilities for each class.\n\n    {BaseSkeleton.fit}\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>>\n    >>> from sklearn import datasets, metrics\n    >>> from sklearn.model_selection import train_test_split\n    >>> from neupy import algorithms\n    >>>\n    >>> dataset = datasets.load_digits()\n    >>> x_train, x_test, y_train, y_test = train_test_split(\n    ...     dataset.data, dataset.target, test_size=0.3\n    ... )\n    >>>\n    >>> pnn = algorithms.PNN(std=10, verbose=False)\n    >>> pnn.train(x_train, y_train)\n    >>>\n    >>> y_predicted = pnn.predict(x_test)\n    >>> metrics.accuracy_score(y_test, y_predicted)\n    0.98888888888888893\n    """"""\n    std = BoundedProperty(minval=0)\n    batch_size = IntProperty(default=128, minval=0, allow_none=True)\n\n    def __init__(self, std, batch_size=128, verbose=False):\n        self.std = std\n        self.batch_size = batch_size\n\n        self.classes = None\n        self.X_train = None\n        self.y_train = None\n\n        super(PNN, self).__init__(batch_size=batch_size, verbose=verbose)\n\n    def train(self, X_train, y_train, copy=True):\n        """"""\n        Trains network. PNN doesn\'t actually train, it just stores\n        input data and use it for prediction.\n\n        Parameters\n        ----------\n        X_train : array-like (n_samples, n_features)\n\n        y_train : array-like (n_samples,)\n            Target variable should be vector or matrix\n            with one feature column.\n\n        copy : bool\n            If value equal to ``True`` than input matrices will\n            be copied. Defaults to ``True``.\n\n        Raises\n        ------\n        ValueError\n            In case if something is wrong with input data.\n        """"""\n        X_train = format_data(X_train, copy=copy)\n        y_train = format_data(y_train, copy=copy, make_float=False)\n\n        self.X_train = X_train\n        self.y_train = y_train\n\n        if X_train.shape[0] != y_train.shape[0]:\n            raise ValueError(\n                ""Number of samples in the input and ""\n                ""target datasets are different"")\n\n        if y_train.shape[1] != 1:\n            raise ValueError(\n                ""Target value should be vector or ""\n                ""matrix with only one column"")\n\n        classes = self.classes = np.unique(y_train)\n        n_classes = classes.size\n        n_samples = X_train.shape[0]\n\n        class_ratios = self.class_ratios = np.zeros(n_classes)\n        row_comb_matrix = self.row_comb_matrix = np.zeros(\n            (n_classes, n_samples))\n\n        for i, class_name in enumerate(classes):\n            class_name = classes[i]\n            class_val_positions = (y_train == class_name)\n            row_comb_matrix[i, class_val_positions.ravel()] = 1\n            class_ratios[i] = np.sum(class_val_positions)\n\n    def predict_proba(self, X):\n        """"""\n        Predict probabilities for each class.\n\n        Parameters\n        ----------\n        X : array-like (n_samples, n_features)\n\n        Returns\n        -------\n        array-like (n_samples, n_classes)\n        """"""\n        outputs = iters.apply_batches(\n            function=self.predict_raw,\n            inputs=format_data(X),\n            batch_size=self.batch_size,\n            show_progressbar=self.logs.enable,\n        )\n        raw_output = np.concatenate(outputs, axis=1)\n\n        total_output_sum = raw_output.sum(axis=0).reshape((-1, 1))\n        return raw_output.T / total_output_sum\n\n    def predict_raw(self, X):\n        """"""\n        Raw prediction.\n\n        Parameters\n        ----------\n        X : array-like (n_samples, n_features)\n\n        Raises\n        ------\n        NotTrained\n            If network hasn\'t been trained.\n\n        ValueError\n            In case if something is wrong with input data.\n\n        Returns\n        -------\n        array-like (n_samples, n_classes)\n        """"""\n        if self.classes is None:\n            raise NotTrained(\n                ""Cannot make a prediction. Network hasn\'t been trained yet"")\n\n        if X.shape[1] != self.X_train.shape[1]:\n            raise ValueError(\n                ""Input data must contain {0} features, got {1}""\n                """".format(self.X_train.shape[1],  X.shape[1]))\n\n        class_ratios = self.class_ratios.reshape((-1, 1))\n        pdf_outputs = pdf_between_data(self.X_train, X, self.std)\n\n        return np.dot(self.row_comb_matrix, pdf_outputs) / class_ratios\n\n    def predict(self, X):\n        """"""\n        Predicts class from the input data.\n\n        Parameters\n        ----------\n        X : array-like (n_samples, n_features)\n\n        Returns\n        -------\n        array-like (n_samples,)\n        """"""\n        outputs = iters.apply_batches(\n            function=self.predict_raw,\n            inputs=format_data(X),\n            batch_size=self.batch_size,\n            show_progressbar=self.logs.enable,\n        )\n\n        raw_output = np.concatenate(outputs, axis=1)\n        return self.classes[raw_output.argmax(axis=0)]\n'"
neupy/algorithms/rbfn/utils.py,0,"b'import math\n\nimport numpy as np\nfrom numpy.core.umath_tests import inner1d\n\n\n__all__ = (\'pdf_between_data\',)\n\n\ndef pdf_between_data(train_data, X, std):\n    """"""\n    Compute PDF between two samples.\n\n    Parameters\n    ----------\n    train_data : array\n        Training dataset.\n\n    X : array\n        Input dataset\n\n    std : float\n        Standard deviation for Probability Density\n        Function (PDF).\n\n    Returns\n    -------\n    array-like\n    """"""\n    n_train_samples = train_data.shape[0]\n    n_samples = X.shape[0]\n\n    results = np.zeros((n_train_samples, n_samples))\n    variance = std ** 2\n    const = std * math.sqrt(2 * math.pi)\n\n    for i, input_row in enumerate(X):\n        inputs = np.tile(input_row, (n_train_samples, 1))\n        class_difference = (train_data - inputs)\n        total_distance = inner1d(class_difference, class_difference)\n        results[:, i] = np.exp(-total_distance / variance) / const\n\n    return results\n'"
tests/algorithms/associative/__init__.py,0,b''
tests/algorithms/associative/test_hebb.py,0,"b'import numpy as np\n\nfrom neupy import algorithms\n\nfrom base import BaseTestCase\n\n\nX = np.array([\n    [0, 1],\n    [1, 1],\n])\n\n\nclass HebbRuleTestCase(BaseTestCase):\n    def test_input_validations_for_hebb_rule(self):\n        invalid_cases = (\n            # Missed required parameters\n            dict(),\n            dict(n_inputs=2),\n            dict(n_inputs=2, n_outputs=1),\n\n            # n_inputs should be greater than n_unconditioned\n            dict(n_inputs=2, n_outputs=2, n_unconditioned=2, verbose=False),\n\n            # Invalid shapes for the arrays\n            dict(n_inputs=2, n_outputs=1, n_unconditioned=1,\n                 weight=np.array([1])),\n            dict(n_inputs=2, n_outputs=1, n_unconditioned=1,\n                 bias=np.array([1, 2, 3])),\n        )\n\n        for invalid_case_params in invalid_cases:\n            with self.assertRaises(ValueError):\n                algorithms.HebbRule(**invalid_case_params)\n\n    def test_hebb_rule_trainig_exception(self):\n        hebb = algorithms.HebbRule(n_inputs=2, n_outputs=3, n_unconditioned=1)\n        err_message = ""expected to have 2 features""\n\n        with self.assertRaisesRegexp(ValueError, err_message):\n            hebb.train(np.ones((6, 3)))\n\n    def test_learning_process(self):\n        hn = algorithms.HebbRule(\n            n_inputs=2,\n            n_outputs=1,\n            n_unconditioned=1,\n            step=1,\n            verbose=False,\n        )\n        hn.train(X, epochs=2)\n\n        test_data = np.array([\n            [0, 0],\n            [0, 1],\n            [1, 0],\n            [1, 1],\n        ])\n        np.testing.assert_array_equal(\n            hn.predict(test_data),\n            np.array([[0, 1, 1, 1]]).T\n        )\n\n    def test_with_weight_decay(self):\n        hn = algorithms.HebbRule(\n            n_inputs=2,\n            n_outputs=1,\n            n_unconditioned=1,\n            step=1,\n            verbose=False,\n            decay_rate=0.1,\n        )\n\n        # Test learning limit\n        hn.train(X, epochs=50)\n        self.assertEqual(np.round(hn.weight[1, 0], 2), 10)\n\n        hn.train(X, epochs=50)\n        self.assertEqual(np.round(hn.weight[1, 0], 2), 10)\n\n    def test_weights(self):\n        # Test default weights\n        hn = algorithms.HebbRule(\n            n_inputs=5,\n            n_outputs=1,\n            n_unconditioned=2,\n            verbose=False,\n        )\n        np.testing.assert_array_equal(\n            hn.weight,\n            np.array([[1, 1, 0, 0, 0]]).T\n        )\n        np.testing.assert_array_equal(\n            hn.bias,\n            np.array([-0.5])\n        )\n\n        # Test custom weights\n        random_weight = np.random.random((5, 1))\n        hn = algorithms.HebbRule(\n            n_inputs=5,\n            n_outputs=1,\n            n_unconditioned=2,\n            weight=random_weight,\n            verbose=False,\n        )\n        np.testing.assert_array_equal(hn.weight, random_weight)\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.HebbRule(\n                n_inputs=2,\n                n_outputs=1,\n                n_unconditioned=1,\n                step=1,\n                verbose=False\n            ),\n            np.array([[0, 1]]),\n            is_feature1d=False,\n        )\n\n    def test_predict_different_inputs(self):\n        inet = algorithms.HebbRule(\n            n_inputs=2,\n            n_outputs=1,\n            n_unconditioned=1,\n            step=1,\n            verbose=False\n        )\n\n        inet.train(X, epochs=10)\n        self.assertInvalidVectorPred(inet, np.array([0, 0]), 0,\n                                     is_feature1d=False)\n'"
tests/algorithms/associative/test_instar.py,0,"b'import numpy as np\n\nfrom neupy import algorithms\n\nfrom base import BaseTestCase\n\n\nX = np.array([\n    [0, 1, -1, -1],\n    [1, 1, -1, -1],\n])\n\n\nclass HebbRuleTestCase(BaseTestCase):\n    def setUp(self):\n        super(HebbRuleTestCase, self).setUp()\n        self.default_properties = dict(\n            n_inputs=4,\n            n_outputs=1,\n            n_unconditioned=1,\n            weight=np.array([[3, 0, 0, 0]]).T,\n        )\n\n    def test_learning_process(self):\n        inet = algorithms.Instar(\n            step=1,\n            verbose=False,\n            **self.default_properties\n        )\n\n        inet.train(X, epochs=10)\n\n        test_input = np.array([[0, 1, -1, -1]])\n        self.assertEqual(inet.predict(test_input), 1)\n\n        np.testing.assert_array_equal(\n            inet.weight,\n            np.array([[3, 1, -1, -1]]).T\n        )\n\n    def test_multiple_outputs(self):\n        X = np.array([\n            [-0.1961, 0.9806],\n        ])\n        innet = algorithms.Instar(\n            n_inputs=2,\n            n_outputs=3,\n            n_unconditioned=1,\n            weight=np.array([\n                [0.7071, 0.7071, -1],\n                [-0.7071, 0.7071, 0],\n            ]),\n            step=0.5,\n            verbose=False\n        )\n        innet.train(X, epochs=1)\n        np.testing.assert_array_almost_equal(\n            innet.weight,\n            np.array([\n                [0.7071, 0.7071, -1],\n                [-0.5704, 0.8439, 0.1368]\n            ]),\n            decimal=4\n        )\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.Instar(\n                step=1,\n                verbose=False,\n                **self.default_properties\n            ),\n            np.array([[0, 1, -1, -1]]),\n            is_feature1d=False,\n        )\n\n    def test_predict_different_inputs(self):\n        inet = algorithms.Instar(\n            step=1,\n            verbose=False,\n            **self.default_properties\n        )\n\n        inet.train(X, epochs=10)\n        self.assertInvalidVectorPred(inet, np.array([0, 1, -1, -1]), 1,\n                                     is_feature1d=False)\n'"
tests/algorithms/associative/test_kohonen.py,0,"b'import numpy as np\n\nfrom neupy import algorithms\n\nfrom base import BaseTestCase\n\n\nX = np.array([\n    [0.1961,  0.9806],\n    [-0.1961,  0.9806],\n    [0.9806,  0.1961],\n    [0.9806, -0.1961],\n    [-0.5812, -0.8137],\n    [-0.8137, -0.5812],\n])\n\n\nclass KohonenTestCase(BaseTestCase):\n    def test_kohonen_success(self):\n        kh = algorithms.Kohonen(\n            n_inputs=2,\n            n_outputs=3,\n            weight=np.array([\n                [0.7071, 0.7071, -1.0000],\n                [-0.7071, 0.7071,  0.0000],\n            ]),\n            step=0.5,\n            verbose=False,\n        )\n\n        # test one iteration update\n        data = np.reshape(X[0, :], (1, X.shape[1]))\n        kh.train(data, epochs=1)\n        np.testing.assert_array_almost_equal(\n            kh.weight,\n            np.array([\n                [0.7071, 0.4516, -1.0000],\n                [-0.7071, 0.84385,  0.0000],\n            ]),\n            decimal=4\n        )\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.Kohonen(\n                n_inputs=1,\n                n_outputs=2,\n                step=0.5,\n                verbose=False\n            ),\n            np.array([1, 2, 3])\n        )\n\n    def test_predict_different_inputs(self):\n        knet = algorithms.Kohonen(\n            n_inputs=1,\n            n_outputs=2,\n            step=0.5,\n            verbose=False,\n        )\n\n        data = np.array([[1, 1, 1]]).T\n        target = np.array([\n            [1, 0],\n            [1, 0],\n            [1, 0],\n        ])\n\n        knet.train(data, epochs=100)\n        self.assertInvalidVectorPred(\n            knet, data.ravel(), target, decimal=2)\n'"
tests/algorithms/associative/test_oja.py,0,"b""import numpy as np\n\nfrom neupy import algorithms, init\nfrom neupy.exceptions import NotTrained\n\nfrom base import BaseTestCase\nfrom helpers import vectors_for_testing\n\n\nclass OjaTestCase(BaseTestCase):\n    def setUp(self):\n        super(OjaTestCase, self).setUp()\n        self.data = np.array([\n            [2, 2],\n            [1, 1],\n            [4, 4],\n            [5, 5],\n        ])\n        self.result = np.array([\n            [2.83],\n            [1.41],\n            [5.66],\n            [7.07],\n        ])\n\n    def test_oja_minimization(self):\n        ojanet = algorithms.Oja(\n            minimized_data_size=1,\n            step=0.01,\n            weight=init.Constant(0.1),\n            verbose=False\n        )\n\n        ojanet.train(self.data, epochs=100)\n        minimized_data = ojanet.predict(self.data)\n        np.testing.assert_array_almost_equal(\n            minimized_data, self.result,\n            decimal=2\n        )\n\n        reconstructed = ojanet.reconstruct(minimized_data)\n        np.testing.assert_array_almost_equal(\n            reconstructed, self.data,\n            decimal=3\n        )\n\n    def test_oja_exceptions(self):\n        ojanet = algorithms.Oja(minimized_data_size=1, step=0.01,\n                                verbose=False)\n\n        with self.assertRaises(NotTrained):\n            # Can't reconstruct without training\n            ojanet.reconstruct(np.random.random((4, 1)))\n\n        with self.assertRaises(NotTrained):\n            # Can't predict without training\n            ojanet.predict(np.random.random((4, 1)))\n\n        ojanet.train(self.data, epochs=2)\n\n        with self.assertRaises(ValueError):\n            # Invalid #feature for reconstruct\n            ojanet.reconstruct(np.random.random((3, 3)))\n\n        with self.assertRaises(ValueError):\n            # Invalid #feature for train\n            ojanet.train(np.random.random((4, 10)))\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.Oja(minimized_data_size=1, verbose=False, step=0.01),\n            np.array([1, 2, 3])\n        )\n\n    def test_predict_different_inputs(self):\n        ojanet = algorithms.Oja(\n            minimized_data_size=1, verbose=False, step=0.01)\n\n        data = np.array([[1, 2, 3]]).T\n        target = np.array([[1, 2, 3]]).T\n\n        ojanet.train(data, epochs=100)\n        self.assertInvalidVectorPred(\n            ojanet, data.ravel(), target, decimal=2)\n\n    def test_reconstruct_different_inputs(self):\n        ojanet = algorithms.Oja(\n            minimized_data_size=1, verbose=False, step=0.01)\n\n        data = np.array([[1, 2, 3]]).T\n        target = np.array([[1, 2, 3]]).T\n        input_vector = data.ravel()\n\n        ojanet.train(data, epochs=100)\n        test_vectors = vectors_for_testing(input_vector)\n\n        for i, test_vector in enumerate(test_vectors, start=1):\n            np.testing.assert_array_almost_equal(\n                ojanet.reconstruct(test_vector),\n                target,\n                decimal=1\n            )\n"""
tests/algorithms/competitive/__init__.py,0,b''
tests/algorithms/competitive/test_art.py,0,"b""import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom neupy import algorithms\n\nfrom base import BaseTestCase\n\n\ndata = np.array([\n    [0, 1, 0],\n    [1, 0, 0],\n    [1, 1, 0],\n])\nlenses = np.array([\n    [1, 1, 1, 1, 1, 3],\n    [2, 1, 1, 1, 2, 2],\n    [3, 1, 1, 2, 1, 3],\n    [4, 1, 1, 2, 2, 1],\n    [5, 1, 2, 1, 1, 3],\n    [6, 1, 2, 1, 2, 2],\n    [7, 1, 2, 2, 1, 3],\n    [8, 1, 2, 2, 2, 1],\n    [9, 2, 1, 1, 1, 3],\n    [10, 2, 1, 1, 2, 2],\n    [11, 2, 1, 2, 1, 3],\n    [12, 2, 1, 2, 2, 1],\n    [13, 2, 2, 1, 1, 3],\n    [14, 2, 2, 1, 2, 2],\n    [15, 2, 2, 2, 1, 3],\n    [16, 2, 2, 2, 2, 3],\n    [17, 3, 1, 1, 1, 3],\n    [18, 3, 1, 1, 2, 3],\n    [19, 3, 1, 2, 1, 3],\n    [20, 3, 1, 2, 2, 1],\n    [21, 3, 2, 1, 1, 3],\n    [22, 3, 2, 1, 2, 2],\n    [23, 3, 2, 2, 1, 3],\n    [24, 3, 2, 2, 2, 3],\n])\n\n\nclass ARTTestCase(BaseTestCase):\n    def test_art_handle_errors(self):\n        with self.assertRaises(ValueError):\n            # Invalid input data dimension\n            artnet = algorithms.ART1(step=0.4, rho=0.1, n_clusters=3,\n                                     verbose=False)\n            artnet.predict(np.array([[[1]]]))\n\n        with self.assertRaises(ValueError):\n            # Non-binary input values\n            artnet = algorithms.ART1(step=0.4, rho=0.1, n_clusters=3,\n                                     verbose=False)\n            artnet.predict(np.array([[0.5]]))\n\n        with self.assertRaises(ValueError):\n            # Invalid data size for second input\n            artnet = algorithms.ART1(step=0.4, rho=0.1, n_clusters=3,\n                                     verbose=False)\n            artnet.predict(np.array([[1]]))\n            artnet.predict(np.array([[1, 1]]))\n\n    def test_simple_art1(self):\n        ann = algorithms.ART1(step=2, rho=0.7, n_clusters=2, verbose=False)\n        classes = ann.predict(data)\n\n        for answer, result in zip([0, 1, 1], classes):\n            self.assertEqual(result, answer)\n\n        self.assertPickledNetwork(ann, data)\n\n    def test_art1_on_real_problem(self):\n        data = pd.DataFrame(lenses)\n\n        encoder = preprocessing.OneHotEncoder(categories='auto')\n        enc_data = encoder.fit_transform(data.values[:, 1:]).toarray()\n\n        artnet = algorithms.ART1(\n            step=1.5,\n            rho=0.7,\n            n_clusters=3,\n            verbose=False,\n        )\n        classes = artnet.predict(enc_data)\n\n        unique_classes = list(np.sort(np.unique(classes)))\n        self.assertEqual(unique_classes, [0, 1, 2])\n"""
tests/algorithms/competitive/test_growing_neural_gas.py,0,"b'import pickle\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nfrom neupy import algorithms\nfrom neupy.algorithms.competitive.growing_neural_gas import (\n    NeuralGasGraph, NeuronNode)\n\nfrom base import BaseTestCase\n\n\nclass GrowingNeuralGasTestCase(BaseTestCase):\n    debug_plot = False\n\n    def setUp(self):\n        super(BaseTestCase, self).setUp()\n        self.data, _ = make_blobs(\n            n_samples=1000,\n            n_features=2,\n            centers=2,\n            cluster_std=0.4,\n            random_state=0\n        )\n\n    def make_plot(self, data, network):\n        plt.figure(figsize=(5, 6))\n        plt.scatter(*data.T, alpha=0.4)\n\n        for node_1, node_2 in network.graph.edges:\n            weights = np.concatenate([node_1.weight, node_2.weight])\n            line, = plt.plot(*weights.T, color=\'black\')\n            plt.setp(line, color=\'black\')\n\n        plt.show()\n\n    def test_neural_gas_exceptions(self):\n        gng = algorithms.GrowingNeuralGas(n_inputs=2)\n\n        with self.assertRaisesRegexp(ValueError, ""more than 2 dimensions""):\n            gng.train(np.random.random((10, 2, 1)), epochs=1)\n\n        with self.assertRaisesRegexp(ValueError, ""have 2 features, but got 1""):\n            gng.train(np.random.random((10, 1)), epochs=1)\n\n    def test_simple_neural_gas(self):\n        gng = algorithms.GrowingNeuralGas(\n            n_inputs=2,\n            n_start_nodes=2,\n            shuffle_data=True,\n\n            step=0.2,\n            neighbour_step=0.05,\n            max_edge_age=10,\n            n_iter_before_neuron_added=50,\n\n            error_decay_rate=0.995,\n            after_split_error_decay_rate=0.5,\n            min_distance_for_update=0.1,\n            max_nodes=100,\n            verbose=False,\n        )\n        data = self.data\n        gng.train(data, epochs=10)\n\n        # Add one useless node to make sure that it\n        # would be deleted at the end\n        first_node = gng.graph.nodes[0]\n        new_node = algorithms.NeuronNode(weight=np.array([[1000., 1000.]]))\n        new_node.useless_node = True\n\n        gng.graph.add_node(new_node)\n        gng.graph.add_edge(new_node, first_node)\n\n        # We run it twice in order to make sure that we won\'t\n        # reset weights on the second run.\n        gng.train(data, epochs=10)\n\n        if self.debug_plot:\n            self.make_plot(data, gng)\n\n        self.assertEqual(len(gng.graph.nodes), gng.max_nodes)\n        self.assertAlmostEqual(gng.errors.train[-1], 0.09, places=2)\n\n        useless_node_present = any(\n            getattr(node, \'useless_node\', False) for node in gng.graph.nodes)\n\n        self.assertFalse(useless_node_present)\n\n        with self.assertRaises(NotImplementedError):\n            gng.predict(data)\n\n        # Check that we can stop training in case if we don\'t get any updates\n        before_epochs = gng.last_epoch\n        gng.min_distance_for_update = 10\n        gng.train(data, epochs=10)\n        self.assertEqual(before_epochs + 1, gng.last_epoch)\n\n    def test_gng_storage(self):\n        gng = algorithms.GrowingNeuralGas(\n            n_inputs=2,\n            step=0.2,\n            verbose=False,\n            n_iter_before_neuron_added=5 * len(self.data),\n        )\n        gng.train(self.data, epochs=10)\n        self.assertEqual(gng.graph.n_nodes, 4)\n\n        gng_recovered = pickle.loads(pickle.dumps(gng))\n        self.assertEqual(gng_recovered.graph.n_nodes, 4)\n\n        gng_recovered.train(self.data, epochs=10)\n        self.assertEqual(gng_recovered.graph.n_nodes, 6)\n\n    def test_gng_repr(self):\n        gng = algorithms.GrowingNeuralGas(\n            n_inputs=2,\n            step=0.2,\n            verbose=False,\n        )\n        gng.train(self.data, epochs=10)\n        self.assertEqual(\n            str(gng.graph),\n            ""<NeuralGasGraph n_nodes=12, n_edges=19>"",\n        )\n\n\nclass NeuralGasGraphTestCase(BaseTestCase):\n    def test_simple_graph(self):\n        graph = NeuralGasGraph()\n\n        node_a = NeuronNode(1)\n        node_b = NeuronNode(2)\n        node_c = NeuronNode(3)\n\n        graph.add_node(node_a)\n        graph.add_node(node_b)\n        graph.add_node(node_c)\n\n        graph.add_edge(node_a, node_b)\n        graph.add_edge(node_b, node_c)\n\n        self.assertEqual(graph.n_nodes, 3)\n        self.assertEqual(graph.n_edges, 2)\n\n        self.assertEqual(graph.find_edge_id(node_a, node_b), (node_a, node_b))\n        self.assertEqual(graph.find_edge_id(node_b, node_a), (node_a, node_b))\n\n        error_message = ""Edge between specified nodes doesn\'t exist""\n        with self.assertRaisesRegexp(ValueError, error_message):\n            graph.find_edge_id(node_a, node_c)\n\n        graph.remove_edge(node_c, node_b)\n\n        self.assertEqual(graph.n_nodes, 3)\n        self.assertEqual(graph.n_edges, 1)\n\n        graph.remove_node(node_c)\n\n        self.assertEqual(graph.n_nodes, 2)\n        self.assertEqual(graph.n_edges, 1)\n\n        error_message = (\n            ""Cannot remove node, because it\'s ""\n            ""connected to 1 other node\\(s\\)""\n        )\n        with self.assertRaisesRegexp(ValueError, error_message):\n            graph.remove_node(node_a)\n\n    def test_node_repr(self):\n        node_a = NeuronNode(1)\n        self.assertEqual(str(node_a), ""<NeuronNode error=0.0>"")\n\n        node_a.error = 1\n        self.assertEqual(str(node_a), ""<NeuronNode error=1.0>"")\n\n        node_a.error = 3.141592654\n        self.assertEqual(str(node_a), ""<NeuronNode error=3.141593>"")\n'"
tests/algorithms/competitive/test_lvq.py,0,"b'from functools import partial\n\nimport numpy as np\nfrom sklearn import datasets\n\nfrom neupy import algorithms, init\nfrom neupy.exceptions import NotTrained\n\nfrom base import BaseTestCase\nfrom helpers import compare_networks\n\n\nclass LVQTestCase(BaseTestCase):\n    def setUp(self):\n        xor_x_train = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n        xor_y_train = np.array([[1, -1, -1, 1]]).T\n\n        self.data = np.concatenate(\n            [\n                xor_x_train,\n                xor_x_train + 0.1,\n                xor_x_train + np.array([[0.1, -0.1]]),\n                xor_x_train - 0.1,\n                xor_x_train - np.array([[0.1, -0.1]]),\n            ],\n            axis=0\n        )\n        target = np.concatenate(\n            [\n                xor_y_train,\n                xor_y_train,\n                xor_y_train,\n                xor_y_train,\n                xor_y_train,\n            ],\n            axis=0\n        )\n        self.target = np.where(target == -1, 0, target)\n        super(LVQTestCase, self).setUp()\n\n    def test_lvq_initialization_exceptions(self):\n        with self.assertRaises(ValueError):\n            # n_sublcasses < n_classes\n            algorithms.LVQ(n_inputs=2, n_subclasses=2, n_classes=3)\n\n        with self.assertRaises(ValueError):\n            # sum(prototypes_per_class) != n_subclasses\n            algorithms.LVQ(n_inputs=2, n_subclasses=10, n_classes=3,\n                           prototypes_per_class=[5, 3, 3])\n\n        with self.assertRaises(ValueError):\n            # len(prototypes_per_class) != n_classes\n            algorithms.LVQ(n_inputs=2, n_subclasses=10, n_classes=3,\n                           prototypes_per_class=[5, 5])\n\n    def test_lvq_training_exceptions(self):\n        lvqnet = algorithms.LVQ(n_inputs=2, n_subclasses=4, n_classes=2)\n\n        with self.assertRaises(NotTrained):\n            lvqnet.predict(np.array([1, 2]))\n\n        with self.assertRaises(ValueError):\n            X_train = np.array([\n                [1, 2],\n                [3, 4],\n                [4, 5],\n            ])\n            y_train = np.array([0, 1, 0])\n            # len(X_train) <= n_subclasses\n            lvqnet.train(X_train, y_train)\n\n        with self.assertRaises(ValueError):\n            X_train = np.array([\n                [1, 2],\n                [3, 4],\n                [5, 6],\n                [7, 8],\n                [9, 10],\n            ])\n            y_train = np.array([0, 0, 0, 0, 1])\n            # there are should be 3 or more samples for\n            # class 1, got only 1\n            lvqnet.train(X_train, y_train)\n\n        with self.assertRaises(ValueError):\n            X_train = np.array([\n                [1, 2],\n                [3, 4],\n                [4, 5],\n                [5, 6],\n                [67, 8],\n            ])\n            y_train = np.array([0, 1, 0, 1, 2])\n            # 3 unique classes instead of 2 expected\n            lvqnet.train(X_train, y_train)\n\n    def test_lvq_weight_initialization_state(self):\n        lvqnet = algorithms.LVQ(n_inputs=2, n_classes=2)\n        self.assertFalse(lvqnet.initialized)\n\n        lvqnet.train(np.random.random((10, 2)),\n                     np.random.random(10).round(),\n                     epochs=1)\n        self.assertTrue(lvqnet.initialized)\n\n        lvqnet = algorithms.LVQ(n_inputs=2, n_classes=3,\n                                weight=np.random.random((2, 3)))\n        self.assertTrue(lvqnet.initialized)\n\n        lvqnet = algorithms.LVQ(n_inputs=2, n_classes=3,\n                                weight=init.Normal())\n        self.assertTrue(lvqnet.initialized)\n        self.assertEqual(lvqnet.weight.shape, (2, 3))\n\n    def test_lvq_with_odd_number_of_subclasses(self):\n        lvqnet = algorithms.LVQ(\n            n_inputs=2,\n            n_subclasses=3,\n            n_classes=2,\n        )\n        self.assertIn(lvqnet.prototypes_per_class, ([2, 1], [1, 2]))\n\n    def test_simple_lvq(self):\n        lvqnet = algorithms.LVQ(\n            n_inputs=2,\n            n_subclasses=4,\n            n_classes=2,\n            shuffle_data=True,\n        )\n\n        lvqnet.train(self.data, self.target, epochs=100)\n        predicted_target = lvqnet.predict(self.data)\n\n        self.assertEqual(lvqnet.errors.train[-1], 0)\n        np.testing.assert_array_equal(\n            predicted_target,\n            self.target[:, 0])\n\n        self.assertPickledNetwork(lvqnet, self.data)\n\n    def test_lvq_step_reduction(self):\n        lvqnet = algorithms.LVQ(\n            n_inputs=2,\n            n_subclasses=4,\n            n_classes=2,\n\n            minstep=0.1,\n            step=1.1,\n            n_updates_to_stepdrop=200,\n        )\n\n        n_expected_updates = 0\n        for i in range(10):\n            n_expected_updates += len(self.data)\n            lvqnet.train(self.data, self.target, epochs=1)\n            expected_step = 1.1 - (i + 1) * 0.1\n            self.assertAlmostEqual(lvqnet.training_step, expected_step)\n\n        self.assertEqual(n_expected_updates, lvqnet.n_updates)\n        self.assertEqual(lvqnet.training_step, lvqnet.minstep)\n\n    def test_lvq_with_disabled_step_reduction(self):\n        lvqnet = algorithms.LVQ(\n            n_inputs=2,\n            n_subclasses=4,\n            n_classes=2,\n            n_updates_to_stepdrop=None,\n        )\n\n        n_expected_updates = 0\n        for i in range(10):\n            n_expected_updates += len(self.data)\n            lvqnet.train(self.data, self.target, epochs=1)\n            self.assertAlmostEqual(lvqnet.training_step, lvqnet.step)\n\n        self.assertEqual(n_expected_updates, lvqnet.n_updates)\n\n    def test_lvq_with_custom_set_of_prototypes(self):\n        lvqnet = algorithms.LVQ(\n            n_inputs=2,\n            n_subclasses=4,\n            n_classes=2,\n\n            prototypes_per_class=[3, 1],\n        )\n\n        lvqnet.train(self.data, self.target, epochs=3)\n        self.assertGreater(lvqnet.errors.train[-1], 0)\n\n        lvqnet.train(self.data, self.target, epochs=30)\n        self.assertEqual(lvqnet.errors.train[-1], 0)\n\n    def test_compare_lvq_and_lvq2(self):\n        dataset = datasets.load_iris()\n        data, target = dataset.data, dataset.target\n\n        # Prepare the same weights for the fair comparison\n        lvq = algorithms.LVQ(n_inputs=4, n_subclasses=3, n_classes=3)\n        lvq.train(data, target, epochs=1)\n        prepared_lvq_weights = lvq.weight\n\n        compare_networks(\n            algorithms.LVQ,\n            partial(algorithms.LVQ2, epsilon=0.1),\n\n            data=[data, target],\n            epochs=10,\n            show_comparison_plot=False,\n\n            n_inputs=4,\n            n_subclasses=3,\n            n_classes=3,\n            weight=prepared_lvq_weights,\n        )\n\n    def test_compare_lvq_and_lvq21(self):\n        dataset = datasets.load_iris()\n        data, target = dataset.data, dataset.target\n\n        # Prepare the same weights for the fair comparison\n        lvq = algorithms.LVQ(n_inputs=4, n_subclasses=3, n_classes=3)\n        lvq.train(data, target, epochs=1)\n        prepared_lvq_weights = lvq.weight\n\n        compare_networks(\n            algorithms.LVQ,\n            partial(algorithms.LVQ21, epsilon=0.1),\n\n            data=[data, target],\n            epochs=10,\n            show_comparison_plot=False,\n\n            n_inputs=4,\n            n_subclasses=3,\n            n_classes=3,\n            weight=prepared_lvq_weights,\n        )\n\n    def test_compare_lvq_and_lvq3(self):\n        dataset = datasets.load_iris()\n        data, target = dataset.data, dataset.target\n\n        # Prepare the same weights for the fair comparison\n        lvq = algorithms.LVQ(n_inputs=4, n_subclasses=6, n_classes=3)\n        lvq.train(data, target, epochs=1)\n        prepared_lvq_weights = lvq.weight\n\n        compare_networks(\n            algorithms.LVQ,\n            partial(algorithms.LVQ3, epsilon=0.4),\n\n            data=[data, target],\n            epochs=100,\n            show_comparison_plot=False,\n\n            n_inputs=4,\n            n_subclasses=6,\n            n_classes=3,\n            prototypes_per_class=[4, 1, 1],\n            step=0.001,\n            weight=prepared_lvq_weights,\n        )\n'"
tests/algorithms/competitive/test_sofm.py,0,"b'import math\n\nimport numpy as np\n\nfrom neupy import algorithms\nfrom neupy.exceptions import WeightInitializationError\nfrom neupy.algorithms.competitive import sofm\nfrom neupy.algorithms.competitive.neighbours import gaussian_df\n\nfrom base import BaseTestCase\n\n\nX = np.array([\n    [0.1961, 0.9806],\n    [-0.1961, 0.9806],\n    [0.9806, 0.1961],\n    [0.9806, -0.1961],\n    [-0.5812, -0.8137],\n    [-0.8137, -0.5812],\n])\nanswers = np.array([\n    [0., 1., 0.],\n    [0., 1., 0.],\n    [1., 0., 0.],\n    [1., 0., 0.],\n    [0., 0., 1.],\n    [0., 0., 1.],\n])\n\n\ndef make_circle(max_samples=100):\n    data = np.random.random((max_samples, 2))\n    x, y = data[:, 0], data[:, 1]\n\n    distance_from_center = ((x - 0.5) ** 2 + (y - 0.5) ** 2)\n    return data[distance_from_center <= 0.5 ** 2]\n\n\nclass SOFMUtilsFunctionTestsCase(BaseTestCase):\n    def test_sofm_gaussian_df_zero_std(self):\n        actual_output = gaussian_df(np.arange(-3, 4), mean=0, std=0)\n        expected_output = np.array([0, 0, 0, 1, 0, 0, 0])\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n    def test_sofm_gaussian_df(self):\n        actual_output = gaussian_df(np.arange(-3, 4), mean=0, std=1)\n        expected_output = np.array([\n            0.23873659, 0.52907781, 0.8528642,\n            1., 0.8528642, 0.52907781, 0.23873659\n        ])\n        np.testing.assert_array_almost_equal(expected_output, actual_output)\n\n    def test_sofm_decay_function(self):\n        actual_output = sofm.decay_function(12, 10, reduction_rate=10)\n        self.assertEqual(6, actual_output)\n\n        actual_output = sofm.decay_function(12, 20, reduction_rate=10)\n        self.assertEqual(4, actual_output)\n\n        actual_output = sofm.decay_function(12, 30, reduction_rate=10)\n        self.assertEqual(3, actual_output)\n\n\nclass SOFMDistanceFunctionsTestCase(BaseTestCase):\n    def assert_invalid_distance_function(self, func, vector, weight,\n                                         expected, decimal=6):\n        np.testing.assert_array_almost_equal(\n            func(vector, weight),\n            expected,\n            decimal=decimal)\n\n    def test_euclid_transform(self):\n        self.assert_invalid_distance_function(\n            sofm.neg_euclid_distance,\n            np.array([[1, 2, 3]]),\n            np.array([\n                [1, 2, 3],\n                [1, 1, 1],\n                [0, 0, 1],\n                [0, 1, 2],\n            ]).T,\n            np.array([[0, -math.sqrt(5), -3, -math.sqrt(3)]])\n        )\n\n    def test_cosine_transform(self):\n        self.assert_invalid_distance_function(\n            sofm.cosine_similarity,\n            np.array([[1, 2, 3]]),\n            np.array([\n                [1, 2, 3],\n                [1, 1, 1],\n                [0, 0, 1],\n                [0, 1, 2],\n            ]).T,\n            np.array([[1, 0.926, 0.802, 0.956]]),\n            decimal=3)\n\n\nclass SOFMNeigboursTestCase(BaseTestCase):\n    def test_sofm_neightbours_exceptions(self):\n        with self.assertRaisesRegexp(ValueError, ""Cannot find center""):\n            sofm.find_neighbours_on_rect_grid(\n                grid=np.zeros((3, 3)),\n                center=(0, 0, 0),\n                radius=1)\n\n        with self.assertRaisesRegexp(ValueError, ""Cannot find center""):\n            sofm.find_step_scaler_on_rect_grid(\n                grid=np.zeros((3, 3)),\n                center=(0, 0, 0),\n                std=1)\n\n    def test_neightbours_in_10d(self):\n        actual_result = sofm.find_neighbours_on_rect_grid(\n            np.zeros([3] * 10),\n            center=[1] * 10,\n            radius=0)\n        self.assertEqual(np.sum(actual_result), 1)\n\n    def test_neightbours_in_3d(self):\n        actual_result = sofm.find_neighbours_on_rect_grid(\n            np.zeros((5, 5, 3)),\n            center=(2, 2, 1),\n            radius=2)\n\n        expected_result = np.array([[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0.]\n        ], [\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [1., 1., 1., 1., 1.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 1., 0., 0.]\n        ], [\n            [0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0.]\n        ]])\n        expected_result = np.transpose(expected_result, (1, 2, 0))\n        np.testing.assert_array_equal(actual_result, expected_result)\n\n    def test_neightbours_in_2d(self):\n        actual_result = sofm.find_neighbours_on_rect_grid(\n            np.zeros((3, 3)),\n            center=(0, 0),\n            radius=1)\n\n        expected_result = np.array([\n            [1., 1., 0.],\n            [1., 0., 0.],\n            [0., 0., 0.]\n        ])\n        np.testing.assert_array_equal(actual_result, expected_result)\n\n        actual_result = sofm.find_neighbours_on_rect_grid(\n            np.zeros((5, 5)),\n            center=(2, 2),\n            radius=2)\n\n        expected_result = np.array([\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [1., 1., 1., 1., 1.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 1., 0., 0.]\n        ])\n        np.testing.assert_array_equal(actual_result, expected_result)\n\n        actual_result = sofm.find_neighbours_on_rect_grid(\n            np.zeros((3, 3)),\n            center=(1, 1),\n            radius=0)\n\n        expected_result = np.array([\n            [0., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 0.]\n        ])\n        np.testing.assert_array_equal(actual_result, expected_result)\n\n    def test_neightbours_in_1d(self):\n        actual_result = sofm.find_neighbours_on_rect_grid(\n            np.zeros(5),\n            center=(2,),\n            radius=1)\n\n        expected_result = np.array([0, 1, 1, 1, 0])\n        np.testing.assert_array_equal(actual_result, expected_result)\n\n    def test_sofm_gaussian_neighbour_2d(self):\n        expected_result = np.array([\n            [0.52907781, 0.69097101, 0.7645389, 0.69097101, 0.52907781],\n            [0.69097101, 0.85286420, 0.9264321, 0.85286420, 0.69097101],\n            [0.76453890, 0.92643210, 1.0000000, 0.92643210, 0.76453890],\n            [0.69097101, 0.85286420, 0.9264321, 0.85286420, 0.69097101],\n            [0.52907781, 0.69097101, 0.7645389, 0.69097101, 0.52907781],\n        ])\n\n        actual_result = sofm.find_step_scaler_on_rect_grid(\n            grid=np.zeros((5, 5)),\n            center=(2, 2),\n            std=1)\n\n        np.testing.assert_array_almost_equal(\n            expected_result, actual_result)\n\n    def test_sofm_gaussian_neighbour_3d(self):\n        expected_result = np.array([\n            [\n                [0.85286420, 0.90190947, 0.85286420],\n                [0.90190947, 0.95095473, 0.90190947],\n                [0.85286420, 0.90190947, 0.85286420],\n            ], [\n                [0.90190947, 0.95095473, 0.90190947],\n                [0.95095473, 1.00000000, 0.95095473],\n                [0.90190947, 0.95095473, 0.90190947],\n            ], [\n                [0.85286420, 0.90190947, 0.85286420],\n                [0.90190947, 0.95095473, 0.90190947],\n                [0.85286420, 0.90190947, 0.85286420],\n            ]\n        ])\n        actual_result = sofm.find_step_scaler_on_rect_grid(\n            grid=np.zeros((3, 3, 3)),\n            center=(1, 1, 1),\n            std=1)\n\n        np.testing.assert_array_almost_equal(\n            expected_result, actual_result)\n\n    def test_sofm_hexagon_grid_neighbours(self):\n        # radius 1 (odd row index)\n        expected_result = np.array([\n            [0, 0, 0, 0, 0],\n            [0, 2, 2, 0, 0],\n            [0, 2, 1, 2, 0],\n            [0, 2, 2, 0, 0],\n            [0, 0, 0, 0, 0],\n        ])\n        actual_result = sofm.find_neighbours_on_hexagon_grid(\n            grid=np.zeros((5, 5)),\n            center=(2, 2),\n            radius=1)\n\n        np.testing.assert_array_almost_equal(\n            expected_result, actual_result)\n\n        # radius 1 (even row index)\n        expected_result = np.array([\n            [0, 0, 2, 2, 0],\n            [0, 2, 1, 2, 0],\n            [0, 0, 2, 2, 0],\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n        ])\n        actual_result = sofm.find_neighbours_on_hexagon_grid(\n            grid=np.zeros((5, 5)),\n            center=(1, 2),\n            radius=1)\n\n        np.testing.assert_array_almost_equal(\n            expected_result, actual_result)\n\n        # radius 1 (partialy broken)\n        expected_result = np.array([\n            [1, 2, 0, 0, 0],\n            [2, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n        ])\n        actual_result = sofm.find_neighbours_on_hexagon_grid(\n            grid=np.zeros((5, 5)),\n            center=(0, 0),\n            radius=1)\n\n        np.testing.assert_array_almost_equal(\n            expected_result, actual_result)\n\n        # radius 2 (partialy broken)\n        expected_result = np.array([\n            [0, 0, 0, 3, 2],\n            [0, 0, 3, 2, 1],\n            [0, 0, 0, 3, 2],\n            [0, 0, 0, 3, 3],\n            [0, 0, 0, 0, 0],\n        ])\n        actual_result = sofm.find_neighbours_on_hexagon_grid(\n            grid=np.zeros((5, 5)),\n            center=(1, 4),\n            radius=2)\n\n        np.testing.assert_array_almost_equal(\n            expected_result, actual_result)\n\n        # radius 2\n        expected_result = np.array([\n            [0, 3, 3, 3, 0],\n            [3, 2, 2, 3, 0],\n            [3, 2, 1, 2, 3],\n            [3, 2, 2, 3, 0],\n            [0, 3, 3, 3, 0],\n        ])\n        actual_result = sofm.find_neighbours_on_hexagon_grid(\n            grid=np.zeros((5, 5)),\n            center=(2, 2),\n            radius=2)\n\n        np.testing.assert_array_almost_equal(\n            expected_result, actual_result)\n\n        # radius 3\n        expected_result = np.array([\n            [0, 0, 4, 4, 4, 4, 0],\n            [0, 4, 3, 3, 3, 4, 0],\n            [0, 4, 3, 2, 2, 3, 4],\n            [4, 3, 2, 1, 2, 3, 4],\n            [0, 4, 3, 2, 2, 3, 4],\n            [0, 4, 3, 3, 3, 4, 0],\n            [0, 0, 4, 4, 4, 4, 0],\n        ])\n        actual_result = sofm.find_neighbours_on_hexagon_grid(\n            grid=np.zeros((7, 7)),\n            center=(3, 3),\n            radius=3)\n\n        np.testing.assert_array_almost_equal(\n            expected_result, actual_result)\n\n\nclass SOFMTestCase(BaseTestCase):\n    def setUp(self):\n        super(SOFMTestCase, self).setUp()\n        self.weight = np.array([\n            [0.65091234, -0.52271686, 0.56344712],\n            [-0.13191953, 2.43582716, -0.19703619]\n        ])\n\n    def test_invalid_attrs(self):\n        with self.assertRaisesRegexp(ValueError, ""Feature grid""):\n            # Invalid feature grid shape\n            algorithms.SOFM(n_inputs=2, n_outputs=4, features_grid=(2, 3))\n\n        with self.assertRaisesRegexp(ValueError, ""n_outputs, features_grid""):\n            algorithms.SOFM(n_inputs=2)\n\n        with self.assertRaisesRegexp(ValueError, ""more than 2 dimensions""):\n            sofm = algorithms.SOFM(n_inputs=2, n_outputs=3, weight=self.weight)\n            sofm.train(np.zeros((10, 2, 1)))\n\n        with self.assertRaisesRegexp(ValueError, ""Only 2D inputs are allowed""):\n            sofm = algorithms.SOFM(n_inputs=2, n_outputs=3, weight=self.weight)\n            sofm.predict(np.zeros((10, 2, 1)))\n\n        with self.assertRaisesRegexp(ValueError, ""one or two dimensional""):\n            algorithms.SOFM(\n                n_inputs=2,\n                features_grid=(3, 1, 1),\n                grid_type=\'hexagon\',\n            )\n\n    def test_sofm_1d_vector_input(self):\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=3,\n            weight=self.weight,\n        )\n        output = sofm.predict(X[0])\n        self.assertEqual(output.shape, (1, 3))\n\n    def test_sofm(self):\n        sn = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=3,\n            weight=X[(2, 0, 4), :].T,\n            learning_radius=0,\n            features_grid=(3,),\n            shuffle_data=True,\n            verbose=False,\n            reduce_radius_after=None,\n            reduce_step_after=None,\n            reduce_std_after=None,\n        )\n        sn.train(X, epochs=100)\n\n        np.testing.assert_array_almost_equal(\n            sn.predict(X), answers)\n\n    def test_sofm_euclide_norm_distance(self):\n        weight = np.array([\n            [1.41700099, 0.52680476],\n            [-0.60938464, 1.56545643],\n            [-0.30243644, 0.13994967],\n            [-0.07456091, 0.54797268],\n            [-1.12894803, 0.32702141],\n            [0.92084690, 0.02683249],\n        ]).T\n        sn = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=6,\n            weight=weight,\n            distance=\'euclid\',\n            learning_radius=1,\n            features_grid=(3, 2),\n            verbose=False\n        )\n\n        sn.train(X, epochs=10)\n\n        answers = np.array([\n            [0., 0., 0., 1., 0., 0.],\n            [0., 0., 0., 1., 0., 0.],\n            [1., 0., 0., 0., 0., 0.],\n            [1., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 1., 0.],\n        ])\n\n        np.testing.assert_array_almost_equal(\n            sn.predict(X),\n            answers\n        )\n\n    def test_sofm_training_with_4d_grid(self):\n        sofm = algorithms.SOFM(\n            n_inputs=4,\n            n_outputs=8,\n            features_grid=(2, 2, 2),\n            verbose=False,\n        )\n\n        data = np.concatenate([X, X], axis=1)\n\n        sofm.train(data, epochs=1)\n        error_after_first_epoch = sofm.errors.train[-1]\n\n        sofm.train(data, epochs=9)\n        self.assertLess(sofm.errors.train[-1], error_after_first_epoch)\n\n    def test_sofm_angle_distance(self):\n        sn = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=3,\n            distance=\'cos\',\n            learning_radius=1,\n            features_grid=(3, 1),\n            weight=X[(0, 2, 4), :].T,\n            verbose=False\n        )\n        sn.train(X, epochs=3)\n\n        answers = np.array([\n            [1., 0., 0.],\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n            [0., 0., 1.],\n        ])\n        np.testing.assert_array_almost_equal(\n            sn.predict(X),\n            answers)\n\n    def test_sofm_weight_norm_after_training_with_custom_weights(self):\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=3,\n            distance=\'cos\',\n            learning_radius=1,\n            features_grid=(3, 1),\n            weight=X[(0, 2, 4), :].T,\n            verbose=False\n        )\n\n        actual_norms = np.linalg.norm(sofm.weight, axis=0)\n        expected_norms = np.array([1, 1, 1])\n        np.testing.assert_array_almost_equal(expected_norms, actual_norms)\n\n        sofm.train(X, epochs=6)\n\n        actual_norms = np.linalg.norm(sofm.weight, axis=0)\n        expected_norms = np.array([1, 1, 1])\n        np.testing.assert_array_almost_equal(expected_norms, actual_norms)\n\n    def test_sofm_weight_norm_before_training(self):\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=3,\n            verbose=False,\n            distance=\'cos\',\n        )\n\n        actual_norms = np.linalg.norm(sofm.weight, axis=0)\n        expected_norms = np.array([1, 1, 1])\n\n        np.testing.assert_array_almost_equal(expected_norms, actual_norms)\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.SOFM(n_inputs=1, n_outputs=1, verbose=False),\n            X.ravel())\n\n    def test_predict_different_inputs(self):\n        sofmnet = algorithms.SOFM(n_inputs=1, n_outputs=2, verbose=False)\n        target = np.array([\n            [1, 0],\n            [1, 0],\n            [0, 1],\n            [1, 0],\n            [1, 0],\n            [1, 0],\n            [1, 0],\n            [0, 1],\n            [0, 1],\n            [0, 1],\n            [0, 1],\n            [0, 1],\n        ])\n\n        sofmnet.train(X.ravel())\n        self.assertInvalidVectorPred(sofmnet, X.ravel(),\n                                     target, decimal=2)\n\n    def test_sofm_std_parameter(self):\n        default_params = dict(\n            n_inputs=2,\n            n_outputs=3,\n            learning_radius=1,\n            weight=self.weight)\n\n        sofm_1 = algorithms.SOFM(std=1, **default_params)\n        sofm_1.train(X[:1], epochs=1)\n        dist_1 = np.linalg.norm(sofm_1.weight[0, :] - sofm_1.weight[1, :])\n\n        sofm_0 = algorithms.SOFM(std=0.1, **default_params)\n        sofm_0.train(X[:1], epochs=1)\n        dist_0 = np.linalg.norm(sofm_0.weight[0, :] - sofm_0.weight[1, :])\n\n        # Since SOFM-1 has bigger std than SOFM-0, two updated\n        # neurons should be closer to each other for SOFM-1 than\n        # for SOFM-0\n        self.assertLess(dist_1, dist_0)\n\n    def test_sofm_n_outputs_as_optional_parameter(self):\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            features_grid=(10, 2, 3),\n        )\n        self.assertEqual(60, sofm.n_outputs)\n\n    def test_sofm_hexagon_grid(self):\n        data = make_circle(max_samples=100)\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=9,\n            learning_radius=1,\n            reduce_radius_after=4,\n            features_grid=(3, 3),\n            verbose=False,\n            grid_type=\'hexagon\',\n        )\n        sofm.train(data, epochs=10)\n        grid = sofm.weight.reshape((2, 3, 3))\n\n        center = grid[:, 1, 1]\n        top_left = grid[:, 0, 0]\n        top_right = grid[:, 0, 2]\n\n        distance_top_left = np.linalg.norm(center - top_left)\n        distance_top_right = np.linalg.norm(center - top_right)\n\n        self.assertLess(distance_top_right, distance_top_left)\n\n        bottom_left = grid[:, 2, 0]\n        bottom_right = grid[:, 2, 2]\n\n        distance_bottom_left = np.linalg.norm(center - bottom_left)\n        distance_bottom_right = np.linalg.norm(center - bottom_right)\n\n        self.assertLess(distance_bottom_right, distance_bottom_left)\n\n    def test_sofm_storage(self):\n        X = np.random.random((100, 10))\n        sofm = algorithms.SOFM(\n            n_inputs=10,\n            features_grid=(10, 10),\n            std=1,\n            step=0.5,\n            learning_radius=2,\n            weight=np.random.random((10, 100))\n        )\n\n        sofm.train(X, epochs=10)\n        self.assertPickledNetwork(sofm, X)\n\n        parameters = sofm.get_params()\n        self.assertIn(\'weight\', parameters)\n        self.assertIsInstance(parameters[\'weight\'], np.ndarray)\n\n\nclass SOFMParameterReductionTestCase(BaseTestCase):\n    def test_sofm_step_reduction(self):\n        X = 4 * np.ones((1, 2))\n        default_params = dict(\n            n_inputs=2,\n            n_outputs=1,\n            step=0.1,\n            weight=np.ones((2, 1)))\n\n        sofm_1 = algorithms.SOFM(reduce_step_after=1, **default_params)\n        sofm_1.train(X, epochs=4)\n        dist_1 = np.linalg.norm(sofm_1.weight.T - X)\n\n        sofm_2 = algorithms.SOFM(reduce_step_after=10, **default_params)\n        sofm_2.train(X, epochs=4)\n        dist_2 = np.linalg.norm(sofm_2.weight.T - X)\n\n        # SOFM-2 suppose to be closer, becase learning rate decreases\n        # slower with bigger reduction factor\n        self.assertLess(dist_2, dist_1)\n\n        sofm_3 = algorithms.SOFM(reduce_step_after=None, **default_params)\n        sofm_3.train(X, epochs=4)\n        dist_3 = np.linalg.norm(sofm_3.weight.T - X)\n\n        # SOFM-3 doesn\'t have any step reduction, so it\n        # converges even faster than SOFM-2\n        self.assertLess(dist_3, dist_2)\n\n    def test_sofm_std_reduction(self):\n        X = 4 * np.ones((1, 2))\n        default_params = dict(\n            n_inputs=2,\n            n_outputs=3,\n            weight=np.ones((2, 3)),\n\n            std=1,\n\n            learning_radius=1,\n            reduce_radius_after=None)\n\n        sofm_1 = algorithms.SOFM(reduce_std_after=1, **default_params)\n        sofm_1.train(X, epochs=4)\n        dist_1 = np.linalg.norm(sofm_1.weight.T - X)\n\n        sofm_2 = algorithms.SOFM(reduce_std_after=10, **default_params)\n        sofm_2.train(X, epochs=4)\n        dist_2 = np.linalg.norm(sofm_2.weight.T - X)\n\n        # SOFM-2 suppose to be closer, becase std decreases\n        # slower with bigger reduction factor\n        self.assertLess(dist_2, dist_1)\n\n        sofm_3 = algorithms.SOFM(reduce_std_after=None, **default_params)\n        sofm_3.train(X, epochs=4)\n        dist_3 = np.linalg.norm(sofm_3.weight.T - X)\n\n        # SOFM-3 doesn\'t have any std reduction, so it\n        # converges even faster than SOFM-2\n        self.assertLess(dist_3, dist_2)\n\n    def test_sofm_learning_radius_reduction(self):\n        X = 4 * np.ones((1, 2))\n        default_params = dict(\n            n_inputs=2,\n            n_outputs=3,\n            weight=np.ones((2, 3)),\n            learning_radius=1)\n\n        sofm_1 = algorithms.SOFM(reduce_radius_after=1, **default_params)\n        sofm_1.train(X, epochs=4)\n        dist_1 = np.linalg.norm(sofm_1.weight.T - X)\n\n        sofm_2 = algorithms.SOFM(reduce_radius_after=3, **default_params)\n        sofm_2.train(X, epochs=4)\n        dist_2 = np.linalg.norm(sofm_2.weight.T - X)\n\n        # SOFM-2 suppose to be closer, becase learning radius\n        # decreases slower with bigger reduction factor\n        self.assertLess(dist_2, dist_1)\n\n        sofm_3 = algorithms.SOFM(reduce_radius_after=None, **default_params)\n        sofm_3.train(X, epochs=4)\n        dist_3 = np.linalg.norm(sofm_3.weight.T - X)\n\n        # SOFM-3 doesn\'t have any learning radius reduction, so it\n        # converges even faster than SOFM-2\n        self.assertLess(dist_3, dist_2)\n\n    def test_sofm_custom_parameter_reduction(self):\n        X = 4 * np.ones((1, 2))\n        default_params = dict(\n            n_inputs=2,\n            n_outputs=3,\n            weight=np.ones((2, 3)),\n\n            std=1,\n            step=0.1,\n            learning_radius=1,\n\n            reduce_radius_after=None,\n            reduce_step_after=None,\n            reduce_std_after=None)\n\n        sofm_1 = algorithms.SOFM(**default_params)\n        sofm_1.train(X, epochs=4)\n        dist_1 = np.linalg.norm(sofm_1.weight.T - X)\n\n        def on_epoch_end_update_radius(network):\n            if network.last_epoch % 2 == 0:\n                network.learning_radius = 0\n            else:\n                network.learning_radius = 1\n\n        def on_epoch_end_update_step(network):\n            network.step = 0.1 / network.last_epoch\n\n        def on_epoch_end_update_std(network):\n            network.std = 1. / network.last_epoch\n\n        testcases = {\n            \'learning_radius\': on_epoch_end_update_radius,\n            \'step\': on_epoch_end_update_step,\n            \'std\': on_epoch_end_update_std,\n        }\n\n        for testcase_name, on_epoch_end in testcases.items():\n            sofm_2 = algorithms.SOFM(\n                signals=on_epoch_end,\n                **default_params\n            )\n            sofm_2.train(X, epochs=4)\n            dist_2 = np.linalg.norm(sofm_2.weight.T - X)\n\n            self.assertLess(dist_1, dist_2,\n                            msg=""Test case name: {}"".format(testcase_name))\n\n\nclass SOFMWeightInitializationTestCase(BaseTestCase):\n    def test_sofm_weight_init_exceptions(self):\n        msg = ""Cannot apply PCA""\n        with self.assertRaisesRegexp(WeightInitializationError, msg):\n            algorithms.SOFM(\n                n_inputs=4,\n                n_outputs=7,\n                weight=\'init_pca\',\n                grid_type=\'hexagon\'\n            )\n\n    def test_sample_data_function(self):\n        X = np.random.random((10, 4))\n        sampled_weights = sofm.sample_data(X, features_grid=(7, 1))\n        self.assertEqual(sampled_weights.shape, (4, 7))\n\n        X = np.random.random((3, 4))\n        sampled_weights = sofm.sample_data(X, features_grid=(7, 1))\n        self.assertEqual(sampled_weights.shape, (4, 7))\n\n    def test_sofm_init_during_the_training(self):\n        sofm = algorithms.SOFM(\n            n_inputs=4,\n            n_outputs=7,\n            weight=\'sample_from_data\',\n        )\n\n        X = np.random.random((10, 4))\n        self.assertTrue(callable(sofm.weight))\n\n        sofm.train(X, epochs=1)\n        self.assertFalse(callable(sofm.weight))\n\n    def test_sample_data_weight_init_in_sofm(self):\n        sofm = algorithms.SOFM(\n            n_inputs=4,\n            n_outputs=7,\n            weight=\'sample_from_data\',\n        )\n\n        X = np.random.random((10, 4))\n        self.assertTrue(callable(sofm.weight))\n\n        sofm.init_weights(X)\n        self.assertFalse(callable(sofm.weight))\n        self.assertEqual(sofm.weight.shape, (4, 7))\n\n    def test_linear_weight_init_in_sofm(self):\n        sofm = algorithms.SOFM(\n            n_inputs=4,\n            features_grid=(3, 3),\n            weight=\'init_pca\',\n        )\n\n        X = np.random.random((100, 4))\n        self.assertTrue(callable(sofm.weight))\n\n        sofm.init_weights(X)\n        self.assertFalse(callable(sofm.weight))\n        self.assertEqual(sofm.weight.shape, (4, 9))\n\n        for row in (0, 3, 6):\n            left = sofm.weight[:, row]\n            center = sofm.weight[:, row + 1]\n            right = sofm.weight[:, row + 2]\n\n            self.assertLess(\n                np.linalg.norm((left - center) ** 2),\n                np.linalg.norm((left - right) ** 2))\n\n        for i in range(3):\n            top = sofm.weight[:, i]\n            center = sofm.weight[:, i + 3]\n            bottom = sofm.weight[:, i + 6]\n\n            self.assertLess(\n                np.linalg.norm((top - center) ** 2),\n                np.linalg.norm((top - bottom) ** 2))\n\n    def test_sofm_double_initialization_exception_cos_distance(self):\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=3,\n            verbose=False,\n            distance=\'cos\',\n            weight=\'sample_from_data\'\n        )\n        sofm.init_weights(X)\n\n        with self.assertRaises(WeightInitializationError):\n            sofm.init_weights(X)\n\n    def test_sofm_double_initialization_exception(self):\n        sofm = algorithms.SOFM(\n            n_inputs=2,\n            n_outputs=3,\n            verbose=False,\n            weight=\'sample_from_data\'\n        )\n        sofm.init_weights(X)\n\n        with self.assertRaises(WeightInitializationError):\n            sofm.init_weights(X)\n'"
tests/algorithms/gd/__init__.py,0,b''
tests/algorithms/gd/test_conjgrad.py,0,"b'from functools import partial\nfrom collections import namedtuple\n\nimport numpy as np\nfrom sklearn import metrics\n\nfrom neupy import algorithms, layers\nfrom neupy.utils import asfloat\nfrom neupy.algorithms.gd import conjgrad as cg\n\nfrom helpers import simple_classification\nfrom helpers import compare_networks\nfrom base import BaseTestCase\n\n\nclass ConjugateGradientTestCase(BaseTestCase):\n    def test_functions(self):\n        Case = namedtuple(""Case"", ""func X answer"")\n\n        testcases = [\n            Case(\n                func=cg.fletcher_reeves,\n                X=(\n                    asfloat(np.array([1.35,  0.3])),\n                    asfloat(np.array([0.11, -0.5])),\n                    asfloat(np.array([0, 0])),\n                ),\n                answer=0.137\n            ),\n            Case(\n                func=cg.polak_ribiere,\n                X=(\n                    asfloat(np.array([1.,  -0.5])),\n                    asfloat(np.array([1.2, -0.45])),\n                    asfloat(np.array([0, 0])),\n                ),\n                answer=0.174\n            ),\n            Case(\n                func=cg.hentenes_stiefel,\n                X=(\n                    asfloat(np.array([1.,  -0.5])),\n                    asfloat(np.array([1.2, -0.45])),\n                    asfloat(np.array([0.2, 0.05])),\n                ),\n                answer=5.118\n            ),\n            Case(\n                func=cg.liu_storey,\n                X=(\n                    asfloat(np.array([1.,  -0.5])),\n                    asfloat(np.array([1.2, -0.45])),\n                    asfloat(np.array([0.2, 0.05])),\n                ),\n                answer=-1.243\n            ),\n            Case(\n                func=cg.dai_yuan,\n                X=(\n                    asfloat(np.array([1.,  -0.5])),\n                    asfloat(np.array([1.2, -0.45])),\n                    asfloat(np.array([0.2, 0.05])),\n                ),\n                answer=38.647\n            ),\n        ]\n\n        for testcase in testcases:\n            result = self.eval(testcase.func(*testcase.X))\n            self.assertAlmostEqual(result, testcase.answer, places=1)\n\n    def test_conjgrad(self):\n        cgnet = algorithms.ConjugateGradient(\n            [\n                layers.Input(10),\n                layers.Sigmoid(5),\n                layers.Sigmoid(1),\n            ],\n            loss=\'binary_crossentropy\',\n            shuffle_data=True,\n            verbose=False,\n            update_function=\'fletcher_reeves\',\n        )\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        cgnet.train(x_train, y_train, x_test, y_test, epochs=50)\n        actual_prediction = cgnet.predict(x_test).round().T\n\n        error = metrics.accuracy_score(actual_prediction[0], y_test)\n        self.assertAlmostEqual(error, 0.9, places=1)\n\n    def test_compare_bp_and_cg(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        compare_networks(\n            # Test classes\n            partial(\n                partial(algorithms.GradientDescent, batch_size=None),\n                step=1.0,\n            ),\n            partial(\n                algorithms.ConjugateGradient,\n                update_function=\'fletcher_reeves\'\n            ),\n            # Test data\n            (asfloat(x_train), asfloat(y_train)),\n            # Network configurations\n            network=layers.join(\n                layers.Input(10),\n                layers.Sigmoid(5),\n                layers.Sigmoid(1),\n            ),\n            loss=\'mse\',\n            shuffle_data=True,\n            # Test configurations\n            epochs=50,\n            show_comparison_plot=False\n        )\n\n    def test_conjugate_gradient_fletcher_reeves_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.ConjugateGradient,\n                update_function=\'fletcher_reeves\',\n                verbose=False,\n            ),\n            epochs=200,\n        )\n\n    def test_conjugate_gradient_dai_yuan_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.ConjugateGradient,\n                update_function=\'dai_yuan\',\n                verbose=False,\n            ),\n            epochs=200,\n        )\n\n    def test_conjugate_gradient_hentenes_stiefel_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.ConjugateGradient,\n                update_function=\'hentenes_stiefel\',\n                verbose=False,\n            ),\n            epochs=500,\n        )\n\n    def test_conjugate_gradient_polak_ribiere_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.ConjugateGradient,\n                update_function=\'polak_ribiere\',\n                verbose=False,\n            ),\n            epochs=1200,\n        )\n\n    def test_conjugate_gradient_liu_storey_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.ConjugateGradient,\n                update_function=\'liu_storey\',\n                verbose=False,\n            ),\n            epochs=1200,\n        )\n\n    def test_conjgrad_assign_step_exception(self):\n        with self.assertRaises(ValueError):\n            # Don\'t have step parameter\n            algorithms.ConjugateGradient(\n                layers.Input(2) > layers.Sigmoid(3) > layers.Sigmoid(1),\n                step=0.01,\n            )\n'"
tests/algorithms/gd/test_gd.py,3,"b'import pickle\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy import algorithms, layers\nfrom neupy.algorithms.gd import objectives\nfrom neupy.exceptions import InvalidConnection\n\nfrom base import BaseTestCase\nfrom helpers import simple_classification\n\n\nclass GradientDescentTestCase(BaseTestCase):\n    def test_large_network_representation(self):\n        optimizer = algorithms.GradientDescent([\n            layers.Input(1),\n            layers.Sigmoid(1),\n            layers.Sigmoid(1),\n            layers.Sigmoid(1),\n            layers.Sigmoid(1),\n            layers.Sigmoid(2),\n        ])\n        self.assertIn(\n            ""(?, 1) -> [... 6 layers ...] -> (?, 2)"",\n            str(optimizer))\n\n    def test_raise_exception_for_multioutputs(self):\n        network = layers.join(\n            layers.Input(5),\n            layers.parallel(\n                layers.Relu(1),\n                layers.Relu(2),\n            )\n        )\n        error_message = ""should have one output layer""\n        with self.assertRaisesRegexp(InvalidConnection, error_message):\n            algorithms.GradientDescent(network)\n\n    def test_network_initializations(self):\n        possible_networks = (\n            # as a list\n            [layers.Input(2), layers.Sigmoid(3), layers.Tanh(1)],\n\n            # as forward sequence with inline operators\n            layers.Input(2) > layers.Relu(10) > layers.Tanh(1),\n            layers.Input(2) >> layers.Relu(10) >> layers.Tanh(1),\n        )\n\n        for i, network in enumerate(possible_networks, start=1):\n            optimizer = algorithms.GradientDescent(network)\n            message = ""[Test #{}] Network: {}"".format(i, network)\n            self.assertEqual(len(optimizer.network.layers), 3, msg=message)\n\n    def test_gd_get_params_method(self):\n        optimizer = algorithms.GradientDescent([\n            layers.Input(2),\n            layers.Sigmoid(3),\n            layers.Sigmoid(1),\n        ])\n\n        self.assertIn(\n            \'network\',\n            optimizer.get_params(with_network=True),\n        )\n        self.assertNotIn(\n            \'network\',\n            optimizer.get_params(with_network=False),\n        )\n\n    def test_gd_storage(self):\n        optimizer = algorithms.GradientDescent(\n            [\n                layers.Input(2),\n                layers.Sigmoid(3),\n                layers.Sigmoid(1),\n            ],\n            step=0.2,\n            shuffle_data=True,\n        )\n        recovered_optimizer = pickle.loads(pickle.dumps(optimizer))\n\n        self.assertAlmostEqual(recovered_optimizer.step, 0.2)\n        self.assertEqual(recovered_optimizer.shuffle_data, True)\n\n    def test_optimizer_with_bad_shape_input_passed(self):\n        optimizer = algorithms.GradientDescent(\n            [\n                layers.Input((10, 10, 3)),\n                layers.Convolution((3, 3, 7)),\n                layers.Reshape(),\n                layers.Sigmoid(1),\n            ],\n            batch_size=None,\n            verbose=False,\n            loss=\'mse\',\n        )\n\n        image = np.random.random((10, 10, 3))\n        optimizer.train(image, [1], epochs=1)\n\n        retrieved_score = optimizer.score(image, [1])\n        self.assertLessEqual(0, retrieved_score)\n        self.assertGreaterEqual(1, retrieved_score)\n\n        prediction = optimizer.predict(image)\n        self.assertEqual(prediction.ndim, 2)\n\n    def test_optimizer_with_bad_shape_target_passed(self):\n        optimizer = algorithms.GradientDescent(\n            [\n                layers.Input((10, 10, 3)),\n                layers.Convolution((3, 3, 3), padding=\'same\'),\n            ],\n            batch_size=None,\n            verbose=False,\n            loss=\'mse\',\n        )\n\n        image = np.random.random((10, 10, 3))\n        optimizer.train(image, image, epochs=1)\n\n        retrieved_score = optimizer.score(image, image)\n        self.assertLessEqual(0, retrieved_score)\n        self.assertGreaterEqual(1, retrieved_score)\n\n    def test_invalid_number_of_inputs(self):\n        optimizer = algorithms.GradientDescent(\n            [\n                layers.parallel(\n                    layers.Input((10, 10, 3)),\n                    layers.Input((10, 10, 3)),\n                ),\n                layers.Concatenate(),\n                layers.Convolution((3, 3, 3), padding=\'same\'),\n            ],\n            batch_size=None,\n            verbose=False,\n            loss=\'mse\',\n        )\n\n        image = np.random.random((10, 10, 3))\n        optimizer.train([image, image], image, epochs=1)\n\n        message = (\n            ""Number of inputs doesn\'t match number ""\n            ""of input layers in the network.""\n        )\n        with self.assertRaisesRegexp(ValueError, message):\n            optimizer.train(image, image, epochs=1)\n\n    def test_gd_custom_target(self):\n        def custom_loss(actual, predicted):\n            actual_shape = tf.shape(actual)\n            n_samples = actual_shape[0]\n            actual = tf.reshape(actual, (n_samples, 1))\n            return objectives.rmse(actual, predicted)\n\n        optimizer = algorithms.GradientDescent(\n            layers.Input(10) >> layers.Sigmoid(1),\n\n            step=0.2,\n            shuffle_data=True,\n            batch_size=None,\n\n            loss=custom_loss,\n            target=tf.placeholder(tf.float32, shape=(None, 1, 1)),\n        )\n        x_train, _, y_train, _ = simple_classification()\n\n        error_message = ""Cannot feed value of shape \\(60, 1\\) for Tensor""\n        with self.assertRaisesRegexp(ValueError, error_message):\n            optimizer.train(x_train, y_train, epochs=1)\n\n        optimizer.train(x_train, y_train.reshape(-1, 1, 1), epochs=1)\n\n    def test_gd_predict_wrong_arguments(self):\n        optimizer = algorithms.GradientDescent(\n            layers.Input(10) >> layers.Sigmoid(1),\n            verbose=False,\n        )\n        input = np.random.random((7, 10))\n        with self.assertRaisesRegexp(TypeError, ""Unknown arguments""):\n            optimizer.predict(input, batchsize=10)\n'"
tests/algorithms/gd/test_hessdiag.py,0,"b'from functools import partial\n\nfrom neupy import algorithms, layers\nfrom neupy import init\n\nfrom helpers import compare_networks\nfrom helpers import simple_classification\nfrom base import BaseTestCase\n\n\nclass HessianDiagonalTestCase(BaseTestCase):\n    def test_hessdiag(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        params = dict(\n            weight=init.Uniform(-0.1, 0.1),\n            bias=init.Uniform(-0.1, 0.1))\n\n        nw = algorithms.HessianDiagonal(\n            network=[\n                layers.Input(10),\n                layers.Sigmoid(20, **params),\n                layers.Sigmoid(1, **params),\n            ],\n            step=0.1,\n            shuffle_data=False,\n            verbose=False,\n            min_eigval=0.1,\n        )\n        nw.train(x_train, y_train, epochs=50)\n        self.assertGreater(0.2, nw.errors.train[-1])\n\n    def test_compare_bp_and_hessian(self):\n        x_train, _, y_train, _ = simple_classification()\n        params = dict(\n            weight=init.Uniform(-0.1, 0.1),\n            bias=init.Uniform(-0.1, 0.1))\n\n        compare_networks(\n            # Test classes\n            partial(algorithms.GradientDescent, batch_size=None),\n            partial(algorithms.HessianDiagonal, min_eigval=0.1),\n            # Test data\n            (x_train, y_train),\n            # Network configurations\n            network=[\n                layers.Input(10),\n                layers.Sigmoid(20, **params),\n                layers.Sigmoid(1, **params),\n            ],\n            step=0.1,\n            shuffle_data=True,\n            verbose=False,\n            # Test configurations\n            epochs=50,\n            show_comparison_plot=False\n        )\n\n    def test_hessian_diagonal_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.HessianDiagonal,\n                verbose=False,\n                show_epoch=100,\n                step=0.25,\n                min_eigval=0.1,\n            ),\n            epochs=6000,\n            min_accepted_loss=0.002\n        )\n'"
tests/algorithms/gd/test_hessian.py,2,"b""from functools import partial\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neupy import algorithms, layers\nfrom neupy.utils import tensorflow_session\nfrom neupy.algorithms.gd.hessian import find_hessian_and_gradient\n\nfrom helpers import compare_networks\nfrom helpers import simple_classification\nfrom base import BaseTestCase\n\n\nclass HessianTestCase(BaseTestCase):\n    def test_compare_bp_and_hessian(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        compare_networks(\n            # Test classes\n            partial(algorithms.GradientDescent, batch_size=None),\n            partial(algorithms.Hessian, penalty_const=1),\n            # Test data\n            (x_train, y_train, x_test, y_test),\n            # Network configurations\n            network=[\n                layers.Input(10),\n                layers.Sigmoid(15),\n                layers.Sigmoid(1)\n            ],\n            shuffle_data=True,\n            verbose=False,\n            show_epoch=1,\n            # Test configurations\n            epochs=5,\n            show_comparison_plot=False\n        )\n\n    def test_hessian_computation(self):\n        x = tf.placeholder(name='x', dtype=tf.float32, shape=(1,))\n        y = tf.placeholder(name='y', dtype=tf.float32, shape=(1,))\n\n        f = x ** 2 + y ** 3 + 7 * x * y\n        # Gradient function:\n        # [2 * x + 7 * y,\n        #  3 * y ** 2 + 7 * x]\n        # Hessian function:\n        # [[2, 7    ]\n        #  [7, 6 * y]]\n        hessian, gradient = find_hessian_and_gradient(f, [x, y])\n\n        session = tensorflow_session()\n        hessian_output, gradient_output = session.run(\n            [hessian, gradient], feed_dict={x: [1], y: [2]})\n\n        np.testing.assert_array_equal(\n            gradient_output,\n            np.array([16, 19])\n        )\n        np.testing.assert_array_equal(\n            hessian_output,\n            np.array([\n                [2, 7],\n                [7, 12],\n            ])\n        )\n\n    def test_hessian_assign_step_exception(self):\n        with self.assertRaises(ValueError):\n            # Don't have step parameter\n            algorithms.Hessian(\n                layers.Input(2) > layers.Sigmoid(3) > layers.Sigmoid(1),\n                step=0.01,\n            )\n\n    def test_hessian_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(algorithms.Hessian, verbose=False, penalty_const=0.1),\n            epochs=350,\n        )\n"""
tests/algorithms/gd/test_levmarq.py,7,"b""from functools import partial\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets, preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom neupy import algorithms, layers\nfrom neupy.utils import asfloat\nfrom neupy.algorithms.gd.lev_marq import compute_jacobian\n\nfrom base import BaseTestCase\n\n\nclass LevenbergMarquardtTestCase(BaseTestCase):\n    def test_jacobian_for_levenberg_marquardt(self):\n        w1 = tf.Variable(asfloat(np.array([[1]])), name='w1')\n        b1 = tf.Variable(asfloat(np.array([0])), name='b1')\n        w2 = tf.Variable(asfloat(np.array([[2]])), name='w2')\n        b2 = tf.Variable(asfloat(np.array([1])), name='b2')\n\n        x = asfloat(np.array([[1, 2, 3]]).T)\n        y = asfloat(np.array([[1, 2, 3]]).T)\n\n        output_1 = tf.matmul(x, tf.transpose(w1)) + b1\n        output = b2 + tf.matmul(output_1 ** 2, w2)\n        error_func = tf.reduce_mean((y - output), axis=1)\n\n        output_expected = asfloat(np.array([[3, 9, 19]]).T)\n\n        np.testing.assert_array_almost_equal(\n            self.eval(output),\n            output_expected\n        )\n\n        jacobian_expected = asfloat(np.array([\n            [-4, -4, -1, -1],\n            [-16, -8, -4, -1],\n            [-36, -12, -9, -1],\n        ]))\n        jacobian_actual = compute_jacobian(error_func, [w1, b1, w2, b2])\n        np.testing.assert_array_almost_equal(\n            jacobian_expected,\n            self.eval(jacobian_actual)\n        )\n\n    def test_levenberg_marquardt_exceptions(self):\n        with self.assertRaises(ValueError):\n            algorithms.LevenbergMarquardt(\n                layers.Input(2) > layers.Sigmoid(3) > layers.Sigmoid(1),\n                loss='categorical_crossentropy')\n\n        with self.assertRaises(ValueError):\n            # Doesn't have step parameter\n            algorithms.LevenbergMarquardt(\n                layers.Input(2) > layers.Sigmoid(3) > layers.Sigmoid(1),\n                step=0.01)\n\n    def test_levenberg_marquardt(self):\n        dataset = datasets.make_regression(n_samples=50, n_features=2)\n        data, target = dataset\n\n        data_scaler = preprocessing.MinMaxScaler()\n        target_scaler = preprocessing.MinMaxScaler()\n\n        x_train, x_test, y_train, y_test = train_test_split(\n            data_scaler.fit_transform(data),\n            target_scaler.fit_transform(target.reshape(-1, 1)),\n            test_size=0.15\n        )\n\n        lmnet = algorithms.LevenbergMarquardt(\n            network=[\n                layers.Input(2),\n                layers.Sigmoid(6),\n                layers.Sigmoid(1),\n            ],\n            mu_update_factor=2,\n            mu=0.1,\n            verbose=False,\n            show_epoch=1,\n        )\n        lmnet.train(x_train, y_train, epochs=4)\n        error = lmnet.score(x_test, y_test)\n\n        self.assertGreater(0.01, error)\n\n    def test_levenberg_marquardt_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(algorithms.LevenbergMarquardt, verbose=False),\n            epochs=50,\n        )\n"""
tests/algorithms/gd/test_objectives.py,0,"b'import numpy as np\n\nfrom neupy.utils import asfloat\nfrom neupy.algorithms.gd import objectives\n\nfrom base import BaseTestCase\n\n\nclass ErrorFuncTestCase(BaseTestCase):\n    def test_mse(self):\n        actual = np.array([0, 1, 2, 3])\n        predicted = np.array([3, 2, 1, 0])\n        self.assertEqual(5, self.eval(objectives.mse(actual, predicted)))\n\n        actual = asfloat(np.array([\n            [0, 1],\n            [2, 3],\n            [4, 5],\n        ]))\n        predicted = asfloat(np.array([\n            [5, 4],\n            [3, 2],\n            [1, 0],\n        ]))\n        self.assertAlmostEqual(\n            asfloat(70 / 6.),\n            self.eval(objectives.mse(actual, predicted)),\n            places=3\n        )\n\n    def test_binary_crossentropy(self):\n        predicted = asfloat(np.array([0.1, 0.9, 0.2, 0.5]))\n        actual = asfloat(np.array([0, 1, 0, 1]))\n\n        error = objectives.binary_crossentropy(actual, predicted)\n        self.assertAlmostEqual(0.28, self.eval(error), places=2)\n\n    def test_categorical_crossentropy(self):\n        predicted = asfloat(np.array([\n            [0.1, 0.9],\n            [0.9, 0.1],\n            [0.2, 0.8],\n            [0.5, 0.5],\n        ]))\n        actual = asfloat(np.array([\n            [0, 1],\n            [1, 0],\n            [0, 1],\n            [1, 0],\n        ]))\n\n        error = objectives.categorical_crossentropy(actual, predicted)\n        self.assertAlmostEqual(0.28, self.eval(error), places=2)\n\n    def test_binary_crossentropy_spatial_data(self):\n        pred_values = np.array([[\n            [0.3, 0.2, 0.1],\n            [0.1, 0.1, 0.1],\n        ]])\n        pred_values = np.transpose(pred_values, (1, 2, 0))\n        pred_values = np.expand_dims(pred_values, axis=0)\n\n        true_values = np.array([[\n            [1, 0, 1],\n            [0, 0, 0],\n        ]])\n        true_values = np.transpose(true_values, (1, 2, 0))\n        true_values = np.expand_dims(true_values, axis=0)\n\n        # Making sure that input values are proper probabilities\n        self.assertTrue(np.all(pred_values.sum(axis=-1) < 1))\n\n        error = objectives.binary_crossentropy(true_values, pred_values)\n        expected_error = -(\n            np.log(0.3) + np.log(0.1) + 3 * np.log(0.9) + np.log(0.8)) / 6\n\n        self.assertAlmostEqual(expected_error, self.eval(error), places=2)\n\n    def test_categorical_crossentropy_spatial_data(self):\n        pred_values = np.array([[\n            [0.3, 0.2, 0.1],\n            [0.1, 0.1, 0.1],\n        ], [\n            [0.7, 0.8, 0.9],\n            [0.9, 0.9, 0.9],\n        ]])\n        pred_values = np.transpose(pred_values, (1, 2, 0))\n        pred_values = np.expand_dims(pred_values, axis=0)\n\n        true_values = np.array([[\n            [1, 0, 1],\n            [0, 0, 0],\n        ], [\n            [0, 1, 0],\n            [1, 1, 1],\n        ]])\n        true_values = np.transpose(true_values, (1, 2, 0))\n        true_values = np.expand_dims(true_values, axis=0)\n\n        # Making sure that input values are proper probabilities\n        self.assertTrue(np.allclose(pred_values.sum(axis=-1), 1))\n        self.assertTrue(np.allclose(true_values.sum(axis=-1), 1))\n\n        error = objectives.categorical_crossentropy(true_values, pred_values)\n        expected_error = -(\n            np.log(0.3) + np.log(0.1) + 3 * np.log(0.9) + np.log(0.8)) / 6\n\n        self.assertAlmostEqual(expected_error, self.eval(error), places=2)\n\n    def test_mae(self):\n        predicted = asfloat(np.array([1, 2, 3]))\n        target = asfloat(np.array([3, 2, 1]))\n\n        actual = objectives.mae(target, predicted)\n        self.assertAlmostEqual(self.eval(actual), 4 / 3., places=3)\n\n    def test_rmse(self):\n        actual = asfloat(np.array([0, 1, 2, 3]))\n        predicted = asfloat(np.array([3, 2, 1, 0]))\n        self.assertAlmostEqual(\n            asfloat(np.sqrt(5)),\n            self.eval(objectives.rmse(actual, predicted))\n        )\n\n    def test_msle(self):\n        actual = np.e ** (np.array([1, 2, 3, 4])) - 1\n        predicted = np.e ** (np.array([4, 3, 2, 1])) - 1\n        self.assertEqual(5, self.eval(objectives.msle(actual, predicted)))\n\n    def test_rmsle(self):\n        actual = np.e ** (np.array([1, 2, 3, 4])) - 1\n        predicted = np.e ** (np.array([4, 3, 2, 1])) - 1\n        self.assertAlmostEqual(\n            asfloat(np.sqrt(5)),\n            self.eval(objectives.rmsle(actual, predicted))\n        )\n\n    def test_binary_hinge(self):\n        targets = np.array([\n            [-1, 1, 1],\n            [-1, -1, 1],\n        ])\n        predictions = np.array([\n            [-0.1, 0.9, 0.5],\n            [0.5, -0.5, 1],\n        ])\n        expected = np.array([\n            [0.9, 0.1, 0.5],\n            [1.5, 0.5, 0],\n        ]).mean()\n\n        actual = objectives.binary_hinge(targets, predictions)\n        self.assertAlmostEqual(expected, self.eval(actual), places=3)\n\n    def test_categorical_hinge(self):\n        targets = asfloat(np.array([\n            [0, 0, 1],\n            [1, 0, 0],\n        ]))\n        predictions = asfloat(np.array([\n            [0.1, 0.2, 0.7],\n            [0.0, 0.9, 0.1],\n        ]))\n        expected = np.array([0.5, 1.9]).mean()\n\n        actual = objectives.categorical_hinge(targets, predictions)\n        self.assertAlmostEqual(expected, self.eval(actual), places=3)\n'"
tests/algorithms/gd/test_optimizers.py,0,"b'from functools import partial\n\nfrom neupy import algorithms, layers\n\nfrom helpers import simple_classification, compare_networks\nfrom base import BaseTestCase\n\n\nclass OptimizersTestCase(BaseTestCase):\n    def setUp(self):\n        super(OptimizersTestCase, self).setUp()\n        self.network = layers.join(\n            layers.Input(10),\n            layers.Sigmoid(20),\n            layers.Sigmoid(1),\n        )\n\n    def test_adadelta(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        optimizer = algorithms.Adadelta(\n            self.network,\n            batch_size=None,\n            verbose=False,\n            rho=0.95,\n            epsilon=1e-5,\n        )\n        optimizer.train(x_train, y_train, x_test, y_test, epochs=100)\n        self.assertGreater(0.05, optimizer.errors.train[-1])\n        self.assertGreater(0.15, optimizer.errors.valid[-1])\n\n    def test_adagrad(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        optimizer = algorithms.Adagrad(\n            self.network,\n            step=0.1,\n            batch_size=None,\n            verbose=False,\n        )\n        optimizer.train(x_train, y_train, x_test, y_test, epochs=150)\n        self.assertGreater(0.15, optimizer.errors.valid[-1])\n\n    def test_adam(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        optimizer = algorithms.Adam(\n            self.network,\n            step=0.1,\n            verbose=False,\n            epsilon=1e-4,\n            beta1=0.9,\n            beta2=0.99,\n        )\n        optimizer.train(x_train, y_train, x_test, y_test, epochs=200)\n        self.assertGreater(0.2, optimizer.errors.valid[-1])\n\n    def test_rmsprop(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        optimizer = algorithms.RMSProp(\n            self.network,\n            step=0.02,\n            batch_size=None,\n            verbose=False,\n            epsilon=1e-5,\n            decay=0.9,\n        )\n        optimizer.train(x_train, y_train, x_test, y_test, epochs=150)\n        self.assertGreater(0.15, optimizer.errors.valid[-1])\n\n    def test_momentum(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        optimizer = algorithms.Momentum(\n            self.network,\n            step=0.35,\n            momentum=0.99,\n            batch_size=None,\n            verbose=False,\n            nesterov=True,\n        )\n\n        optimizer.train(x_train, y_train, x_test, y_test, epochs=30)\n        self.assertGreater(0.15, optimizer.errors.valid[-1])\n\n    def test_adamax(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n        mnet = algorithms.Adamax(\n            self.network,\n            step=0.1,\n            batch_size=None,\n            verbose=False,\n            epsilon=1e-7,\n            beta1=0.9,\n            beta2=0.999,\n        )\n        mnet.train(x_train, y_train, x_test, y_test, epochs=50)\n        self.assertGreater(0.15, mnet.errors.train[-1])\n\n    def test_adamax_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(algorithms.Adamax, step=0.2, verbose=False),\n            epochs=400)\n\n\nclass MomentumTestCase(BaseTestCase):\n    def test_momentum_with_minibatch(self):\n        x_train, _, y_train, _ = simple_classification()\n        compare_networks(\n           # Test classes\n           partial(algorithms.Momentum, batch_size=None),\n           partial(algorithms.Momentum, batch_size=1),\n           # Test data\n           (x_train, y_train),\n           # Network configurations\n           network=[\n               layers.Input(10),\n               layers.Sigmoid(20),\n               layers.Sigmoid(1)\n           ],\n           step=0.25,\n           momentum=0.1,\n           shuffle_data=True,\n           verbose=False,\n           # Test configurations\n           epochs=40,\n           show_comparison_plot=False,\n        )\n\n    def test_nesterov_momentum(self):\n        x_train, _, y_train, _ = simple_classification()\n        compare_networks(\n           # Test classes\n           partial(algorithms.Momentum, nesterov=False),\n           partial(algorithms.Momentum, nesterov=True),\n           # Test data\n           (x_train, y_train),\n           # Network configurations\n           network=[\n               layers.Input(10),\n               layers.Sigmoid(20),\n               layers.Sigmoid(1)\n           ],\n           batch_size=None,\n           step=0.25,\n           momentum=0.9,\n           shuffle_data=True,\n           verbose=False,\n           # Test configurations\n           epochs=10,\n           show_comparison_plot=False,\n        )\n'"
tests/algorithms/gd/test_quasi_newton.py,0,"b'from functools import partial\nfrom collections import namedtuple\n\nimport numpy as np\nfrom sklearn import metrics\n\nfrom neupy.utils import asfloat\nfrom neupy import algorithms, layers, init\nfrom neupy.algorithms.gd import quasi_newton as qn\n\nfrom helpers import simple_classification\nfrom base import BaseTestCase\n\n\nclass QuasiNewtonTestCase(BaseTestCase):\n    def test_exceptions(self):\n        with self.assertRaises(ValueError):\n            # Don\'t have learning rate\n            algorithms.QuasiNewton(\n                layers.Input(2) > layers.Sigmoid(3) > layers.Sigmoid(1),\n                step=0.3,\n            )\n\n    def test_update_functions(self):\n        UpdateFunction = namedtuple(\n            ""UpdateFunction"",\n            ""func input_values output_value is_symmetric"")\n\n        testcases = (\n            UpdateFunction(\n                func=qn.bfgs,\n                input_values=[\n                    asfloat(np.eye(3)),\n                    asfloat(np.array([0.1, 0.2, 0.3])),\n                    asfloat(np.array([0.3, -0.3, -0.5]))\n                ],\n                output_value=np.array([\n                    [1.41049383, 0.32098765, 0.4537037],\n                    [0.32098765, 0.64197531, -0.59259259],\n                    [0.4537037, -0.59259259, 0.02777778],\n                ]),\n                is_symmetric=True\n            ),\n            UpdateFunction(\n                func=qn.sr1,\n                input_values=[\n                    asfloat(np.eye(3)),\n                    asfloat(np.array([0.1, 0.2, 0.3])),\n                    asfloat(np.array([1, -2, -3]))\n                ],\n                output_value=[\n                    [0.94671053, 0.13026316, 0.19539474],\n                    [0.13026316, 0.68157895, -0.47763158],\n                    [0.19539474, -0.47763158, 0.28355263],\n                ],\n                is_symmetric=False\n            ),\n            UpdateFunction(\n                func=qn.dfp,\n                input_values=[\n                    asfloat(np.eye(3)),\n                    asfloat(np.array([0.1, 0.2, 0.3])),\n                    asfloat(np.array([0.3, -0.3, -0.5]))\n                ],\n                output_value=np.array([\n                    [0.73514212, 0.09819121, 0.18217053],\n                    [0.09819121, 0.56847545, -0.6821705],\n                    [0.18217053, -0.6821705, -0.08139535],\n                ]),\n                is_symmetric=True\n            ),\n        )\n\n        for case in testcases:\n            if not case.input_values:\n                continue\n\n            result = self.eval(case.func(*case.input_values))\n\n            if case.is_symmetric:\n                np.testing.assert_array_almost_equal(result, result.T)\n\n            np.testing.assert_array_almost_equal(result, case.output_value)\n\n    def test_quasi_newton_bfgs(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        qnnet = algorithms.QuasiNewton(\n            network=[\n                layers.Input(10),\n                layers.Sigmoid(30, weight=init.Orthogonal()),\n                layers.Sigmoid(1, weight=init.Orthogonal()),\n            ],\n            shuffle_data=True,\n            show_epoch=10,\n            update_function=\'bfgs\',\n        )\n\n        qnnet.train(x_train, y_train, x_test, y_test, epochs=50)\n        result = qnnet.predict(x_test).round().astype(int)\n\n        roc_curve_score = metrics.roc_auc_score(result, y_test)\n        self.assertAlmostEqual(0.92, roc_curve_score, places=2)\n\n    def test_quasi_newton_dfp(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        qnnet = algorithms.QuasiNewton(\n            network=[\n                layers.Input(10),\n                layers.Sigmoid(30, weight=init.Orthogonal()),\n                layers.Sigmoid(1, weight=init.Orthogonal()),\n            ],\n            shuffle_data=True,\n            verbose=False,\n\n            update_function=\'dfp\',\n            h0_scale=2,\n        )\n        qnnet.train(x_train, y_train, x_test, y_test, epochs=10)\n        result = qnnet.predict(x_test).round()\n\n        roc_curve_score = metrics.roc_auc_score(result, y_test)\n        self.assertAlmostEqual(0.92, roc_curve_score, places=2)\n\n    def test_quasi_newton_sr1(self):\n        x_train, x_test, y_train, y_test = simple_classification()\n\n        qnnet = algorithms.QuasiNewton(\n            network=[\n                layers.Input(10),\n                layers.Sigmoid(30, weight=init.Orthogonal()),\n                layers.Sigmoid(1, weight=init.Orthogonal()),\n            ],\n            shuffle_data=True,\n            show_epoch=20,\n            verbose=False,\n\n            update_function=\'sr1\',\n            h0_scale=2,\n        )\n        qnnet.train(x_train, y_train, x_test, y_test, epochs=10)\n        result = qnnet.predict(x_test).round()\n\n        roc_curve_score = metrics.roc_auc_score(result, y_test)\n        self.assertAlmostEqual(0.92, roc_curve_score, places=2)\n\n    def test_quasi_newton_bfgs_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.QuasiNewton,\n                h0_scale=2,\n                update_function=\'bfgs\',\n                verbose=False,\n            ),\n            epochs=100,\n            min_accepted_loss=0.002,\n        )\n\n    def test_quasi_newton_dfp_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.QuasiNewton,\n                update_function=\'dfp\',\n                verbose=False,\n            ),\n            epochs=350,\n            min_accepted_loss=0.002,\n        )\n\n    def test_quasi_newton_sr1_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.QuasiNewton,\n                update_function=\'sr1\',\n                verbose=False,\n                epsilon=1e-5,\n            ),\n            epochs=250,\n            min_accepted_loss=0.002,\n        )\n\n    def test_safe_division(self):\n        value = self.eval(qn.safe_division(2.0, 4.0, epsilon=1e-7))\n        self.assertAlmostEqual(0.5, value)\n\n        value = self.eval(qn.safe_division(1.0, 1e-8, epsilon=1e-7))\n        self.assertAlmostEqual(1e7, value)\n\n    def test_safe_reciprocal(self):\n        value = self.eval(qn.safe_reciprocal(4.0, epsilon=1e-7))\n        self.assertAlmostEqual(0.25, value)\n\n        value = self.eval(qn.safe_reciprocal(1e-8, epsilon=1e-7))\n        self.assertAlmostEqual(1e7, value)\n\n        value = self.eval(qn.safe_reciprocal(1e-8, epsilon=0.01))\n        self.assertAlmostEqual(100, value)\n'"
tests/algorithms/gd/test_rprop.py,0,"b'import copy\nfrom functools import partial\n\nimport numpy as np\n\nfrom neupy import algorithms, init, layers\nfrom neupy.layers import Input, Sigmoid\nfrom neupy.utils import asfloat\n\nfrom helpers import compare_networks\nfrom base import BaseTestCase\n\n\nsimple_x_train = asfloat(np.array([\n    [0.1, 0.1, 0.2],\n    [0.2, 0.3, 0.4],\n    [0.1, 0.7, 0.2],\n]))\nsimple_y_train = asfloat(np.array([\n    [0.2, 0.2],\n    [0.3, 0.3],\n    [0.5, 0.5],\n]))\n\n\nclass RPROPTestCase(BaseTestCase):\n    def setUp(self):\n        super(RPROPTestCase, self).setUp()\n        self.network = Input(3) > Sigmoid(10) > Sigmoid(2)\n\n    def test_rprop(self):\n        nw = algorithms.RPROP(\n            self.network,\n            minstep=0.001,\n            maxstep=1,\n            increase_factor=1.1,\n            decrease_factor=0.1,\n            step=1,\n            verbose=False\n        )\n\n        nw.train(simple_x_train, simple_y_train, epochs=100)\n        self.assertGreater(1e-4, nw.errors.train[-1])\n\n    def test_compare_bp_and_rprop(self):\n        compare_networks(\n            # Test classes\n            partial(algorithms.GradientDescent, batch_size=None),\n            partial(algorithms.RPROP, maxstep=0.1),\n            # Test data\n            (simple_x_train, simple_y_train),\n            # Network configurations\n            network=self.network,\n            step=0.1,\n            shuffle_data=True,\n            verbose=False,\n            # Test configurations\n            epochs=50,\n            show_comparison_plot=False\n        )\n\n    def test_irpropplus(self):\n        options = dict(\n            minstep=0.001,\n            maxstep=1,\n            increase_factor=1.1,\n            decrease_factor=0.1,\n            step=1,\n            verbose=False\n        )\n\n        uniform = init.Uniform()\n        params1 = dict(\n            weight=uniform.sample((3, 10), return_array=True),\n            bias=uniform.sample((10,), return_array=True),\n        )\n        params2 = dict(\n            weight=uniform.sample((10, 2), return_array=True),\n            bias=uniform.sample((2,), return_array=True),\n        )\n\n        network = layers.join(\n            Input(3),\n            Sigmoid(10, **params1),\n            Sigmoid(2, **params2),\n        )\n\n        nw = algorithms.IRPROPPlus(copy.deepcopy(network), **options)\n        nw.train(simple_x_train, simple_y_train, epochs=100)\n        irprop_plus_error = nw.errors.train[-1]\n        self.assertGreater(1e-4, nw.errors.train[-1])\n\n        nw = algorithms.RPROP(copy.deepcopy(network), **options)\n        nw.train(simple_x_train, simple_y_train, epochs=100)\n        rprop_error = nw.errors.train[-1]\n        self.assertGreater(rprop_error, irprop_plus_error)\n\n    def test_rprop_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.RPROP,\n                minstep=1e-5,\n                step=0.05,\n                maxstep=1.0,\n\n                increase_factor=1.5,\n                decrease_factor=0.5,\n\n                verbose=False,\n                show_epoch=100,\n            ),\n            epochs=5000,\n            min_accepted_loss=0.006,\n        )\n\n    def test_irproplus_overfit(self):\n        self.assertCanNetworkOverfit(\n            partial(\n                algorithms.IRPROPPlus,\n                minstep=1e-5,\n                step=0.05,\n                maxstep=1.0,\n\n                increase_factor=1.5,\n                decrease_factor=0.5,\n\n                verbose=False,\n                show_epoch=100,\n            ),\n            epochs=5000,\n            min_accepted_loss=0.005,\n        )\n'"
tests/algorithms/memory/__init__.py,0,b''
tests/algorithms/memory/data.py,0,"b'import numpy as np\n\n\nzero = np.matrix([\n    0, 1, 1, 1, 0,\n    1, 0, 0, 0, 1,\n    1, 0, 0, 0, 1,\n    1, 0, 0, 0, 1,\n    1, 0, 0, 0, 1,\n    0, 1, 1, 1, 0,\n])\n\nhalf_zero = np.matrix([\n    0, 1, 1, 1, 0,\n    1, 0, 0, 0, 1,\n    1, 0, 0, 0, 1,\n    0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0,\n])\n\none = np.matrix([\n    0, 1, 1, 0, 0,\n    0, 0, 1, 0, 0,\n    0, 0, 1, 0, 0,\n    0, 0, 1, 0, 0,\n    0, 0, 1, 0, 0,\n    0, 0, 1, 0, 0,\n])\n\nhalf_one = np.matrix([\n    0, 1, 1, 0, 0,\n    0, 0, 1, 0, 0,\n    0, 0, 1, 0, 0,\n    0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0,\n])\n\ntwo = np.matrix([\n    1, 1, 1, 0, 0,\n    0, 0, 0, 1, 0,\n    0, 0, 0, 1, 0,\n    0, 1, 1, 0, 0,\n    1, 0, 0, 0, 0,\n    1, 1, 1, 1, 0,\n])\n\nhalf_two = np.matrix([\n    1, 1, 0, 0, 0,\n    0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0,\n    0, 1, 0, 0, 0,\n    1, 0, 0, 0, 0,\n    1, 1, 0, 0, 0,\n])\n'"
tests/algorithms/memory/test_bam.py,0,"b""import pickle\n\nimport numpy as np\n\nfrom neupy import algorithms\nfrom neupy.exceptions import NotTrained\n\nfrom algorithms.memory.data import zero, one, half_one, half_zero\nfrom base import BaseTestCase\nfrom helpers import vectors_for_testing\n\n\nzero_hint = np.array([[0, 1, 0, 0]])\none_hint = np.array([[1, 0, 0, 0]])\n\n\nclass BAMTestCase(BaseTestCase):\n    def setUp(self):\n        super(BAMTestCase, self).setUp()\n        self.data = np.concatenate([zero, one], axis=0)\n        self.hints = np.concatenate([zero_hint, one_hint], axis=0)\n\n    def test_bam_exceptions(self):\n        with self.assertRaises(NotTrained):\n            dbnet = algorithms.DiscreteBAM()\n            dbnet.predict(np.array([0, 1]))\n\n        with self.assertRaises(NotTrained):\n            dbnet = algorithms.DiscreteBAM()\n            dbnet.predict_input(np.array([0, 1]))\n\n        with self.assertRaises(ValueError):\n            dbnet = algorithms.DiscreteBAM()\n            dbnet.weight = np.array([[0, 1], [1, 0]])\n            dbnet.train(np.array([0, 1, 1]), np.array([0, 1]))\n\n    def test_bam_X_validation(self):\n        dbnet = algorithms.DiscreteBAM()\n        dbnet.weight = np.array([[0, 1], [1, 0]])\n\n        with self.assertRaises(ValueError):\n            # Invalid discrete input values\n            dbnet.train(np.array([-1, 1]), np.array([0, 1]))\n\n        with self.assertRaises(ValueError):\n            dbnet.train(np.array([0, 1]), np.array([-1, 1]))\n\n        with self.assertRaises(ValueError):\n            dbnet.energy(np.array([-1, 1]), np.array([0, 1]))\n\n        with self.assertRaises(ValueError):\n            dbnet.energy(np.array([0, 1]), np.array([-1, 1]))\n\n        with self.assertRaises(ValueError):\n            dbnet.predict(np.array([-1, 1]))\n\n    def test_discrete_bam_storage(self):\n        network = algorithms.DiscreteBAM(mode='sync')\n        network.train(self.data, self.hints)\n\n        stored_network = pickle.dumps(network)\n        loaded_network = pickle.loads(stored_network)\n\n        network_prediction = network.predict(self.data)\n        loaded_network_prediction = loaded_network.predict(self.data)\n\n        np.testing.assert_array_almost_equal(\n            loaded_network_prediction[0], network_prediction[0])\n\n        np.testing.assert_array_almost_equal(\n            loaded_network_prediction[1], network_prediction[1])\n\n    def test_discrete_bam_sync(self):\n        bamnet = algorithms.DiscreteBAM(mode='sync')\n        bamnet.train(self.data, self.hints)\n\n        data_before = self.data.copy()\n        hints_before = self.hints.copy()\n\n        np.testing.assert_array_almost_equal(\n            bamnet.predict(half_zero)[1],\n            zero_hint\n        )\n        np.testing.assert_array_almost_equal(\n            bamnet.predict_output(half_one)[1],\n            one_hint\n        )\n        np.testing.assert_array_almost_equal(\n            bamnet.predict_input(zero_hint)[0],\n            zero\n        )\n        np.testing.assert_array_almost_equal(\n            bamnet.predict_input(one_hint)[0],\n            one\n        )\n\n        # Test 1d input array prediction\n        np.testing.assert_array_almost_equal(\n            bamnet.predict_input(one_hint.ravel())[0],\n            one\n        )\n\n        # Test 1d output array input prediction\n        np.testing.assert_array_almost_equal(\n            bamnet.predict_output(half_one.ravel())[1],\n            one_hint\n        )\n\n        # Test multiple input values prediction\n        input_matrix = np.vstack([one, zero])\n        output_matrix = np.vstack([one_hint, zero_hint])\n        output_matrix_before = output_matrix.copy()\n        input_matrix_before = input_matrix.copy()\n\n        np.testing.assert_array_almost_equal(\n            bamnet.predict_input(output_matrix)[0],\n            input_matrix\n        )\n        np.testing.assert_array_almost_equal(\n            bamnet.predict(input_matrix)[1],\n            output_matrix\n        )\n\n        np.testing.assert_array_equal(self.data, data_before)\n        np.testing.assert_array_equal(self.hints, hints_before)\n        np.testing.assert_array_equal(output_matrix, output_matrix_before)\n        np.testing.assert_array_equal(input_matrix, input_matrix_before)\n\n    def test_discrete_bam_async(self):\n        bamnet = algorithms.DiscreteBAM(mode='async', n_times=400)\n        data_before = self.data.copy()\n        hints_before = self.hints.copy()\n        bamnet.train(self.data, self.hints)\n\n        input_matrix = np.vstack([one, zero])\n        output_matrix = np.vstack([one_hint, zero_hint])\n        output_matrix_before = output_matrix.copy()\n        input_matrix_before = input_matrix.copy()\n\n        np.testing.assert_array_almost_equal(\n            bamnet.predict_input(output_matrix)[0],\n            input_matrix\n        )\n        np.testing.assert_array_almost_equal(\n            bamnet.predict_output(input_matrix)[1],\n            output_matrix\n        )\n\n        np.testing.assert_array_equal(self.data, data_before)\n        np.testing.assert_array_equal(self.hints, hints_before)\n        np.testing.assert_array_equal(output_matrix, output_matrix_before)\n        np.testing.assert_array_equal(input_matrix, input_matrix_before)\n\n    def test_bam_argument_in_predict_method(self):\n        dbnet = algorithms.DiscreteBAM(mode='async', n_times=1)\n        dbnet.train(self.data, self.hints)\n\n        self.assertTrue(np.any(one != dbnet.predict_output(half_one)[0]))\n        np.testing.assert_array_almost_equal(\n            one, dbnet.predict_output(half_one, n_times=100)[0])\n\n    def test_bam_energy_function(self):\n        input_vector = np.array([[1, 0, 0, 1, 1, 0, 0]])\n        output_vector = np.array([[1, 0]])\n        dbnet = algorithms.DiscreteBAM()\n        dbnet.train(input_vector, output_vector)\n\n        self.assertEqual(-7, dbnet.energy(input_vector, output_vector))\n        self.assertEqual(0, dbnet.energy(\n            np.array([[0, 0, 0, 0, 0, 0, 0]]),\n            np.array([[0, 0]])\n        ))\n        self.assertEqual(-7, dbnet.energy(\n            np.array([[0, 1, 1, 0, 0, 1, 1]]),\n            np.array([[0, 1]])\n        ))\n\n        # Test 1d array\n        self.assertEqual(-7, dbnet.energy(\n            np.array([0, 1, 1, 0, 0, 1, 1]),\n            np.array([0, 1])\n        ))\n\n        # Test multiple input values energy calculation\n        np.testing.assert_array_almost_equal(\n            np.array([-7, 0]),\n            dbnet.energy(\n                np.array([\n                    [0, 1, 1, 0, 0, 1, 1],\n                    [0, 0, 0, 0, 0, 0, 0],\n                ]),\n                np.array([\n                    [0, 1],\n                    [0, 0],\n                ])\n            )\n        )\n\n    def test_bam_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.DiscreteBAM(),\n            np.array([1, 0, 0, 1]),\n            np.array([1, 0]),\n            is_feature1d=False)\n\n    def test_bam_predict_different_inputs(self):\n        bamnet = algorithms.DiscreteBAM()\n\n        data = np.array([[1, 0, 0, 1]])\n        target = np.array([[1, 0]])\n\n        bamnet.train(data, target)\n        test_vectors = vectors_for_testing(\n            data.reshape(data.size), is_feature1d=False)\n\n        for test_vector in test_vectors:\n            np.testing.assert_array_almost_equal(\n                bamnet.predict(test_vector)[1], target)\n"""
tests/algorithms/memory/test_cmac.py,0,"b""import numpy as np\nfrom sklearn import metrics\n\nfrom neupy import algorithms\nfrom base import BaseTestCase\n\n\nclass CMACTestCase(BaseTestCase):\n    def test_cmac(self):\n        X_train = np.reshape(np.linspace(0, 2 * np.pi, 100), (100, 1))\n        X_train_before = X_train.copy()\n\n        X_test = np.reshape(np.linspace(np.pi, 2 * np.pi, 50), (50, 1))\n\n        y_train = np.sin(X_train)\n        y_train_before = y_train.copy()\n        y_test = np.sin(X_test)\n\n        cmac = algorithms.CMAC(\n            quantization=100,\n            associative_unit_size=32,\n            step=0.2,\n            verbose=False,\n        )\n        cmac.train(X_train, y_train, epochs=100)\n\n        predicted_test = cmac.predict(X_test)\n        predicted_test = predicted_test.reshape((len(predicted_test), 1))\n        error = metrics.mean_absolute_error(y_test, predicted_test)\n\n        self.assertAlmostEqual(error, 0.0024, places=4)\n\n        # Test that algorithm didn't modify data samples\n        np.testing.assert_array_equal(X_train, X_train_before)\n        np.testing.assert_array_equal(X_train, X_train_before)\n        np.testing.assert_array_equal(y_train, y_train_before)\n\n        self.assertPickledNetwork(cmac, X_train)\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            network=algorithms.CMAC(),\n            input_vector=np.array([1, 2, 3]),\n            target=np.array([1, 2, 3])\n        )\n\n    def test_predict_different_inputs(self):\n        cmac = algorithms.CMAC()\n\n        data = np.array([[1, 2, 3]]).T\n        target = np.array([[1, 2, 3]]).T\n\n        cmac.train(data, target, epochs=100)\n        self.assertInvalidVectorPred(\n            network=cmac,\n            input_vector=np.array([1, 2, 3]),\n            target=target,\n            decimal=2\n        )\n\n    def test_cmac_multi_output(self):\n        X_train = np.linspace(0, 2 * np.pi, 100)\n        X_train = np.vstack([X_train, X_train])\n\n        X_test = np.linspace(0, 2 * np.pi, 100)\n        X_test = np.vstack([X_test, X_test])\n\n        y_train = np.sin(X_train)\n        y_test = np.sin(X_test)\n\n        cmac = algorithms.CMAC(\n            quantization=100,\n            associative_unit_size=32,\n            step=0.2,\n        )\n        cmac.train(X_train, y_train,\n                   X_test, y_test, epochs=100)\n        predicted_test = cmac.predict(X_test)\n        error = metrics.mean_absolute_error(y_test, predicted_test)\n\n        self.assertAlmostEqual(error, 0, places=6)\n\n    def test_cmac_training_exceptions(self):\n        cmac = algorithms.CMAC(\n            quantization=100,\n            associative_unit_size=32,\n            step=0.2,\n        )\n\n        with self.assertRaises(ValueError):\n            cmac.train(X_train=True, y_train=True,\n                       X_test=None, y_test=True)\n"""
tests/algorithms/memory/test_discrete_hn.py,0,"b'import numpy as np\n\nfrom neupy import algorithms\n\nfrom algorithms.memory.data import (\n    zero, one, two, half_one,\n    half_zero, half_two,\n)\nfrom base import BaseTestCase\n\n\nclass DiscreteHopfieldNetworkTestCase(BaseTestCase):\n    def test_check_limit_option_for_iterative_updates(self):\n        data = np.matrix([\n            [1, 1, 0, 1],\n            [1, 0, 1, 1],\n            [0, 0, 1, 1],\n        ])\n\n        dhnet = algorithms.DiscreteHopfieldNetwork(check_limit=True)\n        dhnet.train(data[0])\n        dhnet.train(data[1])\n\n        with self.assertRaises(ValueError):\n            dhnet.train(data[2])\n\n        # The same must be OK without validation\n        dhnet = algorithms.DiscreteHopfieldNetwork(check_limit=False)\n        dhnet.train(data[0])\n        dhnet.train(data[1])\n        dhnet.train(data[2])\n\n    def test_check_limit_option(self):\n        data = np.matrix([\n            [1, 1, 0, 1],\n            [1, 0, 1, 1],\n            [0, 0, 1, 1],\n        ])\n\n        with self.assertRaises(ValueError):\n            # To many data samples compare to the number of feature\n            dhnet = algorithms.DiscreteHopfieldNetwork(check_limit=True)\n            dhnet.train(data)\n\n        # The same must be OK without validation\n        dhnet = algorithms.DiscreteHopfieldNetwork(check_limit=False)\n        dhnet.train(data)\n\n    def test_X_validation(self):\n        dhnet = algorithms.DiscreteHopfieldNetwork()\n        dhnet.weight = np.array([[0, 1], [1, 0]])\n\n        # Invalid discrete input values\n        with self.assertRaises(ValueError):\n            dhnet.train(np.array([-1, 1]))\n\n        with self.assertRaises(ValueError):\n            dhnet.energy(np.array([-1, 1]))\n\n        with self.assertRaises(ValueError):\n            dhnet.predict(np.array([-1, 1]))\n\n    def test_discrete_hopfield_sync(self):\n        data = np.concatenate([zero, one, two], axis=0)\n        data_before = data.copy()\n        dhnet = algorithms.DiscreteHopfieldNetwork(mode=\'sync\')\n        dhnet.train(data)\n\n        half_zero_before = half_zero.copy()\n        np.testing.assert_array_almost_equal(zero, dhnet.predict(half_zero))\n        np.testing.assert_array_almost_equal(two, dhnet.predict(half_two))\n\n        # Test predicition for the 1d array\n        np.testing.assert_array_almost_equal(\n            one,\n            dhnet.predict(half_one.ravel())\n        )\n\n        multiple_inputs = np.vstack([zero, one, two])\n        np.testing.assert_array_almost_equal(\n            multiple_inputs, dhnet.predict(multiple_inputs)\n        )\n\n        np.testing.assert_array_equal(data_before, data)\n        np.testing.assert_array_equal(half_zero, half_zero_before)\n\n        self.assertPickledNetwork(dhnet, data)\n\n    def test_discrete_hopfield_async(self):\n        data = np.concatenate([zero, one, two], axis=0)\n        data_before = data.copy()\n        dhnet = algorithms.DiscreteHopfieldNetwork(mode=\'async\', n_times=1000)\n        dhnet.train(data)\n\n        half_zero_before = half_zero.copy()\n        np.testing.assert_array_almost_equal(zero, dhnet.predict(half_zero))\n        np.testing.assert_array_almost_equal(one, dhnet.predict(half_one))\n        np.testing.assert_array_almost_equal(two, dhnet.predict(half_two))\n\n        multiple_outputs = np.vstack([zero, one, two])\n        multiple_inputs = np.vstack([half_zero, half_one, half_two])\n        np.testing.assert_array_almost_equal(\n            multiple_outputs,\n            dhnet.predict(multiple_inputs),\n        )\n\n        np.testing.assert_array_equal(data_before, data)\n        np.testing.assert_array_equal(half_zero, half_zero_before)\n\n    def test_energy_function(self):\n        input_vector = np.array([[1, 0, 0, 1, 1, 0, 0]])\n        dhnet = algorithms.DiscreteHopfieldNetwork()\n        dhnet.train(input_vector)\n\n        self.assertEqual(-21, dhnet.energy(input_vector))\n        self.assertEqual(3, dhnet.energy(np.array([[0, 0, 0, 0, 0, 0, 0]])))\n        self.assertEqual(-21, dhnet.energy(np.array([0, 1, 1, 0, 0, 1, 1])))\n\n        # Test energy calculatin for the 1d array\n        self.assertEqual(3, dhnet.energy(np.array([0, 0, 0, 0, 0, 0, 0])))\n\n        np.testing.assert_array_almost_equal(\n            np.array([-21, 3]),\n            dhnet.energy(\n                np.array([\n                    [0, 1, 1, 0, 0, 1, 1],\n                    [0, 0, 0, 0, 0, 0, 0],\n                ])\n            )\n        )\n\n    def test_argument_in_predict_method(self):\n        data = np.concatenate([zero, one, two], axis=0)\n        dhnet = algorithms.DiscreteHopfieldNetwork(mode=\'async\', n_times=1)\n        dhnet.train(data)\n\n        self.assertTrue(np.any(zero != dhnet.predict(half_zero)))\n        np.testing.assert_array_almost_equal(\n            zero,\n            dhnet.predict(half_zero, n_times=100)\n        )\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.DiscreteHopfieldNetwork(check_limit=False),\n            np.array([1, 0, 0, 1]),\n            is_feature1d=False\n        )\n\n    def test_predict_different_inputs(self):\n        data = np.array([[1, 0, 0, 1]])\n\n        dhnet = algorithms.DiscreteHopfieldNetwork()\n        dhnet.train(data)\n\n        self.assertInvalidVectorPred(\n            dhnet, np.array([1, 0, 0, 1]), data,\n            is_feature1d=False,\n        )\n\n    def test_iterative_updates(self):\n        data = np.concatenate([zero, one, two], axis=0)\n        dhnet_full = algorithms.DiscreteHopfieldNetwork(mode=\'sync\')\n        dhnet_full.train(data)\n\n        dhnet_iterative = algorithms.DiscreteHopfieldNetwork(mode=\'sync\')\n        for digit in [zero, one, two]:\n            dhnet_iterative.train(digit)\n\n        np.testing.assert_array_almost_equal(\n            dhnet_iterative.weight, dhnet_full.weight)\n\n    def test_iterative_updates_wrong_feature_shapes_exception(self):\n        dhnet = algorithms.DiscreteHopfieldNetwork()\n        dhnet.train(np.ones((1, 10)))\n\n        with self.assertRaisesRegexp(ValueError, ""invalid number of features""):\n            dhnet.train(np.ones((1, 7)))\n'"
tests/algorithms/minsearch/__init__.py,0,b''
tests/algorithms/minsearch/test_golden_search.py,1,"b'import tensorflow as tf\n\nfrom neupy.algorithms.minsearch.golden_search import fmin_golden_search\n\nfrom base import BaseTestCase\n\n\nclass GoldenSearchTestCase(BaseTestCase):\n    def test_golden_search_exceptions(self):\n        invalid_parameters = (\n            dict(tol=-1),\n            dict(minstep=-1),\n            dict(maxstep=-1),\n            dict(maxiter=-1),\n            dict(tol=0),\n            dict(minstep=0),\n            dict(maxstep=0),\n            dict(maxiter=0),\n        )\n        for params in invalid_parameters:\n            with self.assertRaises(ValueError):\n                fmin_golden_search(lambda x: x, **params)\n\n        with self.assertRaises(ValueError):\n            fmin_golden_search(lambda x: x, minstep=10, maxstep=1)\n\n    def test_golden_search_function(self):\n        def f(x):\n            return tf.sin(x) * x ** -0.5\n\n        def check_updates(step):\n            return f(3 + step)\n\n        best_step = fmin_golden_search(check_updates)\n        self.assertAlmostEqual(1.6, self.eval(best_step), places=2)\n\n        best_step = fmin_golden_search(check_updates, maxstep=1)\n        self.assertAlmostEqual(1, self.eval(best_step), places=2)\n'"
tests/algorithms/minsearch/test_wolfe.py,0,"b'import operator\nfrom functools import reduce\nfrom collections import namedtuple\nfrom itertools import product\n\nimport numpy as np\n\nfrom neupy.algorithms.minsearch import wolfe\n\nfrom base import BaseTestCase\n\n\nCase = namedtuple(\'Case\', \'func_input func_expected\')\n\n\nclass WolfeInterpolationTestCase(BaseTestCase):\n    def assertAlmostEqual(self, left_value, right_value, places=6):\n        if np.isnan(left_value) or np.isnan(right_value):\n            self.assertTrue(np.isnan(left_value) and np.isnan(right_value))\n        else:\n            super(WolfeInterpolationTestCase, self).assertAlmostEqual(\n                left_value, right_value, places\n            )\n\n    def test_line_search_exceptions(self):\n        testcases = [\n            # Invalid c1 values\n            dict(c1=-1, c2=0.5, maxiter=1),\n            dict(c1=0, c2=0.5, maxiter=1),\n            dict(c1=1, c2=0.5, maxiter=1),\n\n            # Invalid c2 values\n            dict(c2=-1, c1=0.5, maxiter=1),\n            dict(c2=0, c1=0.5, maxiter=1),\n            dict(c2=1, c1=0.5, maxiter=1),\n\n            # c1 > c2\n            dict(c1=0.5, c2=0.1, maxiter=1),\n\n            # Invalid `maxiter` values\n            dict(c1=0.05, c2=0.1, maxiter=-10),\n            dict(c1=0.05, c2=0.1, maxiter=0),\n        ]\n\n        def func(x):\n            return x\n\n        for testcase in testcases:\n            error_desc = ""Line search for {}"".format(testcase)\n            with self.assertRaises(ValueError, msg=error_desc):\n                wolfe.line_search(f=func, f_deriv=func, **testcase)\n\n    def test_sequential_and(self):\n        for input_values in product([False, True], repeat=4):\n            expected_value = reduce(operator.and_, input_values)\n            actual_value = self.eval(wolfe.sequential_and(*input_values))\n            self.assertEqual(expected_value, actual_value)\n\n    def test_sequential_or(self):\n        for input_values in product([False, True], repeat=4):\n            expected_value = reduce(operator.or_, input_values)\n            actual_value = self.eval(wolfe.sequential_or(*input_values))\n            self.assertEqual(expected_value, actual_value)\n\n    def test_quadratic_minimizer_exceptions(self):\n        with self.assertRaises(ValueError):\n            # Invalid value for parameter ``bound_size_ratio``\n            wolfe.quadratic_minimizer(\n                x_a=0, y_a=1,\n                x_b=1, y_b=2,\n                y_prime_a=-1,\n                bound_size_ratio=2,\n            )\n\n    def test_quadratic_minimizer(self):\n        testcases = (\n            Case(\n                func_input=dict(\n                    y_prime_a=-1,\n                    x_a=0, y_a=1,\n                    x_b=1, y_b=2,\n                ),\n                func_expected=0.25,\n            ),\n            Case(\n                func_input=dict(\n                    y_prime_a=-1,\n                    x_a=1, y_a=1,\n                    x_b=2, y_b=2,\n                ),\n                func_expected=1.25,\n            ),\n        )\n\n        for testcase in testcases:\n            actual_output = wolfe.quadratic_minimizer(**testcase.func_input)\n            self.assertAlmostEqual(\n                self.eval(actual_output),\n                testcase.func_expected,\n            )\n\n    def test_cubic_minimizer_exceptions(self):\n        with self.assertRaisesRegexp(ValueError, ""bound_size_ratio""):\n            # bound_size_ratio < 0\n            wolfe.cubic_minimizer(0, 1, -1, 5, 10, 10, 60, bound_size_ratio=-1)\n\n        with self.assertRaisesRegexp(ValueError, ""bound_size_ratio""):\n            # bound_size_ratio >= 1\n            wolfe.cubic_minimizer(0, 1, -1, 5, 10, 10, 60, bound_size_ratio=2)\n\n    def test_cubic_minimizer(self):\n        testcases = (\n            Case(\n                func_input=dict(\n                    y_prime_a=-1.,\n                    x_a=0., y_a=1.,\n                    x_b=5., y_b=10.,\n                    x_c=10., y_c=60.,\n                    bound_size_ratio=0.2,\n                ),\n                func_expected=1.06,\n            ),\n        )\n\n        for testcase in testcases:\n            actual_output = wolfe.cubic_minimizer(**testcase.func_input)\n            self.assertAlmostEqual(\n                self.eval(actual_output),\n                testcase.func_expected,\n                places=2,\n            )\n\n    def test_wolfe_linear_search(self):\n        x_current = 3\n        grad = 2 * (x_current - 5.5)\n\n        def square(step):\n            x_new = x_current - step * grad\n            return (x_new - 5.5) ** 2\n\n        def square_deriv(step):\n            # Derivative with respect to the step\n            return -grad * 2 * ((x_current - step * grad) - 5.5)\n\n        x_star = wolfe.line_search(square, square_deriv)\n        x_star = self.eval(x_star)\n\n        self.assertEqual(square(0), 6.25)\n        self.assertAlmostEqual(square(x_star), 0, places=2)\n        self.assertAlmostEqual(x_star, 0.5, places=2)\n'"
tests/algorithms/rbfn/__init__.py,0,b''
tests/algorithms/rbfn/test_grnn.py,0,"b'import numpy as np\nfrom sklearn import datasets, metrics\nfrom sklearn.model_selection import train_test_split\n\nfrom neupy import algorithms\nfrom neupy.exceptions import NotTrained\n\nfrom base import BaseTestCase\n\n\nclass GRNNTestCase(BaseTestCase):\n    def test_grrn_exceptions(self):\n        with self.assertRaises(ValueError):\n            # size of target data not the same as size of\n            # input data.\n            grnet = algorithms.GRNN(std=0.1, verbose=False)\n            grnet.train(np.array([[0], [0]]), np.array([0]))\n\n        with self.assertRaises(ValueError):\n            # 2 features for target data\n            grnet = algorithms.GRNN(std=0.1, verbose=False)\n            grnet.train(np.array([[0], [0]]), np.array([[0, 0]]))\n\n        with self.assertRaises(ValueError):\n            # invalid feature size for prediction data\n            grnet = algorithms.GRNN(std=0.1, verbose=False)\n            grnet.train(np.array([[0], [0]]), np.array([0]))\n            grnet.predict(np.array([[0]]))\n\n        with self.assertRaises(NotTrained):\n            # Prediction without training\n            grnet = algorithms.GRNN(std=0.1, verbose=False)\n            grnet.predict(np.array([[0]]))\n\n        with self.assertRaises(ValueError):\n            # different number of features for\n            # train and test data\n            grnet = algorithms.GRNN(std=0.1, verbose=False)\n            grnet.train(np.array([[0]]), np.array([0]))\n            grnet.predict(np.array([[0, 0]]))\n\n    def test_simple_grnn(self):\n        dataset = datasets.load_diabetes()\n        x_train, x_test, y_train, y_test = train_test_split(\n            dataset.data, dataset.target, test_size=0.3\n        )\n\n        x_train_before = x_train.copy()\n        x_test_before = x_test.copy()\n        y_train_before = y_train.copy()\n\n        grnnet = algorithms.GRNN(std=0.1, verbose=False)\n        grnnet.train(x_train, y_train)\n        result = grnnet.predict(x_test)\n        error = metrics.mean_absolute_error(result, y_test)\n\n        old_result = result.copy()\n        self.assertAlmostEqual(error, 46.3358, places=4)\n\n        # Test problem with variable links\n        np.testing.assert_array_equal(x_train, x_train_before)\n        np.testing.assert_array_equal(x_test, x_test_before)\n        np.testing.assert_array_equal(y_train, y_train_before)\n\n        x_train[:, :] = 0\n        result = grnnet.predict(x_test)\n\n        np.testing.assert_array_almost_equal(result, old_result)\n        self.assertPickledNetwork(grnnet, x_test)\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.GRNN(std=0.1, verbose=False),\n            np.array([1, 2, 3]),\n            np.array([1, 2, 3])\n        )\n\n    def test_predict_different_inputs(self):\n        grnnet = algorithms.GRNN(std=0.1, verbose=False)\n\n        data = np.array([[1, 2, 3]]).T\n        target = np.array([[1, 2, 3]]).T\n\n        grnnet.train(data, target)\n        self.assertInvalidVectorPred(grnnet, data.ravel(), target,\n                                     decimal=2)\n'"
tests/algorithms/rbfn/test_pnn.py,0,"b'from __future__ import division\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\nfrom neupy import algorithms\nfrom neupy.exceptions import NotTrained\n\nfrom base import BaseTestCase\n\n\nclass PNNTestCase(BaseTestCase):\n    def test_handle_errors(self):\n        with self.assertRaises(ValueError):\n            # size of target data not the same as\n            # size of input data.\n            pnnet = algorithms.PNN(std=0.1, verbose=False)\n            pnnet.train(np.array([[0], [0]]), np.array([0]))\n\n        with self.assertRaises(ValueError):\n            # 2-D target vector (must be 1-D)\n            pnnet = algorithms.PNN(std=0.1, verbose=False)\n            pnnet.train(np.array([[0]]), np.array([[0, 0]]))\n\n        with self.assertRaises(ValueError):\n            # invalid feature size for prediction data\n            pnnet = algorithms.PNN(std=0.1, verbose=False)\n            pnnet.train(np.array([[0], [0]]), np.array([0]))\n            pnnet.predict(np.array([[0]]))\n\n        msg = ""hasn\'t been trained""\n        with self.assertRaisesRegexp(NotTrained, msg):\n            # predict without training\n            pnnet = algorithms.PNN(std=0.1, verbose=False)\n            pnnet.predict(np.array([[0]]))\n\n        with self.assertRaises(ValueError):\n            # different number of features for\n            # train and test data\n            grnet = algorithms.PNN(std=0.1, verbose=False)\n            grnet.train(np.array([[0]]), np.array([0]))\n            grnet.predict(np.array([[0, 0]]))\n\n    def test_simple_pnn(self):\n        dataset = datasets.load_iris()\n        data = dataset.data\n        target = dataset.target\n\n        test_data_size = 10\n        skfold = StratifiedKFold(n_splits=test_data_size)\n        avarage_result = 0\n\n        for train, test in skfold.split(data, target):\n            x_train, x_test = data[train], data[test]\n            y_train, y_test = target[train], target[test]\n\n            nw = algorithms.PNN(verbose=False, std=0.1)\n            nw.train(x_train, y_train)\n            result = nw.predict(x_test)\n            avarage_result += sum(y_test == result)\n\n        self.assertEqual(avarage_result / test_data_size, 14.4)\n        self.assertPickledNetwork(nw, x_test)\n\n    def test_digit_prediction(self):\n        dataset = datasets.load_digits()\n        x_train, x_test, y_train, y_test = train_test_split(\n            dataset.data, dataset.target, test_size=0.3\n        )\n\n        nw = algorithms.PNN(verbose=False, std=10)\n        nw.train(x_train, y_train)\n        result = nw.predict(x_test)\n\n        accuracy = metrics.accuracy_score(y_test, result)\n        self.assertAlmostEqual(accuracy, 0.9889, places=4)\n\n    def test_predict_probability(self):\n        dataset = datasets.load_digits()\n        x_train, x_test, y_train, y_test = train_test_split(\n            dataset.data, dataset.target, test_size=0.3)\n\n        x_train_before = x_train.copy()\n        x_test_before = x_test.copy()\n        y_train_before = y_train.copy()\n\n        number_of_classes = len(np.unique(dataset.target))\n\n        pnnet = algorithms.PNN(verbose=False, std=10)\n        pnnet.train(x_train, y_train)\n        result = pnnet.predict_proba(x_test)\n\n        n_test_inputs = x_test.shape[0]\n        self.assertEqual(result.shape, (n_test_inputs, number_of_classes))\n\n        total_classes_prob = np.round(result.sum(axis=1), 10)\n        np.testing.assert_array_equal(\n            total_classes_prob,\n            np.ones(n_test_inputs))\n\n        old_result = result.copy()\n\n        # Test problem with variable links\n        np.testing.assert_array_equal(x_train, x_train_before)\n        np.testing.assert_array_equal(x_test, x_test_before)\n        np.testing.assert_array_equal(y_train, y_train_before)\n\n        x_train[:, :] = 0\n        result = pnnet.predict_proba(x_test)\n        total_classes_prob = np.round(result.sum(axis=1), 10)\n        np.testing.assert_array_almost_equal(result, old_result)\n\n    def test_train_different_inputs(self):\n        self.assertInvalidVectorTrain(\n            algorithms.PNN(std=0.1, verbose=False),\n            np.array([1, 2, 3]),\n            np.array([1, 0, 1]))\n\n    def test_predict_different_inputs(self):\n        pnnet = algorithms.PNN(std=0.1, verbose=False)\n\n        data = np.array([[1, 2, 3]]).T\n        target = np.array([[1, 0, 1]]).T\n\n        pnnet.train(data, target)\n        self.assertInvalidVectorPred(\n            pnnet, data.ravel(), target.ravel(), decimal=2)\n\n    def test_pnn_mini_batches(self):\n        dataset = datasets.load_digits()\n        n_classes = len(np.unique(dataset.target))\n\n        x_train, x_test, y_train, y_test = train_test_split(\n            dataset.data, dataset.target, test_size=0.3)\n\n        pnnet = algorithms.PNN(verbose=False, batch_size=100, std=10)\n        pnnet.train(x_train, y_train)\n\n        y_predicted = pnnet.predict(x_test)\n        self.assertEqual(y_predicted.shape, y_test.shape)\n\n        y_predicted = pnnet.predict_proba(x_test)\n        self.assertEqual(y_predicted.shape, (y_test.shape[0], n_classes))\n\n    def test_pnn_repr(self):\n        pnn = algorithms.PNN(std=0.1)\n\n        self.assertIn(\'PNN\', str(pnn))\n        self.assertIn(\'std\', str(pnn))\n        self.assertIn(\'batch_size\', str(pnn))\n\n    def test_pnn_non_trivial_class_names(self):\n        # Issue #177: https://github.com/itdxer/neupy/issues/177\n        x = np.array([10] * 10 + [20] * 10 + [30] * 10)\n        y = np.array([1] * 10 + [2] * 10 + [3] * 10)\n\n        pnn = algorithms.PNN(std=1)\n        pnn.train(x, y)\n        y_predicted = pnn.predict(x)\n\n        np.testing.assert_array_almost_equal(y, y_predicted)\n        self.assertEqual(sorted(pnn.classes), [1, 2, 3])\n\n    def test_pnn_non_trivial_class_names_as_strings(self):\n        # Issue #177: https://github.com/itdxer/neupy/issues/177\n        x = np.array([10] * 10 + [20] * 10 + [30] * 10)\n        y = np.array([\'cat\'] * 10 + [\'dog\'] * 10 + [\'horse\'] * 10)\n\n        pnn = algorithms.PNN(std=1)\n        pnn.train(x, y)\n        y_predicted = pnn.predict(x)\n\n        np.testing.assert_array_equal(y, y_predicted)\n        self.assertEqual(sorted(pnn.classes), [\'cat\', \'dog\', \'horse\'])\n'"
tests/algorithms/rbm/__init__.py,0,b''
tests/algorithms/rbm/test_rbm.py,0,"b""import pickle\n\nimport numpy as np\n\nfrom neupy import algorithms\nfrom neupy.utils import asfloat\n\nfrom base import BaseTestCase\n\n\nclass BernoulliRBMTestCase(BaseTestCase):\n    def setUp(self):\n        super(BernoulliRBMTestCase, self).setUp()\n        self.data = np.array([\n            [1, 0, 1, 0],\n            [1, 0, 1, 0],\n            [1, 0, 0, 0],  # incomplete sample\n            [1, 0, 1, 0],\n\n            [0, 1, 0, 1],\n            [0, 0, 0, 1],  # incomplete sample\n            [0, 1, 0, 1],\n            [0, 1, 0, 1],\n            [0, 1, 0, 1],\n            [0, 1, 0, 1],\n        ])\n        self.data = asfloat(self.data)\n\n    def test_rbm_score(self):\n        rbm = algorithms.RBM(\n            n_visible=4,\n            n_hidden=1,\n            step=0.5,\n            batch_size=10\n        )\n        rbm.train(self.data, epochs=500)\n\n        rbm_error_for_known = rbm.score(np.array([[0, 1, 0, 1]]))\n        rbm_error_for_unknown = rbm.score(np.array([[0, 1, 0, 0]]))\n\n        self.assertLess(rbm_error_for_unknown, rbm_error_for_known)\n\n    def test_simple_bernoulli_rbm(self):\n        data = self.data\n\n        rbm = algorithms.RBM(n_visible=4, n_hidden=1)\n        rbm.train(data, epochs=100)\n\n        output = rbm.visible_to_hidden(data)\n        np.testing.assert_array_equal(\n            output.round(),\n            np.array([[0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]).T\n        )\n\n        typical_class1_sample = output[0]\n        incomplete_class1_sample = output[2]\n        # Check that probability of a typical case is\n        # closer to 0 (because 0 is a class defined by RBM)\n        # than for the incomplete case.\n        self.assertLess(typical_class1_sample, incomplete_class1_sample)\n\n        typical_class2_sample = output[4]\n        incomplete_class2_sample = output[5]\n        # Same as before but for class 1.\n        self.assertGreater(typical_class2_sample, incomplete_class2_sample)\n\n    def test_rbm_storage(self):\n        data = self.data\n        network = algorithms.RBM(\n            n_visible=4,\n            n_hidden=1,\n            step=0.1,\n            batch_size=10,\n        )\n        network.train(data, epochs=500)\n\n        stored_network = pickle.dumps(network)\n        loaded_network = pickle.loads(stored_network)\n\n        network_hidden = network.visible_to_hidden(data)\n        loaded_network_hidden = loaded_network.visible_to_hidden(data)\n\n        np.testing.assert_array_almost_equal(\n            loaded_network_hidden, network_hidden)\n\n        network_visible = network.hidden_to_visible(network_hidden)\n        loaded_network_visible = loaded_network.hidden_to_visible(\n            loaded_network_hidden)\n\n        np.testing.assert_array_almost_equal(\n            loaded_network_visible, network_visible)\n\n    def test_rbm_batch_size(self):\n        data = self.data\n        batch_size = 7\n\n        self.assertNotEqual(len(data) % batch_size, 0)\n\n        rbm = algorithms.RBM(\n            n_visible=4, n_hidden=1, step=0.1,\n            batch_size=batch_size\n        )\n        # Check if it's possilbe to train RBM in case if\n        # we cannot divide dataset into full mini-batches\n        rbm.train(data, epochs=2)\n\n    def test_rbm_sampling(self):\n        data = self.data\n\n        rbm = algorithms.RBM(n_visible=4, n_hidden=1)\n        rbm.train(data, epochs=500)\n\n        proba_sample = rbm.hidden_to_visible(\n            rbm.visible_to_hidden(data)\n        )\n        proba_sample = proba_sample.round().astype(int)\n\n        np.testing.assert_array_equal(\n            proba_sample.round(),\n            np.array([\n                [1, 0, 1, 0],\n                [1, 0, 1, 0],\n                [1, 0, 1, 0],  # fixed sample\n                [1, 0, 1, 0],\n\n                [0, 1, 0, 1],\n                [0, 1, 0, 1],  # fixed sample\n                [0, 1, 0, 1],\n                [0, 1, 0, 1],\n                [0, 1, 0, 1],\n                [0, 1, 0, 1],\n            ])\n        )\n\n        sampled_data = rbm.gibbs_sampling(data, n_iter=1)\n        self.assertNotEqual(0, np.abs(sampled_data - self.data).sum())\n\n    def test_rbm_predict(self):\n        rbm = algorithms.RBM(n_visible=4, n_hidden=1)\n        hidden_state = rbm.visible_to_hidden(self.data)\n        prediction = rbm.predict(self.data)\n        np.testing.assert_array_almost_equal(hidden_state, prediction)\n"""
tests/layers/graph/__init__.py,0,b''
tests/layers/graph/test_definitions.py,0,"b'import numpy as np\n\nfrom neupy import layers\nfrom neupy.utils import asfloat\nfrom neupy.exceptions import LayerConnectionError\n\nfrom base import BaseTestCase\n\n\nclass OldInlineDefinitionsTestCase(BaseTestCase):\n    def test_inline_network_python_compatibility(self):\n        network = layers.Input(1) > layers.Relu(2)\n        self.assertTrue(network.__bool__())\n        self.assertTrue(network.__nonzero__())\n\n    def test_inline_network_order(self):\n        input_layer_1 = layers.Input(1)\n        relu_2 = layers.Relu(2)\n        relu_3 = layers.Relu(3)\n\n        network_1 = input_layer_1 > relu_2 > relu_3\n        self.assertEqual(list(network_1), [input_layer_1, relu_2, relu_3])\n\n        input_layer_4 = layers.Input(4)\n        relu_5 = layers.Relu(5)\n        relu_6 = layers.Relu(6)\n\n        network_2 = input_layer_4 > relu_5\n        self.assertEqual(list(network_2), [input_layer_4, relu_5])\n\n        network_3 = relu_5 > relu_6\n        self.assertEqual(list(network_3), [relu_5, relu_6])\n\n    def test_inline_network_wtih_different_pointers(self):\n        relu_2 = layers.Relu(2)\n\n        network_1 = layers.Input(1) > relu_2 > layers.Relu(3)\n        network_2 = relu_2 > layers.Relu(4)\n        network_3 = layers.Input(1) > relu_2\n\n        self.assertShapesEqual(network_1.input_shape, (None, 1))\n        self.assertShapesEqual(network_1.output_shape, (None, 3))\n\n        self.assertShapesEqual(network_2.input_shape, None)\n        self.assertShapesEqual(network_2.output_shape, (None, 4))\n\n        self.assertShapesEqual(network_3.input_shape, (None, 1))\n        self.assertShapesEqual(network_3.output_shape, (None, 2))\n\n        self.assertIn(relu_2, network_2.input_layers)\n        self.assertIn(relu_2, network_3.output_layers)\n\n    def test_inline_network_with_parallel_network(self):\n        left_branch = layers.join(\n            layers.Convolution((3, 3, 32)),\n            layers.Relu(),\n            layers.MaxPooling((2, 2)),\n        )\n\n        right_branch = layers.join(\n            layers.Convolution((7, 7, 16)),\n            layers.Relu(),\n        )\n\n        input_layer = layers.Input((10, 10, 3))\n        concat = layers.Concatenate()\n\n        network_concat = input_layer > (left_branch | right_branch) > concat\n        network = network_concat > layers.Reshape() > layers.Softmax()\n\n        self.assertShapesEqual(network_concat.input_shape, (None, 10, 10, 3))\n        self.assertShapesEqual(network_concat.output_shape, (None, 4, 4, 48))\n\n        self.assertShapesEqual(network.input_shape, (None, 10, 10, 3))\n        self.assertShapesEqual(network.output_shape, (None, 768))\n\n\nclass InlineDefinitionsTestCase(BaseTestCase):\n    def test_inline_definition(self):\n        network = layers.Input(2) >> layers.Relu(10) >> layers.Tanh(1)\n        self.assertShapesEqual(network.input_shape, (None, 2))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n\n        input_value = asfloat(np.random.random((10, 2)))\n        output_value = self.eval(network.output(input_value))\n        self.assertEqual(output_value.shape, (10, 1))\n\n    def test_network_shape_multiple_inputs(self):\n        in1 = layers.Input(10)\n        in2 = layers.Input(20)\n        conn = (in1 | in2) >> layers.Concatenate()\n\n        self.assertShapesEqual(conn.input_shape, [(None, 10), (None, 20)])\n        self.assertShapesEqual(conn.output_shape, (None, 30))\n\n    def test_network_shape_multiple_outputs(self):\n        conn = layers.Input(10) >> (layers.Sigmoid(1) | layers.Sigmoid(2))\n        self.assertShapesEqual(conn.input_shape, (None, 10))\n        self.assertShapesEqual(conn.output_shape, [(None, 1), (None, 2)])\n\n    def test_inline_operation_order(self):\n        in1 = layers.Input(10)\n        out1 = layers.Sigmoid(1)\n        out2 = layers.Sigmoid(2)\n        conn = in1 >> out1 | out2\n\n        self.assertShapesEqual(conn.input_shape, [(None, 10), None])\n        self.assertShapesEqual(conn.output_shape, [(None, 1), (None, 2)])\n\n    def test_sequential_partial_definitions(self):\n        # Tree structure:\n        #\n        #                       Sigmoid(10)\n        #                      /\n        # Input(10) - Sigmoid(5)\n        #                      \\\n        #                       Softmax(10)\n        #\n        input_layer = layers.Input(10)\n        minimized = input_layer >> layers.Sigmoid(5)\n        reconstructed = minimized >> layers.Sigmoid(10)\n        classifier = minimized >> layers.Softmax(20)\n\n        x_matrix = asfloat(np.random.random((3, 10)))\n        minimized_output = self.eval(minimized.output(x_matrix))\n        self.assertEqual((3, 5), minimized_output.shape)\n\n        reconstructed_output = self.eval(reconstructed.output(x_matrix))\n        self.assertEqual((3, 10), reconstructed_output.shape)\n\n        classifier_output = self.eval(classifier.output(x_matrix))\n        self.assertEqual((3, 20), classifier_output.shape)\n\n    def test_inplace_seq_operator(self):\n        network = layers.Input(1)\n        network >>= layers.Relu(2)\n        network >>= layers.Relu(3)\n\n        self.assertEqual(len(network), 3)\n        self.assertShapesEqual(network.input_shape, (None, 1))\n        self.assertShapesEqual(network.output_shape, (None, 3))\n\n    def test_inplace_parallel(self):\n        network = layers.Input(10)\n        network |= layers.Input(10)\n        network >>= layers.Concatenate()\n\n        self.assertEqual(len(network), 3)\n        self.assertShapesEqual(network.input_shape, [(None, 10), (None, 10)])\n        self.assertShapesEqual(network.output_shape, (None, 20))\n\n\nclass DefinitionsTestCase(BaseTestCase):\n    def test_one_to_many_parallel_network_output(self):\n        one_to_many = layers.join(\n            layers.Input(4),\n            layers.parallel(\n                layers.Linear(11),\n                layers.Linear(12),\n                layers.Linear(13),\n            ),\n        )\n\n        input_value = asfloat(np.random.random((10, 4)))\n        actual_output = self.eval(one_to_many.output(input_value))\n\n        self.assertEqual(actual_output[0].shape, (10, 11))\n        self.assertEqual(actual_output[1].shape, (10, 12))\n        self.assertEqual(actual_output[2].shape, (10, 13))\n\n    def test_networks_with_complex_parallel_relations(self):\n        input_layer = layers.Input((5, 5, 3))\n        network = layers.join(\n            layers.parallel([\n                layers.Convolution((1, 1, 8)),\n            ], [\n                layers.Convolution((1, 1, 4)),\n                layers.parallel(\n                    layers.Convolution((1, 3, 2), padding=\'same\'),\n                    layers.Convolution((3, 1, 2), padding=\'same\'),\n                ),\n            ], [\n                layers.Convolution((1, 1, 8)),\n                layers.Convolution((3, 3, 4), padding=\'same\'),\n                layers.parallel(\n                    layers.Convolution((1, 3, 2), padding=\'same\'),\n                    layers.Convolution((3, 1, 2), padding=\'same\'),\n                )\n            ], [\n                layers.MaxPooling((3, 3), padding=\'same\', stride=(1, 1)),\n                layers.Convolution((1, 1, 8)),\n            ]),\n            layers.Concatenate(),\n        )\n        self.assertShapesEqual(network.input_shape, [None, None, None, None])\n        self.assertShapesEqual(network.output_shape, (None, None, None, 24))\n\n        # Connect them at the end, because we need to make\n        # sure tha parallel networks defined without input shapes\n        network = layers.join(input_layer, network)\n        self.assertShapesEqual(network.output_shape, (None, 5, 5, 24))\n\n    def test_residual_networks(self):\n        network = layers.join(\n            layers.Input((5, 5, 3)),\n            layers.parallel(\n                layers.Identity(),\n                layers.join(\n                    layers.Convolution((3, 3, 8), padding=\'same\'),\n                    layers.Relu(),\n                ),\n            ),\n            layers.Concatenate(),\n        )\n\n        self.assertShapesEqual((None, 5, 5, 3), network.input_shape)\n        self.assertShapesEqual((None, 5, 5, 11), network.output_shape)\n\n    def test_fail_many_to_many_connection(self):\n        network_a = layers.join(\n            layers.Input(10),\n            layers.parallel(\n                layers.Relu(5),\n                layers.Relu(4),\n            ),\n        )\n        network_b = layers.join(\n            layers.parallel(\n                layers.Relu(5),\n                layers.Relu(4),\n            ),\n            layers.Concatenate(),\n        )\n\n        error_message = ""Cannot make many to many connection between graphs""\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(network_a, network_b)\n\n    def test_fail_when_cycle_created(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(10),\n        )\n\n        error_message = (\n            ""Cannot define connection between layers, ""\n            ""because it creates cycle in the graph""\n        )\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(network, network)\n\n        extra_relu = layers.Relu(5)\n        network = layers.join(network, extra_relu)\n\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.join(network, extra_relu)\n\n\nclass RepeatArchitectureTestCase(BaseTestCase):\n    def test_repeat_layer(self):\n        network = layers.repeat(layers.Relu(10), n=5)\n        self.assertEqual(len(network), 5)\n        self.assertShapesEqual(network.output_shape, (None, 10))\n\n    def test_repeat_network(self):\n        block = layers.join(\n            layers.Convolution((3, 3, 32)),\n            layers.Relu(),\n            layers.BatchNorm(),\n        )\n        network = layers.repeat(block, n=5)\n        self.assertEqual(len(network), 15)\n        self.assertShapesEqual(network.output_shape, (None, None, None, 32))\n\n    def test_failed_to_repeat_network(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5),\n        )\n        network.create_variables()\n\n        relu = network.layers[1]\n        error_message = ""input shape is incompatible with the output shape""\n\n        with self.assertRaisesRegexp(LayerConnectionError, error_message):\n            layers.repeat(relu, n=4)\n\n    def test_wrong_number_of_repeats(self):\n        error_message = ""parameter should be a positive integer""\n        for wrong_value in (0, 1.5, 9. / 3.):\n            with self.assertRaisesRegexp(ValueError, error_message):\n                layers.repeat(layers.Relu(10), n=wrong_value)\n\n    def test_repeat_once(self):\n        input_layer = layers.Relu(10)\n        output_layer = layers.repeat(input_layer, n=1)\n        self.assertIs(output_layer, input_layer)\n\n    def test_repeat_with_name_patterns(self):\n        network = layers.repeat(layers.Relu(10, name=\'rl{}\'), n=4)\n        layer_names = [layer.name for layer in network.layers]\n        self.assertSequenceEqual(layer_names, [\'rl1\', \'rl2\', \'rl3\', \'rl4\'])\n'"
tests/layers/graph/test_extra_features.py,0,"b'from neupy import layers\n\nfrom base import BaseTestCase\n\n\nclass GraphExtraFeaturesTestCase(BaseTestCase):\n    def test_single_input_for_parallel_layers(self):\n        left = layers.Input(10, name=\'input\') >> layers.Sigmoid(5)\n        right = left >> layers.Sigmoid(2)\n        network = (left | right) >> layers.Concatenate()\n\n        self.assertEqual(len(network.input_layers), 1)\n\n        input_layer = network.input_layers[0]\n        self.assertEqual(input_layer.name, \'input\')\n\n    def test_single_output_for_parallel_layers(self):\n        left = layers.Sigmoid(5, name=\'output\')\n        right = layers.Relu(10) >> left\n        network = layers.Input(10) >> (left | right)\n\n        self.assertEqual(len(network.output_layers), 1)\n\n        output_layer = network.output_layers[0]\n        self.assertEqual(output_layer.name, \'output\')\n\n    def test_check_if_network_sequential(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5),\n            layers.Relu(3),\n        )\n        self.assertTrue(network.is_sequential())\n\n        network = layers.join(\n            layers.Input(10),\n            layers.parallel(\n                layers.Relu(5),\n                layers.Relu(3),\n            ),\n            layers.Concatenate(),\n        )\n        self.assertFalse(network.is_sequential())\n\n        network = layers.parallel(\n            layers.Relu(5),\n            layers.Relu(3),\n        )\n        self.assertFalse(network.is_sequential())\n\n    def test_empty_graph(self):\n        graph = layers.LayerGraph()\n        self.assertEqual(len(graph), 0)\n        self.assertEqual(str(graph), ""[empty graph]"")\n'"
tests/layers/graph/test_extra_functions.py,0,"b'from collections import OrderedDict\n\nfrom neupy.layers.graph import (\n    find_outputs_in_graph,\n    topological_sort,\n    filter_graph,\n    is_cyclic,\n    lazy_property,\n)\n\nfrom base import BaseTestCase\n\n\nclass ExtraFunctionsTestCase(BaseTestCase):\n    def setUp(self):\n        super(ExtraFunctionsTestCase, self).setUp()\n\n        # 1 - 2 - 4 - 5\n        # 3 _/     \\_ 6 - 7\n        self.graph = OrderedDict([\n            (1, [2]),\n            (3, [2]),\n            (2, [4]),\n            (4, [5, 6]),\n            (6, [7]),\n            (5, []),\n            (7, []),\n        ])\n\n    def test_is_cyclic(self):\n        self.assertTrue(is_cyclic({1: [2], 2: [3], 3: [1]}))\n        self.assertTrue(is_cyclic({1: [2], 2: [3, 1], 3: []}))\n\n        self.assertFalse(is_cyclic({1: [2], 2: [3, 4, 5], 3: [4, 5]}))\n        self.assertFalse(is_cyclic({1: [2], 2: [3], 3: [4]}))\n        self.assertFalse(is_cyclic(self.graph))\n\n    def test_lazy_property(self):\n        class SomeClass(object):\n            @lazy_property\n            def foo(self):\n                return [1, 2, 3]\n\n        a = SomeClass()\n        self.assertIs(a.foo, a.foo)\n\n    def test_filter_graph(self):\n        filtered_graph = filter_graph({\n            1: [2, 3],\n            2: [3, 4],\n            3: [5],\n            4: [5],\n            5: [],\n        }, include_keys=[2, 3, 4])\n        self.assertDictEqual(filtered_graph, {\n            2: [3, 4],\n            3: [],\n            4: [],\n        })\n\n    def test_topological_sort(self):\n        sorted_nodes = topological_sort(self.graph)\n        self.assertSequenceEqual(sorted_nodes, [5, 7, 6, 4, 2, 1, 3])\n\n        message = ""Cannot apply topological sort to the graphs with cycles""\n        with self.assertRaisesRegexp(RuntimeError, message):\n            topological_sort({1: [2], 2: [3], 3: [1]})\n\n    def test_find_outputs_in_graph(self):\n        outputs = find_outputs_in_graph(self.graph)\n        self.assertSequenceEqual(outputs, [5, 7])\n'"
tests/layers/graph/test_slicing.py,0,"b'import numpy as np\n\nfrom neupy import layers\nfrom neupy.utils import asfloat\n\nfrom base import BaseTestCase\n\n\nclass SliceLayerConnectionsTestCase(BaseTestCase):\n    def test_change_output_layer(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.Relu(5, name=\'relu-1\'),\n            layers.Relu(1, name=\'relu-2\'),\n        )\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n        self.assertEqual(len(network), 3)\n\n        relu_1_network = network.end(\'relu-1\')\n        self.assertShapesEqual(relu_1_network.input_shape, (None, 10))\n        self.assertShapesEqual(relu_1_network.output_shape, (None, 5))\n        self.assertEqual(len(relu_1_network.layers), 2)\n\n        x_test = asfloat(np.ones((7, 10)))\n        y_predicted = self.eval(relu_1_network.output(x_test))\n        self.assertEqual(y_predicted.shape, (7, 5))\n\n    def test_select_network_branch(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.parallel(\n                layers.Relu(1, name=\'relu-1\'),\n                layers.Relu(2, name=\'relu-2\'),\n            )\n        )\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, [(None, 1), (None, 2)])\n        self.assertEqual(len(network), 3)\n\n        relu_1_network = network.end(\'relu-1\')\n        self.assertShapesEqual(relu_1_network.input_shape, (None, 10))\n        self.assertShapesEqual(relu_1_network.output_shape, (None, 1))\n        self.assertEqual(len(relu_1_network), 2)\n\n        x_test = asfloat(np.ones((7, 10)))\n        y_predicted = self.eval(relu_1_network.output(x_test))\n        self.assertEqual(y_predicted.shape, (7, 1))\n\n        relu_2_network = network.end(\'relu-2\')\n        self.assertShapesEqual(relu_2_network.input_shape, (None, 10))\n        self.assertShapesEqual(relu_2_network.output_shape, (None, 2))\n        self.assertEqual(len(relu_2_network), 2)\n\n    def test_cut_output_layers_in_sequence(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.Relu(5, name=\'relu-1\'),\n            layers.Relu(1, name=\'relu-2\'),\n        )\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n        self.assertEqual(len(network), 3)\n\n        cutted_network = network.end(\'relu-1\').end(\'input-1\')\n        self.assertShapesEqual(cutted_network.input_shape, (None, 10))\n        self.assertShapesEqual(cutted_network.output_shape, (None, 10))\n        self.assertEqual(len(cutted_network), 1)\n\n        x_test = asfloat(np.ones((7, 10)))\n        y_predicted = cutted_network.output(x_test)\n        self.assertEqual(y_predicted.shape, (7, 10))\n\n    def test_cut_using_layer_object(self):\n        relu = layers.Relu(2)\n        network = layers.Input(10) >> relu >> layers.Sigmoid(1)\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n        self.assertEqual(len(network), 3)\n\n        cutted_network = network.end(relu)\n        self.assertShapesEqual(cutted_network.input_shape, (None, 10))\n        self.assertShapesEqual(cutted_network.output_shape, (None, 2))\n        self.assertEqual(len(cutted_network), 2)\n\n    def test_unknown_layer_name_exception(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.Relu(5, name=\'relu-1\'),\n            layers.Relu(1, name=\'relu-2\'),\n        )\n        with self.assertRaises(NameError):\n            network.end(\'abc\')\n\n    def test_change_input_layer(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.Relu(5, name=\'relu-1\'),\n            layers.Relu(1, name=\'relu-2\'),\n        )\n        network.create_variables()\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n        self.assertEqual(len(network), 3)\n\n        relu_1_network = network.start(\'relu-1\')\n        self.assertShapesEqual(relu_1_network.input_shape, (None, 10))\n        self.assertShapesEqual(relu_1_network.output_shape, (None, 1))\n        self.assertEqual(len(relu_1_network), 2)\n        self.assertDictEqual(relu_1_network.forward_graph, {\n            network.layer(\'relu-1\'): [network.layer(\'relu-2\')],\n            network.layer(\'relu-2\'): [],\n        })\n\n        x_test = asfloat(np.ones((7, 10)))\n        y_predicted = self.eval(relu_1_network.output(x_test))\n        self.assertEqual(y_predicted.shape, (7, 1))\n\n    def test_cut_input_and_output_layers(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.Relu(8, name=\'relu-0\'),\n            layers.Relu(5, name=\'relu-1\'),\n            layers.Relu(2, name=\'relu-2\'),\n            layers.Relu(1, name=\'relu-3\'),\n        )\n        network.create_variables()\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n        self.assertEqual(len(network), 5)\n\n        cutted_network = network.start(\'relu-1\').end(\'relu-2\')\n\n        self.assertShapesEqual(cutted_network.input_shape, (None, 8))\n        self.assertShapesEqual(cutted_network.output_shape, (None, 2))\n        self.assertEqual(len(cutted_network), 2)\n        self.assertDictEqual(cutted_network.forward_graph, {\n            network.layer(\'relu-1\'): [network.layer(\'relu-2\')],\n            network.layer(\'relu-2\'): [],\n        })\n\n        x_test = asfloat(np.ones((7, 8)))\n        y_predicted = self.eval(cutted_network.output(x_test))\n        self.assertEqual(y_predicted.shape, (7, 2))\n\n    def test_cut_input_layers_in_sequence(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.Relu(5, name=\'relu-1\'),\n            layers.Relu(1, name=\'relu-2\'),\n        )\n        network.create_variables()\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n        self.assertEqual(len(network), 3)\n\n        cutted_network = network.start(\'relu-1\').start(\'relu-2\')\n        self.assertShapesEqual(cutted_network.input_shape, (None, 5))\n        self.assertShapesEqual(cutted_network.output_shape, (None, 1))\n        self.assertEqual(len(cutted_network), 1)\n        self.assertDictEqual(cutted_network.forward_graph, {\n            network.layer(\'relu-2\'): [],\n        })\n\n        x_test = asfloat(np.ones((7, 5)))\n        y_predicted = self.eval(cutted_network.output(x_test))\n        self.assertEqual(y_predicted.shape, (7, 1))\n\n    def test_connect_cutted_layers_to_other_layers(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.Relu(8, name=\'relu-0\'),\n            layers.Relu(5, name=\'relu-1\'),\n            layers.Relu(2, name=\'relu-2\'),\n            layers.Relu(1, name=\'relu-3\'),\n        )\n\n        self.assertShapesEqual(network.input_shape, (None, 10))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n        self.assertEqual(len(network), 5)\n\n        cutted_network = network.start(\'relu-1\').end(\'relu-2\')\n        self.assertEqual(cutted_network.input_shape, None)\n        self.assertShapesEqual(cutted_network.output_shape, (None, 2))\n        self.assertEqual(len(cutted_network), 2)\n        self.assertDictEqual(cutted_network.forward_graph, {\n            network.layer(\'relu-1\'): [network.layer(\'relu-2\')],\n            network.layer(\'relu-2\'): [],\n        })\n\n        new_network = layers.join(\n            layers.Input(8),\n            cutted_network,\n            layers.Sigmoid(11),\n        )\n        self.assertShapesEqual(new_network.input_shape, (None, 8))\n        self.assertShapesEqual(new_network.output_shape, (None, 11))\n        self.assertEqual(len(new_network), 4)\n\n        x_test = asfloat(np.ones((7, 10)))\n        y_predicted = self.eval(network.output(x_test))\n        self.assertEqual(y_predicted.shape, (7, 1))\n\n        x_test = asfloat(np.ones((7, 8)))\n        y_predicted = self.eval(new_network.output(x_test))\n        self.assertEqual(y_predicted.shape, (7, 11))\n\n    def test_get_layer_by_name_from_connection(self):\n        network = layers.join(\n            layers.Input(10, name=\'input-1\'),\n            layers.Relu(8, name=\'relu-0\'),\n            layers.Relu(5, name=\'relu-1\'),\n        )\n\n        reul0 = network.layer(\'relu-0\')\n        self.assertShapesEqual(reul0.output_shape, (None, 8))\n\n        reul1 = network.layer(\'relu-1\')\n        self.assertShapesEqual(reul1.output_shape, (None, 5))\n\n        message = ""Cannot find layer with name \'some-layer-name\'""\n        with self.assertRaisesRegexp(NameError, message):\n            network.layer(\'some-layer-name\')\n\n        message = ""Layer name expected to be a string""\n        with self.assertRaisesRegexp(ValueError, message):\n            network.layer(object)\n\n    def test_ambigous_layer_names_search(self):\n        network = layers.join(\n            layers.Input(10, name=\'input\'),\n            layers.Relu(8, name=\'relu\'),\n            layers.Relu(5, name=\'relu\'),\n        )\n\n        message = (\n            ""Ambiguous layer name `relu`. Network ""\n            ""has 2 layers with the same name""\n        )\n        with self.assertRaisesRegexp(NameError, message):\n            network.layer(\'relu\')\n\n    def test_start_slice_unknown_layer(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(8),\n        )\n        final_layer = layers.Relu(5, name=\'final-relu\')\n\n        message = ""Layer `final-relu` is not used in the graph""\n        with self.assertRaisesRegexp(ValueError, message):\n            network.start(final_layer)\n'"
tests/layers/graph/test_subnetworks.py,0,"b'import numpy as np\n\nfrom neupy import layers\nfrom neupy.utils import asfloat\n\nfrom base import BaseTestCase\n\n\nclass SubnetworksTestCase(BaseTestCase):\n    def test_subnetwork_in_mlp_network(self):\n        network = layers.join(\n            layers.Input(2),\n            layers.Relu(10),\n            layers.Relu(4) >> layers.Relu(7),\n            layers.Relu(3) >> layers.Relu(1),\n        )\n\n        self.assertEqual(len(network), 6)\n        self.assertTrue(network.is_sequential())\n        self.assertShapesEqual(network.input_shape, (None, 2))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n\n    def test_subnetwork_in_conv_network(self):\n        network = layers.join(\n            layers.Input((28, 28, 1)),\n\n            layers.Convolution((3, 3, 8)) >> layers.Relu(),\n            layers.Convolution((3, 3, 8)) >> layers.Relu(),\n            layers.MaxPooling((2, 2)),\n\n            layers.Reshape(),\n            layers.Softmax(1),\n        )\n\n        self.assertEqual(8, len(network))\n        self.assertTrue(network.is_sequential())\n        self.assertShapesEqual(network.input_shape, (None, 28, 28, 1))\n        self.assertShapesEqual(network.output_shape, (None, 1))\n\n        expected_order = [\n            layers.Input,\n            layers.Convolution, layers.Relu,\n            layers.Convolution, layers.Relu,\n            layers.MaxPooling, layers.Reshape, layers.Softmax,\n        ]\n        for actual_layer, expected_layer in zip(network, expected_order):\n            self.assertIsInstance(actual_layer, expected_layer)\n\n    def test_many_to_many_parallel_subnetworks(self):\n        network = layers.parallel(\n            layers.Input(1) >> layers.Linear(11),\n            layers.Input(2) >> layers.Linear(12),\n            layers.Input(3) >> layers.Linear(13),\n        )\n\n        input_value_1 = asfloat(np.random.random((10, 1)))\n        input_value_2 = asfloat(np.random.random((20, 2)))\n        input_value_3 = asfloat(np.random.random((30, 3)))\n\n        actual_output = self.eval(\n            network.output(input_value_1, input_value_2, input_value_3))\n\n        self.assertEqual(actual_output[0].shape, (10, 11))\n        self.assertEqual(actual_output[1].shape, (20, 12))\n        self.assertEqual(actual_output[2].shape, (30, 13))\n'"
tests/layers/graph/test_usage.py,0,"b'import numpy as np\n\nfrom neupy import layers\nfrom neupy.utils import asfloat\n\nfrom base import BaseTestCase\n\n\nclass UsageTestCase(BaseTestCase):\n    def test_network_wrong_number_of_input_values(self):\n        network = layers.join(\n            layers.Input(2),\n            layers.Relu(10),\n            layers.Relu(1),\n        )\n\n        input_value_1 = asfloat(np.random.random((10, 2)))\n        input_value_2 = asfloat(np.random.random((10, 2)))\n\n        with self.assertRaisesRegexp(ValueError, ""but 2 inputs was provided""):\n            network.output(input_value_1, input_value_2)\n\n    def test_multi_outputs_propagation(self):\n        network = layers.join(\n            layers.Input(4),\n            layers.parallel(\n                layers.Linear(2),\n                layers.Linear(3),\n                layers.Linear(4),\n            )\n        )\n        x = asfloat(np.random.random((7, 4)))\n        out1, out2, out3 = self.eval(network.output(x))\n\n        self.assertEqual((7, 2), out1.shape)\n        self.assertEqual((7, 3), out2.shape)\n        self.assertEqual((7, 4), out3.shape)\n\n    def test_multi_inputs_propagation(self):\n        network = layers.join(\n            layers.parallel(\n                layers.Input(10, name=\'input-1\'),\n                layers.Input(4, name=\'input-2\'),\n            ),\n            layers.Concatenate(),\n        )\n        x1 = asfloat(np.random.random((3, 10)))\n        x2 = asfloat(np.random.random((3, 4)))\n\n        out1 = self.eval(network.output(x1, x2))\n        out2 = self.eval(network.output({\'input-2\': x2, \'input-1\': x1}))\n\n        self.assertEqual((3, 14), out1.shape)\n        np.testing.assert_array_almost_equal(out1, out2)\n\n    def test_different_input_types(self):\n        input_layer = layers.Input(10, name=\'input\')\n        network = layers.join(\n            input_layer,\n            layers.Sigmoid(5),\n            layers.Sigmoid(4),\n        )\n\n        x_matrix = asfloat(np.random.random((3, 10)))\n        out1 = self.eval(network.output(x_matrix))\n        self.assertEqual((3, 4), out1.shape)\n\n        out2 = self.eval(network.output({input_layer: x_matrix}))\n        np.testing.assert_array_almost_equal(out1, out2)\n\n        out3 = self.eval(network.output({\'input\': x_matrix}))\n        np.testing.assert_array_almost_equal(out2, out3)\n\n        unknown_layer = layers.Input(5, name=\'unk\')\n        message = ""The `unk` layer doesn\'t appear in the network""\n        with self.assertRaisesRegexp(ValueError, message):\n            network.output({unknown_layer: x_matrix})\n\n    def test_not_an_input_layer_exception(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Sigmoid(2, name=\'sigmoid-2\'),\n            layers.Sigmoid(10),\n        )\n        x_test = asfloat(np.ones((7, 5)))\n\n        with self.assertRaisesRegexp(ValueError, ""is not an input layer""):\n            network.output({\'sigmoid-2\': x_test})\n\n    def test_if_layer_in_the_graph(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(2),\n        )\n        final_layer = layers.Sigmoid(1)\n        self.assertNotIn(final_layer, network)\n\n        network_2 = layers.join(network, final_layer)\n        self.assertIn(final_layer, network_2)\n\n    def test_graph_length(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(3),\n        )\n        self.assertEqual(2, len(network))\n\n        network_2 = layers.join(\n            network,\n            layers.parallel(\n                layers.Relu(1),\n                layers.Relu(2),\n            ),\n        )\n        self.assertEqual(2, len(network))\n        self.assertEqual(4, len(network_2))\n\n    def test_graph_predictions(self):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5),\n            layers.Relu(3),\n        )\n\n        input = np.random.random((100, 10))\n        output = network.predict(input, verbose=False)\n        self.assertEqual(output.shape, (100, 3))\n\n        output = network.predict(input, batch_size=10, verbose=False)\n        self.assertEqual(output.shape, (100, 3))\n\n        with self.assertRaisesRegexp(TypeError, ""Unknown arguments""):\n            network.predict(input, batchsize=10)\n'"
tests/layers/graph/test_visualizations.py,0,"b""import mock\nimport tempfile\nimport graphviz\n\nfrom neupy import layers\n\nfrom base import BaseTestCase\n\n\nclass MockedDigraph(graphviz.Digraph):\n    events = []\n\n    def node(self, node_id, name):\n        self.events.append('add_node')\n        return super(MockedDigraph, self).node(node_id, name)\n\n    def edge(self, node1_id, node2_id, *args, **kwargs):\n        self.events.append('add_edge')\n        return super(MockedDigraph, self).edge(\n            node1_id, node2_id, *args, **kwargs)\n\n    def render(self, filepath, *args, **kwargs):\n        self.events.append(filepath)\n        self.events.append('render')\n\n\n@mock.patch('graphviz.Digraph', side_effect=MockedDigraph)\nclass GraphVisualizationTestCase(BaseTestCase):\n    def setUp(self):\n        super(GraphVisualizationTestCase, self).setUp()\n        MockedDigraph.events = self.events = []\n\n    def test_basic_visualization(self, _):\n        network = layers.join(\n            layers.Input(10),\n            layers.Relu(5),\n            layers.parallel(\n                layers.Relu(2),\n                layers.Relu(3),\n            )\n        )\n        network.show()\n\n        # 4 layers + 2 outputs\n        self.assertEqual(6, self.events.count('add_node'))\n\n        # 3 between layers + 2 between layers and extra output nodes\n        self.assertEqual(5, self.events.count('add_edge'))\n\n        # Always has to be called at the end\n        self.assertEqual(1, self.events.count('render'))\n        self.assertEqual(self.events[-1], 'render')\n\n        filepath = tempfile.mktemp()\n        network.show(filepath)\n        self.assertEqual(self.events[-2], filepath)\n"""
examples/mlp/imdb_review_classification/src/__init__.py,0,b''
examples/mlp/imdb_review_classification/src/preprocessing.py,0,"b'from nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import FunctionTransformer\n\n\nclass CustomTransformerMixin(TransformerMixin):\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n\nclass VoidFunctionTransformer(FunctionTransformer):\n    def transform(self, X, y=None):\n        super(VoidFunctionTransformer, self).transform(X)\n        return X\n\n\nclass TokenizeText(CustomTransformerMixin):\n    def __init__(self, ignore_stopwords=True, *args, **kwargs):\n        self.ignore_stopwords = ignore_stopwords\n        super(TokenizeText, self).__init__(*args, **kwargs)\n\n    def transform(self, texts, y=None):\n        tokenizer = RegexpTokenizer(r\'[a-z]+|\\d+\')\n\n        tokenized_texts = []\n        stoplist = []\n\n        if self.ignore_stopwords:\n            stoplist = stopwords.words(\'english\')\n\n        for text in texts:\n            tokenized_text = []\n            for word in tokenizer.tokenize(text.lower()):\n                if word not in stoplist:\n                    tokenized_text.append(word.strip())\n\n            tokenized_texts.append(tokenized_text)\n        return tokenized_texts\n\n\nclass IgnoreUnknownWords(CustomTransformerMixin):\n    def __init__(self, dictionary, *args, **kwargs):\n        self.dictionary = dictionary\n\n        if len(dictionary) < 0:\n            raise ValueError(""Dictionary is empty"")\n\n        super(IgnoreUnknownWords, self).__init__(*args, **kwargs)\n\n    def transform(self, texts, y=None):\n        dictionary = self.dictionary\n        cleaned_texts = []\n\n        for text in texts:\n            cleaned_text = [word for word in text if word in dictionary]\n            cleaned_texts.append(cleaned_text)\n\n        return cleaned_texts\n'"
examples/mlp/imdb_review_classification/src/utils.py,0,"b""import os\nimport logging\n\n\nCURRENT_DIR = os.path.abspath(os.path.dirname(__file__))\nPROJECT_DIR = os.path.abspath(os.path.join(CURRENT_DIR, '..'))\nDATA_DIR = os.path.join(PROJECT_DIR, 'data')\n\nREVIEWS_FILE = os.path.join(DATA_DIR, 'reviews.csv')\n\nMODELS_DIR = os.path.join(PROJECT_DIR, 'models')\nWORD_EMBEDDING_NN = os.path.join(MODELS_DIR, 'word_embedding.model')\nNN_CLASSIFIER_MODEL = os.path.join(MODELS_DIR, 'nn_classifier.model')\n\n\nif not os.path.exists(MODELS_DIR):\n    os.mkdir(MODELS_DIR)\n\n\ndef create_logger(name, level=logging.INFO):\n    formatter = logging.Formatter(\n        fmt='%(asctime)-15s : %(levelname)s : %(message)s'\n    )\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    logger.addHandler(handler)\n\n    return logger\n"""
examples/mlp/imdb_review_classification/src/word_embedding_nn.py,0,"b'import numpy as np\nfrom gensim import models\n\nfrom .utils import create_logger\n\n\nlogger = create_logger(__name__)\n\n\nclass WordEmbeddingNN(models.Word2Vec):\n    def train(self, data, n_epochs=100):\n        train_method = super(WordEmbeddingNN, self).train\n        logger.info(""Start training network over {} epochs"".format(n_epochs))\n\n        for epoch in range(n_epochs):\n            logger.info(""Training epoch {} / {}"".format(epoch + 1, n_epochs))\n            train_method(data)\n\n    def fit(self, X, y, n_epochs=100):\n        """"""\n        Function overrides `train` method. This trick adds\n        scikit-learn compatibility.\n        """"""\n        self.train(X, n_epochs)\n        return self\n\n    def fit_transform(self):\n        pass\n\n    def transform(self, documents):\n        document_vectors = []\n        for document in documents:\n            document_vector = self[document].mean(axis=0)\n            document_vectors.append(document_vector)\n        return np.array(document_vectors)\n'"
