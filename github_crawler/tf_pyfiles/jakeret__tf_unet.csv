file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\nimport os\nfrom setuptools import find_packages\n\ntry:\n    from setuptools import setup\nexcept ImportError:\n    from distutils.core import setup\n\n\nreadme = open(\'README.rst\').read()\nhistory = open(\'HISTORY.rst\').read().replace(\'.. :changelog:\', \'\')\n\nrequires = [] #during runtime\ntests_require=[\'pytest>=2.3\'] #for testing\n\nPACKAGE_PATH = os.path.abspath(os.path.join(__file__, os.pardir))\n\nsetup(\n    name=\'tf_unet\',\n    version=\'0.1.2\',\n    description=\'Unet TensorFlow implementation\',\n    long_description=readme + \'\\n\\n\' + history,\n    author=\'Joel Akeret\',\n    url=\'https://github.com/jakeret/tf_unet\',\n    packages=find_packages(PACKAGE_PATH, ""test""),\n    package_dir={\'tf_unet\': \'tf_unet\'},\n    include_package_data=True,\n    install_requires=requires,\n    license=\'GPLv3\',\n    zip_safe=False,\n    keywords=\'tf_unet\',\n    classifiers=[\n        \'Development Status :: 2 - Pre-Alpha\',\n        ""Intended Audience :: Science/Research"",\n        \'Intended Audience :: Developers\',\n        ""License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)"",\n        \'Natural Language :: English\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n    ],\n    tests_require=tests_require,\n)\n'"
docs/check_sphinx.py,0,"b'\'\'\'\nCreated on Dec 2, 2013\n\n@author: jakeret\n\'\'\'\nimport py\nimport subprocess\ndef test_linkcheck(tmpdir):\n    doctrees = tmpdir.join(""doctrees"")\n    htmldir = tmpdir.join(""html"")\n    subprocess.check_call(\n        [""sphinx-build"", ""-blinkcheck"",\n          ""-d"", str(doctrees), ""."", str(htmldir)])\n    \ndef test_build_docs(tmpdir):\n    doctrees = tmpdir.join(""doctrees"")\n    htmldir = tmpdir.join(""html"")\n    subprocess.check_call([\n        ""sphinx-build"", ""-bhtml"",\n          ""-d"", str(doctrees), ""."", str(htmldir)])'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# complexity documentation build configuration file, created by\n# sphinx-quickstart on Tue Jul  9 22:26:36 2013.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys, os\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\n\ncwd = os.getcwd()\nparent = os.path.dirname(cwd)\nsys.path.insert(0, parent)\n\nimport mock\n \nMOCK_MODULES = [\'numpy\', \'tensorflow\']\nfor mod_name in MOCK_MODULES:\n    sys.modules[mod_name] = mock.Mock()\n\n\nimport tf_unet\n\n# -- General configuration -----------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named \'sphinx.ext.*\') or your custom ones.\nextensions = [\'sphinx.ext.autodoc\', \'sphinx.ext.coverage\', \'sphinx.ext.mathjax\', \'sphinx.ext.viewcode\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Tensorflow Unet\'\ncopyright = u\'2016, ETH Zurich, Institute for Astronomy\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = tf_unet.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = tf_unet.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ---------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n# html_theme = \'default\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'tf_unetdoc\'\n\n\n# -- Options for LaTeX output --------------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n  (\'index\', \'tf_unet.tex\', u\'Tensorflow Unet Documentation\',\n   u\'Joel Akeret\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output --------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\'index\', \'tf_unet\', u\'Tensorflow Unet Documentation\',\n     [u\'Joel Akeret\'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ------------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (\'index\', \'tf_unet\', u\'Tensorflow Unet Documentation\',\n   u\'Joel Akeret\', \'tf_unet\', \'One line description of project.\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\ntry:\n    import sphinx_eth_theme\n    html_theme = ""sphinx_eth_theme""\n    html_theme_path = [sphinx_eth_theme.get_html_theme_path()]\nexcept ImportError:\n    html_theme = \'default\''"
scripts/__init__.py,0,b''
scripts/launcher.py,0,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\'\'\'\nCreated on Jul 28, 2016\n\nauthor: jakeret\n\nTrains a tf_unet network to segment circles in noisy images.\n\'\'\'\n\nfrom __future__ import print_function, division, absolute_import, unicode_literals\nimport numpy as np\nfrom tf_unet import image_gen\nfrom tf_unet import unet\nfrom tf_unet import util\n\n\nif __name__ == \'__main__\':\n    np.random.seed(98765)\n\n    generator = image_gen.GrayScaleDataProvider(nx=572, ny=572, cnt=20, rectangles=False)\n    \n    net = unet.Unet(channels=generator.channels, \n                    n_class=generator.n_class, \n                    layers=3,\n                    features_root=16)\n    \n    trainer = unet.Trainer(net, optimizer=""momentum"", opt_kwargs=dict(momentum=0.2))\n    path = trainer.train(generator, ""./unet_trained"",\n                         training_iters=32,\n                         epochs=5,\n                         dropout=0.75,# probability to keep units\n                         display_step=2)\n     \n    x_test, y_test = generator(4)\n    prediction = net.predict(path, x_test)\n     \n    print(""Testing error rate: {:.2f}%"".format(unet.error_rate(prediction,\n                                                               util.crop_to_shape(y_test, prediction.shape))))\n'"
scripts/rfi_launcher.py,0,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\'\'\'\nCreated on Jul 28, 2016\n\nauthor: jakeret\n\nTrains a tf_unet network to segment radio frequency interference pattern.\nRequires data from the Bleien Observatory or a HIDE&SEEK simulation.\n\'\'\'\n\nfrom __future__ import print_function, division, absolute_import, unicode_literals\nimport glob\nimport click\nimport h5py\nimport numpy as np\n\nfrom tf_unet import unet\nfrom tf_unet import util\nfrom tf_unet.image_util import BaseDataProvider\n\n\n@click.command()\n@click.option(\'--data_root\', default=""./bleien_data"")\n@click.option(\'--output_path\', default=""./daint_unet_trained_rfi_bleien"")\n@click.option(\'--training_iters\', default=32)\n@click.option(\'--epochs\', default=100)\n@click.option(\'--restore\', default=False)\n@click.option(\'--layers\', default=5)\n@click.option(\'--features_root\', default=64)\ndef launch(data_root, output_path, training_iters, epochs, restore, layers, features_root):\n    print(""Using data from: %s""%data_root)\n    data_provider = DataProvider(600, glob.glob(data_root+""/*""))\n    \n    net = unet.Unet(channels=data_provider.channels, \n                    n_class=data_provider.n_class, \n                    layers=layers, \n                    features_root=features_root,\n                    cost_kwargs=dict(regularizer=0.001),\n                    )\n    \n    path = output_path if restore else util.create_training_path(output_path)\n    trainer = unet.Trainer(net, optimizer=""momentum"", opt_kwargs=dict(momentum=0.2))\n    path = trainer.train(data_provider, path, \n                         training_iters=training_iters, \n                         epochs=epochs, \n                         dropout=0.5, \n                         display_step=2, \n                         restore=restore)\n     \n    x_test, y_test = data_provider(1)\n    prediction = net.predict(path, x_test)\n     \n    print(""Testing error rate: {:.2f}%"".format(unet.error_rate(prediction, util.crop_to_shape(y_test, prediction.shape))))\n    \n\nclass DataProvider(BaseDataProvider):\n    """"""\n    Extends the BaseDataProvider to randomly select the next\n    data chunk\n    """"""\n\n    channels = 1\n    n_class = 2\n\n    def __init__(self, nx, files, a_min=30, a_max=210):\n        super(DataProvider, self).__init__(a_min, a_max)\n        self.nx = nx\n        self.files = files\n\n        assert len(files) > 0, ""No training files""\n        print(""Number of files used: %s""%len(files))\n        self._cylce_file()\n\n    def _read_chunck(self):\n        with h5py.File(self.files[self.file_idx], ""r"") as fp:\n            nx = fp[""data""].shape[1]\n            idx = np.random.randint(0, nx - self.nx)\n\n            sl = slice(idx, (idx+self.nx))\n            data = fp[""data""][:, sl]\n            rfi = fp[""mask""][:, sl]\n        return data, rfi\n\n    def _next_data(self):\n        data, rfi = self._read_chunck()\n        nx = data.shape[1]\n        while nx < self.nx:\n            self._cylce_file()\n            data, rfi = self._read_chunck()\n            nx = data.shape[1]\n\n        return data, rfi\n\n    def _cylce_file(self):\n        self.file_idx = np.random.choice(len(self.files))\n\n\nif __name__ == \'__main__\':\n    launch()\n'"
scripts/ufig_launcher.py,0,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\'\'\'\nCreated on Jul 28, 2016\n\nauthor: jakeret\n\nTrains a tf_unet network to segment stars and galaxies in a wide field image.\nRequires data from a UFIG simulation.\n\'\'\'\n\nfrom __future__ import print_function, division, absolute_import, unicode_literals\nimport click\nimport numpy as np\n\nfrom scipy.ndimage import gaussian_filter\nimport h5py\n\nfrom tf_unet import unet\nfrom tf_unet import util\nfrom tf_unet.image_util import BaseDataProvider\n\n\n@click.command()\n@click.option(\'--data_root\', default=""./ufig_images/1.h5"")\n@click.option(\'--output_path\', default=""./unet_trained_ufig"")\n@click.option(\'--training_iters\', default=20)\n@click.option(\'--epochs\', default=10)\n@click.option(\'--restore\', default=False)\n@click.option(\'--layers\', default=3)\n@click.option(\'--features_root\', default=16)\ndef launch(data_root, output_path, training_iters, epochs, restore, layers, features_root):\n    data_provider = DataProvider(572, data_root)\n    \n    data, label = data_provider(1)\n    weights = None#(1/3) / (label.sum(axis=2).sum(axis=1).sum(axis=0) / data.size)\n    \n    net = unet.Unet(channels=data_provider.channels, \n                    n_class=data_provider.n_class, \n                    layers=layers, \n                    features_root=features_root,\n                    cost_kwargs=dict(regularizer=0.001,\n                                     class_weights=weights),\n                    )\n    \n    path = output_path if restore else util.create_training_path(output_path)\n\n    trainer = unet.Trainer(net, optimizer=""adam"", opt_kwargs=dict(beta1=0.91))\n    path = trainer.train(data_provider, path, \n                         training_iters=training_iters, \n                         epochs=epochs, \n                         dropout=0.5, \n                         display_step=2, \n                         restore=restore)\n     \n    prediction = net.predict(path, data)\n     \n    print(""Testing error rate: {:.2f}%"".format(unet.error_rate(prediction, util.crop_to_shape(label, prediction.shape))))\n    \n\nclass DataProvider(BaseDataProvider):\n    """"""\n    Extends the BaseDataProvider to randomly select the next\n    chunk of the image and randomly applies transformations to the data\n    """"""\n\n    channels = 1\n    n_class = 3\n\n    def __init__(self, nx, path, a_min=0, a_max=20, sigma=1):\n        super(DataProvider, self).__init__(a_min, a_max)\n        self.nx = nx\n        self.path = path\n        self.sigma = sigma\n\n        self._load_data()\n\n    def _load_data(self):\n        with h5py.File(self.path, ""r"") as fp:\n            self.image = gaussian_filter(fp[""image""].value, self.sigma)\n            self.gal_map = fp[""segmaps/galaxy""].value\n            self.star_map = fp[""segmaps/star""].value\n\n    def _transpose_3d(self, a):\n        return np.stack([a[..., i].T for i in range(a.shape[2])], axis=2)\n\n    def _post_process(self, data, labels):\n        op = np.random.randint(0, 4)\n        if op == 0:\n            if np.random.randint(0, 2) == 0:\n                data, labels = self._transpose_3d(data[:,:,np.newaxis]), self._transpose_3d(labels)\n        else:\n            data, labels = np.rot90(data, op), np.rot90(labels, op)\n\n        return data, labels\n\n    def _next_data(self):\n        ix = np.random.randint(0, self.image.shape[0] - self.nx)\n        iy = np.random.randint(0, self.image.shape[1] - self.nx)\n\n        slx = slice(ix, ix+self.nx)\n        sly = slice(iy, iy+self.nx)\n\n        data = self.image[slx, sly]\n        gal_seg = self.gal_map[slx, sly]\n        star_seg = self.star_map[slx, sly]\n\n        labels = np.zeros((self.nx, self.nx, self.n_class), dtype=np.float32)\n        labels[..., 1] = np.clip(gal_seg, 0, 1)\n        labels[..., 2] = np.clip(star_seg, 0, 1)\n        labels[..., 0] = (1+np.clip(labels[...,1] + labels[...,2], 0, 1))%2\n\n        return data, labels\n\n\nif __name__ == \'__main__\':\n    launch()\n'"
scripts/ultrasound_launcher.py,0,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\'\'\'\nCreated on Jul 28, 2016\n\nauthor: jakeret\n\nTrains a tf_unet network to segment nerves in the Ultrasound Kaggle Dataset.\nRequires the Kaggle dataset.\n\'\'\'\n\nfrom __future__ import print_function, division, absolute_import, unicode_literals\nimport os\nimport click\nimport numpy as np\nfrom PIL import Image\n\n\nfrom tf_unet import unet\nfrom tf_unet import util\nfrom tf_unet.image_util import ImageDataProvider\n\nIMG_SIZE = (290, 210)\n\n\n@click.command()\n@click.option(\'--data_root\', default=""../../ultrasound/train"")\n@click.option(\'--output_path\', default=""./unet_trained_ultrasound"")\n@click.option(\'--training_iters\', default=20)\n@click.option(\'--epochs\', default=100)\n@click.option(\'--restore\', default=False)\n@click.option(\'--layers\', default=3)\n@click.option(\'--features_root\', default=32)\ndef launch(data_root, output_path, training_iters, epochs, restore, layers, features_root):\n    print(""Using data from: %s""%data_root)\n\n    if not os.path.exists(data_root):\n        raise IOError(""Kaggle Ultrasound Dataset not found"")\n\n    data_provider = DataProvider(search_path=data_root + ""/*.tif"",\n                                 mean=100,\n                                 std=56)\n\n    net = unet.Unet(channels=data_provider.channels, \n                    n_class=data_provider.n_class, \n                    layers=layers, \n                    features_root=features_root,\n                    #cost=""dice_coefficient"",\n                    )\n    \n    path = output_path if restore else util.create_training_path(output_path)\n\n    trainer = unet.Trainer(net, batch_size=1, norm_grads=False, optimizer=""adam"")\n    path = trainer.train(data_provider, path, \n                         training_iters=training_iters, \n                         epochs=epochs, \n                         dropout=0.5, \n                         display_step=2, \n                         restore=restore)\n     \n    x_test, y_test = data_provider(1)\n    prediction = net.predict(path, x_test)\n     \n    print(""Testing error rate: {:.2f}%"".format(unet.error_rate(prediction, util.crop_to_shape(y_test, prediction.shape))))\n    \n\nclass DataProvider(ImageDataProvider):\n    """"""\n    Extends the default ImageDataProvider to randomly select the next\n    image and ensures that only data sets are used where the mask is not empty.\n    The data then gets mean and std adjusted\n    """"""\n\n    def __init__(self, mean, std, *args, **kwargs):\n        super(DataProvider, self).__init__(*args, **kwargs)\n        self.mean = mean\n        self.std = std\n\n    def _next_data(self):\n        data, mask = super(DataProvider, self)._next_data()\n        while mask.sum() == 0:\n            self._cylce_file()\n            data, mask = super(DataProvider, self)._next_data()\n\n        return data, mask\n\n    def _process_data(self, data):\n        data -= self.mean\n        data /= self.std\n\n        return data\n\n    def _load_file(self, path, dtype=np.float32):\n        image = Image.open(path)\n        return np.array(image.resize(IMG_SIZE), dtype)\n\n    def _cylce_file(self):\n        self.file_idx = np.random.choice(len(self.data_files))\n\n\nif __name__ == \'__main__\':\n    launch()\n'"
tf_unet/__init__.py,0,"b""__author__ = 'Joel Akeret'\n__version__ = '0.1.2'\n__credits__ = 'ETH Zurich, Institute for Astronomy'\n"""
tf_unet/image_gen.py,0,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\'\'\'\nToy example, generates images at random that can be used for training\n\nCreated on Jul 28, 2016\n\nauthor: jakeret\n\'\'\'\nfrom __future__ import print_function, division, absolute_import, unicode_literals\n\nimport numpy as np\nfrom tf_unet.image_util import BaseDataProvider\n\nclass GrayScaleDataProvider(BaseDataProvider):\n    channels = 1\n    n_class = 2\n    \n    def __init__(self, nx, ny, **kwargs):\n        super(GrayScaleDataProvider, self).__init__()\n        self.nx = nx\n        self.ny = ny\n        self.kwargs = kwargs\n        rect = kwargs.get(""rectangles"", False)\n        if rect:\n            self.n_class=3\n        \n    def _next_data(self):\n        return create_image_and_label(self.nx, self.ny, **self.kwargs)\n\nclass RgbDataProvider(BaseDataProvider):\n    channels = 3\n    n_class = 2\n    \n    def __init__(self, nx, ny, **kwargs):\n        super(RgbDataProvider, self).__init__()\n        self.nx = nx\n        self.ny = ny\n        self.kwargs = kwargs\n        rect = kwargs.get(""rectangles"", False)\n        if rect:\n            self.n_class=3\n\n        \n    def _next_data(self):\n        data, label = create_image_and_label(self.nx, self.ny, **self.kwargs)\n        return to_rgb(data), label\n\ndef create_image_and_label(nx,ny, cnt = 10, r_min = 5, r_max = 50, border = 92, sigma = 20, rectangles=False):\n    \n    \n    image = np.ones((nx, ny, 1))\n    label = np.zeros((nx, ny, 3), dtype=np.bool)\n    mask = np.zeros((nx, ny), dtype=np.bool)\n    for _ in range(cnt):\n        a = np.random.randint(border, nx-border)\n        b = np.random.randint(border, ny-border)\n        r = np.random.randint(r_min, r_max)\n        h = np.random.randint(1,255)\n\n        y,x = np.ogrid[-a:nx-a, -b:ny-b]\n        m = x*x + y*y <= r*r\n        mask = np.logical_or(mask, m)\n\n        image[m] = h\n\n    label[mask, 1] = 1\n    \n    if rectangles:\n        mask = np.zeros((nx, ny), dtype=np.bool)\n        for _ in range(cnt//2):\n            a = np.random.randint(nx)\n            b = np.random.randint(ny)\n            r =  np.random.randint(r_min, r_max)\n            h = np.random.randint(1,255)\n    \n            m = np.zeros((nx, ny), dtype=np.bool)\n            m[a:a+r, b:b+r] = True\n            mask = np.logical_or(mask, m)\n            image[m] = h\n            \n        label[mask, 2] = 1\n        \n        label[..., 0] = ~(np.logical_or(label[...,1], label[...,2]))\n    \n    image += np.random.normal(scale=sigma, size=image.shape)\n    image -= np.amin(image)\n    image /= np.amax(image)\n    \n    if rectangles:\n        return image, label\n    else:\n        return image, label[..., 1]\n\n\n\n\ndef to_rgb(img):\n    img = img.reshape(img.shape[0], img.shape[1])\n    img[np.isnan(img)] = 0\n    img -= np.amin(img)\n    img /= np.amax(img)\n    blue = np.clip(4*(0.75-img), 0, 1)\n    red  = np.clip(4*(img-0.25), 0, 1)\n    green= np.clip(44*np.fabs(img-0.5)-1., 0, 1)\n    rgb = np.stack((red, green, blue), axis=2)\n    return rgb\n\n'"
tf_unet/image_util.py,0,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\nauthor: jakeret\n\'\'\'\nfrom __future__ import print_function, division, absolute_import, unicode_literals\n\nimport glob\nimport numpy as np\nfrom PIL import Image\n\n\nclass BaseDataProvider(object):\n    """"""\n    Abstract base class for DataProvider implementation. Subclasses have to\n    overwrite the `_next_data` method that load the next data and label array.\n    This implementation automatically clips the data with the given min/max and\n    normalizes the values to (0,1]. To change this behavoir the `_process_data`\n    method can be overwritten. To enable some post processing such as data\n    augmentation the `_post_process` method can be overwritten.\n\n    :param a_min: (optional) min value used for clipping\n    :param a_max: (optional) max value used for clipping\n\n    """"""\n\n    channels = 1\n    n_class = 2\n\n    def __init__(self, a_min=None, a_max=None):\n        self.a_min = a_min if a_min is not None else -np.inf\n        self.a_max = a_max if a_min is not None else np.inf\n\n    def _load_data_and_label(self):\n        data, label = self._next_data()\n\n        train_data = self._process_data(data)\n        labels = self._process_labels(label)\n\n        train_data, labels = self._post_process(train_data, labels)\n\n        nx = train_data.shape[1]\n        ny = train_data.shape[0]\n\n        return train_data.reshape(1, ny, nx, self.channels), labels.reshape(1, ny, nx, self.n_class),\n\n    def _process_labels(self, label):\n        if self.n_class == 2:\n            nx = label.shape[1]\n            ny = label.shape[0]\n            labels = np.zeros((ny, nx, self.n_class), dtype=np.float32)\n\n            # It is the responsibility of the child class to make sure that the label\n            # is a boolean array, but we a chech here just in case.\n            if label.dtype != \'bool\':\n                label = label.astype(np.bool)\n\n            labels[..., 1] = label\n            labels[..., 0] = ~label\n            return labels\n\n        return label\n\n    def _process_data(self, data):\n        # normalization\n        data = np.clip(np.fabs(data), self.a_min, self.a_max)\n        data -= np.amin(data)\n\n        if np.amax(data) != 0:\n            data /= np.amax(data)\n\n        return data\n\n    def _post_process(self, data, labels):\n        """"""\n        Post processing hook that can be used for data augmentation\n\n        :param data: the data array\n        :param labels: the label array\n        """"""\n        return data, labels\n\n    def __call__(self, n):\n        train_data, labels = self._load_data_and_label()\n        nx = train_data.shape[1]\n        ny = train_data.shape[2]\n\n        X = np.zeros((n, nx, ny, self.channels))\n        Y = np.zeros((n, nx, ny, self.n_class))\n\n        X[0] = train_data\n        Y[0] = labels\n        for i in range(1, n):\n            train_data, labels = self._load_data_and_label()\n            X[i] = train_data\n            Y[i] = labels\n\n        return X, Y\n\n\nclass SimpleDataProvider(BaseDataProvider):\n    """"""\n    A simple data provider for numpy arrays.\n    Assumes that the data and label are numpy array with the dimensions\n    data `[n, X, Y, channels]`, label `[n, X, Y, classes]`. Where\n    `n` is the number of images, `X`, `Y` the size of the image.\n\n    :param data: data numpy array. Shape=[n, X, Y, channels]\n    :param label: label numpy array. Shape=[n, X, Y, classes]\n    :param a_min: (optional) min value used for clipping\n    :param a_max: (optional) max value used for clipping\n\n    """"""\n\n    def __init__(self, data, label, a_min=None, a_max=None):\n        super(SimpleDataProvider, self).__init__(a_min, a_max)\n        self.data = data\n        self.label = label\n        self.file_count = data.shape[0]\n        self.n_class = label.shape[-1]\n        self.channels = data.shape[-1]\n\n    def _next_data(self):\n        idx = np.random.choice(self.file_count)\n        return self.data[idx], self.label[idx]\n\n\nclass ImageDataProvider(BaseDataProvider):\n    """"""\n    Generic data provider for images, supports gray scale and colored images.\n    Assumes that the data images and label images are stored in the same folder\n    and that the labels have a different file suffix\n    e.g. \'train/fish_1.tif\' and \'train/fish_1_mask.tif\'\n    Number of pixels in x and y of the images and masks should be even.\n\n    Usage:\n    data_provider = ImageDataProvider(""..fishes/train/*.tif"")\n\n    :param search_path: a glob search pattern to find all data and label images\n    :param a_min: (optional) min value used for clipping\n    :param a_max: (optional) max value used for clipping\n    :param data_suffix: suffix pattern for the data images. Default \'.tif\'\n    :param mask_suffix: suffix pattern for the label images. Default \'_mask.tif\'\n    :param shuffle_data: if the order of the loaded file path should be randomized. Default \'True\'\n\n    """"""\n\n    def __init__(self, search_path, a_min=None, a_max=None, data_suffix="".tif"", mask_suffix=\'_mask.tif\', shuffle_data=True):\n        super(ImageDataProvider, self).__init__(a_min, a_max)\n        self.data_suffix = data_suffix\n        self.mask_suffix = mask_suffix\n        self.file_idx = -1\n        self.shuffle_data = shuffle_data\n\n        self.data_files = self._find_data_files(search_path)\n\n        if self.shuffle_data:\n            np.random.shuffle(self.data_files)\n\n        assert len(self.data_files) > 0, ""No training files""\n        print(""Number of files used: %s"" % len(self.data_files))\n\n        image_path = self.data_files[0]\n        label_path = image_path.replace(self.data_suffix, self.mask_suffix)\n        img = self._load_file(image_path)\n        mask = self._load_file(label_path)\n        self.channels = 1 if len(img.shape) == 2 else img.shape[-1]\n        self.n_class = 2 if len(mask.shape) == 2 else mask.shape[-1]\n\n        print(""Number of channels: %s""%self.channels)\n        print(""Number of classes: %s""%self.n_class)\n\n    def _find_data_files(self, search_path):\n        all_files = glob.glob(search_path)\n        return [name for name in all_files if self.data_suffix in name and not self.mask_suffix in name]\n\n    def _load_file(self, path, dtype=np.float32):\n        return np.array(Image.open(path), dtype)\n\n    def _cylce_file(self):\n        self.file_idx += 1\n        if self.file_idx >= len(self.data_files):\n            self.file_idx = 0\n            if self.shuffle_data:\n                np.random.shuffle(self.data_files)\n\n    def _next_data(self):\n        self._cylce_file()\n        image_name = self.data_files[self.file_idx]\n        label_name = image_name.replace(self.data_suffix, self.mask_suffix)\n\n        img = self._load_file(image_name, np.float32)\n        label = self._load_file(label_name, np.bool)\n\n        return img,label\n'"
tf_unet/layers.py,24,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\'\'\'\nCreated on Aug 19, 2016\n\nauthor: jakeret\n\'\'\'\nfrom __future__ import print_function, division, absolute_import, unicode_literals\n\nimport tensorflow as tf\n\ndef weight_variable(shape, stddev=0.1, name=""weight""):\n    initial = tf.truncated_normal(shape, stddev=stddev)\n    return tf.Variable(initial, name=name)\n\ndef weight_variable_devonc(shape, stddev=0.1, name=""weight_devonc""):\n    return tf.Variable(tf.truncated_normal(shape, stddev=stddev), name=name)\n\ndef bias_variable(shape, name=""bias""):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial, name=name)\n\ndef conv2d(x, W, b, keep_prob_):\n    with tf.name_scope(""conv2d""):\n        conv_2d = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'VALID\')\n        conv_2d_b = tf.nn.bias_add(conv_2d, b)\n        return tf.nn.dropout(conv_2d_b, keep_prob_)\n\ndef deconv2d(x, W,stride):\n    with tf.name_scope(""deconv2d""):\n        x_shape = tf.shape(x)\n        output_shape = tf.stack([x_shape[0], x_shape[1]*2, x_shape[2]*2, x_shape[3]//2])\n        return tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding=\'VALID\', name=""conv2d_transpose"")\n\ndef max_pool(x,n):\n    return tf.nn.max_pool(x, ksize=[1, n, n, 1], strides=[1, n, n, 1], padding=\'VALID\')\n\ndef crop_and_concat(x1,x2):\n    with tf.name_scope(""crop_and_concat""):\n        x1_shape = tf.shape(x1)\n        x2_shape = tf.shape(x2)\n        # offsets for the top left corner of the crop\n        offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n        size = [-1, x2_shape[1], x2_shape[2], -1]\n        x1_crop = tf.slice(x1, offsets, size)\n        return tf.concat([x1_crop, x2], 3)\n\ndef pixel_wise_softmax(output_map):\n    with tf.name_scope(""pixel_wise_softmax""):\n        max_axis = tf.reduce_max(output_map, axis=3, keepdims=True)\n        exponential_map = tf.exp(output_map - max_axis)\n        normalize = tf.reduce_sum(exponential_map, axis=3, keepdims=True)\n        return exponential_map / normalize\n\ndef cross_entropy(y_,output_map):\n    return -tf.reduce_mean(y_*tf.log(tf.clip_by_value(output_map,1e-10,1.0)), name=""cross_entropy"")\n'"
tf_unet/unet.py,75,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\'\'\'\nCreated on Jul 28, 2016\n\nauthor: jakeret\n\'\'\'\nfrom __future__ import print_function, division, absolute_import, unicode_literals\n\nimport os\nimport shutil\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\n\nimport tensorflow as tf\n\nfrom tf_unet import util\nfrom tf_unet.layers import (weight_variable, weight_variable_devonc, bias_variable,\n                            conv2d, deconv2d, max_pool, crop_and_concat, pixel_wise_softmax,\n                            cross_entropy)\n\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(message)s\')\n\n\ndef create_conv_net(x, keep_prob, channels, n_class, layers=3, features_root=16, filter_size=3, pool_size=2,\n                    summaries=True):\n    """"""\n    Creates a new convolutional unet for the given parametrization.\n\n    :param x: input tensor, shape [?,nx,ny,channels]\n    :param keep_prob: dropout probability tensor\n    :param channels: number of channels in the input image\n    :param n_class: number of output labels\n    :param layers: number of layers in the net\n    :param features_root: number of features in the first layer\n    :param filter_size: size of the convolution filter\n    :param pool_size: size of the max pooling operation\n    :param summaries: Flag if summaries should be created\n    """"""\n\n    logging.info(\n        ""Layers {layers}, features {features}, filter size {filter_size}x{filter_size}, pool size: {pool_size}x{pool_size}"".format(\n            layers=layers,\n            features=features_root,\n            filter_size=filter_size,\n            pool_size=pool_size))\n\n    # Placeholder for the input image\n    with tf.name_scope(""preprocessing""):\n        nx = tf.shape(x)[1]\n        ny = tf.shape(x)[2]\n        x_image = tf.reshape(x, tf.stack([-1, nx, ny, channels]))\n        in_node = x_image\n        batch_size = tf.shape(x_image)[0]\n\n    weights = []\n    biases = []\n    convs = []\n    pools = OrderedDict()\n    deconv = OrderedDict()\n    dw_h_convs = OrderedDict()\n    up_h_convs = OrderedDict()\n\n    in_size = 1000\n    size = in_size\n    # down layers\n    for layer in range(0, layers):\n        with tf.name_scope(""down_conv_{}"".format(str(layer))):\n            features = 2 ** layer * features_root\n            stddev = np.sqrt(2 / (filter_size ** 2 * features))\n            if layer == 0:\n                w1 = weight_variable([filter_size, filter_size, channels, features], stddev, name=""w1"")\n            else:\n                w1 = weight_variable([filter_size, filter_size, features // 2, features], stddev, name=""w1"")\n\n            w2 = weight_variable([filter_size, filter_size, features, features], stddev, name=""w2"")\n            b1 = bias_variable([features], name=""b1"")\n            b2 = bias_variable([features], name=""b2"")\n\n            conv1 = conv2d(in_node, w1, b1, keep_prob)\n            tmp_h_conv = tf.nn.relu(conv1)\n            conv2 = conv2d(tmp_h_conv, w2, b2, keep_prob)\n            dw_h_convs[layer] = tf.nn.relu(conv2)\n\n            weights.append((w1, w2))\n            biases.append((b1, b2))\n            convs.append((conv1, conv2))\n\n            size -= 2 * 2 * (filter_size // 2) # valid conv\n            if layer < layers - 1:\n                pools[layer] = max_pool(dw_h_convs[layer], pool_size)\n                in_node = pools[layer]\n                size /= pool_size\n\n    in_node = dw_h_convs[layers - 1]\n\n    # up layers\n    for layer in range(layers - 2, -1, -1):\n        with tf.name_scope(""up_conv_{}"".format(str(layer))):\n            features = 2 ** (layer + 1) * features_root\n            stddev = np.sqrt(2 / (filter_size ** 2 * features))\n\n            wd = weight_variable_devonc([pool_size, pool_size, features // 2, features], stddev, name=""wd"")\n            bd = bias_variable([features // 2], name=""bd"")\n            h_deconv = tf.nn.relu(deconv2d(in_node, wd, pool_size) + bd)\n            h_deconv_concat = crop_and_concat(dw_h_convs[layer], h_deconv)\n            deconv[layer] = h_deconv_concat\n\n            w1 = weight_variable([filter_size, filter_size, features, features // 2], stddev, name=""w1"")\n            w2 = weight_variable([filter_size, filter_size, features // 2, features // 2], stddev, name=""w2"")\n            b1 = bias_variable([features // 2], name=""b1"")\n            b2 = bias_variable([features // 2], name=""b2"")\n\n            conv1 = conv2d(h_deconv_concat, w1, b1, keep_prob)\n            h_conv = tf.nn.relu(conv1)\n            conv2 = conv2d(h_conv, w2, b2, keep_prob)\n            in_node = tf.nn.relu(conv2)\n            up_h_convs[layer] = in_node\n\n            weights.append((w1, w2))\n            biases.append((b1, b2))\n            convs.append((conv1, conv2))\n\n            size *= pool_size\n            size -= 2 * 2 * (filter_size // 2) # valid conv\n\n    # Output Map\n    with tf.name_scope(""output_map""):\n        weight = weight_variable([1, 1, features_root, n_class], stddev)\n        bias = bias_variable([n_class], name=""bias"")\n        conv = conv2d(in_node, weight, bias, tf.constant(1.0))\n        output_map = tf.nn.relu(conv)\n        up_h_convs[""out""] = output_map\n\n    if summaries:\n        with tf.name_scope(""summaries""):\n            for i, (c1, c2) in enumerate(convs):\n                tf.summary.image(\'summary_conv_%02d_01\' % i, get_image_summary(c1))\n                tf.summary.image(\'summary_conv_%02d_02\' % i, get_image_summary(c2))\n\n            for k in pools.keys():\n                tf.summary.image(\'summary_pool_%02d\' % k, get_image_summary(pools[k]))\n\n            for k in deconv.keys():\n                tf.summary.image(\'summary_deconv_concat_%02d\' % k, get_image_summary(deconv[k]))\n\n            for k in dw_h_convs.keys():\n                tf.summary.histogram(""dw_convolution_%02d"" % k + \'/activations\', dw_h_convs[k])\n\n            for k in up_h_convs.keys():\n                tf.summary.histogram(""up_convolution_%s"" % k + \'/activations\', up_h_convs[k])\n\n    variables = []\n    for w1, w2 in weights:\n        variables.append(w1)\n        variables.append(w2)\n\n    for b1, b2 in biases:\n        variables.append(b1)\n        variables.append(b2)\n\n    return output_map, variables, int(in_size - size)\n\n\nclass Unet(object):\n    """"""\n    A unet implementation\n\n    :param channels: number of channels in the input image\n    :param n_class: number of output labels\n    :param cost: (optional) name of the cost function. Default is \'cross_entropy\'\n    :param cost_kwargs: (optional) kwargs passed to the cost function. See Unet._get_cost for more options\n    """"""\n\n    def __init__(self, channels, n_class, cost=""cross_entropy"", cost_kwargs={}, **kwargs):\n        tf.reset_default_graph()\n\n        self.n_class = n_class\n        self.summaries = kwargs.get(""summaries"", True)\n\n        self.x = tf.placeholder(""float"", shape=[None, None, None, channels], name=""x"")\n        self.y = tf.placeholder(""float"", shape=[None, None, None, n_class], name=""y"")\n        self.keep_prob = tf.placeholder(tf.float32, name=""dropout_probability"")  # dropout (keep probability)\n\n        logits, self.variables, self.offset = create_conv_net(self.x, self.keep_prob, channels, n_class, **kwargs)\n\n        self.cost = self._get_cost(logits, cost, cost_kwargs)\n\n        self.gradients_node = tf.gradients(self.cost, self.variables)\n\n        with tf.name_scope(""cross_entropy""):\n            self.cross_entropy = cross_entropy(tf.reshape(self.y, [-1, n_class]),\n                                               tf.reshape(pixel_wise_softmax(logits), [-1, n_class]))\n\n        with tf.name_scope(""results""):\n            self.predicter = pixel_wise_softmax(logits)\n            self.correct_pred = tf.equal(tf.argmax(self.predicter, 3), tf.argmax(self.y, 3))\n            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n\n    def _get_cost(self, logits, cost_name, cost_kwargs):\n        """"""\n        Constructs the cost function, either cross_entropy, weighted cross_entropy or dice_coefficient.\n        Optional arguments are:\n        class_weights: weights for the different classes in case of multi-class imbalance\n        regularizer: power of the L2 regularizers added to the loss function\n        """"""\n\n        with tf.name_scope(""cost""):\n            flat_logits = tf.reshape(logits, [-1, self.n_class])\n            flat_labels = tf.reshape(self.y, [-1, self.n_class])\n            if cost_name == ""cross_entropy"":\n                class_weights = cost_kwargs.pop(""class_weights"", None)\n\n                if class_weights is not None:\n                    class_weights = tf.constant(np.array(class_weights, dtype=np.float32))\n\n                    weight_map = tf.multiply(flat_labels, class_weights)\n                    weight_map = tf.reduce_sum(weight_map, axis=1)\n\n                    loss_map = tf.nn.softmax_cross_entropy_with_logits_v2(logits=flat_logits,\n                                                                          labels=flat_labels)\n                    weighted_loss = tf.multiply(loss_map, weight_map)\n\n                    loss = tf.reduce_mean(weighted_loss)\n\n                else:\n                    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=flat_logits,\n                                                                                     labels=flat_labels))\n            elif cost_name == ""dice_coefficient"":\n                eps = 1e-5\n                prediction = pixel_wise_softmax(logits)\n                intersection = tf.reduce_sum(prediction * self.y)\n                union = eps + tf.reduce_sum(prediction) + tf.reduce_sum(self.y)\n                loss = -(2 * intersection / (union))\n\n            else:\n                raise ValueError(""Unknown cost function: "" % cost_name)\n\n            regularizer = cost_kwargs.pop(""regularizer"", None)\n            if regularizer is not None:\n                regularizers = sum([tf.nn.l2_loss(variable) for variable in self.variables])\n                loss += (regularizer * regularizers)\n\n            return loss\n\n    def predict(self, model_path, x_test):\n        """"""\n        Uses the model to create a prediction for the given data\n\n        :param model_path: path to the model checkpoint to restore\n        :param x_test: Data to predict on. Shape [n, nx, ny, channels]\n        :returns prediction: The unet prediction Shape [n, px, py, labels] (px=nx-self.offset/2)\n        """"""\n\n        init = tf.global_variables_initializer()\n        with tf.Session() as sess:\n            # Initialize variables\n            sess.run(init)\n\n            # Restore model weights from previously saved model\n            self.restore(sess, model_path)\n\n            y_dummy = np.empty((x_test.shape[0], x_test.shape[1], x_test.shape[2], self.n_class))\n            prediction = sess.run(self.predicter, feed_dict={self.x: x_test, self.y: y_dummy, self.keep_prob: 1.})\n\n        return prediction\n\n    def save(self, sess, model_path):\n        """"""\n        Saves the current session to a checkpoint\n\n        :param sess: current session\n        :param model_path: path to file system location\n        """"""\n\n        saver = tf.train.Saver()\n        save_path = saver.save(sess, model_path)\n        return save_path\n\n    def restore(self, sess, model_path):\n        """"""\n        Restores a session from a checkpoint\n\n        :param sess: current session instance\n        :param model_path: path to file system checkpoint location\n        """"""\n\n        saver = tf.train.Saver()\n        saver.restore(sess, model_path)\n        logging.info(""Model restored from file: %s"" % model_path)\n\n\nclass Trainer(object):\n    """"""\n    Trains a unet instance\n\n    :param net: the unet instance to train\n    :param batch_size: size of training batch\n    :param verification_batch_size: size of verification batch\n    :param norm_grads: (optional) true if normalized gradients should be added to the summaries\n    :param optimizer: (optional) name of the optimizer to use (momentum or adam)\n    :param opt_kwargs: (optional) kwargs passed to the learning rate (momentum opt) and to the optimizer\n\n    """"""\n\n    def __init__(self, net, batch_size=1, verification_batch_size = 4, norm_grads=False, optimizer=""momentum"", opt_kwargs={}):\n        self.net = net\n        self.batch_size = batch_size\n        self.verification_batch_size = verification_batch_size\n        self.norm_grads = norm_grads\n        self.optimizer = optimizer\n        self.opt_kwargs = opt_kwargs\n\n    def _get_optimizer(self, training_iters, global_step):\n        if self.optimizer == ""momentum"":\n            learning_rate = self.opt_kwargs.pop(""learning_rate"", 0.2)\n            decay_rate = self.opt_kwargs.pop(""decay_rate"", 0.95)\n            momentum = self.opt_kwargs.pop(""momentum"", 0.2)\n\n            self.learning_rate_node = tf.train.exponential_decay(learning_rate=learning_rate,\n                                                                 global_step=global_step,\n                                                                 decay_steps=training_iters,\n                                                                 decay_rate=decay_rate,\n                                                                 staircase=True)\n\n            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate_node, momentum=momentum,\n                                                   **self.opt_kwargs).minimize(self.net.cost,\n                                                                               global_step=global_step)\n        elif self.optimizer == ""adam"":\n            learning_rate = self.opt_kwargs.pop(""learning_rate"", 0.001)\n            self.learning_rate_node = tf.Variable(learning_rate, name=""learning_rate"")\n\n            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_node,\n                                               **self.opt_kwargs).minimize(self.net.cost,\n                                                                           global_step=global_step)\n\n        return optimizer\n\n    def _initialize(self, training_iters, output_path, restore, prediction_path):\n        global_step = tf.Variable(0, name=""global_step"")\n\n        self.norm_gradients_node = tf.Variable(tf.constant(0.0, shape=[len(self.net.gradients_node)]), name=""norm_gradients"")\n\n        if self.net.summaries and self.norm_grads:\n            tf.summary.histogram(\'norm_grads\', self.norm_gradients_node)\n\n        tf.summary.scalar(\'loss\', self.net.cost)\n        tf.summary.scalar(\'cross_entropy\', self.net.cross_entropy)\n        tf.summary.scalar(\'accuracy\', self.net.accuracy)\n\n        self.optimizer = self._get_optimizer(training_iters, global_step)\n        tf.summary.scalar(\'learning_rate\', self.learning_rate_node)\n\n        self.summary_op = tf.summary.merge_all()\n        init = tf.global_variables_initializer()\n\n        self.prediction_path = prediction_path\n        abs_prediction_path = os.path.abspath(self.prediction_path)\n        output_path = os.path.abspath(output_path)\n\n        if not restore:\n            logging.info(""Removing \'{:}\'"".format(abs_prediction_path))\n            shutil.rmtree(abs_prediction_path, ignore_errors=True)\n            logging.info(""Removing \'{:}\'"".format(output_path))\n            shutil.rmtree(output_path, ignore_errors=True)\n\n        if not os.path.exists(abs_prediction_path):\n            logging.info(""Allocating \'{:}\'"".format(abs_prediction_path))\n            os.makedirs(abs_prediction_path)\n\n        if not os.path.exists(output_path):\n            logging.info(""Allocating \'{:}\'"".format(output_path))\n            os.makedirs(output_path)\n\n        return init\n\n    def train(self, data_provider, output_path, training_iters=10, epochs=100, dropout=0.75, display_step=1,\n              restore=False, write_graph=False, prediction_path=\'prediction\'):\n        """"""\n        Lauches the training process\n\n        :param data_provider: callable returning training and verification data\n        :param output_path: path where to store checkpoints\n        :param training_iters: number of training mini batch iteration\n        :param epochs: number of epochs\n        :param dropout: dropout probability\n        :param display_step: number of steps till outputting stats\n        :param restore: Flag if previous model should be restored\n        :param write_graph: Flag if the computation graph should be written as protobuf file to the output path\n        :param prediction_path: path where to save predictions on each epoch\n        """"""\n        save_path = os.path.join(output_path, ""model.ckpt"")\n        if epochs == 0:\n            return save_path\n\n        init = self._initialize(training_iters, output_path, restore, prediction_path)\n\n        with tf.Session() as sess:\n            if write_graph:\n                tf.train.write_graph(sess.graph_def, output_path, ""graph.pb"", False)\n\n            sess.run(init)\n\n            if restore:\n                ckpt = tf.train.get_checkpoint_state(output_path)\n                if ckpt and ckpt.model_checkpoint_path:\n                    self.net.restore(sess, ckpt.model_checkpoint_path)\n\n            test_x, test_y = data_provider(self.verification_batch_size)\n            pred_shape = self.store_prediction(sess, test_x, test_y, ""_init"")\n\n            summary_writer = tf.summary.FileWriter(output_path, graph=sess.graph)\n            logging.info(""Start optimization"")\n\n            avg_gradients = None\n            for epoch in range(epochs):\n                total_loss = 0\n                for step in range((epoch * training_iters), ((epoch + 1) * training_iters)):\n                    batch_x, batch_y = data_provider(self.batch_size)\n\n                    # Run optimization op (backprop)\n                    _, loss, lr, gradients = sess.run(\n                        (self.optimizer, self.net.cost, self.learning_rate_node, self.net.gradients_node),\n                        feed_dict={self.net.x: batch_x,\n                                   self.net.y: util.crop_to_shape(batch_y, pred_shape),\n                                   self.net.keep_prob: dropout})\n\n                    if self.net.summaries and self.norm_grads:\n                        avg_gradients = _update_avg_gradients(avg_gradients, gradients, step)\n                        norm_gradients = [np.linalg.norm(gradient) for gradient in avg_gradients]\n                        self.norm_gradients_node.assign(norm_gradients).eval()\n\n                    if step % display_step == 0:\n                        self.output_minibatch_stats(sess, summary_writer, step, batch_x,\n                                                    util.crop_to_shape(batch_y, pred_shape))\n\n                    total_loss += loss\n\n                self.output_epoch_stats(epoch, total_loss, training_iters, lr)\n                self.store_prediction(sess, test_x, test_y, ""epoch_%s"" % epoch)\n\n                save_path = self.net.save(sess, save_path)\n            logging.info(""Optimization Finished!"")\n\n            return save_path\n\n    def store_prediction(self, sess, batch_x, batch_y, name):\n        prediction = sess.run(self.net.predicter, feed_dict={self.net.x: batch_x,\n                                                             self.net.y: batch_y,\n                                                             self.net.keep_prob: 1.})\n        pred_shape = prediction.shape\n\n        loss = sess.run(self.net.cost, feed_dict={self.net.x: batch_x,\n                                                  self.net.y: util.crop_to_shape(batch_y, pred_shape),\n                                                  self.net.keep_prob: 1.})\n\n        logging.info(""Verification error= {:.1f}%, loss= {:.4f}"".format(error_rate(prediction,\n                                                                                   util.crop_to_shape(batch_y,\n                                                                                                      prediction.shape)),\n                                                                        loss))\n\n        img = util.combine_img_prediction(batch_x, batch_y, prediction)\n        util.save_image(img, ""%s/%s.jpg"" % (self.prediction_path, name))\n\n        return pred_shape\n\n    def output_epoch_stats(self, epoch, total_loss, training_iters, lr):\n        logging.info(\n            ""Epoch {:}, Average loss: {:.4f}, learning rate: {:.4f}"".format(epoch, (total_loss / training_iters), lr))\n\n    def output_minibatch_stats(self, sess, summary_writer, step, batch_x, batch_y):\n        # Calculate batch loss and accuracy\n        summary_str, loss, acc, predictions = sess.run([self.summary_op,\n                                                        self.net.cost,\n                                                        self.net.accuracy,\n                                                        self.net.predicter],\n                                                       feed_dict={self.net.x: batch_x,\n                                                                  self.net.y: batch_y,\n                                                                  self.net.keep_prob: 1.})\n        summary_writer.add_summary(summary_str, step)\n        summary_writer.flush()\n        logging.info(\n            ""Iter {:}, Minibatch Loss= {:.4f}, Training Accuracy= {:.4f}, Minibatch error= {:.1f}%"".format(step,\n                                                                                                           loss,\n                                                                                                           acc,\n                                                                                                           error_rate(\n                                                                                                               predictions,\n                                                                                                               batch_y)))\n\n\ndef _update_avg_gradients(avg_gradients, gradients, step):\n    if avg_gradients is None:\n        avg_gradients = [np.zeros_like(gradient) for gradient in gradients]\n    for i in range(len(gradients)):\n        avg_gradients[i] = (avg_gradients[i] * (1.0 - (1.0 / (step + 1)))) + (gradients[i] / (step + 1))\n\n    return avg_gradients\n\n\ndef error_rate(predictions, labels):\n    """"""\n    Return the error rate based on dense predictions and 1-hot labels.\n    """"""\n\n    return 100.0 - (\n            100.0 *\n            np.sum(np.argmax(predictions, 3) == np.argmax(labels, 3)) /\n            (predictions.shape[0] * predictions.shape[1] * predictions.shape[2]))\n\n\ndef get_image_summary(img, idx=0):\n    """"""\n    Make an image summary for 4d tensor image with index idx\n    """"""\n\n    V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))\n    V -= tf.reduce_min(V)\n    V /= tf.reduce_max(V)\n    V *= 255\n\n    img_w = tf.shape(img)[1]\n    img_h = tf.shape(img)[2]\n    V = tf.reshape(V, tf.stack((img_w, img_h, 1)))\n    V = tf.transpose(V, (2, 0, 1))\n    V = tf.reshape(V, tf.stack((-1, img_w, img_h, 1)))\n    return V\n'"
tf_unet/util.py,0,"b'# tf_unet is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# tf_unet is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with tf_unet.  If not, see <http://www.gnu.org/licenses/>.\n\n\n\'\'\'\nCreated on Aug 10, 2016\n\nauthor: jakeret\n\'\'\'\nfrom __future__ import print_function, division, absolute_import, unicode_literals\n\nimport os\n\nimport numpy as np\nfrom PIL import Image\n\ndef plot_prediction(x_test, y_test, prediction, save=False):\n    import matplotlib\n    import matplotlib.pyplot as plt\n\n    test_size = x_test.shape[0]\n    fig, ax = plt.subplots(test_size, 3, figsize=(12,12), sharey=True, sharex=True)\n\n    x_test = crop_to_shape(x_test, prediction.shape)\n    y_test = crop_to_shape(y_test, prediction.shape)\n\n    ax = np.atleast_2d(ax)\n    for i in range(test_size):\n        cax = ax[i, 0].imshow(x_test[i])\n        plt.colorbar(cax, ax=ax[i,0])\n        cax = ax[i, 1].imshow(y_test[i, ..., 1])\n        plt.colorbar(cax, ax=ax[i,1])\n        pred = prediction[i, ..., 1]\n        pred -= np.amin(pred)\n        pred /= np.amax(pred)\n        cax = ax[i, 2].imshow(pred)\n        plt.colorbar(cax, ax=ax[i,2])\n        if i==0:\n            ax[i, 0].set_title(""x"")\n            ax[i, 1].set_title(""y"")\n            ax[i, 2].set_title(""pred"")\n    fig.tight_layout()\n\n    if save:\n        fig.savefig(save)\n    else:\n        fig.show()\n        plt.show()\n\ndef to_rgb(img):\n    """"""\n    Converts the given array into a RGB image. If the number of channels is not\n    3 the array is tiled such that it has 3 channels. Finally, the values are\n    rescaled to [0,255)\n\n    :param img: the array to convert [nx, ny, channels]\n\n    :returns img: the rgb image [nx, ny, 3]\n    """"""\n    img = np.atleast_3d(img)\n    channels = img.shape[2]\n    if channels < 3:\n        img = np.tile(img, 3)\n\n    img[np.isnan(img)] = 0\n    img -= np.amin(img)\n    if np.amax(img) != 0:\n        img /= np.amax(img)\n\n    img *= 255\n    return img\n\ndef crop_to_shape(data, shape):\n    """"""\n    Crops the array to the given image shape by removing the border (expects a tensor of shape [batches, nx, ny, channels].\n\n    :param data: the array to crop\n    :param shape: the target shape\n    """"""\n    diff_nx = (data.shape[1] - shape[1])\n    diff_ny = (data.shape[2] - shape[2])\n\n    offset_nx_left = diff_nx // 2\n    offset_nx_right = diff_nx - offset_nx_left\n    offset_ny_left = diff_ny // 2\n    offset_ny_right = diff_ny - offset_ny_left\n\n    cropped = data[:, offset_nx_left:(-offset_nx_right), offset_ny_left:(-offset_ny_right)]\n\n    assert cropped.shape[1] == shape[1]\n    assert cropped.shape[2] == shape[2]\n    return cropped\n\ndef expand_to_shape(data, shape, border=0):\n    """"""\n    Expands the array to the given image shape by padding it with a border (expects a tensor of shape [batches, nx, ny, channels].\n\n    :param data: the array to expand\n    :param shape: the target shape\n    """"""\n    diff_nx = shape[1] - data.shape[1]\n    diff_ny = shape[2] - data.shape[2]\n\n    offset_nx_left = diff_nx // 2\n    offset_nx_right = diff_nx - offset_nx_left\n    offset_ny_left = diff_ny // 2\n    offset_ny_right = diff_ny - offset_ny_left\n\n    expanded = np.full(shape, border, dtype=np.float32)\n    expanded[:, offset_nx_left:(-offset_nx_right), offset_ny_left:(-offset_ny_right)] = data\n\n    return expanded\n\ndef combine_img_prediction(data, gt, pred):\n    """"""\n    Combines the data, grouth thruth and the prediction into one rgb image\n\n    :param data: the data tensor\n    :param gt: the ground thruth tensor\n    :param pred: the prediction tensor\n\n    :returns img: the concatenated rgb image\n    """"""\n    ny = pred.shape[2]\n    ch = data.shape[3]\n    img = np.concatenate((to_rgb(crop_to_shape(data, pred.shape).reshape(-1, ny, ch)),\n                          to_rgb(crop_to_shape(gt[..., 1], pred.shape).reshape(-1, ny, 1)),\n                          to_rgb(pred[..., 1].reshape(-1, ny, 1))), axis=1)\n    return img\n\ndef save_image(img, path):\n    """"""\n    Writes the image to disk\n\n    :param img: the rgb image to save\n    :param path: the target path\n    """"""\n    Image.fromarray(img.round().astype(np.uint8)).save(path, \'JPEG\', dpi=[300,300], quality=90)\n\n\ndef create_training_path(output_path, prefix=""run_""):\n    """"""\n    Enumerates a new path using the prefix under the given output_path\n    :param output_path: the root path\n    :param prefix: (optional) defaults to `run_`\n    :return: the generated path as string in form `output_path`/`prefix_` + `<number>`\n    """"""\n    idx = 0\n    path = os.path.join(output_path, ""{:}{:03d}"".format(prefix, idx))\n    while os.path.exists(path):\n        idx += 1\n        path = os.path.join(output_path, ""{:}{:03d}"".format(prefix, idx))\n    return path\n'"
