file_path,api_count,code
dl_examples/MNIST/keras-mnist/mnist-keras.py,0,"b""import numpy as np\nnp.random.seed(1337)  \nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import RMSprop\n\n# \xe4\xb8\x8b\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n(X_train, y_train), (X_test, y_test) = mnist.load_data('./mnist.npz') # \xe4\xb8\x8b\xe8\xbd\xbd\xe4\xb8\x8d\xe6\x9d\xa5\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x89\x8b\xe5\x8a\xa8\xe4\xb8\x8b\xe8\xbd\xbd\xe4\xb8\x8b\xe6\x9d\xa5\xe5\x90\x8e\xe7\xbc\x80npz\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xb8\x8b\xe8\xbd\xbd\xe5\x9c\xb0\xe5\x9d\x80\xef\xbc\x9ahttps://s3.amazonaws.com/img-datasets/mnist.npz\n\n# \xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe5\xa4\x84\xe7\x90\x86\nX_train = X_train.reshape(X_train.shape[0], -1) / 255. \nX_test = X_test.reshape(X_test.shape[0], -1) / 255.  \ny_train = np_utils.to_categorical(y_train, num_classes=10)\ny_test = np_utils.to_categorical(y_test, num_classes=10)\n\n# \xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8model.add()\xef\xbc\x8c\xe7\x94\xa8\xe4\xbb\xa5\xe4\xb8\x8b\xe6\x96\xb9\xe5\xbc\x8f\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9e\x84\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\nmodel = Sequential([\n    Dense(400, input_dim=784),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax'),\n])\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\nrmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer=rmsprop,\n              loss='categorical_crossentropy',\n              metrics=['accuracy']) # metrics\xe8\xb5\x8b\xe5\x80\xbc\xe4\xb8\xba'accuracy'\xef\xbc\x8c\xe4\xbc\x9a\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe8\xbe\x93\xe5\x87\xba\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n\n# \xe8\xbf\x99\xe6\xac\xa1\xe6\x88\x91\xe4\xbb\xac\xe7\x94\xa8fit()\xe6\x9d\xa5\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe8\xb7\xaf\nprint('Training ------------')\nmodel.fit(X_train, y_train, epochs=4, batch_size=32)\n\nprint('\\nTesting ------------')\n# \xe8\xaf\x84\xe4\xbb\xb7\xe8\xae\xad\xe7\xbb\x83\xe5\x87\xba\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\nloss, accuracy = model.evaluate(X_test, y_test)\n\nprint('test loss: ', loss)\nprint('test accuracy: ', accuracy)"""
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week03-src/3-1非线性回归.py,14,"b""\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# In[6]:\n\n#\xe4\xbd\xbf\xe7\x94\xa8numpy\xe7\x94\x9f\xe6\x88\x90200\xe4\xb8\xaa\xe9\x9a\x8f\xe6\x9c\xba\xe7\x82\xb9\nx_data = np.linspace(-0.5,0.5,200)[:,np.newaxis]\nnoise = np.random.normal(0,0.02,x_data.shape)\ny_data = np.square(x_data) + noise\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32,[None,1])\ny = tf.placeholder(tf.float32,[None,1])\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe9\x97\xb4\xe5\xb1\x82\nWeights_L1 = tf.Variable(tf.random_normal([1,10]))\nbiases_L1 = tf.Variable(tf.zeros([1,10]))\nWx_plus_b_L1 = tf.matmul(x,Weights_L1) + biases_L1\nL1 = tf.nn.tanh(Wx_plus_b_L1)\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\nWeights_L2 = tf.Variable(tf.random_normal([10,1]))\nbiases_L2 = tf.Variable(tf.zeros([1,1]))\nWx_plus_b_L2 = tf.matmul(L1,Weights_L2) + biases_L2\nprediction = tf.nn.tanh(Wx_plus_b_L2)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\nloss = tf.reduce_mean(tf.square(y-prediction))\n#\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe8\xae\xad\xe7\xbb\x83\ntrain_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n\nwith tf.Session() as sess:\n    #\xe5\x8f\x98\xe9\x87\x8f\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n    sess.run(tf.global_variables_initializer())\n    for _ in range(2000):\n        sess.run(train_step,feed_dict={x:x_data,y:y_data})\n        \n    #\xe8\x8e\xb7\xe5\xbe\x97\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\n    prediction_value = sess.run(prediction,feed_dict={x:x_data})\n    #\xe7\x94\xbb\xe5\x9b\xbe\n    plt.figure()\n    plt.scatter(x_data,y_data)\n    plt.plot(x_data,prediction_value,'r-',lw=5)\n    plt.show()\n\n\n# In[ ]:\n\n\n\n"""
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week03-src/3-2MNIST数据集分类简单版本.py,11,"b'\n# coding: utf-8\n\n# In[2]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# In[3]:\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32,[None,784])\ny = tf.placeholder(tf.float32,[None,10])\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\nprediction = tf.nn.softmax(tf.matmul(x,W)+b)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\nloss = tf.reduce_mean(tf.square(y-prediction))\n#\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\ntrain_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(21):\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n        \n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n        print(""Iter "" + str(epoch) + "",Testing Accuracy "" + str(acc))\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week04-src/4-1交叉熵.py,12,"b'\n# coding: utf-8\n\n# In[ ]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32,[None,784])\ny = tf.placeholder(tf.float32,[None,10])\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\nprediction = tf.nn.softmax(tf.matmul(x,W)+b)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\n# loss = tf.reduce_mean(tf.square(y-prediction))\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n#\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\ntrain_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(21):\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n        \n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n        print(""Iter "" + str(epoch) + "",Testing Accuracy "" + str(acc))\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week04-src/4-2Dropout.py,25,"b'\n# coding: utf-8\n\n# In[3]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32,[None,784])\ny = tf.placeholder(tf.float32,[None,10])\nkeep_prob=tf.placeholder(tf.float32)\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nW1 = tf.Variable(tf.truncated_normal([784,2000],stddev=0.1))\nb1 = tf.Variable(tf.zeros([2000])+0.1)\nL1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\nL1_drop = tf.nn.dropout(L1,keep_prob) \n\nW2 = tf.Variable(tf.truncated_normal([2000,2000],stddev=0.1))\nb2 = tf.Variable(tf.zeros([2000])+0.1)\nL2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\nL2_drop = tf.nn.dropout(L2,keep_prob) \n\nW3 = tf.Variable(tf.truncated_normal([2000,1000],stddev=0.1))\nb3 = tf.Variable(tf.zeros([1000])+0.1)\nL3 = tf.nn.tanh(tf.matmul(L2_drop,W3)+b3)\nL3_drop = tf.nn.dropout(L3,keep_prob) \n\nW4 = tf.Variable(tf.truncated_normal([1000,10],stddev=0.1))\nb4 = tf.Variable(tf.zeros([10])+0.1)\nprediction = tf.nn.softmax(tf.matmul(L3_drop,W4)+b4)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\n# loss = tf.reduce_mean(tf.square(y-prediction))\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n#\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\ntrain_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(31):\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n        \n        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n        print(""Iter "" + str(epoch) + "",Testing Accuracy "" + str(test_acc) +"",Training Accuracy "" + str(train_acc))\n\n\n# In[ ]:\n\nIter 29,Testing Accuracy 0.9727,Training Accuracy 0.995655\nIter 30,Testing Accuracy 0.9722,Training Accuracy 0.995782\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week04-src/4-3优化器.py,13,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# In[2]:\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32,[None,784])\ny = tf.placeholder(tf.float32,[None,10])\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\nprediction = tf.nn.softmax(tf.matmul(x,W)+b)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\n# loss = tf.reduce_mean(tf.square(y-prediction))\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n#\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\n# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\ntrain_step = tf.train.AdamOptimizer(1e-2).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(21):\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n        \n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n        print(""Iter "" + str(epoch) + "",Testing Accuracy "" + str(acc))\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week05-src/5-1第四周作业.py,22,"b'\n# coding: utf-8\n\n# In[2]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32,[None,784])\ny = tf.placeholder(tf.float32,[None,10])\nkeep_prob=tf.placeholder(tf.float32)\nlr = tf.Variable(0.001, dtype=tf.float32)\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nW1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1))\nb1 = tf.Variable(tf.zeros([500])+0.1)\nL1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\nL1_drop = tf.nn.dropout(L1,keep_prob) \n\nW2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1))\nb2 = tf.Variable(tf.zeros([300])+0.1)\nL2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\nL2_drop = tf.nn.dropout(L2,keep_prob) \n\nW3 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1))\nb3 = tf.Variable(tf.zeros([10])+0.1)\nprediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)\n\n#\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n#\xe8\xae\xad\xe7\xbb\x83\ntrain_step = tf.train.AdamOptimizer(lr).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(51):\n        sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch)))\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n        \n        learning_rate = sess.run(lr)\n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n        print (""Iter "" + str(epoch) + "", Testing Accuracy= "" + str(acc) + "", Learning Rate= "" + str(learning_rate))\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week05-src/5-2tensorboard网络结构.py,25,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# In[2]:\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\x91\xbd\xe5\x90\x8d\xe7\xa9\xba\xe9\x97\xb4\nwith tf.name_scope(\'input\'):\n    #\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\n    x = tf.placeholder(tf.float32,[None,784],name=\'x-input\')\n    y = tf.placeholder(tf.float32,[None,10],name=\'y-input\')\n\n    \nwith tf.name_scope(\'layer\'):\n    #\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n    with tf.name_scope(\'wights\'):\n        W = tf.Variable(tf.zeros([784,10]),name=\'W\')\n    with tf.name_scope(\'biases\'):    \n        b = tf.Variable(tf.zeros([10]),name=\'b\')\n    with tf.name_scope(\'wx_plus_b\'):\n        wx_plus_b = tf.matmul(x,W) + b\n    with tf.name_scope(\'softmax\'):\n        prediction = tf.nn.softmax(wx_plus_b)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\n# loss = tf.reduce_mean(tf.square(y-prediction))\nwith tf.name_scope(\'loss\'):\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\nwith tf.name_scope(\'train\'):\n    #\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\n    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\nwith tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n        #\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\n        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n    with tf.name_scope(\'accuracy\'):\n        #\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(init)\n    writer = tf.summary.FileWriter(\'logs/\',sess.graph)\n    for epoch in range(1):\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n        \n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n        print(""Iter "" + str(epoch) + "",Testing Accuracy "" + str(acc))\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week05-src/5-3tensorboard网络运行.py,37,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# In[2]:\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\x8f\x82\xe6\x95\xb0\xe6\xa6\x82\xe8\xa6\x81\ndef variable_summaries(var):\n    with tf.name_scope(\'summaries\'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar(\'mean\', mean)#\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\n        with tf.name_scope(\'stddev\'):\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar(\'stddev\', stddev)#\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\n        tf.summary.scalar(\'max\', tf.reduce_max(var))#\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\n        tf.summary.scalar(\'min\', tf.reduce_min(var))#\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\n        tf.summary.histogram(\'histogram\', var)#\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\n\n#\xe5\x91\xbd\xe5\x90\x8d\xe7\xa9\xba\xe9\x97\xb4\nwith tf.name_scope(\'input\'):\n    #\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\n    x = tf.placeholder(tf.float32,[None,784],name=\'x-input\')\n    y = tf.placeholder(tf.float32,[None,10],name=\'y-input\')\n    \nwith tf.name_scope(\'layer\'):\n    #\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n    with tf.name_scope(\'wights\'):\n        W = tf.Variable(tf.zeros([784,10]),name=\'W\')\n        variable_summaries(W)\n    with tf.name_scope(\'biases\'):    \n        b = tf.Variable(tf.zeros([10]),name=\'b\')\n        variable_summaries(b)\n    with tf.name_scope(\'wx_plus_b\'):\n        wx_plus_b = tf.matmul(x,W) + b\n    with tf.name_scope(\'softmax\'):\n        prediction = tf.nn.softmax(wx_plus_b)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\n# loss = tf.reduce_mean(tf.square(y-prediction))\nwith tf.name_scope(\'loss\'):\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n    tf.summary.scalar(\'loss\',loss)\nwith tf.name_scope(\'train\'):\n    #\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\n    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\nwith tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n        #\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\n        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n    with tf.name_scope(\'accuracy\'):\n        #\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n        tf.summary.scalar(\'accuracy\',accuracy)\n        \n#\xe5\x90\x88\xe5\xb9\xb6\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84summary\nmerged = tf.summary.merge_all()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    writer = tf.summary.FileWriter(\'logs/\',sess.graph)\n    for epoch in range(51):\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            summary,_ = sess.run([merged,train_step],feed_dict={x:batch_xs,y:batch_ys})\n            \n        writer.add_summary(summary,epoch)\n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n        print(""Iter "" + str(epoch) + "",Testing Accuracy "" + str(acc))\n\n\n# In[ ]:\n\n# for i in range(2001):\n#     #m\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1100\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\n#     batch_xs,batch_ys = mnist.train.next_batch(100)\n#     summary,_ = sess.run([merged,train_step],feed_dict={x:batch_xs,y:batch_ys})\n#     writer.add_summary(summary,i)\n#     if i%500 == 0:\n#         print(sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week05-src/5-4tensorboard可视化.py,46,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\n\n# In[2]:\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data/"",one_hot=True)\n#\xe8\xbf\x90\xe8\xa1\x8c\xe6\xac\xa1\xe6\x95\xb0\nmax_steps = 1001\n#\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe9\x87\x8f\nimage_num = 3000\n#\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\nDIR = ""D:/Tensorflow/""\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xbc\x9a\xe8\xaf\x9d\nsess = tf.Session()\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\nembedding = tf.Variable(tf.stack(mnist.test.images[:image_num]), trainable=False, name=\'embedding\')\n\n#\xe5\x8f\x82\xe6\x95\xb0\xe6\xa6\x82\xe8\xa6\x81\ndef variable_summaries(var):\n    with tf.name_scope(\'summaries\'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar(\'mean\', mean)#\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\n        with tf.name_scope(\'stddev\'):\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar(\'stddev\', stddev)#\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\n        tf.summary.scalar(\'max\', tf.reduce_max(var))#\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\n        tf.summary.scalar(\'min\', tf.reduce_min(var))#\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\n        tf.summary.histogram(\'histogram\', var)#\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\n\n#\xe5\x91\xbd\xe5\x90\x8d\xe7\xa9\xba\xe9\x97\xb4\nwith tf.name_scope(\'input\'):\n    #\xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84none\xe8\xa1\xa8\xe7\xa4\xba\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xbb\xbb\xe6\x84\x8f\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n    x = tf.placeholder(tf.float32,[None,784],name=\'x-input\')\n    #\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\n    y = tf.placeholder(tf.float32,[None,10],name=\'y-input\')\n\n#\xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\nwith tf.name_scope(\'input_reshape\'):\n    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n    tf.summary.image(\'input\', image_shaped_input, 10)\n\nwith tf.name_scope(\'layer\'):\n    #\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n    with tf.name_scope(\'weights\'):\n        W = tf.Variable(tf.zeros([784,10]),name=\'W\')\n        variable_summaries(W)\n    with tf.name_scope(\'biases\'):\n        b = tf.Variable(tf.zeros([10]),name=\'b\')\n        variable_summaries(b)\n    with tf.name_scope(\'wx_plus_b\'):\n        wx_plus_b = tf.matmul(x,W) + b\n    with tf.name_scope(\'softmax\'):    \n        prediction = tf.nn.softmax(wx_plus_b)\n\nwith tf.name_scope(\'loss\'):\n    #\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n    tf.summary.scalar(\'loss\',loss)\nwith tf.name_scope(\'train\'):\n    #\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\n    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\nsess.run(tf.global_variables_initializer())\n\nwith tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n        #\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\n        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n    with tf.name_scope(\'accuracy\'):\n        #\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))#\xe6\x8a\x8acorrect_prediction\xe5\x8f\x98\xe4\xb8\xbafloat32\xe7\xb1\xbb\xe5\x9e\x8b\n        tf.summary.scalar(\'accuracy\',accuracy)\n\n#\xe4\xba\xa7\xe7\x94\x9fmetadata\xe6\x96\x87\xe4\xbb\xb6\nif tf.gfile.Exists(DIR + \'projector/projector/metadata.tsv\'):\n    tf.gfile.DeleteRecursively(DIR + \'projector/projector/metadata.tsv\')\nwith open(DIR + \'projector/projector/metadata.tsv\', \'w\') as f:\n    labels = sess.run(tf.argmax(mnist.test.labels[:],1))\n    for i in range(image_num):   \n        f.write(str(labels[i]) + \'\\n\')        \n        \n#\xe5\x90\x88\xe5\xb9\xb6\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84summary\nmerged = tf.summary.merge_all()   \n\n\nprojector_writer = tf.summary.FileWriter(DIR + \'projector/projector\',sess.graph)\nsaver = tf.train.Saver()\nconfig = projector.ProjectorConfig()\nembed = config.embeddings.add()\nembed.tensor_name = embedding.name\nembed.metadata_path = DIR + \'projector/projector/metadata.tsv\'\nembed.sprite.image_path = DIR + \'projector/data/mnist_10k_sprite.png\'\nembed.sprite.single_image_dim.extend([28,28])\nprojector.visualize_embeddings(projector_writer,config)\n\nfor i in range(max_steps):\n    #\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1100\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\n    batch_xs,batch_ys = mnist.train.next_batch(100)\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    summary,_ = sess.run([merged,train_step],feed_dict={x:batch_xs,y:batch_ys},options=run_options,run_metadata=run_metadata)\n    projector_writer.add_run_metadata(run_metadata, \'step%03d\' % i)\n    projector_writer.add_summary(summary, i)\n    \n    if i%100 == 0:\n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n        print (""Iter "" + str(i) + "", Testing Accuracy= "" + str(acc))\n\nsaver.save(sess, DIR + \'projector/projector/a_model.ckpt\', global_step=max_steps)\nprojector_writer.close()\nsess.close()\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week06-src/6-1卷积神经网络应用于MNIST数据集分类.py,22,"b'import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot=True)\n\n# \xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n# \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe5\x80\xbc\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)  # \xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe6\x88\xaa\xe6\x96\xad\xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\n    return tf.Variable(initial)\n\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x81\x8f\xe7\xbd\xae\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\n# \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\ndef conv2d(x, W):\n    # x input tensor of shape \'[batch,in_height,in_width,in_channles]\'\n    # W filter / kernel tensor of shape [filter_height,filter_width,in_channels,out_channels]\n    # `strides[0] = strides[3] = 1`. strides[1]\xe4\xbb\xa3\xe8\xa1\xa8x\xe6\x96\xb9\xe5\x90\x91\xe7\x9a\x84\xe6\xad\xa5\xe9\x95\xbf\xef\xbc\x8cstrides[2]\xe4\xbb\xa3\xe8\xa1\xa8y\xe6\x96\xb9\xe5\x90\x91\xe7\x9a\x84\xe6\xad\xa5\xe9\x95\xbf\n    # padding: A `string` from: `""SAME"", ""VALID""`\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')  # 2d\xe7\x9a\x84\xe6\x84\x8f\xe6\x80\x9d\xe6\x98\xaf\xe4\xba\x8c\xe7\xbb\xb4\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c\n\n\n# \xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\ndef max_pool_2x2(x):\n    # ksize [1,x,y,1]\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32, [None, 784])  # 28*28\ny = tf.placeholder(tf.float32, [None, 10])\n\n# \xe6\x94\xb9\xe5\x8f\x98x\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe8\xbd\xac\xe4\xb8\xba4D\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f[batch, in_height, in_width, in_channels]`\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe5\x80\xbc\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\nW_conv1 = weight_variable([5, 5, 1, 32])  # 5*5\xe7\x9a\x84\xe9\x87\x87\xe6\xa0\xb7\xe7\xaa\x97\xe5\x8f\xa3\xef\xbc\x8c32\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xbb\x8e1\xe4\xb8\xaa\xe5\xb9\xb3\xe9\x9d\xa2\xe6\x8a\xbd\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\nb_conv1 = bias_variable([32])  # \xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\n\n# \xe6\x8a\x8ax_image\xe5\x92\x8c\xe6\x9d\x83\xe5\x80\xbc\xe5\x90\x91\xe9\x87\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\x86\x8d\xe5\x8a\xa0\xe4\xb8\x8a\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\xba\x94\xe7\x94\xa8\xe4\xba\x8erelu\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)  # \xe8\xbf\x9b\xe8\xa1\x8cmax-pooling\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe5\x80\xbc\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\nW_conv2 = weight_variable([5, 5, 32, 64])  # 5*5\xe7\x9a\x84\xe9\x87\x87\xe6\xa0\xb7\xe7\xaa\x97\xe5\x8f\xa3\xef\xbc\x8c64\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xbb\x8e32\xe4\xb8\xaa\xe5\xb9\xb3\xe9\x9d\xa2\xe6\x8a\xbd\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\nb_conv2 = bias_variable([64])  # \xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\n\n# \xe6\x8a\x8ah_pool1\xe5\x92\x8c\xe6\x9d\x83\xe5\x80\xbc\xe5\x90\x91\xe9\x87\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\x86\x8d\xe5\x8a\xa0\xe4\xb8\x8a\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\xba\x94\xe7\x94\xa8\xe4\xba\x8erelu\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)  # \xe8\xbf\x9b\xe8\xa1\x8cmax-pooling\n\n# 28*28\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x90\x8e\xe8\xbf\x98\xe6\x98\xaf28*28\xef\xbc\x88\xe6\x95\xb0\xe7\xbb\x84\xe5\x8f\x98\xe5\xb0\x8f\xe4\xba\x86\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\x8d\xe5\x8f\x98\xef\xbc\x89\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe6\xb1\xa0\xe5\x8c\x96\xe5\x90\x8e\xe5\x8f\x98\xe4\xb8\xba14*14\n# \xe7\xac\xac\xe4\xba\x8c\xe6\xac\xa1\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x90\x8e\xe4\xb8\xba14*14\xef\xbc\x88\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x94\xb9\xe5\x8f\x98\xe5\xb9\xb3\xe9\x9d\xa2\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x89\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe6\xac\xa1\xe6\xb1\xa0\xe5\x8c\x96\xe5\x90\x8e\xe5\x8f\x98\xe4\xb8\xba\xe4\xba\x867*7\n# \xe8\xbf\x9b\xe8\xbf\x87\xe4\xb8\x8a\xe9\x9d\xa2\xe6\x93\x8d\xe4\xbd\x9c\xe5\x90\x8e\xe5\xbe\x97\xe5\x88\xb064\xe5\xbc\xa07*7\xe7\x9a\x84\xe5\xb9\xb3\xe9\x9d\xa2\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe5\x80\xbc\nW_fc1 = weight_variable([7 * 7 * 64, 1024])  # \xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe6\x9c\x897*7*64\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xef\xbc\x8c\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe6\x9c\x891024\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\nb_fc1 = bias_variable([1024])  # 1024\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9\n\n# \xe6\x8a\x8a\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x822\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\x89\x81\xe5\xb9\xb3\xe5\x8c\x96\xe4\xb8\xba1\xe7\xbb\xb4\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n# \xe6\xb1\x82\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# keep_prob\xe7\x94\xa8\xe6\x9d\xa5\xe8\xa1\xa8\xe7\xa4\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\xa6\x82\xe7\x8e\x87\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\n# \xe8\xae\xa1\xe7\xae\x97\xe8\xbe\x93\xe5\x87\xba\nprediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n# \xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n\n# \xe4\xbd\xbf\xe7\x94\xa8AdamOptimizer\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n# \xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))  # argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n\n# \xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(21):\n        for batch in range(n_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.7})\n\n        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n        print(""Iter "" + str(epoch) + "", Testing Accuracy= "" + str(acc))'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week07-src/7-1第六周作业.py,70,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# In[2]:\n\nmnist = input_data.read_data_sets(\'MNIST_data\',one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\x8f\x82\xe6\x95\xb0\xe6\xa6\x82\xe8\xa6\x81\ndef variable_summaries(var):\n    with tf.name_scope(\'summaries\'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar(\'mean\', mean)#\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\n        with tf.name_scope(\'stddev\'):\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar(\'stddev\', stddev)#\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\n        tf.summary.scalar(\'max\', tf.reduce_max(var))#\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\n        tf.summary.scalar(\'min\', tf.reduce_min(var))#\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\n        tf.summary.histogram(\'histogram\', var)#\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe5\x80\xbc\ndef weight_variable(shape,name):\n    initial = tf.truncated_normal(shape,stddev=0.1)#\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe6\x88\xaa\xe6\x96\xad\xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\n    return tf.Variable(initial,name=name)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x81\x8f\xe7\xbd\xae\ndef bias_variable(shape,name):\n    initial = tf.constant(0.1,shape=shape)\n    return tf.Variable(initial,name=name)\n\n#\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\ndef conv2d(x,W):\n    #x input tensor of shape `[batch, in_height, in_width, in_channels]`\n    #W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n    #`strides[0] = strides[3] = 1`. strides[1]\xe4\xbb\xa3\xe8\xa1\xa8x\xe6\x96\xb9\xe5\x90\x91\xe7\x9a\x84\xe6\xad\xa5\xe9\x95\xbf\xef\xbc\x8cstrides[2]\xe4\xbb\xa3\xe8\xa1\xa8y\xe6\x96\xb9\xe5\x90\x91\xe7\x9a\x84\xe6\xad\xa5\xe9\x95\xbf\n    #padding: A `string` from: `""SAME"", ""VALID""`\n    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=\'SAME\')\n\n#\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\ndef max_pool_2x2(x):\n    #ksize [1,x,y,1]\n    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\'SAME\')\n\n#\xe5\x91\xbd\xe5\x90\x8d\xe7\xa9\xba\xe9\x97\xb4\nwith tf.name_scope(\'input\'):\n    #\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\n    x = tf.placeholder(tf.float32,[None,784],name=\'x-input\')\n    y = tf.placeholder(tf.float32,[None,10],name=\'y-input\')\n    with tf.name_scope(\'x_image\'):\n        #\xe6\x94\xb9\xe5\x8f\x98x\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xe8\xbd\xac\xe4\xb8\xba4D\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f[batch, in_height, in_width, in_channels]`\n        x_image = tf.reshape(x,[-1,28,28,1],name=\'x_image\')\n\n\nwith tf.name_scope(\'Conv1\'):\n    #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe5\x80\xbc\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\n    with tf.name_scope(\'W_conv1\'):\n        W_conv1 = weight_variable([5,5,1,32],name=\'W_conv1\')#5*5\xe7\x9a\x84\xe9\x87\x87\xe6\xa0\xb7\xe7\xaa\x97\xe5\x8f\xa3\xef\xbc\x8c32\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xbb\x8e1\xe4\xb8\xaa\xe5\xb9\xb3\xe9\x9d\xa2\xe6\x8a\xbd\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\n    with tf.name_scope(\'b_conv1\'):  \n        b_conv1 = bias_variable([32],name=\'b_conv1\')#\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\n\n    #\xe6\x8a\x8ax_image\xe5\x92\x8c\xe6\x9d\x83\xe5\x80\xbc\xe5\x90\x91\xe9\x87\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\x86\x8d\xe5\x8a\xa0\xe4\xb8\x8a\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\xba\x94\xe7\x94\xa8\xe4\xba\x8erelu\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n    with tf.name_scope(\'conv2d_1\'):\n        conv2d_1 = conv2d(x_image,W_conv1) + b_conv1\n    with tf.name_scope(\'relu\'):\n        h_conv1 = tf.nn.relu(conv2d_1)\n    with tf.name_scope(\'h_pool1\'):\n        h_pool1 = max_pool_2x2(h_conv1)#\xe8\xbf\x9b\xe8\xa1\x8cmax-pooling\n\nwith tf.name_scope(\'Conv2\'):\n    #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe5\x80\xbc\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\n    with tf.name_scope(\'W_conv2\'):\n        W_conv2 = weight_variable([5,5,32,64],name=\'W_conv2\')#5*5\xe7\x9a\x84\xe9\x87\x87\xe6\xa0\xb7\xe7\xaa\x97\xe5\x8f\xa3\xef\xbc\x8c64\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xbb\x8e32\xe4\xb8\xaa\xe5\xb9\xb3\xe9\x9d\xa2\xe6\x8a\xbd\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\n    with tf.name_scope(\'b_conv2\'):  \n        b_conv2 = bias_variable([64],name=\'b_conv2\')#\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\n\n    #\xe6\x8a\x8ah_pool1\xe5\x92\x8c\xe6\x9d\x83\xe5\x80\xbc\xe5\x90\x91\xe9\x87\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\x86\x8d\xe5\x8a\xa0\xe4\xb8\x8a\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\xba\x94\xe7\x94\xa8\xe4\xba\x8erelu\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n    with tf.name_scope(\'conv2d_2\'):\n        conv2d_2 = conv2d(h_pool1,W_conv2) + b_conv2\n    with tf.name_scope(\'relu\'):\n        h_conv2 = tf.nn.relu(conv2d_2)\n    with tf.name_scope(\'h_pool2\'):\n        h_pool2 = max_pool_2x2(h_conv2)#\xe8\xbf\x9b\xe8\xa1\x8cmax-pooling\n\n#28*28\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x90\x8e\xe8\xbf\x98\xe6\x98\xaf28*28\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe6\xb1\xa0\xe5\x8c\x96\xe5\x90\x8e\xe5\x8f\x98\xe4\xb8\xba14*14\n#\xe7\xac\xac\xe4\xba\x8c\xe6\xac\xa1\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x90\x8e\xe4\xb8\xba14*14\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe6\xac\xa1\xe6\xb1\xa0\xe5\x8c\x96\xe5\x90\x8e\xe5\x8f\x98\xe4\xb8\xba\xe4\xba\x867*7\n#\xe8\xbf\x9b\xe8\xbf\x87\xe4\xb8\x8a\xe9\x9d\xa2\xe6\x93\x8d\xe4\xbd\x9c\xe5\x90\x8e\xe5\xbe\x97\xe5\x88\xb064\xe5\xbc\xa07*7\xe7\x9a\x84\xe5\xb9\xb3\xe9\x9d\xa2\n\nwith tf.name_scope(\'fc1\'):\n    #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe5\x80\xbc\n    with tf.name_scope(\'W_fc1\'):\n        W_fc1 = weight_variable([7*7*64,1024],name=\'W_fc1\')#\xe4\xb8\x8a\xe4\xb8\x80\xe5\x9c\xba\xe6\x9c\x897*7*64\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xef\xbc\x8c\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe6\x9c\x891024\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\n    with tf.name_scope(\'b_fc1\'):\n        b_fc1 = bias_variable([1024],name=\'b_fc1\')#1024\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9\n\n    #\xe6\x8a\x8a\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x822\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\x89\x81\xe5\xb9\xb3\xe5\x8c\x96\xe4\xb8\xba1\xe7\xbb\xb4\n    with tf.name_scope(\'h_pool2_flat\'):\n        h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64],name=\'h_pool2_flat\')\n    #\xe6\xb1\x82\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n    with tf.name_scope(\'wx_plus_b1\'):\n        wx_plus_b1 = tf.matmul(h_pool2_flat,W_fc1) + b_fc1\n    with tf.name_scope(\'relu\'):\n        h_fc1 = tf.nn.relu(wx_plus_b1)\n\n    #keep_prob\xe7\x94\xa8\xe6\x9d\xa5\xe8\xa1\xa8\xe7\xa4\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\xa6\x82\xe7\x8e\x87\n    with tf.name_scope(\'keep_prob\'):\n        keep_prob = tf.placeholder(tf.float32,name=\'keep_prob\')\n    with tf.name_scope(\'h_fc1_drop\'):\n        h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob,name=\'h_fc1_drop\')\n\nwith tf.name_scope(\'fc2\'):\n    #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    with tf.name_scope(\'W_fc2\'):\n        W_fc2 = weight_variable([1024,10],name=\'W_fc2\')\n    with tf.name_scope(\'b_fc2\'):    \n        b_fc2 = bias_variable([10],name=\'b_fc2\')\n    with tf.name_scope(\'wx_plus_b2\'):\n        wx_plus_b2 = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n    with tf.name_scope(\'softmax\'):\n        #\xe8\xae\xa1\xe7\xae\x97\xe8\xbe\x93\xe5\x87\xba\n        prediction = tf.nn.softmax(wx_plus_b2)\n\n#\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\nwith tf.name_scope(\'cross_entropy\'):\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction),name=\'cross_entropy\')\n    tf.summary.scalar(\'cross_entropy\',cross_entropy)\n    \n#\xe4\xbd\xbf\xe7\x94\xa8AdamOptimizer\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\nwith tf.name_scope(\'train\'):\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\nwith tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n        #\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\n        correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n    with tf.name_scope(\'accuracy\'):\n        #\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n        tf.summary.scalar(\'accuracy\',accuracy)\n        \n#\xe5\x90\x88\xe5\xb9\xb6\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84summary\nmerged = tf.summary.merge_all()\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    train_writer = tf.summary.FileWriter(\'logs/train\',sess.graph)\n    test_writer = tf.summary.FileWriter(\'logs/test\',sess.graph)\n    for i in range(1001):\n        #\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n        batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n        sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.5})\n        #\xe8\xae\xb0\xe5\xbd\x95\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe8\xae\xa1\xe7\xae\x97\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n        summary = sess.run(merged,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n        train_writer.add_summary(summary,i)\n        #\xe8\xae\xb0\xe5\xbd\x95\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe8\xae\xa1\xe7\xae\x97\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n        batch_xs,batch_ys =  mnist.test.next_batch(batch_size)\n        summary = sess.run(merged,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n        test_writer.add_summary(summary,i)\n    \n        if i%100==0:\n            test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n            train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images[:10000],y:mnist.train.labels[:10000],keep_prob:1.0})\n            print (""Iter "" + str(i) + "", Testing Accuracy= "" + str(test_acc) + "", Training Accuracy= "" + str(train_acc))\n\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week07-src/7-2递归神经网络RNN.py,14,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# In[2]:\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data/"",one_hot=True)\n\n# \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe6\x98\xaf28*28\nn_inputs = 28 #\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe8\xa1\x8c\xef\xbc\x8c\xe4\xb8\x80\xe8\xa1\x8c\xe6\x9c\x8928\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\nmax_time = 28 #\xe4\xb8\x80\xe5\x85\xb128\xe8\xa1\x8c\nlstm_size = 100 #\xe9\x9a\x90\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\nn_classes = 10 # 10\xe4\xb8\xaa\xe5\x88\x86\xe7\xb1\xbb\nbatch_size = 50 #\xe6\xaf\x8f\xe6\x89\xb9\xe6\xac\xa150\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\nn_batch = mnist.train.num_examples // batch_size #\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\n\n#\xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84none\xe8\xa1\xa8\xe7\xa4\xba\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xbb\xbb\xe6\x84\x8f\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\nx = tf.placeholder(tf.float32,[None,784])\n#\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\ny = tf.placeholder(tf.float32,[None,10])\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe5\x80\xbc\nweights = tf.Variable(tf.truncated_normal([lstm_size, n_classes], stddev=0.1))\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\nbiases = tf.Variable(tf.constant(0.1, shape=[n_classes]))\n\n\n#\xe5\xae\x9a\xe4\xb9\x89RNN\xe7\xbd\x91\xe7\xbb\x9c\ndef RNN(X,weights,biases):\n    # inputs=[batch_size, max_time, n_inputs]\n    inputs = tf.reshape(X,[-1,max_time,n_inputs])\n    #\xe5\xae\x9a\xe4\xb9\x89LSTM\xe5\x9f\xba\xe6\x9c\xacCELL\n    lstm_cell = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(lstm_size)\n    # final_state[0]\xe6\x98\xafcell state\n    # final_state[1]\xe6\x98\xafhidden_state\n    outputs,final_state = tf.nn.dynamic_rnn(lstm_cell,inputs,dtype=tf.float32)\n    results = tf.nn.softmax(tf.matmul(final_state[1],weights) + biases)\n    return results\n    \n    \n#\xe8\xae\xa1\xe7\xae\x97RNN\xe7\x9a\x84\xe8\xbf\x94\xe5\x9b\x9e\xe7\xbb\x93\xe6\x9e\x9c\nprediction= RNN(x, weights, biases)  \n#\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n#\xe4\xbd\xbf\xe7\x94\xa8AdamOptimizer\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))#\xe6\x8a\x8acorrect_prediction\xe5\x8f\x98\xe4\xb8\xbafloat32\xe7\xb1\xbb\xe5\x9e\x8b\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(6):\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n        \n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n        print (""Iter "" + str(epoch) + "", Testing Accuracy= "" + str(acc))\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week08-src/8-1saver_save.py,13,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# In[2]:\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1100\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32,[None,784])\ny = tf.placeholder(tf.float32,[None,10])\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82784\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x8210\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\nprediction = tf.nn.softmax(tf.matmul(x,W)+b)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\n# loss = tf.reduce_mean(tf.square(y-prediction))\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n#\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\ntrain_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(11):\n        for batch in range(n_batch):\n            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n        \n        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n        print(""Iter "" + str(epoch) + "",Testing Accuracy "" + str(acc))\n    #\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n    saver.save(sess,\'net/my_net.ckpt\')\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week08-src/8-2saver_restore.py,13,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\n# In[2]:\n\n#\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nmnist = input_data.read_data_sets(""MNIST_data"",one_hot=True)\n\n#\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1100\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87\nbatch_size = 100\n#\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\nn_batch = mnist.train.num_examples // batch_size\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe4\xb8\xaaplaceholder\nx = tf.placeholder(tf.float32,[None,784])\ny = tf.placeholder(tf.float32,[None,10])\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82784\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x8210\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\nprediction = tf.nn.softmax(tf.matmul(x,W)+b)\n\n#\xe4\xba\x8c\xe6\xac\xa1\xe4\xbb\xa3\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\n# loss = tf.reduce_mean(tf.square(y-prediction))\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n#\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\ntrain_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\ninit = tf.global_variables_initializer()\n\n#\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe5\x88\x97\xe8\xa1\xa8\xe4\xb8\xad\ncorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n#\xe6\xb1\x82\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    print(sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n    saver.restore(sess,\'net/my_net.ckpt\')\n    print(sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week08-src/8-3下载google图像识别网络inception-v3并查看结构.py,5,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nimport os\nimport tarfile\nimport requests\n\n\n# In[2]:\n\n#inception\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\x8b\xe8\xbd\xbd\xe5\x9c\xb0\xe5\x9d\x80\ninception_pretrain_model_url = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\n\n#\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xad\x98\xe6\x94\xbe\xe5\x9c\xb0\xe5\x9d\x80\ninception_pretrain_model_dir = ""inception_model""\nif not os.path.exists(inception_pretrain_model_dir):\n    os.makedirs(inception_pretrain_model_dir)\n    \n#\xe8\x8e\xb7\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\nfilename = inception_pretrain_model_url.split(\'/\')[-1]\nfilepath = os.path.join(inception_pretrain_model_dir, filename)\n\n#\xe4\xb8\x8b\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\nif not os.path.exists(filepath):\n    print(""download: "", filename)\n    r = requests.get(inception_pretrain_model_url, stream=True)\n    with open(filepath, \'wb\') as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\nprint(""finish: "", filename)\n#\xe8\xa7\xa3\xe5\x8e\x8b\xe6\x96\x87\xe4\xbb\xb6\ntarfile.open(filepath, \'r:gz\').extractall(inception_pretrain_model_dir)\n \n#\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe5\xad\x98\xe6\x94\xbe\xe6\x96\x87\xe4\xbb\xb6\nlog_dir = \'inception_log\'\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\n#classify_image_graph_def.pb\xe4\xb8\xbagoogle\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\ninception_graph_def_file = os.path.join(inception_pretrain_model_dir, \'classify_image_graph_def.pb\')\nwith tf.Session() as sess:\n    #\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\xbe\xe6\x9d\xa5\xe5\xad\x98\xe6\x94\xbegoogle\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n    with tf.gfile.FastGFile(inception_graph_def_file, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        tf.import_graph_def(graph_def, name=\'\')\n    #\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\n    writer = tf.summary.FileWriter(log_dir, sess.graph)\n    writer.close()\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week08-src/8-4使用inception-v3做各种图像的识别.py,7,"b""\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport re\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\n# In[2]:\n\nclass NodeLookup(object):\n    def __init__(self):  \n        label_lookup_path = 'inception_model/imagenet_2012_challenge_label_map_proto.pbtxt'   \n        uid_lookup_path = 'inception_model/imagenet_synset_to_human_label_map.txt'\n        self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n\n    def load(self, label_lookup_path, uid_lookup_path):\n        # \xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\x86\xe7\xb1\xbb\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2n********\xe5\xaf\xb9\xe5\xba\x94\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\n        proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n        uid_to_human = {}\n        #\xe4\xb8\x80\xe8\xa1\x8c\xe4\xb8\x80\xe8\xa1\x8c\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n        for line in proto_as_ascii_lines :\n            #\xe5\x8e\xbb\xe6\x8e\x89\xe6\x8d\xa2\xe8\xa1\x8c\xe7\xac\xa6\n            line=line.strip('\\n')\n            #\xe6\x8c\x89\xe7\x85\xa7'\\t'\xe5\x88\x86\xe5\x89\xb2\n            parsed_items = line.split('\\t')\n            #\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe7\xbc\x96\xe5\x8f\xb7\n            uid = parsed_items[0]\n            #\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\n            human_string = parsed_items[1]\n            #\xe4\xbf\x9d\xe5\xad\x98\xe7\xbc\x96\xe5\x8f\xb7\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2n********\xe4\xb8\x8e\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\xe6\x98\xa0\xe5\xb0\x84\xe5\x85\xb3\xe7\xb3\xbb\n            uid_to_human[uid] = human_string\n\n        # \xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\x86\xe7\xb1\xbb\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2n********\xe5\xaf\xb9\xe5\xba\x94\xe5\x88\x86\xe7\xb1\xbb\xe7\xbc\x96\xe5\x8f\xb71-1000\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\n        proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n        node_id_to_uid = {}\n        for line in proto_as_ascii:\n            if line.startswith('  target_class:'):\n                #\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe7\xbc\x96\xe5\x8f\xb71-1000\n                target_class = int(line.split(': ')[1])\n            if line.startswith('  target_class_string:'):\n                #\xe8\x8e\xb7\xe5\x8f\x96\xe7\xbc\x96\xe5\x8f\xb7\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2n********\n                target_class_string = line.split(': ')[1]\n                #\xe4\xbf\x9d\xe5\xad\x98\xe5\x88\x86\xe7\xb1\xbb\xe7\xbc\x96\xe5\x8f\xb71-1000\xe4\xb8\x8e\xe7\xbc\x96\xe5\x8f\xb7\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2n********\xe6\x98\xa0\xe5\xb0\x84\xe5\x85\xb3\xe7\xb3\xbb\n                node_id_to_uid[target_class] = target_class_string[1:-2]\n\n        #\xe5\xbb\xba\xe7\xab\x8b\xe5\x88\x86\xe7\xb1\xbb\xe7\xbc\x96\xe5\x8f\xb71-1000\xe5\xaf\xb9\xe5\xba\x94\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\xe7\x9a\x84\xe6\x98\xa0\xe5\xb0\x84\xe5\x85\xb3\xe7\xb3\xbb\n        node_id_to_name = {}\n        for key, val in node_id_to_uid.items():\n            #\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\n            name = uid_to_human[val]\n            #\xe5\xbb\xba\xe7\xab\x8b\xe5\x88\x86\xe7\xb1\xbb\xe7\xbc\x96\xe5\x8f\xb71-1000\xe5\x88\xb0\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\xe7\x9a\x84\xe6\x98\xa0\xe5\xb0\x84\xe5\x85\xb3\xe7\xb3\xbb\n            node_id_to_name[key] = name\n        return node_id_to_name\n\n    #\xe4\xbc\xa0\xe5\x85\xa5\xe5\x88\x86\xe7\xb1\xbb\xe7\xbc\x96\xe5\x8f\xb71-1000\xe8\xbf\x94\xe5\x9b\x9e\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\n    def id_to_string(self, node_id):\n        if node_id not in self.node_lookup:\n            return ''\n        return self.node_lookup[node_id]\n\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\xbe\xe6\x9d\xa5\xe5\xad\x98\xe6\x94\xbegoogle\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\nwith tf.gfile.FastGFile('inception_model/classify_image_graph_def.pb', 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name='')\n\n\nwith tf.Session() as sess:\n    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n    #\xe9\x81\x8d\xe5\x8e\x86\xe7\x9b\xae\xe5\xbd\x95\n    for root,dirs,files in os.walk('images/'):\n        for file in files:\n            #\xe8\xbd\xbd\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\n            image_data = tf.gfile.FastGFile(os.path.join(root,file), 'rb').read()\n            predictions = sess.run(softmax_tensor,{'DecodeJpeg/contents:0': image_data})#\xe5\x9b\xbe\xe7\x89\x87\xe6\xa0\xbc\xe5\xbc\x8f\xe6\x98\xafjpg\xe6\xa0\xbc\xe5\xbc\x8f\n            predictions = np.squeeze(predictions)#\xe6\x8a\x8a\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbd\xac\xe4\xb8\xba1\xe7\xbb\xb4\xe6\x95\xb0\xe6\x8d\xae\n\n            #\xe6\x89\x93\xe5\x8d\xb0\xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\xe5\x8f\x8a\xe5\x90\x8d\xe7\xa7\xb0\n            image_path = os.path.join(root,file)\n            print(image_path)\n            #\xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\n            img=Image.open(image_path)\n            plt.imshow(img)\n            plt.axis('off')\n            plt.show()\n\n            #\xe6\x8e\x92\xe5\xba\x8f\n            top_k = predictions.argsort()[-5:][::-1]\n            node_lookup = NodeLookup()\n            for node_id in top_k:     \n                #\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\n                human_string = node_lookup.id_to_string(node_id)\n                #\xe8\x8e\xb7\xe5\x8f\x96\xe8\xaf\xa5\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\n                score = predictions[node_id]\n                print('%s (score = %.5f)' % (human_string, score))\n            print()\n\n\n# In[ ]:\n\n\n\n"""
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/10-1验证码生成.py,0,"b'\n# coding: utf-8\n\n# In[1]:\n\n# \xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe7\x94\x9f\xe6\x88\x90\xe5\xba\x93\nfrom captcha.image import ImageCaptcha  # pip install captcha\nimport numpy as np\nfrom PIL import Image\nimport random\nimport sys\n \nnumber = [\'0\',\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\']\n# alphabet = [\'a\',\'b\',\'c\',\'d\',\'e\',\'f\',\'g\',\'h\',\'i\',\'j\',\'k\',\'l\',\'m\',\'n\',\'o\',\'p\',\'q\',\'r\',\'s\',\'t\',\'u\',\'v\',\'w\',\'x\',\'y\',\'z\']\n# ALPHABET = [\'A\',\'B\',\'C\',\'D\',\'E\',\'F\',\'G\',\'H\',\'I\',\'J\',\'K\',\'L\',\'M\',\'N\',\'O\',\'P\',\'Q\',\'R\',\'S\',\'T\',\'U\',\'V\',\'W\',\'X\',\'Y\',\'Z\']\n\ndef random_captcha_text(char_set=number, captcha_size=4):\n    # \xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\x88\x97\xe8\xa1\xa8\n    captcha_text = []\n    for i in range(captcha_size):\n        #\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\n        c = random.choice(char_set)\n        #\xe5\x8a\xa0\xe5\x85\xa5\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\x88\x97\xe8\xa1\xa8\n        captcha_text.append(c)\n    return captcha_text\n \n# \xe7\x94\x9f\xe6\x88\x90\xe5\xad\x97\xe7\xac\xa6\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\ndef gen_captcha_text_and_image():\n    image = ImageCaptcha()\n    #\xe8\x8e\xb7\xe5\xbe\x97\xe9\x9a\x8f\xe6\x9c\xba\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\n    captcha_text = random_captcha_text()\n    #\xe6\x8a\x8a\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\x88\x97\xe8\xa1\xa8\xe8\xbd\xac\xe4\xb8\xba\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\n    captcha_text = \'\'.join(captcha_text)\n    #\xe7\x94\x9f\xe6\x88\x90\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\n    captcha = image.generate(captcha_text)\n    image.write(captcha_text, \'captcha/images/\' + captcha_text + \'.jpg\')  # \xe5\x86\x99\xe5\x88\xb0\xe6\x96\x87\xe4\xbb\xb6\n\n#\xe6\x95\xb0\xe9\x87\x8f\xe5\xb0\x91\xe4\xba\x8e10000\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe9\x87\x8d\xe5\x90\x8d\nnum = 10000\nif __name__ == \'__main__\':\n    for i in range(num):\n        gen_captcha_text_and_image()\n        sys.stdout.write(\'\\r>> Creating image %d/%d\' % (i+1, num))\n        sys.stdout.flush()\n    sys.stdout.write(\'\\n\')\n    sys.stdout.flush()\n                        \n    print(""\xe7\x94\x9f\xe6\x88\x90\xe5\xae\x8c\xe6\xaf\x95"")\n\n\n\n\n# \xe4\xba\xa4\xe6\x9b\xbf\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x9a\xe4\xb8\x8d\xe5\x90\x8c\xe4\xbb\xbb\xe5\x8a\xa1\xe6\x9c\x89\xe4\xb8\x8d\xe5\x90\x8c\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x8c\n# \xe8\x81\x94\xe5\x90\x88\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x9a\xe4\xb8\x8d\xe5\x90\x8c\xe4\xbb\xbb\xe5\x8a\xa1\xe6\x9c\x89\xe7\x9b\xb8\xe5\x85\xb3\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe3\x80\x82\xef\xbc\x88\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe8\xaf\x86\xe5\x88\xab\xef\xbc\x89\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/10-2生成tfrecord文件.py,6,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nimport os\nimport random\nimport math\nimport sys\nfrom PIL import Image\nimport numpy as np\n\n\n# In[2]:\n\n#\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x95\xb0\xe9\x87\x8f\n_NUM_TEST = 500\n\n#\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\n_RANDOM_SEED = 0\n\n#\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xb7\xaf\xe5\xbe\x84\nDATASET_DIR = ""D:/Tensorflow/captcha/images/""\n\n#tfrecord\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\nTFRECORD_DIR = ""D:/Tensorflow/captcha/""\n\n\n#\xe5\x88\xa4\xe6\x96\xadtfrecord\xe6\x96\x87\xe4\xbb\xb6\xe6\x98\xaf\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\ndef _dataset_exists(dataset_dir):\n    for split_name in [\'train\', \'test\']:\n        output_filename = os.path.join(dataset_dir,split_name + \'.tfrecords\')\n        if not tf.gfile.Exists(output_filename):\n            return False\n    return True\n\n#\xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89\xe9\xaa\x8c\xe8\xaf\x81\xe7\xa0\x81\xe5\x9b\xbe\xe7\x89\x87\ndef _get_filenames_and_classes(dataset_dir):\n    photo_filenames = []\n    for filename in os.listdir(dataset_dir):\n        #\xe8\x8e\xb7\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n        path = os.path.join(dataset_dir, filename)\n        photo_filenames.append(path)\n    return photo_filenames\n\ndef int64_feature(values):\n    if not isinstance(values, (tuple, list)):\n        values = [values]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\ndef bytes_feature(values):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\ndef image_to_tfexample(image_data, label0, label1, label2, label3):\n    #Abstract base class for protocol messages.\n    return tf.train.Example(features=tf.train.Features(feature={\n      \'image\': bytes_feature(image_data),\n      \'label0\': int64_feature(label0),\n      \'label1\': int64_feature(label1),\n      \'label2\': int64_feature(label2),\n      \'label3\': int64_feature(label3),\n    }))\n\n#\xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe4\xb8\xbaTFRecord\xe6\xa0\xbc\xe5\xbc\x8f\ndef _convert_dataset(split_name, filenames, dataset_dir):\n    assert split_name in [\'train\', \'test\']\n\n    with tf.Session() as sess:\n        #\xe5\xae\x9a\xe4\xb9\x89tfrecord\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84+\xe5\x90\x8d\xe5\xad\x97\n        output_filename = os.path.join(TFRECORD_DIR,split_name + \'.tfrecords\')\n        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n            for i,filename in enumerate(filenames):\n                try:\n                    sys.stdout.write(\'\\r>> Converting image %d/%d\' % (i+1, len(filenames)))\n                    sys.stdout.flush()\n\n                    #\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\n                    image_data = Image.open(filename)  \n                    #\xe6\xa0\xb9\xe6\x8d\xae\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84resize\n                    image_data = image_data.resize((224, 224))\n                    #\xe7\x81\xb0\xe5\xba\xa6\xe5\x8c\x96\n                    image_data = np.array(image_data.convert(\'L\'))\n                    #\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbabytes\n                    image_data = image_data.tobytes()              \n\n                    #\xe8\x8e\xb7\xe5\x8f\x96label\n                    labels = filename.split(\'/\')[-1][0:4]\n                    num_labels = []\n                    for j in range(4):\n                        num_labels.append(int(labels[j]))\n                                            \n                    #\xe7\x94\x9f\xe6\x88\x90protocol\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\n                    example = image_to_tfexample(image_data, num_labels[0], num_labels[1], num_labels[2], num_labels[3])\n                    tfrecord_writer.write(example.SerializeToString())\n                    \n                except IOError as e:\n                    print(\'Could not read:\',filename)\n                    print(\'Error:\',e)\n                    print(\'Skip it\\n\')\n    sys.stdout.write(\'\\n\')\n    sys.stdout.flush()\n\n#\xe5\x88\xa4\xe6\x96\xadtfrecord\xe6\x96\x87\xe4\xbb\xb6\xe6\x98\xaf\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\nif _dataset_exists(TFRECORD_DIR):\n    print(\'tfcecord\xe6\x96\x87\xe4\xbb\xb6\xe5\xb7\xb2\xe5\xad\x98\xe5\x9c\xa8\')\nelse:\n    #\xe8\x8e\xb7\xe5\xbe\x97\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\n    photo_filenames = _get_filenames_and_classes(DATASET_DIR)\n\n    #\xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x87\xe5\x88\x86\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86,\xe5\xb9\xb6\xe6\x89\x93\xe4\xb9\xb1\n    random.seed(_RANDOM_SEED)\n    random.shuffle(photo_filenames)\n    training_filenames = photo_filenames[_NUM_TEST:]\n    testing_filenames = photo_filenames[:_NUM_TEST]\n\n    #\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\n    _convert_dataset(\'train\', training_filenames, DATASET_DIR)\n    _convert_dataset(\'test\', testing_filenames, DATASET_DIR)\n    print(\'\xe7\x94\x9f\xe6\x88\x90tfcecord\xe6\x96\x87\xe4\xbb\xb6\')\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/10-3验证码识别.py,49,"b'\n# coding: utf-8\n\n# In[ ]:\n\nimport os\nimport tensorflow as tf \nfrom PIL import Image\nfrom nets import nets_factory\nimport numpy as np\n\n\n# In[2]:\n\n# \xe4\xb8\x8d\xe5\x90\x8c\xe5\xad\x97\xe7\xac\xa6\xe6\x95\xb0\xe9\x87\x8f\nCHAR_SET_LEN = 10\n# \xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xe5\xba\xa6\nIMAGE_HEIGHT = 60 \n# \xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xe5\xba\xa6\nIMAGE_WIDTH = 160  \n# \xe6\x89\xb9\xe6\xac\xa1\nBATCH_SIZE = 25\n# tfrecord\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\nTFRECORD_FILE = ""D:/Tensorflow/captcha/train.tfrecords""\n\n# placeholder\nx = tf.placeholder(tf.float32, [None, 224, 224])  \ny0 = tf.placeholder(tf.float32, [None]) \ny1 = tf.placeholder(tf.float32, [None]) \ny2 = tf.placeholder(tf.float32, [None]) \ny3 = tf.placeholder(tf.float32, [None])\n\n# \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\nlr = tf.Variable(0.003, dtype=tf.float32)\n\n# \xe4\xbb\x8etfrecord\xe8\xaf\xbb\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\ndef read_and_decode(filename):\n    # \xe6\xa0\xb9\xe6\x8d\xae\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe9\x98\x9f\xe5\x88\x97\n    filename_queue = tf.train.string_input_producer([filename])\n    reader = tf.TFRecordReader()\n    # \xe8\xbf\x94\xe5\x9b\x9e\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x92\x8c\xe6\x96\x87\xe4\xbb\xb6\n    _, serialized_example = reader.read(filename_queue)   \n    features = tf.parse_single_example(serialized_example,\n                                       features={\n                                           \'image\' : tf.FixedLenFeature([], tf.string),\n                                           \'label0\': tf.FixedLenFeature([], tf.int64),\n                                           \'label1\': tf.FixedLenFeature([], tf.int64),\n                                           \'label2\': tf.FixedLenFeature([], tf.int64),\n                                           \'label3\': tf.FixedLenFeature([], tf.int64),\n                                       })\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\n    image = tf.decode_raw(features[\'image\'], tf.uint8)\n    # tf.train.shuffle_batch\xe5\xbf\x85\xe9\xa1\xbb\xe7\xa1\xae\xe5\xae\x9ashape\n    image = tf.reshape(image, [224, 224])\n    # \xe5\x9b\xbe\xe7\x89\x87\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    # \xe8\x8e\xb7\xe5\x8f\x96label\n    label0 = tf.cast(features[\'label0\'], tf.int32)\n    label1 = tf.cast(features[\'label1\'], tf.int32)\n    label2 = tf.cast(features[\'label2\'], tf.int32)\n    label3 = tf.cast(features[\'label3\'], tf.int32)\n\n    return image, label0, label1, label2, label3\n\n\n# In[3]:\n\n# \xe8\x8e\xb7\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\nimage, label0, label1, label2, label3 = read_and_decode(TFRECORD_FILE)\n\n#\xe4\xbd\xbf\xe7\x94\xa8shuffle_batch\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x9a\x8f\xe6\x9c\xba\xe6\x89\x93\xe4\xb9\xb1\nimage_batch, label_batch0, label_batch1, label_batch2, label_batch3 = tf.train.shuffle_batch(\n        [image, label0, label1, label2, label3], batch_size = BATCH_SIZE,\n        capacity = 50000, min_after_dequeue=10000, num_threads=1)\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\ntrain_network_fn = nets_factory.get_network_fn(\n    \'alexnet_v2\',\n    num_classes=CHAR_SET_LEN,\n    weight_decay=0.0005,\n    is_training=True)\n \n    \nwith tf.Session() as sess:\n    # inputs: a tensor of size [batch_size, height, width, channels]\n    X = tf.reshape(x, [BATCH_SIZE, 224, 224, 1])\n    # \xe6\x95\xb0\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe5\xbe\x97\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\n    logits0,logits1,logits2,logits3,end_points = train_network_fn(X)\n    \n    # \xe6\x8a\x8a\xe6\xa0\x87\xe7\xad\xbe\xe8\xbd\xac\xe6\x88\x90one_hot\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\n    one_hot_labels0 = tf.one_hot(indices=tf.cast(y0, tf.int32), depth=CHAR_SET_LEN)\n    one_hot_labels1 = tf.one_hot(indices=tf.cast(y1, tf.int32), depth=CHAR_SET_LEN)\n    one_hot_labels2 = tf.one_hot(indices=tf.cast(y2, tf.int32), depth=CHAR_SET_LEN)\n    one_hot_labels3 = tf.one_hot(indices=tf.cast(y3, tf.int32), depth=CHAR_SET_LEN)\n    \n    # \xe8\xae\xa1\xe7\xae\x97loss\n    loss0 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits0,labels=one_hot_labels0)) \n    loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits1,labels=one_hot_labels1)) \n    loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits2,labels=one_hot_labels2)) \n    loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits3,labels=one_hot_labels3)) \n    # \xe8\xae\xa1\xe7\xae\x97\xe6\x80\xbb\xe7\x9a\x84loss\n    total_loss = (loss0+loss1+loss2+loss3)/4.0\n    # \xe4\xbc\x98\xe5\x8c\x96total_loss\n    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(total_loss) \n    \n    # \xe8\xae\xa1\xe7\xae\x97\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n    correct_prediction0 = tf.equal(tf.argmax(one_hot_labels0,1),tf.argmax(logits0,1))\n    accuracy0 = tf.reduce_mean(tf.cast(correct_prediction0,tf.float32))\n    \n    correct_prediction1 = tf.equal(tf.argmax(one_hot_labels1,1),tf.argmax(logits1,1))\n    accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1,tf.float32))\n    \n    correct_prediction2 = tf.equal(tf.argmax(one_hot_labels2,1),tf.argmax(logits2,1))\n    accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2,tf.float32))\n    \n    correct_prediction3 = tf.equal(tf.argmax(one_hot_labels3,1),tf.argmax(logits3,1))\n    accuracy3 = tf.reduce_mean(tf.cast(correct_prediction3,tf.float32)) \n    \n    # \xe7\x94\xa8\xe4\xba\x8e\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n    saver = tf.train.Saver()\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n    sess.run(tf.global_variables_initializer())\n    \n    # \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x8f\xe8\xb0\x83\xe5\x99\xa8\xef\xbc\x8c\xe7\xae\xa1\xe7\x90\x86\xe7\xba\xbf\xe7\xa8\x8b\n    coord = tf.train.Coordinator()\n    # \xe5\x90\xaf\xe5\x8a\xa8QueueRunner, \xe6\xad\xa4\xe6\x97\xb6\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe9\x98\x9f\xe5\x88\x97\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xbf\x9b\xe9\x98\x9f\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    for i in range(6001):\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\n        b_image, b_label0, b_label1 ,b_label2 ,b_label3 = sess.run([image_batch, label_batch0, label_batch1, label_batch2, label_batch3])\n        # \xe4\xbc\x98\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\n        sess.run(optimizer, feed_dict={x: b_image, y0:b_label0, y1: b_label1, y2: b_label2, y3: b_label3})  \n\n        # \xe6\xaf\x8f\xe8\xbf\xad\xe4\xbb\xa320\xe6\xac\xa1\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe6\xac\xa1loss\xe5\x92\x8c\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87  \n        if i % 20 == 0:  \n            # \xe6\xaf\x8f\xe8\xbf\xad\xe4\xbb\xa32000\xe6\xac\xa1\xe9\x99\x8d\xe4\xbd\x8e\xe4\xb8\x80\xe6\xac\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n            if i%2000 == 0:\n                sess.run(tf.assign(lr, lr/3))\n            acc0,acc1,acc2,acc3,loss_ = sess.run([accuracy0,accuracy1,accuracy2,accuracy3,total_loss],feed_dict={x: b_image,\n                                                                                                                y0: b_label0,\n                                                                                                                y1: b_label1,\n                                                                                                                y2: b_label2,\n                                                                                                                y3: b_label3})      \n            learning_rate = sess.run(lr)\n            print (""Iter:%d  Loss:%.3f  Accuracy:%.2f,%.2f,%.2f,%.2f  Learning_rate:%.4f"" % (i,loss_,acc0,acc1,acc2,acc3,learning_rate))\n             \n            # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n            # if acc0 > 0.90 and acc1 > 0.90 and acc2 > 0.90 and acc3 > 0.90: \n            if i==6000:\n                saver.save(sess, ""./captcha/models/crack_captcha.model"", global_step=i)  \n                break \n                \n    # \xe9\x80\x9a\xe7\x9f\xa5\xe5\x85\xb6\xe4\xbb\x96\xe7\xba\xbf\xe7\xa8\x8b\xe5\x85\xb3\xe9\x97\xad\n    coord.request_stop()\n    # \xe5\x85\xb6\xe4\xbb\x96\xe6\x89\x80\xe6\x9c\x89\xe7\xba\xbf\xe7\xa8\x8b\xe5\x85\xb3\xe9\x97\xad\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\x80\xe5\x87\xbd\xe6\x95\xb0\xe6\x89\x8d\xe8\x83\xbd\xe8\xbf\x94\xe5\x9b\x9e\n    coord.join(threads)\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/10-4captcha_test.py,35,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport os\nimport tensorflow as tf \nfrom PIL import Image\nfrom nets import nets_factory\nimport numpy as np\nimport matplotlib.pyplot as plt  \n\n\n# In[2]:\n\n# \xe4\xb8\x8d\xe5\x90\x8c\xe5\xad\x97\xe7\xac\xa6\xe6\x95\xb0\xe9\x87\x8f\nCHAR_SET_LEN = 10\n# \xe5\x9b\xbe\xe7\x89\x87\xe9\xab\x98\xe5\xba\xa6\nIMAGE_HEIGHT = 60 \n# \xe5\x9b\xbe\xe7\x89\x87\xe5\xae\xbd\xe5\xba\xa6\nIMAGE_WIDTH = 160  \n# \xe6\x89\xb9\xe6\xac\xa1\nBATCH_SIZE = 1\n# tfrecord\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\nTFRECORD_FILE = ""D:/Tensorflow/captcha/test.tfrecords""\n\n# placeholder\nx = tf.placeholder(tf.float32, [None, 224, 224])  \n\n# \xe4\xbb\x8etfrecord\xe8\xaf\xbb\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\ndef read_and_decode(filename):\n    # \xe6\xa0\xb9\xe6\x8d\xae\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe9\x98\x9f\xe5\x88\x97\n    filename_queue = tf.train.string_input_producer([filename])\n    reader = tf.TFRecordReader()\n    # \xe8\xbf\x94\xe5\x9b\x9e\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x92\x8c\xe6\x96\x87\xe4\xbb\xb6\n    _, serialized_example = reader.read(filename_queue)   \n    features = tf.parse_single_example(serialized_example,\n                                       features={\n                                           \'image\' : tf.FixedLenFeature([], tf.string),\n                                           \'label0\': tf.FixedLenFeature([], tf.int64),\n                                           \'label1\': tf.FixedLenFeature([], tf.int64),\n                                           \'label2\': tf.FixedLenFeature([], tf.int64),\n                                           \'label3\': tf.FixedLenFeature([], tf.int64),\n                                       })\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\n    image = tf.decode_raw(features[\'image\'], tf.uint8)\n    # \xe6\xb2\xa1\xe6\x9c\x89\xe7\xbb\x8f\xe8\xbf\x87\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\n    image_raw = tf.reshape(image, [224, 224])\n    # tf.train.shuffle_batch\xe5\xbf\x85\xe9\xa1\xbb\xe7\xa1\xae\xe5\xae\x9ashape\n    image = tf.reshape(image, [224, 224])\n    # \xe5\x9b\xbe\xe7\x89\x87\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    # \xe8\x8e\xb7\xe5\x8f\x96label\n    label0 = tf.cast(features[\'label0\'], tf.int32)\n    label1 = tf.cast(features[\'label1\'], tf.int32)\n    label2 = tf.cast(features[\'label2\'], tf.int32)\n    label3 = tf.cast(features[\'label3\'], tf.int32)\n\n    return image, image_raw, label0, label1, label2, label3\n\n\n# In[3]:\n\n# \xe8\x8e\xb7\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\nimage, image_raw, label0, label1, label2, label3 = read_and_decode(TFRECORD_FILE)\n\n#\xe4\xbd\xbf\xe7\x94\xa8shuffle_batch\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x9a\x8f\xe6\x9c\xba\xe6\x89\x93\xe4\xb9\xb1\nimage_batch, image_raw_batch, label_batch0, label_batch1, label_batch2, label_batch3 = tf.train.shuffle_batch(\n        [image, image_raw, label0, label1, label2, label3], batch_size = BATCH_SIZE,\n        capacity = 50000, min_after_dequeue=10000, num_threads=1)\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\ntrain_network_fn = nets_factory.get_network_fn(\n    \'alexnet_v2\',\n    num_classes=CHAR_SET_LEN,\n    weight_decay=0.0005,\n    is_training=False)\n\nwith tf.Session() as sess:\n    # inputs: a tensor of size [batch_size, height, width, channels]\n    X = tf.reshape(x, [BATCH_SIZE, 224, 224, 1])\n    # \xe6\x95\xb0\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe5\xbe\x97\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\n    logits0,logits1,logits2,logits3,end_points = train_network_fn(X)\n    \n    # \xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\n    predict0 = tf.reshape(logits0, [-1, CHAR_SET_LEN])  \n    predict0 = tf.argmax(predict0, 1)  \n\n    predict1 = tf.reshape(logits1, [-1, CHAR_SET_LEN])  \n    predict1 = tf.argmax(predict1, 1)  \n\n    predict2 = tf.reshape(logits2, [-1, CHAR_SET_LEN])  \n    predict2 = tf.argmax(predict2, 1)  \n\n    predict3 = tf.reshape(logits3, [-1, CHAR_SET_LEN])  \n    predict3 = tf.argmax(predict3, 1)  \n\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n    sess.run(tf.global_variables_initializer())\n    # \xe8\xbd\xbd\xe5\x85\xa5\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n    saver = tf.train.Saver()\n    saver.restore(sess,\'./captcha/models/crack_captcha.model-6000\')\n\n    # \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x8f\xe8\xb0\x83\xe5\x99\xa8\xef\xbc\x8c\xe7\xae\xa1\xe7\x90\x86\xe7\xba\xbf\xe7\xa8\x8b\n    coord = tf.train.Coordinator()\n    # \xe5\x90\xaf\xe5\x8a\xa8QueueRunner, \xe6\xad\xa4\xe6\x97\xb6\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe9\x98\x9f\xe5\x88\x97\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xbf\x9b\xe9\x98\x9f\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    for i in range(10):\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\n        b_image, b_image_raw, b_label0, b_label1 ,b_label2 ,b_label3 = sess.run([image_batch, \n                                                                    image_raw_batch, \n                                                                    label_batch0, \n                                                                    label_batch1, \n                                                                    label_batch2, \n                                                                    label_batch3])\n        # \xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\n        img=Image.fromarray(b_image_raw[0],\'L\')\n        plt.imshow(img)\n        plt.axis(\'off\')\n        plt.show()\n        # \xe6\x89\x93\xe5\x8d\xb0\xe6\xa0\x87\xe7\xad\xbe\n        print(\'label:\',b_label0, b_label1 ,b_label2 ,b_label3)\n        # \xe9\xa2\x84\xe6\xb5\x8b\n        label0,label1,label2,label3 = sess.run([predict0,predict1,predict2,predict3], feed_dict={x: b_image})\n        # \xe6\x89\x93\xe5\x8d\xb0\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\n        print(\'predict:\',label0,label1,label2,label3) \n                \n    # \xe9\x80\x9a\xe7\x9f\xa5\xe5\x85\xb6\xe4\xbb\x96\xe7\xba\xbf\xe7\xa8\x8b\xe5\x85\xb3\xe9\x97\xad\n    coord.request_stop()\n    # \xe5\x85\xb6\xe4\xbb\x96\xe6\x89\x80\xe6\x9c\x89\xe7\xba\xbf\xe7\xa8\x8b\xe5\x85\xb3\xe9\x97\xad\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\x80\xe5\x87\xbd\xe6\x95\xb0\xe6\x89\x8d\xe8\x83\xbd\xe8\xbf\x94\xe5\x9b\x9e\n    coord.join(threads)\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week12-src/12-1第十一周作业.py,33,"b'\n# coding: utf-8\n\n# In[1]:\n\n#! /usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport data_helpers\nfrom text_cnn import TextCNN\nfrom tensorflow.contrib import learn\nfrom six.moves import xrange\nimport pickle\n\n\n# In[2]:\n\n# Parameters\n# ==================================================\n\n# Data loading params\n# validation\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8d\xa0\xe6\xaf\x94\ntf.flags.DEFINE_float(""dev_sample_percentage"", .1, ""Percentage of the training data to use for validation"")\n# \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\ntf.flags.DEFINE_string(""positive_data_file"", ""./data/rt-polaritydata/rt-polarity.pos"", ""Data source for the positive data."")\n# \xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\ntf.flags.DEFINE_string(""negative_data_file"", ""./data/rt-polaritydata/rt-polarity.neg"", ""Data source for the negative data."")\n\n# Model Hyperparameters\n# \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6\ntf.flags.DEFINE_integer(""embedding_dim"", 128, ""Dimensionality of character embedding (default: 128)"")\n# \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\ntf.flags.DEFINE_string(""filter_sizes"", ""3,4,5"", ""Comma-separated filter sizes (default: \'3,4,5\')"")\n# \xe6\xaf\x8f\xe4\xb8\x80\xe7\xa7\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\xaa\xe6\x95\xb0\ntf.flags.DEFINE_integer(""num_filters"", 64, ""Number of filters per filter size (default: 128)"")\n# dropout\xe5\x8f\x82\xe6\x95\xb0\ntf.flags.DEFINE_float(""dropout_keep_prob"", 0.5, ""Dropout keep probability (default: 0.5)"")\n# l2\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0\ntf.flags.DEFINE_float(""l2_reg_lambda"", 0.0005, ""L2 regularization lambda (default: 0.0)"")\n\n# Training parameters\n# \xe6\x89\xb9\xe6\xac\xa1\xe5\xa4\xa7\xe5\xb0\x8f\ntf.flags.DEFINE_integer(""batch_size"", 64, ""Batch Size (default: 64)"")\n# \xe8\xbf\xad\xe4\xbb\xa3\xe5\x91\xa8\xe6\x9c\x9f\ntf.flags.DEFINE_integer(""num_epochs"", 6, ""Number of training epochs (default: 200)"")\n# \xe5\xa4\x9a\xe5\xb0\x91step\xe6\xb5\x8b\xe8\xaf\x95\xe4\xb8\x80\xe6\xac\xa1\ntf.flags.DEFINE_integer(""evaluate_every"", 100, ""Evaluate model on dev set after this many steps (default: 100)"")\n# \xe5\xa4\x9a\xe5\xb0\x91step\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\xe6\xa8\xa1\xe5\x9e\x8b\ntf.flags.DEFINE_integer(""checkpoint_every"", 100, ""Save model after this many steps (default: 100)"")\n# \xe6\x9c\x80\xe5\xa4\x9a\xe4\xbf\x9d\xe5\xad\x98\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\ntf.flags.DEFINE_integer(""num_checkpoints"", 5, ""Number of checkpoints to store (default: 5)"")\n# Misc Parameters\n# tensorFlow \xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x98\xe5\x9c\xa8\xe5\xb9\xb6\xe4\xb8\x94\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe8\xae\xbe\xe5\xa4\x87\xe6\x9d\xa5\xe8\xbf\x90\xe8\xa1\x8c operation\ntf.flags.DEFINE_boolean(""allow_soft_placement"", True, ""Allow device soft device placement"")\n# \xe8\x8e\xb7\xe5\x8f\x96\xe4\xbd\xa0\xe7\x9a\x84 operations \xe5\x92\x8c Tensor \xe8\xa2\xab\xe6\x8c\x87\xe6\xb4\xbe\xe5\x88\xb0\xe5\x93\xaa\xe4\xb8\xaa\xe8\xae\xbe\xe5\xa4\x87\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""Log placement of ops on devices"")\n\n# flags\xe8\xa7\xa3\xe6\x9e\x90\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\n\n# \xe6\x89\x93\xe5\x8d\xb0\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\nprint(""\\nParameters:"")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(""{}={}"".format(attr.upper(), value))\nprint("""")\n\n\n# In[3]:\n\n# Load data\nprint(""Loading data..."")\n# \xe8\xaf\xbb\xe5\x85\xa5\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe8\xbf\x87\xe5\x90\x8e\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\nx_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n\n# \xe4\xb8\x80\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe6\x9c\x80\xe5\xa4\x9a\xe7\x9a\x84\xe8\xaf\x8d\xe6\xb1\x87\xe6\x95\xb0\nmax_document_length = max([len(x.split("" "")) for x in x_text])\nprint(""max_document_length:"",max_document_length)\ntext = np.array([x.split("" "") for x in x_text])\nprint(""text:"",text[:5])\n\n\n# In[4]:\n\nwith open(\'w2v_dict.pickle\', \'rb\') as f:\n    w2v_dict = pickle.load(f)  \n\nx = []\nfor line in text:\n    line_len = len(line)\n    text2num = []\n    for i in xrange(max_document_length):\n        if(i < line_len):\n            try:\n                text2num.append(w2v_dict[line[i]]) # \xe6\x8a\x8a\xe8\xaf\x8d\xe8\xbd\xac\xe4\xb8\xba\xe6\x95\xb0\xe5\xad\x97\n            except:\n                text2num.append(0) # \xe6\xb2\xa1\xe6\x9c\x89\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe8\xaf\x8d\n        else:\n            text2num.append(0) # \xe5\xa1\xab\xe5\x85\x850\n    x.append(text2num)\nx = np.array(x)\n\n\n# In[5]:\n\nprint(""x_shape:"",x.shape)\nprint(""y_shape:"",y.shape)\n\n# Randomly shuffle data\nnp.random.seed(10)\nshuffle_indices = np.random.permutation(np.arange(len(y)))\nx_shuffled = x[shuffle_indices]\ny_shuffled = y[shuffle_indices]\n\n# Split train/test set\n# TODO: This is very crude, should use cross-validation\n# \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x87\xe5\x88\x86\xe4\xb8\xba\xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\ndev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\nx_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\ny_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\nprint(""Train/Dev split: {:d}/{:d}"".format(len(y_train), len(y_dev)))\n\nprint(""x:"",x_train[0:5])\nprint(""y:"",y_train[0:5])\n\n\n# In[6]:\n\n# Training\n# ==================================================\n\nwith tf.Graph().as_default():\n    session_conf = tf.ConfigProto(\n      allow_soft_placement=FLAGS.allow_soft_placement,\n      log_device_placement=FLAGS.log_device_placement)\n    sess = tf.Session(config=session_conf)\n    with sess.as_default():\n        \n        cnn = TextCNN(\n            sequence_length=x_train.shape[1],\n            num_classes=y_train.shape[1],\n            vocab_size=len(w2v_dict),\n            embedding_size=FLAGS.embedding_dim,\n            filter_sizes=list(map(int, FLAGS.filter_sizes.split("",""))),\n            num_filters=FLAGS.num_filters,\n            l2_reg_lambda=FLAGS.l2_reg_lambda)\n\n        # Define Training procedure\n        global_step = tf.Variable(0, name=""global_step"", trainable=False)\n        optimizer = tf.train.AdamOptimizer(1e-3)\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n        # \xe5\xb0\x86\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xba\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xba\x94\xe7\x94\xa8\xe5\x88\xb0\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\x8a\xef\xbc\x8c\xe6\x98\xaf\xe5\x87\xbd\xe6\x95\xb0minimize()\xe7\x9a\x84\xe7\xac\xac\xe4\xba\x8c\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\n        # \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe5\xba\x94\xe7\x94\xa8\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9cOperation\xef\xbc\x8c\xe5\xaf\xb9global_step\xe5\x81\x9a\xe8\x87\xaa\xe5\xa2\x9e\xe6\x93\x8d\xe4\xbd\x9c\n        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n        # Keep track of gradient values and sparsity (optional)\n        grad_summaries = []\n        for g, v in grads_and_vars:\n            if g is not None:\n                grad_hist_summary = tf.summary.histogram(""{}/grad/hist"".format(v.name), g)\n                sparsity_summary = tf.summary.scalar(""{}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                grad_summaries.append(grad_hist_summary)\n                grad_summaries.append(sparsity_summary)\n        grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n        # Output directory for models and summaries\n        # \xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x87\xba\xe8\xb7\xaf\xe5\xbe\x84\n        timestamp = str(int(time.time()))\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", timestamp))\n        print(""Writing to {}\\n"".format(out_dir))\n\n        # Summaries for loss and accuracy\n        loss_summary = tf.summary.scalar(""loss"", cnn.loss)\n        acc_summary = tf.summary.scalar(""accuracy"", cnn.accuracy)\n\n        # Train Summaries\n        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n        train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n        # Dev summaries\n        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n        dev_summary_dir = os.path.join(out_dir, ""summaries"", ""dev"")\n        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n\n        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n        checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n        checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n\n        # Initialize all variables\n        sess.run(tf.global_variables_initializer())\n\n        def train_step(x_batch, y_batch):\n            """"""\n            A single training step\n            """"""\n            feed_dict = {\n              cnn.input_x: x_batch,\n              cnn.input_y: y_batch,\n              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n            }\n            _, step, summaries, loss, accuracy = sess.run(\n                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n                feed_dict)\n            time_str = datetime.datetime.now().isoformat()\n            if (step%10==0):\n                print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n            train_summary_writer.add_summary(summaries, step)\n\n        def dev_step(x_batch, y_batch, writer=None):\n            """"""\n            Evaluates model on a dev set\n            """"""\n            feed_dict = {\n              cnn.input_x: x_batch,\n              cnn.input_y: y_batch,\n              cnn.dropout_keep_prob: 1.0\n            }\n            step, summaries, loss, accuracy = sess.run(\n                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n                feed_dict)\n            time_str = datetime.datetime.now().isoformat()\n            print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n            if writer:\n                writer.add_summary(summaries, step)\n\n        # Generate batches\n        batches = data_helpers.batch_iter(\n            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n        # Training loop. For each batch...\n        for batch in batches:\n            x_batch, y_batch = zip(*batch)\n            train_step(x_batch, y_batch)\n            current_step = tf.train.global_step(sess, global_step)\n            # \xe6\xb5\x8b\xe8\xaf\x95\n            if current_step % FLAGS.evaluate_every == 0:\n                print(""\\nEvaluation:"")\n                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n                print("""")\n            # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n            if current_step % FLAGS.checkpoint_every == 0:\n                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                print(""Saved model checkpoint to {}\\n"".format(path))\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week12-src/12声音分类.py,35,"b'\n# coding: utf-8\n\n# \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x9ahttps://serv.cusp.nyu.edu/projects/urbansounddataset/  \n# \n# librosa\xef\xbc\x9ahttps://github.com/librosa/librosa  \n# \n# \xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x9a  \n# 0 = air_conditioner  \n# 1 = car_horn  \n# 2 = children_playing  \n# 3 = dog_bark  \n# 4 = drilling  \n# 5 = engine_idling  \n# 6 = gun_shot  \n# 7 = jackhammer  \n# 8 = siren  \n# 9 = street_music  \n\n# In[1]:\n\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport librosa # pip install librosa\nfrom tqdm import tqdm # pip install tqdm\nimport random\n\n\n# In[2]:\n\n# Parameters\n# ==================================================\n\n# Data loading params\n# validation\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8d\xa0\xe6\xaf\x94\ntf.flags.DEFINE_float(""dev_sample_percentage"", .2, ""Percentage of the training data to use for validation"")\n# \xe7\x88\xb6\xe7\x9b\xae\xe5\xbd\x95\ntf.flags.DEFINE_string(""parent_dir"", ""audio/"", ""Data source for the data."")\n# \xe5\xad\x90\xe7\x9b\xae\xe5\xbd\x95\ntf.flags.DEFINE_string(""tr_sub_dirs"", [\'fold1/\',\'fold2/\',\'fold3/\'], ""Data source for the data."")\n\n# Model Hyperparameters\n# \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8cMFCC\xe4\xbf\xa1\xe5\x8f\xb7\ntf.flags.DEFINE_integer(""n_inputs"", 40, ""Number of MFCCs (default: 40)"")\n# cell\xe4\xb8\xaa\xe6\x95\xb0\ntf.flags.DEFINE_string(""n_hidden"", 300, ""Number of cells (default: 300)"")\n# \xe5\x88\x86\xe7\xb1\xbb\xe6\x95\xb0\ntf.flags.DEFINE_integer(""n_classes"", 10, ""Number of classes (default: 10)"")\n# \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\ntf.flags.DEFINE_integer(""lr"", 0.005, ""Learning rate (default: 0.005)"")\n# dropout\xe5\x8f\x82\xe6\x95\xb0\ntf.flags.DEFINE_float(""dropout_keep_prob"", 0.5, ""Dropout keep probability (default: 0.5)"")\n\n# Training parameters\n# \xe6\x89\xb9\xe6\xac\xa1\xe5\xa4\xa7\xe5\xb0\x8f\ntf.flags.DEFINE_integer(""batch_size"", 50, ""Batch Size (default: 50)"")\n# \xe8\xbf\xad\xe4\xbb\xa3\xe5\x91\xa8\xe6\x9c\x9f\ntf.flags.DEFINE_integer(""num_epochs"", 100, ""Number of training epochs (default: 100)"")\n# \xe5\xa4\x9a\xe5\xb0\x91step\xe6\xb5\x8b\xe8\xaf\x95\xe4\xb8\x80\xe6\xac\xa1\ntf.flags.DEFINE_integer(""evaluate_every"", 50, ""Evaluate model on dev set after this many steps (default: 50)"")\n# \xe5\xa4\x9a\xe5\xb0\x91step\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\xe6\xa8\xa1\xe5\x9e\x8b\ntf.flags.DEFINE_integer(""checkpoint_every"", 500, ""Save model after this many steps (default: 500)"")\n# \xe6\x9c\x80\xe5\xa4\x9a\xe4\xbf\x9d\xe5\xad\x98\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\ntf.flags.DEFINE_integer(""num_checkpoints"", 2, ""Number of checkpoints to store (default: 2)"")\n\n# flags\xe8\xa7\xa3\xe6\x9e\x90\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\n\n# \xe6\x89\x93\xe5\x8d\xb0\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\nprint(""\\nParameters:"")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(""{}={}"".format(attr.upper(), value))\nprint("""")\n\n\n# In[3]:\n\n# \xe8\x8e\xb7\xe5\xbe\x97\xe8\xae\xad\xe7\xbb\x83\xe7\x94\xa8\xe7\x9a\x84wav\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\xe5\x88\x97\xe8\xa1\xa8  \ndef get_wav_files(parent_dir,sub_dirs): \n    wav_files = []  \n    for l, sub_dir in enumerate(sub_dirs):\n        wav_path = os.path.join(parent_dir, sub_dir)\n        for (dirpath, dirnames, filenames) in os.walk(wav_path):  \n            for filename in filenames:  \n                if filename.endswith(\'.wav\') or filename.endswith(\'.WAV\'):  \n                    filename_path = os.sep.join([dirpath, filename])  \n                    wav_files.append(filename_path)  \n    return wav_files  \n\n# \xe8\x8e\xb7\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6mfcc\xe7\x89\xb9\xe5\xbe\x81\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe6\xa0\x87\xe7\xad\xbe\ndef extract_features(wav_files):\n    inputs = []\n    labels = []\n    \n    for wav_file in tqdm(wav_files):\n        # \xe8\xaf\xbb\xe5\x85\xa5\xe9\x9f\xb3\xe9\xa2\x91\xe6\x96\x87\xe4\xbb\xb6\n        audio,fs = librosa.load(wav_file)\n\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe9\x9f\xb3\xe9\xa2\x91mfcc\xe7\x89\xb9\xe5\xbe\x81\n        # [n_steps, n_inputs]\n        mfccs = np.transpose(librosa.feature.mfcc(y=audio, sr=fs, n_mfcc=FLAGS.n_inputs), [1,0]) \n        inputs.append(mfccs.tolist()) \n    #\xe8\x8e\xb7\xe5\x8f\x96label\n    for wav_file in wav_files:\n        label = wav_file.split(\'/\')[-1].split(\'-\')[1]\n        labels.append(label) \n    return inputs, np.array(labels, dtype=np.int)\n\n\n# In[4]:\n\n# \xe8\x8e\xb7\xe5\xbe\x97\xe8\xae\xad\xe7\xbb\x83\xe7\x94\xa8\xe7\x9a\x84wav\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\xe5\x88\x97\xe8\xa1\xa8 \nwav_files = get_wav_files(FLAGS.parent_dir,FLAGS.tr_sub_dirs)\n# \xe8\x8e\xb7\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6mfcc\xe7\x89\xb9\xe5\xbe\x81\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe6\xa0\x87\xe7\xad\xbe\ntr_features,tr_labels = extract_features(wav_files)\n\nnp.save(\'tr_features.npy\',tr_features)\nnp.save(\'tr_labels.npy\',tr_labels)\n\n# tr_features=np.load(\'tr_features.npy\')\n# tr_labels=np.load(\'tr_labels.npy\')\n\n\n# In[5]:\n\n#(batch,step,input)\n#(50,173,40)\n\n# \xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84step\nwav_max_len = max([len(feature) for feature in tr_features])\nprint(""max_len:"",wav_max_len)\n\n# \xe5\xa1\xab\xe5\x85\x850\ntr_data = []\nfor mfccs in tr_features:  \n    while len(mfccs) < wav_max_len: #\xe5\x8f\xaa\xe8\xa6\x81\xe5\xb0\x8f\xe4\xba\x8ewav_max_len\xe5\xb0\xb1\xe8\xa1\xa5n_inputs\xe4\xb8\xaa0\n        mfccs.append([0] * FLAGS.n_inputs) \n    tr_data.append(mfccs)\n\ntr_data = np.array(tr_data)\n\n\n# In[6]:\n\n# Randomly shuffle data\nnp.random.seed(10)\nshuffle_indices = np.random.permutation(np.arange(len(tr_data)))\nx_shuffled = tr_data[shuffle_indices]\ny_shuffled = tr_labels[shuffle_indices]\n\n# Split train/test set\n# TODO: This is very crude, should use cross-validation\n# \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x87\xe5\x88\x86\xe4\xb8\xba\xe4\xb8\xa4\xe9\x83\xa8\xe5\x88\x86\ndev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y_shuffled)))\ntrain_x, test_x = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\ntrain_y, test_y = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n\n\n# In[7]:\n\n# placeholder\nx = tf.placeholder(""float"", [None, wav_max_len, FLAGS.n_inputs])\ny = tf.placeholder(""float"", [None])\ndropout = tf.placeholder(tf.float32)\n# learning rate\nlr = tf.Variable(FLAGS.lr, dtype=tf.float32, trainable=False)\n\n# \xe5\xae\x9a\xe4\xb9\x89RNN\xe7\xbd\x91\xe7\xbb\x9c\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe5\x88\xb6\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\nweights = tf.Variable(tf.truncated_normal([FLAGS.n_hidden, FLAGS.n_classes], stddev=0.1))\nbiases = tf.Variable(tf.constant(0.1, shape=[FLAGS.n_classes]))\n\n# \xe5\xa4\x9a\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\nnum_layers = 3\ndef grucell():\n    cell = tf.contrib.rnn.GRUCell(FLAGS.n_hidden)\n#     cell = tf.contrib.rnn.LSTMCell(FLAGS.n_hidden)\n    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n    return cell\ncell = tf.contrib.rnn.MultiRNNCell([grucell() for _ in range(num_layers)])\n\n\noutputs,final_state = tf.nn.dynamic_rnn(cell,x,dtype=tf.float32)\n\n# \xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\nprediction = tf.nn.softmax(tf.matmul(final_state[0],weights) + biases)\n\n# labels\xe8\xbd\xacone_hot\xe6\xa0\xbc\xe5\xbc\x8f\none_hot_labels = tf.one_hot(indices=tf.cast(y, tf.int32), depth=FLAGS.n_classes)\n\n# loss\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=one_hot_labels))\n\n# optimizer\noptimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cross_entropy)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(prediction,1), tf.argmax(one_hot_labels,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n\n# In[8]:\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    """"""\n        Generates a batch iterator for a dataset.\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    # \xe6\xaf\x8f\xe4\xb8\xaaepoch\xe7\x9a\x84num_batch\n    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n    print(""num_batches_per_epoch:"",num_batches_per_epoch)\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n\n\n# In[9]:\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n# \xe5\xae\x9a\xe4\xb9\x89saver\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    sess.run(init) \n\n    # Generate batches\n    batches = batch_iter(list(zip(train_x, train_y)), FLAGS.batch_size, FLAGS.num_epochs)\n\n    for i,batch in enumerate(batches):\n        i = i + 1\n        x_batch, y_batch = zip(*batch)\n        sess.run([optimizer], feed_dict={x: x_batch, y: y_batch, dropout: FLAGS.dropout_keep_prob})\n        \n        # \xe6\xb5\x8b\xe8\xaf\x95\n        if i % FLAGS.evaluate_every == 0:\n            sess.run(tf.assign(lr, FLAGS.lr * (0.99 ** (i // FLAGS.evaluate_every))))\n            learning_rate = sess.run(lr)\n            tr_acc, _loss = sess.run([accuracy, cross_entropy], feed_dict={x: train_x, y: train_y, dropout: 1.0})\n            ts_acc = sess.run(accuracy, feed_dict={x: test_x, y: test_y, dropout: 1.0})\n            print(""Iter {}, loss {:.5f}, tr_acc {:.5f}, ts_acc {:.5f}, lr {:.5f}"".format(i, _loss, tr_acc, ts_acc, learning_rate))\n\n        # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        if i % FLAGS.checkpoint_every == 0:\n            path = saver.save(sess, ""sounds_models/model"", global_step=i)\n            print(""Saved model checkpoint to {}\\n"".format(path))\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week12-src/text_cnn.py,33,"b'#coding:utf-8\nimport tensorflow as tf\nimport numpy as np\nimport pickle\n\n\nclass TextCNN(object):\n    """"""\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    """"""\n    # sequence_length-\xe6\x9c\x80\xe9\x95\xbf\xe8\xaf\x8d\xe6\xb1\x87\xe6\x95\xb0\n    # num_classes-\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\xb0\n    # vocab_size-\xe6\x80\xbb\xe8\xaf\x8d\xe6\xb1\x87\xe6\x95\xb0\n    # embedding_size-\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6\n    # filter_sizes-\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb83\xef\xbc\x8c4\xef\xbc\x8c5\n    # num_filters-\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\xe9\x87\x8f\n    # l2_reg_lambda-l2\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\xb3\xbb\xe6\x95\xb0\n    def __init__(\n      self, sequence_length, num_classes, vocab_size,\n      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n\n        # Placeholders for input, output and dropout\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        \n        with open(\'embeddings.pickle\',\'rb\') as f:\n            embeddings = pickle.load(f)\n\n        # Keeping track of l2 regularization loss (optional)\n        l2_loss = tf.constant(0.0)\n\n        # Embedding layer\n        with tf.device(\'/cpu:0\'), tf.name_scope(""embedding""):\n            self.W = tf.Variable(embeddings, name=\'W\')\n            # self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=""W"")\n            # [batch_size, sequence_length, embedding_size]\n            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n            # \xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c[batch_size, sequence_length, embedding_size, 1]\n            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n\n        # Create a convolution + maxpool layer for each filter size\n        \n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.name_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")\n                conv = tf.nn.conv2d(\n                    self.embedded_chars_expanded,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv"")\n                # Apply nonlinearity\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=\'VALID\',\n                    name=""pool"")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        self.h_pool = tf.concat(pooled_outputs, 3)\n        # \xe6\x8a\x8a\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe5\x8f\x98\xe6\x88\x90\xe4\xb8\x80\xe7\xbb\xb4\xe5\x90\x91\xe9\x87\x8f\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n        \n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # Final (unnormalized) scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.get_variable(\n                ""W"",\n                shape=[num_filters_total, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=""b"")\n            l2_loss += tf.nn.l2_loss(W)\n            l2_loss += tf.nn.l2_loss(b)\n            self.scores = tf.nn.softmax(tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores""))\n            self.predictions = tf.argmax(self.scores, 1, name=""predictions"")\n\n        # CalculateMean cross-entropy loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # Accuracy\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week09-src/retain/9-1测试训练好的模型.py,6,"b""\n# coding: utf-8\n\n# In[1]:\n\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport re\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\n# In[2]:\n\nlines = tf.gfile.GFile('retrain/output_labels.txt').readlines()\nuid_to_human = {}\n#\xe4\xb8\x80\xe8\xa1\x8c\xe4\xb8\x80\xe8\xa1\x8c\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\nfor uid,line in enumerate(lines) :\n    #\xe5\x8e\xbb\xe6\x8e\x89\xe6\x8d\xa2\xe8\xa1\x8c\xe7\xac\xa6\n    line=line.strip('\\n')\n    uid_to_human[uid] = line\n\ndef id_to_string(node_id):\n    if node_id not in uid_to_human:\n        return ''\n    return uid_to_human[node_id]\n\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\xbe\xe6\x9d\xa5\xe5\xad\x98\xe6\x94\xbegoogle\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\nwith tf.gfile.FastGFile('retrain/output_graph.pb', 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name='')\n\n\nwith tf.Session() as sess:\n    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n    #\xe9\x81\x8d\xe5\x8e\x86\xe7\x9b\xae\xe5\xbd\x95\n    for root,dirs,files in os.walk('retrain/images/'):\n        for file in files:\n            #\xe8\xbd\xbd\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\n            image_data = tf.gfile.FastGFile(os.path.join(root,file), 'rb').read()\n            predictions = sess.run(softmax_tensor,{'DecodeJpeg/contents:0': image_data})#\xe5\x9b\xbe\xe7\x89\x87\xe6\xa0\xbc\xe5\xbc\x8f\xe6\x98\xafjpg\xe6\xa0\xbc\xe5\xbc\x8f\n            predictions = np.squeeze(predictions)#\xe6\x8a\x8a\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbd\xac\xe4\xb8\xba1\xe7\xbb\xb4\xe6\x95\xb0\xe6\x8d\xae\n\n            #\xe6\x89\x93\xe5\x8d\xb0\xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\xe5\x8f\x8a\xe5\x90\x8d\xe7\xa7\xb0\n            image_path = os.path.join(root,file)\n            print(image_path)\n            #\xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe7\x89\x87\n            img=Image.open(image_path)\n            plt.imshow(img)\n            plt.axis('off')\n            plt.show()\n\n            #\xe6\x8e\x92\xe5\xba\x8f\n            top_k = predictions.argsort()[::-1]\n            print(top_k)\n            for node_id in top_k:     \n                #\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\n                human_string = id_to_string(node_id)\n                #\xe8\x8e\xb7\xe5\x8f\x96\xe8\xaf\xa5\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\n                score = predictions[node_id]\n                print('%s (score = %.5f)' % (human_string, score))\n            print()\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n"""
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week09-src/slim/9-2生成tfrecord.py,9,"b'\n# coding: utf-8\n\n# In[2]:\n\nimport tensorflow as tf\nimport os\nimport random\nimport math\nimport sys\n\n\n# In[3]:\n\n#\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\x95\xb0\xe9\x87\x8f\n_NUM_TEST = 500\n#\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\n_RANDOM_SEED = 0\n#\xe6\x95\xb0\xe6\x8d\xae\xe5\x9d\x97\n_NUM_SHARDS = 5\n#\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xb7\xaf\xe5\xbe\x84\nDATASET_DIR = ""D:/Tensorflow/slim/images/""\n#\xe6\xa0\x87\xe7\xad\xbe\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\xad\x97\nLABELS_FILENAME = ""D:/Tensorflow/slim/images/labels.txt""\n\n#\xe5\xae\x9a\xe4\xb9\x89tfrecord\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84+\xe5\x90\x8d\xe5\xad\x97\ndef _get_dataset_filename(dataset_dir, split_name, shard_id):\n    output_filename = \'image_%s_%05d-of-%05d.tfrecord\' % (split_name, shard_id, _NUM_SHARDS)\n    return os.path.join(dataset_dir, output_filename)\n\n#\xe5\x88\xa4\xe6\x96\xadtfrecord\xe6\x96\x87\xe4\xbb\xb6\xe6\x98\xaf\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\ndef _dataset_exists(dataset_dir):\n    for split_name in [\'train\', \'test\']:\n        for shard_id in range(_NUM_SHARDS):\n            #\xe5\xae\x9a\xe4\xb9\x89tfrecord\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84+\xe5\x90\x8d\xe5\xad\x97\n            output_filename = _get_dataset_filename(dataset_dir, split_name, shard_id)\n        if not tf.gfile.Exists(output_filename):\n            return False\n    return True\n\n#\xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89\xe6\x96\x87\xe4\xbb\xb6\xe4\xbb\xa5\xe5\x8f\x8a\xe5\x88\x86\xe7\xb1\xbb\ndef _get_filenames_and_classes(dataset_dir):\n    #\xe6\x95\xb0\xe6\x8d\xae\xe7\x9b\xae\xe5\xbd\x95\n    directories = []\n    #\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x8d\xe7\xa7\xb0\n    class_names = []\n    for filename in os.listdir(dataset_dir):\n        #\xe5\x90\x88\xe5\xb9\xb6\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n        path = os.path.join(dataset_dir, filename)\n        #\xe5\x88\xa4\xe6\x96\xad\xe8\xaf\xa5\xe8\xb7\xaf\xe5\xbe\x84\xe6\x98\xaf\xe5\x90\xa6\xe4\xb8\xba\xe7\x9b\xae\xe5\xbd\x95\n        if os.path.isdir(path):\n            #\xe5\x8a\xa0\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe7\x9b\xae\xe5\xbd\x95\n            directories.append(path)\n            #\xe5\x8a\xa0\xe5\x85\xa5\xe7\xb1\xbb\xe5\x88\xab\xe5\x90\x8d\xe7\xa7\xb0\n            class_names.append(filename)\n\n    photo_filenames = []\n    #\xe5\xbe\xaa\xe7\x8e\xaf\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\n    for directory in directories:\n        for filename in os.listdir(directory):\n            path = os.path.join(directory, filename)\n            #\xe6\x8a\x8a\xe5\x9b\xbe\xe7\x89\x87\xe5\x8a\xa0\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe5\x88\x97\xe8\xa1\xa8\n            photo_filenames.append(path)\n\n    return photo_filenames, class_names\n\ndef int64_feature(values):\n    if not isinstance(values, (tuple, list)):\n        values = [values]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\ndef bytes_feature(values):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\ndef image_to_tfexample(image_data, image_format, class_id):\n    #Abstract base class for protocol messages.\n    return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n    }))\n\ndef write_label_file(labels_to_class_names, dataset_dir,filename=LABELS_FILENAME):\n    labels_filename = os.path.join(dataset_dir, filename)\n    with tf.gfile.Open(labels_filename, \'w\') as f:\n        for label in labels_to_class_names:\n            class_name = labels_to_class_names[label]\n            f.write(\'%d:%s\\n\' % (label, class_name))\n\n#\xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe4\xb8\xbaTFRecord\xe6\xa0\xbc\xe5\xbc\x8f\ndef _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n    assert split_name in [\'train\', \'test\']\n    #\xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe5\x9d\x97\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\x91\xe6\x95\xb0\xe6\x8d\xae\n    num_per_shard = int(len(filenames) / _NUM_SHARDS)\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            for shard_id in range(_NUM_SHARDS):\n                #\xe5\xae\x9a\xe4\xb9\x89tfrecord\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84+\xe5\x90\x8d\xe5\xad\x97\n                output_filename = _get_dataset_filename(dataset_dir, split_name, shard_id)\n                with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n                    #\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe5\x9d\x97\xe5\xbc\x80\xe5\xa7\x8b\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n                    start_ndx = shard_id * num_per_shard\n                    #\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe5\x9d\x97\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n                    end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n                    for i in range(start_ndx, end_ndx):\n                        try:\n                            sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\' % (i+1, len(filenames), shard_id))\n                            sys.stdout.flush()\n                            #\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\n                            image_data = tf.gfile.FastGFile(filenames[i], \'r\').read()\n                            #\xe8\x8e\xb7\xe5\xbe\x97\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe5\x90\x8d\xe7\xa7\xb0\n                            class_name = os.path.basename(os.path.dirname(filenames[i]))\n                            #\xe6\x89\xbe\xe5\x88\xb0\xe7\xb1\xbb\xe5\x88\xab\xe5\x90\x8d\xe7\xa7\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id\n                            class_id = class_names_to_ids[class_name]\n                            #\xe7\x94\x9f\xe6\x88\x90tfrecord\xe6\x96\x87\xe4\xbb\xb6\n                            example = image_to_tfexample(image_data, b\'jpg\', class_id)\n                            tfrecord_writer.write(example.SerializeToString())\n                        except IOError as e:\n                            print(""Could not read:"",filenames[i])\n                            print(""Error:"",e)\n                            print(""Skip it\\n"")\n                            \n    sys.stdout.write(\'\\n\')\n    sys.stdout.flush()\n\n\nif __name__ == \'__main__\':\n    #\xe5\x88\xa4\xe6\x96\xadtfrecord\xe6\x96\x87\xe4\xbb\xb6\xe6\x98\xaf\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\n    if _dataset_exists(DATASET_DIR):\n        print(\'tfcecord\xe6\x96\x87\xe4\xbb\xb6\xe5\xb7\xb2\xe5\xad\x98\xe5\x9c\xa8\')\n    else:\n        #\xe8\x8e\xb7\xe5\xbe\x97\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\xa5\xe5\x8f\x8a\xe5\x88\x86\xe7\xb1\xbb\n        photo_filenames, class_names = _get_filenames_and_classes(DATASET_DIR)\n        #\xe6\x8a\x8a\xe5\x88\x86\xe7\xb1\xbb\xe8\xbd\xac\xe4\xb8\xba\xe5\xad\x97\xe5\x85\xb8\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x8c\xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8e{\'house\': 3, \'flower\': 1, \'plane\': 4, \'guitar\': 2, \'animal\': 0}\n        class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n\n        #\xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x87\xe5\x88\x86\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n        random.seed(_RANDOM_SEED)\n        random.shuffle(photo_filenames)\n        training_filenames = photo_filenames[_NUM_TEST:]\n        testing_filenames = photo_filenames[:_NUM_TEST]\n\n        #\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\n        _convert_dataset(\'train\', training_filenames, class_names_to_ids, DATASET_DIR)\n        _convert_dataset(\'test\', testing_filenames, class_names_to_ids, DATASET_DIR)\n\n        #\xe8\xbe\x93\xe5\x87\xbalabels\xe6\x96\x87\xe4\xbb\xb6\n        labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n        write_label_file(labels_to_class_names, DATASET_DIR)\n\n\n# In[ ]:\n\n\n\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/__init__.py,0,b'\n'
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/alexnet.py,14,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net0 = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8_0\')\n        net1 = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8_1\')\n        net2 = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8_2\')\n        net3 = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8_3\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net0 = tf.squeeze(net0, [1, 2], name=\'fc8_0/squeezed\')\n        end_points[sc.name + \'/fc8_0\'] = net0\n        net1 = tf.squeeze(net1, [1, 2], name=\'fc8_1/squeezed\')\n        end_points[sc.name + \'/fc8_1\'] = net1\n        net2 = tf.squeeze(net2, [1, 2], name=\'fc8_2/squeezed\')\n        end_points[sc.name + \'/fc8_2\'] = net2\n        net3 = tf.squeeze(net3, [1, 2], name=\'fc8_3/squeezed\')\n        end_points[sc.name + \'/fc8_3\'] = net3\n\n\n      return net0,net1,net2,net3,end_points\nalexnet_v2.default_image_size = 224\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/alexnet_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/cifarnet.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer(),\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_resnet_v2.py,39,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,\n                        reuse=None,\n                        scope=\'InceptionResnetV2\'):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n\n        # 149 x 149 x 32\n        net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                          scope=\'Conv2d_1a_3x3\')\n        end_points[\'Conv2d_1a_3x3\'] = net\n        # 147 x 147 x 32\n        net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                          scope=\'Conv2d_2a_3x3\')\n        end_points[\'Conv2d_2a_3x3\'] = net\n        # 147 x 147 x 64\n        net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n        end_points[\'Conv2d_2b_3x3\'] = net\n        # 73 x 73 x 64\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_3a_3x3\')\n        end_points[\'MaxPool_3a_3x3\'] = net\n        # 73 x 73 x 80\n        net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                          scope=\'Conv2d_3b_1x1\')\n        end_points[\'Conv2d_3b_1x1\'] = net\n        # 71 x 71 x 192\n        net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                          scope=\'Conv2d_4a_3x3\')\n        end_points[\'Conv2d_4a_3x3\'] = net\n        # 35 x 35 x 192\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_5a_3x3\')\n        end_points[\'MaxPool_5a_3x3\'] = net\n\n        # 35 x 35 x 320\n        with tf.variable_scope(\'Mixed_5b\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                        scope=\'Conv2d_0b_5x5\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                        scope=\'Conv2d_0c_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                         scope=\'AvgPool_0a_3x3\')\n            tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                       scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_1,\n                              tower_conv2_2, tower_pool_1])\n\n        end_points[\'Mixed_5b\'] = net\n        net = slim.repeat(net, 10, block35, scale=0.17)\n\n        # 17 x 17 x 1088\n        with tf.variable_scope(\'Mixed_6a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n                                     scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                        stride=2, padding=\'VALID\',\n                                        scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_2, tower_pool])\n\n        end_points[\'Mixed_6a\'] = net\n        net = slim.repeat(net, 20, block17, scale=0.10)\n\n        # Auxiliary tower\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = slim.avg_pool2d(net, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n        with tf.variable_scope(\'Mixed_7a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1,\n                              tower_conv2_2, tower_pool])\n\n        end_points[\'Mixed_7a\'] = net\n\n        net = slim.repeat(net, 9, block8, scale=0.20)\n        net = block8(net, activation_fn=None)\n\n        net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n        end_points[\'Conv2d_7b_1x1\'] = net\n\n        with tf.variable_scope(\'Logits\'):\n          end_points[\'PrePool\'] = net\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a_8x8\')\n          net = slim.flatten(net)\n\n          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                             scope=\'Dropout\')\n\n          end_points[\'PreLogitsFlatten\'] = net\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_resnet_v2_test.py,20,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'PrePool\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'PrePool\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_v1.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'AvgPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_v1_test.py,25,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_v2.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_v2_test.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_v3.py,79,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_v3_test.py,29,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_v4.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/inception_v4_test.py,24,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/lenet.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/nets_factory_test.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\nslim = tf.contrib.slim\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnArgScope(self):\n    batch_size = 5\n    num_classes = 10\n    net = \'cifarnet\'\n    with self.test_session(use_gpu=True):\n      net_fn = nets_factory.get_network_fn(net, num_classes)\n      image_size = getattr(net_fn, \'default_image_size\', 224)\n      with slim.arg_scope([slim.model_variable, slim.variable],\n                          device=\'/CPU:0\'):\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        net_fn(inputs)\n      weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \'CifarNet/conv1\')[0]\n      self.assertDeviceEqual(\'/CPU:0\', weights.device)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/overfeat.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/overfeat_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          unit_depth, unit_depth_bottleneck, unit_stride = unit\n\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=1,\n                                rate=rate)\n            rate *= unit_stride\n\n          else:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=unit_stride,\n                                rate=1)\n            current_stride *= unit_stride\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/resnet_v1.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/resnet_v1_test.py,45,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = dict(tf.get_collection(\'end_points\'))\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    bottleneck = resnet_v1.bottleneck\n    blocks = [resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n              resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 1)])]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          depth, depth_bottleneck, stride = unit\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net,\n                                depth=depth,\n                                depth_bottleneck=depth_bottleneck,\n                                stride=stride,\n                                rate=1)\n    return net\n\n  def _atrousValues(self, bottleneck):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n\n    Args:\n      bottleneck: The bottleneck function.\n    """"""\n    blocks = [\n        resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n        resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 2)]),\n        resnet_utils.Block(\'block3\', bottleneck, [(16, 4, 1), (16, 4, 2)]),\n        resnet_utils.Block(\'block4\', bottleneck, [(32, 8, 1), (32, 8, 1)])\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testAtrousValuesBottleneck(self):\n    self._atrousValues(resnet_v1.bottleneck)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    bottleneck = resnet_v1.bottleneck\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(4, 1, 1)] * 2 + [(4, 1, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(8, 2, 1)] * 2 + [(8, 2, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(16, 4, 1)] * 2 + [(16, 4, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(32, 8, 1)] * 2)]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\nAnother difference is that \'v2\' ResNets do not include an activation function in\nthe main pathway. Also see [2; Fig. 4e].\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/resnet_v2_test.py,45,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = dict(tf.get_collection(\'end_points\'))\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    bottleneck = resnet_v2.bottleneck\n    blocks = [resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n              resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 1)])]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          depth, depth_bottleneck, stride = unit\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net,\n                                depth=depth,\n                                depth_bottleneck=depth_bottleneck,\n                                stride=stride,\n                                rate=1)\n    return net\n\n  def _atrousValues(self, bottleneck):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n\n    Args:\n      bottleneck: The bottleneck function.\n    """"""\n    blocks = [\n        resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n        resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 2)]),\n        resnet_utils.Block(\'block3\', bottleneck, [(16, 4, 1), (16, 4, 2)]),\n        resnet_utils.Block(\'block4\', bottleneck, [(32, 8, 1), (32, 8, 1)])\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testAtrousValuesBottleneck(self):\n    self._atrousValues(resnet_v2.bottleneck)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    bottleneck = resnet_v2.bottleneck\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(4, 1, 1)] * 2 + [(4, 1, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(8, 2, 1)] * 2 + [(8, 2, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(16, 4, 1)] * 2 + [(16, 4, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(32, 8, 1)] * 2)]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/vgg.py,9,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\',\n          fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week10-src/nets/vgg_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week11-src/cnn-text-classification-tf-master/data_helpers.py,0,"b'#coding:utf-8\nimport numpy as np\nimport re\nimport itertools\nfrom collections import Counter\n\n\ndef clean_str(string):\n    """"""\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    """"""\n    # \xe4\xb8\x8d\xe6\x98\xaf\xe7\x89\xb9\xe5\xae\x9a\xe5\xad\x97\xe7\xac\xa6\xe9\x83\xbd\xe5\x8f\x98\xe6\x88\x90\xe7\xa9\xba\xe6\xa0\xbc\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n    # \xe5\x8a\xa0\xe7\xa9\xba\xe6\xa0\xbc\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r"","", "" , "", string)\n    string = re.sub(r""!"", "" ! "", string)\n    string = re.sub(r""\\("", "" \\( "", string)\n    string = re.sub(r""\\)"", "" \\) "", string)\n    string = re.sub(r""\\?"", "" \\? "", string)\n    # \xe5\x8c\xb9\xe9\x85\x8d2\xe4\xb8\xaa\xe6\x88\x96\xe5\xa4\x9a\xe4\xb8\xaa\xe7\xa9\xba\xe7\x99\xbd\xe5\xad\x97\xe7\xac\xa6\xe5\x8f\x98\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa"" ""\xe7\xa9\xba\xe6\xa0\xbc\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    # \xe5\x8e\xbb\xe6\x8e\x89\xe5\x8f\xa5\xe5\xad\x90\xe9\xa6\x96\xe5\xb0\xbe\xe7\x9a\x84\xe7\xa9\xba\xe7\x99\xbd\xe7\xac\xa6\xef\xbc\x8c\xe5\x86\x8d\xe8\xbd\xac\xe5\xb0\x8f\xe5\x86\x99\n    return string.strip().lower()\n\n\ndef load_data_and_labels(positive_data_file, negative_data_file):\n    """"""\n    Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    """"""\n    # Load data from files\n    positive_examples = list(open(positive_data_file, ""r"", encoding=""utf-8"").readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(negative_data_file, ""r"", encoding=""utf-8"").readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    """"""\n    Generates a batch iterator for a dataset.\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n    print(""num_batches_per_epoch:"",num_batches_per_epoch)\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week11-src/cnn-text-classification-tf-master/eval.py,13,"b'#! /usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport data_helpers\nfrom text_cnn import TextCNN\nfrom tensorflow.contrib import learn\nimport csv\n\n# Parameters\n# ==================================================\n\n# Data Parameters\ntf.flags.DEFINE_string(""positive_data_file"", ""./data/rt-polaritydata/rt-polarity.pos"", ""Data source for the positive data."")\ntf.flags.DEFINE_string(""negative_data_file"", ""./data/rt-polaritydata/rt-polarity.neg"", ""Data source for the positive data."")\n\n# Eval Parameters\ntf.flags.DEFINE_integer(""batch_size"", 64, ""Batch Size (default: 64)"")\ntf.flags.DEFINE_string(""checkpoint_dir"", """", ""Checkpoint directory from training run"")\ntf.flags.DEFINE_boolean(""eval_train"", False, ""Evaluate on all training data"")\n\n# Misc Parameters\ntf.flags.DEFINE_boolean(""allow_soft_placement"", True, ""Allow device soft device placement"")\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""Log placement of ops on devices"")\n\n\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\nprint(""\\nParameters:"")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(""{}={}"".format(attr.upper(), value))\nprint("""")\n\n# CHANGE THIS: Load data. Load your own data here\nif FLAGS.eval_train:\n    x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n    y_test = np.argmax(y_test, axis=1)\nelse:\n    x_raw = [""a masterpiece four years in the making"", ""everything is off.""]\n    y_test = [1, 0]\n\n# Map data into vocabulary\nvocab_path = os.path.join(FLAGS.checkpoint_dir, "".."", ""vocab"")\nvocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\nx_test = np.array(list(vocab_processor.transform(x_raw)))\n\nprint(""\\nEvaluating...\\n"")\n\n# Evaluation\n# ==================================================\ncheckpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\ngraph = tf.Graph()\nwith graph.as_default():\n    session_conf = tf.ConfigProto(\n      allow_soft_placement=FLAGS.allow_soft_placement,\n      log_device_placement=FLAGS.log_device_placement)\n    sess = tf.Session(config=session_conf)\n    with sess.as_default():\n        # Load the saved meta graph and restore variables\n        saver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))\n        saver.restore(sess, checkpoint_file)\n\n        # Get the placeholders from the graph by name\n        input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n        # input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n        dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n\n        # Tensors we want to evaluate\n        predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n\n        # Generate batches for one epoch\n        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n\n        # Collect the predictions here\n        all_predictions = []\n\n        for x_test_batch in batches:\n            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n            all_predictions = np.concatenate([all_predictions, batch_predictions])\n\n# Print accuracy if y_test is defined\nif y_test is not None:\n    correct_predictions = float(sum(all_predictions == y_test))\n    print(""Total number of test examples: {}"".format(len(y_test)))\n    print(""Accuracy: {:g}"".format(correct_predictions/float(len(y_test))))\n\n# Save the evaluation to a csv\npredictions_human_readable = np.column_stack((np.array(x_raw), all_predictions))\nout_path = os.path.join(FLAGS.checkpoint_dir, "".."", ""prediction.csv"")\nprint(""Saving evaluation to {0}"".format(out_path))\nwith open(out_path, \'w\') as f:\n    csv.writer(f).writerows(predictions_human_readable)'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week11-src/cnn-text-classification-tf-master/text_cnn.py,33,"b'#coding:utf-8\nimport tensorflow as tf\nimport numpy as np\nimport pickle\n\n\nclass TextCNN(object):\n    """"""\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    """"""\n    # sequence_length-\xe6\x9c\x80\xe9\x95\xbf\xe8\xaf\x8d\xe6\xb1\x87\xe6\x95\xb0\n    # num_classes-\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\xb0\n    # vocab_size-\xe6\x80\xbb\xe8\xaf\x8d\xe6\xb1\x87\xe6\x95\xb0\n    # embedding_size-\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6\n    # filter_sizes-\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb83\xef\xbc\x8c4\xef\xbc\x8c5\n    # num_filters-\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\xe9\x87\x8f\n    # l2_reg_lambda-l2\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\xb3\xbb\xe6\x95\xb0\n    def __init__(\n      self, sequence_length, num_classes, vocab_size,\n      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n\n        # Placeholders for input, output and dropout\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n\n        # Keeping track of l2 regularization loss (optional)\n        l2_loss = tf.constant(0.0)\n\n        # Embedding layer\n        with tf.device(\'/cpu:0\'), tf.name_scope(""embedding""):\n            self.W = tf.Variable(\n               tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n               name=""W"")\n            # [batch_size, sequence_length, embedding_size]\n            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n            # \xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c[batch_size, sequence_length, embedding_size, 1]\n            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n\n        # Create a convolution + maxpool layer for each filter size\n        \n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.name_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")\n                conv = tf.nn.conv2d(\n                    self.embedded_chars_expanded,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv"")\n                # Apply nonlinearity\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=\'VALID\',\n                    name=""pool"")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        self.h_pool = tf.concat(pooled_outputs, 3)\n        # \xe6\x8a\x8a\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe5\x8f\x98\xe6\x88\x90\xe4\xb8\x80\xe7\xbb\xb4\xe5\x90\x91\xe9\x87\x8f\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n        \n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # Final (unnormalized) scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.get_variable(\n                ""W"",\n                shape=[num_filters_total, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=""b"")\n            l2_loss += tf.nn.l2_loss(W)\n            l2_loss += tf.nn.l2_loss(b)\n            self.scores = tf.nn.softmax(tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores""))\n            self.predictions = tf.argmax(self.scores, 1, name=""predictions"")\n\n        # CalculateMean cross-entropy loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # Accuracy\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week11-src/cnn-text-classification-tf-master/train.py,33,"b'#! /usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport data_helpers\nfrom text_cnn import TextCNN\nfrom tensorflow.contrib import learn\n\n# Parameters\n# ==================================================\n\n# Data loading params\ntf.flags.DEFINE_float(""dev_sample_percentage"", .1, ""Percentage of the training data to use for validation"")\ntf.flags.DEFINE_string(""positive_data_file"", ""./data/rt-polaritydata/rt-polarity.pos"", ""Data source for the positive data."")\ntf.flags.DEFINE_string(""negative_data_file"", ""./data/rt-polaritydata/rt-polarity.neg"", ""Data source for the negative data."")\n\n# Model Hyperparameters\ntf.flags.DEFINE_integer(""embedding_dim"", 128, ""Dimensionality of character embedding (default: 128)"")\ntf.flags.DEFINE_string(""filter_sizes"", ""3,4,5"", ""Comma-separated filter sizes (default: \'3,4,5\')"")\ntf.flags.DEFINE_integer(""num_filters"", 128, ""Number of filters per filter size (default: 128)"")\ntf.flags.DEFINE_float(""dropout_keep_prob"", 0.5, ""Dropout keep probability (default: 0.5)"")\ntf.flags.DEFINE_float(""l2_reg_lambda"", 0.0, ""L2 regularization lambda (default: 0.0)"")\n\n# Training parameters\ntf.flags.DEFINE_integer(""batch_size"", 64, ""Batch Size (default: 64)"")\ntf.flags.DEFINE_integer(""num_epochs"", 200, ""Number of training epochs (default: 200)"")\ntf.flags.DEFINE_integer(""evaluate_every"", 100, ""Evaluate model on dev set after this many steps (default: 100)"")\ntf.flags.DEFINE_integer(""checkpoint_every"", 100, ""Save model after this many steps (default: 100)"")\ntf.flags.DEFINE_integer(""num_checkpoints"", 5, ""Number of checkpoints to store (default: 5)"")\n# Misc Parameters\ntf.flags.DEFINE_boolean(""allow_soft_placement"", True, ""Allow device soft device placement"")\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""Log placement of ops on devices"")\n\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\nprint(""\\nParameters:"")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(""{}={}"".format(attr.upper(), value))\nprint("""")\n\n\n# Data Preparation\n# ==================================================\n\n# Load data\nprint(""Loading data..."")\nx_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n\n# Build vocabulary\nmax_document_length = max([len(x.split("" "")) for x in x_text])\nvocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\nx = np.array(list(vocab_processor.fit_transform(x_text)))\n\n# Randomly shuffle data\nnp.random.seed(10)\nshuffle_indices = np.random.permutation(np.arange(len(y)))\nx_shuffled = x[shuffle_indices]\ny_shuffled = y[shuffle_indices]\n\n# Split train/test set\n# TODO: This is very crude, should use cross-validation\ndev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\nx_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\ny_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\nprint(""Vocabulary Size: {:d}"".format(len(vocab_processor.vocabulary_)))\nprint(""Train/Dev split: {:d}/{:d}"".format(len(y_train), len(y_dev)))\n\n\n# Training\n# ==================================================\n\nwith tf.Graph().as_default():\n    session_conf = tf.ConfigProto(\n      allow_soft_placement=FLAGS.allow_soft_placement,\n      log_device_placement=FLAGS.log_device_placement)\n    sess = tf.Session(config=session_conf)\n    with sess.as_default():\n        cnn = TextCNN(\n            sequence_length=x_train.shape[1],\n            num_classes=y_train.shape[1],\n            vocab_size=len(vocab_processor.vocabulary_),\n            embedding_size=FLAGS.embedding_dim,\n            filter_sizes=list(map(int, FLAGS.filter_sizes.split("",""))),\n            num_filters=FLAGS.num_filters,\n            l2_reg_lambda=FLAGS.l2_reg_lambda)\n\n        # Define Training procedure\n        global_step = tf.Variable(0, name=""global_step"", trainable=False)\n        optimizer = tf.train.AdamOptimizer(1e-3)\n        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n        # Keep track of gradient values and sparsity (optional)\n        grad_summaries = []\n        for g, v in grads_and_vars:\n            if g is not None:\n                grad_hist_summary = tf.summary.histogram(""{}/grad/hist"".format(v.name), g)\n                sparsity_summary = tf.summary.scalar(""{}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                grad_summaries.append(grad_hist_summary)\n                grad_summaries.append(sparsity_summary)\n        grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n        # Output directory for models and summaries\n        timestamp = str(int(time.time()))\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", timestamp))\n        print(""Writing to {}\\n"".format(out_dir))\n\n        # Summaries for loss and accuracy\n        loss_summary = tf.summary.scalar(""loss"", cnn.loss)\n        acc_summary = tf.summary.scalar(""accuracy"", cnn.accuracy)\n\n        # Train Summaries\n        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n        train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n        # Dev summaries\n        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n        dev_summary_dir = os.path.join(out_dir, ""summaries"", ""dev"")\n        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n\n        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n        checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n        checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n\n        # Write vocabulary\n        vocab_processor.save(os.path.join(out_dir, ""vocab""))\n\n        # Initialize all variables\n        sess.run(tf.global_variables_initializer())\n\n        def train_step(x_batch, y_batch):\n            """"""\n            A single training step\n            """"""\n            feed_dict = {\n              cnn.input_x: x_batch,\n              cnn.input_y: y_batch,\n              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n            }\n            _, step, summaries, loss, accuracy = sess.run(\n                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n                feed_dict)\n            time_str = datetime.datetime.now().isoformat()\n            print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n            train_summary_writer.add_summary(summaries, step)\n\n        def dev_step(x_batch, y_batch, writer=None):\n            """"""\n            Evaluates model on a dev set\n            """"""\n            feed_dict = {\n              cnn.input_x: x_batch,\n              cnn.input_y: y_batch,\n              cnn.dropout_keep_prob: 1.0\n            }\n            step, summaries, loss, accuracy = sess.run(\n                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n                feed_dict)\n            time_str = datetime.datetime.now().isoformat()\n            print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n            if writer:\n                writer.add_summary(summaries, step)\n\n        # Generate batches\n        batches = data_helpers.batch_iter(\n            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n        # Training loop. For each batch...\n        for batch in batches:\n            x_batch, y_batch = zip(*batch)\n            train_step(x_batch, y_batch)\n            current_step = tf.train.global_step(sess, global_step)\n            if current_step % FLAGS.evaluate_every == 0:\n                print(""\\nEvaluation:"")\n                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n                print("""")\n            if current_step % FLAGS.checkpoint_every == 0:\n                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                print(""Saved model checkpoint to {}\\n"".format(path))\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week09-src/slim/datasets/dataset_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A factory-pattern class which returns classification image/label pairs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import cifar10\nfrom datasets import flowers\nfrom datasets import imagenet\nfrom datasets import mnist\nfrom datasets import myimages\n\ndatasets_map = {\n    \'cifar10\': cifar10,\n    \'flowers\': flowers,\n    \'imagenet\': imagenet,\n    \'mnist\': mnist,\n    \'myimages\':myimages,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Given a dataset name and a split_name returns a Dataset.\n\n  Args:\n    name: String, the name of the dataset.\n    split_name: A train/test split name.\n    dataset_dir: The directory where the dataset files are stored.\n    file_pattern: The file pattern to use for matching the dataset source files.\n    reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n      reader defined by each dataset is used.\n\n  Returns:\n    A `Dataset` class.\n\n  Raises:\n    ValueError: If the dataset `name` is unknown.\n  """"""\n  if name not in datasets_map:\n    raise ValueError(\'Name of dataset unknown %s\' % name)\n  return datasets_map[name].get_split(\n      split_name,\n      dataset_dir,\n      file_pattern,\n      reader)\n'"
3.dl_framework/tensorflow/《深度学习框架Tensorflow学习与应用》笔记/code/week09-src/slim/datasets/myimages.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the flowers dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/datasets/download_and_convert_flowers.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'image_%s_*.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 1000, \'test\': 500}\n\n_NUM_CLASSES = 5\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying size.\',\n    \'label\': \'A single integer between 0 and 4\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading flowers.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
