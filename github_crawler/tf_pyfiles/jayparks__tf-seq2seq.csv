file_path,api_count,code
decode.py,14,"b'\n#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport math\nimport time\nimport json\nimport random\n\nfrom collections import OrderedDict\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom data.data_iterator import TextIterator\n\nimport data.util as util\nimport data.data_utils as data_utils\nfrom data.data_utils import prepare_batch\nfrom data.data_utils import prepare_train_batch\n\nfrom seq2seq_model import Seq2SeqModel\n\n# Decoding parameters\ntf.app.flags.DEFINE_integer(\'beam_width\', 12, \'Beam width used in beamsearch\')\ntf.app.flags.DEFINE_integer(\'decode_batch_size\', 80, \'Batch size used for decoding\')\ntf.app.flags.DEFINE_integer(\'max_decode_step\', 500, \'Maximum time step limit to decode\')\ntf.app.flags.DEFINE_boolean(\'write_n_best\', False, \'Write n-best list (n=beam_width)\')\ntf.app.flags.DEFINE_string(\'model_path\', None, \'Path to a specific model checkpoint.\')\ntf.app.flags.DEFINE_string(\'decode_input\', \'data/newstest2012.bpe.de\', \'Decoding input path\')\ntf.app.flags.DEFINE_string(\'decode_output\', \'data/newstest2012.bpe.de.trans\', \'Decoding output path\')\n\n# Runtime parameters\ntf.app.flags.DEFINE_boolean(\'allow_soft_placement\', True, \'Allow device soft placement\')\ntf.app.flags.DEFINE_boolean(\'log_device_placement\', False, \'Log placement of ops on devices\')\n\nFLAGS = tf.app.flags.FLAGS\n\ndef load_config(FLAGS):\n    \n    config = util.unicode_to_utf8(\n        json.load(open(\'%s.json\' % FLAGS.model_path, \'rb\')))\n    for key, value in FLAGS.__flags.items():\n        config[key] = value\n\n    return config\n\n\ndef load_model(session, config):\n    \n    model = Seq2SeqModel(config, \'decode\')\n    if tf.train.checkpoint_exists(FLAGS.model_path):\n        print \'Reloading model parameters..\'\n        model.restore(session, FLAGS.model_path)\n    else:\n        raise ValueError(\n            \'No such file:[{}]\'.format(FLAGS.model_path))\n    return model\n\n\ndef decode():\n    # Load model config\n    config = load_config(FLAGS)\n\n    # Load source data to decode\n    test_set = TextIterator(source=config[\'decode_input\'],\n                            batch_size=config[\'decode_batch_size\'],\n                            source_dict=config[\'source_vocabulary\'],\n                            maxlen=None,\n                            n_words_source=config[\'num_encoder_symbols\'])\n\n    # Load inverse dictionary used in decoding\n    target_inverse_dict = data_utils.load_inverse_dict(config[\'target_vocabulary\'])\n    \n    # Initiate TF session\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement, \n        log_device_placement=FLAGS.log_device_placement, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n\n        # Reload existing checkpoint\n        model = load_model(sess, config)\n        try:\n            print \'Decoding {}..\'.format(FLAGS.decode_input)\n            if FLAGS.write_n_best:\n                fout = [data_utils.fopen((""%s_%d"" % (FLAGS.decode_output, k)), \'w\') \\\n                        for k in range(FLAGS.beam_width)]\n            else:\n                fout = [data_utils.fopen(FLAGS.decode_output, \'w\')]\n            \n            for idx, source_seq in enumerate(test_set):\n                source, source_len = prepare_batch(source_seq)\n                # predicted_ids: GreedyDecoder; [batch_size, max_time_step, 1]\n                # BeamSearchDecoder; [batch_size, max_time_step, beam_width]\n                predicted_ids = model.predict(sess, encoder_inputs=source, \n                                              encoder_inputs_length=source_len)\n                   \n                # Write decoding results\n                for k, f in reversed(list(enumerate(fout))):\n                    for seq in predicted_ids:\n                        f.write(str(data_utils.seq2words(seq[:,k], target_inverse_dict)) + \'\\n\')\n                    if not FLAGS.write_n_best:\n                        break\n                print \'  {}th line decoded\'.format(idx * FLAGS.decode_batch_size)\n                \n            print \'Decoding terminated\'\n        except IOError:\n            pass\n        finally:\n            [f.close() for f in fout]\n\n\ndef main(_):\n    decode()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n'"
seq2seq_model.py,51,"b'\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.seq2seq as seq2seq\n\nfrom tensorflow.python.ops.rnn_cell import GRUCell\nfrom tensorflow.python.ops.rnn_cell import LSTMCell\nfrom tensorflow.python.ops.rnn_cell import MultiRNNCell\nfrom tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.layers.core import Dense\nfrom tensorflow.python.util import nest\n\nfrom tensorflow.contrib.seq2seq.python.ops import attention_wrapper\nfrom tensorflow.contrib.seq2seq.python.ops import beam_search_decoder\n\nimport data.data_utils as data_utils\n\nclass Seq2SeqModel(object):\n\n    def __init__(self, config, mode):\n\n        assert mode.lower() in [\'train\', \'decode\']\n\n        self.config = config\n        self.mode = mode.lower()\n\n        self.cell_type = config[\'cell_type\']\n        self.hidden_units = config[\'hidden_units\']\n        self.depth = config[\'depth\']\n        self.attention_type = config[\'attention_type\']\n        self.embedding_size = config[\'embedding_size\']\n        #self.bidirectional = config.bidirectional\n       \n        self.num_encoder_symbols = config[\'num_encoder_symbols\']\n        self.num_decoder_symbols = config[\'num_decoder_symbols\']\n\n        self.use_residual = config[\'use_residual\']\n        self.attn_input_feeding = config[\'attn_input_feeding\']\n        self.use_dropout = config[\'use_dropout\']\n        self.keep_prob = 1.0 - config[\'dropout_rate\']\n\n        self.optimizer = config[\'optimizer\']\n        self.learning_rate = config[\'learning_rate\']\n        self.max_gradient_norm = config[\'max_gradient_norm\']\n        self.global_step = tf.Variable(0, trainable=False, name=\'global_step\')\n        self.global_epoch_step = tf.Variable(0, trainable=False, name=\'global_epoch_step\')\n        self.global_epoch_step_op = \\\n\t    tf.assign(self.global_epoch_step, self.global_epoch_step+1)\n\n        self.dtype = tf.float16 if config[\'use_fp16\'] else tf.float32\n        self.keep_prob_placeholder = tf.placeholder(self.dtype, shape=[], name=\'keep_prob\')\n\n        self.use_beamsearch_decode=False \n        if self.mode == \'decode\':\n            self.beam_width = config[\'beam_width\']\n            self.use_beamsearch_decode = True if self.beam_width > 1 else False\n            self.max_decode_step = config[\'max_decode_step\']\n \n        self.build_model()\n\n       \n    def build_model(self):\n        print(""building model.."")\n\n        # Building encoder and decoder networks\n        self.init_placeholders()\n        self.build_encoder()\n        self.build_decoder()\n\n        # Merge all the training summaries\n        self.summary_op = tf.summary.merge_all()\n\n\n    def init_placeholders(self):       \n        # encoder_inputs: [batch_size, max_time_steps]\n        self.encoder_inputs = tf.placeholder(dtype=tf.int32,\n            shape=(None, None), name=\'encoder_inputs\')\n\n        # encoder_inputs_length: [batch_size]\n        self.encoder_inputs_length = tf.placeholder(\n            dtype=tf.int32, shape=(None,), name=\'encoder_inputs_length\')\n\n        # get dynamic batch_size\n        self.batch_size = tf.shape(self.encoder_inputs)[0]\n        if self.mode == \'train\':\n\n            # decoder_inputs: [batch_size, max_time_steps]\n            self.decoder_inputs = tf.placeholder(\n                dtype=tf.int32, shape=(None, None), name=\'decoder_inputs\')\n            # decoder_inputs_length: [batch_size]\n            self.decoder_inputs_length = tf.placeholder(\n                dtype=tf.int32, shape=(None,), name=\'decoder_inputs_length\')\n\n            decoder_start_token = tf.ones(\n                shape=[self.batch_size, 1], dtype=tf.int32) * data_utils.start_token\n            decoder_end_token = tf.ones(\n                shape=[self.batch_size, 1], dtype=tf.int32) * data_utils.end_token            \n\n            # decoder_inputs_train: [batch_size , max_time_steps + 1]\n            # insert _GO symbol in front of each decoder input\n            self.decoder_inputs_train = tf.concat([decoder_start_token,\n                                                  self.decoder_inputs], axis=1)\n\n            # decoder_inputs_length_train: [batch_size]\n            self.decoder_inputs_length_train = self.decoder_inputs_length + 1\n\n            # decoder_targets_train: [batch_size, max_time_steps + 1]\n            # insert EOS symbol at the end of each decoder input\n            self.decoder_targets_train = tf.concat([self.decoder_inputs,\n                                                   decoder_end_token], axis=1)\n\n\n    def build_encoder(self):\n        print(""building encoder.."")\n        with tf.variable_scope(\'encoder\'):\n            # Building encoder_cell\n            self.encoder_cell = self.build_encoder_cell()\n            \n            # Initialize encoder_embeddings to have variance=1.\n            sqrt3 = math.sqrt(3)  # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n            initializer = tf.random_uniform_initializer(-sqrt3, sqrt3, dtype=self.dtype)\n             \n            self.encoder_embeddings = tf.get_variable(name=\'embedding\',\n                shape=[self.num_encoder_symbols, self.embedding_size],\n                initializer=initializer, dtype=self.dtype)\n            \n            # Embedded_inputs: [batch_size, time_step, embedding_size]\n            self.encoder_inputs_embedded = tf.nn.embedding_lookup(\n                params=self.encoder_embeddings, ids=self.encoder_inputs)\n       \n            # Input projection layer to feed embedded inputs to the cell\n            # ** Essential when use_residual=True to match input/output dims\n            input_layer = Dense(self.hidden_units, dtype=self.dtype, name=\'input_projection\')\n\n            # Embedded inputs having gone through input projection layer\n            self.encoder_inputs_embedded = input_layer(self.encoder_inputs_embedded)\n    \n            # Encode input sequences into context vectors:\n            # encoder_outputs: [batch_size, max_time_step, cell_output_size]\n            # encoder_state: [batch_size, cell_output_size]\n            self.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(\n                cell=self.encoder_cell, inputs=self.encoder_inputs_embedded,\n                sequence_length=self.encoder_inputs_length, dtype=self.dtype,\n                time_major=False)\n\n\n    def build_decoder(self):\n        print(""building decoder and attention.."")\n        with tf.variable_scope(\'decoder\'):\n            # Building decoder_cell and decoder_initial_state\n            self.decoder_cell, self.decoder_initial_state = self.build_decoder_cell()\n\n            # Initialize decoder embeddings to have variance=1.\n            sqrt3 = math.sqrt(3)  # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n            initializer = tf.random_uniform_initializer(-sqrt3, sqrt3, dtype=self.dtype)\n             \n            self.decoder_embeddings = tf.get_variable(name=\'embedding\',\n                shape=[self.num_decoder_symbols, self.embedding_size],\n                initializer=initializer, dtype=self.dtype)\n\n            # Input projection layer to feed embedded inputs to the cell\n            # ** Essential when use_residual=True to match input/output dims\n            input_layer = Dense(self.hidden_units, dtype=self.dtype, name=\'input_projection\')\n\n            # Output projection layer to convert cell_outputs to logits\n            output_layer = Dense(self.num_decoder_symbols, name=\'output_projection\')\n\n            if self.mode == \'train\':\n                # decoder_inputs_embedded: [batch_size, max_time_step + 1, embedding_size]\n                self.decoder_inputs_embedded = tf.nn.embedding_lookup(\n                    params=self.decoder_embeddings, ids=self.decoder_inputs_train)\n               \n                # Embedded inputs having gone through input projection layer\n                self.decoder_inputs_embedded = input_layer(self.decoder_inputs_embedded)\n\n                # Helper to feed inputs for training: read inputs from dense ground truth vectors\n                training_helper = seq2seq.TrainingHelper(inputs=self.decoder_inputs_embedded,\n                                                   sequence_length=self.decoder_inputs_length_train,\n                                                   time_major=False,\n                                                   name=\'training_helper\')\n\n                training_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n                                                   helper=training_helper,\n                                                   initial_state=self.decoder_initial_state,\n                                                   output_layer=output_layer)\n                                                   #output_layer=None)\n                    \n                # Maximum decoder time_steps in current batch\n                max_decoder_length = tf.reduce_max(self.decoder_inputs_length_train)\n\n                # decoder_outputs_train: BasicDecoderOutput\n                #                        namedtuple(rnn_outputs, sample_id)\n                # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, num_decoder_symbols] if output_time_major=False\n                #                                   [max_time_step + 1, batch_size, num_decoder_symbols] if output_time_major=True\n                # decoder_outputs_train.sample_id: [batch_size], tf.int32\n                (self.decoder_outputs_train, self.decoder_last_state_train, \n                 self.decoder_outputs_length_train) = (seq2seq.dynamic_decode(\n                    decoder=training_decoder,\n                    output_time_major=False,\n                    impute_finished=True,\n                    maximum_iterations=max_decoder_length))\n                 \n                # More efficient to do the projection on the batch-time-concatenated tensor\n                # logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n                # self.decoder_logits_train = output_layer(self.decoder_outputs_train.rnn_output)\n                self.decoder_logits_train = tf.identity(self.decoder_outputs_train.rnn_output) \n                # Use argmax to extract decoder symbols to emit\n                self.decoder_pred_train = tf.argmax(self.decoder_logits_train, axis=-1,\n                                                    name=\'decoder_pred_train\')\n\n                # masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n                masks = tf.sequence_mask(lengths=self.decoder_inputs_length_train, \n                                         maxlen=max_decoder_length, dtype=self.dtype, name=\'masks\')\n\n                # Computes per word average cross-entropy over a batch\n                # Internally calls \'nn_ops.sparse_softmax_cross_entropy_with_logits\' by default\n                self.loss = seq2seq.sequence_loss(logits=self.decoder_logits_train, \n                                                  targets=self.decoder_targets_train,\n                                                  weights=masks,\n                                                  average_across_timesteps=True,\n                                                  average_across_batch=True,)\n                # Training summary for the current batch_loss\n                tf.summary.scalar(\'loss\', self.loss)\n\n                # Contruct graphs for minimizing loss\n                self.init_optimizer()\n\n            elif self.mode == \'decode\':\n        \n                # Start_tokens: [batch_size,] `int32` vector\n                start_tokens = tf.ones([self.batch_size,], tf.int32) * data_utils.start_token\n                end_token = data_utils.end_token\n\n                def embed_and_input_proj(inputs):\n                    return input_layer(tf.nn.embedding_lookup(self.decoder_embeddings, inputs))\n                    \n                if not self.use_beamsearch_decode:\n                    # Helper to feed inputs for greedy decoding: uses the argmax of the output\n                    decoding_helper = seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n                                                                    end_token=end_token,\n                                                                    embedding=embed_and_input_proj)\n                    # Basic decoder performs greedy decoding at each time step\n                    print(""building greedy decoder.."")\n                    inference_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n                                                             helper=decoding_helper,\n                                                             initial_state=self.decoder_initial_state,\n                                                             output_layer=output_layer)\n                else:\n                    # Beamsearch is used to approximately find the most likely translation\n                    print(""building beamsearch decoder.."")\n                    inference_decoder = beam_search_decoder.BeamSearchDecoder(cell=self.decoder_cell,\n                                                               embedding=embed_and_input_proj,\n                                                               start_tokens=start_tokens,\n                                                               end_token=end_token,\n                                                               initial_state=self.decoder_initial_state,\n                                                               beam_width=self.beam_width,\n                                                               output_layer=output_layer,)\n                # For GreedyDecoder, return\n                # decoder_outputs_decode: BasicDecoderOutput instance\n                #                         namedtuple(rnn_outputs, sample_id)\n                # decoder_outputs_decode.rnn_output: [batch_size, max_time_step, num_decoder_symbols] \tif output_time_major=False\n                #                                    [max_time_step, batch_size, num_decoder_symbols] \tif output_time_major=True\n                # decoder_outputs_decode.sample_id: [batch_size, max_time_step], tf.int32\t\tif output_time_major=False\n                #                                   [max_time_step, batch_size], tf.int32               if output_time_major=True \n                \n                # For BeamSearchDecoder, return\n                # decoder_outputs_decode: FinalBeamSearchDecoderOutput instance\n                #                         namedtuple(predicted_ids, beam_search_decoder_output)\n                # decoder_outputs_decode.predicted_ids: [batch_size, max_time_step, beam_width] if output_time_major=False\n                #                                       [max_time_step, batch_size, beam_width] if output_time_major=True\n                # decoder_outputs_decode.beam_search_decoder_output: BeamSearchDecoderOutput instance\n                #                                                    namedtuple(scores, predicted_ids, parent_ids)\n\n                (self.decoder_outputs_decode, self.decoder_last_state_decode,\n                 self.decoder_outputs_length_decode) = (seq2seq.dynamic_decode(\n                    decoder=inference_decoder,\n                    output_time_major=False,\n                    #impute_finished=True,\t# error occurs\n                    maximum_iterations=self.max_decode_step))\n\n                if not self.use_beamsearch_decode:\n                    # decoder_outputs_decode.sample_id: [batch_size, max_time_step]\n                    # Or use argmax to find decoder symbols to emit:\n                    # self.decoder_pred_decode = tf.argmax(self.decoder_outputs_decode.rnn_output,\n                    #                                      axis=-1, name=\'decoder_pred_decode\')\n\n                    # Here, we use expand_dims to be compatible with the result of the beamsearch decoder\n                    # decoder_pred_decode: [batch_size, max_time_step, 1] (output_major=False)\n                    self.decoder_pred_decode = tf.expand_dims(self.decoder_outputs_decode.sample_id, -1)\n\n                else:\n                    # Use beam search to approximately find the most likely translation\n                    # decoder_pred_decode: [batch_size, max_time_step, beam_width] (output_major=False)\n                    self.decoder_pred_decode = self.decoder_outputs_decode.predicted_ids\n\n\n    def build_single_cell(self):\n        cell_type = LSTMCell\n        if (self.cell_type.lower() == \'gru\'):\n            cell_type = GRUCell\n        cell = cell_type(self.hidden_units)\n\n        if self.use_dropout:\n            cell = DropoutWrapper(cell, dtype=self.dtype,\n                                  output_keep_prob=self.keep_prob_placeholder,)\n        if self.use_residual:\n            cell = ResidualWrapper(cell)\n            \n        return cell\n\n\n    # Building encoder cell\n    def build_encoder_cell (self):\n\n        return MultiRNNCell([self.build_single_cell() for i in range(self.depth)])\n\n\n    # Building decoder cell and attention. Also returns decoder_initial_state\n    def build_decoder_cell(self):\n\n        encoder_outputs = self.encoder_outputs\n        encoder_last_state = self.encoder_last_state\n        encoder_inputs_length = self.encoder_inputs_length\n\n        # To use BeamSearchDecoder, encoder_outputs, encoder_last_state, encoder_inputs_length \n        # needs to be tiled so that: [batch_size, .., ..] -> [batch_size x beam_width, .., ..]\n        if self.use_beamsearch_decode:\n            print (""use beamsearch decoding.."")\n            encoder_outputs = seq2seq.tile_batch(\n                self.encoder_outputs, multiplier=self.beam_width)\n            encoder_last_state = nest.map_structure(\n                lambda s: seq2seq.tile_batch(s, self.beam_width), self.encoder_last_state)\n            encoder_inputs_length = seq2seq.tile_batch(\n                self.encoder_inputs_length, multiplier=self.beam_width)\n\n        # Building attention mechanism: Default Bahdanau\n        # \'Bahdanau\' style attention: https://arxiv.org/abs/1409.0473\n        self.attention_mechanism = attention_wrapper.BahdanauAttention(\n            num_units=self.hidden_units, memory=encoder_outputs,\n            memory_sequence_length=encoder_inputs_length,) \n        # \'Luong\' style attention: https://arxiv.org/abs/1508.04025\n        if self.attention_type.lower() == \'luong\':\n            self.attention_mechanism = attention_wrapper.LuongAttention(\n                num_units=self.hidden_units, memory=encoder_outputs, \n                memory_sequence_length=encoder_inputs_length,)\n \n        # Building decoder_cell\n        self.decoder_cell_list = [\n            self.build_single_cell() for i in range(self.depth)]\n        decoder_initial_state = encoder_last_state\n\n        def attn_decoder_input_fn(inputs, attention):\n            if not self.attn_input_feeding:\n                return inputs\n\n            # Essential when use_residual=True\n            _input_layer = Dense(self.hidden_units, dtype=self.dtype,\n                                 name=\'attn_input_feeding\')\n            return _input_layer(array_ops.concat([inputs, attention], -1))\n\n        # AttentionWrapper wraps RNNCell with the attention_mechanism\n        # Note: We implement Attention mechanism only on the top decoder layer\n        self.decoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\n            cell=self.decoder_cell_list[-1],\n            attention_mechanism=self.attention_mechanism,\n            attention_layer_size=self.hidden_units,\n            cell_input_fn=attn_decoder_input_fn,\n            initial_cell_state=encoder_last_state[-1],\n            alignment_history=False,\n            name=\'Attention_Wrapper\')\n\n        # To be compatible with AttentionWrapper, the encoder last state\n        # of the top layer should be converted into the AttentionWrapperState form\n        # We can easily do this by calling AttentionWrapper.zero_state\n\n        # Also if beamsearch decoding is used, the batch_size argument in .zero_state\n        # should be ${decoder_beam_width} times to the origianl batch_size\n        batch_size = self.batch_size if not self.use_beamsearch_decode \\\n                     else self.batch_size * self.beam_width\n        initial_state = [state for state in encoder_last_state]\n\n        initial_state[-1] = self.decoder_cell_list[-1].zero_state(\n          batch_size=batch_size, dtype=self.dtype)\n        decoder_initial_state = tuple(initial_state)\n\n        return MultiRNNCell(self.decoder_cell_list), decoder_initial_state\n\n\n    def init_optimizer(self):\n        print(""setting optimizer.."")\n        # Gradients and SGD update operation for training the model\n        trainable_params = tf.trainable_variables()\n        if self.optimizer.lower() == \'adadelta\':\n            self.opt = tf.train.AdadeltaOptimizer(learning_rate=self.learning_rate)\n        elif self.optimizer.lower() == \'adam\':\n            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n        elif self.optimizer.lower() == \'rmsprop\':\n            self.opt = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate)\n        else:\n            self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n\n        # Compute gradients of loss w.r.t. all trainable variables\n        gradients = tf.gradients(self.loss, trainable_params)\n\n        # Clip gradients by a given maximum_gradient_norm\n        clip_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n\n        # Update the model\n        self.updates = self.opt.apply_gradients(\n            zip(clip_gradients, trainable_params), global_step=self.global_step)\n\n    def save(self, sess, path, var_list=None, global_step=None):\n        # var_list = None returns the list of all saveable variables\n        saver = tf.train.Saver(var_list)\n\n        # temporary code\n        #del tf.get_collection_ref(\'LAYER_NAME_UIDS\')[0]\n        save_path = saver.save(sess, save_path=path, global_step=global_step)\n        print(\'model saved at %s\' % save_path)\n        \n\n    def restore(self, sess, path, var_list=None):\n        # var_list = None returns the list of all saveable variables\n        saver = tf.train.Saver(var_list)\n        saver.restore(sess, save_path=path)\n        print(\'model restored from %s\' % path)\n\n\n    def train(self, sess, encoder_inputs, encoder_inputs_length, \n              decoder_inputs, decoder_inputs_length):\n        """"""Run a train step of the model feeding the given inputs.\n\n        Args:\n          session: tensorflow session to use.\n          encoder_inputs: a numpy int matrix of [batch_size, max_source_time_steps]\n              to feed as encoder inputs\n          encoder_inputs_length: a numpy int vector of [batch_size] \n              to feed as sequence lengths for each element in the given batch\n          decoder_inputs: a numpy int matrix of [batch_size, max_target_time_steps]\n              to feed as decoder inputs\n          decoder_inputs_length: a numpy int vector of [batch_size]\n              to feed as sequence lengths for each element in the given batch\n\n        Returns:\n          A triple consisting of gradient norm (or None if we did not do backward),\n          average perplexity, and the outputs.\n        """"""\n        # Check if the model is \'training\' mode\n        if self.mode.lower() != \'train\':\n            raise ValueError(""train step can only be operated in train mode"")\n\n        input_feed = self.check_feeds(encoder_inputs, encoder_inputs_length,\n                                      decoder_inputs, decoder_inputs_length, False)\n        # Input feeds for dropout\n        input_feed[self.keep_prob_placeholder.name] = self.keep_prob\n \n        output_feed = [self.updates,\t# Update Op that does optimization\n                       self.loss,\t# Loss for current batch\n                       self.summary_op]\t# Training summary\n        \n        outputs = sess.run(output_feed, input_feed)\n        return outputs[1], outputs[2]\t# loss, summary\n\n\n    def eval(self, sess, encoder_inputs, encoder_inputs_length,\n             decoder_inputs, decoder_inputs_length):\n        """"""Run a evaluation step of the model feeding the given inputs.\n\n        Args:\n          session: tensorflow session to use.\n          encoder_inputs: a numpy int matrix of [batch_size, max_source_time_steps]\n              to feed as encoder inputs\n          encoder_inputs_length: a numpy int vector of [batch_size] \n              to feed as sequence lengths for each element in the given batch\n          decoder_inputs: a numpy int matrix of [batch_size, max_target_time_steps]\n              to feed as decoder inputs\n          decoder_inputs_length: a numpy int vector of [batch_size]\n              to feed as sequence lengths for each element in the given batch\n\n        Returns:\n          A triple consisting of gradient norm (or None if we did not do backward),\n          average perplexity, and the outputs.\n        """"""\n        \n        input_feed = self.check_feeds(encoder_inputs, encoder_inputs_length,\n                                      decoder_inputs, decoder_inputs_length, False)\n        # Input feeds for dropout\n        input_feed[self.keep_prob_placeholder.name] = 1.0\n\n        output_feed = [self.loss,\t# Loss for current batch\n                       self.summary_op]\t# Training summary\n        outputs = sess.run(output_feed, input_feed)\n        return outputs[0], outputs[1]\t# loss\n\n\n    def predict(self, sess, encoder_inputs, encoder_inputs_length):\n        \n        input_feed = self.check_feeds(encoder_inputs, encoder_inputs_length, \n                                      decoder_inputs=None, decoder_inputs_length=None, \n                                      decode=True)\n\n        # Input feeds for dropout\n        input_feed[self.keep_prob_placeholder.name] = 1.0\n \n        output_feed = [self.decoder_pred_decode]\n        outputs = sess.run(output_feed, input_feed)\n\n\t\t\t\t# GreedyDecoder: [batch_size, max_time_step]\n        return outputs[0]\t# BeamSearchDecoder: [batch_size, max_time_step, beam_width]\n\n\n    def check_feeds(self, encoder_inputs, encoder_inputs_length, \n                    decoder_inputs, decoder_inputs_length, decode):\n        """"""\n        Args:\n          encoder_inputs: a numpy int matrix of [batch_size, max_source_time_steps]\n              to feed as encoder inputs\n          encoder_inputs_length: a numpy int vector of [batch_size] \n              to feed as sequence lengths for each element in the given batch\n          decoder_inputs: a numpy int matrix of [batch_size, max_target_time_steps]\n              to feed as decoder inputs\n          decoder_inputs_length: a numpy int vector of [batch_size]\n              to feed as sequence lengths for each element in the given batch\n          decode: a scalar boolean that indicates decode mode\n        Returns:\n          A feed for the model that consists of encoder_inputs, encoder_inputs_length,\n          decoder_inputs, decoder_inputs_length\n        """""" \n\n        input_batch_size = encoder_inputs.shape[0]\n        if input_batch_size != encoder_inputs_length.shape[0]:\n            raise ValueError(""Encoder inputs and their lengths must be equal in their ""\n                ""batch_size, %d != %d"" % (input_batch_size, encoder_inputs_length.shape[0]))\n\n        if not decode:\n            target_batch_size = decoder_inputs.shape[0]\n            if target_batch_size != input_batch_size:\n                raise ValueError(""Encoder inputs and Decoder inputs must be equal in their ""\n                    ""batch_size, %d != %d"" % (input_batch_size, target_batch_size))\n            if target_batch_size != decoder_inputs_length.shape[0]:\n                raise ValueError(""Decoder targets and their lengths must be equal in their ""\n                    ""batch_size, %d != %d"" % (target_batch_size, decoder_inputs_length.shape[0]))\n\n        input_feed = {}\n    \n        input_feed[self.encoder_inputs.name] = encoder_inputs\n        input_feed[self.encoder_inputs_length.name] = encoder_inputs_length\n\n        if not decode:\n            input_feed[self.decoder_inputs.name] = decoder_inputs\n            input_feed[self.decoder_inputs_length.name] = decoder_inputs_length\n\n        return input_feed \n\n'"
train.py,42,"b'\n#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport math\nimport time\nimport json\nimport random\n\nfrom collections import OrderedDict\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom data.data_iterator import TextIterator\nfrom data.data_iterator import BiTextIterator\n\nimport data.data_utils as data_utils\nfrom data.data_utils import prepare_batch\nfrom data.data_utils import prepare_train_batch\n\nfrom seq2seq_model import Seq2SeqModel\n\n\n# Data loading parameters\ntf.app.flags.DEFINE_string(\'source_vocabulary\', \'data/europarl-v7.1.4M.de.json\', \'Path to source vocabulary\')\ntf.app.flags.DEFINE_string(\'target_vocabulary\', \'data/europarl-v7.1.4M.fr.json\', \'Path to target vocabulary\')\ntf.app.flags.DEFINE_string(\'source_train_data\', \'data/europarl-v7.1.4M.de\', \'Path to source training data\')\ntf.app.flags.DEFINE_string(\'target_train_data\', \'data/europarl-v7.1.4M.fr\', \'Path to target training data\')\ntf.app.flags.DEFINE_string(\'source_valid_data\', \'data/newstest2012.bpe.de\', \'Path to source validation data\')\ntf.app.flags.DEFINE_string(\'target_valid_data\', \'data/newstest2012.bpe.fr\', \'Path to target validation data\')\n\n# Network parameters\ntf.app.flags.DEFINE_string(\'cell_type\', \'lstm\', \'RNN cell for encoder and decoder, default: lstm\')\ntf.app.flags.DEFINE_string(\'attention_type\', \'bahdanau\', \'Attention mechanism: (bahdanau, luong), default: bahdanau\')\ntf.app.flags.DEFINE_integer(\'hidden_units\', 1024, \'Number of hidden units in each layer\')\ntf.app.flags.DEFINE_integer(\'depth\', 2, \'Number of layers in each encoder and decoder\')\ntf.app.flags.DEFINE_integer(\'embedding_size\', 500, \'Embedding dimensions of encoder and decoder inputs\')\ntf.app.flags.DEFINE_integer(\'num_encoder_symbols\', 30000, \'Source vocabulary size\')\ntf.app.flags.DEFINE_integer(\'num_decoder_symbols\', 30000, \'Target vocabulary size\')\n\ntf.app.flags.DEFINE_boolean(\'use_residual\', True, \'Use residual connection between layers\')\ntf.app.flags.DEFINE_boolean(\'attn_input_feeding\', False, \'Use input feeding method in attentional decoder\')\ntf.app.flags.DEFINE_boolean(\'use_dropout\', True, \'Use dropout in each rnn cell\')\ntf.app.flags.DEFINE_float(\'dropout_rate\', 0.3, \'Dropout probability for input/output/state units (0.0: no dropout)\')\n\n# Training parameters\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.0002, \'Learning rate\')\ntf.app.flags.DEFINE_float(\'max_gradient_norm\', 1.0, \'Clip gradients to this norm\')\ntf.app.flags.DEFINE_integer(\'batch_size\', 128, \'Batch size\')\ntf.app.flags.DEFINE_integer(\'max_epochs\', 10, \'Maximum # of training epochs\')\ntf.app.flags.DEFINE_integer(\'max_load_batches\', 20, \'Maximum # of batches to load at one time\')\ntf.app.flags.DEFINE_integer(\'max_seq_length\', 50, \'Maximum sequence length\')\ntf.app.flags.DEFINE_integer(\'display_freq\', 100, \'Display training status every this iteration\')\ntf.app.flags.DEFINE_integer(\'save_freq\', 11500, \'Save model checkpoint every this iteration\')\ntf.app.flags.DEFINE_integer(\'valid_freq\', 1150000, \'Evaluate model every this iteration: valid_data needed\')\ntf.app.flags.DEFINE_string(\'optimizer\', \'adam\', \'Optimizer for training: (adadelta, adam, rmsprop)\')\ntf.app.flags.DEFINE_string(\'model_dir\', \'model/\', \'Path to save model checkpoints\')\ntf.app.flags.DEFINE_string(\'model_name\', \'translate.ckpt\', \'File name used for model checkpoints\')\ntf.app.flags.DEFINE_boolean(\'shuffle_each_epoch\', True, \'Shuffle training dataset for each epoch\')\ntf.app.flags.DEFINE_boolean(\'sort_by_length\', True, \'Sort pre-fetched minibatches by their target sequence lengths\')\ntf.app.flags.DEFINE_boolean(\'use_fp16\', False, \'Use half precision float16 instead of float32 as dtype\')\n\n# Runtime parameters\ntf.app.flags.DEFINE_boolean(\'allow_soft_placement\', True, \'Allow device soft placement\')\ntf.app.flags.DEFINE_boolean(\'log_device_placement\', False, \'Log placement of ops on devices\')\n\nFLAGS = tf.app.flags.FLAGS\n\ndef create_model(session, FLAGS):\n\n    config = OrderedDict(sorted(FLAGS.__flags.items()))\n    model = Seq2SeqModel(config, \'train\')\n\n    ckpt = tf.train.get_checkpoint_state(FLAGS.model_dir)\n    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n        print \'Reloading model parameters..\'\n        model.restore(session, ckpt.model_checkpoint_path)\n        \n    else:\n        if not os.path.exists(FLAGS.model_dir):\n            os.makedirs(FLAGS.model_dir)\n        print \'Created new model parameters..\'\n        session.run(tf.global_variables_initializer())\n   \n    return model\n\ndef train():\n    # Load parallel data to train\n    print \'Loading training data..\'\n    train_set = BiTextIterator(source=FLAGS.source_train_data,\n                               target=FLAGS.target_train_data,\n                               source_dict=FLAGS.source_vocabulary,\n                               target_dict=FLAGS.target_vocabulary,\n                               batch_size=FLAGS.batch_size,\n                               maxlen=FLAGS.max_seq_length,\n                               n_words_source=FLAGS.num_encoder_symbols,\n                               n_words_target=FLAGS.num_decoder_symbols,\n                               shuffle_each_epoch=FLAGS.shuffle_each_epoch,\n                               sort_by_length=FLAGS.sort_by_length,\n                               maxibatch_size=FLAGS.max_load_batches)\n\n    if FLAGS.source_valid_data and FLAGS.target_valid_data:\n        print \'Loading validation data..\'\n        valid_set = BiTextIterator(source=FLAGS.source_valid_data,\n                                   target=FLAGS.target_valid_data,\n                                   source_dict=FLAGS.source_vocabulary,\n                                   target_dict=FLAGS.target_vocabulary,\n                                   batch_size=FLAGS.batch_size,\n                                   maxlen=None,\n                                   n_words_source=FLAGS.num_encoder_symbols,\n                                   n_words_target=FLAGS.num_decoder_symbols)\n    else:\n        valid_set = None\n\n    # Initiate TF session\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement, \n        log_device_placement=FLAGS.log_device_placement, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n        \n        # Create a new model or reload existing checkpoint\n        model = create_model(sess, FLAGS)\n        \n        # Create a log writer object\n        log_writer = tf.summary.FileWriter(FLAGS.model_dir, graph=sess.graph)\n\n        \n\n        step_time, loss = 0.0, 0.0\n        words_seen, sents_seen = 0, 0\n        start_time = time.time()\n\n        # Training loop\n        print \'Training..\'\n        for epoch_idx in xrange(FLAGS.max_epochs):\n            if model.global_epoch_step.eval() >= FLAGS.max_epochs:\n                print \'Training is already complete.\', \\\n                      \'current epoch:{}, max epoch:{}\'.format(model.global_epoch_step.eval(), FLAGS.max_epochs)\n                break\n\n            for source_seq, target_seq in train_set:    \n                # Get a batch from training parallel data\n                source, source_len, target, target_len = prepare_train_batch(source_seq, target_seq,\n                                                                             FLAGS.max_seq_length)\n                if source is None or target is None:\n                    print \'No samples under max_seq_length \', FLAGS.max_seq_length\n                    continue\n\n                # Execute a single training step\n                step_loss, summary = model.train(sess, encoder_inputs=source, encoder_inputs_length=source_len, \n                                                 decoder_inputs=target, decoder_inputs_length=target_len)\n\n                loss += float(step_loss) / FLAGS.display_freq\n                words_seen += float(np.sum(source_len+target_len))\n                sents_seen += float(source.shape[0]) # batch_size\n\n                if model.global_step.eval() % FLAGS.display_freq == 0:\n\n                    avg_perplexity = math.exp(float(loss)) if loss < 300 else float(""inf"")\n\n                    time_elapsed = time.time() - start_time\n                    step_time = time_elapsed / FLAGS.display_freq\n\n                    words_per_sec = words_seen / time_elapsed\n                    sents_per_sec = sents_seen / time_elapsed\n\n                    print \'Epoch \', model.global_epoch_step.eval(), \'Step \', model.global_step.eval(), \\\n                          \'Perplexity {0:.2f}\'.format(avg_perplexity), \'Step-time \', step_time, \\\n                          \'{0:.2f} sents/s\'.format(sents_per_sec), \'{0:.2f} words/s\'.format(words_per_sec)\n\n                    loss = 0\n                    words_seen = 0\n                    sents_seen = 0\n                    start_time = time.time()\n\n                    # Record training summary for the current batch\n                    log_writer.add_summary(summary, model.global_step.eval())\n\n                # Execute a validation step\n                if valid_set and model.global_step.eval() % FLAGS.valid_freq == 0:\n                    print \'Validation step\'\n                    valid_loss = 0.0\n                    valid_sents_seen = 0\n                    for source_seq, target_seq in valid_set:\n                        # Get a batch from validation parallel data\n                        source, source_len, target, target_len = prepare_train_batch(source_seq, target_seq)\n\n                        # Compute validation loss: average per word cross entropy loss\n                        step_loss, summary = model.eval(sess, encoder_inputs=source, encoder_inputs_length=source_len,\n                                                        decoder_inputs=target, decoder_inputs_length=target_len)\n                        batch_size = source.shape[0]\n\n                        valid_loss += step_loss * batch_size\n                        valid_sents_seen += batch_size\n                        print \'  {} samples seen\'.format(valid_sents_seen)\n\n                    valid_loss = valid_loss / valid_sents_seen\n                    print \'Valid perplexity: {0:.2f}\'.format(math.exp(valid_loss))\n\n                # Save the model checkpoint\n                if model.global_step.eval() % FLAGS.save_freq == 0:\n                    print \'Saving the model..\'\n                    checkpoint_path = os.path.join(FLAGS.model_dir, FLAGS.model_name)\n                    model.save(sess, checkpoint_path, global_step=model.global_step)\n                    json.dump(model.config,\n                              open(\'%s-%d.json\' % (checkpoint_path, model.global_step.eval()), \'wb\'),\n                              indent=2)\n\n            # Increase the epoch index of the model\n            model.global_epoch_step_op.eval()\n            print \'Epoch {0:} DONE\'.format(model.global_epoch_step.eval())\n        \n        print \'Saving the last model..\'\n        checkpoint_path = os.path.join(FLAGS.model_dir, FLAGS.model_name)\n        model.save(sess, checkpoint_path, global_step=model.global_step)\n        json.dump(model.config,\n                  open(\'%s-%d.json\' % (checkpoint_path, model.global_step.eval()), \'wb\'),\n                  indent=2)\n        \n    print \'Training Terminated\'\n\n\n\ndef main(_):\n    train()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
data/__init__.py,0,b''
data/build_dictionary.py,0,"b""#!/usr/bin/env python\n\nimport numpy\nimport json\n\nimport sys\nimport fileinput\n\nfrom collections import OrderedDict\nfrom data_utils  import extra_tokens\n\ndef main():\n    for filename in sys.argv[1:]:\n        print 'Processing', filename\n        word_freqs = OrderedDict()\n        with open(filename, 'r') as f:\n            for line in f:\n                words_in = line.strip().split(' ')\n                for w in words_in:\n                    if w not in word_freqs:\n                        word_freqs[w] = 0\n                    word_freqs[w] += 1\n        words = word_freqs.keys()\n        freqs = word_freqs.values()\n\n        sorted_idx = numpy.argsort(freqs)\n        sorted_words = [words[ii] for ii in sorted_idx[::-1]]\n\n        worddict = OrderedDict()\n        for ii, ww in enumerate(extra_tokens):\n            worddict[ww] = ii\n        for ii, ww in enumerate(sorted_words):\n            worddict[ww] = ii + len(extra_tokens)\n\n        with open('%s.json'%filename, 'wb') as f:\n            json.dump(worddict, f, indent=2, ensure_ascii=False)\n\n        print 'Done'\n\nif __name__ == '__main__':\n    main()\n"""
data/data_iterator.py,0,"b'\nimport numpy as np\nimport shuffle\nfrom util import load_dict\n\nimport data_utils\n\n\'\'\'\nMuch of this code is based on the data_iterator.py of\nnematus project (https://github.com/rsennrich/nematus)\n\'\'\'\n\nclass TextIterator:\n    """"""Simple Text iterator.""""""\n    def __init__(self, source, source_dict,\n                 batch_size=128, maxlen=None,\n                 n_words_source=-1,\n                 skip_empty=False,\n                 shuffle_each_epoch=False,\n                 sort_by_length=False,\n                 maxibatch_size=20,\n                 ):\n\n        if shuffle_each_epoch:\n            self.source_orig = source\n            self.source = shuffle.main([self.source_orig], temporary=True)\n        else:\n            self.source = data_utils.fopen(source, \'r\')\n\n        self.source_dict = load_dict(source_dict)\n        self.batch_size = batch_size\n        self.maxlen = maxlen\n        self.skip_empty = skip_empty\n\n        self.n_words_source = n_words_source\n        \n        if self.n_words_source > 0:\n            for key, idx in self.source_dict.items():\n                if idx >= self.n_words_source:\n                    del self.source_dict[key]\n\n        self.shuffle = shuffle_each_epoch\n        self.sort_by_length = sort_by_length\n\n        self.shuffle = shuffle_each_epoch\n        self.sort_by_length = sort_by_length\n\n        self.source_buffer = []\n        self.k = batch_size * maxibatch_size\n        \n        self.end_of_data = False\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        return sum([1 for _ in self])\n    \n    def reset(self):\n        if self.shuffle:\n            self.source = shuffle.main([self.source_orig], temporary=True)\n        else:\n            self.source.seek(0)\n\n    def next(self):\n        if self.end_of_data:\n            self.end_of_data = False\n            self.reset()\n            raise StopIteration\n\n        source = []\n\n        # fill buffer, if it\'s empty\n        if len(self.source_buffer) == 0:\n            for k_ in xrange(self.k):\n                ss = self.source.readline()\n                if ss == """":\n                    break\n                self.source_buffer.append(ss.strip().split())\n    \n            # sort by buffer\n            if self.sort_by_length:\n                slen = np.array([len(s) for s in self.source_buffer])\n                sidx = slen.argsort()\n    \n                _sbuf = [self.source_buffer[i] for i in sidx]\n    \n                self.source_buffer = _sbuf\n            else:\n                self.source_buffer.reverse()\n    \n        if len(self.source_buffer) == 0:\n            self.end_of_data = False\n            self.reset()\n            raise StopIteration\n    \n        try:\n            # actual work here\n            while True:\n                # read from source file and map to word index\n                try:\n                    ss = self.source_buffer.pop()\n                except IndexError:\n                    break\n                ss = [self.source_dict[w] if w in self.source_dict\n                      else data_utils.unk_token for w in ss]\n    \n                if self.maxlen and len(ss) > self.maxlen:\n                    continue\n                if self.skip_empty and (not ss):\n                    continue\n                source.append(ss)\n    \n                if len(source) >= self.batch_size:\n                    break\n        except IOError:\n            self.end_of_data = True\n    \n        # all sentence pairs in maxibatch filtered out because of length\n        if len(source) == 0:\n            source = self.next()\n    \n        return source\n\nclass BiTextIterator:\n    """"""Simple Bitext iterator.""""""\n    def __init__(self, source, target,\n                 source_dict, target_dict,\n                 batch_size=128,\n                 maxlen=100,\n                 n_words_source=-1,\n                 n_words_target=-1,\n                 skip_empty=False,\n                 shuffle_each_epoch=False,\n                 sort_by_length=True,\n                 maxibatch_size=20):\n        if shuffle_each_epoch:\n            self.source_orig = source\n            self.target_orig = target\n            self.source, self.target = shuffle.main([self.source_orig, self.target_orig], temporary=True)\n        else:\n            self.source = data_utils.fopen(source, \'r\')\n            self.target = data_utils.fopen(target, \'r\')\n\n        self.source_dict = load_dict(source_dict)\n        self.target_dict = load_dict(target_dict)\n\n        self.batch_size = batch_size\n        self.maxlen = maxlen\n        self.skip_empty = skip_empty\n\n        self.n_words_source = n_words_source\n        self.n_words_target = n_words_target\n\n        if self.n_words_source > 0:\n            for key, idx in self.source_dict.items():\n                if idx >= self.n_words_source:\n                    del self.source_dict[key]\n\n        if self.n_words_target > 0:\n            for key, idx in self.target_dict.items():\n                if idx >= self.n_words_target:\n                    del self.target_dict[key]\n\n        self.shuffle = shuffle_each_epoch\n        self.sort_by_length = sort_by_length\n\n        self.source_buffer = []\n        self.target_buffer = []\n        self.k = batch_size * maxibatch_size\n        \n        self.end_of_data = False\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        return sum([1 for _ in self])\n    \n    def reset(self):\n        if self.shuffle:\n            self.source, self.target = shuffle.main([self.source_orig, self.target_orig], temporary=True)\n        else:\n            self.source.seek(0)\n            self.target.seek(0)\n\n    def next(self):\n        if self.end_of_data:\n            self.end_of_data = False\n            self.reset()\n            raise StopIteration\n\n        source = []\n        target = []\n\n        # fill buffer, if it\'s empty\n        assert len(self.source_buffer) == len(self.target_buffer), \'Buffer size mismatch!\'\n\n        if len(self.source_buffer) == 0:\n            for k_ in xrange(self.k):\n                ss = self.source.readline()\n                if ss == """":\n                    break\n                tt = self.target.readline()\n                if tt == """":\n                    break\n                self.source_buffer.append(ss.strip().split())\n                self.target_buffer.append(tt.strip().split())\n\n            # sort by target buffer\n            if self.sort_by_length:\n                tlen = np.array([len(t) for t in self.target_buffer])\n                tidx = tlen.argsort()\n\n                _sbuf = [self.source_buffer[i] for i in tidx]\n                _tbuf = [self.target_buffer[i] for i in tidx]\n\n                self.source_buffer = _sbuf\n                self.target_buffer = _tbuf\n\n            else:\n                self.source_buffer.reverse()\n                self.target_buffer.reverse()\n\n        if len(self.source_buffer) == 0 or len(self.target_buffer) == 0:\n            self.end_of_data = False\n            self.reset()\n            raise StopIteration\n\n        try:\n\n            # actual work here\n            while True:\n\n                # read from source file and map to word index\n                try:\n                    ss = self.source_buffer.pop()\n                except IndexError:\n                    break\n                ss = [self.source_dict[w] if w in self.source_dict\n                      else data_utils.unk_token for w in ss]\n\n                # read from source file and map to word index\n                tt = self.target_buffer.pop()\n                tt = [self.target_dict[w] if w in self.target_dict \n                      else data_utils.unk_token for w in tt]\n                if self.n_words_target > 0:\n                    tt = [w if w < self.n_words_target \n                          else data_utils.unk_token for w in tt]\n\n                if self.maxlen:\n                    if len(ss) > self.maxlen and len(tt) > self.maxlen:\n                        continue\n                if self.skip_empty and (not ss or not tt):\n                    continue\n\n                source.append(ss)\n                target.append(tt)\n\n                if len(source) >= self.batch_size or \\\n                        len(target) >= self.batch_size:\n                    break\n        except IOError:\n            self.end_of_data = True\n\n        # all sentence pairs in maxibatch filtered out because of length\n        if len(source) == 0 or len(target) == 0:\n            source, target = self.next()\n\n        return source, target\n'"
data/data_statistics.py,0,"b'\nimport sys\nimport numpy as np\n\ndef main(argv):\n    for input_file in argv:\n        lengths = []\n        with open(input_file, \'r\') as corpus:\n            for line in corpus:\n                lengths.append(len(line.split()))\n            print(""%s: size=%d, avg_length=%.2f, std=%.2f, min=%d, max=%d"" \n              % (input_file, len(lengths), np.mean(lengths), np.std(lengths), np.min(lengths), np.max(lengths)))\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
data/data_utils.py,0,"b""import numpy as np\n\nimport gzip\nfrom util import load_dict\n\n# Extra vocabulary symbols\n_GO = '_GO'\nEOS = '_EOS' # also function as PAD\nUNK = '_UNK'\n\nextra_tokens = [_GO, EOS, UNK]\n\nstart_token = extra_tokens.index(_GO)\t# start_token = 0\nend_token = extra_tokens.index(EOS)\t# end_token = 1\nunk_token = extra_tokens.index(UNK)\n\n\ndef fopen(filename, mode='r'):\n    if filename.endswith('.gz'):\n        return gzip.open(filename, mode)\n    return open(filename, mode)\n\n\ndef load_inverse_dict(dict_path):\n    orig_dict = load_dict(dict_path)\n    idict = {}\n    for words, idx in orig_dict.iteritems():\n        idict[idx] = words\n    return idict\n\n\ndef seq2words(seq, inverse_target_dictionary):\n    words = []\n    for w in seq:\n        if w == end_token:\n            break\n        if w in inverse_target_dictionary:\n            words.append(inverse_target_dictionary[w])\n        else:\n            words.append(UNK)\n    return ' '.join(words)\n\n\n# batch preparation of a given sequence\ndef prepare_batch(seqs_x, maxlen=None):\n    # seqs_x: a list of sentences\n    lengths_x = [len(s) for s in seqs_x]\n\n    if maxlen is not None:\n        new_seqs_x = []\n        new_lengths_x = []\n        for l_x, s_x in zip(lengths_x, seqs_x):\n            if l_x <= maxlen:\n                new_seqs_x.append(s_x)\n                new_lengths_x.append(l_x)\n        lengths_x = new_lengths_x\n        seqs_x = new_seqs_x\n\n        if len(lengths_x) < 1:\n            return None, None\n\n    batch_size = len(seqs_x)\n    \n    x_lengths = np.array(lengths_x)\n    maxlen_x = np.max(x_lengths)\n\n    x = np.ones((batch_size, maxlen_x)).astype('int32') * end_token\n    \n    for idx, s_x in enumerate(seqs_x):\n        x[idx, :lengths_x[idx]] = s_x\n    return x, x_lengths\n\n\n# batch preparation of a given sequence pair for training\ndef prepare_train_batch(seqs_x, seqs_y, maxlen=None):\n    # seqs_x, seqs_y: a list of sentences\n    lengths_x = [len(s) for s in seqs_x]\n    lengths_y = [len(s) for s in seqs_y]\n\n    if maxlen is not None:\n        new_seqs_x = []\n        new_seqs_y = []\n        new_lengths_x = []\n        new_lengths_y = []\n        for l_x, s_x, l_y, s_y in zip(lengths_x, seqs_x, lengths_y, seqs_y):\n            if l_x <= maxlen and l_y <= maxlen:\n                new_seqs_x.append(s_x)\n                new_lengths_x.append(l_x)\n                new_seqs_y.append(s_y)\n                new_lengths_y.append(l_y)\n        lengths_x = new_lengths_x\n        seqs_x = new_seqs_x\n        lengths_y = new_lengths_y\n        seqs_y = new_seqs_y\n\n        if len(lengths_x) < 1 or len(lengths_y) < 1:\n            return None, None, None, None\n\n    batch_size = len(seqs_x)\n    \n    x_lengths = np.array(lengths_x)\n    y_lengths = np.array(lengths_y)\n\n    maxlen_x = np.max(x_lengths)\n    maxlen_y = np.max(y_lengths)\n\n    x = np.ones((batch_size, maxlen_x)).astype('int32') * end_token\n    y = np.ones((batch_size, maxlen_y)).astype('int32') * end_token\n    \n    for idx, [s_x, s_y] in enumerate(zip(seqs_x, seqs_y)):\n        x[idx, :lengths_x[idx]] = s_x\n        y[idx, :lengths_y[idx]] = s_y\n    return x, x_lengths, y, y_lengths\n"""
data/shuffle.py,1,"b'import os\nimport sys\nimport random\n\nimport tempfile\nfrom subprocess import call\n\n\'\'\'\nThis code comes from shuffle.py of\nnematus proejct (https://github.com/rsennrich/nematus)\n\'\'\'\n\ndef main(files, temporary=False):\n\n    tf_os, tpath = tempfile.mkstemp()\n    tf = open(tpath, \'w\')\n\n    fds = [open(ff) for ff in files]\n\n    for l in fds[0]:\n        lines = [l.strip()] + [ff.readline().strip() for ff in fds[1:]]\n        print >>tf, ""|||"".join(lines)\n\n    [ff.close() for ff in fds]\n    tf.close()\n\n    lines = open(tpath, \'r\').readlines()\n    random.shuffle(lines)\n\n    if temporary:\n        fds = []\n        for ff in files:\n            path, filename = os.path.split(os.path.realpath(ff))\n            fds.append(tempfile.TemporaryFile(prefix=filename+\'.shuf\', dir=path))\n    else:\n        fds = [open(ff+\'.shuf\',\'w\') for ff in files]\n\n    for l in lines:\n        s = l.strip().split(\'|||\')\n        for ii, fd in enumerate(fds):\n            print >>fd, s[ii]\n\n    if temporary:\n        [ff.seek(0) for ff in fds]\n    else:\n        [ff.close() for ff in fds]\n\n    os.close(tf_os)\n    os.remove(tpath)\n\n    return fds\n\nif __name__ == \'__main__\':\n    main(sys.argv[1:])\n\n    \n\n\n'"
data/strip_sgml.py,0,"b'import sys\nimport re\n\n\'\'\'\nThis code comes from strip_sgml.py of\nnematus proejct (https://github.com/rsennrich/nematus)\n\'\'\'\n\ndef main():\n    fin = sys.stdin\n    fout = sys.stdout\n    for l in fin:\n        line = l.strip()\n        text = re.sub(\'<[^<]+>\', """", line).strip()\n        if len(text) == 0:\n            continue\n        print >>fout, text\n                \n\nif __name__ == ""__main__"":\n    main()\n\n'"
data/util.py,0,"b'\'\'\'\nUtility functions\n\'\'\'\n\n\'\'\' \nThis code is based on the util.py of \nnematus project (https://github.com/rsennrich/nematus)\n\'\'\'\n\nimport sys\nimport json\nimport cPickle as pkl\n\n#json loads strings as unicode; we currently still work with Python 2 strings, and need conversion\ndef unicode_to_utf8(d):\n    return dict((key.encode(""UTF-8""), value) for (key,value) in d.items())\n\ndef load_dict(filename):\n    try:\n        with open(filename, \'rb\') as f:\n            return unicode_to_utf8(json.load(f))\n    except:\n        with open(filename, \'rb\') as f:\n            return pkl.load(f)\n\n\ndef load_config(basename):\n    try:\n        with open(\'%s.json\' % basename, \'rb\') as f:\n            return json.load(f)\n    except:\n        try:\n            with open(\'%s.pkl\' % basename, \'rb\') as f:\n                return pkl.load(f)\n        except:\n            sys.stderr.write(\'Error: config file {0}.json is missing\\n\'.format(basename))\n            sys.exit(1)\n\n'"
data/subword_nmt/apply_bpe.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Use operations learned with learn_bpe.py to encode a new text.\nThe text will not be smaller, but use only a fixed vocabulary, with rare words\nencoded as variable-length sequences of subword units.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n\nfrom __future__ import unicode_literals, division\n\nimport sys\nimport codecs\nimport argparse\nfrom collections import defaultdict\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n# python 2/3 compatibility\nif sys.version_info < (3, 0):\n  sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n  sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n  sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n\nclass BPE(object):\n\n    def __init__(self, codes, separator=\'@@\'):\n        self.bpe_codes = [tuple(item.split()) for item in codes]\n        # some hacking to deal with duplicates (only consider first instance)\n        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n\n        self.separator = separator\n\n    def segment(self, sentence):\n        """"""segment single sentence (whitespace-tokenized string) with BPE encoding""""""\n\n        output = []\n        for word in sentence.split():\n            new_word = encode(word, self.bpe_codes)\n\n            for item in new_word[:-1]:\n                output.append(item + self.separator)\n            output.append(new_word[-1])\n\n        return \' \'.join(output)\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""learn BPE-based word segmentation"")\n\n    parser.add_argument(\n        \'--input\', \'-i\', type=argparse.FileType(\'r\'), default=sys.stdin,\n        metavar=\'PATH\',\n        help=""Input file (default: standard input)."")\n    parser.add_argument(\n        \'--codes\', \'-c\', type=argparse.FileType(\'r\'), metavar=\'PATH\',\n        required=True,\n        help=""File with BPE codes (created by learn_bpe.py)."")\n    parser.add_argument(\n        \'--output\', \'-o\', type=argparse.FileType(\'w\'), default=sys.stdout,\n        metavar=\'PATH\',\n        help=""Output file (default: standard output)"")\n    parser.add_argument(\n        \'--separator\', \'-s\', type=str, default=\'@@\', metavar=\'STR\',\n        help=""Separator between non-final subword units (default: \'%(default)s\'))"")\n\n    return parser\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\ndef encode(orig, bpe_codes, cache={}):\n    """"""Encode word based on list of BPE merge operations, which are applied consecutively\n    """"""\n\n    if orig in cache:\n        return cache[orig]\n\n    word = tuple(orig) + (\'</w>\',)\n    pairs = get_pairs(word)\n\n    while True:\n        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float(\'inf\')))\n        if bigram not in bpe_codes:\n            break\n        first, second = bigram\n        new_word = []\n        i = 0\n        while i < len(word):\n            try:\n                j = word.index(first, i)\n                new_word.extend(word[i:j])\n                i = j\n            except:\n                new_word.extend(word[i:])\n                break\n\n            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                new_word.append(first+second)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_word = tuple(new_word)\n        word = new_word\n        if len(word) == 1:\n            break\n        else:\n            pairs = get_pairs(word)\n\n    # don\'t print end-of-word symbols\n    if word[-1] == \'</w>\':\n        word = word[:-1]\n    elif word[-1].endswith(\'</w>\'):\n        word = word[:-1] + (word[-1].replace(\'</w>\',\'\'),)\n\n    cache[orig] = word\n    return word\n\n\nif __name__ == \'__main__\':\n    parser = create_parser()\n    args = parser.parse_args()\n\n    bpe = BPE(args.codes, args.separator)\n\n    for line in args.input:\n        args.output.write(bpe.segment(line).strip())\n        args.output.write(\'\\n\')\n'"
data/subword_nmt/chrF.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Compute chrF3 for machine translation evaluation\n\nReference:\nMaja Popovi\xc4\x87 (2015). chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translationn, pages 392\xe2\x80\x93395, Lisbon, Portugal.\n""""""\n\nfrom __future__ import print_function, unicode_literals, division\nimport sys\nimport codecs\nimport io\nimport argparse\nfrom collections import defaultdict\nfrom math import log, exp\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n# python 2/3 compatibility\nif sys.version_info < (3, 0):\n  sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n  sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n  sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""learn BPE-based word segmentation"")\n\n    parser.add_argument(\n        \'--ref\', \'-r\', type=argparse.FileType(\'r\'), required=True,\n        metavar=\'PATH\',\n        help=""Reference file"")\n    parser.add_argument(\n        \'--hyp\', type=argparse.FileType(\'r\'), metavar=\'PATH\',\n        default=sys.stdin,\n        help=""Hypothesis file (default: stdin)."")\n    parser.add_argument(\n        \'--beta\', \'-b\', type=float, default=3,\n        metavar=\'FLOAT\',\n        help=""beta parameter (default: \'%(default)s\')"")\n    parser.add_argument(\n        \'--ngram\', \'-n\', type=int, default=6,\n        metavar=\'INT\',\n        help=""ngram order (default: \'%(default)s\')"")\n    parser.add_argument(\n        \'--space\', \'-s\', action=\'store_true\',\n        help=""take spaces into account (default: \'%(default)s\')"")\n    parser.add_argument(\n        \'--precision\', action=\'store_true\',\n        help=""report precision (default: \'%(default)s\')"")\n    parser.add_argument(\n        \'--recall\', action=\'store_true\',\n        help=""report recall (default: \'%(default)s\')"")\n\n    return parser\n\ndef extract_ngrams(words, max_length=4, spaces=False):\n\n    if not spaces:\n        words = \'\'.join(words.split())\n    else:\n        words = words.strip()\n\n    results = defaultdict(lambda: defaultdict(int))\n    for length in range(max_length):\n        for start_pos in range(len(words)):\n            end_pos = start_pos + length + 1\n            if end_pos <= len(words):\n                results[length][tuple(words[start_pos: end_pos])] += 1\n    return results\n\n\ndef get_correct(ngrams_ref, ngrams_test, correct, total):\n\n    for rank in ngrams_test:\n        for chain in ngrams_test[rank]:\n            total[rank] += ngrams_test[rank][chain]\n            if chain in ngrams_ref[rank]:\n                correct[rank] += min(ngrams_test[rank][chain], ngrams_ref[rank][chain])\n\n    return correct, total\n\n\ndef f1(correct, total_hyp, total_ref, max_length, beta=3, smooth=0):\n\n    precision = 0\n    recall = 0\n\n    for i in range(max_length):\n      if total_hyp[i] + smooth and total_ref[i] + smooth:\n        precision += (correct[i] + smooth) / (total_hyp[i] + smooth)\n        recall += (correct[i] + smooth) / (total_ref[i] + smooth)\n\n    precision /= max_length\n    recall /= max_length\n\n    return (1 + beta**2) * (precision*recall) / ((beta**2 * precision) + recall), precision, recall\n\ndef main(args):\n\n    correct = [0]*args.ngram\n    total = [0]*args.ngram\n    total_ref = [0]*args.ngram\n    for line in args.ref:\n      line2 = args.hyp.readline()\n\n      ngrams_ref = extract_ngrams(line, max_length=args.ngram, spaces=args.space)\n      ngrams_test = extract_ngrams(line2, max_length=args.ngram, spaces=args.space)\n\n      get_correct(ngrams_ref, ngrams_test, correct, total)\n\n      for rank in ngrams_ref:\n          for chain in ngrams_ref[rank]:\n              total_ref[rank] += ngrams_ref[rank][chain]\n\n    chrf, precision, recall = f1(correct, total, total_ref, args.ngram, args.beta)\n\n    print(\'chrF3: {0:.4f}\'.format(chrf))\n    if args.precision:\n        print(\'chrPrec: {0:.4f}\'.format(precision))\n    if args.recall:\n        print(\'chrRec: {0:.4f}\'.format(recall))\n\nif __name__ == \'__main__\':\n\n    parser = create_parser()\n    args = parser.parse_args()\n\n    main(args)\n'"
data/subword_nmt/learn_bpe.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Use byte pair encoding (BPE) to learn a variable-length encoding of the vocabulary in a text.\nUnlike the original BPE, it does not compress the plain text, but can be used to reduce the vocabulary\nof a text to a configurable number of symbols, with only a small increase in the number of tokens.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n\nfrom __future__ import unicode_literals\n\nimport sys\nimport codecs\nimport re\nimport copy\nimport argparse\nfrom collections import defaultdict, Counter\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n# python 2/3 compatibility\nif sys.version_info < (3, 0):\n  sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n  sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n  sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""learn BPE-based word segmentation"")\n\n    parser.add_argument(\n        \'--input\', \'-i\', type=argparse.FileType(\'r\'), default=sys.stdin,\n        metavar=\'PATH\',\n        help=""Input text (default: standard input)."")\n    parser.add_argument(\n        \'--output\', \'-o\', type=argparse.FileType(\'w\'), default=sys.stdout,\n        metavar=\'PATH\',\n        help=""Output file for BPE codes (default: standard output)"")\n    parser.add_argument(\n        \'--symbols\', \'-s\', type=int, default=10000,\n        help=""Create this many new symbols (each representing a character n-gram) (default: %(default)s))"")\n    parser.add_argument(\n        \'--verbose\', \'-v\', action=""store_true"",\n        help=""verbose mode."")\n\n    return parser\n\ndef get_vocabulary(fobj):\n    """"""Read text and return dictionary that encodes vocabulary\n    """"""\n    vocab = Counter()\n    for line in fobj:\n        for word in line.split():\n            vocab[word] += 1\n    return vocab\n\ndef update_pair_statistics(pair, changed, stats, indices):\n    """"""Minimally update the indices and frequency of symbol pairs\n\n    if we merge a pair of symbols, only pairs that overlap with occurrences\n    of this pair are affected, and need to be updated.\n    """"""\n    stats[pair] = 0\n    indices[pair] = defaultdict(int)\n    first, second = pair\n    new_pair = first+second\n    for j, word, old_word, freq in changed:\n\n        # find all instances of pair, and update frequency/indices around it\n        i = 0\n        while True:\n            try:\n                i = old_word.index(first, i)\n            except ValueError:\n                break\n            if i < len(old_word)-1 and old_word[i+1] == second:\n                if i:\n                    prev = old_word[i-1:i+1]\n                    stats[prev] -= freq\n                    indices[prev][j] -= 1\n                if i < len(old_word)-2:\n                    # don\'t double-count consecutive pairs\n                    if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:\n                        nex = old_word[i+1:i+3]\n                        stats[nex] -= freq\n                        indices[nex][j] -= 1\n                i += 2\n            else:\n                i += 1\n\n        i = 0\n        while True:\n            try:\n                i = word.index(new_pair, i)\n            except ValueError:\n                break\n            if i:\n                prev = word[i-1:i+1]\n                stats[prev] += freq\n                indices[prev][j] += 1\n            # don\'t double-count consecutive pairs\n            if i < len(word)-1 and word[i+1] != new_pair:\n                nex = word[i:i+2]\n                stats[nex] += freq\n                indices[nex][j] += 1\n            i += 1\n\n\ndef get_pair_statistics(vocab):\n    """"""Count frequency of all symbol pairs, and create index""""""\n\n    # data structure of pair frequencies\n    stats = defaultdict(int)\n\n    #index from pairs to words\n    indices = defaultdict(lambda: defaultdict(int))\n\n    for i, (word, freq) in enumerate(vocab):\n        prev_char = word[0]\n        for char in word[1:]:\n            stats[prev_char, char] += freq\n            indices[prev_char, char][i] += 1\n            prev_char = char\n\n    return stats, indices\n\n\ndef replace_pair(pair, vocab, indices):\n    """"""Replace all occurrences of a symbol pair (\'A\', \'B\') with a new symbol \'AB\'""""""\n    first, second = pair\n    pair_str = \'\'.join(pair)\n    pair_str = pair_str.replace(\'\\\\\',\'\\\\\\\\\')\n    changes = []\n    pattern = re.compile(r\'(?<!\\S)\' + re.escape(first + \' \' + second) + r\'(?!\\S)\')\n    if sys.version_info < (3, 0):\n        iterator = indices[pair].iteritems()\n    else:\n        iterator = indices[pair].items()\n    for j, freq in iterator:\n        if freq < 1:\n            continue\n        word, freq = vocab[j]\n        new_word = \' \'.join(word)\n        new_word = pattern.sub(pair_str, new_word)\n        new_word = tuple(new_word.split())\n\n        vocab[j] = (new_word, freq)\n        changes.append((j, new_word, word, freq))\n\n    return changes\n\ndef prune_stats(stats, big_stats, threshold):\n    """"""Prune statistics dict for efficiency of max()\n\n    The frequency of a symbol pair never increases, so pruning is generally safe\n    (until we the most frequent pair is less frequent than a pair we previously pruned)\n    big_stats keeps full statistics for when we need to access pruned items\n    """"""\n    for item,freq in list(stats.items()):\n        if freq < threshold:\n            del stats[item]\n            if freq < 0:\n                big_stats[item] += freq\n            else:\n                big_stats[item] = freq\n\nif __name__ == \'__main__\':\n\n    parser = create_parser()\n    args = parser.parse_args()\n\n    vocab = get_vocabulary(args.input)\n    vocab = dict([(tuple(x)+(\'</w>\',) ,y) for (x,y) in vocab.items()])\n    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n\n    stats, indices = get_pair_statistics(sorted_vocab)\n    big_stats = copy.deepcopy(stats)\n    # threshold is inspired by Zipfian assumption, but should only affect speed\n    threshold = max(stats.values()) / 10\n    for i in range(args.symbols):\n        if stats:\n            most_frequent = max(stats, key=stats.get)\n\n        # we probably missed the best pair because of pruning; go back to full statistics\n        if not stats or (i and stats[most_frequent] < threshold):\n            prune_stats(stats, big_stats, threshold)\n            stats = copy.deepcopy(big_stats)\n            most_frequent = max(stats, key=stats.get)\n            # threshold is inspired by Zipfian assumption, but should only affect speed\n            threshold = stats[most_frequent] * i/(i+10000.0)\n            prune_stats(stats, big_stats, threshold)\n\n        if stats[most_frequent] < 2:\n            sys.stderr.write(\'no pair has frequency > 1. Stopping\\n\')\n            break\n\n        if args.verbose:\n            sys.stderr.write(\'pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n\'.format(i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n        args.output.write(\'{0} {1}\\n\'.format(*most_frequent))\n        changes = replace_pair(most_frequent, sorted_vocab, indices)\n        update_pair_statistics(most_frequent, changes, stats, indices)\n        stats[most_frequent] = 0\n        if not i % 100:\n            prune_stats(stats, big_stats, threshold)\n'"
data/subword_nmt/segment-char-ngrams.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\nfrom __future__ import unicode_literals, division\n\nimport sys\nimport codecs\nimport argparse\n\n# hack for python2/3 compatibility\nfrom io import open\nargparse.open = open\n\n# python 2/3 compatibility\nif sys.version_info < (3, 0):\n  sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr)\n  sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout)\n  sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin)\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""segment rare words into character n-grams"")\n\n    parser.add_argument(\n        \'--input\', \'-i\', type=argparse.FileType(\'r\'), default=sys.stdin,\n        metavar=\'PATH\',\n        help=""Input file (default: standard input)."")\n    parser.add_argument(\n        \'--vocab\', type=argparse.FileType(\'r\'), metavar=\'PATH\',\n        required=True,\n        help=""Vocabulary file."")\n    parser.add_argument(\n        \'--shortlist\', type=int, metavar=\'INT\', default=0,\n        help=""do not segment INT most frequent words in vocabulary (default: \'%(default)s\'))."")\n    parser.add_argument(\n        \'-n\', type=int, metavar=\'INT\', default=2,\n        help=""segment rare words into character n-grams of size INT (default: \'%(default)s\'))."")\n    parser.add_argument(\n        \'--output\', \'-o\', type=argparse.FileType(\'w\'), default=sys.stdout,\n        metavar=\'PATH\',\n        help=""Output file (default: standard output)"")\n    parser.add_argument(\n        \'--separator\', \'-s\', type=str, default=\'@@\', metavar=\'STR\',\n        help=""Separator between non-final subword units (default: \'%(default)s\'))"")\n\n    return parser\n\n\nif __name__ == \'__main__\':\n\n    parser = create_parser()\n    args = parser.parse_args()\n\n    vocab = [line.split()[0] for line in args.vocab if len(line.split()) == 2]\n    vocab = dict((y,x) for (x,y) in enumerate(vocab))\n\n    for line in args.input:\n      for word in line.split():\n        if word not in vocab or vocab[word] > args.shortlist:\n          i = 0\n          while i*args.n < len(word):\n            args.output.write(word[i*args.n:i*args.n+args.n])\n            i += 1\n            if i*args.n < len(word):\n              args.output.write(args.separator)\n            args.output.write(\' \')\n        else:\n          args.output.write(word + \' \')\n      args.output.write(\'\\n\')\n'"
