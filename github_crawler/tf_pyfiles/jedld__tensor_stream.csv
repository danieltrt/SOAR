file_path,api_count,code
spec/fixtures/neural_network_raw.py,18,"b'"""""" Neural Network.\n\nA 2-Hidden Layers Fully Connected Neural Network (a.k.a Multilayer Perceptron)\nimplementation with TensorFlow. This example is using the MNIST database\nof handwritten digits (http://yann.lecun.com/exdb/mnist/).\n\nLinks:\n    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n""""""\n\nfrom __future__ import print_function\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\nimport tensorflow as tf\n\n# Parameters\nlearning_rate = 0.1\nnum_steps = 500\nbatch_size = 128\ndisplay_step = 100\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of neurons\nn_hidden_2 = 256 # 2nd layer number of neurons\nnum_input = 784 # MNIST data input (img shape: 28*28)\nnum_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph input\nX = tf.placeholder(""float"", [None, num_input])\nY = tf.placeholder(""float"", [None, num_classes])\n\n# Store layers weight & bias\nweights = {\n    \'h1\': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n    \'h2\': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    \'out\': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n}\nbiases = {\n    \'b1\': tf.Variable(tf.random_normal([n_hidden_1])),\n    \'b2\': tf.Variable(tf.random_normal([n_hidden_2])),\n    \'out\': tf.Variable(tf.random_normal([num_classes]))\n}\n\n\n# Create model\ndef neural_net(x):\n    # Hidden fully connected layer with 256 neurons\n    layer_1 = tf.add(tf.matmul(x, weights[\'h1\']), biases[\'b1\'])\n    # Hidden fully connected layer with 256 neurons\n    layer_2 = tf.add(tf.matmul(layer_1, weights[\'h2\']), biases[\'b2\'])\n    # Output fully connected layer with a neuron for each class\n    out_layer = tf.matmul(layer_2, weights[\'out\']) + biases[\'out\']\n    return out_layer\n\n# Construct model\nlogits = neural_net(X)\nprediction = tf.nn.softmax(logits)\n\n# Define loss and optimizer\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=logits, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initialize the variables (i.e. assign their default value)\ninit = tf.global_variables_initializer()\n\n# Start training\nwith tf.Session() as sess:\n\n    # Run the initializer\n    sess.run(init)\n\n    for step in range(1, num_steps+1):\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        # Run optimization op (backprop)\n        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n        if step % display_step == 0 or step == 1:\n            # Calculate batch loss and accuracy\n            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n                                                                 Y: batch_y})\n            print(""Step "" + str(step) + "", Minibatch Loss= "" + \\\n                  ""{:.4f}"".format(loss) + "", Training Accuracy= "" + \\\n                  ""{:.3f}"".format(acc))\n\n    print(""Optimization Finished!"")\n\n    # Calculate accuracy for MNIST test images\n    print(""Testing Accuracy:"", \\\n        sess.run(accuracy, feed_dict={X: mnist.test.images,\n                                      Y: mnist.test.labels}))\n'"
spec/fixtures/test_samples/test.py,11,"b'import tensorflow as tf\n\ntest_inputs = [\n  [0.5937, 0.2343, 1.4332, 0.4395],\n  [-1.0227, -0.6915, 1.2367, 0.3452],\n  [-0.5675, 1.0374, 1.0429, 0.8839],\n  [-0.1066, -0.0469, -1.6317, -1.4836],\n  [0.7835, -3.0105, 1.713, -0.4536],\n  [-0.3076, 1.3662, -0.6537, 0.0905],\n  [-0.2459, 0.2243, -2.7048, 0.848],\n]\n\nnum_inputs = 4\nnum_neurons = 5\ninputs = tf.placeholder(""float"", shape=(None, num_inputs))\nbiases = tf.constant([0.5012, 1.302, -1.6217, 0.669, 0.1494], name=\'b1\')\nbiases2 = tf.constant([0.2012, 1.102, -1.5217, 0.469, 0.0494], name=\'b2\')\n\nweights = tf.constant([\n  [-0.9135, 1.0376, 0.8537, 0.4376, 1.3255],\n  [-0.5921, -1.4081, 1.0614, -0.5283, 1.1832],\n  [0.7285, -0.7844, 0.1793, -0.5275, -0.4426],\n  [-1.4976, 0.4433, 2.2317, -2.0479, 0.7791]], name=\'w\')\n\nweights_layer2 = tf.constant([\n  [-1.0465, -0.8766, 1.6849, -0.6625, 0.7928],\n  [2.0412, 1.3564, 0.7905, 0.6434, -2.5495],\n  [2.4276, -0.6893, -1.5917, 0.0911, 0.9112],\n  [-0.012, 0.0794, 1.3829, -1.018, -0.9328],\n  [0.061, 0.9791, -2.1727, -0.9553, -1.434]], name=\'w2\')\n\n\nsess = tf.Session()\n\nlayer_1 =  tf.matmul(inputs, weights) + biases\nneural_net = tf.matmul(layer_1, weights_layer2) + biases2\n\noutput = sess.run(neural_net, feed_dict={ inputs: test_inputs })\n\ng0 = tf.gradients(layer_1, [weights, biases])\ng = tf.gradients(neural_net, [weights, biases])\ng2 = tf.gradients(neural_net, [weights_layer2, biases2])\n\nweight_gradient0, biases_gradient0 = sess.run(g0, feed_dict = { inputs: test_inputs })\nweight_gradient, biases_gradient = sess.run(g, feed_dict = { inputs: test_inputs })\nweight_gradient2, biases_gradient2 = sess.run(g2, feed_dict = { inputs => test_inputs })\n'"
spec/fixtures/test_samples/test2.py,23,"b'import tensorflow as tf\n\nbatch_x = [\n          [0.686274, 0.10196, 0.6509, 1.0, 0.9686, 0.49803, 0.0, 0.0, 0.0, 0.0],\n          [0.543244, 0.10123, 0.4509, 0.0, 0.6986, 0.39803, 1.0, 0.0, 0.0, 0.0]]\n\nbatch_y = [\n          [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n          [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n]\n\nnum_input = 10\nnum_classes = 10\nn_hidden_1 = 4 # 1st layer number of neurons\nn_hidden_2 = 4 # 2nd layer number of neurons\n\nX = batch_x #tf.placeholder(tf.float32, shape=[None, num_input])\nY = batch_y # tf.placeholder(tf.float32, shape=[None, num_classes])\n\nh1_init = tf.constant([[0.5937, 0.2343, 1.4332, 0.4395],\n          [-1.0227, -0.6915, 1.2367, 0.3452],\n          [-0.5675, 1.0374, 1.0429, 0.8839],\n          [-0.1066, -0.0469, -1.6317, -1.4836],\n          [0.7835, -3.0105, 1.713, -0.4536],\n          [-0.3076, 1.3662, -0.6537, 0.0905],\n          [-0.2459, 0.2243, -2.7048, 0.848],\n          [0.3589, 0.3542, -0.0959, -1.327],\n          [-0.4685, 0.0844, 0.2794, 2.1275],\n          [-1.0733, 0.6189, 0.845, 0.033]])\n\nh2_init = tf.constant([[0.5012, 1.302, -1.6217, 0.669], [0.1494, -0.7837, -0.2978, 1.7745], [1.9727, -0.5312, -0.7391, 0.9187], [-0.6412, -1.4434, -0.8801, 0.9343]])\nh3_init = tf.constant([[0.5012, 1.302, -1.6217, 0.669, 0.1494, -0.7837, -0.2978, 1.7745, 1.9727, -0.5312],\n  [-0.7391, 0.9187, -0.6412, -1.4434, -0.8801, 0.9343, -0.1665, -0.0032, 0.2959, -2.0488],\n  [-0.9135, 1.0376, 0.8537, 0.4376, 1.3255, -0.5921, -1.4081, 1.0614, -0.5283, 1.1832],\n  [0.7285, -0.7844, 0.1793, -0.5275, -0.4426, -1.4976, 0.4433, 2.2317, -2.0479, 0.7791]])\n\n\nb1_init = tf.constant([0.1494, -0.7837, -0.2978, 1.7745])\n\nb2_init = tf.constant([1.9727, -0.5312, -0.7391, 0.9187])\nout_init = tf.constant([-0.6412, -1.4434, -0.8801, 0.9343, -0.1665, -0.0032, 0.2959, -2.0488, -0.9135, 1.0376])\n\nh1 = tf.Variable(h1_init, dtype=tf.float32, name=\'h1\')\nh2 = tf.Variable(h2_init, dtype=tf.float32, name=\'h2\')\nh3 = tf.Variable(h3_init, dtype=tf.float32, name=\'out\')\n\nb1 = tf.Variable(b1_init, dtype=tf.float32, name=\'b1\')\nb2 = tf.Variable(b2_init, dtype=tf.float32, name=\'b2\')\nout = tf.Variable(out_init, dtype=tf.float32, name=\'out2\')\n\nlayer_1 = tf.add(tf.matmul(X, h1), b1)\n# Hidden fully connected layer with 256 neurons\nlayer_2 = tf.add(tf.matmul(layer_1, h2), b2)\n# Output fully connected layer with a neuron for each class\n\nsess = tf.Session()\n\nlogits = tf.matmul(layer_2, h3) + out\nprediction = tf.nn.softmax(logits)\n\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain_op = optimizer.minimize(loss_op)\ninit = tf.global_variables_initializer()\n\nsess.run(init)\n# print(sess.run(layer_1))\ntf.gradients(loss_op, [logits])\nprint(""------------"")\n\nprint(""H1: "", sess.run(h1))\nprint(""------------ Running train 1"")\n# sess.run(train_op, feed_dict={ X: batch_x, Y: batch_y })\nsess.run(train_op)\nprint(""H1:"", sess.run(h1))\nprint(""H2:"", sess.run(h2))\nprint(""H3:"", sess.run(h3))\n\nprint(sess.run(b1))\nprint(sess.run(b2))\nprint(sess.run(out))\n\n# sess.run(train_op, feed_dict={ X: batch_x, Y: batch_y })\nprint(""------------- Running train 2"")\nsess.run(train_op)\nprint(""H1:"", sess.run(h1))'"
