file_path,api_count,code
setup.py,0,"b'from setuptools import setup\n\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\nsetup(\n  name=\'tensorrec\',\n  packages=[\'tensorrec\'],\n  version=\'0.26.2\',\n  description=\'A TensorFlow recommendation algorithm and framework in Python.\',\n  author=\'James Kirk\',\n  author_email=\'james.f.kirk@gmail.com\',\n  url=\'https://github.com/jfkirk/tensorrec\',\n  keywords=[\'machine-learning\', \'tensorflow\', \'recommendation-system\', \'python\', \'recommender-system\'],\n  classifiers=[],\n  install_requires=[\n      ""numpy>=1.14.1"",\n      ""scipy>=0.19.1"",\n      ""six==1.11.0"",\n      ""tensorflow>=1.7.0"",\n  ],\n)\n'"
examples/attention_example.py,0,"b'from tensorrec import TensorRec\nfrom tensorrec.eval import fit_and_eval\nfrom tensorrec.representation_graphs import (\n    LinearRepresentationGraph, NormalizedLinearRepresentationGraph\n)\nfrom tensorrec.loss_graphs import BalancedWMRBLossGraph\n\nfrom test.datasets import get_movielens_100k\n\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\n\n# Load the movielens dataset\ntrain_interactions, test_interactions, user_features, item_features, _ = get_movielens_100k(negative_value=0)\n\n# Construct parameters for fitting\nepochs = 500\nalpha = 0.00001\nn_components = 10\nverbose = True\nlearning_rate = .01\nn_sampled_items = int(item_features.shape[0] * .1)\nfit_kwargs = {\'epochs\': epochs, \'alpha\': alpha, \'verbose\': verbose, \'learning_rate\': learning_rate,\n              \'n_sampled_items\': n_sampled_items}\n\n# Build two models -- one without an attention graph, one with a linear attention graph\nmodel_without_attention = TensorRec(\n    n_components=10,\n    n_tastes=3,\n    user_repr_graph=NormalizedLinearRepresentationGraph(),\n    attention_graph=None,\n    loss_graph=BalancedWMRBLossGraph(),\n)\n\nmodel_with_attention = TensorRec(\n    n_components=10,\n    n_tastes=3,\n    user_repr_graph=NormalizedLinearRepresentationGraph(),\n    attention_graph=LinearRepresentationGraph(),\n    loss_graph=BalancedWMRBLossGraph(),\n)\n\nresults_without_attention = fit_and_eval(model=model_without_attention,\n                                         user_features=user_features,\n                                         item_features=item_features,\n                                         train_interactions=train_interactions,\n                                         test_interactions=test_interactions,\n                                         fit_kwargs=fit_kwargs)\nresults_with_attention = fit_and_eval(model=model_with_attention,\n                                      user_features=user_features,\n                                      item_features=item_features,\n                                      train_interactions=train_interactions,\n                                      test_interactions=test_interactions,\n                                      fit_kwargs=fit_kwargs)\n\nlogging.info(""Results without attention: {}"".format(results_without_attention))\nlogging.info(""Results with attention:    {}"".format(results_with_attention))\n'"
examples/check_movielens_losses.py,0,"b'from tensorrec import TensorRec\nfrom tensorrec.eval import fit_and_eval\nfrom tensorrec.representation_graphs import (\n    LinearRepresentationGraph, ReLURepresentationGraph, NormalizedLinearRepresentationGraph\n)\nfrom tensorrec.loss_graphs import WMRBLossGraph, BalancedWMRBLossGraph\nfrom tensorrec.prediction_graphs import (\n    DotProductPredictionGraph, CosineSimilarityPredictionGraph, EuclideanSimilarityPredictionGraph\n)\nfrom tensorrec.util import append_to_string_at_point\n\nfrom test.datasets import get_movielens_100k\n\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\n\n# Load the movielens dataset\ntrain_interactions, test_interactions, user_features, item_features, _ = get_movielens_100k(negative_value=0)\n\n# Construct parameters for fitting\nepochs = 300\nalpha = 0.00001\nn_components = 10\nverbose = True\nlearning_rate = .01\nn_sampled_items = int(item_features.shape[0] * .1)\nbiased = False\nfit_kwargs = {\'epochs\': epochs, \'alpha\': alpha, \'verbose\': verbose, \'learning_rate\': learning_rate,\n              \'n_sampled_items\': n_sampled_items}\n\nres_strings = []\n\n# Build results header\nheader = ""Loss Graph""\nheader = append_to_string_at_point(header, \'Prediction Graph\', 30)\nheader = append_to_string_at_point(header, \'ItemRepr Graph\', 66)\nheader = append_to_string_at_point(header, \'Biased\', 98)\nheader = append_to_string_at_point(header, \'N Tastes\', 108)\nheader = append_to_string_at_point(header, \'Recall at 30\', 120)\nheader = append_to_string_at_point(header, \'Precision at 5\', 141)\nheader = append_to_string_at_point(header, \'NDCG at 30\', 164)\nres_strings.append(header)\n\n# Iterate through many possibilities for model configuration\nfor loss_graph in (WMRBLossGraph, BalancedWMRBLossGraph):\n    for pred_graph in (DotProductPredictionGraph, CosineSimilarityPredictionGraph,\n                       EuclideanSimilarityPredictionGraph):\n        for repr_graph in (LinearRepresentationGraph, ReLURepresentationGraph):\n            for n_tastes in (1, 3):\n\n                # Build the model, fit, and get a result packet\n                model = TensorRec(n_components=n_components,\n                                  n_tastes=n_tastes,\n                                  biased=biased,\n                                  loss_graph=loss_graph(),\n                                  prediction_graph=pred_graph(),\n                                  user_repr_graph=NormalizedLinearRepresentationGraph(),\n                                  item_repr_graph=repr_graph())\n                result = fit_and_eval(model, user_features, item_features, train_interactions, test_interactions,\n                                      fit_kwargs)\n\n                # Build results row for this configuration\n                res_string = ""{}"".format(loss_graph.__name__)\n                res_string = append_to_string_at_point(res_string, pred_graph.__name__, 30)\n                res_string = append_to_string_at_point(res_string, repr_graph.__name__, 66)\n                res_string = append_to_string_at_point(res_string, biased, 98)\n                res_string = append_to_string_at_point(res_string, n_tastes, 108)\n                res_string = append_to_string_at_point(res_string, "": {}"".format(result[0]), 118)\n                res_string = append_to_string_at_point(res_string, result[1], 141)\n                res_string = append_to_string_at_point(res_string, result[2], 164)\n                res_strings.append(res_string)\n                print(res_string)\n\nprint(\'--------------------------------------------------\')\nfor res_string in res_strings:\n    print(res_string)\n'"
examples/getting_started.py,0,"b'from collections import defaultdict\nimport csv\nimport numpy\nimport random\nfrom scipy import sparse\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nimport tensorrec\n\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\n\n# Open and read in the ratings file\n# NOTE: This expects the ratings.csv file to be in the same folder as this Python file\n# You can download the MovieLens dataset, including ratings.csv, here:\n# https://grouplens.org/datasets/movielens/\nprint(\'Loading ratings\')\nwith open(\'ratings.csv\', \'r\') as ratings_file:\n    ratings_file_reader = csv.reader(ratings_file)\n    raw_ratings = list(ratings_file_reader)\n    raw_ratings_header = raw_ratings.pop(0)\n\n# Iterate through the input to map MovieLens IDs to new internal IDs\n# The new internal IDs will be created by the defaultdict on insertion\nmovielens_to_internal_user_ids = defaultdict(lambda: len(movielens_to_internal_user_ids))\nmovielens_to_internal_item_ids = defaultdict(lambda: len(movielens_to_internal_item_ids))\nfor row in raw_ratings:\n    row[0] = movielens_to_internal_user_ids[int(row[0])]\n    row[1] = movielens_to_internal_item_ids[int(row[1])]\n    row[2] = float(row[2])\nn_users = len(movielens_to_internal_user_ids)\nn_items = len(movielens_to_internal_item_ids)\n\n# Look at an example raw rating\nprint(""Raw ratings example:\\n{}\\n{}"".format(raw_ratings_header, raw_ratings[0]))\n\n# Shuffle the ratings and split them in to train/test sets 80%/20%\nrandom.shuffle(raw_ratings)  # Shuffles the list in-place\ncutoff = int(.8 * len(raw_ratings))\ntrain_ratings = raw_ratings[:cutoff]\ntest_ratings = raw_ratings[cutoff:]\nprint(""{} train ratings, {} test ratings"".format(len(train_ratings), len(test_ratings)))\n\n\n# This method converts a list of (user, item, rating, time) to a sparse matrix\ndef interactions_list_to_sparse_matrix(interactions):\n    users_column, items_column, ratings_column, _ = zip(*interactions)\n    return sparse.coo_matrix((ratings_column, (users_column, items_column)),\n                             shape=(n_users, n_items))\n\n\n# Create sparse matrices of interaction data\nsparse_train_ratings = interactions_list_to_sparse_matrix(train_ratings)\nsparse_test_ratings = interactions_list_to_sparse_matrix(test_ratings)\n\n# Construct indicator features for users and items\nuser_indicator_features = sparse.identity(n_users)\nitem_indicator_features = sparse.identity(n_items)\n\n# Build a matrix factorization collaborative filter model\ncf_model = tensorrec.TensorRec(n_components=5)\n\n# Fit the collaborative filter model\nprint(""Training collaborative filter"")\ncf_model.fit(interactions=sparse_train_ratings,\n             user_features=user_indicator_features,\n             item_features=item_indicator_features)\n\n# Create sets of train/test interactions that are only ratings >= 4.0\nsparse_train_ratings_4plus = sparse_train_ratings.multiply(sparse_train_ratings >= 4.0)\nsparse_test_ratings_4plus = sparse_test_ratings.multiply(sparse_test_ratings >= 4.0)\n\n\n# This method consumes item ranks for each user and prints out recall@10 train/test metrics\ndef check_results(ranks):\n    train_recall_at_10 = tensorrec.eval.recall_at_k(\n        test_interactions=sparse_train_ratings_4plus,\n        predicted_ranks=ranks,\n        k=10\n    ).mean()\n    test_recall_at_10 = tensorrec.eval.recall_at_k(\n        test_interactions=sparse_test_ratings_4plus,\n        predicted_ranks=ranks,\n        k=10\n    ).mean()\n    print(""Recall at 10: Train: {:.4f} Test: {:.4f}"".format(train_recall_at_10,\n                                                            test_recall_at_10))\n\n\n# Check the results of the MF CF model\nprint(""Matrix factorization collaborative filter:"")\npredicted_ranks = cf_model.predict_rank(user_features=user_indicator_features,\n                                        item_features=item_indicator_features)\ncheck_results(predicted_ranks)\n\n# Let\'s try a new loss function: WMRB\nprint(""Training collaborative filter with WMRB loss"")\nranking_cf_model = tensorrec.TensorRec(n_components=5,\n                                       loss_graph=tensorrec.loss_graphs.WMRBLossGraph())\nranking_cf_model.fit(interactions=sparse_train_ratings_4plus,\n                     user_features=user_indicator_features,\n                     item_features=item_indicator_features,\n                     n_sampled_items=int(n_items * .01))\n\n# Check the results of the WMRB MF CF model\nprint(""WMRB matrix factorization collaborative filter:"")\npredicted_ranks = ranking_cf_model.predict_rank(user_features=user_indicator_features,\n                                                item_features=item_indicator_features)\ncheck_results(predicted_ranks)\n\n# To improve the recommendations, lets read in the movie genres\nprint(\'Loading movie metadata\')\nwith open(\'movies.csv\', \'r\') as movies_file:\n    movies_file_reader = csv.reader(movies_file)\n    raw_movie_metadata = list(movies_file_reader)\n    raw_movie_metadata_header = raw_movie_metadata.pop(0)\n\n# Map the MovieLens IDs to our internal IDs and keep track of the genres and titles\nmovie_genres_by_internal_id = {}\nmovie_titles_by_internal_id = {}\nfor row in raw_movie_metadata:\n    row[0] = movielens_to_internal_item_ids[int(row[0])]  # Map to IDs\n    row[2] = row[2].split(\'|\')  # Split up the genres\n    movie_genres_by_internal_id[row[0]] = row[2]\n    movie_titles_by_internal_id[row[0]] = row[1]\n\n# Look at an example movie metadata row\nprint(""Raw metadata example:\\n{}\\n{}"".format(raw_movie_metadata_header,\n                                             raw_movie_metadata[0]))\n\n# Build a list of genres where the index is the internal movie ID and\n# the value is a list of [Genre, Genre, ...]\nmovie_genres = [movie_genres_by_internal_id[internal_id]\n                for internal_id in range(n_items)]\n\n# Transform the genres into binarized labels using scikit\'s MultiLabelBinarizer\nmovie_genre_features = MultiLabelBinarizer().fit_transform(movie_genres)\nn_genres = movie_genre_features.shape[1]\nprint(""Binarized genres example for movie {}:\\n{}"".format(movie_titles_by_internal_id[0],\n                                                          movie_genre_features[0]))\n\n# Coerce the movie genre features to a sparse matrix, which TensorRec expects\nmovie_genre_features = sparse.coo_matrix(movie_genre_features)\n\n# Fit a content-based model using the genres as item features\nprint(""Training content-based recommender"")\ncontent_model = tensorrec.TensorRec(\n    n_components=n_genres,\n    item_repr_graph=tensorrec.representation_graphs.FeaturePassThroughRepresentationGraph(),\n    loss_graph=tensorrec.loss_graphs.WMRBLossGraph()\n)\ncontent_model.fit(interactions=sparse_train_ratings_4plus,\n                  user_features=user_indicator_features,\n                  item_features=movie_genre_features,\n                  n_sampled_items=int(n_items * .01))\n\n# Check the results of the content-based model\nprint(""Content-based recommender:"")\npredicted_ranks = content_model.predict_rank(user_features=user_indicator_features,\n                                             item_features=movie_genre_features)\ncheck_results(predicted_ranks)\n\n# Try concatenating the genres on to the indicator features for a hybrid recommender system\nfull_item_features = sparse.hstack([item_indicator_features, movie_genre_features])\n\nprint(""Training hybrid recommender"")\nhybrid_model = tensorrec.TensorRec(\n    n_components=5,\n    loss_graph=tensorrec.loss_graphs.WMRBLossGraph()\n)\nhybrid_model.fit(interactions=sparse_train_ratings_4plus,\n                 user_features=user_indicator_features,\n                 item_features=full_item_features,\n                 n_sampled_items=int(n_items * .01))\n\nprint(""Hybrid recommender:"")\npredicted_ranks = hybrid_model.predict_rank(user_features=user_indicator_features,\n                                            item_features=full_item_features)\ncheck_results(predicted_ranks)\n\n# Print out movies that User #432 has liked\nprint(""User 432 liked:"")\nfor m in sparse_train_ratings_4plus[432].indices:\n    print(movie_titles_by_internal_id[m])\n\n# Pull user 432\'s features out of the user features matrix and predict movie ranks for just that user\nu432_features = sparse.csr_matrix(user_indicator_features)[432]\nu432_rankings = hybrid_model.predict_rank(user_features=u432_features,\n                                          item_features=full_item_features)[0]\n\n# Get internal IDs of User 432\'s top 10 recommendations\n# These are sorted by item ID, not by rank\n# This may contain items with which User 432 has already interacted\nu432_top_ten_recs = numpy.where(u432_rankings <= 10)[0]\nprint(""User 432 recommendations:"")\nfor m in u432_top_ten_recs:\n    print(movie_titles_by_internal_id[m])\n\n# Print out User 432\'s held-out interactions\nprint(""User 432\'s held-out movies:"")\nfor m in sparse_test_ratings_4plus[432].indices:\n    print(movie_titles_by_internal_id[m])\n'"
examples/keras_example.py,0,"b'import keras as ks\n\nfrom tensorrec import TensorRec\nfrom tensorrec.eval import fit_and_eval, eval_random_ranks_on_dataset\nfrom tensorrec.loss_graphs import BalancedWMRBLossGraph\nfrom tensorrec.representation_graphs import (\n    AbstractKerasRepresentationGraph, NormalizedLinearRepresentationGraph, LinearRepresentationGraph\n)\nfrom tensorrec.util import append_to_string_at_point\n\nfrom test.datasets import get_book_crossing\n\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\n\n# Build results header string\nresult_strings = []\nheader = ""UserRepr Graph""\nheader = append_to_string_at_point(header, \'ItemRepr Graph\', 40)\nheader = append_to_string_at_point(header, \'Rec. In-Sample\', 70)\nheader = append_to_string_at_point(header, \'Rec. Out-sample\', 90)\nheader = append_to_string_at_point(header, \'Prec. In-Sample\', 110)\nheader = append_to_string_at_point(header, \'Prec. Out-sample\', 130)\nheader = append_to_string_at_point(header, \'NDCG In-Sample\', 150)\nheader = append_to_string_at_point(header, \'NDCG Out-sample\', 170)\nresult_strings.append(header)\n\n# Load the book crossing dataset\ntrain_interactions, test_interactions, user_features, item_features, _ = get_book_crossing(user_indicators=False,\n                                                                                           item_indicators=True,\n                                                                                           cold_start_users=True)\n\n# Establish baseline metrics with random ranks for warm and cold start users\nrandom_warm_result = eval_random_ranks_on_dataset(train_interactions, recall_k=100, precision_k=100, ndcg_k=100)\nrandom_cold_result = eval_random_ranks_on_dataset(test_interactions, recall_k=100, precision_k=100, ndcg_k=100)\nres_string = \'RANDOM BASELINE\'\nres_string = append_to_string_at_point(res_string, "": {:0.4f}"".format(random_warm_result[0]), 68)\nres_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(random_cold_result[0]), 90)\nres_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(random_warm_result[1]), 110)\nres_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(random_cold_result[1]), 130)\nres_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(random_warm_result[2]), 150)\nres_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(random_cold_result[2]), 170)\nlogging.info(header)\nlogging.info(res_string)\nresult_strings.append(res_string)\n\n\n# Construct a Keras representation graph by inheriting tensorrec.representation_graphs.AbstractKerasRepresentationGraph\nclass DeepRepresentationGraph(AbstractKerasRepresentationGraph):\n\n    # This method returns an ordered list of Keras layers connecting the user/item features to the user/item\n    # representation. When TensorRec learns, the learning will happen in these layers.\n    def create_layers(self, n_features, n_components):\n        return [\n            ks.layers.Dense(n_components * 16, activation=\'relu\'),\n            ks.layers.Dense(n_components * 8, activation=\'relu\'),\n            ks.layers.Dense(n_components * 2, activation=\'relu\'),\n            ks.layers.Dense(n_components, activation=\'tanh\'),\n        ]\n\n\n# Try different configurations using DeepRepresentationGraph for both item_repr and user_repr. If\n# DeepRepresentationGraph is used, a deep neural network will learn to represent the users or items.\nfor user_repr in (NormalizedLinearRepresentationGraph, DeepRepresentationGraph):\n    for item_repr in (LinearRepresentationGraph, DeepRepresentationGraph):\n        model = TensorRec(n_components=20,\n                          item_repr_graph=item_repr(),\n                          user_repr_graph=user_repr(),\n                          loss_graph=BalancedWMRBLossGraph(),\n                          biased=False)\n\n        # Fit the model and get a result packet\n        fit_kwargs = {\'epochs\': 500, \'learning_rate\': .01, \'n_sampled_items\': 100, \'verbose\': True}\n        result = fit_and_eval(model, user_features, item_features, train_interactions, test_interactions, fit_kwargs,\n                              recall_k=100, precision_k=100, ndcg_k=100)\n\n        # Build results row for this configuration\n        res_string = ""{}"".format(user_repr.__name__)\n        res_string = append_to_string_at_point(res_string, item_repr.__name__, 40)\n        res_string = append_to_string_at_point(res_string, "": {:0.4f}"".format(result[3]), 68)\n        res_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(result[0]), 90)\n        res_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(result[4]), 110)\n        res_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(result[1]), 130)\n        res_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(result[5]), 150)\n        res_string = append_to_string_at_point(res_string, ""{:0.4f}"".format(result[2]), 170)\n        logging.info(header)\n        logging.info(res_string)\n        result_strings.append(res_string)\n\n# Log the final results of all models\nfor res_string in result_strings:\n    logging.info(res_string)\n'"
examples/plot_movielens.py,0,"b'import glob\nimport os\n\nimport imageio\nimageio.plugins.ffmpeg.download()  # noqa\n\nimport matplotlib.pyplot as plt\nimport moviepy.editor as mpy\nimport numpy as np\n\nfrom tensorrec import TensorRec\nfrom tensorrec.eval import precision_at_k, recall_at_k\nfrom tensorrec.input_utils import create_tensorrec_dataset_from_sparse_matrix\nfrom tensorrec.loss_graphs import BalancedWMRBLossGraph\nfrom tensorrec.representation_graphs import ReLURepresentationGraph\n\nfrom test.datasets import get_movielens_100k\n\nimport logging\nlogging.getLogger().setLevel(logging.INFO)\n\n# Load the movielens dataset\ntrain_interactions, test_interactions, user_features, item_features, item_titles = \\\n    get_movielens_100k(negative_value=-1.0)\n\n# Assemble parameters for fitting. \'epochs\' is 1 in the fit_kwargs because we will be calling fit_partial 1000 times to\n# run 1000 epochs.\nepochs = 1000\nfit_kwargs = {\'epochs\': 1, \'alpha\': 0.0001, \'verbose\': True, \'learning_rate\': .01,\n              \'n_sampled_items\': int(item_features.shape[0] * .1)}\n\n# Build the TensorRec model\nmodel = TensorRec(n_components=2,\n                  biased=False,\n                  loss_graph=BalancedWMRBLossGraph(),\n                  item_repr_graph=ReLURepresentationGraph(),\n                  n_tastes=3)\n\n# Make some random selections of movies and users we want to plot\nmovies_to_plot = np.random.choice(a=item_features.shape[0], size=50, replace=False)\nuser_to_plot = np.random.choice(a=user_features.shape[0], size=100, replace=False)\n\n# Coerce data to datasets for faster fitting\ntrain_interactions_ds = create_tensorrec_dataset_from_sparse_matrix(train_interactions)\nuser_features_ds = create_tensorrec_dataset_from_sparse_matrix(user_features)\nitem_features_ds = create_tensorrec_dataset_from_sparse_matrix(item_features)\n\n# Iterate through 1000 epochs, outputting a JPG plot each epoch\n_, ax = plt.subplots()\nfor epoch in range(epochs):\n    model.fit_partial(interactions=train_interactions_ds,\n                      user_features=user_features_ds,\n                      item_features=item_features_ds,\n                      **fit_kwargs)\n\n    # The position of a movie or user is that movie\'s/user\'s 2-dimensional representation.\n    movie_positions = model.predict_item_representation(item_features_ds)\n    user_positions = model.predict_user_representation(user_features_ds)\n\n    # Handle multiple tastes, if applicable. If there are more than 1 taste per user, only the first of each user\'s\n    # tastes will be plotted.\n    if model.n_tastes > 1:\n        user_positions = user_positions[0]\n\n    ax.grid(b=True, which=\'both\')\n    ax.axhline(y=0, color=\'k\')\n    ax.axvline(x=0, color=\'k\')\n    ax.scatter(*zip(*user_positions[user_to_plot]), color=\'r\', s=1)\n    ax.scatter(*zip(*movie_positions[movies_to_plot]), s=2)\n    ax.set_aspect(\'equal\')\n\n    for i, movie in enumerate(movies_to_plot):\n        movie_name = item_titles[movie]\n        movie_position = movie_positions[movie]\n        # Comment this line to remove movie titles to the plot.\n        ax.annotate(movie_name, movie_position[0:2], fontsize=\'x-small\')\n\n    file = \'/tmp/tensorrec/movielens/epoch_{}.jpg\'.format(epoch)\n    plt.savefig(file)\n    plt.cla()\n\n    logging.info(""Finished epoch {}"".format(epoch))\n\nranks = model.predict_rank(user_features=user_features,\n                           item_features=item_features,)\np_at_k = precision_at_k(ranks, test_interactions, k=5)\nr_at_k = recall_at_k(ranks, test_interactions, k=30)\n\nlogging.info(""Precision@5: {}, Recall@30: {}"".format(np.mean(p_at_k), np.mean(r_at_k)))\n\n# Use the collected JPG files to create an MP4 video of the model fitting, then delete the JPGs.\nfps = 12\nfile_list = glob.glob(\'/tmp/tensorrec/movielens/*.jpg\')\nlist.sort(file_list, key=lambda x: int(x.split(\'_\')[1].split(\'.jpg\')[0]))\nclip = mpy.ImageSequenceClip(file_list, fps=fps)\nvid_file = \'/tmp/tensorrec/movielens/movielens.mp4\'\nclip.write_videofile(filename=vid_file, fps=fps, codec=\'mpeg4\', preset=\'veryslow\', ffmpeg_params=[\'-qscale:v\', \'10\'])\nfor file in file_list:\n    os.remove(file)\n'"
tensorrec/__init__.py,0,"b""from .tensorrec import TensorRec\nfrom . import eval\nfrom . import input_utils\nfrom . import loss_graphs\nfrom . import representation_graphs\nfrom . import prediction_graphs\nfrom . import session_management\nfrom . import util\n\n__version__ = '0.26.2'\n\n__all__ = [\n    TensorRec, eval, util, loss_graphs, representation_graphs, prediction_graphs, session_management, input_utils\n]\n\n# Suppress TensorFlow logs\nimport logging\nlogging.getLogger('tensorflow').disabled = True\n"""
tensorrec/errors.py,0,"b'from .util import lazyval\n\n\nclass TensorRecException(Exception):\n    msg = None\n\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n\n    @lazyval\n    def message(self):\n        return str(self)\n\n    def __str__(self):\n        msg = self.msg.format(**self.kwargs)\n        return msg\n\n    __unicode__ = __str__\n    __repr__ = __str__\n\n\nclass ModelNotBiasedException(TensorRecException):\n    msg = \'Cannot predict {actor} bias for unbiased model\'\n\n\nclass ModelNotFitException(TensorRecException):\n    msg = ""{method}() has been called before model fitting. Call fit() or fit_partial() before calling {method}().""\n\n\nclass ModelWithoutAttentionException(TensorRecException):\n    msg = ""This TensorRec model does not use attention. Try re-building TensorRec with a valid \'attention_graph\' arg.""\n\n\nclass BatchNonSparseInputException(TensorRecException):\n    msg = \'In order to support user batching at fit time, interactions and user_features must both be scipy.sparse \' \\\n          \'matrices.\'\n\n\nclass TfVersionException(TensorRecException):\n    msg = ""You need to have at least TensorFlow version 1.7 installed in order to use TensorRec properly. You have "" \\\n          ""currently installed TensorFlow: {tf_version}""\n'"
tensorrec/eval.py,0,"b'import numpy as np\nimport scipy.sparse as sp\n\nfrom .tensorrec import TensorRec\n\n\ndef precision_at_k(predicted_ranks, test_interactions, k=10, preserve_rows=False):\n    """"""\n    Modified from LightFM.\n    :param predicted_ranks: Numpy matrix\n    The results of model.predict_rank()\n    :param test_interactions: scipy.sparse matrix\n    Test interactions matrix of shape [n_users, n_items]\n    :param k: int\n    The rank at which to stop evaluating precision.\n    :param preserve_rows: bool\n    If True, a value of 0 will be returned for every user without test interactions.\n    If False, only users with test interactions will be returned.\n    :return: np.array\n    """"""\n    positive_test_interactions = test_interactions > 0\n    ranks = sp.csr_matrix(predicted_ranks * positive_test_interactions.A)\n    ranks.data = np.less(ranks.data, (k + 1), ranks.data)\n\n    precision = np.squeeze(np.array(ranks.sum(axis=1))).astype(float) / k\n\n    if not preserve_rows:\n        precision = precision[positive_test_interactions.getnnz(axis=1) > 0]\n\n    return precision\n\n\ndef recall_at_k(predicted_ranks, test_interactions, k=10, preserve_rows=False):\n    """"""\n    Modified from LightFM.\n    :param predicted_ranks: Numpy matrix\n    The results of model.predict_rank()\n    :param test_interactions: scipy.sparse matrix\n    Test interactions matrix of shape [n_users, n_items]\n    :param k: int\n    The rank at which to stop evaluating recall.\n    :param preserve_rows: bool\n    If True, a value of 0 will be returned for every user without test interactions.\n    If False, only users with test interactions will be returned.\n    :return: np.array\n    """"""\n    positive_test_interactions = test_interactions > 0\n    ranks = sp.csr_matrix(predicted_ranks * positive_test_interactions.A)\n    ranks.data = np.less(ranks.data, (k + 1), ranks.data)\n\n    retrieved = np.squeeze(positive_test_interactions.getnnz(axis=1))\n    hit = np.squeeze(np.array(ranks.sum(axis=1)))\n\n    if not preserve_rows:\n        hit = hit[positive_test_interactions.getnnz(axis=1) > 0]\n        retrieved = retrieved[positive_test_interactions.getnnz(axis=1) > 0]\n\n    return hit.astype(float) / retrieved.astype(float)\n\n\ndef _setup_ndcg(predicted_ranks, test_interactions, k=10):\n\n    pos_inter = test_interactions > 0\n    ror = sp.csr_matrix(predicted_ranks * pos_inter.A)\n\n    relevance = sp.csr_matrix(test_interactions.A * pos_inter.A)\n\n    k_mask = np.less(ror.data, k + 1)\n    ror_at_k = np.maximum(np.multiply(ror.data, k_mask), 1)\n\n    return relevance, k_mask, ror, ror_at_k\n\n\ndef _idcg(hits, k=10):\n    sorted_hits = hits[np.argsort(-hits)][:min(len(hits), k)]\n    idgc = np.sum((2**sorted_hits-1)/np.log2(np.arange(len(sorted_hits)) + 2))\n    return idgc\n\n\ndef _dcg(relevance, k_mask, ror_at_k, ror):\n    numer = (2**np.multiply(relevance.data, k_mask)) - 1\n    denom = np.log2(ror_at_k + 1)\n    ror.data = numer/denom  # ranks at 1\n    dcg = ror.sum(axis=1).flatten()\n\n    return dcg\n\n\ndef ndcg_at_k(predicted_ranks, test_interactions, k=10, preserve_rows=False):\n    """"""\n    Calculate Normalized Discounted Cumulative Gain @K.\n    :param predicted_ranks: Numpy matrix\n    The results of model.predict_rank()\n    :param test_interactions: scipy.sparse matrix\n    Test interactions matrix of shape [n_users, n_items]\n    :param k: int\n    The rank at which to stop evaluating NDCG.\n    :param preserve_rows: bool\n    If True, a value of 0 will be returned for every user without test interactions.\n    If False, only users with test interactions will be returned.\n    :return: np.array\n    """"""\n\n    relevance, k_mask, ranks_of_relevant, ror_at_k = _setup_ndcg(predicted_ranks,\n                                                                 test_interactions,\n                                                                 k)\n\n    dcg = np.asarray(_dcg(relevance, k_mask, ror_at_k, ranks_of_relevant))[0]\n    idcg = np.apply_along_axis(_idcg, 1, relevance.A)\n\n    ndcg = dcg/idcg\n\n    if not preserve_rows:\n        positive_test_interactions = test_interactions > 0\n        ndcg = ndcg[positive_test_interactions.getnnz(axis=1) > 0]\n\n    return ndcg\n\n\ndef f1_score_at_k(predicted_ranks, test_interactions, k=10, preserve_rows=False):\n    """"""\n    :param model: TensorRec\n    A trained TensorRec model.\n    :param test_interactions: scipy.sparse matrix\n    Test interactions matrix of shape [n_users, n_items]\n    :param user_features: scipy.sparse matrix\n    User features matrix of shape [n_users, n_user_features]\n    :param item_features: scipy.sparse matrix\n    Item features matrix of shape [n_items, n_item_features]\n    :param k: int\n    The rank at which to stop evaluating recall.\n    :param preserve_rows: bool\n    If True, a value of 0 will be returned for every user without test interactions.\n    If False, only users with test interactions will be returned.\n    :return: np.array\n    """"""\n    p_at_k = precision_at_k(predicted_ranks=predicted_ranks,\n                            test_interactions=test_interactions,\n                            k=k, preserve_rows=preserve_rows)\n    r_at_k = recall_at_k(predicted_ranks=predicted_ranks,\n                         test_interactions=test_interactions,\n                         k=k, preserve_rows=preserve_rows)\n\n    mean_p = np.mean(p_at_k)\n    mean_r = np.mean(r_at_k)\n\n    f1_score = (2.0 * mean_p * mean_r) / (mean_p + mean_r)\n    return f1_score\n\n\ndef fit_and_eval(model, user_features, item_features, train_interactions, test_interactions, fit_kwargs, recall_k=30,\n                 precision_k=5, ndcg_k=30):\n\n    model.fit(user_features=user_features, item_features=item_features,\n              interactions=train_interactions, **fit_kwargs)\n    predicted_ranks = model.predict_rank(user_features=user_features, item_features=item_features)\n\n    p_at_k = precision_at_k(predicted_ranks, test_interactions, k=precision_k)\n    r_at_k = recall_at_k(predicted_ranks, test_interactions, k=recall_k)\n    n_at_k = ndcg_at_k(predicted_ranks, test_interactions, k=ndcg_k)\n\n    p_at_k_insample = precision_at_k(predicted_ranks, train_interactions, k=precision_k)\n    r_at_k_insample = recall_at_k(predicted_ranks, train_interactions, k=recall_k)\n    n_at_k_insample = ndcg_at_k(predicted_ranks, train_interactions, k=ndcg_k)\n\n    return (np.mean(r_at_k), np.mean(p_at_k), np.mean(n_at_k), np.mean(r_at_k_insample), np.mean(p_at_k_insample),\n            np.mean(n_at_k_insample))\n\n\ndef grid_check_model_on_dataset(train_interactions, test_interactions, user_features, item_features):\n\n    results = []\n    for n_components in [2, 4, 8, 16, 32, 64, 128, 256]:\n        for epochs in [2, 4, 8, 16, 32, 64, 128, 256]:\n            model = TensorRec(n_components=n_components)\n            scores = fit_and_eval(model, user_features, item_features, train_interactions, test_interactions,\n                                  fit_kwargs={\'epochs\': epochs})\n            results.append((n_components, scores))\n            print(n_components, epochs, scores)\n\n\ndef eval_random_ranks_on_dataset(interactions, recall_k=30, precision_k=5, ndcg_k=30):\n\n    n_users, n_items = interactions.shape\n\n    random_guesses = np.array([np.random.choice(a=n_items, size=n_items, replace=False) + 1 for _ in range(n_users)])\n\n    p_at_k = precision_at_k(random_guesses, interactions, k=precision_k)\n    r_at_k = recall_at_k(random_guesses, interactions, k=recall_k)\n    n_at_k = ndcg_at_k(random_guesses, interactions, k=ndcg_k)\n\n    return np.mean(r_at_k), np.mean(p_at_k), np.mean(n_at_k)\n'"
tensorrec/input_utils.py,19,"b'import numpy as np\nfrom scipy import sparse as sp\nimport tensorflow as tf\n\nfrom .session_management import get_session\n\n\ndef create_tensorrec_iterator(name):\n    """"""\n    Creates a TensorFlow Iterator that is ready for the standard TensorRec data format.\n    :param name: str\n    The name for this Iterator.\n    :return: tf.data.Iterator\n    """"""\n    return tf.data.Iterator.from_structure(\n            output_types=(tf.int64, tf.int64, tf.float32, tf.int64, tf.int64),\n            output_shapes=([None], [None], [None], [], []),\n            shared_name=name\n    )\n\n\ndef create_tensorrec_dataset_from_sparse_matrix(sparse_matrix):\n    """"""\n    Creates a TensorFlow Dataset containing the data from the given sparse matrix.\n    :param sparse_matrix: scipy.sparse matrix\n    The data to be contained in this Dataset.\n    :return: tf.data.Dataset\n    """"""\n    if not isinstance(sparse_matrix, sp.coo_matrix):\n        sparse_matrix = sp.coo_matrix(sparse_matrix)\n\n    row_index = np.array([sparse_matrix.row], dtype=np.int64)\n    col_index = np.array([sparse_matrix.col], dtype=np.int64)\n    values = np.array([sparse_matrix.data], dtype=np.float32)\n    n_dim_0 = np.array([sparse_matrix.shape[0]], dtype=np.int64)\n    n_dim_1 = np.array([sparse_matrix.shape[1]], dtype=np.int64)\n\n    tensor_slices = (row_index, col_index, values, n_dim_0, n_dim_1)\n\n    return tf.data.Dataset.from_tensor_slices(tensor_slices)\n\n\ndef write_tfrecord_from_sparse_matrix(tfrecord_path, sparse_matrix):\n    """"""\n    Writes the contents of a sparse matrix to a TFRecord file.\n    :param tfrecord_path: str\n    :param sparse_matrix: scipy.sparse matrix\n    :return: str\n    The tfrecord path\n    """"""\n    dataset = create_tensorrec_dataset_from_sparse_matrix(sparse_matrix=sparse_matrix)\n    return write_tfrecord_from_tensorrec_dataset(tfrecord_path=tfrecord_path,\n                                                 dataset=dataset)\n\n\ndef get_dimensions_from_tensorrec_dataset(dataset):\n    """"""\n    Given a TensorFlow Dataset in the standard TensorRec format, returns the dimensions of the SparseTensor to be\n    populated by the Dataset.\n    :param dataset: tf.data.Dataset\n    :return: (int, int)\n    """"""\n    session = get_session()\n    iterator = create_tensorrec_iterator(\'dims_iterator\')\n    initializer = iterator.make_initializer(dataset)\n    _, _, _, tf_d0, tf_d1 = iterator.get_next()\n    session.run(initializer)\n    d0, d1 = session.run([tf_d0, tf_d1])\n    return d0, d1\n\n\ndef write_tfrecord_from_tensorrec_dataset(tfrecord_path, dataset):\n    """"""\n    Writes the contents of a TensorRec Dataset to a TFRecord file.\n    :param tfrecord_path: str\n    :param dataset: tf.data.Dataset\n    :return: str\n    The tfrecord path\n    """"""\n    session = get_session()\n    iterator = create_tensorrec_iterator(\'dataset_writing_iterator\')\n    initializer = iterator.make_initializer(dataset)\n    tf_row_index, tf_col_index, tf_values, tf_d0, tf_d1 = iterator.get_next()\n    session.run(initializer)\n    row_index, col_index, values, d0, d1 = session.run([tf_row_index, tf_col_index, tf_values, tf_d0, tf_d1])\n\n    def _int64_feature(int_values):\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=int_values))\n\n    def _float_feature(float_values):\n        return tf.train.Feature(float_list=tf.train.FloatList(value=float_values))\n\n    writer = tf.python_io.TFRecordWriter(tfrecord_path)\n    feature = {\n        \'row_index\': _int64_feature(row_index),\n        \'col_index\': _int64_feature(col_index),\n        \'values\': _float_feature(values),\n        \'d0\': _int64_feature([d0]),\n        \'d1\': _int64_feature([d1]),\n    }\n    example = tf.train.Example(features=tf.train.Features(feature=feature))\n    writer.write(example.SerializeToString())\n    writer.close()\n    return tfrecord_path\n\n\ndef create_tensorrec_dataset_from_tfrecord(tfrecord_path):\n    """"""\n    Loads a TFRecord file and creates a Dataset with the contents.\n    :param tfrecord_path: str\n    :return: tf.data.Dataset\n    """"""\n\n    def parse_tensorrec_tfrecord(example_proto):\n        features = {\n            \'row_index\': tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n            \'col_index\': tf.FixedLenSequenceFeature((), tf.int64, allow_missing=True),\n            \'values\': tf.FixedLenSequenceFeature((), tf.float32, allow_missing=True),\n            \'d0\': tf.FixedLenFeature((), tf.int64),\n            \'d1\': tf.FixedLenFeature((), tf.int64),\n        }\n        parsed_features = tf.parse_single_example(example_proto, features)\n        return (parsed_features[\'row_index\'], parsed_features[\'col_index\'], parsed_features[\'values\'],\n                parsed_features[\'d0\'], parsed_features[\'d1\'])\n\n    dataset = tf.data.TFRecordDataset(tfrecord_path).map(parse_tensorrec_tfrecord)\n    return dataset\n'"
tensorrec/loss_graphs.py,62,"b'import abc\nimport tensorflow as tf\n\n\nclass AbstractLossGraph(object):\n    __metaclass__ = abc.ABCMeta\n\n    # If True, dense prediction results will be passed to the loss function\n    is_dense = False\n\n    # If True, randomly sampled predictions will be passed to the loss function\n    is_sample_based = False\n    # If True, and if is_sample_based is True, predictions will be sampled with replacement\n    is_sampled_with_replacement = False\n\n    @abc.abstractmethod\n    def connect_loss_graph(self, tf_prediction_serial, tf_interactions_serial, tf_interactions, tf_n_users, tf_n_items,\n                           tf_prediction, tf_rankings, tf_sample_predictions, tf_n_sampled_items):\n        """"""\n        This method is responsible for consuming a number of possible nodes from the graph and calculating loss from\n        those nodes.\n\n        The following parameters are always passed in\n        :param tf_prediction_serial: tf.Tensor\n        The recommendation scores as a Tensor of shape [n_samples, 1]\n        :param tf_interactions_serial: tf.Tensor\n        The sample interactions corresponding to tf_prediction_serial as a Tensor of shape [n_samples, 1]\n        :param tf_interactions: tf.SparseTensor\n        The sample interactions as a SparseTensor of shape [n_users, n_items]\n        :param tf_n_users: tf.placeholder\n        The number of users in tf_interactions\n        :param tf_n_items: tf.placeholder\n        The number of items in tf_interactions\n\n        The following parameters are passed in if is_dense is True\n        :param tf_prediction: tf.Tensor\n        The recommendation scores as a Tensor of shape [n_users, n_items]\n        :param tf_rankings: tf.Tensor\n        The item ranks as a Tensor of shape [n_users, n_items]\n\n        The following parameters are passed in if is_sample_based is True\n        :param tf_sample_predictions: tf.Tensor\n        The recommendation scores of a sample of items of shape [n_users, n_sampled_items]\n        :param tf_n_sampled_items: tf.placeholder\n        The number of items per user in tf_sample_predictions\n\n        :return: tf.Tensor\n        The loss value.\n        """"""\n        pass\n\n\nclass RMSELossGraph(AbstractLossGraph):\n    """"""\n    This loss function returns the root mean square error between the predictions and the true interactions.\n    Interactions can be any positive or negative values, and this loss function is sensitive to magnitude.\n    """"""\n    def connect_loss_graph(self, tf_prediction_serial, tf_interactions_serial, **kwargs):\n        return tf.sqrt(tf.reduce_mean(tf.square(tf_interactions_serial - tf_prediction_serial)))\n\n\nclass RMSEDenseLossGraph(AbstractLossGraph):\n    """"""\n    This loss function returns the root mean square error between the predictions and the true interactions, including\n    all non-interacted values as 0s.\n    Interactions can be any positive or negative values, and this loss function is sensitive to magnitude.\n    """"""\n    is_dense = True\n\n    def connect_loss_graph(self, tf_interactions, tf_prediction, **kwargs):\n        error = tf.sparse_add(tf_interactions, -1.0 * tf_prediction)\n        return tf.sqrt(tf.reduce_mean(tf.square(error)))\n\n\nclass SeparationLossGraph(AbstractLossGraph):\n    """"""\n    This loss function models the explicit positive and negative interaction predictions as normal distributions and\n    returns the probability of overlap between the two distributions.\n    Interactions can be any positive or negative values, but this loss function ignores the magnitude of the\n    interaction -- interactions are grouped in to {i <= 0} and {i > 0}.\n    """"""\n    def connect_loss_graph(self, tf_prediction_serial, tf_interactions_serial, **kwargs):\n\n        tf_positive_mask = tf.greater(tf_interactions_serial, 0.0)\n        tf_negative_mask = tf.less_equal(tf_interactions_serial, 0.0)\n\n        tf_positive_predictions = tf.boolean_mask(tf_prediction_serial, tf_positive_mask)\n        tf_negative_predictions = tf.boolean_mask(tf_prediction_serial, tf_negative_mask)\n\n        tf_pos_mean, tf_pos_var = tf.nn.moments(tf_positive_predictions, axes=[0])\n        tf_neg_mean, tf_neg_var = tf.nn.moments(tf_negative_predictions, axes=[0])\n\n        tf_overlap_distribution = tf.contrib.distributions.Normal(loc=(tf_neg_mean - tf_pos_mean),\n                                                                  scale=tf.sqrt(tf_neg_var + tf_pos_var))\n\n        loss = 1.0 - tf_overlap_distribution.cdf(0.0)\n        return loss\n\n\nclass SeparationDenseLossGraph(AbstractLossGraph):\n    """"""\n    This loss function models all positive and negative interaction predictions as normal distributions and\n    returns the probability of overlap between the two distributions. This loss function includes non-interacted items\n    as negative interactions.\n    Interactions can be any positive or negative values, but this loss function ignores the magnitude of the\n    interaction -- interactions are grouped in to {i <= 0} and {i > 0}.\n    """"""\n    is_dense = True\n\n    def connect_loss_graph(self, tf_prediction, tf_interactions, **kwargs):\n\n        interactions_shape = tf.shape(tf_interactions)\n        int_serial_shape = tf.cast([interactions_shape[0] * interactions_shape[1]], tf.int32)\n        tf_interactions_serial = tf.reshape(tf.sparse_tensor_to_dense(tf_interactions),\n                                            shape=int_serial_shape)\n\n        prediction_shape = tf.shape(tf_prediction)\n        pred_serial_shape = tf.cast([prediction_shape[0] * prediction_shape[1]], tf.int32)\n        tf_prediction_serial = tf.reshape(tf_prediction, shape=pred_serial_shape)\n\n        tf_positive_mask = tf.greater(tf_interactions_serial, 0.0)\n        tf_negative_mask = tf.less_equal(tf_interactions_serial, 0.0)\n\n        tf_positive_predictions = tf.boolean_mask(tf_prediction_serial, tf_positive_mask)\n        tf_negative_predictions = tf.boolean_mask(tf_prediction_serial, tf_negative_mask)\n\n        tf_pos_mean, tf_pos_var = tf.nn.moments(tf_positive_predictions, axes=[0])\n        tf_neg_mean, tf_neg_var = tf.nn.moments(tf_negative_predictions, axes=[0])\n\n        tf_overlap_distribution = tf.contrib.distributions.Normal(loc=(tf_neg_mean - tf_pos_mean),\n                                                                  scale=tf.sqrt(tf_neg_var + tf_pos_var))\n\n        loss = 1.0 - tf_overlap_distribution.cdf(0.0)\n        return loss\n\n\nclass WMRBLossGraph(AbstractLossGraph):\n    """"""\n    Approximation of http://ceur-ws.org/Vol-1905/recsys2017_poster3.pdf\n    Interactions can be any positive values, but magnitude is ignored. Negative interactions are ignored.\n    """"""\n    is_sample_based = True\n\n    def connect_loss_graph(self, tf_prediction_serial, tf_interactions, tf_sample_predictions, tf_n_items,\n                           tf_n_sampled_items, **kwargs):\n\n        return self.weighted_margin_rank_batch(tf_prediction_serial=tf_prediction_serial,\n                                               tf_interactions=tf_interactions,\n                                               tf_sample_predictions=tf_sample_predictions,\n                                               tf_n_items=tf_n_items,\n                                               tf_n_sampled_items=tf_n_sampled_items)\n\n    def weighted_margin_rank_batch(self, tf_prediction_serial, tf_interactions, tf_sample_predictions, tf_n_items,\n                                   tf_n_sampled_items):\n        positive_interaction_mask = tf.greater(tf_interactions.values, 0.0)\n        positive_interaction_indices = tf.boolean_mask(tf_interactions.indices,\n                                                       positive_interaction_mask)\n\n        # [ n_positive_interactions ]\n        positive_predictions = tf.boolean_mask(tf_prediction_serial,\n                                               positive_interaction_mask)\n\n        n_items = tf.cast(tf_n_items, dtype=tf.float32)\n        n_sampled_items = tf.cast(tf_n_sampled_items, dtype=tf.float32)\n\n        # [ n_positive_interactions, n_sampled_items ]\n        mapped_predictions_sample_per_interaction = tf.gather(params=tf_sample_predictions,\n                                                              indices=tf.transpose(positive_interaction_indices)[0])\n\n        # [ n_positive_interactions, n_sampled_items ]\n        summation_term = tf.maximum(1.0\n                                    - tf.expand_dims(positive_predictions, axis=1)\n                                    + mapped_predictions_sample_per_interaction,\n                                    0.0)\n\n        # [ n_positive_interactions ]\n        sampled_margin_rank = (n_items / n_sampled_items) * tf.reduce_sum(summation_term, axis=1)\n\n        loss = tf.log(sampled_margin_rank + 1.0)\n        return loss\n\n\nclass BalancedWMRBLossGraph(WMRBLossGraph):\n    """"""\n    This loss graph extends WMRB by making it sensitive to interaction magnitude and weighting the loss of each item by\n    1 / sum(interactions) per item.\n    Interactions can be any positive values. Negative interactions are ignored.\n    """"""\n    def weighted_margin_rank_batch(self, tf_prediction_serial, tf_interactions, tf_sample_predictions, tf_n_items,\n                                   tf_n_sampled_items):\n        positive_interaction_mask = tf.greater(tf_interactions.values, 0.0)\n        positive_interaction_indices = tf.boolean_mask(tf_interactions.indices,\n                                                       positive_interaction_mask)\n        positive_interaction_values = tf.boolean_mask(tf_interactions.values,\n                                                      positive_interaction_mask)\n\n        positive_interactions = tf.SparseTensor(indices=positive_interaction_indices,\n                                                values=positive_interaction_values,\n                                                dense_shape=tf_interactions.dense_shape)\n        listening_sum_per_item = tf.sparse_reduce_sum(positive_interactions, axis=0)\n        gathered_sums = tf.gather(params=listening_sum_per_item,\n                                  indices=tf.transpose(positive_interaction_indices)[1])\n\n        # [ n_positive_interactions ]\n        positive_predictions = tf.boolean_mask(tf_prediction_serial,\n                                               positive_interaction_mask)\n\n        n_items = tf.cast(tf_n_items, dtype=tf.float32)\n        n_sampled_items = tf.cast(tf_n_sampled_items, dtype=tf.float32)\n\n        # [ n_positive_interactions, n_sampled_items ]\n        mapped_predictions_sample_per_interaction = tf.gather(params=tf_sample_predictions,\n                                                              indices=tf.transpose(positive_interaction_indices)[0])\n\n        # [ n_positive_interactions, n_sampled_items ]\n        summation_term = tf.maximum(1.0\n                                    - tf.expand_dims(positive_predictions, axis=1)\n                                    + mapped_predictions_sample_per_interaction,\n                                    0.0)\n\n        # [ n_positive_interactions ]\n        sampled_margin_rank = ((n_items / n_sampled_items)\n                               * tf.reduce_sum(summation_term, axis=1)\n                               * positive_interaction_values / gathered_sums)\n\n        loss = tf.log(sampled_margin_rank + 1.0)\n        return loss\n'"
tensorrec/prediction_graphs.py,29,"b'import abc\nimport tensorflow as tf\n\nfrom .recommendation_graphs import relative_cosine\n\n\nclass AbstractPredictionGraph(object):\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def connect_dense_prediction_graph(self, tf_user_representation, tf_item_representation):\n        """"""\n        This method is responsible for consuming user and item representations and calculating prediction scores for all\n        possible user-item pairs based on these representations.\n        :param tf_user_representation: tf.Tensor\n        The user representations as a Tensor of shape [n_users, n_components]\n        :param tf_item_representation: tf.Tensor\n        The item representations as a Tensor of shape [n_items, n_components]\n        :return: tf.Tensor\n        The predictions as a Tensor of shape [n_users, n_items]\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def connect_serial_prediction_graph(self, tf_user_representation, tf_item_representation, tf_x_user, tf_x_item):\n        """"""\n        This method is responsible for consuming user and item representations and indices and calculating prediction\n        scores for particular user-item pairs.\n        :param tf_user_representation: tf.Tensor\n        The user representations as a Tensor of shape [n_users, n_components]\n        :param tf_item_representation: tf.Tensor\n        The item representations as a Tensor of shape [n_items, n_components]\n        :param tf_x_user: tf.Tensor\n        The users for whom to predict as a Tensor of shape [n_interactions]\n        :param tf_x_item: tf.Tensor\n        The items for which to predict as a Tensor of shape [n_interactions]\n        :return: tf.Tensor\n        The predictions as a Tensor of shape [n_interactions]\n        """"""\n        pass\n\n\nclass DotProductPredictionGraph(AbstractPredictionGraph):\n    """"""\n    This prediction function calculates the prediction as the dot product between the user and item representations.\n    Prediction = user_repr * item_repr\n    """"""\n\n    def connect_dense_prediction_graph(self, tf_user_representation, tf_item_representation):\n        return tf.matmul(tf_user_representation, tf_item_representation, transpose_b=True)\n\n    def connect_serial_prediction_graph(self, tf_user_representation, tf_item_representation, tf_x_user, tf_x_item):\n        gathered_user_reprs = tf.gather(tf_user_representation, tf_x_user)\n        gathered_item_reprs = tf.gather(tf_item_representation, tf_x_item)\n        return tf.reduce_sum(tf.multiply(gathered_user_reprs, gathered_item_reprs), axis=1)\n\n\nclass CosineSimilarityPredictionGraph(AbstractPredictionGraph):\n    """"""\n    This prediction function calculates the prediction as the cosine between the user and item representations.\n    Prediction = cos(user_repr, item_repr)\n    """"""\n\n    def connect_dense_prediction_graph(self, tf_user_representation, tf_item_representation):\n        return relative_cosine(tf_tensor_1=tf_user_representation, tf_tensor_2=tf_item_representation)\n\n    def connect_serial_prediction_graph(self, tf_user_representation, tf_item_representation, tf_x_user, tf_x_item):\n        normalized_users = tf.nn.l2_normalize(tf_user_representation, 1)\n        normalized_items = tf.nn.l2_normalize(tf_item_representation, 1)\n        gathered_user_reprs = tf.gather(normalized_users, tf_x_user)\n        gathered_item_reprs = tf.gather(normalized_items, tf_x_item)\n        return tf.reduce_sum(tf.multiply(gathered_user_reprs, gathered_item_reprs), axis=1)\n\n\nclass EuclideanSimilarityPredictionGraph(AbstractPredictionGraph):\n    """"""\n    This prediction function calculates the prediction as the negative euclidean distance between the user and\n    item representations.\n    Prediction = -1 * sqrt(sum((user_repr - item_repr)^2))\n    """"""\n\n    epsilon = 1e-16\n\n    def connect_dense_prediction_graph(self, tf_user_representation, tf_item_representation):\n\n        # [ n_users, 1 ]\n        r_user = tf.reduce_sum(tf_user_representation ** 2, 1, keep_dims=True)\n\n        # [ n_items, 1 ]\n        r_item = tf.reduce_sum(tf_item_representation ** 2, 1, keep_dims=True)\n\n        # [ n_users, n_items ]\n        distance = (r_user\n                    - 2.0 * tf.matmul(tf_user_representation, tf_item_representation, transpose_b=True)\n                    + tf.transpose(r_item))\n\n        # For numeric stability\n        distance = tf.maximum(distance, self.epsilon)\n\n        return -1.0 * tf.sqrt(distance)\n\n    def connect_serial_prediction_graph(self, tf_user_representation, tf_item_representation, tf_x_user, tf_x_item):\n\n        # [ n_interactions, n_components ]\n        gathered_user_reprs = tf.gather(tf_user_representation, tf_x_user)\n        gathered_item_reprs = tf.gather(tf_item_representation, tf_x_item)\n\n        # [ n_interactions, n_components ]\n        delta = tf.pow(gathered_user_reprs - gathered_item_reprs, 2)\n\n        # [ n_interactions, 1 ]\n        distance = tf.reduce_sum(delta, axis=1)\n\n        # For numeric stability\n        distance = tf.maximum(distance, self.epsilon)\n\n        return -1.0 * tf.sqrt(distance)\n'"
tensorrec/recommendation_graphs.py,22,"b'import tensorflow as tf\n\n\ndef project_biases(tf_features, n_features):\n    """"""\n    Projects the biases from the feature space to calculate bias per actor\n    :param tf_features:\n    :param n_features:\n    :return:\n    """"""\n    tf_feature_biases = tf.Variable(tf.zeros([n_features, 1]))\n\n    # The reduce sum is to perform a rank reduction\n    tf_projected_biases = tf.reduce_sum(\n        tf.sparse_tensor_dense_matmul(tf_features, tf_feature_biases),\n        axis=1\n    )\n\n    return tf_feature_biases, tf_projected_biases\n\n\ndef split_sparse_tensor_indices(tf_sparse_tensor, n_dimensions):\n    """"""\n    Separates the each dimension of a sparse tensor\'s indices into independent tensors\n    :param tf_sparse_tensor:\n    :param n_dimensions:\n    :return:\n    """"""\n    tf_transposed_indices = tf.transpose(tf_sparse_tensor.indices)\n    return (tf_transposed_indices[i] for i in range(n_dimensions))\n\n\ndef bias_prediction_dense(tf_prediction, tf_projected_user_biases, tf_projected_item_biases):\n    """"""\n    Broadcasts user and item biases across their respective axes of the dense predictions\n    :param tf_prediction:\n    :param tf_projected_user_biases:\n    :param tf_projected_item_biases:\n    :return:\n    """"""\n    return tf_prediction + tf.expand_dims(tf_projected_user_biases, 1) + tf.expand_dims(tf_projected_item_biases, 0)\n\n\ndef bias_prediction_serial(tf_prediction_serial, tf_projected_user_biases, tf_projected_item_biases, tf_x_user,\n                           tf_x_item):\n    """"""\n    Calculates the bias per user/item pair and adds it to the serial predictions\n    :param tf_prediction_serial:\n    :param tf_projected_user_biases:\n    :param tf_projected_item_biases:\n    :param tf_x_user:\n    :param tf_x_item:\n    :return:\n    """"""\n    gathered_user_biases = tf.gather(tf_projected_user_biases, tf_x_user)\n    gathered_item_biases = tf.gather(tf_projected_item_biases, tf_x_item)\n    return tf_prediction_serial + gathered_user_biases + gathered_item_biases\n\n\ndef densify_sampled_item_predictions(tf_sample_predictions_serial, tf_n_sampled_items, tf_n_users):\n    """"""\n    Turns the serial predictions of the sample items in to a dense matrix of shape [ n_users, n_sampled_items ]\n    :param tf_sample_predictions_serial:\n    :param tf_n_sampled_items:\n    :param tf_n_users:\n    :return:\n    """"""\n    densified_shape = tf.cast(tf.stack([tf_n_users, tf_n_sampled_items]), tf.int32)\n    densified_predictions = tf.reshape(tf_sample_predictions_serial, shape=densified_shape)\n    return densified_predictions\n\n\ndef rank_predictions(tf_prediction):\n    """"""\n    Double-sortation serves as a ranking process\n    The +1 is so the top-ranked has a non-zero rank\n    :param tf_prediction:\n    :return:\n    """"""\n    tf_prediction_item_size = tf.shape(tf_prediction)[1]\n    tf_indices_of_ranks = tf.nn.top_k(tf_prediction, k=tf_prediction_item_size)[1]\n    return tf.nn.top_k(-tf_indices_of_ranks, k=tf_prediction_item_size)[1] + 1\n\n\ndef collapse_mixture_of_tastes(tastes_predictions, tastes_attentions):\n    """"""\n    Collapses a list of prediction nodes in to a single prediction node.\n    :param tastes_predictions:\n    :param tastes_attentions:\n    :return:\n    """"""\n    stacked_predictions = tf.stack(tastes_predictions)\n\n    # If there is attention, the attentions are used to weight each prediction\n    if tastes_attentions is not None:\n\n        # Stack the attentions and perform softmax across the tastes\n        stacked_attentions = tf.stack(tastes_attentions)\n        softmax_attentions = tf.nn.softmax(stacked_attentions, axis=0)\n\n        # The softmax\'d attentions serve as weights for the taste predictions\n        weighted_predictions = tf.multiply(stacked_predictions, softmax_attentions)\n        result_prediction = tf.reduce_sum(weighted_predictions, axis=0)\n\n    # If there is no attention, the max prediction is returned\n    else:\n        result_prediction = tf.reduce_max(stacked_predictions, axis=0)\n\n    return result_prediction\n\n\ndef relative_cosine(tf_tensor_1, tf_tensor_2):\n    """"""\n    Returns the cosine of every row in tensor_1 against every row in tensor_2.\n    :param tf_tensor_1:\n    :param tf_tensor_2:\n    :return:\n    """"""\n    normalized_t1 = tf.nn.l2_normalize(tf_tensor_1, 1)\n    normalized_t2 = tf.nn.l2_normalize(tf_tensor_2, 1)\n    return tf.matmul(normalized_t1, normalized_t2, transpose_b=True)\n\n\ndef predict_similar_items(prediction_graph_factory, tf_item_representation, tf_similar_items_ids):\n    """"""\n    Calculates the similarity between the given item ids and all other items using the prediction graph.\n    :param prediction_graph_factory:\n    :param tf_item_representation:\n    :param tf_similar_items_ids:\n    :return:\n    """"""\n    gathered_items = tf.gather(tf_item_representation, tf_similar_items_ids)\n    sims = prediction_graph_factory.connect_dense_prediction_graph(\n        tf_user_representation=gathered_items,\n        tf_item_representation=tf_item_representation\n    )\n    return sims\n'"
tensorrec/representation_graphs.py,15,"b'import abc\nimport tensorflow as tf\n\n\nclass AbstractRepresentationGraph(object):\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def connect_representation_graph(self, tf_features, n_components, n_features, node_name_ending):\n        """"""\n        This method is responsible for connecting the user/item features to their respective latent representations.\n        :param tf_features: tf.SparseTensor\n        The user/item features as a SparseTensor of shape [ n_users, n_features ]\n        :param n_components: int\n        The size of the latent representation per user/item.\n        :param n_features: int\n        The size of the input features per user/item.\n        :param node_name_ending: str\n        A string, either \'user\' or \'item\', which can be added to TensorFlow node names for clarity.\n        :return: tf.Tensor\n        The user/item representation as a Tensor of shape [ n_users, n_components ]\n        """"""\n        pass\n\n\nclass LinearRepresentationGraph(AbstractRepresentationGraph):\n    """"""\n    Calculates the representation by passing the features through a linear embedding.\n    Rough approximation of http://ceur-ws.org/Vol-1448/paper4.pdf\n    """"""\n\n    def connect_representation_graph(self, tf_features, n_components, n_features, node_name_ending):\n\n        # Weights are normalized before building the variable\n        raw_weights = tf.random_normal([n_features, n_components], stddev=1.0)\n        normalized_weights = tf.nn.l2_normalize(raw_weights, 1)\n\n        # Create variable nodes\n        tf_linear_weights = tf.Variable(normalized_weights, name=\'linear_weights_{}\'.format(node_name_ending))\n        tf_repr = tf.sparse_tensor_dense_matmul(tf_features, tf_linear_weights)\n\n        # Return repr layer and variables\n        return tf_repr, [tf_linear_weights]\n\n\nclass NormalizedLinearRepresentationGraph(LinearRepresentationGraph):\n    """"""\n    Calculates the representation by passing the features through a linear embedding. Embeddings are L2 normalized,\n    meaning all embeddings have equal magnitude. This can be useful as a user representation in mixture-of-tastes\n    models, preventing one taste from having a much larger magnitude than others and dominating the recommendations.\n    """"""\n\n    def connect_representation_graph(self, tf_features, n_components, n_features, node_name_ending):\n        tf_repr, weights_list = super(NormalizedLinearRepresentationGraph, self).connect_representation_graph(\n            tf_features=tf_features, n_components=n_components, n_features=n_features, node_name_ending=node_name_ending\n        )\n        normalized_repr = tf.nn.l2_normalize(tf_repr, 1)\n        return normalized_repr, weights_list\n\n\nclass FeaturePassThroughRepresentationGraph(AbstractRepresentationGraph):\n    """"""\n    Uses the features as the representation. This representation graph does no transformation to the features.\n    """"""\n\n    def connect_representation_graph(self, tf_features, n_components, n_features, node_name_ending):\n\n        if n_components != n_features:\n            raise ValueError(\'{} requires n_features and n_components to be equal. Either adjust n_components or use a \'\n                             \'different representation graph. n_features = {}, n_components = {}\'.format(\n                                self.__class__.__name__, n_features, n_components\n                             ))\n\n        return tf.sparse_tensor_to_dense(tf_features, validate_indices=False), []\n\n\nclass WeightedFeaturePassThroughRepresentationGraph(FeaturePassThroughRepresentationGraph):\n    """"""\n    Uses the features as the representation. This representation graph learns weights for each feature.\n    """"""\n\n    def connect_representation_graph(self, tf_features, n_components, n_features, node_name_ending):\n        dense_repr, _ = super(WeightedFeaturePassThroughRepresentationGraph, self).connect_representation_graph(\n            tf_features=tf_features, n_components=n_components, n_features=n_features, node_name_ending=node_name_ending\n        )\n\n        weights = tf.ones([1, n_components])\n        weighted_repr = tf.multiply(dense_repr, weights)\n        return weighted_repr, [weights]\n\n\nclass ReLURepresentationGraph(AbstractRepresentationGraph):\n    """"""\n    Calculates the representations by passing the features through a single-layer ReLU neural network.\n    :param relu_size: int or None\n    The number of nodes in the ReLU layer. If None, the layer will be of size 4*n_components.\n    """"""\n\n    def __init__(self, relu_size=None):\n        self.relu_size = relu_size\n\n    def connect_representation_graph(self, tf_features, n_components, n_features, node_name_ending):\n\n        # Infer ReLU layer size if necessary\n        if self.relu_size is None:\n            relu_size = 4 * n_components\n        else:\n            relu_size = self.relu_size\n\n        # Create variable nodes\n        tf_relu_weights = tf.Variable(tf.random_normal([n_features, relu_size], stddev=.5),\n                                      name=\'relu_weights_{}\'.format(node_name_ending))\n        tf_relu_biases = tf.Variable(tf.zeros([1, relu_size]),\n                                     name=\'relu_biases_{}\'.format(node_name_ending))\n        tf_linear_weights = tf.Variable(tf.random_normal([relu_size, n_components], stddev=.5),\n                                        name=\'linear_weights_{}\'.format(node_name_ending))\n\n        # Create ReLU layer\n        tf_relu = tf.nn.relu(tf.add(tf.sparse_tensor_dense_matmul(tf_features, tf_relu_weights),\n                                    tf_relu_biases))\n        tf_repr = tf.matmul(tf_relu, tf_linear_weights)\n\n        # Return repr layer and variables\n        return tf_repr, [tf_relu_weights, tf_linear_weights, tf_relu_biases]\n\n\nclass AbstractKerasRepresentationGraph(AbstractRepresentationGraph):\n    """"""\n    This abstract RepresentationGraph allows you to use Keras layers as a representation function by overriding the\n    create_layers() method.\n    """"""\n    __metaclass__ = abc.ABCMeta\n\n    def connect_representation_graph(self, tf_features, n_components, n_features, node_name_ending):\n        layers = self.create_layers(n_features=n_features, n_components=n_components)\n\n        weights = []\n        last_layer = tf_features\n\n        # Iterate through layers, connecting each one and extracting weights/biases\n        for layer in layers:\n            last_layer = layer(last_layer)\n            if hasattr(layer, \'weights\'):\n                weights.extend(layer.weights)\n\n        return last_layer, weights\n\n    @abc.abstractmethod\n    def create_layers(self, n_features, n_components):\n        """"""\n        Returns a list of Keras layers.\n        :param n_features: int\n        The input size of the first Keras layer.\n        :param n_components: int\n        The output size of the final Keras layer.\n        :return: list\n        A list of Keras layers.\n        """"""\n        pass\n'"
tensorrec/session_management.py,3,"b""import tensorflow as tf\n\n_session = None\n\n\ndef get_session():\n    global _session\n\n    # Build/retrieve the session if it doesn't exist\n    if _session is None:\n        if tf.get_default_session() is not None:\n            _session = tf.get_default_session()\n        else:\n            _session = tf.Session()\n\n    return _session\n\n\ndef set_session(session):\n    global _session\n    _session = session\n"""
tensorrec/tensorrec.py,23,"b'from functools import partial\nfrom itertools import cycle\nimport logging\nimport numpy as np\nimport os\nimport pickle\nfrom scipy import sparse as sp\nimport tensorflow as tf\n\nfrom .errors import (\n    ModelNotBiasedException, ModelNotFitException, ModelWithoutAttentionException, BatchNonSparseInputException,\n    TfVersionException\n)\nfrom .input_utils import create_tensorrec_iterator, get_dimensions_from_tensorrec_dataset\nfrom .loss_graphs import AbstractLossGraph, RMSELossGraph\nfrom .prediction_graphs import AbstractPredictionGraph, DotProductPredictionGraph\nfrom .recommendation_graphs import (\n    project_biases, split_sparse_tensor_indices, bias_prediction_dense, bias_prediction_serial, rank_predictions,\n    densify_sampled_item_predictions, collapse_mixture_of_tastes, predict_similar_items\n)\nfrom .representation_graphs import AbstractRepresentationGraph, LinearRepresentationGraph\nfrom .session_management import get_session\nfrom .util import sample_items, calculate_batched_alpha, datasets_from_raw_input\n\n\nclass TensorRec(object):\n\n    def __init__(self,\n                 n_components=100,\n                 n_tastes=1,\n                 user_repr_graph=LinearRepresentationGraph(),\n                 item_repr_graph=LinearRepresentationGraph(),\n                 attention_graph=None,\n                 prediction_graph=DotProductPredictionGraph(),\n                 loss_graph=RMSELossGraph(),\n                 biased=True,):\n        """"""\n        A TensorRec recommendation model.\n        :param n_components: Integer\n        The dimension of a single output of the representation function. Must be >= 1.\n        :param n_tastes: Integer\n        The number of tastes/reprs to be calculated for each user. Must be >= 1.\n        :param user_repr_graph: AbstractRepresentationGraph\n        An object which inherits AbstractRepresentationGraph that contains a method to calculate user representations.\n        See tensorrec.representation_graphs for examples.\n        :param item_repr_graph: AbstractRepresentationGraph\n        An object which inherits AbstractRepresentationGraph that contains a method to calculate item representations.\n        See tensorrec.representation_graphs for examples.\n        :param attention_graph: AbstractRepresentationGraph or None\n        Optional. An object which inherits AbstractRepresentationGraph that contains a method to calculate user\n        attention. Any valid repr_graph is also a valid attention graph. If None, no attention process will be applied.\n        :param prediction_graph: AbstractPredictionGraph\n        An object which inherits AbstractPredictionGraph that contains a method to calculate predictions from a pair of\n        user/item reprs.\n        See tensorrec.prediction_graphs for examples.\n        :param loss_graph: AbstractLossGraph\n        An object which inherits AbstractLossGraph that contains a method to calculate the loss function.\n        See tensorrec.loss_graphs for examples.\n        :param biased: bool\n        If True, a bias value will be calculated for every user feature and item feature.\n        """"""\n\n        # Check TensorFlow version\n        major, minor, patch = tf.__version__.split(""."")\n        if int(major) < 1 or int(major) == 1 and int(minor) < 7:\n            raise TfVersionException(tf_version=tf.__version__)\n\n        # Arg Check\n        if (n_components is None) or (n_tastes is None) or (user_repr_graph is None) or (item_repr_graph is None) \\\n                or (prediction_graph is None) or (loss_graph is None):\n            raise ValueError(""All arguments to TensorRec() must be non-None"")\n        if n_components < 1:\n            raise ValueError(""n_components must be >= 1"")\n        if n_tastes < 1:\n            raise ValueError(""n_tastes must be >= 1"")\n        if not isinstance(user_repr_graph, AbstractRepresentationGraph):\n            raise ValueError(""user_repr_graph must inherit AbstractRepresentationGraph"")\n        if not isinstance(item_repr_graph, AbstractRepresentationGraph):\n            raise ValueError(""item_repr_graph must inherit AbstractRepresentationGraph"")\n        if not isinstance(prediction_graph, AbstractPredictionGraph):\n            raise ValueError(""prediction_graph must inherit AbstractPredictionGraph"")\n        if not isinstance(loss_graph, AbstractLossGraph):\n            raise ValueError(""loss_graph must inherit AbstractLossGraph"")\n        if attention_graph is not None:\n            if not isinstance(attention_graph, AbstractRepresentationGraph):\n                raise ValueError(""attention_graph must be None or inherit AbstractRepresentationGraph"")\n            if n_tastes == 1:\n                raise ValueError(""attention_graph must be None if n_tastes == 1"")\n\n        self.n_components = n_components\n        self.n_tastes = n_tastes\n        self.user_repr_graph_factory = user_repr_graph\n        self.item_repr_graph_factory = item_repr_graph\n        self.attention_graph_factory = attention_graph\n        self.prediction_graph_factory = prediction_graph\n        self.loss_graph_factory = loss_graph\n        self.biased = biased\n\n        # A list of the attr names of every graph hook attr\n        self.graph_tensor_hook_attr_names = [\n\n            # Top-level API nodes\n            \'tf_user_representation\', \'tf_item_representation\', \'tf_prediction_serial\', \'tf_prediction\', \'tf_rankings\',\n            \'tf_predict_similar_items\', \'tf_rank_similar_items\',\n\n            # Training nodes\n            \'tf_basic_loss\', \'tf_weight_reg_loss\', \'tf_loss\',\n\n            # Feed placeholders\n            \'tf_learning_rate\', \'tf_alpha\', \'tf_sample_indices\', \'tf_n_sampled_items\', \'tf_similar_items_ids\',\n        ]\n        if self.biased:\n            self.graph_tensor_hook_attr_names += [\'tf_projected_user_biases\', \'tf_projected_item_biases\']\n        if self.attention_graph_factory is not None:\n            self.graph_tensor_hook_attr_names += [\'tf_user_attention_representation\']\n\n        self.graph_operation_hook_attr_names = [\n            # AdamOptimizer\n            \'tf_optimizer\',\n        ]\n        self.graph_iterator_hook_attr_names = [\n            # Input data iterators\n            \'tf_user_feature_iterator\', \'tf_item_feature_iterator\', \'tf_interaction_iterator\',\n        ]\n\n        # Calling the break routine during __init__ creates all the attrs on the TensorRec object with an initial value\n        # of None\n        self._break_graph_hooks()\n\n        # A map of every graph hook attr name to the node name after construction\n        # Tensors and operations are stored separated because they are handled differently by TensorFlow\n        self.graph_tensor_hook_node_names = {}\n        self.graph_operation_hook_node_names = {}\n        self.graph_iterator_hook_node_names = {}\n\n    def _break_graph_hooks(self):\n        for graph_tensor_hook_attr_name in self.graph_tensor_hook_attr_names:\n            self.__setattr__(graph_tensor_hook_attr_name, None)\n        for graph_operation_hook_attr_name in self.graph_operation_hook_attr_names:\n            self.__setattr__(graph_operation_hook_attr_name, None)\n        for graph_iterator_hook_attr_name in self.graph_iterator_hook_attr_names:\n            self.__setattr__(graph_iterator_hook_attr_name, None)\n\n    def _attach_graph_hooks(self):\n        session = get_session()\n\n        for graph_tensor_hook_attr_name in self.graph_tensor_hook_attr_names:\n            graph_tensor_hook_node_name = self.graph_tensor_hook_node_names[graph_tensor_hook_attr_name]\n            node = session.graph.get_tensor_by_name(name=graph_tensor_hook_node_name)\n            self.__setattr__(graph_tensor_hook_attr_name, node)\n\n        for graph_operation_hook_attr_name in self.graph_operation_hook_attr_names:\n            graph_operation_hook_node_name = self.graph_operation_hook_node_names[graph_operation_hook_attr_name]\n            node = session.graph.get_operation_by_name(name=graph_operation_hook_node_name)\n            self.__setattr__(graph_operation_hook_attr_name, node)\n\n        for graph_iterator_hook_attr_name in self.graph_iterator_hook_attr_names:\n            iterator_resource_name, output_types, output_shapes, output_classes = \\\n                self.graph_iterator_hook_node_names[graph_iterator_hook_attr_name]\n            iterator_resource = session.graph.get_tensor_by_name(name=iterator_resource_name)\n            iterator = tf.data.Iterator(iterator_resource, None, output_types, output_shapes, output_classes)\n            self.__setattr__(graph_iterator_hook_attr_name, iterator)\n\n    def _record_graph_hook_names(self):\n\n        # Record serializable node names/info for each graph hook\n        for graph_tensor_hook_attr_name in self.graph_tensor_hook_attr_names:\n            hook = self.__getattribute__(graph_tensor_hook_attr_name)\n            self.graph_tensor_hook_node_names[graph_tensor_hook_attr_name] = hook.name\n\n        for graph_operation_hook_attr_name in self.graph_operation_hook_attr_names:\n            hook = self.__getattribute__(graph_operation_hook_attr_name)\n            self.graph_operation_hook_node_names[graph_operation_hook_attr_name] = hook.name\n\n        for graph_iterator_hook_attr_name in self.graph_iterator_hook_attr_names:\n            hook = self.__getattribute__(graph_iterator_hook_attr_name)\n            iterator_resource_name = hook._iterator_resource.name\n            output_types = hook.output_types\n            output_shapes = hook.output_shapes\n            output_classes = hook.output_classes\n            self.graph_iterator_hook_node_names[graph_iterator_hook_attr_name] = (\n                iterator_resource_name, output_types, output_shapes, output_classes\n            )\n\n    def _create_batched_dataset_initializers(self, interactions, user_features, item_features, user_batch_size=None):\n\n        if user_batch_size is not None:\n\n            # Raise exception if interactions and user_features aren\'t sparse matrices\n            if (not sp.issparse(interactions)) or (not sp.issparse(user_features)):\n                raise BatchNonSparseInputException()\n\n            # Coerce to CSR for fast batching\n            if not isinstance(interactions, sp.csr_matrix):\n                interactions = sp.csr_matrix(interactions)\n            if not isinstance(user_features, sp.csr_matrix):\n                user_features = sp.csr_matrix(user_features)\n\n            n_users = user_features.shape[0]\n\n            interactions_batched = []\n            user_features_batched = []\n\n            start_batch = 0\n            while start_batch < n_users:\n\n                # min() ensures that the batch bounds doesn\'t go past the end of the index\n                end_batch = min(start_batch + user_batch_size, n_users)\n\n                interactions_batched.append(interactions[start_batch:end_batch])\n                user_features_batched.append(user_features[start_batch:end_batch])\n\n                start_batch = end_batch\n\n            # Overwrite the input with the new, batched input\n            interactions = interactions_batched\n            user_features = user_features_batched\n\n        # TODO this is hand-wavy and begging for a cleaner refactor\n        (int_ds, uf_ds, if_ds), (int_init, uf_init, if_init) = self._create_datasets_and_initializers(\n            interactions=interactions, user_features=user_features, item_features=item_features\n        )\n\n        # Ensure that lengths make sense\n        if len(int_init) != len(uf_init):\n            raise ValueError(\'Number of batches in user_features and interactions must be equal.\')\n        if (len(if_init) > 1) and (len(if_init) != len(uf_init)):\n            raise ValueError(\'Number of batches in item_features must be 1 or equal to the number of batches in \'\n                             \'user_features.\')\n\n        # Cycle item features when zipping because there should only be one\n        datasets = [ds_set for ds_set in zip(int_ds, uf_ds, cycle(if_ds))]\n        initializers = [init_set for init_set in zip(int_init, uf_init, cycle(if_init))]\n\n        return datasets, initializers\n\n    def _create_datasets_and_initializers(self, interactions=None, user_features=None, item_features=None):\n\n        datasets = []\n        initializers = []\n\n        if interactions is not None:\n            interactions_datasets = datasets_from_raw_input(raw_input=interactions)\n            interactions_initializers = [self.tf_interaction_iterator.make_initializer(dataset)\n                                         for dataset in interactions_datasets]\n            datasets.append(interactions_datasets)\n            initializers.append(interactions_initializers)\n\n        if user_features is not None:\n            user_features_datasets = datasets_from_raw_input(raw_input=user_features)\n            user_features_initializers = [self.tf_user_feature_iterator.make_initializer(dataset)\n                                          for dataset in user_features_datasets]\n            datasets.append(user_features_datasets)\n            initializers.append(user_features_initializers)\n\n        if item_features is not None:\n            item_features_datasets = datasets_from_raw_input(raw_input=item_features)\n            item_features_initializers = [self.tf_item_feature_iterator.make_initializer(dataset)\n                                          for dataset in item_features_datasets]\n            datasets.append(item_features_datasets)\n            initializers.append(item_features_initializers)\n\n        return datasets, initializers\n\n    def _build_input_iterators(self):\n        self.tf_user_feature_iterator = create_tensorrec_iterator(name=\'tf_user_feature_iterator\')\n        self.tf_item_feature_iterator = create_tensorrec_iterator(name=\'tf_item_feature_iterator\')\n        self.tf_interaction_iterator = create_tensorrec_iterator(name=\'tf_interaction_iterator\')\n\n    def _build_tf_graph(self, n_user_features, n_item_features):\n\n        # Build placeholders\n        self.tf_n_sampled_items = tf.placeholder(\'int64\')\n        self.tf_similar_items_ids = tf.placeholder(\'int64\', [None])\n        self.tf_learning_rate = tf.placeholder(\'float\', None)\n        self.tf_alpha = tf.placeholder(\'float\', None)\n\n        tf_user_feature_rows, tf_user_feature_cols, tf_user_feature_values, tf_n_users, _ = \\\n            self.tf_user_feature_iterator.get_next()\n        tf_item_feature_rows, tf_item_feature_cols, tf_item_feature_values, tf_n_items, _ = \\\n            self.tf_item_feature_iterator.get_next()\n        tf_interaction_rows, tf_interaction_cols, tf_interaction_values, _, _ = \\\n            self.tf_interaction_iterator.get_next()\n\n        tf_user_feature_indices = tf.stack([tf_user_feature_rows, tf_user_feature_cols], axis=1)\n        tf_item_feature_indices = tf.stack([tf_item_feature_rows, tf_item_feature_cols], axis=1)\n        tf_interaction_indices = tf.stack([tf_interaction_rows, tf_interaction_cols], axis=1)\n\n        # Construct the features and interactions as sparse matrices\n        tf_user_features = tf.SparseTensor(tf_user_feature_indices, tf_user_feature_values,\n                                           [tf_n_users, n_user_features])\n        tf_item_features = tf.SparseTensor(tf_item_feature_indices, tf_item_feature_values,\n                                           [tf_n_items, n_item_features])\n        tf_interactions = tf.SparseTensor(tf_interaction_indices, tf_interaction_values,\n                                          [tf_n_users, tf_n_items])\n\n        # Construct the sampling py_func\n        sample_items_partial = partial(sample_items, replace=self.loss_graph_factory.is_sampled_with_replacement)\n        self.tf_sample_indices = tf.py_func(func=sample_items_partial,\n                                            inp=[tf_n_items, tf_n_users, self.tf_n_sampled_items],\n                                            Tout=tf.int64)\n        self.tf_sample_indices.set_shape([None, None])\n\n        # Collect the weights for regularization\n        tf_weights = []\n\n        # Build the item representations\n        self.tf_item_representation, item_weights = \\\n            self.item_repr_graph_factory.connect_representation_graph(tf_features=tf_item_features,\n                                                                      n_components=self.n_components,\n                                                                      n_features=n_item_features,\n                                                                      node_name_ending=\'item\')\n        tf_weights.extend(item_weights)\n\n        tf_x_user, tf_x_item = split_sparse_tensor_indices(tf_sparse_tensor=tf_interactions, n_dimensions=2)\n        tf_transposed_sample_indices = tf.transpose(self.tf_sample_indices)\n        tf_x_user_sample = tf_transposed_sample_indices[0]\n        tf_x_item_sample = tf_transposed_sample_indices[1]\n\n        # These lists will hold the reprs and predictions for each taste\n        tastes_tf_user_representations = []\n        tastes_tf_predictions = []\n        tastes_tf_prediction_serials = []\n        tastes_tf_sample_prediction_serials = []\n\n        # If this model does not use attention, Nones are used as sentinels in place of the attentions\n        if self.attention_graph_factory is not None:\n            tastes_tf_attentions = []\n            tastes_tf_attention_serials = []\n            tastes_tf_sample_attention_serials = []\n            tastes_tf_attention_representations = []\n        else:\n            tastes_tf_attentions = None\n            tastes_tf_attention_serials = None\n            tastes_tf_sample_attention_serials = None\n            tastes_tf_attention_representations = None\n\n        # Build n_tastes user representations and predictions\n        for taste in range(self.n_tastes):\n            tf_user_representation, user_weights = \\\n                self.user_repr_graph_factory.connect_representation_graph(tf_features=tf_user_features,\n                                                                          n_components=self.n_components,\n                                                                          n_features=n_user_features,\n                                                                          node_name_ending=\'user_{}\'.format(taste))\n            tastes_tf_user_representations.append(tf_user_representation)\n            tf_weights.extend(user_weights)\n\n            # Connect attention, if applicable\n            if self.attention_graph_factory is not None:\n                tf_attention_representation, attention_weights = \\\n                    self.attention_graph_factory.connect_representation_graph(tf_features=tf_user_features,\n                                                                              n_components=self.n_components,\n                                                                              n_features=n_user_features,\n                                                                              node_name_ending=\'attn_{}\'.format(taste))\n                tf_weights.extend(attention_weights)\n\n                tf_attention = self.prediction_graph_factory.connect_dense_prediction_graph(\n                    tf_user_representation=tf_attention_representation,\n                    tf_item_representation=self.tf_item_representation\n                )\n                tf_attention_serial = self.prediction_graph_factory.connect_serial_prediction_graph(\n                    tf_user_representation=tf_attention_representation,\n                    tf_item_representation=self.tf_item_representation,\n                    tf_x_user=tf_x_user,\n                    tf_x_item=tf_x_item,\n                )\n                tf_sample_attention_serial = self.prediction_graph_factory.connect_serial_prediction_graph(\n                    tf_user_representation=tf_user_representation,\n                    tf_item_representation=self.tf_item_representation,\n                    tf_x_user=tf_x_user_sample,\n                    tf_x_item=tf_x_item_sample,\n                )\n\n                tastes_tf_attentions.append(tf_attention)\n                tastes_tf_attention_serials.append(tf_attention_serial)\n                tastes_tf_sample_attention_serials.append(tf_sample_attention_serial)\n                tastes_tf_attention_representations.append(tf_attention_representation)\n\n            # Connect the configurable prediction graphs for each taste\n            tf_prediction = self.prediction_graph_factory.connect_dense_prediction_graph(\n                tf_user_representation=tf_user_representation,\n                tf_item_representation=self.tf_item_representation\n            )\n            tf_prediction_serial = self.prediction_graph_factory.connect_serial_prediction_graph(\n                tf_user_representation=tf_user_representation,\n                tf_item_representation=self.tf_item_representation,\n                tf_x_user=tf_x_user,\n                tf_x_item=tf_x_item,\n            )\n            tf_sample_predictions_serial = self.prediction_graph_factory.connect_serial_prediction_graph(\n                tf_user_representation=tf_user_representation,\n                tf_item_representation=self.tf_item_representation,\n                tf_x_user=tf_x_user_sample,\n                tf_x_item=tf_x_item_sample,\n            )\n\n            # Append to tastes\n            tastes_tf_predictions.append(tf_prediction)\n            tastes_tf_prediction_serials.append(tf_prediction_serial)\n            tastes_tf_sample_prediction_serials.append(tf_sample_predictions_serial)\n\n        # If attention is in the graph, build the API node\n        if self.attention_graph_factory is not None:\n            self.tf_user_attention_representation = tf.stack(tastes_tf_attention_representations)\n\n        self.tf_user_representation = tf.stack(tastes_tf_user_representations)\n        self.tf_prediction = collapse_mixture_of_tastes(\n            tastes_predictions=tastes_tf_predictions,\n            tastes_attentions=tastes_tf_attentions\n        )\n        self.tf_prediction_serial = collapse_mixture_of_tastes(\n            tastes_predictions=tastes_tf_prediction_serials,\n            tastes_attentions=tastes_tf_attention_serials\n        )\n        tf_sample_predictions_serial = collapse_mixture_of_tastes(\n            tastes_predictions=tastes_tf_sample_prediction_serials,\n            tastes_attentions=tastes_tf_sample_attention_serials\n        )\n\n        # Add biases, if this is a biased estimator\n        if self.biased:\n            tf_user_feature_biases, self.tf_projected_user_biases = project_biases(\n                tf_features=tf_user_features, n_features=n_user_features\n            )\n            tf_item_feature_biases, self.tf_projected_item_biases = project_biases(\n                tf_features=tf_item_features, n_features=n_item_features\n            )\n\n            tf_weights.append(tf_user_feature_biases)\n            tf_weights.append(tf_item_feature_biases)\n\n            self.tf_prediction = bias_prediction_dense(\n                tf_prediction=self.tf_prediction,\n                tf_projected_user_biases=self.tf_projected_user_biases,\n                tf_projected_item_biases=self.tf_projected_item_biases)\n\n            self.tf_prediction_serial = bias_prediction_serial(\n                tf_prediction_serial=self.tf_prediction_serial,\n                tf_projected_user_biases=self.tf_projected_user_biases,\n                tf_projected_item_biases=self.tf_projected_item_biases,\n                tf_x_user=tf_x_user,\n                tf_x_item=tf_x_item)\n\n            tf_sample_predictions_serial = bias_prediction_serial(\n                tf_prediction_serial=tf_sample_predictions_serial,\n                tf_projected_user_biases=self.tf_projected_user_biases,\n                tf_projected_item_biases=self.tf_projected_item_biases,\n                tf_x_user=tf_x_user_sample,\n                tf_x_item=tf_x_item_sample)\n\n        tf_interactions_serial = tf_interactions.values\n\n        # Construct API nodes\n        self.tf_rankings = rank_predictions(tf_prediction=self.tf_prediction)\n        self.tf_predict_similar_items = predict_similar_items(prediction_graph_factory=self.prediction_graph_factory,\n                                                              tf_item_representation=self.tf_item_representation,\n                                                              tf_similar_items_ids=self.tf_similar_items_ids)\n        self.tf_rank_similar_items = rank_predictions(tf_prediction=self.tf_predict_similar_items)\n\n        # Compose loss function args\n        # This composition is for execution safety: it prevents loss functions that are incorrectly configured from\n        # having visibility of certain nodes.\n        loss_graph_kwargs = {\n            \'tf_prediction_serial\': self.tf_prediction_serial,\n            \'tf_interactions_serial\': tf_interactions_serial,\n            \'tf_interactions\': tf_interactions,\n            \'tf_n_users\': tf_n_users,\n            \'tf_n_items\': tf_n_items,\n        }\n        if self.loss_graph_factory.is_dense:\n            loss_graph_kwargs.update({\n                \'tf_prediction\': self.tf_prediction,\n                \'tf_rankings\': self.tf_rankings,\n            })\n        if self.loss_graph_factory.is_sample_based:\n            tf_sample_predictions = densify_sampled_item_predictions(\n                tf_sample_predictions_serial=tf_sample_predictions_serial,\n                tf_n_sampled_items=self.tf_n_sampled_items,\n                tf_n_users=tf_n_users,\n            )\n            loss_graph_kwargs.update({\'tf_sample_predictions\': tf_sample_predictions,\n                                      \'tf_n_sampled_items\': self.tf_n_sampled_items})\n\n        # Build loss graph\n        self.tf_basic_loss = self.loss_graph_factory.connect_loss_graph(**loss_graph_kwargs)\n\n        self.tf_weight_reg_loss = sum(tf.nn.l2_loss(weights) for weights in tf_weights)\n        self.tf_loss = self.tf_basic_loss + (self.tf_alpha * self.tf_weight_reg_loss)\n        self.tf_optimizer = tf.train.AdamOptimizer(learning_rate=self.tf_learning_rate).minimize(self.tf_loss)\n\n        # Record the new node names\n        self._record_graph_hook_names()\n\n    def fit(self, interactions, user_features, item_features, epochs=100, learning_rate=0.1, alpha=0.00001,\n            verbose=False, user_batch_size=None, n_sampled_items=None):\n        """"""\n        Constructs the TensorRec graph and fits the model.\n        :param interactions: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of interactions of shape [n_users, n_items].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param user_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of user features of shape [n_users, n_user_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param item_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of item features of shape [n_items, n_item_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param epochs: Integer\n        The number of epochs to fit the model.\n        :param learning_rate: Float\n        The learning rate of the model.\n        :param alpha:\n        The weight regularization loss coefficient.\n        :param verbose: boolean\n        If true, the model will print a number of status statements during fitting.\n        :param user_batch_size: int or None\n        The maximum number of users per batch, or None for all users.\n        :param n_sampled_items: int or None\n        The number of items to sample per user for use in loss functions. Must be non-None if\n        self.loss_graph_factory.is_sample_based is True.\n        """"""\n\n        # Pass-through to fit_partial\n        self.fit_partial(interactions=interactions,\n                         user_features=user_features,\n                         item_features=item_features,\n                         epochs=epochs,\n                         learning_rate=learning_rate,\n                         alpha=alpha,\n                         verbose=verbose,\n                         user_batch_size=user_batch_size,\n                         n_sampled_items=n_sampled_items)\n\n    def fit_partial(self, interactions, user_features, item_features, epochs=1, learning_rate=0.1,\n                    alpha=0.00001, verbose=False, user_batch_size=None, n_sampled_items=None):\n        """"""\n        Constructs the TensorRec graph and fits the model.\n        :param interactions: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of interactions of shape [n_users, n_items].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param user_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of user features of shape [n_users, n_user_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param item_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of item features of shape [n_items, n_item_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param epochs: Integer\n        The number of epochs to fit the model.\n        :param learning_rate: Float\n        The learning rate of the model.\n        :param alpha:\n        The weight regularization loss coefficient.\n        :param verbose: boolean\n        If true, the model will print a number of status statements during fitting.\n        :param user_batch_size: int or None\n        The maximum number of users per batch, or None for all users.\n        :param n_sampled_items: int or None\n        The number of items to sample per user for use in loss functions. Must be non-None if\n        self.loss_graph_factory.is_sample_based is True.\n        """"""\n\n        session = get_session()\n\n        # Arg checking\n        if self.loss_graph_factory.is_sample_based:\n            if (n_sampled_items is None) or (n_sampled_items <= 0):\n                raise ValueError(""n_sampled_items must be an integer >0"")\n        if (n_sampled_items is not None) and (not self.loss_graph_factory.is_sample_based):\n            logging.warning(\'n_sampled_items was specified, but the loss graph is not sample-based\')\n\n        # Check if the iterators have been constructed. If not, build them.\n        if self.tf_interaction_iterator is None:\n            self._build_input_iterators()\n\n        if verbose:\n            logging.info(\'Processing interaction and feature data\')\n\n        dataset_sets, initializer_sets = self._create_batched_dataset_initializers(interactions=interactions,\n                                                                                   user_features=user_features,\n                                                                                   item_features=item_features,\n                                                                                   user_batch_size=user_batch_size)\n\n        # Check if the graph has been constructed by checking the dense prediction node\n        # If it hasn\'t been constructed, initialize it\n        if self.tf_prediction is None:\n\n            # Check input dimensions\n            first_batch = dataset_sets[0]\n            _, n_user_features = get_dimensions_from_tensorrec_dataset(first_batch[1])\n            _, n_item_features = get_dimensions_from_tensorrec_dataset(first_batch[2])\n\n            # Numbers of features are either learned at fit time from the shape of these two matrices or specified at\n            # TensorRec construction and cannot be changed.\n            self._build_tf_graph(n_user_features=n_user_features, n_item_features=n_item_features)\n            session.run(tf.global_variables_initializer())\n\n        # Build the shared feed dict\n        feed_dict = {self.tf_learning_rate: learning_rate,\n                     self.tf_alpha: calculate_batched_alpha(num_batches=len(initializer_sets), alpha=alpha)}\n        if self.loss_graph_factory.is_sample_based:\n            feed_dict[self.tf_n_sampled_items] = n_sampled_items\n\n        if verbose:\n            logging.info(\'Beginning fitting\')\n\n        for epoch in range(epochs):\n            for batch, initializers in enumerate(initializer_sets):\n\n                session.run(initializers)\n                if not verbose:\n                    session.run(self.tf_optimizer, feed_dict=feed_dict)\n\n                else:\n                    _, loss, serial_predictions, wr_loss = session.run(\n                        [self.tf_optimizer, self.tf_basic_loss, self.tf_prediction_serial, self.tf_weight_reg_loss],\n                        feed_dict=feed_dict\n                    )\n                    mean_loss = np.mean(loss)\n                    mean_pred = np.mean(serial_predictions)\n                    weight_reg_l2_loss = alpha * wr_loss\n                    logging.info(\'EPOCH {} BATCH {} loss = {}, weight_reg_l2_loss = {}, mean_pred = {}\'.format(\n                        epoch, batch, mean_loss, weight_reg_l2_loss, mean_pred\n                    ))\n\n    def predict(self, user_features, item_features):\n        """"""\n        Predict recommendation scores for the given users and items.\n        :param user_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of user features of shape [n_users, n_user_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param item_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of item features of shape [n_items, n_item_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :return: np.ndarray\n        The predictions in an ndarray of shape [n_users, n_items]\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'predict\')\n\n        _, initializers = self._create_datasets_and_initializers(interactions=None,\n                                                                 user_features=user_features,\n                                                                 item_features=item_features)\n        get_session().run(initializers)\n\n        predictions = self.tf_prediction.eval(session=get_session())\n\n        return predictions\n\n    def predict_similar_items(self, item_features, item_ids, n_similar):\n        """"""\n        Predicts the most similar items to the given item_ids.\n        :param item_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of item features of shape [n_items, n_item_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param item_ids: list or np.array\n        The ids of the items of interest.\n        E.g. [4, 8, 12] to get sims for items 4, 8, and 12.\n        :param n_similar: int\n        The number of similar items to get per item of interest.\n        :return: list of lists of tuples\n        The first level list corresponds to input arg item_ids.\n        The second level list is of length n_similar and contains tuples of (item_id, score) for each similar item.\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'predict_similar_items\')\n\n        _, initializers = self._create_datasets_and_initializers(interactions=None,\n                                                                 user_features=None,\n                                                                 item_features=item_features)\n        get_session().run(initializers)\n\n        feed_dict = {self.tf_similar_items_ids: np.array(item_ids)}\n        sims = self.tf_predict_similar_items.eval(session=get_session(), feed_dict=feed_dict)\n\n        results = []\n        for i in range(len(item_ids)):\n            item_sims = sims[i]\n            best = np.argpartition(item_sims, -n_similar)[-n_similar:]\n            item_results = sorted(zip(best, item_sims[best]), key=lambda x: -x[1])\n            results.append(item_results)\n\n        return results\n\n    def predict_rank(self, user_features, item_features):\n        """"""\n        Predict recommendation ranks for the given users and items.\n        :param user_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of user features of shape [n_users, n_user_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :param item_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of item features of shape [n_items, n_item_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :return: np.ndarray\n        The ranks in an ndarray of shape [n_users, n_items]\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'predict_rank\')\n\n        _, initializers = self._create_datasets_and_initializers(interactions=None,\n                                                                 user_features=user_features,\n                                                                 item_features=item_features)\n        get_session().run(initializers)\n\n        rankings = self.tf_rankings.eval(session=get_session())\n\n        return rankings\n\n    def predict_user_representation(self, user_features):\n        """"""\n        Predict latent representation vectors for the given users.\n        :param user_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of user features of shape [n_users, n_user_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :return: np.ndarray\n        The latent user representations in an ndarray of shape [n_users, n_components]\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'predict_user_representation\')\n\n        _, initializers = self._create_datasets_and_initializers(interactions=None,\n                                                                 user_features=user_features,\n                                                                 item_features=None)\n        get_session().run(initializers)\n\n        user_repr = self.tf_user_representation.eval(session=get_session())\n\n        # If there is only one user repr per user, collapse from rank 3 to rank 2\n        if self.n_tastes == 1:\n            user_repr = np.sum(user_repr, axis=0)\n\n        return user_repr\n\n    def predict_user_attention_representation(self, user_features):\n        """"""\n        Predict latent attention representation vectors for the given users.\n        :param user_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of user features of shape [n_users, n_user_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :return: np.ndarray\n        The latent user attention representations in an ndarray of shape [n_users, n_components]\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'predict_user_attention_representation\')\n\n        if self.attention_graph_factory is None:\n            raise ModelWithoutAttentionException()\n\n        _, initializers = self._create_datasets_and_initializers(interactions=None,\n                                                                 user_features=user_features,\n                                                                 item_features=None)\n        get_session().run(initializers)\n        user_attn_repr = self.tf_user_attention_representation.eval(session=get_session())\n\n        # If there is only one user attn repr per user, collapse from rank 3 to rank 2\n        if self.n_tastes == 1:\n            user_attn_repr = np.sum(user_attn_repr, axis=0)\n\n        return user_attn_repr\n\n    def predict_item_representation(self, item_features):\n        """"""\n        Predict representation vectors for the given items.\n        :param item_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of item features of shape [n_items, n_item_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :return: np.ndarray\n        The latent item representations in an ndarray of shape [n_items, n_components]\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'predict_item_representation\')\n\n        _, initializers = self._create_datasets_and_initializers(interactions=None,\n                                                                 user_features=None,\n                                                                 item_features=item_features)\n        get_session().run(initializers)\n        item_repr = self.tf_item_representation.eval(session=get_session())\n        return item_repr\n\n    def predict_user_bias(self, user_features):\n        """"""\n        Predict bias values for the given users.\n        :param user_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of user features of shape [n_users, n_user_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :return: np.ndarray\n        The user biases in an ndarray of shape [n_users]\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'predict_user_bias\')\n\n        if not self.biased:\n            raise ModelNotBiasedException(actor=\'user\')\n\n        _, initializers = self._create_datasets_and_initializers(interactions=None,\n                                                                 user_features=user_features,\n                                                                 item_features=None)\n        get_session().run(initializers)\n        predictions = self.tf_projected_user_biases.eval(session=get_session())\n        return predictions\n\n    def predict_item_bias(self, item_features):\n        """"""\n        Predict bias values for the given items.\n        :param item_features: scipy.sparse matrix, tensorflow.data.Dataset, str, or list\n        A matrix of item features of shape [n_items, n_item_features].\n        If a Dataset, the Dataset must follow the format used in tensorrec.input_utils.\n        If a str, the string must be the path to a TFRecord file.\n        If a list, the list must contain scipy.sparse matrices, tensorflow.data.Datasets, or strs.\n        :return: np.ndarray\n        The item biases in an ndarray of shape [n_items]\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'predict_item_bias\')\n\n        if not self.biased:\n            raise ModelNotBiasedException(actor=\'item\')\n\n        _, initializers = self._create_datasets_and_initializers(interactions=None,\n                                                                 user_features=None,\n                                                                 item_features=item_features)\n        get_session().run(initializers)\n        predictions = self.tf_projected_item_biases.eval(session=get_session())\n        return predictions\n\n    def save_model(self, directory_path):\n        """"""\n        Saves the model to files in the given directory.\n        :param directory_path: str\n        The path to the directory in which to save the model.\n        :return:\n        """"""\n\n        # Ensure that the model has been fit\n        if self.tf_prediction is None:\n            raise ModelNotFitException(method=\'save_model\')\n\n        if not os.path.exists(directory_path):\n            os.makedirs(directory_path)\n\n        saver = tf.train.Saver()\n        session_path = os.path.join(directory_path, \'tensorrec_session.cpkt\')\n        saver.save(sess=get_session(), save_path=session_path)\n\n        # Break connections to the graph before saving the python object\n        self._break_graph_hooks()\n        tensorrec_path = os.path.join(directory_path, \'tensorrec.pkl\')\n        with open(tensorrec_path, \'wb\') as file:\n            pickle.dump(file=file, obj=self)\n\n        # Reconnect to the graph after saving\n        self._attach_graph_hooks()\n\n    @classmethod\n    def load_model(cls, directory_path):\n        """"""\n        Loads the TensorRec model and TensorFlow session saved in the given directory.\n        :param directory_path: str\n        The path to the directory containing the saved model.\n        :return:\n        """"""\n\n        graph_path = os.path.join(directory_path, \'tensorrec_session.cpkt.meta\')\n        saver = tf.train.import_meta_graph(graph_path)\n\n        session_path = os.path.join(directory_path, \'tensorrec_session.cpkt\')\n        saver.restore(sess=get_session(), save_path=session_path)\n\n        tensorrec_path = os.path.join(directory_path, \'tensorrec.pkl\')\n        with open(tensorrec_path, \'rb\') as file:\n            model = pickle.load(file=file)\n        model._attach_graph_hooks()\n        return model\n'"
tensorrec/util.py,3,"b'import math\nimport numpy as np\nimport random\nimport scipy.sparse as sp\nimport six\nimport tensorflow as tf\nfrom weakref import WeakKeyDictionary\n\nfrom .input_utils import create_tensorrec_dataset_from_sparse_matrix, create_tensorrec_dataset_from_tfrecord\n\n\ndef sample_items(n_items, n_users, n_sampled_items, replace):\n    items_per_user = [np.random.choice(a=n_items, size=n_sampled_items, replace=replace)\n                      for _ in range(n_users)]\n\n    sample_indices = []\n    for user, users_items in enumerate(items_per_user):\n        for item in users_items:\n            sample_indices.append((user, item))\n\n    return np.array(sample_indices, np.int64)\n\n\ndef calculate_batched_alpha(num_batches, alpha):\n    if num_batches < 1:\n        raise ValueError(\'num_batches must be >=1, num_batches={}\'.format(num_batches))\n    elif num_batches > 1:\n        batched_alpha = alpha / (math.e * math.log(num_batches))\n    else:\n        batched_alpha = alpha\n    return batched_alpha\n\n\ndef datasets_from_raw_input(raw_input):\n\n    if isinstance(raw_input, tf.data.Dataset):\n        return [raw_input]\n\n    if sp.issparse(raw_input):\n        return [create_tensorrec_dataset_from_sparse_matrix(raw_input)]\n\n    if isinstance(raw_input, six.string_types):\n        return [create_tensorrec_dataset_from_tfrecord(raw_input)]\n\n    if isinstance(raw_input, list):\n\n        if all([isinstance(input_val, tf.data.Dataset) for input_val in raw_input]):\n            return raw_input\n\n        if all([sp.issparse(input_val) for input_val in raw_input]):\n            return [create_tensorrec_dataset_from_sparse_matrix(input_sparse_matrix)\n                    for input_sparse_matrix in raw_input]\n\n        if all([isinstance(input_val, six.string_types) for input_val in raw_input]):\n            return [create_tensorrec_dataset_from_tfrecord(input_str) for input_str in raw_input]\n\n    raise ValueError(\'Input must be a scipy sparse matrix, an iterable of scipy sprase matrices, or a TensorFlow \'\n                     \'Dataset\')\n\n\ndef generate_dummy_data(num_users=15000, num_items=30000, interaction_density=.00045, num_user_features=200,\n                        num_item_features=200, n_features_per_user=20, n_features_per_item=20,  pos_int_ratio=.5,\n                        return_datasets=False):\n\n    if pos_int_ratio <= 0.0:\n        raise Exception(""pos_int_ratio must be > 0"")\n\n    print(""Generating positive interactions"")\n    interactions = sp.rand(num_users, num_items, density=interaction_density * pos_int_ratio)\n    if pos_int_ratio < 1.0:\n        print(""Generating negative interactions"")\n        interactions += -1 * sp.rand(num_users, num_items, density=interaction_density * (1 - pos_int_ratio))\n\n    print(""Generating user features"")\n    user_features = sp.rand(num_users, num_user_features, density=float(n_features_per_user) / num_user_features)\n\n    print(""Generating item features"")\n    item_features = sp.rand(num_items, num_item_features, density=float(n_features_per_item) / num_item_features)\n\n    if return_datasets:\n        interactions = create_tensorrec_dataset_from_sparse_matrix(interactions)\n        user_features = create_tensorrec_dataset_from_sparse_matrix(user_features)\n        item_features = create_tensorrec_dataset_from_sparse_matrix(item_features)\n\n    return interactions, user_features, item_features\n\n\ndef generate_dummy_data_with_indicator(num_users=15000, num_items=30000, interaction_density=.00045, pos_int_ratio=.5):\n\n    n_user_features = int(num_users * 1.2)\n    n_user_tags = num_users * 3\n    n_item_features = int(num_items * 1.2)\n    n_item_tags = num_items * 3\n    n_interactions = (num_users * num_items) * interaction_density\n\n    user_features = sp.lil_matrix((num_users, n_user_features))\n    for i in range(num_users):\n        user_features[i, i] = 1\n\n    for i in range(n_user_tags):\n        user_features[random.randrange(num_users), random.randrange(num_users, n_user_features)] = 1\n\n    item_features = sp.lil_matrix((num_items, n_item_features))\n    for i in range(num_items):\n        item_features[i, i] = 1\n\n    for i in range(n_item_tags):\n        item_features[random.randrange(num_items), random.randrange(num_items, n_item_features)] = 1\n\n    interactions = sp.lil_matrix((num_users, num_items))\n    for i in range(int(n_interactions * pos_int_ratio)):\n        interactions[random.randrange(num_users), random.randrange(num_items)] = 1\n\n    for i in range(int(n_interactions * (1 - pos_int_ratio))):\n        interactions[random.randrange(num_users), random.randrange(num_items)] = -1\n\n    return interactions, user_features, item_features\n\n\ndef append_to_string_at_point(string, value, point):\n    for _ in range(0, (point - len(string))):\n        string += "" ""\n    string += ""{}"".format(value)\n    return string\n\n\ndef simple_tf_print(tensor, places=100):\n    return tf.Print(tensor, [tensor, tf.shape(tensor)], summarize=places)\n\n\nclass lazyval(object):\n    """"""\n    Decorator that marks that an attribute of an instance should not be\n    computed until needed, and that the value should be memoized.\n\n    Lifted from quantopian/trading_calendars\n    """"""\n    def __init__(self, get):\n        self._get = get\n        self._cache = WeakKeyDictionary()\n\n    def __get__(self, instance, owner):\n        if instance is None:\n            return self\n        try:\n            return self._cache[instance]\n        except KeyError:\n            self._cache[instance] = val = self._get(instance)\n            return val\n\n    def __set__(self, instance, value):\n        raise AttributeError(""Can\'t set read-only attribute."")\n\n    def __delitem__(self, instance):\n        del self._cache[instance]\n'"
test/__init__.py,0,"b""# Suppress TensorFlow logging when testing\nimport logging\nlogging.getLogger('tensorflow').disabled = True\n"""
test/datasets.py,0,"b'from io import BytesIO\nimport os\nimport requests\nfrom zipfile import ZipFile\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom sklearn.base import TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom lightfm import datasets\n\n\nclass TupleExtractor(TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n\n    def transform(self, X):\n        result = np.array([row[self.key] for row in X])\n        return result\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n\nclass IndicatorFeature(TransformerMixin):\n\n    def transform(self, X):\n        return sp.identity(n=len(X))\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n\n# TODO This class is a hack to work with scikit 0.19.1. Remove this after scikit 0.20 is stable released.\nclass PipelineLabelBinarizer(TransformerMixin):\n\n    def __init__(self, *args, **kwargs):\n        self.encoder = LabelBinarizer(*args, **kwargs)\n\n    def fit(self, x, y=None):\n        self.encoder.fit(x)\n        return self\n\n    def transform(self, x, y=None):\n        return self.encoder.transform(x)\n\n\ndef _download_and_unpack_zip(url, local_path, skip_if_not_empty):\n\n    os.makedirs(local_path, exist_ok=True)\n    files_in_dir = os.listdir(local_path)\n    files_in_dir = [file for file in files_in_dir if file[0] != \'.\']  # Remove hidden files\n\n    if skip_if_not_empty and len(files_in_dir) > 0:\n        local_paths = {filename: os.path.join(local_path, filename) for filename in files_in_dir}\n        return local_paths\n\n    request = requests.get(url)\n    zip_file = ZipFile(BytesIO(request.content))\n    files = zip_file.namelist()\n\n    local_paths = {}\n    for filename in files:\n        contents = zip_file.read(filename)\n        write_path = os.path.join(local_path, filename)\n        with open(write_path, \'wb+\') as local_file:\n            local_file.write(contents)\n        local_paths[filename] = write_path\n    return local_paths\n\n\ndef _split_interactions_warm_start(interactions):\n\n    mask_size = len(interactions.data)\n    mask = np.random.choice(a=[False, True], size=mask_size, p=[.2, .8])\n    not_mask = np.invert(mask)\n\n    train_interactions = sp.coo_matrix((interactions.data[mask],\n                                        (interactions.row[mask],\n                                         interactions.col[mask])),\n                                       shape=interactions.shape)\n    test_interactions = sp.coo_matrix((interactions.data[not_mask],\n                                       (interactions.row[not_mask],\n                                        interactions.col[not_mask])),\n                                      shape=interactions.shape)\n\n    return train_interactions, test_interactions\n\n\ndef _split_interactions_cold_start_users(interactions):\n\n    mask_size = interactions.shape[0]\n    sample_rows = np.random.choice(a=mask_size, size=int(.8 * mask_size), replace=False)\n\n    train_mask = np.array([True if row in sample_rows else False for row in interactions.row])\n    test_mask = np.invert(train_mask)\n\n    train_interactions = sp.coo_matrix((interactions.data[train_mask],\n                                        (interactions.row[train_mask],\n                                         interactions.col[train_mask])),\n                                       shape=interactions.shape)\n    test_interactions = sp.coo_matrix((interactions.data[test_mask],\n                                       (interactions.row[test_mask],\n                                        interactions.col[test_mask])),\n                                      shape=interactions.shape)\n\n    return train_interactions, test_interactions\n\n\ndef _split_interactions_cold_start_items(interactions):\n\n    mask_size = interactions.shape[1]\n    sample_cols = np.random.choice(a=mask_size, size=int(.8 * mask_size), replace=False)\n\n    train_mask = np.array([True if col in sample_cols else False for col in interactions.col])\n    test_mask = np.invert(train_mask)\n\n    train_interactions = sp.coo_matrix((interactions.data[train_mask],\n                                        (interactions.row[train_mask],\n                                         interactions.col[train_mask])),\n                                       shape=interactions.shape)\n    test_interactions = sp.coo_matrix((interactions.data[test_mask],\n                                       (interactions.row[test_mask],\n                                        interactions.col[test_mask])),\n                                      shape=interactions.shape)\n\n    return train_interactions, test_interactions\n\n\ndef get_movielens_100k(min_positive_score=4, negative_value=0):\n    movielens_100k_dict = datasets.fetch_movielens(indicator_features=True, genre_features=True)\n\n    def flip_ratings(ratings_matrix):\n        ratings_matrix.data = np.array([1 if rating >= min_positive_score else negative_value\n                                        for rating in ratings_matrix.data])\n        return ratings_matrix\n\n    test_interactions = flip_ratings(movielens_100k_dict[\'test\'])\n    train_interactions = flip_ratings(movielens_100k_dict[\'train\'])\n\n    # Create indicator features for all users\n    num_users = train_interactions.shape[0]\n    user_features = sp.identity(num_users)\n\n    # Movie titles\n    titles = movielens_100k_dict[\'item_labels\']\n\n    return train_interactions, test_interactions, user_features, movielens_100k_dict[\'item_features\'], titles\n\n\ndef get_book_crossing(min_positive_score=7, min_interactions_per_book=5, user_indicators=False, item_indicators=False,\n                      cold_start_users=False, cold_start_items=False):\n    """"""\n    Dataset from http://www2.informatik.uni-freiburg.de/~cziegler/BX/\n\n    Improving Recommendation Lists Through Topic Diversification,\n    Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, Georg Lausen; Proceedings of the 14th International World\n    Wide Web Conference (WWW \'05), May 10-14, 2005, Chiba, Japan. To appear.\n\n    :param min_positive_score:\n    :return:\n    """"""\n\n    if cold_start_items and cold_start_users:\n        raise ValueError(""get_book_crossing() can\'t return both cold_start_users and cold_start_items. Set one to ""\n                         ""False."")\n\n    paths = _download_and_unpack_zip(url=\'http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip\',\n                                     local_path=\'/tmp/tensorrec/book-crossing\',\n                                     skip_if_not_empty=True)\n\n    ratings = []\n    with open(paths[\'BX-Book-Ratings.csv\'], \'rb\') as ratings_file:\n        ratings_lines = ratings_file.readlines()\n        for line in ratings_lines:\n\n            split_line = line.decode(\'iso-8859-1\').replace(\'\\r\', \'\').replace(\'\\n\', \'\').split(\'"";""\')\n            split_line = [snip.replace(\'""\', \'\') for snip in split_line]\n\n            # Throw out header\n            if split_line[0] == \'User-ID\':\n                continue\n\n            user_id = int(split_line[0])\n            isin = split_line[1]\n            rating = int(split_line[2])\n            rating = 1 if rating >= min_positive_score else 0\n\n            # Skip 0 scores\n            if rating == 0:\n                continue\n\n            ratings.append((user_id, isin, rating))\n\n    book_metadata_raw = {}\n    with open(paths[\'BX-Books.csv\'], \'rb\') as books_file:\n        books_lines = books_file.readlines()\n        for line in books_lines:\n\n            split_line = line.decode(\'iso-8859-1\').replace(\'\\r\', \'\').replace(\'\\n\', \'\').split(\'"";""\')\n            split_line = [snip.replace(\'""\', \'\') for snip in split_line]\n\n            # Throw out header\n            if split_line[0] == \'ISBN\':\n                continue\n\n            # Store tuple of title, author, year, publisher, cover image URL\n            isbn = split_line[0]\n            title = split_line[1]\n            author = split_line[2]\n            year = split_line[3]\n            publisher = split_line[4]\n            cover_url = split_line[5]\n\n            # Convert year back to string for encoding\n            book_metadata_raw[isbn] = (title, author, year, publisher, cover_url)\n\n    user_metadata_raw = {}\n    with open(paths[\'BX-Users.csv\'], \'rb\') as users_file:\n        users_lines = users_file.readlines()\n        for line in users_lines:\n\n            split_line = line.decode(\'iso-8859-1\').replace(\'\\r\', \'\').replace(\'\\n\', \'\').replace(\'NULL\', \'""NULL""\')\\\n                .split(\'"";""\')\n            split_line = [snip.replace(\'""\', \'\') for snip in split_line]\n\n            # Throw out header\n            if split_line[0] == \'User-ID\':\n                continue\n\n            # Store tuple of location, age\n            try:\n                user_id = int(split_line[0])\n            except ValueError:\n                continue  # Pass broken rows\n\n            location = split_line[1]\n\n            try:\n                age = split_line[2]\n            except IndexError:\n                continue  # Pass broken rows\n\n            # Convert age back to string for encoding\n            user_metadata_raw[user_id] = (location, age)\n\n    isbn_counter = {}\n    for _, isbn, _ in ratings:\n        isbn_counter[isbn] = isbn_counter.get(isbn, 0) + 1\n    ratings = [r for r in ratings if isbn_counter[r[1]] >= min_interactions_per_book]\n\n    # Build map between users, ISBNs, and rows/columns\n    user_to_row_map = {}\n    isbn_to_column_map = {}\n    for user, isbn, _ in ratings:\n        if user not in user_to_row_map:\n            user_to_row_map[user] = len(user_to_row_map)\n        if isbn not in isbn_to_column_map:\n            isbn_to_column_map[isbn] = len(isbn_to_column_map)\n    row_to_user_map = {row: user for user, row in user_to_row_map.items()}\n    column_to_isbn_map = {col: isbn for isbn, col in isbn_to_column_map.items()}\n\n    interactions_raw = [(user_to_row_map[user], isbn_to_column_map[isbn], score) for user, isbn, score in ratings]\n    r, c, v = zip(*interactions_raw)\n    interactions = sp.coo_matrix((v, (r, c)), dtype=np.float64)\n\n    user_metadata = []\n    for row in range(max(row_to_user_map) + 1):\n        user = row_to_user_map[row]\n        user_metadata.append(user_metadata_raw[user])\n\n    book_metadata = []\n    for col in range(max(column_to_isbn_map) + 1):\n        isbn = column_to_isbn_map[col]\n        try:\n            book_metadata.append(book_metadata_raw[isbn])\n        except KeyError:\n            book_metadata.append((\'\', \'\', \'\', \'\', \'\'))\n\n    book_transformers = [\n        (\'title_pipeline\', Pipeline([\n            (\'title_extractor\', TupleExtractor(0)),\n            (\'title_vectorizer\', CountVectorizer(min_df=2)),\n        ])),\n        (\'author_pipeline\', Pipeline([\n            (\'author_extractor\', TupleExtractor(1)),\n            (\'author_vectorizer\', PipelineLabelBinarizer(sparse_output=True)),\n        ])),\n        (\'year_pipeline\', Pipeline([\n            (\'year_extractor\', TupleExtractor(2)),\n            (\'year_vectorizer\', PipelineLabelBinarizer(sparse_output=True)),\n        ])),\n        (\'publisher_pipeline\', Pipeline([\n            (\'publisher_extractor\', TupleExtractor(3)),\n            (\'publisher_vectorizer\', PipelineLabelBinarizer(sparse_output=True)),\n        ])),\n    ]\n    if item_indicators:\n        book_transformers.append((\'indicator\', IndicatorFeature()))\n    book_pipeline = FeatureUnion(book_transformers)\n    book_features = book_pipeline.fit_transform(book_metadata)\n    book_titles = [book[0] for book in book_metadata]\n\n    user_transformers = [\n        (\'location_pipeline\', Pipeline([\n            (\'location_extractor\', TupleExtractor(0)),\n            (\'location_vectorizer\', CountVectorizer(min_df=2)),\n        ])),\n        (\'age_pipeline\', Pipeline([\n            (\'age_extractor\', TupleExtractor(1)),\n            (\'age_vectorizer\', PipelineLabelBinarizer(sparse_output=True)),\n        ])),\n    ]\n    if user_indicators:\n        user_transformers.append((\'indicator\', IndicatorFeature()))\n    user_pipeline = FeatureUnion(user_transformers)\n    user_features = user_pipeline.fit_transform(user_metadata)\n\n    if cold_start_users:\n        train_interactions, test_interactions = _split_interactions_cold_start_users(interactions)\n    elif cold_start_items:\n        train_interactions, test_interactions = _split_interactions_cold_start_items(interactions)\n    else:\n        train_interactions, test_interactions = _split_interactions_warm_start(interactions)\n\n    return train_interactions, test_interactions, user_features, book_features, book_titles\n'"
test/test_eval.py,0,"b'from unittest import TestCase\nfrom numpy.random import shuffle\nimport numpy as np\nimport scipy.sparse as sps\n\nfrom tensorrec import TensorRec\nfrom tensorrec.eval import recall_at_k, precision_at_k, f1_score_at_k, ndcg_at_k\nfrom tensorrec.eval import _setup_ndcg, _idcg, _dcg\nfrom tensorrec.util import generate_dummy_data_with_indicator\n\n\nclass EvalTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data_with_indicator(\n            num_users=10, num_items=12, interaction_density=.5)\n        model = TensorRec(n_components=10)\n        model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n        cls.model = model\n        cls.ranks = model.predict_rank(user_features=cls.user_features, item_features=cls.item_features)\n\n    def test_recall_at_k(self):\n        # Check non-none results with and without preserve_rows\n        self.assertIsNotNone(recall_at_k(predicted_ranks=self.ranks,\n                                         test_interactions=self.interactions,\n                                         k=5,\n                                         preserve_rows=False))\n        self.assertIsNotNone(recall_at_k(predicted_ranks=self.ranks,\n                                         test_interactions=self.interactions,\n                                         k=5,\n                                         preserve_rows=True))\n\n    def test_precision_at_k(self):\n        # Check non-none results with and without preserve_rows\n        self.assertIsNotNone(precision_at_k(predicted_ranks=self.ranks,\n                                            test_interactions=self.interactions,\n                                            k=5,\n                                            preserve_rows=False))\n        self.assertIsNotNone(precision_at_k(predicted_ranks=self.ranks,\n                                            test_interactions=self.interactions,\n                                            k=5,\n                                            preserve_rows=True))\n\n    def test_f1_score_at_k(self):\n        # Check non-none results with and without preserve_rows\n        self.assertIsNotNone(f1_score_at_k(predicted_ranks=self.ranks,\n                                           test_interactions=self.interactions,\n                                           k=5,\n                                           preserve_rows=False))\n        self.assertIsNotNone(f1_score_at_k(predicted_ranks=self.ranks,\n                                           test_interactions=self.interactions,\n                                           k=5,\n                                           preserve_rows=True))\n\n    def test_ndcg_at_k(self):\n\n        ndcg10 = np.mean(ndcg_at_k(\n            predicted_ranks=self.ranks,\n            test_interactions=self.interactions,\n            k=10,\n            preserve_rows=False\n        ))\n\n        ndcg20 = np.mean(ndcg_at_k(\n            predicted_ranks=self.ranks,\n            test_interactions=self.interactions,\n            k=20,\n            preserve_rows=False\n        ))\n\n        ndcg5 = np.mean(ndcg_at_k(\n            predicted_ranks=self.ranks,\n            test_interactions=self.interactions,\n            k=5,\n            preserve_rows=False\n        ))\n\n        self.assertIsNotNone(ndcg10)\n        self.assertGreaterEqual(ndcg10, ndcg5)\n        self.assertEqual(ndcg20, ndcg10)\n        self.assertLess(ndcg20, 1)\n\n    def test_idcg_at_k(self):\n\n        ordered = np.array([3, 3, 3, 2, 2, 2, 1, 0])\n        wiki_idcg = np.array([3, 3, 3, 2, 2, 2, 1, 0])\n\n        # note different than on page calculation, they use linear\n        shuffle(wiki_idcg)\n        exponential_calc = _idcg(wiki_idcg)\n        self.assertAlmostEqual(exponential_calc, 18.77105, 3)\n\n        ordered_calc = _idcg(ordered)\n        self.assertEqual(ordered_calc, exponential_calc)\n\n        binary = np.array([1, 1, 1, 1, 0, 0])\n        # By hand\n        terms = [(2**e-1)/(np.log2(i+2)) for i, e in enumerate(list(binary))]\n        est_idcg = np.sum(terms)\n        hits = binary\n        self.assertEqual(_idcg(hits), est_idcg)\n\n    def test_ndcg_setup(self):\n        wiki_rel = sps.lil_matrix(np.array([3, 2, 3, 0, 1, 2]))\n        wiki_rank = np.array([1, 2, 3, 4, 5, 6])\n\n        # at 10\n        rel, k_mask, ror, ror_at_k = _setup_ndcg(wiki_rank, wiki_rel)\n\n        self.assertIsNotNone(rel)\n        self.assertEqual(len(k_mask), 5)  # all non-zero elements in relevance where rank <= k\n        self.assertEqual(len(k_mask), len(ror_at_k))\n        ror_cnt = sum([v[0] == v[1] for v in zip(ror.data, [1, 2, 3, 5, 6])])\n        self.assertEqual(ror_cnt, len(k_mask))  # check length is k mask and correct index\n\n    def test_dcg(self):\n        wiki_rel = sps.lil_matrix(np.array([3, 3, 1, 0, 2]))\n        wiki_rank = np.array([1, 2, 3, 4, 5])\n        rel, k_mask, ror, ror_at_k = _setup_ndcg(wiki_rank, wiki_rel)\n\n        # by hand...\n        mat = np.array([3, 3, 1, 2])\n        ranks = np.array([1, 2, 3, 5])\n\n        numer_bh = 2**np.multiply(mat, [1, 1, 1, 1]) - 1\n        denom_bh = np.log2(ranks + 1)\n\n        # in function calc\n        numer_func = (2 ** np.multiply(rel.data, k_mask)) - 1\n        denom_func = np.log2(ror_at_k + 1)\n\n        numer_all_eq = sum([v[0] == v[1] for v in zip(numer_bh, numer_func)])\n        self.assertEqual(numer_all_eq, len(mat))\n\n        denom_all_eq = sum([v[0] == v[1] for v in zip(denom_bh, denom_func)])\n        self.assertEqual(denom_all_eq, len(mat))\n\n        by_hand_dcg = np.sum(numer_bh/denom_bh)\n        func_dcg = _dcg(rel, k_mask, ror_at_k, ror)\n        self.assertEqual(by_hand_dcg, func_dcg)\n\n        # functional NDCG should equal example another example\n        # http://people.cs.georgetown.edu/~nazli/classes/ir-Slides/Evaluation-13.pdf\n        # about .96 but since we use Burges 2005 formulation\n        # it will be a little different\n        v = func_dcg/_idcg(np.array([3, 3, 1, 0, 2]))\n        self.assertAlmostEqual(v.item(0), .979762, 3)\n'"
test/test_loss_graphs.py,0,"b'from unittest import TestCase\n\nfrom tensorrec import TensorRec\nfrom tensorrec.loss_graphs import (\n    RMSELossGraph, RMSEDenseLossGraph, WMRBLossGraph, BalancedWMRBLossGraph\n)\nfrom tensorrec.util import generate_dummy_data_with_indicator\n\n\nclass LossGraphsTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data_with_indicator(\n            num_users=10, num_items=12, interaction_density=.5)\n\n    def test_rmse_loss(self):\n        model = TensorRec(loss_graph=RMSELossGraph())\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5)\n\n    def test_rmse_loss_biased(self):\n        model = TensorRec(loss_graph=RMSELossGraph(), biased=True)\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5)\n\n    def test_rmse_dense_loss(self):\n        model = TensorRec(loss_graph=RMSEDenseLossGraph())\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5)\n\n    def test_rmse_dense_loss_biased(self):\n        model = TensorRec(loss_graph=RMSEDenseLossGraph(), biased=True)\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5)\n\n    def test_wmrb_loss(self):\n        model = TensorRec(loss_graph=WMRBLossGraph())\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5, n_sampled_items=10)\n\n    def test_wmrb_loss_biased(self):\n        model = TensorRec(loss_graph=WMRBLossGraph(), biased=True)\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5, n_sampled_items=10)\n\n    def test_balanced_wmrb_loss(self):\n        model = TensorRec(loss_graph=BalancedWMRBLossGraph())\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5, n_sampled_items=10)\n\n    def test_balanced_wmrb_loss_biased(self):\n        model = TensorRec(loss_graph=BalancedWMRBLossGraph(), biased=True)\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5, n_sampled_items=10)\n'"
test/test_movielens.py,0,"b'from unittest import TestCase\n\nfrom tensorrec import TensorRec\n\nfrom test.datasets import get_movielens_100k\n\n\nclass MovieLensTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.movielens_100k = get_movielens_100k()\n\n    def test_movie_lens_fit(self):\n        """"""\n        This test checks whether the movielens getter works and that the resulting data is viable for fitting/testing a\n        TensorRec model.\n        """"""\n        train_interactions, test_interactions, user_features, item_features, _ = self.movielens_100k\n\n        model = TensorRec()\n        model.fit(interactions=train_interactions,\n                  user_features=user_features,\n                  item_features=item_features,\n                  epochs=5)\n        predictions = model.predict(user_features=user_features,\n                                    item_features=item_features)\n\n        self.assertIsNotNone(predictions)\n'"
test/test_prediction_graphs.py,0,"b'import math\nimport numpy as np\nfrom unittest import TestCase\n\nfrom tensorrec import TensorRec\nfrom tensorrec.prediction_graphs import (\n    DotProductPredictionGraph, CosineSimilarityPredictionGraph, EuclideanSimilarityPredictionGraph\n)\nfrom tensorrec.session_management import get_session\nfrom tensorrec.util import generate_dummy_data_with_indicator\n\n\nclass PredictionGraphsTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data_with_indicator(\n            num_users=10, num_items=12, interaction_density=.5)\n\n    def test_dot_product(self):\n        model = TensorRec(prediction_graph=DotProductPredictionGraph())\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5)\n\n    def test_cos_distance(self):\n        model = TensorRec(prediction_graph=CosineSimilarityPredictionGraph())\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=5)\n\n\nclass DotProductTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.session = get_session()\n\n    def test_dense_prediction(self):\n        graph = DotProductPredictionGraph()\n        array_1 = np.array([\n            [1.0, 1.0],\n            [10.0, 10.0],\n            [-1.0, 1.0],\n        ])\n        array_2 = np.array([\n            [1.0, 1.0],\n            [-1.0, -1.0],\n            [-1.0, 1.0],\n        ])\n        result = graph.connect_dense_prediction_graph(tf_user_representation=array_1,\n                                                      tf_item_representation=array_2).eval(session=self.session)\n\n        expected_result = np.array([\n            [2.0, -2.0, 0.0],\n            [20.0, -20.0, 0.0],\n            [0.0, 0.0, 2.0]\n        ])\n        self.assertTrue(np.allclose(result, expected_result))\n\n    def test_serial_prediction(self):\n        graph = DotProductPredictionGraph()\n        array_1 = np.array([\n            [1.0, 1.0],\n            [10.0, 10.0],\n            [-1.0, 1.0],\n        ])\n        array_2 = np.array([\n            [1.0, 1.0],\n            [-1.0, -1.0],\n            [-1.0, 1.0],\n        ])\n\n        x_user = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n        x_item = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n\n        result = graph.connect_serial_prediction_graph(tf_user_representation=array_1,\n                                                       tf_item_representation=array_2,\n                                                       tf_x_user=x_user,\n                                                       tf_x_item=x_item,).eval(session=self.session)\n\n        expected_result = np.array([2.0, -2.0, 0.0,\n                                    20.0, -20.0, 0.0,\n                                    0.0, 0.0, 2.0])\n        self.assertTrue(np.allclose(result, expected_result))\n\n\nclass CosineSimilarityTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.session = get_session()\n\n    def test_dense_prediction(self):\n        graph = CosineSimilarityPredictionGraph()\n        array_1 = np.array([\n            [1.0, 1.0],\n            [10.0, 10.0],\n            [-1.0, 1.0],\n        ])\n        array_2 = np.array([\n            [1.0, 1.0],\n            [-1.0, -1.0],\n            [-1.0, 1.0],\n        ])\n        result = graph.connect_dense_prediction_graph(tf_user_representation=array_1,\n                                                      tf_item_representation=array_2).eval(session=self.session)\n\n        expected_result = np.array([\n            [1.0, -1.0, 0.0],\n            [1.0, -1.0, 0.0],\n            [0.0, 0.0, 1.0]\n        ])\n        self.assertTrue(np.allclose(result, expected_result))\n\n    def test_serial_prediction(self):\n        graph = CosineSimilarityPredictionGraph()\n        array_1 = np.array([\n            [1.0, 1.0],\n            [10.0, 10.0],\n            [-1.0, 1.0],\n        ])\n        array_2 = np.array([\n            [1.0, 1.0],\n            [-1.0, -1.0],\n            [-1.0, 1.0],\n        ])\n\n        x_user = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n        x_item = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n\n        result = graph.connect_serial_prediction_graph(tf_user_representation=array_1,\n                                                       tf_item_representation=array_2,\n                                                       tf_x_user=x_user,\n                                                       tf_x_item=x_item,).eval(session=self.session)\n\n        expected_result = np.array([1.0, -1.0, 0.0,\n                                    1.0, -1.0, 0.0,\n                                    0.0, 0.0, 1.0])\n        self.assertTrue(np.allclose(result, expected_result))\n\n\nclass EuclideanSimilarityTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.session = get_session()\n\n    def test_dense_prediction(self):\n        graph = EuclideanSimilarityPredictionGraph()\n        array_1 = np.array([\n            [1.0, 1.0],\n            [2.0, 2.0],\n            [-1.0, 1.0],\n        ])\n        array_2 = np.array([\n            [1.0, 1.0],\n            [-1.0, -1.0],\n            [-1.0, 1.0],\n        ])\n\n        result = graph.connect_dense_prediction_graph(tf_user_representation=array_1,\n                                                      tf_item_representation=array_2).eval(session=self.session)\n\n        expected_result = np.array([\n            [0.0, -math.sqrt(8.0), -2.0],\n            [-math.sqrt(2.0), -math.sqrt(18.0), -math.sqrt(10.0)],\n            [-2.0, -2.0, 0.0]]\n        )\n        self.assertTrue(np.allclose(result, expected_result))\n\n    def test_serial_prediction(self):\n        graph = EuclideanSimilarityPredictionGraph()\n        array_1 = np.array([\n            [1.0, 1.0],\n            [2.0, 2.0],\n            [-1.0, 1.0],\n        ])\n        array_2 = np.array([\n            [1.0, 1.0],\n            [-1.0, -1.0],\n            [-1.0, 1.0],\n        ])\n\n        x_user = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n        x_item = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n\n        result = graph.connect_serial_prediction_graph(tf_user_representation=array_1,\n                                                       tf_item_representation=array_2,\n                                                       tf_x_user=x_user,\n                                                       tf_x_item=x_item,).eval(session=self.session)\n\n        expected_result = np.array([0.0, -math.sqrt(8.0), -2.0,\n                                    -math.sqrt(2.0), -math.sqrt(18.0), -math.sqrt(10.0),\n                                    -2.0, -2.0, 0.0])\n        self.assertTrue(np.allclose(result, expected_result))\n'"
test/test_readme.py,9,"b'from unittest import TestCase\n\n\nclass ReadmeTestCase(TestCase):\n\n    def test_basic_usage(self):\n        import numpy as np\n        import tensorrec\n\n        # Build the model with default parameters\n        model = tensorrec.TensorRec()\n\n        # Generate some dummy data\n        interactions, user_features, item_features = tensorrec.util.generate_dummy_data(num_users=100,\n                                                                                        num_items=150,\n                                                                                        interaction_density=.05)\n\n        # Fit the model for 5 epochs\n        model.fit(interactions, user_features, item_features, epochs=5, verbose=True)\n\n        # Predict scores and ranks for all users and all items\n        predictions = model.predict(user_features=user_features,\n                                    item_features=item_features)\n        predicted_ranks = model.predict_rank(user_features=user_features,\n                                             item_features=item_features)\n\n        # Calculate and print the recall at 10\n        r_at_k = tensorrec.eval.recall_at_k(predicted_ranks, interactions, k=10)\n        print(np.mean(r_at_k))\n\n        self.assertIsNotNone(predictions)\n\n    def test_custom_repr_graph(self):\n        import tensorflow as tf\n        import tensorrec\n\n        # Define a custom representation function graph\n        class TanhRepresentationGraph(tensorrec.representation_graphs.AbstractRepresentationGraph):\n            def connect_representation_graph(self, tf_features, n_components, n_features, node_name_ending):\n                """"""\n                This representation function embeds the user/item features by passing them through a single tanh layer.\n                :param tf_features: tf.SparseTensor\n                The user/item features as a SparseTensor of dimensions [n_users/items, n_features]\n                :param n_components: int\n                The dimensionality of the resulting representation.\n                :param n_features: int\n                The number of features in tf_features\n                :param node_name_ending: String\n                Either \'user\' or \'item\'\n                :return:\n                A tuple of (tf.Tensor, list) where the first value is the resulting representation in n_components\n                dimensions and the second value is a list containing all tf.Variables which should be subject to\n                regularization.\n                """"""\n                tf_tanh_weights = tf.Variable(tf.random_normal([n_features, n_components], stddev=.5),\n                                              name=\'tanh_weights_%s\' % node_name_ending)\n\n                tf_repr = tf.nn.tanh(tf.sparse_tensor_dense_matmul(tf_features, tf_tanh_weights))\n\n                # Return repr layer and variables\n                return tf_repr, [tf_tanh_weights]\n\n        # Build a model with the custom representation function\n        model = tensorrec.TensorRec(user_repr_graph=TanhRepresentationGraph(),\n                                    item_repr_graph=TanhRepresentationGraph())\n\n        # Generate some dummy data\n        interactions, user_features, item_features = tensorrec.util.generate_dummy_data(num_users=100,\n                                                                                        num_items=150,\n                                                                                        interaction_density=.05)\n\n        # Fit the model for 5 epochs\n        model.fit(interactions, user_features, item_features, epochs=5, verbose=True)\n\n        self.assertIsNotNone(model)\n\n    def test_custom_loss_graph(self):\n        import tensorflow as tf\n        import tensorrec\n\n        # Define a custom loss graph\n        class SimpleLossGraph(tensorrec.loss_graphs.AbstractLossGraph):\n            def connect_loss_graph(self, tf_prediction_serial, tf_interactions_serial, **kwargs):\n                """"""\n                This loss function returns the absolute simple error between the predictions and the interactions.\n                :param tf_prediction_serial: tf.Tensor\n                The recommendation scores as a Tensor of shape [n_samples, 1]\n                :param tf_interactions_serial: tf.Tensor\n                The sample interactions corresponding to tf_prediction_serial as a Tensor of shape [n_samples, 1]\n                :param kwargs:\n                Other TensorFlow nodes.\n                :return:\n                A tf.Tensor containing the learning loss.\n                """"""\n                return tf.reduce_mean(tf.abs(tf_interactions_serial - tf_prediction_serial))\n\n        # Build a model with the custom loss function\n        model = tensorrec.TensorRec(loss_graph=SimpleLossGraph())\n\n        # Generate some dummy data\n        interactions, user_features, item_features = tensorrec.util.generate_dummy_data(num_users=100,\n                                                                                        num_items=150,\n                                                                                        interaction_density=.05)\n\n        # Fit the model for 5 epochs\n        model.fit(interactions, user_features, item_features, epochs=5, verbose=True)\n\n        self.assertIsNotNone(model)\n'"
test/test_recommendation_graphs.py,3,"b'import numpy as np\nimport tensorflow as tf\nimport scipy.sparse as sp\nfrom unittest import TestCase\n\nfrom tensorrec.prediction_graphs import CosineSimilarityPredictionGraph\nfrom tensorrec.recommendation_graphs import (\n    project_biases, split_sparse_tensor_indices, bias_prediction_dense, bias_prediction_serial,\n    densify_sampled_item_predictions, rank_predictions, collapse_mixture_of_tastes, predict_similar_items\n)\nfrom tensorrec.session_management import get_session\n\n\nclass RecommendationGraphsTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.session = get_session()\n\n    def test_project_biases(self):\n        features = sp.coo_matrix([\n            [1.0, 0, 0, 1.0],\n            [0, 1.0, 0, 0],\n            [0, 0, 1.0, 2.0],\n        ], dtype=np.float32)\n        n_features = 4\n\n        sparse_tensor_features = tf.SparseTensor(np.mat([features.row, features.col]).transpose(),\n                                                 features.data,\n                                                 features.shape)\n        tf_feature_biases, tf_projected_biases = project_biases(tf_features=sparse_tensor_features,\n                                                                n_features=n_features)\n\n        self.session.run(tf.global_variables_initializer())\n        assign_op = tf_feature_biases.assign(value=[[-.5], [.5], [0], [2.0]])\n        self.session.run(assign_op)\n\n        result = tf_projected_biases.eval(session=self.session)\n        expected_result = np.array([1.5, .5, 4.0])\n        self.assertTrue((result == expected_result).all())\n\n    def test_split_sparse_tensor_indices(self):\n        interactions = sp.coo_matrix([\n            [1.0, 0, 0, 1.0],\n            [0, 1.0, 0, 0],\n            [0, 0, 1.0, 2.0],\n        ], dtype=np.float32)\n        sparse_tensor_interactions = tf.SparseTensor(np.mat([interactions.row, interactions.col]).transpose(),\n                                                     interactions.data,\n                                                     interactions.shape)\n\n        x_user, x_item = split_sparse_tensor_indices(tf_sparse_tensor=sparse_tensor_interactions, n_dimensions=2)\n\n        x_user = x_user.eval(session=self.session)\n        x_item = x_item.eval(session=self.session)\n\n        expected_user = np.array([0, 0, 1, 2, 2])\n        expected_item = np.array([0, 3, 1, 2, 3])\n\n        self.assertTrue((x_user == expected_user).all())\n        self.assertTrue((x_item == expected_item).all())\n\n    def test_bias_prediction_dense(self):\n        predictions = np.array([\n            [1.0, 0, 0, 1.0],\n            [0, 1.0, 0, 0],\n            [0, 0, 1.0, 2.0],\n        ], dtype=np.float32)\n\n        projected_user_biases = np.array([0.0, 1.0, 2.0])\n        projected_item_biases = np.array([0.0, -1.0, -2.0, -3.0])\n\n        biased_predictions = bias_prediction_dense(\n            tf_prediction=predictions,\n            tf_projected_user_biases=projected_user_biases,\n            tf_projected_item_biases=projected_item_biases\n        ).eval(session=self.session)\n\n        expected_biased_predictions = np.array([\n            [1.0, -1.0, -2.0, -2.0],\n            [1.0, 1.0, -1.0, -2.0],\n            [2.0, 1.0, 1.0, 1.0],\n        ], dtype=np.float32)\n\n        self.assertTrue((biased_predictions == expected_biased_predictions).all())\n\n    def test_bias_prediction_serial(self):\n        predictions = np.array([1.0, 0, 0, 1.0, 0, 1.0, 0, 0, 0, 0, 1.0, 2.0], dtype=np.float32)\n\n        projected_user_biases = np.array([0.0, 1.0, 2.0])\n        projected_item_biases = np.array([0.0, -1.0, -2.0, -3.0])\n\n        x_user = np.array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2])\n        x_item = np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3])\n\n        biased_predictions = bias_prediction_serial(\n            tf_prediction_serial=predictions,\n            tf_projected_user_biases=projected_user_biases,\n            tf_projected_item_biases=projected_item_biases,\n            tf_x_user=x_user,\n            tf_x_item=x_item,\n        ).eval(session=self.session)\n\n        expected_biased_predictions = np.array([1.0, -1.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 1.0, 1.0],\n                                               dtype=np.float32)\n\n        self.assertTrue((biased_predictions == expected_biased_predictions).all())\n\n    def test_densify_sampled_item_predictions(self):\n        input_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n        result = densify_sampled_item_predictions(\n            tf_sample_predictions_serial=input_data,\n            tf_n_sampled_items=4,\n            tf_n_users=3,\n        ).eval(session=self.session)\n\n        expected_result = np.array([\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12]\n        ])\n        self.assertTrue((result == expected_result).all())\n\n    def test_rank_predictions(self):\n        predictions = np.array([\n            [1.0, 2.0, 3.0, 4.0],\n            [4.0, 3.0, 2.0, 1.0],\n            [3.0, 4.0, 1.0, 2.0],\n        ], dtype=np.float32)\n\n        ranked = rank_predictions(predictions).eval(session=self.session)\n\n        expected_ranks = np.array([\n            [4, 3, 2, 1],\n            [1, 2, 3, 4],\n            [2, 1, 4, 3],\n        ], dtype=np.int)\n        self.assertTrue((ranked == expected_ranks).all())\n\n    def test_collapse_mixture_of_tastes(self):\n        predictions = [\n            np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32),\n            np.array([4.0, 3.0, 2.0, 1.0], dtype=np.float32),\n            np.array([3.0, 4.0, 1.0, 2.0], dtype=np.float32),\n        ]\n\n        collapsed_predictions = collapse_mixture_of_tastes(tastes_predictions=predictions,\n                                                           tastes_attentions=None).eval(session=self.session)\n\n        expected_predictions = np.array([[4.0, 4.0, 3.0, 4.0]], dtype=np.float32)\n        self.assertTrue((collapsed_predictions == expected_predictions).all())\n\n    def test_collapse_mixture_of_tastes_with_attention(self):\n        predictions = [\n            np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32),\n            np.array([4.0, 3.0, 2.0, 1.0], dtype=np.float32),\n            np.array([3.0, 4.0, 1.0, 2.0], dtype=np.float32),\n        ]\n        attentions = [\n            np.array([1.0, 2.0, -3.0, 4.0], dtype=np.float32),\n            np.array([0.0, 3.0, 2.0, 1.0], dtype=np.float32),\n            np.array([3.0, 0.0, 1.0, 2.0], dtype=np.float32),\n        ]\n\n        collapsed_predictions = collapse_mixture_of_tastes(tastes_predictions=predictions,\n                                                           tastes_attentions=attentions).eval(session=self.session)\n\n        expected_predictions = np.array([[2.8136194, 2.7756228, 1.7372786, 3.6455793]], dtype=np.float32)\n        self.assertTrue((collapsed_predictions == expected_predictions).all())\n\n    def test_predict_similar_items(self):\n        reprs = np.array([\n            [1.0, 0.0, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0],\n            [0.0, -1.0, 0.0, 0.0],\n        ], dtype=np.float32)\n        sims = predict_similar_items(prediction_graph_factory=CosineSimilarityPredictionGraph(),\n                                     tf_similar_items_ids=[1],\n                                     tf_item_representation=reprs).eval(session=self.session)\n\n        expected_sims = np.array([[0.0,  1.0, -1.0]], dtype=np.float32)\n        self.assertTrue((sims == expected_sims).all())\n'"
test/test_representation_graphs.py,0,"b'from nose_parameterized import parameterized\nfrom unittest import TestCase\n\nfrom tensorrec import TensorRec\nfrom tensorrec.representation_graphs import (\n    LinearRepresentationGraph, NormalizedLinearRepresentationGraph, FeaturePassThroughRepresentationGraph,\n    WeightedFeaturePassThroughRepresentationGraph, ReLURepresentationGraph\n)\nfrom tensorrec.util import generate_dummy_data\n\n\nclass RepresentationGraphTestCase(TestCase):\n\n    @parameterized.expand([\n        [""linear"", LinearRepresentationGraph, LinearRepresentationGraph, 50, 60, 20],\n        [""norm_lin"", NormalizedLinearRepresentationGraph, NormalizedLinearRepresentationGraph, 50, 60, 20],\n        [""fpt_user"", FeaturePassThroughRepresentationGraph, NormalizedLinearRepresentationGraph, 50, 60, 50],\n        [""fpt_item"", NormalizedLinearRepresentationGraph, FeaturePassThroughRepresentationGraph, 50, 60, 60],\n        [""fpt_both"", FeaturePassThroughRepresentationGraph, FeaturePassThroughRepresentationGraph, 50, 50, 50],\n        [""weighted_fpt"", WeightedFeaturePassThroughRepresentationGraph, WeightedFeaturePassThroughRepresentationGraph,\n         50, 50, 50],\n        [""relu"", ReLURepresentationGraph, ReLURepresentationGraph, 50, 60, 20],\n    ])\n    def test_fit(self, name, user_repr, item_repr, n_user_features, n_item_features, n_components):\n        interactions, user_features, item_features = generate_dummy_data(\n            num_users=15, num_items=30, interaction_density=.5, num_user_features=n_user_features,\n            num_item_features=n_item_features, n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n        model = TensorRec(n_components=n_components,\n                          user_repr_graph=user_repr(),\n                          item_repr_graph=item_repr())\n        model.fit(interactions, user_features, item_features, epochs=10)\n\n        # Ensure that the nodes have been built\n        self.assertIsNotNone(model.tf_prediction)\n\n\nclass IdentityRepresentationGraphTestCase(TestCase):\n\n    def test_fit_fail_on_bad_dims(self):\n        interactions, user_features, item_features = generate_dummy_data(\n            num_users=15, num_items=30, interaction_density=.5, num_user_features=30,\n            num_item_features=20, n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n\n        with self.assertRaises(ValueError):\n            model = TensorRec(n_components=25,\n                              user_repr_graph=FeaturePassThroughRepresentationGraph(),\n                              item_repr_graph=LinearRepresentationGraph())\n            model.fit(interactions, user_features, item_features, epochs=10)\n\n        with self.assertRaises(ValueError):\n            model = TensorRec(n_components=25,\n                              user_repr_graph=LinearRepresentationGraph(),\n                              item_repr_graph=FeaturePassThroughRepresentationGraph())\n            model.fit(interactions, user_features, item_features, epochs=10)\n'"
test/test_tensorrec.py,2,"b""import numpy as np\nimport os\nimport shutil\nimport tempfile\nfrom unittest import TestCase\n\nimport tensorflow as tf\n\nfrom tensorrec import TensorRec\nfrom tensorrec.errors import (\n    ModelNotBiasedException, ModelNotFitException, ModelWithoutAttentionException, BatchNonSparseInputException\n)\nfrom tensorrec.input_utils import create_tensorrec_dataset_from_sparse_matrix, write_tfrecord_from_sparse_matrix\nfrom tensorrec.representation_graphs import NormalizedLinearRepresentationGraph, LinearRepresentationGraph\nfrom tensorrec.session_management import set_session\nfrom tensorrec.util import generate_dummy_data\n\n\nclass TensorRecTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.n_user_features = 200\n        cls.n_item_features = 150\n\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data(\n            num_users=15, num_items=30, interaction_density=.5, num_user_features=cls.n_user_features,\n            num_item_features=cls.n_item_features, n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n\n        set_session(None)\n        cls.temp_dir = tempfile.mkdtemp()\n        cls.interactions_path = os.path.join(cls.temp_dir, 'interactions.tfrecord')\n        cls.user_features_path = os.path.join(cls.temp_dir, 'user_features.tfrecord')\n        cls.item_features_path = os.path.join(cls.temp_dir, 'item_features.tfrecord')\n\n        write_tfrecord_from_sparse_matrix(cls.user_features_path, cls.user_features)\n        write_tfrecord_from_sparse_matrix(cls.item_features_path, cls.item_features)\n        write_tfrecord_from_sparse_matrix(cls.interactions_path, cls.interactions)\n\n    @classmethod\n    def tearDownClass(cls):\n        shutil.rmtree(cls.temp_dir)\n\n    def test_init(self):\n        self.assertIsNotNone(TensorRec())\n\n    def test_init_fail_0_components(self):\n        with self.assertRaises(ValueError):\n            TensorRec(n_components=0)\n\n    def test_init_fail_none_factory(self):\n        with self.assertRaises(ValueError):\n            TensorRec(user_repr_graph=None)\n        with self.assertRaises(ValueError):\n            TensorRec(item_repr_graph=None)\n        with self.assertRaises(ValueError):\n            TensorRec(loss_graph=None)\n\n    def test_init_fail_bad_loss_graph(self):\n        with self.assertRaises(ValueError):\n            TensorRec(loss_graph=np.mean)\n\n    def test_init_fail_attention_with_1_taste(self):\n        with self.assertRaises(ValueError):\n            TensorRec(n_tastes=1, attention_graph=LinearRepresentationGraph())\n\n    def test_init_fail_bad_attention_graph(self):\n        with self.assertRaises(ValueError):\n            TensorRec(attention_graph=np.mean)\n\n    def test_predict_fail_unfit(self):\n        model = TensorRec()\n        with self.assertRaises(ModelNotFitException):\n            model.predict(self.user_features, self.item_features)\n        with self.assertRaises(ModelNotFitException):\n            model.predict_rank(self.user_features, self.item_features)\n\n        with self.assertRaises(ModelNotFitException):\n            model.predict_user_representation(self.user_features)\n        with self.assertRaises(ModelNotFitException):\n            model.predict_item_representation(self.item_features)\n        with self.assertRaises(ModelNotFitException):\n            model.predict_user_attention_representation(self.user_features)\n\n        with self.assertRaises(ModelNotFitException):\n            model.predict_similar_items(self.item_features, item_ids=[1], n_similar=5)\n\n        with self.assertRaises(ModelNotFitException):\n            model.predict_item_bias(self.item_features)\n        with self.assertRaises(ModelNotFitException):\n            model.predict_user_bias(self.user_features)\n\n    def test_fit_verbose(self):\n        model = TensorRec(n_components=10)\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=10, verbose=True)\n        # Ensure that the nodes have been built\n        self.assertIsNotNone(model.tf_prediction)\n\n    def test_fit_batched(self):\n        model = TensorRec(n_components=10)\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=10, user_batch_size=2)\n        # Ensure that the nodes have been built\n        self.assertIsNotNone(model.tf_prediction)\n\n    def test_fit_fail_bad_input(self):\n        model = TensorRec(n_components=10)\n        with self.assertRaises(ValueError):\n            model.fit(np.array([1, 2, 3, 4]), self.user_features, self.item_features, epochs=10)\n        with self.assertRaises(ValueError):\n            model.fit(self.interactions, np.array([1, 2, 3, 4]), self.item_features, epochs=10)\n        with self.assertRaises(ValueError):\n            model.fit(self.interactions, self.user_features, np.array([1, 2, 3, 4]), epochs=10)\n\n    def test_fit_fail_mismatched_batches(self):\n        model = TensorRec(n_components=10)\n        with self.assertRaises(ValueError):\n            model.fit(self.interactions,\n                      [self.user_features, self.user_features],\n                      [self.item_features, self.item_features, self.item_features],\n                      epochs=10)\n\n        with self.assertRaises(ValueError):\n            model.fit(self.interactions,\n                      [self.user_features, self.user_features],\n                      [self.item_features, self.item_features],\n                      epochs=10)\n\n        model.fit([self.interactions, self.interactions],\n                  [self.user_features, self.user_features],\n                  self.item_features,\n                  epochs=10)\n\n        model.fit([self.interactions, self.interactions],\n                  [self.user_features, self.user_features],\n                  [self.item_features, self.item_features],\n                  epochs=10)\n\n    def test_fit_fail_batching_dataset(self):\n        model = TensorRec(n_components=10)\n\n        interactions_as_dataset = create_tensorrec_dataset_from_sparse_matrix(self.interactions)\n        with self.assertRaises(BatchNonSparseInputException):\n            model.fit(interactions_as_dataset, self.user_features, self.item_features, epochs=10, user_batch_size=2)\n\n    def test_fit_user_feature_as_dataset(self):\n        uf_as_dataset = create_tensorrec_dataset_from_sparse_matrix(self.user_features)\n        model = TensorRec(n_components=10)\n        model.fit(self.interactions, uf_as_dataset, self.item_features, epochs=10)\n\n    def test_fit_item_feature_as_dataset(self):\n        if_as_dataset = create_tensorrec_dataset_from_sparse_matrix(self.item_features)\n        model = TensorRec(n_components=10)\n        model.fit(self.interactions, self.user_features, if_as_dataset, epochs=10)\n\n    def test_fit_interactions_as_dataset(self):\n        int_as_dataset = create_tensorrec_dataset_from_sparse_matrix(self.interactions)\n        model = TensorRec(n_components=10)\n        model.fit(int_as_dataset, self.user_features, self.item_features, epochs=10)\n\n    def test_fit_from_datasets(self):\n        uf_as_dataset = create_tensorrec_dataset_from_sparse_matrix(self.user_features)\n        if_as_dataset = create_tensorrec_dataset_from_sparse_matrix(self.item_features)\n        int_as_dataset = create_tensorrec_dataset_from_sparse_matrix(self.interactions)\n        model = TensorRec(n_components=10)\n        model.fit(int_as_dataset, uf_as_dataset, if_as_dataset, epochs=10)\n\n    def test_fit_from_tfrecords(self):\n        set_session(None)\n        model = TensorRec(n_components=10)\n        model.fit(self.interactions_path, self.user_features_path, self.item_features_path, epochs=10)\n\n\nclass TensorRecAPITestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.n_users = 15\n        cls.n_items = 30\n\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data(\n            num_users=cls.n_users, num_items=cls.n_items, interaction_density=.5, num_user_features=200,\n            num_item_features=200, n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n\n        cls.standard_model = TensorRec(n_components=10)\n        cls.standard_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n        cls.unbiased_model = TensorRec(n_components=10, biased=False)\n        cls.unbiased_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n    def test_fit(self):\n        # Ensure that the nodes have been built\n        self.assertIsNotNone(self.standard_model.tf_prediction)\n\n    def test_predict(self):\n        predictions = self.standard_model.predict(user_features=self.user_features,\n                                                  item_features=self.item_features)\n\n        self.assertEqual(predictions.shape, (self.n_users, self.n_items))\n\n    def test_predict_rank(self):\n        ranks = self.standard_model.predict_rank(user_features=self.user_features,\n                                                 item_features=self.item_features)\n\n        self.assertEqual(ranks.shape, (self.n_users, self.n_items))\n        for x in range(ranks.shape[0]):\n            for y in range(ranks.shape[1]):\n                val = ranks[x][y]\n                self.assertGreater(val, 0)\n\n    def test_predict_similar_items(self):\n        sims = self.standard_model.predict_similar_items(item_features=self.item_features,\n                                                         item_ids=[6, 12],\n                                                         n_similar=5)\n\n        # Two items, two rows of sims\n        self.assertEqual(len(sims), 2)\n\n        for item_sims in sims:\n            # Should equal n_similar\n            self.assertEqual(len(item_sims), 5)\n\n    def test_fit_predict_unbiased(self):\n        predictions = self.unbiased_model.predict(user_features=self.user_features, item_features=self.item_features)\n        self.assertEqual(predictions.shape, (self.n_users, self.n_items))\n\n    def test_predict_user_repr(self):\n        user_repr = self.unbiased_model.predict_user_representation(self.user_features)\n        self.assertEqual(user_repr.shape, (self.n_users, 10))\n\n    def test_predict_item_repr(self):\n        item_repr = self.unbiased_model.predict_item_representation(self.item_features)\n        self.assertEqual(item_repr.shape, (self.n_items, 10))\n\n    def test_predict_user_bias_unbiased_model(self):\n        self.assertRaises(\n            ModelNotBiasedException,\n            self.unbiased_model.predict_user_bias,\n            self.user_features)\n\n    def test_predict_item_bias_unbiased_model(self):\n        self.assertRaises(\n            ModelNotBiasedException,\n            self.unbiased_model.predict_item_bias,\n            self.item_features)\n\n    def test_predict_user_attn_repr(self):\n        # This test will be overwritten by the tests that have attention\n        with self.assertRaises(ModelWithoutAttentionException):\n            self.unbiased_model.predict_user_attention_representation(self.user_features)\n\n\nclass TensorRecBiasedPrediction(TestCase):\n    # TODO: Collapse these into TensorRecTestCase once the fit bug is fixed\n\n    @classmethod\n    def setUpClass(cls):\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data(\n            num_users=15, num_items=30, interaction_density=.5, num_user_features=200, num_item_features=200,\n            n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n\n        cls.standard_model = TensorRec(n_components=10)\n        cls.standard_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n    def test_predict_user_bias(self):\n        user_bias = self.standard_model.predict_user_bias(self.user_features)\n        self.assertTrue(any(user_bias))  # Make sure it isn't all 0s\n\n    def test_predict_item_bias(self):\n        item_bias = self.standard_model.predict_item_bias(self.item_features)\n        self.assertTrue(any(item_bias))  # Make sure it isn't all 0s\n\n\nclass TensorRecAPINTastesTestCase(TensorRecAPITestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.n_users = 15\n        cls.n_items = 30\n\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data(\n            num_users=cls.n_users, num_items=cls.n_items, interaction_density=.5, num_user_features=200,\n            num_item_features=200, n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n\n        cls.standard_model = TensorRec(n_components=10,\n                                       n_tastes=3,\n                                       user_repr_graph=NormalizedLinearRepresentationGraph())\n        cls.standard_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n        cls.unbiased_model = TensorRec(n_components=10,\n                                       n_tastes=3,\n                                       biased=False,\n                                       user_repr_graph=NormalizedLinearRepresentationGraph())\n        cls.unbiased_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n    def test_predict_user_repr(self):\n        user_repr = self.unbiased_model.predict_user_representation(self.user_features)\n\n        # 3 tastes, shape[0] users, 10 components\n        self.assertEqual(user_repr.shape, (3, self.user_features.shape[0], 10))\n\n\nclass TensorRecAPIAttentionTestCase(TensorRecAPINTastesTestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.n_users = 15\n        cls.n_items = 30\n\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data(\n            num_users=cls.n_users, num_items=cls.n_items, interaction_density=.5, num_user_features=200,\n            num_item_features=200, n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n\n        cls.standard_model = TensorRec(n_components=10,\n                                       n_tastes=3,\n                                       user_repr_graph=NormalizedLinearRepresentationGraph(),\n                                       attention_graph=LinearRepresentationGraph())\n        cls.standard_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n        cls.unbiased_model = TensorRec(n_components=10,\n                                       n_tastes=3,\n                                       biased=False,\n                                       user_repr_graph=NormalizedLinearRepresentationGraph(),\n                                       attention_graph=LinearRepresentationGraph())\n        cls.unbiased_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n    def test_predict_user_attn_repr(self):\n        user_attn_repr = self.unbiased_model.predict_user_attention_representation(self.user_features)\n\n        # attn repr should have shape [n_tastes, n_users, n_components]\n        self.assertEqual(user_attn_repr.shape, (3, self.user_features.shape[0], 10))\n\n\nclass TensorRecAPIDatasetInputTestCase(TensorRecAPITestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        cls.n_users = 15\n        cls.n_items = 30\n\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data(\n            num_users=cls.n_users, num_items=cls.n_items, interaction_density=.5, num_user_features=200,\n            num_item_features=200, n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5,\n            return_datasets=True\n        )\n\n        cls.standard_model = TensorRec(n_components=10)\n        cls.standard_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n        cls.unbiased_model = TensorRec(n_components=10, biased=False)\n        cls.unbiased_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n\nclass TensorRecAPITFRecordInputTestCase(TensorRecAPITestCase):\n\n    @classmethod\n    def setUpClass(cls):\n\n        # Blow away an existing session to avoid 'tf_map_func not found' error\n        set_session(None)\n\n        cls.n_users = 15\n        cls.n_items = 30\n\n        int_ds, uf_ds, if_ds = generate_dummy_data(\n            num_users=cls.n_users, num_items=cls.n_items, interaction_density=.5, num_user_features=200,\n            num_item_features=200, n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n\n        cls.temp_dir = tempfile.mkdtemp()\n        cls.interactions = os.path.join(cls.temp_dir, 'interactions.tfrecord')\n        cls.user_features = os.path.join(cls.temp_dir, 'user_features.tfrecord')\n        cls.item_features = os.path.join(cls.temp_dir, 'item_features.tfrecord')\n\n        write_tfrecord_from_sparse_matrix(cls.interactions, int_ds)\n        write_tfrecord_from_sparse_matrix(cls.user_features, uf_ds)\n        write_tfrecord_from_sparse_matrix(cls.item_features, if_ds)\n\n        cls.standard_model = TensorRec(n_components=10)\n        cls.standard_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n        cls.unbiased_model = TensorRec(n_components=10, biased=False)\n        cls.unbiased_model.fit(cls.interactions, cls.user_features, cls.item_features, epochs=10)\n\n    @classmethod\n    def tearDownClass(cls):\n        shutil.rmtree(cls.temp_dir)\n\n\nclass TensorRecSavingTestCase(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.interactions, cls.user_features, cls.item_features = generate_dummy_data(\n            num_users=15, num_items=30, interaction_density=.5, num_user_features=200, num_item_features=200,\n            n_features_per_user=20, n_features_per_item=20, pos_int_ratio=.5\n        )\n        tf.reset_default_graph()\n        set_session(None)\n\n    def setUp(self):\n        # Create a temporary directory\n        self.test_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        # Remove the directory after the test\n        shutil.rmtree(self.test_dir)\n\n    def test_save_and_load_model(self):\n        model = TensorRec(n_components=10)\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=10)\n\n        predictions = model.predict(user_features=self.user_features, item_features=self.item_features)\n        ranks = model.predict_rank(user_features=self.user_features, item_features=self.item_features)\n        model.save_model(directory_path=self.test_dir)\n\n        # Check that, after saving, the same predictions come back\n        predictions_after_save = model.predict(user_features=self.user_features, item_features=self.item_features)\n        ranks_after_save = model.predict_rank(user_features=self.user_features, item_features=self.item_features)\n        self.assertTrue((predictions == predictions_after_save).all())\n        self.assertTrue((ranks == ranks_after_save).all())\n\n        # Blow away the session\n        set_session(None)\n        tf.reset_default_graph()\n\n        # Reload the model, predict, and check for equal predictions\n        new_model = TensorRec.load_model(directory_path=self.test_dir)\n        new_predictions = new_model.predict(user_features=self.user_features, item_features=self.item_features)\n        new_ranks = new_model.predict_rank(user_features=self.user_features, item_features=self.item_features)\n\n        self.assertTrue((predictions == new_predictions).all())\n        self.assertTrue((ranks == new_ranks).all())\n\n    def test_save_and_load_model_same_session(self):\n        model = TensorRec(n_components=10)\n        model.fit(self.interactions, self.user_features, self.item_features, epochs=10)\n\n        predictions = model.predict(user_features=self.user_features, item_features=self.item_features)\n        ranks = model.predict_rank(user_features=self.user_features, item_features=self.item_features)\n        model.save_model(directory_path=self.test_dir)\n\n        # Reload the model, predict, and check for equal predictions\n        new_model = TensorRec.load_model(directory_path=self.test_dir)\n        new_predictions = new_model.predict(user_features=self.user_features, item_features=self.item_features)\n        new_ranks = new_model.predict_rank(user_features=self.user_features, item_features=self.item_features)\n\n        self.assertTrue((predictions == new_predictions).all())\n        self.assertTrue((ranks == new_ranks).all())\n"""
test/test_util.py,0,"b'from unittest import TestCase\n\nfrom tensorrec.util import calculate_batched_alpha\n\n\nclass UtilTestcase(TestCase):\n\n    def test_calculate_batched_alpha(self):\n\n        # Test no-transformation\n        n_batches = 1\n        alpha = .01\n        self.assertEqual(alpha, calculate_batched_alpha(num_batches=n_batches, alpha=alpha))\n\n        # Test two batches\n        n_batches = 2\n        alpha = .01\n        self.assertAlmostEqual(.53074 * alpha,\n                               calculate_batched_alpha(num_batches=n_batches, alpha=alpha),\n                               places=5)\n\n        # Test a bad number of batches\n        with self.assertRaises(ValueError):\n            calculate_batched_alpha(num_batches=0, alpha=alpha)\n'"
