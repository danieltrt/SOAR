file_path,api_count,code
layers.py,2,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as tcl\n\n\ndef leaky_relu(x, alpha=0.2):\n    return tf.maximum(tf.minimum(0.0, alpha * x), x)\n\n\ndef leaky_relu_batch_norm(x, alpha=0.2):\n    return leaky_relu(tcl.batch_norm(x), alpha)\n\n\ndef relu_batch_norm(x):\n    return tf.nn.relu(tcl.batch_norm(x))\n'"
visualize.py,0,"b""import numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\n\nimport matplotlib.pyplot as plt\n\n\ndef split(x):\n    assert type(x) == int\n    t = int(np.floor(np.sqrt(x)))\n    for a in range(t, 0, -1):\n        if x % a == 0:\n            return a, x / a\n\n\ndef grid_transform(x, size):\n    a, b = split(x.shape[0])\n    h, w, c = size[0], size[1], size[2]\n    a = int(a)\n    b = int(b)\n    x = np.reshape(x, [a, b, h, w, c])\n    x = np.transpose(x, [0, 2, 1, 3, 4])\n    x = np.reshape(x, [a * h, b * w, c])\n    if x.shape[2] == 1:\n        x = np.squeeze(x, axis=2)\n    return x\n\n\ndef grid_show(fig, x, size):\n    ax = fig.add_subplot(111)\n    x = grid_transform(x, size)\n    if len(x.shape) > 2:\n        ax.imshow(x)\n    else:\n        ax.imshow(x, cmap='gray')\n"""
wgan.py,12,"b""import os\nimport time\nimport argparse\nimport importlib\nimport tensorflow as tf\nimport tensorflow.contrib as tc\n\nfrom visualize import *\n\n\nclass WassersteinGAN(object):\n    def __init__(self, g_net, d_net, x_sampler, z_sampler, data, model):\n        self.model = model\n        self.data = data\n        self.g_net = g_net\n        self.d_net = d_net\n        self.x_sampler = x_sampler\n        self.z_sampler = z_sampler\n        self.x_dim = self.d_net.x_dim\n        self.z_dim = self.g_net.z_dim\n        self.x = tf.placeholder(tf.float32, [None, self.x_dim], name='x')\n        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n\n        self.x_ = self.g_net(self.z)\n\n        self.d = self.d_net(self.x, reuse=False)\n        self.d_ = self.d_net(self.x_)\n\n        self.g_loss = tf.reduce_mean(self.d_)\n        self.d_loss = tf.reduce_mean(self.d) - tf.reduce_mean(self.d_)\n\n        self.reg = tc.layers.apply_regularization(\n            tc.layers.l1_regularizer(2.5e-5),\n            weights_list=[var for var in tf.global_variables() if 'weights' in var.name]\n        )\n        self.g_loss_reg = self.g_loss + self.reg\n        self.d_loss_reg = self.d_loss + self.reg\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self.d_rmsprop = tf.train.RMSPropOptimizer(learning_rate=5e-5)\\\n                .minimize(self.d_loss_reg, var_list=self.d_net.vars)\n            self.g_rmsprop = tf.train.RMSPropOptimizer(learning_rate=5e-5)\\\n                .minimize(self.g_loss_reg, var_list=self.g_net.vars)\n\n        self.d_clip = [v.assign(tf.clip_by_value(v, -0.01, 0.01)) for v in self.d_net.vars]\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n\n    def train(self, batch_size=64, num_batches=1000000):\n        plt.ion()\n        self.sess.run(tf.global_variables_initializer())\n        start_time = time.time()\n        for t in range(0, num_batches):\n            d_iters = 5\n            if t % 500 == 0 or t < 25:\n                 d_iters = 100\n\n            for _ in range(0, d_iters):\n                bx = self.x_sampler(batch_size)\n                bz = self.z_sampler(batch_size, self.z_dim)\n                self.sess.run(self.d_clip)\n                self.sess.run(self.d_rmsprop, feed_dict={self.x: bx, self.z: bz})\n\n            bz = self.z_sampler(batch_size, self.z_dim)\n            self.sess.run(self.g_rmsprop, feed_dict={self.z: bz, self.x: bx})\n\n            if t % 100 == 0:\n                bx = self.x_sampler(batch_size)\n                bz = self.z_sampler(batch_size, self.z_dim)\n\n                d_loss = self.sess.run(\n                    self.d_loss, feed_dict={self.x: bx, self.z: bz}\n                )\n                g_loss = self.sess.run(\n                    self.g_loss, feed_dict={self.z: bz, self.x: bx}\n                )\n                print('Iter [%8d] Time [%5.4f] d_loss [%.4f] g_loss [%.4f]' %\n                        (t, time.time() - start_time, d_loss - g_loss, g_loss))\n\n            if t % 100 == 0:\n                bz = self.z_sampler(batch_size, self.z_dim)\n                bx = self.sess.run(self.x_, feed_dict={self.z: bz})\n                bx = xs.data2img(bx)\n                fig = plt.figure(self.data + '.' + self.model)\n                grid_show(fig, bx, xs.shape)\n                fig.savefig('logs/{}/{}.pdf'.format(self.data, t/100))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('')\n    parser.add_argument('--data', type=str, default='mnist')\n    parser.add_argument('--model', type=str, default='dcgan')\n    parser.add_argument('--gpus', type=str, default='0')\n    args = parser.parse_args()\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n    data = importlib.import_module(args.data)\n    model = importlib.import_module(args.data + '.' + args.model)\n    xs = data.DataSampler()\n    zs = data.NoiseSampler()\n    d_net = model.Discriminator()\n    g_net = model.Generator()\n    wgan = WassersteinGAN(g_net, d_net, xs, zs, args.data, args.model)\n    wgan.train()\n"""
wgan_v2.py,14,"b""import os\nimport time\nimport argparse\nimport importlib\nimport tensorflow as tf\nfrom scipy.misc import imsave\n\nfrom visualize import *\n\n\nclass WassersteinGAN(object):\n    def __init__(self, g_net, d_net, x_sampler, z_sampler, data, model, scale=10.0):\n        self.model = model\n        self.data = data\n        self.g_net = g_net\n        self.d_net = d_net\n        self.x_sampler = x_sampler\n        self.z_sampler = z_sampler\n        self.x_dim = self.d_net.x_dim\n        self.z_dim = self.g_net.z_dim\n        self.x = tf.placeholder(tf.float32, [None, self.x_dim], name='x')\n        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n\n        self.x_ = self.g_net(self.z)\n\n        self.d = self.d_net(self.x, reuse=False)\n        self.d_ = self.d_net(self.x_)\n\n        self.g_loss = tf.reduce_mean(self.d_)\n        self.d_loss = tf.reduce_mean(self.d) - tf.reduce_mean(self.d_)\n\n        epsilon = tf.random_uniform([], 0.0, 1.0)\n        x_hat = epsilon * self.x + (1 - epsilon) * self.x_\n        d_hat = self.d_net(x_hat)\n\n        ddx = tf.gradients(d_hat, x_hat)[0]\n        print(ddx.get_shape().as_list())\n        ddx = tf.sqrt(tf.reduce_sum(tf.square(ddx), axis=1))\n        ddx = tf.reduce_mean(tf.square(ddx - 1.0) * scale)\n\n        self.d_loss = self.d_loss + ddx\n\n        self.d_adam, self.g_adam = None, None\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self.d_adam = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9)\\\n                .minimize(self.d_loss, var_list=self.d_net.vars)\n            self.g_adam = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9)\\\n                .minimize(self.g_loss, var_list=self.g_net.vars)\n\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n\n    def train(self, batch_size=64, num_batches=1000000):\n        plt.ion()\n        self.sess.run(tf.global_variables_initializer())\n        start_time = time.time()\n        for t in range(0, num_batches):\n            d_iters = 5\n            #if t % 500 == 0 or t < 25:\n            #     d_iters = 100\n\n            for _ in range(0, d_iters):\n                bx = self.x_sampler(batch_size)\n                bz = self.z_sampler(batch_size, self.z_dim)\n                self.sess.run(self.d_adam, feed_dict={self.x: bx, self.z: bz})\n\n            bz = self.z_sampler(batch_size, self.z_dim)\n            self.sess.run(self.g_adam, feed_dict={self.z: bz, self.x: bx})\n\n            if t % 100 == 0:\n                bx = self.x_sampler(batch_size)\n                bz = self.z_sampler(batch_size, self.z_dim)\n\n                d_loss = self.sess.run(\n                    self.d_loss, feed_dict={self.x: bx, self.z: bz}\n                )\n                g_loss = self.sess.run(\n                    self.g_loss, feed_dict={self.z: bz}\n                )\n                print('Iter [%8d] Time [%5.4f] d_loss [%.4f] g_loss [%.4f]' %\n                        (t, time.time() - start_time, d_loss, g_loss))\n\n            if t % 100 == 0:\n                bz = self.z_sampler(batch_size, self.z_dim)\n                bx = self.sess.run(self.x_, feed_dict={self.z: bz})\n                bx = xs.data2img(bx)\n                #fig = plt.figure(self.data + '.' + self.model)\n                #grid_show(fig, bx, xs.shape)\n                bx = grid_transform(bx, xs.shape)\n                imsave('logs/{}/{}.png'.format(self.data, t/100), bx)\n                #fig.savefig('logs/{}/{}.png'.format(self.data, t/100))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('')\n    parser.add_argument('--data', type=str, default='mnist')\n    parser.add_argument('--model', type=str, default='dcgan')\n    parser.add_argument('--gpus', type=str, default='0')\n    args = parser.parse_args()\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n    data = importlib.import_module(args.data)\n    model = importlib.import_module(args.data + '.' + args.model)\n    xs = data.DataSampler()\n    zs = data.NoiseSampler()\n    d_net = model.Discriminator()\n    g_net = model.Generator()\n    wgan = WassersteinGAN(g_net, d_net, xs, zs, args.data, args.model)\n    wgan.train()\n"""
lsun/__init__.py,0,"b'import os\nimport numpy as np\n\n\nclass DataSampler(object):\n    def __init__(self):\n        self.shape = [64, 64, 3]\n        self.name = ""lsun""\n        self.db_path = ""/media/jiaming/data/data/lsun/bedroom""\n        self.db_files = os.listdir(self.db_path)\n        self.cur_batch_ptr = 0\n        self.cur_batch = self.load_new_data()\n        self.train_batch_ptr = 0\n        self.train_size = len(self.db_files) * 10000\n        self.test_size = self.train_size\n\n    def load_new_data(self):\n        filename = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                self.db_path, self.db_files[self.cur_batch_ptr])\n        self.cur_batch_ptr += 1\n        if self.cur_batch_ptr == len(self.db_files):\n            self.cur_batch_ptr = 0\n        return np.load(filename) * 2.0 - 1.0\n\n    def __call__(self, batch_size):\n        prev_batch_ptr = self.train_batch_ptr\n        self.train_batch_ptr += batch_size\n        if self.train_batch_ptr > self.cur_batch.shape[0]:\n            self.train_batch_ptr = batch_size\n            prev_batch_ptr = 0\n            self.cur_batch = self.load_new_data()\n        x = self.cur_batch[prev_batch_ptr:self.train_batch_ptr, :, :, :]\n        return np.reshape(x, [batch_size, -1])\n\n    def data2img(self, data):\n        rescaled = np.divide(data + 1.0, 2.0)\n        return np.reshape(np.clip(rescaled, 0.0, 1.0), [data.shape[0]] + self.shape)\n\n\nclass NoiseSampler(object):\n    def __call__(self, batch_size, z_dim):\n        return np.random.uniform(-1.0, 1.0, [batch_size, z_dim])'"
lsun/dcgan.py,19,"b""import tensorflow as tf\nimport tensorflow.contrib.layers as tcl\n\nfrom layers import *\n\n\nclass Discriminator(object):\n    def __init__(self):\n        self.x_dim = 64 * 64 * 3\n        self.name = 'lsun/dcgan/d_net'\n\n    def __call__(self, x, reuse=True):\n        with tf.variable_scope(self.name) as vs:\n            if reuse:\n                vs.reuse_variables()\n            bs = tf.shape(x)[0]\n            x = tf.reshape(x, [bs, 64, 64, 3])\n            conv1 = tcl.conv2d(\n                x, 64, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=leaky_relu\n            )\n            conv2 = tcl.conv2d(\n                conv1, 128, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=leaky_relu_batch_norm\n            )\n            conv3 = tcl.conv2d(\n                conv2, 256, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=leaky_relu_batch_norm\n            )\n            conv4 = tcl.conv2d(\n                conv3, 512, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=leaky_relu_batch_norm\n            )\n            conv4 = tcl.flatten(conv4)\n            fc = tcl.fully_connected(conv4, 1, activation_fn=tf.identity)\n            return fc\n\n    @property\n    def vars(self):\n        return [var for var in tf.global_variables() if self.name in var.name]\n\n\nclass Generator(object):\n    def __init__(self):\n        self.z_dim = 100\n        self.x_dim = 64 * 64 * 3\n        self.name = 'lsun/dcgan/g_net'\n\n    def __call__(self, z):\n        with tf.variable_scope(self.name) as vs:\n            bs = tf.shape(z)[0]\n            fc = tcl.fully_connected(z, 4 * 4 * 1024, activation_fn=tf.identity)\n            conv1 = tf.reshape(fc, tf.stack([bs, 4, 4, 1024]))\n            conv1 = relu_batch_norm(conv1)\n            conv2 = tcl.conv2d_transpose(\n                conv1, 512, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=relu_batch_norm\n            )\n            conv3 = tcl.conv2d_transpose(\n                conv2, 256, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=relu_batch_norm\n            )\n            conv4 = tcl.conv2d_transpose(\n                conv3, 128, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=relu_batch_norm\n            )\n            conv5 = tcl.conv2d_transpose(\n                conv4, 3, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=tf.tanh)\n            return conv5\n\n    @property\n    def vars(self):\n        return [var for var in tf.global_variables() if self.name in var.name]"""
mnist/__init__.py,0,"b""import numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('/data/mnist')\n\n\nclass DataSampler(object):\n    def __init__(self):\n        self.shape = [28, 28, 1]\n\n    def __call__(self, batch_size):\n        return mnist.train.next_batch(batch_size)[0]\n\n    def data2img(self, data):\n        return np.reshape(data, [data.shape[0]] + self.shape)\n\n\nclass NoiseSampler(object):\n    def __call__(self, batch_size, z_dim):\n        return np.random.uniform(-1.0, 1.0, [batch_size, z_dim])"""
mnist/dcgan.py,28,"b""import tensorflow as tf\nimport tensorflow.contrib as tc\nimport tensorflow.contrib.layers as tcl\n\n\ndef leaky_relu(x, alpha=0.2):\n    return tf.maximum(tf.minimum(0.0, alpha * x), x)\n\n\nclass Discriminator(object):\n    def __init__(self):\n        self.x_dim = 784\n        self.name = 'mnist/dcgan/d_net'\n\n    def __call__(self, x, reuse=True):\n        with tf.variable_scope(self.name) as vs:\n            if reuse:\n                vs.reuse_variables()\n            bs = tf.shape(x)[0]\n            x = tf.reshape(x, [bs, 28, 28, 1])\n            conv1 = tc.layers.convolution2d(\n                x, 64, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=tf.identity\n            )\n            conv1 = leaky_relu(conv1)\n            conv2 = tc.layers.convolution2d(\n                conv1, 128, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=tf.identity\n            )\n            conv2 = leaky_relu(conv2)\n            conv2 = tcl.flatten(conv2)\n            fc1 = tc.layers.fully_connected(\n                conv2, 1024,\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=tf.identity\n            )\n            fc1 = leaky_relu(fc1)\n            fc2 = tc.layers.fully_connected(fc1, 1, activation_fn=tf.identity)\n            return fc2\n\n    @property\n    def vars(self):\n        return [var for var in tf.global_variables() if self.name in var.name]\n\n\nclass Generator(object):\n    def __init__(self):\n        self.z_dim = 100\n        self.x_dim = 784\n        self.name = 'mnist/dcgan/g_net'\n\n    def __call__(self, z):\n        with tf.variable_scope(self.name) as vs:\n            bs = tf.shape(z)[0]\n            fc1 = tc.layers.fully_connected(\n                z, 1024,\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                weights_regularizer=tc.layers.l2_regularizer(2.5e-5),\n                activation_fn=tf.identity\n            )\n            fc1 = tc.layers.batch_norm(fc1)\n            fc1 = tf.nn.relu(fc1)\n            fc2 = tc.layers.fully_connected(\n                fc1, 7 * 7 * 128,\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                weights_regularizer=tc.layers.l2_regularizer(2.5e-5),\n                activation_fn=tf.identity\n            )\n            fc2 = tf.reshape(fc2, tf.stack([bs, 7, 7, 128]))\n            fc2 = tc.layers.batch_norm(fc2)\n            fc2 = tf.nn.relu(fc2)\n            conv1 = tc.layers.convolution2d_transpose(\n                fc2, 64, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                weights_regularizer=tc.layers.l2_regularizer(2.5e-5),\n                activation_fn=tf.identity\n            )\n            conv1 = tc.layers.batch_norm(conv1)\n            conv1 = tf.nn.relu(conv1)\n            conv2 = tc.layers.convolution2d_transpose(\n                conv1, 1, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                weights_regularizer=tc.layers.l2_regularizer(2.5e-5),\n                activation_fn=tf.sigmoid\n            )\n            conv2 = tf.reshape(conv2, tf.stack([bs, 784]))\n            return conv2\n\n    @property\n    def vars(self):\n        return [var for var in tf.global_variables() if self.name in var.name]"""
mnist/mlp.py,20,"b""import tensorflow as tf\nimport tensorflow.contrib as tc\nimport tensorflow.contrib.layers as tcl\n\ndef leaky_relu(x, alpha=0.1):\n    return tf.maximum(tf.minimum(0.0, alpha * x), x)\n\n\nclass Discriminator(object):\n    def __init__(self):\n        self.x_dim = 784\n        self.name = 'mnist/mlp/d_net'\n\n    def __call__(self, x, reuse=True):\n        with tf.variable_scope(self.name) as vs:\n            if reuse:\n                vs.reuse_variables()\n            bs = tf.shape(x)[0]\n            x = tf.reshape(x, [bs, 28, 28, 1])\n            conv1 = tc.layers.convolution2d(\n                x, 64, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=tf.identity\n            )\n            conv1 = leaky_relu(conv1)\n            conv2 = tc.layers.convolution2d(\n                conv1, 128, [4, 4], [2, 2],\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=tf.identity\n            )\n            conv2 = leaky_relu(tc.layers.batch_norm(conv2))\n            conv2 = tcl.flatten(conv2)\n            fc1 = tc.layers.fully_connected(\n                conv2, 1024,\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                activation_fn=tf.identity\n            )\n            fc1 = leaky_relu(tc.layers.batch_norm(fc1))\n            fc2 = tc.layers.fully_connected(fc1, 1, activation_fn=tf.identity)\n            return fc2\n\n    @property\n    def vars(self):\n        return [var for var in tf.global_variables() if self.name in var.name]\n\n    def loss(self, prediction, target):\n        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(prediction, target))\n\n\nclass Generator(object):\n    def __init__(self):\n        self.z_dim = 100\n        self.x_dim = 784\n        self.name = 'mnist/mlp/g_net'\n\n    def __call__(self, z):\n        with tf.variable_scope(self.name) as vs:\n            fc = z\n            fc = tcl.fully_connected(\n                fc, 512,\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                weights_regularizer=tc.layers.l2_regularizer(2.5e-5),\n                activation_fn=tcl.batch_norm\n            )\n            fc = leaky_relu(fc)\n            fc = tcl.fully_connected(\n                fc, 512,\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                weights_regularizer=tc.layers.l2_regularizer(2.5e-5),\n                activation_fn=tcl.batch_norm\n            )\n            fc = leaky_relu(fc)\n            fc = tcl.fully_connected(\n                fc, 512,\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                weights_regularizer=tc.layers.l2_regularizer(2.5e-5),\n                activation_fn=tcl.batch_norm\n            )\n            fc = leaky_relu(fc)\n            fc = tc.layers.fully_connected(\n                fc, 784,\n                weights_initializer=tf.random_normal_initializer(stddev=0.02),\n                weights_regularizer=tc.layers.l2_regularizer(2.5e-5),\n                activation_fn=tf.sigmoid\n            )\n            return fc\n\n    @property\n    def vars(self):\n        return [var for var in tf.global_variables() if self.name in var.name]"""
