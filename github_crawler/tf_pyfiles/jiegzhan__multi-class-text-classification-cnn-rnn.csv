file_path,api_count,code
data_helper.py,0,"b'import os\nimport re\nimport sys\nimport json\nimport pickle\nimport logging\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport gensim as gs\nfrom pprint import pprint\nfrom collections import Counter\nfrom tensorflow.contrib import learn\n\nlogging.getLogger().setLevel(logging.INFO)\n\ndef clean_str(s):\n\ts = re.sub(r""[^A-Za-z0-9:(),!?\\\'\\`]"", "" "", s)\n\ts = re.sub(r"" : "", "":"", s)\n\ts = re.sub(r""\\\'s"", "" \\\'s"", s)\n\ts = re.sub(r""\\\'ve"", "" \\\'ve"", s)\n\ts = re.sub(r""n\\\'t"", "" n\\\'t"", s)\n\ts = re.sub(r""\\\'re"", "" \\\'re"", s)\n\ts = re.sub(r""\\\'d"", "" \\\'d"", s)\n\ts = re.sub(r""\\\'ll"", "" \\\'ll"", s)\n\ts = re.sub(r"","", "" , "", s)\n\ts = re.sub(r""!"", "" ! "", s)\n\ts = re.sub(r""\\("", "" \\( "", s)\n\ts = re.sub(r""\\)"", "" \\) "", s)\n\ts = re.sub(r""\\?"", "" \\? "", s)\n\ts = re.sub(r""\\s{2,}"", "" "", s)\n\treturn s.strip().lower()\n\ndef load_embeddings(vocabulary):\n\tword_embeddings = {}\n\tfor word in vocabulary:\n\t\tword_embeddings[word] = np.random.uniform(-0.25, 0.25, 300)\n\treturn word_embeddings\n\ndef pad_sentences(sentences, padding_word=""<PAD/>"", forced_sequence_length=None):\n\t""""""Pad setences during training or prediction""""""\n\tif forced_sequence_length is None: # Train\n\t\tsequence_length = max(len(x) for x in sentences)\n\telse: # Prediction\n\t\tlogging.critical(\'This is prediction, reading the trained sequence length\')\n\t\tsequence_length = forced_sequence_length\n\tlogging.critical(\'The maximum length is {}\'.format(sequence_length))\n\n\tpadded_sentences = []\n\tfor i in range(len(sentences)):\n\t\tsentence = sentences[i]\n\t\tnum_padding = sequence_length - len(sentence)\n\n\t\tif num_padding < 0: # Prediction: cut off the sentence if it is longer than the sequence length\n\t\t\tlogging.info(\'This sentence has to be cut off because it is longer than trained sequence length\')\n\t\t\tpadded_sentence = sentence[0:sequence_length]\n\t\telse:\n\t\t\tpadded_sentence = sentence + [padding_word] * num_padding\n\t\tpadded_sentences.append(padded_sentence)\n\treturn padded_sentences\n\ndef build_vocab(sentences):\n\tword_counts = Counter(itertools.chain(*sentences))\n\tvocabulary_inv = [word[0] for word in word_counts.most_common()]\n\tvocabulary = {word: index for index, word in enumerate(vocabulary_inv)}\n\treturn vocabulary, vocabulary_inv\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n\tdata = np.array(data)\n\tdata_size = len(data)\n\tnum_batches_per_epoch = int(data_size / batch_size) + 1\n\n\tfor epoch in range(num_epochs):\n\t\tif shuffle:\n\t\t\tshuffle_indices = np.random.permutation(np.arange(data_size))\n\t\t\tshuffled_data = data[shuffle_indices]\n\t\telse:\n\t\t\tshuffled_data = data\n\n\t\tfor batch_num in range(num_batches_per_epoch):\n\t\t\tstart_index = batch_num * batch_size\n\t\t\tend_index = min((batch_num + 1) * batch_size, data_size)\n\t\t\tyield shuffled_data[start_index:end_index]\n\ndef load_data(filename):\n\tdf = pd.read_csv(filename, compression=\'zip\')\n\tselected = [\'Category\', \'Descript\']\n\tnon_selected = list(set(df.columns) - set(selected))\n\n\tdf = df.drop(non_selected, axis=1)\n\tdf = df.dropna(axis=0, how=\'any\', subset=selected)\n\tdf = df.reindex(np.random.permutation(df.index))\n\n\tlabels = sorted(list(set(df[selected[0]].tolist())))\n\tnum_labels = len(labels)\n\tone_hot = np.zeros((num_labels, num_labels), int)\n\tnp.fill_diagonal(one_hot, 1)\n\tlabel_dict = dict(zip(labels, one_hot))\n\n\tx_raw= df[selected[1]].apply(lambda x: clean_str(x).split(\' \')).tolist()\n\ty_raw = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n\n\tx_raw = pad_sentences(x_raw)\n\tvocabulary, vocabulary_inv = build_vocab(x_raw)\n\n\tx = np.array([[vocabulary[word] for word in sentence] for sentence in x_raw])\n\ty = np.array(y_raw)\n\treturn x, y, vocabulary, vocabulary_inv, df, labels\n\nif __name__ == ""__main__"":\n\ttrain_file = \'./data/train.csv.zip\'\n\tload_data(train_file)\n'"
predict.py,5,"b'import os\nimport sys\nimport json\nimport shutil\nimport pickle\nimport logging\nimport data_helper\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom text_cnn_rnn import TextCNNRNN\n\nlogging.getLogger().setLevel(logging.INFO)\n\ndef load_trained_params(trained_dir):\n\tparams = json.loads(open(trained_dir + \'trained_parameters.json\').read())\n\twords_index = json.loads(open(trained_dir + \'words_index.json\').read())\n\tlabels = json.loads(open(trained_dir + \'labels.json\').read())\n\n\twith open(trained_dir + \'embeddings.pickle\', \'rb\') as input_file:\n\t\tfetched_embedding = pickle.load(input_file)\n\tembedding_mat = np.array(fetched_embedding, dtype = np.float32)\n\treturn params, words_index, labels, embedding_mat\n\ndef load_test_data(test_file, labels):\n\tdf = pd.read_csv(test_file, sep=\'|\')\n\tselect = [\'Descript\']\n\n\tdf = df.dropna(axis=0, how=\'any\', subset=select)\n\ttest_examples = df[select[0]].apply(lambda x: data_helper.clean_str(x).split(\' \')).tolist()\n\n\tnum_labels = len(labels)\n\tone_hot = np.zeros((num_labels, num_labels), int)\n\tnp.fill_diagonal(one_hot, 1)\n\tlabel_dict = dict(zip(labels, one_hot))\n\n\ty_ = None\n\tif \'Category\' in df.columns:\n\t\tselect.append(\'Category\')\n\t\ty_ = df[select[1]].apply(lambda x: label_dict[x]).tolist()\n\n\tnot_select = list(set(df.columns) - set(select))\n\tdf = df.drop(not_select, axis=1)\n\treturn test_examples, y_, df\n\ndef map_word_to_index(examples, words_index):\n\tx_ = []\n\tfor example in examples:\n\t\ttemp = []\n\t\tfor word in example:\n\t\t\tif word in words_index:\n\t\t\t\ttemp.append(words_index[word])\n\t\t\telse:\n\t\t\t\ttemp.append(0)\n\t\tx_.append(temp)\n\treturn x_\n\ndef predict_unseen_data():\n\ttrained_dir = sys.argv[1]\n\tif not trained_dir.endswith(\'/\'):\n\t\ttrained_dir += \'/\'\n\ttest_file = sys.argv[2]\n\n\tparams, words_index, labels, embedding_mat = load_trained_params(trained_dir)\n\tx_, y_, df = load_test_data(test_file, labels)\n\tx_ = data_helper.pad_sentences(x_, forced_sequence_length=params[\'sequence_length\'])\n\tx_ = map_word_to_index(x_, words_index)\n\n\tx_test, y_test = np.asarray(x_), None\n\tif y_ is not None:\n\t\ty_test = np.asarray(y_)\n\n\ttimestamp = trained_dir.split(\'/\')[-2].split(\'_\')[-1]\n\tpredicted_dir = \'./predicted_results_\' + timestamp + \'/\'\n\tif os.path.exists(predicted_dir):\n\t\tshutil.rmtree(predicted_dir)\n\tos.makedirs(predicted_dir)\n\n\twith tf.Graph().as_default():\n\t\tsession_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n\t\tsess = tf.Session(config=session_conf)\n\t\twith sess.as_default():\n\t\t\tcnn_rnn = TextCNNRNN(\n\t\t\t\tembedding_mat = embedding_mat,\n\t\t\t\tnon_static = params[\'non_static\'],\n\t\t\t\thidden_unit = params[\'hidden_unit\'],\n\t\t\t\tsequence_length = len(x_test[0]),\n\t\t\t\tmax_pool_size = params[\'max_pool_size\'],\n\t\t\t\tfilter_sizes = map(int, params[\'filter_sizes\'].split("","")),\n\t\t\t\tnum_filters = params[\'num_filters\'],\n\t\t\t\tnum_classes = len(labels),\n\t\t\t\tembedding_size = params[\'embedding_dim\'],\n\t\t\t\tl2_reg_lambda = params[\'l2_reg_lambda\'])\n\n\t\t\tdef real_len(batches):\n\t\t\t\treturn [np.ceil(np.argmin(batch + [0]) * 1.0 / params[\'max_pool_size\']) for batch in batches]\n\n\t\t\tdef predict_step(x_batch):\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tcnn_rnn.input_x: x_batch,\n\t\t\t\t\tcnn_rnn.dropout_keep_prob: 1.0,\n\t\t\t\t\tcnn_rnn.batch_size: len(x_batch),\n\t\t\t\t\tcnn_rnn.pad: np.zeros([len(x_batch), 1, params[\'embedding_dim\'], 1]),\n\t\t\t\t\tcnn_rnn.real_len: real_len(x_batch),\n\t\t\t\t}\n\t\t\t\tpredictions = sess.run([cnn_rnn.predictions], feed_dict)\n\t\t\t\treturn predictions\n\n\t\t\tcheckpoint_file = trained_dir + \'best_model.ckpt\'\n\t\t\tsaver = tf.train.Saver(tf.all_variables())\n\t\t\tsaver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))\n\t\t\tsaver.restore(sess, checkpoint_file)\n\t\t\tlogging.critical(\'{} has been loaded\'.format(checkpoint_file))\n\n\t\t\tbatches = data_helper.batch_iter(list(x_test), params[\'batch_size\'], 1, shuffle=False)\n\n\t\t\tpredictions, predict_labels = [], []\n\t\t\tfor x_batch in batches:\n\t\t\t\tbatch_predictions = predict_step(x_batch)[0]\n\t\t\t\tfor batch_prediction in batch_predictions:\n\t\t\t\t\tpredictions.append(batch_prediction)\n\t\t\t\t\tpredict_labels.append(labels[batch_prediction])\n\n\t\t\t# Save the predictions back to file\n\t\t\tdf[\'NEW_PREDICTED\'] = predict_labels\n\t\t\tcolumns = sorted(df.columns, reverse=True)\n\t\t\tdf.to_csv(predicted_dir + \'predictions_all.csv\', index=False, columns=columns, sep=\'|\')\n\n\t\t\tif y_test is not None:\n\t\t\t\ty_test = np.array(np.argmax(y_test, axis=1))\n\t\t\t\taccuracy = sum(np.array(predictions) == y_test) / float(len(y_test))\n\t\t\t\tlogging.critical(\'The prediction accuracy is: {}\'.format(accuracy))\n\n\t\t\tlogging.critical(\'Prediction is complete, all files have been saved: {}\'.format(predicted_dir))\n\nif __name__ == \'__main__\':\n\t# python3 predict.py ./trained_results_1478563595/ ./data/small_samples.csv\n\tpredict_unseen_data()\n'"
text_cnn_rnn.py,57,"b'import numpy as np\nimport tensorflow as tf\n\nclass TextCNNRNN(object):\n\tdef __init__(self, embedding_mat, non_static, hidden_unit, sequence_length, max_pool_size,\n\t\tnum_classes, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n\n\t\tself.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\'input_x\')\n\t\tself.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\'input_y\')\n\t\tself.dropout_keep_prob = tf.placeholder(tf.float32, name=\'dropout_keep_prob\')\n\t\tself.batch_size = tf.placeholder(tf.int32, [])\n\t\tself.pad = tf.placeholder(tf.float32, [None, 1, embedding_size, 1], name=\'pad\')\n\t\tself.real_len = tf.placeholder(tf.int32, [None], name=\'real_len\')\n\n\t\tl2_loss = tf.constant(0.0)\n\n\t\twith tf.device(\'/cpu:0\'), tf.name_scope(\'embedding\'):\n\t\t\tif not non_static:\n\t\t\t\tW = tf.constant(embedding_mat, name=\'W\')\n\t\t\telse:\n\t\t\t\tW = tf.Variable(embedding_mat, name=\'W\')\n\t\t\tself.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n\t\t\temb = tf.expand_dims(self.embedded_chars, -1)\n\n\t\tpooled_concat = []\n\t\treduced = np.int32(np.ceil((sequence_length) * 1.0 / max_pool_size))\n\t\t\n\t\tfor i, filter_size in enumerate(filter_sizes):\n\t\t\twith tf.name_scope(\'conv-maxpool-%s\' % filter_size):\n\n\t\t\t\t# Zero paddings so that the convolution output have dimension batch x sequence_length x emb_size x channel\n\t\t\t\tnum_prio = (filter_size-1) // 2\n\t\t\t\tnum_post = (filter_size-1) - num_prio\n\t\t\t\tpad_prio = tf.concat([self.pad] * num_prio,1)\n\t\t\t\tpad_post = tf.concat([self.pad] * num_post,1)\n\t\t\t\temb_pad = tf.concat([pad_prio, emb, pad_post],1)\n\n\t\t\t\tfilter_shape = [filter_size, embedding_size, 1, num_filters]\n\t\t\t\tW = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\'W\')\n\t\t\t\tb = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\'b\')\n\t\t\t\tconv = tf.nn.conv2d(emb_pad, W, strides=[1, 1, 1, 1], padding=\'VALID\', name=\'conv\')\n\n\t\t\t\th = tf.nn.relu(tf.nn.bias_add(conv, b), name=\'relu\')\n\n\t\t\t\t# Maxpooling over the outputs\n\t\t\t\tpooled = tf.nn.max_pool(h, ksize=[1, max_pool_size, 1, 1], strides=[1, max_pool_size, 1, 1], padding=\'SAME\', name=\'pool\')\n\t\t\t\tpooled = tf.reshape(pooled, [-1, reduced, num_filters])\n\t\t\t\tpooled_concat.append(pooled)\n\n\t\tpooled_concat = tf.concat(pooled_concat,2)\n\t\tpooled_concat = tf.nn.dropout(pooled_concat, self.dropout_keep_prob)\n\n\t\t# lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_unit)\n\n\t\t#lstm_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_unit)\n\t\tlstm_cell = tf.contrib.rnn.GRUCell(num_units=hidden_unit)\n\n\t\t#lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)\n\t\tlstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)\n\t\t\n\n\t\tself._initial_state = lstm_cell.zero_state(self.batch_size, tf.float32)\n\t\t#inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, reduced, pooled_concat)]\n\t\tinputs = [tf.squeeze(input_, [1]) for input_ in tf.split(pooled_concat,num_or_size_splits=int(reduced),axis=1)]\n\t\t#outputs, state = tf.nn.rnn(lstm_cell, inputs, initial_state=self._initial_state, sequence_length=self.real_len)\n\t\toutputs, state = tf.contrib.rnn.static_rnn(lstm_cell, inputs, initial_state=self._initial_state, sequence_length=self.real_len)\n\n\t\t# Collect the appropriate last words into variable output (dimension = batch x embedding_size)\n\t\toutput = outputs[0]\n\t\twith tf.variable_scope(\'Output\'):\n\t\t\ttf.get_variable_scope().reuse_variables()\n\t\t\tone = tf.ones([1, hidden_unit], tf.float32)\n\t\t\tfor i in range(1,len(outputs)):\n\t\t\t\tind = self.real_len < (i+1)\n\t\t\t\tind = tf.to_float(ind)\n\t\t\t\tind = tf.expand_dims(ind, -1)\n\t\t\t\tmat = tf.matmul(ind, one)\n\t\t\t\toutput = tf.add(tf.multiply(output, mat),tf.multiply(outputs[i], 1.0 - mat))\n\n\t\twith tf.name_scope(\'output\'):\n\t\t\tself.W = tf.Variable(tf.truncated_normal([hidden_unit, num_classes], stddev=0.1), name=\'W\')\n\t\t\tb = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\'b\')\n\t\t\tl2_loss += tf.nn.l2_loss(W)\n\t\t\tl2_loss += tf.nn.l2_loss(b)\n\t\t\tself.scores = tf.nn.xw_plus_b(output, self.W, b, name=\'scores\')\n\t\t\tself.predictions = tf.argmax(self.scores, 1, name=\'predictions\')\n\n\t\twith tf.name_scope(\'loss\'):\n\t\t\tlosses = tf.nn.softmax_cross_entropy_with_logits(labels = self.input_y, logits = self.scores) #  only named arguments accepted            \n\t\t\tself.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n\t\twith tf.name_scope(\'accuracy\'):\n\t\t\tcorrect_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n\t\t\tself.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=\'accuracy\')\n\n\t\twith tf.name_scope(\'num_correct\'):\n\t\t\tcorrect = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n\t\t\tself.num_correct = tf.reduce_sum(tf.cast(correct, \'float\'))\n'"
train.py,8,"b'import os\nimport sys\nimport json\nimport time\nimport shutil\nimport pickle\nimport logging\nimport data_helper\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom text_cnn_rnn import TextCNNRNN\nfrom sklearn.model_selection import train_test_split\n\nlogging.getLogger().setLevel(logging.INFO)\n\ndef train_cnn_rnn():\n\tinput_file = sys.argv[1]\n\tx_, y_, vocabulary, vocabulary_inv, df, labels = data_helper.load_data(input_file)\n\n\ttraining_config = sys.argv[2]\n\tparams = json.loads(open(training_config).read())\n\n\t# Assign a 300 dimension vector to each word\n\tword_embeddings = data_helper.load_embeddings(vocabulary)\n\tembedding_mat = [word_embeddings[word] for index, word in enumerate(vocabulary_inv)]\n\tembedding_mat = np.array(embedding_mat, dtype = np.float32)\n\n\t# Split the original dataset into train set and test set\n\tx, x_test, y, y_test = train_test_split(x_, y_, test_size=0.1)\n\n\t# Split the train set into train set and dev set\n\tx_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size=0.1)\n\n\tlogging.info(\'x_train: {}, x_dev: {}, x_test: {}\'.format(len(x_train), len(x_dev), len(x_test)))\n\tlogging.info(\'y_train: {}, y_dev: {}, y_test: {}\'.format(len(y_train), len(y_dev), len(y_test)))\n\n\t# Create a directory, everything related to the training will be saved in this directory\n\ttimestamp = str(int(time.time()))\n\ttrained_dir = \'./trained_results_\' + timestamp + \'/\'\n\tif os.path.exists(trained_dir):\n\t\tshutil.rmtree(trained_dir)\n\tos.makedirs(trained_dir)\n\n\tgraph = tf.Graph()\n\twith graph.as_default():\n\t\tsession_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n\t\tsess = tf.Session(config=session_conf)\n\t\twith sess.as_default():\n\t\t\tcnn_rnn = TextCNNRNN(\n\t\t\t\tembedding_mat=embedding_mat,\n\t\t\t\tsequence_length=x_train.shape[1],\n\t\t\t\tnum_classes = y_train.shape[1],\n\t\t\t\tnon_static=params[\'non_static\'],\n\t\t\t\thidden_unit=params[\'hidden_unit\'],\n\t\t\t\tmax_pool_size=params[\'max_pool_size\'],\n\t\t\t\tfilter_sizes=map(int, params[\'filter_sizes\'].split("","")),\n\t\t\t\tnum_filters = params[\'num_filters\'],\n\t\t\t\tembedding_size = params[\'embedding_dim\'],\n\t\t\t\tl2_reg_lambda = params[\'l2_reg_lambda\'])\n\n\t\t\tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t\t\toptimizer = tf.train.RMSPropOptimizer(1e-3, decay=0.9)\n\t\t\tgrads_and_vars = optimizer.compute_gradients(cnn_rnn.loss)\n\t\t\ttrain_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n\t\t\t# Checkpoint files will be saved in this directory during training\n\t\t\tcheckpoint_dir = \'./checkpoints_\' + timestamp + \'/\'\n\t\t\tif os.path.exists(checkpoint_dir):\n\t\t\t\tshutil.rmtree(checkpoint_dir)\n\t\t\tos.makedirs(checkpoint_dir)\n\t\t\tcheckpoint_prefix = os.path.join(checkpoint_dir, \'model\')\n\n\t\t\tdef real_len(batches):\n\t\t\t\treturn [np.ceil(np.argmin(batch + [0]) * 1.0 / params[\'max_pool_size\']) for batch in batches]\n\n\t\t\tdef train_step(x_batch, y_batch):\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tcnn_rnn.input_x: x_batch,\n\t\t\t\t\tcnn_rnn.input_y: y_batch,\n\t\t\t\t\tcnn_rnn.dropout_keep_prob: params[\'dropout_keep_prob\'],\n\t\t\t\t\tcnn_rnn.batch_size: len(x_batch),\n\t\t\t\t\tcnn_rnn.pad: np.zeros([len(x_batch), 1, params[\'embedding_dim\'], 1]),\n\t\t\t\t\tcnn_rnn.real_len: real_len(x_batch),\n\t\t\t\t}\n\t\t\t\t_, step, loss, accuracy = sess.run([train_op, global_step, cnn_rnn.loss, cnn_rnn.accuracy], feed_dict)\n\n\t\t\tdef dev_step(x_batch, y_batch):\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tcnn_rnn.input_x: x_batch,\n\t\t\t\t\tcnn_rnn.input_y: y_batch,\n\t\t\t\t\tcnn_rnn.dropout_keep_prob: 1.0,\n\t\t\t\t\tcnn_rnn.batch_size: len(x_batch),\n\t\t\t\t\tcnn_rnn.pad: np.zeros([len(x_batch), 1, params[\'embedding_dim\'], 1]),\n\t\t\t\t\tcnn_rnn.real_len: real_len(x_batch),\n\t\t\t\t}\n\t\t\t\tstep, loss, accuracy, num_correct, predictions = sess.run(\n\t\t\t\t\t[global_step, cnn_rnn.loss, cnn_rnn.accuracy, cnn_rnn.num_correct, cnn_rnn.predictions], feed_dict)\n\t\t\t\treturn accuracy, loss, num_correct, predictions\n\n\t\t\tsaver = tf.train.Saver()\n\t\t\tsess.run(tf.global_variables_initializer())\n\n\t\t\t# Training starts here\n\t\t\ttrain_batches = data_helper.batch_iter(list(zip(x_train, y_train)), params[\'batch_size\'], params[\'num_epochs\'])\n\t\t\tbest_accuracy, best_at_step = 0, 0\n\n\t\t\t# Train the model with x_train and y_train\n\t\t\tfor train_batch in train_batches:\n\t\t\t\tx_train_batch, y_train_batch = zip(*train_batch)\n\t\t\t\ttrain_step(x_train_batch, y_train_batch)\n\t\t\t\tcurrent_step = tf.train.global_step(sess, global_step)\n\n\t\t\t\t# Evaluate the model with x_dev and y_dev\n\t\t\t\tif current_step % params[\'evaluate_every\'] == 0:\n\t\t\t\t\tdev_batches = data_helper.batch_iter(list(zip(x_dev, y_dev)), params[\'batch_size\'], 1)\n\n\t\t\t\t\ttotal_dev_correct = 0\n\t\t\t\t\tfor dev_batch in dev_batches:\n\t\t\t\t\t\tx_dev_batch, y_dev_batch = zip(*dev_batch)\n\t\t\t\t\t\tacc, loss, num_dev_correct, predictions = dev_step(x_dev_batch, y_dev_batch)\n\t\t\t\t\t\ttotal_dev_correct += num_dev_correct\n\t\t\t\t\taccuracy = float(total_dev_correct) / len(y_dev)\n\t\t\t\t\tlogging.info(\'Accuracy on dev set: {}\'.format(accuracy))\n\n\t\t\t\t\tif accuracy >= best_accuracy:\n\t\t\t\t\t\tbest_accuracy, best_at_step = accuracy, current_step\n\t\t\t\t\t\tpath = saver.save(sess, checkpoint_prefix, global_step=current_step)\n\t\t\t\t\t\tlogging.critical(\'Saved model {} at step {}\'.format(path, best_at_step))\n\t\t\t\t\t\tlogging.critical(\'Best accuracy {} at step {}\'.format(best_accuracy, best_at_step))\n\t\t\tlogging.critical(\'Training is complete, testing the best model on x_test and y_test\')\n\n\t\t\t# Save the model files to trained_dir. predict.py needs trained model files. \n\t\t\tsaver.save(sess, trained_dir + ""best_model.ckpt"")\n\n\t\t\t# Evaluate x_test and y_test\n\t\t\tsaver.restore(sess, checkpoint_prefix + \'-\' + str(best_at_step))\n\t\t\ttest_batches = data_helper.batch_iter(list(zip(x_test, y_test)), params[\'batch_size\'], 1, shuffle=False)\n\t\t\ttotal_test_correct = 0\n\t\t\tfor test_batch in test_batches:\n\t\t\t\tx_test_batch, y_test_batch = zip(*test_batch)\n\t\t\t\tacc, loss, num_test_correct, predictions = dev_step(x_test_batch, y_test_batch)\n\t\t\t\ttotal_test_correct += int(num_test_correct)\n\t\t\tlogging.critical(\'Accuracy on test set: {}\'.format(float(total_test_correct) / len(y_test)))\n\n\t# Save trained parameters and files since predict.py needs them\n\twith open(trained_dir + \'words_index.json\', \'w\') as outfile:\n\t\tjson.dump(vocabulary, outfile, indent=4, ensure_ascii=False)\n\twith open(trained_dir + \'embeddings.pickle\', \'wb\') as outfile:\n\t\tpickle.dump(embedding_mat, outfile, pickle.HIGHEST_PROTOCOL)\n\twith open(trained_dir + \'labels.json\', \'w\') as outfile:\n\t\tjson.dump(labels, outfile, indent=4, ensure_ascii=False)\n\n\tparams[\'sequence_length\'] = x_train.shape[1]\n\twith open(trained_dir + \'trained_parameters.json\', \'w\') as outfile:\n\t\tjson.dump(params, outfile, indent=4, sort_keys=True, ensure_ascii=False)\n\nif __name__ == \'__main__\':\n\t# python3 train.py ./data/train.csv.zip ./training_config.json\n\ttrain_cnn_rnn()\n'"
