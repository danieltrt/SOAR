file_path,api_count,code
data_helper.py,0,"b'import re\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\ndef clean_str(s):\n\t""""""Clean sentence""""""\n\ts = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", s)\n\ts = re.sub(r""\\\'s"", "" \\\'s"", s)\n\ts = re.sub(r""\\\'ve"", "" \\\'ve"", s)\n\ts = re.sub(r""n\\\'t"", "" n\\\'t"", s)\n\ts = re.sub(r""\\\'re"", "" \\\'re"", s)\n\ts = re.sub(r""\\\'d"", "" \\\'d"", s)\n\ts = re.sub(r""\\\'ll"", "" \\\'ll"", s)\n\ts = re.sub(r"","", "" , "", s)\n\ts = re.sub(r""!"", "" ! "", s)\n\ts = re.sub(r""\\("", "" \\( "", s)\n\ts = re.sub(r""\\)"", "" \\) "", s)\n\ts = re.sub(r""\\?"", "" \\? "", s)\n\ts = re.sub(r""\\s{2,}"", "" "", s)\n\ts = re.sub(r\'\\S*(x{2,}|X{2,})\\S*\',""xxx"", s)\n\ts = re.sub(r\'[^\\x00-\\x7F]+\', """", s)\n\treturn s.strip().lower()\n\ndef load_data_and_labels(filename):\n\t""""""Load sentences and labels""""""\n\tdf = pd.read_csv(filename, compression=\'zip\', dtype={\'consumer_complaint_narrative\': object})\n\tselected = [\'product\', \'consumer_complaint_narrative\']\n\tnon_selected = list(set(df.columns) - set(selected))\n\n\tdf = df.drop(non_selected, axis=1) # Drop non selected columns\n\tdf = df.dropna(axis=0, how=\'any\', subset=selected) # Drop null rows\n\tdf = df.reindex(np.random.permutation(df.index)) # Shuffle the dataframe\n\n\t# Map the actual labels to one hot labels\n\tlabels = sorted(list(set(df[selected[0]].tolist())))\n\tone_hot = np.zeros((len(labels), len(labels)), int)\n\tnp.fill_diagonal(one_hot, 1)\n\tlabel_dict = dict(zip(labels, one_hot))\n\n\tx_raw = df[selected[1]].apply(lambda x: clean_str(x)).tolist()\n\ty_raw = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n\treturn x_raw, y_raw, df, labels\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n\t""""""Iterate the data batch by batch""""""\n\tdata = np.array(data)\n\tdata_size = len(data)\n\tnum_batches_per_epoch = int(data_size / batch_size) + 1\n\n\tfor epoch in range(num_epochs):\n\t\tif shuffle:\n\t\t\tshuffle_indices = np.random.permutation(np.arange(data_size))\n\t\t\tshuffled_data = data[shuffle_indices]\n\t\telse:\n\t\t\tshuffled_data = data\n\n\t\tfor batch_num in range(num_batches_per_epoch):\n\t\t\tstart_index = batch_num * batch_size\n\t\t\tend_index = min((batch_num + 1) * batch_size, data_size)\n\t\t\tyield shuffled_data[start_index:end_index]\n\nif __name__ == \'__main__\':\n\tinput_file = \'./data/consumer_complaints.csv.zip\'\n\tload_data_and_labels(input_file)\n'"
predict.py,5,"b'import os\nimport sys\nimport json\nimport logging\nimport data_helper\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\n\nlogging.getLogger().setLevel(logging.INFO)\n\ndef predict_unseen_data():\n\t""""""Step 0: load trained model and parameters""""""\n\tparams = json.loads(open(\'./parameters.json\').read())\n\tcheckpoint_dir = sys.argv[1]\n\tif not checkpoint_dir.endswith(\'/\'):\n\t\tcheckpoint_dir += \'/\'\n\tcheckpoint_file = tf.train.latest_checkpoint(checkpoint_dir + \'checkpoints\')\n\tlogging.critical(\'Loaded the trained model: {}\'.format(checkpoint_file))\n\n\t""""""Step 1: load data for prediction""""""\n\ttest_file = sys.argv[2]\n\ttest_examples = json.loads(open(test_file).read())\n\n\t# labels.json was saved during training, and it has to be loaded during prediction\n\tlabels = json.loads(open(\'./labels.json\').read())\n\tone_hot = np.zeros((len(labels), len(labels)), int)\n\tnp.fill_diagonal(one_hot, 1)\n\tlabel_dict = dict(zip(labels, one_hot))\n\n\tx_raw = [example[\'consumer_complaint_narrative\'] for example in test_examples]\n\tx_test = [data_helper.clean_str(x) for x in x_raw]\n\tlogging.info(\'The number of x_test: {}\'.format(len(x_test)))\n\n\ty_test = None\n\tif \'product\' in test_examples[0]:\n\t\ty_raw = [example[\'product\'] for example in test_examples]\n\t\ty_test = [label_dict[y] for y in y_raw]\n\t\tlogging.info(\'The number of y_test: {}\'.format(len(y_test)))\n\n\tvocab_path = os.path.join(checkpoint_dir, ""vocab.pickle"")\n\tvocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n\tx_test = np.array(list(vocab_processor.transform(x_test)))\n\n\t""""""Step 2: compute the predictions""""""\n\tgraph = tf.Graph()\n\twith graph.as_default():\n\t\tsession_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n\t\tsess = tf.Session(config=session_conf)\n\n\t\twith sess.as_default():\n\t\t\tsaver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))\n\t\t\tsaver.restore(sess, checkpoint_file)\n\n\t\t\tinput_x = graph.get_operation_by_name(""input_x"").outputs[0]\n\t\t\tdropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n\t\t\tpredictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n\n\t\t\tbatches = data_helper.batch_iter(list(x_test), params[\'batch_size\'], 1, shuffle=False)\n\t\t\tall_predictions = []\n\t\t\tfor x_test_batch in batches:\n\t\t\t\tbatch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n\t\t\t\tall_predictions = np.concatenate([all_predictions, batch_predictions])\n\n\tif y_test is not None:\n\t\ty_test = np.argmax(y_test, axis=1)\n\t\tcorrect_predictions = sum(all_predictions == y_test)\n\n\t\t# Save the actual labels back to file\n\t\tactual_labels = [labels[int(prediction)] for prediction in all_predictions]\n\n\t\tfor idx, example in enumerate(test_examples):\n\t\t\texample[\'new_prediction\'] = actual_labels[idx]\n\t\t\n\t\twith open(\'./data/small_samples_prediction.json\', \'w\') as outfile:\n\t\t\tjson.dump(test_examples, outfile, indent=4)\n\n\t\tlogging.critical(\'The accuracy is: {}\'.format(correct_predictions / float(len(y_test))))\n\t\tlogging.critical(\'The prediction is complete\')\n\nif __name__ == \'__main__\':\n\t# python3 predict.py ./trained_model_1478649295/ ./data/small_samples.json\n\tpredict_unseen_data()\n'"
text_cnn.py,35,"b""import numpy as np\nimport tensorflow as tf\n\nclass TextCNN(object):\n\tdef __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n\t\t# Placeholders for input, output and dropout\n\t\tself.input_x = tf.placeholder(tf.int32, [None, sequence_length], name='input_x')\n\t\tself.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n\t\tself.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n\n\t\t# Keeping track of l2 regularization loss (optional)\n\t\tl2_loss = tf.constant(0.0)\n\n\t\t# Embedding layer\n\t\twith tf.device('/cpu:0'), tf.name_scope('embedding'):\n\t\t\tW = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name='W')\n\t\t\tself.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n\t\t\tself.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n\n\t\t# Create a convolution + maxpool layer for each filter size\n\t\tpooled_outputs = []\n\t\tfor i, filter_size in enumerate(filter_sizes):\n\t\t\twith tf.name_scope('conv-maxpool-%s' % filter_size):\n\t\t\t\t# Convolution Layer\n\t\t\t\tfilter_shape = [filter_size, embedding_size, 1, num_filters]\n\t\t\t\tW = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n\t\t\t\tb = tf.Variable(tf.constant(0.1, shape=[num_filters]), name='b')\n\t\t\t\tconv = tf.nn.conv2d(\n\t\t\t\t\tself.embedded_chars_expanded,\n\t\t\t\t\tW,\n\t\t\t\t\tstrides=[1, 1, 1, 1],\n\t\t\t\t\tpadding='VALID',\n\t\t\t\t\tname='conv')\n\n\t\t\t\t# Apply nonlinearity\n\t\t\t\th = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n\n\t\t\t\t# Maxpooling over the outputs\n\t\t\t\tpooled = tf.nn.max_pool(\n\t\t\t\t\th,\n\t\t\t\t\tksize=[1, sequence_length - filter_size + 1, 1, 1],\n\t\t\t\t\tstrides=[1, 1, 1, 1],\n\t\t\t\t\tpadding='VALID',\n\t\t\t\t\tname='pool')\n\t\t\t\tpooled_outputs.append(pooled)\n\n\t\t# Combine all the pooled features\n\t\tnum_filters_total = num_filters * len(filter_sizes)\n\t\tself.h_pool = tf.concat(pooled_outputs,3)\n\t\tself.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n\t\t# Add dropout\n\t\twith tf.name_scope('dropout'):\n\t\t\tself.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n\t\t# Final (unnormalized) scores and predictions\n\t\twith tf.name_scope('output'):\n\t\t\tW = tf.get_variable(\n\t\t\t\t'W',\n\t\t\t\tshape=[num_filters_total, num_classes],\n\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer())\n\t\t\tb = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b')\n\t\t\tl2_loss += tf.nn.l2_loss(W)\n\t\t\tl2_loss += tf.nn.l2_loss(b)\n\t\t\tself.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name='scores')\n\t\t\tself.predictions = tf.argmax(self.scores, 1, name='predictions')\n\n\t\t# Calculate mean cross-entropy loss\n\t\twith tf.name_scope('loss'):\n\t\t\tlosses = tf.nn.softmax_cross_entropy_with_logits(labels = self.input_y, logits = self.scores) #  only named arguments accepted            \n\t\t\tself.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n\t\t# Accuracy\n\t\twith tf.name_scope('accuracy'):\n\t\t\tcorrect_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n\t\t\tself.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n\n\t\twith tf.name_scope('num_correct'):\n\t\t\tcorrect_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n\t\t\tself.num_correct = tf.reduce_sum(tf.cast(correct_predictions, 'float'), name='num_correct')\n"""
train.py,8,"b'import os\nimport sys\nimport json\nimport time\nimport logging\nimport data_helper\nimport numpy as np\nimport tensorflow as tf\nfrom text_cnn import TextCNN\nfrom tensorflow.contrib import learn\nfrom sklearn.model_selection import train_test_split\n\nlogging.getLogger().setLevel(logging.INFO)\n\ndef train_cnn():\n\t""""""Step 0: load sentences, labels, and training parameters""""""\n\ttrain_file = sys.argv[1]\n\tx_raw, y_raw, df, labels = data_helper.load_data_and_labels(train_file)\n\n\tparameter_file = sys.argv[2]\n\tparams = json.loads(open(parameter_file).read())\n\n\t""""""Step 1: pad each sentence to the same length and map each word to an id""""""\n\tmax_document_length = max([len(x.split(\' \')) for x in x_raw])\n\tlogging.info(\'The maximum length of all sentences: {}\'.format(max_document_length))\n\tvocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n\tx = np.array(list(vocab_processor.fit_transform(x_raw)))\n\ty = np.array(y_raw)\n\n\t""""""Step 2: split the original dataset into train and test sets""""""\n\tx_, x_test, y_, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n\n\t""""""Step 3: shuffle the train set and split the train set into train and dev sets""""""\n\tshuffle_indices = np.random.permutation(np.arange(len(y_)))\n\tx_shuffled = x_[shuffle_indices]\n\ty_shuffled = y_[shuffle_indices]\n\tx_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)\n\n\t""""""Step 4: save the labels into labels.json since predict.py needs it""""""\n\twith open(\'./labels.json\', \'w\') as outfile:\n\t\tjson.dump(labels, outfile, indent=4)\n\n\tlogging.info(\'x_train: {}, x_dev: {}, x_test: {}\'.format(len(x_train), len(x_dev), len(x_test)))\n\tlogging.info(\'y_train: {}, y_dev: {}, y_test: {}\'.format(len(y_train), len(y_dev), len(y_test)))\n\n\t""""""Step 5: build a graph and cnn object""""""\n\tgraph = tf.Graph()\n\twith graph.as_default():\n\t\tsession_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n\t\tsess = tf.Session(config=session_conf)\n\t\twith sess.as_default():\n\t\t\tcnn = TextCNN(\n\t\t\t\tsequence_length=x_train.shape[1],\n\t\t\t\tnum_classes=y_train.shape[1],\n\t\t\t\tvocab_size=len(vocab_processor.vocabulary_),\n\t\t\t\tembedding_size=params[\'embedding_dim\'],\n\t\t\t\tfilter_sizes=list(map(int, params[\'filter_sizes\'].split("",""))),\n\t\t\t\tnum_filters=params[\'num_filters\'],\n\t\t\t\tl2_reg_lambda=params[\'l2_reg_lambda\'])\n\n\t\t\tglobal_step = tf.Variable(0, name=""global_step"", trainable=False)\n\t\t\toptimizer = tf.train.AdamOptimizer(1e-3)\n\t\t\tgrads_and_vars = optimizer.compute_gradients(cnn.loss)\n\t\t\ttrain_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n\t\t\ttimestamp = str(int(time.time()))\n\t\t\tout_dir = os.path.abspath(os.path.join(os.path.curdir, ""trained_model_"" + timestamp))\n\n\t\t\tcheckpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n\t\t\tcheckpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n\t\t\tif not os.path.exists(checkpoint_dir):\n\t\t\t\tos.makedirs(checkpoint_dir)\n\t\t\tsaver = tf.train.Saver()\n\n\t\t\t# One training step: train the model with one batch\n\t\t\tdef train_step(x_batch, y_batch):\n\t\t\t\tfeed_dict = {\n\t\t\t\t\tcnn.input_x: x_batch,\n\t\t\t\t\tcnn.input_y: y_batch,\n\t\t\t\t\tcnn.dropout_keep_prob: params[\'dropout_keep_prob\']}\n\t\t\t\t_, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n\n\t\t\t# One evaluation step: evaluate the model with one batch\n\t\t\tdef dev_step(x_batch, y_batch):\n\t\t\t\tfeed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0}\n\t\t\t\tstep, loss, acc, num_correct = sess.run([global_step, cnn.loss, cnn.accuracy, cnn.num_correct], feed_dict)\n\t\t\t\treturn num_correct\n\n\t\t\t# Save the word_to_id map since predict.py needs it\n\t\t\tvocab_processor.save(os.path.join(out_dir, ""vocab.pickle""))\n\t\t\tsess.run(tf.global_variables_initializer())\n\n\t\t\t# Training starts here\n\t\t\ttrain_batches = data_helper.batch_iter(list(zip(x_train, y_train)), params[\'batch_size\'], params[\'num_epochs\'])\n\t\t\tbest_accuracy, best_at_step = 0, 0\n\n\t\t\t""""""Step 6: train the cnn model with x_train and y_train (batch by batch)""""""\n\t\t\tfor train_batch in train_batches:\n\t\t\t\tx_train_batch, y_train_batch = zip(*train_batch)\n\t\t\t\ttrain_step(x_train_batch, y_train_batch)\n\t\t\t\tcurrent_step = tf.train.global_step(sess, global_step)\n\n\t\t\t\t""""""Step 6.1: evaluate the model with x_dev and y_dev (batch by batch)""""""\n\t\t\t\tif current_step % params[\'evaluate_every\'] == 0:\n\t\t\t\t\tdev_batches = data_helper.batch_iter(list(zip(x_dev, y_dev)), params[\'batch_size\'], 1)\n\t\t\t\t\ttotal_dev_correct = 0\n\t\t\t\t\tfor dev_batch in dev_batches:\n\t\t\t\t\t\tx_dev_batch, y_dev_batch = zip(*dev_batch)\n\t\t\t\t\t\tnum_dev_correct = dev_step(x_dev_batch, y_dev_batch)\n\t\t\t\t\t\ttotal_dev_correct += num_dev_correct\n\n\t\t\t\t\tdev_accuracy = float(total_dev_correct) / len(y_dev)\n\t\t\t\t\tlogging.critical(\'Accuracy on dev set: {}\'.format(dev_accuracy))\n\n\t\t\t\t\t""""""Step 6.2: save the model if it is the best based on accuracy on dev set""""""\n\t\t\t\t\tif dev_accuracy >= best_accuracy:\n\t\t\t\t\t\tbest_accuracy, best_at_step = dev_accuracy, current_step\n\t\t\t\t\t\tpath = saver.save(sess, checkpoint_prefix, global_step=current_step)\n\t\t\t\t\t\tlogging.critical(\'Saved model at {} at step {}\'.format(path, best_at_step))\n\t\t\t\t\t\tlogging.critical(\'Best accuracy is {} at step {}\'.format(best_accuracy, best_at_step))\n\n\t\t\t""""""Step 7: predict x_test (batch by batch)""""""\n\t\t\ttest_batches = data_helper.batch_iter(list(zip(x_test, y_test)), params[\'batch_size\'], 1)\n\t\t\ttotal_test_correct = 0\n\t\t\tfor test_batch in test_batches:\n\t\t\t\tx_test_batch, y_test_batch = zip(*test_batch)\n\t\t\t\tnum_test_correct = dev_step(x_test_batch, y_test_batch)\n\t\t\t\ttotal_test_correct += num_test_correct\n\n\t\t\ttest_accuracy = float(total_test_correct) / len(y_test)\n\t\t\tlogging.critical(\'Accuracy on test set is {} based on the best model {}\'.format(test_accuracy, path))\n\t\t\tlogging.critical(\'The training is complete\')\n\nif __name__ == \'__main__\':\n\t# python3 train.py ./data/consumer_complaints.csv.zip ./parameters.json\n\ttrain_cnn()\n'"
