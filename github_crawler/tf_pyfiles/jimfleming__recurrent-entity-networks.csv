file_path,api_count,code
setup.py,0,"b""from setuptools import find_packages\nfrom setuptools import setup\n\nREQUIRED_PACKAGES = [\n    'tqdm==4.11.2'\n]\n\nsetup(name='entity_networks',\n      version='0.1',\n      install_requires=REQUIRED_PACKAGES,\n      packages=find_packages(),\n      include_package_data=True)\n"""
entity_networks/__init__.py,0,b''
entity_networks/create_instances.py,2,"b""from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport json\nimport random\nimport argparse\nimport tensorflow as tf\n\nfrom tqdm import tqdm\n\nfrom entity_networks.inputs import generate_input_fn\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--data-dir',\n        help='Directory containing data',\n        default='data/babi/records/')\n    args = parser.parse_args()\n\n    tasks_dir = 'tasks/'\n\n    if not os.path.exists(tasks_dir):\n        os.makedirs(tasks_dir)\n\n    task_names = [\n        'qa1_single-supporting-fact',\n        'qa2_two-supporting-facts',\n        'qa3_three-supporting-facts',\n        'qa4_two-arg-relations',\n        'qa5_three-arg-relations',\n        'qa6_yes-no-questions',\n        'qa7_counting',\n        'qa8_lists-sets',\n        'qa9_simple-negation',\n        'qa10_indefinite-knowledge',\n        'qa11_basic-coreference',\n        'qa12_conjunction',\n        'qa13_compound-coreference',\n        'qa14_time-reasoning',\n        'qa15_basic-deduction',\n        'qa16_basic-induction',\n        'qa17_positional-reasoning',\n        'qa18_size-reasoning',\n        'qa19_path-finding',\n        'qa20_agents-motivations',\n    ]\n\n    for task_name in tqdm(task_name.iteritems()):\n        metadata_path = os.path.join(args.data_dir, '{}_10k.json'.format(task_name))\n        with open(metadata_path) as metadata_file:\n            metadata = json.load(metadata_file)\n\n        filename = os.path.join(data_dir, '{}_10k_{}.tfrecords'.format(dataset_id, 'test'))\n        input_fn = generate_input_fn(\n            filename=eval_filename,\n            metadata=metadata,\n            batch_size=BATCH_SIZE,\n            num_epochs=1,\n            shuffle=False)\n\n        with tf.Graph().as_default():\n            features, answer = input_fn()\n\n            story = features['story']\n            query = features['query']\n\n            instances = []\n\n            with tf.train.SingularMonitoredSession() as sess:\n                while not sess.should_stop():\n                    story_, query_, answer_ = sess.run([story, query, answer])\n\n                    instance = {\n                        'story': story_[0].tolist(),\n                        'query': query_[0].tolist(),\n                        'answer': answer_[0].tolist(),\n                    }\n\n                    instances.append(instance)\n\n            metadata['instances'] = random.sample(instances, k=10)\n\n            output_path = os.path.join(tasks_dir, '{}.json'.format(task_name))\n            with open(output_path, 'w') as f:\n                f.write(json.dumps(metadata))\n\nif __name__ == '__main__':\n    main()\n"""
entity_networks/dynamic_memory_cell.py,23,"b'""Define a dynamic memory cell.""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport tensorflow as tf\n\nclass DynamicMemoryCell(tf.contrib.rnn.RNNCell):\n    """"""\n    Implementation of a dynamic memory cell as a gated recurrent network.\n    The cell\'s hidden state is divided into blocks and each block\'s weights are tied.\n    """"""\n\n    def __init__(self,\n                 num_blocks,\n                 num_units_per_block,\n                 keys,\n                 initializer=None,\n                 recurrent_initializer=None,\n                 activation=tf.nn.relu):\n        self._num_blocks = num_blocks # M\n        self._num_units_per_block = num_units_per_block # d\n        self._keys = keys\n        self._activation = activation # \\phi\n        self._initializer = initializer\n        self._recurrent_initializer = recurrent_initializer\n\n    @property\n    def state_size(self):\n        ""Return the total state size of the cell, across all blocks.""\n        return self._num_blocks * self._num_units_per_block\n\n    @property\n    def output_size(self):\n        ""Return the total output size of the cell, across all blocks.""\n        return self._num_blocks * self._num_units_per_block\n\n    def zero_state(self, batch_size, dtype):\n        ""Initialize the memory to the key values.""\n        zero_state = tf.concat([tf.expand_dims(key, axis=0) for key in self._keys], axis=1)\n        zero_state_batch = tf.tile(zero_state, [batch_size, 1])\n        return zero_state_batch\n\n    def get_gate(self, state_j, key_j, inputs):\n        """"""\n        Implements the gate (scalar for each block). Equation 2:\n\n        g_j <- \\sigma(s_t^T h_j + s_t^T w_j)\n        """"""\n        a = tf.reduce_sum(inputs * state_j, axis=1)\n        b = tf.reduce_sum(inputs * key_j, axis=1)\n        return tf.sigmoid(a + b)\n\n    def get_candidate(self, state_j, key_j, inputs, U, V, W, U_bias):\n        """"""\n        Represents the new memory candidate that will be weighted by the\n        gate value and combined with the existing memory. Equation 3:\n\n        h_j^~ <- \\phi(U h_j + V w_j + W s_t)\n        """"""\n        key_V = tf.matmul(key_j, V)\n        state_U = tf.matmul(state_j, U) + U_bias\n        inputs_W = tf.matmul(inputs, W)\n        return self._activation(state_U + inputs_W + key_V)\n\n    def __call__(self, inputs, state, scope=None):\n        with tf.variable_scope(scope or type(self).__name__, initializer=self._initializer):\n            U = tf.get_variable(\'U\', [self._num_units_per_block, self._num_units_per_block],\n                                initializer=self._recurrent_initializer)\n            V = tf.get_variable(\'V\', [self._num_units_per_block, self._num_units_per_block],\n                                initializer=self._recurrent_initializer)\n            W = tf.get_variable(\'W\', [self._num_units_per_block, self._num_units_per_block],\n                                initializer=self._recurrent_initializer)\n\n            U_bias = tf.get_variable(\'U_bias\', [self._num_units_per_block])\n\n            # Split the hidden state into blocks (each U, V, W are shared across blocks).\n            state = tf.split(state, self._num_blocks, axis=1)\n\n            next_states = []\n            for j, state_j in enumerate(state): # Hidden State (j)\n                key_j = tf.expand_dims(self._keys[j], axis=0)\n                gate_j = self.get_gate(state_j, key_j, inputs)\n                candidate_j = self.get_candidate(state_j, key_j, inputs, U, V, W, U_bias)\n\n                # Equation 4: h_j <- h_j + g_j * h_j^~\n                # Perform an update of the hidden state (memory).\n                state_j_next = state_j + tf.expand_dims(gate_j, -1) * candidate_j\n\n                # Equation 5: h_j <- h_j / \\norm{h_j}\n                # Forget previous memories by normalization.\n                state_j_next_norm = tf.norm(\n                    tensor=state_j_next,\n                    ord=\'euclidean\',\n                    axis=-1,\n                    keep_dims=True)\n                state_j_next_norm = tf.where(\n                    tf.greater(state_j_next_norm, 0.0),\n                    state_j_next_norm,\n                    tf.ones_like(state_j_next_norm))\n                state_j_next = state_j_next / state_j_next_norm\n\n                next_states.append(state_j_next)\n            state_next = tf.concat(next_states, axis=1)\n        return state_next, state_next\n'"
entity_networks/experiment.py,7,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport json\nimport tensorflow as tf\n\nfrom entity_networks.hooks import EarlyStoppingHook\nfrom entity_networks.inputs import generate_input_fn\nfrom entity_networks.serving import generate_serving_input_fn\nfrom entity_networks.model import model_fn\n\nBATCH_SIZE = 32\nNUM_BLOCKS = 20\nEMBEDDING_SIZE = 100\nCLIP_GRADIENTS = 40.0\n\ndef generate_experiment_fn(data_dir, dataset_id, num_epochs,\n                           learning_rate_min, learning_rate_max,\n                           learning_rate_step_size, gradient_noise_scale):\n    ""Return _experiment_fn for use with learn_runner.""\n    def _experiment_fn(output_dir):\n        metadata_path = os.path.join(data_dir, \'{}_10k.json\'.format(dataset_id))\n        with tf.gfile.Open(metadata_path) as metadata_file:\n            metadata = json.load(metadata_file)\n\n        train_filename = os.path.join(data_dir, \'{}_10k_{}.tfrecords\'.format(dataset_id, \'train\'))\n        eval_filename = os.path.join(data_dir, \'{}_10k_{}.tfrecords\'.format(dataset_id, \'test\'))\n\n        train_input_fn = generate_input_fn(\n            filename=train_filename,\n            metadata=metadata,\n            batch_size=BATCH_SIZE,\n            num_epochs=num_epochs,\n            shuffle=True)\n\n        eval_input_fn = generate_input_fn(\n            filename=eval_filename,\n            metadata=metadata,\n            batch_size=BATCH_SIZE,\n            num_epochs=1,\n            shuffle=False)\n\n        vocab_size = metadata[\'vocab_size\']\n        task_size = metadata[\'task_size\']\n        train_steps_per_epoch = task_size // BATCH_SIZE\n\n        run_config = tf.contrib.learn.RunConfig(\n            save_summary_steps=train_steps_per_epoch,\n            save_checkpoints_steps=5 * train_steps_per_epoch,\n            save_checkpoints_secs=None)\n\n        params = {\n            \'vocab_size\': vocab_size,\n            \'embedding_size\': EMBEDDING_SIZE,\n            \'num_blocks\': NUM_BLOCKS,\n            \'learning_rate_min\': learning_rate_min,\n            \'learning_rate_max\': learning_rate_max,\n            \'learning_rate_step_size\': learning_rate_step_size * train_steps_per_epoch,\n            \'clip_gradients\': CLIP_GRADIENTS,\n            \'gradient_noise_scale\': gradient_noise_scale,\n        }\n\n        estimator = tf.contrib.learn.Estimator(\n            model_dir=output_dir,\n            model_fn=model_fn,\n            config=run_config,\n            params=params)\n\n        eval_metrics = {\n            \'accuracy\': tf.contrib.learn.MetricSpec(\n                metric_fn=tf.contrib.metrics.streaming_accuracy)\n        }\n\n        train_monitors = [\n            EarlyStoppingHook(\n                input_fn=eval_input_fn,\n                estimator=estimator,\n                metrics=eval_metrics,\n                metric_name=\'accuracy\',\n                every_steps=5 * train_steps_per_epoch,\n                max_patience=50 * train_steps_per_epoch,\n                minimize=False)\n        ]\n\n        serving_input_fn = generate_serving_input_fn(metadata)\n        export_strategy = tf.contrib.learn.utils.make_export_strategy(\n            serving_input_fn)\n\n        experiment = tf.contrib.learn.Experiment(\n            estimator=estimator,\n            train_input_fn=train_input_fn,\n            eval_input_fn=eval_input_fn,\n            eval_metrics=eval_metrics,\n            train_monitors=train_monitors,\n            train_steps=None,\n            eval_steps=None,\n            export_strategies=[export_strategy],\n            min_eval_frequency=100)\n        return experiment\n\n    return _experiment_fn\n'"
entity_networks/hooks.py,3,"b""from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom tensorflow.python.training import basic_session_run_hooks\n\nclass EarlyStoppingHook(tf.train.SessionRunHook):\n\n    def __init__(self, input_fn, estimator, metrics,\n                 metric_name='loss', every_steps=100,\n                 max_patience=100, minimize=True):\n        self._input_fn = input_fn\n        self._estimator = estimator\n        self._metrics = metrics\n\n        self._metric_name = metric_name\n        self._every_steps = every_steps\n        self._max_patience = max_patience\n        self._minimize = minimize\n\n        self._timer = basic_session_run_hooks.SecondOrStepTimer(\n            every_steps=every_steps,\n            every_secs=None)\n\n        self._global_step = None\n        self._best_value = None\n        self._best_step = None\n\n    def begin(self):\n        self._global_step = tf.train.get_global_step()\n        if self._global_step is None:\n            raise RuntimeError('Global step should be created to use EarlyStoppingHook.')\n\n    def before_run(self, run_context):\n        return tf.train.SessionRunArgs(self._global_step)\n\n    def after_run(self, run_context, run_values):\n        global_step = run_values.results\n\n        if not self._timer.should_trigger_for_step(global_step):\n            return\n\n        self._timer.update_last_triggered_step(global_step)\n\n        results = self._estimator.evaluate(\n            input_fn=self._input_fn,\n            metrics=self._metrics)\n\n        if self._metric_name not in results:\n            raise ValueError('Metric {} missing from outputs {}.' \\\n                .format(self._metric_name, set(results.keys())))\n\n        current_value = results[self._metric_name]\n\n        if (self._best_value is None) or \\\n           (self._minimize and current_value < self._best_value) or \\\n           (not self._minimize and current_value > self._best_value):\n            self._best_value = current_value\n            self._best_step = global_step\n\n        should_stop = (global_step - self._best_step >= self._max_patience)\n        if should_stop:\n            print('Stopping... Best step: {} with {} = {}.' \\\n                .format(self._best_step, self._metric_name, self._best_value))\n            run_context.request_stop()\n"""
entity_networks/inputs.py,8,"b'""""""\nModule responsible for input data.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\ndef generate_input_fn(filename, metadata, batch_size, num_epochs=None, shuffle=False):\n    ""Return _input_fn for use with Experiment.""\n    def _input_fn():\n        max_story_length = metadata[\'max_story_length\']\n        max_sentence_length = metadata[\'max_sentence_length\']\n        max_query_length = metadata[\'max_query_length\']\n\n        with tf.device(\'/cpu:0\'):\n            story_feature = tf.FixedLenFeature(\n                shape=[max_story_length, max_sentence_length],\n                dtype=tf.int64)\n            query_feature = tf.FixedLenFeature(\n                shape=[1, max_query_length],\n                dtype=tf.int64)\n            answer_feature = tf.FixedLenFeature(\n                shape=[],\n                dtype=tf.int64)\n\n            features = {\n                \'story\': story_feature,\n                \'query\': query_feature,\n                \'answer\': answer_feature,\n            }\n\n            record_features = tf.contrib.learn.read_batch_record_features(\n                file_pattern=filename,\n                features=features,\n                batch_size=batch_size,\n                randomize_input=shuffle,\n                num_epochs=num_epochs)\n\n            story = record_features[\'story\']\n            query = record_features[\'query\']\n            answer = record_features[\'answer\']\n\n            features = {\n                \'story\': story,\n                \'query\': query,\n            }\n\n            return features, answer\n\n    return _input_fn\n'"
entity_networks/main.py,1,"b'""Training task script.""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport argparse\nimport tensorflow as tf\n\nfrom tensorflow.contrib.learn.python.learn import learn_runner\n\nfrom entity_networks.experiment import generate_experiment_fn\n\ndef main():\n    ""Entrypoint for training.""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \'--data-dir\',\n        help=\'Directory containing data\',\n        default=\'data/babi/records/\')\n    parser.add_argument(\n        \'--dataset-id\',\n        help=\'Unique id identifying dataset\',\n        required=True)\n    parser.add_argument(\n        \'--job-dir\',\n        help=\'Location to write checkpoints, summaries, and export models\',\n        required=True)\n    parser.add_argument(\n        \'--num-epochs\',\n        help=\'Maximum number of epochs on which to train\',\n        default=200,\n        type=int)\n    parser.add_argument(\n        \'--lr-min\',\n        help=\'Minimum learning rate\',\n        default=2e-4,\n        type=float)\n    parser.add_argument(\n        \'--lr-max\',\n        help=\'Maximum learning rate\',\n        default=1e-2,\n        type=float)\n    parser.add_argument(\n        \'--lr-step-size\',\n        help=\'Learning rate step size (in epochs)\',\n        default=10,\n        type=int)\n    parser.add_argument(\n        \'--grad-noise\',\n        help=\'Gradient noise scale\',\n        default=0.005,\n        type=float)\n\n    args = parser.parse_args()\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    experiment_fn = generate_experiment_fn(\n        data_dir=args.data_dir,\n        dataset_id=args.dataset_id,\n        num_epochs=args.num_epochs,\n        learning_rate_min=args.lr_min,\n        learning_rate_max=args.lr_max,\n        learning_rate_step_size=args.lr_step_size,\n        gradient_noise_scale=args.grad_noise)\n    learn_runner.run(experiment_fn, args.job_dir)\n\nif __name__ == \'__main__\':\n    main()\n'"
entity_networks/model.py,38,"b'""""""\nDefine the recurrent entity network model.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom functools import partial\n\nimport tensorflow as tf\n\nfrom entity_networks.dynamic_memory_cell import DynamicMemoryCell\nfrom entity_networks.model_ops import cyclic_learning_rate, \\\n                                      get_sequence_length, \\\n                                      count_parameters, \\\n                                      prelu\n\nOPTIMIZER_SUMMARIES = [\n    ""learning_rate"",\n    ""loss"",\n    ""gradients"",\n    ""gradient_norm"",\n]\n\ndef get_input_encoding(inputs, initializer=None, scope=None):\n    """"""\n    Implementation of the learned multiplicative mask from Section 2.1, Equation 1.\n    This module is also described in [End-To-End Memory Networks](https://arxiv.org/abs/1502.01852)\n    as Position Encoding (PE). The mask allows the ordering of words in a sentence to affect the\n    encoding.\n    """"""\n    with tf.variable_scope(scope, \'Encoding\', initializer=initializer):\n        _, _, max_sentence_length, embedding_size = inputs.get_shape().as_list()\n        positional_mask = tf.get_variable(\n            name=\'positional_mask\',\n            shape=[max_sentence_length, embedding_size])\n        encoded_input = tf.reduce_sum(inputs * positional_mask, axis=2)\n        return encoded_input\n\ndef get_output_module(\n        last_state,\n        encoded_query,\n        num_blocks,\n        vocab_size,\n        activation=tf.nn.relu,\n        initializer=None,\n        scope=None):\n    """"""\n    Implementation of Section 2.3, Equation 6. This module is also described in more detail here:\n    [End-To-End Memory Networks](https://arxiv.org/abs/1502.01852).\n    """"""\n    with tf.variable_scope(scope, \'Output\', initializer=initializer):\n        last_state = tf.stack(tf.split(last_state, num_blocks, axis=1), axis=1)\n        _, _, embedding_size = last_state.get_shape().as_list()\n\n        # Use the encoded_query to attend over memories\n        # (hidden states of dynamic last_state cell blocks)\n        attention = tf.reduce_sum(last_state * encoded_query, axis=2)\n\n        # Subtract max for numerical stability (softmax is shift invariant)\n        attention_max = tf.reduce_max(attention, axis=-1, keep_dims=True)\n        attention = tf.nn.softmax(attention - attention_max)\n        attention = tf.expand_dims(attention, axis=2)\n\n        # Weight memories by attention vectors\n        u = tf.reduce_sum(last_state * attention, axis=1)\n\n        # R acts as the decoder matrix to convert from internal state to the output vocabulary size\n        R = tf.get_variable(\'R\', [embedding_size, vocab_size])\n        H = tf.get_variable(\'H\', [embedding_size, embedding_size])\n\n        q = tf.squeeze(encoded_query, axis=1)\n        y = tf.matmul(activation(q + tf.matmul(u, H)), R)\n        return y\n    outputs = None\n    return outputs\n\ndef get_outputs(inputs, params):\n    ""Return the outputs from the model which will be used in the loss function.""\n    embedding_size = params[\'embedding_size\']\n    num_blocks = params[\'num_blocks\']\n    vocab_size = params[\'vocab_size\']\n\n    story = inputs[\'story\']\n    query = inputs[\'query\']\n\n    batch_size = tf.shape(story)[0]\n\n    normal_initializer = tf.random_normal_initializer(stddev=0.1)\n    ones_initializer = tf.constant_initializer(1.0)\n\n    # Extend the vocab to include keys for the dynamic memory cell,\n    # allowing the initialization of the memory to be learned.\n    vocab_size = vocab_size + num_blocks\n\n    with tf.variable_scope(\'EntityNetwork\', initializer=normal_initializer):\n        # PReLU activations have their alpha parameters initialized to 1\n        # so they may be identity before training.\n        alpha = tf.get_variable(\n            name=\'alpha\',\n            shape=embedding_size,\n            initializer=ones_initializer)\n        activation = partial(prelu, alpha=alpha)\n\n        # Embeddings\n        embedding_params = tf.get_variable(\n            name=\'embedding_params\',\n            shape=[vocab_size, embedding_size])\n\n        # The embedding mask forces the special ""pad"" embedding to zeros.\n        embedding_mask = tf.constant(\n            value=[0 if i == 0 else 1 for i in range(vocab_size)],\n            shape=[vocab_size, 1],\n            dtype=tf.float32)\n        embedding_params_masked = embedding_params * embedding_mask\n\n        story_embedding = tf.nn.embedding_lookup(embedding_params_masked, story)\n        query_embedding = tf.nn.embedding_lookup(embedding_params_masked, query)\n\n        # Input Module\n        encoded_story = get_input_encoding(\n            inputs=story_embedding,\n            initializer=ones_initializer,\n            scope=\'StoryEncoding\')\n        encoded_query = get_input_encoding(\n            inputs=query_embedding,\n            initializer=ones_initializer,\n            scope=\'QueryEncoding\')\n\n        # Memory Module\n        # We define the keys outside of the cell so they may be used for memory initialization.\n        # Keys are initialized to a range outside of the main vocab.\n        keys = [key for key in range(vocab_size - num_blocks, vocab_size)]\n        keys = tf.nn.embedding_lookup(embedding_params_masked, keys)\n        keys = tf.split(keys, num_blocks, axis=0)\n        keys = [tf.squeeze(key, axis=0) for key in keys]\n\n        cell = DynamicMemoryCell(\n            num_blocks=num_blocks,\n            num_units_per_block=embedding_size,\n            keys=keys,\n            initializer=normal_initializer,\n            recurrent_initializer=normal_initializer,\n            activation=activation)\n\n        # Recurrence\n        initial_state = cell.zero_state(batch_size, tf.float32)\n        sequence_length = get_sequence_length(encoded_story)\n        _, last_state = tf.nn.dynamic_rnn(\n            cell=cell,\n            inputs=encoded_story,\n            sequence_length=sequence_length,\n            initial_state=initial_state)\n\n        # Output Module\n        outputs = get_output_module(\n            last_state=last_state,\n            encoded_query=encoded_query,\n            num_blocks=num_blocks,\n            vocab_size=vocab_size,\n            initializer=normal_initializer,\n            activation=activation)\n\n        parameters = count_parameters()\n        print(\'Parameters: {}\'.format(parameters))\n\n        return outputs\n\ndef get_predictions(outputs):\n    ""Return the actual predictions for use with evaluation metrics or TF Serving.""\n    predictions = tf.argmax(outputs, axis=-1)\n    return predictions\n\ndef get_loss(outputs, labels, mode):\n    ""Return the loss function which will be used with an optimizer.""\n\n    loss = None\n    if mode == tf.contrib.learn.ModeKeys.INFER:\n        return loss\n\n    loss = tf.losses.sparse_softmax_cross_entropy(\n        logits=outputs,\n        labels=labels)\n    return loss\n\ndef get_train_op(loss, params, mode):\n    ""Return the trainining operation which will be used to train the model.""\n\n    train_op = None\n    if mode != tf.contrib.learn.ModeKeys.TRAIN:\n        return train_op\n\n    global_step = tf.contrib.framework.get_or_create_global_step()\n\n    learning_rate = cyclic_learning_rate(\n        learning_rate_min=params[\'learning_rate_min\'],\n        learning_rate_max=params[\'learning_rate_max\'],\n        step_size=params[\'learning_rate_step_size\'],\n        mode=\'triangular\',\n        global_step=global_step)\n    tf.summary.scalar(\'learning_rate\', learning_rate)\n\n    train_op = tf.contrib.layers.optimize_loss(\n        loss=loss,\n        global_step=global_step,\n        learning_rate=learning_rate,\n        optimizer=\'Adam\',\n        clip_gradients=params[\'clip_gradients\'],\n        gradient_noise_scale=params[\'gradient_noise_scale\'],\n        summaries=OPTIMIZER_SUMMARIES)\n\n    return train_op\n\ndef model_fn(features, labels, mode, params):\n    ""Return ModelFnOps for use with Estimator.""\n\n    outputs = get_outputs(features, params)\n    predictions = get_predictions(outputs)\n    loss = get_loss(outputs, labels, mode)\n    train_op = get_train_op(loss, params, mode)\n\n    return tf.contrib.learn.ModelFnOps(\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        mode=mode)\n'"
entity_networks/model_ops.py,11,"b'""Utilities for model construction.""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport math\nimport numpy as np\nimport tensorflow as tf\n\ndef count_parameters():\n    ""Count the number of parameters listed under TRAINABLE_VARIABLES.""\n    num_parameters = sum([np.prod(tvar.get_shape().as_list())\n                          for tvar in tf.trainable_variables()])\n    return num_parameters\n\ndef get_sequence_length(sequence, scope=None):\n    ""Determine the length of a sequence that has been padded with zeros.""\n    with tf.variable_scope(scope, \'SequenceLength\'):\n        used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=[-1]))\n        length = tf.cast(tf.reduce_sum(used, reduction_indices=[-1]), tf.int32)\n        return length\n\ndef cyclic_learning_rate(\n        learning_rate_min,\n        learning_rate_max,\n        step_size,\n        global_step,\n        mode=\'triangular\',\n        scope=None):\n    with tf.variable_scope(scope, \'CyclicLearningRate\'):\n        cycle = tf.floor(1 + tf.to_float(global_step) / (2 * step_size))\n\n        if mode == \'triangular\':\n            scale = 1\n        elif mode == \'triangular2\':\n            scale = 2**(cycle - 1)\n        else:\n            raise ValueError(\'Unrecognized mode: {}\'.format(mode))\n\n        x = tf.abs(tf.to_float(global_step) / step_size - 2 * cycle + 1)\n        lr = learning_rate_min + (learning_rate_max - learning_rate_min) * \\\n            tf.maximum(0.0, 1 - x) / scale\n\n        return lr\n\ndef prelu(features, alpha, scope=None):\n    """"""\n    Implementation of [Parametric ReLU](https://arxiv.org/abs/1502.01852) borrowed from Keras.\n    """"""\n    with tf.variable_scope(scope, \'PReLU\'):\n        pos = tf.nn.relu(features)\n        neg = alpha * (features - tf.abs(features)) * 0.5\n        return pos + neg\n'"
entity_networks/prep_data.py,11,"b'""""""\nLoads and pre-processes a bAbI dataset into TFRecords.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport re\nimport json\nimport tarfile\nimport tensorflow as tf\n\nfrom tqdm import tqdm\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'source_path\',\n    \'data/babi_tasks_data_1_20_v1.2.tar.gz\',\n    \'Tar containing bAbI sources.\')\ntf.app.flags.DEFINE_string(\'output_dir\', \'data/records/\', \'Dataset destination.\')\ntf.app.flags.DEFINE_boolean(\'only_1k\', False, \'Whether to use bAbI 1k or bAbI 10k (default).\')\n\nSPLIT_RE = re.compile(r\'(\\W+)?\')\n\nPAD_TOKEN = \'_PAD\'\nPAD_ID = 0\n\ndef tokenize(sentence):\n    ""Tokenize a string by splitting on non-word characters and stripping whitespace.""\n    return [token.strip().lower() for token in re.split(SPLIT_RE, sentence) if token.strip()]\n\ndef parse_stories(lines, only_supporting=False):\n    """"""\n    Parse the bAbI task format described here: https://research.facebook.com/research/babi/\n    If only_supporting is True, only the sentences that support the answer are kept.\n    """"""\n    stories = []\n    story = []\n    for line in lines:\n        line = line.decode(\'utf-8\').strip()\n        nid, line = line.split(\' \', 1)\n        nid = int(nid)\n        if nid == 1:\n            story = []\n        if \'\\t\' in line:\n            query, answer, supporting = line.split(\'\\t\')\n            query = tokenize(query)\n            substory = None\n            if only_supporting:\n                # Only select the related substory\n                supporting = map(int, supporting.split())\n                substory = [story[i - 1] for i in supporting]\n            else:\n                # Provide all the substories\n                substory = [x for x in story if x]\n            stories.append((substory, query, answer))\n            story.append(\'\')\n        else:\n            sentence = tokenize(line)\n            story.append(sentence)\n    return stories\n\ndef save_dataset(stories, path):\n    """"""\n    Save the stories into TFRecords.\n\n    NOTE: Since each sentence is a consistent length from padding, we use\n    `tf.train.Example`, rather than a `tf.train.SequenceExample`, which is\n    _slightly_ faster.\n    """"""\n    writer = tf.python_io.TFRecordWriter(path)\n    for story, query, answer in stories:\n        story_flat = [token_id for sentence in story for token_id in sentence]\n\n        story_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=story_flat))\n        query_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=query))\n        answer_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[answer]))\n\n        features = tf.train.Features(feature={\n            \'story\': story_feature,\n            \'query\': query_feature,\n            \'answer\': answer_feature,\n        })\n\n        example = tf.train.Example(features=features)\n        writer.write(example.SerializeToString())\n    writer.close()\n\ndef tokenize_stories(stories, token_to_id):\n    ""Convert all tokens into their unique ids.""\n    story_ids = []\n    for story, query, answer in stories:\n        story = [[token_to_id[token] for token in sentence] for sentence in story]\n        query = [token_to_id[token] for token in query]\n        answer = token_to_id[answer]\n        story_ids.append((story, query, answer))\n    return story_ids\n\ndef get_tokenizer(stories):\n    ""Recover unique tokens as a vocab and map the tokens to ids.""\n    tokens_all = []\n    for story, query, answer in stories:\n        tokens_all.extend([token for sentence in story for token in sentence] + query + [answer])\n    vocab = [PAD_TOKEN] + sorted(set(tokens_all))\n    token_to_id = {token: i for i, token in enumerate(vocab)}\n    return vocab, token_to_id\n\ndef pad_stories(stories, max_sentence_length, max_story_length, max_query_length):\n    ""Pad sentences, stories, and queries to a consistence length.""\n    for story, query, _ in stories:\n        for sentence in story:\n            for _ in range(max_sentence_length - len(sentence)):\n                sentence.append(PAD_ID)\n            assert len(sentence) == max_sentence_length\n\n        for _ in range(max_story_length - len(story)):\n            story.append([PAD_ID for _ in range(max_sentence_length)])\n\n        for _ in range(max_query_length - len(query)):\n            query.append(PAD_ID)\n\n        assert len(story) == max_story_length\n        assert len(query) == max_query_length\n\n    return stories\n\ndef truncate_stories(stories, max_length):\n    ""Truncate a story to the specified maximum length.""\n    stories_truncated = []\n    for story, query, answer in stories:\n        story_truncated = story[-max_length:]\n        stories_truncated.append((story_truncated, query, answer))\n    return stories_truncated\n\ndef main():\n    ""Main entrypoint.""\n\n    if not os.path.exists(FLAGS.output_dir):\n        os.makedirs(FLAGS.output_dir)\n\n    task_names = [\n        \'qa1_single-supporting-fact\',\n        \'qa2_two-supporting-facts\',\n        \'qa3_three-supporting-facts\',\n        \'qa4_two-arg-relations\',\n        \'qa5_three-arg-relations\',\n        \'qa6_yes-no-questions\',\n        \'qa7_counting\',\n        \'qa8_lists-sets\',\n        \'qa9_simple-negation\',\n        \'qa10_indefinite-knowledge\',\n        \'qa11_basic-coreference\',\n        \'qa12_conjunction\',\n        \'qa13_compound-coreference\',\n        \'qa14_time-reasoning\',\n        \'qa15_basic-deduction\',\n        \'qa16_basic-induction\',\n        \'qa17_positional-reasoning\',\n        \'qa18_size-reasoning\',\n        \'qa19_path-finding\',\n        \'qa20_agents-motivations\',\n    ]\n\n    task_titles = [\n        \'Task 1: Single Supporting Fact\',\n        \'Task 2: Two Supporting Facts\',\n        \'Task 3: Three Supporting Facts\',\n        \'Task 4: Two Argument Relations\',\n        \'Task 5: Three Argument Relations\',\n        \'Task 6: Yes/No Questions\',\n        \'Task 7: Counting\',\n        \'Task 8: Lists/Sets\',\n        \'Task 9: Simple Negation\',\n        \'Task 10: IndefiniteKnowledg\',\n        \'Task 11: Basic Coreference\',\n        \'Task 12: Conjunction\',\n        \'Task 13: Compound Coreference\',\n        \'Task 14: Time Reasoning\',\n        \'Task 15: Basic Deduction\',\n        \'Task 16: Basic Induction\',\n        \'Task 17: Positional Reasoning\',\n        \'Task 18: Size Reasoning\',\n        \'Task 19: Path Finding\',\n        \'Task 20: Agent Motivations\',\n    ]\n\n    task_ids = [\n        \'qa1\',\n        \'qa2\',\n        \'qa3\',\n        \'qa4\',\n        \'qa5\',\n        \'qa6\',\n        \'qa7\',\n        \'qa8\',\n        \'qa9\',\n        \'qa10\',\n        \'qa11\',\n        \'qa12\',\n        \'qa13\',\n        \'qa14\',\n        \'qa15\',\n        \'qa16\',\n        \'qa17\',\n        \'qa18\',\n        \'qa19\',\n        \'qa20\',\n    ]\n\n    for task_id, task_name, task_title in tqdm(zip(task_ids, task_names, task_titles), \\\n            desc=\'Processing datasets into records...\'):\n        if FLAGS.only_1k:\n            stories_path_train = os.path.join(\'tasks_1-20_v1-2/en/\', task_name + \'_train.txt\')\n            stories_path_test = os.path.join(\'tasks_1-20_v1-2/en/\', task_name + \'_test.txt\')\n            dataset_path_train = os.path.join(FLAGS.output_dir, task_id + \'_1k_train.tfrecords\')\n            dataset_path_test = os.path.join(FLAGS.output_dir, task_id + \'_1k_test.tfrecords\')\n            metadata_path = os.path.join(FLAGS.output_dir, task_id + \'_1k.json\')\n            task_size = 1000\n        else:\n            stories_path_train = os.path.join(\'tasks_1-20_v1-2/en-10k/\', task_name + \'_train.txt\')\n            stories_path_test = os.path.join(\'tasks_1-20_v1-2/en-10k/\', task_name + \'_test.txt\')\n            dataset_path_train = os.path.join(FLAGS.output_dir, task_id + \'_10k_train.tfrecords\')\n            dataset_path_test = os.path.join(FLAGS.output_dir, task_id + \'_10k_test.tfrecords\')\n            metadata_path = os.path.join(FLAGS.output_dir, task_id + \'_10k.json\')\n            task_size = 10000\n\n        # From the entity networks paper:\n        # > Copying previous works (Sukhbaatar et al., 2015; Xiong et al., 2016),\n        # > the capacity of the memory was limited to the most recent 70 sentences,\n        # > except for task 3 which was limited to 130 sentences.\n        if task_id == \'qa3\':\n            truncated_story_length = 130\n        else:\n            truncated_story_length = 70\n\n        tar = tarfile.open(FLAGS.source_path)\n\n        f_train = tar.extractfile(stories_path_train)\n        f_test = tar.extractfile(stories_path_test)\n\n        stories_train = parse_stories(f_train.readlines())\n        stories_test = parse_stories(f_test.readlines())\n\n        stories_train = truncate_stories(stories_train, truncated_story_length)\n        stories_test = truncate_stories(stories_test, truncated_story_length)\n\n        vocab, token_to_id = get_tokenizer(stories_train + stories_test)\n        vocab_size = len(vocab)\n\n        stories_token_train = tokenize_stories(stories_train, token_to_id)\n        stories_token_test = tokenize_stories(stories_test, token_to_id)\n        stories_token_all = stories_token_train + stories_token_test\n\n        story_lengths = [len(sentence) for story, _, _ in stories_token_all for sentence in story]\n        max_sentence_length = max(story_lengths)\n        max_story_length = max([len(story) for story, _, _ in stories_token_all])\n        max_query_length = max([len(query) for _, query, _ in stories_token_all])\n\n        with open(metadata_path, \'w\') as f:\n            metadata = {\n                \'task_id\': task_id,\n                \'task_name\': task_name,\n                \'task_title\': task_title,\n                \'task_size\': task_size,\n                \'max_query_length\': max_query_length,\n                \'max_story_length\': max_story_length,\n                \'max_sentence_length\': max_sentence_length,\n                \'vocab\': vocab,\n                \'vocab_size\': vocab_size,\n                \'filenames\': {\n                    \'train\': os.path.basename(dataset_path_train),\n                    \'test\': os.path.basename(dataset_path_test),\n                }\n            }\n            json.dump(metadata, f)\n\n        stories_pad_train = pad_stories(stories_token_train, \\\n            max_sentence_length, max_story_length, max_query_length)\n        stories_pad_test = pad_stories(stories_token_test, \\\n            max_sentence_length, max_story_length, max_query_length)\n\n        save_dataset(stories_pad_train, dataset_path_train)\n        save_dataset(stories_pad_test, dataset_path_test)\n\nif __name__ == \'__main__\':\n    main()\n'"
entity_networks/serving.py,6,"b'""""""\nServing input function definition.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\n\ndef generate_serving_input_fn(metadata):\n    ""Returns _serving_input_fn for use with an export strategy.""\n    max_story_length = metadata[\'max_story_length\']\n    max_sentence_length = metadata[\'max_sentence_length\']\n    max_query_length = metadata[\'max_query_length\']\n\n    def _serving_input_fn():\n        story_placeholder = tf.placeholder(\n            shape=[max_story_length, max_sentence_length],\n            dtype=tf.int64,\n            name=\'story\')\n        query_placeholder = tf.placeholder(\n            shape=[1, max_query_length],\n            dtype=tf.int64,\n            name=\'query\')\n\n        feature_placeholders = {\n            \'story\': story_placeholder,\n            \'query\': query_placeholder\n        }\n\n        features = {\n            key: tf.expand_dims(tensor, axis=0)\n            for key, tensor in feature_placeholders.items()\n        }\n\n        input_fn_ops = tf.contrib.learn.utils.input_fn_utils.InputFnOps(\n            features=features,\n            labels=None,\n            default_inputs=feature_placeholders)\n\n        return input_fn_ops\n\n    return _serving_input_fn\n'"
