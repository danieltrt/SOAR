file_path,api_count,code
main.py,0,"b'# Copyright 2017 Jin Fagang. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =======================================================================\nimport argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Shakespeare Drama Composer.\')\n\n    help_ = ""set -t to compose type, support \'s\' for shakespeare, \'f\' for fiction ""\n    parser.add_argument(\'-t\', \'--type\', default=\'s\', choices=[\'s\', \'f\'], help=help_)\n\n    help_ = \'choose to train or generate.\'\n    parser.add_argument(\'--train\', dest=\'train\', action=\'store_true\', help=help_)\n    parser.add_argument(\'--no-train\', dest=\'train\', action=\'store_false\', help=help_)\n    parser.set_defaults(train=True)\n\n    args_ = parser.parse_args()\n    return args_\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    if args.type == \'s\':\n        from inference import shakespeare\n        if args.train:\n            shakespeare.main(True)\n        else:\n            shakespeare.main(False)\n    elif args.type == \'f\':\n        from inference import fiction\n        if args.train:\n            fiction.main(True)\n        else:\n            fiction.main(False)\n    else:\n        print(\'[INFO] write option can only be poem or lyric right now.\')'"
datasets/fiction.py,0,"b'# Copyright 2017 Jin Fagang. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =======================================================================\n""""""\nprocess Shakespeare corpora and generate RNN model need batched data\n""""""\nimport numpy as np\nfrom utils.corpus_cleaner_cn import CorpusCleanerCN\nimport jieba\nimport collections\n\n\nclass Fiction(object):\n    def __init__(self, batch_size, n_steps):\n        self.corpora_file_path = \'./datasets/Fiction/fiction\'\n        self.raw_corpus = self.process_raw_data()\n        self.batch_size = batch_size\n        self.n_steps = n_steps\n\n        self.vocab_size, self.vocab_int_map, self.data = self.prepare_data()\n        self.n_chunks = self.data.shape[0] // batch_size\n        self.batch_index = 0\n\n    def prepare_data(self):\n        corpus_cut = np.array([jieba.lcut(s) for s in self.raw_corpus])\n        vocabs = []\n        for l in corpus_cut:\n            for i in l:\n                vocabs.append(i)\n        # vocabs = reduce(lambda x, y: x+y, corpus_cut)\n        # count every vocab frequency\n        # but currently we don\'t think about the \'most\' frequent one, just let it go\n        counter = collections.Counter(vocabs)\n        counter = counter.most_common()\n        vocabs_set, _ = zip(*counter)\n        vocab_int_map = {vocab: index for index, vocab in enumerate(vocabs_set)}\n\n        data_flatten = np.array([vocab_int_map[v] for v in vocabs])\n        data = np.array([data_flatten[i: i+self.n_steps+1] for i in range(data_flatten.shape[0] // (self.n_steps + 1))])\n        # let\'s shuffle data to see anything happens\n        np.random.shuffle(data)\n        return len(vocabs_set), vocab_int_map, data\n\n    def next_batch(self):\n        next_batch = self.data[self.batch_index: self.batch_index + self.batch_size]\n        self.batch_index += 1\n        if self.batch_index == self.n_chunks:\n            self.batch_index = 0\n\n        batch_x = next_batch[:, 0: self.n_steps]\n        batch_y = next_batch[:, -1]\n        return batch_x, batch_y\n\n    def process_raw_data(self):\n        cleaner = CorpusCleanerCN()\n        raw = cleaner.clean(self.corpora_file_path, clean_level=\'normal\', min_drop=5, is_save=True)\n        return raw\n'"
datasets/shakespeare.py,0,"b'# Copyright 2017 Jin Fagang. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =======================================================================\n""""""\nprocess Shakespeare corpora and generate RNN model need batched data\n""""""\nimport numpy as np\nfrom utils.corpus_cleaner_cn import CorpusCleanerCN\nimport jieba\nimport collections\n\n\nclass Shakespeare(object):\n    def __init__(self, batch_size, n_steps):\n        self.corpora_file_path = \'./datasets/Shakespeare/Shakespeare_cn.txt\'\n        self.raw_corpus = self.process_raw_data()\n        self.batch_size = batch_size\n        self.n_steps = n_steps\n\n        self.vocab_size, self.vocab_int_map, self.data = self.prepare_data()\n        self.n_chunks = self.data.shape[0] // batch_size\n        self.batch_index = 0\n\n    def prepare_data(self):\n        corpus_cut = np.array([jieba.lcut(s) for s in self.raw_corpus])\n        vocabs = []\n        for l in corpus_cut:\n            for i in l:\n                vocabs.append(i)\n        # vocabs = reduce(lambda x, y: x+y, corpus_cut)\n        # count every vocab frequency\n        # but currently we don\'t think about the \'most\' frequent one, just let it go\n        counter = collections.Counter(vocabs)\n        counter = counter.most_common()\n        vocabs_set, _ = zip(*counter)\n        vocab_int_map = {vocab: index for index, vocab in enumerate(vocabs_set)}\n\n        data_flatten = np.array([vocab_int_map[v] for v in vocabs])\n        #step=3\n        data = np.array([data_flatten[i: i+self.n_steps+1] for i in range(0,data_flatten.shape[0]-self.n_steps -1,3)])\n        # let\'s shuffle data to see anything happens\n        np.random.shuffle(data)\n        return len(vocabs_set), vocab_int_map, data\n\n    def next_batch(self):\n        next_batch = self.data[self.batch_index*self.batch_size: self.batch_index*self.batch_size + self.batch_size]\n        self.batch_index += 1\n        #if self.batch_index == self.n_chunks:\n         #   self.batch_index = 0\n\n        batch_x = next_batch[:, 0: self.n_steps]\n        batch_y = next_batch[:, -1]\n        return batch_x, batch_y\n\n    def process_raw_data(self):\n        cleaner = CorpusCleanerCN()\n        raw = cleaner.clean(self.corpora_file_path, clean_level=\'normal\', min_drop=5, is_save=True)\n        return raw\n\n'"
inference/fiction.py,19,"b'# Copyright 2017 Jin Fagang. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =======================================================================\nimport tensorflow as tf\nfrom models.rnn_model_no_state import RNNModel\nimport logging\nimport os\nimport numpy as np\nimport jieba\nimport pickle\n\nlogging.basicConfig(level=logging.CRITICAL,\n                    format=\'%(asctime)s %(filename)s line:%(lineno)d %(levelname)s %(message)s\',\n                    datefmt=\'%a, %d %b %Y %H:%M:%S\')\njieba.default_logger.disabled = True\n\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 360, \'batch size.\')\ntf.app.flags.DEFINE_integer(\'n_steps\', 6, \'length of inputs columns.\')\ntf.app.flags.DEFINE_integer(\'n_units\', 4, \'number units in rnn cell.\')\ntf.app.flags.DEFINE_integer(\'n_layers\', 2, \'number of layer of stack rnn model.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.001, \'learning rate.\')\n# set this to \'main.py\' relative path\ntf.app.flags.DEFINE_string(\'checkpoints_dir\', \'./checkpoints/fiction/\', \'checkpoints save path.\')\n\ntf.app.flags.DEFINE_string(\'model_prefix\', \'fiction\', \'model save prefix.\')\ntf.app.flags.DEFINE_integer(\'epochs\', 10050, \'train how many epochs.\')\ntf.app.flags.DEFINE_boolean(\'is_restore\', True, \'to restore from previous or not.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef running(is_train=True):\n    if not os.path.exists(FLAGS.checkpoints_dir):\n        os.makedirs(FLAGS.checkpoints_dir)\n\n    # get fiction and dump it into pkl\n    # fiction = Fiction(batch_size=FLAGS.batch_size, n_steps=FLAGS.n_steps)\n    with open(\'./datasets/Fiction/fiction.pkl\', \'rb\') as f:\n        fiction = pickle.load(f)\n    vocab_size = fiction.vocab_size\n\n    if is_train:\n        inputs = tf.placeholder(tf.int32, [FLAGS.batch_size, FLAGS.n_steps])\n        labels = tf.placeholder(tf.int32, [FLAGS.batch_size])\n    else:\n        inputs = tf.placeholder(tf.int32, [1, FLAGS.n_steps])\n        labels = None\n    model = RNNModel(inputs, labels, n_units=FLAGS.n_units, n_layers=FLAGS.n_layers,\n                     lr=FLAGS.learning_rate, vocab_size=vocab_size)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    saver = tf.train.Saver(max_to_keep=2)\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n\n        start_epoch = 0\n        checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoints_dir)\n        if FLAGS.is_restore:\n            if os.path.exists(checkpoint):\n                saver.restore(sess, checkpoint)\n                logging.info(""restore from the checkpoint {0}"".format(checkpoint))\n                start_epoch += int(checkpoint.split(\'-\')[-1])\n        if is_train:\n            logging.info(\'training start...\')\n            epoch = 0\n            try:\n                for epoch in range(start_epoch, FLAGS.epochs):\n                    for batch in range(fiction.n_chunks):\n                        batch_x, batch_y = fiction.next_batch()\n                        loss, _ = sess.run([model.loss, model.train_op],\n                                           feed_dict={inputs: batch_x, labels: batch_y})\n                        logging.info(\'epoch: %s,  batch: %s, loss: %s\' % (epoch, batch, loss))\n                    if epoch % 60 == 0:\n                        saver.save(sess, os.path.join(FLAGS.checkpoints_dir, FLAGS.model_prefix), global_step=epoch)\n                logging.info(\'optimization done! enjoy your Fiction composer!\')\n            except KeyboardInterrupt:\n                logging.info(\'interrupt manually, try saving checkpoint for now...\')\n                saver.save(sess, os.path.join(FLAGS.checkpoints_dir, FLAGS.model_prefix), global_step=epoch)\n                logging.info(\'last epoch were saved, next time will start from epoch {}.\'.format(epoch))\n        else:\n            logging.info(\'I am thinking to compose  Jin Yong fiction novel...\')\n            if checkpoint:\n                saver.restore(sess, checkpoint)\n                logging.info(""restore from the checkpoint {0}"".format(checkpoint))\n                start_epoch += int(checkpoint.split(\'-\')[-1])\n\n            vocab_int_map = fiction.vocab_int_map\n            start_sentence = input(\'please input a sentence in Chinese: \')\n            sent = process_sent(start_sentence, vocab_int_map, FLAGS.n_steps)\n\n            next_words = []\n            for i in range(2000):\n                outputs = sess.run([model.outputs], feed_dict={inputs: sent})\n                next_word_index = np.argmax(outputs)\n                next_words.append(next_word_index)\n                sent = np.append(sent, next_word_index)\n                sent = np.array([sent[1:]])\n            drama_text = [{v: k for k, v in vocab_int_map.items()}[i] for i in next_words]\n            drama_text.insert(0, start_sentence)\n            pretty_print(drama_text)\n\n\ndef process_sent(sent, vocab_int, steps):\n    """"""\n    this file token sentence and make it into numpy array, return a fixed length 2d array\n    :param sent: \n    :param vocab_int: \n    :param steps: \n    :return: \n    """"""\n    sent_list = jieba.lcut(sent)\n    # if words not in vocab dict then let this word be a random index which maybe other words\n    index_list = [vocab_int[i] if i in vocab_int.keys() else np.random.randint(0, 90) for i in sent_list]\n    if len(index_list) < steps:\n        index_list = np.hstack((index_list, np.random.randint(0, 90, steps - len(index_list))))\n    else:\n        index_list = index_list[0: steps]\n    return np.array([index_list])\n\n\ndef pretty_print(words_list):\n    """"""\n    print the words list sentence by sentence\n    :param words_list: \n    :return: \n    """"""\n    print(\'I am thinking to compose Jin Yong fiction, hand on a minute...\')\n    all_punctuations = [\'\xe3\x80\x82\', \'\xef\xbc\x9f\', \'\xef\xbc\x81\', \'\xef\xbc\x8c\', \',\']\n    enter_punctuations = [\'\xe3\x80\x82\', \'\xef\xbc\x9f\', \'\xef\xbc\x81\', \'.\']\n    token = \'TOKEN\'\n    add_token = [i+token if i in enter_punctuations else i for i in words_list]\n    split_token = \'\'.join(add_token).split(token)\n    drop_extra = [i for i in split_token if i not in enter_punctuations]\n    print(\'Here is what I got: \')\n    for i in drop_extra:\n        print(i)\n\n\ndef main(is_train):\n    running(is_train)\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
inference/shakespeare.py,20,"b'# Copyright 2017 Jin Fagang. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =======================================================================\nimport tensorflow as tf\nfrom models.rnn_model import RNNModel\nimport logging\nimport os\nimport numpy as np\nimport jieba\nimport pickle\n\nlogging.basicConfig(level=logging.CRITICAL,\n                    format=\'%(asctime)s %(filename)s line:%(lineno)d %(levelname)s %(message)s\',\n                    datefmt=\'%a, %d %b %Y %H:%M:%S\')\njieba.default_logger.disabled = True\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 260, \'batch size.\')\ntf.app.flags.DEFINE_integer(\'n_steps\', 9, \'length of inputs columns.\')\ntf.app.flags.DEFINE_integer(\'n_units\', 10, \'number units in rnn cell.\')\ntf.app.flags.DEFINE_integer(\'n_layers\', 5, \'number of layer of stack rnn model.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.001, \'learning rate.\')\n# set this to \'main.py\' relative path\ntf.app.flags.DEFINE_string(\'checkpoints_dir\', os.path.abspath(\'./checkpoints/shakespeare/\'), \'checkpoints save path.\')\ntf.app.flags.DEFINE_string(\'file_path\', os.path.abspath(\'./dataset/data/poems.txt\'), \'file name of poems.\')\n\ntf.app.flags.DEFINE_string(\'model_prefix\', \'shakespeare\', \'model save prefix.\')\ntf.app.flags.DEFINE_integer(\'epochs\', 1050, \'train how many epochs.\')\ntf.app.flags.DEFINE_boolean(\'is_restore\', True, \'to restore from previous or not.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef running(is_train=True):\n    if not os.path.exists(FLAGS.checkpoints_dir):\n        os.makedirs(FLAGS.checkpoints_dir)\n\n    # shakespeare = Shakespeare(batch_size=FLAGS.batch_size, n_steps=FLAGS.n_steps)\n    # with open(\'./datasets/Shakespeare/shakespeare.pkl\', \'wb\') as f:\n    #     pickle.dump(shakespeare, f)\n    with open(\'./datasets/Shakespeare/shakespeare.pkl\', \'rb\') as f:\n        shakespeare = pickle.load(f)\n    vocab_size = shakespeare.vocab_size\n\n    if is_train:\n        inputs = tf.placeholder(tf.int32, [FLAGS.batch_size, FLAGS.n_steps])\n        labels = tf.placeholder(tf.int32, [FLAGS.batch_size])\n    else:\n        inputs = tf.placeholder(tf.int32, [1, FLAGS.n_steps])\n        labels = None\n    model = RNNModel(inputs, labels, n_units=FLAGS.n_units, n_layers=FLAGS.n_layers,\n                     lr=FLAGS.learning_rate, vocab_size=vocab_size)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    saver = tf.train.Saver(max_to_keep=2)\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n\n        start_epoch = 0\n        checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoints_dir)\n        if FLAGS.is_restore:\n            if checkpoint:\n                saver.restore(sess, checkpoint)\n                logging.info(""restore from the checkpoint {0}"".format(checkpoint))\n                start_epoch += int(checkpoint.split(\'-\')[-1])\n        if is_train:\n            logging.info(\'training start...\')\n            epoch = 0\n            try:\n                for epoch in range(start_epoch, FLAGS.epochs):\n                    shakespeare.batch_index=0\n                    for batch in range(shakespeare.n_chunks):\n                        batch_x, batch_y = shakespeare.next_batch()\n                        loss, _ = sess.run([model.loss, model.train_op],\n                                           feed_dict={inputs: batch_x, labels: batch_y})\n                        logging.info(\'epoch: %s,  batch: %s, loss: %s\' % (epoch, batch, loss))\n                    if epoch % 60 == 0:\n                        saver.save(sess, os.path.join(FLAGS.checkpoints_dir, FLAGS.model_prefix), global_step=epoch)\n                logging.info(\'optimization done! enjoy your Shakespeare composer!\')\n            except KeyboardInterrupt:\n                logging.info(\'interrupt manually, try saving checkpoint for now...\')\n                saver.save(sess, os.path.join(FLAGS.checkpoints_dir, FLAGS.model_prefix), global_step=epoch)\n                logging.info(\'last epoch were saved, next time will start from epoch {}.\'.format(epoch))\n        else:\n            logging.info(\'I am thinking to compose Shakespeare drama...\')\n            if checkpoint:\n                saver.restore(sess, checkpoint)\n                logging.info(""restore from the checkpoint {0}"".format(checkpoint))\n                start_epoch += int(checkpoint.split(\'-\')[-1])\n\n            vocab_int_map = shakespeare.vocab_int_map\n            start_sentence = input(\'please input a sentence in Chinese: \')\n            sent = process_sent(start_sentence, vocab_int_map, FLAGS.n_steps)\n            outputs, last_state = sess.run([model.outputs, model.states], feed_dict={inputs: sent})\n            sent = np.append(sent, np.argmax(outputs))\n            sent = np.array([sent[1:]])\n\n            next_words = []\n            for i in range(2000):\n                outputs, last_state = sess.run([model.outputs, model.states],\n                                               feed_dict={inputs: sent, model.states: last_state})\n                next_word_index = np.argmax(outputs)\n                next_words.append(next_word_index)\n                sent = np.append(sent, next_word_index)\n                sent = np.array([sent[1:]])\n            drama_text = [{v: k for k, v in vocab_int_map.items()}[i] for i in next_words]\n            drama_text.insert(0, start_sentence)\n            pretty_print(drama_text)\n\n\ndef process_sent(sent, vocab_int, steps):\n    """"""\n    this file token sentence and make it into numpy array, return a fixed length 2d array\n    :param sent: \n    :param vocab_int: \n    :param steps: \n    :return: \n    """"""\n    sent_list = jieba.lcut(sent)\n    # if words not in vocab dict then let this word be a random index which maybe other words\n    index_list = [vocab_int[i] if i in vocab_int.keys() else np.random.randint(0, 90) for i in sent_list]\n    if len(index_list) < steps:\n        index_list = np.hstack((index_list, np.random.randint(0, 90, steps - len(index_list))))\n    else:\n        index_list = index_list[0: steps]\n    return np.array([index_list])\n\n\ndef pretty_print(words_list):\n    """"""\n    print the words list sentence by sentence\n    :param words_list: \n    :return: \n    """"""\n    print(\'I am thinking to compose drama...\')\n    all_punctuations = [\'\xe3\x80\x82\', \'\xef\xbc\x9f\', \'\xef\xbc\x81\', \'\xef\xbc\x8c\', \',\']\n    enter_punctuations = [\'\xe3\x80\x82\', \'\xef\xbc\x9f\', \'\xef\xbc\x81\', \'.\']\n    token = \'TOKEN\'\n    add_token = [i+token if i in enter_punctuations else i for i in words_list]\n    split_token = \'\'.join(add_token).split(token)\n    drop_extra = [i for i in split_token if i not in enter_punctuations]\n    print(\'Here is what I got: \')\n    for i in drop_extra:\n        print(i)\n\n\ndef main(is_train):\n    running(is_train)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n\n\n'"
models/rnn_model.py,13,"b'# Copyright 2017 Jin Fagang. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =======================================================================\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n\nclass RNNModel(object):\n    def __init__(self, inputs, labels, n_units, n_layers, lr, vocab_size):\n        self.inputs = inputs\n        self.labels = labels\n        self.n_units = n_units\n        self.n_layers = n_layers\n        self.vocab_size = vocab_size\n        self.lr = lr\n\n        self.outputs, self.states = self.rnn_model()\n\n        if self.labels is not None:\n            self.train_op, self.loss = self.update()\n\n    def rnn_model(self):\n        cell = rnn.BasicLSTMCell(num_units=self.n_units)\n        multi_cell = rnn.MultiRNNCell([cell]*self.n_layers)\n        # we only need one output so get it wrapped to out one value which is next word index\n        cell_wrapped = rnn.OutputProjectionWrapper(multi_cell, output_size=1)\n\n        # get input embed\n        embedding = tf.Variable(initial_value=tf.random_uniform([self.vocab_size, self.n_units], -1.0, 1.0))\n        inputs = tf.nn.embedding_lookup(embedding, self.inputs)\n        # what is inputs dim??\n\n        # add initial state into dynamic rnn, if I am not result would be bad, I tried, don\'t know why\n        if self.labels is not None:\n            initial_state = cell_wrapped.zero_state(int(inputs.get_shape()[0]), tf.float32)\n        else:\n            initial_state = cell_wrapped.zero_state(1, tf.float32)\n        outputs, states = tf.nn.dynamic_rnn(cell_wrapped, inputs=inputs, dtype=tf.float32, initial_state=initial_state)\n        outputs = tf.reshape(outputs, [int(outputs.get_shape()[0]), int(inputs.get_shape()[1])])\n\n        w = tf.Variable(tf.truncated_normal([int(inputs.get_shape()[1]), self.vocab_size]))\n        b = tf.Variable(tf.zeros([self.vocab_size]))\n\n        logits = tf.nn.bias_add(tf.matmul(outputs, w), b)\n        return logits, states\n\n    def update(self):\n        labels_one_hot = tf.one_hot(tf.reshape(self.labels, [-1]), depth=self.vocab_size)\n        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=self.outputs)\n        total_loss = tf.reduce_mean(loss)\n        train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(loss=loss)\n        return train_op, total_loss\n\n\n\n\n\n\n\n\n'"
models/rnn_model_no_state.py,11,"b'# Copyright 2017 Jin Fagang. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =======================================================================\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n\nclass RNNModel(object):\n    def __init__(self, inputs, labels, n_units, n_layers, lr, vocab_size):\n        self.inputs = inputs\n        self.labels = labels\n        self.n_units = n_units\n        self.n_layers = n_layers\n        self.vocab_size = vocab_size\n        self.lr = lr\n\n        self.outputs = self.rnn_model()\n\n        if self.labels is not None:\n            self.train_op, self.loss = self.update()\n\n    def rnn_model(self):\n        cell = rnn.BasicLSTMCell(num_units=self.n_units)\n        multi_cell = rnn.MultiRNNCell([cell]*self.n_layers)\n        # we only need one output so get it wrapped to out one value which is next word index\n        cell_wrapped = rnn.OutputProjectionWrapper(multi_cell, output_size=1)\n\n        # get input embed\n        embedding = tf.Variable(initial_value=tf.random_uniform([self.vocab_size, self.n_units], -1.0, 1.0))\n        inputs = tf.nn.embedding_lookup(embedding, self.inputs)\n        # what is inputs dim??\n\n        outputs, states = tf.nn.dynamic_rnn(cell_wrapped, inputs=inputs, dtype=tf.float32)\n        outputs = tf.reshape(outputs, [int(outputs.get_shape()[0]), int(inputs.get_shape()[1])])\n\n        w = tf.Variable(tf.truncated_normal([int(inputs.get_shape()[1]), self.vocab_size]))\n        b = tf.Variable(tf.zeros([self.vocab_size]))\n\n        logits = tf.nn.bias_add(tf.matmul(outputs, w), b)\n        return logits\n\n    def update(self):\n        labels_one_hot = tf.one_hot(tf.reshape(self.labels, [-1]), depth=self.vocab_size)\n        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=self.outputs)\n        total_loss = tf.reduce_mean(loss)\n        train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(loss=loss)\n        return train_op, total_loss\n'"
utils/corpus_cleaner_cn.py,0,"b'# -*- coding: utf-8 -*-\n# file: corpus_cleaner_cn.py\n# author: JinTian\n# time: 08/03/2017 8:02 PM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\n""""""\nthis script using for clean Chinese corpus.\nyou can set level for clean, i.e.:\nlevel=\'all\', will clean all character that not Chinese, include punctuations\nlevel=\'normal\', this will generate corpus like normal use, reserve alphabets and numbers\nlevel=\'clean\', this will remove all except Chinese and Chinese punctuations\n\nbesides, if you want remove complex Chinese characters, just set this to be true:\nsimple_only=True\n""""""\nimport numpy as np\nimport os\nimport string\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG,\n                    format=\'%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\',\n                    datefmt=\'%a, %d %b %Y %H:%M:%S\')\n\n\nclass CorpusCleanerCN(object):\n\n    def __init__(self):\n        self.cn_punctuation = [\'\xef\xbc\x8c\', \'\xe3\x80\x82\', \'\xef\xbc\x81\', \'\xef\xbc\x9f\', \'""\', \'""\', \'\xe3\x80\x81\']\n        self.en_punctuation = [\',\', \'.\', \'?\', \'!\', \'""\', \'""\']\n\n    def clean(self, file_name, clean_level, min_drop=1, is_save=True, summary=True):\n        if os.path.dirname(file_name):\n            base_dir = os.path.dirname(file_name)\n        else:\n            logging.error(\'not set dir. please check\')\n\n        save_file = os.path.join(base_dir, os.path.basename(file_name).split(\'.\')[0] + \'_cleaned.txt\')\n        with open(file_name, \'r+\') as f:\n            clean_content = []\n            for l in f.readlines():\n                l = l.strip()\n                if l == \'\':\n                    pass\n                elif len(l) < min_drop:\n                    pass\n                else:\n                    l = list(l)\n                    should_remove_words = []\n                    for w in l:\n                        if not self.should_reserve(w, clean_level):\n                            should_remove_words.append(w)\n                    clean_line = [c for c in l if c not in should_remove_words]\n                    clean_line = \'\'.join(clean_line)\n                    if clean_line != \'\':\n                        clean_content.append(clean_line)\n        if is_save:\n            with open(save_file, \'w+\') as f:\n                for l in clean_content:\n                    f.write(l + \'\\n\')\n            logging.info(\'cleaned file have been saved to %s.\' % save_file)\n        if summary:\n            logging.info(\'corpora all lines: %s\' % len(clean_content))\n            logging.info(\'max length in lines: %s\' % np.max([len(i) for i in clean_content]))\n            logging.info(\'min length in lines: %s\' % np.min([len(i) for i in clean_content]))\n            logging.info(\'average length in lines: %s\' % np.average([len(i) for i in clean_content]))\n        return clean_content\n\n    def should_reserve(self, w, clean_level):\n        if w == \' \':\n            return True\n        else:\n            if clean_level == \'all\':\n                # only reserve Chinese characters\n                if w in self.cn_punctuation or w in string.punctuation or self.is_alphabet(w):\n                    return False\n                else:\n                    return self.is_chinese(w)\n            elif clean_level == \'normal\':\n                # reserve Chinese characters, English alphabet, number\n                if self.is_chinese(w) or self.is_alphabet(w) or self.is_number(w):\n                    return True\n                elif w in self.cn_punctuation or w in self.en_punctuation:\n                    return True\n                else:\n                    return False\n            elif clean_level == \'clean\':\n                if self.is_chinese(w):\n                    return True\n                elif w in self.cn_punctuation:\n                    return True\n                else:\n                    return False\n            else:\n                raise ""clean_level not support %s, please set for all, normal, clean"" % clean_level\n\n    @staticmethod\n    def is_chinese(uchar):\n        """"""is chinese""""""\n        if u\'\\u4e00\' <= uchar <= u\'\\u9fa5\':\n            return True\n        else:\n            return False\n\n    @staticmethod\n    def is_number(uchar):\n        """"""is number""""""\n        if u\'\\u0030\' <= uchar <= u\'\\u0039\':\n            return True\n        else:\n            return False\n\n    @staticmethod\n    def is_alphabet(uchar):\n        """"""is alphabet""""""\n        if (u\'\\u0041\' <= uchar <= u\'\\u005a\') or (u\'\\u0061\' <= uchar <= u\'\\u007a\'):\n            return True\n        else:\n            return False\n\n    @staticmethod\n    def semi_angle_to_sbc(uchar):\n        """"""\xe5\x8d\x8a\xe8\xa7\x92\xe8\xbd\xac\xe5\x85\xa8\xe8\xa7\x92""""""\n        inside_code = ord(uchar)\n        if inside_code < 0x0020 or inside_code > 0x7e:\n            return uchar\n        if inside_code == 0x0020:\n            inside_code = 0x3000\n        else:\n            inside_code += 0xfee0\n        return chr(inside_code)\n\n    @staticmethod\n    def sbc_to_semi_angle(uchar):\n        """"""\xe5\x85\xa8\xe8\xa7\x92\xe8\xbd\xac\xe5\x8d\x8a\xe8\xa7\x92""""""\n        inside_code = ord(uchar)\n        if inside_code == 0x3000:\n            inside_code = 0x0020\n        else:\n            inside_code -= 0xfee0\n        if inside_code < 0x0020 or inside_code > 0x7e:\n            return uchar\n        return chr(inside_code)\n\n\n\n\n\n\n\n\n'"
utils/corpus_cleaner_en.py,0,"b'# Copyright 2017 Jin Fagang. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =======================================================================\n""""""\nthis file will clean English corpus\nit will lower words and remove noun english punctuations\n""""""'"
