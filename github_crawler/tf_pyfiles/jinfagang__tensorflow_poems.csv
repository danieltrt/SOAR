file_path,api_count,code
compose_poem.py,5,"b'# -*- coding: utf-8 -*-\n# file: main.py\n# author: JinTian\n# time: 11/03/2017 9:53 AM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\nimport tensorflow as tf\nfrom poems.model import rnn_model\nfrom poems.poems import process_poems\nimport numpy as np\n\nstart_token = \'B\'\nend_token = \'E\'\nmodel_dir = \'./model/\'\ncorpus_file = \'./data/poems.txt\'\n\nlr = 0.0002\n\n\ndef to_word(predict, vocabs):\n    predict = predict[0]       \n    predict /= np.sum(predict)\n    sample = np.random.choice(np.arange(len(predict)), p=predict)\n    if sample > len(vocabs):\n        return vocabs[-1]\n    else:\n        return vocabs[sample]\n\n\ndef gen_poem(begin_word):\n    batch_size = 1\n    print(\'## loading corpus from %s\' % model_dir)\n    poems_vector, word_int_map, vocabularies = process_poems(corpus_file)\n\n    input_data = tf.placeholder(tf.int32, [batch_size, None])\n\n    end_points = rnn_model(model=\'lstm\', input_data=input_data, output_data=None, vocab_size=len(\n        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=lr)\n\n    saver = tf.train.Saver(tf.global_variables())\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        checkpoint = tf.train.latest_checkpoint(model_dir)\n        saver.restore(sess, checkpoint)\n\n        x = np.array([list(map(word_int_map.get, start_token))])\n\n        [predict, last_state] = sess.run([end_points[\'prediction\'], end_points[\'last_state\']],\n                                         feed_dict={input_data: x})\n        word = begin_word or to_word(predict, vocabularies)\n        poem_ = \'\'\n\n        i = 0\n        while word != end_token:\n            poem_ += word\n            i += 1\n            if i > 24:\n                break\n            x = np.array([[word_int_map[word]]])\n            [predict, last_state] = sess.run([end_points[\'prediction\'], end_points[\'last_state\']],\n                                             feed_dict={input_data: x, end_points[\'initial_state\']: last_state})\n            word = to_word(predict, vocabularies)\n\n        return poem_\n\n\ndef pretty_print_poem(poem_):\n    poem_sentences = poem_.split(\'\xe3\x80\x82\')\n    for s in poem_sentences:\n        if s != \'\' and len(s) > 10:\n            print(s + \'\xe3\x80\x82\')\n\nif __name__ == \'__main__\':\n    begin_char = input(\'## please input the first character:\')\n    poem = gen_poem(begin_char)\n    pretty_print_poem(poem_=poem)'"
train.py,14,"b'# -*- coding: utf-8 -*-\n# file: main.py\n# author: JinTian\n# time: 11/03/2017 9:53 AM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\nimport os\nimport tensorflow as tf\nfrom poems.model import rnn_model\nfrom poems.poems import process_poems, generate_batch\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 64, \'batch size.\')\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'learning rate.\')\ntf.app.flags.DEFINE_string(\'model_dir\', os.path.abspath(\'./model\'), \'model save path.\')\ntf.app.flags.DEFINE_string(\'file_path\', os.path.abspath(\'./data/poems.txt\'), \'file name of poems.\')\ntf.app.flags.DEFINE_string(\'model_prefix\', \'poems\', \'model save prefix.\')\ntf.app.flags.DEFINE_integer(\'epochs\', 50, \'train how many epochs.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef run_training():\n    if not os.path.exists(FLAGS.model_dir):\n        os.makedirs(FLAGS.model_dir)\n\n    poems_vector, word_to_int, vocabularies = process_poems(FLAGS.file_path)\n    batches_inputs, batches_outputs = generate_batch(FLAGS.batch_size, poems_vector, word_to_int)\n\n    input_data = tf.placeholder(tf.int32, [FLAGS.batch_size, None])\n    output_targets = tf.placeholder(tf.int32, [FLAGS.batch_size, None])\n\n    end_points = rnn_model(model=\'lstm\', input_data=input_data, output_data=output_targets, vocab_size=len(\n        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=FLAGS.learning_rate)\n\n    saver = tf.train.Saver(tf.global_variables())\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n    with tf.Session() as sess:\n        # sess = tf_debug.LocalCLIDebugWrapperSession(sess=sess)\n        # sess.add_tensor_filter(""has_inf_or_nan"", tf_debug.has_inf_or_nan)\n        sess.run(init_op)\n\n        start_epoch = 0\n        checkpoint = tf.train.latest_checkpoint(FLAGS.model_dir)\n        if checkpoint:\n            saver.restore(sess, checkpoint)\n            print(""## restore from the checkpoint {0}"".format(checkpoint))\n            start_epoch += int(checkpoint.split(\'-\')[-1])\n        print(\'## start training...\')\n        try:\n            n_chunk = len(poems_vector) // FLAGS.batch_size\n            for epoch in range(start_epoch, FLAGS.epochs):\n                n = 0\n                for batch in range(n_chunk):\n                    loss, _, _ = sess.run([\n                        end_points[\'total_loss\'],\n                        end_points[\'last_state\'],\n                        end_points[\'train_op\']\n                    ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n                    n += 1\n                    print(\'Epoch: %d, batch: %d, training loss: %.6f\' % (epoch, batch, loss))\n                if epoch % 6 == 0:\n                    saver.save(sess, os.path.join(FLAGS.model_dir, FLAGS.model_prefix), global_step=epoch)\n        except KeyboardInterrupt:\n            print(\'## Interrupt manually, try saving checkpoint for now...\')\n            saver.save(sess, os.path.join(FLAGS.model_dir, FLAGS.model_prefix), global_step=epoch)\n            print(\'## Last epoch were saved, next time will start from epoch {}.\'.format(epoch))\n\n\ndef main(_):\n    run_training()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
poems/model.py,19,"b'# -*- coding: utf-8 -*-\n# file: model.py\n# author: JinTian\n# time: 07/03/2017 3:07 PM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\nimport tensorflow as tf\nimport numpy as np\n\n\ndef rnn_model(model, input_data, output_data, vocab_size, rnn_size=128, num_layers=2, batch_size=64,\n              learning_rate=0.01):\n    """"""\n    construct rnn seq2seq model.\n    :param model: model class\n    :param input_data: input data placeholder\n    :param output_data: output data placeholder\n    :param vocab_size:\n    :param rnn_size:\n    :param num_layers:\n    :param batch_size:\n    :param learning_rate:\n    :return:\n    """"""\n    end_points = {}\n\n    if model == \'rnn\':\n        cell_fun = tf.contrib.rnn.BasicRNNCell\n    elif model == \'gru\':\n        cell_fun = tf.contrib.rnn.GRUCell\n    elif model == \'lstm\':\n        cell_fun = tf.contrib.rnn.BasicLSTMCell\n\n    cell = cell_fun(rnn_size, state_is_tuple=True)\n    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n\n    if output_data is not None:\n        initial_state = cell.zero_state(batch_size, tf.float32)\n    else:\n        initial_state = cell.zero_state(1, tf.float32)\n\n    with tf.device(""/cpu:0""):\n        embedding = tf.get_variable(\'embedding\', initializer=tf.random_uniform(\n            [vocab_size + 1, rnn_size], -1.0, 1.0))\n        inputs = tf.nn.embedding_lookup(embedding, input_data)\n\n    # [batch_size, ?, rnn_size] = [64, ?, 128]\n    outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n    output = tf.reshape(outputs, [-1, rnn_size])\n\n    weights = tf.Variable(tf.truncated_normal([rnn_size, vocab_size + 1]))\n    bias = tf.Variable(tf.zeros(shape=[vocab_size + 1]))\n    logits = tf.nn.bias_add(tf.matmul(output, weights), bias=bias)\n    # [?, vocab_size+1]\n\n    if output_data is not None:\n        # output_data must be one-hot encode\n        labels = tf.one_hot(tf.reshape(output_data, [-1]), depth=vocab_size + 1)\n        # should be [?, vocab_size+1]\n\n        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n        # loss shape should be [?, vocab_size+1]\n        total_loss = tf.reduce_mean(loss)\n        train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n\n        end_points[\'initial_state\'] = initial_state\n        end_points[\'output\'] = output\n        end_points[\'train_op\'] = train_op\n        end_points[\'total_loss\'] = total_loss\n        end_points[\'loss\'] = loss\n        end_points[\'last_state\'] = last_state\n    else:\n        prediction = tf.nn.softmax(logits)\n\n        end_points[\'initial_state\'] = initial_state\n        end_points[\'last_state\'] = last_state\n        end_points[\'prediction\'] = prediction\n\n    return end_points\n'"
poems/poems.py,0,"b'# -*- coding: utf-8 -*-\n# file: poems.py\n# author: JinTian\n# time: 08/03/2017 7:39 PM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\nimport collections\nimport numpy as np\n\nstart_token = \'B\'\nend_token = \'E\'\n\n\ndef process_poems(file_name):\n    # poems -> list of numbers\n    poems = []\n    with open(file_name, ""r"", encoding=\'utf-8\', ) as f:\n        for line in f.readlines():\n            try:\n                title, content = line.strip().split(\':\')\n                content = content.replace(\' \', \'\')\n                if \'_\' in content or \'(\' in content or \'\xef\xbc\x88\' in content or \'\xe3\x80\x8a\' in content or \'[\' in content or \\\n                        start_token in content or end_token in content:\n                    continue\n                if len(content) < 5 or len(content) > 79:\n                    continue\n                content = start_token + content + end_token\n                poems.append(content)\n            except ValueError as e:\n                pass\n    # poems = sorted(poems, key=len)\n\n    all_words = [word for poem in poems for word in poem]\n    counter = collections.Counter(all_words)\n    words = sorted(counter.keys(), key=lambda x: counter[x], reverse=True)\n\n    words.append(\' \')\n    L = len(words)\n    word_int_map = dict(zip(words, range(L)))\n    poems_vector = [list(map(lambda word: word_int_map.get(word, L), poem)) for poem in poems]\n\n    return poems_vector, word_int_map, words\n\n\ndef generate_batch(batch_size, poems_vec, word_to_int):\n    n_chunk = len(poems_vec) // batch_size\n    x_batches = []\n    y_batches = []\n    for i in range(n_chunk):\n        start_index = i * batch_size\n        end_index = start_index + batch_size\n\n        batches = poems_vec[start_index:end_index]\n        length = max(map(len, batches))\n        x_data = np.full((batch_size, length), word_to_int[\' \'], np.int32)\n        for row, batch in enumerate(batches):\n            x_data[row, :len(batch)] = batch\n        y_data = np.copy(x_data)\n        y_data[:, :-1] = x_data[:, 1:]\n        """"""\n        x_data             y_data\n        [6,2,4,6,9]       [2,4,6,9,9]\n        [1,4,2,8,5]       [4,2,8,5,5]\n        """"""\n        x_batches.append(x_data)\n        y_batches.append(y_data)\n    return x_batches, y_batches\n'"
utils/clean_cn.py,0,"b'# -*- coding: utf-8 -*-\n# file: clean_cn.py\n# author: JinTian\n# time: 08/03/2017 8:02 PM\n# Copyright 2017 JinTian. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ------------------------------------------------------------------------\n""""""\nthis script using for clean Chinese corpus.\nyou can set level for clean, i.e.:\nlevel=\'all\', will clean all character that not Chinese, include punctuations\nlevel=\'normal\', this will generate corpus like normal use, reserve alphabets and numbers\nlevel=\'clean\', this will remove all except Chinese and Chinese punctuations\n\nbesides, if you want remove complex Chinese characters, just set this to be true:\nsimple_only=True\n""""""\n\nimport os\nimport string\n\n\ncn_punctuation_set = [\'\xef\xbc\x8c\', \'\xe3\x80\x82\', \'\xef\xbc\x81\', \'\xef\xbc\x9f\', \'""\', \'""\', \'\xe3\x80\x81\']\nen_punctuation_set = [\',\', \'.\', \'?\', \'!\', \'""\', \'""\']\n\n\ndef clean_cn_corpus(file_name, clean_level=\'all\', simple_only=True, is_save=True):\n    """"""\n    clean Chinese corpus.\n    :param file_name:\n    :param clean_level:\n    :param simple_only:\n    :param is_save:\n    :return: clean corpus in list type.\n    """"""\n    if os.path.dirname(file_name):\n        base_dir = os.path.dirname(file_name)\n    else:\n        print(\'not set dir. please check\')\n\n    save_file = os.path.join(base_dir, os.path.basename(file_name).split(\'.\')[0] + \'_cleaned.txt\')\n    with open(file_name, \'r+\') as f:\n        clean_content = []\n        for l in f.readlines():\n            l = l.strip()\n            if l:\n                l = list(l)\n                should_remove_words = [w for w in l if not should_reserve(w, clean_level)]\n                clean_line = \'\'.join(c for c in l if c not in should_remove_words)\n                if clean_line:\n                    clean_content.append(clean_line)\n    if is_save:\n        with open(save_file, \'w+\') as f:\n            for l in clean_content:\n                f.write(l + \'\\n\')\n        print(\'[INFO] cleaned file have been saved to %s.\' % save_file)\n    return clean_content\n\n\ndef should_reserve(w, clean_level):\n    if w == \' \':\n        return True\n    else:\n        if clean_level == \'all\':\n            # only reserve Chinese characters\n            if w in cn_punctuation_set or w in string.punctuation or is_alphabet(w):\n                return False\n            else:\n                return is_chinese(w)\n        elif clean_level == \'normal\':\n            # reserve Chinese characters, English alphabet, number\n            if is_chinese(w) or is_alphabet(w) or is_number(w):\n                return True\n            elif w in cn_punctuation_set or w in en_punctuation_set:\n                return True\n            else:\n                return False\n        elif clean_level == \'clean\':\n            if is_chinese(w):\n                return True\n            elif w in cn_punctuation_set:\n                return True\n            else:\n                return False\n        else:\n            raise ""clean_level not support %s, please set for all, normal, clean"" % clean_level\n\n\ndef is_chinese(uchar):\n    """"""is chinese""""""\n    return \'\\u4e00\' <= uchar <= \'\\u9fa5\'\n\n\ndef is_number(uchar):\n    """"""is number""""""\n    return \'\\u0030\' <= uchar <= \'\\u0039\'\n\n\ndef is_alphabet(uchar):\n    """"""is alphabet""""""\n    return (\'\\u0041\' <= uchar <= \'\\u005a\') or (\'\\u0061\' <= uchar <= \'\\u007a\')\n\ndef semi_angle_to_sbc(uchar):\n    """"""\xe5\x8d\x8a\xe8\xa7\x92\xe8\xbd\xac\xe5\x85\xa8\xe8\xa7\x92""""""\n    inside_code = ord(uchar)\n    if inside_code < 0x0020 or inside_code > 0x7e:\n        return uchar\n    if inside_code == 0x0020:\n        inside_code = 0x3000\n    else:\n        inside_code += 0xfee0\n    return chr(inside_code)\n\n\ndef sbc_to_semi_angle(uchar):\n    """"""\xe5\x85\xa8\xe8\xa7\x92\xe8\xbd\xac\xe5\x8d\x8a\xe8\xa7\x92""""""\n    inside_code = ord(uchar)\n    if inside_code == 0x3000:\n        inside_code = 0x0020\n    else:\n        inside_code -= 0xfee0\n    if inside_code < 0x0020 or inside_code > 0x7e:\n        return uchar\n    return chr(inside_code)\n\n\n\n\n\n\n\n\n'"
utils/make_regulated_verse.py,0,"b'# -*- coding: utf-8 -*-\n#\n#MIT License\n#\n#Copyright (c) 2018 damtharvey\n#\n#Permission is hereby granted, free of charge, to any person obtaining a copy\n#of this software and associated documentation files (the ""Software""), to deal\n#in the Software without restriction, including without limitation the rights\n#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n#copies of the Software, and to permit persons to whom the Software is\n#furnished to do so, subject to the following conditions:\n#\n#The above copyright notice and this permission notice shall be included in all\n#copies or substantial portions of the Software.\n#\n#THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n#SOFTWARE.\n""""""\nThese functions can process data from\ngithub.com/chinese-poetry/chinese-poetry/json to make regulated verse data\nready to use for training.\n\nIf you don\'t have jsons already, just\ngit clone https://github.com/chinese-poetry/chinese-poetry.git\nto get it.\n\nRegulated verse forms are expected to be tuples of (number of couplets,\ncharacters per couplet). For reference:\nwujue \xe4\xba\x94\xe8\xa8\x80\xe7\xb5\x95\xe5\x8f\xa5 = (2, 10)\nqijue \xe4\xb8\x83\xe8\xa8\x80\xe7\xb5\x95\xe5\x8f\xa5 = (2, 14)\nwulv \xe4\xba\x94\xe8\xa8\x80\xe5\xbe\x8b\xe8\xa9\xa9 = (4, 10)\nqilv \xe4\xb8\x83\xe8\xa8\x80\xe5\xbe\x8b\xe8\xa9\xa9 = (4, 14)\n""""""\n\nimport glob\nimport os\nimport pandas as pd\n\n\ndef unregulated(paragraphs):\n    """"""\n    Return True if the df row describes unregulated verse.\n    """"""\n    if all(len(couplet) == len(paragraphs[0]) for couplet in paragraphs):\n        return False\n    else:\n        return True\n\ndef get_poems_in_df(df, form):\n    """"""\n    Return a txt-friendly string of only poems in df of the specified form.\n    """"""\n    big_string = """"\n    for row in range(len(df)):\n        if len(df[""strains""][row]) != form[0] or \\\n           len(df[""strains""][row][0]) - 2 != form[1]:\n            continue\n        if ""\xe2\x97\x8b"" in str(df[""paragraphs""][row]):\n            continue\n        if unregulated(df[""paragraphs""][row]):\n            continue\n        big_string += df[""title""][row] + "":""\n        for couplet in df[""paragraphs""][row]:\n            big_string += couplet\n        big_string += ""\\n""\n    return big_string\n\ndef get_poems_in_dir(dir, form, save_dir):\n    """"""\n    Save to save_dir poems of form in dir in separate txt files by df.\n    """"""\n    files = [f for f in os.listdir(dir) if ""poet"" in f]\n    for file in files: # Restart partway through if kernel dies.\n        with open(os.path.join(save_dir, file[:-5] + "".txt""), ""w"") as data:\n            print(""Now reading "" + file)\n            df = pd.read_json(os.path.join(dir, file))\n            poems = get_poems_in_df(df, form)\n            data.write(poems)\n            print(str(len(poems)) + "" chars written to ""\n                  + save_dir + ""/"" + file[:-5] + "".txt"")\n    return 0\n\ndef combine_txt(txts_dir, save_file):\n    """"""\n    Combine .txt files in txts_dir and save to save_file.\n    """"""\n    read_files = glob.glob(os.path.join(txts_dir, ""*.txt""))\n    with open(save_file, ""wb"") as outfile:\n        for f in read_files:\n            with open(f, ""rb"") as infile:\n                outfile.write(infile.read())\n    return 0'"
