file_path,api_count,code
DCSCN.py,55,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nVer: 2.0\n\nDCSCN model implementation (Transposed-CNN / Pixel Shuffler version)\nSee Detail: https://github.com/jiny2001/dcscn-super-resolution/\n\nPlease note this model is updated version of the paper.\nIf you want to check original source code and results of the paper, please see https://github.com/jiny2001/dcscn-super-resolution/tree/ver1.\n\nAdditional support for using depthwise separable convolutions in place of each convolutional layer was provided by Chew Jing Wei\n(https://github.com/tehtea).\n""""""\n\nimport logging\nimport math\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom helper import loader, tf_graph, utilty as util\n\nBICUBIC_METHOD_STRING = ""bicubic""\n\n\nclass SuperResolution(tf_graph.TensorflowGraph):\n    def __init__(self, flags, model_name=""""):\n\n        super().__init__(flags)\n\n        # Model Parameters\n        self.scale = flags.scale\n        self.layers = flags.layers\n        self.filters = flags.filters\n        self.min_filters = min(flags.filters, flags.min_filters)\n        self.filters_decay_gamma = flags.filters_decay_gamma\n        self.use_nin = flags.use_nin\n        self.nin_filters = flags.nin_filters\n        self.nin_filters2 = flags.nin_filters2\n        self.reconstruct_layers = max(flags.reconstruct_layers, 1)\n        self.reconstruct_filters = flags.reconstruct_filters\n        self.resampling_method = BICUBIC_METHOD_STRING\n        self.pixel_shuffler = flags.pixel_shuffler\n        self.pixel_shuffler_filters = flags.pixel_shuffler_filters\n        self.self_ensemble = flags.self_ensemble\n        self.depthwise_separable = flags.depthwise_separable\n        \n        # Training Parameters\n        self.l2_decay = flags.l2_decay\n        self.optimizer = flags.optimizer\n        self.beta1 = flags.beta1\n        self.beta2 = flags.beta2\n        self.epsilon = flags.epsilon\n        self.momentum = flags.momentum\n        self.batch_num = flags.batch_num\n        self.batch_image_size = flags.batch_image_size\n        if flags.stride_size == 0:\n            self.stride_size = flags.batch_image_size // 2\n        else:\n            self.stride_size = flags.stride_size\n        self.clipping_norm = flags.clipping_norm\n        self.use_l1_loss = flags.use_l1_loss\n\n        # Learning Rate Control for Training\n        self.initial_lr = flags.initial_lr\n        self.lr_decay = flags.lr_decay\n        self.lr_decay_epoch = flags.lr_decay_epoch\n\n        # Dataset or Others\n        self.training_images = int(math.ceil(flags.training_images / flags.batch_num) * flags.batch_num)\n        self.train = None\n        self.test = None\n\n        # Image Processing Parameters\n        self.max_value = flags.max_value\n        self.channels = flags.channels\n        self.output_channels = 1\n        self.psnr_calc_border_size = flags.psnr_calc_border_size\n        if self.psnr_calc_border_size < 0:\n            self.psnr_calc_border_size = self.scale\n\n        # Environment (all directory name should not contain tailing \'/\'  )\n        self.batch_dir = flags.batch_dir\n\n        # initialize variables\n        self.name = self.get_model_name(model_name)\n        self.total_epochs = 0\n        lr = self.initial_lr\n        while lr > flags.end_lr:\n            self.total_epochs += self.lr_decay_epoch\n            lr *= self.lr_decay\n\n        # initialize environment\n        util.make_dir(self.checkpoint_dir)\n        util.make_dir(flags.graph_dir)\n        util.make_dir(self.tf_log_dir)\n        if flags.initialize_tf_log:\n            util.clean_dir(self.tf_log_dir)\n        util.set_logging(flags.log_filename, stream_log_level=logging.INFO, file_log_level=logging.INFO,\n                         tf_log_level=tf.logging.WARN)\n        logging.info(""\\nDCSCN v2-------------------------------------"")\n        logging.info(""%s [%s]"" % (util.get_now_date(), self.name))\n\n        self.init_train_step()\n\n    def get_model_name(self, model_name, name_postfix=""""):\n        if model_name is """":\n            name = ""dcscn_L%d_F%d"" % (self.layers, self.filters)\n            if self.min_filters != 0:\n                name += ""to%d"" % self.min_filters\n            if self.filters_decay_gamma != 1.5:\n                name += ""_G%2.2f"" % self.filters_decay_gamma\n            if self.cnn_size != 3:\n                name += ""_C%d"" % self.cnn_size\n            if self.scale != 2:\n                name += ""_Sc%d"" % self.scale\n            if self.use_nin:\n                name += ""_NIN""\n                if self.nin_filters != 0:\n                    name += ""_A%d"" % self.nin_filters\n                if self.nin_filters2 != self.nin_filters // 2:\n                    name += ""_B%d"" % self.nin_filters2\n            if self.pixel_shuffler:\n                name += ""_PS""\n            if self.max_value != 255.0:\n                name += ""_M%2.1f"" % self.max_value\n            if self.activator != ""prelu"":\n                name += ""_%s"" % self.activator\n            if self.batch_norm:\n                name += ""_BN""\n            if self.depthwise_separable:\n                name += ""_DS""\n            if self.reconstruct_layers >= 1:\n                name += ""_R%d"" % self.reconstruct_layers\n                if self.reconstruct_filters != 1:\n                    name += ""F%d"" % self.reconstruct_filters\n            if name_postfix is not """":\n                name += ""_"" + name_postfix\n        else:\n            name = ""dcscn_%s"" % model_name\n\n        return name\n\n    def load_dynamic_datasets(self, data_dir, batch_image_size):\n        """""" loads datasets\n        Opens image directory as a datasets. Images will be loaded when build_input_batch() is called.\n        """"""\n\n        self.train = loader.DynamicDataSets(self.scale, batch_image_size, channels=self.channels,\n                                            resampling_method=self.resampling_method)\n        self.train.set_data_dir(data_dir)\n\n    def load_datasets(self, data_dir, batch_dir, batch_image_size, stride_size=0):\n        """""" build input patch images and loads as a datasets\n        Opens image directory as a datasets.\n        Each images are splitted into patch images and converted to input image. Since loading\n        (especially from PNG/JPG) and building input-LR images needs much computation in the\n        training phase, building pre-processed images makes training much faster. However, images\n        are limited by divided grids.\n        """"""\n\n        batch_dir += ""/scale%d"" % self.scale\n\n        self.train = loader.BatchDataSets(self.scale, batch_dir, batch_image_size, stride_size, channels=self.channels,\n                                          resampling_method=self.resampling_method)\n\n        if not self.train.is_batch_exist():\n            self.train.build_batch(data_dir)\n        else:\n            self.train.load_batch_counts()\n        self.train.load_all_batch_images()\n\n    def init_epoch_index(self):\n\n        self.batch_input = self.batch_num * [None]\n        self.batch_input_bicubic = self.batch_num * [None]\n        self.batch_true = self.batch_num * [None]\n\n        self.training_psnr_sum = 0\n        self.training_loss_sum = 0\n        self.training_step = 0\n        self.train.init_batch_index()\n\n    def build_input_batch(self):\n\n        for i in range(self.batch_num):\n            self.batch_input[i], self.batch_input_bicubic[i], self.batch_true[i] = self.train.load_batch_image(\n                self.max_value)\n\n    def load_graph(self, frozen_graph_filename=\'./model_to_freeze/frozen_model_optimized.pb\'):\n        """""" \n        load an existing frozen graph into the current graph.\n        """"""\n        # self.name =  ""frozen_model"" #TODO: Generalise this line\n\n        # We load the protobuf file from the disk and parse it to retrieve the \n        # unserialized graph_def\n        with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n\n         # load the graph def into the current graph\n        with self.as_default() as graph:\n            tf.import_graph_def(graph_def, name=""prefix"")\n\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        # get input and output tensors\n\n         # input\n        self.x = self.get_tensor_by_name(""prefix/x:0"")\n        self.x2 = self.get_tensor_by_name(""prefix/x2:0"")\n        if self.dropout_rate < 1:\n            self.dropout = self.get_tensor_by_name(""prefix/dropout_keep_rate:0"")\n\n         # output\n        self.y_ = self.get_tensor_by_name(\'prefix/output:0\')\n\n         # close existing session and re-initialize it\n        self.sess.close()\n        super().init_session()\n\n    def build_graph(self):\n\n        self.x = tf.placeholder(tf.float32, shape=[None, None, None, self.channels], name=""x"")\n        self.y = tf.placeholder(tf.float32, shape=[None, None, None, self.output_channels], name=""y"")\n        self.x2 = tf.placeholder(tf.float32, shape=[None, None, None, self.output_channels], name=""x2"")\n        self.dropout = tf.placeholder(tf.float32, shape=[], name=""dropout_keep_rate"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        # building feature extraction layers\n\n        output_feature_num = self.filters\n        total_output_feature_num = 0\n        input_feature_num = self.channels\n        input_tensor = self.x\n\n        if self.save_weights:\n            with tf.name_scope(""X""):\n                util.add_summaries(""output"", self.name, self.x, save_stddev=True, save_mean=True)\n\n        for i in range(self.layers):\n            if self.min_filters != 0 and i > 0:\n                x1 = i / float(self.layers - 1)\n                y1 = pow(x1, 1.0 / self.filters_decay_gamma)\n                output_feature_num = int((self.filters - self.min_filters) * (1 - y1) + self.min_filters)\n\n            if (self.depthwise_separable):\n                self.build_depthwise_separable_conv(""CNN%d"" % (i + 1), input_tensor, self.cnn_size, input_feature_num,\n                                            output_feature_num, use_bias=True, activator=self.activator,\n                                            use_batch_norm=self.batch_norm, dropout_rate=self.dropout_rate)\n            else:\n                self.build_conv(""CNN%d"" % (i + 1), input_tensor, self.cnn_size, input_feature_num,\n                                output_feature_num, use_bias=True, activator=self.activator,\n                                use_batch_norm=self.batch_norm, dropout_rate=self.dropout_rate)\n            input_feature_num = output_feature_num\n            input_tensor = self.H[-1]\n            total_output_feature_num += output_feature_num\n\n        with tf.variable_scope(""Concat""):\n            self.H_concat = tf.concat(self.H, 3, name=""H_concat"")\n        self.features += "" Total: (%d)"" % total_output_feature_num\n\n        # building reconstruction layers ---\n\n        if self.use_nin:\n            if (self.depthwise_separable):\n                self.build_depthwise_separable_conv(""A1"", self.H_concat, 1, total_output_feature_num, self.nin_filters,\n                            dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n                self.receptive_fields -= (self.cnn_size - 1)\n                self.build_depthwise_separable_conv(""B1"", self.H_concat, 1, total_output_feature_num, self.nin_filters2,\n                                dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n                self.build_depthwise_separable_conv(""B2"", self.H[-1], 3, self.nin_filters2, self.nin_filters2,\n                                dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n            else:\n                self.build_conv(""A1"", self.H_concat, 1, total_output_feature_num, self.nin_filters,\n                            dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n                self.receptive_fields -= (self.cnn_size - 1)\n                self.build_conv(""B1"", self.H_concat, 1, total_output_feature_num, self.nin_filters2,\n                                dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n                self.build_conv(""B2"", self.H[-1], 3, self.nin_filters2, self.nin_filters2,\n                                dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n\n            self.H.append(tf.concat([self.H[-1], self.H[-3]], 3, name=""Concat2""))\n            input_channels = self.nin_filters + self.nin_filters2\n        else:\n            if (self.depthwise_separable):\n                self.build_depthwise_separable_conv(""C"", self.H_concat, 1, total_output_feature_num, self.filters,\n                    dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n            else:\n                self.build_conv(""C"", self.H_concat, 1, total_output_feature_num, self.filters,\n                    dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n            input_channels = self.filters\n\n        # building upsampling layer\n        if self.pixel_shuffler:\n            if self.pixel_shuffler_filters != 0:\n                output_channels = self.pixel_shuffler_filters\n            else:\n                output_channels = input_channels\n            if self.scale == 4:\n                self.build_pixel_shuffler_layer(""Up-PS"", self.H[-1], 2, \n                                                input_channels, input_channels, \n                                                depthwise_separable=self.depthwise_separable)\n                self.build_pixel_shuffler_layer(""Up-PS2"", self.H[-1], 2, \n                                                input_channels, output_channels, \n                                                depthwise_separable=self.depthwise_separable)\n            else:\n                self.build_pixel_shuffler_layer(""Up-PS"", self.H[-1], self.scale, \n                                                input_channels, output_channels, \n                                                depthwise_separable=self.depthwise_separable)\n            input_channels = output_channels\n        else:\n            self.build_transposed_conv(""Up-TCNN"", self.H[-1], self.scale, input_channels)\n\n        for i in range(self.reconstruct_layers - 1):\n            self.build_conv(""R-CNN%d"" % (i + 1), self.H[-1], self.cnn_size, input_channels, self.reconstruct_filters,\n                            dropout_rate=self.dropout_rate, use_bias=True, activator=self.activator)\n            input_channels = self.reconstruct_filters\n\n        if (self.depthwise_separable):\n            self.build_depthwise_separable_conv(""R-CNN%d"" % self.reconstruct_layers, self.H[-1], \n                        self.cnn_size, input_channels, self.output_channels)\n        else:\n            self.build_conv(""R-CNN%d"" % self.reconstruct_layers, self.H[-1], self.cnn_size, input_channels,\n                        self.output_channels)\n\n        self.y_ = tf.add(self.H[-1], self.x2, name=""output"")\n\n        if self.save_weights:\n            with tf.name_scope(""Y_""):\n                util.add_summaries(""output"", self.name, self.y_, save_stddev=True, save_mean=True)\n\n        logging.info(""Feature:%s Complexity:%s Receptive Fields:%d"" % (\n            self.features, ""{:,}"".format(self.complexity), self.receptive_fields))\n\n    def build_optimizer(self):\n        """"""\n        Build loss function. We use 6+scale as a border\tand we don\'t calculate MSE on the border.\n        """"""\n\n        self.lr_input = tf.placeholder(tf.float32, shape=[], name=""LearningRate"")\n\n        diff = tf.subtract(self.y_, self.y, ""diff"")\n\n        if self.use_l1_loss:\n            self.mse = tf.reduce_mean(tf.square(diff, name=""diff_square""), name=""mse"")\n            self.image_loss = tf.reduce_mean(tf.abs(diff, name=""diff_abs""), name=""image_loss"")\n        else:\n            self.mse = tf.reduce_mean(tf.square(diff, name=""diff_square""), name=""mse"")\n            self.image_loss = tf.identity(self.mse, name=""image_loss"")\n\n        if self.l2_decay > 0:\n            l2_norm_losses = [tf.nn.l2_loss(w) for w in self.Weights]\n            l2_norm_loss = self.l2_decay * tf.add_n(l2_norm_losses)\n            if self.enable_log:\n                tf.summary.scalar(""L2WeightDecayLoss/"" + self.name, l2_norm_loss)\n\n            self.loss = self.image_loss + l2_norm_loss\n        else:\n            self.loss = self.image_loss\n\n        if self.enable_log:\n            tf.summary.scalar(""Loss/"" + self.name, self.loss)\n\n        if self.batch_norm:\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                self.training_optimizer = self.add_optimizer_op(self.loss, self.lr_input)\n        else:\n            self.training_optimizer = self.add_optimizer_op(self.loss, self.lr_input)\n\n        util.print_num_of_total_parameters(output_detail=True)\n\n    def get_psnr_tensor(self, mse):\n\n        with tf.variable_scope(\'get_PSNR\'):\n            value = tf.constant(self.max_value, dtype=mse.dtype) / tf.sqrt(mse)\n            numerator = tf.log(value)\n            denominator = tf.log(tf.constant(10, dtype=mse.dtype))\n            return tf.constant(20, dtype=mse.dtype) * numerator / denominator\n\n    def add_optimizer_op(self, loss, lr_input):\n\n        if self.optimizer == ""gd"":\n            optimizer = tf.train.GradientDescentOptimizer(lr_input)\n        elif self.optimizer == ""adadelta"":\n            optimizer = tf.train.AdadeltaOptimizer(lr_input)\n        elif self.optimizer == ""adagrad"":\n            optimizer = tf.train.AdagradOptimizer(lr_input)\n        elif self.optimizer == ""adam"":\n            optimizer = tf.train.AdamOptimizer(lr_input, beta1=self.beta1, beta2=self.beta2, epsilon=self.epsilon)\n        elif self.optimizer == ""momentum"":\n            optimizer = tf.train.MomentumOptimizer(lr_input, self.momentum)\n        elif self.optimizer == ""rmsprop"":\n            optimizer = tf.train.RMSPropOptimizer(lr_input, momentum=self.momentum)\n        else:\n            print(""Optimizer arg should be one of [gd, adadelta, adagrad, adam, momentum, rmsprop]."")\n            return None\n\n        if self.clipping_norm > 0 or self.save_weights:\n            trainables = tf.trainable_variables()\n            grads = tf.gradients(loss, trainables)\n\n            if self.save_weights:\n                for i in range(len(grads)):\n                    util.add_summaries("""", self.name, grads[i], header_name=grads[i].name + ""/"", save_stddev=True,\n                                       save_mean=True)\n\n        if self.clipping_norm > 0:\n            clipped_grads, _ = tf.clip_by_global_norm(grads, clip_norm=self.clipping_norm)\n            grad_var_pairs = zip(clipped_grads, trainables)\n            training_optimizer = optimizer.apply_gradients(grad_var_pairs)\n        else:\n            training_optimizer = optimizer.minimize(loss)\n\n        return training_optimizer\n\n    def train_batch(self):\n\n        feed_dict = {self.x: self.batch_input, self.x2: self.batch_input_bicubic, self.y: self.batch_true,\n                     self.lr_input: self.lr, self.dropout: self.dropout_rate, self.is_training: 1}\n\n        _, image_loss, mse = self.sess.run([self.training_optimizer, self.image_loss, self.mse], feed_dict=feed_dict)\n        self.training_loss_sum += image_loss\n        self.training_psnr_sum += util.get_psnr(mse, max_value=self.max_value)\n\n        self.training_step += 1\n        self.step += 1\n\n    def log_to_tensorboard(self, test_filename, psnr, save_meta_data=True):\n\n        if self.enable_log is False:\n            return\n\n        # todo\n        save_meta_data = False\n\n        org_image = util.set_image_alignment(util.load_image(test_filename, print_console=False), self.scale)\n\n        if len(org_image.shape) >= 3 and org_image.shape[2] == 3 and self.channels == 1:\n            org_image = util.convert_rgb_to_y(org_image)\n\n        input_image = util.resize_image_by_pil(org_image, 1.0 / self.scale, resampling_method=self.resampling_method)\n        bicubic_image = util.resize_image_by_pil(input_image, self.scale, resampling_method=self.resampling_method)\n\n        if self.max_value != 255.0:\n            input_image = np.multiply(input_image, self.max_value / 255.0)  # type: np.ndarray\n            bicubic_image = np.multiply(bicubic_image, self.max_value / 255.0)  # type: np.ndarray\n            org_image = np.multiply(org_image, self.max_value / 255.0)  # type: np.ndarray\n\n        feed_dict = {self.x: input_image.reshape([1, input_image.shape[0], input_image.shape[1], input_image.shape[2]]),\n                     self.x2: bicubic_image.reshape(\n                         [1, bicubic_image.shape[0], bicubic_image.shape[1], bicubic_image.shape[2]]),\n                     self.y: org_image.reshape([1, org_image.shape[0], org_image.shape[1], org_image.shape[2]]),\n                     self.dropout: 1.0,\n                     self.is_training: 0}\n\n        if save_meta_data:\n            # profiler = tf.profiler.Profile(self.sess.graph)\n\n            run_metadata = tf.RunMetadata()\n            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n            summary_str, _ = self.sess.run([self.summary_op, self.loss], feed_dict=feed_dict, options=run_options,\n                                           run_metadata=run_metadata)\n            self.test_writer.add_run_metadata(run_metadata, ""step%d"" % self.epochs_completed)\n\n            filename = self.checkpoint_dir + ""/"" + self.name + ""_metadata.txt""\n            with open(filename, ""w"") as out:\n                out.write(str(run_metadata))\n\n            # filename = self.checkpoint_dir + ""/"" + self.name + ""_memory.txt""\n            # tf.profiler.write_op_log(\n            # \ttf.get_default_graph(),\n            # \tlog_dir=self.checkpoint_dir,\n            # \t#op_log=op_log,\n            # \trun_meta=run_metadata)\n\n            tf.contrib.tfprof.model_analyzer.print_model_analysis(\n                tf.get_default_graph(), run_meta=run_metadata,\n                tfprof_options=tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY)\n\n        else:\n            summary_str, _ = self.sess.run([self.summary_op, self.loss], feed_dict=feed_dict)\n\n        self.train_writer.add_summary(summary_str, self.epochs_completed)\n        if not self.use_l1_loss:\n            if self.training_step != 0:\n                util.log_scalar_value(self.train_writer, \'PSNR\', self.training_psnr_sum / self.training_step,\n                                      self.epochs_completed)\n        util.log_scalar_value(self.train_writer, \'LR\', self.lr, self.epochs_completed)\n        self.train_writer.flush()\n\n        util.log_scalar_value(self.test_writer, \'PSNR\', psnr, self.epochs_completed)\n        self.test_writer.flush()\n\n    def update_epoch_and_lr(self):\n\n        self.epochs_completed_in_stage += 1\n\n        if self.epochs_completed_in_stage >= self.lr_decay_epoch:\n\n            # set new learning rate\n            self.lr *= self.lr_decay\n            self.epochs_completed_in_stage = 0\n            return True\n        else:\n            return False\n\n    def print_status(self, psnr, ssim, log=False):\n\n        if self.step == 0:\n            logging.info(""Initial PSNR:%f SSIM:%f"" % (psnr, ssim))\n        else:\n            processing_time = (time.time() - self.start_time) / self.step\n            if self.use_l1_loss:\n                line_a = ""%s Step:%s PSNR:%f SSIM:%f (Training Loss:%0.3f)"" % (\n                    util.get_now_date(), ""{:,}"".format(self.step), psnr, ssim,\n                    self.training_loss_sum / self.training_step)\n            else:\n                line_a = ""%s Step:%s PSNR:%f SSIM:%f (Training PSNR:%0.3f)"" % (\n                    util.get_now_date(), ""{:,}"".format(self.step), psnr, ssim,\n                    self.training_psnr_sum / self.training_step)\n            estimated = processing_time * (self.total_epochs - self.epochs_completed) * (\n                self.training_images // self.batch_num)\n            h = estimated // (60 * 60)\n            estimated -= h * 60 * 60\n            m = estimated // 60\n            s = estimated - m * 60\n            line_b = ""Epoch:%d LR:%f (%2.3fsec/step) Estimated:%d:%d:%d"" % (\n                self.epochs_completed, self.lr, processing_time, h, m, s)\n            if log:\n                logging.info(line_a)\n                logging.info(line_b)\n            else:\n                print(line_a)\n                print(line_b)\n\n    def print_weight_variables(self):\n\n        for bias in self.Biases:\n            util.print_filter_biases(bias)\n\n        for weight in self.Weights:\n            util.print_filter_weights(weight)\n\n    def evaluate(self, test_filenames):\n\n        total_psnr = total_ssim = 0\n        if len(test_filenames) == 0:\n            return 0, 0\n\n        for filename in test_filenames:\n            psnr, ssim = self.do_for_evaluate(filename, print_console=False)\n            total_psnr += psnr\n            total_ssim += ssim\n\n        return total_psnr / len(test_filenames), total_ssim / len(test_filenames)\n\n    def do(self, input_image, bicubic_input_image=None):\n\n        h, w = input_image.shape[:2]\n        ch = input_image.shape[2] if len(input_image.shape) > 2 else 1\n\n        if bicubic_input_image is None:\n            bicubic_input_image = util.resize_image_by_pil(input_image, self.scale,\n                                                           resampling_method=self.resampling_method)\n        if self.max_value != 255.0:\n            input_image = np.multiply(input_image, self.max_value / 255.0)  # type: np.ndarray\n            bicubic_input_image = np.multiply(bicubic_input_image, self.max_value / 255.0)  # type: np.ndarray\n\n        if self.self_ensemble > 1:\n            output = np.zeros([self.scale * h, self.scale * w, 1])\n\n            for i in range(self.self_ensemble):\n                image = util.flip(input_image, i)\n                bicubic_image = util.flip(bicubic_input_image, i)\n                y = self.sess.run(self.y_, feed_dict={self.x: image.reshape(1, image.shape[0], image.shape[1], ch),\n                                                      self.x2: bicubic_image.reshape(1, self.scale * image.shape[0],\n                                                                                     self.scale * image.shape[1],\n                                                                                     ch),\n                                                      self.dropout: 1.0, self.is_training: 0})\n                restored = util.flip(y[0], i, invert=True)\n                output += restored\n\n            output /= self.self_ensemble\n        else:\n            y = self.sess.run(self.y_, feed_dict={self.x: input_image.reshape(1, h, w, ch),\n                                                  self.x2: bicubic_input_image.reshape(1, self.scale * h,\n                                                                                       self.scale * w, ch),\n                                                  self.dropout: 1.0, self.is_training: 0})\n            output = y[0]\n\n        if self.max_value != 255.0:\n            hr_image = np.multiply(output, 255.0 / self.max_value)\n        else:\n            hr_image = output\n\n        return hr_image\n\n    def do_for_file(self, file_path, output_folder=""output""):\n\n        org_image = util.load_image(file_path)\n\n        filename, extension = os.path.splitext(os.path.basename(file_path))\n        output_folder += ""/"" + self.name + ""/""\n        util.save_image(output_folder + filename + extension, org_image)\n\n        scaled_image = util.resize_image_by_pil(org_image, self.scale, resampling_method=self.resampling_method)\n        util.save_image(output_folder + filename + ""_bicubic"" + extension, scaled_image)\n\n        if len(org_image.shape) >= 3 and org_image.shape[2] == 3 and self.channels == 1:\n            input_y_image = util.convert_rgb_to_y(org_image)\n            scaled_image = util.resize_image_by_pil(input_y_image, self.scale, resampling_method=self.resampling_method)\n            util.save_image(output_folder + filename + ""_bicubic_y"" + extension, scaled_image)\n            output_y_image = self.do(input_y_image)\n            util.save_image(output_folder + filename + ""_result_y"" + extension, output_y_image)\n\n            scaled_ycbcr_image = util.convert_rgb_to_ycbcr(\n                util.resize_image_by_pil(org_image, self.scale, self.resampling_method))\n            image = util.convert_y_and_cbcr_to_rgb(output_y_image, scaled_ycbcr_image[:, :, 1:3])\n        else:\n            scaled_image = util.resize_image_by_pil(org_image, self.scale, resampling_method=self.resampling_method)\n            util.save_image(output_folder + filename + ""_bicubic_y"" + extension, scaled_image)\n            image = self.do(org_image)\n\n        util.save_image(output_folder + filename + ""_result"" + extension, image)\n\n    def do_for_evaluate_with_output(self, file_path, output_directory, print_console=False):\n\n        filename, extension = os.path.splitext(file_path)\n        output_directory += ""/"" + self.name + ""/""\n        util.make_dir(output_directory)\n\n        true_image = util.set_image_alignment(util.load_image(file_path, print_console=False), self.scale)\n        input_image = util.resize_image_by_pil(true_image, 1.0/ self.scale, resampling_method=self.resampling_method)\n        input_bicubic_image = util.resize_image_by_pil(input_image, self.scale, resampling_method=self.resampling_method)\n        util.save_image(output_directory + filename + ""_input_bicubic"" + extension, input_bicubic_image)\n\n        if true_image.shape[2] == 3 and self.channels == 1:\n\n            # for color images\n            input_y_image = loader.build_input_image(true_image, channels=self.channels, scale=self.scale,\n                                                     alignment=self.scale, convert_ycbcr=True)\n            input_bicubic_y_image = util.resize_image_by_pil(input_y_image, self.scale,\n                                                             resampling_method=self.resampling_method)\n\n            true_ycbcr_image = util.convert_rgb_to_ycbcr(true_image)\n\n            output_y_image = self.do(input_y_image, input_bicubic_y_image)\n            psnr, ssim = util.compute_psnr_and_ssim(true_ycbcr_image[:, :, 0:1], output_y_image,\n                                                    border_size=self.psnr_calc_border_size)\n            loss_image = util.get_loss_image(true_ycbcr_image[:, :, 0:1], output_y_image,\n                                             border_size=self.psnr_calc_border_size)\n\n            output_color_image = util.convert_y_and_cbcr_to_rgb(output_y_image, true_ycbcr_image[:, :, 1:3])\n\n            util.save_image(output_directory + file_path, true_image)\n            util.save_image(output_directory + filename + ""_input"" + extension, input_y_image)\n            util.save_image(output_directory + filename + ""_input_bicubic_y"" + extension, input_bicubic_y_image)\n            util.save_image(output_directory + filename + ""_true_y"" + extension, true_ycbcr_image[:, :, 0:1])\n            util.save_image(output_directory + filename + ""_result"" + extension, output_y_image)\n            util.save_image(output_directory + filename + ""_result_c"" + extension, output_color_image)\n            util.save_image(output_directory + filename + ""_loss"" + extension, loss_image)\n\n        elif true_image.shape[2] == 1 and self.channels == 1:\n\n            # for monochrome images\n            input_image = loader.build_input_image(true_image, channels=self.channels, scale=self.scale,\n                                                   alignment=self.scale)\n            input_bicubic_y_image = util.resize_image_by_pil(input_image, self.scale,\n                                                             resampling_method=self.resampling_method)\n            output_image = self.do(input_image, input_bicubic_y_image)\n            psnr, ssim = util.compute_psnr_and_ssim(true_image, output_image, border_size=self.psnr_calc_border_size)\n            util.save_image(output_directory + file_path, true_image)\n            util.save_image(output_directory + filename + ""_result"" + extension, output_image)\n        else:\n            return None, None\n\n        if print_console:\n            print(""[%s] PSNR:%f, SSIM:%f"" % (filename, psnr, ssim))\n\n        return psnr, ssim\n\n    def do_for_evaluate(self, file_path, print_console=False):\n\n        true_image = util.set_image_alignment(util.load_image(file_path, print_console=False), self.scale)\n\n        if true_image.shape[2] == 3 and self.channels == 1:\n\n            # for color images\n            input_y_image = loader.build_input_image(true_image, channels=self.channels, scale=self.scale,\n                                                     alignment=self.scale, convert_ycbcr=True)\n            true_y_image = util.convert_rgb_to_y(true_image)\n            input_bicubic_y_image = util.resize_image_by_pil(input_y_image, self.scale,\n                                                             resampling_method=self.resampling_method)\n            output_y_image = self.do(input_y_image, input_bicubic_y_image)\n            psnr, ssim = util.compute_psnr_and_ssim(true_y_image, output_y_image,\n                                                    border_size=self.psnr_calc_border_size)\n\n        elif true_image.shape[2] == 1 and self.channels == 1:\n\n            # for monochrome images\n            input_image = loader.build_input_image(true_image, channels=self.channels, scale=self.scale,\n                                                   alignment=self.scale)\n            input_bicubic_y_image = util.resize_image_by_pil(input_image, self.scale,\n                                                             resampling_method=self.resampling_method)\n            output_image = self.do(input_image, input_bicubic_y_image)\n            psnr, ssim = util.compute_psnr_and_ssim(true_image, output_image, border_size=self.psnr_calc_border_size)\n        else:\n            return None, None\n\n        if print_console:\n            print(""[%s] PSNR:%f, SSIM:%f"" % (file_path, psnr, ssim))\n\n        return psnr, ssim\n\n    def evaluate_bicubic(self, file_path, print_console=False):\n\n        true_image = util.set_image_alignment(util.load_image(file_path, print_console=False), self.scale)\n\n        if true_image.shape[2] == 3 and self.channels == 1:\n            input_image = loader.build_input_image(true_image, channels=self.channels, scale=self.scale,\n                                                   alignment=self.scale, convert_ycbcr=True)\n            true_image = util.convert_rgb_to_y(true_image)\n        elif true_image.shape[2] == 1 and self.channels == 1:\n            input_image = loader.build_input_image(true_image, channels=self.channels, scale=self.scale,\n                                                   alignment=self.scale)\n        else:\n            return None, None\n\n        input_bicubic_image = util.resize_image_by_pil(input_image, self.scale, resampling_method=self.resampling_method)\n        psnr, ssim = util.compute_psnr_and_ssim(true_image, input_bicubic_image, border_size=self.psnr_calc_border_size)\n\n        if print_console:\n            print(""PSNR:%f, SSIM:%f"" % (psnr, ssim))\n\n        return psnr, ssim\n\n    def init_train_step(self):\n        self.lr = self.initial_lr\n        self.epochs_completed = 0\n        self.epochs_completed_in_stage = 0\n        self.min_validation_mse = -1\n        self.min_validation_epoch = -1\n        self.step = 0\n\n        self.start_time = time.time()\n\n    def end_train_step(self):\n        self.total_time = time.time() - self.start_time\n\n    def print_steps_completed(self, output_to_logging=False):\n\n        if self.step == 0:\n            return\n\n        processing_time = self.total_time / self.step\n        h = self.total_time // (60 * 60)\n        m = (self.total_time - h * 60 * 60) // 60\n        s = (self.total_time - h * 60 * 60 - m * 60)\n\n        status = ""Finished at Total Epoch:%d Steps:%s Time:%02d:%02d:%02d (%2.3fsec/step) %d x %d x %d patches"" % (\n            self.epochs_completed, ""{:,}"".format(self.step), h, m, s, processing_time,\n            self.batch_image_size, self.batch_image_size, self.training_images)\n\n        if output_to_logging:\n            logging.info(status)\n        else:\n            print(status)\n\n    def log_model_analysis(self):\n        run_metadata = tf.RunMetadata()\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\n        _, loss = self.sess.run([self.optimizer, self.loss], feed_dict={self.x: self.batch_input,\n                                                                        self.x2: self.batch_input_bicubic,\n                                                                        self.y: self.batch_true,\n                                                                        self.lr_input: self.lr,\n                                                                        self.dropout: self.dropout_rate},\n                                options=run_options, run_metadata=run_metadata)\n\n        # tf.contrib.tfprof.model_analyzer.print_model_analysis(\n        #   tf.get_default_graph(),\n        #   run_meta=run_metadata,\n        #   tfprof_options=tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY)\n        self.first_training = False\n'"
augmentation.py,1,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nAuthor: Jin Yamanaka\nGithub: https://github.com/jiny2001/dcscn-image-super-resolution\n\nCreate Augmented training images\n\nPut your images under data/[your dataset name]/ and specify [your dataset name] for --dataset.\n\n--augment_level 2-8: will generate flipped / rotated images\n\n""""""\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom helper import args, utilty as util\n\nargs.flags.DEFINE_integer(""augment_level"", 4, ""Augmentation level. 4:+LR/UD/LR-UD flipped, 7:+rotated"")\n\nFLAGS = args.get()\n\n\ndef main(not_parsed_args):\n    if len(not_parsed_args) > 1:\n        print(""Unknown args:%s"" % not_parsed_args)\n        exit()\n\n    print(""Building x%d augmented data."" % FLAGS.augment_level)\n\n    training_filenames = util.get_files_in_directory(FLAGS.data_dir + ""/"" + FLAGS.dataset + ""/"")\n    target_dir = FLAGS.data_dir + ""/"" + FLAGS.dataset + (""_%d/"" % FLAGS.augment_level)\n    util.make_dir(target_dir)\n\n    for file_path in training_filenames:\n        org_image = util.load_image(file_path)\n\n        filename = os.path.basename(file_path)\n        filename, extension = os.path.splitext(filename)\n\n        new_filename = target_dir + filename\n        util.save_image(new_filename + extension, org_image)\n\n        if FLAGS.augment_level >= 2:\n            ud_image = np.flipud(org_image)\n            util.save_image(new_filename + ""_v"" + extension, ud_image)\n        if FLAGS.augment_level >= 3:\n            lr_image = np.fliplr(org_image)\n            util.save_image(new_filename + ""_h"" + extension, lr_image)\n        if FLAGS.augment_level >= 4:\n            lr_image = np.fliplr(org_image)\n            lrud_image = np.flipud(lr_image)\n            util.save_image(new_filename + ""_hv"" + extension, lrud_image)\n\n        if FLAGS.augment_level >= 5:\n            rotated_image1 = np.rot90(org_image)\n            util.save_image(new_filename + ""_r1"" + extension, rotated_image1)\n        if FLAGS.augment_level >= 6:\n            rotated_image2 = np.rot90(org_image, -1)\n            util.save_image(new_filename + ""_r2"" + extension, rotated_image2)\n\n        if FLAGS.augment_level >= 7:\n            rotated_image1 = np.rot90(org_image)\n            ud_image = np.flipud(rotated_image1)\n            util.save_image(new_filename + ""_r1_v"" + extension, ud_image)\n        if FLAGS.augment_level >= 8:\n            rotated_image2 = np.rot90(org_image, -1)\n            ud_image = np.flipud(rotated_image2)\n            util.save_image(new_filename + ""_r2_v"" + extension, ud_image)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
convert_y.py,1,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nAuthor: Jin Yamanaka\nGithub: https://github.com/jiny2001/dcscn-image-super-resolution\n\nConvert RGB(A)-(PNG or Jpeg) Image to Y-BMP images\n\nPut your images under data/[your dataset name]/ and specify [your dataset name] for --dataset.\n\n\n""""""\nimport os\n\nimport tensorflow as tf\n\nfrom helper import args, utilty as util\n\nFLAGS = args.get()\n\n\ndef main(not_parsed_args):\n    if len(not_parsed_args) > 1:\n        print(""Unknown args:%s"" % not_parsed_args)\n        exit()\n\n    print(""Building Y channel data..."")\n\n    training_filenames = util.get_files_in_directory(FLAGS.data_dir + ""/"" + FLAGS.dataset + ""/"")\n    target_dir = FLAGS.data_dir + ""/"" + FLAGS.dataset + ""_y/""\n    util.make_dir(target_dir)\n\n    for file_path in training_filenames:\n        org_image = util.load_image(file_path)\n        if org_image.shape[2] == 3:\n            org_image = util.convert_rgb_to_y(org_image)\n\n        filename = os.path.basename(file_path)\n        filename, extension = os.path.splitext(filename)\n\n        new_filename = target_dir + filename\n        util.save_image(new_filename + "".bmp"", org_image)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
evaluate.py,1,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nVer: 2.0\n\nFunctions for evaluating model performance\n\nPut your images under data/[your dataset name]/ and specify [your dataset name] for --test_dataset.\nThis script will create LR images from your test dataset and evaluate the model\'s performance.\n\n--save_results=True: will provide generated HR images and bi-cubic HR images.\nsee output/[model_name]/data/[your test data]/ for checking result images.\n\nAlso you must put same model args as you trained.\n\nFor ex, if you trained like below,\n> python train.py --scale=3\n\nThen you must run evaluate.py like below.\n> python evaluate.py --scale=3 --file=your_image_file_path\n\n\nIf you trained like below,\n> python train.py --dataset=bsd200 --layers=8 --filters=96 --training_images=30000\n\nThen you must run evaluate.py like below.\n> python evaluate.py --layers=8 --filters=96 --file=your_image_file_path\n""""""\n\nimport logging\n\nimport tensorflow as tf\n\nimport time\nimport DCSCN\nfrom helper import args, utilty as util\n\nargs.flags.DEFINE_boolean(""save_results"", True, ""Save result, bicubic and loss images."")\nargs.flags.DEFINE_boolean(""compute_bicubic"", False, ""Compute bicubic performance."")\n\nFLAGS = args.get()\n\n\ndef main(not_parsed_args):\n    if len(not_parsed_args) > 1:\n        print(""Unknown args:%s"" % not_parsed_args)\n        exit()\n\n    model = DCSCN.SuperResolution(FLAGS, model_name=FLAGS.model_name)\n    if (FLAGS.frozenInference):\n        model.load_graph(FLAGS.frozen_graph_path)\n        model.build_summary_saver(with_saver=False) # no need because we are not saving any variables\n    else:\n        model.build_graph()\n        model.build_summary_saver()\n    model.init_all_variables()\n\n    if FLAGS.test_dataset == ""all"":\n        test_list = [\'set5\', \'set14\', \'bsd100\']\n    else:\n        test_list = [FLAGS.test_dataset]\n\n    for i in range(FLAGS.tests):\n        if (not FLAGS.frozenInference):\n            model.load_model(FLAGS.load_model_name, trial=i, output_log=True if FLAGS.tests > 1 else False)\n\n        if FLAGS.compute_bicubic:\n            for test_data in test_list:\n                print(test_data)\n                evaluate_bicubic(model, test_data)\n\n        for test_data in test_list:\n            evaluate_model(model, test_data)\n\n\ndef evaluate_bicubic(model, test_data):\n    test_filenames = util.get_files_in_directory(FLAGS.data_dir + ""/"" + test_data)\n    total_psnr = total_ssim = 0\n\n    for filename in test_filenames:\n        psnr, ssim = model.evaluate_bicubic(filename, print_console=False)\n        total_psnr += psnr\n        total_ssim += ssim\n\n    logging.info(""Bicubic Average [%s] PSNR:%f, SSIM:%f"" % (\n        test_data, total_psnr / len(test_filenames), total_ssim / len(test_filenames)))\n\n\ndef evaluate_model(model, test_data):\n    test_filenames = util.get_files_in_directory(FLAGS.data_dir + ""/"" + test_data)\n    total_psnr = total_ssim = total_time = 0\n\n    for filename in test_filenames:\n        start = time.time()\n        if FLAGS.save_results:\n            psnr, ssim = model.do_for_evaluate_with_output(filename, output_directory=FLAGS.output_dir,\n                                                           print_console=False)\n        else:\n            psnr, ssim = model.do_for_evaluate(filename, print_console=False)\n        end = time.time()\n        elapsed_time = end - start\n        total_psnr += psnr\n        total_ssim += ssim\n        total_time += elapsed_time\n\n    logging.info(""Model Average [%s] PSNR:%f, SSIM:%f, Time (s): %f"" % (\n        test_data, total_psnr / len(test_filenames), total_ssim / len(test_filenames), total_time / len(test_filenames)))\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
sr.py,1,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nAuthor: Jin Yamanaka\nGithub: https://github.com/jiny2001/dcscn-image-super-resolution\nVer: 2.0\n\nApply Super Resolution for image file.\n\n--file=[your image filename]: will generate HR images.\nsee output/[model_name]/ for checking result images.\n\nAlso you must put same model args as you trained.\n\nFor ex, if you trained like below,\n> python train.py --scale=3\n\nThen you must run sr.py like below.\n> python sr.py --scale=3 --file=your_image_file_path\n\n\nIf you trained like below,\n> python train.py --dataset=bsd200 --layers=8 --filters=96 --training_images=30000\n\nThen you must run sr.py like below.\n> python sr.py --layers=8 --filters=96 --file=your_image_file_path\n\n""""""\n\nimport tensorflow as tf\n\nimport DCSCN\nfrom helper import args\n\nargs.flags.DEFINE_string(""file"", ""image.jpg"", ""Target filename"")\nFLAGS = args.get()\n\n\ndef main(_):\n    model = DCSCN.SuperResolution(FLAGS, model_name=FLAGS.model_name)\n    model.build_graph()\n    model.build_optimizer()\n    model.build_summary_saver()\n\n    model.init_all_variables()\n    model.load_model()\n\n    model.do_for_file(FLAGS.file, FLAGS.output_dir)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
train.py,1,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nAuthor: Jin Yamanaka\nGithub: https://github.com/jiny2001/dcscn-image-super-resolution\nVer: 2.0\n\nDCSCN training functions.\nTesting Environment: Python 3.6.1, tensorflow >= 1.3.0\n""""""\n\nimport logging\nimport sys\n\nimport tensorflow as tf\n\nimport DCSCN\nfrom helper import args, utilty as util\n\nFLAGS = args.get()\n\n\ndef main(not_parsed_args):\n    if len(not_parsed_args) > 1:\n        print(""Unknown args:%s"" % not_parsed_args)\n        exit()\n\n    model = DCSCN.SuperResolution(FLAGS, model_name=FLAGS.model_name)\n\n    if FLAGS.build_batch:\n        model.load_datasets(FLAGS.data_dir + ""/"" + FLAGS.dataset, FLAGS.batch_dir + ""/"" + FLAGS.dataset,\n                            FLAGS.batch_image_size, FLAGS.stride_size)\n    else:\n        model.load_dynamic_datasets(FLAGS.data_dir + ""/"" + FLAGS.dataset, FLAGS.batch_image_size)\n    model.build_graph()\n    model.build_optimizer()\n    model.build_summary_saver()\n\n    logging.info(""\\n"" + str(sys.argv))\n    logging.info(""Test Data:"" + FLAGS.test_dataset + "" Training Data:"" + FLAGS.dataset)\n    util.print_num_of_total_parameters(output_to_logging=True)\n\n    total_psnr = total_ssim = 0\n\n    for i in range(FLAGS.tests):\n        psnr, ssim = train(model, FLAGS, i)\n        total_psnr += psnr\n        total_ssim += ssim\n\n        logging.info(""\\nTrial(%d) %s"" % (i, util.get_now_date()))\n        model.print_steps_completed(output_to_logging=True)\n        logging.info(""PSNR:%f, SSIM:%f\\n"" % (psnr, ssim))\n\n    if FLAGS.tests > 1:\n        logging.info(""\\n=== Final Average [%s] PSNR:%f, SSIM:%f ==="" % (\n            FLAGS.test_dataset, total_psnr / FLAGS.tests, total_ssim / FLAGS.tests))\n\n    model.copy_log_to_archive(""archive"")\n\n\ndef train(model, flags, trial):\n    test_filenames = util.get_files_in_directory(flags.data_dir + ""/"" + flags.test_dataset)\n    if len(test_filenames) <= 0:\n        print(""Can\'t load images from [%s]"" % (flags.data_dir + ""/"" + flags.test_dataset))\n        exit()\n\n    model.init_all_variables()\n    if flags.load_model_name != """":\n        model.load_model(flags.load_model_name, output_log=True)\n\n    model.init_train_step()\n    model.init_epoch_index()\n    model_updated = True\n\n    psnr, ssim = model.evaluate(test_filenames)\n    model.print_status(psnr, ssim, log=True)\n    model.log_to_tensorboard(test_filenames[0], psnr, save_meta_data=True)\n\n    while model.lr > flags.end_lr:\n\n        model.build_input_batch()\n        model.train_batch()\n\n        if model.training_step * model.batch_num >= model.training_images:\n\n            # one training epoch finished\n            model.epochs_completed += 1\n            psnr, ssim = model.evaluate(test_filenames)\n            model.print_status(psnr, ssim, log=model_updated)\n            model.log_to_tensorboard(test_filenames[0], psnr, save_meta_data=model_updated)\n            model.save_model(trial=trial, output_log=False)\n\n            model_updated = model.update_epoch_and_lr()\n            model.init_epoch_index()\n\n    model.end_train_step()\n\n    # save last generation anyway\n    model.save_model(trial=trial, output_log=True)\n\n    # outputs result\n    evaluate_model(model, flags.test_dataset)\n\n    if FLAGS.do_benchmark:\n        for test_data in [\'set5\', \'set14\', \'bsd100\']:\n            if test_data != flags.test_dataset:\n                evaluate_model(model, test_data)\n\n    return psnr, ssim\n\n\ndef evaluate_model(model, test_data):\n    test_filenames = util.get_files_in_directory(FLAGS.data_dir + ""/"" + test_data)\n    total_psnr = total_ssim = 0\n\n    for filename in test_filenames:\n        psnr, ssim = model.do_for_evaluate_with_output(filename, output_directory=FLAGS.output_dir, print_console=False)\n        total_psnr += psnr\n        total_ssim += ssim\n\n    logging.info(""Model Average [%s] PSNR:%f, SSIM:%f"" % (\n        test_data, total_psnr / len(test_filenames), total_ssim / len(test_filenames)))\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
helper/args.py,2,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nVer: 2\n\nfunctions for sharing arguments and their default values\n""""""\n\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Model (network) Parameters\nflags.DEFINE_integer(""scale"", 2, ""Scale factor for Super Resolution (should be 2 or more)"")\nflags.DEFINE_integer(""layers"", 12, ""Number of layers of feature xxtraction CNNs"")\nflags.DEFINE_integer(""filters"", 196, ""Number of filters of first feature-extraction CNNs"")\nflags.DEFINE_integer(""min_filters"", 48, ""Number of filters of last feature-extraction CNNs"")\nflags.DEFINE_float(""filters_decay_gamma"", 1.5,\n                   ""Number of CNN filters are decayed from [filters] to [min_filters] by this gamma"")\nflags.DEFINE_boolean(""use_nin"", True, ""Use Network In Network"")\nflags.DEFINE_integer(""nin_filters"", 64, ""Number of CNN filters in A1 at Reconstruction network"")\nflags.DEFINE_integer(""nin_filters2"", 32, ""Number of CNN filters in B1 and B2 at Reconstruction net."")\nflags.DEFINE_integer(""cnn_size"", 3, ""Size of CNN filters"")\nflags.DEFINE_integer(""reconstruct_layers"", 1, ""Number of Reconstruct CNN Layers. (can be 0.)"")\nflags.DEFINE_integer(""reconstruct_filters"", 32, ""Number of Reconstruct CNN Filters"")\nflags.DEFINE_float(""dropout_rate"", 0.8, ""Output nodes should be kept by this probability. If 1, don\'t use dropout."")\nflags.DEFINE_string(""activator"", ""prelu"", ""Activator can be [relu, leaky_relu, prelu, sigmoid, tanh, selu]"")\nflags.DEFINE_boolean(""pixel_shuffler"", True, ""Use Pixel Shuffler instead of transposed CNN"")\nflags.DEFINE_integer(""pixel_shuffler_filters"", 0,\n                     ""Num of Pixel Shuffler output channels. 0 means use same channels as input."")\nflags.DEFINE_integer(""self_ensemble"", 8, ""Number of using self ensemble method. [1 - 8]"")\nflags.DEFINE_boolean(""batch_norm"", False, ""use batch normalization after each CNNs"")\nflags.DEFINE_boolean(""depthwise_separable"", False, ""use depthwise seperable convolutions for each CNN layer instead"")\n\n# Training Parameters\nflags.DEFINE_boolean(""bicubic_init"", True, ""make bicubic interpolation values as initial input for x2"")\nflags.DEFINE_float(""clipping_norm"", 5, ""Norm for gradient clipping. If it\'s <= 0 we don\'t use gradient clipping."")\nflags.DEFINE_string(""initializer"", ""he"", ""Initializer for weights can be [uniform, stddev, xavier, he, identity, zero]"")\nflags.DEFINE_float(""weight_dev"", 0.01, ""Initial weight stddev (won\'t be used when you use he or xavier initializer)"")\nflags.DEFINE_float(""l2_decay"", 0.0001, ""l2_decay"")\nflags.DEFINE_string(""optimizer"", ""adam"", ""Optimizer can be [gd, momentum, adadelta, adagrad, adam, rmsprop]"")\nflags.DEFINE_float(""beta1"", 0.9, ""Beta1 for adam optimizer"")\nflags.DEFINE_float(""beta2"", 0.999, ""Beta2 for adam optimizer"")\nflags.DEFINE_float(""epsilon"", 1e-8, ""epsilon for adam optimizer"")\nflags.DEFINE_float(""momentum"", 0.9, ""Momentum for momentum optimizer and rmsprop optimizer"")\nflags.DEFINE_integer(""batch_num"", 20, ""Number of mini-batch images for training"")\nflags.DEFINE_integer(""batch_image_size"", 48, ""Image size for mini-batch"")\nflags.DEFINE_integer(""stride_size"", 0, ""Stride size for mini-batch. If it is 0, use half of batch_image_size"")\nflags.DEFINE_integer(""training_images"", 24000, ""Number of training on each epoch"")\nflags.DEFINE_boolean(""use_l1_loss"", False, ""Use L1 Error as loss function instead of MSE Error."")\n\n# Learning Rate Control for Training\nflags.DEFINE_float(""initial_lr"", 0.002, ""Initial learning rate"")\nflags.DEFINE_float(""lr_decay"", 0.5, ""Learning rate decay rate"")\nflags.DEFINE_integer(""lr_decay_epoch"", 9, ""After this epochs are completed, learning rate will be decayed by lr_decay."")\nflags.DEFINE_float(""end_lr"", 2e-5, ""Training end learning rate. If the current learning rate gets lower than this""\n                                   ""value, then training will be finished."")\n\n# Dataset or Others\nflags.DEFINE_string(""dataset"", ""bsd200"", ""Training dataset dir. [yang91, general100, bsd200, other]"")\nflags.DEFINE_string(""test_dataset"", ""set5"", ""Directory for test dataset [set5, set14, bsd100, urban100, all]"")\nflags.DEFINE_integer(""tests"", 1, ""Number of training sets"")\nflags.DEFINE_boolean(""do_benchmark"", False, ""Evaluate the performance for set5, set14 and bsd100 after the training."")\n\n# Image Processing\nflags.DEFINE_float(""max_value"", 255, ""For normalize image pixel value"")\nflags.DEFINE_integer(""channels"", 1, ""Number of image channels used. Now it should be 1. using only Y from YCbCr."")\nflags.DEFINE_integer(""psnr_calc_border_size"", -1,\n                     ""Cropping border size for calculating PSNR. if < 0, use 2 + scale for default."")\nflags.DEFINE_boolean(""build_batch"", False, ""Build pre-processed input batch. Makes training significantly faster but ""\n                                           ""the patches are limited to be on the grid."")\n# flags.DEFINE_integer(""input_image_width"", -1, ""The width of the input image. Put -1 if you do not want to have a fixed input size"")\n# flags.DEFINE_integer(""input_image_height"", -1, ""The height of the input image. Put -1 if you do not want to hae a fixed input size"")\n\n# Environment (all directory name should not contain \'/\' after )\nflags.DEFINE_string(""checkpoint_dir"", ""models"", ""Directory for checkpoints"")\nflags.DEFINE_string(""graph_dir"", ""graphs"", ""Directory for graphs"")\nflags.DEFINE_string(""data_dir"", ""data"", ""Directory for original images"")\nflags.DEFINE_string(""batch_dir"", ""batch_data"", ""Directory for training batch images"")\nflags.DEFINE_string(""output_dir"", ""output"", ""Directory for output test images"")\nflags.DEFINE_string(""tf_log_dir"", ""tf_log"", ""Directory for tensorboard log"")\nflags.DEFINE_string(""log_filename"", ""log.txt"", ""log filename"")\nflags.DEFINE_string(""model_name"", """", ""model name for save files and tensorboard log"")\nflags.DEFINE_string(""load_model_name"", """", ""Filename of model loading before start [filename or \'default\']"")\n\n# Debugging or Logging\nflags.DEFINE_boolean(""initialize_tf_log"", True, ""Clear all tensorboard log before start"")\nflags.DEFINE_boolean(""enable_log"", True, ""Enables tensorboard-log. Save loss."")\nflags.DEFINE_boolean(""save_weights"", True, ""Save weights and biases/gradients"")\nflags.DEFINE_boolean(""save_images"", False, ""Save CNN weights as images"")\nflags.DEFINE_integer(""save_images_num"", 20, ""Number of CNN images saved"")\nflags.DEFINE_boolean(""save_meta_data"", False, """")\nflags.DEFINE_integer(""gpu_device_id"", 0, ""Device ID of GPUs which will be used to compute."")\n\n# frozen model configurations\nflags.DEFINE_boolean(""frozenInference"", False, ""Flag for whether the model to evaluate is frozen."")\nflags.DEFINE_string(""frozen_graph_path"", \'./model_to_freeze/frozen_model_optimized.pb\', ""the path to a frozen model if performing inference from it"")\n\ndef get():\n    print(""Python Interpreter version:%s"" % sys.version[:3])\n    print(""tensorflow version:%s"" % tf.__version__)\n    print(""numpy version:%s"" % np.__version__)\n\n    # check which library you are using\n    # np.show_config()\n    return FLAGS\n'"
helper/custom_freeze_graph.py,7,"b'import os\nimport argparse\n\nimport tensorflow as tf\n\n# from: https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc\n\n# The original freeze_graph function\n# from tensorflow.python.tools.freeze_graph import freeze_graph\n\ndir = os.path.dirname(os.path.realpath(__file__))\n\n\ndef freeze_graph(model_dir, output_node_names):\n    """"""Extract the sub graph defined by the output nodes and convert\n    all its variables into constant\n    Args:\n        model_dir: the root folder containing the checkpoint state file\n        output_node_names: a string, containing all the output node\'s names,\n                            comma separated\n    """"""\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            ""Export directory doesn\'t exists. Please specify an export ""\n            ""directory: %s"" % model_dir)\n\n    if not output_node_names:\n        print(""You need to supply the name of a node to --output_node_names."")\n        return -1\n\n     # We retrieve our checkpoint fullpath\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    print(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n     # We precise the file fullname of our freezed graph\n    absolute_model_dir = ""/"".join(input_checkpoint.split(\'/\')[:-1])\n    output_graph = absolute_model_dir + ""/frozen_model.pb""\n\n     # We clear devices to allow TensorFlow to control on which device it will load operations\n    clear_devices = True\n\n     # We start a session using a temporary fresh Graph\n    with tf.Session(graph=tf.Graph()) as sess:\n        # We import the meta graph in the current default Graph\n        saver = tf.train.import_meta_graph(input_checkpoint + \'.meta\', clear_devices=clear_devices)\n\n         # We restore the weights\n        saver.restore(sess, input_checkpoint)\n\n         # We use a built-in TF helper to export variables to constants\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess, # The session is used to retrieve the weights\n            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n            output_node_names.split("","") # The output node names are used to select the usefull nodes\n        ) \n\n         # # Finally we serialize and dump the output graph to the filesystem\n        with tf.gfile.GFile(output_graph, ""wb"") as f:\n            f.write(output_graph_def.SerializeToString())\n        print(""%d ops in the final graph."" % len(output_graph_def.node))\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--model_dir"", type=str, default="""", help=""Model folder to export"")\n    parser.add_argument(""--output_node_names"", type=str, default="""", help=""The name of the output nodes, comma separated."")\n    args = parser.parse_args()\n\n    freeze_graph(args.model_dir, args.output_node_names) \n'"
helper/loader.py,0,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nVer: 2\n\nfunctions for loading/converting data\n""""""\n\nimport configparser\nimport logging\nimport os\nimport random\n\nimport numpy as np\nfrom scipy import misc\n\nfrom helper import utilty as util\n\nINPUT_IMAGE_DIR = ""input""\nINTERPOLATED_IMAGE_DIR = ""interpolated""\nTRUE_IMAGE_DIR = ""true""\n\n\ndef build_image_set(file_path, channels=1, scale=1, convert_ycbcr=True, resampling_method=""bicubic"",\n                    print_console=True):\n    true_image = util.set_image_alignment(util.load_image(file_path, print_console=print_console), scale)\n\n    if channels == 1 and true_image.shape[2] == 3 and convert_ycbcr:\n        true_image = util.convert_rgb_to_y(true_image)\n\n    input_image = util.resize_image_by_pil(true_image, 1.0 / scale, resampling_method=resampling_method)\n    input_interpolated_image = util.resize_image_by_pil(input_image, scale, resampling_method=resampling_method)\n\n    return input_image, input_interpolated_image, true_image\n\n\ndef load_input_image(filename, width=0, height=0, channels=1, scale=1, alignment=0, convert_ycbcr=True,\n                     print_console=True):\n    image = util.load_image(filename, print_console=print_console)\n    return build_input_image(image, width, height, channels, scale, alignment, convert_ycbcr)\n\n\ndef build_input_image(image, width=0, height=0, channels=1, scale=1, alignment=0, convert_ycbcr=True):\n    """"""\n    build input image from file.\n    crop, adjust the image alignment for the scale factor, resize, convert color space.\n    """"""\n\n    if width != 0 and height != 0:\n        if image.shape[0] != height or image.shape[1] != width:\n            x = (image.shape[1] - width) // 2\n            y = (image.shape[0] - height) // 2\n            image = image[y: y + height, x: x + width, :]\n\n    if alignment > 1:\n        image = util.set_image_alignment(image, alignment)\n\n    if channels == 1 and image.shape[2] == 3:\n        if convert_ycbcr:\n            image = util.convert_rgb_to_y(image)\n    else:\n        if convert_ycbcr:\n            image = util.convert_rgb_to_ycbcr(image)\n\n    if scale != 1:\n        image = util.resize_image_by_pil(image, 1.0 / scale)\n\n    return image\n\n\nclass BatchDataSets:\n    def __init__(self, scale, batch_dir, batch_image_size, stride_size=0, channels=1, resampling_method=""bicubic""):\n\n        self.scale = scale\n        self.batch_image_size = batch_image_size\n        if stride_size == 0:\n            self.stride = batch_image_size // 2\n        else:\n            self.stride = stride_size\n        self.channels = channels\n        self.resampling_method = resampling_method\n        self.count = 0\n        self.batch_dir = batch_dir\n        self.batch_index = None\n\n    def build_batch(self, data_dir):\n        """""" Build batch images and. """"""\n\n        print(""Building batch images for %s..."" % self.batch_dir)\n        filenames = util.get_files_in_directory(data_dir)\n        images_count = 0\n\n        util.make_dir(self.batch_dir)\n        util.clean_dir(self.batch_dir)\n        util.make_dir(self.batch_dir + ""/"" + INPUT_IMAGE_DIR)\n        util.make_dir(self.batch_dir + ""/"" + INTERPOLATED_IMAGE_DIR)\n        util.make_dir(self.batch_dir + ""/"" + TRUE_IMAGE_DIR)\n\n        processed_images = 0\n        for filename in filenames:\n            output_window_size = self.batch_image_size * self.scale\n            output_window_stride = self.stride * self.scale\n\n            input_image, input_interpolated_image, true_image = \\\n                build_image_set(filename, channels=self.channels, resampling_method=self.resampling_method,\n                                scale=self.scale, print_console=False)\n\n            # split into batch images\n            input_batch_images = util.get_split_images(input_image, self.batch_image_size, stride=self.stride)\n            input_interpolated_batch_images = util.get_split_images(input_interpolated_image, output_window_size,\n                                                                    stride=output_window_stride)\n\n            if input_batch_images is None or input_interpolated_batch_images is None:\n                # if the original image size * scale is less than batch image size\n                continue\n            input_count = input_batch_images.shape[0]\n\n            true_batch_images = util.get_split_images(true_image, output_window_size, stride=output_window_stride)\n\n            for i in range(input_count):\n                self.save_input_batch_image(images_count, input_batch_images[i])\n                self.save_interpolated_batch_image(images_count, input_interpolated_batch_images[i])\n                self.save_true_batch_image(images_count, true_batch_images[i])\n                images_count += 1\n            processed_images += 1\n            if processed_images % 10 == 0:\n                print(\'.\', end=\'\', flush=True)\n\n        print(""Finished"")\n        self.count = images_count\n\n        print(""%d mini-batch images are built(saved)."" % images_count)\n\n        config = configparser.ConfigParser()\n        config.add_section(""batch"")\n        config.set(""batch"", ""count"", str(images_count))\n        config.set(""batch"", ""scale"", str(self.scale))\n        config.set(""batch"", ""batch_image_size"", str(self.batch_image_size))\n        config.set(""batch"", ""stride"", str(self.stride))\n        config.set(""batch"", ""channels"", str(self.channels))\n\n        with open(self.batch_dir + ""/batch_images.ini"", ""w"") as configfile:\n            config.write(configfile)\n\n    def load_batch_counts(self):\n        """""" load already built batch images. """"""\n\n        if not os.path.isdir(self.batch_dir):\n            self.count = 0\n            return\n\n        config = configparser.ConfigParser()\n        try:\n            with open(self.batch_dir + ""/batch_images.ini"") as f:\n                config.read_file(f)\n            self.count = config.getint(""batch"", ""count"")\n\n        except IOError:\n            self.count = 0\n            return\n\n    def load_all_batch_images(self):\n\n        print(""Allocating memory for all batch images."")\n        self.input_images = np.zeros(shape=[self.count, self.batch_image_size, self.batch_image_size, 1],\n                                     dtype=np.uint8)  # type: np.ndarray\n        self.input_interpolated_images = np.zeros(\n            shape=[self.count, self.batch_image_size * self.scale, self.batch_image_size * self.scale, 1],\n            dtype=np.uint8)  # type: np.ndarray\n        self.true_images = np.zeros(\n            shape=[self.count, self.batch_image_size * self.scale, self.batch_image_size * self.scale, 1],\n            dtype=np.uint8)  # type: np.ndarray\n\n        print(""Loading all batch images."")\n        for i in range(self.count):\n            self.input_images[i] = self.load_input_batch_image(i)\n            self.input_interpolated_images[i] = self.load_interpolated_batch_image(i)\n            self.true_images[i] = self.load_true_batch_image(i)\n            if i % 1000 == 0:\n                print(\'.\', end=\'\', flush=True)\n        print(""Load finished."")\n\n    def release_batch_images(self):\n\n        if hasattr(self, \'input_images\'):\n            del self.input_images\n        self.input_images = None\n\n        if hasattr(self, \'input_interpolated_images\'):\n            del self.input_interpolated_images\n        self.input_interpolated_images = None\n\n        if hasattr(self, \'true_images\'):\n            del self.true_images\n        self.true_images = None\n\n    def is_batch_exist(self):\n        if not os.path.isdir(self.batch_dir):\n            return False\n\n        config = configparser.ConfigParser()\n        try:\n            with open(self.batch_dir + ""/batch_images.ini"") as f:\n                config.read_file(f)\n\n            if config.getint(""batch"", ""count"") <= 0:\n                return False\n\n            if config.getint(""batch"", ""scale"") != self.scale:\n                return False\n            if config.getint(""batch"", ""batch_image_size"") != self.batch_image_size:\n                return False\n            if config.getint(""batch"", ""stride"") != self.stride:\n                return False\n            if config.getint(""batch"", ""channels"") != self.channels:\n                return False\n\n            return True\n\n        except IOError:\n            return False\n\n    def init_batch_index(self):\n        self.batch_index = random.sample(range(0, self.count), self.count)\n        self.index = 0\n\n    def get_next_image_no(self):\n\n        if self.index >= self.count:\n            self.init_batch_index()\n\n        image_no = self.batch_index[self.index]\n        self.index += 1\n        return image_no\n\n    def load_batch_image_from_disk(self, image_number):\n\n        image_number = image_number % self.count\n\n        input_image = self.load_input_batch_image(image_number)\n        input_interpolated = self.load_interpolated_batch_image(image_number)\n        true = self.load_true_batch_image(image_number)\n\n        return input_image, input_interpolated, true\n\n    def load_batch_image(self, max_value):\n\n        number = self.get_next_image_no()\n        if max_value == 255:\n            return self.input_images[number], self.input_interpolated_images[number], self.true_images[number]\n        else:\n            scale = max_value / 255.0\n            return np.multiply(self.input_images[number], scale), \\\n                np.multiply(self.input_interpolated_images[number], scale), \\\n                np.multiply(self.true_images[number], scale)\n\n    def load_input_batch_image(self, image_number):\n        image = misc.imread(self.batch_dir + ""/"" + INPUT_IMAGE_DIR + ""/%06d.bmp"" % image_number)\n        return image.reshape(image.shape[0], image.shape[1], 1)\n\n    def load_interpolated_batch_image(self, image_number):\n        image = misc.imread(self.batch_dir + ""/"" + INTERPOLATED_IMAGE_DIR + ""/%06d.bmp"" % image_number)\n        return image.reshape(image.shape[0], image.shape[1], 1)\n\n    def load_true_batch_image(self, image_number):\n        image = misc.imread(self.batch_dir + ""/"" + TRUE_IMAGE_DIR + ""/%06d.bmp"" % image_number)\n        return image.reshape(image.shape[0], image.shape[1], 1)\n\n    def save_input_batch_image(self, image_number, image):\n        return util.save_image(self.batch_dir + ""/"" + INPUT_IMAGE_DIR + ""/%06d.bmp"" % image_number, image)\n\n    def save_interpolated_batch_image(self, image_number, image):\n        return util.save_image(self.batch_dir + ""/"" + INTERPOLATED_IMAGE_DIR + ""/%06d.bmp"" % image_number, image)\n\n    def save_true_batch_image(self, image_number, image):\n        return util.save_image(self.batch_dir + ""/"" + TRUE_IMAGE_DIR + ""/%06d.bmp"" % image_number, image)\n\n\nclass DynamicDataSets:\n    def __init__(self, scale, batch_image_size, channels=1, resampling_method=""bicubic""):\n\n        self.scale = scale\n        self.batch_image_size = batch_image_size\n        self.channels = channels\n        self.resampling_method = resampling_method\n\n        self.filenames = []\n        self.count = 0\n        self.batch_index = None\n\n    def set_data_dir(self, data_dir):\n        self.filenames = util.get_files_in_directory(data_dir)\n        self.count = len(self.filenames)\n        if self.count <= 0:\n            logging.error(""Data Directory is empty."")\n            exit(-1)\n\n    def init_batch_index(self):\n        self.batch_index = random.sample(range(0, self.count), self.count)\n        self.index = 0\n\n    def get_next_image_no(self):\n\n        if self.index >= self.count:\n            self.init_batch_index()\n\n        image_no = self.batch_index[self.index]\n        self.index += 1\n        return image_no\n\n    def load_batch_image(self, max_value):\n\n        """""" index won\'t be used. """"""\n\n        image = None\n        while image is None:\n            image = self.load_random_patch(self.filenames[self.get_next_image_no()])\n\n        if random.randrange(2) == 0:\n            image = np.fliplr(image)\n\n        input_image = util.resize_image_by_pil(image, 1 / self.scale)\n        input_bicubic_image = util.resize_image_by_pil(input_image, self.scale)\n\n        if max_value != 255:\n            scale = max_value / 255.0\n            input_image = np.multiply(input_image, scale)\n            input_bicubic_image = np.multiply(input_bicubic_image, scale)\n            image = np.multiply(image, scale)\n\n        return input_image, input_bicubic_image, image\n\n    def load_random_patch(self, filename):\n\n        image = util.load_image(filename, print_console=False)\n        height, width = image.shape[0:2]\n\n        load_batch_size = self.batch_image_size * self.scale\n\n        if height < load_batch_size or width < load_batch_size:\n            print(""Error: %s should have more than %d x %d size."" % (filename, load_batch_size, load_batch_size))\n            return None\n\n        if height == load_batch_size:\n            y = 0\n        else:\n            y = random.randrange(height - load_batch_size)\n\n        if width == load_batch_size:\n            x = 0\n        else:\n            x = random.randrange(width - load_batch_size)\n        image = image[y:y + load_batch_size, x:x + load_batch_size, :]\n        image = build_input_image(image, channels=self.channels, convert_ycbcr=True)\n\n        return image\n'"
helper/optimize_for_inference.py,0,"b'# pylint: disable=g-bad-file-header\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Removes parts of a graph that are only needed for training.\n\nThere are several common transformations that can be applied to GraphDefs\ncreated to train a model, that help reduce the amount of computation needed when\nthe network is used only for inference. These include:\n\n - Removing training-only operations like checkpoint saving.\n\n - Stripping out parts of the graph that are never reached.\n\n - Removing debug operations like CheckNumerics.\n\n - Folding batch normalization ops into the pre-calculated weights.\n\n - Fusing common operations into unified versions.\n\nThis script takes either a frozen binary GraphDef file (where the weight\nvariables have been converted into constants by the freeze_graph script), or a\ntext GraphDef proto file (the weight variables are stored in a separate\ncheckpoint file), and outputs a new GraphDef with the optimizations applied.\n\nIf the input graph is a text graph file, make sure to include the node that\nrestores the variable weights in output_names. That node is usually named\n""restore_all"".\n\nAn example of command-line usage is:\n\nbazel build tensorflow/python/tools:optimize_for_inference && \\\nbazel-bin/tensorflow/python/tools/optimize_for_inference \\\n--input=frozen_inception_graph.pb \\\n--output=optimized_inception_graph.pb \\\n--frozen_graph=True \\\n--input_names=Mul \\\n--output_names=softmax\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nfrom google.protobuf import text_format\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import graph_io\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.tools import optimize_for_inference_lib\n\nFLAGS = None\n\n\ndef main(unused_args):\n    if not gfile.Exists(FLAGS.input):\n        print(""Input graph file \'"" + FLAGS.input + ""\' does not exist!"")\n        return -1\n\n    input_graph_def = graph_pb2.GraphDef()\n    with gfile.Open(FLAGS.input, ""rb"") as f:\n        data = f.read()\n        if FLAGS.frozen_graph:\n            input_graph_def.ParseFromString(data)\n        else:\n            text_format.Merge(data.decode(""utf-8""), input_graph_def)\n\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n        input_graph_def,\n        FLAGS.input_names.split("",""),\n        FLAGS.output_names.split("",""),\n        FLAGS.placeholder_type_enum,\n        FLAGS.toco_compatible)\n\n    if FLAGS.frozen_graph:\n        f = gfile.FastGFile(FLAGS.output, ""w"")\n        f.write(output_graph_def.SerializeToString())\n    else:\n        graph_io.write_graph(output_graph_def,\n                             os.path.dirname(FLAGS.output),\n                             os.path.basename(FLAGS.output))\n    return 0\n\n\ndef parse_args():\n    """"""Parses command line arguments.""""""\n    parser = argparse.ArgumentParser()\n    parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n    parser.add_argument(\n        ""--input"",\n        type=str,\n        default="""",\n        help=""TensorFlow \\\'GraphDef\\\' file to load."")\n    parser.add_argument(\n        ""--output"",\n        type=str,\n        default="""",\n        help=""File to save the output graph to."")\n    parser.add_argument(\n        ""--input_names"",\n        type=str,\n        default="""",\n        help=""Input node names, comma separated."")\n    parser.add_argument(\n        ""--output_names"",\n        type=str,\n        default="""",\n        help=""Output node names, comma separated."")\n    parser.add_argument(\n        ""--frozen_graph"",\n        nargs=""?"",\n        const=True,\n        type=""bool"",\n        default=True,\n        help=""""""\\\n      If true, the input graph is a binary frozen GraphDef\n      file; if false, it is a text GraphDef proto file.\\\n      """""")\n    parser.add_argument(\n        ""--placeholder_type_enum"",\n        type=int,\n        default=dtypes.float32.as_datatype_enum,\n        help=""The AttrValue enum to use for placeholders."")\n    parser.add_argument(\n        ""--toco_compatible"",\n        type=bool,\n        default=False,\n        help=""""""\\\n      If true, only use ops compatible with Tensorflow\n      Lite Optimizing Converter.\\\n      """""")\n    return parser.parse_known_args()\n\n\nif __name__ == ""__main__"":\n    FLAGS, unparsed = parse_args()\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
helper/tf_graph.py,34,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\nVer: 2\n\nfunctions for building tensorflow graph\n""""""\n\nimport logging\nimport os\nimport shutil\n\nimport tensorflow as tf\n\nfrom helper import utilty as util\n\n\nclass TensorflowGraph(tf.Graph):\n\n    def __init__(self, flags):\n        # inherit tf.Graph so some builtin methods can be used\n        super().__init__()\n\n        self.name = """"\n\n        # graph settings\n        self.dropout_rate = flags.dropout_rate\n        self.activator = flags.activator\n        self.batch_norm = flags.batch_norm\n        self.cnn_size = flags.cnn_size\n        self.cnn_stride = 1\n        self.initializer = flags.initializer\n        self.weight_dev = flags.weight_dev\n\n        # graph placeholders / objects\n        self.is_training = None\n        self.dropout = False\n        self.saver = None\n        self.summary_op = None\n        self.train_writer = None\n        self.test_writer = None\n\n        # Debugging or Logging\n        self.enable_log = flags.enable_log\n        self.save_weights = flags.save_weights and flags.enable_log\n        self.save_images = flags.save_images and flags.enable_log\n        self.save_images_num = flags.save_images_num\n        self.save_meta_data = flags.save_meta_data and flags.enable_log\n        self.log_weight_image_num = 32\n\n        # Environment (all directory name should not contain \'/\' after )\n        self.checkpoint_dir = flags.checkpoint_dir\n        self.tf_log_dir = flags.tf_log_dir\n\n        # status / attributes\n        self.Weights = []\n        self.Biases = []\n        self.features = """"\n        self.H = []\n        self.receptive_fields = 0\n        self.complexity = 0\n        self.pix_per_input = 1\n\n        self.init_session(flags.gpu_device_id)\n\n    def init_session(self, device_id=0):\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True  ## just for use the necesary memory of GPU\n        config.gpu_options.visible_device_list = str(device_id)  ## this values depends of numbers of GPUs\n\n        print(""Session and graph initialized."")\n        self.sess = tf.InteractiveSession(config=config, graph=self)\n\n    def init_all_variables(self):\n        self.sess.run(tf.global_variables_initializer())\n        print(""Model initialized."")\n\n    def build_activator(self, input_tensor, features: int, activator="""", leaky_relu_alpha=0.1, base_name=""""):\n        features = int(features)\n        if activator is None or """":\n            return\n        elif activator == ""relu"":\n            output = tf.nn.relu(input_tensor, name=base_name + ""_relu"")\n        elif activator == ""sigmoid"":\n            output = tf.nn.sigmoid(input_tensor, name=base_name + ""_sigmoid"")\n        elif activator == ""tanh"":\n            output = tf.nn.tanh(input_tensor, name=base_name + ""_tanh"")\n        elif activator == ""leaky_relu"":\n            output = tf.maximum(input_tensor, leaky_relu_alpha * input_tensor, name=base_name + ""_leaky"")\n        elif activator == ""prelu"":\n            with tf.variable_scope(""prelu""):\n                alphas = tf.Variable(tf.constant(0.1, shape=[features]), name=base_name + ""_prelu"")\n                if self.save_weights:\n                    util.add_summaries(""prelu_alpha"", self.name, alphas, save_stddev=False, save_mean=False)\n                output = tf.nn.relu(input_tensor) + tf.multiply(alphas, (input_tensor - tf.abs(input_tensor))) * 0.5\n        elif activator == ""selu"":\n            output = tf.nn.selu(input_tensor, name=base_name + ""_selu"")\n        else:\n            raise NameError(\'Not implemented activator:%s\' % activator)\n\n        self.complexity += (self.pix_per_input * features)\n\n        return output\n\n    def conv2d(self, input_tensor, w, stride, bias=None, use_batch_norm=False, name=""""):\n        output = tf.nn.conv2d(input_tensor, w, strides=[1, stride, stride, 1], padding=""SAME"", name=name + ""_conv"")\n        self.complexity += self.pix_per_input * int(w.shape[0] * w.shape[1] * w.shape[2] * w.shape[3])\n\n        if bias is not None:\n            output = tf.add(output, bias, name=name + ""_add"")\n            self.complexity += self.pix_per_input * int(bias.shape[0])\n\n        if use_batch_norm:\n            output = tf.layers.batch_normalization(output, training=self.is_training, name=\'BN\')\n\n        return output\n\n    def build_conv(self, name, input_tensor, cnn_size, input_feature_num, output_feature_num, use_bias=False,\n                   activator=None, use_batch_norm=False, dropout_rate=1.0):\n        with tf.variable_scope(name):\n            w = util.weight([cnn_size, cnn_size, input_feature_num, output_feature_num],\n                            stddev=self.weight_dev, name=""conv_W"", initializer=self.initializer)\n\n            b = util.bias([output_feature_num], name=""conv_B"") if use_bias else None\n            h = self.conv2d(input_tensor, w, self.cnn_stride, bias=b, use_batch_norm=use_batch_norm, name=name)\n\n            if activator is not None:\n                h = self.build_activator(h, output_feature_num, activator, base_name=name)\n\n            if dropout_rate < 1.0:\n                h = tf.nn.dropout(h, self.dropout, name=""dropout"")\n\n            self.H.append(h)\n\n            if self.save_weights:\n                util.add_summaries(""weight"", self.name, w, save_stddev=True, save_mean=True)\n                util.add_summaries(""output"", self.name, h, save_stddev=True, save_mean=True)\n                if use_bias:\n                    util.add_summaries(""bias"", self.name, b, save_stddev=True, save_mean=True)\n\n            if self.save_images and cnn_size > 1:\n                util.log_cnn_weights_as_images(self.name, w, max_outputs=self.save_images_num)\n\n        if self.receptive_fields == 0:\n            self.receptive_fields = cnn_size\n        else:\n            self.receptive_fields += (cnn_size - 1)\n        self.features += ""%d "" % output_feature_num\n\n        self.Weights.append(w)\n        if use_bias:\n            self.Biases.append(b)\n\n        return h\n\n    def depthwise_separable_conv2d(self, input_tensor, w, stride, channel_multiplier=1, bias=None, use_batch_norm=False, name=""""):\n        # w format is filter_height, filter_width, in_channels, out_channels\n        depthwise_filter = util.weight([int(w.shape[0]), int(w.shape[1]), int(w.shape[2]), channel_multiplier],\n                                        stddev=self.weight_dev, name=""depthwise_W"", initializer=self.initializer)\n        pointwise_filter = util.weight([1, 1, channel_multiplier * int(w.shape[2]), int(w.shape[3])],\n                                        stddev=self.weight_dev, name=""pointwise_W"", initializer=self.initializer)\n        output = tf.nn.separable_conv2d(input_tensor, \\\n            depthwise_filter, \\\n            pointwise_filter, \\\n            strides=[1, stride, stride, 1], \\\n            padding=""SAME"", \\\n            name=name + ""_conv"")\n        self.complexity += (self.pix_per_input * int(w.shape[0] * w.shape[1] * w.shape[2] * channel_multiplier) + \\\n                            self.pix_per_input * int(w.shape[2] * w.shape[3]))\n\n        if bias is not None:\n            output = tf.add(output, bias, name=name + ""_add"")\n            self.complexity += self.pix_per_input * int(bias.shape[0])\n\n        if use_batch_norm:\n            output = tf.layers.batch_normalization(output, training=self.is_training, name=\'BN\')\n\n        return output\n\n     # adding the use of depthwise separable convolutions\n    def build_depthwise_separable_conv(self, name, input_tensor, cnn_size, input_feature_num, output_feature_num, use_bias=False,\n                    activator=None, use_batch_norm=False, dropout_rate=1.0):\n        with tf.variable_scope(name):\n            w = util.weight([cnn_size, cnn_size, input_feature_num, output_feature_num],\n                            stddev=self.weight_dev, name=""conv_W"", initializer=self.initializer)\n\n            b = util.bias([output_feature_num], name=""conv_B"") if use_bias else None\n            h = self.depthwise_separable_conv2d(input_tensor, w, self.cnn_stride, bias=b, use_batch_norm=use_batch_norm, name=name)\n            \n            if activator is not None:\n                h = self.build_activator(h, output_feature_num, activator, base_name=name)\n\n            if dropout_rate < 1.0:\n                h = tf.nn.dropout(h, self.dropout, name=""dropout"")\n\n            self.H.append(h)\n\n            if self.save_weights:\n                util.add_summaries(""weight"", self.name, w, save_stddev=True, save_mean=True)\n                util.add_summaries(""output"", self.name, h, save_stddev=True, save_mean=True)\n                if use_bias:\n                    util.add_summaries(""bias"", self.name, b, save_stddev=True, save_mean=True)\n\n            if self.save_images and cnn_size > 1:\n                util.log_cnn_weights_as_images(self.name, w, max_outputs=self.save_images_num)\n\n        if self.receptive_fields == 0:\n            self.receptive_fields = cnn_size\n        else:\n            self.receptive_fields += (cnn_size - 1)\n        self.features += ""%d "" % output_feature_num\n\n        self.Weights.append(w)\n        if use_bias:\n            self.Biases.append(b)\n\n        return h\n\n\n    def build_transposed_conv(self, name, input_tensor, scale, channels):\n        with tf.variable_scope(name):\n            w = util.upscale_weight(scale=scale, channels=channels, name=""Tconv_W"")\n\n            batch_size = tf.shape(input_tensor)[0]\n            height = tf.shape(input_tensor)[1] * scale\n            width = tf.shape(input_tensor)[2] * scale\n\n            h = tf.nn.conv2d_transpose(input_tensor, w, output_shape=[batch_size, height, width, channels],\n                                       strides=[1, scale, scale, 1], name=name)\n\n        self.pix_per_input *= scale * scale\n        self.complexity += self.pix_per_input * util.get_upscale_filter_size(scale) * util.get_upscale_filter_size(\n            scale) * channels * channels\n        self.receptive_fields += 1\n\n        self.Weights.append(w)\n        self.H.append(h)\n\n    def build_pixel_shuffler_layer(self, name, h, scale, input_filters, output_filters, activator=None, depthwise_separable=False):\n        with tf.variable_scope(name):\n            if (depthwise_separable):\n                self.build_depthwise_separable_conv(name + ""_CNN"", h, self.cnn_size, input_filters, scale * scale * output_filters,\n                            use_batch_norm=False,\n                            use_bias=True)\n            else:\n                self.build_conv(name + ""_CNN"", h, self.cnn_size, input_filters, scale * scale * output_filters,\n                            use_batch_norm=False,\n                            use_bias=True)\n            self.H.append(tf.depth_to_space(self.H[-1], scale))\n            self.build_activator(self.H[-1], output_filters, activator, base_name=name)\n\n    def copy_log_to_archive(self, archive_name):\n        archive_directory = self.tf_log_dir + \'_\' + archive_name\n        model_archive_directory = archive_directory + \'/\' + self.name\n        util.make_dir(archive_directory)\n        util.delete_dir(model_archive_directory)\n        try:\n            shutil.copytree(self.tf_log_dir, model_archive_directory)\n            print(""tensorboard log archived to [%s]."" % model_archive_directory)\n        except OSError as e:\n            print(e)\n            print(""NG: tensorboard log archived to [%s]."" % model_archive_directory)\n\n    def load_model(self, name="""", trial=0, output_log=False):\n        if name == """" or name == ""default"":\n            name = self.name\n\n        if trial > 0:\n            filename = self.checkpoint_dir + ""/"" + name + ""_"" + str(trial) + "".ckpt""\n        else:\n            filename = self.checkpoint_dir + ""/"" + name + "".ckpt""\n\n        if not os.path.isfile(filename + "".index""):\n            print(""Error. [%s] is not exist!"" % filename)\n            exit(-1)\n\n        self.saver.restore(self.sess, filename)\n        if output_log:\n            logging.info(""Model restored [ %s ]."" % filename)\n        else:\n            print(""Model restored [ %s ]."" % filename)\n\n    def save_model(self, name="""", trial=0, output_log=False):\n        if name == """" or name == ""default"":\n            name = self.name\n\n        if trial > 0:\n            filename = self.checkpoint_dir + ""/"" + name + ""_"" + str(trial) + "".ckpt""\n        else:\n            filename = self.checkpoint_dir + ""/"" + name + "".ckpt""\n\n        self.saver.save(self.sess, filename)\n\n        if output_log:\n            logging.info(""Model saved [%s]."" % filename)\n        else:\n            print(""Model saved [%s]."" % filename)\n\n    def build_summary_saver(self, with_saver=True):\n        if self.enable_log:\n            self.summary_op = tf.summary.merge_all()\n            self.train_writer = tf.summary.FileWriter(self.tf_log_dir + ""/train"")\n            self.test_writer = tf.summary.FileWriter(self.tf_log_dir + ""/test"", graph=self.sess.graph)\n\n        if (with_saver):\n            self.saver = tf.train.Saver(max_to_keep=None)\n'"
helper/utilty.py,30,"b'""""""\nPaper: ""Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network""\n\nutility functions\n""""""\n\nimport datetime\nimport logging\nimport math\nimport os\nimport time\nfrom os import listdir\n\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nfrom os.path import isfile, join\nfrom scipy import misc\n\nfrom skimage.measure import compare_psnr, compare_ssim\n\n\nclass Timer:\n    def __init__(self, timer_count=100):\n        self.times = np.zeros(timer_count)\n        self.start_times = np.zeros(timer_count)\n        self.counts = np.zeros(timer_count)\n        self.timer_count = timer_count\n\n    def start(self, timer_id):\n        self.start_times[timer_id] = time.time()\n\n    def end(self, timer_id):\n        self.times[timer_id] += time.time() - self.start_times[timer_id]\n        self.counts[timer_id] += 1\n\n    def print(self):\n        for i in range(self.timer_count):\n            if self.counts[i] > 0:\n                total = 0\n                print(""Average of %d: %s[ms]"" % (i, ""{:,}"".format(self.times[i] * 1000 / self.counts[i])))\n                total += self.times[i]\n                print(""Total of %d: %s"" % (i, ""{:,}"".format(total)))\n\n\n# utilities for save / load\n\nclass LoadError(Exception):\n    def __init__(self, message):\n        self.message = message\n\n\ndef make_dir(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\ndef delete_dir(directory):\n    if os.path.exists(directory):\n        clean_dir(directory)\n        os.rmdir(directory)\n\n\ndef get_files_in_directory(path):\n    if not path.endswith(\'/\'):\n        path = path + ""/""\n    file_list = [path + f for f in listdir(path) if (isfile(join(path, f)) and not f.startswith(\'.\'))]\n    return file_list\n\n\ndef remove_generic(path, __func__):\n    try:\n        __func__(path)\n    except OSError as error:\n        print(""OS error: {0}"".format(error))\n\n\ndef clean_dir(path):\n    if not os.path.isdir(path):\n        return\n\n    files = os.listdir(path)\n    for x in files:\n        full_path = os.path.join(path, x)\n        if os.path.isfile(full_path):\n            f = os.remove\n            remove_generic(full_path, f)\n        elif os.path.isdir(full_path):\n            clean_dir(full_path)\n            f = os.rmdir\n            remove_generic(full_path, f)\n\n\ndef set_logging(filename, stream_log_level, file_log_level, tf_log_level):\n    stream_log = logging.StreamHandler()\n    stream_log.setLevel(stream_log_level)\n\n    file_log = logging.FileHandler(filename=filename)\n    file_log.setLevel(file_log_level)\n\n    logger = logging.getLogger()\n    logger.handlers = []\n    logger.addHandler(stream_log)\n    logger.addHandler(file_log)\n    logger.setLevel(min(stream_log_level, file_log_level))\n\n    tf.logging.set_verbosity(tf_log_level)\n\n\ndef save_image(filename, image, print_console=False):\n    if len(image.shape) >= 3 and image.shape[2] == 1:\n        image = image.reshape(image.shape[0], image.shape[1])\n\n    directory = os.path.dirname(filename)\n    if directory != """" and not os.path.exists(directory):\n        os.makedirs(directory)\n\n    image = misc.toimage(image, cmin=0, cmax=255)  # to avoid range rescaling\n    misc.imsave(filename, image)\n\n    if print_console:\n        print(""Saved [%s]"" % filename)\n\n\ndef save_image_data(filename, image):\n    directory = os.path.dirname(filename)\n    if directory != """" and not os.path.exists(directory):\n        os.makedirs(directory)\n\n    np.save(filename, image)\n    print(""Saved [%s]"" % filename)\n\n\ndef convert_rgb_to_y(image):\n    if len(image.shape) <= 2 or image.shape[2] == 1:\n        return image\n\n    xform = np.array([[65.738 / 256.0, 129.057 / 256.0, 25.064 / 256.0]])\n    y_image = image.dot(xform.T) + 16.0\n\n    return y_image\n\n\ndef convert_rgb_to_ycbcr(image):\n    if len(image.shape) < 2 or image.shape[2] == 1:\n        return image\n\n    xform = np.array(\n        [[65.738 / 256.0, 129.057 / 256.0, 25.064 / 256.0],\n         [- 37.945 / 256.0, - 74.494 / 256.0, 112.439 / 256.0],\n         [112.439 / 256.0, - 94.154 / 256.0, - 18.285 / 256.0]])\n\n    ycbcr_image = image.dot(xform.T)\n    ycbcr_image[:, :, 0] += 16.0\n    ycbcr_image[:, :, [1, 2]] += 128.0\n\n    return ycbcr_image\n\n\ndef convert_ycbcr_to_rgb(ycbcr_image):\n    rgb_image = np.zeros([ycbcr_image.shape[0], ycbcr_image.shape[1], 3])  # type: np.ndarray\n\n    rgb_image[:, :, 0] = ycbcr_image[:, :, 0] - 16.0\n    rgb_image[:, :, [1, 2]] = ycbcr_image[:, :, [1, 2]] - 128.0\n    xform = np.array(\n        [[298.082 / 256.0, 0, 408.583 / 256.0],\n         [298.082 / 256.0, -100.291 / 256.0, -208.120 / 256.0],\n         [298.082 / 256.0, 516.412 / 256.0, 0]])\n    rgb_image = rgb_image.dot(xform.T)\n\n    return rgb_image\n\n\ndef convert_y_and_cbcr_to_rgb(y_image, cbcr_image):\n    if len(y_image.shape) <= 2:\n        y_image = y_image.reshape[y_image.shape[0], y_image.shape[1], 1]\n\n    if len(y_image.shape) == 3 and y_image.shape[2] == 3:\n        y_image = y_image[:, :, 0:1]\n\n    ycbcr_image = np.zeros([y_image.shape[0], y_image.shape[1], 3])\n    ycbcr_image[:, :, 0] = y_image[:, :, 0]\n    ycbcr_image[:, :, 1:3] = cbcr_image[:, :, 0:2]\n\n    return convert_ycbcr_to_rgb(ycbcr_image)\n\n\ndef set_image_alignment(image, alignment):\n    alignment = int(alignment)\n    width, height = image.shape[1], image.shape[0]\n    width = (width // alignment) * alignment\n    height = (height // alignment) * alignment\n\n    if image.shape[1] != width or image.shape[0] != height:\n        image = image[:height, :width, :]\n\n    if len(image.shape) >= 3 and image.shape[2] >= 4:\n        image = image[:, :, 0:3]\n\n    return image\n\n\ndef resize_image_by_pil(image, scale, resampling_method=""bicubic""):\n    width, height = image.shape[1], image.shape[0]\n    new_width = int(width * scale)\n    new_height = int(height * scale)\n\n    if resampling_method == ""bicubic"":\n        method = Image.BICUBIC\n    elif resampling_method == ""bilinear"":\n        method = Image.BILINEAR\n    elif resampling_method == ""nearest"":\n        method = Image.NEAREST\n    else:\n        method = Image.LANCZOS\n\n    if len(image.shape) == 3 and image.shape[2] == 3:\n        image = Image.fromarray(image, ""RGB"")\n        image = image.resize([new_width, new_height], resample=method)\n        image = np.asarray(image)\n    elif len(image.shape) == 3 and image.shape[2] == 4:\n        # the image may has an alpha channel\n        image = Image.fromarray(image, ""RGB"")\n        image = image.resize([new_width, new_height], resample=method)\n        image = np.asarray(image)\n    else:\n        image = Image.fromarray(image.reshape(height, width))\n        image = image.resize([new_width, new_height], resample=method)\n        image = np.asarray(image)\n        image = image.reshape(new_height, new_width, 1)\n    return image\n\n\ndef load_image(filename, width=0, height=0, channels=0, alignment=0, print_console=True):\n    if not os.path.isfile(filename):\n        raise LoadError(""File not found [%s]"" % filename)\n\n    try:\n        image = np.atleast_3d(misc.imread(filename))\n\n        if (width != 0 and image.shape[1] != width) or (height != 0 and image.shape[0] != height):\n            raise LoadError(""Attributes mismatch"")\n        if channels != 0 and image.shape[2] != channels:\n            raise LoadError(""Attributes mismatch"")\n        if alignment != 0 and ((width % alignment) != 0 or (height % alignment) != 0):\n            raise LoadError(""Attributes mismatch"")\n\n        # if there is alpha plane, cut it\n        if image.shape[2] >= 4:\n            image = image[:, :, 0:3]\n\n        if print_console:\n            print(""Loaded [%s]: %d x %d x %d"" % (filename, image.shape[1], image.shape[0], image.shape[2]))\n    except IndexError:\n        print(""IndexError: file:[%s] shape[%s]"" % (filename, image.shape))\n        return None\n\n    return image\n\n\ndef load_image_data(filename, width=0, height=0, channels=0, alignment=0, print_console=True):\n    if not os.path.isfile(filename):\n        raise LoadError(""File not found"")\n    image = np.load(filename)\n\n    if (width != 0 and image.shape[1] != width) or (height != 0 and image.shape[0] != height):\n        raise LoadError(""Attributes mismatch"")\n    if channels != 0 and image.shape[2] != channels:\n        raise LoadError(""Attributes mismatch"")\n    if alignment != 0 and ((width % alignment) != 0 or (height % alignment) != 0):\n        raise LoadError(""Attributes mismatch"")\n\n    if print_console:\n        print(""Loaded [%s]: %d x %d x %d"" % (filename, image.shape[1], image.shape[0], image.shape[2]))\n    return image\n\n\ndef get_split_images(image, window_size, stride=None, enable_duplicate=False):\n    if len(image.shape) == 3 and image.shape[2] == 1:\n        image = image.reshape(image.shape[0], image.shape[1])\n\n    window_size = int(window_size)\n    size = image.itemsize  # byte size of each value\n    height, width = image.shape\n    if stride is None:\n        stride = window_size\n    else:\n        stride = int(stride)\n\n    if height < window_size or width < window_size:\n        return None\n\n    new_height = 1 + (height - window_size) // stride\n    new_width = 1 + (width - window_size) // stride\n\n    shape = (new_height, new_width, window_size, window_size)\n    strides = size * np.array([width * stride, stride, width, 1])\n    windows = np.lib.stride_tricks.as_strided(image, shape=shape, strides=strides)\n    windows = windows.reshape(windows.shape[0] * windows.shape[1], windows.shape[2], windows.shape[3], 1)\n\n    if enable_duplicate:\n        extra_windows = []\n        if (height - window_size) % stride != 0:\n            for x in range(0, width - window_size, stride):\n                extra_windows.append(image[height - window_size - 1:height - 1, x:x + window_size:])\n\n        if (width - window_size) % stride != 0:\n            for y in range(0, height - window_size, stride):\n                extra_windows.append(image[y: y + window_size, width - window_size - 1:width - 1])\n\n        if len(extra_windows) > 0:\n            org_size = windows.shape[0]\n            windows = np.resize(windows,\n                                [org_size + len(extra_windows), windows.shape[1], windows.shape[2], windows.shape[3]])\n            for i in range(len(extra_windows)):\n                extra_windows[i] = extra_windows[i].reshape([extra_windows[i].shape[0], extra_windows[i].shape[1], 1])\n                windows[org_size + i] = extra_windows[i]\n\n    return windows\n\n\n# divide images with given stride. note return image size may not equal to window size.\ndef get_divided_images(image, window_size, stride, min_size=0):\n    h, w = image.shape[:2]\n    divided_images = []\n\n    for y in range(0, h, stride):\n        for x in range(0, w, stride):\n\n            new_h = window_size if y + window_size <= h else h - y\n            new_w = window_size if x + window_size <= w else w - x\n            if new_h < min_size or new_w < min_size:\n                continue\n\n            divided_images.append(image[y:y + new_h, x:x + new_w, :])\n\n    return divided_images\n\n\ndef xavier_cnn_initializer(shape, uniform=True):\n    fan_in = shape[0] * shape[1] * shape[2]\n    fan_out = shape[0] * shape[1] * shape[3]\n    n = fan_in + fan_out\n    if uniform:\n        init_range = math.sqrt(6.0 / n)\n        return tf.random_uniform(shape, minval=-init_range, maxval=init_range)\n    else:\n        stddev = math.sqrt(3.0 / n)\n        return tf.truncated_normal(shape=shape, stddev=stddev)\n\n\ndef he_initializer(shape):\n    n = shape[0] * shape[1] * shape[2]\n    stddev = math.sqrt(2.0 / n)\n    return tf.truncated_normal(shape=shape, stddev=stddev)\n\n\ndef upsample_filter(size):\n    factor = (size + 1) // 2\n    if size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:size, :size]\n\n    return (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n\n\ndef get_upscale_filter_size(scale):\n    return 2 * scale - scale % 2\n\n\ndef upscale_weight(scale, channels, name=""weight""):\n    cnn_size = get_upscale_filter_size(scale)\n\n    initial = np.zeros(shape=[cnn_size, cnn_size, channels, channels], dtype=np.float32)\n    filter_matrix = upsample_filter(cnn_size)\n\n    for i in range(channels):\n        initial[:, :, i, i] = filter_matrix\n\n    return tf.Variable(initial, name=name)\n\n\ndef weight(shape, stddev=0.01, name=""weight"", uniform=False, initializer=""stddev""):\n    if initializer == ""xavier"":\n        initial = xavier_cnn_initializer(shape, uniform=uniform)\n    elif initializer == ""he"":\n        initial = he_initializer(shape)\n    elif initializer == ""uniform"":\n        initial = tf.random_uniform(shape, minval=-2.0 * stddev, maxval=2.0 * stddev)\n    elif initializer == ""stddev"":\n        initial = tf.truncated_normal(shape=shape, stddev=stddev)\n    elif initializer == ""identity"":\n        initial = he_initializer(shape)\n        if len(shape) == 4:\n            initial = initial.eval()\n            i = shape[0] // 2\n            j = shape[1] // 2\n            for k in range(min(shape[2], shape[3])):\n                initial[i][j][k][k] = 1.0\n    else:\n        initial = tf.zeros(shape)\n\n    return tf.Variable(initial, name=name)\n\n\ndef bias(shape, initial_value=0.0, name=None):\n    initial = tf.constant(initial_value, shape=shape)\n\n    if name is None:\n        return tf.Variable(initial)\n    else:\n        return tf.Variable(initial, name=name)\n\n\n# utilities for logging -----\n\ndef add_summaries(scope_name, model_name, var, header_name="""", save_stddev=True, save_mean=False, save_max=False,\n                  save_min=False):\n    with tf.name_scope(scope_name):\n        mean_var = tf.reduce_mean(var)\n        if save_mean:\n            tf.summary.scalar(header_name + ""mean/"" + model_name, mean_var)\n\n        if save_stddev:\n            stddev_var = tf.sqrt(tf.reduce_mean(tf.square(var - mean_var)))\n            tf.summary.scalar(header_name + ""stddev/"" + model_name, stddev_var)\n\n        if save_max:\n            tf.summary.scalar(header_name + ""max/"" + model_name, tf.reduce_max(var))\n\n        if save_min:\n            tf.summary.scalar(header_name + ""min/"" + model_name, tf.reduce_min(var))\n        tf.summary.histogram(header_name + model_name, var)\n\n\ndef log_scalar_value(writer, name, value, step):\n    summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n    writer.add_summary(summary, step)\n\n\ndef log_fcn_output_as_images(image, width, height, filters, model_name, max_outputs=20):\n    """"""\n    input tensor should be [ N, H * W * C ]\n    so transform to [ N H W C ] and visualize only first channel\n    """"""\n    reshaped_image = tf.reshape(image, [-1, height, width, filters])\n    tf.summary.image(model_name, reshaped_image[:, :, :, :1], max_outputs=max_outputs)\n\n\ndef log_cnn_weights_as_images(model_name, weights, max_outputs=20):\n    """"""\n    input tensor should be [ W, H, In_Ch, Out_Ch ]\n    so transform to [ In_Ch * Out_Ch, W, H ] and visualize it\n    """"""\n    shapes = get_shapes(weights)\n    weights = tf.reshape(weights, [shapes[0], shapes[1], shapes[2] * shapes[3]])\n    weights_transposed = tf.transpose(weights, [2, 0, 1])\n    weights_transposed = tf.reshape(weights_transposed, [shapes[2] * shapes[3], shapes[0], shapes[1], 1])\n    tf.summary.image(model_name, weights_transposed, max_outputs=max_outputs)\n\n\ndef get_shapes(input_tensor):\n    return input_tensor.get_shape().as_list()\n\n\ndef get_now_date():\n    d = datetime.datetime.today()\n    return ""%s/%s/%s %s:%s:%s"" % (d.year, d.month, d.day, d.hour, d.minute, d.second)\n\n\ndef get_loss_image(image1, image2, scale=1.0, border_size=0):\n    if len(image1.shape) == 2:\n        image1 = image1.reshape(image1.shape[0], image1.shape[1], 1)\n    if len(image2.shape) == 2:\n        image2 = image2.reshape(image2.shape[0], image2.shape[1], 1)\n\n    if image1.shape[0] != image2.shape[0] or image1.shape[1] != image2.shape[1] or image1.shape[2] != image2.shape[2]:\n        return None\n\n    image1 = trim_image_as_file(image1)\n    image2 = trim_image_as_file(image2)\n\n    loss_image = np.multiply(np.square(np.subtract(image1, image2)), scale)\n    loss_image = np.minimum(loss_image, 255.0)\n    if border_size > 0:\n        loss_image = loss_image[border_size:-border_size, border_size:-border_size, :]\n\n    return loss_image\n\n\ndef trim_image_as_file(image):\n    image = np.rint(image)\n    image = np.clip(image, 0, 255)\n    if image.dtype != np.float32:\n        image = image.astype(np.float32)\n    return image\n\n\ndef compute_psnr_and_ssim(image1, image2, border_size=0):\n    """"""\n    Computes PSNR and SSIM index from 2 images.\n    We round it and clip to 0 - 255. Then shave \'scale\' pixels from each border.\n    """"""\n    if len(image1.shape) == 2:\n        image1 = image1.reshape(image1.shape[0], image1.shape[1], 1)\n    if len(image2.shape) == 2:\n        image2 = image2.reshape(image2.shape[0], image2.shape[1], 1)\n\n    if image1.shape[0] != image2.shape[0] or image1.shape[1] != image2.shape[1] or image1.shape[2] != image2.shape[2]:\n        return None\n\n    image1 = trim_image_as_file(image1)\n    image2 = trim_image_as_file(image2)\n\n    if border_size > 0:\n        image1 = image1[border_size:-border_size, border_size:-border_size, :]\n        image2 = image2[border_size:-border_size, border_size:-border_size, :]\n\n    psnr = compare_psnr(image1, image2, data_range=255)\n    ssim = compare_ssim(image1, image2, win_size=11, gaussian_weights=True, multichannel=True, K1=0.01, K2=0.03,\n                        sigma=1.5, data_range=255)\n    return psnr, ssim\n\n\ndef print_filter_weights(tensor):\n    print(""Tensor[%s] shape=%s"" % (tensor.name, str(tensor.get_shape())))\n    weight_value = tensor.eval()\n    for i in range(weight_value.shape[3]):\n        values = """"\n        for x in range(weight_value.shape[0]):\n            for y in range(weight_value.shape[1]):\n                for c in range(weight_value.shape[2]):\n                    values += ""%2.3f "" % weight_value[y][x][c][i]\n        print(values)\n    print(""\\n"")\n\n\ndef print_filter_biases(tensor):\n    print(""Tensor[%s] shape=%s"" % (tensor.name, str(tensor.get_shape())))\n    bias = tensor.eval()\n    values = """"\n    for i in range(bias.shape[0]):\n        values += ""%2.3f "" % bias[i]\n    print(values + ""\\n"")\n\n\ndef get_psnr(mse, max_value=255.0):\n    if mse is None or mse == float(\'Inf\') or mse == 0:\n        psnr = 0\n    else:\n        psnr = 20 * math.log(max_value / math.sqrt(mse), 10)\n    return psnr\n\n\ndef print_num_of_total_parameters(output_detail=False, output_to_logging=False):\n    total_parameters = 0\n    parameters_string = """"\n\n    for variable in tf.trainable_variables():\n\n        shape = variable.get_shape()\n        variable_parameters = 1\n        for dim in shape:\n            variable_parameters *= dim.value\n        total_parameters += variable_parameters\n        if len(shape) == 1:\n            parameters_string += (""%s %d, "" % (variable.name, variable_parameters))\n        else:\n            parameters_string += (""%s %s=%d, "" % (variable.name, str(shape), variable_parameters))\n\n    if output_to_logging:\n        if output_detail:\n            logging.info(parameters_string)\n        logging.info(""Total %d variables, %s params"" % (len(tf.trainable_variables()), ""{:,}"".format(total_parameters)))\n    else:\n        if output_detail:\n            print(parameters_string)\n        print(""Total %d variables, %s params"" % (len(tf.trainable_variables()), ""{:,}"".format(total_parameters)))\n\n\ndef flip(image, flip_type, invert=False):\n    if flip_type == 0:\n        return image\n    elif flip_type == 1:\n        return np.flipud(image)\n    elif flip_type == 2:\n        return np.fliplr(image)\n    elif flip_type == 3:\n        return np.flipud(np.fliplr(image))\n    elif flip_type == 4:\n        return np.rot90(image, 1 if invert is False else -1)\n    elif flip_type == 5:\n        return np.rot90(image, -1 if invert is False else 1)\n    elif flip_type == 6:\n        if invert is False:\n            return np.flipud(np.rot90(image))\n        else:\n            return np.rot90(np.flipud(image), -1)\n    elif flip_type == 7:\n        if invert is False:\n            return np.flipud(np.rot90(image, -1))\n        else:\n            return np.rot90(np.flipud(image), 1)\n'"
