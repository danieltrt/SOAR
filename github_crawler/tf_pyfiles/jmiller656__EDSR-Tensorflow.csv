file_path,api_count,code
data.py,0,"b'import scipy.misc\nimport random\nimport numpy as np\nimport os\n\ntrain_set = []\ntest_set = []\nbatch_index = 0\n\n""""""\nLoad set of images in a directory.\nThis will automatically allocate a \nrandom 20% of the images as a test set\n\ndata_dir: path to directory containing images\n""""""\ndef load_dataset(data_dir, img_size):\n\t""""""img_files = os.listdir(data_dir)\n\ttest_size = int(len(img_files)*0.2)\n\ttest_indices = random.sample(range(len(img_files)),test_size)\n\tfor i in range(len(img_files)):\n\t\t#img = scipy.misc.imread(data_dir+img_files[i])\n\t\tif i in test_indices:\n\t\t\ttest_set.append(data_dir+""/""+img_files[i])\n\t\telse:\n\t\t\ttrain_set.append(data_dir+""/""+img_files[i])\n\treturn""""""\n\tglobal train_set\n\tglobal test_set\n\timgs = []\n\timg_files = os.listdir(data_dir)\n\tfor img in img_files:\n\t\ttry:\n\t\t\ttmp= scipy.misc.imread(data_dir+""/""+img)\n\t\t\tx,y,z = tmp.shape\n\t\t\tcoords_x = x / img_size\n\t\t\tcoords_y = y/img_size\n\t\t\tcoords = [ (q,r) for q in range(coords_x) for r in range(coords_y) ]\n\t\t\tfor coord in coords:\n\t\t\t\timgs.append((data_dir+""/""+img,coord))\n\t\texcept:\n\t\t\tprint ""oops""\n\ttest_size = min(10,int( len(imgs)*0.2))\n\trandom.shuffle(imgs)\n\ttest_set = imgs[:test_size]\n\ttrain_set = imgs[test_size:][:200]\n\treturn\n\n""""""\nGet test set from the loaded dataset\n\nsize (optional): if this argument is chosen,\neach element of the test set will be cropped\nto the first (size x size) pixels in the image.\n\nreturns the test set of your data\n""""""\ndef get_test_set(original_size,shrunk_size):\n\t""""""for i in range(len(test_set)):\n\t\timg = scipy.misc.imread(test_set[i])\n\t\tif img.shape:\n\t\t\timg = crop_center(img,original_size,original_size)\t\t\n\t\t\tx_img = scipy.misc.imresize(img,(shrunk_size,shrunk_size))\n\t\t\ty_imgs.append(img)\n\t\t\tx_imgs.append(x_img)""""""\n\timgs = test_set\n\tget_image(imgs[0],original_size)\n\tx = [scipy.misc.imresize(get_image(q,original_size),(shrunk_size,shrunk_size)) for q in imgs]#scipy.misc.imread(q[0])[q[1][0]*original_size:(q[1][0]+1)*original_size,q[1][1]*original_size:(q[1][1]+1)*original_size].resize(shrunk_size,shrunk_size) for q in imgs]\n\ty = [get_image(q,original_size) for q in imgs]#scipy.misc.imread(q[0])[q[1][0]*original_size:(q[1][0]+1)*original_size,q[1][1]*original_size:(q[1][1]+1)*original_size] for q in imgs]\n\treturn x,y\n\ndef get_image(imgtuple,size):\n\timg = scipy.misc.imread(imgtuple[0])\n\tx,y = imgtuple[1]\n\timg = img[x*size:(x+1)*size,y*size:(y+1)*size]\n\treturn img\n\t\n\n""""""\nGet a batch of images from the training\nset of images.\n\nbatch_size: size of the batch\noriginal_size: size for target images\nshrunk_size: size for shrunk images\n\nreturns x,y where:\n\t-x is the input set of shape [-1,shrunk_size,shrunk_size,channels]\n\t-y is the target set of shape [-1,original_size,original_size,channels]\n""""""\ndef get_batch(batch_size,original_size,shrunk_size):\n\tglobal batch_index\n\t""""""img_indices = random.sample(range(len(train_set)),batch_size)\n\tfor i in range(len(img_indices)):\n\t\tindex = img_indices[i]\n\t\timg = scipy.misc.imread(train_set[index])\n\t\tif img.shape:\n\t\t\timg = crop_center(img,original_size,original_size)\n\t\t\tx_img = scipy.misc.imresize(img,(shrunk_size,shrunk_size))\n\t\t\tx.append(x_img)\n\t\t\ty.append(img)""""""\n\tmax_counter = len(train_set)/batch_size\n\tcounter = batch_index % max_counter\n\twindow = [x for x in range(counter*batch_size,(counter+1)*batch_size)]\n\timgs = [train_set[q] for q in window]\n\tx = [scipy.misc.imresize(get_image(q,original_size),(shrunk_size,shrunk_size)) for q in imgs]#scipy.misc.imread(q[0])[q[1][0]*original_size:(q[1][0]+1)*original_size,q[1][1]*original_size:(q[1][1]+1)*original_size].resize(shrunk_size,shrunk_size) for q in imgs]\n\ty = [get_image(q,original_size) for q in imgs]#scipy.misc.imread(q[0])[q[1][0]*original_size:(q[1][0]+1)*original_size,q[1][1]*original_size:(q[1][1]+1)*original_size] for q in imgs]\n\tbatch_index = (batch_index+1)%max_counter\n\treturn x,y\n\n""""""\nSimple method to crop center of image\n\nimg: image to crop\ncropx: width of crop\ncropy: height of crop\nreturns cropped image\n""""""\ndef crop_center(img,cropx,cropy):\n\ty,x,_ = img.shape\n\tstartx = random.sample(range(x-cropx-1),1)[0]#x//2-(cropx//2)\n\tstarty = random.sample(range(y-cropy-1),1)[0]#y//2-(cropy//2)\n\treturn img[starty:starty+cropy,startx:startx+cropx]\n\n\n\n\n\n'"
model.py,22,"b'import tensorflow.contrib.slim as slim\nimport scipy.misc\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport numpy as np\nimport shutil\nimport utils\nimport os\n\n""""""\nAn implementation of the neural network used for\nsuper-resolution of images as described in:\n\n`Enhanced Deep Residual Networks for Single Image Super-Resolution`\n(https://arxiv.org/pdf/1707.02921.pdf)\n\n(single scale baseline-style model)\n""""""\nclass EDSR(object):\n\n\tdef __init__(self,img_size=32,num_layers=32,feature_size=256,scale=2,output_channels=3):\n\t\tprint(""Building EDSR..."")\n\t\tself.img_size = img_size\n\t\tself.scale = scale\n\t\tself.output_channels = output_channels\n\n\t\t#Placeholder for image inputs\n\t\tself.input = x = tf.placeholder(tf.float32,[None,img_size,img_size,output_channels])\n\t\t#Placeholder for upscaled image ground-truth\n\t\tself.target = y = tf.placeholder(tf.float32,[None,img_size*scale,img_size*scale,output_channels])\n\t\n\t\t""""""\n\t\tPreprocessing as mentioned in the paper, by subtracting the mean\n\t\tHowever, the subtract the mean of the entire dataset they use. As of\n\t\tnow, I am subtracting the mean of each batch\n\t\t""""""\n\t\tmean_x = 127#tf.reduce_mean(self.input)\n\t\timage_input =x- mean_x\n\t\tmean_y = 127#tf.reduce_mean(self.target)\n\t\timage_target =y- mean_y\n\n\t\t#One convolution before res blocks and to convert to required feature depth\n\t\tx = slim.conv2d(image_input,feature_size,[3,3])\n\t\n\t\t#Store the output of the first convolution to add later\n\t\tconv_1 = x\t\n\n\t\t""""""\n\t\tThis creates `num_layers` number of resBlocks\n\t\ta resBlock is defined in the paper as\n\t\t(excuse the ugly ASCII graph)\n\t\tx\n\t\t|\\\n\t\t| \\\n\t\t|  conv2d\n\t\t|  relu\n\t\t|  conv2d\n\t\t| /\n\t\t|/\n\t\t+ (addition here)\n\t\t|\n\t\tresult\n\t\t""""""\n\n\t\t""""""\n\t\tDoing scaling here as mentioned in the paper:\n\n\t\t`we found that increasing the number of feature\n\t\tmaps above a certain level would make the training procedure\n\t\tnumerically unstable. A similar phenomenon was\n\t\treported by Szegedy et al. We resolve this issue by\n\t\tadopting the residual scaling with factor 0.1. In each\n\t\tresidual block, constant scaling layers are placed after the\n\t\tlast convolution layers. These modules stabilize the training\n\t\tprocedure greatly when using a large number of filters.\n\t\tIn the test phase, this layer can be integrated into the previous\n\t\tconvolution layer for the computational efficiency.\'\n\n\t\t""""""\n\t\tscaling_factor = 0.1\n\t\t\n\t\t#Add the residual blocks to the model\n\t\tfor i in range(num_layers):\n\t\t\tx = utils.resBlock(x,feature_size,scale=scaling_factor)\n\n\t\t#One more convolution, and then we add the output of our first conv layer\n\t\tx = slim.conv2d(x,feature_size,[3,3])\n\t\tx += conv_1\n\t\t\n\t\t#Upsample output of the convolution\t\t\n\t\tx = utils.upsample(x,scale,feature_size,None)\n\n\t\t#One final convolution on the upsampling output\n\t\toutput = x#slim.conv2d(x,output_channels,[3,3])\n\t\tself.out = tf.clip_by_value(output+mean_x,0.0,255.0)\n\n\t\tself.loss = loss = tf.reduce_mean(tf.losses.absolute_difference(image_target,output))\n\t\n\t\t#Calculating Peak Signal-to-noise-ratio\n\t\t#Using equations from here: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n\t\tmse = tf.reduce_mean(tf.squared_difference(image_target,output))\t\n\t\tPSNR = tf.constant(255**2,dtype=tf.float32)/mse\n\t\tPSNR = tf.constant(10,dtype=tf.float32)*utils.log10(PSNR)\n\t\n\t\t#Scalar to keep track for loss\n\t\ttf.summary.scalar(""loss"",self.loss)\n\t\ttf.summary.scalar(""PSNR"",PSNR)\n\t\t#Image summaries for input, target, and output\n\t\ttf.summary.image(""input_image"",tf.cast(self.input,tf.uint8))\n\t\ttf.summary.image(""target_image"",tf.cast(self.target,tf.uint8))\n\t\ttf.summary.image(""output_image"",tf.cast(self.out,tf.uint8))\n\t\t\n\t\t#Tensorflow graph setup... session, saver, etc.\n\t\tself.sess = tf.Session()\n\t\tself.saver = tf.train.Saver()\n\t\tprint(""Done building!"")\n\t\n\t""""""\n\tSave the current state of the network to file\n\t""""""\n\tdef save(self,savedir=\'saved_models\'):\n\t\tprint(""Saving..."")\n\t\tself.saver.save(self.sess,savedir+""/model"")\n\t\tprint(""Saved!"")\n\t\t\n\t""""""\n\tResume network from previously saved weights\n\t""""""\n\tdef resume(self,savedir=\'saved_models\'):\n\t\tprint(""Restoring..."")\n\t\tself.saver.restore(self.sess,tf.train.latest_checkpoint(savedir))\n\t\tprint(""Restored!"")\t\n\n\t""""""\n\tCompute the output of this network given a specific input\n\n\tx: either one of these things:\n\t\t1. A numpy array of shape [image_width,image_height,3]\n\t\t2. A numpy array of shape [n,input_size,input_size,3]\n\n\treturn: \tFor the first case, we go over the entire image and run super-resolution over windows of the image\n\t\t\tthat are of size [input_size,input_size,3]. We then stitch the output of these back together into the\n\t\t\tnew super-resolution image and return that\n\n\treturn  \tFor the second case, we return a numpy array of shape [n,input_size*scale,input_size*scale,3]\n\t""""""\n\tdef predict(self,x):\n\t\tprint(""Predicting..."")\n\t\tif (len(x.shape) == 3) and not(x.shape[0] == self.img_size and x.shape[1] == self.img_size):\n\t\t\tnum_across = x.shape[0]//self.img_size\n\t\t\tnum_down = x.shape[1]//self.img_size\n\t\t\ttmp_image = np.zeros([x.shape[0]*self.scale,x.shape[1]*self.scale,3])\n\t\t\tfor i in range(num_across):\n\t\t\t\tfor j in range(num_down):\n\t\t\t\t\ttmp = self.sess.run(self.out,feed_dict={self.input:[x[i*self.img_size:(i+1)*self.img_size,j*self.img_size:(j+1)*self.img_size]]})[0]\n\t\t\t\t\ttmp_image[i*tmp.shape[0]:(i+1)*tmp.shape[0],j*tmp.shape[1]:(j+1)*tmp.shape[1]] = tmp\n\t\t\t#this added section fixes bottom right corner when testing\n\t\t\tif (x.shape[0]%self.img_size != 0 and  x.shape[1]%self.img_size != 0):\n\t\t\t\ttmp = self.sess.run(self.out,feed_dict={self.input:[x[-1*self.img_size:,-1*self.img_size:]]})[0]\n\t\t\t\ttmp_image[-1*tmp.shape[0]:,-1*tmp.shape[1]:] = tmp\n\t\t\t\t\t\n\t\t\tif x.shape[0]%self.img_size != 0:\n\t\t\t\tfor j in range(num_down):\n\t\t\t\t\ttmp = self.sess.run(self.out,feed_dict={self.input:[x[-1*self.img_size:,j*self.img_size:(j+1)*self.img_size]]})[0]\n\t\t\t\t\ttmp_image[-1*tmp.shape[0]:,j*tmp.shape[1]:(j+1)*tmp.shape[1]] = tmp\n\t\t\tif x.shape[1]%self.img_size != 0:\n\t\t\t\tfor j in range(num_across):\n                                        tmp = self.sess.run(self.out,feed_dict={self.input:[x[j*self.img_size:(j+1)*self.img_size,-1*self.img_size:]]})[0]\n                                        tmp_image[j*tmp.shape[0]:(j+1)*tmp.shape[0],-1*tmp.shape[1]:] = tmp\n\t\t\treturn tmp_image\n\t\telse:\n\t\t\treturn self.sess.run(self.out,feed_dict={self.input:x})\n\n\t""""""\n\tFunction to setup your input data pipeline\n\t""""""\n\tdef set_data_fn(self,fn,args,test_set_fn=None,test_set_args=None):\n\t\tself.data = fn\n\t\tself.args = args\n\t\tself.test_data = test_set_fn\n\t\tself.test_args = test_set_args\n\n\t""""""\n\tTrain the neural network\n\t""""""\n\tdef train(self,iterations=1000,save_dir=""saved_models""):\n\t\t#Removing previous save directory if there is one\n\t\tif os.path.exists(save_dir):\n\t\t\tshutil.rmtree(save_dir)\n\t\t#Make new save directory\n\t\tos.mkdir(save_dir)\n\t\t#Just a tf thing, to merge all summaries into one\n\t\tmerged = tf.summary.merge_all()\n\t\t#Using adam optimizer as mentioned in the paper\n\t\toptimizer = tf.train.AdamOptimizer()\n\t\t#This is the train operation for our objective\n\t\ttrain_op = optimizer.minimize(self.loss)\t\n\t\t#Operation to initialize all variables\n\t\tinit = tf.global_variables_initializer()\n\t\tprint(""Begin training..."")\n\t\twith self.sess as sess:\n\t\t\t#Initialize all variables\n\t\t\tsess.run(init)\n\t\t\ttest_exists = self.test_data\n\t\t\t#create summary writer for train\n\t\t\ttrain_writer = tf.summary.FileWriter(save_dir+""/train"",sess.graph)\n\n\t\t\t#If we\'re using a test set, include another summary writer for that\n\t\t\tif test_exists:\n\t\t\t\ttest_writer = tf.summary.FileWriter(save_dir+""/test"",sess.graph)\n\t\t\t\ttest_x,test_y = self.test_data(*self.test_args)\n\t\t\t\ttest_feed = {self.input:test_x,self.target:test_y}\n\n\t\t\t#This is our training loop\n\t\t\tfor i in tqdm(range(iterations)):\n\t\t\t\t#Use the data function we were passed to get a batch every iteration\n\t\t\t\tx,y = self.data(*self.args)\n\t\t\t\t#Create feed dictionary for the batch\n\t\t\t\tfeed = {\n\t\t\t\t\tself.input:x,\n\t\t\t\t\tself.target:y\n\t\t\t\t}\n\t\t\t\t#Run the train op and calculate the train summary\n\t\t\t\tsummary,_ = sess.run([merged,train_op],feed)\n\t\t\t\t#If we\'re testing, don\'t train on test set. But do calculate summary\n\t\t\t\tif test_exists:\n\t\t\t\t\tt_summary = sess.run(merged,test_feed)\n\t\t\t\t\t#Write test summary\n\t\t\t\t\ttest_writer.add_summary(t_summary,i)\n\t\t\t\t#Write train summary for this step\n\t\t\t\ttrain_writer.add_summary(summary,i)\n\t\t\t#Save our trained model\t\t\n\t\t\tself.save()\t\t\n'"
test.py,0,"b'from model import EDSR\nimport scipy.misc\nimport argparse\nimport data\nimport os\nparser = argparse.ArgumentParser()\nparser.add_argument(""--dataset"",default=""data/General-100"")\nparser.add_argument(""--imgsize"",default=100,type=int)\nparser.add_argument(""--scale"",default=2,type=int)\nparser.add_argument(""--layers"",default=32,type=int)\nparser.add_argument(""--featuresize"",default=256,type=int)\nparser.add_argument(""--batchsize"",default=10,type=int)\nparser.add_argument(""--savedir"",default=""saved_models"")\nparser.add_argument(""--iterations"",default=1000,type=int)\nparser.add_argument(""--numimgs"",default=5,type=int)\nparser.add_argument(""--outdir"",default=""out"")\nparser.add_argument(""--image"")\nargs = parser.parse_args()\nif not os.path.exists(args.outdir):\n\tos.mkdir(args.outdir)\ndown_size = args.imgsize//args.scale\nnetwork = EDSR(down_size,args.layers,args.featuresize,scale=args.scale)\nnetwork.resume(args.savedir)\nif args.image:\n\tx = scipy.misc.imread(args.image)\nelse:\n\tprint(""No image argument given"")\ninputs = x\noutputs = network.predict(x)\nif args.image:\n\tscipy.misc.imsave(args.outdir+""/input_""+args.image,inputs)\n\tscipy.misc.imsave(args.outdir+""/output_""+args.image,outputs)\n'"
train.py,0,"b'import data\nimport argparse\nfrom model import EDSR\nparser = argparse.ArgumentParser()\nparser.add_argument(""--dataset"",default=""data/General-100"")\nparser.add_argument(""--imgsize"",default=100,type=int)\nparser.add_argument(""--scale"",default=2,type=int)\nparser.add_argument(""--layers"",default=32,type=int)\nparser.add_argument(""--featuresize"",default=256,type=int)\nparser.add_argument(""--batchsize"",default=10,type=int)\nparser.add_argument(""--savedir"",default=\'saved_models\')\nparser.add_argument(""--iterations"",default=1000,type=int)\nargs = parser.parse_args()\ndata.load_dataset(args.dataset,args.imgsize)\nif args.imgsize % args.scale != 0:\n    print(f""Image size {args.imgsize} is not evenly divisible by scale {arg.scale}"")\n    exit()\ndown_size = args.imgsize//args.scale\nnetwork = EDSR(down_size,args.layers,args.featuresize,args.scale)\nnetwork.set_data_fn(data.get_batch,(args.batchsize,args.imgsize,down_size),data.get_test_set,(args.imgsize,down_size))\nnetwork.train(args.iterations,args.savedir)\n'"
utils.py,14,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n""""""\nCreates a convolutional residual block\nas defined in the paper. More on\nthis inside model.py\n\nx: input to pass through the residual block\nchannels: number of channels to compute\nstride: convolution stride\n""""""\ndef resBlock(x,channels=64,kernel_size=[3,3],scale=1):\n\ttmp = slim.conv2d(x,channels,kernel_size,activation_fn=None)\n\ttmp = tf.nn.relu(tmp)\n\ttmp = slim.conv2d(tmp,channels,kernel_size,activation_fn=None)\n\ttmp *= scale\n\treturn x + tmp\n\n""""""\nMethod to upscale an image using\nconv2d transpose. Based on upscaling\nmethod defined in the paper\n\nx: input to be upscaled\nscale: scale increase of upsample\nfeatures: number of features to compute\nactivation: activation function\n""""""\ndef upsample(x,scale=2,features=64,activation=tf.nn.relu):\n\tassert scale in [2,3,4]\n\tx = slim.conv2d(x,features,[3,3],activation_fn=activation)\n\tif scale == 2:\n\t\tps_features = 3*(scale**2)\n\t\tx = slim.conv2d(x,ps_features,[3,3],activation_fn=activation)\n\t\t#x = slim.conv2d_transpose(x,ps_features,6,stride=1,activation_fn=activation)\n\t\tx = PS(x,2,color=True)\n\telif scale == 3:\n\t\tps_features =3*(scale**2)\n\t\tx = slim.conv2d(x,ps_features,[3,3],activation_fn=activation)\n\t\t#x = slim.conv2d_transpose(x,ps_features,9,stride=1,activation_fn=activation)\n\t\tx = PS(x,3,color=True)\n\telif scale == 4:\n\t\tps_features = 3*(2**2)\n\t\tfor i in range(2):\n\t\t\tx = slim.conv2d(x,ps_features,[3,3],activation_fn=activation)\n\t\t\t#x = slim.conv2d_transpose(x,ps_features,6,stride=1,activation_fn=activation)\n\t\t\tx = PS(x,2,color=True)\n\treturn x\n\n""""""\nBorrowed from https://github.com/tetrachrome/subpixel\nUsed for subpixel phase shifting after deconv operations\n""""""\ndef _phase_shift(I, r):\n\tbsize, a, b, c = I.get_shape().as_list()\n\tbsize = tf.shape(I)[0] # Handling Dimension(None) type for undefined batch dim\n\tX = tf.reshape(I, (bsize, a, b, r, r))\n\tX = tf.transpose(X, (0, 1, 2, 4, 3))  # bsize, a, b, 1, 1\n\tX = tf.split(X, a, 1)  # a, [bsize, b, r, r]\n\tX = tf.concat([tf.squeeze(x, axis=1) for x in X],2)  # bsize, b, a*r, r\n\tX = tf.split(X, b, 1)  # b, [bsize, a*r, r]\n\tX = tf.concat([tf.squeeze(x, axis=1) for x in X],2)  # bsize, a*r, b*r\n\treturn tf.reshape(X, (bsize, a*r, b*r, 1))\n\n""""""\nBorrowed from https://github.com/tetrachrome/subpixel\nUsed for subpixel phase shifting after deconv operations\n""""""\ndef PS(X, r, color=False):\n\tif color:\n\t\tXc = tf.split(X, 3, 3)\n\t\tX = tf.concat([_phase_shift(x, r) for x in Xc],3)\n\telse:\n\t\tX = _phase_shift(X, r)\n\treturn X\n\n""""""\nTensorflow log base 10.\nFound here: https://github.com/tensorflow/tensorflow/issues/1666\n""""""\ndef log10(x):\n  numerator = tf.log(x)\n  denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n  return numerator / denominator\n'"
