file_path,api_count,code
bigdata.py,8,"b'# License: See LICENSE\n# Fit a straight line, of the form y=m*x+b\n\nimport tensorflow as tf\nimport numpy as np\n\n\'\'\'\nYour dataset.\n\'\'\'\nxs = np.linspace(0.0, 8.0, 8000000) # 8-million features\nys = 0.3*xs-0.8+np.random.normal(scale=0.25, size=len(xs)) # 8-million labels\n\n\'\'\'\nInitial guesses, which will be refined by TensorFlow.\n\'\'\'\nm_initial = -0.5 # Initial guesses\nb_initial =  1.0\n\n\'\'\'\nDefine free variables to be solved.\n\'\'\'\nm = tf.Variable(m_initial) # Parameters\nb = tf.Variable(b_initial)\n\n\'\'\'\nDefine placeholders for big data.\n\'\'\'\n_BATCH = 8 # Use only eight points at a time.\nxs_placeholder = tf.placeholder(tf.float32, [_BATCH])\nys_placeholder = tf.placeholder(tf.float32, [_BATCH]) \n\n\'\'\'\nDefine the error between the data and the model as a tensor (distributed computing).\n\'\'\'\nys_model = m*xs_placeholder+b # Tensorflow knows this is a vector operation\ntotal_error = tf.reduce_sum((ys_placeholder-ys_model)**2) # Sum up every item in the vector\n\n\'\'\'\nOnce cost function is defined, create gradient descent optimizer.\n\'\'\'\noptimizer_operation = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(total_error) # Does one step\n\n\'\'\'\nCreate operator for initialization.\n\'\'\'\ninitializer_operation = tf.global_variables_initializer()\n\n\'\'\'\nAll calculations are done in a session.\n\'\'\'\nwith tf.Session() as session:\n\n\tsession.run(initializer_operation) # Call operator\n\n\t_EPOCHS = 10000 # Number of ""sweeps"" across data\n\tfor iteration in range(_EPOCHS):\n\t\trandom_indices = np.random.randint(len(xs), size=_BATCH) # Randomly sample the data\n\t\tfeed = {\n\t\t\txs_placeholder: xs[random_indices],\n\t\t\tys_placeholder: ys[random_indices]\n\t\t}\n\t\tsession.run(optimizer_operation, feed_dict=feed) # Call operator\n\n\tslope, intercept = session.run((m, b)) # Call ""m"" and ""b"", which are operators\n\tprint(\'Slope:\', slope, \'Intercept:\', intercept)\n\n'"
serial.py,5,"b'# License: See LICENSE\n# Fit a straight line, of the form y=m*x+b\n\nimport tensorflow as tf\n\n\'\'\'\nYour dataset.\n\'\'\'\nxs = [ 0.00,  1.00,  2.00, 3.00, 4.00, 5.00, 6.00, 7.00] # Features\nys = [-0.82, -0.94, -0.12, 0.26, 0.39, 0.64, 1.02, 1.00] # Labels\n\n\'\'\'\nInitial guesses, which will be refined by TensorFlow.\n\'\'\'\nm_initial = -0.5 # Initial guesses\nb_initial =  1.0\n\n\'\'\'\nDefine free variables to be solved.\n\'\'\'\nm = tf.Variable(m_initial) # Parameters\nb = tf.Variable(b_initial)\n\n\'\'\'\nDefine the error between the data and the model one point at a time (slow).\n\'\'\'\ntotal_error = 0.0\nfor x, y in zip(xs, ys):\n\ty_model = m*x + b # Output of the model aka yhat\n\ttotal_error += (y-y_model)**2 # Difference squared - this is the ""cost"" to be minimized\n\n\'\'\'\nOnce cost function is defined, create gradient descent optimizer.\n\'\'\'\noptimizer_operation = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(total_error) # Does one step\n\n\'\'\'\nCreate operator for initialization.\n\'\'\'\ninitializer_operation = tf.global_variables_initializer()\n\n\'\'\'\nAll calculations are done in a session.\n\'\'\'\nwith tf.Session() as session:\n\n\tsession.run(initializer_operation) # Call operator\n\n\t_EPOCHS = 10000 # number of ""sweeps"" across data\n\tfor iteration in range(_EPOCHS):\n\t\tsession.run(optimizer_operation) # Call operator\n\n\tslope, intercept = session.run((m, b)) # Call ""m"" and ""b"", which are operators\n\tprint(\'Slope:\', slope, \'Intercept:\', intercept)\n\n'"
tensor.py,6,"b'# License: See LICENSE\n# Fit a straight line, of the form y=m*x+b\n\nimport tensorflow as tf\n\n\'\'\'\nYour dataset.\n\'\'\'\nxs = [ 0.00,  1.00,  2.00, 3.00, 4.00, 5.00, 6.00, 7.00] # Features\nys = [-0.82, -0.94, -0.12, 0.26, 0.39, 0.64, 1.02, 1.00] # Labels\n\n\'\'\'\nInitial guesses, which will be refined by TensorFlow.\n\'\'\'\nm_initial = -0.5 # Initial guesses\nb_initial =  1.0\n\n\'\'\'\nDefine free variables to be solved.\n\'\'\'\nm = tf.Variable(m_initial) # Parameters\nb = tf.Variable(b_initial)\n\n\'\'\'\nDefine the error between the data and the model as a tensor (distributed computing).\n\'\'\'\nys_model = m*xs+b # Tensorflow knows this is a vector operation\ntotal_error = tf.reduce_sum((ys-ys_model)**2) # Sum up every item in the vector\n\n\'\'\'\nOnce cost function is defined, create gradient descent optimizer.\n\'\'\'\noptimizer_operation = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(total_error) # Does one step\n\n\'\'\'\nCreate operator for initialization.\n\'\'\'\ninitializer_operation = tf.global_variables_initializer()\n\n\'\'\'\nAll calculations are done in a session.\n\'\'\'\nwith tf.Session() as session:\n\n\tsession.run(initializer_operation) # Call operator\n\n\t_EPOCHS = 10000 # number of ""sweeps"" across data\n\tfor iteration in range(_EPOCHS):\n\t\tsession.run(optimizer_operation) # Call operator\n\n\tslope, intercept = session.run((m, b)) # Call ""m"" and ""b"", which are operators\n\tprint(\'Slope:\', slope, \'Intercept:\', intercept)\n\n'"
