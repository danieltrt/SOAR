file_path,api_count,code
TensorMolServer.py,0,"b""'''\nsudo pip install redis flask gevent flask-socketio\nalso execute redis-server\n'''\n#from TensorMol import *\nfrom __future__ import absolute_import\nfrom gevent import monkey\nmonkey.patch_all()\nimport redis\nfrom flask import Flask, render_template\nfrom flask_socketio import SocketIO\n\napp = Flask(__name__)\ndb = redis.StrictRedis('localhost',6379,0)\nsocketio = SocketIO(app)\n\n@app.route('/')\ndef main():\n\tc = db.incr('counter')\n\treturn render_template('main.html', counter = c)\n\n@socketio.on('connect', namespace='/tms')\ndef ws_conn():\n\tc = db.incr('user_count')\n\tsocketio.emit('msg',{'count':c}, namespace = '/tms')\n\n@socketio.on('disconnect', namespace='/tms')\ndef ws_disconn():\n        c = db.decr('user_count')\n        socketio.emit('msg',{'count':c}, namespace = '/tms')\n\nif __name__=='__main__':\n\t#app.run(debug=True)\n\tsocketio.run(app)\n"""
memory_util.py,8,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nimport os\nimport re\nimport sys\nimport tempfile\nimport tensorflow as tf\n\ndebug_messages = False\n\ndef vlog(level):\n  os.environ[\'TF_CPP_MIN_VLOG_LEVEL\'] = str(level)\n\n# this helper is here in case we later want to capture huge stderr that doesn\'t fit in RAM\nclass TemporaryFileHelper:\n  """"""Provides a way to fetch contents of temporary file."""""" \n  def __init__(self, temporary_file):\n    self.temporary_file = temporary_file\n  def getvalue(self):\n    return open(self.temporary_file.name).read() \n\n\nSTDOUT=1\nSTDERR=2\nclass capture_stderr:\n  """"""Utility to capture output, use as follows\n     with util.capture_stderr() as stderr:\n        sess = tf.Session()\n\n    print(""Captured:"", stderr.getvalue()).\n    """"""\n\n  def __init__(self, fd=STDERR):\n    self.fd = fd\n    self.prevfd = None\n\n  def __enter__(self):\n    t = tempfile.NamedTemporaryFile()\n    self.prevfd = os.dup(self.fd)\n    os.dup2(t.fileno(), self.fd)\n    return TemporaryFileHelper(t)\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    os.dup2(self.prevfd, self.fd)\n\n\n################################################################################\n# LOG_MEMORY_PARSING\n################################################################################\n# Until https://github.com/tensorflow/tensorflow/issues/6716 is resolved, the\n# reliable way to get access to tensor deallocation information is to parse\n# __LOG_MEMORY__ from VLOG print statements. This is sensitive to print order\n# run unbuffered to prevent interleaving:\n#   python -u script.py\n\n# Regex\'es to parse __LOG_MEMORY__ statements\n# Each regex is preceded by an example of line it\'s meant to pass\n\n# I 5143420588.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: -6 kernel_name: ""Unknown (from Proto)"" tensor { dtype: DT_INT32 shape { dim { size: 3 } } allocation_description { requested_bytes: 12 allocated_bytes: 12 allocator_name: ""cpu"" allocation_id: 3 has_single_reference: true ptr: 29496256 } } }\ntensor_allocation_regex = re.compile(""""""MemoryLogTensorAllocation.*?step_id: (?P<step_id>[-0123456789]+).*kernel_name: \\""(?P<kernel_name>[^""]+)\\"".*?allocated_bytes: (?P<allocated_bytes>\\d+).*allocator_name: \\""(?P<allocator_name>[^""]+)\\"".*allocation_id: (?P<allocation_id>\\d+).*"""""")\n\n# I 6795349363.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogRawAllocation { step_id: -3 operation: ""TF_AllocateTensor"" num_bytes: 1000000 ptr: 80910752 allocation_id: 99 allocator_name: ""cpu"" }\nraw_allocation_regex = re.compile(""""""MemoryLogRawAllocation.*?step_id: (?P<step_id>[-0123456789]+).*operation: \\""(?P<kernel_name>[^""]+)\\"".*?num_bytes: (?P<allocated_bytes>\\d+).*allocation_id: (?P<allocation_id>\\d+).*allocator_name: ""(?P<allocator_name>[^""]+)"".*"""""")\n\n# I 5143420588.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: 1 kernel_name: ""Const"" tensor { dtype: DT_INT32 shape { dim { size: 3 } } allocation_description { requested_bytes: 12 allocated_bytes: 12 allocator_name: ""cpu"" allocation_id: 3 ptr: 29496256 } } }\n# 2017-01-26 10:13:30: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: 2 kernel_name: ""a0"" tensor { dtype: DT_FLOAT shape { dim { size: 250000 } } allocation_description { requested_bytes: 1000000 allocated_bytes: 1000192 allocator_name: ""gpu_bfc"" allocation_id: 3 ptr: 30076651520 } } }\n#tensor_output_regex = re.compile(""""""MemoryLogTensorOutput.* step_id: (?P<step_id>[-0123456789]+) kernel_name: \\""(?P<kernel_name>[^""]+).*allocated_bytes: (?P<allocated_bytes>\\d+).*allocation_id: (?P<allocation_id>\\d+).*"""""")   \ntensor_output_regex = re.compile(""""""MemoryLogTensorOutput.* step_id: (?P<step_id>[-0123456789]+) kernel_name: \\""(?P<kernel_name>[^""]+).*allocated_bytes: (?P<allocated_bytes>\\d+).*allocator_name: \\""(?P<allocator_name>[^""]+)\\"".*allocation_id: (?P<allocation_id>\\d+).*"""""")\n\n# some Shape lines are missing bytes info so have separate regex for them\n# I 5162643141.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: 5 kernel_name: ""gradients/Shape"" tensor { dtype: DT_INT32 shape { dim { } } } }\ntensor_output_regex_no_bytes = re.compile(""""""MemoryLogTensorOutput.* step_id: (?P<step_id>[-0123456789]+) kernel_name: \\""(?P<kernel_name>[^""]+).*"""""")\n\n\n# 5143420588.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 2 allocator_name: ""cpu"" }\ntensor_deallocation_regex = re.compile(""""""allocation_id: (?P<allocation_id>\\d+).*allocator_name: \\""(?P<allocator_name>[^""]+)\\"".*"""""")\n\n# I 6796000229.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogRawDeallocation { step_id: -3 operation: ""TensorFlow C Api"" allocation_id: 177 allocator_name: ""cpu"" }\nraw_deallocation_regex = re.compile(""""""allocation_id: (?P<allocation_id>\\d+).*allocator_name: \\""(?P<allocator_name>[^""]+)\\"".*"""""")\n\n# I 5143420588.000000 file tensorflow/core/framework/log_memory.cc:41] __LOG_MEMORY__ MemoryLogStep { step_id: 1 handle: ""->Print:0//0/;0"" }\ntensor_logstep_regex = re.compile(""""""MemoryLogStep.*?step_id: (?P<step_id>[-0123456789]+).*"""""")\n\n\ndef _parse_logline(l):\n    if \'MemoryLogTensorOutput\' in l:\n        m = tensor_output_regex.search(l)\n        if not m:\n            m = tensor_output_regex_no_bytes.search(l)\n\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogTensorOutput""\n            \n    elif \'MemoryLogTensorAllocation\' in l:\n        m = tensor_allocation_regex.search(l)\n\n        # Broadcast args give weird allocation messages without size, ignore\n        # I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorAllocation { step_id: 2 kernel_name: ""gradients/node_5_grad/BroadcastGradientArgs"" tensor { dtype: DT_INT32 shape { dim { } } } }\n        if not m:\n            return {""type"": ""MemoryLogTensorAllocation"", ""line"": l,\n                    ""allocation_id"": ""-1""}\n\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogTensorAllocation""\n        if debug_messages:\n            print((""Got allocation for %s, %s""%(d[""allocation_id""], d[""kernel_name""])))\n    elif \'MemoryLogTensorDeallocation\' in l:\n        m = tensor_deallocation_regex.search(l)\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogTensorDeallocation""\n        if debug_messages:\n            print((""Got deallocation for %s""%(d[""allocation_id""])))\n    elif \'MemoryLogStep\' in l:\n        m = tensor_logstep_regex.search(l)\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogStep""\n    elif \'MemoryLogRawAllocation\' in l:\n        m = raw_allocation_regex.search(l)\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogRawAllocation""\n    elif \'MemoryLogRawDeallocation\' in l:\n        m = raw_deallocation_regex.search(l)\n        assert m, l\n        d = m.groupdict()\n        d[""type""] = ""MemoryLogRawDeallocation""\n    else:\n        assert False, ""Unknown log line: ""+l\n        \n    if not ""allocation_id"" in d:\n        d[""allocation_id""] = ""-1""\n\n    d[""line""] = l\n    return d\n\ndef memory_timeline(log):\n    if hasattr(log, \'getvalue\'):\n        log = log.getvalue()\n    \n    def unique_alloc_id(line):\n        if line[""allocation_id""] == ""-1"":\n            return ""-1""\n        return line[""allocation_id""]+""-""+line[""allocator_name""]\n    \n    def get_alloc_names(line):\n        alloc_id = unique_alloc_id(line)\n        for entry in reversed(allocation_map.get(alloc_id, [])):\n            kernel_name = entry.get(""kernel_name"", ""unknown"")\n            if not ""unknown"" in kernel_name:\n                return kernel_name+""(""+unique_alloc_id(line)+"")""\n        # couldn\'t find an allocation message with name of kernel\n        return ""(""+alloc_id+"")""\n\n    def get_alloc_bytes(line):\n        for entry in allocation_map.get(unique_alloc_id(line), []):\n            if ""allocated_bytes"" in entry:\n                return entry[""allocated_bytes""]\n        return ""0""\n\n    def get_alloc_type(line):\n        for entry in allocation_map.get(unique_alloc_id(line), []):\n            if ""allocator_name"" in entry:\n                return entry[""allocator_name""]\n        return ""0""\n\n    parsed_lines = []\n    for l in log.split(""\\n""):\n        if \'LOG_MEMORY\' in l: # and not \'step_id: -6\' in l:\n            parsed_lines.append(_parse_logline(l))\n\n    allocation_map = {} # map of <allocation_id>-<allocator_name>->parsed_logline of allocation\n    for line in parsed_lines:\n        if (line[""type""] == ""MemoryLogTensorAllocation"" or line[""type""] == ""MemoryLogRawAllocation"" or\n            line[""type""] == ""MemoryLogTensorOutput""):\n            allocation_map.setdefault(unique_alloc_id(line), []).append(line)\n    if debug_messages:\n        print(allocation_map)\n    result = []\n    for i, line in enumerate(parsed_lines):\n        # skip lines without allocation_id, ie lines like\n        # I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogStep { step_id: 2 handle: ""->/gradients/a1_grad/TanhGrad/0/;1"" }\n\n        if int(line[""allocation_id""]) == -1:\n            continue\n        alloc_names = get_alloc_names(line)\n        # if line doesn\'t specify bytes, look in history if there was corresponding TensorOutput or TensorAllocation msg\n        if int(line.get(\'allocated_bytes\', -1)) < 0:\n            alloc_bytes = get_alloc_bytes(line)\n        else:\n            alloc_bytes = line.get(\'allocated_bytes\', -1)\n        alloc_type = get_alloc_type(line)\n        if line[""type""] == ""MemoryLogTensorOutput"":\n            continue\n        if line[""type""] == ""MemoryLogTensorDeallocation"" or line[""type""]==""MemoryLogRawDeallocation"":\n            alloc_bytes = ""-"" + alloc_bytes\n        result.append((i, alloc_names, alloc_bytes, alloc_type))\n    return result\n\ndef peak_memory(log, gpu_only=False):\n    """"""Peak memory used across all devices.""""""\n    peak_memory = -123456789 # to catch bugs\n    total_memory = 0\n    for record in memory_timeline(log):\n        i, kernel_name, allocated_bytes, allocator_type = record\n        allocated_bytes = int(allocated_bytes)\n        if gpu_only:\n            if not allocator_type.startswith(""gpu""):\n                continue\n        total_memory += allocated_bytes\n        peak_memory = max(total_memory, peak_memory)\n    return peak_memory\n    \ndef print_memory_timeline(log, gpu_only=False, ignore_less_than_bytes=0):\n      \n    total_memory = 0\n    for record in memory_timeline(log):\n        i, kernel_name, allocated_bytes, allocator_type = record\n        allocated_bytes = int(allocated_bytes)\n        if gpu_only:\n            if not allocator_type.startswith(""gpu""):\n                continue\n        if abs(allocated_bytes)<ignore_less_than_bytes:\n            continue  # ignore small allocations\n        total_memory += allocated_bytes\n        print((""%9d %42s %11d %11d %s""%(i, kernel_name, allocated_bytes, total_memory, allocator_type)))\n\nimport matplotlib.pyplot as plt\ndef plot_memory_timeline(log, gpu_only=False, ignore_less_than_bytes=1000):\n      \n    total_memory = 0\n    timestamps = []\n    data = []\n    current_time = 0\n    for record in memory_timeline(log):\n        timestamp, kernel_name, allocated_bytes, allocator_type = record\n        allocated_bytes = int(allocated_bytes)\n        \n        if abs(allocated_bytes)<ignore_less_than_bytes:\n            continue  # ignore small allocations\n        if gpu_only:\n            if not record[3].startswith(""gpu""):\n                continue\n        timestamps.append(current_time-.00000001)\n        data.append(total_memory)\n        total_memory += int(record[2])\n        timestamps.append(current_time)\n        data.append(total_memory)\n        current_time+=1\n    plt.plot(timestamps, data)\n\n################################################################################\n# smart initialize\n################################################################################\n\ndef smart_initialize(variables=None, sess=None):\n  """"""Initializes all uninitialized variables in correct order. Initializers\n  are only run for uninitialized variables, so it\'s safe to run this multiple\n  times.\n\n  Args:\n      sess: session to use. Use default session if None.\n  """"""\n\n  from tensorflow.contrib import graph_editor as ge\n  def make_initializer(var): \n    def f():\n      return tf.assign(var, var.initial_value).op\n    return f\n  \n  def make_noop(): return tf.no_op()\n\n  def make_safe_initializer(var):\n    """"""Returns initializer op that only runs for uninitialized ops.""""""\n    return tf.cond(tf.is_variable_initialized(var), make_noop,\n                   make_initializer(var), name=""safe_init_""+var.op.name).op\n\n  if not sess:\n    sess = tf.get_default_session()\n  g = tf.get_default_graph()\n\n  if not variables:\n    variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n      \n  safe_initializers = {}\n  for v in variables:\n    safe_initializers[v.op.name] = make_safe_initializer(v)\n      \n  # initializers access variable vaue through read-only value cached in\n  # <varname>/read, so add control dependency to trigger safe_initializer\n  # on read access\n  for v in variables:\n    var_name = v.op.name\n    var_cache = g.get_operation_by_name(var_name+""/read"")\n    ge.reroute.add_control_inputs(var_cache, [safe_initializers[var_name]])\n\n  sess.run(tf.group(*safe_initializers.values()))\n    \n  # remove initializer dependencies to avoid slowing down future variable reads\n  for v in variables:\n    var_name = v.op.name\n    var_cache = g.get_operation_by_name(var_name+""/read"")\n    ge.reroute.remove_control_inputs(var_cache, [safe_initializers[var_name]])\n\n'"
setup.py,0,"b'# To make TensorMol available.\n# sudo pip install -e .\n#\n# to make and upload a source dist \n# python setup.py sdist\n# twine upload dist/*\n# And of course also be me. \n# \n\nfrom __future__ import absolute_import,print_function\nfrom distutils.core import setup, Extension\nimport numpy\nimport os\n\nprint(""Numpy Include Dir: "",numpy.get_include())\n\nLLVM=os.popen(\'cc --version | grep clang\').read().count(""LLVM"")\nif (not LLVM):\n\tMolEmb = Extension(\n\t\'MolEmb\',\n\tsources=[\'./C_API/MolEmb.cpp\'],\n\textra_compile_args=[\'-std=c++0x\',\'-g\',\'-fopenmp\',\'-w\'],\n\textra_link_args=[\'-lgomp\'],\n        include_dirs=[numpy.get_include()]+[\'./C_API/\'])\nelse:\n        MolEmb = Extension(\n        \'MolEmb\',\n        sources=[\'./C_API/MolEmb.cpp\'],\n        extra_compile_args=[\'-std=c++0x\'],\n        extra_link_args=[],\n        include_dirs=[numpy.get_include()]+[\'./C_API/\'])\n\n\n# run the setup\nsetup(name=\'TensorMol\',\n      version=\'0.2\',\n      description=\'TensorFlow+Molecules = TensorMol\',\n      url=\'http://github.com/jparkhill/TensorMol\',\n      author=\'john parkhill\',\n      author_email=\'john.parkhill@gmail.com\',\n      license=\'GPL3\',\n      packages=[\'TensorMol\'],\n      zip_safe=False,\n      include_package_data=True,\n      ext_modules=[MolEmb])\n'"
C_API/test.py,0,"b'import numpy\na=numpy.array([[0.0 , 0 ,0 ],[1.0, 0.0, -1.0]])\nimport Make_CM\n'"
TensorMol/PhysicalData.py,0,"b'# The spatial unit of TensorMol is Angstrom.\n# Tne energy unit of Tensormol is Hartree except in MD where it is kcal/mol\n# The time unit of Tensormol is the Fs.\n#\n# These should be all Caps. etc...\n#\nfrom __future__ import absolute_import\nimport numpy as np\nfrom math import pi as Pi\n\nELEHEATFORM = {1:-0.497912, 6:-37.844411, 7:-54.581501, 8:-75.062219, 9:-99.716370}     # ref: https://figshare.com/articles/Atomref%3A_Reference_thermochemical_energies_of_H%2C_C%2C_N%2C_O%2C_F_atoms./1057643\nbond_length_thresh = {""HH"": 1.1, ""HC"": 1.4, ""HN"": 1.4, ""HO"": 1.3, ""CC"": 1.7, ""CN"": 1.7, ""CO"": 1.7, ""NN"": 1.7, ""NO"": 1.7, ""OO"": 1.7 } # https://figshare.com/articles/Atomref%3A_Reference_thermochemical_energies_of_H%2C_C%2C_N%2C_O%2C_F_atoms./1057643\nORBITALNAMES = [""1s"",""2s"",""2p"",""3s"",""3p"",""4s"",""3d"",""4p"",""5s"",""4d"",""5p""]\nNCOREELEC = [0,0, 2,2, 2,2,2,2,2, 2, 10,10, 10,10,10,10,10, 10, 18,18, 18,18,18,18,18,18,18,18,18,18, 18,18,18,18,18, 18, 36,36, 36,36,36,36,36,36,36,36,36,36, 36,36,36,36,36, 36]\nele_U = {1:-0.500273, 6:-37.8462793, 7:-54.58449,  8:-75.060612}\nele_E_david = {1: -0.5026682859, 6:-37.8387398670, 8:-75.0586028553}\natoi = {\'H\':1,\'He\':2,\'Li\':3,\'Be\':4,\'B\':5,\'C\':6,\'N\':7,\'O\':8,\'F\':9,\'Ne\':10,\'Na\':11,\'Mg\':12,\'Al\':13,\'Si\':14,\'P\':15,\'S\':16,\'Cl\':17,\'Ar\':18,\'K\':19,\'Ca\':20,\'Sc\':21,\'Ti\':22,\'Si\':23,\'V\':24,\'Cr\':25,\'Br\':35, \'Cs\':55, \'Pb\':82}\nitoa = [\'X\',\'H\',\'He\',\'Li\',\'Be\',\'B\',\'C\',\'N\',\'O\',\'F\',\'Ne\',\'Na\',\'Mg\',\'Al\',\'Si\',\'P\',\'S\',\'Cl\',\'Ar\',\'K\',\'Ca\',\'Sc\',\'Ti\',\'Si\',\'V\',\'Cr\',\'Br\',\'Cs\',\'Pb\']\natoc = {1: 40, 6: 100, 7: 150, 8: 200, 9:240}\natom_valance = {1:1, 8:2, 7:3, 6:4}\nbond_index = {""HH"": 1, ""HC"": 2, ""HN"": 3, ""HO"": 4, ""CC"": 5, ""CN"": 6, ""CO"": 7, ""NN"": 8, ""NO"": 9, ""OO"": 10}\ndihed_pair = {1006:1, 1007:2, 1008:3, 6006:4, 6007:5, 6008:6,  7006:7, 7007:8, 7008:9, 8006:10, 8007:11, 8008:12}  # atomic_1*1000 + atomic_2 hacky way to do that\nATOMICRADII = {1:53.0, 2:31.0, 3:167.0, 4:112.0, 5:87.0, 6:67.0, 7:56.0, 8:48.0, 9:42.0, 10:38.0, 11:190.0, 12:145.0, 13:118.0, 14:111.0, 15:98.0, 16:88.0, 17:79.0, 18:71.0, 19:243, 20:194.0,21:184.0,22:176.0,23:171.0,24.0:171.0,25:166.0,27:161.0,28:156.0,29:152.0,30:149.0,31:145.0,32:142.0} # units in pm, ref: https://en.wikipedia.org/wiki/Atomic_radius\nATOMICMASSESAMU = np.array([1.00794, 4.002602, 6.941, 9.012182, 10.811, 12.0107, 14.0067, 15.9994, 18.9984032, 20.1791, 22.98976928, 24.3050, 26.9815386, 28.0855, 30.973762, 32.065, 35.453, 39.948, 39.0983, 40.078,44.955912,47.867,50.9415,51.9961,54.938045,55.845,58.933195,58.6934,63.546,65.38,69.723,72.63,74.92160,78.96,79.904,83.798,85.4678,87.62,88.90585,91.224,92.90638,95.96,98.,101.07,102.90550,106.42,107.8682,112.411,114.818,118.710,121.760,127.60,126.90447,131.293,132.9054519,137.327,138.90547,140.116,140.90765,144.242,145.,150.36,151.964,157.25,158.92535,162.500,164.93032,167.259,168.93421,173.054,174.9668,178.49,180.94788,183.84,186.207,190.23,192.217,195.084,196.966569,200.59,204.3833,207.2,208.98040,209.,210.,222.,223.,226.,227.,232.03806,231.03586,238.02891,237.,244.,243.,247.,247.,251.,252.,257.])\natomic_vdw_radius = {1:1.001, 2:1.012, 3:0.825, 4:1.408, 5:1.485, 6:1.452, 7:1.397, 8:1.342, 9:1.287, 10:1.243} # ref: http://onlinelibrary.wiley.com/doi/10.1002/jcc.20495/epdf   unit in angstrom\nC6_coff = {1:0.14, 2:0.08, 3:1.16, 4:1.61, 5:3.13, 6:1.75, 7:1.23, 8:0.70, 9:0.75, 10:0.63}  # ref: http://onlinelibrary.wiley.com/doi/10.1002/jcc.20495/epdf unit in Jnm^6/mol\nS6 = {""PBE"": 0.75, ""BLYP"":1.2, ""B-P86"":1.05, ""TPSS"":1.0, ""B3LYP"":1.05}  # s6 scaler of different DF of Grimmer C6 scheme\nGOLDENRATIO = (np.sqrt(5.)+1.0)/2.0\nKAYBEETEE = 0.000950048 # At 300K\nBOHRPERA = 1.889725989\nANGSTROMPERMETER = pow(10.0,10.0)\nBOHRPERM = BOHRPERA*ANGSTROMPERMETER\nBOHRINM = 0.52917720859*pow(10.0,-10.0)\nKJPERHARTREE = 2625.499638\nJOULEPERHARTREE = KJPERHARTREE*1000.0\nJOULEPERKCAL = 4183.9953\nKCALPERHARTREE = 627.509474\nWAVENUMBERPERHARTREE = 219474.63\nELECTRONPERPROTONMASS = 1836.15267\nFEMTOPERUNIT = pow(10.0,-15.0)\nPICOPERUNIT = pow(10.0,-12.0)\nSPEEDOFLIGHT=299792458.0 #m/s\nMASSOFELECTRON = 548.579909*pow(10.0,-9.0) # In kilograms/mol\nFSPERAU = 0.0241888\nAVOCONST = 6.02214086*np.power(10.0,23.0)\nAUPERDEBYE = 0.393456\nIDEALGASR =  8.3144621 # J/molK\nAMUINKG = 1.660538782*pow(10.0,-27.0)\nSECPERATOMIC = 2.418884326505*pow(10.0,-17.0) # Atomic unit of time.\n#Convert evals from H/(kg bohr^2) to J/(kg m^2) = 1/s^2 */\nKCONVERT = (4.359744*pow(10.0,-18.0))/(BOHRINM * BOHRINM * AMUINKG);\nCMCONVERT = 1.0/(2.0 * Pi * SPEEDOFLIGHT * 100.0);\n# These are in Kilograms/mol\nATOMICMASSES = 0.000999977*ATOMICMASSESAMU\n'"
TensorMol/TMParams.py,1,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nimport logging, time, os, sys\nfrom math import pi as Pi\nimport numpy as np\nimport tensorflow as tf\n\nclass TMParams(dict):\n\tdef __init__(self, *args, **kwargs ):\n\t\tmyparam = kwargs.pop(\'myparam\', \'\')\n\t\tdict.__init__(self, *args, **kwargs )\n\t\tself[""GIT_REVISION""] = os.popen(""git rev-parse --short HEAD"").read()\n\t\tself[""CheckLevel""] = 1 # whether to test the consistency of several things...\n\t\tself[""PrintTMTimer""] = False # whether to emit timing messages.\n\t\tself[""MAX_ATOMIC_NUMBER""] = 10\n\t\t# Parameters of MolEmb\n\t\tself[""RBFS""] = np.array([[0.35, 0.35], [0.70, 0.35], [1.05, 0.35], [1.40, 0.35], [1.75, 0.35], [2.10, 0.35], [2.45, 0.35],\n\t\t\t\t\t\t\t\t\t[2.80, 0.35], [3.15, 0.35], [3.50, 0.35], [3.85, 0.35], [4.20, 0.35], [4.55, 0.35], [4.90, 0.35]])\n\t\tself[""ANES""] = np.array([2.20, 1.0, 1.0, 1.0, 1.0, 2.55, 3.04, 3.44]) #pauling electronegativity\n\t\tself[""SRBF""] = np.zeros((self[""RBFS""].shape[0],self[""RBFS""].shape[0]))\n\t\tself[""SH_LMAX""]=4\n\t\tself[""SH_NRAD""]=14\n\t\tself[""SH_ORTH""]=1\n\t\tself[""SH_rot_invar""] = False\n\t\tself[""SH_MAXNR""]=self[""RBFS""].shape[0]\n\t\tself[""AN1_r_Rc""] = 4.6  # orgin ANI1 set\n\t\tself[""AN1_a_Rc""] = 3.1  # orgin ANI1 set\n\t\tself[""AN1_eta""] = 4.0\n\t\tself[""AN1_zeta""] = 8.0\n\t\t#self[""AN1_num_r_Rs""] = 40\n\t\t#self[""AN1_num_a_Rs""] = 10\n\t\t#self[""AN1_num_a_As""] = 10\n\t\tself[""AN1_num_r_Rs""] = 32\n\t\tself[""AN1_num_a_Rs""] = 8\n\t\tself[""AN1_num_a_As""] = 8\n\t\tself[""AN1_r_Rs""] = np.array([ self[""AN1_r_Rc""]*i/self[""AN1_num_r_Rs""] for i in range (0, self[""AN1_num_r_Rs""])])\n\t\tself[""AN1_a_Rs""] = np.array([ self[""AN1_a_Rc""]*i/self[""AN1_num_a_Rs""] for i in range (0, self[""AN1_num_a_Rs""])])\n\t\tself[""AN1_a_As""] = np.array([ 2.0*Pi*i/self[""AN1_num_a_As""] for i in range (0, self[""AN1_num_a_As""])])\n\t\tself[""GradScalar""] = 1.0\n\t\tself[""DipoleScalar""] = 1.0\n\t\t# SET GENERATION parameters\n\t\tself[""RotateSet""] = 0\n\t\tself[""TransformSet""] = 1\n\t\tself[""NModePts""] = 10\n\t\tself[""NDistorts""] = 5\n\t\tself[""GoK""] = 0.05\n\t\tself[""dig_ngrid""] = 20\n\t\tself[""dig_SamplingType""]=""Smooth""\n\t\tself[""BlurRadius""] = 0.05\n\t\tself[""Classify""] = False # Whether to use a classifier histogram scheme rather than normal output.\n\t\t# MBE PARAMS\n\t\tself[""Embedded_Charge_Order""] = 2\n\t\tself[""MBE_ORDER""] = 3\n\t\t# Training Parameters\n\t\tself[""MonitorSet""] = None\n\t\tself[""NetNameSuffix""] = """"\n\t\tself[""NeuronType""] = ""relu""\n\t\tself[""tf_prec""] = ""tf.float64"" # Do not change this to 32.\n\t\tself[""learning_rate""] = 0.001\n\t\tself[""learning_rate_dipole""] = 0.0001\n\t\tself[""learning_rate_energy""] = 0.00001\n\t\tself[""momentum""] = 0.9\n\t\tself[""max_steps""] = 1001\n\t\tself[""batch_size""] = 1000\n\t\tself[""test_freq""] = 10\n\t\tself[""HiddenLayers""] = [200, 200, 200]\n\t\tself[""hidden1""] = 512\n\t\tself[""hidden2""] = 512\n\t\tself[""hidden3""] = 512\n\t\tself[""GradWeight""] = 0.01\n\t\tself[""TestRatio""] = 0.2\n\t\tself[""Profiling""] = False\n\t\tself[""max_checkpoints""] = 1\n\t\tself[""KeepProb""] = 0.7\n\t\tself[""weight_decay""] = 0.001\n\t\tself[""ConvFilter""] = [32, 64]\n\t\tself[""ConvKernelSize""] = [[8,1],[4,1]]\n\t\tself[""ConvStrides""] = [[8,1],[4,1]]\n\t\tself[""sigmoid_alpha""] = 100.0\n\t\tself[""EnergyScalar""] = 1.0\n\t\tself[""GradScalar""] = 1.0/20.0\n\t\tself[""DipoleScaler""]=1.0\n\t\t# DATA usage parameters\n\t\tself[""InNormRoutine""] = None\n\t\tself[""OutNormRoutine""] = None\n\t\tself[""RandomizeData""] = True\n\t\tself[""MxTimePerElement""] = 36000\n\t\tself[""MxMemPerElement""]=16000 # Max Array for an element in MB\n\t\tself[""ChopTo""] = None\n\t\tself[""RotAvOutputs""] = 1 # Rotational averaging of force outputs.\n\t\tself[""OctahedralAveraging""] = 0 # Octahedrally Average Outputs\n\t\tself[""train_gradients""] = True\n\t\tself[""train_dipole""] = True\n\t\tself[""train_quadrupole""] = False\n\t\tself[""train_rotation""] = True\n\t\t# Opt Parameters\n\t\tself[""OptMaxCycles""]=50\n\t\tself[""OptThresh""]=0.0001\n\t\tself[""OptMaxStep""]=0.1\n\t\tself[""OptStepSize""] = 0.1\n\t\tself[""OptMomentum""] = 0.5\n\t\tself[""OptMomentumDecay""] = 0.8\n\t\tself[""OptPrintLvl""] = 1\n\t\tself[""OptLatticeStep""] = 0.050\n\t\tself[""GSSearchAlpha""] = 0.05\n\t\tself[""SDStep""] = 0.05\n\t\tself[""MaxBFGS""] = 7\n\t\tself[""NebSolver""] = ""Verlet""\n\t\tself[""NebNumBeads""] = 18 # It\'s important to have enough beads.\n\t\tself[""NebK""] = 0.07\n\t\tself[""NebKMax""] = 1.0\n\t\tself[""NebClimbingImage""] = True\n\t\tself[""DiisSize""] = 20\n\t\tself[""RemoveInvariant""] = True\n\t\t# MD Parameters\n\t\tself[""MDMaxStep""] = 20000\n\t\tself[""MDdt""] = 0.2 # In fs.\n\t\tself[""MDTemp""] = 300.0\n\t\tself[""MDV0""] = ""Random""\n\t\tself[""MDThermostat""] = None # None, ""Rescaling"", ""Nose"", ""NoseHooverChain""\n\t\tself[""MDLogTrajectory""] = True\n\t\tself[""MDUpdateCharges""] = True\n\t\tself[""MDIrForceMin""] = False\n\t\tself[""MDAnnealT0""] = 20.0\n\t\tself[""MDAnnealTF""] = 300.0\n\t\tself[""MDAnnealKickBack""] = 1.0\n\t\tself[""MDAnnealSteps""] = 1000\n\t\t# MD applied pulse parameters\n\t\tself[""MDFieldVec""] = np.array([1.0,0.0,0.0])\n\t\tself[""MDFieldAmp""] = 0.0\n\t\tself[""MDFieldFreq""] = 1.0/1.2\n\t\tself[""MDFieldTau""] = 1.2\n\t\tself[""MDFieldT0""] = 3.0\n\t\t# Metadynamics parameters\n\t\tself[""MetaBumpTime""] = 15.0\n\t\tself[""MetaBowlK""] = 0.0\n\t\tself[""MetaMaxBumps""] = 500\n\t\tself[""MetaMDBumpHeight""] = 0.05\n\t\tself[""MetaMDBumpWidth""] = 0.1\n\t\t# parameters of electrostatic embedding\n\t\tself[""AddEcc""] = True\n\t\tself[""OPR12""] = ""Poly"" # Poly = Polynomial cutoff or Damped-Shifted-Force\n\t\tself[""Poly_Width""] = 4.6\n\t\tself[""Elu_Width""] = 4.6\n\t\tself[""EEOn""] = True # Whether to calculate/read in the required data at all...\n\t\tself[""EESwitchFunc""] = ""CosLR"" # options are Cosine, and Tanh.\n\t\tself[""EEVdw""] = True # 1/r => 0.5*(Tanh[(r - EECutoff)/EEdr] + 1)/r\n\t\tself[""EEOrder""] = 2 # 1/r => 0.5*(Tanh[(r - EECutoff)/EEdr] + 1)/r\n\t\tself[""EEdr""] = 1.0 # 1/r => 0.5*(Tanh[(r - EECutoff)/EEdr] + 1)/r\n\t\tself[""EECutoff""] = 5.0 #switch between 0 and 1/r occurs at Angstroms.\n\t\tself[""EECutoffOn""] = 4.4 # switch on between 0 and 1/r occurs at Angstroms.\n\t\tself[""EECutoffOff""] = 15.0 # switch off between 0 and 1/r occurs at Angstroms.\n\t\tself[""Erf_Width""] = 0.2\n\t\tself[""DSFAlpha""] = 0.18\n\t\t#paths -- Allows for different placement of fast reads/writes.\n\t\tself[""tm_root""] = "".""\n\t\tself[""sets_dir""] = self[""tm_root""]+""/datasets/""\n\t\tself[""networks_directory""] = self[""tm_root""]+""/networks/""\n\t\tself[""output_root""] = "".""\n\t\tself[""results_dir""] = self[""output_root""]+""/results/""\n\t\tself[""dens_dir""] = self[""output_root""]+""/densities/""\n\t\tself[""log_dir""] = self[""output_root""]+""/logs/""\n\t\t# Garbage we\'re putting here for now.\n\t\tself[""Qchem_RIMP2_Block""] = ""$rem\\n   jobtype   sp\\n   method   rimp2\\n   MAX_SCF_CYCLES  200\\n   basis   cc-pvtz\\n   aux_basis rimp2-cc-pvtz\\n   symmetry   false\\n   INCFOCK 0\\n   thresh 12\\n   SCF_CONVERGENCE 12\\n$end\\n""\n\t\tnp.set_printoptions(formatter={\'float\': \'{: .8f}\'.format}) #Set pretty printing for numpy arrays\n\n\tdef __str__(self):\n\t\ttore=""""\n\t\tfor k in self.keys():\n\t\t\ttore = tore+k+"":""+str(self[k])+""\\n""\n\t\treturn tore\n\ndef TMBanner():\n\tprint(""--------------------------"")\n\ttry:\n\t\tif sys.version_info[0] < 3:\n\t\t\tprint((""    ""+unichr(0x1350)+unichr(0x2107)+unichr(0x2115)+unichr(0x405)+unichr(0x29be)+unichr(0x2c64)+\'-\'+unichr(0x164f)+unichr(0x29be)+unichr(0x2112)+""  0.1""))\n\t\telse:\n\t\t\tprint((""    ""+chr(0x1350)+chr(0x2107)+chr(0x2115)+chr(0x405)+chr(0x29be)+chr(0x2c64)+\'-\'+chr(0x164f)+chr(0x29be)+chr(0x2112)+""  0.1""))\n\texcept:\n\t\tpass\n\tprint(""--------------------------"")\n\tprint(""By using this software you accept the terms of the GNU public license in "")\n\tprint(""COPYING, and agree to attribute the use of this software in publications as: \\n"")\n\tprint(""K.Yao, J. E. Herr, D. Toth, R. McIntyre, J. Garside, J. Parckhill. TensorMol 0.2 (2018)"")\n\tprint(""--------------------------"")\n\ndef TMLogger(path_):\n\t#Delete Jupyter notebook root logger handler\n\tlogger = logging.getLogger()\n\tlogger.handlers = []\n\ttore=logging.getLogger(\'TensorMol\')\n\ttore.setLevel(logging.DEBUG)\n\t# Check path and make if it doesn\'t exist...\n\tif not os.path.exists(path_):\n\t\tos.makedirs(path_)\n\tfh = logging.FileHandler(filename=path_+time.strftime(""%a_%b_%d_%H.%M.%S_%Y"")+\'.log\')\n\tfh.setLevel(logging.DEBUG)\n\tch = logging.StreamHandler(sys.stdout)\n\tch.setLevel(logging.INFO)\n\tfformatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n\tpformatter = logging.Formatter(\'%(message)s\')\n\tfh.setFormatter(fformatter)\n\tch.setFormatter(pformatter)\n\ttore.addHandler(fh)\n\ttore.addHandler(ch)\n\treturn tore\n'"
TensorMol/Util.py,13,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport gc, random, os, sys, re, atexit\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\nimport random, math, time, itertools, warnings\nfrom math import pi as Pi\nimport scipy.special\n#from collections import defaultdict\n#from collections import Counter\nfrom TensorMol.TMParams import *\nTMBanner()\nfrom TensorMol.PhysicalData import *\nwarnings.simplefilter(action = ""ignore"", category = FutureWarning)\n#\n# GLOBALS\n#\tAny global variables of the code must be put here, and must be in all caps.\n#\tGlobal variables are almost never acceptable except in these few cases\n\n# PARAMETERS\nPARAMS = TMParams()\n#\n# -- begin Environment set up.\n#\n# if any of these paths do not exist create them.\ndef MakeDirIfAbsent(path):\n\tif sys.version_info[0]+sys.version_info[1]*0.1<3.2:\n\t\ttry:\n\t\t\tos.makedirs(path)\n\t\texcept OSError:\n\t\t\tif not os.path.isdir(path):\n\t\t\t\traise\n\telse:\n\t\tos.makedirs(path, exist_ok=True)\nMakeDirIfAbsent(PARAMS[""sets_dir""])\nMakeDirIfAbsent(PARAMS[""networks_directory""])\nMakeDirIfAbsent(PARAMS[""log_dir""])\nMakeDirIfAbsent(PARAMS[""results_dir""])\nMakeDirIfAbsent(PARAMS[""dens_dir""])\nMakeDirIfAbsent(PARAMS[""log_dir""])\nLOGGER = TMLogger(PARAMS[""log_dir""])\nMAX_ATOMIC_NUMBER = 10\n# Derived Quantities and useful things.\n#  TODO: Migrate these to PARAMS\nHAS_PYSCF = False\nHAS_EMB = False\nHAS_TF = False\nGRIDS = None\nHAS_GRIDS=False\n\nLOGGER.info(""Searching for Installed Optional Packages..."")\ntry:\n\tfrom pyscf import scf\n\tfrom pyscf import gto\n\tfrom pyscf import dft\n\tfrom pyscf import mp\n\tHAS_PYSCF = True\n\tLOGGER.debug(""Pyscf has been found"")\nexcept Exception as Ex:\n\tLOGGER.info(""Pyscf is not installed -- no ab-initio sampling"")\n\tpass\n\ntry:\n\timport MolEmb\n\tHAS_EMB = True\n\tLOGGER.debug(""MolEmb has been found."")\nexcept Exception as Ex:\n\tprint(""MolEmb is not installed. Please cd C_API; sudo python setup.py install"",Ex)\n\tpass\n\ntry:\n\timport tensorflow as tf\n\tLOGGER.debug(""Tensorflow version ""+tf.__version__+"" has been found"")\n\tHAS_TF = True\nexcept:\n\tLOGGER.info(""Tensorflow not Installed, very limited functionality"")\n\tpass\n\ntry:\n\timport multiprocessing\n\tN_CORES=multiprocessing.cpu_count()\n\tLOGGER.debug(""Found ""+str(N_CORES)+"" CPUs to thread over... "")\nexcept:\n\tLOGGER.info(""Only a single CPU, :( did you lose a war?"")\n\tpass\nLOGGER.debug(""TensorMol ready..."")\n\nLOGGER.debug(""TMPARAMS----------"")\nLOGGER.debug(PARAMS)\nLOGGER.debug(""TMPARAMS~~~~~~~~~~"")\n\nif (HAS_PYSCF and HAS_GRIDS):\n\tfrom TensorMol.Grids import *\n\tGRIDS = Grids()\n\tGRIDS.Populate()\nprint(""--------------------------"")\n#\n# -- end Environment set up.\n#\n\n# A simple timing decorator.\nTMTIMER = {}\nTMSTARTTIME = time.time()\ndef PrintTMTIMER():\n\tLOGGER.info(""=======    Accumulated Time Information    ======="")\n\tLOGGER.info(""Category   |||   Time Per Call   |||   Total Elapsed     "")\n\tfor key in TMTIMER.keys():\n\t\tif (TMTIMER[key][1]>0):\n\t\t\tLOGGER.info(key+"" ||| %0.5f ||| %0.5f "",TMTIMER[key][0]/(TMTIMER[key][1]),TMTIMER[key][0])\ndef TMTiming(nm_=""Obs""):\n\tif (not nm_ in TMTIMER.keys()):\n\t\tTMTIMER[nm_] = [0.,0]\n\tdef wrap(f):\n\t\tdef wf(*args,**kwargs):\n\t\t\tt0 = time.time()\n\t\t\toutput = f(*args,**kwargs)\n\t\t\tTMTIMER[nm_][0] += time.time()-t0\n\t\t\tTMTIMER[nm_][1] += 1\n\t\t\treturn output\n\t\tLOGGER.debug(""TMTimed ""+nm_+str(TMTIMER[nm_]))\n\t\treturn wf\n\treturn wrap\n\n@atexit.register\ndef exitTensorMol():\n\tPrintTMTIMER()\n\tLOGGER.info(""Total Time : %0.5f s"",time.time()-TMSTARTTIME)\n\tLOGGER.info(""~ Adios Homeshake ~"")\n\n#\n# All the garbage below here needs to be removed, organized\n# and the perpretrators genetalia scattered to the four winds.\n#\n\ndef complement(a,b):\n\treturn [i for i in a if b.count(i)==0]\n\ndef scitodeci(sci):\n\ttmp=re.search(r\'(\\d+\\.?\\d+)\\*\\^(-?\\d+)\',sci)\n\treturn float(tmp.group(1))*pow(10,float(tmp.group(2)))\n\ndef AtomicNumber(Symb):\n\ttry:\n\t\treturn atoi[Symb]\n\texcept Exception as Ex:\n\t\traise Exception(""Unknown Atom"")\n\treturn 0\n\ndef AtomicSymbol(number):\n\ttry:\n\t\treturn atoi.keys()[atoi.values().index(number)]\n\texcept Exception as Ex:\n\t\traise Exception(""Unknown Atom"")\n\treturn 0\n\ndef LtoS(l):\n\ts=""""\n\tfor i in l:\n\t\ts+=str(i)+"" ""\n\treturn s\n\ndef nCr(n, r):\n\tf = math.factorial\n\treturn int(f(n)/f(r)/f(n-r))\n\n# Wow... Holy shit kun. Stop putting stuff here. Totally inappropriate.\n\ndef DSF(R, R_c, alpha):\t# http://aip.scitation.org.proxy.library.nd.edu/doi/pdf/10.1063/1.2206581 damp shifted force\n\tif R > R_c:\n\t\treturn 0.0\n\telse:\n\t\ttwooversqrtpi = 1.1283791671\n\t\tXX = alpha*R_c\n\t\tZZ = scipy.special.erfc(XX)/R_c\n\t\tYY = twooversqrtpi*alpha*math.exp(-XX*XX)/R_c\n\t\tLR = (scipy.special.erfc(alpha*R)/R - ZZ + (R-R_c)*(ZZ/R_c+YY))\n\t\treturn LR\n\ndef DSF_Gradient(R, R_c, alpha):\n\tif R > R_c:\n\t\treturn 0.0\n\telse:\n\t\ttwooversqrtpi = 1.1283791671\n\t\tXX = alpha*R_c\n\t\tZZ = scipy.special.erfc(XX)/R_c\n\t\tYY = twooversqrtpi*alpha*math.exp(-XX*XX)/R_c\n\t\tgrads = -((scipy.special.erfc(alpha*R)/R/R + twooversqrtpi*alpha*math.exp(-alpha*R*alpha*R)/R)-(ZZ/R_c + YY))\n\t\treturn grads\n\ndef EluAjust(x, a, x0, shift):\n\tif x > x0:\n\t\treturn a*(x-x0)+shift\n\telse:\n\t\treturn a*(math.exp(x-x0)-1.0)+shift\n\ndef sigmoid_with_param(x, prec=tf.float64):\n\treturn tf.log(1.0+tf.exp(tf.multiply(tf.cast(PARAMS[""sigmoid_alpha""], dtype=prec), x)))/tf.cast(PARAMS[""sigmoid_alpha""], dtype=prec)\n\ndef guassian_act(x, prec=tf.float64):\n\treturn tf.exp(-x*x)\n\ndef guassian_rev_tozero(x, prec=tf.float64):\n\treturn tf.where(tf.greater(x, 0.0), 1.0-tf.exp(-x*x), tf.zeros_like(x))\n\ndef guassian_rev_tozero_tolinear(x, prec=tf.float64):\n\ta = 0.5\n\tb = -0.06469509698101589\n\tx0 = 0.2687204431537632\n\tstep1 = tf.where(tf.greater(x, 0.0), 1.0-tf.exp(-x*x), tf.zeros_like(x))\n\treturn tf.where(tf.greater(x, x0), a*x+b, step1)\n\ndef square_tozero_tolinear(x, prec=tf.float64):\n\ta = 1.0\n\tb = -0.0025\n\tx0 = 0.005\n\tstep1 = tf.where(tf.greater(x, 0.0), 100.0*x*x, tf.zeros_like(x))\n\treturn tf.where(tf.greater(x, x0), a*x+b, step1)\n'"
TensorMol/__init__.py,1,"b'""""""Code Conventions and Style Guide:\n\n- Write code modularly. In proper directories, with minimal imports.\n- USE HARD TABS. configure whatever editor you are using to use hard tabs.\n- UseCapitalizationToSeparateWords in names.\n- Prefer long interperable words to ambiguous abbreviations. MakesDipoleTensor() >> mdt123()\n- Avoid_the_underscore to separate words which takes longer to type than a cap. MakesDipoleTensor() >> Makes_Dipole_Tensor\n- The underscore is a good way to denote a function argument. TakesSet(aset_)\n- Keep functions to fewer than 5 parameters\n- Keep files and classes to < 2000 lines.\n- Keep classes to < 20 member variables.\n- Keep loops to a depth < 6\n- Use functional programming constructs whenever possible.\n- Use Google-style docstrings, you asshole, and use Args: and Returns:\n- Commit your changes once a day at least.\n- Use tf.Tensor and np.array rather than python list whenever possible\n- It\'s NOT okay to put default parameters in __init__() and change them all the time. Add them to TMPARAMS.py so they become logged and attached to results.\n- import TensorMol as tm; works as desired, don\'t mess that up.\n\nViolators are subject to having their code and reproductive fitness mocked publically in comments.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n__version__=""0.2""\nfrom TensorMol.Util import * # Populates the PARAMS and LOGGER.\nfrom TensorMol.PhysicalData import *\nfrom .Containers import *\nfrom .Math import *\nfrom .TFNetworks import *\nfrom .Interfaces import *\nfrom .ForceModels import *\nfrom .ForceModifiers import *\nfrom .Simulations import *\n#__all__ = [""Util"", ""PhysicalData"", ""Math"" , ""Containers"" ,""Containers.Mol"" ,""Containers.Sets"" ,""TFNetworks"",""Interfaces"",""ForceModels"",""ForceModifiers"",""Simulations""]\nLOGGER.debug(""TensorMol import complete."")\n'"
datasets/rna_solvate.py,0,"b'from simtk.openmm.app import *\nfrom simtk.openmm import *\nfrom simtk.unit import *\nfrom sys import stdout\nfrom pdbfixer import PDBFixer\nimport numpy as np\n\nfixer = PDBFixer(filename=\'rna_ac.pdb\')\n#fixer.findMissingResidues()\nfixer.missingResidues = {}\n# Pull out and save the coordinates of the desired ligand. \n#fixer.findMissingAtoms()\n#fixer.addMissingAtoms()\n#fixer.addMissingHydrogens(7.0)\nmnx = min([p[0] for p in fixer.positions])._value\nmny = min([p[1] for p in fixer.positions])._value\nmnz = min([p[2] for p in fixer.positions])._value\nfixer.positions._value = [p - Vec3(mnx,mny,mnz) for p in fixer.positions._value]\nmaxSize = max(max((pos[i] for pos in fixer.positions))-min((pos[i] for pos in fixer.positions)) for i in range(3))\nboxSize = maxSize*Vec3(1, 1, 1)\nboxVectors = (maxSize*Vec3(1, 0, 0),maxSize*Vec3(0, 1, 0),maxSize*Vec3(0, 0, 1))\n\n#\n# This is basically the pdbfixer code, but without the amber lines. \n#\nmodeller = Modeller(fixer.topology, fixer.positions)\nforcefield = ForceField(\'amber99sb.xml\', \'tip5p.xml\')\nsystem = forcefield.createSystem(fixer.topology, nonbondedMethod=PME, nonbondedCutoff=0.05*nanometer, constraints=HBonds)\nmodeller.addSolvent(forcefield, padding=0.05*nanometer, boxSize=None, boxVectors=None)\n#modeller.addSolvent(forcefield, padding=0.4*nanometer, boxSize, boxVectors=boxVectors, model=\'tip5p\')\n# modeller.addSolvent(forcefield, padding=padding, boxSize=boxSize, boxVectors=boxVectors, positiveIon=positiveIon, negativeIon=negativeIon, ionicStrength=ionicStrength)\nfixer.topology = modeller.topology\nfixer.positions = modeller.positions\n\nproatoms = [atom.element._symbol for atom in modeller.topology.atoms()]\nprocoords = np.array([fixer.positions[atom.index]._value for atom in modeller.topology.atoms()])\n\ndef WriteXYZfile(atoms,coords,nm_=""out.xyz""):\n    natom = len(atoms)\n    f = open(nm_,""w"")\n    f.write(str(natom)+""\\n""+""\\n"")\n    for i in range(natom): \n        f.write(atoms[i]+"" ""+str(coords[i][0])+"" ""+str(coords[i][1])+"" ""+str(coords[i][2])+""\\n"")\n        \n#\n# This will directly generate XYZ files for both the protein and the substrate. \n#\nWriteXYZfile(CofactorAtoms,CofactorCoords*10.0,""cofactor.xyz"")\nWriteXYZfile(proatoms,procoords*10.0,""protein.xyz"")\n#PDBFile.writeFile(fixer.topology, fixer.positions, open(\'3gm0_fixed.pdb\', \'w\'))\n'"
datasets/solvate.py,0,"b'#\n# This little script shows you how to make a reasonably solvated xyz \n# from a pdb file using OpenMM and SimTk. \n# Well it makes a pdb from which you can generate an xyz in VMD\n# or other reasonable converters. JAP 2017\n#\n\nfrom simtk.openmm.app import *\nfrom simtk.openmm import *\nfrom simtk.unit import *\nfrom sys import stdout\nfrom pdbfixer import PDBFixer\nimport numpy as np\n\n#fixer = PDBFixer(filename=\'2mzx.pdb\')\n#fixer.findMissingResidues()\n#fixer.findMissingAtoms()\n#fixer.addMissingAtoms()\n#fixer.addMissingHydrogens(7.0)\n##fixer.addSolvent(fixer.topology.getUnitCellDimensions())\n#PDBFile.writeFile(fixer.topology, fixer.positions, open(\'output.pdb\', \'w\'))\n\n\npdb = PDBFile(\'output.pdb\')\np = 0.8\n# Center the thing and set the box size. \nx = pdb.getPositions(asNumpy=True)\nprint x \nxmn = np.min(x[:,0])._value - p \nymn = np.min(x[:,1])._value - p \nzmn = np.min(x[:,2])._value - p \nx = pdb.getPositions(asNumpy=True,frame=0)\nx._value -= np.array([xmn,ymn,zmn])\nprint x\nxmx = np.max(x[:,0])._value + p \nymx = np.max(x[:,1])._value + p \nzmx = np.max(x[:,2])._value + p \nprint xmx,ymx,zmx\npdb.positions = x\npdb.topology.setUnitCellDimensions([xmx,ymx,zmx])\nmodeller = Modeller(pdb.topology, pdb.positions)\nprint modeller.topology.getUnitCellDimensions()\n\nforcefield = ForceField(\'amber99sb.xml\', \'tip5p.xml\')\nprint ""adding Hydrogens""\nmodeller.addHydrogens(forcefield, pH=5.0)\nmodeller.addSolvent(forcefield, padding=0.4*nanometer, model=\'tip5p\')\nsystem = forcefield.createSystem(pdb.topology, nonbondedMethod=PME, nonbondedCutoff=0.05*nanometer, constraints=HBonds)\n\nf = open(""solvated.pdb"",""w"")\npdb.writeFile(modeller.topology,modeller.positions,f)\n\nexit(0)\n\nintegrator = LangevinIntegrator(300*kelvin, 1/picosecond, 0.002*picoseconds)\nsimulation = Simulation(pdb.topology, system, integrator) \nsimulation.context.setPositions(pdb.positions)\nsimulation.minimizeEnergy(maxIterations=25)\nsimulation.reporters.append(PDBReporter(\'output_exercise1.pdb\', 5))\nsimulation.step(1000)\n'"
doc/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# TensorMol documentation build configuration file, created by\n# sphinx-quickstart on Fri Dec 30 14:20:37 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'..\'))\nprint sys.path\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.3\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\'sphinx.ext.todo\',\n\t\'sphinx.ext.napoleon\',\n    \'sphinx.ext.imgmath\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n#MolEmb doesn\'t import propertly so mock it out. \n\n#from unittest.mock import MagicMock\nfrom mock import Mock as MagicMock\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n            return MagicMock()\nMOCK_MODULES = [\'pygtk\', \'gtk\', \'gobject\', \'argparse\', \'numpy\', \'pandas\',\'tensorflow\',\'MolEmb\',\'scipy\',\'scipy.special\']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'TensorMol\'\ncopyright = u\'2016, K. Yao, J. E. Herr, J. Parkhill\'\nauthor = u\'K. Yao, J. E. Herr, J. Parkhill\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'TensorMoldoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'TensorMol.tex\', u\'TensorMol Documentation\',\n     u\'K. Yao, J. E. Herr, J. Parkhill\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'tensormol\', u\'TensorMol Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'TensorMol\', u\'TensorMol Documentation\',\n     author, \'TensorMol\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
notebooks/InnerProduct.py,0,"b'# Creates inner product of  TensorMol vectors\nfrom TensorMol import *\nimport numpy as np \nnm = PullFreqData()\na = MSet(""david_test.xyz"")\na.ReadXYZ(""david_test"")\nmanager= TFMolManage(""Mol_uneq_chemspider_ANI1_Sym_fc_sqdiff_BP_1"" , None, False, RandomTData_=False, Trainable_=False)\nm = a.mols[7]\nmasses = np.array(list(map(lambda x: ATOMICMASSESAMU[x-1],m.atoms)))\nresized_masses = np.transpose(np.resize(masses,(3,masses.shape[0]))).reshape((-1))\nprint ""resized_masses:"", resized_masses\nnm = nm.reshape((33,-1))\nIP = np.zeros((33,33))\nfor i in range(nm.shape[0]):\n    for j in range(nm.shape[0]):\n        IP[i, j] = np.dot(nm[i,:], nm[j, :]*np.sqrt(resized_masses))\nprint IP\n\n\n\nprint ""m.Natoms()"", m.NAtoms(), 3*m.NAtoms()\nEnergyField = lambda x: manager.Eval_BPForceSingle(Mol(m.atoms,x),True)[0]\nw,v = HarmonicSpectra(EnergyField, m.coords, masses)\nv = v.real \nprint ""v:"", v\nprint ""nm:"",nm\nprint ""3n:"", m.NAtoms()*3\nprint ""v.shape:"", v.shape\nprint ""nm.shape:"", nm.shape\nfor i in range(3*m.NAtoms()): \n\tnm = v[:,i].reshape((m.NAtoms(),3))\n\t# nm *= np.sqrt(np.array([map(lambda x: ATOMICMASSESAMU[x-1],m.atoms)])).T\n\tfor alpha in np.append(np.linspace(-.1,.1,30),np.linspace(.1,-.1,30)):\n\t\tmdisp = Mol(m.atoms,m.coords+alpha*nm)\n\t\tmdisp.WriteXYZfile(""./results/"",""NormalMode_""+str(i))\n\n\n#for i in range(v.shape[0]):\n#    for j in range(v.shape[0]):\n#        IP[i, j] = np.dot(v[:,i], v[:, j])\n#print IP\n\nnm = nm.reshape((33,-1))\nIP = np.zeros((39,39))\nfor i in range(v.shape[0]):\n    for j in range(nm.shape[0]):\n        IP[i, j] = np.dot(v[:,i], nm[j, :])#*np.sqrt(resized_masses))\nprint IP\nnp.savetxt(""IPmat.dat"",IP)\n'"
notebooks/MatchFrequency.py,0,"b'import numpy as np \nexpv = np.loadtxt(""adenine_exp.dat"")\nTMv = np.loadtxt(""adenine_TM.dat"")\nfound = []\nnfound = []\nfor i in range(TMv.shape[0]):\n    least_diff = float(\'inf\')\n    least_index = None\n    for j in range(expv.shape[0]):\n        if j in found:\n            continue\n    \tif j not in found:\n        \tdiff = abs(expv[j] - TMv[i])\n        \tif diff < least_diff:\n            \t\tleast_diff = diff\n            \t\tleast_index = j\n    if least_index != None:\n    \tfound.append(least_index)\nfor i in range(len(expv)):\n    if i not in found:\n        nfound.append(i)\nprint expv[found]\nl = 0 \nwhile l<2: \n    for i in range(len(found)):\n        for j in range(i+1, len(found)):\n            for k in range(len(nfound)):\n                diff = abs(TMv[i] - expv[found[i]]) + abs(TMv[j] - expv[found[j]])\n                diff1 = abs(TMv[i] - expv[found[j]]) + abs(TMv[j] - expv[found[i]])\n                if diff1 < diff:\n                    l = 0 \n                    found[i], found[j] = found[j], found[i]\n                    diff = diff1\n                diff2 = abs(TMv[i] - expv[found[i]]) + abs(TMv[j] - expv[nfound[k]])\n                if diff2 < diff:\n                    l = 0 \n                    found[i], found[j] = found[i], nfound[k]\n                    diff2 = diff\n                diff3 = abs(TMv[i] - expv[nfound[k]]) + abs(TMv[j] - expv[found[j]])\n                if diff3 < diff:\n                    l = 0 \n                    found[i], found[j] = nfound[k], found[j]\n                    diff3 = diff\n                diff4 = abs(TMv[i] - expv[found[j]]) + abs(TMv[j] - expv[nfound[k]])\n                if diff4 < diff:\n                    l = 0 \n                    found[i], found[j] = found[j], nfound[k]\n                    diff4 = diff\n                diff5 = abs(TMv[i] - expv[nfound[k]]) + abs(TMv[j] - expv[found[i]])\n                if diff5 < diff:\n                    l = 0 \n                    found[i], found[j] = nfound[k], found[i]\n                    diff5 = diff\n    l += 1\nprint expv[found]\nprint TMv'"
notebooks/MatchNormalModes.py,0,"b'# qnm = q chem normal modes\n# tnm = tensormol normal modes\nimport numpy as np \nqnm = np.load(""adenine_nm.npy"")\ntnm = np.load(""adenine_TM.dat"")\nfound = []\nnfound = []\nfor i in range(tnm.shape[0]):\n    least_diff = float(\'inf\')\n    least_index = None\n    for j in range(qnm.shape[0]):\n        if j in found:\n            continue\n    \tif j not in found:\n        \tdiff = abs(qnm[j] - tnm[i])\n        \tif diff < least_diff:\n            \t\tleast_diff = diff\n            \t\tleast_index = j\n    if least_index != None:\n    \tfound.append(least_index)\nfor i in range(len(qnm)):\n    if i not in found:\n        nfound.append(i)\nprint qnm[found]\nl = 0 \nwhile l<2: \n    for i in range(len(found)):\n        for j in range(i+1, len(found)):\n            for k in range(len(nfound)):\n                diff = abs(tnm[i] - qnm[found[i]]) + abs(tnm[j] - qnm[found[j]])\n                diff1 = abs(tnm[i] - qnm[found[j]]) + abs(tnm[j] - qnm[found[i]])\n                if diff1 < diff:\n                    l = 0 \n                    found[i], found[j] = found[j], found[i]\n                    diff = diff1\n                diff2 = abs(tnm[i] - qnm[found[i]]) + abs(tnm[j] - qnm[nfound[k]])\n                if diff2 < diff:\n                    l = 0 \n                    found[i], found[j] = found[i], nfound[k]\n                    diff2 = diff\n                diff3 = abs(tnm[i] - qnm[nfound[k]]) + abs(tnm[j] - qnm[found[j]])\n                if diff3 < diff:\n                    l = 0 \n                    found[i], found[j] = nfound[k], found[j]\n                    diff3 = diff\n                diff4 = abs(tnm[i] - qnm[found[j]]) + abs(tnm[j] - qnm[nfound[k]])\n                if diff4 < diff:\n                    l = 0 \n                    found[i], found[j] = found[j], nfound[k]\n                    diff4 = diff\n                diff5 = abs(tnm[i] - qnm[nfound[k]]) + abs(tnm[j] - qnm[found[i]])\n                if diff5 < diff:\n                    l = 0 \n                    found[i], found[j] = nfound[k], found[i]\n                    diff5 = diff\n    l += 1'"
samples/SampleMethodsTest.py,3,"b'""""""\nRoutines that will create managers from trained networks,\ndevelop energies and forces, and gather statistics\nfor data tables and figures.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nimport os\nimport numpy as np\nfrom math import *\nfrom random import *\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nPARAMS[""Profiling""] = 0\n\ndef GetEnergyAndForceFromManager(MName_, a):\n\t""""""\n\tMName: name of the manager. (specifies sampling method)\n\tset_: An MSet() dataset which has been loaded.\n\t""""""\n\t# If you wanna do eq.\n\t# a = MSet(""sampling_mols"")\n\t# a.ReadXYZ()\n\t#\n\tTreatedAtoms = a.AtomTypes()\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""AtomizationEnergy"")\n\ttset = TensorMolData_BP_Direct_Linear(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""GradScalar""] = 1\n\tPARAMS[""NeuronType""] = ""relu""\n\tPARAMS[""HiddenLayers""] = [200,200,200]\n\tmanager = TFMolManage(MName_ , tset, False, RandomTData_=False, Trainable_=False)\n\tnmols = len(a.mols)\n\tEErr = np.zeros(nmols)\n\tFErr = np.zeros(nmols)\n\tfor i, mol in enumerate(a.mols):\n\t\tmol.properties[""atomization""] = mol.properties[""energy""]\n\t\tmol.properties[""gradients""] = mol.properties[""forces""]\n\t\tfor j in range(0, mol.NAtoms()):\n\t\t\tmol.properties[""atomization""] -= ele_E_david[mol.atoms[j]]\n\t\tq_en = mol.properties[""atomization""]\n\t\tq_f = mol.properties[""gradients""]\n\t\t#qchem_energies.append(q_en)\n\t\ten,grad=manager.Eval_BPEnergy_Direct_Grad_Linear(Mol(mol.atoms, mol.coords))\n\t\tnew_grad = grad/-JOULEPERHARTREE\n\t\t#print ""new_grad:"", new_grad\n\t\t#print ""q_f:"", q_f\n\t\tEErr[i] = (en - q_en)\n\t\tF_diff = new_grad - q_f\n\t\tFErr[i] = np.sqrt(np.sum(F_diff*F_diff)/mol.NAtoms())\n\t\t#print ""FErr[i]"", FErr[i]\n\tfinal_E_err = (np.sqrt(np.sum(EErr*EErr)/nmols))*KCALPERHARTREE\n\tfinal_F_err = (np.sum(FErr)/nmols)*KCALPERHARTREE\n\tprint(np.sum(FErr,axis=0), nmols)\n\tprint(""RMS energy error: "", (np.sqrt(np.sum(EErr*EErr)/nmols))*KCALPERHARTREE)\n\tprint(""<|F_err|> error: "", (np.sum(FErr)/nmols)*KCALPERHARTREE)\n\treturn final_E_err, final_F_err\n\ndef CompareAllData():\n\t""""""\n\tCompares all errors from every network tested against every dataset.\n\tResults are stored in a dictionary; the keys for the dictionary are\n\tthe managers.\n\t""""""\n\tf = open(""/media/sdb1/dtoth/TensorMol/results/SamplingData.txt"", \'w\')\n\t# print ""File opened""\n\tmanagers = [""Mol_DavidMD_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1"",""Mol_DavidMetaMD_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1"",""Mol_DavidNM_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1"",""Mol_DavidRandom_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1"",""Mol_Hybrid_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1"",""Mol_GoldStd_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1""]\n\tSetNames = [""DavidMD"",""DavidMetaMD"",""DavidNM"",""DavidRandom"", ""Hybrid"",""GoldStd""]\n\tMSets = [MSet(name) for name in SetNames]\n\tfor aset in MSets:\n\t\taset.Load()\n\tresults = {}\n\tprint(""Loaded Sets... "")\n\n\tfor i in managers:\n\t\tfor j in range(len(MSets)):\n\t\t\ten, f = GetEnergyAndForceFromManager(i,MSets[j])\n\t\t\tresults[(i,SetNames[j])] = (en, f)\n\tprint(results)\n\ttry:\n\t\tf.write(results)\n\texcept:\n\t\tf.write(str(results))\n\ndef TestOptimization(MName_):\n\t""""""\n\tDistorts a set of molecules from their equilibrium geometries,\n\tthen optimizes the distorted geometries using a trained network.\n\n\tArgs:\n\n\t\tMName_: Trained network\n\n\tReturns:\n\n\t\tnp.mean(rms_list): Average value of all of the RMS errors for all molecules\n\t""""""\n\n\ta = MSet(""david_test"")\n\t# a.Load()\n\ta.ReadXYZ()\n\t# shuffle(a.mols)\n\tmol = a.mols[-2]\n\tPARAMS[""hidden1""] = 200\n\tPARAMS[""hidden2""] = 200\n\tPARAMS[""hidden3""] = 200\n\tPARAMS[""Hidden layers""] = [200,200,200]\n\tTreatedAtoms = a.AtomTypes()\n\tprint (""TreatedAtoms:"", TreatedAtoms)\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""AtomizationEnergy"")\n\ttset = TensorMolData_BP_Direct_Linear(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager = TFMolManage(MName_ , tset, False, RandomTData_=False, Trainable_=False)\n\trms_list = []\n\t#for mol in a.mols:\n\tEnergyForceField = lambda x: manager.Eval_BPEnergy_Direct_Grad_Linear(Mol(mol.atoms,x))\n\tmol.Distort(0.2)\n\tPARAMS[""OptMaxCycles""] = 2000\n\tmolp = GeomOptimizer(EnergyForceField).Opt(mol)\n\ttmp_rms = mol.rms_inv(molp)\n\trms_list.append(tmp_rms)\n\tprint(""RMS:"", tmp_rms)\n\treturn np.mean(rms_list)\n\ndef GetEnergyVariance(MName_):\n\t""""""\n\tUseful routine to get the data for\n\ta plot of the energy variance within a dataset (MSet).\n\n\tArgs:\n\t\tMName_: Name of dataset as a string\n\t""""""\n\n\ta = MSet(MName_)\n\ta.Load()\n\n\tfor mol in a.mols:\n\t\tmol.properties[""energy""] = mol.properties[""atomization""]\n\n\tens = np.array([mol.properties[""energy""] for m in a.mols])\n\tensmav = ens - np.average(ens)\n\n\tnp.savetxt(MName_, ensmav)\n\n\n# GetEnergyAndForceFromManager(""Mol_DavidMetaMD_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1"", ""Hybrid"")\n# CompareAllData()\nprint(TestOptimization(""Mol_DavidMetaMD_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1""))\n# GetEnergyVariance()\n'"
samples/mill_exp.py,2,"b'from __future__ import absolute_import\n#import memory_util\n#memory_util.vlog(1)\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nfrom TensorMol.MBE.NN_MBE import *\nfrom TensorMol.Interfaces.TMIPIinterface import *\nimport random\n\ndef Prepare():\n\tif (1):\n\t\ta=MSet(""watercube"", center_=False)\n\t\ta.ReadXYZ(""watercube"")\n\t\trepeat = 5\n\t\tspace  = 3.0\n\t\tmol_cout = np.zeros(repeat**3,dtype=int)\n\t\ttm_coords = []\n\t\ttm_atoms = []\n\t\tportion = np.array([1.0,1.0,1.0,1,0, 9.0])/np.sum(np.array([1,1,1,1,9]))\n\t\tprint (""portion:"", portion)\n\t\tfor i in range(0, repeat):\n\t\t\tfor j in range(0, repeat):\n\t\t\t\tfor k in range(0, repeat):\n\t\t\t\t\ts = random.random()\n\t\t\t\t\tprint (""s:"",s)\n\t\t\t\t\tif s < np.sum(portion[:1]):\n\t\t\t\t\t\tindex = 0\n\t\t\t\t\telif s < np.sum(portion[:2]):\n\t\t\t\t\t\tindex = 1\n\t\t\t\t\telif s < np.sum(portion[:3]):\n\t\t\t\t\t\tindex = 2\n\t\t\t\t\telif s < np.sum(portion[:4]):\n\t\t\t\t\t\tindex = 3\n\t\t\t\t\telse:\n\t\t\t\t\t\tindex = 4\n\t\t\t\t\tprint (""index:"",index)\n\t\t\t\t\tm = a.mols[index]\n\t\t\t\t\ttm_coords += list(m.coords+np.asarray([i*space,j*space,k*space]))\n\t\t\t\t\ttm_atoms += list(m.atoms)\n\t\ttm_coords  = np.asarray(tm_coords )\n\t\ttm_atoms = np.asarray(tm_atoms, dtype=int)\n\t\ttm = Mol(tm_atoms, tm_coords)\n\t\ttm.WriteXYZfile(fpath=""./datasets"", fname=""reactor"")\n\tif (0):\n\t\ta=MSet(""watercube"", center_=False)\n\t\ta.ReadXYZ(""watercube"")\n\t\tm = a.mols[0]\n\t\tm.coords = m.coords+1\n\t\trepeat = 4\n\t\tspace = 4.0\n\t\ttm_coords = np.zeros((repeat**3*m.NAtoms(),3))\n\t\ttm_atoms = np.zeros(repeat**3*m.NAtoms(), dtype=int)\n\t\tp = 0\n\t\tfor i in range(0, repeat):\n\t\t\tfor j in range(0, repeat):\n\t\t\t\tfor k in range(0, repeat):\n\t\t\t\t\ttm_coords[p*m.NAtoms():(p+1)*m.NAtoms()]=m.coords+np.asarray([i*space,j*space,k*space])\n\t\t\t\t\ttm_atoms[p*m.NAtoms():(p+1)*m.NAtoms()]=m.atoms\n\t\t\t\t\tp += 1\n\t\ttm = Mol(tm_atoms, tm_coords)\n\t\ttm.WriteXYZfile(fpath=""./datasets"", fname=""watercube"")\n\n\ndef Eval():\n\tif (1):\n\t\ta=MSet(""reactor"", center_=True)\n\t\ta.ReadXYZ(""reactor"")\n\t\tTreatedAtoms = np.array([1,6,7,8], dtype=np.uint8)\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t#PARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""DSFAlpha""] = 0.18*BOHRPERA\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 2\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_chemspider12_clean_maxatom35_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_act_sigmoid100"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\n\t\tm = a.mols[0]\n\t\tprint (""m.coords:"", np.max(m.coords,axis=0), np.min(m.coords,axis=0))\n\t\tm.coords = m.coords - np.min(m.coords,axis=0)\n\t\tprint (""m.coords:"", np.max(m.coords,axis=0), np.min(m.coords,axis=0))\n\t\t#m.coords += np.asarray([1,1,1])\n\t\t#print manager.EvalBPDirectEEUpdateSinglePeriodic(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t#print manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t#return\n\t\t#charge = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[6]\n\t\t#bp_atom = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[2]\n\t\t#for i in range (0, m.NAtoms()):\n\t\t#\tprint i+1, charge[0][i],bp_atom[0][i]\n\n\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tif DoForce:\n\t\t\t\treturn energy, force\n\t\t\telse:\n\t\t\t\treturn energy\n\n\t\tdef GetEnergyForceForMol(m):\n\t\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\t\ttmpm = Mol(m.atoms,x_)\n\t\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\t\tenergy = Etotal[0]\n\t\t\t\tforce = gradient[0]\n\t\t\t\tif DoForce:\n\t\t\t\t\treturn energy, force\n\t\t\t\telse:\n\t\t\t\t\treturn energy\n\t\t\treturn EnAndForce\n\n\t\tdef EnForceCharge(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force, atom_charge[0]\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn atom_charge[0]\n\n\t\tdef EnergyField(x_):\n\t\t\treturn EnAndForce(x_,True)[0]\n\n\t\tdef DipoleField(x_):\n\t\t\tq = np.asarray(ChargeField(x_))\n\t\t\tdipole = np.zeros(3)\n\t\t\tfor i in  range(0, q.shape[0]):\n\t\t\t\tdipole += q[i]*x_[i]\n\t\t\treturn dipole\n\n\t\tdef DFTForceField(x_, DoForce=True):\n\t\t\tif DoForce:\n\t\t\t\treturn QchemDFT(Mol(m.atoms,x_),basis_ = \'6-31g*\',xc_=\'b3lyp\', jobtype_=\'force\', threads=24)\n\t\t\telse:\n\t\t\t\treturn np.asarray([QchemDFT(Mol(m.atoms,x_),basis_ = \'6-31g*\',xc_=\'b3lyp\', jobtype_=\'sp\', threads=24)])[0]\n\t\t#DFTForceField = lambda x: np.asarray([QchemDFT(Mol(m.atoms,x),basis_ = \'6-31g\',xc_=\'b3lyp\', jobtype_=\'sp\', threads=12)])[0]\n\t\tDFTDipoleField = lambda x: QchemDFT(Mol(m.atoms,x),basis_ = \'6-31g\',xc_=\'b3lyp\', jobtype_=\'dipole\', threads=12)\n\t\t#ForceField = lambda x: EnAndForce(x)[-1]\n\t\t#EnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\n\t\t#PARAMS[""OptMaxCycles""]=200\n\t\t#Opt = GeomOptimizer(EnAndForce)\n\t\t#m=Opt.Opt(m)\n\t\t#print (""m.coords:"", m.coords)\n\t\t#return\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDTemp""]= 300.0\n\t\tmeta = BoxedMetaDynamics(EnergyForceField, m, name_=""BoxMetaReactor"", Box_=np.array(18.0*np.eye(3)))\n\t\tmeta.Prop()\n\t\treturn\n\n\tif (0):\n\t\ta=MSet(""watercube"", center_=False)\n\t\ta.ReadXYZ(""watercube"")\n\t\tTreatedAtoms = np.array([1,6,7,8], dtype=np.uint8)\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t#PARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""DSFAlpha""] = 0.18*BOHRPERA\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 2\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_chemspider12_clean_maxatom35_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_act_sigmoid100"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\n\t\tm = a.mols[1]\n\t\tprint (""m.coords:"", np.max(m.coords,axis=0), np.min(m.coords,axis=0))\n\t\tm.coords += np.asarray([0,1,0])\n\t\t#print manager.EvalBPDirectEEUpdateSinglePeriodic(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t#print manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t#return\n\t\t#charge = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[6]\n\t\t#bp_atom = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[2]\n\t\t#for i in range (0, m.NAtoms()):\n\t\t#\tprint i+1, charge[0][i],bp_atom[0][i]\n\n\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tif DoForce:\n\t\t\t\treturn energy, force\n\t\t\telse:\n\t\t\t\treturn energy\n\n\t\tdef GetEnergyForceForMol(m):\n\t\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\t\ttmpm = Mol(m.atoms,x_)\n\t\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\t\tenergy = Etotal[0]\n\t\t\t\tforce = gradient[0]\n\t\t\t\tif DoForce:\n\t\t\t\t\treturn energy, force\n\t\t\t\telse:\n\t\t\t\t\treturn energy\n\t\t\treturn EnAndForce\n\n\t\tdef EnForceCharge(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force, atom_charge[0]\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn atom_charge[0]\n\n\t\tdef EnergyField(x_):\n\t\t\treturn EnAndForce(x_,True)[0]\n\n\t\tdef DipoleField(x_):\n\t\t\tq = np.asarray(ChargeField(x_))\n\t\t\tdipole = np.zeros(3)\n\t\t\tfor i in  range(0, q.shape[0]):\n\t\t\t\tdipole += q[i]*x_[i]\n\t\t\treturn dipole\n\n\t\tdef DFTForceField(x_, DoForce=True):\n\t\t\tif DoForce:\n\t\t\t\treturn QchemDFT(Mol(m.atoms,x_),basis_ = \'6-31g*\',xc_=\'b3lyp\', jobtype_=\'force\', threads=24)\n\t\t\telse:\n\t\t\t\treturn np.asarray([QchemDFT(Mol(m.atoms,x_),basis_ = \'6-31g*\',xc_=\'b3lyp\', jobtype_=\'sp\', threads=24)])[0]\n\t\t#DFTForceField = lambda x: np.asarray([QchemDFT(Mol(m.atoms,x),basis_ = \'6-31g\',xc_=\'b3lyp\', jobtype_=\'sp\', threads=12)])[0]\n\t\tDFTDipoleField = lambda x: QchemDFT(Mol(m.atoms,x),basis_ = \'6-31g\',xc_=\'b3lyp\', jobtype_=\'dipole\', threads=12)\n\t\t#ForceField = lambda x: EnAndForce(x)[-1]\n\t\t#EnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\n\t\t#PARAMS[""OptMaxCycles""]=200\n\t\t#Opt = GeomOptimizer(EnAndForce)\n\t\t#m=Opt.Opt(m)\n\t\t#print (""m.coords:"", m.coords)\n\t\t#return\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDTemp""]= 300.0\n\t\tmeta = BoxedMetaDynamics(EnergyForceField, m, name_=""BoxMetaTest"", Box_=np.array(16.0*np.eye(3)))\n\t\tmeta.Prop()\n\t\treturn\n#Prepare()\nEval()\n'"
samples/test.py,1,"b'""""""\nGenerates artificial data for H_3, learns a potential for it, tests it in optimizations and whatnot.\n""""""\nfrom TensorMol import *\n\ndef GenerateData(model_=""Huckel""):\n\t""""""\n\tGenerate random configurations in a reasonable range.\n\tand calculate their energies and forces.\n\t""""""\n\tnsamp = 10000\n\tcrds = np.random.uniform(4.0,size = (nsamp,3,3))\n\tst = MSet()\n\tMDL = None\n\tnatom = 4\n\tANS = np.array([3,1,1])\n\tif (model_==""Morse""):\n\t\tMDL = MorseModel()\n\telse:\n\t\tMDL = QuantumElectrostatic()\n\tfor s in range(nsamp):\n\t\tif (model_==""Morse""):\n\t\t\tst.mols.append(Mol(np.array([1.,1.,1.]),crds[s]))\n\t\t\ten,f = MDL(crds[s])\n\t\t\tst.mols[-1].properties[""dipole""] = np.array([0.,0.,0.])\n\t\telse:\n\t\t\tst.mols.append(Mol(np.array([3.,1.,1.]),crds[s]))\n\t\t\ten, f, d, q = MDL(crds[s])\n\t\t\tst.mols[-1].properties[""dipole""] = d\n\t\t\tst.mols[-1].properties[""charges""] = q\n\t\tst.mols[-1].properties[""energy""] = en\n\t\tst.mols[-1].properties[""force""] = f\n\t\tst.mols[-1].properties[""gradients""] = -1.0*f\n\t\tst.mols[-1].CalculateAtomization()\n\treturn st\n\ndef TestTraining_John():\n\tPARAMS[""train_dipole""] = True\n\ttset = GenerateData()\n\tnet = BehlerParinelloDirectGauSH(tset)\n\tnet.train()\n\treturn\n\ndef TestTraining():\n\ta = GenerateData()\n\tTreatedAtoms = a.AtomTypes()\n\tPARAMS[""NetNameSuffix""] = ""training_sample""\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 15 # Train for 5 epochs in total\n\tPARAMS[""batch_size""] =  100\n\tPARAMS[""test_freq""] = 5 # Test for every epoch\n\tPARAMS[""tf_prec""] = ""tf.float64"" # double precsion\n\tPARAMS[""EnergyScalar""] = 1.0\n\tPARAMS[""GradScalar""] = 1.0/20.0\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param"" # choose activation function\n\tPARAMS[""sigmoid_alpha""] = 100.0  # activation params\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0] # each layer\'s keep probability for dropout\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""AtomizationEnergy"")\n\ttset = TensorMolData_BP_Direct_EandG_Release(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EandG_SymFunction"")\n\tPARAMS[\'Profiling\']=0\n\tmanager.Train(1)\n\n\ndef TestOpt():\n\treturn\n\ndef TestMD():\n\treturn\n\nTestTraining()\n'"
samples/test_ani1.py,0,"b'from TensorMol import *\nimport cProfile\n\n# John\'s tests\nif (1):\n\tif (0):\n\t\t\t\tmanager= TFMolManage(""Mol_gdb9_energy_1_6_7_8_cleaned_ConnectedBond_Angle_Bond_BP_fc_sqdiff_BP_1"" , None, False)\n\t\ttset = TensorMolData_Bond_BP(MSet(),MolDigester([]),""gdb9_energy_1_6_7_8_cleaned_for_test_ConnectedBond_Angle_Bond_BP"")\n\t\t\t\tmanager.TData = tset\n\t\t\t\tmanager.Test()\n\n\t\tif (0):\n\t\t\t\t# 1 - Get molecules into memory\n\t\t\t\ta=MSet(""gdb9_energy_1_6_7_8_cleaned"")\n\t\t\t\ta.Load()\n\t\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\t\t\tTreatedBonds = list(a.BondTypes())\n\t\t\t\tprint ""TreatedBonds "", TreatedBonds\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\ttset.BuildTrain(""gdb9_energy_1_6_7_8_cleaned"")\n\t\tif (0):\n\t\t\t\ttset = TensorMolData_BP(MSet(),MolDigester([]),""gdb9_energy_1_6_7_8_cleaned_ANI1_Sym"")\n\t\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP"") # Initialzie a manager than manage the training of neural network.\n\t\t\t\tmanager.Train(maxstep=501)\n\tif (0):\n\t\tmanager= TFMolManage(""Mol_gdb9_energy_1_6_7_8_cleaned_ConnectedBond_Angle_Bond_BP_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Continue_Training(maxsteps=901)\n\tif (0):\n\t\t\t\tmanager= TFMolManage(""Mol_gdb9_energy_1_6_7_8_cleaned_ConnectedBond_Angle_Bond_BP_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Test()\n\n\t\tif (0):\n\t\t\t\t# 1 - Get molecules into memory\n\t\t\t\ta=MSet(""gdb9_energy_1_6_7_8_cleaned"")\n\t\t\t\ta.Load()\n\t\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\t\t\tTreatedBonds = list(a.BondTypes())\n\t\t\t\tprint ""TreatedBonds "", TreatedBonds\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Center_Bond_BP"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_Bond_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\ttset.BuildTrain(""gdb9_energy_1_6_7_8_cleaned"")\n\n\tif (0):\n\t\t\t\ttset = TensorMolData_Bond_BP(MSet(),MolDigester([]),""gdb9_energy_1_6_7_8_cleaned_ANI1_Sym_Center_Bond_BP"")\n\t\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP"") # Initialzie a manager than manage the training of neural network.\n\t\t\t\tmanager.Train(maxstep=501)\n\n\n\t\tif (0):\n\t\t\t\t# 1 - Get molecules into memory\n\t\t\t\ta=MSet(""gdb9_energy_1_6_7_8_cleaned"")\n\t\t\t\ta.Load()\n\t\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\t\t\tTreatedBonds = list(a.BondTypes())\n\t\t\t\tprint ""TreatedBonds "", TreatedBonds\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Bond_BP"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_Bond_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\ttset.BuildTrain(""gdb9_energy_1_6_7_8_cleaned"")\n\n\t\tif (0):\n\t\t\t\ttset = TensorMolData_Bond_BP(MSet(),MolDigester([]),""gdb9_energy_1_6_7_8_cleaned_ANI1_Sym_Bond_BP"")\n\t\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP"") # Initialzie a manager than manage the training of neural network.\n\t\t\t\tmanager.Train(maxstep=501)\n\n\tif (0):\n\t\t\t\tmanager= TFMolManage(""Mol_uneq_chemspider_ANI1_Sym_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Continue_Training(maxsteps=1001)\n\t\tif (0):\n\t\t\t\ta = MSet(""CCdihe"")\n\t\t\t\ta.ReadXYZ(""CCdihe"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\ta.Save()\n\t\t\t\ta.Load()\n\t\t\t\tmanager= TFMolManage(""Mol_uneq_chemspider_ANI1_Sym_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Eval_BP(a)\n\n\tif (0):\n\t\tTreatedAtoms = np.asarray([1,6,7,8])\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\ta = MSet()\n\t\t\t\ttset = TensorMolData_BP(a,d, order_=1, num_indis_=1, type_=""mol"")\n\t\ttset.eles = list(TreatedAtoms)\n\t\tmanager= TFMolManage(""Mol_uneq_chemspider_ANI1_Sym_fc_sqdiff_BP_1"" , None, False)\n\t\tmanager.TData = tset\n\t\tmanager.name += ""_noset""\n\t\tmanager.Save()\n\n\tif (0):\n\t\ta = MSet(""gradient_test"")\n\t\t\t\ta.ReadXYZ(""gradient_test"")\n\t\tTreatedAtoms = np.asarray([1,6,7,8])\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym"", OType_=""Energy"")\n\t\ttset = TensorMolData_BP(a,d, order_=1, num_indis_=1, type_=""mol"")\n\t\tfor mol in a.mols:\n\t\t\t\t\tins, grads = tset.dig.EvalDigest(mol, True)\n\n\tif (1):\n\t\ta = MSet(""gradient_test_0"")\n\t\ta.ReadXYZ(""gradient_test_0"")\n\t\ta.Make_Graphs()\n\t\tmanager= TFMolManage(""Mol_uneq_chemspider_ANI1_Sym_fc_sqdiff_BP_1_noset"" , None, False)\n\t\tmanager.Eval_BP(a)\n\n\tif (1):\n\t\ta = MSet(""gradient_test_1"")\n\t\ta.ReadXYZ(""gradient_test_1"")\n\t\ta.Make_Graphs()\n\t\tmanager.Eval_BP(a)\n\n\tif (1):\n\t\ta = MSet(""gradient_test_2"")\n\t\t\t\ta.ReadXYZ(""gradient_test_2"")\n\t\t\t\ta.Make_Graphs()\n\t\tmanager.Eval_BP(a)\n\n\t\ta = MSet(""gradient_test_3"")\n\t\t\t\ta.ReadXYZ(""gradient_test_3"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\tmanager.Eval_BP(a)\n\tif (0):\n\t\t\t\ta = MSet(""gradient_test"")\n\t\t\t\ta.ReadXYZ(""gradient_test"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\tTreatedAtoms = np.asarray([1,6,7,8])\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\tfor mol in a.mols:\n\t\t\t\t\t\tins, grads = tset.dig.EvalDigest(mol, True)\n\t\t\tprint ins, grads, grads.shape\n\n\tif (0):\n\t\t\t\ta = MSet(""xave"")\n\t\t\t\ta.ReadXYZ(""xave"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\ta.Save()\n\t\t\t\ta.Load()\n\t\t\t\tmanager= TFMolManage(""Mol_gdb9_energy_1_6_7_8_cleaned_ConnectedBond_Angle_Bond_BP_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Eval_Bond_BP(a)\n\tif (0):\n\t\ta = MSet(""ANI1_SYM_test"")\n\t\ta.ReadXYZ(""ANI1_SYM_test"")\n\t\t\t\ta.Make_Graphs()\n\t\tTreatedAtoms = np.asarray([1,6,7,8])\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Center_Bond_BP"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t#tset.BuildTrain(""SNB_bondstrength"")\n\t\tfor mol in a.mols:\n\t\t\tprint mol.bonds\n\t\t\t\t\t\tins = tset.dig.EvalDigest(mol)\n\t\t\tprint ins\n\t\t\tname = mol.name.split()[1]\n\t\t\tnp.savetxt(name+""_center.txt"" , ins)\n\n\tif (0):\n\t\t\t\ta = MSet(""linear_mol"")\n\t\t\t\ta.ReadXYZ(""linear_mol"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\tTreatedAtoms = np.asarray([1,6,7,8])\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\t#tset.BuildTrain(""SNB_bondstrength"")\n\t\t\t\tfor mol in a.mols:\n\t\t\t\t\t\tins = tset.dig.EvalDigest(mol)\n\t\t\tprint ""ins:"", ins\n\t\t\t\t\t\t#name = mol.name.split()[1]\n\t\t\t\t\t\t#np.savetxt(name+"".txt"" , ins)\n\n\tif (0):\n\t\t\t\ta = MSet(""uneq_chemspider"")\n\t\t#a.ReadXYZ(""uneq_chemspider"")\n\t\ta.Load()\n\t\t\t\t#a.Make_Graphs()\n\t\t#a.Save()\n\t\tb = MSet(""chemspider_9heavy_tmcleaned_opt_allowedbond_for_freq_compare"")\n\t\tb.Load()\n\t\t#b.ReadXYZ(""chemspider_9heavy_tmcleaned_opt_allowedbond_for_freq_compare"")\n\t\t#b.Make_Graphs()\n\t\t#b.Save()\n\t\timport re\n\t\tfor a_mol_index, a_mol in enumerate(a.mols):\n\t\t\t#dist_mat = MolEmb.Make_DistMat(a.mols[a_mol_index].coords)\n\t\t\tindex = a.mols[a_mol_index].name.split()[2]\n\t\t\t#b_mol_index = a_mol_index / 50\n\t\t\tmatch = re.search(r\'(\\d+)_\\d+\', index)\n\t\t\tb_mol_index = int(match.group(1))\n\t\t\tprint a_mol_index, b_mol_index\n\t\t\tif not np.array_equal(a.mols[a_mol_index].atoms, b.mols[b_mol_index].atoms):\n\t\t\t\tprint ""a mol:"", a.mols[a_mol_index].atoms\n\t\t\t\tprint ""b.mol:"", b.mols[b_mol_index].atoms\n\t\t\t\traise Exception(""not the same mol!"")\n\t\t\ta.mols[a_mol_index].bonds = b.mols[b_mol_index].bonds\n\t\t\tfor bond_index in range (0, a.mols[a_mol_index].bonds.shape[0]):\n\t\t\t\ta.mols[a_mol_index].bonds[bond_index][1] = a.mols[a_mol_index].DistMatrix[int(a.mols[a_mol_index].bonds[bond_index][2])][int(a.mols[a_mol_index].bonds[bond_index][3])]\n\t\t\t#print a.mols[a_mol_index].bonds\n\t\ta.Save()\n\tif (0):\n\t\t\t\ta = MSet(""uneq_chemspider"")\n\t\t\t\t#a.ReadXYZ(""uneq_chemspider"")\n\t\t\t\ta.Load()\n\t\tfor mol in a.mols:\n\t\t\tprint mol.bonds\n\n\t\tif (0):\n\t\t\t\t# 1 - Get molecules into memory\n\t\t\t\ta=MSet(""uneq_chemspider"")\n\t\t\t\ta.Load()\n\t\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\t\t\tTreatedBonds = list(a.BondTypes())\n\t\t\t\tprint ""TreatedBonds "", TreatedBonds\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\ttset.BuildTrain(""uneq_chemspider"")\n\n\tif (0):\n\t\t\t\ttset = TensorMolData_BP(MSet(),MolDigester([]),""uneq_chemspider_ANI1_Sym"")\n\t\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP"") # Initialzie a manager than manage the training of neural network.\n\t\t\t\tmanager.Train(maxstep=501)\n\n\n\tif (0):\n\t\t\t\t# 1 - Get molecules into memory\n\t\t\t\ta=MSet(""uneq_chemspider"")\n\t\t\t\ta.Load()\n\t\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\t\t\tTreatedBonds = list(a.BondTypes())\n\t\t\t\tprint ""TreatedBonds "", TreatedBonds\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Center_Bond_BP"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_Bond_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\ttset.BuildTrain(""uneq_chemspider"")\n\n\t\tif (0):\n\t\t\t\ttset = TensorMolData_Bond_BP(MSet(),MolDigester([]),""uneq_chemspider_ANI1_Sym_Center_Bond_BP"")\n\t\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP"") # Initialzie a manager than manage the training of neural network.\n\t\t\t\tmanager.Train(maxstep=501)\n\n\n\tif (0):\n\t\t\t\ta = MSet(""CH_bondstrength"")\n\t\t\t\ta.ReadXYZ(""CH_bondstrength"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\ta.Save()\n\t\t\t\ta.Load()\n\t\t\t\tmanager= TFMolManage(""Mol_gdb9_energy_1_6_7_8_cleaned_ConnectedBond_Angle_Bond_BP_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Eval_Bond_BP(a, True)\n\n\tif (0):\n\t\t\t\ta = MSet(""aminoacids"")\n\t\t\t\ta.ReadXYZ(""aminoacids"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\ta.Save()\n\t\t\t\ta.Load()\n\t\t\t\tmanager= TFMolManage(""Mol_gdb9_energy_1_6_7_8_cleaned_ConnectedBond_Angle_Bond_BP_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Eval_Bond_BP(a, True)\n\n\tif (0):\n\t\t\t\ta = MSet(""1_1_Ostrech"")\n\t\t\t\ta.ReadXYZ(""1_1_Ostrech"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\ta.Save()\n\t\t\t\ta.Load()\n\t\t\t\tmanager= TFMolManage(""Mol_gdb9_energy_1_6_7_8_cleaned_ConnectedBond_Angle_Bond_BP_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Eval_Bond_BP(a)\n\tif (0):\n\t\ta = MSet(""SNB_bondstrength"")\n\t\t\t\ta.ReadXYZ(""SNB_bondstrength"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\ta.Save()\n\t\t\t\ta.Load()\n\t\t#for mol in a.mols:\n\t\t#\tmol.Find_Bond_Index()\n\t\t#\tmol.Define_Conjugation()\n\t\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ConnectedBond_Angle_CM_Bond_BP"", OType_=""Atomization"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_Bond_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\ttset.BuildTrain(""SNB_bondstrength"")\n\n\n\tif (0):\n\t\tb = MSet(""ethy"")\n\t\t\t\tb.ReadXYZ(""ethygroup"")\n\t\t\t\tb.Make_Graphs()\n\t\tb.Bonds_Between_All()\n\t\ta=MSet(""gdb9_energy_1_6_7_8_cleaned"")\n\t\t\t\ta.Load()\n\t\ta.AppendSet(b)\n\t\ta.Save()\n\tif (0):\n\t\t\t\t# 1 - Get molecules into memory\n\t\t\t\ta=MSet(""gdb9_energy_1_6_7_8_cleaned_ethy"")\n\t\t\t\ta.Load()\n\t\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\t\tprint ""TreatedAtoms "", TreatedAtoms\n\t\t\t\tTreatedBonds = list(a.BondTypes())\n\t\t\t\tprint ""TreatedBonds "", TreatedBonds\n\t\t\t\td = MolDigester(TreatedAtoms, name_=""ConnectedBond_Angle_Bond_BP"", OType_=""Energy"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t\t\ttset = TensorMolData_Bond_BP(a,d, order_=1, num_indis_=1, type_=""mol"") # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t\t\ttset.BuildTrain(""gdb9_energy_1_6_7_8_cleaned_ethy"")\n\tif (0):\n\t\t\t\ttset = TensorMolData_Bond_BP(MSet(),MolDigester([]),""gdb9_energy_1_6_7_8_cleaned_ethy_ConnectedBond_Angle_Bond_BP"")\n\t\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP"") # Initialzie a manager than manage the training of neural network.\n\t\t\t\tmanager.Train(maxstep=501)\n\tif (0):\n\t\t\t\ta = MSet(""SNB_bondstrength"")\n\t\t\t\ta.ReadXYZ(""SNB_bondstrength"")\n\t\t\t\ta.Make_Graphs()\n\t\t\t\ta.Save()\n\t\t\t\ta.Load()\n\t\t\t\tmanager= TFMolManage(""Mol_gdb9_energy_1_6_7_8_cleaned_ethy_ConnectedBond_Angle_Bond_BP_fc_sqdiff_BP_1"" , None, False)\n\t\t\t\tmanager.Eval_Bond_BP(a, True)\n'"
samples/test_chemspider12.py,9,"b'\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nfrom TensorMol.MBE.NN_MBE import *\n\n\ndef TrainPrepare():\n\tif (0):\n\t\tWB97XDAtom={}\n\t\tWB97XDAtom[1]=-0.5026682866\n\t\tWB97XDAtom[6]=-37.8387398698\n\t\tWB97XDAtom[7]=-54.5806161811\n\t\tWB97XDAtom[8]=-75.0586028656\n\t\ta = MSet(""chemspider12_clean"")\n\t\tdic_list = pickle.load(open(""./datasets/chemspider12_wb97xd_goodones.dat"", ""rb""))\n\t\tfor mol_index, dic in enumerate(dic_list):\n\t\t\tatoms = []\n\t\t\tprint (""mol_index:"", mol_index)\n\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\tmol.properties[\'charges\'] = dic[\'charges\']\n\t\t\tmol.properties[\'dipole\'] = np.asarray(dic[\'dipole\'])\n\t\t\tmol.properties[\'quadropole\'] = dic[\'quad\']\n\t\t\tmol.properties[\'energy\'] = dic[\'scf_energy\']\n\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\tmol.properties[\'atomization\'] = dic[\'scf_energy\']\n\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\tmol.properties[\'atomization\'] -= WB97XDAtom[mol.atoms[i]]\n\t\t\ta.mols.append(mol)\n\t\ta.mols[100].WriteXYZfile(fname=""chemspider12_test"")\n\t\tprint(a.mols[100].properties)\n\t\ta.Save()\n\n\tif (0):\n\t\ta = MSet(""chemspider12_clean"")\n\t\ta.Load()\n\t\tb = MSet(""chemspider12_clean_maxatom35"")\n\t\thist = np.zeros((10))\n\t\tfor mol in a.mols:\n\t\t\tif mol.NAtoms() <= 35:\n\t\t\t\tb.mols.append(mol)\n\t\tb.Save()\n\n\tif (0):\n \t\ta = MSet(""chemspider12_clean_maxatom35"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\n\t\tb = MSet(""chemspider12_clean_maxatom35_mini"")\n\t\tfor i in range(0, int(0.01*len(a.mols))):\n\t\t\tb.mols.append(a.mols[i])\n\t\tb.Save()\n\n\t\tc = MSet(""chemspider12_clean_maxatom35_small"")\n\t\tfor i in range(0, int(0.05*len(a.mols))):\n\t\t\tc.mols.append(a.mols[i])\n\t\tc.Save()\n\n#H2O_wbxd_1to21_with_prontonated_with_ch4.dat\n\tif (1):\n\t\tWB97XDAtom={}\n\t\tWB97XDAtom[1]=-0.5026682866\n\t\tWB97XDAtom[6]=-37.8387398698\n\t\tWB97XDAtom[7]=-54.5806161811\n\t\tWB97XDAtom[8]=-75.0586028656\n\t\ta = MSet(""chemspider12_maxatom35_H2O_with_CH4"")\n\t\tdic_list = pickle.load(open(""./datasets/H2O_wbxd_1to21_with_prontonated_with_ch4.dat"", ""rb""))\n\t\tfor mol_index, dic in enumerate(dic_list):\n\t\t\tatoms = []\n\t\t\tprint (""mol_index:"", mol_index)\n\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\tif mol.NAtoms() <= 35:\n\t\t\t\tmol.properties[\'dipole\'] = np.asarray(dic[\'dipole\'])\n\t\t\t\tmol.properties[\'energy\'] = dic[\'scf_energy\']\n\t\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\t\tmol.properties[\'atomization\'] = dic[\'scf_energy\']\n\t\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\t\tmol.properties[\'atomization\'] -= WB97XDAtom[mol.atoms[i]]\n\t\t\t\t\ta.mols.append(mol)\n\t\tb = MSet(""chemspider12_clean_maxatom35"")\n\t\tb.Load()\n\t\tprint (""nmol in water/ch4:"", len(a.mols))\n\t\tprint (""nmol in chemspider12:"", len(b.mols))\n\t\trandom.shuffle(b.mols)\n\t\tfor i in range(0, len(b.mols)-len(a.mols)):\n\t\t\ta.mols.append(b.mols[i])\n\t\tprint (""nmol in  water/ch4+chemspider12:"", len(b.mols))\n\t\ta.Save()\n\ndef Train():\n\tif (0):\n\t\ta = MSet(""chemspider12_clean_maxatom35"")\n\t\ta.Load()\n\t\t#random.shuffle(a.mols)\n\t\t#for i in range(150000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  60   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/30.0\n\t\tPARAMS[""DipoleScalar""]= 1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t#PARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 1\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\n\tif (0):\n\t\ta = MSet(""chemspider12_clean_maxatom35"")\n\t\ta.Load()\n\t\t#random.shuffle(a.mols)\n\t\t#for i in range(150000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  60   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/30.0\n\t\tPARAMS[""DipoleScalar""]= 1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t#PARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 1\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\n\tif (0):\n\t\ta = MSet(""chemspider12_clean_maxatom35"")\n\t\ta.Load()\n\t\t#random.shuffle(a.mols)\n\t\t#for i in range(150000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  60   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 1\n\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+usual, dropout07+sigmoid100\n\t\ta = MSet(""chemspider12_clean_maxatom35"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 2\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+usual, dropout07+sigmoid100+rightalpha\n\t\ta = MSet(""chemspider12_clean_maxatom35"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100_rightalpha""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 2\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n#chemspider12_maxatom35_H2O_with_CH4\n\n\tif (1): # Normalize+Dropout+500+usual, dropout07+sigmoid100+rightalpha\n\t\ta = MSet(""chemspider12_maxatom35_H2O_with_CH4"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100_rightalpha""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 2\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+usual, dropout07+sigmoid100+nodropout\n\t\ta = MSet(""chemspider12_clean_maxatom35"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100_nodropout""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 2\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\tif (0):\n\t\t#a = MSet(""chemspider12_clean_mini"")\n\t\ta = MSet(""chemspider12_clean_maxatom35_small"")\n\t\ta.Load()\n\t\t#random.shuffle(a.mols)\n\t\t#for i in range(150000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  60   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 2\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/30.0\n\t\tPARAMS[""DipoleScalar""]= 1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""AddEcc""] = False\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 20\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\ndef Eval():\n\tif (1):\n\t\t#a=MSet(""aspirin"", center_=False)\n\t\t#a.ReadXYZ(""aspirin"")\n\t\t#a=MSet(""chemspider_IR_test_mol"", center_=False)\n\t\t#a.ReadXYZ(""chemspider_IR_test_mol"")\n\t\ta=MSet(""decalin_reaction"", center_=False)\n\t\ta.ReadXYZ(""decalin_reaction"")\n\t\t#a=MSet(""IR_debug"", center_=False)\n\t\t#a.ReadXYZ(""IR_debug"")\n\t\tTreatedAtoms = np.array([1,6,7,8], dtype=np.uint8)\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t#PARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""DSFAlpha""] = 0.18*BOHRPERA\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 2\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_chemspider12_clean_maxatom35_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_act_sigmoid100"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\n\t\tm = a.mols[0]\n\t\teq_coords = m.coords.copy()\n\t\t#print manager.EvalBPDirectEEUpdateSinglePeriodic(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t#print manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t#return\n\t\t#charge = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[6]\n\t\t#bp_atom = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[2]\n\t\t#for i in range (0, m.NAtoms()):\n\t\t#\tprint i+1, charge[0][i],bp_atom[0][i]\n\n\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tif DoForce:\n\t\t\t\treturn energy, force\n\t\t\telse:\n\t\t\t\treturn energy\n\n\t\tdef GetEnergyForceForMol(m):\n\t\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\t\ttmpm = Mol(m.atoms,x_)\n\t\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\t\tenergy = Etotal[0]\n\t\t\t\tforce = gradient[0]\n\t\t\t\tif DoForce:\n\t\t\t\t\treturn energy, force\n\t\t\t\telse:\n\t\t\t\t\treturn energy\n\t\t\treturn EnAndForce\n\n\t\tdef EnForceCharge(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force, atom_charge[0]\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn atom_charge[0]\n\n\t\tdef EnergyField(x_):\n\t\t\treturn EnAndForce(x_,True)[0]\n\n\t\tdef DipoleField(x_):\n\t\t\t#q = np.asarray([-0.045124, -0.077051, -0.103131, -0.086186, -0.325407, -0.181912,  0.206067, -0.339910, -0.318228, -0.345008,  0.453050,  0.299429, -0.345770,  0.269782,  0.136701,  0.118730,  0.118365,  0.119473,  0.150804,  0.149447,  0.145880])\n\t\t\tq = np.asarray(ChargeField(x_))\n\t\t\t#q = ChargeField(eq_coords)\n\t\t\t#dipole = Dipole(x_, q)\n\t\t\t#dipole = np.einsum(""ax,a..."", x_ , q)\n\t\t\tdipole = np.zeros(3)\n\t\t\tfor i in  range(0, q.shape[0]):\n\t\t\t\tdipole += q[i]*x_[i]\n\t\t\treturn dipole\n\n\t\tDFTForceField = lambda x: np.asarray([QchemDFT(Mol(m.atoms,x),basis_ = \'6-31g\',xc_=\'b3lyp\', jobtype_=\'sp\', threads=12)])[0]\n\t\tDFTDipoleField = lambda x: QchemDFT(Mol(m.atoms,x),basis_ = \'6-31g\',xc_=\'b3lyp\', jobtype_=\'dipole\', threads=12)\n\t\t#ForceField = lambda x: EnAndForce(x)[-1]\n\t\t#EnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\tPARAMS[""OptMaxCycles""]=1\n\t\tOpt = GeomOptimizer(EnAndForce)\n\t\tm=Opt.Opt(a.mols[1])\n\t\t#return\n\t\t##return\n \t\t#masses = np.array(list(map(lambda x: ATOMICMASSESAMU[x-1],m.atoms)))\n\t\t#w,v = HarmonicSpectra(DFTForceField, m.coords, m.atoms, WriteNM_=False)\n\t\t#w,v = HarmonicSpectra(DFTForceField, m.coords, m.atoms,  WriteNM_=True, Mu_ = DFTDipoleField)\n\t\t#w,v = HarmonicSpectra(EnergyField, m.coords, m.atoms, WriteNM_=True, Mu_ = DFTDipoleField)\n\t\t#w,v = HarmonicSpectra(EnergyField, m.coords, m.atoms, WriteNM_=True, Mu_ = DipoleField)\n\t\t#return\n\n\n\t\t#PARAMS[""OptMaxCycles""]=200\n\t\t#Opt = GeomOptimizer(EnAndForce)\n\t\t#a.mols[0] = Opt.Opt(a.mols[0],""1"")\n\t\t#a.mols[1] = Opt.Opt(a.mols[1],""2"")\n\t\t#a.mols[-2], a.mols[-1] = a.mols[-2].AlignAtoms(a.mols[-1])\n\t\tPARAMS[""OptMaxCycles""]=20000\n\t\tPARAMS[""NebSolver""]=""SD""\n\t\tPARAMS[""NebNumBeads""] = 21\n\t\tPARAMS[""MaxBFGS""] = 12\n\t\tPARAMS[""NebK""] = 0.2\n\t\tneb = NudgedElasticBand(EnAndForce,a.mols[-2],a.mols[-1])\n\t\tBeads = neb.Opt()\n\t\treturn\n\t\t##m.coords[0] = m.coords[0] + 0.1\n                #PARAMS[""MDThermostat""] = ""Nose""\n                #PARAMS[""MDTemp""] = 300\n                #PARAMS[""MDdt""] = 0.1\n                #PARAMS[""RemoveInvariant""]=True\n                #PARAMS[""MDV0""] = None\n                #PARAMS[""MDMaxStep""] = 10000\n                #md = VelocityVerlet(None, m, ""water_trimer_nothermo"",EnergyForceField)\n                #md.Prop()\n\t\t##return\n\n\t\t##PARAMS[""OptMaxCycles""]=1000\n\t\t##Opt = GeomOptimizer(EnergyForceField)\n\t\t##m=Opt.Opt(m)\n\n\t\t#PARAMS[""MDdt""] = 0.2\n\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t#PARAMS[""MDMaxStep""] = 2000\n\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t#PARAMS[""MDV0""] = None\n\t\t#PARAMS[""MDAnnealTF""] = 1.0\n\t\t#PARAMS[""MDAnnealT0""] = 300.0\n\t\t#PARAMS[""MDAnnealSteps""] = 1000\n\t\t#anneal = Annealer(EnergyForceField, None, m, ""AnnealAspirin"")\n\t\t#anneal.Prop()\n\t\t#m.coords = anneal.Minx.copy()\n\t\t#m.WriteXYZfile(""./results/"", ""Anneal_opt_dropout_sigmoid100_10water"")\n\t\t#return\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 300.0\n\t\tPARAMS[""MDAnnealT0""] = 0.1\n\t\tPARAMS[""MDAnnealSteps""] = 10000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""WarmAspirin"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.x.copy()\n\t\t#m.WriteXYZfile(""./results/"", ""warm"")\n\t\tPARAMS[""MDThermostat""] = None\n\t\tPARAMS[""MDTemp""] = 0\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 40000\n\t\tmd = IRTrajectory(EnergyForceField, ChargeField, m, ""Aspirin_IR_300K_sigma100_wrongalpha"", anneal.v)\n\t\tmd.Prop()\n\t\tWriteDerDipoleCorrelationFunction(md.mu_his)\n\t\treturn\n\n#TrainPrepare()\n#Train()\nEval()\n'"
samples/test_chemspider9.py,13,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nfrom TensorMol.MBE.NN_MBE import *\n\n\ndef TrainPrepare():\n\n\tif (0):\n\t\ta = MSet(""chemspider9_force"")\n\t\tdic_list = pickle.load(open(""./datasets/chemspider9_force.dat"", ""rb""))\n\t\tfor dic in dic_list:\n\t\t\tatoms = []\n\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\tmol.properties[\'charges\'] = dic[\'charges\']\n\t\t\tmol.properties[\'dipole\'] = dic[\'dipole\']\n\t\t\tmol.properties[\'quadropole\'] = dic[\'quad\']\n\t\t\tmol.properties[\'energy\'] = dic[\'scf_energy\']\n\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\tmol.CalculateAtomization()\n\t\t\ta.mols.append(mol)\n\t\ta.Save()\n\n\n\tif (1):\n\t\tB3LYP631GstarAtom={}\n\t\tB3LYP631GstarAtom[1]=-0.5002727827\n\t\tB3LYP631GstarAtom[6]=-37.8462793509\n\t\tB3LYP631GstarAtom[7]=-54.5844908554\n\t\tB3LYP631GstarAtom[8]=-75.0606111011\n\t\ta = MSet(""chemspider9_metady_force"")\n\t\tdic_list = pickle.load(open(""./datasets/chemspider9_metady_force.dat"", ""rb""))\n\t\tfor dic in dic_list:\n\t\t\tatoms = []\n\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\tmol.properties[\'charges\'] = dic[\'charges\']\n\t\t\tmol.properties[\'dipole\'] = np.asarray(dic[\'dipole\'])\n\t\t\tmol.properties[\'quadropole\'] = dic[\'quad\']\n\t\t\tmol.properties[\'energy\'] = dic[\'scf_energy\']\n\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\tmol.properties[\'atomization\'] = dic[\'scf_energy\']\n\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\tmol.properties[\'atomization\'] -= B3LYP631GstarAtom[mol.atoms[i]]\n\t\t\t\ta.mols.append(mol)\n\t\ta.mols[100].WriteXYZfile(fname=""metady_test"")\n\t\tprint(a.mols[100].properties)\n\t\ta.Save()\n\n\n\t\tif (0):\n\t\t\tRIMP2Atom={}\n\t\t\tRIMP2Atom[1]=-0.4998098112\n\t\t\tRIMP2Atom[8]=-74.9659650581\n\t\t\ta = MSet(""H2O_augmented_more_cutoff5_rimp2_force_dipole"")\n\t\t\tdic_list = pickle.load(open(""./datasets/H2O_augmented_more_cutoff5_rimp2_force_dipole.dat"", ""rb""))\n\t\t\tfor dic in dic_list:\n\t\t\t\tatoms = []\n\t\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\t\tmol.properties[\'mul_charges\'] = dic[\'mul_charges\']\n\t\t\t\tmol.properties[\'dipole\'] = dic[\'dipole\']\n\t\t\t\tmol.properties[\'scf_dipole\'] = dic[\'scf_dipole\']\n\t\t\t\tmol.properties[\'energy\'] = dic[\'energy\']\n\t\t\t\tmol.properties[\'scf_energy\'] = dic[\'scf_energy\']\n\t\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\t\tmol.properties[\'atomization\'] = dic[\'energy\']\n\t\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\t\tmol.properties[\'atomization\'] -= RIMP2Atom[mol.atoms[i]]\n\t\t\t\t\ta.mols.append(mol)\n\t\t\ta.Save()\n\n\tif (1):\n\t\tRIMP2Atom={}\n\t\tRIMP2Atom[1]=-0.4998098112\n\t\tRIMP2Atom[8]=-74.9659650581\n\t\ta = MSet(""H2O_augmented_more_bowl02_rimp2_force_dipole"")\n\t\tdic_list_1 = pickle.load(open(""./datasets/H2O_augmented_more_cutoff5_rimp2_force_dipole.dat"", ""rb""))\n\t\tdic_list_2 = pickle.load(open(""./datasets/H2O_long_dist_pair.dat"", ""rb""))\n\t\tdic_list_3 = pickle.load(open(""./datasets/H2O_metady_bowl02.dat"", ""rb""))\n\t\tdic_list = dic_list_1 + dic_list_2 + dic_list_3\n\t\trandom.shuffle(dic_list)\n\t\tfor dic in dic_list:\n\t\t\t\tatoms = []\n\t\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\t\tmol.properties[\'mul_charges\'] = dic[\'mul_charges\']\n\t\t\t\tmol.properties[\'dipole\'] = dic[\'dipole\']\n\t\t\t\tmol.properties[\'scf_dipole\'] = dic[\'scf_dipole\']\n\t\t\t\tmol.properties[\'energy\'] = dic[\'energy\']\n\t\t\t\tmol.properties[\'scf_energy\'] = dic[\'scf_energy\']\n\t\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\t\tmol.properties[\'atomization\'] = dic[\'energy\']\n\t\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\t\tmol.properties[\'atomization\'] -= RIMP2Atom[mol.atoms[i]]\n\t\t\t\ta.mols.append(mol)\n\n\t\tb = MSet(""H2O_bowl02_rimp2_force_dipole"")\n\t\tfor dic in dic_list_3:\n\t\t\tatoms = []\n\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\tmol.properties[\'mul_charges\'] = dic[\'mul_charges\']\n\t\t\tmol.properties[\'dipole\'] = dic[\'dipole\']\n\t\t\tmol.properties[\'scf_dipole\'] = dic[\'scf_dipole\']\n\t\t\tmol.properties[\'energy\'] = dic[\'energy\']\n\t\t\tmol.properties[\'scf_energy\'] = dic[\'scf_energy\']\n\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\tmol.properties[\'atomization\'] = dic[\'energy\']\n\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\tmol.properties[\'atomization\'] -= RIMP2Atom[mol.atoms[i]]\n\t\t\tb.mols.append(mol)\n\t\tprint(""number of a mols:"", len(a.mols))\n\t\tprint(""number of b mols:"", len(b.mols))\n\t\ta.Save()\n\t\tb.Save()\n\n\n\tif (0):\n\t\ta = MSet(""chemspider9_force"")\n\t\ta.Load()\n\t\trmsgrad = np.zeros((len(a.mols)))\n\t\tfor i, mol in enumerate(a.mols):\n\t\t\trmsgrad[i] = (np.sum(np.square(mol.properties[\'gradients\'])))**0.5\n\t\tmeangrad = np.mean(rmsgrad)\n\t\tprint(""mean:"", meangrad, ""std:"", np.std(rmsgrad))\n\t\tnp.savetxt(""chemspider9_force_dist.dat"", rmsgrad)\n\t\tfor i, mol in enumerate(a.mols):\n\t\t\trmsgrad = (np.sum(np.square(mol.properties[\'gradients\'])))**0.5\n\t\t\tif 2 > rmsgrad > 1.5:\n\t\t\t\tmol.WriteXYZfile(fname=""large_force"")\n\t\t\t\tprint(rmsgrad)\n\n\tif (0):\n\t\ta = MSet(""chemspider9_force"")\n\t\ta.Load()\n\t\tb = MSet(""chemspider9_force_cleaned"")\n\t\tfor i, mol in enumerate(a.mols):\n\t\t\trmsgrad = (np.sum(np.square(mol.properties[\'gradients\'])))**0.5\n\t\t\tif rmsgrad <= 1.5:\n\t\t\t\tb.mols.append(mol)\n\t\tb.Save()\n\t\tc = MSet(""chemspider9_force_cleaned_debug"")\n\t\tc.mols = b.mols[:1000]\n\t\tc.Save()\n\n\tif (0):\n\t\ta = MSet(""chemspider9_force"")\n\t\ta.Load()\n\t\tprint(a.mols[0].properties)\n\t\ta.mols[0].WriteXYZfile(fname=""test"")\n\ndef TrainForceField():\n\tif (0):\n\t\ta = MSet(""chemspider9_force_cleaned"")\n\t\ta.Load()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""hidden1""] = 1000\n\t\tPARAMS[""hidden2""] = 1000\n\t\tPARAMS[""hidden3""] = 1000\n\t\tPARAMS[""learning_rate""] = 0.001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] = 28\n\t\tPARAMS[""test_freq""] = 2\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 0.05\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE"") # Initialzie a manager than manage the training of neural network.\n\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\tmanager.Train(maxstep=101)\n\n\n\t\tif (0):\n\t\t\ta = MSet(""H2O_augmented_more_cutoff5_rimp2_force_dipole"")\n\t\t\ta.Load()\n\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\tPARAMS[""learning_rate""] = 0.00001\n\t\t\tPARAMS[""momentum""] = 0.95\n\t\t\tPARAMS[""max_steps""] = 1101\n\t\t\tPARAMS[""batch_size""] = 1000\n\t\t\tPARAMS[""test_freq""] = 10\n\t\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\t\tPARAMS[""GradScaler""] = 1.0\n\t\t\tPARAMS[""DipoleScaler""]=1.0\n\t\t\tPARAMS[""NeuronType""] = ""relu""\n\t\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\t\tPARAMS[""EECutoff""] = 15.0\n\t\t\tPARAMS[""EECutoffOn""] = 4.0\n\t\t\tPARAMS[""Erf_Width""] = 0.2\n\t\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\t\tPARAMS[""SwitchEpoch""] = 100\n\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE"") # Initialzie a manager than manage the training of neural network.\n\t\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\t\tmanager.Train()\n\n\tif (0):\n\t\ta = MSet(""H2O_augmented_more_cutoff5_rimp2_force_dipole"")\n\t\ta.Load()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.0001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 2\n\t\tPARAMS[""batch_size""] = 1000\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 4.4\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\tmanager=TFMolManage(""Mol_H2O_augmented_more_cutoff5_rimp2_force_dipole_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_1"",tset,False,""fc_sqdiff_BP_Direct_EE"") # Initialzie a manager than manage the training of neural network.\n\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\tmanager.Continue_Training(target=""All"")\n\n\t#New radius\n\n\t\tif (0):\n\t\t\ta = MSet(""H2O_augmented_more_cutoff5_rimp2_force_dipole"")\n\t\t\ta.Load()\n\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\tPARAMS[""learning_rate""] = 0.00001\n\t\t\tPARAMS[""momentum""] = 0.95\n\t\t\tPARAMS[""max_steps""] = 901\n\t\t\tPARAMS[""batch_size""] = 1000\n\t\t\tPARAMS[""test_freq""] = 10\n\t\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\t\tPARAMS[""GradScaler""] = 1.0\n\t\t\tPARAMS[""DipoleScaler""]=1.0\n\t\t\tPARAMS[""NeuronType""] = ""relu""\n\t\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\t\tPARAMS[""EECutoff""] = 15.0\n\t\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\t\tPARAMS[""AN1_r_Rc""] = 8.0\n\t\t\tPARAMS[""AN1_num_r_Rs""] = 64\n\t\t\tPARAMS[""Erf_Width""] = 0.4\n\t\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\t\tPARAMS[""SwitchEpoch""] = 100\n\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE"") # Initialzie a manager than manage the training of neural network.\n\t\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\t\tmanager.Train()\n\n\t#New radius: 5 A\n\t\tif (1):\n\t\t\ta = MSet(""H2O_augmented_more_cutoff5_rimp2_force_dipole"")\n\t\t\ta.Load()\n\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\tPARAMS[""learning_rate""] = 0.00001\n\t\t\tPARAMS[""momentum""] = 0.95\n\t\t\tPARAMS[""max_steps""] = 901\n\t\t\tPARAMS[""batch_size""] = 1000\n\t\t\tPARAMS[""test_freq""] = 10\n\t\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\t\tPARAMS[""GradScaler""] = 1.0\n\t\t\tPARAMS[""DipoleScaler""]=1.0\n\t\t\tPARAMS[""NeuronType""] = ""relu""\n\t\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\t\tPARAMS[""EECutoff""] = 15.0\n\t\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\t\tPARAMS[""AN1_r_Rc""] = 5.0\n\t\t\tPARAMS[""Erf_Width""] = 0.4\n\t\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\t\tPARAMS[""SwitchEpoch""] = 100\n\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE"") # Initialzie a manager than manage the training of neural network.\n\t\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\t\tmanager.Train()\n\n\t# With Charge Encode\n\t\tif (0):\n\t\t\ta = MSet(""H2O_augmented_more_cutoff5_rimp2_force_dipole"")\n\t\t\ta.Load()\n\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\tPARAMS[""learning_rate""] = 0.00001\n\t\t\tPARAMS[""momentum""] = 0.95\n\t\t\tPARAMS[""max_steps""] = 901\n\t\t\tPARAMS[""batch_size""] = 1000\n\t\t\tPARAMS[""test_freq""] = 10\n\t\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\t\tPARAMS[""GradScaler""] = 1.0\n\t\t\tPARAMS[""DipoleScaler""]=1.0\n\t\t\tPARAMS[""NeuronType""] = ""relu""\n\t\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\t\tPARAMS[""EECutoff""] = 15.0\n\t\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\t\tPARAMS[""AN1_r_Rc""] = 8.0\n\t\t\tPARAMS[""AN1_num_r_Rs""] = 64\n\t\t\tPARAMS[""Erf_Width""] = 0.4\n\t\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\t\tPARAMS[""SwitchEpoch""] = 100\n\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode"") # Initialzie a manager than manage the training of neural network.\n\t\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\t\tmanager.Train()\n\n\n\t#New radius: 8 A and long distance pair\n\n\t\tif (0):\n\t\t\ta = MSet(""H2O_augmented_more_rimp2_force_dipole"")\n\t\t\ta.Load()\n\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\tPARAMS[""learning_rate""] = 0.00001\n\t\t\tPARAMS[""momentum""] = 0.95\n\t\t\tPARAMS[""max_steps""] = 901\n\t\t\tPARAMS[""batch_size""] = 1000\n\t\t\tPARAMS[""test_freq""] = 10\n\t\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\t\tPARAMS[""GradScaler""] = 1.0\n\t\t\tPARAMS[""DipoleScaler""]=1.0\n\t\t\tPARAMS[""NeuronType""] = ""relu""\n\t\t\tPARAMS[""HiddenLayers""] = [512, 512, 512]\n\t\t\tPARAMS[""EECutoff""] = 15.0\n\t\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\t\tPARAMS[""AN1_r_Rc""] = 8.0\n\t\t\tPARAMS[""AN1_num_r_Rs""] = 64\n\t\t\tPARAMS[""Erf_Width""] = 0.4\n\t\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\t\tPARAMS[""SwitchEpoch""] = 100\n\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE"") # Initialzie a manager than manage the training of neural network.\n\t\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\t\tmanager.Train()\n\n\n\t\tif (1):\n\t\t#New radius: 8 A and long distance pair and bowl potential with K=0.2\n\t\t\t#a = MSet(""H2O_augmented_more_bowl02_rimp2_force_dipole"")\n\t\t\ta =  MSet(""H2O_bowl02_rimp2_force_dipole"")\n\t\t\ta.Load()\n\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\tPARAMS[""learning_rate""] = 0.00001\n\t\t\tPARAMS[""momentum""] = 0.95\n\t\t\tPARAMS[""max_steps""] = 901\n\t\t\tPARAMS[""batch_size""] = 1000\n\t\t\tPARAMS[""test_freq""] = 10\n\t\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\t\tPARAMS[""GradScaler""] = 1.0\n\t\t\tPARAMS[""DipoleScaler""]=1.0\n\t\t\tPARAMS[""NeuronType""] = ""relu""\n\t\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\t\tPARAMS[""EECutoff""] = 15.0\n\t\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\t\tPARAMS[""AN1_r_Rc""] = 8.0\n\t\t\tPARAMS[""AN1_num_r_Rs""] = 64\n\t\t\tPARAMS[""Erf_Width""] = 0.4\n\t\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\t\tPARAMS[""SwitchEpoch""] = 100\n\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE"") # Initialzie a manager than manage the training of neural network.\n\t\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\t\tmanager.Train()\n\n\n\t# With Chemspider9 Metadyn\n\t\tif (0):\n\t\t\ta = MSet(""chemspider9_metady_force"")\n\t\t\ta.Load()\n\t\t\tTreatedAtoms = a.AtomTypes()\n\t\t\tPARAMS[""learning_rate""] = 0.00001\n\t\t\tPARAMS[""momentum""] = 0.95\n\t\t\tPARAMS[""max_steps""] = 101\n\t\t\tPARAMS[""batch_size""] = 35\n\t\t\tPARAMS[""test_freq""] = 2\n\t\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\t\tPARAMS[""GradScaler""] = 1.0\n\t\t\tPARAMS[""DipoleScaler""]=1.0\n\t\t\tPARAMS[""NeuronType""] = ""relu""\n\t\t\tPARAMS[""HiddenLayers""] = [1000, 1000, 1000]\n\t\t\tPARAMS[""EECutoff""] = 15.0\n\t\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\t\tPARAMS[""Erf_Width""] = 0.4\n\t\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\t\tPARAMS[""SwitchEpoch""] = 10\n\t\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode"") # Initialzie a manager than manage the training of neural network.\n\t\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\t\tmanager.Train()\n\ndef EvalForceField():\n\tif (0):\n\t\ta=MSet(""H2O_force_test"", center_=False)\n\t\ta.ReadXYZ(""H2O_force_test"")\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 300\n\t\tPARAMS[""batch_size""] = 1000\n\t\tPARAMS[""test_freq""] = 10\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\tPARAMS[""AN1_r_Rc""] = 5.0\n\t\tPARAMS[""AN1_num_r_Rs""] = 32\n\t\tPARAMS[""Erf_Width""] = 0.4\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 100\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_augmented_more_cutoff5_rimp2_force_dipole_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_1"",tset,False,""fc_sqdiff_BP_Direct_EE"",False,False)\n\t\t#out_list = manager.EvalBPDirectEESet(a, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t#print out_list\n\t\t#print ""gradient: "", out_list[-1]/BOHRPERA\n\t\t#print manager.EvalBPDirectEESingle(a.mols[0], PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\tm = a.mols[4]\n\t\t#def EnAndForce(x_):\n\t\t\t\t#\t\tm.coords = x_\n\t\t\t\t#\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\t\t#\t\tenergy = Etotal[0]\n\t\t\t\t#\t\tforce = gradient[0]\n\t\t\t\t#\t\treturn energy, force\n\t\t\t\t#ForceField = lambda x: EnAndForce(x)[-1]\n\t\t\t\t#EnergyForceField = lambda x: EnAndForce(x)\n\n\t\t#PARAMS[""MDdt""] = 0.2\n\t\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t\t#PARAMS[""MDMaxStep""] = 10000\n\t\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t\t#PARAMS[""MDV0""] = None\n\t\t#PARAMS[""MDAnnealTF""] = 300.0\n\t\t\t\t#PARAMS[""MDAnnealT0""] = 0.0\n\t\t#PARAMS[""MDAnnealSteps""] = 2000\n\t   \t \t#anneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t   \t \t#anneal.Prop()\n\t   \t \t#m.coords = anneal.Minx.copy()\n\t   \t \t#m.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\n\t\t#Opt = GeomOptimizer(EnergyForceField)\n\t\t#Opt.Opt(m)\n\t\t\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t\t\t#PARAMS[""MDTemp""] = 30\n\t\t\t\t#PARAMS[""MDdt""] = 0.1\n\t\t\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t\t\t#PARAMS[""MDV0""] = None\n\t\t\t\t#PARAMS[""MDMaxStep""] = 10000\n\t\t\t\t#md = VelocityVerlet(None, m, ""11OO"",EnergyForceField)\n\t\t\t\t#md.Prop()\n\n\t\t#mset=MSet(""NeigborMB_test"")\n\t\t#mset.ReadXYZ(""NeigborMB_test"")\n\t\t#MBEterms = MBNeighbors(mset.mols[0].coords, mset.mols[0].atoms, [[0,1,2],[3,4,5],[6,7,8],[9,10,11],[12,13,14]])\n\t\t#mbe =  NN_MBE_Linear(manager)\n\t\t#def EnAndForce(x_):\n\t\t\t\t#\t\tmset.mols[0].coords = x_\n\t\t#\tMBEterms.Update(mset.mols[0].coords, 10.0, 10.0)\n\t\t\t\t#\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(b.mols[0], PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t#\tmbe.EnergyForceDipole(MBEterms)\n\t\t\t\t#\t\tenergy = Etotal[0]\n\t\t\t\t#\t\tforce = gradient[0]\n\t\t\t\t#\t\treturn energy, force\n\t\t\t\t#EnergyForceField = lambda x: EnAndForce(x)\n\n\t\t#Opt = GeomOptimizer(EnergyForceField)\n\t\t#Opt.Opt(b.mols[0])\n\t\t#MBEterms.Update(mset.mols[0].coords, 10.0, 10.0)\n\t\t#mbe =  NN_MBE_Linear(manager)\n\t\t#mbe.EnergyForceDipole(MBEterms)\n\n\t\tdef EnAndForce(x_):\n\t\t\t\t\t\tm.coords = x_\n\t\t\t\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\t\t\t\tenergy = Etotal[0]\n\t\t\t\t\t\tforce = gradient[0]\n\t\t\t\t\t\treturn energy, force\n\n\t\tdef EnForceCharge(x_):\n\t\t\t\t\t\tm.coords = x_\n\t\t\t\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\t\t\t\tenergy = Etotal[0]\n\t\t\t\t\t\tforce = gradient[0]\n\t\t\t\t\t\treturn energy, force, atom_charge\n\n\t\tdef ChargeField(x_):\n\t\t\t\t\t\tm.coords = x_\n\t\t\t\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\t\t\t\tenergy = Etotal[0]\n\t\t\t\t\t\tforce = gradient[0]\n\t\t\t\t\t\treturn atom_charge[0]\n\n\t\t\t\t\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\t\t\t\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\t\t\t\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\t#PARAMS[""OptMaxCycles""]=200\n\t\t#Opt = GeomOptimizer(EnergyForceField)\n\t\t#m=Opt.Opt(m)\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 300.0\n\t\tPARAMS[""MDAnnealT0""] = 0.0\n\t\tPARAMS[""MDAnnealSteps""] = 10000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.Minx.copy()\n\t\tm.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\t\tPARAMS[""MDThermostat""] = None\n\t\tPARAMS[""MDTemp""] = 0\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 100000\n\t\tmd = IRTrajectory(EnAndForce, ChargeField, m, ""IR"")\n\t\tmd.Prop()\n\t\tWriteDerDipoleCorrelationFunction(md.mu_his)\n\n\n\n\tif (0):\n\t\ta=MSet(""NeigborMB_test"", center_=False)\n\t\ta.ReadXYZ(""NeigborMB_test"")\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 300\n\t\tPARAMS[""batch_size""] = 1000\n\t\tPARAMS[""test_freq""] = 10\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\tPARAMS[""AN1_r_Rc""] = 5.0\n\t\tPARAMS[""AN1_num_r_Rs""] = 32\n\t\tPARAMS[""Erf_Width""] = 0.4\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 100\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_augmented_more_cutoff5_rimp2_force_dipole_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_1"",tset,False,""fc_sqdiff_BP_Direct_EE"",False,False)\n\t\t#out_list = manager.EvalBPDirectEESet(a, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t#print out_list\n\t\t#print ""gradient: "", out_list[-1]/BOHRPERA\n\t\t#print manager.EvalBPDirectEESingle(a.mols[0], PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\tm = a.mols[5]\n\t\t#PARAMS[""MDdt""] = 0.2\n\t\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t\t#PARAMS[""MDMaxStep""] = 2000\n\t\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t\t#PARAMS[""MDV0""] = None\n\t\t#PARAMS[""MDAnnealTF""] = 0.0\n\t\t\t\t#PARAMS[""MDAnnealT0""] = 300.0\n\t\t#PARAMS[""MDAnnealSteps""] = 2000\n\t   \t \t#anneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t   \t \t#anneal.Prop()\n\t   \t \t#m.coords = anneal.Minx.copy()\n\t   \t \t#m.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\n\t\tmono_index = []\n\t\tfor i in range (0, 10):\n\t\t\tmono_index.append([i*3, i*3+1, i*3+2])\n\t\tMBEterms = MBNeighbors(m.coords, m.atoms, mono_index)\n\t\tmbe =  NN_MBE_Linear(manager)\n\t\tdef EnAndForce(x_):\n\t\t\t\t\t\tm.coords = x_\n\t\t\t\t\t\tMBEterms.Update(m.coords, 10, 10)\n\t\t\t\t\t\tEtotal, gradient, charge = mbe.EnergyForceDipole(MBEterms)\n\t\t\t\t\t\tenergy = Etotal\n\t\t\t\t\t\tforce = gradient\n\t\t\t\t\t\treturn energy, force\n\n\t\t\t\t\t\tEnergyForceField = lambda x: EnAndForce(x)\n\t\tprint(EnergyForceField(m.coords))\n\t\t#Opt = GeomOptimizer(EnergyForceField)\n\t\t#Opt.Opt(m)\n\n\t\t#PARAMS[""MDdt""] = 0.2\n\t\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t\t#PARAMS[""MDMaxStep""] = 2000\n\t\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t\t#PARAMS[""MDV0""] = None\n\t\t#PARAMS[""MDAnnealTF""] = 0.0\n\t\t\t\t#PARAMS[""MDAnnealT0""] = 100.0\n\t\t#PARAMS[""MDAnnealSteps""] = 200\n\t   \t \t#anneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t   \t \t#anneal.Prop()\n\t   \t \t#m.coords = anneal.Minx.copy()\n\t   \t \t#m.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tMBEterms.Update(m.coords, 20.0, 20.0)\n\t\t\tEtotal, gradient, charge = mbe.EnergyForceDipole(MBEterms)\n\t\t\treturn charge\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\t#PARAMS[""MDdt""] = 0.1\n\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t#PARAMS[""MDMaxStep""] = 10000\n\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t#PARAMS[""MDV0""] = None\n\t\t#PARAMS[""MDAnnealTF""] = 30.0\n\t\t#PARAMS[""MDAnnealT0""] = 0.1\n\t\t#PARAMS[""MDAnnealSteps""] = 10000\n\t\t#anneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\t#anneal.Prop()\n\t\t#m.coords = anneal.x.copy()\n\n\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDTemp""] = 30\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\twarm = VelocityVerlet(ForceField, m,""warm"",EnergyForceField)\n\t\twarm.Prop()\n\t\tm.coords = warm.x.copy()\n\n\t\tPARAMS[""MDThermostat""] = None\n\t\tPARAMS[""MDTemp""] = 0\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 40000\n\t\tmd = IRTrajectory(EnAndForce, ChargeField, m, ""IR"", warm.v)\n\t\tmd.Prop()\n\t\tWriteDerDipoleCorrelationFunction(md.mu_his)\n\n\n\tif (1):\n\t\tos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\t\ta = MSet(""chemspider9_metady_force"")\n\t\ta.Load()\n\t\tb = MSet(""IRSmallmols"")\n\t\tb.ReadXYZ()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] = 35\n\t\tPARAMS[""test_freq""] = 2\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [1000, 1000, 1000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\tPARAMS[""Erf_Width""] = 0.4\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 10\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\tmanager=TFMolManage(""Mol_chemspider9_metady_force_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode"", False, False) # Initialzie a manager than manage the training of neural network.\n\t\t#print manager.EvalBPDirectEESet(b, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t#print manager.EvalBPDirectEESingle(a.mols[1], PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t#print a.mols[1].properties, ""Dipole in a.u.:"",a.mols[1].properties[""dipole""]*0.393456\n\t\tm = b.mols[8]\n\t\tdef EnAndForce(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force\n\n\t\tdef EnForceCharge(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force, atom_charge\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn atom_charge[0]\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\tPARAMS[""OptMaxCycles""]=200\n\t\tOpt = GeomOptimizer(EnergyForceField)\n\t\tm=Opt.Opt(m)\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 300.0\n\t\tPARAMS[""MDAnnealT0""] = 0.1\n\t\tPARAMS[""MDAnnealSteps""] = 10000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.Minx.copy()\n\t\tm.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\t\tPARAMS[""MDThermostat""] = None\n\t\tPARAMS[""MDTemp""] = 0\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 40000\n\t\tmd = IRTrajectory(EnAndForce, ChargeField, m, ""IR"", anneal.v)\n\t\tmd.Prop()\n\t\tWriteDerDipoleCorrelationFunction(md.mu_his)\n\n\n\tif (0):\n\t\tos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\t\ta = MSet(""chemspider9_metady_force"")\n\t\ta.Load()\n\t\tb = MSet(""chemspider12_metady_test"")\n\t\tb.ReadXYZ()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] = 35\n\t\tPARAMS[""test_freq""] = 2\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [1000, 1000, 1000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 7.0\n\t\tPARAMS[""Erf_Width""] = 0.4\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 10\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\tmanager=TFMolManage(""Mol_chemspider9_metady_force_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode"", False, False) # Initialzie a manager than manage the training of neural network.\n\t\t#print manager.EvalBPDirectEESet(b, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t#print manager.EvalBPDirectEESingle(a.mols[1], PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t#print a.mols[1].properties, ""Dipole in a.u.:"",a.mols[1].properties[""dipole""]*0.393456\n\n\t\tm = b.mols[1]\n\t\tdef EnAndForce(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEESingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force\n\n\t\tForceField = lambda x: QchemDFT(Mol(m.atoms,x),basis_ = \'6-31g*\',xc_=\'b3lyp\', jobtype_=\'force\', filename_=\'chemspider12_meta_test\', path_=\'./qchem/\', threads=24)\n\t\t#ForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\n\t\t\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t#PARAMS[""MDTemp""] = 600\n\t\t\t\t#PARAMS[""MDdt""] = 1.0\n\t\t\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t\t\t#PARAMS[""MDV0""] = None\n\t\t\t\t#PARAMS[""MDMaxStep""] = 10000\n\t\t\t\t#warm = VelocityVerlet(ForceField, m,""warm_chemspider12"",EnergyForceField)\n\t\t\t\t#warm.Prop()\n\n\t\tPARAMS[""MDdt""] = 0.5\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDTemp""]= 500.0\n\t\tPARAMS[""MetaBowlK""] = 0.0\n\t\tmeta = MetaDynamics(ForceField, m)\n\t\tmeta.Prop()\n#TestCoulomb()\n#TrainPrepare()\n#TrainForceField()\nEvalForceField()\n'"
samples/test_h2o.py,29,"b'from __future__ import absolute_import\n#import memory_util\n#memory_util.vlog(1)\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nfrom TensorMol.MBE.NN_MBE import *\nfrom TensorMol.Interfaces.TMIPIinterface import *\nimport random\n\ndef TrainPrepare():\n\tif (0):\n\t\timport math, random\n\t\ta = MSet(""H2O_wb97xd_1to21"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#a=MSet(""H2O_cluster_meta"", center_=False)\n\t\t#a.ReadXYZ(""H2O_cluster_meta"")\n\t\tHbondcut = 2.2\n\t\tHbondangle = 30.0*math.pi/180.0\n\t\tHOcut = 1.1\n\t\tsinglemax = 0.20 * len(a.mols)\n\t\tdoublemax = 0.10 * len(a.mols)\n\t\tsingle_record = np.zeros((21,2))\n\t\tsingle_record[:,0] = range(1,22)\n\t\tdouble_record = np.zeros((21,2))\n\t\tdouble_record[:,0] = range(1,22)\n\t\tdef MakeH3O(mol, Hbonds_set, xyzname=""H2O_meta_with_H3O_more"", doublepro = False):\n\t\t\tnew_m = Mol(mol.atoms, mol.coords)\n\t\t\tfor i, Hbonds in enumerate(Hbonds_set):\n\t\t\t\tO1_index = Hbonds[0]\n\t\t\t\tH_index = Hbonds[1]\n\t\t\t\tO2_index = Hbonds[2]\n\t\t\t\tO2_H_index = []\n\t\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\t\tif mol.atoms[j] == 1 and len(O2_H_index) <=2 :\n\t\t\t\t\t\tdist = np.sum(np.square(mol.coords[O2_index] - mol.coords[j]))**0.5\n\t\t\t\t\t\tif dist < HOcut:\n\t\t\t\t\t\t\tO2_H_index.append(j)\n\t\t\t\tif len(O2_H_index) != 2:\n\t\t\t\t\treturn\n\t\t\t\tO2H_1 = mol.coords[O2_index] - mol.coords[O2_H_index[0]]\n\t\t\t\tO2H_2 = mol.coords[O2_index] - mol.coords[O2_H_index[1]]\n\t\t\t\ty_axis = np.cross(O2H_1, O2H_2)\n\t\t\t\ty_axis = y_axis/np.sum(np.square(y_axis))**0.5\n\n\n\t\t\t\tx_axis = mol.coords[O2_index] - (mol.coords[O2_H_index[0]]+mol.coords[O2_H_index[1]])/2.0\n\t\t\t\tx_axis = x_axis/np.sum(np.square(x_axis))**0.5\n\t\t\t\tOH_vec = mol.coords[O1_index] - mol.coords[H_index]\n\n\t\t\t\tangle_cri = math.pi/3\n\n\t\t\t\tif np.dot(x_axis, OH_vec)/np.sum(np.square(OH_vec))**0.5 < math.cos(math.pi/3):\n\t\t\t\t\treturn\n\n\t\t\t\tt_angle = math.pi/180.0*(180.0-131.75 + 10*(2.0*random.random() - 1.0))\n\n\t\t\t\tt_length = (1.0 + 0.1*(2.0*random.random() - 1.0))\n\n\t\t\t\t#print y_axis, x_axis\n\t\t\t\tvec1 =(math.tan(t_angle)*y_axis + x_axis)\n\t\t\t\tvec1 = vec1/np.sum(np.square(vec1))**0.5\n\t\t\t\tvec1 = vec1*t_length + mol.coords[O2_index]\n\n\t\t\t\tvec2 = -(math.tan(t_angle)*y_axis) + x_axis\n\t\t\t\tvec2 = vec2/np.sum(np.square(vec2))**0.5\n\t\t\t\tvec2 = vec2*t_length + mol.coords[O2_index]\n\n\t\t\t\tif np.sum(np.square(mol.coords[H_index] - vec1))**0.5 < np.sum(np.square(mol.coords[H_index] - vec2))**0.5:\n\t\t\t\t\tvec = vec1\n\t\t\t\telse:\n\t\t\t\t\tvec = vec2\n\n\t\t\t\tif (not doublepro and i==0) or i==1:\n\t\t\t\t\tvec =  random.random()*(vec - mol.coords[H_index]) + mol.coords[H_index]\n\t\t\t\tnew_m.coords[H_index] = vec\n\t\t\tnew_m.WriteXYZfile(fname=xyzname)\n\t\t\tif doublepro:\n\t\t\t\tdouble_record[int(mol.NAtoms())/3-1,1] += 1\n\t\t\telse:\n\t\t\t\tsingle_record[int(mol.NAtoms())/3-1,1] += 1\n\t\t\treturn 1\n\n\t\tsinglepro = 0\n\t\tdoublepro = 0\n\t\tfor mol_index, m in enumerate(a.mols):\n\t\t\ti_ran = random.randint(0, m.NAtoms()-1)\n\t\t\tfor i_ini in range (0, m.NAtoms()):\n\t\t\t\ti  = i_ini + i_ran\n\t\t\t\tif i > m.NAtoms()-1:\n\t\t\t\t\ti = i - m.NAtoms() + 1\n\t\t\t\tif m.atoms[i] == 8: #it is a O:\n\t\t\t\t\tH1_index = -1\n\t\t\t\t\tH2_index = -1\n\t\t\t\t\tHbonds = []\n\t\t\t\t\tfor j in range (0, m.NAtoms()):\n\t\t\t\t\t\tif H1_index != -1 and H2_index != -1:\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\tif m.atoms[j] == 1:\n\t\t\t\t\t\t\tdist = np.sum(np.square(m.coords[i] - m.coords[j]))**0.5\n\t\t\t\t\t\t\tif dist < HOcut and H1_index == -1:\n\t\t\t\t\t\t\t\tH1_index = j\n\t\t\t\t\t\t\telif dist < HOcut and H1_index != -1:\n\t\t\t\t\t\t\t\tH2_index = j\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\tif H1_index != -1 and H2_index != -1:\n\t\t\t\t\t\tHbondflag1 = False\n\t\t\t\t\t\tfor j in range (0, m.NAtoms()):\n\t\t\t\t\t\t\tif j==i:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\tif m.atoms[j] == 8:\n\t\t\t\t\t\t\t\tHbonddist = np.sum(np.square(m.coords[H1_index] - m.coords[j]))**0.5\n\t\t\t\t\t\t\t\tif Hbonddist < Hbondcut:\n\t\t\t\t\t\t\t\t\tOOdist = np.sum(np.square(m.coords[i] - m.coords[j]))**0.5\n\t\t\t\t\t\t\t\t\tHOdist = np.sum(np.square(m.coords[H1_index] - m.coords[i]))**0.5\n\t\t\t\t\t\t\t\t\tangle = (OOdist**2 + HOdist**2 - Hbonddist**2)/(2*OOdist*HOdist)\n\t\t\t\t\t\t\t\t\tif angle > math.cos(Hbondangle):\n\t\t\t\t\t\t\t\t\t\tHbondflag1 = True\n\t\t\t\t\t\t\t\t\t\tHbonds.append([i, H1_index, j])\n\t\t\t\t\t\tHbondflag2 = False\n\t\t\t\t\t\tfor j in range (0, m.NAtoms()):\n\t\t\t\t\t\t\tif j==i:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\tif m.atoms[j] == 8:\n\t\t\t\t\t\t\t\tHbonddist = np.sum(np.square(m.coords[H2_index] - m.coords[j]))**0.5\n\t\t\t\t\t\t\t\tif Hbonddist < Hbondcut:\n\t\t\t\t\t\t\t\t\tOOdist = np.sum(np.square(m.coords[i] - m.coords[j]))**0.5\n\t\t\t\t\t\t\t\t\tHOdist = np.sum(np.square(m.coords[H2_index] - m.coords[i]))**0.5\n\t\t\t\t\t\t\t\t\tangle = (OOdist**2 + HOdist**2 - Hbonddist**2)/(2*OOdist*HOdist)\n\t\t\t\t\t\t\t\t\tif angle > math.cos(Hbondangle):\n\t\t\t\t\t\t\t\t\t\tHbondflag2 = True\n\t\t\t\t\t\t\t\t\t\tHbonds.append([i, H2_index, j])\n\t\t\t\t\tif len(Hbonds) == 1 and singlepro < singlemax:\n\t\t\t\t\t\tif MakeH3O(m, Hbonds, xyzname=""H2O_meta_with_H3O_single_more"", doublepro=False):\n\t\t\t\t\t\t\tsinglepro += 1\n\t\t\t\t\t\t\tprint (single_record)\n\t\t\t\t\t\t\tprint (""single pronated..."", singlepro, "" mol_index:"", mol_index)\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif len(Hbonds) == 2 and doublepro < doublemax:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t#if MakeH3O(m, Hbonds, xyzname=""H2O_meta_with_H3O_double"", doublepro=True):\n\t\t\t\t\t\t#\tdoublepro += 1\n\t\t\t\t\t\t#\tprint (double_record)\n\t\t\t\t\t\t#\tprint (""double pronated..."", doublepro, "" mol_index:"", mol_index)\n\t\t\t\t\t\t#break\n\t\t\t\t\telse:\n\t\t\t\t\t\tcontinue\n\n\n\tif (0):\n\t\tWB97XDAtom={}\n\t\tWB97XDAtom[1]=-0.5026682866\n\t\tWB97XDAtom[6]=-37.8387398698\n\t\tWB97XDAtom[7]=-54.5806161811\n\t\tWB97XDAtom[8]=-75.0586028656\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\tdic_list = pickle.load(open(""./datasets/H2O_wbxd_1to21_with_prontonated.dat"", ""rb""))\n\t\tfor mol_index, dic in enumerate(dic_list):\n\t\t\tatoms = []\n\t\t\tprint (""mol_index:"", mol_index)\n\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\t#print (dic.keys())\n\t\t\t#print (dic[\'xyz\'])\n\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\t#mol.properties[\'charges\'] = dic[\'charges\']\n\t\t\tmol.properties[\'dipole\'] = np.asarray(dic[\'dipole\'])\n\t\t\t#mol.properties[\'quadropole\'] = dic[\'quad\']\n\t\t\tmol.properties[\'energy\'] = dic[\'scf_energy\']\n\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\tmol.properties[\'atomization\'] = dic[\'scf_energy\']\n\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\tmol.properties[\'atomization\'] -= WB97XDAtom[mol.atoms[i]]\n\t\t\ta.mols.append(mol)\n\t\t#a.mols[10000].WriteXYZfile(fname=""H2O_sample.xyz"")\n\t\t#print(a.mols[100].properties)\n\t\ta.Save()\n\n\tif (0):\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\ttotal_water = 0.0\n\t\ttotal_atomization = 0.0\n\t\tfor mol in a.mols:\n\t\t\ttotal_water += mol.NAtoms()/3\n\t\t\ttotal_atomization += mol.properties[\'atomization\']\n\t\tavg_atomization = total_atomization/total_water\n\t\tprint (""avg_atomization:"", avg_atomization)  # (\'avg_atomization:\', -0.35551059977287547)\n\t\tfor mol in a.mols:\n\t\t\tmol.properties[\'atomization_old\'] = mol.properties[\'atomization\']\n\t\t\tmol.properties[\'atomization\'] = mol.properties[\'atomization\']-mol.NAtoms()/3.0*avg_atomization\n\t\t\tprint (""mol.properties[\'atomization\']:,mol.properties[\'atomization_old\']"", mol.properties[\'atomization\'], mol.properties[\'atomization_old\'])\n\t\ta.Save()\n\n\n\t#istring = \'$molecule\\n0 1 \\n\'\n\t#crds = m_.coords.copy()\n\t#crds[abs(crds)<0.0000] *=0.0\n\t#for j in range(len(m_.atoms)):\n\t#\tistring=istring+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\t#istring =istring + \'$end\\n\\n$rem\\njobtype \'+jobtype_+\'\\nbasis \'+basis_+\'\\nmethod \'+xc_+\'\\nthresh 11\\nsymmetry false\\nsym_ignore true\\n$end\\n\'\n\t#with open(path_+filename_+\'.in\',\'w\') as fin:\n\t#\tfin.write(istring)\n\tif (1): #H2O_wb97xd_1to21_with_prontonated\n\t\ta = MSet(""H2O_wb97xd_1to21"")\n\t\ta.Load()\n\t\timport random\n\t\trandom.shuffle(a.mols)\n\t\tnfolder = 100\n\t\timport os\n\t\tfor i in range(1, nfolder+1):\n\t\t\tos.mkdir(""water_aug_ccpvdz_""+str(i))\n\t\tmol_per_folder = len(a.mols)/nfolder+1\n\t\tfor i in range(0, len(a.mols)):\n\t\t\tfolder_index = i/mol_per_folder+1\n\t\t\tfile_index = i%mol_per_folder+1\n\t\t\tm_ = a.mols[i]\n\t\t\tistring = """"\n\t\t\tif i!=0:\n\t\t\t\tistring += ""@@@\\n\\n""\n\t\t\tistring = \'$molecule\\n0 1 \\n\'\n\t\t\tcrds = m_.coords.copy()\n\t\t\tfor j in range(len(m_.atoms)):\n\t\t\t\tistring=istring+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\t\t\tistring =istring + \'$end\\n\\n$rem\\njobtype force\\nbasis aug-cc-pvdz\\nmethod wB97X-D\\nmax_scf_cycles  200\\nsymmetry false\\nsym_ignore true\\n$end\\n\\n\'\n\t\t\twith open(\'water_aug_ccpvdz_\'+str(folder_index)+\'/h2o_aug_ccpvdz_\'+str(file_index)+\'.in\',\'w\') as fin:\n\t\t\t\tfin.write(istring)\n\t\t\t\tfin.close()\n\t\treturn\n\t\t#b = MSet(""H2O_wb97xd_1to21_with_prontonated_original"")\n\t\t#for mol in a.mols:\n\t\t#\tmol.properties[\'atomization\'] = mol.properties[\'atomization_old\']\n\t\t#\tb.mols.append(mol)\n\t\t#\tprint (""mol.properties[\'atomization\']:,mol.properties[\'atomization_old\']"", mol.properties[\'atomization\'], mol.properties[\'atomization_old\'])\n\t\t#b.Save()\n\tif (0):\n\t\tWB97XDAtom={}\n\t\tWB97XDAtom[1]=-0.5026682866\n\t\tWB97XDAtom[6]=-37.8387398698\n\t\tWB97XDAtom[7]=-54.5806161811\n\t\tWB97XDAtom[8]=-75.0586028656\n\t\tch4_min_atomization = -0.6654760227400112\n\t\twater_avg_atomization = -0.35551059977287\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated_with_ch4"")\n\t\tdic_list = pickle.load(open(""./datasets/H2O_wbxd_1to21_with_prontonated_with_ch4.dat"", ""rb""))\n\t\tfor mol_index, dic in enumerate(dic_list):\n\t\t\tatoms = []\n\t\t\tprint (""mol_index:"", mol_index)\n\t\t\tfor atom in dic[\'atoms\']:\n\t\t\t\tatoms.append(AtomicNumber(atom))\n\t\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\t\t#print (dic.keys())\n\t\t\t#print (dic[\'xyz\'])\n\t\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\t\t#mol.properties[\'charges\'] = dic[\'charges\']\n\t\t\tmol.properties[\'dipole\'] = np.asarray(dic[\'dipole\'])\n\t\t\t#mol.properties[\'quadropole\'] = dic[\'quad\']\n\t\t\tmol.properties[\'energy\'] = dic[\'scf_energy\']\n\t\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\t\tmol.properties[\'atomization_old\'] = dic[\'scf_energy\']\n\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\tmol.properties[\'atomization_old\'] -= WB97XDAtom[mol.atoms[i]]\n\t\t\ta.mols.append(mol)\n\t\t\tif 6 in mol.atoms: # contain one CH4\n\t\t\t\tmol.properties[\'atomization\'] = mol.properties[\'atomization_old\'] - (mol.NAtoms()-5)/3*water_avg_atomization - ch4_min_atomization\n\t\t\telse:\n\t\t\t\tmol.properties[\'atomization\'] = mol.properties[\'atomization_old\'] - mol.NAtoms()/3*water_avg_atomization\n\t\t\tprint (""mol.properties[\'atomization\']:"", mol.properties[\'atomization\'])\n\t\t#a.mols[10000].WriteXYZfile(fname=""H2O_sample.xyz"")\n\t\t#print(a.mols[100].properties)\n\t\ta.Save()\ndef Train():\n\tif (0):\n\t\ta = MSet(""H2O_wb97xd_1to10"")\n\t\ta.Load()\n\t\t#random.shuffle(a.mols)\n\t\t#for i in range(150000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 201\n\t\tPARAMS[""batch_size""] =  300   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 5\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 40\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\t#tset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t#manager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode"") # Initialzie a manager than manage the training of neural network.\n\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_Update"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\t\t#with memory_util.capture_stderr() as stderr:\n\t\t#\tmanager.Train(1)\n\t\t#memory_util.print_memory_timeline(stderr, ignore_less_than_bytes=1000)\n\n\tif (0):\n\t\ta = MSet(""H2O_wb97xd_1to10"")\n\t\ta.Load()\n\t\t#random.shuffle(a.mols)\n\t\t#for i in range(150000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 201\n\t\tPARAMS[""batch_size""] =  300   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 5\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 40\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\t#tset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\t#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n\t\t#manager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode"") # Initialzie a manager than manage the training of neural network.\n\t\t#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\n\tif (0):\n\t\ta = MSet(""H2O_wb97xd_1to21"")\n\t\ta.Load()\n\t\t#random.shuffle(a.mols)\n\t\t#for i in range(300000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0):\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#for i in range(360000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\n\tif (0): # Normalize\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#for i in range(360000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [200, 200, 200]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\n\tif (0): # Normalize+Dropout\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#for i in range(340000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+2000+more dropout+just energy\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tb=MSet(""H2O_Dimer_wb97xd"", center_=False)\n\t\tb.ReadXYZ(""H2O_Dimer_wb97xd"")\n\t\t#for i in range(350000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""JustEnergy""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  130   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 0.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [1000, 1000, 1000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""MonitorSet""] = b\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 0.5, 0.5]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+2000+more dropout+just gradient\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\tb=MSet(""H2O_Dimer_wb97xd"", center_=False)\n\t\tb.ReadXYZ(""H2O_Dimer_wb97xd"")\n\t\trandom.shuffle(a.mols)\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""JustGrad""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 0.0\n\t\tPARAMS[""GradScalar""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [1000, 1000, 1000]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""MonitorSet""] = b\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 0.5, 0.5]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+more dropout+as usual\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#for i in range(350000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""500_twolayerdropout05""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 0.5, 0.5]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+usual, dropout07+act_square_tozero_tolinear\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tb=MSet(""H2O_Dimer_wb97xd"", center_=False)\n\t\tb.ReadXYZ(""H2O_Dimer_wb97xd"")\n\t\t#for i in range(350000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_square_tozero_tolinear""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""square_tozero_tolinear""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""MonitorSet""] = b\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+usual, dropout07+act_gaussian\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tb=MSet(""H2O_Dimer_wb97xd"", center_=False)\n\t\tb.ReadXYZ(""H2O_Dimer_wb97xd"")\n\t\t#for i in range(350000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_gaussian""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""gaussian""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""MonitorSet""] = b\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+usual, dropout07+sigmoid100\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tb=MSet(""H2O_Dimer_wb97xd"", center_=False)\n\t\tb.ReadXYZ(""H2O_Dimer_wb97xd"")\n\t\t#for i in range(350000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100_rightalpha_dropout07""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""MonitorSet""] = b\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\n\tif (1): # Normalize+Dropout+500+usual, dropout07+sigmoid100+nograd train\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tb=MSet(""H2O_Dimer_wb97xd"", center_=False)\n\t\tb.ReadXYZ(""H2O_Dimer_wb97xd"")\n\t\tfor i in range(350000):\n\t\t\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100_rightalpha_dropout07_nogradtrain""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 3\n\t\tPARAMS[""batch_size""] =  300   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""MonitorSet""] = b\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 1\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_NoGradTrain"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+usual, dropout07+sigmoid100+noEcc\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tb=MSet(""H2O_Dimer_wb97xd"", center_=False)\n\t\tb.ReadXYZ(""H2O_Dimer_wb97xd"")\n\t\t#for i in range(350000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100_noEcc""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t\tPARAMS[""sigmoid_alpha""] = 100.0\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""MonitorSet""] = b\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = False\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+500+usual+angular13\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tb=MSet(""H2O_Dimer_wb97xd"", center_=False)\n\t\tb.ReadXYZ(""H2O_Dimer_wb97xd"")\n\t\t#for i in range(350000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""angular13""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 201\n\t\tPARAMS[""batch_size""] =  300   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""MonitorSet""] = b\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\tPARAMS[""AN1_a_Rc""] = 1.3\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 30\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout last fc  07+500+avgPool\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#for i in range(350000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""lastfc07""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 100\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 0.7, 1.0, 1.0]\n\t\t#PARAMS[""KeepProb""] = 0.7\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\tPARAMS[""AvgWindowSize""] = 1\n\t\tPARAMS[""ChopPadding""] = 50\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_AvgPool"")\n\t\tPARAMS[\'Profiling\']= 0\n\t\tmanager.Train(1)\n\n\tif (0): # Normalize+Dropout+Conv\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#for i in range(360000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 21\n\t\tPARAMS[""batch_size""] =  500   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""ConvFilter""] = [32, 64]\n\t\tPARAMS[""ConvKernelSize""] = [[8,1],[4,1]]\n\t\tPARAMS[""ConvStrides""] = [[8,1],[4,1]]\n\t\tPARAMS[""HiddenLayers""] = [500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = 1.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.01\n\t\tPARAMS[""learning_rate_energy""] = 0.001\n\t\tPARAMS[""SwitchEpoch""] = 4\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_Conv"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0):\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated_with_ch4"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#for i in range(680000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 71\n\t\tPARAMS[""batch_size""] =  80   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""AddEcc""] = False\n\t\tPARAMS[""learning_rate_dipole""] = 0.1\n\t\tPARAMS[""learning_rate_energy""] = 0.01\n\t\tPARAMS[""SwitchEpoch""] = 10\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\n\tif (0):\n\t\ta = MSet(""H2O_wb97xd_1to21_with_prontonated_with_ch4"")\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\t#for i in range(360000):\n\t\t#\ta.mols.pop()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 71\n\t\tPARAMS[""batch_size""] =  80   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 10\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\ndef Eval():\n\tif (1):\n\t\t#a = MSet(""water_tiny"", center_=False)\n\t\t#a.ReadXYZ(""water_tiny"")\n\t\t#a=MSet(""H2O_cluster_meta"", center_=False)\n\t\t#a.ReadXYZ(""H2O_cluster_meta"")\n\t\ta=MSet(""H2O_100_cluster"", center_=False)\n\t\ta.ReadXYZ(""H2O_100_cluster"")\n\t\tTreatedAtoms = a.AtomTypes()\n\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_with_prontonated_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu"",False,False)\n\t\tm = a.mols[-1]\n\t\t#print manager.EvalBPDirectEEUpdateSinglePeriodic(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t#print manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t#return\n\t\t#charge = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[6]\n\t\t#bp_atom = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[2]\n\t\t#for i in range (0, m.NAtoms()):\n\t\t#\tprint i+1, charge[0][i],bp_atom[0][i]\n\n\t\tdef EnAndForce(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force\n\n\t\tdef EnForceCharge(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force, atom_charge\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn atom_charge[0]\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\tPARAMS[""OptMaxCycles""]=200\n\t\tOpt = GeomOptimizer(EnergyForceField)\n\t\tm=Opt.Opt(m)\n\n\n\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t#PARAMS[""MDTemp""] = 300\n\t\t#PARAMS[""MDdt""] = 0.2\n\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t#PARAMS[""MDV0""] = None\n\t\t#PARAMS[""MDMaxStep""] = 10000\n\t\t#md = VelocityVerlet(None, m, ""water_tiny_noperi"",EnergyForceField)\n\t\t#md.Prop()\n\n\n\t\tPARAMS[""OptMaxCycles""]=200\n\t\tOpt = GeomOptimizer(EnergyForceField)\n\t\tm=Opt.Opt(m)\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 2000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 0.0\n\t\tPARAMS[""MDAnnealT0""] = 300.0\n\t\tPARAMS[""MDAnnealSteps""] = 1000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.Minx.copy()\n\t\tm.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 2.0\n\t\tPARAMS[""MDAnnealT0""] = 0.1\n\t\tPARAMS[""MDAnnealSteps""] = 10000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""Anneal_2K"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.x.copy()\n\t\tm.WriteXYZfile(""./results/"", ""Anneal_warm_10K"")\n\t\tPARAMS[""MDThermostat""] = None\n\t\tPARAMS[""MDTemp""] = 0\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 40000\n\t\tmd = IRTrajectory(EnAndForce, ChargeField, m, ""water_10_IR_2K"", anneal.v)\n\t\tmd.Prop()\n\t\tWriteDerDipoleCorrelationFunction(md.mu_his)\n\t\treturn\n\n\n\tif (0):\n\t\ta = MSet(""water_tiny"", center_=False)\n\t\ta.ReadXYZ(""water_tiny"")\n\t\t#a=MSet(""H2O_cluster_meta"", center_=False)\n\t\t#a.ReadXYZ(""H2O_cluster_meta"")\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw"",False,False)\n\t\tm = a.mols[0]\n\t\t#print manager.EvalBPDirectEEUpdateSinglePeriodic(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\tprint (manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True))\n\t\treturn\n\t\t#charge = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[6]\n\t\t#bp_atom = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[2]\n\t\t#for i in range (0, m.NAtoms()):\n\t\t#\tprint i+1, charge[0][i],bp_atom[0][i]\n\n\t\tdef EnAndForce(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force\n\n\t\tdef EnForceCharge(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force, atom_charge\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn atom_charge[0]\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\t#PARAMS[""OptMaxCycles""]=200\n\t\t#Opt = GeomOptimizer(EnergyForceField)\n\t\t#m=Opt.Opt(m)\n\n\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDTemp""] = 1000\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tmd = VelocityVerlet(None, m, ""water_tiny_noperi"",EnergyForceField)\n\t\tmd.Prop()\n\t\treturn\n\n\t\t#PARAMS[""OptMaxCycles""]=1000\n\t\t#Opt = GeomOptimizer(EnergyForceField)\n\t\t#m=Opt.Opt(m)\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 2000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 1.0\n\t\tPARAMS[""MDAnnealT0""] = 300.0\n\t\tPARAMS[""MDAnnealSteps""] = 1000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.Minx.copy()\n\t\tm.WriteXYZfile(""./results/"", ""Anneal_opt_648"")\n\t\treturn\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 30.0\n\t\tPARAMS[""MDAnnealT0""] = 0.1\n\t\tPARAMS[""MDAnnealSteps""] = 10000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.Minx.copy()\n\t\tm.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\t\tPARAMS[""MDThermostat""] = None\n\t\tPARAMS[""MDTemp""] = 0\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 40000\n\t\tmd = IRTrajectory(EnAndForce, ChargeField, m, ""water_10_IR"", anneal.v)\n\t\tmd.Prop()\n\t\tWriteDerDipoleCorrelationFunction(md.mu_his)\n\n\t\toutlist = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], HasVdw=True)\n\t\tprint (outlist)\n\t\tprint (np.sum(outlist[5].reshape((-1,3)),axis=-1))\n\t\tprint (np.sum((outlist[5].reshape((-1,1))*m.coords), axis=0))\n\n\t\tcharge = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)[5]\n\t\tEcc = 0.0\n\t\tfor i in range (0, m.NAtoms()):\n\t\t\tfor j in range (i+1, m.NAtoms()):\n\t\t\t\tdist = (np.sum(np.square(m.coords[i] - m.coords[j])))**0.5 *  BOHRPERA\n\t\t\t\tif dist < PARAMS[""EECutoffOn""]*BOHRPERA:\n\t\t\t\t\tcut = 0.0\n\t\t\t\telif dist > (PARAMS[""EECutoffOn""]+PARAMS[""Poly_Width""])*BOHRPERA:\n\t\t\t\t\tcut = 1.0\n\t\t\t\telse:\n\t\t\t\t\tt = (dist-PARAMS[""EECutoffOn""]*BOHRPERA)/(PARAMS[""Poly_Width""]*BOHRPERA)\n\t\t\t\t\tcut = -t*t*(2.0*t-3.0)\n\t\t\t\tEcc +=  cut*charge[0][i]*charge[0][j]/dist\n\t\tprint (""Ecc manual:"", Ecc)\n\n\tif (0):  # i-pi test\n\t\ta=MSet(""H2O_cluster_meta"", center_=False)\n\t\ta.ReadXYZ(""H2O_cluster_meta"")\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_with_prontonated_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu"",False,False)\n\t\tm = a.mols[9]\n\t\tprint (m.coords)\n\t\tdef EnAndForce(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force\n\n\t\tdef EnForceCharge(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force, atom_charge\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn atom_charge[0]\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\t\tinterface = TMIPIManger(EnergyForceField, TCP_IP=""localhost"", TCP_PORT= 31415)\n\t\tinterface.md_run()\n\n\tif (0):\n\t\ta=MSet(""H2O_cluster_meta"", center_=False)\n\t\ta.ReadXYZ(""H2O_cluster_meta"")\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 201\n\t\tPARAMS[""batch_size""] =  300   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 5\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScaler""] = 1.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0.0\n\t\t#PARAMS[""EECutoffOn""] = 7.0\n\t\t#PARAMS[""Erf_Width""] = 0.4\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 40\n\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to10_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_Update_1"",tset,False,""fc_sqdiff_BP_Direct_EE_Update"",False,False)\n\n\t\tm = a.mols[-2]\n\t\toutlist =  manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\tprint (outlist[4])\n\t\treturn\n\t\tprint (""self.Ree_on:"", manager.Instances.Ree_on)\n\t\tprint (outlist)\n\t\tprint (np.sum(outlist[4].reshape((-1,3)),axis=-1))\n\t\tprint (np.sum((outlist[4].reshape((-1,1))*m.coords), axis=0))\n\t\tcharge = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])[4]\n\t\tEcc = 0.0\n\t\tfor i in range (0, m.NAtoms()):\n\t\t\tfor j in range (i+1, m.NAtoms()):\n\t\t\t\tdist = (np.sum(np.square(m.coords[i] - m.coords[j])))**0.5 *  BOHRPERA\n\t\t\t\tif dist < PARAMS[""EECutoffOn""]*BOHRPERA:\n\t\t\t\t\tcut = 0.0\n\t\t\t\telif dist > (PARAMS[""EECutoffOn""]+PARAMS[""Poly_Width""])*BOHRPERA:\n\t\t\t\t\tcut = 1.0\n\t\t\t\telse:\n\t\t\t\t\tt = (dist-PARAMS[""EECutoffOn""]*BOHRPERA)/(PARAMS[""Poly_Width""]*BOHRPERA)\n\t\t\t\t\tcut = -t*t*(2.0*t-3.0)\n\t\t\t\tEcc +=  cut*charge[0][i]*charge[0][j]/dist\n\t\tprint (""Ecc manual:"", Ecc)\n\t\treturn\n\n\t\tdef EnAndForce(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force\n\n\t\tdef EnForceCharge(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn energy, force, atom_charge\n\n\t\tdef ChargeField(x_):\n\t\t\tm.coords = x_\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\treturn atom_charge[0]\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\tPARAMS[""OptMaxCycles""]=200\n\t\tOpt = GeomOptimizer(EnergyForceField)\n\t\tm=Opt.Opt(m)\n\n\t\t#PARAMS[""MDdt""] = 0.2\n\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t#PARAMS[""MDMaxStep""] = 2000\n\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t#PARAMS[""MDV0""] = None\n\t\t#PARAMS[""MDAnnealTF""] = 0.0\n\t\t#PARAMS[""MDAnnealT0""] = 300.0\n\t\t#PARAMS[""MDAnnealSteps""] = 1000\n\t\t#anneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\t#anneal.Prop()\n\t\t#m.coords = anneal.Minx.copy()\n\t\t#m.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 30.0\n\t\tPARAMS[""MDAnnealT0""] = 0.1\n\t\tPARAMS[""MDAnnealSteps""] = 10000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.Minx.copy()\n\t\tm.WriteXYZfile(""./results/"", ""Anneal_opt"")\n\t\tPARAMS[""MDThermostat""] = None\n\t\tPARAMS[""MDTemp""] = 0\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 40000\n\t\tmd = IRTrajectory(EnAndForce, ChargeField, m, ""water_10_IR"", anneal.v)\n\t\tmd.Prop()\n\t\tWriteDerDipoleCorrelationFunction(md.mu_his)\ndef GetOldKuns(a):\n\t# Prepare the force field.\n\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""NeuronType""] = ""relu""\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Poly_Width""] = 4.6\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 101\n\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\tPARAMS[""test_freq""] = 1\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""GradScalar""] = 1.0/20.0\n\tPARAMS[""DipoleScaler""]=1.0\n\tPARAMS[""NeuronType""] = ""relu""\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\t#PARAMS[""Erf_Width""] = 1.0\n\t#PARAMS[""Poly_Width""] = 4.6\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t#PARAMS[""AN1_num_r_Rs""] = 64\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""learning_rate_dipole""] = 0.0001\n\tPARAMS[""learning_rate_energy""] = 0.00001\n\tPARAMS[""SwitchEpoch""] = 15\n\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_with_prontonated_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu"",False,False)\ndef GetKunsWithDropout(a):\n\tTreatedAtoms = a.AtomTypes()\n\tPARAMS[""NetNameSuffix""] = """"\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 101\n\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\tPARAMS[""test_freq""] = 1\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""EnergyScalar""] = 1.0\n\tPARAMS[""GradScalar""] = 1.0/20.0\n\tPARAMS[""DipoleScaler""]=1.0\n\tPARAMS[""NeuronType""] = ""relu""\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\tPARAMS[""learning_rate_dipole""] = 0.0001\n\tPARAMS[""learning_rate_energy""] = 0.00001\n\tPARAMS[""SwitchEpoch""] = 15\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_with_prontonated_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\treturn manager\ndef GetKunsSmooth(a):\n\tTreatedAtoms = a.AtomTypes()\n\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100""\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 101\n\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\tPARAMS[""test_freq""] = 1\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""EnergyScalar""] = 1.0\n\tPARAMS[""GradScalar""] = 1.0/20.0\n\tPARAMS[""DipoleScaler""]=1.0\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = False\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\tPARAMS[""learning_rate_dipole""] = 0.0001\n\tPARAMS[""learning_rate_energy""] = 0.00001\n\tPARAMS[""SwitchEpoch""] = 15\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_with_prontonated_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_act_sigmoid100_rightalpha_dropout"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\t#manager=TFMolManage(""Mol_H2O_wb97xd_1to21_with_prontonated_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_act_sigmoid100_rightalpha_dropout07"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\treturn manager\ndef GetKunsSmoothNoDropout(a):\n\tTreatedAtoms = a.AtomTypes()\n\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100""\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 101\n\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\tPARAMS[""test_freq""] = 1\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""EnergyScalar""] = 1.0\n\tPARAMS[""GradScalar""] = 1.0/20.0\n\tPARAMS[""DipoleScaler""]=1.0\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0]\n\tPARAMS[""learning_rate_dipole""] = 0.0001\n\tPARAMS[""learning_rate_energy""] = 0.00001\n\tPARAMS[""SwitchEpoch""] = 15\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_with_prontonated_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_act_sigmoid100_rightalpha_nodropout"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\treturn manager\n\ndef GetJohns():\n\t""""""\n\tReturns a routine mapping mols to energy.\n\t""""""\n\tPARAMS[""RBFS""] = np.stack((np.linspace(0.1, 5.0, 32), np.repeat(0.25, 32)), axis=1)\n\tPARAMS[""SH_NRAD""] = 32\n\tPARAMS[""SH_LMAX""] = 4\n\tPARAMS[""NeuronType""] = \'elu\'\n\tPARAMS[""tf_prec""] = \'tf.float32\'\n\tmanager = TFMolManageDirect(name=""BehlerParinelloDirectGauSH_H2O_wb97xd_1to21_with_prontonated_Mon_Nov_13_11.35.07_2017"", network_type = ""BehlerParinelloDirectGauSH"")\n\treturn manager\n\ndef TestJohnWater():\n\ta = MSet()\n\ta.mols.append(Mol(np.array([1,1,8]),np.array([[0.9,0.1,0.1],[1.,0.9,1.],[0.1,0.1,0.1]])))\n\tm = a.mols[0]\n\tmanager = GetJohns()\n\tdef GetAPeriodicForce(m_):\n\t\tdef Frc(x_,DoForce=True):\n\t\t\ttmp = manager.evaluate_mol(Mol(m_.atoms,x_),DoForce)\n\t\t\tif (DoForce):\n\t\t\t\treturn tmp[0],tmp[1]*JOULEPERHARTREE\n\t\t\telse:\n\t\t\t\treturn tmp\n\t\treturn Frc\n\tPARAMS[""OptMaxCycles""]=60\n\tF = GetAPeriodicForce(m)\n\tprint(""F at 0"", F(m.coords))\n\tOpt = GeomOptimizer(F)\n\ta.mols[-1] = Opt.Opt(a.mols[-1])\n\tm = a.mols[-1]\n\t# Tesselate that water to create a box\n\tntess = 4\n\tlatv = 2.8*np.eye(3)\n\t# Start with a water in a ten angstrom box.\n\tlat = Lattice(latv)\n\tmc = lat.CenteredInLattice(m)\n\tmt = Mol(*lat.TessNTimes(mc.atoms,mc.coords,ntess))\n\tnreal = mt.NAtoms()\n\tmt.Distort(0.01)\n\tPARAMS[""OptMaxCycles""]=100\n\tF = GetAPeriodicForce(mt)\n\tOpt = GeomOptimizer(F)\n\tmt = Opt.Opt(mt,""UCopt"")\n\ndef BoxAndDensity():\n\t# Prepare a Box of water at a desired density\n\t# from a rough water molecule.\n\ta = MSet()\n\ta.mols.append(Mol(np.array([1,1,8]),np.array([[0.9,0.1,0.1],[1.,0.9,1.],[0.1,0.1,0.1]])))\n\tm = a.mols[0]\n\tmanager = GetKunsSmoothNoDropout(a)\n\tdef EnAndForceAPeriodic(x_):\n\t\t""""""\n\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t""""""\n\t\tmtmp = Mol(m.atoms,x_)\n\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t#print(""EnAndForceAPeriodic: "", en,f)\n\t\treturn en[0], f[0]\n\tdef EnAndForce(z_, x_, nreal_, DoForce = True):\n\t\t""""""\n\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t""""""\n\t\tmtmp = Mol(z_,x_)\n\t\tif (DoForce):\n\t\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], nreal_,True)\n\t\t\treturn en[0], f[0]\n\t\telse:\n\t\t\ten = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], nreal_, True, DoForce)\n\t\t\treturn en[0]\n\tif 0:\n\t\t# opt the first water.\n\t\tPARAMS[""OptMaxCycles""]=60\n\t\tOpt = GeomOptimizer(EnAndForceAPeriodic)\n\t\ta.mols[-1] = Opt.Opt(a.mols[-1])\n\t\tm = a.mols[-1]\n\n\t\t# Tesselate that water to create a box\n\t\tntess = 4\n\t\tlatv = 2.8*np.eye(3)\n\t\t# Start with a water in a ten angstrom box.\n\t\tlat = Lattice(latv)\n\t\tmc = lat.CenteredInLattice(m)\n\t\tmt = Mol(*lat.TessNTimes(mc.atoms,mc.coords,ntess))\n\t\tnreal = mt.NAtoms()\n\t\tmt.Distort(0.01)\n\t\tdef EnAndForceAPeriodic(x_):\n\t\t\t""""""\n\t\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t\t""""""\n\t\t\tmtmp = Mol(mt.atoms,x_)\n\t\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], mt.NAtoms())\n\t\t\t#print(""EnAndForceAPeriodic: "", en,f)\n\t\t\treturn en[0], f[0]\n\n\tif 0:\n\t\tPARAMS[""OptMaxCycles""]=100\n\t\tOpt = GeomOptimizer(EnAndForceAPeriodic)\n\t\tmt = Opt.Opt(mt,""UCopt"")\n\n\tif 0:\n\t\t# Anneal the tesselation.\n\t\tEnAndForceAPeriodic = lambda x_: EnAndForce(mt.atoms,x_,mt.NAtoms())\n\t\tPARAMS[""MDAnnealT0""] = 20.0\n\t\tPARAMS[""MDAnnealSteps""] = 200\n\t\taper = Annealer(EnAndForceAPeriodic,None,mt)\n\t\taper.Prop()\n\t\tmt.coords = aper.Minx\n\n\tif 0:\n\t\ts = MSet(""water64"")\n\t\ts.ReadXYZ()\n\t\tmt = s.mols[0]\n\t\t# Optimize the tesselated system.\n\t\tlat0 = (np.max(mt.coords)-np.min(mt.coords)+0.5)*np.eye(3)\n\t\tlat0[0,1] = 0.01\n\t\tlat0[1,0] -= 0.01\n\t\tlat0[0,2] = 0.01\n\t\tlat0[2,0] -= 0.01\n\t\tm = Lattice(lat0).CenteredInLattice(mt)\n\telif 1:\n\t\ts = MSet(""water64"")\n\t\ts.ReadXYZ()\n\t\tm = s.mols[-1]\n\t\tm.properties[""Lattice""] = np.eye(3)*12.42867\n\t\t# try a huge supercell\n\t\tif 1:\n\t\t\tntess = 5\n\t\t\tlatv = np.eye(3)*12.42867\n\t\t\t# Start with a water in a ten angstrom box.\n\t\t\tlat = Lattice(latv)\n\t\t\tm = Mol(*lat.TessNTimes(m.atoms,m.coords,ntess))\n\t\t\tm.properties[""Lattice""] = np.eye(3)*ntess*12.42867\n\n\t\t\tdef EnAndForceAPeriodic(x_):\n\t\t\t\t""""""\n\t\t\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t\t\t""""""\n\t\t\t\tmtmp = Mol(m.atoms,x_)\n\t\t\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t\t\t#print(""EnAndForceAPeriodic: "", en,f)\n\t\t\t\treturn en[0], f[0]\n\n\t\t\tanneal = Annealer(EnAndForceAPeriodic, None, m, ""Anneal"")\n\t\t\tanneal.Prop()\n\n\telse:\n\t\tPARAMS[""OptMaxCycles""]=60\n\t\tOpt = GeomOptimizer(EnAndForceAPeriodic)\n\t\ta.mols[-1] = Opt.Opt(a.mols[-1])\n\t\tm = a.mols[-1]\n\t\t# Tesselate that water to create a box\n\t\tntess = 4\n\t\tlatv = 2.8*np.eye(3)\n\t\t# Start with a water in a ten angstrom box.\n\t\tlat = Lattice(latv)\n\t\tmc = lat.CenteredInLattice(m)\n\t\tmt = Mol(*lat.TessNTimes(mc.atoms,mc.coords,ntess))\n\t\tnreal = mt.NAtoms()\n\t\tmt.Distort(0.01)\n\t\tm = mt\n\t\tm.properties[""Lattice""] = np.eye(3)*12.42867\n\n\tPF = PeriodicForce(m,m.properties[""Lattice""])\n\tPF.BindForce(EnAndForce, 12.0)\n\tPF.RDF(m.coords,8,8,20.0,0.01,""RDF0"")\n\tprint(""Original Density, Lattice: "", PF.Density(), PF.lattice.lattice)\n\n\t# Test that the energy is invariant to translations of atoms through the cell.\n\tif 0:\n\t\tfor i in range(4):\n\t\t\tprint(""En0:"", PF(m.coords)[0])\n\t\t\tm.coords += (np.random.random((1,3))-0.5)*3.0\n\t\t\tm.coords = PF.lattice.ModuloLattice(m.coords)\n\t\t\tprint(""En:""+str(i), PF(m.coords)[0])\n\t\t\t#Mol(*PF.lattice.TessLattice(m.atoms,m.coords,12.0)).WriteXYZfile(""./results/"", ""TessCHECK"")\n\tif 0:\n\t\t# Try optimizing that....\n\t\tPARAMS[""OptMaxCycles""]=100\n\t\tPOpt = PeriodicGeomOptimizer(PF)\n\t\tm = POpt.OptToDensity(m,1.0)\n\t\t#m = POpt.OptToDensity(m)\n\t\t#m = POpt.Opt(m)\n\t\tPF.mol0.coords = m.coords\n\t\tPF.mol0.properties[""Lattice""] = PF.lattice.lattice.copy()\n\t\tPF.mol0.WriteXYZfile(""./results"", ""Water64"", ""w"", wprop=True)\n\tif 0:\n\t\tPARAMS[""MDAnnealT0""] = 20.0\n\t\tPARAMS[""MDAnnealTF""] = 300.0\n\t\tPARAMS[""MDAnnealSteps""] = 10\n\t\tPARAMS[""MDdt""] = 0.3\n\t\ttraj = PeriodicAnnealer(PF,""PeriodicWarm"")\n\t\ttraj.Prop()\n\t\tPF.mol0.coords = traj.Minx\n\n\tif 0:\n\t\tPARAMS[""MDTemp""] = 330.0\n\t\tPARAMS[""MDMaxStep""] = 100000\n\t\ttraj = PeriodicMonteCarlo(PF,""PeriodicWaterMC"")\n\t\ttraj.Prop()\n\n\tif 0:\n\t\tPARAMS[""MDAnnealT0""] = 20.0\n\t\tPARAMS[""MDAnnealTF""] = 300.0\n\t\tPARAMS[""MDAnnealSteps""] = 1000\n\t\ttraj = PeriodicAnnealer(PF,""PeriodicWarm"")\n\t\ttraj.Prop()\n\n\t# Finally do thermostatted MD.\n\tPARAMS[""MDTemp""] = 300.0\n\tPARAMS[""MDdt""] = 0.05 # In fs.\n\ttraj = PeriodicVelocityVerlet(PF,""PeriodicWaterMD"")\n\ttraj.Prop()\ndef TestSmoothIR():\n\t# Prepare a Box of water at a desired density\n\t# from a rough water molecule.\n\n\ta=MSet(""H2O_cluster_meta"", center_=False)\n\ta.ReadXYZ(""H2O_cluster_meta"")\n\t#a = MSet()\n\t#a.mols.append(Mol(np.array([1,1,8,1,1,8]),np.array([[0.9,0.1,0.1],[1.,0.9,1.],[0.1,0.1,0.1],[2.9,0.1,0.1],[3.,0.9,1.],[2.1,0.1,0.1]])))\n\t#a.mols.append(Mol(np.array([1,1,8]),np.array([[0.9,0.1,0.1],[1.,0.9,1.],[0.1,0.1,0.1]])))\n\tm = a.mols[9]\n\tmanager = GetKunsSmoothNoDropout(a)\n\t#manager = GetKunsSmooth(a)\n\t#def EnAndForceAPeriodic(x_, DoForce = True):\n\t#\t""""""\n\t#\tThis is the primitive form of force routine required by PeriodicForce.\n\t#\t""""""\n\t#\tmtmp = Mol(m.atoms,x_)\n\t#\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t#\tenergy = Etotal[0]\n\t#\tforce = gradient[0]\n\t#\treturn energy, force\n\t#def EnergyField(x_):\n\t#\treturn EnAndForceAPeriodic(x_,False)[0]\n\t#\n\tdef EnAndForceAPeriodic(x_,DoForce=True):\n\t\t""""""\n\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t""""""\n\t\tmtmp = Mol(m.atoms,x_)\n\t\tif (DoForce):\n\t\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms(),True, DoForce)\n\t\t\treturn en[0], f[0]\n\t\telse:\n\t\t\ten = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms(), True, DoForce)\n\t\t\treturn en[0]\n\tdef EnergyField(x_):\n\t\treturn EnAndForceAPeriodic(x_,False)\n\tdef EnAndForce(z_, x_, nreal_, DoForce = True):\n\t\t""""""\n\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t""""""\n\t\tmtmp = Mol(z_,x_)\n\t\tif (DoForce):\n\t\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], nreal_,True, DoForce)\n\t\t\treturn en[0], f[0]\n\t\telse:\n\t\t\ten = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], nreal_, True, DoForce)\n\t\t\treturn en[0]\n\t# opt the first water.\n\tPARAMS[""OptMaxCycles""]=1\n\tOpt = GeomOptimizer(EnAndForceAPeriodic)\n\tm = Opt.Opt(m)\n\treturn\n\t#m = a.mols[-1]\n\tmasses = np.array(list(map(lambda x: ATOMICMASSESAMU[x-1],m.atoms)))\n\tw,v = HarmonicSpectra(EnergyField, m.coords, m.atoms)\n\treturn\n\tPYSCFFIELD = lambda x: PyscfDft(Mol(m.atoms,x))\n\tQCHEMFIELD = lambda x: QchemDFT(Mol(m.atoms,x))\n\tHarmonicSpectra(PYSCFFIELD, m.coords, m.atoms,None,0.005)\n\texit(0)\ndef TestNeb():\n\ta = MSet(""water3"")\n\ta.ReadXYZ()\n\t#a.mols.append(Mol(np.array([1,1,8,1,1,8]),np.array([[0.9,0.1,0.1],[0.1,0.9,.1],[0.1,0.1,0.1],[-.6,-.6,.1],[0.,0.9,6.1],[0.1,0.1,6.1]])))\n\t#a.mols.append(Mol(np.array([1,1,8,1,1,8]),np.array([[0.9,0.1,0.1],[0.1,0.9,.1],[0.1,0.1,0.1],[-.6,-.6,6.1],[0.,0.9,6.1],[0.1,0.1,6.1]])))\n\t#manager = GetKunsSmooth(a)\n\tmanager =GetKunsSmoothNoDropout(a)\n\tm = a.mols[0]\n\n\tdef EnAndForceAPeriodic(x_, DoForce = True):\n\t\t""""""\n\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t""""""\n\t\tmtmp = Mol(m.atoms,x_)\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\tenergy = Etotal[0]\n\t\tforce = gradient[0]\n\t\treturn energy, force\n\tdef EnergyField(x_):\n\t\treturn EnAndForceAPeriodic(x_,False)[0]\n\t#def EnAndForceAPeriodic(x_,DoForce=True):\n\t#\t""""""\n\t#\tThis is the primitive form of force routine required by PeriodicForce.\n\t#\t""""""\n\t#\tmtmp = Mol(m.atoms,x_)\n\t#\tif (DoForce):\n\t#\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms(),True, DoForce)\n\t#\t\treturn en[0], f[0]\n\t#\telse:\n\t#\t\ten = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms(), True, DoForce)\n\t#\t\treturn en[0]\n\t#def EnergyField(x_):\n\t#\treturn EnAndForceAPeriodic(x_,False)\n\tdef EnAndForce(z_, x_, nreal_, DoForce = True):\n\t\t""""""\n\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t""""""\n\t\tmtmp = Mol(z_,x_)\n\t\tif (DoForce):\n\t\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], nreal_,True, DoForce)\n\t\t\treturn en[0], f[0]\n\t\telse:\n\t\t\ten = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], nreal_, True, DoForce)\n\t\t\treturn en[0]\n\t# opt the first water dimer.\n\t#PARAMS[""OptMaxCycles""]=200\n\t#Opt = GeomOptimizer(EnAndForceAPeriodic)\n\t#a.mols[0] = Opt.Opt(a.mols[0],""1"")\n\t#a.mols[1] = Opt.Opt(a.mols[1],""2"")\n\tPARAMS[""OptMaxCycles""]=2000\n\tPARAMS[""NebSolver""]=""SD""\n\tPARAMS[""MaxBFGS""] = 12\n\tneb = NudgedElasticBand(EnAndForceAPeriodic,a.mols[0],a.mols[1])\n\tBeads = neb.Opt()\n\texit(0)\n\ndef TestEE():\n\ta = MSet(""a"")\n\ta.ReadXYZ()\n\tmanager =GetKunsSmoothNoDropout(a)\n\tm = a.mols[0]\n\tdef EnAndForceAPeriodic(x_, DoForce = True):\n\t\t""""""\n\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t""""""\n\t\tmtmp = Mol(m.atoms,x_)\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\tenergy = Etotal[0]\n\t\tforce = gradient[0]\n\t\treturn energy, force\n\te1 = QchemDFT(a.mols[0],basis_ = \'6-311g**\',xc_=\'wB97X-D\', jobtype_=\'sp\', filename_=\'tmp\', path_=\'./qchem/\', threads=False)\n\n#TrainPrepare()\n#Train()\n#Eval()\n#BoxAndDensity()\n#TestSmoothIR()\n#BoxAndDensity()\n#TestSmoothIR()\n#TestNeb()\n#TestJohnWater()\nTestEE()\n'"
samples/test_h2o_peri.py,2,"b'from __future__ import absolute_import\n#import memory_util\n#memory_util.vlog(1)\nfrom TensorMol import *\nfrom MolEmb import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nfrom TensorMol.MBE.NN_MBE import *\nfrom TensorMol.Interfaces.TMIPIinterface import *\nimport random\n\ndef TestPeriodicLJVoxel():\n\t""""""\n\tTests a Simple Periodic optimization.\n\tTrying to find the HCP minimum for the LJ crystal.\n\t""""""\n\tif (0):\n\t\ta=MSet(""water_tiny"", center_=False)\n\t\ta.ReadXYZ(""water_tiny"")\n\t\tm = a.mols[-1]\n\t\tm.coords = m.coords - np.min(m.coords) + 3.4\n\t\tTreatedAtoms = a.AtomTypes()\n\n\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw"",False,False)\n\n\t\tcellsize = 9.3215\n\t\tm.coords = np.mod(m.coords, cellsize)\n\t\tlat = cellsize*np.eye(3)\n\t\tPF = TFPeriodicVoxelForce(15.0,lat)\n\n\t\tzp = np.zeros(m.NAtoms()*PF.tess.shape[0], dtype=np.int32)\n\t\txp = np.zeros((m.NAtoms()*PF.tess.shape[0], 3))\n\t\tfor i in range(0, PF.tess.shape[0]):\n\t\t\tzp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.atoms\n\t\t\txp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.coords + cellsize*PF.tess[i]\n\t\tm_periodic = Mol(zp, xp)\n\t\toutput =  manager.EvalBPDirectEEUpdateSinglePeriodic(m_periodic, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\tprint (""energy:"", output[0])#, "" gradient:"", -output[-1]/JOULEPERHARTREE)\n\n\n\t\tdef EnAndForce(x_):\n\t\t\tx_ = np.mod(x_, cellsize)\n\t\t\txp = np.zeros((m.NAtoms()*PF.tess.shape[0], 3))\n\t\t\tfor i in range(0, PF.tess.shape[0]):\n\t\t\t\txp[i*m.NAtoms():(i+1)*m.NAtoms()] = x_ + cellsize*PF.tess[i]\n\t\t\tm_periodic.coords = xp\n\t\t\tm_periodic.coords[:m.NAtoms()] = x_\n\t\t\tEtotal, gradient  = manager.EvalBPDirectEEUpdateSinglePeriodic(m_periodic, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tprint (""energy:"", energy)\n\t\t\treturn energy, force\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\t#EnergyForceField(m.coords)\n\t\t#return\n\t\tPARAMS[""OptMaxCycles""]=200\n\t\tOpt = GeomOptimizer(EnergyForceField)\n\t\tm=Opt.Opt(m)\n\t\t#return\n\n\t\tPARAMS[""MDdt""] = 0.2\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDMaxStep""] = 2000\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDAnnealTF""] = 1\n\t\tPARAMS[""MDAnnealT0""] = 300.0\n\t\tPARAMS[""MDAnnealSteps""] = 2000\n\t\tanneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\tanneal.Prop()\n\t\tm.coords = anneal.Minx.copy()\n\t\tm.WriteXYZfile(fname=""H2O_tiny_opt"")\n\t\treturn\n\t\t#interface = TMIPIManger(EnergyForceField, TCP_IP=""localhost"", TCP_PORT= 31415)\n\t\t#interface.md_run()\n\t\t#\n\t\t#return\n\t\tPARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDTemp""] = 300\n\t\tPARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\tmd = VelocityVerlet(None, m, ""water_peri_10cut"",EnergyForceField)\n\t\tmd.Prop()\n\t\treturn\n\n\tif (1):\n\t\t#a=MSet(""H2O_cluster_meta"", center_=False)\n\t\t#a.ReadXYZ(""H2O_cluster_meta"")\n\t\t#a=MSet(""water_tiny"", center_=False)\n\t\t#a.ReadXYZ(""water_tiny"")\n\t\ta=MSet(""water_small"", center_=False)\n\t\ta.ReadXYZ(""water_small"")\n\t\tm = a.mols[-1]\n\t\tm.coords = m.coords - np.min(m.coords) + 3.4\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\t#PARAMS[""Poly_Width""] = 4.6\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_with_prontonated_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu"",False,False)\n\t\t#print np.sum(manager.EvalBPDirectEEUpdateSinglePeriodic(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())[6])\n\t\t#print manager.EvalBPDirectEEUpdateSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\n\t\t#cellsize = 6.0\n\t\t#cellsize = 9.3215\n\t\tcellsize = 18.643\n\t\tm.coords = np.mod(m.coords, cellsize)\n\t\tlat = cellsize*np.eye(3)\n\t\tPF = TFPeriodicVoxelForce(15.0,lat)\n\t\t#zp, xp = PF(m.atoms,m.coords,lat)  # tessilation in TFPeriodic seems broken\n\n\t\t#PF.tess = np.array([[0,0,0]])\n\t\tzp = np.zeros(m.NAtoms()*PF.tess.shape[0], dtype=np.int32)\n\t\txp = np.zeros((m.NAtoms()*PF.tess.shape[0], 3))\n\t\tfor i in range(0, PF.tess.shape[0]):\n\t\t\tzp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.atoms\n\t\t\txp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.coords + cellsize*PF.tess[i]\n\t\t#print (zp.shape, xp)\n\t\tm_periodic = Mol(zp, xp)\n\t\t#m_periodic.WriteXYZfile(fname=""H2O_tiny_opt_peri"")\n\t\t#return\n\t\tdef EnAndForce(x_):\n\t\t\tx_ = np.mod(x_, cellsize)\n\t\t\txp = np.zeros((m.NAtoms()*PF.tess.shape[0], 3))\n\t\t\tfor i in range(0, PF.tess.shape[0]):\n\t\t\t\txp[i*m.NAtoms():(i+1)*m.NAtoms()] = x_ + cellsize*PF.tess[i]\n\t\t\tm_periodic.coords = xp\n\t\t\tm_periodic.coords[:m.NAtoms()] = x_\n\t\t\tEtotal, gradient  = manager.EvalBPDirectEEUpdateSinglePeriodic(m_periodic, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t\t#Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient  = manager.EvalBPDirectEEUpdateSinglePeriodic(m_periodic, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tprint (""energy:"", energy)\n\t\t\treturn energy, force\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\t#EnergyForceField(m.coords)\n\t\t#PARAMS[""OptMaxCycles""]=50\n\t\t#Opt = GeomOptimizer(EnergyForceField)\n\t\t#m=Opt.Opt(m)\n\n\t\t#PARAMS[""MDdt""] = 0.2\n\t\t#PARAMS[""RemoveInvariant""]=True\n\t\t#PARAMS[""MDMaxStep""] = 200\n\t\t#PARAMS[""MDThermostat""] = ""Nose""\n\t\t#PARAMS[""MDV0""] = None\n\t\t#PARAMS[""MDAnnealTF""] = 1.0\n\t\t#PARAMS[""MDAnnealT0""] = 300.0\n\t\t#PARAMS[""MDAnnealSteps""] = 2000\n\t\t#anneal = Annealer(EnergyForceField, None, m, ""Anneal"")\n\t\t#anneal.Prop()\n\t\t#m.coords = anneal.Minx.copy()\n\t\t#m.WriteXYZfile(fname=""H2O_small_opt"")\n\t\t#return\n\t\t#interface = TMIPIManger(EnergyForceField, TCP_IP=""localhost"", TCP_PORT= 31415)\n\t\t#interface.md_run()\n\t\t#\n\t\t#return\n                PARAMS[""MDThermostat""] = ""Nose""\n                PARAMS[""MDTemp""] = 300\n                PARAMS[""MDdt""] = 0.2\n                PARAMS[""RemoveInvariant""]=True\n                PARAMS[""MDV0""] = None\n                PARAMS[""MDMaxStep""] = 100000\n                md = VelocityVerlet(None, m, ""water_peri_15cut"",EnergyForceField)\n                md.Prop()\n\t\treturn\n\tif (0):\n\t\ta=MSet(""water_tiny"", center_=False)\n\t\ta.ReadXYZ(""water_tiny"")\n\t\tm = a.mols[-1]\n\t\tm.coords = m.coords - np.min(m.coords)\n\t\tprint(""Original coords:"", m.coords)\n\t\t# Generate a Periodic Force field.\n\t\tcellsize = 9.3215\n\t\tlat = cellsize*np.eye(3)\n\t\tPF = TFPeriodicVoxelForce(15.0,lat)\n\t\t#zp, xp = PF(m.atoms,m.coords,lat)  # tessilation in TFPeriodic seems broken\n\n\t\tzp = np.zeros(m.NAtoms()*PF.tess.shape[0], dtype=np.int32)\n\t\txp = np.zeros((m.NAtoms()*PF.tess.shape[0], 3))\n\t\tfor i in range(0, PF.tess.shape[0]):\n\t\t\tzp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.atoms\n\t\t\txp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.coords + cellsize*PF.tess[i]\n\n\n\t\t#print (zp, xp)\n\t\tt0 = time.time()\n\t\tNL = NeighborListSetWithImages(xp.reshape((1,-1,3)), np.array([zp.shape[0]]), np.array([m.NAtoms()]), True, True, zp.reshape((1,-1)), sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexPeriodic(4.6, 3.1, np.array([1,8]), np.array([[1,1],[1,8],[8,8]]))\n\t\tprint (""time cost:"", time.time() - t0)\n\t\tNLEE = NeighborListSetWithImages(xp.reshape((1,-1,3)), np.array([zp.shape[0]]), np.array([m.NAtoms()]), False, False,  None)\n\t\trad_eep = NLEE.buildPairs(15.0)\n\t\tprint (""time cost:"", time.time() - t0)\n\t\tprint (mil_j[:50], mil_jk[:50])\n\t\tprint (rad_p_ele.shape, rad_eep.shape,  ang_t_elep.shape)\n\t\treturn\n\t\tprint (ang_t_elep)\n\t\tcount = 0\n\t\tfor i in range (0, rad_p_ele.shape[0]):\n\t\t\tif rad_p_ele[i][1] == 0:\n\t\t\t\t#print (ang_t_elep[i])\n\t\t\t\tcount += 1\n\t\tprint (""count"", count)\n\t\tmp = Mol(zp, xp)\n\n\n\t\tmp.WriteXYZfile(fname=""H2O_peri_tiny"")\n\t\tcutoff = 4.6\n\n\t\tmostatom=[]\n\t\tmostatomele=[]\n\t\tfor i in range (0, m.NAtoms()):\n\t\t\taround = 0\n\t\t\tfor j in range (0, mp.NAtoms()):\n\t\t\t\tdist = np.sum(np.square(m.coords[i] - mp.coords[j]))**0.5\n\t\t\t\tif i!=j and  dist <  cutoff:\n\t\t\t\t\taround += 1\n\t\t\t\t\tif i == 7:\n\t\t\t\t\t\tmostatom.append(mp.coords[j])\n\t\t\t\t\t\tmostatomele.append(mp.atoms[j])\n\t\t\t\t\t\tprint (j, mp.coords[j])\n\t\t\tprint (around)\n\t\tmostatom = np.asarray(mostatom)\n\t\tmostatomele =  np.asarray(mostatomele, dtype=np.int32)\n\t\tmost_mol = Mol(mostatomele, mostatom)\n\t\tmost_mol.WriteXYZfile(fname=""H2O_most"")\n\t\t\t#for j in range(0, rad_p_ele.shape[0]):\n\t\t\t#\tprint (np.sum(np.square(m.coords[i] - mp.coords[int(rad_p_ele[j][1])]))**0.5)\n\t\treturn\n\ndef UnittoPeri():\n\ta=MSet(""MDTrajectorywater_tiny_real_dropout_train_withsigmoid100"", center_=False, path_=""./results/"")\n\ta.ReadXYZ(""MDTrajectorywater_tiny_real_dropout_train_withsigmoid100"")\n\tlat = 9.3215*4/3\n\tmaxtess = 2\n\tsteps = 1\n\tfor i in range(len(a.mols)-100, len(a.mols)):\n\t\tprint (""i:"", i)\n\t\tindex = i\n\t\tm = a.mols[index]\n\t\tzp = np.zeros(m.NAtoms()*((2*maxtess-1)**3), dtype=np.int32)\n\t\txp = np.zeros((m.NAtoms()*((2*maxtess-1)**3),3))\n\t\tntess = 0\n\t\tfor j in range(-maxtess+1, maxtess):\n\t\t\tfor k in range(-maxtess+1, maxtess):\n\t\t\t\tfor l in range(-maxtess+1, maxtess):\n\t\t\t\t\tzp[ntess*m.NAtoms():(ntess+1)*m.NAtoms()] = m.atoms\n\t\t\t\t\txp[ntess*m.NAtoms():(ntess+1)*m.NAtoms()] = m.coords + np.array([j*lat, k*lat, l*lat])\n\t\t\t\t\tntess += 1\n\t\tmp = Mol(zp, xp)\n\t\tmp.WriteXYZfile(fpath=""./datasets"", fname=""MDTrajectorywater_tiny_real_dropout_train_withsigmoid100_tessilated"")\n\ndef KickOutTrans():\n\ta=MSet(""H2O_wb97xd_1to21"")\n\ta.Load()\n\trandom.shuffle(a.mols)\n\tmaxsample = 0.01*len(a.mols)\n\tsampled = 0\n\tfor m in a.mols:\n\t\tcontain_trans = False\n\t\tif m.NAtoms() >= 27 and sampled < maxsample:\n\t\t\tnmol = m.NAtoms()/3\n\t\t\tHtodel = random.randint(0, nmol-1)\n\t\t\tOHtodel = random.randint(0, nmol-1)\n\t\t\tdist = np.sum(np.square(m.coords[OHtodel*3+2]-m.coords[Htodel*3]))**0.5\n\t\t\tif Htodel == OHtodel or dist < 4.0:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tToDel = [Htodel*3+2, OHtodel*3, OHtodel*3+1]\n\t\t\t\txb = []\n\t\t\t\tzb = []\n\t\t\t\tfor j in range(0, m.NAtoms()):\n\t\t\t\t\tif j not in ToDel:\n\t\t\t\t\t\txb.append(m.coords[j])\n\t\t\t\t\t\tzb.append(m.atoms[j])\n\t\t\t\tmb = Mol(np.asarray(zb, np.int32), np.asarray(xb))\n\t\t\t\tmb.WriteXYZfile(fname=""H2O_prontonated_opt_2"")\n\t\t\t\tsampled += 1\n\ndef GetRDF():\n\ta = MSet(""water_tiny_dropout_md"")\n\ta.ReadXYZ(""water_tiny_dropout_md"")\n\tm = a.mols[0]\n\tdr = 0.0001\n\tr_max = 9.0\n\tunit_num = 648/8\n\taccu_count = np.zeros((int(r_max/dr),2))\n\taccu_count[:,0] = np.arange(0, accu_count.shape[0])*dr\n\t#bin_count = np.zeros((int(r_max/dr),2))\n\t#bin_count[:,0] = np.arange(0, bin_count.shape[0])*dr\n\tfor mol_index in range(0, len(a.mols)-1, 1):\n\t\tm = a.mols[mol_index]\n\t\tfor i in range(0, unit_num):\n\t\t\tif m.atoms[i] == 8:\n\t\t\t\tprint mol_index, i\n\t\t\t\tfor j in range(0, m.NAtoms()):\n\t\t\t\t\tif i!=j and m.atoms[j] == 8:\n\t\t\t\t\t\tdist = np.sum(np.square(m.coords[i]-m.coords[j]))**0.5\n\t\t\t\t\t\tif dist < r_max:\n\t\t\t\t\t\t\tbin_index = int(dist/dr)\n\t\t\t\t\t\t\taccu_count[:bin_index,1] += 1\n\t\t\t\t\t\t\t#bin_count[bin_index,1] += 1\n\t#for i in range(0, bin_count.shape[0]):\n\t#\tr = i*dr+dr/2.0\n\t#\tbin_count[i,1] = bin_count[i,1]/(r**2)\n\t#np.savetxt(""OO_rdf_dropout.dat"", bin_count)\n\tnp.savetxt(""OO_accu_dropout.dat"", accu_count)\n#Make_DistMat_ForReal\n\ndef GetRDF_Update():\n\ta=MSet(""MDTrajectoryPeriodicWaterMD_Nose300_RigthAlpha_NoDropout_HalfEcc"", path_=""./results/"")\n\ta.ReadXYZ(""MDTrajectoryPeriodicWaterMD_Nose300_RigthAlpha_NoDropout_HalfEcc"")\n\n\tm = a.mols[-1]\n\tm.properties[""Lattice""] = np.eye(3)*12.42867\n\tPF = PeriodicForce(m,m.properties[""Lattice""])\n\tgi = PF.RDF(m.coords,8,8,10.0,0.01)\n\tgi2 = PF.RDF(m.coords,8,8,10.0,0.01)\n\tav = 1\n\tfor i in range(len(a.mols)/2, len(a.mols)):\n\t\t#gi += PF.RDF(a.mols[i].coords,8,8,10.0,0.01)\n\t\tgi2 += PF.RDF_inC(a.mols[i].coords,a.mols[i].atoms,12.42867,8,8,10.0, 0.01)\n\t\tav += 1\n\t\t#print(i,"" Gi: "",gi/av)\n\t\tif (i%100==0):\n\t\t\tprint(i,"" Gi: "",gi2/av)\n\t\t\t#np.savetxt(""./results/rdf_64_sigmoid_""+str(i)+"".txt"",gi2/av)\n\tnp.savetxt(""./results/rdf_OO_sigmoid100_rightalpha_nodropout_halfEcc_""+str(i)+""_longtime.txt"",gi2/av)\n\treturn\n\n\tdr = 0.001\n\tr_max = 10.0\n\tlat = 9.3215\n\tnatom = a.mols[0].NAtoms()\n\trdf_type = [8,1]\n\tbin_count = np.zeros((int(r_max/dr),2))\n\tbin_count[:,0] = np.arange(0, bin_count.shape[0])*dr\n\tprint (""bin_count:"", bin_count)\n\tmaxtess = 2\n\tfor mol_index in range(len(a.mols)/4, len(a.mols)):\n\t\tt = time.time()\n\t\t#print (""mol_index:"", mol_index)\n\t\tm = a.mols[mol_index]\n\t\trdf_index =  GetRDF_Bin(m.coords, m.atoms, r_max, dr, lat, 8, 1)\n\t\tbin_count[rdf_index, 1] += 1\n\t\t#zp = np.zeros(m.NAtoms()*((2*maxtess-1)**3), dtype=np.int32)\n\t\t#xp = np.zeros((m.NAtoms()*((2*maxtess-1)**3),3))\n\t\t#ntess = 0\n\t\t#for j in range(-maxtess+1, maxtess):\n\t\t#\tfor k in range(-maxtess+1, maxtess):\n\t\t#\t\tfor l in range(-maxtess+1, maxtess):\n\t\t#\t\t\tzp[ntess*m.NAtoms():(ntess+1)*m.NAtoms()] = m.atoms\n\t\t#\t\t\txp[ntess*m.NAtoms():(ntess+1)*m.NAtoms()] = m.coords + np.array([j*lat, k*lat, l*lat])\n\t\t#\t\t\tntess += 1\n\t\t#t_cstart = time.time()\n\t\t#dist_mat = Make_DistMat_ForReal(xp, natom)\n\t\t#print (""dist_mat:"", dist_mat)\n\t\t#print (""dist_time:"", time.time() - t_cstart)\n\t\t#for i in range(0, natom):\n\t\t#\tif zp[i] == rdf_type[0]:\n\t\t#\t\tfor j in range(0, xp.shape[0]):\n\t\t#\t\t\tif zp[j] == rdf_type[1] and i!=j:\n\t\t#\t\t\t\tdist = dist_mat[i][j]\n\t\t#\t\t\t\tif dist < r_max:\n\t\t#\t\t\t\t\tbin_index = int(dist/dr)\n\t\t#\t\t\t\t\tbin_count[bin_index,1] += 1\n\t\t#print (""time per case:"",time.time() - t)\n\tfor i in range(0, bin_count.shape[0]):\n\t\tr = i*dr+dr/2.0\n\t\tbin_count[i,1] = bin_count[i,1]/(r**2)\n\tnp.savetxt(""OH_rdf_real_dropout.dat"", bin_count)\n\n\n#TestPeriodicLJVoxel()\n#UnittoPeri()\n#KickOutTrans()\n#GetRDF()\nGetRDF_Update()\n'"
samples/test_ipi.py,1,"b'from __future__ import absolute_import\n#import memory_util\n#memory_util.vlog(1)\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nfrom TensorMol.MBE.NN_MBE import *\nfrom TensorMol.Interfaces.TMIPIinterface import *\nimport random\n\ndef TestIPI():\n\tif (1):\n\t\t#a=MSet(""H2O_cluster_meta"", center_=False)\n\t\t#a.ReadXYZ(""H2O_cluster_meta"")\n\t\ta=MSet(""water_tiny"", center_=False)\n\t\ta.ReadXYZ(""water_tiny"")\n\t\tm = a.mols[-1]\n\t\tm.coords = m.coords - np.min(m.coords) + 0.00001\n\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 101\n\t\tPARAMS[""batch_size""] =  150   # 40 the max min-batch size it can go without memory error for training\n\t\tPARAMS[""test_freq""] = 1\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""]=1.0\n\t\tPARAMS[""NeuronType""] = ""relu""\n\t\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\t#PARAMS[""Erf_Width""] = 1.0\n\t\tPARAMS[""Poly_Width""] = 4.6\n\t\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t\t#PARAMS[""AN1_num_r_Rs""] = 64\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\t#PARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001\n\t\tPARAMS[""learning_rate_energy""] = 0.00001\n\t\tPARAMS[""SwitchEpoch""] = 15\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage(""Mol_H2O_wb97xd_1to21_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_1"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw"",False,False)\n\t\tcellsize = 9.3215\n\t\tlat = cellsize*np.eye(3)\n\t\tPF = TFPeriodicVoxelForce(15.0,lat)\n\t\tzp = np.zeros(m.NAtoms()*PF.tess.shape[0], dtype=np.int32)\n\t\txp = np.zeros((m.NAtoms()*PF.tess.shape[0], 3))\n\t\tfor i in range(0, PF.tess.shape[0]):\n\t\t\tzp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.atoms\n\t\t\txp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.coords + cellsize*PF.tess[i]\n\n\t\tm_periodic = Mol(zp, xp)\n\n\t\tdef EnAndForce(x_):\n\t\t\tx_ = np.mod(x_, cellsize)\n\t\t\txp = np.zeros((m.NAtoms()*PF.tess.shape[0], 3))\n\t\t\tfor i in range(0, PF.tess.shape[0]):\n\t\t\t\txp[i*m.NAtoms():(i+1)*m.NAtoms()] = x_ + cellsize*PF.tess[i]\n\t\t\tm_periodic.coords = xp\n\t\t\tm_periodic.coords[:m.NAtoms()] = x_\n\t\t\tEtotal, gradient  = manager.EvalBPDirectEEUpdateSinglePeriodic(m_periodic, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], m.NAtoms())\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tprint (""energy:"", energy)\n\t\t\treturn energy, force\n\n\t\tForceField = lambda x: EnAndForce(x)[-1]\n\t\tEnergyField = lambda x: EnAndForce(x)[0]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\tinterface = TMIPIManger(EnergyForceField, TCP_IP=""localhost"", TCP_PORT= 31415)\n\t\tinterface.md_run()\n\t\treturn\n\nTestIPI()\n'"
samples/test_jeherr.py,57,"b'from TensorMol import *\nimport time\nimport random\nPARAMS[""max_checkpoints""] = 3\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\n# Takes two nearly identical crystal lattices and interpolates a core/shell structure, must be oriented identically and stoichiometric\ndef InterpolateGeometries():\n\ta=MSet(\'cspbbr3_tess\')\n\t#a.ReadGDB9Unpacked(path=\'/media/sdb2/jeherr/TensorMol/datasets/cspbbr3/pb_tess_6sc/\')\n\t#a.Save()\n\ta.Load()\n\tmol1 = a.mols[0]\n\tmol2 = a.mols[1]\n\tmol2.RotateX()\n\tmol1.AlignAtoms(mol2)\n\toptimizer = Optimizer(None)\n\toptimizer.Interpolate_OptForce(mol1, mol2)\n\tmol1.WriteXYZfile(fpath=\'./results/cspbbr3_tess\', fname=\'cspbbr3_6sc_pb_tess_goopt\', mode=\'w\')\n\t# mol2.WriteXYZfile(fpath=\'./results/cspbbr3_tess\', fname=\'cspbbr3_6sc_ortho_rot\', mode=\'w\')\n\ndef read_unpacked_set(set_name=""chemspider12"", paths=""/media/sdb2/jeherr/TensorMol/datasets/chemspider12/*/"", properties=[""name"", ""energy"", ""gradients"", ""dipole""]):\n\timport glob\n\ta=MSet(set_name)\n\tfor path in glob.iglob(paths):\n\t\ta.read_xyz_set_with_properties(paths, properties)\n\tprint len(a.mols), "" Molecules""\n\ta.Save()\n\ndef TrainKRR(set_ = ""SmallMols"", dig_ = ""GauSH"", OType_ =""Force""):\n\ta=MSet(""SmallMols_rand"")\n\ta.Load()\n\tTreatedAtoms = a.AtomTypes()\n\td = Digester(TreatedAtoms, name_=dig_,OType_ =OType_)\n\ttset = TensorData(a,d)\n\ttset.BuildTrainMolwise(""SmallMols"",TreatedAtoms)\n\tmanager=TFManage("""",tset,True,""KRR_sqdiff"")\n\treturn\n\ndef RandomSmallSet(set_, size_):\n\t"""""" Returns an MSet of random molecules chosen from a larger set """"""\n\tprint ""Selecting a subset of ""+str(set_)+"" of size ""+str(size_)\n\ta=MSet(set_)\n\ta.Load()\n\tb=MSet(set_+""_rand"")\n\tmols = random.sample(range(len(a.mols)), size_)\n\tfor i in mols:\n\t\tb.mols.append(a.mols[i])\n\tb.Save()\n\treturn b\n\ndef TestMetadynamics():\n\ta = MSet(""nicotine_opt"")\n\ta.ReadXYZ()\n\tm = a.mols[-1]\n\t# ForceField = lambda x: QchemDFT(Mol(m.atoms,x),basis_ = \'6-311g**\',xc_=\'wB97X-D\', jobtype_=\'force\', filename_=\'jmols2\', path_=\'./qchem/\', threads=8)\n\tmanager = TFMolManageDirect(name=""BehlerParinelloDirectSymFunc_nicotine_vib_Tue_Nov_21_09.11.26_2017"", network_type = ""BehlerParinelloDirectSymFunc"")\n\tdef force_field(coords):\n\t\tenergy, forces = manager.evaluate_mol(Mol(m.atoms, coords), True)\n\t\treturn energy, forces * JOULEPERHARTREE\n\tmasses = np.array(list(map(lambda x: ATOMICMASSESAMU[x-1],m.atoms)))\n\tprint ""Masses:"", masses\n\tPARAMS[""MDdt""] = 0.5\n\tPARAMS[""RemoveInvariant""]=True\n\tPARAMS[""MDMaxStep""] = 50000\n\tPARAMS[""MDThermostat""] = ""Andersen""\n\tPARAMS[""MDTemp""]= 300.0\n\tPARAMS[""MDV0""] = ""Thermal""\n\tPARAMS[""MetaMDBumpHeight""] = 0.00\n\tPARAMS[""MetaMDBumpWidth""] = 0.01\n\tmeta = MetaDynamics(force_field, m, EandF_=force_field)\n\tmeta.Prop()\n\ndef test_md():\n\tPARAMS[""RBFS""] = np.array([[0.35, 0.35], [0.70, 0.35], [1.05, 0.35], [1.40, 0.35], [1.75, 0.35], [2.10, 0.35], [2.45, 0.35],\n\t\t\t\t\t\t\t\t[2.80, 0.35], [3.15, 0.35], [3.50, 0.35], [3.85, 0.35], [4.20, 0.35], [4.55, 0.35], [4.90, 0.35]])\n\tPARAMS[""ANES""] = np.array([2.20, 1.0, 1.0, 1.0, 1.0, 2.55, 3.04, 3.44]) #pauling electronegativity\n\tPARAMS[""SH_NRAD""] = 14\n\tPARAMS[""SH_LMAX""] = 4\n\ta = MSet(""OptMols"")\n\ta.ReadXYZ()\n\tmol = a.mols[4]\n\tmanager=TFManage(Name_=""SmallMols_GauSH_fc_sqdiff_GauSH_direct"",Train_=False,NetType_=""fc_sqdiff_GauSH_direct"")\n\tforce_field = lambda x: manager.evaluate_mol_forces_direct(x)\n\tmasses = np.array(map(lambda x: ATOMICMASSESAMU[x-1], mol.atoms))\n\tprint ""Masses:"", masses\n\tPARAMS[""MDdt""] = 0.2\n\tPARAMS[""RemoveInvariant""]=True\n\tPARAMS[""MDMaxStep""] = 20000\n\tPARAMS[""MDThermostat""] = ""Nose""\n\tPARAMS[""MDTemp""]= 300.0\n\tmd = VelocityVerlet(force_field, mol)\n\tmd.Prop()\n\ndef TestTFBond():\n\ta=MSet(""chemspider_all_rand"")\n\ta.Load()\n\td = MolDigester(a.BondTypes(), name_=""CZ"", OType_=""AtomizationEnergy"")\n\ttset = TensorMolData_BPBond_Direct(a,d)\n\tmanager=TFMolManage("""",tset,True,""fc_sqdiff_BPBond_Direct"")\n\ndef TestTFGauSH():\n\ttf_precision = eval(PARAMS[""tf_prec""])\n\tTensorMol.RawEmbeddings.data_precision = tf_precision\n\tnp.set_printoptions(threshold=100000)\n\ta=MSet(""SmallMols_rand"")\n\ta.Load()\n\tmaxnatoms = a.MaxNAtoms()\n\tzlist = []\n\txyzlist = []\n\tlabelslist = []\n\tnatomlist = []\n\tfor i, mol in enumerate(a.mols):\n\t\tpaddedxyz = np.zeros((maxnatoms,3), dtype=np.float32)\n\t\tpaddedxyz[:mol.atoms.shape[0]] = mol.coords\n\t\tpaddedz = np.zeros((maxnatoms), dtype=np.int32)\n\t\tpaddedz[:mol.atoms.shape[0]] = mol.atoms\n\t\tpaddedlabels = np.zeros((maxnatoms, 3), dtype=np.float32)\n\t\tpaddedlabels[:mol.atoms.shape[0]] = mol.properties[""forces""]\n\t\txyzlist.append(paddedxyz)\n\t\tzlist.append(paddedz)\n\t\tlabelslist.append(paddedlabels)\n\t\tnatomlist.append(mol.NAtoms())\n\t\tif i == 999:\n\t\t\tbreak\n\txyzstack = tf.stack(xyzlist)\n\tzstack = tf.stack(zlist)\n\tlabelstack = tf.stack(labelslist)\n\tnatomstack = tf.stack(natomlist)\n\tgaussian_params = tf.Variable(PARAMS[""RBFS""], trainable=True, dtype=tf.float32)\n\tatomic_embed_factors = tf.Variable(PARAMS[""ANES""], trainable=True, dtype=tf.float32)\n\telements = tf.constant([1, 6, 7, 8], dtype=tf.int32)\n\ttmp = tf_gaussian_spherical_harmonics_channel(xyzstack, zstack, elements, gaussian_params, 4)\n\tsess = tf.Session()\n\tsess.run(tf.global_variables_initializer())\n\toptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\trun_metadata = tf.RunMetadata()\n\t# for i in range(a.mols[0].atoms.shape[0]):\n\t# \tprint a.mols[0].atoms[i], ""   "", a.mols[0].coords[i,0], ""   "", a.mols[0].coords[i,1], ""   "", a.mols[0].coords[i,2]\n\ttmp2 = sess.run(tmp, options=options, run_metadata=run_metadata)\n\tprint tmp2\n\t# print tmp2[1]\n\t# print tmp2.shape\n\t# print tmp3\n\tfetched_timeline = timeline.Timeline(run_metadata.step_stats)\n\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\twith open(\'timeline_step_tmp_tm_nocheck_h2o.json\', \'w\') as f:\n\t\tf.write(chrome_trace)\n\t# print tmp2[3].shape\n\t# print a.mols[0].atoms.shape\n\t# TreatedAtoms = a.AtomTypes()\n\t# d = Digester(TreatedAtoms, name_=""GauSH"", OType_=""Force"")\n\t# # tset = TensorData(a,d)\n\t# mol_ = a.mols[0]\n\t# print d.Emb(mol_, -1, mol_.coords[0], MakeOutputs=False)[0]\n\t# print mol_.atoms[0]\n\ndef test_gaussian_overlap():\n\tgaussian_params = tf.Variable(PARAMS[""RBFS""], trainable=True, dtype=tf.float32)\n\ttf_precision = eval(PARAMS[""tf_prec""])\n\tTensorMol.RawEmbeddings.data_precision = tf_precision\n\ttmp = tf_gaussian_overlap(gaussian_params)\n\tsess = tf.Session()\n\tsess.run(tf.global_variables_initializer())\n\ttmp2 = sess.run(tmp)\n\tprint tmp2\n\ndef train_forces_GauSH_direct(set_ = ""SmallMols""):\n\tPARAMS[""RBFS""] = np.array([[0.35, 0.35], [0.70, 0.35], [1.05, 0.35], [1.40, 0.35], [1.75, 0.35], [2.10, 0.35], [2.45, 0.35],[2.80, 0.35], [3.15, 0.35], [3.50, 0.35], [3.85, 0.35], [4.20, 0.35], [4.55, 0.35], [4.90, 0.35]])\n\tPARAMS[""ANES""] = np.array([2.20, 1.0, 1.0, 1.0, 1.0, 2.55, 3.04, 3.44]) #pauling electronegativity\n\tPARAMS[""SH_NRAD""] = 14\n\tPARAMS[""SH_LMAX""] = 4\n\tPARAMS[""HiddenLayers""] = [512, 512, 512, 512, 512, 512, 512]\n\tPARAMS[""max_steps""] = 20000\n\tPARAMS[""test_freq""] = 5\n\tPARAMS[""batch_size""] = 200\n\tPARAMS[""NeuronType""] = ""elu""\n\tPARAMS[""learning_rate""] = 0.0001\n\ta=MSet(set_)\n\ta.Load()\n\tTreatedAtoms = a.AtomTypes()\n\tprint ""Number of Mols: "", len(a.mols)\n\td = Digester(TreatedAtoms, name_=""GauSH"", OType_=""Force"")\n\ttset = TensorDataDirect(a,d)\n\tmanager=TFManage("""",tset,True,""fc_sqdiff_GauSH_direct"")\n\ndef test_tf_neighbor():\n\tnp.set_printoptions(threshold=100000)\n\ta=MSet(""SmallMols_rand"")\n\ta.Load()\n\tmaxnatoms = a.MaxNAtoms()\n\tzlist = []\n\txyzlist = []\n\tlabelslist = []\n\tfor i, mol in enumerate(a.mols):\n\t\tpaddedxyz = np.zeros((maxnatoms,3), dtype=np.float32)\n\t\tpaddedxyz[:mol.atoms.shape[0]] = mol.coords\n\t\tpaddedz = np.zeros((maxnatoms), dtype=np.int32)\n\t\tpaddedz[:mol.atoms.shape[0]] = mol.atoms\n\t\tpaddedlabels = np.zeros((maxnatoms, 3), dtype=np.float32)\n\t\tpaddedlabels[:mol.atoms.shape[0]] = mol.properties[""forces""]\n\t\txyzlist.append(paddedxyz)\n\t\tzlist.append(paddedz)\n\t\tlabelslist.append(paddedlabels)\n\t\tif i == 99:\n\t\t\tbreak\n\txyzstack = tf.stack(xyzlist)\n\tzstack = tf.stack(zlist)\n\tlabelstack = tf.stack(labelslist)\n\tgaussian_params = tf.Variable(PARAMS[""RBFS""], trainable=True, dtype=tf.float32)\n\tatomic_embed_factors = tf.Variable(PARAMS[""ANES""], trainable=True, dtype=tf.float32)\n\telement = tf.constant(1, dtype=tf.int32)\n\tr_cutoff = tf.constant(5.0, dtype=tf.float32)\n\telement_pairs = tf.constant([[1,1,1], [1,1,6], [1,1,7], [1,1,8], [1,6,6], [1,6,7], [1,6,8], [1,7,7], [1,7,8], [1,8,8],\n\t\t\t\t\t\t\t\t[6,6,6], [6,6,7], [6,6,8], [6,7,7], [6,7,8], [6,8,8], [7,7,7], [7,7,8], [7,8,8], [8,8,8]], dtype=tf.int32)\n\ttmp = tf_triples_list(xyzstack, zstack, r_cutoff, element_pairs)\n\tsess = tf.Session()\n\tsess.run(tf.global_variables_initializer())\n\toptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\trun_metadata = tf.RunMetadata()\n\t# for i in range(a.mols[0].atoms.shape[0]):\n\t# \tprint a.mols[0].atoms[i], ""   "", a.mols[0].coords[i,0], ""   "", a.mols[0].coords[i,1], ""   "", a.mols[0].coords[i,2]\n\ttmp3 = sess.run([tmp], options=options, run_metadata=run_metadata)\n\t# print tmp3\n\tfetched_timeline = timeline.Timeline(run_metadata.step_stats)\n\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\twith open(\'timeline_step_tmp_tm_nocheck_h2o.json\', \'w\') as f:\n\t\tf.write(chrome_trace)\n\tprint tmp3\n\t# print tmp4[1]\n\t# print tmp4\n\t# TreatedAtoms = a.AtomTypes()\n\t# d = Digester(TreatedAtoms, name_=""GauSH"", OType_=""Force"")\n\t# # tset = TensorData(a,d)\n\t# mol_ = a.mols[0]\n\t# print d.Emb(mol_, -1, mol_.coords[0], MakeOutputs=False)[0]\n\t# print mol_.atoms[0]\n\ndef train_energy_pairs_triples():\n\tPARAMS[""HiddenLayers""] = [512, 512, 512]\n\tPARAMS[""learning_rate""] = 0.0001\n\tPARAMS[""max_steps""] = 1000\n\tPARAMS[""test_freq""] = 5\n\tPARAMS[""batch_size""] = 200\n\tPARAMS[""NeuronType""] = ""relu""\n\t# PARAMS[""tf_prec""] = ""tf.float64""\n\t# PARAMS[""self.profiling""] = True\n\ta=MSet(""SmallMols"")\n\ta.Load()\n\tTreatedAtoms = a.AtomTypes()\n\tprint ""Number of Mols: "", len(a.mols)\n\td = Digester(TreatedAtoms, name_=""GauSH"", OType_=""AtomizationEnergy"")\n\ttset = TensorMolData_BP_Direct(a,d)\n\tmanager=TFMolManage("""",tset,True,""pairs_triples"", Trainable_=True)\n\ndef train_energy_symm_func(mset):\n\tPARAMS[""train_energy_gradients""] = False\n\tPARAMS[""weight_decay""] = None\n\tPARAMS[""HiddenLayers""] = [512, 512, 512]\n\tPARAMS[""learning_rate""] = 0.0001\n\tPARAMS[""max_steps""] = 1000\n\tPARAMS[""test_freq""] = 5\n\tPARAMS[""batch_size""] = 200\n\tPARAMS[""NeuronType""] = ""elu""\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\ta=MSet(mset)\n\ta.Load()\n\tprint ""Number of Mols: "", len(a.mols)\n\tmanager = TFMolManageDirect(a, network_type = ""BPSymFunc"")\n\ndef train_energy_GauSH(mset):\n\tPARAMS[""RBFS""] = np.stack((np.linspace(0.1, 6.0, 16), np.repeat(0.30, 16)), axis=1)\n\tPARAMS[""SH_NRAD""] = 16\n\tPARAMS[""SH_LMAX""] = 5\n\tPARAMS[""SH_rot_invar""] = False\n\tPARAMS[""EECutoffOn""] = 0.0\n\tPARAMS[""Elu_Width""] = 6.0\n\tPARAMS[""train_gradients""] = True\n\tPARAMS[""train_dipole""] = False\n\tPARAMS[""train_rotation""] = True\n\tPARAMS[""weight_decay""] = None\n\tPARAMS[""HiddenLayers""] = [512, 512, 512]\n\tPARAMS[""learning_rate""] = 0.00005\n\tPARAMS[""max_steps""] = 1000\n\tPARAMS[""test_freq""] = 5\n\tPARAMS[""batch_size""] = 100\n\tPARAMS[""NeuronType""] = ""shifted_softplus""\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""Profiling""] = False\n\tmanager = TFMolManageDirect(mset, network_type = ""BPGauSH"")\n\ndef test_h2o():\n\tPARAMS[""OptMaxCycles""]=60\n\tPARAMS[""OptMaxCycles""]=500\n\tPARAMS[""OptStepSize""] = 0.1\n\tPARAMS[""OptThresh""]=0.0001\n\tPARAMS[""MDAnnealT0""] = 20.0\n\tPARAMS[""MDAnnealSteps""] = 2000\n\ta = MSet(""water"")\n\t# a.ReadXYZ()\n\ta.mols.append(Mol(np.array([1,1,8]),np.array([[0.9,0.1,0.1],[1.,0.9,1.],[0.1,0.1,0.1]])))\n\tmol = a.mols[0]\n\tmanager = TFMolManageDirect(name=""BehlerParinelloDirectGauSH_H2O_wb97xd_1to21_with_prontonated_Mon_Nov_13_11.11.15_2017"", network_type = ""BehlerParinelloDirectGauSH"")\n\tdef force_field(mol, eval_forces=True):\n\t\tif eval_forces:\n\t\t\tenergy, forces = manager.evaluate_mol(mol, True)\n\t\t\tforces = RemoveInvariantForce(mol.coords, forces, mol.atoms)\n\t\t\treturn energy, forces\n\t\telse:\n\t\t\tenergy = manager.evaluate_mol(mol, False)\n\t\t\treturn energy\n\tOpt = GeometryOptimizer(force_field)\n\topt_mol = Opt.opt_conjugate_gradient(mol)\n\ndef evaluate_BPSymFunc(mset):\n\ta=MSet(mset)\n\ta.Load()\n\toutput, labels = [], []\n\tmanager = TFMolManageDirect(name=""BehlerParinelloDirectSymFunc_nicotine_metamd_10000_Tue_Nov_07_22.35.07_2017"", network_type = ""BehlerParinelloDirectSymFunc"")\n\trandom.shuffle(a.mols)\n\tbatch = []\n\tfor i in range(len(a.mols) / 100):\n\t\tfor j in range(100):\n\t\t\tlabels.append(a.mols[i*100+j].properties[""atomization""])\n\t\t\tbatch.append(a.mols[i*100+j])\n\t\toutput.append(manager.evaluate_batch(batch, eval_forces=False))\n\t\tbatch = []\n\toutput = np.concatenate(output)\n\tlabels = np.array(labels)\n\tprint ""MAE:"", np.mean(np.abs(output-labels))*627.509\n\tprint ""RMSE:"",np.sqrt(np.mean(np.square(output-labels)))*627.509\n\ndef water_dimer_plot():\n\tPARAMS[""RBFS""] = np.stack((np.linspace(0.1, 5.0, 32), np.repeat(0.25, 32)), axis=1)\n\tPARAMS[""SH_NRAD""] = 32\n\tPARAMS[""SH_LMAX""] = 4\n\tdef qchemdft(m_,ghostatoms,basis_ = \'6-31g*\',xc_=\'b3lyp\', jobtype_=\'force\', filename_=\'tmp\', path_=\'./qchem/\', threads=False):\n\t\tistring = \'$molecule\\n0 1 \\n\'\n\t\tcrds = m_.coords.copy()\n\t\tcrds[abs(crds)<0.0000] *=0.0\n\t\tfor j in range(len(m_.atoms)):\n\t\t\tif j in ghostatoms:\n\t\t\t\tistring=istring+""@""+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\t\t\telse:\n\t\t\t\tistring=istring+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\t\tif jobtype_ == ""dipole"":\n\t\t\tistring =istring + \'$end\\n\\n$rem\\njobtype sp\\nbasis \'+basis_+\'\\nmethod \'+xc_+\'\\nthresh 11\\nsymmetry false\\nsym_ignore true\\n$end\\n\'\n\t\telse:\n\t\t\tistring =istring + \'$end\\n\\n$rem\\njobtype \'+jobtype_+\'\\nbasis \'+basis_+\'\\nmethod \'+xc_+\'\\nthresh 11\\nsymmetry false\\nsym_ignore true\\n$end\\n\'\n\t\twith open(path_+filename_+\'.in\',\'w\') as fin:\n\t\t\tfin.write(istring)\n\t\twith open(path_+filename_+\'.out\',\'a\') as fout:\n\t\t\tif threads:\n\t\t\t\tproc = subprocess.Popen([\'qchem\', \'-nt\', str(threads), path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\t\telse:\n\t\t\t\tproc = subprocess.Popen([\'qchem\', path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\t\tout, err = proc.communicate()\n\t\t\tfout.write(out)\n\t\tlines = out.split(\'\\n\')\n\t\tif jobtype_ == \'force\':\n\t\t\tForces = np.zeros((m_.atoms.shape[0],3))\n\t\t\tfor i, line in enumerate(lines):\n\t\t\t\tif line.count(\'Convergence criterion met\')>0:\n\t\t\t\t\tEnergy = float(line.split()[1])\n\t\t\t\tif line.count(""Gradient of SCF Energy"") > 0:\n\t\t\t\t\tk = 0\n\t\t\t\t\tl = 0\n\t\t\t\t\tfor j in range(1, m_.atoms.shape[0]+1):\n\t\t\t\t\t\tForces[j-1,:] = float(lines[i+k+2].split()[l+1]), float(lines[i+k+3].split()[l+1]), float(lines[i+k+4].split()[l+1])\n\t\t\t\t\t\tl += 1\n\t\t\t\t\t\tif (j % 6) == 0:\n\t\t\t\t\t\t\tk += 4\n\t\t\t\t\t\t\tl = 0\n\t\t\t# return Energy, Forces\n\t\t\treturn Energy, -Forces*JOULEPERHARTREE/BOHRPERA\n\t\telif jobtype_ == \'sp\':\n\t\t\tfor line in lines:\n\t\t\t\tif line.count(\'Convergence criterion met\')>0:\n\t\t\t\t\tEnergy = float(line.split()[1])\n\t\t\treturn Energy\n\t\telse:\n\t\t\traise Exception(""jobtype needs formatted for return variables"")\n\n\ta = MSet(""water_dimer"")\n\ta.ReadXYZ()\n\tmanager = TFMolManageDirect(name=""BehlerParinelloDirectGauSH_H2O_wb97xd_1to21_with_prontonated_Mon_Nov_13_11.35.07_2017"", network_type = ""BehlerParinelloDirectGauSH"")\n\tqchemff = lambda x, y: qchemdft(x, y, basis_ = \'6-311g**\',xc_=\'wb97x-d\', jobtype_=\'sp\', filename_=\'tmp\', path_=\'./qchem/\', threads=8)\n\tcp_correction = []\n\tfor mol in a.mols:\n\t\th2o1 = qchemff(Mol(mol.atoms[:3], mol.coords[:3]), [])\n\t\th2o2 = qchemff(Mol(mol.atoms[3:], mol.coords[3:]), [])\n\t\t# h2o1cp = qchemff(mol, [3, 4, 5])\n\t\t# h2o2cp = qchemff(mol, [0, 1, 2])\n\t\tdimer = qchemff(mol, [])\n\t\t# cpc = h2o1cp - h2o1 + h2o2cp - h2o2\n\t\t# cp_correction.append(cpc)\n\t\tbond_e = dimer - h2o1 - h2o2\n\t\tprint ""{%.10f, %.10f},"" % (np.linalg.norm(mol.coords[1] - mol.coords[3]), bond_e * 627.509)\n\tprint ""TensorMol evaluation""\n\tfor i, mol in enumerate(a.mols):\n\t\th2o1 = manager.evaluate_mol(Mol(mol.atoms[:3], mol.coords[:3]), False)\n\t\th2o2 = manager.evaluate_mol(Mol(mol.atoms[3:], mol.coords[3:]), False)\n\t\tdimer = manager.evaluate_mol(mol, False)\n\t\tbond_e = dimer - h2o1 - h2o2\n\t\tprint ""{%.10f, %.10f},"" % (np.linalg.norm(mol.coords[1] - mol.coords[3]), bond_e * 627.509)\n\ndef nicotine_cc_stretch_plot():\n\tdef qchemdft(m_,ghostatoms,basis_ = \'6-31g*\',xc_=\'b3lyp\', jobtype_=\'force\', filename_=\'tmp\', path_=\'./qchem/\', threads=False):\n\t\tistring = \'$molecule\\n0 1 \\n\'\n\t\tcrds = m_.coords.copy()\n\t\tcrds[abs(crds)<0.0000] *=0.0\n\t\tfor j in range(len(m_.atoms)):\n\t\t\tif j in ghostatoms:\n\t\t\t\tistring=istring+""@""+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\t\t\telse:\n\t\t\t\tistring=istring+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\t\tif jobtype_ == ""dipole"":\n\t\t\tistring =istring + \'$end\\n\\n$rem\\njobtype sp\\nbasis \'+basis_+\'\\nmethod \'+xc_+\'\\nthresh 11\\nsymmetry false\\nsym_ignore true\\n$end\\n\'\n\t\telse:\n\t\t\tistring =istring + \'$end\\n\\n$rem\\njobtype \'+jobtype_+\'\\nbasis \'+basis_+\'\\nmethod \'+xc_+\'\\nthresh 11\\nsymmetry false\\nsym_ignore true\\n$end\\n\'\n\t\twith open(path_+filename_+\'.in\',\'w\') as fin:\n\t\t\tfin.write(istring)\n\t\twith open(path_+filename_+\'.out\',\'a\') as fout:\n\t\t\tif threads:\n\t\t\t\tproc = subprocess.Popen([\'qchem\', \'-nt\', str(threads), path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\t\telse:\n\t\t\t\tproc = subprocess.Popen([\'qchem\', path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\t\tout, err = proc.communicate()\n\t\t\tfout.write(out)\n\t\tlines = out.split(\'\\n\')\n\t\tif jobtype_ == \'force\':\n\t\t\tForces = np.zeros((m_.atoms.shape[0],3))\n\t\t\tfor i, line in enumerate(lines):\n\t\t\t\tif line.count(\'Convergence criterion met\')>0:\n\t\t\t\t\tEnergy = float(line.split()[1])\n\t\t\t\tif line.count(""Gradient of SCF Energy"") > 0:\n\t\t\t\t\tk = 0\n\t\t\t\t\tl = 0\n\t\t\t\t\tfor j in range(1, m_.atoms.shape[0]+1):\n\t\t\t\t\t\tForces[j-1,:] = float(lines[i+k+2].split()[l+1]), float(lines[i+k+3].split()[l+1]), float(lines[i+k+4].split()[l+1])\n\t\t\t\t\t\tl += 1\n\t\t\t\t\t\tif (j % 6) == 0:\n\t\t\t\t\t\t\tk += 4\n\t\t\t\t\t\t\tl = 0\n\t\t\t# return Energy, Forces\n\t\t\treturn Energy, -Forces*JOULEPERHARTREE/BOHRPERA\n\t\telif jobtype_ == \'sp\':\n\t\t\tfor line in lines:\n\t\t\t\tif line.count(\'Convergence criterion met\')>0:\n\t\t\t\t\tEnergy = float(line.split()[1])\n\t\t\treturn Energy\n\t\telse:\n\t\t\traise Exception(""jobtype needs formatted for return variables"")\n\n\ta = MSet(""nicotine_vib_collision"")\n\ta.ReadXYZ()\n\tmanager = TFMolManageDirect(name=""BehlerParinelloDirectSymFunc_nicotine_full_Fri_Nov_24_15.52.20_2017"", network_type = ""BehlerParinelloDirectSymFunc"")\n\tqchemff = lambda x, y: qchemdft(x, y, basis_ = \'6-311g**\',xc_=\'wb97x-d\', jobtype_=\'sp\', filename_=\'tmp\', path_=\'./qchem/\', threads=8)\n\tf=open(""vib_collision_energies.dat"", ""w"")\n\tfor i, mol in enumerate(a.mols):\n\t\t# energy = qchemff(mol, [])\n\t\tenergy = manager.evaluate_mol(mol, eval_forces=False)\n\t\tf.write(str(i * 0.06)+"" ""+str(energy[0] * 627.509)+""\\n"")\n\t# print ""TensorMol evaluation""\n\t# for i, mol in enumerate(a.mols):\n\t# \th2o1 = manager.evaluate_mol(Mol(mol.atoms[:3], mol.coords[:3]), False)\n\t# \th2o2 = manager.evaluate_mol(Mol(mol.atoms[3:], mol.coords[3:]), False)\n\t# \tdimer = manager.evaluate_mol(mol, False)\n\t# \tbond_e = dimer - h2o1 - h2o2\n\t# \tprint ""{%.10f, %.10f},"" % (np.linalg.norm(mol.coords[1] - mol.coords[3]), bond_e * 627.509)\n\ndef meta_statistics():\n\tPARAMS[""MDdt""] = 0.5 # In fs.\n\tPARAMS[""MDMaxStep""] = 20000\n\tPARAMS[""MetaBumpTime""] = 10.0\n\tPARAMS[""MetaMaxBumps""] = 1000\n\tPARAMS[""MetaBowlK""] = 0.0\n\tPARAMS[""MDThermostat""]=""Andersen""\n\tPARAMS[""MDTemp""]=300.0\n\tPARAMS[""MDV0""]=None\n\ta=MSet(""nicotine_opt"")\n\ta.ReadXYZ()\n\tm=a.mols[0]\n\tmanager=TFMolManageDirect(name=""BehlerParinelloDirectSymFunc_nicotine_full_Fri_Nov_24_15.52.20_2017"", network_type = ""BehlerParinelloDirectSymFunc"")\n\tdef force_field(coords):\n\t\tenergy, forces = manager.evaluate_mol(Mol(m.atoms, coords), True)\n\t\treturn energy, forces * JOULEPERHARTREE\n\t# for metap in [[0.5, 0.5], [1.0, 0.5], [0.5, 1.0], [1.0, 1.0], [0.5, 1.5], [1.5, 0.5], [1.5, 1.5], [1.0, 2.0], [2.0, 1.0], [2.0, 2.0], [3.0, 3.0]]:\n\tfor metap in [[0.0, 0.01]]:\n\t\tPARAMS[""MetaMDBumpHeight""] = metap[0]\n\t\tPARAMS[""MetaMDBumpWidth""] = metap[1]\n\t\ttraj = MetaDynamics(None, m,""MetaMD_nicotine_aimd_sample""+str(metap[0])+""_""+str(metap[1]), force_field)\n\t\ttraj.Prop()\n\ndef meta_stat_plot():\n\tfor metap in [[0.0, 0.01], [0.5, 2.0], [0.5, 0.5], [1.0, 1.0], [0.5, 1.5], [1.5, 0.5], [1.5, 1.5], [1.0, 2.0], [2.0, 1.0]]:\n\t\tf1=open(""nicotine_metastat_ehist_""+str(metap[0])+""_""+str(metap[1])+"".dat"", ""w"")\n\t\t# f2=open(""nicotine_metastat_evar_""+str(metap[0])+""_""+str(metap[1])+"".dat"", ""w"")\n\t\tf3=open(""nicotine_metastat_dvar_""+str(metap[0])+""_""+str(metap[1])+"".dat"", ""w"")\n\t\tf=open(""./results/MDLogMetaMD_nicotine_""+str(metap[0])+""_""+str(metap[1])+"".txt"", ""r"")\n\t\tlines = f.readlines()\n\t\tfor i, line in enumerate(lines):\n\t\t\tif i == 19501:\n\t\t\t\tbreak\n\t\t\tsline = line.split()\n\t\t\tf1.write(str(sline[7])+""\\n"")\n\t\t\t# f2.write(str(sline[0])+"" ""+str(sline[9])+""\\n"")\n\t\t\tf3.write(str(sline[0])+"" ""+str(sline[10])+""\\n"")\n\t\tf.close()\n\t\tf1.close()\n\t\t# f2.close()\n\t\tf3.close()\n\ndef harmonic_freq():\n\t# PARAMS[""RBFS""] = np.stack((np.linspace(0.1, 6.0, 16), np.repeat(0.35, 16)), axis=1)\n\t# PARAMS[""SH_NRAD""] = 16\n\t# PARAMS[""SH_LMAX""] = 4\n\t# PARAMS[""EECutoffOn""] = 0.0\n\t# PARAMS[""Elu_Width""] = 6.0\n\t# PARAMS[""HiddenLayers""] = [512, 512, 512]\n\t# PARAMS[""NeuronType""] = ""shifted_softplus""\n\t# PARAMS[""tf_prec""] = ""tf.float32""\n\t# manager = TFMolManageDirect(name=""BehlerParinelloDirectGauSH_H2O_wb97xd_1to21_with_prontonated_Mon_Dec_11_11.43.09_2017"", network_type = ""BehlerParinelloDirectGauSH"")\n\t# def GetChemSpiderNetwork(a, Solvation_=False):\n\t# \tTreatedAtoms = np.array([1,6,7,8], dtype=np.uint8)\n\t# \tPARAMS[""tf_prec""] = ""tf.float64""\n\t# \tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\t# \tPARAMS[""sigmoid_alpha""] = 100.0\n\t# \tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\t# \tPARAMS[""EECutoff""] = 15.0\n\t# \tPARAMS[""EECutoffOn""] = 0\n\t# \tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t# \tPARAMS[""EECutoffOff""] = 15.0\n\t# \tPARAMS[""AddEcc""] = True\n\t# \tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t# \td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t# \ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t# \tif Solvation_:\n\t# \t\tPARAMS[""DSFAlpha""] = 0.18\n\t# \t\tmanager=TFMolManage(""chemspider12_solvation"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\t# \telse:\n\t# \t\tPARAMS[""DSFAlpha""] = 0.18*BOHRPERA\n\t# \t\tmanager=TFMolManage(""chemspider12_nosolvation"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\t# \treturn manager\n\n\tPARAMS[""OptMaxCycles""]= 2000\n\tPARAMS[""OptThresh""] =0.0001\n\ta=MSet(""nicotine_opt_qcorder"")\n\ta.ReadXYZ()\n\t# m=a.mols[0]\n\tmanager = TFMolManageDirect(name=""BehlerParinelloDirectSymFunc_nicotine_vib_Tue_Nov_21_09.11.26_2017"", network_type = ""BehlerParinelloDirectSymFunc"")\n\t# dipole_manager = GetChemSpiderNetwork(a, False)\n\tdef force_field(coords, eval_forces=True):\n\t\tif eval_forces:\n\t\t\tenergy, forces = manager.evaluate_mol(Mol(a.mols[0].atoms, coords), True)\n\t\t\treturn energy, forces * JOULEPERHARTREE\n\t\telse:\n\t\t\tenergy = manager.evaluate_mol(Mol(a.mols[0].atoms, coords), False)\n\t\t\treturn energy\n\tdef energy_field(coords):\n\t\tenergy = manager.evaluate_mol(Mol(a.mols[0].atoms, coords), False)\n\t\treturn energy\n\t# def ChargeField(x_):\n\t# \tmtmp = Mol(m.atoms,x_)\n\t# \tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = dipole_manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t# \tenergy = Etotal[0]\n\t# \tforce = gradient[0]\n\t# \treturn atom_charge[0]\n\tdef dipole_field(coords):\n\t\tq = np.array([-0.355885, -0.306275, -0.138541, -0.129072, -0.199879,  0.092443, -0.073758,  0.004807, -0.280214,\n\t\t\t\t\t-0.207116, -0.201989,  0.060910,  0.142512,  0.138947,  0.136766,  0.118485\n\t\t\t\t\t, 0.101182,  0.127422, 0.123743,  0.136352, 0.126561,  0.111861,  0.118059, 0.121731,  0.107663, 0.123283])\n\t\t# q = np.asarray(ChargeField(coords))\n\t\tdipole = np.zeros(3)\n\t\tfor i in range(0, q.shape[0]):\n\t\t\tdipole += q[i]*coords[i]*BOHRPERA\n\t\treturn dipole\n\tOpt = GeomOptimizer(force_field)\n\tm=Opt.Opt(a.mols[0],""nicotine_nn_opt"")\n\tm.WriteXYZfile(""./results/"", ""optimized_nicotine"")\n\tmasses = np.array(map(lambda x: ATOMICMASSESAMU[x-1],m.atoms))\n\tw,v = HarmonicSpectra(energy_field, m.coords, m.atoms, WriteNM_=True, Mu_ = dipole_field)\n\ndef water_ir():\n\tPARAMS[""RBFS""] = np.stack((np.linspace(0.1, 6.0, 16), np.repeat(0.35, 16)), axis=1)\n\tPARAMS[""SH_NRAD""] = 16\n\tPARAMS[""SH_LMAX""] = 4\n\tPARAMS[""EECutoffOn""] = 0.0\n\tPARAMS[""Elu_Width""] = 6.0\n\tPARAMS[""HiddenLayers""] = [512, 512, 512]\n\tPARAMS[""NeuronType""] = ""shifted_softplus""\n\tPARAMS[""tf_prec""] = ""tf.float32""\n\tPARAMS[""OptMaxCycles""]= 2000\n\tPARAMS[""OptThresh""] =0.001\n\n\ta = MSet()\n\ta.mols.append(Mol(np.array([1,1,8]),np.array([[1.02068516794,-0.0953531498283,-0.117982957286],[0.697763661362,0.883054985795,0.981867638617],[0.282216817502,0.305964294644,0.341190303806]])))\n\t# m = a.mols[0]\n\n\tmanager = TFMolManageDirect(name=""BehlerParinelloDirectGauSH_H2O_wb97xd_1to21_with_prontonated_Mon_Dec_11_11.43.09_2017"",\n\tnetwork_type = ""BehlerParinelloDirectGauSH"")\n\t# def force_field(coords, eval_forces=True):\n\t# \tif eval_forces:\n\t# \t\tenergy, forces = manager.evaluate_mol(Mol(a.mols[0].atoms, coords), True)\n\t# \t\treturn energy, forces * JOULEPERHARTREE\n\t# \telse:\n\t# \t\tenergy = manager.evaluate_mol(Mol(a.mols[0].atoms, coords), False)\n\t# \t\treturn energy\n\t#\n\t# Opt = GeomOptimizer(force_field)\n\t# mo = Opt.Opt(m)\n\n\t# Tesselate that water to create a box\n\tntess = 4\n\tlatv = 2.8*np.eye(3)\n\t# Start with a water in a ten angstrom box.\n\tlat = Lattice(latv)\n\tmc = lat.CenteredInLattice(a.mols[-1])\n\tmt = Mol(*lat.TessNTimes(mc.atoms,mc.coords,ntess))\n\tprint mt.NAtoms()\n\tnreal = mt.NAtoms()\n\tmt.Distort(0.01)\n\tdef force_field(coords, eval_forces=True):\n\t\tif eval_forces:\n\t\t\tenergy, forces = manager.evaluate_mol(Mol(mt.atoms, coords), True)\n\t\t\treturn energy, forces * JOULEPERHARTREE\n\t\telse:\n\t\t\tenergy = manager.evaluate_mol(Mol(mt.atoms, coords), False)\n\t\t\treturn energy\n\tOpt = GeomOptimizer(force_field)\n\tmt = Opt.Opt(mt,""UCopt"")\n\ndef train_Poly_GauSH():\n\tPARAMS[""RBFS""] = np.stack((np.linspace(0.1, 6.0, 16), np.repeat(0.35, 16)), axis=1)\n\tPARAMS[""SH_NRAD""] = 16\n\tPARAMS[""SH_LMAX""] = 4\n\tPARAMS[""EECutoffOn""] = 0.0\n\tPARAMS[""Elu_Width""] = 6.0\n\tPARAMS[""train_gradients""] = False\n\tPARAMS[""train_dipole""] = False\n\tPARAMS[""train_rotation""] = True\n\tPARAMS[""weight_decay""] = None\n\tPARAMS[""HiddenLayers""] = [512, 512, 512]\n\tPARAMS[""learning_rate""] = 0.0001\n\tPARAMS[""max_steps""] = 500\n\tPARAMS[""test_freq""] = 5\n\tPARAMS[""batch_size""] = 400\n\tPARAMS[""NeuronType""] = ""shifted_softplus""\n\tPARAMS[""tf_prec""] = ""tf.float32""\n\tPARAMS[""Profiling""] = False\n\ta=MSet(""H2O_augmented_more_cutoff5_b3lyp_force"")\n\ta.Load()\n\tmanager = TFMolManageDirect(a, network_type = ""BehlerParinelloDirectGauSH"")\n\ndef GetWaterNetwork():\n\ta=MSet(""water_hexamer_bag"")\n\ta.ReadXYZ()\n\tTreatedAtoms = a.AtomTypes()\n\tPARAMS[""MDdt""] = 0.5\n\tPARAMS[""RemoveInvariant""]=True\n\tPARAMS[""MDMaxStep""] = 50000\n\tPARAMS[""MDThermostat""] = ""Andersen""\n\tPARAMS[""MDTemp""]= 300.0\n\tPARAMS[""MDV0""] = ""Random""\n\tPARAMS[""MetaMDBumpHeight""] = 1.0\n\tPARAMS[""MetaMDBumpWidth""] = 2.0\n\tPARAMS[""MetaBowlK""] = 0.2\n\tPARAMS[""MetaBumpTime""] = 5.0\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0]\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""water_network"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\tdef EnAndForce(x_, DoForce=True):\n\t\tmtmp = Mol(m.atoms,x_)\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\tenergy = Etotal[0]\n\t\tforce = gradient[0]\n\t\tif DoForce:\n\t\t\treturn energy, force\n\t\telse:\n\t\t\treturn energy\n\tm=a.mols[0]\n\tPARAMS[""OptMaxCycles""]= 2000\n\tPARAMS[""OptThresh""] =0.00002\n\tOpt = GeomOptimizer(EnAndForce)\n\tmo=Opt.Opt(a.mols[0],""morphine_tm_opt"")\n\tmo.WriteXYZfile(""./results/"", ""opt_h2o_hex_bag"")\n\tmasses = np.array(list(map(lambda x: ATOMICMASSESAMU[x-1],mo.atoms)))\n\tmeta = MetaDynamics(EnAndForce, mo, EandF_=EnAndForce, name_=""water_hexamer"")\n\tmeta.Prop()\n\ndef water_meta_opt():\n\ta=MSet(""water10"")\n\ta.ReadXYZ()\n\tTreatedAtoms = a.AtomTypes()\n\tm=a.mols[0]\n\tPARAMS[""MDdt""] = 0.5\n\tPARAMS[""RemoveInvariant""] = True\n\tPARAMS[""MDMaxStep""] = 50000\n\tPARAMS[""MDThermostat""] = ""Andersen""\n\tPARAMS[""MDTemp""]= 600.0\n\tPARAMS[""MDV0""] = ""Random""\n\tPARAMS[""MetaMDBumpHeight""] = 1.0\n\tPARAMS[""MetaMDBumpWidth""] = 2.0\n\tPARAMS[""MetaBowlK""] = 0.2\n\tPARAMS[""MetaBumpTime""] = 5.0\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0]\n\tPARAMS[""OptMaxCycles""]= 2000\n\tPARAMS[""OptThresh""] =0.00002\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""water_network"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\tatomization_energy = 0.0\n\tfor atom in mol.atoms:\n\t\tif atom in ele_U:\n\t\t\tatomization_energy += ele_U[atom]\n\tdef EnAndForce(x_, DoForce=True):\n\t\tmtmp = Mol(m.atoms,x_)\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\tenergy = Etotal[0] + atomization_energy\n\t\tforce = gradient[0]\n\t\tif DoForce:\n\t\t\treturn energy, force\n\t\telse:\n\t\t\treturn energy\n\n\ndef water_meta_react():\n\ta=MSet(""water10"")\n\ta.ReadXYZ()\n\tTreatedAtoms = a.AtomTypes()\n\tm=a.mols[0]\n\tPARAMS[""MDdt""] = 0.5\n\tPARAMS[""RemoveInvariant""] = True\n\tPARAMS[""MDMaxStep""] = 50000\n\tPARAMS[""MDThermostat""] = ""Andersen""\n\tPARAMS[""MDTemp""]= 600.0\n\tPARAMS[""MDV0""] = ""Random""\n\tPARAMS[""MetaMDBumpHeight""] = 2.0\n\tPARAMS[""MetaMDBumpWidth""] = 3.0\n\tPARAMS[""MetaBowlK""] = 0.2\n\tPARAMS[""MetaBumpTime""] = 5.0\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0]\n\tPARAMS[""OptMaxCycles""]= 2000\n\tPARAMS[""OptThresh""] =0.00002\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""water_network"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\tatomization_energy = 0.0\n\tfor atom in m.atoms:\n\t\tif atom in ele_U:\n\t\t\tatomization_energy += ele_U[atom]\n\tdef EnAndForce(x_, DoForce=True):\n\t\tmtmp = Mol(m.atoms,x_)\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\tenergy = Etotal[0] + atomization_energy\n\t\tforce = gradient[0]\n\t\tif DoForce:\n\t\t\treturn energy, force\n\t\telse:\n\t\t\treturn energy\n\tmeta = MetaDynamics(EnAndForce, m,name_=""water_10react"", EandF_=EnAndForce)\n\tmeta.Prop()\n\ndef meta_opt():\n\ta=MSet(""water10"")\n\ta.ReadXYZ()\n\tTreatedAtoms = a.AtomTypes()\n\tm=a.mols[0]\n\tPARAMS[""MDdt""] = 0.5\n\tPARAMS[""RemoveInvariant""] = True\n\tPARAMS[""MDMaxStep""] = 50000\n\tPARAMS[""MDThermostat""] = ""Andersen""\n\tPARAMS[""MDTemp""]= 600.0\n\tPARAMS[""MDV0""] = ""Random""\n\tPARAMS[""MetaMDBumpHeight""] = 2.0\n\tPARAMS[""MetaMDBumpWidth""] = 3.0\n\tPARAMS[""MetaMaxBumps""] = 2000\n\tPARAMS[""MetaBowlK""] = 0.2\n\tPARAMS[""MetaBumpTime""] = 5.0\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0]\n\tPARAMS[""OptMaxCycles""]= 2000\n\tPARAMS[""OptThresh""] =0.00002\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""water_network"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\tatomization_energy = 0.0\n\tfor atom in m.atoms:\n\t\tif atom in ele_U:\n\t\t\tatomization_energy += ele_U[atom]\n\tdef EnAndForce(x_, DoForce=True):\n\t\tmtmp = Mol(m.atoms,x_)\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\tenergy = Etotal[0] + atomization_energy\n\t\tforce = gradient[0]\n\t\tif DoForce:\n\t\t\treturn energy, force\n\t\telse:\n\t\t\treturn energy\n#\tPARAMS[""OptMaxCycles""]=500\n\tweb = LocalReactions(EnAndForce,m,50)\n\texit(0)\n\tOpt = MetaOptimizer(EnAndForce,m,Box_=False)\n\tOpt.MetaOpt(m)\n\ndef water_web():\n\ta=MSet(""WebPath"")\n\ta.ReadXYZ()\n\tTreatedAtoms = a.AtomTypes()\n\tm=a.mols[0]\n\tPARAMS[""MDdt""] = 0.5\n\tPARAMS[""RemoveInvariant""] = True\n\tPARAMS[""MDMaxStep""] = 50000\n\tPARAMS[""MDThermostat""] = ""Andersen""\n\tPARAMS[""MDTemp""]= 600.0\n\tPARAMS[""MDV0""] = ""Random""\n\tPARAMS[""MetaMDBumpHeight""] = 2.0\n\tPARAMS[""MetaMDBumpWidth""] = 3.0\n\tPARAMS[""MetaMaxBumps""] = 2000\n\tPARAMS[""MetaBowlK""] = 0.2\n\tPARAMS[""MetaBumpTime""] = 5.0\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0]\n\tPARAMS[""OptMaxCycles""]= 2000\n\tPARAMS[""OptThresh""] =0.00002\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""water_network"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\tatomization_energy = 0.0\n\tfor atom in m.atoms:\n\t\tif atom in ele_U:\n\t\t\tatomization_energy += ele_U[atom]\n\tdef EnAndForce(x_, DoForce=True):\n\t\tmtmp = Mol(m.atoms,x_)\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\tenergy = Etotal[0] + atomization_energy\n\t\tforce = gradient[0]\n\t\tif DoForce:\n\t\t\treturn energy, force\n\t\telse:\n\t\t\treturn energy\n\tf=open(""web_energies.dat"", ""w"")\n\tfor i, mol in enumerate(a.mols):\n\t\ten = EnAndForce(mol.coords, DoForce=False)\n\t\tf.write(str(i)+""  ""+str(en*627.509)+""\\n"")\n\tf.close()\n\n# PARAMS[""RBFS""] = np.stack((np.linspace(0.1, 5.0, 32), np.repeat(0.25, 32)), axis=1)\n# PARAMS[""SH_NRAD""] = 32\n# PARAMS[""SH_LMAX""] = 4\n# a = MSet(""water_dimer"")\n# a.ReadXYZ()\n# manager = TFMolManageDirect(name=""BehlerParinelloDirectGauSH_H2O_wb97xd_1to21_with_prontonated_Mon_Nov_13_11.35.07_2017"", network_type = ""BehlerParinelloDirectGauSH"")\n# print manager.evaluate_mol(a.mols[0], False).shape\n# for i in range(100):\n# \tmol = Mol(a.mols[0].atoms, rot_coords[0,i])\n#  \tmol.WriteXYZfile()\n\n# InterpoleGeometries()\n# read_unpacked_set()\n# TrainKRR(set_=""SmallMols_rand"", dig_ = ""GauSH"", OType_=""Force"")\n# RandomSmallSet(""SmallMols"", 10000)\n# TestMetadynamics()\n# test_md()\n# TestTFBond()\n# TestTFGauSH()\n# test_gaussian_overlap()\n# train_forces_GauSH_direct(""SmallMols_rand"")\n# test_tf_neighbor()\n# train_energy_pairs_triples()\n# train_energy_symm_func(""water_wb97xd_6311gss"")\ntrain_energy_GauSH(""water_wb97xd_6311gss"")\n# test_h2o()\n# evaluate_BPSymFunc(""nicotine_vib"")\n# water_dimer_plot()\n# nicotine_cc_stretch_plot()\n# meta_statistics()\n# meta_stat_plot()\n# harmonic_freq()\n# train_Poly_GauSH()\n#water_ir()\n# GetWaterNetwork()\n# water_meta_opt()\n# water_meta_react()\n# meta_opt()\n# water_web()\n\n# f=open(""nicotine_md_aimd_log.dat"",""r"")\n# f2=open(""nicotine_md_aimd_energies.dat"", ""w"")\n# lines=f.readlines()\n# for line in lines:\n# \tf2.write(str(float(line.split()[0])/1000.0)+"" ""+str(float(line.split()[7]) * 627.509)+""\\n"")\n# f.close()\n# f2.close()\n\n# import pickle\n# water_data = pickle.load(open(""./datasets/H2O_wbxd_1to21_with_prontonated.dat"",""rb""))\n# a=MSet(""water_clusters"")\n# for i, mol in enumerate(water_data):\n# \ta.mols.append(Mol(np.array(mol[""atoms""]), mol[""xyz""]))\n# \ta.mols[-1].properties[""name""] = mol[""name""]\n# \ta.mols[-1].properties[""energy""] = mol[""scf_energy""]\n# \ta.mols[-1].properties[""dipole""] = np.array(mol[""dipole""])\n# \ta.mols[-1].properties[""gradients""] = mol[""gradients""]\n# \ttry:\n# \t\ta.mols[-1].properties[""quadrupole""] = np.array(mol[""quad""])\n# \t\ta.mols[-1].properties[""mulliken_charges""] = np.array(mol[""charges""])\n# \texcept Exception as Ex:\n# \t\tprint Ex\n# \t\tprint i\n# \t\tpass\n# a.Save()\n\n# PARAMS[""tf_prec""] = ""tf.float32""\n# PARAMS[""RBFS""] = np.stack((np.linspace(0.1, 6.0, 16), np.repeat(0.35, 16)), axis=1)\n# PARAMS[""SH_NRAD""] = 16\n# a = MSet(""SmallMols_rand"")\n# a.Load()\n# # a.mols.append(Mol(np.array([1,1,8]),np.array([[0.9,0.1,0.1],[1.,0.9,1.],[0.1,0.1,0.1]])))\n# # # # Tesselate that water to create a box\n# # ntess = 16\n# # latv = 2.8*np.eye(3)\n# # # # # Start with a water in a ten angstrom box.\n# # lat = Lattice(latv)\n# # mc = lat.CenteredInLattice(a.mols[0])\n# # mt = Mol(*lat.TessNTimes(mc.atoms,mc.coords,ntess))\n# # # # mt.WriteXYZfile()\n# b=MSet()\n# for i in range(1):\n# \tb.mols.append(a.mols[i])\n# \tnew_mol = copy.deepcopy(a.mols[i])\n# \tnew_mol.RotateRandomUniform()\n# \tb.mols.append(new_mol)\n# # # a=MSet(""SmallMols_rand"")\n# # # a.Load()\n# maxnatoms = b.MaxNAtoms()\n# zlist = []\n# xyzlist = []\n# n_atoms_list = []\n# for i, mol in enumerate(b.mols):\n# \tpaddedxyz = np.zeros((maxnatoms,3), dtype=np.float32)\n# \tpaddedxyz[:mol.atoms.shape[0]] = mol.coords\n# \tpaddedz = np.zeros((maxnatoms), dtype=np.int32)\n# \tpaddedz[:mol.atoms.shape[0]] = mol.atoms\n# \txyzlist.append(paddedxyz)\n# \tzlist.append(paddedz)\n# \tn_atoms_list.append(mol.NAtoms())\n# \tif i == 99:\n# \t\tbreak\n# xyzstack = tf.stack(xyzlist)\n# zstack = tf.stack(zlist)\n# natomsstack = tf.stack(n_atoms_list)\n# r_cutoff = 7.0\n# gaussian_params = tf.Variable(PARAMS[""RBFS""], trainable=True, dtype=tf.float32)\n# # atomic_embed_factors = tf.Variable(PARAMS[""ANES""], trainable=True, dtype=tf.float32)\n# elements = tf.constant([1, 8], dtype=tf.int32)\n# # tmp = tf_neighbor_list_sort(xyzstack, zstack, natomsstack, elements, r_cutoff)\n# # tmp = tf_sparse_gauss_harmonics_echannel(xyzstack, zstack, natomsstack, elements, gaussian_params, 4, r_cutoff)\n# tmp2 = tf_gauss_harmonics_echannel(xyzstack, zstack, elements, gaussian_params, 8)\n# sess = tf.Session()\n# sess.run(tf.global_variables_initializer())\n# # options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n# # run_metadata = tf.RunMetadata()\n# # # for i in range(a.mols[0].atoms.shape[0]):\n# # # \tprint a.mols[0].atoms[i], ""   "", a.mols[0].coords[i,0], ""   "", a.mols[0].coords[i,1], ""   "", a.mols[0].coords[i,2]\n# @TMTiming(""test"")\n# def get_pairs():\n# \ttmp3 = sess.run(tmp2)\n# \treturn tmp3\n# tmp5 = get_pairs()\n# print tmp5[:13].shape\n# print tmp5[13:].shape\n# print np.allclose(tmp5[:13], tmp5[13:], 1e-03)\n# # print np.isclose(tmp5[0][0], tmp6[0][0], 1e-01)\n# # fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n# # chrome_trace = fetched_timeline.generate_chrome_trace_format()\n# # with open(\'timeline_step_tmp_tm_nocheck_h2o.json\', \'w\') as f:\n# # \tf.write(chrome_trace)\n'"
samples/test_jg.py,1,"b'from TensorMol import *\nimport os,sys\n\na=MSet()\nm=Mol()\nm.atoms = np.array([6, 1, 1, 1, 1],dtype=np.uint8)\nm.coords = np.array([[0, 0, 0], [-1, 0, 0], [0, -0.5, -1], [0, 1, 0], [1, 0, 0]],dtype=np.float64)\na.mols.append(m)\n\ndef GetChemSpiderNetwork(a, Solvation_=False):\n    TreatedAtoms = np.array([1,6,7,8], dtype=np.uint8)\n    PARAMS[""tm_root""] = ""/home/animal/Packages/TensorMol""\n    PARAMS[""tf_prec""] = ""tf.float64""\n    PARAMS[""NeuronType""] = ""sigmoid_with_param""\n    PARAMS[""sigmoid_alpha""] = 100.0\n    PARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n    PARAMS[""EECutoff""] = 15.0\n    PARAMS[""EECutoffOn""] = 0\n    PARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n    PARAMS[""EECutoffOff""] = 15.0\n    PARAMS[""AddEcc""] = True\n    PARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n    d = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n    tset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n    PARAMS[""DSFAlpha""] = 0.18*BOHRPERA\n\n    manager=TFMolManage(""chemspider12_nosolvation"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n    return manager\n\nmanager = GetChemSpiderNetwork(a, False) # load chemspider network\n\ndef EnAndForce(x_, DoForce=True):\n    mtmp = Mol(m.atoms,x_)\n    Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n    energy = Etotal[0]\n    force = gradient[0]\n    if DoForce:\n        return energy, force\n    else:\n        return energy\n\n\n# Perform geometry optimization\nPARAMS[""OptMaxCycles""]= 2000\nPARAMS[""OptThresh""] =0.00002\nOpt = GeomOptimizer(EnAndForce)\nm=Opt.Opt(m)\n'"
samples/test_mbe_general.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nfrom TensorMol.MBE.NN_MBE import *\nfrom TensorMol.MBE.MBE_Opt import *\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\n# step to test a BruteForce MBE model\nif (0):\n\tif (0):\n\t\ta=MSet(""H2O_cluster"")\n                a.ReadXYZ(""H2O_cluster"")\n\t\tmanager= TFMolManage(""Mol_H2O_augmented_more_squeeze_cutoff5_ANI1_Sym_fc_sqdiff_BP_1"", None, False)\n\t\tdipole_manager= TFMolManage(""Mol_H2O_agumented_more_squeeze_cutoff5_multipole2_ANI1_Sym_Dipole_BP_2_1"", None, False)\n\t\tdef EnAndForce(x_):\n                        a.mols[0].coords = x_\n                        energy, force = manager.Eval_BPForceSet(a)\n                        energy = energy[0]\n                        force = force[0]\n                        return energy, force\n                ForceField = lambda x: EnAndForce(x)[1]\n                EnergyForceField = lambda x: EnAndForce(x)\n\n                def ChargeField(x_):\n                        a.mols[0].coords = x_\n                        dipole, charge = dipole_manager.Eval_BPDipole_2(a.mols[0])\n                        return np.asarray(charge[0])\n\t\toptimizer = Optimizer(manager)\n\t\toptimizer.OptANI1(a.mols[0])\n\t\tPARAMS[""MDdt""] = 0.2\n       \t \tPARAMS[""RemoveInvariant""]=True\n       \t \tPARAMS[""MDMaxStep""] = 10000\n       \t \tPARAMS[""MDThermostat""] = ""Nose""\n       \t \tPARAMS[""MDV0""] = None\n       \t \tPARAMS[""MDTemp""]= 1.0\n       \t \t#annealIndo = Annealer(EnergyForceField, ChargeField, a.mols[0], ""Anneal"")\n       \t \t#annealIndo.Prop()\n       \t \t#a.mols[0].coords = annealIndo.Minx.copy()\n       \t \t#a.mols[0].WriteXYZfile(""./results/"", ""h2o_dimer_opt"")\n\n\n\tif (0):\n\t\t#a=MSet(""H2O_mono"")\n\t\t#a.ReadXYZ(""H2O_mono"")\n\t\ta=MSet(""H2O_4water"")\n                a.ReadXYZ(""H2O_4water"")\n\t\tmanager= TFMolManage(""Mol_H2O_augmented_more_cutoff5_ANI1_Sym_fc_sqdiff_BP_1"", None, False)\n\t\tdipole_manager= TFMolManage(""Mol_H2O_agumented_more_cutoff5_multipole2_ANI1_Sym_Dipole_BP_2_1"", None, False)\n                def EnAndForce(x_):\n                        a.mols[0].coords = x_\n\t\t\tenergy, force = manager.Eval_BPForceSet(a)\n                \tenergy = energy[0]\n                \tforce = force[0]\n                        return energy, force\n                ForceField = lambda x: EnAndForce(x)[1]\n                EnergyForceField = lambda x: EnAndForce(x)\n\n\t\tdef ChargeField(x_):\n                        a.mols[0].coords = x_\n                        dipole, charge = dipole_manager.Eval_BPDipole_2(a.mols[0])\n                        return np.asarray(charge[0])\n\n\t\t#PARAMS[""MDThermostat""] = ""Nose""\n                #PARAMS[""MDTemp""] = 200.0\n                #PARAMS[""MDdt""] = 0.2\n                #md = VelocityVerlet(ForceField,a.mols[0],""H2O_udp_md"",EnergyForceField)\n                #md.Prop()\n\n\n\t\t#optimizer = Optimizer(manager)\n        \t#optimizer.OptANI1(a.mols[0])\n\n\t\tPARAMS[""MDFieldAmp""] = 0.0 #0.00000001\n\t\tPARAMS[""MDFieldTau""] = 0.4\n\t\tPARAMS[""MDFieldFreq""] = 0.8\n\t\tPARAMS[""MDFieldVec""] = np.array([1.0,0.0,0.0])\n\t       \tPARAMS[""MDThermostat""] = ""Nose""\n                PARAMS[""MDTemp""] = 30\n                PARAMS[""MDdt""] = 0.1\n\t\tPARAMS[""RemoveInvariant""]=True\n\t\tPARAMS[""MDV0""] = None\n\t\tPARAMS[""MDMaxStep""] = 10000\n\t\twarm = VelocityVerlet(ForceField, a.mols[0],""warm"",EnergyForceField)\n\t\twarm.Prop()\n\t\ta.mols[0].coords = warm.x.copy()\n\t\tPARAMS[""MDMaxStep""] = 40000\n\t\tmd = IRTrajectory(EnergyForceField, ChargeField, a.mols[0],""H2O_udp_IR"",warm.v.copy())\n                md.Prop()\n\t\tWriteDerDipoleCorrelationFunction(md.mu_his,""H2O_udp_IR.txt"")\n\n\tif (1):\n\t\ta=FragableMSetBF(""H2O_cluster"")\n\t\ta.ReadXYZ(""H2O_cluster"")\n\t\t#a=FragableMSetBF(""H2O_dimer"")\n                #a.ReadXYZ(""H2O_dimer"")\n\n\t\tprint(""Generate_All_MBE_term_General: "")\n\t\ta.Generate_All_MBE_term_General([{""atom"":""HOH"", ""charge"":0}])\n\t\tprint(""End of Generate_All_MBE_term_General"")\n\n\t\tmanager= TFMolManage(""Mol_H2O_augmented_more_squeeze_cutoff5_ANI1_Sym_fc_sqdiff_BP_1"", None, False, Trainable_ = False)\n\t\tdipole_manager= TFMolManage(""Mol_H2O_agumented_more_cutoff5_multipole2_ANI1_Sym_Dipole_BP_2_1"", None, False, Trainable_ = False)\n\t\tmbe = NN_MBE_BF(manager, dipole_manager)\n\n\t\tOpt = MBE_Optimizer(mbe)\n\t\tfor mol in a.mols:\n\t\t\t#Opt.MBE_Opt(mol)\n\t\t\t#mbe.NN_Energy_Force(mol)\n\t\t\t#mbe.NN_Energy(mol, False)\n\t\t\t#mbe.NN_Dipole(mol)\n\t\t\tmbe.NN_Charge(mol, True)\n\t\t\tmbe.NN_Energy_Force(mol, True)\n\n\n\tif (0):\n\t\ta = MSet(""H2O_udp"")\n\t\ta.ReadXYZ(""H2O_udp"")\n\t\t#manager= TFMolManage(""Mol_H2O_augmented_more_squeeze_cutoff5_ANI1_Sym_fc_sqdiff_BP_1"", None, False, Trainable_ = False)\n                dipole_manager= TFMolManage(""Mol_H2O_agumented_more_cutoff5_multipole2_ANI1_Sym_Dipole_BP_2_1"", None, False, Trainable_ = False)\n\t\t#for mol in a.mols:\n\t\t\t#manager.Eval_BPForceSingle(mol)\n\t\t#\tdipole_manager.Eval_BPDipoleGrad_2(mol)\n\t\tprint(dipole_manager.Eval_BPDipoleGrad_2(a))\n\nif (1):\n                a=FragableMSetBF(""H2O_cluster_larger"")\n                a.ReadXYZ(""H2O_cluster_larger"")\n\n                print(""Generate_All_MBE_term_General: "")\n                a.Generate_All_MBE_term_General([{""atom"":""HOH"", ""charge"":0}])\n                print(""End of Generate_All_MBE_term_General"")\n\t\t#manager= TFMolManage(""Mol_H2O_augmented_more_400K_squeeze_cutoff5_ANI1_Sym_fc_sqdiff_BP_1"",None,False)\n                manager= TFMolManage(""Mol_H2O_augmented_more_squeeze_cutoff5_ANI1_Sym_fc_sqdiff_BP_1"", None, False)\n                dipole_manager= TFMolManage(""Mol_H2O_agumented_more_cutoff5_multipole2_ANI1_Sym_Dipole_BP_2_1"", None, False)\n                mbe = NN_MBE_BF(manager, dipole_manager)\n                def EnAndForce(x_):\n\t\t\ta.mols[0].coords = x_\n\t\t\ta.mols[0].Reset_Frags()\n\t\t\tt = time.time()\n\t\t\t#mbe.NN_Charge(a.mols[0], True)\n\t\t\t#print ""NN_Charge cost:"", time.time() -t\n\t\t\treturn mbe.NN_Energy_Force(a.mols[0], False)\n\t\tForceField = lambda x: EnAndForce(x)[1]\n\t\tEnergyForceField = lambda x: EnAndForce(x)\n\n\t\tdef ChargeField(x_):\n                        a.mols[0].coords = x_\n                        charge =  mbe.NN_Charge(a.mols[0])\n                        return charge\n\n\t\t#PARAMS[""MDThermostat""] = None\n\t\t#PARAMS[""MDTemp""] = 200.0\n\t\t#PARAMS[""MDdt""] = 0.2\n\t\t#md = VelocityVerlet(ForceField,a.mols[0],""MBE_test_opt"",EnergyForceField)\n\t\t#md.Prop()\n\n\t\t#PARAMS[""MDdt""] = 0.2\n                #PARAMS[""RemoveInvariant""]=True\n                #PARAMS[""MDMaxStep""] = 10000\n                #PARAMS[""MDThermostat""] = ""Nose""\n                #PARAMS[""MDV0""] = None\n                #PARAMS[""MDTemp""]= 1.0\n                #annealH2O = Annealer(EnergyForceField, ChargeField, a.mols[0], ""Anneal"")\n                #annealH2O.Prop()\n\t\t#a.mols[0].coords = annealH2O.Minx.copy()\n                #a.mols[0].WriteXYZfile(""./results/"", ""h2o_cluster_anneal_opt"")\n\n\t\tPARAMS[""MDFieldAmp""] = 0.0 #0.00000001\n                PARAMS[""MDFieldTau""] = 0.4\n                PARAMS[""MDFieldFreq""] = 0.8\n                PARAMS[""MDFieldVec""] = np.array([1.0,0.0,0.0])\n                PARAMS[""MDThermostat""] = ""Nose""\n\t\tPARAMS[""MDTemp""] = 200\n                #PARAMS[""MDTemp""] = 30\n                PARAMS[""MDdt""] = 0.1\n                PARAMS[""RemoveInvariant""]=True\n                PARAMS[""MDV0""] = None\n                PARAMS[""MDMaxStep""] = 10000\n                warm = VelocityVerlet(ForceField, a.mols[0],""warm"",EnergyForceField)\n                warm.Prop()\n                a.mols[0].coords = warm.x.copy()\n                PARAMS[""MDMaxStep""] = 40000\n                md = IRTrajectory(EnergyForceField, ChargeField, a.mols[0],""H2O_cluster_IR"",warm.v.copy())\n                md.Prop()\n                WriteDerDipoleCorrelationFunction(md.mu_his,""H2O_cluster_IR.txt"")\n\n\nif (0):\n                a=MSet(""water_small_md"")\n                a.ReadXYZ(""water_small_md"")\n\t\tOO_dist_list = []\n\t\tOO_dist_list_trimer = []\n\t\tmin_accept = 2.7\n\t\tmax_accept = 5\n\t\ttri_max_accept = 4\n\t\tdimer_accept_rate = 1.0/100.0\n\t\ttrimer_accept_rate = 1.0/6.0\n\t\tt = time.time()\n\t\tmono_index = 0\n\t\tmono_max = 30000\n\t\tfor mol_index, mol in enumerate(a.mols):\n\t          if 1000 < mol_index :\n\t\t\tprint(mol_index)\n\t\t\tdist_mat =  MolEmb.Make_DistMat(mol.coords)\n\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\tif mono_index < mono_max and mol.atoms[i] == 8:\n\t\t\t\t\tm = Mol(mol.atoms[[i,i+1,i+2]], mol.coords[[i,i+1,i+2]])\n                                \tm.WriteXYZfile(fname=""monomer"")\n\t\t\t\t\tmono_index += 1\n\t\t\t\tfor j in range (i+1, mol.NAtoms()):\n\t\t\t\t\tif dist_mat[i][j] < max_accept and mol.atoms[i] ==8 and mol.atoms[j] == 8:\n\t\t\t\t\t\tp_accept = ((dist_mat[i][j]- max_accept) / (min_accept - max_accept))**2\n\t\t\t\t\t\tif (random.random() < p_accept*dimer_accept_rate):\n\t\t\t\t\t\t\tm = Mol(mol.atoms[[i,i+1,i+2, j, j+1, j+2]], mol.coords[[i,i+1,i+2, j, j+1, j+2]])\n\t\t\t\t\t\t\tm.WriteXYZfile(fname=""dimer"")\n\t\t\t\t\t\t\t#OO_dist_list.append(dist_mat[i][j])\n\t\t\t\t\tif dist_mat[i][j] < tri_max_accept and mol.atoms[i] ==8 and mol.atoms[j] == 8:\n\t\t\t\t\t\tfor k in range (j+1, mol.NAtoms()):\n\t\t\t\t\t\t\tif dist_mat[i][k] < tri_max_accept and dist_mat[j][k] < tri_max_accept and mol.atoms[k] ==8:\n\t\t\t\t\t\t\t\tp_accept_trimer = ((dist_mat[i][j]- tri_max_accept) / (min_accept - tri_max_accept))**2* ((dist_mat[i][k]- tri_max_accept) / (min_accept - tri_max_accept))**2 * ((dist_mat[j][k]- tri_max_accept) / (min_accept - tri_max_accept))**2\n\t\t\t\t\t\t\t\tif (random.random() < p_accept_trimer*trimer_accept_rate ):\n\t\t\t\t\t\t\t\t\tm = Mol(mol.atoms[[i,i+1,i+2, j, j+1, j+2, k,k+1, k+2]], mol.coords[[i,i+1,i+2, j, j+1, j+2, k, k+1, k+2]])\n\t\t                                                        m.WriteXYZfile(fname=""trimer"")\n\t\t\t\t\t\t\t\t\t#OO_dist_list_trimer.append(dist_mat[i][j]*dist_mat[i][k]*dist_mat[j][k])\n\t\t#print ""len of list per case trimer:"", len(OO_dist_list_trimer)/100.0\n\t\t#print ""len of list per case:"", len(OO_dist_list)/100.0\n\t\t#print ""time per case:"", (time.time() - t)/ 100.0\n\t\t#OO_dist_list = np.asarray(OO_dist_list)\n\t\t#OO_dist_list_trimer = np.asarray(OO_dist_list_trimer)\n\t\t#np.savetxt(""OO_dist.dat"", OO_dist_list)\n\t\t#np.savetxt(""OO_dist_trimer.dat"", OO_dist_list_trimer)\n\n\n\n# steps to train a NN-MBE model\nif (0):\n\t#Load .xyz files.\n\n\tif (1):\n\t\ta=FragableMSet(""NaClH2O"")\n\t\ta.ReadXYZ(""NaClH2O"")\n\t\ta.Generate_All_Pairs(pair_list=[{""pair"":""NaCl"", ""mono"":[""Na"",""Cl""], ""center"":[0,0]}])\n                #a.Generate_All_MBE_term_General([{""atom"":""OHHNa"", ""charge"":1}, {""atom"":""OHHCl"", ""charge"":-1},{""atom"":""OHH"", ""charge"":0}], cutoff=10, center_atom=[0,0,0]) # Generate all the many-body terms with  certain radius cutoff.\n                a.Generate_All_MBE_term_General([{""atom"":""OHH"", ""charge"":0}, {""atom"":""NaCl"", ""charge"":0}], cutoff=12, center_atom=[0, -1]) # Generate all the many-body terms with  certain radius cutoff.  # -1 means center of mass\n                a.Save() # Save the training set, by default it is saved in ./datasets.\n\n\tif (1):\n                #a=MSet(""NaCl_H2O_NaH2Ogroup"")\n                a=FragableMSet(""NaClH2O"")\n                a.Load() # Load generated training set (.pdb file).\n                a.Calculate_All_Frag_Energy_General(method=""qchem"")  # Use PySCF or Qchem to calcuate the MP2 many-body energy of each order.\n                #a.Get_All_Qchem_Frag_Energy_General()\n                a.Save()\n\n\n\n\t# Do the permutation if it is necessary.\n\tif (0):\n\t\ta=MSet(""H2O_tinker_amoeba"")\n\t\ta.Load()\n\t\ta.Get_Permute_Frags(indis=[1,2]) # Include all the possible permutations of each fragment.\n\t\ta.Save()\n\n\t# Prepare data for neural newtork training.\n\tif (0):\n\t\ta=MSet(""H2O_tinker_amoeba"")\n                a.Load()\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tprint(""TreatedAtoms "", TreatedAtoms)\n\t\td = MolDigester(TreatedAtoms, name_=""SymFunc"")  # Initialize a digester that apply descriptor for the fragments.\n\t\t#tset = TensorMolData(a,d, order_=2, num_indis_=2) # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\t#tset.BuildTrain(""H2O_tinker_amoeba"") # Genearte training data with the loaded molecule set and the chosen digester, by default it is saved in ./trainsets.\n\t\ttset = TensorMolData_BP(a,d, order_=2, num_indis_=2) # Initialize TensorMolData that contain the training data for the neural network for certain order of many-body expansion.\n\t\ttset.BuildTrain(""H2O_tinker_amoeba"")\n\n\t# doing the KRR for the set for debug purpose.\n\tif (0):\n\t\ttset = TensorMolData_BP(MSet(),MolDigester([]),""H2O_tinker_amoeba_GauInv_1"") # Load the generated data for training the neural network.\n\t\ttset.KRR()\n\n\t# testing the BP TensorMolData\n\tif (0):\n                tset = TensorMolData_BP(MSet(),MolDigester([]),""H2O_tinker_amoeba_SymFunc_1"")\n\t\t#tset.LoadDataToScratch(True)\n\n\t# Train the neural network.\n\tif (0):\n\t\ttset = TensorMolData_BP(MSet(),MolDigester([]),""H2O_tinker_amoeba_SymFunc_2"") # Load the generated data for training the neural network.\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP"") # Initialzie a manager than manage the training of neural network.\n\t\tmanager.Train(maxstep=20000)  # train the neural network for 500 steps, by default it trainse 10000 steps and saved in ./networks.\n\n\t# Test the neural network.\n\tif (0):\n\t\tmanager = TFMolManage(""H2O_tinker_amoebaCoulomb_fc_sqdiff_2"", None, False) # Load pre-trained network.\n\t\tmanager.Test(""nn_acc_pred.dat"") # Save the test result of our trained network.\n\n\n# steps to evaluate the many-body energy using  NN-MBE model\nif (0):\n\t# load molecule\n\ta=MSet(""H2O_opt"")\n\ta.ReadGDB9Unpacked(""./H2O_opt/"")\n\t# load pre-trained networks {many-body order: network name}\n\ttfm = {1:""H2O_tinker_amoebaCoulomb_fc_sqdiff_1"", 2:""H2O_tinker_amoebaCoulomb_fc_sqdiff_2""}\n\t# launch NN-MBE model\n\tnn_mbe = NN_MBE(tfm)\n\t# evaluate using NN-MBE model\n\tfor mol in a.mols:\n\t\t#mol.Generate_All_MBE_term(atom_group=3, cutoff=5, center_atom=0)\n\t\tnn_mbe.NN_Energy(mol)\n\n# use NN-MBE model to optimize molecule.\nif (0):\n\t# load molecule\n        a=MSet(""H2O_opt"")\n        a.ReadGDB9Unpacked(""./H2O_opt/"")\n        # load pre-trained networks {many-body order: network name}\n        tfm = {1:""H2O_tinker_amoebaCoulomb_fc_sqdiff_1"", 2:""H2O_tinker_amoebaCoulomb_fc_sqdiff_2""}\n        # launch NN-MBE model\n        nn_mbe = NN_MBE(tfm)\n\t# launch Optimizer\n        opt=MBE_Optimizer(nn_mbe)\n\t# Optimize\n        for mol in a.mols:\n\t\t#mol.Generate_All_MBE_term(atom_group=3, cutoff=5, center_atom=0)\n\t\t#opt.NN_Opt(mol)\n        \topt.MBE_LBFGS_Opt(mol)\n'"
samples/test_memory_error.py,1,"b'from __future__ import absolute_import\nimport memory_util\nmemory_util.vlog(1)\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""2""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nfrom TensorMol.MBE.NN_MBE import *\n\na = MSet(""chemspider9_metady_force"")\na.Load()\n#for i in range(141776):\n#\ta.mols.pop()\nTreatedAtoms = a.AtomTypes()\nPARAMS[""learning_rate""] = 0.00001\nPARAMS[""momentum""] = 0.95\nPARAMS[""max_steps""] = 101\nPARAMS[""batch_size""] =  400   # 40 the max min-batch size it can go without memory error for training\nPARAMS[""test_freq""] = 2\nPARAMS[""tf_prec""] = ""tf.float64""\nPARAMS[""GradScaler""] = 1.0\nPARAMS[""DipoleScaler""]=1.0\nPARAMS[""NeuronType""] = ""relu""\nPARAMS[""HiddenLayers""] = [1000, 1000, 1000]\nPARAMS[""EECutoff""] = 15.0\nPARAMS[""EECutoffOn""] = 7.0\nPARAMS[""Erf_Width""] = 0.4\n#PARAMS[""AN1_r_Rc""] = 8.0\n#PARAMS[""AN1_num_r_Rs""] = 64\nPARAMS[""EECutoffOff""] = 15.0\nPARAMS[""learning_rate_dipole""] = 0.0001\nPARAMS[""learning_rate_energy""] = 0.00001\nPARAMS[""SwitchEpoch""] = 10\nd = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\ntset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n#tset = TensorMolData_BP_Direct_EE(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n#tset = TensorMolData_BP_Multipole_2_Direct(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = False)\n#manager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode"") # Initialzie a manager than manage the training of neural network.\n#manager=TFMolManage("""",tset,False,""Dipole_BP_2_Direct"")\nmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_Update"")\nPARAMS[\'Profiling\']=1\nwith memory_util.capture_stderr() as stderr:\n\tmanager.Train(1)\nmemory_util.print_memory_timeline(stderr, ignore_less_than_bytes=1000)\n'"
samples/test_neb.py,1,"b'\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""""\n\ndef GetChemSpider12(a):\n\tTreatedAtoms = np.array([1,6,7,8], dtype=np.uint8)\n\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100""\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 21\n\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\tPARAMS[""test_freq""] = 1\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""EnergyScalar""] = 1.0\n\tPARAMS[""GradScalar""] = 1.0/20.0\n\tPARAMS[""DipoleScaler""]=1.0\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\t#PARAMS[""Erf_Width""] = 1.0\n\t#PARAMS[""Poly_Width""] = 4.6\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t#PARAMS[""AN1_num_r_Rs""] = 64\n\tPARAMS[""EECutoffOff""] = 15.0\n\t#PARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t#PARAMS[""KeepProb""] = 0.7\n\tPARAMS[""learning_rate_dipole""] = 0.0001\n\tPARAMS[""learning_rate_energy""] = 0.00001\n\tPARAMS[""SwitchEpoch""] = 2\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""Mol_chemspider12_maxatom35_H2O_with_CH4_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_act_sigmoid100_rightalpha"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\treturn manager\n\ndef Eval():\n\ta=MSet(""EndiandricC"", center_=False)\n\ta.ReadXYZ()\n\t# Optimize all three structures.\n\tmanager = GetChemSpider12(a)\n\tdef GetEnergyForceForMol(m):\n\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\ttmpm = Mol(m.atoms,x_)\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(tmpm, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tif DoForce:\n\t\t\t\treturn energy, force\n\t\t\telse:\n\t\t\t\treturn energy\n\t\treturn EnAndForce\n\tif 0:\n\t\t# Optimize all three steps of the reaction.\n\t\tPARAMS[""OptMaxCycles""]=20\n\t\tprint(""Optimizing "", len(a.mols), "" mols"")\n\t\tfor i in range(6):\n\t\t\tF = GetEnergyForceForMol(a.mols[i])\n\t\t\tOpt = GeomOptimizer(F)\n\t\t\ta.mols[i] = Opt.Opt(a.mols[i])\n\t\t\ta.mols[i].WriteXYZfile(""./results/"", ""OptMol""+str(i))\n\n\t# The set consists of PreF, PreG, G, F, B, C\n\t# Important transitions are 1<=>2, 2<>3, 1<>4, 3<>6, 4<>5\n\n\t# Achieve element alignment.\n#\ta.mols[0], a.mols[1] = a.mols[0].AlignAtoms(a.mols[1])\n#\ta.mols[0].WriteXYZfile(""./results/"", ""Aligned""+str(0))\n\n\t# Finally do the NEB. between each.\n\tPARAMS[""OptMaxCycles""]=500\n\tPARAMS[""NebSolver""]=""Verlet""\n\tPARAMS[""SDStep""] = 0.05\n\tPARAMS[""NebNumBeads""] = 22\n\tPARAMS[""MaxBFGS""] = 12\n\ta.mols[0], a.mols[1] = a.mols[0].AlignAtoms(a.mols[1])\n\ta.mols[0].WriteXYZfile(""./results/"", ""Aligned""+str(0))\n\ta.mols[0].WriteXYZfile(""./results/"", ""Aligned""+str(1))\n\tF = GetEnergyForceForMol(a.mols[0])\n\tneb = NudgedElasticBand(F,a.mols[0],a.mols[1])\n\tBeads = neb.Opt(""NebStep1"")\n\ndef TestBetaHairpin():\n\ta=MSet(""2evq"", center_=False)\n\ta.ReadXYZ()\n\ta.OnlyAtoms([1, 6, 7, 8])\n\t# Optimize all three structures.\n\tmanager = GetChemSpider12(a)\n\tdef F(z_, x_, nreal_, DoForce = True):\n\t\t""""""\n\t\tThis is the primitive form of force routine required by PeriodicForce.\n\t\t""""""\n\t\tmtmp = Mol(z_,x_)\n\t\tif (DoForce):\n\t\t\ten,f = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], nreal_,True)\n\t\t\treturn en[0], f[0]\n\t\telse:\n\t\t\ten = manager.EvalBPDirectEEUpdateSinglePeriodic(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], nreal_, True, DoForce)\n\t\t\treturn en[0]\n\tm = a.mols[0]\n\txmn = np.amin(m.coords,axis=0)\n\tm.coords -= xmn\n\txmx = np.amax(m.coords,axis=0)\n\tprint(""Xmn,Xmx"",xmn,xmx)\n\tm.properties[""lattice""] = np.array([[xmx[0],0.,0.],[0.,xmx[1],0.],[0.,0.,xmx[2]]])\n\tPF = PeriodicForce(m,m.properties[""lattice""])\n\tPF.BindForce(F, 15.0)\n\n\tif 0:\n\t\tfor i in range(4):\n\t\t\tprint(""En0:"", PF(m.coords)[0])\n\t\t\tm.coords += (np.random.random((1,3))-0.5)*3.0\n\t\t\tm.coords = PF.lattice.ModuloLattice(m.coords)\n\t\t\tprint(""En:""+str(i), PF(m.coords)[0])\n\n\tPARAMS[""OptMaxCycles""]=100\n\tPOpt = PeriodicGeomOptimizer(PF)\n\tPF.mol0=POpt.Opt(m,""ProOpt"")\n\n\tPARAMS[""MDTemp""] = 300.0\n\tPARAMS[""MDdt""] = 0.2 # In fs.\n\tPARAMS[""MDMaxStep""]=2000\n\ttraj = PeriodicVelocityVerlet(PF,""Protein0"")\n\ttraj.Prop()\n\ndef TestUrey():\n\ta = MSet(""2mzx_open"")\n\ta.ReadXYZ()\n\tm = a.mols[0]\n\tm.coords -= np.min(m.coords)\n\t# Optimize all three structures.\n\tmanager = GetChemSpider12(a)\n\tdef GetEnergyForceForMol(m):\n\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\ttmpm = Mol(m.atoms,x_)\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(tmpm, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tif DoForce:\n\t\t\t\treturn energy, force\n\t\t\telse:\n\t\t\t\treturn energy\n\t\treturn EnAndForce\n\tF = GetEnergyForceForMol(m)\n\tPARAMS[""OptMaxCycles""]=500\n\tOpt = MetaOptimizer(F,m,Box_=False)\n\tOpt.Opt(m)\n\ndef MetadynamicsStatistics():\n\t""""""\n\tGather statistics about the metadynamics exploration process varying bump depth, and width.\n\t""""""\n\tsugarXYZ=""""""23\n\n \tC   0.469801362563  -0.186971976654  -0.917684108862\n\tO   -0.859493679862  0.107094904765  -0.545217785597\n\tC   -1.33087192983  -0.507316368828  0.650893179939\n\tC   -0.438298062157  -0.0755659372548  1.80797104148\n\tO   -0.778094885449  -0.696482563857  3.03079166065\n\tC   1.00192237607  -0.469457749055  1.52906960905\n\tC   1.477875031  0.077136161307  0.194458937693\n\tO   1.68692471022  1.45839447946  0.287157941131\n\tO   1.85082739088  0.082940968659  2.50886417356\n\tC   -2.78317030311  -0.10545520088  0.824144527485\n\tO   0.616447904421  -1.51600440404  -1.31396642612\n\tH   -1.26896203668  -1.59866670272  0.550821100756\n\tH   -0.480848486767  1.0203835586  1.89601850937\n\tH   1.05389278947  -1.56571961461  1.52835050017\n\tH   2.41481082624  -0.436974077322  -0.0657764020213\n\tH   0.69100263804  0.494721792561  -1.74781805697\n\tH   -0.193905967702  -1.76927170884  -1.75808894835\n\tH   2.04490869072  1.58866981496  1.17215002368\n\tH   1.58881143012  -0.273592522211  3.36016004218\n\tH   -1.59032043389  -0.31753857396  3.3666667337\n\tH   -2.86651153898  0.952637274148  1.08604018776\n\tH   -3.31879024248  -0.265179614329  -0.112506279571\n\tH   -3.27220758287  -0.69654193988  1.59910983889\n\t""""""\n\tm = Mol()\n\tm.FromXYZString(sugarXYZ)\n\n\t#from MolEmb import EmptyInterfacedFunction, Make_NListNaive, Make_NListLinear\n\t#print(""READ MOL XFOIUDOFIUDFO"")\n\t#print(m.coords,15.0,m.NAtoms(),True)\n\t#EmptyInterfacedFunction(np.zeros((10,3)),13)\n\t#print(""Passed test"")\n\t#return\n\n\tdef GetEnergyForceForMol(m):\n\t\ts = MSet()\n\t\ts.mols.append(m)\n\t\tmanager = GetChemSpider12(s)\n\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\ttmpm = Mol(m.atoms,x_)\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(tmpm, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tif DoForce:\n\t\t\t\treturn energy, force\n\t\t\telse:\n\t\t\t\treturn energy\n\t\treturn EnAndForce\n\tF = GetEnergyForceForMol(m)\n\n\tOpt = GeomOptimizer(F)\n\tm = Opt.Opt(m)\n\tPARAMS[""MDdt""] = 0.5 # In fs.\n\tPARAMS[""MDMaxStep""] = 8000\n\tPARAMS[""MetaBumpTime""] = 10.0\n\tPARAMS[""MetaMaxBumps""] = 500\n\tPARAMS[""MetaBowlK""] = 0.0\n\tPARAMS[""MDThermostat""]=""Andersen""\n\tPARAMS[""MDTemp""]=300.0\n\tPARAMS[""MDV0""]=None\n\tif 0:\n\t\tPARAMS[""MetaMDBumpHeight""] = 0.000\n\t\tPARAMS[""MetaMDBumpWidth""] = 0.5\n\t\ttraj = MetaDynamics(None, m,""MetaMD_000_05"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 0.500\n\t\tPARAMS[""MetaMDBumpWidth""] = 0.5\n\t\ttraj = MetaDynamics(None, m,""MetaMD_050_05"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 0.500\n\t\tPARAMS[""MetaMDBumpWidth""] = 1.0\n\t\ttraj = MetaDynamics(None, m,""MetaMD_050_10"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 0.500\n\t\tPARAMS[""MetaMDBumpWidth""] = 2.0\n\t\ttraj = MetaDynamics(None, m,""MetaMD_050_20"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 1.000\n\t\tPARAMS[""MetaMDBumpWidth""] = 1.0\n\t\ttraj = MetaDynamics(None, m,""MetaMD_100_10"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 1.000\n\t\tPARAMS[""MetaMDBumpWidth""] = 2.0\n\t\ttraj = MetaDynamics(None, m,""MetaMD_100_20"",F)\n\t\ttraj.Prop()\n\tPARAMS[""MetaBumpTime""] = 10.0\n\tPARAMS[""MetaMDBumpHeight""] = 2.00\n\tPARAMS[""MetaMDBumpWidth""] = 1.0\n\tPARAMS[""MetaBowlK""] = 0.1\n\ttraj = MetaDynamics(None, m,""MetaMD_100_10_X"",F)\n\ttraj.Prop()\n\n\n\ndef Bullvalene():\n\t""""""\n\tGather statistics about the metadynamics exploration process varying bump depth, and width.\n\t""""""\n\tsugarXYZ=""""""20\n\nC          0.98112       -0.46991        0.42376\nC          1.06359       -0.55769       -0.77982\nC          0.18965        0.16579       -1.73869\nC         -1.20916       -0.20825       -1.40751\nC         -1.72118       -0.05451       -0.32246\nC         -1.28117        0.48316        0.96604\nC         -0.33043        1.60964        0.94931\nC          0.10659        2.11114       -0.35456\nC          0.32812        1.61317       -1.43439\nC          0.12451        0.26717        1.35413\nH          1.68037       -1.05279        0.90989\nH          1.79150       -1.17816       -1.16736\nH          0.43620       -0.05734       -2.75571\nH         -1.78386       -0.62829       -2.15471\nH         -2.70043       -0.37943       -0.29970\nH         -2.23007        0.30013        1.42539\nH         -0.30281        2.58361        1.39148\nH          0.26267        3.13127       -0.35191\nH          0.63457        2.23708       -2.19717\nH          0.61944       -0.13762        2.21209\n\t""""""\n\tm = Mol()\n\tm.FromXYZString(sugarXYZ)\n\n\t#from MolEmb import EmptyInterfacedFunction, Make_NListNaive, Make_NListLinear\n\t#print(""READ MOL XFOIUDOFIUDFO"")\n\t#print(m.coords,15.0,m.NAtoms(),True)\n\t#EmptyInterfacedFunction(np.zeros((10,3)),13)\n\t#print(""Passed test"")\n\t#return\n\n\tdef GetEnergyForceForMol(m):\n\t\ts = MSet()\n\t\ts.mols.append(m)\n\t\tmanager = GetChemSpider12(s)\n\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\ttmpm = Mol(m.atoms,x_)\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(tmpm, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tif DoForce:\n\t\t\t\treturn energy, force\n\t\t\telse:\n\t\t\t\treturn energy\n\t\treturn EnAndForce\n\tF = GetEnergyForceForMol(m)\n\n\t#MOpt = MetaOptimizer(F,m)\n\t#m = MOpt.MetaOpt(m)\n\tw = LocalReactions(F,m,10)\n\texit(0)\n\n\tPARAMS[""MDdt""] = 0.5 # In fs.\n\tPARAMS[""MDMaxStep""] = 8000\n\tPARAMS[""MetaBumpTime""] = 10.0\n\tPARAMS[""MetaMaxBumps""] = 500\n\tPARAMS[""MetaBowlK""] = 0.0\n\tPARAMS[""MDThermostat""]=""Andersen""\n\tPARAMS[""MDTemp""]=2500.0\n\tPARAMS[""MDV0""]=None\n\tif 0:\n\t\tPARAMS[""MetaMDBumpHeight""] = 0.000\n\t\tPARAMS[""MetaMDBumpWidth""] = 0.5\n\t\ttraj = MetaDynamics(None, m,""MetaMD_000_05"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 0.500\n\t\tPARAMS[""MetaMDBumpWidth""] = 0.5\n\t\ttraj = MetaDynamics(None, m,""MetaMD_050_05"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 0.500\n\t\tPARAMS[""MetaMDBumpWidth""] = 1.0\n\t\ttraj = MetaDynamics(None, m,""MetaMD_050_10"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 0.500\n\t\tPARAMS[""MetaMDBumpWidth""] = 2.0\n\t\ttraj = MetaDynamics(None, m,""MetaMD_050_20"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 1.000\n\t\tPARAMS[""MetaMDBumpWidth""] = 1.0\n\t\ttraj = MetaDynamics(None, m,""MetaMD_100_10"",F)\n\t\ttraj.Prop()\n\t\tPARAMS[""MetaMDBumpHeight""] = 1.000\n\t\tPARAMS[""MetaMDBumpWidth""] = 2.0\n\t\ttraj = MetaDynamics(None, m,""MetaMD_100_20"",F)\n\t\ttraj.Prop()\n\tPARAMS[""MetaBumpTime""] = 10.0\n\tPARAMS[""MetaMDBumpHeight""] = 0.70\n\tPARAMS[""MetaMDBumpWidth""] = 1.0\n\tPARAMS[""MetaBowlK""] = 0.0\n#\ttraj = MetaDynamics(None, m,""MetaMD_100_10_X"",F)\n\ttraj = VelocityVerlet(None, m,""Bullvalene"",F)\n\ttraj.Prop()\n\n\n#Eval()\n#TestBetaHairpin()\n#TestUrey()\nBullvalene()\n'"
samples/test_nicotine.py,1,"b'from __future__ import absolute_import\n#import memory_util\n#memory_util.vlog(1)\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\nfrom TensorMol.MBE.NN_MBE import *\nfrom TensorMol.Interfaces.TMIPIinterface import *\nimport random\n\ndef TrainPrepare():\n\tif (1):\n\t\tWB97XDAtom={}\n\t\tWB97XDAtom[1]=-0.5026682866\n\t\tWB97XDAtom[6]=-37.8387398698\n\t\tWB97XDAtom[7]=-54.5806161811\n\t\tWB97XDAtom[8]=-75.0586028656\n\t\ta = MSet(""nicotine_aimd_rand"")\n\t\ta.Load()\n\t\tb = MSet(""nicotine_aimd_rand_train"")\n\t\tfor mol_index, mol in enumerate(a.mols):\n\t\t\tprint (""mol_index:"", mol_index)\n\t\t\tmol.properties[\'gradients\'] = -mol.properties[\'forces\']\n\t\t\tmol.properties[\'atomization\'] =  mol.properties[\'energy\']\n\t\t\tfor i in range (0, mol.NAtoms()):\n\t\t\t\tmol.properties[\'atomization\'] -= WB97XDAtom[mol.atoms[i]]\n\t\t\t\tb.mols.append(mol)\n\t\t\t\tb.Save()\n\ndef Train():\n\tif (1):\n\t\ta = MSet(""nicotine_aimd_rand_train"")\n\t\ta.Load()\n\t\tprint (len(a.mols))\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""HiddenLayers""] = [200,200,200]\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 5001\n\t\tPARAMS[""batch_size""] = 200\n\t\tPARAMS[""test_freq""] = 10\n\t\tPARAMS[""tf_prec""] = ""tf.float64""\n\t\t#PARAMS[""AN1_num_r_Rs""] = 16\n\t\t#PARAMS[""AN1_num_a_Rs""] = 4\n\t\t#PARAMS[""AN1_num_a_As""] = 4\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""AtomizationEnergy"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_Linear(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_Grad_Linear"") # Initialzie a manager than manage the training of neural network.\n\t\tmanager.Train(maxstep=1001)\n\n#TrainPrepare()\nTrain()\n'"
samples/test_profile.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom Util import *\nfrom Sets import *\nfrom TensorData import *\nfrom TFManage import *\nfrom Opt import *\nimport cProfile, pstats, StringIO\npr = cProfile.Profile()\n\n# 1 - Get molecules into memory\na=MSet(""gdb9"")\na.ReadGDB9Unpacked()\na.Save()\nb=a.DistortedClone()\nb.Save()\n\nb=MSet(""gdb9_NEQ"")\nb.Load()\n# Choose allowed atoms.\nTreatedAtoms = b.AtomTypes()\n# 2 - Choose Digester\nd = Digester(TreatedAtoms, name_=""GauSH"",OType_ =""Force"")\n# 4 - Generate training set samples.\ntset = TensorData(b,d,None,10) #100s/element\npr.enable()\ntset.BuildTrain(""gdb9_NEQ"",TreatedAtoms) # generates dataset numpy arrays for each atom.\npr.disable()\n\ns = StringIO.StringIO()\nsortby = \'cumulative\'\nps = pstats.Stats(pr, stream=s).sort_stats(sortby)\nps.print_stats()\nprint(s.getvalue())\n'"
samples/test_rawembed.py,23,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nfrom TensorMol.MBE.NN_MBE import *\nfrom TensorMol.MBE.MBE_Opt import *\nfrom ..TFDescriptors.RawSymFunc import *\nfrom TensorMol.ForceModifiers.Neighbors import *\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""""\n\nif (0):\n\timport numpy as np\n\tPi = 3.1415\n\txyz_array = np.array([[[0.,0.,0.],[1.0,0.,0.],[0.,0.,5.],[1.,1.,5.],[0,0,0],[0,0,0],[0,0,0]],[[0.,0.,0.],[1.0,0.,0.],[0.,0.,5.],[1.,1.,5.],[0,0,0],[0,0,0],[0,0,0]]], dtype=np.float64)\n\txyz = tf.Variable(xyz_array,trainable=False)\n\tZ_array = np.array([[1,1,7,8,0,0,0],[1,1,7,8,0,0,0]], dtype = np.int32)\n\tZ = tf.Variable(Z_array,trainable=False)\n\n\teleps = tf.Variable([[1,1],[1,7],[1,8],[7,8],[7,7]], dtype=tf.int32)\n\tzetas = tf.Variable([[8.0]], dtype=tf.float64)\n\tetas = tf.Variable([[4.0]], dtype=tf.float64)\n\tAN1_num_a_As = 8\n\tthetas = tf.Variable([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype=tf.float64)\n\tAN1_num_r_Rs = 22\n\tAN1_r_Rc = 4.6\n\tAN1_a_Rc = 3.1\n\trs = tf.Variable([ AN1_r_Rc*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype=tf.float64)\n\tRa_cut = AN1_r_Rc\n\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\tp1 = tf.tile(tf.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_r_Rs,1])\n\tp2 = tf.tile(tf.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_r_Rs,1])\n\tp3 = tf.tile(tf.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_r_Rs,1])\n\tp4 = tf.tile(tf.reshape(rs,[1,1,1,AN1_num_r_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\tSFPa = tf.concat([p1,p2,p3,p4],axis=4)\n\tSFPa = tf.transpose(SFPa, perm=[4,0,1,2,3])\n\n\teles = tf.Variable([[1],[7],[8]], dtype=tf.int32)\n\tetas_R = tf.Variable([[4.0]], dtype=tf.float64)\n\tAN1_num_r_Rs = 22\n\tAN1_r_Rc = 4.6\n\trs_R = tf.Variable([ AN1_r_Rc*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype=tf.float64)\n\tRr_cut = AN1_r_Rc\n\t# Create a parameter tensor. 2 x  neta X nr\n\tp1_R = tf.tile(tf.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\tp2_R = tf.tile(tf.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\tSFPr = tf.concat([p1_R,p2_R],axis=2)\n\tSFPr = tf.transpose(SFPr, perm=[2,0,1])\n\n\n\t# A test of the above.\n\tinit = tf.global_variables_initializer()\n\twith tf.Session() as session:\n\t        session.run(init)\n\t        GM = session.run(tf.gradients(TFSymSet(xyz, Z, eles, SFPr, Rr_cut, eleps, SFPa, Ra_cut), xyz))\n\t        print((GM, GM[0].shape))\n\nif (1):\n\tmset = MSet(""H2O_augmented_more_cutoff5"")\n        mset.Load()\n\tSymMaker = ANISym(mset)\n\tSymMaker.TestPeriodic()\n\t#SymMaker.Generate_ANISYM()\n\nif (0):\n\tmset = MSet(""Neigbor_test"")\n\tmset.ReadXYZ(""Neigbor_test"")\n\txyzs = np.zeros((1, 6, 3),dtype=np.float64)\n\tZs = np.zeros((1, 6), dtype=np.int64)\n\tnnz_atom = np.zeros((1), dtype=np.int64)\n\tfor i, mol in enumerate(mset.mols):\n                xyzs[i][:mol.NAtoms()] = mol.coords\n                Zs[i][:mol.NAtoms()] = mol.atoms\n\t\tnnz_atom[i] = mol.NAtoms()\n\tprint(xyzs, nnz_atom, Zs)\n\tNL = NeighborListSet(xyzs, nnz_atom, True, True, Zs)\n\trad_p, ang_t = NL.buildPairsAndTriples(4.6, 3.1)\n\tprint((""rad_p:"", rad_p, "" ang_t:"", ang_t))\n\nif (0):\n\tmset=MSet(""NeigborMB_test"")\n\tmset.ReadXYZ(""NeigborMB_test"")\n\tMBEterms = MBNeighbors(mset.mols[0].coords, mset.mols[0].atoms, [[0,1,2],[3,4,5],[6,7,8],[9,10,11],[12,13,14]])\n\tMBEterms.Update(mset.mols[0].coords, 10.0, 10.0)\n\tprint(MBEterms.singC)\n\tprint(MBEterms.pairC)\n\tprint(MBEterms.tripC)\n\t#print MBEterms.pairs\n\t#print MBEterms.trips\n\t#print MBEterms.pairz\n\t#print MBEterms.tripz\n\nif (0):\n\tmset=MSet(""NeigborMB_test"")\n\tmset.ReadXYZ(""NeigborMB_test"")\n\tm = mset.mols[-1]\n\tcl = CellList(m.coords, 5.0)\n\tcl.Update(m.coords)\n'"
samples/test_ryker.py,1,"b'from TensorMol import *\nimport os\nimport numpy as np\nimport time\n\ndef test():\n\ta = MSet(""water_aug_cc_pvdz"")\n\ta.Load()\n\tfor mol in a.mols:\n\t\tmol.properties[""quadrupole""] = mol.properties[""quads""]\n\t\tmol.properties[""dipole""] = mol.properties[""dipoles""]\n\t\tmol.properties[""gradients""] = mol.properties[""forces""]\n\t\tdel mol.properties[""quads""]\n\t\tdel mol.properties[""dipoles""]\n\t\tdel mol.properties[""forces""]\n\t\tmol.CalculateAtomization()\n\ta.Save()\n\ndef make_mini_set(filename):\n\ta = MSet(filename)\n\ta.Load()\n\tb = MSet(""water_aug_cc_pvdz_mini"")\n\tfor i in range(1100):\n\t\tb.mols.append(a.mols[i])\n\tb.Save()\n\ndef train_energy_symm_func(mset):\n\tPARAMS[""train_energy_gradients""] = False\n\tPARAMS[""weight_decay""] = None\n\tPARAMS[""HiddenLayers""] = [512, 512, 512]\n\tPARAMS[""learning_rate""] = 0.0001\n\tPARAMS[""max_steps""] = 500\n\tPARAMS[""test_freq""] = 1\n\tPARAMS[""batch_size""] = 100\n\tPARAMS[""NeuronType""] = ""shifted_softplus""\n\tPARAMS[""tf_prec""] = ""tf.float32""\n\tPARAMS[""train_dipole""] = True\n\tPARAMS[""train_quadrupole""] = True\n\tmanager = TFMolManageDirect(mset, network_type = ""BPSymFunc"")\n\ndef get_losses(filename):\n\t# Returns train_loss, energy_loss, grad_loss, ...\n\t# test_train_loss, test_energy_loss, test_grad_loss\n\twith open(filename,""r"") as log:\n\t\tlog = log.readlines()\n\n\tkeep_phrase = ""TensorMol - INFO - step:""\n\ttrain_loss = []\n\tenergy_loss = []\n\tgrad_loss = []\n\n\ttest_train_loss = []\n\ttest_energy_loss = []\n\ttest_grad_loss = []\n\n\tfor line in log:\n\t\tif (keep_phrase in line) and (line[79] == \' \'):\n\t\t\ta = line.split()\n\t\t\ttrain_loss.append(float(a[13]))\n\t\t\tenergy_loss.append(float(a[15]))\n\t\t\tgrad_loss.append(float(a[17]))\n\t\tif (keep_phrase in line) and (line[79] == \'t\'):\n\t\t\ta = line.split()\n\t\t\ttest_train_loss.append(float(a[13]))\n\t\t\ttest_energy_loss.append(float(a[15]))\n\t\t\ttest_grad_loss.append(float(a[17]))\n\n\tprint(str(train_loss) + ""\\n\\n"" + str(energy_loss) + ""\\n\\n"" + str(grad_loss) + ""\\n"")\n\tprint(str(test_train_loss) + ""\\n\\n"" + str(test_energy_loss) + ""\\n\\n"" + str(test_grad_loss) + ""\\n"")\n\treturn train_loss, energy_loss, grad_loss, test_train_loss, test_energy_loss, test_grad_loss\n\ndef optimize_taxol():\n\tTaxol = MSet(""Taxol"")\n\tTaxol.ReadXYZ()\n\tGeomOptimizer(""EnergyForceField"").Opt(Taxol, filename=""OptLog"", Debug=False)\n\n#test()\n#make_mini_set(""water_aug_cc_pvdz"")\ntrain_energy_symm_func(""water_aug_cc_pvdz_mini"")\n#get_losses(""networks/nicotine_aimd_log.txt"")\n#optimize_taxol()\n'"
samples/test_sample_method.py,2,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""2""\nfrom TensorMol.ForceModels.ElectrostaticsTF import *\n\ndef TrainPrepare():\n\tif (0):\n\t\ta = MSet(""chemspider9_force"")\n\t\tdic_list = pickle.load(open(""./datasets/chemspider9_force.dat"", ""rb""))\n\t\tfor dic in dic_list:\n\t\t\tatoms = []\n\t\tfor atom in dic[\'atoms\']:\n\t\t\tatoms.append(AtomicNumber(atom))\n\t\tatoms = np.asarray(atoms, dtype=np.uint8)\n\t\tmol = Mol(atoms, dic[\'xyz\'])\n\t\tmol.properties[\'charges\'] = dic[\'charges\']\n\t\tmol.properties[\'dipole\'] = dic[\'dipole\']\n\t\tmol.properties[\'quadropole\'] = dic[\'quad\']\n\t\tmol.properties[\'energy\'] = dic[\'scf_energy\']\n\t\tmol.properties[\'gradients\'] = dic[\'gradients\']\n\t\tmol.CalculateAtomization()\n\t\ta.mols.append(mol)\n\t\ta.Save()\n\n\tif (0):\n\t\ta = MSet(""chemspider9_force"")\n\t\ta.Load()\n\t\trmsgrad = np.zeros((len(a.mols)))\n\t\tfor i, mol in enumerate(a.mols):\n\t\t\trmsgrad[i] = (np.sum(np.square(mol.properties[\'gradients\'])))**0.5\n\t\tmeangrad = np.mean(rmsgrad)\n\t\tprint(""mean:"", meangrad, ""std:"", np.std(rmsgrad))\n\t\tnp.savetxt(""chemspider9_force_dist.dat"", rmsgrad)\n\t\tfor i, mol in enumerate(a.mols):\n\t\t\trmsgrad = (np.sum(np.square(mol.properties[\'gradients\'])))**0.5\n\t\t\tif 2 > rmsgrad > 1.5:\n\t\t\t\tmol.WriteXYZfile(fname=""large_force"")\n\t\t\t\tprint(rmsgrad)\n\n\tif (0):\n\t\ta = MSet(""chemspider9_force"")\n\t\ta.Load()\n\t\tb = MSet(""chemspider9_force_cleaned"")\n\t\tfor i, mol in enumerate(a.mols):\n\t\t\trmsgrad = (np.sum(np.square(mol.properties[\'gradients\'])))**0.5\n\t\t\tif rmsgrad <= 1.5:\n\t\t\t\tb.mols.append(mol)\n\t\tb.Save()\n\t\tc = MSet(""chemspider9_force_cleaned_debug"")\n\t\tc.mols = b.mols[:1000]\n\t\tc.Save()\n\n\tif (1):\n\t\ta = MSet(""GoldStd"")\n\t\ta.Load()\n\t\tfor mol in a.mols:\n\t\t\tmol.properties[""gradients""] = mol.properties[""forces""]\n\t\t\t#mol.properties[""energy""] = mol.properties[""energy""] * KCALPERHARTREE\n\t\t\tmol.properties[""atomization""] = mol.properties[""energy""]\n\t\t\t#mol.properties[""energy""] = mol.properties[""energy""]/KCALPERHARTREE\n\t\t\tfor i in range(0, mol.NAtoms()):\n\t\t\t\tmol.properties[""atomization""] -= ele_E_david[mol.atoms[i]]\n\t\t#\tif abs(mol.properties[""energy""]) > 1000:\n                 #               mol.properties[""energy""] = mol.properties[""energy""] / KCALPERHARTREE\n\t\t\tprint(mol.properties[""atomization""], mol.properties[""energy""])\n\t\ta.Save()\n\t\tprint(a.mols[0].properties)\n\t\ta.mols[0].WriteXYZfile(fname=""test"")\n\ndef TrainForceField(SetName_ = ""GoldStd""):\n\ta = MSet(SetName_)\n\ta.Load()\n\tTreatedAtoms = a.AtomTypes()\n\t# PARAMS[""hidden1""] = 512\n\t# PARAMS[""hidden2""] = 512\n\t# PARAMS[""hidden3""] = 512\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 201\n\tPARAMS[""batch_size""] = 100\n\tPARAMS[""test_freq""] = 5\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""GradScalar""] = 1\n\tPARAMS[""NeuronType""] = ""relu""\n\tPARAMS[""HiddenLayers""] = [200,200,200]\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""AtomizationEnergy"")  # Initialize a digester that apply descriptor for the fragme\n\ttset = TensorMolData_BP_Direct_Linear(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_Grad_Linear"") # Initialzie a manager than manage the training of neural network.\n\tmanager.Train(maxstep=101)\n\ndef TestIRLinearDirect():\n\t""""""\n\tTest the IR spectrum produced by a network created and trained with TrainForceField()\n\tIntended to be used with MolInstance_DirectBP_EE soon...\n\t""""""\n\ta = MSet(""sampling_mols"")\n\ta.ReadXYZ()\n\tm = a.mols[0]\n\tTreatedAtoms = a.AtomTypes()\n\tPARAMS[""hidden1""] = 512\n\tPARAMS[""hidden2""] = 512\n\tPARAMS[""hidden3""] = 512\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 101\n\tPARAMS[""batch_size""] = 100\n\tPARAMS[""test_freq""] = 2\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""GradScalar""] = 1\n\tPARAMS[""NeuronType""] = ""relu""\n\tPARAMS[""HiddenLayers""] = [512,512,512]\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""AtomizationEnergy"")  # Initialize a digester that apply descriptor for the fragme\n\ttset = TensorMolData_BP_Direct_Linear(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True) # Initialize TensorMolData that contain the training data fo\n\tmanager= TFMolManage(""Mol_DavidMetaMD_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_Grad_Linear_1"" , tset, False, RandomTData_=False, Trainable_=False)\n\tForceField = lambda x: manager.Eval_BPEnergy_Direct_Grad_Linear(Mol(m.atoms,x),True,False)\n\tEnergyForceField = lambda x: manager.Eval_BPEnergy_Direct_Grad_Linear(Mol(m.atoms,x))\n\tChargeField = lambda x: np.random.random((m.NAtoms()))\n\tPARAMS[""OptMomentum""] = 0.0\n\tPARAMS[""OptMomentumDecay""] = 0.9\n\tPARAMS[""OptStepSize""] = 0.02\n\tPARAMS[""OptMaxCycles""]=200\n\tPARAMS[""MDdt""] = 0.2\n\tPARAMS[""RemoveInvariant""]=True\n\tPARAMS[""MDMaxStep""] = 100\n\tPARAMS[""MDThermostat""] = ""Nose""\n\tPARAMS[""MDV0""] = None\n\tPARAMS[""MDTemp""]= 300.0\n\tm = GeomOptimizer(EnergyForceField).Opt(m)\n\tannealx_ = Annealer(EnergyForceField, None, m, ""Anneal"")\n\tannealx_.Prop()\n\tm.coords = annealx_.Minx.copy()\n\t# now actually collect the IR.\n\tPARAMS[""MDMaxStep""] = 40000\n\tPARAMS[""MDThermostat""] = None\n\tmd = IRTrajectory(EnergyForceField, ChargeField, m,""THP_udp_grad_IR"",annealx_.v.copy())\n\tmd.Prop()\n\tWriteDerDipoleCorrelationFunction(md.mu_his,""THP_udp_grad_IR.txt"")\n\na=MSet(""SmallMols"")\na.Load()\nfor mol in a.mols:\n\tmol.properties[""gradients""] = -1 * mol.properties[""forces""]\na.Save()\n\n#TestCoulomb()\n#FTrainPrepare()\nTrainForceField(SetName_ = ""SmallMols"")\n# TestIRLinearDirect()\n'"
samples/test_tensormol01.py,2,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nprint(dir())\nimport os,sys\nos.environ[""CUDA_VISIBLE_DEVICES""]="""" # set to use CPU\n\n# Functions that load pretrained network\ndef GetWaterNetwork(a):\n\tTreatedAtoms = a.AtomTypes()\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [500, 500, 500]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0]\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""water_network"",tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\treturn manager\n\ndef GetChemSpiderNetwork(a, Solvation_=False):\n\tTreatedAtoms = np.array([1,6,7,8], dtype=np.uint8)\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\tPARAMS[""EECutoffOff""] = 15.0\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tif Solvation_:\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tmanager=TFMolManage(""chemspider12_solvation"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\telse:\n\t\tPARAMS[""DSFAlpha""] = 0.18*BOHRPERA\n\t\tmanager=TFMolManage(""chemspider12_nosolvation"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\treturn manager\n\na=MSet(""morphine"", center_=False)\na.ReadXYZ(""morphine"")\nmanager = GetChemSpiderNetwork(a, False) # load chemspider network\n\n#Use this for testing water network\n#a=MSet(""water10"", center_=False)\n#a.ReadXYZ(""water10"")\n#manager = GetWaterNetwork(a)  # load water network\n\nm = a.mols[0]\n\n# Make wrapper functions for energy, force and dipole\ndef EnAndForce(x_, DoForce=True):\n\tmtmp = Mol(m.atoms,x_)\n\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\tenergy = Etotal[0]\n\tforce = gradient[0]\n\tif DoForce:\n\t\treturn energy, force\n\telse:\n\t\treturn energy\nEnergyForceField =  lambda x: EnAndForce(x)\n\ndef ChargeField(x_):\n\tmtmp = Mol(m.atoms,x_)\n\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(mtmp, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\tenergy = Etotal[0]\n\tforce = gradient[0]\n\treturn atom_charge[0]\n\ndef EnergyField(x_):\n\treturn EnAndForce(x_,True)[0]\n\ndef DipoleField(x_):\n\tq = np.asarray(ChargeField(x_))\n\tdipole = np.zeros(3)\n\tfor i in  range(0, q.shape[0]):\n\t\tdipole += q[i]*x_[i]*BOHRPERA\n\treturn dipole\n\n# Perform geometry optimization\nif (0):\n\tPARAMS[""OptMaxCycles""]= 2000\n\tPARAMS[""OptThresh""] =0.00002\n\tOpt = GeomOptimizer(EnAndForce)\n\tm=Opt.Opt(a.mols[0],""morphine_tm_opt"")\n\tm.WriteXYZfile(""./results/"", ""optimized_morphine"")\n\n# Run molecular dynamic\nif (0):\n\tPARAMS[""MDThermostat""] = ""Nose"" # use None for\n\tPARAMS[""MDTemp""] = 300\n\tPARAMS[""MDdt""] = 0.1 # fs\n\tPARAMS[""RemoveInvariant""]=True\n\tPARAMS[""MDV0""] = ""Random""\n\tPARAMS[""MDMaxStep""] = 100000\n\tmd = VelocityVerlet(None, m, ""morphine_md_300K"", EnAndForce)\n\tmd.Prop()\n\n#Perform Harmonic frequency analysis\nif (0):\n\tmasses = np.array(list(map(lambda x: ATOMICMASSESAMU[x-1],m.atoms)))\n\tw,v = HarmonicSpectra(EnergyField, m.coords, m.atoms, WriteNM_=True, Mu_ = DipoleField)\n\n# Generate Realtime IR spectrum\nif (0):\n\t# run an annealer to warm system to the target T\n\tPARAMS[""MDdt""] = 0.1 #fs\n\tPARAMS[""RemoveInvariant""]=True\n\tPARAMS[""MDThermostat""] = ""Nose""\n\tPARAMS[""MDV0""] = ""Random""\n\tPARAMS[""MDAnnealTF""] = 300.0\n\tPARAMS[""MDAnnealT0""] = 300.0\n\tPARAMS[""MDAnnealKickBack""] = 5.0\n\tPARAMS[""MDAnnealSteps""] = 5000\n\tanneal = Annealer(EnAndForce, None, m, ""morphine_aneal"")\n\tanneal.Prop()\n\tm.coords = anneal.x.copy()\n\n\t# run an energy conserved MD trajectory for realtime IR\n\tPARAMS[""MDThermostat""] = None\n\tPARAMS[""MDTemp""] = 0\n\tPARAMS[""MDdt""] = 0.1\n\tPARAMS[""MDV0""] = None\n\tPARAMS[""MDMaxStep""] = 100000\n\tmd = IRTrajectory(EnAndForce, ChargeField, m, ""morphine_IR_300K"", anneal.v)\n\tmd.Prop()\n\tWriteDerDipoleCorrelationFunction(md.mu_his) # CorrelationFunction is stored in ""./results/MutMu0.txt"" by default\n\n#Evaluate a Set\nif (0):\n\ta=MSet(""set_test"", center_=False)\n\ta.ReadXYZ(""set_test"")\n\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSet(a, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\tprint (""Etotal:"", Etotal)\n\n\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(a.mols[0], PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\tprint (""Etotal:"", Etotal)\n\t\n\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(a.mols[1], PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\tprint (""Etotal:"", Etotal)\n\n\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(a.mols[2], PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\tprint (""Etotal:"", Etotal)\n'"
samples/test_thread.py,0,"b'import numpy as np\nimport multiprocessing as mp\nimport tensorflow as tf\nimport time\n\ndef MakeWork():\n\ttmp = np.array(range(np.power(2,22)),dtype=np.float64)\n\ttmp = np.sqrt(tmp)\n\ttmp=tmp*tmp\n\ttmp=tmp.reshape((np.power(2,11),np.power(2,11)))\n\tprint (tmp.shape)\n\ttmp=np.dot(tmp,tmp)\n\tnp.linalg.eig(tmp)\n\tprint (""Work Complete"")\n\nt0 = time.time()\nthreads = []\nfor i in range(8):\n\tt = mp.Process(target=MakeWork)\n\tthreads.append(t)\n\tt.start()\nfor t in threads:\n\tt.join()\n\nt1 = time.time()\nfor i in range(8):\n\tMakeWork()\nt2 = time.time()\n\nprint (""Times: "", t1-t0, t2-t1)\nprint (""Speedup: "", (t2-t1)/(t1-t0))\n'"
samples/test_xmas.py,1,"b'""""""\nThis test is a ludicrous sort of silly thing.\nIt takes two molecules, (a christmas tree made of cyclodextrin, and a fullerene.)\nIt then shoots the fullerene at the cyclodextrins, because why not?\nI suppose it also gives some example of velocity initalization and transposition.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""""\n\ndef GetChemSpider12(a):\n\tTreatedAtoms = np.array([1,6,7,8], dtype=np.uint8)\n\tPARAMS[""NetNameSuffix""] = ""act_sigmoid100""\n\tPARAMS[""learning_rate""] = 0.00001\n\tPARAMS[""momentum""] = 0.95\n\tPARAMS[""max_steps""] = 21\n\tPARAMS[""batch_size""] =  50   # 40 the max min-batch size it can go without memory error for training\n\tPARAMS[""test_freq""] = 1\n\tPARAMS[""tf_prec""] = ""tf.float64""\n\tPARAMS[""EnergyScalar""] = 1.0\n\tPARAMS[""GradScalar""] = 1.0/20.0\n\tPARAMS[""DipoleScaler""]=1.0\n\tPARAMS[""NeuronType""] = ""sigmoid_with_param""\n\tPARAMS[""sigmoid_alpha""] = 100.0\n\tPARAMS[""HiddenLayers""] = [2000, 2000, 2000]\n\tPARAMS[""EECutoff""] = 15.0\n\tPARAMS[""EECutoffOn""] = 0\n\t#PARAMS[""Erf_Width""] = 1.0\n\t#PARAMS[""Poly_Width""] = 4.6\n\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t#PARAMS[""AN1_r_Rc""] = 8.0\n\t#PARAMS[""AN1_num_r_Rs""] = 64\n\tPARAMS[""EECutoffOff""] = 15.0\n\t#PARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""DSFAlpha""] = 0.18\n\tPARAMS[""AddEcc""] = True\n\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 0.7]\n\t#PARAMS[""KeepProb""] = 0.7\n\tPARAMS[""learning_rate_dipole""] = 0.0001\n\tPARAMS[""learning_rate_energy""] = 0.00001\n\tPARAMS[""SwitchEpoch""] = 2\n\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\ttset = TensorMolData_BP_Direct_EE_WithEle(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\tmanager=TFMolManage(""Mol_chemspider12_maxatom35_H2O_with_CH4_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_act_sigmoid100_rightalpha"", tset,False,""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout"",False,False)\n\treturn manager\n\ntreeXYZ = """"""485\n\tEnergy:    2246.0845828\nC         13.57199       -7.12447       -3.85872\nC         13.28281       -6.17099       -5.02709\nH         12.28931       -5.73847       -4.86949\nC         13.41517       -6.90904       -6.36843\nH         14.31516       -7.53621       -6.29089\nC         13.75082       -5.94432       -7.52788\nH         14.83186       -5.98894       -7.71432\nC         13.38359       -4.48688       -7.22865\nH         13.75349       -3.84531       -8.03655\nC         13.92773       -4.00643       -5.85542\nH         13.19282       -3.38736       -5.33043\nO         14.24706       -5.10085       -4.97518\nO         15.13840       -3.28096       -6.07706\nC         15.11654       -1.84177       -5.95620\nH         16.06958       -1.57423       -6.43120\nC         15.18769       -1.36269       -4.47858\nH         15.33705       -2.23009       -3.82441\nO         13.95313       -0.76685       -4.03269\nC         13.40845        0.30392       -4.80912\nH         12.33047        0.10524       -4.85460\nC         13.91129        0.33445       -6.26548\nH         14.87901        0.83668       -6.36095\nC         14.01389       -1.11488       -6.75132\nH         13.04127       -1.60880       -6.66212\nO         14.33119       -1.13011       -8.15000\nO         13.01981        1.06824       -7.11941\nO         13.62791        1.56665       -4.15603\nC         12.41931        2.12311       -3.59705\nH         11.77573        1.29880       -3.26451\nC         11.64906        3.02582       -4.58262\nH         11.32542        2.38650       -5.41202\nO         10.45407        3.53087       -3.99457\nC         10.55670        4.21845       -2.75487\nH         11.01141        5.19529       -2.95543\nC         11.44410        3.48178       -1.71200\nH         10.91322        2.64249       -1.25658\nC         12.73995        2.93085       -2.33442\nH         13.45501        3.73506       -2.53595\nO         13.37832        2.09126       -1.35403\nO         11.78299        4.35493       -0.62319\nO          9.21943        4.54988       -2.27770\nC          8.37069        3.41590       -1.98688\nH          8.98125        2.66394       -1.48502\nC          7.75843        2.80071       -3.26940\nH          8.37666        3.07060       -4.12162\nO          6.48825        3.27891       -3.72339\nC          5.51062        3.74844       -2.80376\nH          4.92677        4.48590       -3.37004\nC          6.10998        4.53483       -1.61899\nH          5.32255        4.69198       -0.87212\nC          7.29192        3.81846       -0.96542\nH          7.73903        4.50327       -0.23322\nO          6.84044        2.67108       -0.23928\nO          6.53349        5.84119       -2.06259\nO          4.57984        2.73784       -2.38674\nC          3.71528        2.26113       -3.45140\nH          3.54556        3.07356       -4.16844\nC          2.34159        1.83433       -2.87311\nH          2.02194        2.65750       -2.21978\nO          2.37792        0.68414       -2.02097\nC          3.19960       -0.42339       -2.39012\nH          3.52099       -0.76871       -1.40163\nC          4.46977       -0.12263       -3.20194\nH          4.74575       -0.96942       -3.82605\nC          4.36144        1.06885       -4.14666\nH          5.36109        1.33756       -4.48065\nO          3.66418        0.68001       -5.33601\nO          5.55462        0.02022       -2.28440\nO          2.44689       -1.49688       -2.97619\nC          2.82672       -2.80868       -2.48012\nH          3.07358       -2.71412       -1.41550\nC          3.99288       -3.46501       -3.27074\nH          4.34328       -2.82325       -4.07992\nO          3.59797       -4.71006       -3.86892\nC          2.52765       -4.55038       -4.79122\nH          2.80342       -3.82805       -5.56894\nC          1.25843       -4.09145       -4.02012\nH          0.49532       -4.87822       -4.01500\nC          1.59774       -3.73088       -2.56367\nH          1.81547       -4.65255       -2.00761\nO          0.44849       -3.12812       -1.94675\nO          0.64182       -2.97269       -4.67737\nO          2.19709       -5.78640       -5.44181\nC          3.24245       -6.67568       -5.85623\nH          2.66975       -7.43295       -6.41107\nC          3.89537       -7.44518       -4.69380\nH          4.30231       -6.79581       -3.91820\nO          4.95713       -8.24581       -5.21189\nC          6.10108       -7.48288       -5.62265\nH          6.65457       -7.21318       -4.72280\nC          5.73365       -6.17937       -6.38929\nH          6.39646       -6.06023       -7.25256\nC          4.27608       -6.16282       -6.88043\nH          4.00554       -5.14952       -7.21022\nO          4.22559       -6.98624       -8.06373\nO          5.95775       -5.03325       -5.55347\nO          6.82812       -8.40234       -6.43188\nC          8.26264       -8.46491       -6.43820\nH          8.45944       -9.52379       -6.65396\nC          9.09483       -8.10262       -5.18317\nH          8.86713       -7.07656       -4.87107\nO         10.51324       -8.19541       -5.42279\nC         11.02480       -7.50095       -6.56177\nH         10.90185       -6.42883       -6.39443\nO         12.40140       -7.85885       -6.70846\nC         10.27084       -7.89867       -7.84914\nH         10.63825       -7.32435       -8.70510\nC          8.77426       -7.67475       -7.65281\nH          8.56506       -6.60712       -7.53081\nO          8.09025       -8.10825       -8.84035\nO         10.48273       -9.28265       -8.19218\nC          8.82581       -9.05758       -4.00260\nO          7.50512       -8.97504       -3.49539\nC          2.85438       -8.35755       -4.01823\nO          1.75899       -7.63562       -3.46855\nC          5.17868       -3.79598       -2.34720\nO          5.68702       -2.64406       -1.67770\nC          1.22788        1.67132       -3.92457\nO          1.14292        2.81376       -4.77403\nC          7.87301        1.27298       -3.18405\nO          9.23897        0.84389       -3.28059\nC         12.45056        4.17609       -5.21450\nO         13.43255        3.68794       -6.12527\nC         16.40402       -0.42683       -4.26257\nO         16.58054       -0.11712       -2.88083\nO         11.94461       -4.38392       -7.28229\nO         13.09578       -6.35389       -8.74234\nO         13.52777       -6.40201       -2.62642\nH         12.83886       -7.93472       -3.80999\nH         14.57315       -7.56190       -3.93882\nH         13.75486       -0.44594       -8.54967\nH         13.12734        2.01299       -6.85925\nH         13.97210        1.48651       -1.84715\nH         12.40533        3.84278       -0.06846\nH          7.50868        2.47298        0.43983\nH          7.37208        5.72536       -2.55168\nH          3.86125        1.35045       -6.01516\nH          5.64076       -0.83837       -1.81865\nH          0.75813       -2.62531       -1.17134\nH          0.20561       -2.47185       -3.95635\nH          4.31800       -7.91466       -7.78648\nH          6.91758       -4.88290       -5.50362\nH          7.15598       -8.25178       -8.58846\nH          9.85308       -9.46702       -8.91771\nH          9.04226      -10.09646       -4.27794\nH          9.50778       -8.81388       -3.17962\nH          6.94338       -9.57692       -4.02000\nH          2.46348       -9.09709       -4.73152\nH          3.32913       -8.93273       -3.21645\nH          1.37070       -7.10691       -4.19945\nH          4.87900       -4.53395       -1.59423\nH          5.99308       -4.24599       -2.92610\nH          6.41934       -2.96616       -1.11659\nH          1.36251        0.77710       -4.53829\nH          0.26274        1.55389       -3.41725\nH          0.42691        2.63482       -5.40929\nH          7.52721        0.92999       -2.21107\nH          7.32504        0.78090       -3.98801\nH          9.21410       -0.12537       -3.19288\nH         11.77244        4.83470       -5.77012\nH         12.94063        4.79568       -4.45707\nH         13.92547        4.47356       -6.43187\nH         17.32076       -0.91459       -4.61515\nH         16.29500        0.52201       -4.79213\nH         17.36510        0.45793       -2.81952\nH         11.73829       -4.69060       -8.19170\nH         13.13159       -7.33317       -8.74922\nH         14.11869       -5.63414       -2.75647\nH         17.61134       -8.07851        7.27725\nO         17.16533       -7.51258        6.61820\nH         16.94615       -7.48351        3.87923\nO         16.72046       -5.58541        4.39472\nH         16.93940       -3.64314        6.50543\nH         16.36717       -6.99935        1.86617\nH         16.51890       -9.17834        5.61488\nH         16.63503       -5.87869        6.83307\nH         16.46898       -3.61772        4.25236\nC         16.08435       -6.80698        3.98593\nC         16.09514       -8.28468        6.08803\nC         15.94401       -4.44077        4.75123\nC         15.97958       -4.11914        6.27035\nC         15.51358       -6.81213        2.53171\nO         15.91663       -5.26982        7.12048\nH         15.53903       -9.41211        3.31132\nC         15.22621       -7.50321        5.07282\nH         15.46699       -8.63364        6.91577\nO         15.01563       -5.54781        2.07640\nH         15.43953       -2.19221        2.86546\nO         14.61693       -8.40656        0.93824\nH         14.43031       -7.66853        0.32452\nH         15.26685       -1.68356        5.30180\nO         15.20608       -2.45685        7.83140\nC         14.43484       -7.88573        2.26176\nC         14.52699       -8.98901        3.31867\nH         15.32528       -3.10507        8.55105\nO         14.60330       -4.44008        4.26873\nC         14.79683       -3.21648        6.68392\nH         14.66626       -6.74656        5.62685\nH         14.42147       -5.24513        2.79575\nH         14.50793       -3.70639        1.60254\nC         14.36112       -2.32846        2.99147\nO         14.25291       -8.43091        4.59472\nH         14.24935      -11.14258        4.68011\nC         14.38074       -2.29082        5.53266\nO         13.90238      -11.56155        5.49077\nC         14.01533       -3.11631        4.27198\nO         13.67979      -10.12456        3.10648\nO         13.86545       -3.00550        1.83991\nH         13.89267       -1.33911        3.01764\nH         13.43833       -7.42994        2.28923\nH         13.94086       -3.82494        7.00067\nO         13.38018       -1.30999        5.83793\nC         12.54775      -11.13356        5.61316\nH         12.94073       -3.30963        4.22350\nH         12.53281      -10.14234        6.07995\nC         12.27700       -9.93605        3.37611\nH         12.16433      -10.10648        1.20374\nH         12.05523      -12.06824        3.76016\nH         12.04959      -11.81600        6.30963\nH         12.56692        0.44943        5.41800\nC         11.81270      -11.12019        4.25932\nH         12.08962       -8.99855        3.91449\nC         12.09855       -1.77066        6.29435\nC         11.49670       -9.91286        2.05157\nO         12.15803       -2.01724        7.69289\nH         11.81167       -2.69338        5.78497\nO         11.63818        0.36751        5.11882\nO         10.97639       -8.59038        1.81766\nH         10.79323      -11.92563        1.84050\nH         11.33179       -4.40890        8.13692\nC         11.06938       -0.66160        5.94557\nC         10.35107      -10.93727        2.01759\nO         10.40859      -11.07879        4.51764\nH         11.28021        0.41835        7.81223\nC         10.89408       -2.29850        8.32331\nH         11.11314       -2.24907        9.39796\nH         10.33116       -8.71493        1.09041\nC         10.46859       -3.74315        8.02195\nC         10.48339       -0.05986        7.22947\nC          9.57588      -10.99722        3.36062\nH         10.26360       -1.09826        5.34387\nO          9.51776      -10.64168        0.88583\nH         10.11898       -3.85039        6.98981\nC          9.83157       -1.19287        8.05496\nH          8.95744      -11.89945        3.34539\nO          9.44930       -4.18381        8.90274\nO          9.54997        0.98715        6.92665\nH          8.72034      -11.19860        0.93279\nO          8.76208       -9.83496        3.50960\nH          9.31265       -5.13309        8.74939\nH          9.49140       -0.80119        9.02151\nH          9.09480        0.74393        6.09971\nO          8.69167       -1.72621        7.34177\nH          8.06261       -8.26357        1.40322\nC          7.34329      -10.05205        3.40508\nH          7.13590      -10.74650        2.58144\nC          6.99680       -8.26313        1.64193\nH          7.32997      -10.24411        5.57109\nH          7.19581       -7.90112        3.74069\nC          7.48244       -0.98538        7.60227\nH          7.72263       -0.00345        8.02129\nO          6.98444      -12.06468        4.74425\nC          6.73867       -8.66664        3.09921\nC          6.77337      -10.64047        4.71312\nO          7.26387       -1.89514        9.87871\nH          6.59935       -7.26209        1.44474\nO          6.33054       -9.16585        0.75699\nO          6.87585       -0.73459        6.33409\nH          6.85724       -3.80184        8.25021\nH          6.62691       -3.52589        6.09574\nH          6.39922      -12.41859        5.44365\nC          6.58540       -1.70049        8.62598\nH          6.72486       -2.54377       10.37309\nC          6.09053       -3.03711        8.07836\nC          5.78757       -2.99578        6.56932\nH          5.93343       -1.73159        4.86757\nC          5.79120       -1.58164        5.94679\nO          5.31651       -8.64375        3.28796\nH          5.38133       -9.10547        0.97376\nH          5.55861       -8.25657        5.32206\nC          5.26899      -10.37763        4.90257\nH          5.51438       -4.69738        4.74351\nH          5.41116      -10.36270        6.87007\nC          4.96868       -8.88417        4.64853\nH          5.73403       -1.06179        8.86927\nO          4.88259      -10.83538        6.20728\nO          4.89942       -5.96919        6.21914\nH          4.69680      -10.99200        4.19397\nC          4.66001       -4.79823        5.42370\nO          4.96725       -3.47490        8.86095\nH          4.83910       -8.30885        7.44212\nO          4.21361       -6.45410        3.04713\nO          4.54510       -3.65455        6.27153\nH          4.22435       -7.41325        2.84681\nC          4.46430       -0.80165        6.03272\nH          4.58766        0.18178        5.56569\nH          4.57776       -4.22934        8.37701\nC          3.92531       -7.73577        7.60156\nO          4.08193       -7.05755        8.84992\nO          3.57571       -8.63099        4.82380\nO          4.04303       -0.58605        7.36871\nC          3.69395       -6.70318        6.49431\nH          4.07049       -7.73114        9.55245\nC          3.36183       -4.84778        4.56120\nH          3.45688       -4.11146        3.75443\nC          3.18067       -6.25903        4.00844\nH          3.66662       -1.33553        5.50627\nC          3.14267       -7.30132        5.17102\nH          3.06770       -8.40974        7.69092\nH          3.24893       -0.02468        7.33888\nH          2.97274       -5.99034        6.91230\nH          2.23231       -6.32919        3.46246\nH          2.51382       -3.77721        5.94297\nO          2.20155       -4.46504        5.31709\nH          2.07468       -7.45039        5.38510\nH          2.56343       -8.81643       18.39700\nO          3.41345       -8.67494       17.94238\nH          2.85271       -9.79904       16.33975\nC          3.64067       -9.79721       17.09907\nH          3.53986      -10.71569       17.68638\nH          5.03609       -6.16807       16.66610\nH          4.32041       -7.80224       15.68633\nO          6.36350       -4.72907       18.33333\nH          3.56987       -9.46112       13.60524\nH          6.33375       -4.13504       17.56019\nH          3.57666      -11.15588       14.90276\nC          5.21898       -8.42813       15.62772\nC          6.10676       -6.36405       16.53693\nC          5.04552       -9.72306       16.46273\nO          5.31997       -7.56100       13.36749\nC          4.59531       -9.85394       13.61588\nC          6.81328       -6.01928       17.86834\nO          6.56726       -5.46775       15.52226\nH          6.52300       -6.74853       18.63317\nC          4.60993      -11.10128       14.54428\nH          5.32920       -7.84772       12.43363\nC          5.51300       -8.74975       14.15617\nH          6.47076       -5.16883       13.36601\nH          5.75650       -9.68887       17.29828\nO          6.34856       -7.71972       16.17163\nO          4.88815      -10.08270       12.23612\nO          5.38818      -10.91495       15.72063\nO          4.72520      -12.40089       13.98341\nO          7.36836       -5.05994       12.98945\nC          8.00027       -5.45244       15.29726\nC          8.34851       -5.98256       17.72204\nH          8.27726       -4.39814       15.40700\nH          6.56522       -9.02899       14.02348\nH          8.73976       -4.99873       18.00892\nH          5.80881      -10.42545       12.20517\nC          8.22130       -5.83524       13.83231\nO          8.94059       -6.91588       18.64199\nH          7.98667       -6.88842       13.65677\nH          5.99499      -13.58356       16.31753\nC          8.80802       -6.32008       16.29342\nC          5.93165      -13.04652       13.59563\nH          5.51999      -12.61966       11.50233\nH          5.58393      -14.08720       13.53062\nH          8.56342       -7.37131       16.11469\nC          6.37705      -12.72234       12.17670\nC          6.87368      -13.94387       15.77436\nH          9.24647       -5.64257       13.51692\nO          7.05887      -11.44561       12.15157\nH          9.89372       -6.94054       18.43912\nC          7.16776      -13.07387       14.53859\nH          7.42907      -12.06725       14.87877\nH         10.36692       -4.35665       15.72366\nH          6.69421      -14.98569       15.48823\nO         10.23993       -6.36695       16.25581\nO          7.98027      -13.94501       16.67165\nC         10.99377       -5.24456       15.83547\nH          7.69906      -11.54503       11.40772\nH          6.74218      -14.61028       11.15590\nC          7.32019      -13.81949       11.64815\nO         12.32474       -3.52290       16.93397\nH         11.78263       -5.24780       17.86163\nH          7.44580      -15.23303       13.23962\nO          8.39445      -13.57734       13.88279\nH         10.44902       -7.47962       13.70251\nC         12.10522       -4.94644       16.85854\nO          8.08742      -13.18268       10.62267\nO         11.59628       -5.51326       14.56320\nC          8.12558      -14.48482       12.81344\nH          8.78427      -14.07710       16.13689\nH         12.75549       -3.24266       16.10650\nH         10.98816       -8.39479       15.10698\nC         11.33031       -7.89097       14.19827\nH          8.74690      -13.83875       10.29905\nC         12.34328       -6.76349       14.50018\nH         11.08236       -9.34118       12.94291\nC         13.41789       -5.64969       16.48029\nO          9.20723      -15.31327       12.37810\nO         11.84924       -8.87931       13.32643\nH         12.69285       -7.66280       16.49527\nC         13.19713       -7.00697       15.77824\nH         10.19281      -12.79394       11.83596\nO         14.20761       -5.84896       17.66662\nH         13.01398       -6.66609       13.63876\nH         14.02710       -5.00234       15.83585\nH         10.73541      -14.17032       14.41942\nO         10.47064      -13.87785       10.12470\nC         10.82744      -13.61966       11.49620\nH         12.82492       -9.65649       15.75587\nC         10.57365      -14.87715       12.33548\nH         10.85804      -13.13309        9.61471\nH         11.95038      -12.39739       13.51984\nC         11.26965      -14.82092       13.71971\nH         14.93823       -6.43712       17.38956\nH         10.37267      -16.70107       14.40136\nO         14.53564       -7.49223       15.55679\nH         13.60108      -10.71359       17.59876\nC         13.88212       -9.91025       15.85609\nO         13.09176      -12.17850       15.81561\nO         14.13942       -9.96692       17.26686\nH         13.01434      -10.75475       13.49367\nH         11.07370      -15.68578       11.78261\nO         12.39967      -11.94356       10.82087\nC         11.35598      -16.22798       14.33223\nC         12.28921      -13.18039       11.53951\nC         12.67736      -13.04360       13.01034\nC         14.77437       -8.84934       15.16420\nO         11.90149      -16.14806       15.64951\nO         12.62948      -14.32724       13.61874\nC         14.08897      -11.30606       15.25786\nH         13.30021      -13.08737       15.51201\nC         13.97334      -11.20940       13.73417\nO         14.78902       -8.93271       13.74047\nH         13.34426      -11.69543       10.85484\nH         12.02233      -16.86135       13.73492\nH         12.92577      -13.90969       11.02347\nO         14.00644      -12.49816       13.10711\nH         12.24454      -17.03188       15.87025\nC         15.03868      -10.24432       13.19621\nH         15.80319       -9.02168       15.49919\nH         15.05132      -11.72827       15.56704\nH         14.89672      -10.14251       12.11292\nC         16.48631      -10.73142       13.40694\nH         16.78451      -10.69997       14.45774\nH         16.60931      -11.75405       13.03733\nO         17.39186       -9.90281       12.68247\nH         18.29458      -10.16003       12.94216\nC         -1.31810       -0.91669       -0.19502\nC         -2.80154       -0.47827       -0.18049\nC         -0.40159        0.34291       -0.23833\nO         -1.02225       -1.82408       -1.22997\nO         -3.69909       -1.54058       -0.33047\nO         -0.44081        0.97618       -1.49119\nC          1.04426        0.00810        0.23733\nC          1.82345        1.31400        0.58105\nO          1.78628       -0.66343       -0.75864\nC          3.31942        1.01601        0.71767\nO          1.27056        1.94053        1.71022\nO          3.85495        0.85340        1.78888\nH         -1.08163       -1.52061        0.71301\nH         -3.04613        0.17550       -1.03772\nH         -3.01496        0.08462        0.74884\nH         -0.83196        1.12438        0.43197\nH         -1.46904       -1.52463       -2.01290\nH         -3.50281       -2.18331        0.34021\nH          0.08545        0.46483       -2.09499\nH          0.98150       -0.63337        1.15244\nH          1.66832        2.08171       -0.21273\nH          1.35900       -1.49887       -0.90016\nH          3.89522        0.94952       -0.21814\nH          1.54150        1.45212        2.47913\nC         15.49603      -11.09219       22.32711\nC         14.41103       -9.99245       22.22547\nC         12.98262      -10.61255       22.32052\nC         11.88827       -9.49933       22.23549\nC         10.50055      -10.12820       22.37381\nO         16.76583      -10.56405       22.58812\nO         14.61961       -9.34788       20.98880\nO         12.80598      -11.37270       23.48949\nO         12.05197       -8.71751       21.07462\nO          9.72433      -10.20141       21.45139\nH         15.50807      -11.72174       21.41573\nH         15.31786      -11.75137       23.19491\nH         14.54535       -9.24458       23.04818\nH         12.83074      -11.36709       21.51473\nH         12.03065       -8.74767       23.04734\nH         10.24107      -10.52409       23.36775\nH         17.00019      -10.01122       21.85273\nH         13.86880       -8.76921       20.85183\nH         13.12004      -10.85438       24.22112\nH         11.63580       -9.17305       20.35029\n""""""\n\nballXYZ = """"""20\n\nC          5.52099        0.88543       -0.73505\nC          4.32460        0.06439       -0.82090\nC          3.17513        2.86436       -2.01768\nC          3.15538        0.91408       -0.82084\nC          5.46288        2.86075       -2.03795\nC          3.56604        2.30614       -0.73468\nC          6.17378        0.68584       -2.01786\nC          5.01570        2.24585       -0.80945\nC          4.31280       -0.77831       -2.00514\nC          2.35753        0.64250       -2.00465\nC          4.31308        3.33411       -2.78994\nC          6.26835        1.91329       -2.79043\nC          5.45075       -0.30857       -2.77739\nC          3.16300       -0.30496       -2.75713\nC          2.45211        1.86995       -2.77722\nC          4.30128        2.49140       -3.97417\nC          5.47050        1.64171       -3.97424\nC          5.05984        0.24965       -4.06040\nC          3.61019        0.30995       -3.98563\nC          3.10489        1.67036       -4.06002\n""""""\n\ndef Xmas():\n\t""""""\n\tShoot a tree with a ball.\n\t""""""\n\ttree = Mol()\n\ttree.FromXYZString(treeXYZ)\n\ttree.coords -= tree.Center()\n\tball = Mol()\n\tball.FromXYZString(ballXYZ)\n\tball.coords -= ball.Center()\n\tball.coords -= np.array([15.0,0.0,0.0])\n\tntree = tree.NAtoms()\n\ttoshoot = Mol(np.concatenate([tree.atoms,ball.atoms],axis=0),np.concatenate([tree.coords,ball.coords],axis=0))\n\n\t# The tree is at rest, and the ball is headed towards the tree :)\n\tv0 = np.zeros(toshoot.coords.shape)\n\tv0[ntree:] -= np.array([-0.1581,0.0,0.0]) # Angstrom/fs.ch\n\n\t# Everything\'s ready! now just propagate.\n\tdef GetEnergyForceForMol(m):\n\t\ts = MSet()\n\t\ts.mols.append(m)\n\t\tmanager = GetChemSpider12(s)\n\t\tdef EnAndForce(x_, DoForce=True):\n\t\t\ttmpm = Mol(m.atoms,x_)\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = manager.EvalBPDirectEEUpdateSingle(tmpm, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tenergy = Etotal[0]\n\t\t\tforce = gradient[0]\n\t\t\tif DoForce:\n\t\t\t\treturn energy, force\n\t\t\telse:\n\t\t\t\treturn energy\n\t\treturn EnAndForce\n\tF = GetEnergyForceForMol(toshoot)\n\tPARAMS[""MDThermostat""]=None\n\tPARAMS[""MDV0""]=None\n\ttraj = VelocityVerlet(None, toshoot,""MerryXmas"",F)\n\ttraj.v = v0.copy()\n\ttraj.Prop()\n\nXmas()\n'"
samples/tmp.py,14,"b'import numpy as np\nimport tensorflow as tf\nfrom TensorMol.RawEmbeddings import *\n\nsample_xyzs = tf.tile(tf.expand_dims(tf.constant([[0.000, 0.000, 0.000],[0.757, 0.586, 0.000],[-0.757, 0.586, 0.000]], dtype=tf.float32), axis=0), [1000000, 1, 1])\n\nrotation_params = tf.concat([np.pi * tf.expand_dims(tf.tile(tf.linspace(0.0, 2.0, 100), [10000]), axis=1),\n\t\tnp.pi * tf.reshape(tf.tile(tf.expand_dims(tf.linspace(0.0, 2.0, 100), axis=1), [1,10000]), [1000000,1]),\n\t\ttf.reshape(tf.tile(tf.expand_dims(tf.expand_dims(tf.linspace(0.2, 1.8, 100), axis=1),\n\t\taxis=2), [100,1,100]), [1000000,1])], axis=1)\n\nrotated_xyzs = tf_random_rotate(sample_xyzs, rotation_params)\ngrads = tf.gradients(rotated_xyzs, rotation_params)[0]\nbad_params_idx = tf.where(tf.is_inf(grads))\nbad_params = tf.gather_nd(rotation_params, bad_params_idx)\nlarge_grad_idx = tf.where(tf.greater(tf.abs(grads), 1))\nlarge_grad = tf.gather_nd(rotation_params, large_grad_idx)\n\nz=tf.Variable(0.0)\nr=tf.sqrt(z)\ngradr=tf.gradients(r, z)\n\n\nwith tf.Session() as sess:\n\tsess.run(tf.global_variables_initializer())\n\tbad_p = sess.run(gradr)\n\nprint bad_p\n\n'"
samples/training_sample.py,4,"b'from __future__ import absolute_import\nfrom TensorMol import *\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""]=""2"" # Choose your GPU, here is set to use CPU\nfrom TensorMol.Interfaces.TMIPIinterface import *\nimport random\n\ndef Train():\n\tif (1):   # learning energy and gradient\n\t# energy should be in hartree, gradient should be in hartree/angstrom\n\t\ta = MSet(""water_mini"") # water_mini.pdb is in folder ""./datasets/""\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""training_sample""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 15 # Train for 5 epochs in total\n\t\tPARAMS[""batch_size""] =  100\n\t\tPARAMS[""test_freq""] = 1 # Test for every epoch\n\t\tPARAMS[""tf_prec""] = ""tf.float64"" # double precsion\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param"" # choose activation function\n\t\tPARAMS[""sigmoid_alpha""] = 100.0  # activation params\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0] # each layer\'s keep probability for dropout\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""AtomizationEnergy"")\n\t\ttset = TensorMolData_BP_Direct_EandG_Release(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EandG_SymFunction"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\tif (0): # learning energy, gradient and dipole\n\t# energy should be in hartree, gradient should be in hartree/angstrom, dipole should be in a.u.\n\t\ta = MSet(""water_mini"") # water_mini.pdb is in folder ""./datasets/""\n\t\ta.Load()\n\t\trandom.shuffle(a.mols)\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""training_sample""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 15 # Train for 5 epochs in total\n\t\tPARAMS[""batch_size""] =  100\n\t\tPARAMS[""test_freq""] = 1 # Test for every epoch\n\t\tPARAMS[""tf_prec""] = ""tf.float64"" # double precsion\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""] = 1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param"" # choose activation function\n\t\tPARAMS[""sigmoid_alpha""] = 100.0  # activation params\n\t\tPARAMS[""HiddenLayers""] = [100, 100, 100]  # number of neurons in each layer\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0] # each layer\'s keep probability for dropout\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001  # learning rate for dipole learning\n\t\tPARAMS[""learning_rate_energy""] = 0.00001 # learning rate for energy & grads learning\n\t\tPARAMS[""SwitchEpoch""] = 5  # Train dipole for 2 epochs, then train energy & grads\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle_Release(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EE_SymFunction"")\n\t\tPARAMS[\'Profiling\']=0\n\t\tmanager.Train(1)\n\n\ndef Eval():\n\tif (1): # evaluate the energy & gradient learning model\n\t\ta=MSet(""H2O_trimer_move"", center_=False) # Evaluate on a water trimers\n\t\ta.ReadXYZ(""H2O_trimer_move"")\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""training_sample""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 15 # Train for 5 epochs in total\n\t\tPARAMS[""batch_size""] =  100\n\t\tPARAMS[""test_freq""] = 1 # Test for every epoch\n\t\tPARAMS[""tf_prec""] = ""tf.float64"" # double precsion\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param"" # choose activation function\n\t\tPARAMS[""sigmoid_alpha""] = 100.0  # activation params\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0] # each layer\'s keep probability for dropout\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""AtomizationEnergy"")\n\t\ttset = TensorMolData_BP_Direct_EandG_Release(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager=TFMolManage("""",tset,False,""fc_sqdiff_BP_Direct_EandG_SymFunction"")\n\t\tmanager = TFMolManage(""Mol_water_mini_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EandG_SymFunction_training_sample"", tset,False, ""fc_sqdiff_BP_Direct_EandG_SymFunction"", False, False)\n\t\ttotal_e = []\n\t\tfor m in a.mols:\n\t\t\tEtotal, Ebp, Ebp_atom, force = manager.EvalBPDirectEandGLinearSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""])\n\t\t\tprint (""Unit of energy: a.u"")\n\t\t\tprint (""Etotal: %8.6f"" %(Etotal))\n\t\t\tprint (""Unit of diple: Joules/Angstrom"")\n\t\t\tprint (""force:"", force)\n\t\t\ttotal_e.append(Etotal)\n\t\tnp.savetxt(""EanGlearning.dat"", np.asarray(total_e))\n\n\tif (1): # evalate the energy, gradient and dipole learning model\n\t\ta=MSet(""H2O_trimer_move"", center_=False) # Evaluate on a water trimers\n\t\ta.ReadXYZ(""H2O_trimer_move"")\n\t\tTreatedAtoms = a.AtomTypes()\n\t\tPARAMS[""NetNameSuffix""] = ""training_sample""\n\t\tPARAMS[""learning_rate""] = 0.00001\n\t\tPARAMS[""momentum""] = 0.95\n\t\tPARAMS[""max_steps""] = 5 # Train for 5 epochs in total\n\t\tPARAMS[""batch_size""] =  100\n\t\tPARAMS[""test_freq""] = 1 # Test for every epoch\n\t\tPARAMS[""tf_prec""] = ""tf.float64"" # double precsion\n\t\tPARAMS[""EnergyScalar""] = 1.0\n\t\tPARAMS[""GradScalar""] = 1.0/20.0\n\t\tPARAMS[""DipoleScaler""] = 1.0\n\t\tPARAMS[""NeuronType""] = ""sigmoid_with_param"" # choose activation function\n\t\tPARAMS[""sigmoid_alpha""] = 100.0  # activation params\n\t\tPARAMS[""HiddenLayers""] = [100, 100, 100]  # number of neurons in each layer\n\t\tPARAMS[""EECutoff""] = 15.0\n\t\tPARAMS[""EECutoffOn""] = 0\n\t\tPARAMS[""Elu_Width""] = 4.6  # when elu is used EECutoffOn should always equal to 0\n\t\tPARAMS[""EECutoffOff""] = 15.0\n\t\tPARAMS[""DSFAlpha""] = 0.18\n\t\tPARAMS[""AddEcc""] = True\n\t\tPARAMS[""KeepProb""] = [1.0, 1.0, 1.0, 1.0] # each layer\'s keep probability for dropout\n\t\tPARAMS[""learning_rate_dipole""] = 0.0001  # learning rate for dipole learning\n\t\tPARAMS[""learning_rate_energy""] = 0.00001 # learning rate for energy & grads learning\n\t\tPARAMS[""SwitchEpoch""] = 2  # Train dipole for 2 epochs, then train energy & grads\n\n\t\td = MolDigester(TreatedAtoms, name_=""ANI1_Sym_Direct"", OType_=""EnergyAndDipole"")  # Initialize a digester that apply descriptor for the fragme\n\t\ttset = TensorMolData_BP_Direct_EE_WithEle_Release(a, d, order_=1, num_indis_=1, type_=""mol"",  WithGrad_ = True)\n\t\tmanager = TFMolManage(""Mol_water_mini_ANI1_Sym_Direct_fc_sqdiff_BP_Direct_EE_SymFunction_training_sample"", tset,False, ""fc_sqdiff_BP_Direct_EE_SymFunction"", False, False)\n\n\t\ttotal_e = []\n\t\tfor m in a.mols:\n\t\t\tEtotal, Ebp, Ebp_atom ,Ecc, Evdw, mol_dipole, atom_charge, force = manager.EvalBPDirectEELinearSingle(m, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""], True)\n\t\t\tprint (""Unit of energy: a.u"")\n\t\t\tprint (""Etotal: %8.6f  Ebp: %8.6f  Ecc: %8.6f  Evdw: %8.6f"" %(Etotal, Ebp, Ecc, Evdw))\n\t\t\tprint (""Unit of diple: a.u"")\n\t\t\tprint (""Dipole: "", mol_dipole)\n\t\t\tprint (""Unit of diple: Joules/Angstrom"")\n\t\t\tprint (""force:"", force)\n\t\t\ttotal_e.append(Etotal)\n\t\tnp.savetxt(""EElearning.dat"", np.asarray(total_e))\n\nTrain()  # Training should be finished in about an hour depends on your computer\nEval() # Evaluate the network that you trained\n'"
C_API/test/setup.py,0,"b""from distutils.core import setup, Extension\nimport numpy\n\n# define the extension module\nMake_CM = Extension('Make_CM', sources=['Make_CM.cpp'],extra_compile_args=['-std=c++0x'],\n                          include_dirs=[numpy.get_include()])\n\n# run the setup\nsetup(ext_modules=[Make_CM])\n"""
C_API/test/test.py,0,"b'import Make_CM,numpy\n\n\na=[]\nb=numpy.zeros((3,10))\nc=numpy.zeros((5,6))\na.append(b)\na.append(c)\nA=40\nB=30\nprint Make_CM.Make_CM(a,A,B)\n'"
TensorMol/Containers/Digest.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom .Mol import *\nfrom ..Util import *\nimport os,sys,re\nimport numpy as np\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\nfrom ..Math import LinearOperations\nif (HAS_EMB):\n\timport MolEmb\n\nclass Digester:\n\t""""""\n\tAn Embedding gives some chemical description of a molecular\n\tEnvironment around a point. This one is for networks that will embed properties of atoms.\n\tplease refer to /C_API/setup.py\n\n\tnote: Molecule embeddings and Behler-Parrinello are in DigestMol.\n\t""""""\n\tdef __init__(self, eles_, name_=""GauSH"", OType_=""Disp""):\n\t\t""""""\n\n\t\tArgs:\n\t\t\teles_ : a list of elements in the Tensordata that I\'ll digest\n\t\t\tname_: type of digester to reduce molecules to NN inputs.\n\t\t\tOType_: property of the molecule which will be learned (energy, force, etc)\n\t\t""""""\n\n\t\t # In Atomic units at 300K\n\t\t# These are the key variables which determine the type of digestion.\n\t\tself.name = name_ # Embedding type.\n\t\tself.eshape=None  #shape of an embedded case\n\t\tself.lshape=None  #shape of the labels of an embedded case.\n\t\tself.OType = OType_ # Output Type: HardP, SmoothP, StoP, Disp, Force, Energy etc. See Emb() for options.\n\n\t\tself.NTrainSamples=1 # Samples per atom. Should be made a parameter.\n\t\tif (self.OType == ""SmoothP"" or self.OType == ""Disp""):\n\t\t\tself.NTrainSamples=1 #Smoothprobability only needs one sample because it fits the go-probability and pgaussians-center.\n\n\t\tself.eles = np.array(eles_)\n\t\tself.eles.sort() # Consistent list of atoms in the order they are treated.\n\t\tself.neles = len(eles_) # Consistent list of atoms in the order they are treated.\n\t\tself.nsym = self.neles+(self.neles+1)*self.neles  # channel of sym functions\n\t\tself.npgaussian = self.neles # channel of PGaussian\n\t\t# Instead self.emb should know it\'s return shape or it should be testable.\n\n\t\tself.SamplingType = PARAMS[""dig_SamplingType""]\n\t\tself.TrainSampDistance=2.0 #how far in Angs to sample on average.\n\t\tself.ngrid = PARAMS[""dig_ngrid""] #this is a shitty parameter if we go with anything other than RDF and should be replaced.\n\t\tself.BlurRadius = PARAMS[""BlurRadius""] # Stdev of gaussian used as prob of atom\n\t\tself.SensRadius=6.0 # Distance which is used for input.\n\t\tself.embtime=0.0\n\t\tself.outtime=0.0\n\t\tself.Print()\n\t\treturn\n\n\tdef Print(self):\n\t\tLOGGER.info(""-------------------- "")\n\t\tLOGGER.info(""Digester Information "")\n\t\tLOGGER.info(""self.name: ""+self.name)\n\t\tLOGGER.info(""self.OType: ""+self.OType)\n\t\tLOGGER.debug(""self.NTrainSamples: ""+str(self.NTrainSamples))\n\t\tLOGGER.debug(""self.TrainSampDistance: ""+str(self.TrainSampDistance))\n\t\tLOGGER.debug(""self.OType: ""+self.OType)\n\t\tLOGGER.info(""-------------------- "")\n\t\treturn\n\n\tdef MakeSamples_v2(self,point):    # with sampling function f(x)=M/(x+1)^2+N; f(0)=maxdisp,f(maxdisp)=0; when maxdisp =5.0, 38 % lie in (0, 0.1)\n\t\tdisps = samplingfunc_v2(self.TrainSampDistance * np.random.random(self.NTrainSamples), self.TrainSampDistance)\n\t\ttheta  = np.random.random(self.NTrainSamples)* math.pi\n\t\tphi = np.random.random(self.NTrainSamples)* math.pi * 2\n\t\tgrids  = np.zeros((self.NTrainSamples,3),dtype=np.float64)\n\t\tgrids[:,0] = disps*np.cos(theta)\n\t\tgrids[:,1] = disps*np.sin(theta)*np.cos(phi)\n\t\tgrids[:,2] = disps*np.sin(theta)*np.sin(phi)\n\t\treturn grids + point\n\n#\n#  Embedding functions, called by batch digests. Use outside of Digester() is discouraged.\n#  Instead call a batch digest routine.\n#\n\n\tdef Emb(self, mol_, at_, xyz_, MakeOutputs=True, MakeGradients=False, Transforms=None):\n\t\t""""""\n\t\tGenerates various molecular embeddings.\n\n\t\tArgs:\n\t\t\tmol_: a Molecule to be digested\n\t\t\tat_: an atom to be digested or moved. if at_ < 0 it usually returns arrays for each atom in the molecule\n\t\t\txyz_: makes inputs with at_ moved to these positions.\n\t\t\tMakeOutputs: generates outputs according to self.OType.\n\t\t\tMakeGradients: Generate nuclear derivatives of inputs.\n\t\t\tTransforms: Generate inputs for all the linear transformations appended.\n\n\t\tReturns:\n\t\t\tOutput embeddings, and possibly labels and gradients.\n\t\t\tif at_ < 0 the first dimension loops over atoms in mol_\n\t\t""""""\n\t\t#start = time.time()\n\t\tif (self.name==""CZ""):\n\t\t\tif (at_ > 0):\n\t\t\t\traise Exception(""CoordZ embedding is done moleculewise always"")\n\t\t\tIns = np.zeros((mol_.NAtoms,4))\n\t\t\tIns[:,0] = mol_.atoms\n\t\t\tIns[:,1:] = mol_.coords\n\t\telif (self.name==""Coulomb""):\n\t\t\tIns= MolEmb.Make_CM(mol_.coords, xyz_, mol_.atoms , self.eles ,  self.SensRadius, self.ngrid, at_, 0.0)\n\t\telif (self.name==""GauSH""):\n\t\t\tif (Transforms == None):\n\t\t\t\tIns =  MolEmb.Make_SH(PARAMS, mol_.coords, mol_.atoms, at_)\n\t\t\telse:\n\t\t\t\tIns =  MolEmb.Make_SH_Transf(PARAMS, mol_.coords, mol_.atoms, at_, Transforms)\n\t\telif (self.name==""GauInv""):\n\t\t\tIns= MolEmb.Make_Inv(PARAMS, mol_.coords, mol_.atoms, at_)\n\t\telif (self.name==""RDF""):\n\t\t\tIns= MolEmb.Make_RDF(mol_.coords, xyz_, mol_.atoms , self.eles ,  self.SensRadius, self.ngrid, at_, 0.0)\n\t\telif (self.name==""SensoryBasis""):\n\t\t\tIns= mol_.OverlapEmbeddings(mol_.coords, xyz_, mol_.atoms , self.eles ,  self.SensRadius, self.ngrid, at_, 0.0)\n\t\telif (self.name==""SymFunc""):\n\t\t\tIns= self.make_sym(mol_.coords, xyz_, mol_.atoms , self.eles ,  self.SensRadius, self.ngrid, at_, 0.0)\n\t\telif(self.name == ""ANI1_Sym""):\n\t\t\tIns = MolEmb.Make_ANI1_Sym(PARAMS, mol_.coords,  mol_.atoms, self.eles, at_)\n\t\telse:\n\t\t\traise Exception(""Unknown Embedding Function."")\n\t\t#self.embtime += (time.time() - start)\n\t\t#start = time.time()\n\t\tOuts=None\n\t\tif (MakeOutputs):\n\t\t\tif (self.OType==""HardP""):\n\t\t\t\tOuts = self.HardCut(xyz_-coords_[at_])\n\t\t\telif (self.OType==""SmoothP""):\n\t\t\t\tOuts = mol_.FitGoProb(at_)\n\t\t\telif (self.OType==""Disp""):\n\t\t\t\tOuts = mol_.GoDisp(at_)\n\t\t\telif (self.OType==""GoForce""):\n\t\t\t\tOuts = mol_.GoForce(at_)\n\t\t\telif (self.OType==""GoForceSphere""):\n\t\t\t\tOuts = mol_.GoForce(at_, 1) # See if the network is better at doing spherical=>spherical\n\t\t\telif (self.OType==""Force""):\n\t\t\t\tif ( ""forces"" in mol_.properties):\n\t\t\t\t\tif (at_<0):\n\t\t\t\t\t\tOuts = mol_.properties[\'forces\']\n\t\t\t\t\t\t#print ""Outs"", Outs\n\t\t\t\t\telse:\n\t\t\t\t\t\tOuts = mol_.properties[\'forces\'][at_].reshape((1,3))\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Mol Is missing force. "")\n\t\t\telif (self.OType==""Del_Force""):\n\t\t\t\tif ( ""forces"" in mol_.properties):\n\t\t\t\t\tif ( ""mmff94forces"" in mol_.properties):\n\t\t\t\t\t\tif (at_<0):\n\t\t\t\t\t\t\tOuts = mol_.properties[\'forces\']\n\t\t\t\t\t\t\tIns = np.append(Ins, mol_.properties[""mmff94forces""],axis=1)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tOuts = mol_.properties[\'forces\'][at_].reshape((1,3))\n\t\t\t\t\t\t\tIns = np.append(Ins, mol_.properties[\'mmff94forces\'][at_].reshape((1,3)), axis=1)\n\t\t\t\t\telse:\n\t\t\t\t\t\traise Exception(""Mol Is missing MMFF94 force. "")\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Mol Is missing force. "")\n\t\t\telif (self.OType==""ForceSphere""):\n\t\t\t\tif ( ""sphere_forces"" in mol_.properties):\n\t\t\t\t\tif (at_<0):\n\t\t\t\t\t\tOuts = mol_.properties[\'sphere_forces\']\n\t\t\t\t\t\t#print ""Outs"", Outs\n\t\t\t\t\telse:\n\t\t\t\t\t\tOuts = mol_.properties[\'sphere_forces\'][at_].reshape((1,3))\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Mol Is missing spherical force. "")\n\t\t\telif (self.OType==""ForceMag""):\n\t\t\t\tif ( ""forces"" in mol_.properties):\n\t\t\t\t\tif (at_<0):\n\t\t\t\t\t\tOuts = np.array(np.linalg.norm(mol_.properties[\'forces\'], axis=1))\n\t\t\t\t\telse:\n\t\t\t\t\t\tOuts = np.array(np.linalg.norm(mol_.properties[\'forces\'][at_])).reshape((1,1))\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Mol Is missing force. "")\n\t\t\telif (self.OType==""StoP""):\n\t\t\t\tens_ = mol_.EnergiesOfAtomMoves(xyz_,at_)\n\t\t\t\tif (ens_==None):\n\t\t\t\t\traise Exception(""Empty energies..."")\n\t\t\t\tprint(ens_.min(), ens_.max())\n\t\t\t\tEs=ens_-ens_.min()\n\t\t\t\tBoltz=np.exp(-1.0*Es/PARAMS[""KAYBEETEE""])\n\t\t\t\trnds = np.random.rand(len(xyz_))\n\t\t\t\tOuts = np.array([1 if rnds[i]<Boltz[i] else 0 for i in range(len(ens_))])\n\t\t\telif (self.OType==""Energy""):\n\t\t\t\tif (""energy"" in mol_.properties):\n\t\t\t\t\tens_ = mol_.properties[""energy""]\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Empty energies..."")\n\t\t\telif (self.OType==""AtomizationEnergy""):\n\t\t\t\tif (""atomization"" in mol_.properties):\n\t\t\t\t\tens_ = mol_.properties[""atomization""]\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Empty energies..."")\n\t\t\telif (self.OType==""CalcEnergy""):\n\t\t\t\tens_ = mol_.EnergiesOfAtomMoves(xyz_,at_)\n\t\t\t\tif (ens_==None):\n\t\t\t\t\traise Exception(""Empty energies..."")\n\t\t\t\tE0=np.min(ens_)\n\t\t\t\tEs=ens_-E0\n\t\t\t\tOuts = Es\n\t\t\telse:\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\t#self.outtime += (time.time() - start)\n\t\t\t#print ""Embtime: "", self.embtime, "" OutTime: "", self.outtime\n\t\t\treturn Ins,Outs\n\t\telse:\n\t\t\treturn Ins\n\n#\n#  Various types of Batch Digests.\n#\n\n\tdef TrainDigestMolwise(self, mol_, MakeOutputs_=True):\n\t\t""""""\n\t\tReturns list of inputs and outputs for a molecule.\n\t\tUses self.Emb() uses Mol to get the Desired output type (Energy,Force,Probability etc.)\n\t\tThis version works mol-wise to try to speed up and avoid calling C++ so much...\n\n\t\tArgs:\n\t\t\tmol_: a molecule to be digested\n\t\t\teles_: A list of elements coming from Tensordata to order the output.\n\n\t\tReturns:\n\t\t\tTwo lists: containing inputs and outputs in order of eles_\n\t\t""""""\n\t\treturn self.Emb(mol_,-1,mol_.coords[0], MakeOutputs_) # will deal with getting energies if it\'s needed.\n\n\tdef TrainDigest(self, mol_, ele_, MakeDebug=False):\n\t\t""""""\n\t\tReturns list of inputs and outputs for a molecule.\n\t\tUses self.Emb() uses Mol to get the Desired output type (Energy,Force,Probability etc.)\n\n\t\tArgs:\n\t\t\tmol_: a molecule to be digested\n\t\t\tele_: an element for which training data will be made.\n\t\t\tMakeDebug: if MakeDebug is True, it also returns a list with debug information to trace possible errors in digestion.\n\t\t""""""\n\t\tif (self.eshape==None or self.lshape==None):\n\t\t\ttinps, touts = self.Emb(mol_,0,np.array([[0.0,0.0,0.0]]))\n\t\t\tself.eshape = list(tinps[0].shape)\n\t\t\tself.lshape = list(touts[0].shape)\n\t\t\tLOGGER.debug(""Assigned Digester shapes: ""+str(self.eshape)+str(self.lshape))\n\t\tncase = mol_.NumOfAtomsE(ele_)*self.NTrainSamples\n\t\tins = np.zeros(shape=tuple([ncase]+list(self.eshape)),dtype=np.float64)\n\t\touts = np.zeros(shape=tuple([ncase]+list(self.lshape)),dtype=np.float64)\n\t\tdbg=[]\n\t\tcasep=0\n\t\tfor i in range(len(mol_.atoms)):\n\t\t\tif (mol_.atoms[i]==ele_):\n\t\t\t\tif (self.OType == ""SmoothP"" or self.OType == ""Disp"" or self.OType == ""Force""):\n\t\t\t\t\tinputs, outputs = self.Emb(mol_,i,mol_.coords[i]) # will deal with getting energies if it\'s needed.\n\t\t\t\telif(self.SamplingType==""Smooth""): #If Smooth is now a property of the Digester: OType SmoothP\n\t\t\t\t\tsamps=PointsNear(mol_.coords[i], self.NTrainSamples, self.TrainSampDistance)\n\t\t\t\t\tinputs, outputs = self.Emb(mol_,i,samps) # will deal with getting energies if it\'s needed.\n\t\t\t\telse:\n\t\t\t\t\tsamps=self.MakeSamples_v2(mol_.coords[i])\n\t\t\t\t\tinputs, outputs = self.Emb(mol_,i,samps)\n\t\t\t\t# Here we should write a short routine to debug/print the inputs and outputs.\n\t\t\t\t#\t\t\t\tprint ""Smooth"",outputs\n\t\t\t\t#print i, mol_.atoms, mol_.coords,mol_.coords[i],""Samples:"",samps,""inputs "", inputs, ""Outputs"",outputs, ""Distances"",np.array(map(np.linalg.norm,samps-mol_.coords[i]))\n\n\t\t\t\tins[casep:casep+self.NTrainSamples] = np.array(inputs)\n\t\t\t\touts[casep:casep+self.NTrainSamples] = outputs\n\t\t\t\tcasep += self.NTrainSamples\n\t\tif (MakeDebug):\n\t\t\treturn ins,outs,dbg\n\t\telse:\n\t\t\treturn ins,outs\n\n\tdef UniformDigest(self, mol_, at_, mxstep, num):\n\t\t"""""" Returns list of inputs sampled on a uniform cubic grid around at """"""\n\t\tncase = num*num*num\n\t\tsamps=MakeUniform(mol_.coords[at_],mxstep,num)\n\t\tif (self.name==""SymFunc""):\n\t\t\tinputs = self.Emb(self, mol_, at_, samps, None, False) #(self.EmbF())(mol_.coords, samps, mol_.atoms, self.eles ,  self.SensRadius, self.ngrid, at_, 0.0)\n\t\t\tinputs = np.asarray(inputs)\n\t\telse:\n\t\t\tinputs = self.Emb(self, mol_, at_, samps, None, False)\n\t\t\tinputs = np.assrray(inputs[0])\n\t\treturn samps, inputs\n\n\tdef emb_vary_coords(self, coords, xyz, atoms, eles, Radius, ngrid, vary_at, tar_at):\n\t\treturn  MolEmb.Make_CM_vary_coords(coords, xyz, atoms, eles, Radius, ngrid, vary_at, tar_at)\n\n\tdef make_sym(self, coords_, xyz_, ats_,  eles , SensRadius, ngrid, at_, dummy):    #coords_, xyz_, ats_, self.eles ,  self.SensRadius, self.ngrid, at_, 0.0\n\t\tzeta=[]\n\t\teta1=[]\n\t\teta2=[]\n\t\tRs=[]\n\t\tfor i in range (0, ngrid):\n\t\t\tzeta.append(1.5**i)    # set some value for zeta, eta, Rs\n\t\t\teta1.append(0.008*(2**i))\n\t\t\teta2.append(0.002*(2**i))\n\t\t\tRs.append(i*SensRadius/float(ngrid))\n\t\tSYM =  MolEmb.Make_Sym(coords_, xyz_, ats_, eles, at_, SensRadius, zeta, eta1, eta2, Rs)\n\t\tSYM = np.asarray(SYM[0], dtype=np.float64)\n\t\tSYM = SYM.reshape((SYM.shape[0]/self.nsym, self.nsym,  SYM.shape[1] *  SYM.shape[2]))\n\t\treturn SYM\n'"
TensorMol/Containers/DigestMol.py,0,"b'""""""\n Calculate an embeeding for a molecule, such as coulomb matrix\n TODO: Should inherit from Digest.py (which needs cleanup)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .Mol import *\nfrom ..Util import *\n\nclass MolDigester:\n\tdef __init__(self, eles_, name_=""Coulomb"", OType_=""FragEnergy"", SensRadius_=6):\n\t\tself.name = name_\n\t\tself.OType = OType_\n\t\tself.lshape = None  # output is just the energy\n\t\tself.eshape = None\n\t\tself.egshape = None # this is PER ATOM\n\t\tself.SensRadius = SensRadius_\n\t\tself.eles = eles_\n\t\tself.neles = len(eles_) # Consistent list of atoms in the order they are treated.\n\t\tself.ngrid = 5 #this is a shitty parameter if we go with anything other than RDF and should be replaced.\n\t\tself.nsym = self.neles+(self.neles+1)*self.neles  # channel of sym functions\n\t\tif (name_ == ""CZ""):\n\t\t\tself.eshape = 1 # HACK\n\n\tdef make_sym_update(self, mol, Rc = 4.0, g1_para_mat = None, g2_para_mat = None): # para_mat should contains the parameters of Sym_Func, on the order of: G1:Rs, eta1 ; G2: zeta, eta2\n\t\tif g1_para_mat == None or g2_para_mat == None: # use the default setting\n\t\t\tngrid = 5\n\t\tRs = []; eta1 = []; zeta = []; eta2 = []\n\t\tfor i in range (0, ngrid):\n\t\t\tzeta.append(1.5**i); eta1.append(0.008*(2**i)); eta2.append(0.002*(2**i)); Rs.append(i*Rc/float(ngrid))\n\t\tg1_para_mat = np.array(np.meshgrid(Rs, eta1)).T.reshape((-1,2))\n\t\tg2_para_mat = np.array(np.meshgrid(zeta, eta2)).T.reshape((-1,2))\n\t\tSYM_Ins = MolEmb.Make_Sym_Update(mol.coords,  mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), Rc, g1_para_mat,g2_para_mat,  -1) # -1 means do it for all atoms\n\t\tSYM_Ins_deri = np.zeros((SYM_Ins.shape[0], SYM_Ins.shape[1]))\n\t\treturn SYM_Ins, SYM_Ins_deri\n\n\tdef make_sym(self, mol):\n\t\tzeta=[]\n\t\teta1=[]\n\t\teta2=[]\n\t\tRs=[]\n\t\teles = list(set(list(mol.atoms)))\n\t\tSYM = []\n\t\tfor i in range (0, self.ngrid):\n\t\t\tzeta.append(1.5**i)    # set some value for zeta, eta, Rs\n\t\t\teta1.append(0.008*(2**i))\n\t\t\teta2.append(0.002*(2**i))\n\t\t\tRs.append(i*self.SensRadius/float(self.ngrid))\n\t\tfor i in range (0, mol.NAtoms()):\n\t\t\tsym =  MolEmb.Make_Sym(mol.coords, (mol.coords[i]).reshape((1,-1)), mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), i, self.SensRadius, zeta, eta1, eta2, Rs)\n\t\t\tsym = np.asarray(sym[0], dtype=np.float64)\n\t\t\tsym = sym.reshape((self.nsym*sym.shape[1] *  sym.shape[2]))\n\t\t\t#print ""sym"", sym\n\t\t\tSYM.append(sym)\n\t\tSYM =  np.asarray(SYM)\n\t\tprint(""mol.atoms"", mol.atoms, ""SYM"", SYM)\n\t\tSYM_deri = np.zeros((SYM.shape[0], SYM.shape[1])) # debug, it will take some work to implement to derivative of sym func.\n\t\treturn SYM, SYM_deri\n\n\tdef make_cm_bp(self, mol):\n\t\tCM_BP = []\n\t\tngrids = 10\n\t\tfor i in range (0, mol.NAtoms()):\n\t\t\tcm_bp = MolEmb.Make_CM(mol.coords, (mol.coords[i]).reshape((1,-1)), mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), self.SensRadius, ngrids, i,  0.0 )\n\t\t\tcm_bp = np.asarray(cm_bp[0], dtype=np.float64)\n\t\t\tcm_bp.flatten()\n\t\t\tCM_BP.append(cm_bp)\n\t\tCM_BP = np.array(CM_BP)\n\t\tCM_BP = CM_BP.reshape((CM_BP.shape[0],-1))\n\t\tCM_BP_deri = np.zeros((CM_BP.shape[0], CM_BP.shape[1])) # debug, it will take some work to implement to derivative of coloumb_bp func.\n\t\treturn \tCM_BP, CM_BP_deri\n\n\tdef make_cm_bond_bp(self, mol):\n\t\tCM_Bond_BP = []\n\t\tngrids = 10\n\t\tfor i in range (0, mol.NBonds()):\n\t\t\tcm_bp_1 = MolEmb.Make_CM(mol.coords, mol.coords[int(mol.bonds[i][2])].reshape((1,-1)), mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), self.SensRadius, ngrids, int(mol.bonds[i][2]),  0.0 )\n\t\t\tcm_bp_2 = MolEmb.Make_CM(mol.coords, mol.coords[int(mol.bonds[i][3])].reshape((1,-1)), mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), self.SensRadius, ngrids, int(mol.bonds[i][3]),  0.0 )\n\t\t\tdist = mol.bonds[i][1]\n\t\t\tcm_bp_1  = cm_bp_1[0][0].reshape(-1)\n\t\t\tcm_bp_2  = cm_bp_2[0][0].reshape(-1)\n\t\t\tcm_bond_bp = np.asarray(list(cm_bp_1)+list(cm_bp_2)+[1.0/dist], dtype=np.float64)\n\t\t\tcm_bond_bp.flatten()\n\t\t\tCM_Bond_BP.append(cm_bond_bp)\n\t\tCM_Bond_BP = np.array(CM_Bond_BP)\n\t\tCM_Bond_BP_deri = np.zeros((CM_Bond_BP.shape[0], CM_Bond_BP.shape[1])) # debug, it will take some work to implement to derivative of coloumb_bp func.\n\t\treturn  CM_Bond_BP, CM_Bond_BP_deri\n\n\tdef make_ANI1_sym(self, mol, MakeGradients_ = False):  # ANI-1 default setting\n\t\teles = list(set(list(mol.atoms)))\n\t\tt = time.time()\n\t\tANI1_Ins = MolEmb.Make_ANI1_Sym(PARAMS, mol.coords,  mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), -1) # -1 means do it for all atoms\n\t\tLOGGER.debug(""descriptor time: %f"", time.time() - t)\n\t\tif (MakeGradients_):\n\t\t\tt = time.time()\n\t\t\tANI1_Ins_deri = MolEmb.Make_ANI1_Sym_deri(PARAMS, mol.coords,  mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), -1)\n\t\t\tLOGGER.debug(""derivative time: %f"", time.time() - t)\n\t\t\treturn ANI1_Ins, ANI1_Ins_deri\n\t\telse:\n\t\t\treturn ANI1_Ins, None\n\n\tdef make_ANI1_sym_bond_bp(self, mol):  # ANI-1 default setting\n\t\tANI1_Ins_bond_bp = []\n\t\tfor i in range (0, mol.NBonds()):\n\t\t\tANI1_Ins_1 = MolEmb.Make_ANI1_Sym(PARAMS, mol.coords,  mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), int(mol.bonds[i][2])) # -1 means do it for all atoms\n\t\t\tANI1_Ins_2 = MolEmb.Make_ANI1_Sym(PARAMS, mol.coords,  mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), int(mol.bonds[i][3])) # -1 means do it for all atoms\n\t\t\tdist = mol.bonds[i][1]\n\t\t\tANI1_Ins_1 = ANI1_Ins_1.reshape((-1))\n\t\t\tANI1_Ins_2 = ANI1_Ins_2.reshape((-1))\n\t\t\tANI1_Ins_bond_bp.append(np.asarray(list(ANI1_Ins_1)+list(ANI1_Ins_2)+[1.0/dist], dtype=np.float64))\n\t\tANI1_Ins_bond_bp = np.asarray(ANI1_Ins_bond_bp)\n\t\tANI1_Ins_bond_bp_deri = np.zeros((ANI1_Ins_bond_bp.shape[0], ANI1_Ins_bond_bp.shape[1]))\n\t\treturn ANI1_Ins_bond_bp, ANI1_Ins_bond_bp_deri\n\n\tdef make_ANI1_sym_center_bond_bp(self, mol):  # ANI-1 default setting\n\t\tANI1_Ins_bond_bp = []\n\t\tfor i in range (0, mol.NBonds()):\n\t\t\tcenter = (mol.coords[ int(mol.bonds[i][2])] + mol.coords[ int(mol.bonds[i][3])] ) /2.0\n\t\t\ttmpcoords = np.zeros((mol.NAtoms()+1, 3))\n\t\t\ttmpcoords[:-1] = mol.coords\n\t\t\ttmpcoords[-1] = center\n\t\t\ttmpatoms = np.zeros(mol.NAtoms()+1)\n\t\t\ttmpatoms[:-1] = mol.atoms\n\t\t\ttmpatoms[-1] = 0\n\t\t\tANI1_Ins = MolEmb.Make_ANI1_Sym(PARAMS, tmpcoords, tmpatoms.astype(np.uint8), self.eles.astype(np.uint8), mol.NAtoms()) # -1 means do it for all atoms\n\t\t\tdist = mol.bonds[i][1]\n\t\t\tANI1_Ins = ANI1_Ins.reshape((-1))\n\t\t\tANI1_Ins_bond_bp.append(np.asarray(list(ANI1_Ins)+[1.0/dist], dtype=np.float64))\n\t\tANI1_Ins_bond_bp = np.asarray(ANI1_Ins_bond_bp)\n\t\tANI1_Ins_bond_bp_deri = np.zeros((ANI1_Ins_bond_bp.shape[0], ANI1_Ins_bond_bp.shape[1]))\n\t\treturn ANI1_Ins_bond_bp, ANI1_Ins_bond_bp_deri\n\n\tdef make_cm_bp(self, mol):\n\t\tCM_BP = []\n\t\tngrids = 10\n\t\tfor i in range (0, mol.NAtoms()):\n\t\t\tcm_bp = MolEmb.Make_CM(mol.coords, (mol.coords[i]).reshape((1,-1)), mol.atoms.astype(np.uint8), self.eles.astype(np.uint8), self.SensRadius, ngrids, i,  0.0 )\n\t\t\tcm_bp = np.asarray(cm_bp[0], dtype=np.float64)\n\t\t\tcm_bp.flatten()\n\t\t\tCM_BP.append(cm_bp)\n\t\tCM_BP = np.array(CM_BP)\n\t\tCM_BP = CM_BP.reshape((CM_BP.shape[0],-1))\n\t\tCM_BP_deri = np.zeros((CM_BP.shape[0], CM_BP.shape[1])) # debug, it will take some work to implement to derivative of coloumb_bp func.\n\t\treturn \tCM_BP, CM_BP_deri\n\n\tdef make_connectedbond_bond_bp(self, mol):\n\t\tConnectedBond_Bond_BP = []\n\t\tfor i in range (0, mol.NBonds()):\n\t\t\tatom1 = int(mol.bonds[i][2])\n\t\t\tatom2 = int(mol.bonds[i][3])\n\t\t\t#print ""bond :"", i, ""index:"", atom1, atom2, ""ele type:"", mol.atoms[atom1], mol.atoms[atom2]\n\t\t\tconnected_bond1 = [[] for j in range (0, self.neles)]\n\t\t\tfor node in mol.atom_nodes[atom1].connected_nodes:\n\t\t\t\tatom_index = node.node_index\n\t\t\t\tif atom_index != atom2:\n\t\t\t\t\tdist = mol.DistMatrix[atom1][atom_index]\n\t\t\t\t\tconnected_bond1[list(self.eles).index(mol.atoms[atom_index])].append(1.0/dist)\n\t\t\tconnected_bond1 = [b[0:self.ngrid] + [0]*(self.ngrid-len(b)) for b in connected_bond1 ]\n\t\t\tconnected_bond1 = [item for sublist in connected_bond1 for item in sublist]\n\n\t\t\tconnected_bond2 = [[] for j in range (0, self.neles)]\n\t\t\tfor node in mol.atom_nodes[atom2].connected_nodes:\n\t\t\t\tatom_index = node.node_index\n\t\t\t\tif atom_index != atom1:\n\t\t\t\t\tdist = mol.DistMatrix[atom2][atom_index]\n\t\t\t\t\tconnected_bond2[list(self.eles).index(mol.atoms[atom_index])].append(1.0/dist)\n\t\t\tconnected_bond2 = [b[0:self.ngrid] + [0]*(self.ngrid-len(b)) for b in connected_bond2 ]\n\t\t\tconnected_bond2 = [item for sublist in connected_bond2 for item in sublist]\n\n\t\t\tdist = mol.bonds[i][1]\n\t\t\tconnectedbond_bond_bp = np.asarray(connected_bond1+ connected_bond2 +[1.0/dist], dtype=np.float64)\n\t\t\t#print ""connectedbond_bond_bp"", connectedbond_bond_bp\n\t\t\tConnectedBond_Bond_BP.append(connectedbond_bond_bp)\n\t\tConnectedBond_Bond_BP = np.array(ConnectedBond_Bond_BP)\n\t\tConnectedBond_Bond_BP_deri = np.zeros((ConnectedBond_Bond_BP.shape[0], ConnectedBond_Bond_BP.shape[1]))\n\t\treturn  ConnectedBond_Bond_BP, ConnectedBond_Bond_BP_deri\n\n\tdef make_connectedbond_angle_bond_bp(self, mol):\n\t\tself.ngrid = 3\n\t\tConnectedBond_Angle_Bond_BP = []\n\t\tfor i in range (0, mol.NBonds()):\n\t\t\tatom1 = int(mol.bonds[i][2])\n\t\t\tatom2 = int(mol.bonds[i][3])\n\t\t\tbond_dist =  mol.bonds[i][1]\n\t\t\tconnected_bond1 = [[] for j in range (0, self.neles)]\n\t\t\tconnected_angle1 = [[] for j in range (0, self.neles)]\n\t\t\tfor node in mol.atom_nodes[atom1].connected_nodes:\n\t\t\t\tatom_index = node.node_index\n\t\t\t\tif atom_index != atom2:\n\t\t\t\t\tdist = mol.DistMatrix[atom1][atom_index]\n\t\t\t\t\tdist2 = mol.DistMatrix[atom2][atom_index]\n\t\t\t\t\tconnected_bond1[list(self.eles).index(mol.atoms[atom_index])].append(1.0/dist)\n\t\t\t\t\tconnected_angle1[list(self.eles).index(mol.atoms[atom_index])].append((bond_dist**2+dist**2-dist2**2)/(2*bond_dist*dist))\n\t\t\t#print ""before sorting 1:"",connected_bond1, connected_angle1\n\t\t\tfor j  in range (0, len(connected_bond1)):\n\t\t\t\tconnected_angle1[j] = [x for (y, x) in sorted(zip(connected_bond1[j], connected_angle1[j]))]\n\t\t\t\tconnected_bond1[j].sort()\n\t\t\t\tconnected_bond1[j].reverse()\n\t\t\t\tconnected_angle1[j].reverse()\n\t\t\tconnected_bond1 = [b[0:self.ngrid] + [0]*(self.ngrid-len(b)) for b in connected_bond1 ]\n\t\t\tconnected_bond1 = [item for sublist in connected_bond1 for item in sublist]\n\t\t\tconnected_angle1 = [b[0:self.ngrid] + [0]*(self.ngrid-len(b)) for b in connected_angle1 ]\n\t\t\tconnected_angle1 = [item for sublist in connected_angle1 for item in sublist]\n\n\t\t\tconnected_bond2 = [[] for j in range (0, self.neles)]\n\t\t\tconnected_angle2 = [[] for j in range (0, self.neles)]\n\t\t\tfor node in mol.atom_nodes[atom2].connected_nodes:\n\t\t\t\tatom_index = node.node_index\n\t\t\t\tif atom_index != atom1:\n\t\t\t\t\tdist = mol.DistMatrix[atom2][atom_index]\n\t\t\t\t\tdist2 = mol.DistMatrix[atom1][atom_index]\n\t\t\t\t\tconnected_bond2[list(self.eles).index(mol.atoms[atom_index])].append(1.0/dist)\n\t\t\t\t\tconnected_angle2[list(self.eles).index(mol.atoms[atom_index])].append((bond_dist**2+dist**2-dist2**2)/(2*bond_dist*dist))\n\n\t\t\tfor j  in range (0, len(connected_bond2)):\n\t\t\t\tconnected_angle2[j] = [x for (y, x) in sorted(zip(connected_bond2[j], connected_angle2[j]))]\n\t\t\t\tconnected_bond2[j].sort()\n\t\t\t\tconnected_bond2[j].reverse()\n\t\t\t\tconnected_angle2[j].reverse()\n\t\t\tconnected_bond2 = [b[0:self.ngrid] + [0]*(self.ngrid-len(b)) for b in connected_bond2 ]\n\t\t\tconnected_bond2 = [item for sublist in connected_bond2 for item in sublist]\n\t\t\tconnected_angle2 = [b[0:self.ngrid] + [0]*(self.ngrid-len(b)) for b in connected_angle2 ]\n\t\t\tconnected_angle2 = [item for sublist in connected_angle2 for item in sublist]\n\t\t\tconnectedbond_angle_bond_bp = np.asarray(connected_bond1 + connected_bond2 + connected_angle1 + connected_angle2 + [1.0/bond_dist], dtype=np.float64)\n\t\t\tConnectedBond_Angle_Bond_BP.append(connectedbond_angle_bond_bp)\n\t\tConnectedBond_Angle_Bond_BP = np.array(ConnectedBond_Angle_Bond_BP)\n\t\tConnectedBond_Angle_Bond_BP_deri = np.zeros((ConnectedBond_Angle_Bond_BP.shape[0], ConnectedBond_Angle_Bond_BP.shape[1]))\n\t\treturn  ConnectedBond_Angle_Bond_BP, ConnectedBond_Angle_Bond_BP_deri\n\n\tdef make_gauinv(self, mol):\n\t\t"""""" This is a totally inefficient way of doing this\n\t\t\tMolEmb should loop atoms. """"""\n\t\tGauInv = []\n\t\tfor i in range (0, mol.NAtoms()):\n\t\t\tgauinv = MolEmb.Make_Inv(mol.coords, (mol.coords[i]).reshape((1,-1)), mol.atoms.astype(np.uint8),self.SensRadius, i)\n\t\t\tgauinv = np.asarray(gauinv[0], dtype = np.float64 )\n\t\t\tgauinv = gauinv.reshape(-1)\n\t\t\tGauInv.append(gauinv)\n\t\tGauInv = np.asarray(GauInv)\n\t\tGauInv_deri = np.zeros((GauInv.shape[0], GauInv.shape[1]))\n\t\treturn GauInv, GauInv_deri\n\n\tdef make_cm(self, mol_):\n\t\tnatoms  = mol_.NAtoms()\n\t\tCM=np.zeros((natoms, natoms))\n\t\tderi_CM = np.zeros((natoms, natoms, 6))\n\t\txyz = (mol_.coords).copy()\n\t\t#ele = (mol_.atoms).copy()\n\t\tele = np.zeros(natoms)  # for eebug\n\t\tele.fill(1.0)   # for debug, set ele value to 1\n\t\tcode = """"""\n\t\tdouble dist = 0.0;\n\t\tdouble deri[6] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0};\n\t\tfor (int j=0; j < natoms; j++) {\n\t\t\tfor (int k=0; k<natoms; k++) {\n\t\t\t\tif (k==j) {\n\t\t\t\t\tdist=1.0;\n\t\t\t\t\tderi[0] =0.0;\n\t\t\t\t\tderi[1] =0.0;\n\t\t\t\t\tderi[2] =0.0;\n\t\t\t\t\tderi[3] =0.0;\n\t\t\t\t\tderi[4] =0.0;\n\t\t\t\t\tderi[5] =0.0;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tdist=sqrt(pow(xyz[j*3+0]-xyz[k*3+0],2) + pow(xyz[j*3+1]-xyz[k*3+1],2) + pow(xyz[j*3+2]-xyz[k*3+2],2));\n\t\t\t\t\tderi[0] = -(xyz[j*3+0]-xyz[k*3+0])/(dist*dist*dist);\n\t\t\t\t\tderi[1] = -(xyz[j*3+1]-xyz[k*3+1])/(dist*dist*dist);\n\t\t\t\t\tderi[2] = -(xyz[j*3+2]-xyz[k*3+2])/(dist*dist*dist);\n\t\t\t\t\tderi[3] = -deri[0];\n\t\t\t\t\tderi[4] = -deri[1];\n\t\t\t\t\tderi[5] = -deri[2];\n\t\t\t\t}\n\t\t\t\tCM[natoms*j+k]= ele[j]*ele[k]/dist;\n\t\t\t\tderi_CM[natoms*j*6+k*6+0]=ele[j]*ele[k]*deri[0];\n\t\t\t\tderi_CM[natoms*j*6+k*6+1]=ele[j]*ele[k]*deri[1];\n\t\t\t\tderi_CM[natoms*j*6+k*6+2]=ele[j]*ele[k]*deri[2];\n\t\t\t\tderi_CM[natoms*j*6+k*6+3]=ele[j]*ele[k]*deri[3];\n\t\t\t\tderi_CM[natoms*j*6+k*6+4]=ele[j]*ele[k]*deri[4];\n\t\t\t\tderi_CM[natoms*j*6+k*6+5]=ele[j]*ele[k]*deri[5];\n\t\t\t}\n\n\t\t}\n\n\t\t""""""\n\t\tfrom scipy.weave import inline\n\t\tres = inline(code, [\'CM\',\'natoms\',\'xyz\',\'ele\', \'deri_CM\'],headers=[\'<math.h>\',\'<iostream>\'], compiler=\'gcc\')\n\t\treturn CM, deri_CM\n\n\tdef GetUpTri(self, CM):\n\t\tCM = CM[np.triu_indices(CM.shape[0], 1)].copy() ##for debug purpose, ignore the diagnoal element\n\t\t#index = np.array([2,3,4,6,7,8,9,10,11]) # for debug, ignore the AA, BB block, only remain AB\n\t\t#CM = CM[index].copy()  # for debug\n\t\treturn  CM  #for debug purpose, ignore the diagnoal element\n\n\tdef EvalDigest(self, mol_, gradient_ = True):\n\t\tif gradient_:\n\t\t\treturn self.Emb(mol_,False, True)\n\t\telse:\n\t\t\treturn self.Emb(mol_,False, False)\n\n\tdef Emb(self, mol_, MakeOutputs=True, MakeGradients=False):\n\t\t""""""\n\t\tGenerates various molecular embeddings.\n\t\tIf the embedding has BP on the end it comes out atomwise and includes all atoms in the molecule.\n\n\t\tArgs:\n\t\t\tmol_: a Molecule to be digested\n\t\t\tMakeOutputs: generates outputs according to self.OType.\n\t\t\tMakeGradients: generates outputs according to self.OType.\n\n\t\tReturns:\n\t\t\tOutput embeddings, and possibly labels and gradients.\n\n\t\tTODO:\n\t\t\tHook up the gradients.\n\t\t""""""\n\t\tIns=None\n\t\tGrads=None\n\t\tOuts=None\n\t\tif (self.name==""CZ""):\n\t\t\tIns = np.zeros((mol_.NAtoms(),4))\n\t\t\tIns[:,0] = mol_.atoms\n\t\t\tIns[:,1:] = mol_.coords\n\t\telif (self.name == ""Coulomb""):\n\t\t\tCM, deri_CM = self.make_cm(mol_)\n\t\t\tIns = self.GetUpTri(CM)\n\t\telif(self.name == ""Coulomb_BP""):\n\t\t\tIns, deri_CM_BP =  self.make_cm_bp(mol_)\n\t\t\tIns = Ins.reshape([Ins.shape[0],-1])\n\t\telif(self.name == ""Coulomb_Bond_BP""):\n\t\t    Ins, Grads =  self.make_cm_bond_bp(mol_)\n\t\t    Ins = Ins.reshape([Ins.shape[0],-1])\n\t\telif(self.name == ""SymFunc""):\n\t\t\tIns, SYM_deri = self.make_sym(mol_)\n\t\telif(self.name == ""SymFunc_Update""):\n\t\t\tIns, Grads = self.make_sym_update(mol_)\n\t\telif(self.name == ""GauInv_BP""):\n\t\t\tIns =  MolEmb.Make_Inv(PARAMS, mol_.coords, mol_.atoms, -1)\n\t\telif(self.name == ""GauSH_BP""):\n\t\t    # Here I have to add options to octahedrally average SH\n\t\t\tIns =  MolEmb.Make_SH(PARAMS, mol_.coords, mol_.atoms, -1)\n\t\telif(self.name == ""ConnectedBond_Bond_BP""):\n\t\t\tIns, Grads =  self.make_connectedbond_bond_bp(mol_)\n\t\t\tIns = Ins.reshape([Ins.shape[0],-1])\n\t\telif(self.name == ""ConnectedBond_Angle_Bond_BP""):\n\t\t\tIns, Grads = self.make_connectedbond_angle_bond_bp(mol_)\n\t\telif(self.name == ""ANI1_Sym""):\n\t\t\tIns, Grads = self.make_ANI1_sym(mol_, MakeGradients_ = MakeGradients)\n\t\telif(self.name == ""ANI1_Sym_Bond_BP""):\n\t\t\tIns, Grads = self.make_ANI1_sym_bond_bp(mol_)\n\t\telif(self.name == ""ANI1_Sym_Center_Bond_BP""):\n\t\t\tIns, Grads = self.make_ANI1_sym_center_bond_bp(mol_)\n\t\telif (self.name==""GauSH""):\n\t\t\tIns =  MolEmb.Make_SH(PARAMS, mol_.coords, mol_.atoms, -1)\n\t\telse:\n\t\t\traise Exception(""Unknown MolDigester Type."", self.name)\n\t\tif (self.eshape == None):\n\t\t\tself.eshape=Ins.shape[1:] # The first dimension is atoms. eshape is per-atom.\n\t\t\tif (MakeGradients and self.egshape == None):\n\t\t\t\tself.egshape=Grads.shape[1:]\n\t\t\t\tprint(""Grads Shape: "", Grads.shape)\n\t\tif (MakeOutputs):\n\t\t\tif (self.OType == ""Energy""):\n\t\t\t\tOuts = np.array([mol_.properties[""energy""]])\n\t\t\telif (self.OType == ""AEAndForce""):\n\t\t\t\ten = mol_.properties[""atomization""]\n\t\t\t\tfrce = mol_.properties[""force""]\n\t\t\t\tOuts = np.zeros(3*mol_.NAtoms()+1, dtype = np.float64)\n\t\t\t\tOuts[0] = en\n\t\t\t\tOuts[1:] = frce.flatten()\n\t\t\telif (self.OType == ""Force""):\n\t\t\t\t# Internally Angstroms are the position unit.\n\t\t\t\t# I\'m debugging with kcal/angstrom forces.\n\t\t\t\tOuts = mol_.properties[""forces""]/KCALPERHARTREE\n\t\t\telif (self.OType == ""AtomizationEnergy""):\n\t\t\t    Outs = np.array([mol_.properties[""atomization""]])\n\t\t\telif (self.OType == ""EleEmbAtEn""):\n\t\t\t\tif (PARAMS[""EEOrder""]==2):\n\t\t\t\t\tAE = mol_.properties[""atomization""]\n\t\t\t\t\tif (PARAMS[""EEVdw""]==True):\n\t\t\t\t\t\tAE -= mol_.properties[""Vdw""]\n\t\t\t\t\tOuts = np.zeros(5) # AtEnergy, monopole, 3-dipole.\n\t\t\t\t\tOuts[0] = AE\n\t\t\t\t\tOuts[1] = 0.0\n\t\t\t\t\tOuts[1:] = mol_.properties[""dipole""]\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Code higher orders... "")\n\t\t\telif (self.OType == ""Multipole""):\n\t\t\t\tif (PARAMS[""EEOrder""]==2):\n\t\t\t\t\tOuts = np.zeros(4) # monopole, 3-dipole.\n\t\t\t\t\ttry:\n\t\t\t\t\t\tOuts[0] = mol_.properties[\'charge\']\n\t\t\t\t\texcept:\n\t\t\t\t\t\tOuts[0] =  0.0\n\t\t\t\t\tOuts[1:] = mol_.properties[""dipole""]*AUPERDEBYE\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Code higher orders... "")\n\t\t\telif (self.OType == ""Multipole2""):\n\t\t\t\tif (PARAMS[""EEOrder""]==2):\n\t\t\t\t\tOuts = np.zeros(3) # monopole, 3-dipole.\n\t\t\t\t\tOuts = mol_.properties[""dipole""]*AUPERDEBYE\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Code higher orders... "")\n\t\t\telif (self.OType == ""Atomization_novdw""):\n\t\t\t    Outs = np.array([mol_.properties[""atomization""] - mol_.properties[""vdw""]])\n\t\t\telif (self.OType == ""FragEnergy""):\n\t\t\t\tOuts = np.array([mol_.properties[""frag_mbe_energy""]])\n\t\t\telif (self.OType == ""GoEnergy""):\n\t\t\t\tOuts = np.array([mol_.GoEnergy()])\n\t\t\telse:\n\t\t\t\traise Exception(""Unknown Output Type... ""+self.OType)\n\n\t\t\tif (self.lshape == None):\n\t\t\t\tself.lshape=Outs.shape\n\n\t\t\tif (MakeGradients):\n\t\t\t\treturn Ins, Grads, Outs\n\t\t\telse:\n\t\t\t\treturn Ins, Outs\n\t\telse:\n\t\t\tif (MakeGradients):\n\t\t\t\treturn Ins, Grads\n\t\t\telse:\n\t\t\t\treturn Ins\n\n\tdef TrainDigest(self, mol_):\n\t\t""""""\n\t\tReturns list of inputs and outputs for a molecule.\n\t\tUses self.Emb() uses Mol to get the Desired output type (Energy,Force,Probability etc.)\n\n\t\tArgs:\n\t\t\tmol_: a molecule to be digested\n\t\t""""""\n\t\tif (self.OType == ""AEAndForce""):\n\t\t\treturn self.Emb(mol_,True,True)\n\t\telse:\n\t\t\treturn self.Emb(mol_,True,False)\n\n\tdef Print(self):\n\t\tprint(""Digest name: "", self.name)\n\nclass MolDigesterEE(MolDigester):\n\tdef __init__(self, eles_, name_=""ANI1_Sym"", OType_=""EnergyChargesDipole"", SensRadius_=6):\n\t\t""""""\n\t\tA digester for electrostatically embedded BP.\n\t\t""""""\n\t\tMolDigester.__init__(self, eles_, name_=""ANI1_Sym"", OType_=""EnergyChargesDipole"", SensRadius_=6)\n\t\treturn\n'"
TensorMol/Containers/Mol.py,5,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Util import *\nfrom ..Math import *\nimport MolEmb\n\nclass Mol:\n\t"""""" Provides a general purpose molecule""""""\n\tdef __init__(self, atoms_ =  np.zeros(1,dtype=np.uint8), coords_ = np.zeros(shape=(1,1),dtype=np.float)):\n\t\t""""""\n\t\tArgs:\n\t\t\tatoms_: np.array(dtype=uint8) of atomic numbers.\n\t\t\tcoords_: np.array(dtype=uint8) of coordinates.\n\t\t""""""\n\t\tself.atoms = atoms_.copy()\n\t\tself.coords = coords_.copy()\n\t\tself.properties = {}\n\t\tself.name=None\n\t\t#things below here are sometimes populated if it is useful.\n\t\tself.PESSamples = [] # a list of tuples (atom, new coordinates, energy) for storage.\n\t\tself.ecoords = None # equilibrium coordinates.\n\t\tself.DistMatrix = None # a list of equilbrium distances, for GO-models.\n\t\treturn\n\n\tdef ToFragSet(self,frags):\n\t\t""""""\n\t\tDivides this molecule into a set of molecules\n\t\tbased on fragments\n\n\t\tArgs:\n\t\t\tfrags: list of integer lists\n\t\tReturns:\n\t\t\tAn MSet with many mols in it divided by frags.\n\t\t""""""\n\t\tmset = MSet(""Fset"",PARAMS[""sets_dir""],False)\n\t\tfor frag in frags:\n\t\t\tmset.mols.append(Mol(self.atoms[frags],self.coords[frags]))\n\t\treturn mset\n\n\tdef BuildElectronConfiguration(self,charge_=0,spin_=1):\n\t\t""""""\n\t\tfill up electronic configuration.\n\t\t""""""\n\t\tnelectron = sum(self.atoms) - charge_\n\t\tnalpha = (nelectron+spin_)//2\n\t\tnbeta = nalpha - self.spin\n\t\tbasis = []\n#\t\tVALENCEBASIS = [[[1,0]],[[1,0]],[[1,0],[2,0]],[[1,0],[2,0]]]\n#\t\tfor atom in self.atoms:\n#\t\t\tbasis.append(VALENCEBASIS[atom])\n\t\tself.properties[""basis""] = basis\n\t\tself.properties[""charge""] = charge_\n\t\tself.properties[""spin""] = spin_\n\t\tself.properties[""nalpha""] = nalpha\n\t\tself.properties[""nbeta""] = nbeta\n\t\treturn\n\n\tdef AtomTypes(self):\n\t\treturn np.unique(self.atoms)\n\n\tdef Num_of_Heavy_Atom(self):\n\t\treturn len([1 for i in self.atoms if i!=1])\n\n\tdef NEles(self):\n\t\treturn len(self.AtomTypes())\n\n\tdef IsIsomer(self,other):\n\t\treturn np.array_equals(np.sort(self.atoms),np.sort(other.atoms))\n\n\tdef NAtoms(self):\n\t\treturn self.atoms.shape[0]\n\n\tdef NumOfAtomsE(self, e):\n\t\treturn sum( [1 if at==e else 0 for at in self.atoms ] )\n\n\tdef CalculateAtomization(self):\n\t\tif (""roomT_H"" in self.properties):\n\t\t\tAE = self.properties[""roomT_H""]\n\t\t\tfor i in range (0, self.atoms.shape[0]):\n\t\t\t\tif (self.atoms[i] in ELEHEATFORM):\n\t\t\t\t\tAE = AE - ELEHEATFORM[self.atoms[i]]\n\t\t\tself.properties[""atomization""] = AE\n\t\telif (""energy"" in self.properties):\n\t\t\tAE = self.properties[""energy""]\n\t\t\tfor i in range (0, self.atoms.shape[0]):\n\t\t\t\tif (self.atoms[i] in ele_U):\n\t\t\t\t\tAE = AE - ele_U[self.atoms[i]]\n\t\t\tself.properties[""atomization""] = AE\n\t\telse:\n\t\t\traise Exception(""Missing energy to calculate atomization... "")\n\t\treturn\n\n\tdef Calculate_vdw(self):\n\t\tc = 0.38088\n\t\tself.vdw = 0.0\n\t\ts6 = S6[\'B3LYP\']\n\t\tself.properties[""vdw""] = 0.0\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\tatom1 = self.atoms[i]\n\t\t\tfor j in range (i+1, self.NAtoms()):\n\t\t\t\tatom2 = self.atoms[j]\n\t\t\t\tself.properties[""vdw""] += -s6*c*((C6_coff[atom1]*C6_coff[atom2])**0.5)/(self.DistMatrix[i][j])**6 * (1.0/(1.0+6.0*(self.DistMatrix[i][j]/(atomic_vdw_radius[atom1]+atomic_vdw_radius[atom2]))**-12))\n\t\treturn\n\tdef Rotate(self, axis, ang, origin=np.array([0.0, 0.0, 0.0])):\n\t\t""""""\n\t\tRotate atomic coordinates and forces if present.\n\n\t\tArgs:\n\t\t\taxis: vector for rotation axis\n\t\t\tang: radians of rotation\n\t\t\torigin: origin of rotation axis.\n\t\t""""""\n\t\trm = RotationMatrix(axis, ang)\n\t\tcrds = np.copy(self.coords)\n\t\tcrds -= origin\n\t\tfor i in range(len(self.coords)):\n\t\t\tself.coords[i] = np.dot(rm,crds[i])\n\t\tif (""forces"" in self.properties.keys()):\n\t\t\t# Must also rotate the force vectors\n\t\t\told_endpoints = crds+self.properties[""forces""]\n\t\t\tnew_forces = np.zeros(crds.shape)\n\t\t\tfor i in range(len(self.coords)):\n\t\t\t\tnew_endpoint = np.dot(rm,old_endpoints[i])\n\t\t\t\tnew_forces[i] = new_endpoint - self.coords[i]\n\t\t\tself.properties[""forces""] = new_forces\n\t\tself.coords += origin\n\tdef RotateRandomUniform(self, randnums=None, origin=np.array([0.0, 0.0, 0.0])):\n\t\t""""""\n\t\tRotate atomic coordinates and forces if present.\n\n\t\tArgs:\n\t\t\trandnums: theta, phi, and z for rotation, if None then rotation is random\n\t\t\torigin: origin of rotation axis.\n\t\t""""""\n\t\trm = RotationMatrix_v2(randnums)\n\t\tcrds = np.copy(self.coords)\n\t\tcrds -= origin\n\t\tself.coords = np.einsum(""ij,kj->ki"",rm, crds)\n\t\tif (""forces"" in self.properties.keys()):\n\t\t\t# Must also rotate the force vectors\n\t\t\told_endpoints = crds+self.properties[""forces""]\n\t\t\tnew_endpoint = np.einsum(""ij,kj->ki"",rm, old_endpoints)\n\t\t\tnew_forces = new_endpoint - self.coords\n\t\t\tself.properties[""forces""] = new_forces\n\t\tif (""mmff94forces"" in self.properties.keys()):\n\t\t\t# Must also rotate the force vectors\n\t\t\told_endpoints = crds+self.properties[""mmff94forces""]\n\t\t\tnew_endpoint = np.einsum(""ij,kj->ki"",rm, old_endpoints)\n\t\t\tnew_forces = new_endpoint - self.coords\n\t\t\tself.properties[""mmff94forces""] = new_forces\n\t\tself.coords += origin\n\tdef Transform(self,ltransf,center=np.array([0.0,0.0,0.0])):\n\t\tcrds=np.copy(self.coords)\n\t\tfor i in range(len(self.coords)):\n\t\t\tself.coords[i] = np.dot(ltransf,crds[i]-center) + center\n\tdef AtomsWithin(self,rad, pt):\n\t\t# Returns indices of atoms within radius of point.\n\t\tdists = map(lambda x: np.linalg.norm(x-pt),self.coords)\n\t\treturn [i for i in range(self.NAtoms()) if dists[i]<rad]\n\tdef Distort(self,disp=0.38,movechance=.20):\n\t\t\'\'\' Randomly distort my coords, but save eq. coords first \'\'\'\n\t\tself.BuildDistanceMatrix()\n\t\te0= self.GoEnergy(self.coords)\n\t\tfor i in range(0, self.atoms.shape[0]):\n\t\t\tfor j in range(0, 3):\n\t\t\t\tif (random.uniform(0, 1)<movechance):\n\t\t\t\t\t#only accept collisionless moves.\n\t\t\t\t\taccepted = False\n\t\t\t\t\tmaxiter = 100\n\t\t\t\t\twhile (not accepted and maxiter>0):\n\t\t\t\t\t\ttmp = self.coords\n\t\t\t\t\t\ttmp[i,j] += np.random.normal(0.0, disp)\n\t\t\t\t\t\tmindist = np.min([ np.linalg.norm(tmp[i,:]-tmp[k,:]) if i!=k else 1.0 for k in range(self.NAtoms()) ])\n\t\t\t\t\t\tif (mindist>0.35):\n\t\t\t\t\t\t\taccepted = True\n\t\t\t\t\t\t\tself.coords = tmp\n\t\t\t\t\t\tmaxiter=maxiter-1\n\n\tdef DistortAN(self,movechance=.15):\n\t\t\'\'\' Randomly replace atom types. \'\'\'\n\t\tfor i in range(0, self.atoms.shape[0]):\n\t\t\tif (random.uniform(0, 1)<movechance):\n\t\t\t\tself.atoms[i] = random.random_integers(1,PARAMS[""MAX_ATOMIC_NUMBER""])\n\n\tdef read_xyz_with_properties(self, path, properties, center=True):\n\t\ttry:\n\t\t\tf=open(path,""r"")\n\t\t\tlines=f.readlines()\n\t\t\tnatoms=int(lines[0])\n\t\t\tself.atoms.resize((natoms))\n\t\t\tself.coords.resize((natoms,3))\n\t\t\tfor i in range(natoms):\n\t\t\t\tline = lines[i+2].split()\n\t\t\t\tself.atoms[i]=AtomicNumber(line[0])\n\t\t\t\tfor j in range(3):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tself.coords[i,j]=float(line[j+1])\n\t\t\t\t\texcept:\n\t\t\t\t\t\tself.coords[i,j]=scitodeci(line[j+1])\n\t\t\tif center:\n\t\t\t\tself.coords -= self.Center()\n\t\t\tproperties_line = lines[1]\n\t\t\tfor i, mol_property in enumerate(properties):\n\t\t\t\tif mol_property == ""name"":\n\t\t\t\t\tself.properties[""name""] = properties_line.split("";"")[i]\n\t\t\t\tif mol_property == ""energy"":\n\t\t\t\t\tself.properties[""energy""] = float(properties_line.split("";"")[i])\n\t\t\t\t\tself.CalculateAtomization()\n\t\t\t\tif mol_property == ""gradients"":\n\t\t\t\t\tself.properties[\'gradients\'] = np.zeros((natoms, 3))\n\t\t\t\t\tread_forces = (properties_line.split("";"")[i]).split("","")\n\t\t\t\t\tfor j in range(natoms):\n\t\t\t\t\t\tfor k in range(3):\n\t\t\t\t\t\t\tself.properties[\'gradients\'][j,k] = float(read_forces[j*3+k])\n\t\t\t\tif mol_property == ""dipole"":\n\t\t\t\t\tself.properties[\'dipole\'] = np.zeros((3))\n\t\t\t\t\tread_dipoles = (properties_line.split("";"")[i]).split("","")\n\t\t\t\t\tfor j in range(3):\n\t\t\t\t\t\tself.properties[\'dipole\'][j] = float(read_dipoles[j])\n\t\t\t\tif mol_property == ""mulliken_charges"":\n\t\t\t\t\tself.properties[""mulliken_charges""] = np.zeros((natoms))\n\t\t\t\t\tread_charges = (properties_line.split("";"")[i]).split("","")\n\t\t\t\t\tfor j in range(natoms):\n\t\t\t\t\t\tself.properties[""mulliken_charges""] = float(read_charges[j])\n\t\t\tf.close()\n\t\texcept Exception as Ex:\n\t\t\tprint(""Read Failed."", Ex)\n\t\t\traise Ex\n\t\treturn\n\n\tdef ReadGDB9(self,path,filename):\n\t\ttry:\n\t\t\tf=open(path,""r"")\n\t\t\tlines=f.readlines()\n\t\t\tnatoms=int(lines[0])\n\t\t\tself.name = filename[0:-4]\n\t\t\tself.atoms.resize((natoms))\n\t\t\tself.coords.resize((natoms,3))\n\t\t\ttry:\n\t\t\t\tself.properties[""energy""] = float((lines[1].split())[12])\n\t\t\t\tself.properties[""roomT_H""] = float((lines[1].split())[14])\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tfor i in range(natoms):\n\t\t\t\tline = lines[i+2].split()\n\t\t\t\tself.atoms[i]=AtomicNumber(line[0])\n\t\t\t\ttry:\n\t\t\t\t\tself.coords[i,0]=float(line[1])\n\t\t\t\texcept:\n\t\t\t\t\tself.coords[i,0]=scitodeci(line[1])\n\t\t\t\ttry:\n\t\t\t\t\tself.coords[i,1]=float(line[2])\n\t\t\t\texcept:\n\t\t\t\t\tself.coords[i,1]=scitodeci(line[2])\n\t\t\t\ttry:\n\t\t\t\t\tself.coords[i,2]=float(line[3])\n\t\t\t\texcept:\n\t\t\t\t\tself.coords[i,2]=scitodeci(line[3])\n\t\t\tf.close()\n\t\texcept Exception as Ex:\n\t\t\tprint(""Read Failed."", Ex)\n\t\t\traise Ex\n\t\tif ((""energy"" in self.properties) or (""roomT_H"" in self.properties)):\n\t\t\tself.CalculateAtomization()\n\t\treturn\n\tdef Clean(self):\n\t\tself.DistMatrix = None\n\tdef ParseProperties(self,s_):\n\t\t""""""\n\t\tThe format of a property string is\n\t\tComment: PropertyName1 Array ;PropertyName2 Array;\n\t\tThe Property names and contents cannot contain ; :\n\t\t""""""\n\t\tt = s_.split(""Comment:"")\n\t\tt2 = t[1].split("";;;"")\n\t\ttore = {}\n\t\tfor prop in t2:\n\t\t\ts = prop.split()\n\t\t\tif (len(s)<1):\n\t\t\t\tcontinue\n\t\t\telif (s[0]==\'energy\'):\n\t\t\t\ttore[""energy""] = float(s[1])\n\t\t\telif (s[0]==\'Lattice\'):\n\t\t\t\ttore[""Lattice""] = np.fromstring(s[1]).reshape((3,3))\n\t\treturn tore\n\tdef PropertyString(self):\n\t\ttore = """"\n\t\tfor prop in self.properties.keys():\n\t\t\ttry:\n\t\t\t\tif (prop == ""energy""):\n\t\t\t\t\ttore = tore +"";;;""+prop+"" ""+str(self.properties[""energy""])\n\t\t\t\telif (prop == ""Lattice""):\n\t\t\t\t\ttore = tore +"";;;""+prop+"" ""+(self.properties[prop]).tostring()\n\t\t\t\telse:\n\t\t\t\t\ttore = tore +"";;;""+prop+"" ""+str(self.properties[prop])\n\t\t\texcept Exception as Ex:\n\t\t\t\t# print ""Problem with energy"", string\n\t\t\t\tpass\n\t\treturn tore\n\tdef FromXYZString(self,string):\n\t\tlines = string.split(""\\n"")\n\t\tnatoms=int(lines[0])\n\t\tif (len(lines[1].split())>1):\n\t\t\ttry:\n\t\t\t\tself.properties = self.ParseProperties(lines[1])\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""Problem with energy"", Ex)\n\t\t\t\tpass\n\t\tself.atoms.resize((natoms))\n\t\tself.coords.resize((natoms,3))\n\t\tfor i in range(natoms):\n\t\t\tline = lines[i+2].split()\n\t\t\tif len(line)==0:\n\t\t\t\treturn\n\t\t\tself.atoms[i]=AtomicNumber(line[0])\n\t\t\ttry:\n\t\t\t\tself.coords[i,0]=float(line[1])\n\t\t\texcept:\n\t\t\t\tself.coords[i,0]=scitodeci(line[1])\n\t\t\ttry:\n\t\t\t\tself.coords[i,1]=float(line[2])\n\t\t\texcept:\n\t\t\t\tself.coords[i,1]=scitodeci(line[2])\n\t\t\ttry:\n\t\t\t\tself.coords[i,2]=float(line[3])\n\t\t\texcept:\n\t\t\t\tself.coords[i,2]=scitodeci(line[3])\n\t\tif (""energy"" in self.properties):\n\t\t\tself.CalculateAtomization()\n\t\treturn\n\n\tdef __str__(self,wprop=False):\n\t\tlines =""""\n\t\tnatom = self.atoms.shape[0]\n\t\tif (wprop):\n\t\t\tlines = lines+(str(natom)+""\\nComment: ""+self.PropertyString()+""\\n"")\n\t\telse:\n\t\t\tlines = lines+(str(natom)+""\\nComment: \\n"")\n\t\tfor i in range (natom):\n\t\t\tatom_name =  list(atoi.keys())[list(atoi.values()).index(self.atoms[i])]\n\t\t\tif (i<natom-1):\n\t\t\t\tlines = lines+(atom_name+""   ""+str(self.coords[i][0])+ ""  ""+str(self.coords[i][1])+ ""  ""+str(self.coords[i][2])+""\\n"")\n\t\t\telse:\n\t\t\t\tlines = lines+(atom_name+""   ""+str(self.coords[i][0])+ ""  ""+str(self.coords[i][1])+ ""  ""+str(self.coords[i][2]))\n\t\treturn lines\n\n\tdef __repr__(self):\n\t\treturn self.__str__()\n\n\tdef WriteXYZfile(self, fpath=""."", fname=""mol"", mode=""a"", wprop = False):\n\t\tif not os.path.exists(os.path.dirname(fpath+""/""+fname+"".xyz"")):\n\t\t\ttry:\n\t\t\t\tos.makedirs(os.path.dirname(fpath+""/""+fname+"".xyz""))\n\t\t\texcept OSError as exc:\n\t\t\t\tif exc.errno != errno.EEXIST:\n\t\t\t\t\traise\n\t\twith open(fpath+""/""+fname+"".xyz"", mode) as f:\n\t\t\tfor line in self.__str__(wprop).split(""\\n""):\n\t\t\t\tf.write(line+""\\n"")\n\n\tdef WriteSmiles(self, fpath=""."", fname=""gdb9_smiles"", mode = ""a""):\n\t\tif not os.path.exists(os.path.dirname(fpath+""/""+fname+"".dat"")):\n\t\t\ttry:\n\t\t\t\tos.makedirs(os.path.dirname(fpath+""/""+fname+"".dat""))\n\t\t\texcept OSError as exc:\n\t\t\t\tif exc.errno != errno.EEXIST:\n\t\t\t\t\traise\n\t\twith open(fpath+""/""+fname+"".dat"", mode) as f:\n\t\t\tf.write(self.name+ ""  ""+ self.smiles+""\\n"")\n\t\t\tf.close()\n\t\treturn\n\n\tdef XYZtoGridIndex(self, xyz, ngrids = 250,padding = 2.0):\n\t\tMax = (self.coords).max() + padding\n\t\tMin = (self.coords).min() - padding\n\t\tbinsize = (Max-Min)/float(ngrids-1)\n\t\tx_index = math.floor((xyz[0]-Min)/binsize)\n\t\ty_index = math.floor((xyz[1]-Min)/binsize)\n\t\tz_index = math.floor((xyz[2]-Min)/binsize)\n\t\t#index=int(x_index+y_index*ngrids+z_index*ngrids*ngrids)\n\t\treturn x_index, y_index, z_index\n\n\tdef MolDots(self, ngrids = 250 , padding =2.0, width = 2):\n\t\tgrids = self.MolGrids()\n\t\tfor i in range (0, self.atoms.shape[0]):\n\t\t\tx_index, y_index, z_index = self.XYZtoGridIndex(self.coords[i])\n\t\t\tfor m in range (-width, width):\n\t\t\t\tfor n in range (-width, width):\n\t\t\t\t\tfor k in range (-width, width):\n\t\t\t\t\t\tindex = (x_index)+m + (y_index+n)*ngrids + (z_index+k)*ngrids*ngrids\n\t\t\t\t\t\tgrids[index] = atoc[self.atoms[i]]\n\t\treturn grids\n\n\tdef Center(self, CenterOf=""Atom"", MomentOrder = 1.):\n\t\t\'\'\' Returns the center of atom or mass\n\n\t\tArgs:\n\t\t\tCenterOf: Whether to return center of atom position or mass.\n\t\t\tMomentOrder: Option to do nth order moment.\n\t\tReturns:\n\t\t\tCenter of Atom, Mass, or a higher-order moment.\n\t\t\'\'\'\n\t\tif (CenterOf == ""Mass""):\n\t\t\tm = np.array(map(lambda x: ATOMICMASSES[x-1],self.atoms))\n\t\t\treturn np.einsum(""ax,a->x"",np.power(self.coords,MomentOrder),m)/np.sum(m)\n\t\telse:\n\t\t\treturn np.average(np.power(self.coords,MomentOrder),axis=0)\n\n\tdef rms(self, m):\n\t\t"""""" Cartesian coordinate difference. """"""\n\t\terr  = 0.0\n\t\tfor i in range (0, (self.coords).shape[0]):\n\t\t\terr += np.linalg.norm(m.coords[i] - self.coords[i])\n\t\treturn err/self.coords.shape[0]\n\n\tdef rms_inv(self, m):\n\t\t"""""" Invariant coordinate difference. """"""\n\t\tmdm = MolEmb.Make_DistMat(self.coords)\n\t\todm = MolEmb.Make_DistMat(m.coords)\n\t\ttmp = (mdm-odm)\n\t\treturn np.sqrt(np.sum(tmp*tmp)/(mdm.shape[0]*mdm.shape[0]))\n\n\tdef BondMatrix(self,tol_ = 1.6):\n\t\t""""""\n\t\tReturns a natom x natom matrix representing the bonds\n\t\tof this molecule.\n\t\t""""""\n\t\tmdm = MolEmb.Make_DistMat(self.coords)\n\t\treturn np.where(mdm < 1.3,np.ones(mdm.shape),np.zeros(mdm.shape))\n\n\tdef SpanningGrid(self,num=250,pad=4.,Flatten=True, Cubic = True):\n\t\t\'\'\' Returns a regular grid the molecule fits into \'\'\'\n\t\txmin=np.min(self.coords[:,0])-pad\n\t\txmax=np.max(self.coords[:,0])+pad\n\t\tymin=np.min(self.coords[:,1])-pad\n\t\tymax=np.max(self.coords[:,1])+pad\n\t\tzmin=np.min(self.coords[:,2])-pad\n\t\tzmax=np.max(self.coords[:,2])+pad\n\t\tlx = xmax-xmin\n\t\tly = ymax-ymin\n\t\tlz = zmax-zmin\n\t\tif (Cubic):\n\t\t\tmlen = np.max([lx,ly,lz])\n\t\t\txmax = xmin + mlen\n\t\t\tymax = ymin + mlen\n\t\t\tzmax = zmin + mlen\n\t\tgrids = np.mgrid[xmin:xmax:num*1j, ymin:ymax:num*1j, zmin:zmax:num*1j]\n\t\tgrids = grids.transpose((1,2,3,0))\n\t\tif (not Flatten):\n\t\t\treturn grids.rshape()\n\t\tgrids = grids.reshape((grids.shape[0]*grids.shape[1]*grids.shape[2], grids.shape[3]))\n\t\treturn grids, (xmax-xmin)*(ymax-ymin)*(zmax-zmin)\n\n\tdef AddPointstoMolDots(self, grids, points, value, ngrids =250):  # points: x,y,z,prob    prob is in (0,1)\n\t\tpoints = points.reshape((-1,3))  # flat it\n\t\tvalue = value.reshape(points.shape[0]) # flat it\n\t\tvalue = value/value.max()\n\t\tfor i in range (0, points.shape[0]):\n\t\t\tx_index, y_index, z_index = self.XYZtoGridIndex(points[i])\n\t\t\tindex = x_index + y_index*ngrids + z_index*ngrids*ngrids\n\t\t\tif grids[index] <  int(value[i]*250):\n\t\t\t\tgrids[index] = int(value[i]*250)\n\t\treturn grids\n\n\tdef SortAtoms(self):\n\t\t"""""" First sorts by element, then sorts by distance to the center of the molecule\n\t\t\tThis improves alignment. """"""\n\t\torder = np.argsort(self.atoms)\n\t\tself.atoms = self.atoms[order]\n\t\tself.coords = self.coords[order,:]\n\t\tself.coords = self.coords - self.Center()\n\t\tself.ElementBounds = [[0,0] for i in range(self.NEles())]\n\t\tfor e, ele in enumerate(self.AtomTypes()):\n\t\t\tinblock=False\n\t\t\tfor i in range(0, self.NAtoms()):\n\t\t\t\tif (not inblock and self.atoms[i]==ele):\n\t\t\t\t\tself.ElementBounds[e][0] = i\n\t\t\t\t\tinblock=True\n\t\t\t\telif (inblock and (self.atoms[i]!=ele or i==self.NAtoms()-1)):\n\t\t\t\t\tself.ElementBounds[e][1] = i\n\t\t\t\t\tinblock=False\n\t\t\t\t\tbreak\n\t\tfor e in range(self.NEles()):\n\t\t\tblk = self.coords[self.ElementBounds[e][0]:self.ElementBounds[e][1],:].copy()\n\t\t\tdists = np.sqrt(np.sum(blk*blk,axis=1))\n\t\t\tinds = np.argsort(dists)\n\t\t\tself.coords[self.ElementBounds[e][0]:self.ElementBounds[e][1],:] = blk[inds]\n\t\treturn\n\n\tdef WriteInterpolation(self,b,n=10):\n\t\tfor i in range(n): # Check the interpolation.\n\t\t\tm=Mol(self.atoms,self.coords*((9.-i)/9.)+b.coords*((i)/9.))\n\t\t\tm.WriteXYZfile(PARAMS[""results_dir""], ""Interp""+str(n))\n\n\tdef AlignAtoms(self, m):\n\t\t""""""\n\t\tAlign the geometries and atom order of myself and another molecule.\n\t\tThis alters both molecules, centering them, and also permutes\n\t\ttheir atoms.\n\n\t\tArgs:\n\t\t\tm: A molecule to be aligned with me.\n\t\t""""""\n\t\tassert self.NAtoms() == m.NAtoms(), ""Number of atoms do not match""\n\t\tself.coords -= self.Center()\n\t\tm.coords -= m.Center()\n\t\t# try to achieve best rotation alignment between them by aligning the second moments of position\n\t\tsdm = MolEmb.Make_DistMat(self.coords)\n\t\td = sdm-MolEmb.Make_DistMat(m.coords)\n\t\tBestMomentOverlap = np.sum(d*d)\n\t\tBestTriple=[0.,0.,0.]\n\t\tfor a in np.linspace(-Pi,Pi,20):\n\t\t\tfor b in np.linspace(-Pi,Pi,20):\n\t\t\t\tfor c in np.linspace(-Pi,Pi,20):\n\t\t\t\t\ttmpm = Mol(m.atoms,m.coords)\n\t\t\t\t\ttmpm.Rotate([1.,0.,0.],a)\n\t\t\t\t\ttmpm.Rotate([0.,1.,0.],b)\n\t\t\t\t\ttmpm.Rotate([0.,0.,1.],c)\n\t\t\t\t\td = sdm-MolEmb.Make_DistMat(tmpm.coords)\n\t\t\t\t\tlap = np.sum(d*d)\n\t\t\t\t\tif ( lap < BestMomentOverlap ):\n\t\t\t\t\t\tBestTriple = [a,b,c]\n\t\t\t\t\t\tBestMomentOverlap = lap\n\t\tm.Rotate([1.,0.,0.],BestTriple[0])\n\t\tm.Rotate([0.,1.,0.],BestTriple[1])\n\t\tm.Rotate([0.,0.,1.],BestTriple[2])\n\t\t#print(""After centering and Rotation ---- "")\n\t\t#print(""Self \\n""+self.__str__())\n\t\t#print(""Other \\n""+m.__str__())\n\t\tself.SortAtoms()\n\t\tm.SortAtoms()\n\t\t# Greedy assignment\n\t\tfor e in range(self.NEles()):\n\t\t\tmones = range(self.ElementBounds[e][0],self.ElementBounds[e][1])\n\t\t\tmtwos = range(self.ElementBounds[e][0],self.ElementBounds[e][1])\n\t\t\tassignedmones=[]\n\t\t\tassignedmtwos=[]\n\t\t\tfor b in mtwos:\n\t\t\t\tacs = self.coords[mones]\n\t\t\t\ttmp = acs - m.coords[b]\n\t\t\t\tbest = np.argsort(np.sqrt(np.sum(tmp*tmp,axis=1)))[0]\n\t\t\t\t#print ""Matching "", m.coords[b],"" to "", self.coords[mones[best]]\n\t\t\t\t#print ""Matching "", b,"" to "", mones[best]\n\t\t\t\tassignedmtwos.append(b)\n\t\t\t\tassignedmones.append(mones[best])\n\t\t\t\tmones = complement(mones,assignedmones)\n\t\t\tself.coords[mtwos] = self.coords[assignedmones]\n\t\t\tm.coords[mtwos] = m.coords[assignedmtwos]\n\t\tself.DistMatrix = MolEmb.Make_DistMat(self.coords)\n\t\tm.DistMatrix = MolEmb.Make_DistMat(m.coords)\n\t\tdiff = np.linalg.norm(self.DistMatrix - m.DistMatrix)\n\t\ttmp_coords=m.coords.copy()\n\t\ttmp_dm = MolEmb.Make_DistMat(tmp_coords)\n\t\tk = 0\n\t\tsteps = 1\n\t\twhile (k < 2):\n\t\t\tfor i in range(m.NAtoms()):\n\t\t\t\tfor j in range(i+1,m.NAtoms()):\n\t\t\t\t\tif m.atoms[i] != m.atoms[j]:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tir = tmp_dm[i].copy() - self.DistMatrix[i]\n\t\t\t\t\tjr = tmp_dm[j].copy() - self.DistMatrix[j]\n\t\t\t\t\tirp = tmp_dm[j].copy()\n\t\t\t\t\tirp[i], irp[j] = irp[j], irp[i]\n\t\t\t\t\tjrp = tmp_dm[i].copy()\n\t\t\t\t\tjrp[i], jrp[j] = jrp[j], jrp[i]\n\t\t\t\t\tirp -= self.DistMatrix[i]\n\t\t\t\t\tjrp -= self.DistMatrix[j]\n\t\t\t\t\tif (np.linalg.norm(irp)+np.linalg.norm(jrp) < np.linalg.norm(ir)+np.linalg.norm(jr)):\n\t\t\t\t\t\tk = 0\n\t\t\t\t\t\tperm=range(m.NAtoms())\n\t\t\t\t\t\tperm[i] = j\n\t\t\t\t\t\tperm[j] = i\n\t\t\t\t\t\ttmp_coords=tmp_coords[perm]\n\t\t\t\t\t\ttmp_dm = MolEmb.Make_DistMat(tmp_coords)\n\t\t\t\t\t\t#print(np.linalg.norm(self.DistMatrix - tmp_dm))\n\t\t\t\t\t\tsteps = steps+1\n\t\t\t\t#print(i)\n\t\t\tk+=1\n\t\tm.coords=tmp_coords.copy()\n\t\t#print(""best"",tmp_coords)\n\t\t#print(""self"",self.coords)\n\t\tself.WriteInterpolation(Mol(self.atoms,tmp_coords),10)\n\t\treturn Mol(self.atoms,self.coords), Mol(self.atoms,tmp_coords)\n\n# ---------------------------------------------------------------\n#  Functions related to energy models and sampling.\n#  all this shit should be moved into a ""class Calculator""\n# ---------------------------------------------------------------\n\n\tdef BuildDistanceMatrix(self):\n\t\tself.DistMatrix = MolEmb.Make_DistMat(self.coords)\n\n\tdef GoEnergy(self,x):\n\t\t\'\'\' The GO potential enforces equilibrium bond lengths. This is the lennard jones soft version\'\'\'\n\t\tif (self.DistMatrix is None):\n\t\t\tprint(""Build DistMatrix"")\n\t\t\traise Exception(""dmat"")\n\t\txmat = np.array(x).reshape(self.NAtoms(),3)\n\t\tnewd = MolEmb.Make_DistMat(xmat)\n\t\tnewd -= self.DistMatrix\n\t\tnewd = newd*newd\n\t\treturn PARAMS[""GoK""]*np.sum(newd)\n\n\tdef GoForce(self, at_=-1, spherical = 0):\n\t\t\'\'\'\n\t\t\tThe GO potential enforces equilibrium bond lengths, and this is the force of that potential.\n\t\t\tArgs: at_ an atom index, if at_ = -1 it returns an array for each atom.\n\t\t\'\'\'\n\t\tif (spherical):\n\t\t\trthph = MolEmb.Make_GoForce(self.coords,self.DistMatrix,at_,1)\n\t\t\trthph[:,0] = rthph[:,0]*PARAMS[""GoK""]\n\t\t\treturn rthph\n\t\telse:\n\t\t\treturn PARAMS[""GoK""]*MolEmb.Make_GoForce(self.coords,self.DistMatrix,at_,0)\n\n\tdef GoForceLocal(self, at_=-1):\n\t\t\'\'\' The GO potential enforces equilibrium bond lengths, and this is the force of that potential.\n\t\t\tA MUCH FASTER VERSION OF THIS ROUTINE IS NOW AVAILABLE, see MolEmb::Make_Go\n\t\t\'\'\'\n\t\treturn PARAMS[""GoK""]*MolEmb.Make_GoForceLocal(self.coords,self.DistMatrix,at_)\n\n\tdef NumericGoHessian(self):\n\t\tif (self.DistMatrix==None):\n\t\t\tprint(""Build DistMatrix"")\n\t\t\traise Exception(""dmat"")\n\t\tdisp=0.001\n\t\thess=np.zeros((self.NAtoms()*3,self.NAtoms()*3))\n\t\tfor i in range(self.NAtoms()):\n\t\t\tfor j in range(self.NAtoms()):\n\t\t\t\tfor ip in range(3):\n\t\t\t\t\tfor jp in range(3):\n\t\t\t\t\t\tif (j*3+jp >= i*3+ip):\n\t\t\t\t\t\t\ttmp = self.coords.flatten()\n\t\t\t\t\t\t\ttmp[i*3+ip] += disp\n\t\t\t\t\t\t\ttmp[j*3+jp] += disp\n\t\t\t\t\t\t\tf1 = self.GoEnergy(tmp)\n\t\t\t\t\t\t\ttmp = self.coords.flatten()\n\t\t\t\t\t\t\ttmp[i*3+ip] += disp\n\t\t\t\t\t\t\ttmp[j*3+jp] -= disp\n\t\t\t\t\t\t\tf2 = self.GoEnergy(tmp)\n\t\t\t\t\t\t\ttmp = self.coords.flatten()\n\t\t\t\t\t\t\ttmp[i*3+ip] -= disp\n\t\t\t\t\t\t\ttmp[j*3+jp] += disp\n\t\t\t\t\t\t\tf3 = self.GoEnergy(tmp)\n\t\t\t\t\t\t\ttmp = self.coords.flatten()\n\t\t\t\t\t\t\ttmp[i*3+ip] -= disp\n\t\t\t\t\t\t\ttmp[j*3+jp] -= disp\n\t\t\t\t\t\t\tf4 = self.GoEnergy(tmp)\n\t\t\t\t\t\t\thess[i*3+ip,j*3+jp] = (f1-f2-f3+f4)/(4.0*disp*disp)\n\t\treturn (hess+hess.T-np.diag(np.diag(hess)))\n\n\tdef GoHessian(self):\n\t\treturn PARAMS[""GoK""]*MolEmb.Make_GoHess(self.coords,self.DistMatrix)\n\n\tdef ScanNormalModes(self,npts=11,disp=0.2):\n\t\t""These modes are normal""\n\t\tself.BuildDistanceMatrix()\n\t\thess = self.GoHessian()\n\t\tw,v = np.linalg.eig(hess)\n\t\tthresh = pow(10.0,-6.0)\n\t\tnumincl = np.sum([1 if abs(w[i])>thresh else 0 for i in range(len(w))])\n\t\ttore = np.zeros((numincl,npts,self.NAtoms(),3))\n\t\tnout = 0\n\t\tfor a in range(self.NAtoms()):\n\t\t\tfor ap in range(3):\n\t\t\t\tif (abs(w[a*3+ap])<thresh):\n\t\t\t\t\tcontinue\n\t\t\t\ttmp = v[:,a*3+ap]/np.linalg.norm(v[:,a*3+ap])\n\t\t\t\teigv = np.reshape(tmp,(self.NAtoms(),3))\n\t\t\t\tfor d in range(npts):\n\t\t\t\t\ttore[nout,d,:,:] = (self.coords+disp*(self.NAtoms()*(d-npts/2.0+0.37)/npts)*eigv).real\n\t\t\t\t\t#print disp*(self.NAtoms()*(d-npts/2.0+0.37)/npts)*eigv\n\t\t\t\t\t#print d, self.GoEnergy(tore[nout,d,:,:].flatten())#, PARAMS[""GoK""]*MolEmb.Make_GoForce(tore[nout,d,:,:],self.DistMatrix,-1)\n\t\t\t\tnout = nout+1\n\t\treturn tore\n\n\tdef SoftCutGoForce(self, cutdist=6):\n\t\tif (self.DistMatrix==None):\n\t\t\tprint(""Build DistMatrix"")\n\t\t\traise Exception(""dmat"")\n\t\tforces = np.zeros((self.NAtoms(),3))\n\t\tfor i in range(len(self.coords)):\n\t\t\tforces[i]=self.SoftCutGoForceOneAtom(i, cutdist)\n\t\treturn forces\n\n\tdef GoForce_Scan(self, maxstep, ngrid):\n\t\t#scan near by regime and return the samllest force\n\t\tforces = np.zeros((self.NAtoms(),3))\n\t\tTmpForce = np.zeros((self.NAtoms(), ngrid*ngrid*ngrid,3),dtype=np.float)\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\tprint(""Atom: "", i)\n\t\t\tsave_i = self.coords[i].copy()\n\t\t\tsamps=MakeUniform(self.coords[i],maxstep,ngrid)\n\t\t\tfor m in range (0, samps.shape[0]):\n\t\t\t\tself.coords[i] = samps[m].copy()\n\t\t\t\tfor j in range(len(self.coords)):\n\t\t\t\t\t# compute force on i due to all j\'s\n\t\t\t\t\tu = self.coords[j]-samps[m]\n\t\t\t\t\tdij = np.linalg.norm(u)\n\t\t\t\t\tif (dij != 0.0):\n\t\t\t\t\t\tu = u/np.linalg.norm(u)\n\t\t\t\t\tTmpForce[i][m] += 0.5*(dij-self.DistMatrix[i,j])*u\n\t\t\tself.coords[i] = save_i.copy()\n\t\t\tTmpAbsForce = (TmpForce[i,:,0]**2+TmpForce[i,:,1]**2+TmpForce[i,:,2]**2)**0.5\n\t\t\tforces[i] = samps[np.argmin(TmpAbsForce)]\n\t\treturn forces\n\n\tdef EnergyAfterAtomMove(self,s,i,Type=""GO""):\n\t\tif (Type==""GO""):\n\t\t\tout = np.zeros(s.shape[:-1])\n\t\t\tMolEmb.Make_Go(s,self.DistMatrix,out,self.coords,i)\n\t\t\treturn out\n\t\telse:\n\t\t\traise Exception(""Unknown Energy"")\n\n\t#Most parameters are unneccesary.\n\tdef OverlapEmbeddings(self, d1, coords, d2 , d3 ,  d4 , d5, i, d6):#(self,coord,i):\n\t\treturn np.array([GRIDS.EmbedAtom(self,j,i) for j in coords])\n\n\tdef FitGoProb(self,ii,Print=False):\n\t\t\'\'\'\n\t\tGenerates a Go-potential for atom i on a uniform grid of 4A with 50 pts/direction\n\t\tAnd fits that go potential with the H@0 basis centered at the same point\n\t\tIn practice 9 (1A) gaussians separated on a 1A grid around the sensory point appears to work for moderate distortions.\n\t\t\'\'\'\n\t\tPs = self.POfAtomMoves(GRIDS.MyGrid(),ii)\n\t\tPc = np.dot(GRIDS.MyGrid().T,Ps)\n\t\tif (Print):\n\t\t\tprint(""Desired Displacement"", Pc)  # should equal the point for a Go-Model at equilibrium\n\t\tV=GRIDS.Vectorize(Ps)#,True)\n\t\tout = np.zeros(shape=(1,GRIDS.NGau3+3))\n\t\tout[0,:GRIDS.NGau3]+=V\n\t\tout[0,GRIDS.NGau3:]+=Pc\n\t\treturn out\n\n\tdef UseGoProb(self,ii,inputs):\n\t\t\'\'\'\n\t\tThe opposite of the routine above. It takes the digested probability vectors and uses it to calculate desired new positions.\n\t\t\'\'\'\n\t\tpdisp=inputs[-3:]\n\t\treturn pdisp\n\n\tdef EnergiesOfAtomMoves(self,samps,i):\n\t\treturn np.array([self.energyAfterAtomMove(s,i) for s in samps])\n\n\tdef POfAtomMoves(self,samps,i):\n\t\t\'\'\' Arguments are given relative to the coordinate of i\'\'\'\n\t\tif (self.DistMatrix==None):\n\t\t\traise Exception(""BuildDMat"")\n\t\tEs=np.zeros(samps.shape[0],dtype=np.float64)\n\t\tMolEmb.Make_Go(samps+self.coords[i],self.DistMatrix,Es,self.coords,i)\n\t\tEs=np.nan_to_num(Es)\n\t\tEs=Es-np.min(Es)\n\t\tPs = np.exp(-1.0*Es/(0.25*np.std(Es)))\n\t\tPs=np.nan_to_num(Ps)\n\t\tZ = np.sum(Ps)\n\t\tPs /= Z\n\t\treturn Ps\n\n\tdef ForceFromXYZ(self, path):\n\t\t""""""\n\t\tReads the forces from the comment line in the md_dataset,\n\t\tand if no forces exist sets them to zero. Switched on by\n\t\thas_force=True in the ReadGDB9Unpacked routine\n\t\t""""""\n\t\ttry:\n\t\t\tf = open(path, \'r\')\n\t\t\tlines = f.readlines()\n\t\t\tnatoms = int(lines[0])\n\t\t\tforces=np.zeros((natoms,3))\n\t\t\tread_forces = ((lines[1].strip().split(\';\'))[1]).replace(""],["", "","").replace(""["","""").replace(""]"","""").split("","")\n\t\t\tfor j in range(natoms):\n\t\t\t\tfor k in range(3):\n\t\t\t\t\tforces[j,k] = float(read_forces[j*3+k])\n\t\t\tself.properties[\'forces\'] = forces\n\t\texcept Exception as Ex:\n\t\t\tprint(""Reading Force Failed."", Ex)\n\n\tdef MMFF94FromXYZ(self, path):\n\t\t""""""\n\t\tReads the forces from the comment line in the md_dataset,\n\t\tand if no forces exist sets them to zero. Switched on by\n\t\thas_force=True in the ReadGDB9Unpacked routine\n\t\tTODO: Move this out of Mol please to AbInitio (JAP)\n\t\t""""""\n\t\ttry:\n\t\t\tf = open(path, \'r\')\n\t\t\tlines = f.readlines()\n\t\t\tnatoms = int(lines[0])\n\t\t\tforces=np.zeros((natoms,3))\n\t\t\tread_forces = ((lines[1].strip().split(\';\'))[3]).replace(""],["", "","").replace(""["","""").replace(""]"","""").split("","")\n\t\t\tfor j in range(natoms):\n\t\t\t\tfor k in range(3):\n\t\t\t\t\tforces[j,k] = float(read_forces[j*3+k])\n\t\t\tself.properties[\'mmff94forces\'] = forces\n\t\texcept Exception as Ex:\n\t\t\tprint(""Reading MMFF94 Force Failed."", Ex)\n\n\tdef ChargeFromXYZ(self, path):\n\t\t""""""\n\t\tReads the forces from the comment line in the md_dataset,\n\t\tand if no forces exist sets them to zero. Switched on by\n\t\thas_force=True in the ReadGDB9Unpacked routine\n\t\t""""""\n\t\ttry:\n\t\t\tf = open(path, \'r\')\n\t\t\tlines = f.readlines()\n\t\t\tnatoms = int(lines[0])\n\t\t\tcharges=np.zeros((natoms))\n\t\t\tread_charges = ((lines[1].strip().split(\';\'))[2]).replace(""["","""").replace(""]"","""").split("","")\n\t\t\tfor j in range(natoms):\n\t\t\t\tcharges[j] = float(read_charges[j])\n\t\t\tself.properties[\'mulliken\'] = charges\n\t\texcept Exception as Ex:\n\t\t\tprint(""Reading Charges Failed."", Ex)\n\n\n\tdef EnergyFromXYZ(self, path):\n\t\t""""""\n\t\tReads the energy from the comment line in the md_dataset.\n\t\tSwitched on by has_energy=True in the ReadGDB9Unpacked routine\n\t\t""""""\n\t\ttry:\n\t\t\tf = open(path, \'r\')\n\t\t\tlines = f.readlines()\n\t\t\tenergy = float((lines[1].strip().split(\';\'))[0])\n\t\t\tself.properties[\'energy\'] = energy\n\t\texcept Exception as Ex:\n\t\t\tprint(""Reading Energy Failed."", Ex)\n\n\tdef MakeBonds(self):\n\t\tself.BuildDistanceMatrix()\n\t\tmaxnb = 0\n\t\tbonds = []\n\t\tfor i in range(self.NAtoms()):\n\t\t\tfor j in range(i+1,self.NAtoms()):\n\t\t\t\tif self.DistMatrix[i,j] < 3.0:\n\t\t\t\t\tbonds.append([i,j])\n\t\tbonds = np.asarray(bonds, dtype=np.int)\n\t\tself.properties[""bonds""] = bonds\n\t\tself.nbonds = bonds.shape[0]\n\t\tf=np.vectorize(lambda x: self.atoms[x])\n\t\tself.bondtypes = np.unique(f(bonds), axis=0)\n\t\treturn self.nbonds\n\n\tdef BondTypes(self):\n\t\treturn np.unique(self.bonds[:,0]).astype(int)\n\n\tdef AtomName(self, i):\n\t\treturn atoi.keys()[atoi.values().index(self.atoms[i])]\n\n\tdef AllAtomNames(self):\n\t\tnames=[]\n\t\tfor i in range (0, self.atoms.shape[0]):\n\t\t\tnames.append(atoi.keys()[atoi.values().index(self.atoms[i])])\n\t\treturn names\n\n\tdef Set_Qchem_Data_Path(self):\n\t\tself.qchem_data_path=""./qchem""+""/""+self.properties[""set_name""]+""/""+self.name\n\t\treturn\n\n\tdef Make_Spherical_Forces(self):\n\t\tself.properties[""sphere_forces""] = CartToSphereV(self.properties[""forces""])\n\n\tdef MultipoleInputs(self):\n\t\t""""""\n\t\t\tThese are the quantities (in Atomic Units)\n\t\t\twhich you multiply the atomic charges by (and sum)\n\t\t\tin order to calculate the multipoles of a molecule\n\t\t\tup to PARAMS[""EEOrder""]\n\n\t\t\tReturns:\n\t\t\t\t(NAtoms X (monopole, dipole x, ... quad x... etc. ))\n\t\t""""""\n\t\ttore = None\n\t\tcom = self.Center(OfMass=True)\n\t\tif (PARAMS[""EEOrder""] == 2):\n\t\t\ttore = np.zeros((self.NAtoms,4))\n\t\t\tfor i in range(self.NAtoms()):\n\t\t\t\ttore[i,0] = 1.0\n\t\t\t\ttore[i,1:] = self.coords[i]-com\n\t\telse:\n\t\t\traise Exception(""Implement... "")\n\t\treturn tore\n\nclass Frag_of_Mol(Mol):\n\tdef __init__(self, atoms_=None, coords_=None):\n\t\tMol.__init__(self, atoms_, coords_)\n\t\tself.atom_nodes = None\n\t\tself.undefined_bond_type =  None # whether the dangling bond can be connected  to H or not\n\t\tself.undefined_bonds = None  # capture the undefined bonds of each atom\n\n\tdef FromXYZString(self,string, set_name = None):\n\t\tMol.FromXYZString(self,string)\n\t\tself.properties[""set_name""] = set_name\n\t\treturn\n\n\tdef Make_AtomNodes(self):\n\t\tatom_nodes = []\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\tif i in self.undefined_bonds.keys():\n\t\t\t\tatom_nodes.append(AtomNode(self.atoms[i], i,  self.undefined_bond_type, self.undefined_bonds[i]))\n\t\t\telse:\n\t\t\t\tatom_nodes.append(AtomNode(self.atoms[i], i, self.undefined_bond_type))\n\t\tself.atom_nodes = atom_nodes\n\t\treturn\n'"
TensorMol/Containers/MolFrag.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Util import *\nimport numpy as np\nimport random, math\nfrom .Mol import *\n#from ..ForceModels.Electrostatics import *\n\ndef Submit_Script_Lines(order=str(3), sub_order =str(1), index=str(1), mincase = str(0), maxcase = str(1000), name = ""MBE"", ncore = str(4), queue=""long""):\n\tlines = ""#!/bin/csh\\n""+""# Submit a job for 8  processors\\n""+""#$ -N ""+name+""\\n#$ -t ""+mincase+""-""+maxcase+"":1\\n""+""#$ -pe smp ""+ncore+""\\n""+""#$ -r n\\n""+""#$ -q ""+queue+""\\n\\n\\n""\n\tlines += ""module load gcc/5.2.0\\nsetenv  QC /afs/crc.nd.edu/group/parkhill/qchem85\\nsetenv  QCAUX /afs/crc.nd.edu/group/parkhill/QCAUX_1022\\nsetenv  QCPLATFORM LINUX_Ix86\\n\\n\\n""\n\tlines += ""/afs/crc.nd.edu/group/parkhill/qchem85/bin/qchem  -nt ""+ncore+""   ""+str(order)+""/""+""${SGE_TASK_ID}/""+sub_order+""/""+index+"".in  ""+str(order)+""/""+""${SGE_TASK_ID}/""+sub_order+""/""+index+"".out\\n\\nrm MBE*.o*""\n\treturn lines\n\ndef String_To_Atoms(s=""""):\n\tl = list(s)\n\tatom_l = []\n\ttmp = """"\n\tfor i, c in enumerate(l):\n\t\tif  ord(\'A\') <= ord(c) <= ord(\'Z\'):\n\t\t\ttmp=c\n\t\telse:\n\t\t\ttmp += c\n\t\tif i==len(l)-1:\n\t\t\tatom_l.append(tmp)\n\t\telif ord(\'A\') <= ord(l[i+1]) <= ord(\'Z\'):\n\t\t\tatom_l.append(tmp)\n\t\telse:\n\t\t\tcontinue\n\treturn atom_l\n\ndef Binominal_Combination(indis=[0,1,2], group=3):\n\tif (group==1):\n\t\tindex=list(itertools.permutations(indis))\n\t\tnew_index =[]\n\t\tfor i in range (0, len(index)):\n\t\t\tnew_index.append(list(index[i]))\n\t\treturn new_index\n\telse:\n\t\tindex=list(itertools.permutations(indis))\n\t\tnew_index=[]\n\t\tfor sub_list in Binominal_Combination(indis, group-1):\n\t\t\tfor sub_index in index:\n\t\t\t\tnew_index.append(list(sub_list)+list(sub_index))\n\t\treturn new_index\n\nclass FragableCluster(Mol):\n\t"""""" Provides a cluster which can be fragmented into molecules""""""\n\tdef __init__(self, atoms_ =  None, coords_ = None):\n\t\tMol.__init__(self,atoms_,coords_)\n\t\tself.mbe_order = PARAMS[""MBE_ORDER""]\n\t\tself.frag_list = []    # list of [{""atom"":.., ""charge"":..},{""atom"":.., ""charge"":..},{""atom"":.., ""charge"":..}]\n\t\tself.type_of_frags = []  # store the type of frag (1st order) in the self.mbe_frags:  [1,1,1 (H2O), 2,2,2(Na),3,3,3(Cl)]\n\t\tself.type_of_frags_dict = {}\n\t\tself.atoms_of_frags = [] # store the index of atoms of each frag\n\t\tself.mbe_frags=dict()    # list of  frag of each order N, dic[\'N\'=list of frags]\n\t\tself.mbe_frags_deri=dict()\n\t\tself.mbe_frags_energy=dict()  # MBE energy of each order N, dic[\'N\'= E_N]\n\t\tself.mbe_energy=dict()   # sum of MBE energy up to order N, dic[\'N\'=E_sum]\n\t\tself.mbe_deri =None\n\t\tself.ngroup=None\n\t\treturn\n\n\tdef Reset_Frags(self):\n\t\tself.mbe_frags=dict()    # list of  frag of each order N, dic[\'N\'=list of frags]\n\t\tself.mbe_frags_deri=dict()\n\t\tself.mbe_permute_frags=dict() # list of all the permuted frags\n\t\tself.mbe_frags_energy=dict()  # MBE energy of each order N, dic[\'N\'= E_N]\n\t\tself.energy=None\n\t\tself.mbe_energy=dict()   # sum of MBE energy up to order N, dic[\'N\'=E_sum]\n\t\tself.mbe_deri =None\n\t\tself.nn_energy=None\n\t\treturn\n\n\tdef Sort_frag_list(self):\n\t\ta=[]\n\t\tfor dic in self.frag_list:\n\t\t\ta.append(len(dic[""atom""]))\n\t\tself.frag_list = [x for (y,x) in sorted(zip(a,self.frag_list))]\n\t\tself.frag_list.reverse()\n\t\treturn self.frag_list\n\n\tdef Generate_All_Pairs(self, pair_list=[]):\n\t\tmono = []\n\t\tfor pair in pair_list:\n\t\t\tfor frag in pair[\'mono\']:\n\t\t\t\tmono.append([atoi[atom] for atom in  String_To_Atoms(frag)])\n\t\ttmp = []\n\t\tfor frag in mono:\n\t\t\tif frag not in tmp:\n\t\t\t\ttmp.append(frag)\n\t\tmono = tmp\n\t\tmono.sort(key=lambda x:len(x))\n\t\tmono.reverse()\n\t\tdic_mono = {}\n\t\tdic_mono_index = {}\n\t\tmasked = []\n\t\tfor frag_atoms in mono:\n\t\t\tfrag_name = LtoS(frag_atoms)\n\t\t\tdic_mono[frag_name] = []\n\t\t\tdic_mono_index[frag_name] = []\n\t\t\tnum_frag_atoms = len(frag_atoms)\n\t\t\tj = 0\n\t\t\twhile (j < self.NAtoms()):\n\t\t\t\tif j in masked:\n\t\t\t\t\tj += 1\n\t\t\t\telse:\n\t\t\t\t\ttmp_list = list(self.atoms[j:j+num_frag_atoms])\n\t\t\t\t\tif tmp_list == frag_atoms:\n\t\t\t\t\t\tdic_mono[frag_name].append(self.coords[j:j+num_frag_atoms,:].copy())\n\t\t\t\t\t\tdic_mono_index[frag_name].append(range (j, j+num_frag_atoms))\n\t\t\t\t\t\tmasked += range (j, j+num_frag_atoms)\n\t\t\t\t\t\tj += num_frag_atoms\n\t\t\t\t\telse:\n\t\t\t\t\t\tj += 1\n\t\thappy_atoms = []\n\t\tfor pair in pair_list:\n\t\t\thappy_atoms = self.PairUp(dic_mono, dic_mono_index, pair, happy_atoms)  #it is amazing that the dictionary is passed by pointer...\n\t\tleft_atoms = list(set(range (0, self.NAtoms())) - set(happy_atoms))\n\t\tsorted_atoms = happy_atoms + left_atoms\n\t\tself.atoms = self.atoms[sorted_atoms]\n\t\tself.coords = self.coords[sorted_atoms]\n\t\treturn\n\n\tdef PairUp(self, dic_mono, dic_mono_index, pair, happy_atoms):  # stable marriage pairing  Ref: https://en.wikipedia.org/wiki/Stable_marriage_problem\n\t\tmono_1 = LtoS([atoi[atom] for atom in  String_To_Atoms(pair[""mono""][0])])\n\t\tmono_2 = LtoS([atoi[atom] for atom in  String_To_Atoms(pair[""mono""][1])])\n\t\tcenter_1 = pair[""center""][0]\n\t\tcenter_2 = pair[""center""][1]\n\t\tswitched = False\n\t\tif len(dic_mono[mono_1]) > len(dic_mono[mono_2]):\n\t\t\tmono_1, mono_2 = mono_2, mono_1\n\t\t\tcenter_1, center_2  = center_2, center_1\n\t\t\tswitched = True\n\t\tmono_1_pair = [-1]*len(dic_mono[mono_1])\n\t\tdist_matrix = np.zeros((len(dic_mono[mono_1]), len(dic_mono[mono_2])))\n\t\tfor i in range (0, len(dic_mono[mono_1])):\n\t\t\tfor j in range (0, len(dic_mono[mono_2])):\n\t\t\t\tdist_matrix[i][j] = np.linalg.norm(dic_mono[mono_1][i][center_1] - dic_mono[mono_2][j][center_2])\n\t\tmono_1_prefer = []\n\t\tmono_2_prefer = []\n\t\tfor i in range (0, len(dic_mono[mono_1])):\n\t\t\ts = list(dist_matrix[i])\n\t\t\tmono_1_prefer.append(sorted(range(len(s)), key=lambda k: s[k]))\n\t\tfor i in range (0, len(dic_mono[mono_2])):\n\t\t\ts = list(dist_matrix[:,i])\n\t\t\tmono_2_prefer.append(sorted(range(len(s)), key=lambda k: s[k]))\n\t\tmono_1_info = [-1]*len(dic_mono[mono_1]) # -1 means they are not paired, and the number means the Nth most prefered are chosen\n\t\tmono_2_info = [-1]*len(dic_mono[mono_2])\n\t\tmono_1_history = [0]*len(dic_mono[mono_1]) # history of the man\'s proposed\n\t\t# first round  mono_1 is the man, mono_2 is woman,  num of man > num of woman\n\t\tfor i in range (0, len(dic_mono[mono_1])):\n\t\t\ttarget = mono_1_prefer[i][0]\n\t\t\tif i == mono_2_prefer[target][0]:  # Congs! find true lovers\n\t\t\t\tmono_1_info[i] = 0\n\t\t\t\tmono_2_info[target] = 0\n\t\t\tmono_1_history[i] += 1\n\t\twhile (-1 in mono_1_info):\n\t\t\tfor i in range (0, len(dic_mono[mono_1])):\n\t\t\t\tif mono_1_info[i] == -1:\n\t\t\t\t\ttarget = mono_1_prefer[i][mono_1_history[i]] # propose\n\t\t\t\t\tif mono_2_info[target] == -1: # met a single woman\n\t\t\t\t\t\tmono_1_info[i] = mono_1_history[i]\n\t\t\t\t\t\tmono_2_info[target] = mono_2_prefer[target].index(i)\n\t\t\t\t\t\tmono_1_history[i] += 1\n\t\t\t\t\telif mono_2_info[target] > mono_2_prefer[target].index(i):   # this man is the better choice than the previous one\n\t\t\t\t\t\tpoorguy = mono_2_prefer[target][mono_2_info[target]]\n\t\t\t\t\t\tmono_1_info[poorguy] = -1   # this poor guy is abandoned...\n\t\t\t\t\t\tmono_1_info[i] = mono_1_history[i]\n\t\t\t\t\t\tmono_2_info[target] = mono_2_prefer[target].index(i)\n\t\t\t\t\t\tmono_1_history[i] += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tmono_1_history[i] += 1\n\t\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\tcontinue\n\t\tfinal_pairs = []\n\t\tfor i in range (0, len(dic_mono[mono_1])):\n\t\t\tfinal_pairs.append([i, mono_1_prefer[i][mono_1_info[i]]])\n\t\tfor i in range (0, len(final_pairs)):\n\t\t\tif switched == False:\n\t\t\t\t#print dic_mono_index[mono_1][final_pairs[i][0]], dic_mono_index[mono_2][final_pairs[i][1]]\n\t\t\t\thappy_atoms += dic_mono_index[mono_1][final_pairs[i][0]]\n\t\t\t\thappy_atoms += dic_mono_index[mono_2][final_pairs[i][1]]\n\t\t\telse:\n\t\t\t\thappy_atoms += dic_mono_index[mono_2][final_pairs[i][1]]\n\t\t\t\thappy_atoms += dic_mono_index[mono_1][final_pairs[i][0]]\n\t\tindices_1 = [item[0] for item in final_pairs]\n\t\tindices_2 = [item[1] for item in final_pairs]\n\t\tdic_mono_index[mono_1] = [i for j, i in enumerate(dic_mono_index[mono_1]) if j not in indices_1]\n\t\tdic_mono_index[mono_2] = [i for j, i in enumerate(dic_mono_index[mono_2]) if j not in indices_2]\n\t\tdic_mono[mono_1] = [i for j, i in enumerate(dic_mono[mono_1]) if j not in indices_1]\n\t\tdic_mono[mono_2] = [i for j, i in enumerate(dic_mono[mono_2]) if j not in indices_2]\n\t\t#print dic_mono_index[mono_1], dic_mono_index[mono_2], happy_atoms\n\t\treturn happy_atoms\n\n\tdef Generate_All_MBE_term_General(self, frag_list=[], cutoff=10, center_atom=[]):\n\t\tself.frag_list = frag_list\n\t\t#self.Sort_frag_list()  # debug, not sure it is necessary\n\t\tif center_atom == []:\n\t\t\tcenter_atom = [0]*len(frag_list)\n\t\tfor i in range (1, self.mbe_order+1):\n\t\t\tprint(""Generating order"", i)\n\t\t\tself.Generate_MBE_term_General(i, cutoff, center_atom)\n\t\treturn\n\n\tdef Generate_MBE_term_General(self, order,  cutoff=10, center_atom=[]):\n\t\tif order in self.mbe_frags.keys():\n\t\t\tprint((""MBE order"", order, ""already generated..skipping..""))\n\t\t\treturn\n\t\tif order==1:\n\t\t\tself.mbe_frags[order] = []\n\t\t\tmasked=[]\n\t\t\tfrag_index = 0\n\t\t\t# Generating MBE frags with stable marriage\n\t\t\tfor i, dic in enumerate(self.frag_list):\n\t\t\t\tself.type_of_frags_dict[i] = []\n\t\t\t\tfrag_atoms = String_To_Atoms(dic[""atom""])\n\t\t\t\tfrag_atoms = [atoi[atom] for atom in frag_atoms]\n\t\t\t\tnum_frag_atoms = len(frag_atoms)\n\t\t\t\tj = 0\n\t\t\t\twhile (j < self.NAtoms()):\n\t\t\t\t\tif j in masked:\n\t\t\t\t\t\tj += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\ttmp_list = list(self.atoms[j:j+num_frag_atoms])\n\t\t\t\t\t\tif tmp_list == frag_atoms:\n\t\t\t\t\t\t\tself.atoms_of_frags.append([])\n\t\t\t\t\t\t\tmasked += range (j, j+num_frag_atoms)\n\t\t\t\t\t\t\tself.atoms_of_frags[-1]=range (j, j+num_frag_atoms)\n\t\t\t\t\t\t\tself.type_of_frags.append(i)\n\t\t\t\t\t\t\tself.type_of_frags_dict[i].append(frag_index)\n\t\t\t\t\t\t\ttmp_coord = self.coords[j:j+num_frag_atoms,:].copy()\n\t\t\t\t\t\t\ttmp_atom  = self.atoms[j:j+num_frag_atoms].copy()\n\t\t\t\t\t\t\tmbe_terms = [frag_index]\n\t\t\t\t\t\t\tmbe_dist = None\n\t\t\t\t\t\t\tatom_group = [num_frag_atoms]\n\t\t\t\t\t\t\tdic[\'num_electron\'] = sum(list(tmp_atom))-dic[\'charge\']\n\t\t\t\t\t\t\tfrag_type = [dic]\n\t\t\t\t\t\t\tfrag_type_index = [i]\n\t\t\t\t\t\t\ttmp_mol = Frag(tmp_atom, tmp_coord, mbe_terms, mbe_dist, atom_group, frag_type, frag_type_index, FragOrder_=order)\n\t\t\t\t\t\t\tself.mbe_frags[order].append(tmp_mol)\n\t\t\t\t\t\t\tj += num_frag_atoms\n\t\t\t\t\t\t\tfrag_index += 1\n\t\t\t\t\t\t\t#print self.atoms_of_frags, tmp_list, self.type_of_frags\n\t\t\t\t\t\t\t#print self.mbe_frags[order][-1].atoms, self.mbe_frags[order][-1].coords, self.mbe_frags[order][-1].index\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tj += 1\n\t\telse:\n\t\t\tnum_of_each_frag = {}\n\t\t\tfrag_list_length = len(self.frag_list)\n\t\t\tfrag_list_index = range (0, frag_list_length)\n\t\t\tfrag_list_index_list = list(itertools.product(frag_list_index, repeat=order))\n\t\t\ttmp_index_list = []\n\t\t\tfor i in range (0, len(frag_list_index_list)):\n\t\t\t\ttmp_index = list(frag_list_index_list[i])\n\t\t\t\ttmp_index.sort()\n\t\t\t\tif tmp_index not in tmp_index_list:\n\t\t\t\t\ttmp_index_list.append(tmp_index)\n\t\t\t\t\tnum_of_each_frag[LtoS(tmp_index)]=0\n\t\t\tself.mbe_frags[order] = []\n\t\t\tmbe_terms=[]\n\t\t\tmbe_dist=[]\n\t\t\tngroup = len(self.mbe_frags[1])\t#\n\t\t\tatomlist=list(range(0,ngroup))\n\t\t\ttime_log = time.time()\n\t\t\tprint((""generating the combinations for order: "", order))\n\t\t\tmax_case = 5000\n\t\t\ttime_now=time.time()\n\t\t\tfor index_list in tmp_index_list:\n\t\t\t\tfrag_case = 0\n\t\t\t\tsample_index = []\n\t\t\t\tfor i in index_list:\n\t\t\t\t\tsample_index.append(self.type_of_frags_dict[i])\n\t\t\t\tprint(""begin the most time consuming step: "")\n\t\t\t\ttmp_time  = time.time()\n\t\t\t\tsub_combinations = list(itertools.product(*sample_index))\n\t\t\t\tprint((""end of the most time consuming step. time cost:"", time.time() - tmp_time))\n\t\t\t\t#shuffle_time = time.time()\n\t\t\t\t#new_begin = random.randint(1,len(sub_combinations)-2)\n\t\t\t\t#sub_combinations = sub_combinations[new_begin:]+sub_combinations[:new_begin] # debug, random shuffle the list, so the pairs are chosen randomly, this is not necessary for generate training cases\n\t\t\t\t#random.shuffle(sub_combinations)  # debug, random shuffle the list, so the pairs are chosen randomly, this is not necessary for generate training cases\n\t\t\t\t#print  ""time to shuffle it"", time.time()-shuffle_time\n\t\t\t\tfor i in range (0, len(sub_combinations)):\n\t\t\t\t\tterm = list(sub_combinations[i])\n\t\t\t\t\tif len(list(set(term))) < len(term):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tpairs=list(itertools.combinations(term, 2))\n\t\t\t\t\tsaveindex=[]\n\t\t\t\t\tdist = [float(\'inf\')]*len(pairs)\n\t\t\t\t\tflag=1\n\t\t\t\t\tnpairs=len(pairs)\n\t\t\t\t\tfor j in range (0, npairs):\n\t\t\t\t\t\t#print self.type_of_frags[pairs[j][0]], self.type_of_frags[pairs[j][1]], pairs[j][0], pairs[j][1]\n\t\t\t\t\t\tif self.type_of_frags[pairs[j][0]] == -1 :\n\t\t\t\t\t\t\tcenter_1 = self.Center()\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tcenter_1 = self.mbe_frags[1][pairs[j][0]].coords[center_atom[self.type_of_frags[pairs[j][0]]]]\n\t\t\t\t\t\tif self.type_of_frags[pairs[j][1]] == -1 :\n\t\t\t\t\t\t\tcenter_2 = self.Center()\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tcenter_2 = self.mbe_frags[1][pairs[j][1]].coords[center_atom[self.type_of_frags[pairs[j][1]]]]\n\t\t\t\t\t\tdist[j] = np.linalg.norm(center_1- center_2)\n\t\t\t\t\t\tif dist[j] > cutoff:\n\t\t\t\t\t\t\tflag = 0\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\tif flag == 1:   # we find a frag\n\t\t\t\t\t\tif frag_case%100==0:\n\t\t\t\t\t\t\tprint(""working on frag:"", frag_case, ""frag_type:"", index_list, "" i:"", i)\n\t\t\t\t\t\tfrag_case  += 1\n\t\t\t\t\t\tif  frag_case >=  max_case:   # just for generating training case\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tmbe_terms.append(term)\n\t\t\t\t\t\tmbe_dist.append(dist)\n\t\t\tprint((""finished..takes"", time_log-time.time(),""second""))\n\t\t\tmbe_frags = []\n\t\t\tfor i in range (0, len(mbe_terms)):\n\t\t\t\tfrag_type = []\n\t\t\t\tfrag_type_index = []\n\t\t\t\tatom_group = []\n\t\t\t\tfor index in mbe_terms[i]:\n\t\t\t\t\tfrag_type.append(self.frag_list[self.type_of_frags[index]])\n\t\t\t\t\tfrag_type_index.append(self.type_of_frags[index])\n\t\t\t\t\tatom_group.append(self.mbe_frags[1][index].atoms.shape[0])\n\t\t\t\ttmp_coord = np.zeros((sum(atom_group), 3))\n\t\t\t\ttmp_atom = np.zeros(sum(atom_group), dtype=np.uint8)\n\t\t\t\tpointer = 0\n\t\t\t\tfor j, index in enumerate(mbe_terms[i]):\n\t\t\t\t\ttmp_coord[pointer:pointer+atom_group[j],:] = self.mbe_frags[1][index].coords\n\t\t\t\t\ttmp_atom[pointer:pointer+atom_group[j]] = self.mbe_frags[1][index].atoms\n\t\t\t\t\tpointer += atom_group[j]\n\t\t\t\ttmp_mol = Frag(tmp_atom, tmp_coord, mbe_terms[i], mbe_dist[i], atom_group, frag_type, frag_type_index, FragOrder_=order)\n\t\t\t\tself.mbe_frags[order].append(tmp_mol)\n\t\t\t# completed self.mbe_frags\n\t\t\tdel sub_combinations\n\t\treturn\n\n\tdef Calculate_Frag_Energy_General(self, order, method=""pyscf""):\n\t\tif order in self.mbe_frags_energy.keys():\n\t\t\tprint((""MBE order"", order, ""already calculated..skipping..""))\n\t\t\treturn 0\n\t\tmbe_frags_energy = 0.0\n\t\tfragnum=0\n\t\ttime_log=time.time()\n\t\tif method == ""qchem"":\n\t\t\torder_path = self.qchem_data_path+""/""+str(order)\n\t\t\tif not os.path.isdir(order_path):\n\t\t\t\tos.mkdir(order_path)\n\t\t\tos.chdir(order_path)\n\t\t\ttime0 =time.time()\n\t\t\tfor frag in self.mbe_frags[order]:  # just for generating the training set..\n\t\t\t\tfragnum += 1\n\t\t\t\tif fragnum%100 == 0:\n\t\t\t\t\tprint(""working on frag:"", fragnum)\n\t\t\t\t\tprint(""total time:"", time.time() - time0)\n\t\t\t\ttime0 = time.time()\n\t\t\t\tfrag.Write_Qchem_Frag_MBE_Input_All_General(fragnum)\n\t\t\tos.chdir(""../../../.."")\n\t\telif method == ""pyscf"":\n\t\t\tfor frag in self.mbe_frags[order]:\n\t\t\t\tfragnum += 1\n\t\t\t\tprint(""doing the "",fragnum)\n\t\t\t\tfrag.PySCF_Frag_MBE_Energy_All()\n\t\t\t\tfrag.Set_Frag_MBE_Energy()\n\t\t\t\tmbe_frags_energy += frag.frag_mbe_energy\n\t\t\tprint(""Finished, spent "", time.time()-time_log,"" seconds"")\n\t\t\ttime_log = time.time()\n\t\t\tself.mbe_frags_energy[order] = mbe_frags_energy\n\t\telse:\n\t\t\traise Exception(""unknow ab-initio software!"")\n\t\treturn\n\n\tdef Get_Qchem_Frag_Energy(self, order):\n\t\tfragnum = 0\n\t\tpath = self.qchem_data_path+""/""+str(order)\n\t\tmbe_frags_energy = 0.0\n\t\tfor frag in self.mbe_frags[order]:\n\t\t\tfragnum += 1\n\t\t\tfrag.Get_Qchem_Frag_MBE_Energy_All(fragnum, path)\n\t\t\tprint(""working on molecule:"", self.name,"" frag:"",fragnum, "" order:"",order)\n\t\t\tfrag.Set_Frag_MBE_Energy()\n\t\t\tmbe_frags_energy += frag.frag_mbe_energy\n\t\t\t#if order==2:\n\t\t#\t\tprint frag.frag_mbe_energy, frag.dist[0]\n\t\tself.mbe_frags_energy[order] = mbe_frags_energy\n\t\treturn\n\n\tdef Get_All_Qchem_Frag_Energy_General(self):\n\t\tfor i in range (1, self.mbe_order+1):\n\t\t\tself.Get_Qchem_Frag_Energy(i)\n\t\treturn\n\n\tdef Calculate_All_Frag_Energy_General(self, method=""pyscf""):\n\t\tif method == ""qchem"":\n\t\t\tif not os.path.isdir(""./qchem""):\n\t\t\t\tos.mkdir(""./qchem"")\n\t\t\tif not os.path.isdir(""./qchem""+""/""+self.properties[""set_name""]):\n\t\t\t\tos.mkdir(""./qchem""+""/""+self.properties[""set_name""])\n\t\t\tself.qchem_data_path=""./qchem""+""/""+self.properties[""set_name""]+""/""+self.name\n\t\t\tprint(""set_name"",  self.properties[""set_name""], "" self.name"", self.name)\n\t\t\tif not os.path.isdir(self.qchem_data_path):\n\t\t\t\tos.mkdir(self.qchem_data_path)\n\t\tfor i in range (1, self.mbe_order+1):\n\t\t\tprint(""calculating for MBE order"", i)\n\t\t\tself.Calculate_Frag_Energy_General(i, method)\n\t\tif method == ""qchem"":\n\t\t\tself.Write_Qchem_Submit_Script()\n\t\treturn\n\n\tdef Write_Qchem_Submit_Script(self):     # this is for submitting the jobs on notre dame crc\n\t\tif not os.path.isdir(""./qchem""):\n\t\t\tos.mkdir(""./qchem"")\n\t\t\tif not os.path.isdir(""./qchem""+""/""+self.properties[""set_name""]):\n\t\t\t\tos.mkdir(""./qchem""+""/""+self.properties[""set_name""])\n\t\t\t\tself.qchem_data_path=""./qchem""+""/""+self.properties[""set_name""]+""/""+self.name\n\t\tif not os.path.isdir(self.qchem_data_path):\n\t\t\tos.mkdir(self.qchem_data_path)\n\t\tos.chdir(self.qchem_data_path)\n\t\tfor i in range (1, self.mbe_order+1):\n\t\t\tnum_frag = len(self.mbe_frags[i])\n\t\t\tfor j in range (1, i+1):\n\t\t\t\tindex=nCr(i, j)\n\t\t\t\tfor k in range (1, index+1):\n\t\t\t\t\tsubmit_file = open(""qchem_order_""+str(i)+""_suborder_""+str(j)+""_index_""+str(k)+"".sub"",""w+"")\n\t\t\t\t\tlines = Submit_Script_Lines(order=str(i), sub_order =str(j), index=str(k), mincase = str(1), maxcase = str(num_frag), name = ""MBE_""+str(i)+""_""+str(j)+""_""+str(index), ncore = str(4), queue=""long"")\n\t\t\t\t\tsubmit_file.write(lines)\n\t\t\t\t\tsubmit_file.close()\n\n\t\tpython_submit = open(""submit_all.py"",""w+"")\n\t\tline = \'import os,sys\\n\\nfor file in os.listdir("".""):\\n        if file.endswith("".sub""):\\n                cmd = ""qsub ""+file\\n                os.system(cmd)\\n\'\n\t\tpython_submit.write(line)\n\t\tpython_submit.close()\n\t\tos.chdir(""../../../"")\n\t\treturn\n\n\tdef Set_MBE_Energy(self):\n\t\tfor i in range (1, self.mbe_order+1):\n\t\t\tself.mbe_energy[i] = 0.0\n\t\t\tfor j in range (1, i+1):\n\t\t\t\tself.mbe_energy[i] += self.mbe_frags_energy[j]\n\t\treturn\n\n\tdef MBE(self,  atom_group=1, cutoff=10, center_atom=0, max_case = 1000000):\n\t\tself.Generate_All_MBE_term_General(atom_group, cutoff, center_atom, max_case)\n\t\tself.Calculate_All_Frag_Energy_General()\n\t\tself.Set_MBE_Energy()\n\t\treturn\n\n\tdef Set_Frag_Force_with_Order(self, cm_deri, nn_deri, order):\n\t\tself.mbe_frags_deri[order]=np.zeros((self.NAtoms(),3))\n\t\tatom_group = self.mbe_frags[order][0].atom_group  # get the number of  atoms per group by looking at the frags.\n\t\tfor i in range (0, len(self.mbe_frags[order])):\n\t\t\tderi = self.mbe_frags[order][i].Frag_Force(cm_deri[i], nn_deri[i])\n\t\t\tderi = deri.reshape((order, deri.shape[0]/order, -1))\n\t\t\tindex_list = self.mbe_frags[order][i].index\n\t\t\tfor j in range (0,  len(index_list)):\n\t\t\t\tself.mbe_frags_deri[order][index_list[j]*atom_group:(index_list[j]+1)*atom_group] += deri[j]\n\t\treturn\n\n\tdef Set_MBE_Force(self):\n\t\tself.mbe_deri = np.zeros((self.NAtoms(), 3))\n\t\tfor order in range (1, self.mbe_order+1): # we ignore the 1st order term since we are dealing with helium, debug\n\t\t\tif order in self.mbe_frags_deri.keys():\n\t\t\t\tself.mbe_deri += self.mbe_frags_deri[order]\n\t\treturn self.mbe_deri\n\nclass Frag(Mol):\n\t"""""" Provides a MBE frag of general purpose cluster""""""\n\tdef __init__(self, atoms_ =  None, coords_ = None, index_=None, dist_=None, atom_group_=1, frag_type_=None, frag_type_index_=None, FragOrder_=None):\n\t\tMol.__init__(self, atoms_, coords_)\n\t\tself.atom_group = atom_group_\n\t\tif FragOrder_==None:\n\t\t\tself.FragOrder = self.coords.shape[0]/self.atom_group\n\t\telse:\n\t\t\tself.FragOrder = FragOrder_\n\t\tif (index_!=None):\n\t\t\tself.index = index_\n\t\telse:\n\t\t\tself.index = None\n\t\tif (dist_!=None):\n\t\t\tself.dist = dist_\n\t\telse:\n\t\t\tself.dist = None\n\t\tif (frag_type_!=None):\n\t\t\tself.frag_type = frag_type_\n\t\telse:\n\t\t\tself.frag_type = None\n\t\tif (frag_type_!=None):\n\t\t\tself.frag_type_index = frag_type_index_\n\t\telse:\n\t\t\tself.frag_type_index = None\n\t\tself.frag_mbe_energies=dict()\n\t\tself.frag_mbe_energy = None\n\t\tself.frag_energy = None\n\t\tself.permute_index = range (0, self.FragOrder)\n\t\tself.permute_sub_index = None\n\t\treturn\n\n\tdef PySCF_Frag_MBE_Energy(self,order):   # calculate the MBE of order N of each frag\n\t\t""""""\n\t\t# Below is the old version of PySCF_Frag_MBE_Energy, not working for General MBE, needs to be rewritten, KY\n\n\t\tinner_index = range(0, self.FragOrder)\n\t\treal_frag_index=list(itertools.combinations(inner_index,order))\n\t\tghost_frag_index=[]\n\t\tfor i in range (0, len(real_frag_index)):\n\t\t\tghost_frag_index.append(list(set(inner_index)-set(real_frag_index[i])))\n\n\t\ti =0\n\t\twhile(i< len(real_frag_index)):\n\t\t\t#for i in range (0, len(real_frag_index)):\n\t\t\tpyscfatomstring=""""\n\t\t\tmol = gto.Mole()\n\t\t\tfor j in range (0, order):\n\t\t\t\tfor k in range (0, self.atom_group):\n\t\t\t\t\ts = self.coords[real_frag_index[i][j]*self.atom_group+k]\n\t\t\t\t\tpyscfatomstring=pyscfatomstring+str(self.AtomName(real_frag_index[i][j]*self.atom_group+k))+"" ""+str(s[0])+"" ""+str(s[1])+"" ""+str(s[2])+"";""\n\t\t\tfor j in range (0, self.FragOrder - order):\n\t\t\t\tfor k in range (0, self.atom_group):\n\t\t\t\t\ts = self.coords[ghost_frag_index[i][j]*self.atom_group+k]\n\t\t\t\t\tpyscfatomstring=pyscfatomstring+""GHOST""+str(j*self.atom_group+k)+"" ""+str(s[0])+"" ""+str(s[1])+"" ""+str(s[2])+"";""\n\t\t\tpyscfatomstring=pyscfatomstring[:-1]+""  ""\n\t\t\tmol.atom =pyscfatomstring\n\n\t\t\tmol.basis ={}\n\t\t\tele_set = list(set(self.AllAtomNames()))\n\t\t\tfor ele in ele_set:\n\t\t\t\tmol.basis[str(ele)]=""cc-pvqz""\n\n\t\t\tfor j in range (0, self.FragOrder - order):\n\t\t\t\tfor k in range (0, self.atom_group):\n\t\t\t\t\tatom_type = self.AtomName(ghost_frag_index[i][j]*self.atom_group+k)\n\t\t\t\t\tmol.basis[\'GHOST\'+str(j*self.atom_group+k)]=gto.basis.load(\'cc-pvqz\',str(atom_type))\n\t\t\tmol.verbose=0\n\t\t\ttry:\n\t\t\t\tprint ""doing case "", i\n\t\t\t\ttime_log = time.time()\n\t\t\t\tmol.build()\n\t\t\t\tmf=scf.RHF(mol)\n\t\t\t\thf_en = mf.kernel()\n\t\t\t\tmp2 = mp.MP2(mf)\n\t\t\t\tmp2_en = mp2.kernel()\n\t\t\t\ten = hf_en + mp2_en[0]\n\t\t\t\t#print ""hf_en"", hf_en, ""mp2_en"", mp2_en[0], "" en"", en\n\t\t\t\tself.frag_mbe_energies[LtoS(real_frag_index[i])]=en\n\t\t\t\tprint (""pyscf time.."", time.time()-time_log)\n\t\t\t\ti = i+1\n\t\t\t\tgc.collect()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint ""PYSCF Calculation error... :"",Ex\n\t\t\t\tprint ""Mol.atom:"", mol.atom\n\t\t\t\tprint ""Pyscf string:"", pyscfatomstring\n\t\t""""""\n\t\traise Exception(""Pyscf for General MBE has not be implemented yet"")\n\t\treturn\n\n\tdef Get_Qchem_Frag_MBE_Energy(self, order, path):\n\t\t#print ""path:"", path, ""order:"", order\n\t\tonlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\t\t#print ""onlyfiles:"", onlyfiles, ""path"", path, ""order"", order\n\t\tfor outfile_name in onlyfiles:\n\t\t\tif ( outfile_name[-4:]!=\'.out\' ):\n\t\t\t\t\tcontinue\n\t\t\toutfile = open(path+""/""+outfile_name,""r+"")\n\t\t\toutfile_lines = outfile.readlines()\n\t\t\tkey = None\n\t\t\trimp2 = None\n\t\t\tfor line in outfile_lines:\n\t\t\t\tif ""!"" in line:\n\t\t\t\t\tkey = line[1:-1]\n\t\t\t\t\tcontinue\n\t\t\t\tif ""non-Brillouin singles"" in line:\n\t\t\t\t\tnonB_single = float(line.split()[3])\n\t\t\t\t\tcontinue\n\t\t\t\tif ""RIMP2         total energy"" in line:\n\t\t\t\t\trimp2 = float(line.split()[4])\n\t\t\t\t\tcontinue\n\t\t\t\tif ""fatal error"" in line:\n\t\t\t\t\tprint(""fata error! file:"", path+""/""+outfile_name)\n\t\t\tif nonB_single != 0.0:\n\t\t\t\tprint(""Warning: non-Brillouin singles do not equal to zero, non-Brillouin singles="",nonB_single,path,outfile_name)\n\t\t\tif key!=None and rimp2!=None:\n\t\t\t\t#print ""key:"", key, ""length:"", len(key)\n\t\t\t\tself.frag_mbe_energies[key] = rimp2\n\t\t\telse:\n\t\t\t\tprint(""Qchem Calculation error on "",path,outfile_name)\n\t\t\t\traise Exception(""Qchem Error"")\n\t\treturn\n\n\tdef Write_Qchem_Frag_MBE_Input_General(self,order):   # calculate the MBE of order N of each frag\n\t\tinner_index = range(0, self.FragOrder)\n\t\treal_frag_index=list(itertools.combinations(inner_index,order))\n\t\tghost_frag_index=[]\n\t\tfor i in range (0, len(real_frag_index)):\n\t\t\tghost_frag_index.append(list(set(inner_index)-set(real_frag_index[i])))\n\t\ti =0\n\t\twhile(i< len(real_frag_index)):\n\t\t\tcharge = 0\n\t\t\tnum_ele = 0\n\t\t\tfor j in range (0, order):\n\t\t\t\tcharge += self.frag_type[real_frag_index[i][j]][""charge""]\n\t\t\t\tnum_ele += self.frag_type[real_frag_index[i][j]][""num_electron""]\n\t\t\tif num_ele%2 == 0:   # here we always prefer the low spin state\n\t\t\t\tspin = 1\n\t\t\telse:\n\t\t\t\tspin = 2\n\t\t\tqchemstring=""$molecule\\n""+str(charge)+"" ""+str(spin)+""\\n""\n\t\t\tfor j in range (0, order):\n\t\t\t\tpointer = sum(self.atom_group[:real_frag_index[i][j]])\n\t\t\t\tfor k in range (0, self.atom_group[real_frag_index[i][j]]):\n\t\t\t\t\ts = self.coords[pointer+k]\n\t\t\t\t\tqchemstring+=str(self.AtomName(pointer+k))+"" ""+str(s[0])+"" ""+str(s[1])+"" ""+str(s[2])+""\\n""\n\t\t\tfor j in range (0, self.FragOrder - order):\n\t\t\t\tpointer = sum(self.atom_group[:ghost_frag_index[i][j]])\n\t\t\t\tfor k in range (0, self.atom_group[ghost_frag_index[i][j]]):\n\t\t\t\t\ts = self.coords[pointer+k]\n\t\t\t\t\tqchemstring+=""@""+str(self.AtomName(pointer+k))+"" ""+str(s[0])+"" ""+str(s[1])+"" ""+str(s[2])+""\\n""\n\t\t\tqchemstring += ""$end\\n""\n\t\t\tqchemstring += ""!""+LtoS(real_frag_index[i])+""\\n""\n\t\t\tqchemstring += PARAMS[""Qchem_RIMP2_Block""]\n\t\t\tqchem_input=open(str(i+1)+"".in"",""w+"")\n\t\t\tqchem_input.write(qchemstring)\n\t\t\tqchem_input.close()\n\t\t\ti = i+1\n\t\t\t#gc.collect()  # speed up the function by 1000 times just deleting this single line!\n\t\t\treturn\n\n\n\tdef Write_Qchem_Frag_MBE_Input_All_General(self, fragnum):\n\t\tif not os.path.isdir(str(fragnum)):\n\t\t\tos.mkdir(str(fragnum))\n\t\tos.chdir(str(fragnum))\n\t\tfor i in range (0, self.FragOrder):\n\t\t\tif not os.path.isdir(str(i+1)):\n\t\t\t\tos.mkdir(str(i+1))\n\t\t\tos.chdir(str(i+1))\n\t\t\tself.Write_Qchem_Frag_MBE_Input_General(i+1)\n\t\t\tos.chdir("".."")\n\t\tos.chdir("".."")\n\t\treturn\n\n\tdef Write_Qchem_Frag_MBE_Input_All(self, fragnum):\n\t\tif not os.path.isdir(str(fragnum)):\n\t\t\tos.mkdir(str(fragnum))\n\t\tos.chdir(str(fragnum))\n\t\tfor i in range (0, self.FragOrder):\n\t\t\tif not os.path.isdir(str(i+1)):\n\t\t\t\tos.mkdir(str(i+1))\n\t\t\tos.chdir(str(i+1))\n\t\t\tself.Write_Qchem_Frag_MBE_Input(i+1)\n\t\t\tos.chdir("".."")\n\t\tos.chdir("".."")\n\t\treturn\n\n\tdef Get_Qchem_Frag_MBE_Energy_All(self, fragnum, path):\n\t\tif not os.path.isdir(path+""/""+str(fragnum)):\n\t\t\traise Exception(path+""/""+str(fragnum),""is not calculated"")\n\t\toldpath = path\n\t\tfor i in range (0, self.FragOrder):\n\t\t\tpath = oldpath+""/""+str(fragnum)+""/""+str(i+1)\n\t\t\tself.Get_Qchem_Frag_MBE_Energy(i+1, path)\n\t\treturn\n\n\tdef PySCF_Frag_MBE_Energy_All(self):\n\t\tfor i in range (0, self.FragOrder):\n\t\t\tself.PySCF_Frag_MBE_Energy(i+1)\n\t\treturn\n\n\tdef Set_Frag_MBE_Energy(self):\n\t\tself.frag_mbe_energy =  self.Frag_MBE_Energy()\n\t\tself.frag_energy = self.frag_mbe_energies[LtoS(self.permute_index)]\n\t\tprint(""self.frag_type: "", self.frag_type)\n\t\tprint(""self.frag_mbe_energy: "", self.frag_mbe_energy)\n\t\treturn\n\n\tdef Frag_MBE_Energy(self,  index=None):     # Get MBE energy recursively\n\t\tif index==None:\n\t\t\tindex=range(0, self.FragOrder)\n\t\torder = len(index)\n\t\tif order==0:\n\t\t\treturn 0\n\t\tenergy = self.frag_mbe_energies[LtoS(index)]\n\t\tfor i in range (0, order):\n\t\t\tsub_index = list(itertools.combinations(index, i))\n\t\t\tfor j in range (0, len(sub_index)):\n\t\t\t\ttry:\n\t\t\t\t\tenergy=energy-self.Frag_MBE_Energy( sub_index[j])\n\t\t\t\texcept Exception as Ex:\n\t\t\t\t\tprint(""missing frag energy, error"", Ex)\n\t\treturn  energy\n\n\tdef CopyTo(self, target):\n\t\ttarget.FragOrder = self.FragOrder\n\t\ttarget.frag_mbe_energies=self.frag_mbe_energies\n\t\ttarget.frag_mbe_energy = self.frag_mbe_energy\n\t\ttarget.frag_energy = self.frag_energy\n\t\ttarget.permute_index = self.permute_index\n\n\tdef Frag_Force(self, cm_deri, nn_deri):\n\t\treturn self.Combine_CM_NN_Deri(cm_deri, nn_deri)\n\n\tdef Combine_CM_NN_Deri(self, cm_deri, nn_deri):\n\t\tnatom = self.NAtoms()\n\t\tfrag_deri = np.zeros((natom, 3))\n\t\tfor i in range (0, natom):  ## debug, this is for not including the diagnol\n\t\t\tfor j in range (0, natom):  # debug, this is for not including the diagnol\n\t\t\t\tif j >= i:\n\t\t\t\t\tcm_dx = cm_deri[i][j][0]\n\t\t\t\t\tcm_dy = cm_deri[i][j][1]\n\t\t\t\t\tcm_dz = cm_deri[i][j][2]\n\t\t\t\t\tnn_deri_index = i*(natom+natom-i-1)/2 + (j-i-1) # debug, this is for not including the diagnol\n\t\t\t\t\t#nn_deri_index = i*(natom+natom-i+1)/2 + (j-i)  # debug, this is for including the diagnol in the CM\n\t\t\t\t\tnn_dcm = nn_deri[nn_deri_index]\n\t\t\t\telse:\n\t\t\t\t\tcm_dx = cm_deri[j][i][3]\n\t\t\t\t\tcm_dy = cm_deri[j][i][4]\n\t\t\t\t\tcm_dz = cm_deri[j][i][5]\n\t\t\t\t\tnn_deri_index = j*(natom+natom-j-1)/2 + (i-j-1)  #debug , this is for not including the diangol\n\t\t\t\t\t#nn_deri_index = j*(natom+natom-j+1)/2 + (i-j)    # debug, this is for including the diagnoal in the CM\n\t\t\t\t\tnn_dcm = nn_deri[nn_deri_index]\n\t\t\t\tfrag_deri[i][0] += nn_dcm * cm_dx\n\t\t\t\tfrag_deri[i][1] += nn_dcm * cm_dy\n\t\t\t\tfrag_deri[i][2] += nn_dcm * cm_dz\n\t\treturn frag_deri\n\nclass FragableClusterBF(Mol):\n\t"""""" All the monomers can pair with each other, no cutoff""""""\n\tdef __init__(self, atoms_ =  None, coords_ = None, center_= ""COM"", cutoff_ = 4.6, width_ = 0.4):\n\t\tMol.__init__(self,atoms_,coords_)\n\t\tself.mbe_order = PARAMS[""MBE_ORDER""]\n\t\tprint(""MBE order:"", self.mbe_order)\n\t\tself.center = center_\n\t\tself.properties[\'cutoff\'] = cutoff_\n\t\tself.properties[\'cutoff_width\'] = width_\n\t\tself.frag_list = []    # list of [{""atom"":.., ""charge"":..},{""atom"":.., ""charge"":..},{""atom"":.., ""charge"":..}]\n\t\tself.type_of_frags = []  # store the type of frag (1st order) in the self.mbe_frags:  [1,1,1 (H2O), 2,2,2(Na),3,3,3(Cl)]\n\t\tself.type_of_frags_dict = {}\n\t\tself.atoms_of_frags = [] # store the index of atoms of each frag\n\t\tself.mbe_frags=dict()    # list of  frag of each order N, dic[\'N\'=list of frags]\n\t\tself.mbe_frags_deri=dict()\n\t\tself.type_of_frags_dict = {}\n\t\tself.mbe_frags_energy=dict()  # MBE energy of each order N, dic[\'N\'= E_N]\n\t\tself.mbe_energy=dict()   # sum of MBE energy up to order N, dic[\'N\'=E_sum]\n\t\tself.frag_energy_sum = dict() # sum of the energis of all the frags in certain oder\n\t\tself.mbe_force =dict()\n\t\tself.nn_force = None\n\t\tself.frag_force_sum = dict()\n\t\tself.frag_dipole_sum = dict()\n\t\tself.mbe_dipole=dict()\n\t\tself.nn_dipole = 0\n\t\tself.nn_energy = 0.0\n\t\tself.frag_charge_sum = dict()\n\t\tself.mbe_charge = dict()\n\t\tself.nn_charge = None\n\t\treturn\n\n\tdef Reset_Frags(self):\n\t\t#self.mbe_frags = {}\n\t\tself.mbe_frags_deri=dict()\n\t\tself.mbe_frags_energy=dict()  # MBE energy of each order N, dic[\'N\'= E_N]\n\t\tself.energy=None\n\t\tself.mbe_energy=dict()   # sum of MBE energy up to order N, dic[\'N\'=E_sum]\n\t\tself.frag_energy_sum = dict()\n\t\tself.mbe_deri =None\n\t\tself.nn_energy=None\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tfor mol_frag in self.mbe_frags[order]:\n\t\t\t\t#print ""old: "", mol_frag.coords\n\t\t\t\tmol_frag.coords = self.coords[mol_frag.properties[""mbe_atom_index""]]\n\t\t\t\t#print ""new:"", mol_frag.coords\n\t\t#self.mbe_frags = {}\n\t\treturn\n\n\n\tdef Generate_All_MBE_term_General(self, frag_list=[]):\n\t\tself.frag_list = frag_list\n\t\tfor i in range (1, self.mbe_order+1):\n\t\t\tprint(""Generating order"", i)\n\t\t\tself.Generate_MBE_term_General(i)\n\t\treturn\n\n\tdef Generate_MBE_term_General(self, order):\n\t\tif order in self.mbe_frags.keys():\n\t\t\treturn\n\t\tif order==1:\n\t\t\tself.mbe_frags[order] = []\n\t\t\tmasked=[]\n\t\t\tfrag_index = 0\n\t\t\tfor i, dic in enumerate(self.frag_list):\n\t\t\t\tself.type_of_frags_dict[i] = []\n\t\t\t\tfrag_atoms = String_To_Atoms(dic[""atom""])\n\t\t\t\tfrag_atoms = [atoi[atom] for atom in frag_atoms]\n\t\t\t\tnum_frag_atoms = len(frag_atoms)\n\t\t\t\tj = 0\n\t\t\t\twhile (j < self.NAtoms()):\n\t\t\t\t\tif j in masked:\n\t\t\t\t\t\tj += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\ttmp_list = list(self.atoms[j:j+num_frag_atoms])\n\t\t\t\t\tif tmp_list == frag_atoms:\n\t\t\t\t\t\tself.atoms_of_frags.append([])\n\t\t\t\t\t\tmasked += range (j, j+num_frag_atoms)\n\t\t\t\t\t\tself.atoms_of_frags[-1]=range (j, j+num_frag_atoms)\n\t\t\t\t\t\tself.type_of_frags.append(i)\n\t\t\t\t\t\tself.type_of_frags_dict[i].append(frag_index)\n\t\t\t\t\t\ttmp_coord = self.coords[j:j+num_frag_atoms,:].copy()\n\t\t\t\t\t\ttmp_atom  = self.atoms[j:j+num_frag_atoms].copy()\n\t\t\t\t\t\tmbe_terms = [frag_index]\n\t\t\t\t\t\ttmp_mol = Mol(tmp_atom, tmp_coord)\n\t\t\t\t\t\ttmp_mol.properties[""mbe_atom_index""] = range(j, j+num_frag_atoms)\n\t\t\t\t\t\tif self.center == ""Heaviest"": # take the first heaviest one\n\t\t\t\t\t\t\ttmp_mol.properties[""center_atom""] = np.where(tmp_mol.atoms == max(tmp_mol.atoms))[0][0]\n\t\t\t\t\t\t\ttmp_mol.properties[""center""] = tmp_mol.coords[np.where(tmp_mol.atoms == max(tmp_mol.atoms))[0][0]]\n\t\t\t\t\t\telif self.center == ""COM"":\n\t\t\t\t\t\t\ttmp_mol.properties[""center""] = tmp_mol.Center(""Mass"")\n\t\t\t\t\t\telif self.center == ""COP"":\n\t\t\t\t\t\t\ttmp_mol.properties[""center""] = tmp_mol.Center(""Atom"")\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tprint(""This type of center is not implemented yet, set to COM as center"")\n\t\t\t\t\t\t\ttmp_mol.properties[""center""] = tmp_mol.Center(""Mass"")\n\t\t\t\t\t\tself.mbe_frags[order].append(tmp_mol)\n\t\t\t\t\t\tj += num_frag_atoms\n\t\t\t\t\t\tfrag_index += 1\n\t\t\t\t\t\t#print self.atoms_of_frags, tmp_list, self.type_of_frags\n\t\t\t\t\t\t#print self.mbe_frags[order][-1].atoms, self.mbe_frags[order][-1].coords\n\t\t\t\t\telse:\n\t\t\t\t\t\tj += 1\n\t\telse:\n\t\t\tself.mbe_frags[order] = []\n\t\t\tmbe_terms=[]\n\t\t\ttime_log = time.time()\n\t\t\ttime_now=time.time()\n\t\t\tfrag_case = 0\n\t\t\tsample_index = range(len(self.mbe_frags[1]))\n\t\t\ttmp_time  = time.time()\n\t\t\tsub_combinations = list(itertools.combinations(sample_index, order))\n\t\t\tfor i in range (0, len(sub_combinations)):\n\t\t\t\tterm = list(sub_combinations[i])\n\t\t\t\tif len(list(set(term))) < len(term):\n\t\t\t\t\tcontinue\n\t\t\t\tmbe_terms.append(term)\n\t\t\t\tfrag_case  += 1\n\t\t\tfor i in range (0, len(mbe_terms)):\n\t\t\t\tatom_group = []\n\t\t\t\tfor index in mbe_terms[i]:\n\t\t\t\t\tatom_group.append(self.mbe_frags[1][index].atoms.shape[0])\n\t\t\t\ttmp_coord = np.zeros((sum(atom_group), 3))\n\t\t\t\ttmp_atom = np.zeros(sum(atom_group), dtype=np.uint8)\n\t\t\t\tpointer = 0\n\t\t\t\tmbe_atom_index = []\n\t\t\t\tfrag_mono_center = []\n\t\t\t\tnatom_each_mono = []\n\t\t\t\tfor j, index in enumerate(mbe_terms[i]):\n\t\t\t\t\ttmp_coord[pointer:pointer+atom_group[j],:] = self.mbe_frags[1][index].coords\n\t\t\t\t\ttmp_atom[pointer:pointer+atom_group[j]] = self.mbe_frags[1][index].atoms\n\t\t\t\t\tmbe_atom_index += self.mbe_frags[1][index].properties[""mbe_atom_index""]\n\t\t\t\t\tnatom_each_mono.append(len(self.mbe_frags[1][index].properties[""mbe_atom_index""]))\n\t\t\t\t\tfrag_mono_center.append(self.mbe_frags[1][index].properties[""center""])\n\t\t\t\t\tpointer += atom_group[j]\n\t\t\t\ttmp_mol = Mol(tmp_atom, tmp_coord)\n\t\t\t\ttmp_mol.properties[""mbe_atom_index""] = mbe_atom_index\n\t\t\t\ttmp_mol.properties[""mono_index""] = mbe_terms[i]\n\t\t\t\ttmp_mol.properties[""natom_each_mono""] = natom_each_mono\n\t\t\t\ttmp_mol.properties[""center""] = frag_mono_center\n\t\t\t\t#print ""tmp_coords: "", tmp_mol.coords\n\t\t\t\tself.mbe_frags[order].append(tmp_mol)\n\t\t\tdel sub_combinations\n\t\treturn\n\n\tdef MBE_Energy(self):\n\t\tmono_num = len(self.mbe_frags[1])\n\t\tself.nn_energy = 0.0\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tself.mbe_energy[order] = self.frag_energy_sum[order]\n\t\t\tif order == 1:\n\t\t\t\tself.nn_energy += self.mbe_energy[order]\n\t\t\t\tcontinue\n\t\t\tfor sub_order in range (1, order):\n\t\t\t\tself.mbe_energy[order] -= nCr(mono_num-sub_order, order-sub_order)*self.mbe_energy[sub_order]\n\t\t\tprint(""order: "", order, self.mbe_energy[order])\n\t\t\tself.nn_energy += self.mbe_energy[order]\n\t\tprint(self.mbe_energy, self.nn_energy)\n\t\treturn\n\n\tdef MBE_Energy_Embed(self):\n\t\tself.properties[""mbe_energy_embed""] = dict()\n\t\tmono_num = len(self.mbe_frags[1])\n\t\tself.nn_energy = 0.0\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tself.mbe_energy[order] = self.frag_energy_sum[order]\n\t\t\tself.properties[""mbe_energy_embed""][order] = self.mbe_energy[order]\n\t\t\tif order == 1:\n\t\t\t\tself.nn_energy += self.properties[""mbe_energy_embed""][order]\n\t\t\t\tcontinue\n\t\t\tif order == 2:\n\t\t\t\tt = time.time()\n\t\t\t\tself.properties[""mbe_energy_embed""][order] = 0.0\n\t\t\t\tself.mbe_energy[order] = 0.0\n\t\t\t\tcc_2b_sum = 0.0\n\t\t\t\tnn_2b_sum = 0.0\n\t\t\t\tavg_2b_sum = 0.0\n\t\t\t\t#replu_2b_sum = 0.0\n\t\t\t\tfor mol_frag in self.mbe_frags[order]:\n\t\t\t\t\tmol_frag.DistMatrix = MolEmb.Make_DistMat(mol_frag.coords)  # update the distance matrix\n\t\t\t\t\tdist =  (sum(np.square(mol_frag.properties[""center""][0] - mol_frag.properties[""center""][1])))**0.5\n\t\t\t\t\tnn_2b = mol_frag.properties[""nn_energy""] - self.mbe_frags[1][mol_frag.properties[""mono_index""][0]].properties[""nn_energy""] -  self.mbe_frags[1][mol_frag.properties[""mono_index""][1]].properties[""nn_energy""]\n\t\t\t\t\tcc_2b = Dimer_ChargeCharge(mol_frag)\n\t\t\t\t\t#replu_2b = Dimer_Replusive(mol_frag)\n\t\t\t\t\tavg_2b = (1.0-math.tanh((dist - self.properties[""cutoff""])/self.properties[""cutoff_width""]))/2.0*nn_2b + (1.0+math.tanh((dist - self.properties[""cutoff""])/self.properties[""cutoff_width""]))/2.0*cc_2b\n\t\t\t\t\t#avg_2b = (1.0-math.tanh((dist - self.properties[""cutoff""])/self.properties[""cutoff_width""]))/2.0*nn_2b + (1.0+math.tanh((dist - self.properties[""cutoff""])/self.properties[""cutoff_width""]))/2.0*cc_2b + replu_2b\n\t\t\t\t\tself.mbe_energy[order] += nn_2b\n\t\t\t\t\tmol_frag.properties[""nn_2b""] = nn_2b\n\t\t\t\t\tmol_frag.properties[""cc_2b""] = cc_2b\n\t\t\t\t\tself.properties[""mbe_energy_embed""][order] += avg_2b\n\t\t\t\t\tcc_2b_sum += cc_2b\n\t\t\t\t\tnn_2b_sum += nn_2b\n\t\t\t\t\tavg_2b_sum  += avg_2b\n\t\t\t\t\t#replu_2b_sum  += replu_2b\n\t\t\t\tself.nn_energy += self.properties[""mbe_energy_embed""][order]\n\t\t\t\t#print ""order 2 energy cost:"", time.time() -t\n\t\t\t\tcontinue\n\t\t\tfor sub_order in range (1, order):\n\t\t\t\tself.mbe_energy[order] -= nCr(mono_num-sub_order, order-sub_order)*self.mbe_energy[sub_order]\n\t\t\tself.properties[""mbe_energy_embed""][order]  = self.mbe_energy[order]\n\t\t\tself.nn_energy += self.properties[""mbe_energy_embed""][order]\n\t\tprint(self.properties[""mbe_energy_embed""], self.nn_energy)\n\t\treturn\n\n\tdef MBE_Dipole(self):\n\t\tmono_num = len(self.mbe_frags[1])\n\t\tself.nn_dipole = 0.0\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tself.mbe_dipole[order] = self.frag_dipole_sum[order]\n\t\t\tif order == 1:\n\t\t\t\tself.nn_dipole += self.mbe_dipole[order]\n\t\t\t\tcontinue\n\t\t\tfor sub_order in range (1, order):\n\t\t\t\tself.mbe_dipole[order] -= nCr(mono_num-sub_order, order-sub_order)*self.mbe_dipole[sub_order]\n\t\t\tself.nn_dipole += self.mbe_dipole[order]\n\t\tprint(self.mbe_dipole, self.nn_dipole)\n\t\treturn\n\n\tdef MBE_Force(self):\n\t\tmono_num = len(self.mbe_frags[1])\n\t\tself.nn_force = np.zeros((self.NAtoms(), 3))\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tself.mbe_force[order] = self.frag_force_sum[order]\n\t\t\tif order == 1:\n\t\t\t\tself.nn_force += self.mbe_force[order]\n\t\t\t\tcontinue\n\t\t\tfor sub_order in range (1, order):\n\t\t\t\tself.mbe_force[order] -= nCr(mono_num-sub_order, order-sub_order)*self.mbe_force[sub_order]\n\t\t\tself.nn_force += self.mbe_force[order]\n\t\tself.properties[""mbe_deri""] = -self.nn_force/JOULEPERHARTREE\n\t\t#print self.mbe_force, self.nn_force\n\t\treturn\n\n\tdef MBE_Force_Embed(self):\n\t\tself.properties[""mbe_energy_embed_force""] = dict()\n\t\tmono_num = len(self.mbe_frags[1])\n\t\tself.nn_force = np.zeros((self.NAtoms(), 3))\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tself.mbe_force[order] = self.frag_force_sum[order]\n\t\t\tif order == 1:\n\t\t\t\tself.properties[""mbe_energy_embed_force""][order] = self.mbe_force[order]\n\t\t\t\tself.nn_force += self.properties[""mbe_energy_embed_force""][order]\n\t\t\t\tcontinue\n\t\t\tif order == 2:\n\t\t\t\tt = time.time()\n\t\t\t\tcc_2b_grad_sum = np.zeros((self.NAtoms(), 3))\n\t\t\t\tnn_2b_grad_sum = np.zeros((self.NAtoms(), 3))\n\t\t\t\tavg_2b_grad_sum = np.zeros((self.NAtoms(), 3))\n\t\t\t\t#replu_2b_grad_sum = np.zeros((self.NAtoms(), 3))\n\t\t\t\tfor mol_frag in self.mbe_frags[order]:\n\t\t\t\t\tdist =  (sum(np.square(mol_frag.properties[""center""][0] - mol_frag.properties[""center""][1])))**0.5\n\t\t\t\t\tA = math.tanh((dist - self.properties[""cutoff""])/self.properties[""cutoff_width""])/2.0\n\t\t\t\t\tcc_2b_grad = np.zeros((self.NAtoms(), 3))\n\t\t\t\t\tnn_2b_grad = np.zeros((self.NAtoms(), 3))\n\t\t\t\t\tavg_2b_grad = np.zeros((self.NAtoms(), 3))\n\t\t\t\t\tcutoff_2b_grad = np.zeros((self.NAtoms(), 3))\n\t\t\t\t\t#replu_2b_grad = np.zeros((self.NAtoms(), 3))\n\t\t\t\t\tcutoff_2b_grad[mol_frag.properties[""mbe_atom_index""]] = Dimer_Cutoff_Grad(mol_frag, dist, self.properties[""cutoff""], self.properties[""cutoff_width""])\n\t\t\t\t\t#replu_2b_grad[mol_frag.properties[""mbe_atom_index""]] = Dimer_Replusive_Grad(mol_frag)\n\t\t\t\t\tcc_2b_grad[mol_frag.properties[""mbe_atom_index""]] = Dimer_ChargeCharge_Grad(mol_frag)\n\t\t\t\t\tmono_1_grads = self.mbe_frags[1][mol_frag.properties[""mono_index""][0]].properties[""nn_energy_grads""]\n\t\t\t\t\tmono_2_grads = self.mbe_frags[1][mol_frag.properties[""mono_index""][1]].properties[""nn_energy_grads""]\n\t\t\t\t\tnn_2b_grad[mol_frag.properties[""mbe_atom_index""]] = -(mol_frag.properties[""nn_energy_grads""] - np.lib.pad(mono_1_grads,((0, mol_frag.properties[""natom_each_mono""][1]),(0,0)),\'constant\', constant_values = (0)) - np.lib.pad(mono_2_grads,((mol_frag.properties[""natom_each_mono""][0], 0),(0,0)),\'constant\', constant_values = (0)))/JOULEPERHARTREE\n\t\t\t\t\tavg_2b_grad = (0.5 - A)*nn_2b_grad - cutoff_2b_grad*mol_frag.properties[""nn_2b""] + (0.5 + A)*cc_2b_grad + cutoff_2b_grad*mol_frag.properties[""cc_2b""]\n\t\t\t\t\t#avg_2b_grad = (0.5 - A)*nn_2b_grad - cutoff_2b_grad*mol_frag.properties[""nn_2b""] + (0.5 + A)*cc_2b_grad + cutoff_2b_grad*mol_frag.properties[""cc_2b""]+replu_2b_grad\n\t\t\t\t\tnn_2b_grad_sum += nn_2b_grad\n\t\t\t\t\tcc_2b_grad_sum += cc_2b_grad\n\t\t\t\t\tavg_2b_grad_sum += avg_2b_grad\n\t\t\t\t\t#replu_2b_grad_sum += replu_2b_grad\n\t\t\t\tself.mbe_force[order] = -nn_2b_grad_sum * JOULEPERHARTREE\n\t\t\t\tself.properties[""mbe_energy_embed_force""][order] = -avg_2b_grad_sum*JOULEPERHARTREE\n\t\t\t\tself.nn_force +=  self.properties[""mbe_energy_embed_force""][order]\n\t\t\t\tcontinue\n\t\t\tfor sub_order in range (1, order):\n\t\t\t\tself.mbe_force[order] -= nCr(mono_num-sub_order, order-sub_order)*self.mbe_force[sub_order]\n\t\t\tself.properties[""mbe_energy_embed_force""][order] = self.mbe_force[order]\n\t\t\tself.nn_force += self.properties[""mbe_energy_embed_force""][order]\n\t\t\tself.properties[""mbe_deri""] = -self.nn_force/JOULEPERHARTREE\n\t\treturn\n\n\tdef MBE_Charge(self):\n\t\tmono_num = len(self.mbe_frags[1])\n\t\tself.nn_charge = np.zeros(self.NAtoms())\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tself.mbe_charge[order] = self.frag_charge_sum[order]\n\t\t\tif order == 1:\n\t\t\t\tself.nn_charge += self.mbe_charge[order]\n\t\t\t\tcontinue\n\t\t\tfor sub_order in range (1, order):\n\t\t\t\tself.mbe_charge[order] -= nCr(mono_num-sub_order, order-sub_order)*self.mbe_charge[sub_order]\n\t\t\tself.nn_charge += self.mbe_charge[order]\n\t\t\t#if order == PARAMS[""Embedded_Charge_Order""]:\n\t\t\t#\tself.properties[\'embedded_charge\'] = np.copy(self.nn_charge)\n\t\t#print self.nn_charge, "" sum:"", np.sum(self.nn_charge)\n\t\treturn\n'"
TensorMol/Containers/MolGraph.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Util import *\nimport numpy as np\nimport random, math\nfrom .Mol import *\nfrom ..PhysicalData import *\n\ndef AtomName_From_List(atom_list):\n\tname = """"\n\tfor i in atom_list:\n\t\tname += atoi.keys()[atoi.values().index(i)]\n\treturn name\n\ndef Subset(A, B): # check whether B is subset of A\n\tchecked_index = []\n\tfound = 0\n\tfor value in B:\n\t\tfor i in range (0, len(A)):\n\t\t\tif value==A[i] and i not in checked_index:\n\t\t\t\tchecked_index.append(i)\n\t\t\t\tfound += 1\n\t\t\t\tbreak\n\tif found == len(B):\n\t\treturn True\n\telse:\n\t\treturn False\n\ndef Setdiff(A, B): # return the element of A that not included in B\n\tdiff = []\n\tchecked_index = []\n\tfor value in A:\n\t\tfound = 0\n\t\tfor i in range (0, len(B)):\n\t\t\tif value == B[i] and i not in checked_index:\n\t\t\t\tfound = 1\n\t\t\t\tchecked_index.append(i)\n\t\t\t\tbreak\n\t\tif found == 0:\n\t\t\tdiff.append(value)\n\treturn diff\n\nclass MolGraph(Mol):\n\tdef __init__(self, atoms_ =  np.zeros(1,dtype=np.uint8), coords_ = np.zeros(shape=(1,1),dtype=np.float), bond_length_thresh_ =  None):\n\t\t"""""" graph of a molecule """"""\n\t\tMol.__init__(self, atoms_, coords_)\n\t\t# self.name= self.name+""_graph""\n\t\tself.num_atom_connected = None # connected  atoms of each atom\n\t\tself.atom_nodes = None\n\t\tself.bonds = None  #{connection type, length, atom_index_1, atom_index_2}\n\t\tself.bond_type = None # define whether it is a single, double or triple bond\n\t\tself.bond_conju = None # whether a bond is in a conjugated system\n\t\tself.bond_index = None # the bond index between two atoms\n\t\tself.atom_index_in_mono = []\n\t\tself.Bonds_Between  = None\n\t\tself.H_Bonds_Between = None\n\t\tself.nx_mol_graph = None\n\t\tself.shortest_path = None\n\t\tif not bond_length_thresh_:\n\t\t\tself.bond_length_thresh = bond_length_thresh\n\t\tself.Make_Mol_Graph()\n\t\treturn\n\n\tdef NAtoms(self):\n\t\treturn self.atoms.shape[0]\n\n\tdef NBonds(self):\n\t\treturn self.bonds.shape[0]\n\n\tdef AtomTypes(self):\n\t\treturn np.unique(self.atoms)\n\n\tdef BondTypes(self):\n\t\treturn np.unique(self.bonds[:,0]).astype(int)\n\n\tdef Make_Mol_Graph(self):\n\t\tt = time.time()\n\t\tself.Make_AtomNodes()\n\t\tself.Connect_AtomNodes()\n\t\t#self.Make_Bonds()\n\t\treturn\n\n\tdef Find_Bond_Index(self):\n\t\t#print ""name"", self.name, ""\\n\\n\\nbonds"", self.bonds, "" bond_type:"", self.bond_type\n\t\tself.bond_index = dict()\n\t\tfor i in range (0, self.NBonds()):\n\t\t\tpair = [int(self.bonds[i][2]), int(self.bonds[i][3])]\n\t\t\tpair.sort()\n\t\t\tself.bond_index[LtoS(pair)] = i\n\t\treturn\n\n\tdef Make_AtomNodes(self):\n\t\tself.atom_nodes = []\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\tself.atom_nodes.append(AtomNode(self.atoms[i], i))\n\t\treturn\n\n\tdef Connect_AtomNodes(self):\n\t\tself.DistMatrix = MolEmb.Make_DistMat(self.coords)\n\t\tself.num_atom_connected = []\n\t\tassigned = []\n\t\tassigned_mono = np.zeros(self.NAtoms(), dtype=int)\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\t#print (""conecting nodes"", i)\n\t\t\tfor j in range (i+1, self.NAtoms()):\n\t\t\t\tdist = self.DistMatrix[i][j]\n\t\t\t\tatom_pair=[self.atoms[i], self.atoms[j]]\n\t\t\t\tatom_pair.sort()\n\t\t\t\tbond_name = AtomName_From_List(atom_pair)\n\t\t\t\tif dist <= self.bond_length_thresh[bond_name]:\n\t\t\t\t\tself.atom_nodes[i].Append(self.atom_nodes[j])\n\t\t\t\t\tself.atom_nodes[j].Append(self.atom_nodes[i])\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\tself.num_atom_connected.append(len(self.atom_nodes[i].connected_nodes))\n\t\treturn\n\n\tdef Group_Atoms(self):\n\t\tself.DistMatrix = MolEmb.Make_DistMat(self.coords)\n\t\tself.num_atom_connected = []\n\t\tassigned = []\n\t\tassigned_mono = np.zeros(self.NAtoms(), dtype=int)\n\t\tto_merge = []\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\tfor j in range (i+1, self.NAtoms()):\n\t\t\t\tdist = self.DistMatrix[i][j]\n\t\t\t\tif dist < 1.7:\n\t\t\t\t\tatom_pair=[self.atoms[i], self.atoms[j]]\n\t\t\t\t\tatom_pair.sort()\n\t\t\t\t\tbond_name = AtomName_From_List(atom_pair)\n\t\t\t\t\tif dist <= self.bond_length_thresh[bond_name]:\n\t\t\t\t\t\tif i not in assigned and j not in assigned:\n\t\t\t\t\t\t\tself.atom_index_in_mono.append([i,j])\n\t\t\t\t\t\t\tassigned += [i,j]\n\t\t\t\t\t\t\tassigned_mono[[i,j]]=len(self.atom_index_in_mono)-1\n\t\t\t\t\t\telif i in assigned and j in assigned:\n\t\t\t\t\t\t\tif assigned_mono[i] == assigned_mono[j]:\n\t\t\t\t\t\t\t\tpass\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tto_merge.append([assigned_mono[i],  assigned_mono[j]])\n\t\t\t\t\t\telif i in assigned and j not in assigned:\n\t\t\t\t\t\t\tassigned += [j]\n\t\t\t\t\t\t\tself.atom_index_in_mono[assigned_mono[i]].append(j)\n\t\t\t\t\t\t\tassigned_mono[j] = assigned_mono[i]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tassigned += [i]\n\t\t\t\t\t\t\tself.atom_index_in_mono[assigned_mono[j]].append(i)\n\t\t\t\t\t\t\tassigned_mono[i] = assigned_mono[j]\n\t\tl = to_merge\n\t\tout = []\n\t\twhile len(l)>0:\n\t\t\tfirst = l[0]\n\t\t\trest = l[1:]\n\t\t\tfirst = set(first)\n\t\t\tlf = -1\n\t\t\twhile len(first)>lf:\n\t\t\t\tlf = len(first)\n\t\t\t\trest2 = []\n\t\t\t\tfor r in rest:\n\t\t\t\t\tif len(first.intersection(set(r)))>0:\n\t\t\t\t\t\tfirst |= set(r)\n\t\t\t\t\telse:\n\t\t\t\t\t\trest2.append(r)\n\t\t\t\trest = rest2\n\t\t\tout.append(first)\n\t\t\tl = rest\n\t\ttmp1 = []\n\t\ttmp2 = []\n\t\tfor merge_index in out:\n\t\t\ttmp = []\n\t\t\tfor i in merge_index:\n\t\t\t\ttmp += self.atom_index_in_mono[i]\n\t\t\ttmp1.append(tmp)\n\t\tflat_out = [item for sublist in out for item in sublist]\n\t\tfor i in range(0, len(self.atom_index_in_mono)):\n\t\t\tif i not in flat_out:\n\t\t\t\ttmp2.append(self.atom_index_in_mono[i])\n\t\tself.atom_index_in_mono = tmp1 + tmp2\n\t\tprint (""self.atom_index_in_mono:"", self.atom_index_in_mono)\n\t\treturn\n\n\tdef Make_Bonds(self):\n\t\tself.bonds = []\n\t\tvisited_pairs = []\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\tfor node in self.atom_nodes[i].connected_nodes:\n\t\t\t\tj  = node.node_index\n\t\t\t\tpair_index =  [i, j]\n\t\t\t\tatom_pair=[self.atoms[i], self.atoms[j]]\n\t\t\t\tpair_index = [x for (y, x) in sorted(zip(atom_pair, pair_index))]\n\t\t\t\tif pair_index not in visited_pairs:\n\t\t\t\t\tvisited_pairs.append(pair_index)\n\t\t\t\t\tatom_pair.sort()\n\t\t\t\t\tbond_name = AtomName_From_List(atom_pair)\n\t\t\t\t\tbond_type = bond_index[bond_name]\n\t\t\t\t\tdist = self.DistMatrix[i][j]\n\t\t\t\t\tself.bonds.append(np.array([bond_type, dist, pair_index[0], pair_index[1]]))\n\t\tself.bonds = np.asarray(self.bonds)\n\t\tself.Find_Bond_Index()\n\t\treturn\n\n\tdef GetNextNode_DFS(self, visited_list, node_stack):\n\t\tnode = node_stack.pop()\n\t\tvisited_list.append(node.node_index)\n\t\tfor next_node in node.connected_nodes:\n\t\t\tif next_node.node_index not in visited_list and next_node not in node_stack:\n\t\t\t\tnode_stack.append(next_node)\n\t\treturn node, visited_list, node_stack\n\n\tdef Calculate_Bond_Type(self):\n\t\tself.bond_type = [0 for i in range (0, self.NBonds())]\n\t\tleft_atoms = range (0, self.NAtoms())\n\t\tleft_connections = list(self.num_atom_connected)\n\t\tleft_valance = [ atom_valance[at] for at in self.atoms ]\n\t\tbond_of_atom = [[] for i in  range (0, self.NAtoms())] # index of the bonds that the atom are connected\n\t\tfor i in range (0, self.NBonds()):\n\t\t\tbond_of_atom[int(self.bonds[i][2])].append(i)\n\t\t\tbond_of_atom[int(self.bonds[i][3])].append(i)\n\t\tflag = 1\n\t\twhile (flag == 1):  # finish the easy assigment\n\t\t\tflag  = self.Define_Easy_Bonds(bond_of_atom, left_connections, left_atoms, left_valance)\n\t\t\tif (flag == -1):\n\t\t\t\tprint(""error when define bond type.."")\n\t\t\t\tself.bond_type = [-1 for i in range (0, self.NBonds())]\n\t\t\t\treturn\n\t\tsave_bond_type = list(self.bond_type)\n\t\tif left_atoms: # begin try and error\n\t\t\ttry_index = bond_of_atom[left_atoms[0]][0]\n\t\t\tfor try_type in range (1, left_valance[left_atoms[0]] - left_connections[left_atoms[0]]+2):\n\t\t\t\tself.bond_type = list(save_bond_type)\n\t\t\t\timport copy\n\t\t\t\tcp_bond_of_atom = copy.deepcopy(bond_of_atom)\n\t\t\t\tcp_left_connections = list(left_connections)\n\t\t\t\tcp_left_atoms = list(left_atoms)\n\t\t\t\tcp_left_valance = list(left_valance)\n\t\t\t\tself.bond_type[try_index] = try_type\n\t\t\t\tcp_bond_of_atom[left_atoms[0]].pop(0)\n\t\t\t\tcp_left_connections[left_atoms[0]] -= 1\n\t\t\t\tcp_left_valance[left_atoms[0]] -= try_type\n\t\t\t\tother_at = (int(self.bonds[try_index][2]) if left_atoms[0] != int(self.bonds[try_index][2]) else int(self.bonds[try_index][3]))\n\t\t\t\tcp_bond_of_atom[other_at].pop(cp_bond_of_atom[other_at].index(try_index))\n\t\t\t\tcp_left_connections[other_at] -= 1\n\t\t\t\tcp_left_valance[other_at] -= try_type\n\t\t\t\tflag = 1\n\t\t\t\twhile(flag == 1):\n\t\t\t\t\tflag  = self.Define_Easy_Bonds(cp_bond_of_atom, cp_left_connections, cp_left_atoms, cp_left_valance, True)\n\t\t\t\tif not cp_left_atoms and flag == 0 :\n\t\t\t\t\tleft_atoms = []\n\t\t\t\t\tbreak\n\t\tif  left_atoms or flag != 0  :\n\t\t\tprint(""error when define bond type.."")\n\t\t\tself.bond_type = [-1 for i in range (0, self.NBonds())]\n\t\t\treturn\n\t\treturn\n\n\tdef Find_Frag(self, frag, ignored_ele=[1], frag_head=0, avail_atoms=None):   # ignore all the H for assigment\n\t\tif avail_atoms==None:\n\t\t\tavail_atoms = range(0, self.NAtoms())\n\t\tfrag_head_node = frag.atom_nodes[frag_head]\n\t\tfrag_node_stack = [frag_head_node]\n\t\tfrag_visited_list = []\n\t\tall_mol_visited_list = [[]]\n\t\twhile(frag_node_stack):   # if node stack is not empty\n\t\t\tcurrent_frag_node = frag_node_stack[-1]\n\t\t\tupdated_all_mol_visited_list = []\n\t\t\tfor mol_visited_list in all_mol_visited_list:\n\t\t\t\tpossible_node = []\n\t\t\t\tif mol_visited_list ==[]:\n\t\t\t\t\tpossible_node = [self.atom_nodes[i] for i in avail_atoms]\n\t\t\t\t\tfor mol_node in possible_node:\n\t\t\t\t\t\tif mol_node.node_index not in mol_visited_list and self.Compare_Node(mol_node, current_frag_node) and self.Check_Connection(mol_node, current_frag_node, mol_visited_list, frag_visited_list):\n\t\t\t\t\t\t\tupdated_all_mol_visited_list.append(mol_visited_list+[mol_node.node_index])\n\t\t\t\t\t\t\tif mol_node.node_type in ignored_ele:# just once\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tconnected_node_index_in_frag = []\n\t\t\t\t\tfor connected_node_in_frag in current_frag_node.connected_nodes:\n\t\t\t\t\t\tif connected_node_in_frag.node_index in frag_visited_list:\n\t\t\t\t\t\t\tconnected_node_index_in_frag.append(frag_visited_list.index(connected_node_in_frag.node_index))\n\t\t\t\t\tfor connected_node_index in connected_node_index_in_frag:\n\t\t\t\t\t\tconnected_node_in_mol = self.atom_nodes[mol_visited_list[connected_node_index]]\n\t\t\t\t\t\tfor target_node in connected_node_in_mol.connected_nodes:\n\t\t\t\t\t\t\tif target_node.node_index not in mol_visited_list and self.Compare_Node(target_node, current_frag_node) and self.Check_Connection(target_node, current_frag_node, mol_visited_list, frag_visited_list) and target_node.node_index in avail_atoms:\n\t\t\t\t\t\t\t\tupdated_all_mol_visited_list.append(mol_visited_list+[target_node.node_index])\n\t\t\t\t\t\t\t\tif target_node.node_type in ignored_ele:\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\tall_mol_visited_list = list(updated_all_mol_visited_list)\n\t\t\tnext_frag_node, frag_visited_list, frag_node_stack  = self.GetNextNode_DFS(frag_visited_list, frag_node_stack)\n\t\tfrags_in_mol = []\n\t\talready_included = []\n\t\tfor mol_visited_list in all_mol_visited_list:\n\t\t\tmol_visited_list.sort()\n\t\t\tif mol_visited_list not in already_included:\n\t\t\t\talready_included.append(mol_visited_list)\n\t\t\t\tsorted_mol_visited_list = [x for (y, x) in sorted(zip(frag_visited_list,mol_visited_list))]## sort the index order of frags in mol to the same as the frag\n\t\t\t\tfrags_in_mol.append(sorted_mol_visited_list)\n\t\treturn frags_in_mol\n\n\tdef Check_Connection(self, mol_node, frag_node, mol_visited_list, frag_visited_list):  # the connection of mol_node should be the same as frag_node in the list we visited so far.\n\t\tmol_node_connection_index_found = []\n\t\tfor node in mol_node.connected_nodes:\n\t\t\tif node.node_index in mol_visited_list:\n\t\t\t\tmol_node_connection_index_found.append(mol_visited_list.index(node.node_index))\n\t\tfrag_node_connection_index_found = []\n\t\tfor node in frag_node.connected_nodes:\n\t\t\tif node.node_index in frag_visited_list:\n\t\t\t\tfrag_node_connection_index_found.append(frag_visited_list.index(node.node_index))\n\t\tif set(mol_node_connection_index_found) == set(frag_node_connection_index_found):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef Compare_Node(self, mol_node, frag_node):\n\t\tif mol_node.node_type == frag_node.node_type and mol_node.num_of_bonds == frag_node.num_of_bonds  and Subset(mol_node.connected_atoms, frag_node.connected_atoms):\n\t\t\tif frag_node.undefined_bond_type == ""heavy"": #  check whether the dangling bond is connected to H in the mol\n\t\t\t\tif 1 in Setdiff(mol_node.connected_atoms, frag_node.connected_atoms):   # the dangling bond is connected to H\n\t\t\t\t\treturn False\n\t\t\t\telse:\n\t\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef IsIsomer(self,other):\n\t\treturn np.array_equals(np.sort(self.atoms),np.sort(other.atoms))\n\n\tdef AtomName(self, i):\n\t\treturn atoi.keys()[atoi.values().index(self.atoms[i])]\n\n\tdef AllAtomNames(self):\n\t\tnames=[]\n\t\tfor i in range (0, self.atoms.shape[0]):\n\t\t\tnames.append(atoi.keys()[atoi.values().index(self.atoms[i])])\n\t\treturn names\n\nclass Frag_of_MolGraph(MolGraph):\n\tdef __init__(self, mol_, undefined_bonds_ = None, undefined_bond_type_ = ""any"", bond_length_thresh_ =  None):\n\t\tMol.__init__(self, mol_.atoms, mol_.coords)\n\t\tself.undefined_bond_type = undefined_bond_type_  # whether the dangling bond can be connected  to H or not\n\t\tself.undefined_bonds = undefined_bonds_  # capture the undefined bonds of each atom\n\t\tif bond_length_thresh_ ==  None:\n\t\t\tself.bond_length_thresh = bond_length_thresh\n\n\tdef FromXYZString(self,string, set_name = None):\n\t\tself.properties[""set_name""] = set_name\n\t\tlines = string.split(""\\n"")\n\t\tnatoms=int(lines[0])\n\t\tself.atoms.resize((natoms))\n\t\tself.coords.resize((natoms,3))\n\t\tfor i in range(natoms):\n\t\t\tline = lines[i+2].split()\n\t\t\tif len(line)==0:\n\t\t\t\treturn\n\t\t\tself.atoms[i]=AtomicNumber(line[0])\n\t\t\ttry:\n\t\t\t\tself.coords[i,0]=float(line[1])\n\t\t\texcept:\n\t\t\t\tself.coords[i,0]=scitodeci(line[1])\n\t\t\ttry:\n\t\t\t\tself.coords[i,1]=float(line[2])\n\t\t\texcept:\n\t\t\t\tself.coords[i,1]=scitodeci(line[2])\n\t\t\ttry:\n\t\t\t\tself.coords[i,2]=float(line[3])\n\t\t\texcept:\n\t\t\t\tself.coords[i,2]=scitodeci(line[3])\n\t\timport ast\n\t\ttry:\n\t\t\tself.undefined_bonds = ast.literal_eval(lines[1][lines[1].index(""{""):lines[1].index(""}"")+1])\n\t\t\tif ""type"" in self.undefined_bonds.keys():\n\t\t\t\tself.undefined_bond_type = self.undefined_bonds[""type""]\n\t\t\telse:\n\t\t\t\tself.undefined_bond_type = ""any""\n\t\texcept:\n\t\t\tself.name = lines[1] #debug\n\t\t\tself.undefined_bonds = {}\n\t\t\tself.undefined_bond_type = ""any""\n\t\treturn\n\n\tdef Make_AtomNodes(self):\n\t\tatom_nodes = []\n\t\tfor i in range (0, self.NAtoms()):\n\t\t\tif i in self.undefined_bonds.keys():\n\t\t\t\tatom_nodes.append(AtomNode(self.atoms[i], i,  self.undefined_bond_type, self.undefined_bonds[i]))\n\t\t\telse:\n\t\t\t\tatom_nodes.append(AtomNode(self.atoms[i], i, self.undefined_bond_type))\n\t\tself.atom_nodes = atom_nodes\n\t\treturn\n\nclass AtomNode:\n\t"""""" Treat each atom as a node for the purpose of building the molecule graph """"""\n\tdef __init__(self, node_type_=None, node_index_=None, undefined_bond_type_=""any"", undefined_bond_ = 0):\n\t\tself.node_type = node_type_\n\t\tself.node_index = node_index_\n\t\tself.connected_nodes = []\n\t\tself.undefined_bond = undefined_bond_\n\t\tself.undefined_bond_type = undefined_bond_type_\n\t\tself.num_of_bonds = None\n\t\tself.connected_atoms = None\n\t\tself.Update_Node()\n\t\treturn\n\n\tdef Append(self, node):\n\t\tself.connected_nodes.append(node)\n\t\tself.Update_Node()\n\t\treturn\n\n\tdef Num_of_Bonds(self):\n\t\tself.num_of_bonds = len(self.connected_nodes)+self.undefined_bond\n\t\treturn len(self.connected_nodes)+self.undefined_bond\n\n\tdef Connected_Atoms(self):\n\t\tconnected_atoms = []\n\t\tfor node in self.connected_nodes:\n\t\t\tconnected_atoms.append(node.node_type)\n\t\tself.connected_atoms = connected_atoms\n\t\treturn connected_atoms\n\n\tdef Update_Node(self):\n\t\tself.Num_of_Bonds()\n\t\tself.Connected_Atoms()\n\t\tself.connected_nodes = [x for (y, x) in sorted(zip(self.connected_atoms, self.connected_nodes))]\n\t\tself.connected_atoms.sort()\n\t\treturn\n'"
TensorMol/Containers/PickleTM.py,2,"b'from __future__ import absolute_import\nimport pickle,sys\nif sys.version_info > (3, 0):\n\timport copyreg\nelse:\n\timport copy_reg\n\nrenametable = {\n\t\'TensorMol.TensorMolData_EE\': \'TensorMol.TensorMolDataEE\',\n\t\'TensorMol.TFMolInstance_EE\': \'TensorMol.TFMolInstanceEE\',\n\t\'TensorMol.TensorMolData\': \'TensorMol.Containers.TensorMolData\',\n\t\'TensorMol.Mol\': \'TensorMol.Containers.Mol\',\n\t\'TensorMol.Sets\': \'TensorMol.Containers.Sets\',\n\t\'DigestMol\': \'TensorMol.Containers.DigestMol\',\n\t\'TensorMol.DigestMol\': \'TensorMol.Containers.DigestMol\',\n\t\'TensorMol.TFMolInstanceDirect\': \'TensorMol.TFNetworks.TFMolInstanceDirect\',\n\t\'TensorMol.Transformer\': \'TensorMol.ForceModifiers.Transformer\',\n\t\'TensorMolData_EE\': \'TensorMolDataEE\'\n\t}\n\ndef PickleMapName(name):\n\t""""""\n\tIf you change the name of a function or module, then pickle, you can fix it with this.\n\t""""""\n\tif name in renametable:\n\t\t#print(""REMAPPING PICKLE LOAD:"",name,""TO"",renametable[name])\n\t\treturn renametable[name]\n\t#else:\n\t#\tprint(""NOT REMAPPING PICKLE LOAD:"",name)\n\treturn name\n\ndef mapped_load_global(self):\n\tmodule = PickleMapName(self.readline()[:-1])\n\tname = PickleMapName(self.readline()[:-1])\n\tprint(""Finding "", module,name)\n\tklass = self.find_class(module, name)\n\tself.append(klass)\n\nclass MyUnpickler(pickle.Unpickler):\n\tdef find_class(self, module, name):\n\t\treturn pickle.Unpickler.find_class(self,PickleMapName(module),PickleMapName(name))\n\ndef UnPickleTM(file):\n\t""""""\n\tEventually we need to figure out how the mechanics of dispatch tables changed.\n\tSince we only use this as a hack anyways, I\'ll just comment out what changed\n\tbetween python2.7x and python3x.\n\t""""""\n\ttmp = None\n\tif sys.version_info[0] < 3:\n\t\tf = open(file,""rb"")\n\t\tunpickler = pickle.Unpickler(f)\n\t\tunpickler.dispatch[pickle.GLOBAL] = mapped_load_global\n\t\ttmp = unpickler.load()\n\t\tf.close()\n\telse:\n\t\tf = open(file,""rb"")\n\t\tunpickler = MyUnpickler(f,encoding=\'latin1\')\n\t\ttmp = unpickler.load()\n\t\tf.close()\n\ttmp.pop(\'evaluate\',None)\n\ttmp.pop(\'MolInstance_fc_sqdiff_BP\',None)\n\ttmp.pop(\'Eval_BPForceSingle\',None)\n\ttmp.pop(\'TFMolManage\',None)\n\ttmp.pop(\'TFManage\',None)\n\ttmp.pop(\'Prepare\',None)\n\ttmp.pop(\'load\',None)\n\ttmp.pop(\'Load\',None)\n\ttmp.pop(\'TensorMol.TFMolManage.path\',None)\n\ttmp.pop(\'TensorMol.TFMolManage.Load\',None)\n\ttmp.pop(\'TensorMol.TFMolManage.Prepare\',None)\n\ttmp.pop(\'TensorMol.TFInstance\',None)\n\ttmp.pop(\'TensorMol.TFInstance.train_dir\',None)\n\ttmp.pop(\'TensorMol.TFMolInstance.train_dir\',None)\n\ttmp.pop(\'TensorMol.TFInstance.chk_file\',None)\n\ttmp.pop(\'TensorMol.TFMolInstance.chk_file\',None)\n\ttmp.pop(\'save\',None)\n\ttmp.pop(\'Save\',None)\n\ttmp.pop(\'Trainable\',None)\n\ttmp.pop(\'TFMolManage.Trainable\',None)\n\ttmp.pop(\'__init__\',None)\n\treturn tmp\n'"
TensorMol/Containers/Sets.py,3,"b'#\n# A molecule set is not a training set.\n#\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .Mol import *\nfrom .MolGraph import *\nfrom ..Util import *\nfrom .MolFrag import *\nimport numpy as np\nimport os,sys,re,copy,time\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\n\nclass MSet:\n\t"""""" A molecular database which\n\t\tprovides structures """"""\n\tdef __init__(self, name_ =""gdb9"", path_=""./datasets/"", center_=True):\n\t\tself.mols=[]\n\t\tself.path=path_\n\t\tself.name=name_\n\t\tself.suffix="".pdb"" #Pickle Database? Poor choice.\n\t\tself.center=center_\n\n\tdef Save(self, filename=None):\n\t\tfor mol in self.mols:\n\t\t\tmol.Clean()\n\t\tif filename == None:\n\t\t\tfilename = self.name\n\t\tLOGGER.info(""Saving set to: %s "", self.path+filename+self.suffix)\n\t\t#print ""Saving set to: "", self.path+self.name+self.suffix\n\t\tf=open(self.path+filename+self.suffix,""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\n\tdef Load(self, filename=None):\n\t\tif filename == None:\n\t\t\tfilename = self.name\n\t\tfrom ..Containers.PickleTM import UnPickleTM as UnPickleTM\n\t\ttmp = UnPickleTM(self.path+filename+self.suffix)\n\t\tself.__dict__.update(tmp)\n\t\tLOGGER.info(""Loaded, ""+str(len(self.mols))+"" molecules ""+str(self.NAtoms())+"" Atoms total ""+str(self.AtomTypes())+"" Types "")\n\t\treturn\n\n\tdef DistortAlongNormals(self, npts=8, random=True, disp=.2):\n\t\t\'\'\'\n\t\tCreate a distorted copy of a set\n\n\t\tArgs:\n\t\t\tnpts: the number of points to sample along the normal mode coordinate.\n\t\t\trandom: whether to randomize the order of the new set.\n\t\t\tdisp: the maximum displacement of atoms along the mode\n\n\t\tReturns:\n\t\t\tA set containing distorted versions of the original set.\n\t\t\'\'\'\n\t\tprint(""Making distorted clone of:"", self.name)\n\t\ts = MSet(self.name+""_NEQ"")\n\t\tord = range(len(self.mols))\n\t\tif(random):\n\t\t\tnp.random.seed(int(time.time()))\n\t\t\tord=np.random.permutation(len(self.mols))\n\t\tfor j in ord:\n\t\t\tnewcoords = self.mols[j].ScanNormalModes(npts,disp)\n\t\t\tfor i in range(newcoords.shape[0]): # Loop modes\n\t\t\t\tfor k in range(newcoords.shape[1]): # loop points\n\t\t\t\t\ts.mols.append(Mol(self.mols[j].atoms,newcoords[i,k,:,:]))\n\t\t\t\t\ts.mols[-1].DistMatrix = self.mols[j].DistMatrix\n\t\treturn s\n\n\tdef RotatedClone(self, NRots=3):\n\t\t""""""\n\t\tRotate every molecule NRots Times.\n\t\tWe should toss some reflections in the mix too...\n\t\t""""""\n\t\tprint(""Making Rotated clone of:"", self.name)\n\t\ts = MSet(self.name)\n\t\tord = range(len(self.mols))\n\t\tif(random):\n\t\t\tnp.random.seed(int(time.time()))\n\t\t\tord=np.random.permutation(len(self.mols))\n\t\tfor j in ord:\n\t\t\tfor i in range (0, NRots):\n\t\t\t\ts.mols.append(copy.deepcopy(self.mols[j]))\n\t\t\t\ts.mols[-1].coords -= s.mols[-1].Center()\n\t\t\t\ts.mols[-1].RotateRandomUniform()\n\t\treturn s\n\n\tdef DistortedClone(self, NDistorts=1, random=True):\n\t\t\t\'\'\' Create a distorted copy of a set\'\'\'\n\t\t\tprint(""Making distorted clone of:"", self.name)\n\t\t\ts = MSet(self.name+""_NEQ"")\n\t\t\tord = range(len(self.mols))\n\t\t\tif(random):\n\t\t\t\tnp.random.seed(int(time.time()))\n\t\t\t\tord=np.random.permutation(len(self.mols))\n\t\t\tfor j in ord:\n\t\t\t\tfor i in range (0, NDistorts):\n\t\t\t\t\ts.mols.append(copy.deepcopy(self.mols[j]))\n\t\t\t\t\ts.mols[-1].Distort()\n\t\t\treturn s\n\n\tdef TransformedClone(self, transfs):\n\t\t\'\'\' make a linearly transformed copy of a set. \'\'\'\n\t\tLOGGER.info(""Making Transformed clone of:""+self.name)\n\t\ts = MSet(self.name)\n\t\tord = range(len(self.mols))\n\t\tfor j in ord:\n\t\t\tfor k in range(len(transfs)):\n\t\t\t\ts.mols.append(copy.deepcopy(self.mols[j]))\n\t\t\t\ts.mols[-1].Transform(transfs[k])\n\t\treturn s\n\n\tdef CenterSet(self):\n\t\t""""""\n\t\tTranslates every Mol such that the center is at 0.\n\t\t""""""\n\t\tfor mol in self.mols:\n\t\t\tmol.coords -= mol.Center()\n\n\tdef cut_max_num_atoms(self, max_n_atoms):\n\t\tcut_down_mols = []\n\t\tfor mol in self.mols:\n\t\t\tif mol.atoms.shape[0] <= max_n_atoms:\n\t\t\t\tcut_down_mols.append(mol)\n\t\tself.mols = cut_down_mols\n\n\tdef NAtoms(self):\n\t\tnat=0\n\t\tfor m in self.mols:\n\t\t\tnat += m.NAtoms()\n\t\treturn nat\n\n\tdef MaxNAtoms(self):\n\t\treturn np.max([m.NAtoms() for m in self.mols])\n\n\tdef AtomTypes(self):\n\t\ttypes = np.array([],dtype=np.uint8)\n\t\tfor m in self.mols:\n\t\t\ttypes = np.union1d(types,m.AtomTypes())\n\t\treturn types\n\n\tdef BondTypes(self):\n\t\treturn np.asarray([x for x in itertools.product(self.AtomTypes().tolist(), repeat=2)])\n\n\tdef read_xyz_set_with_properties(self, path, properties=[]):\n\t\t""""""\n\t\tReads xyz files from a directory with properties in the comment line and adds them to the\n\t\tset.mols list\n\n\t\tArgs:\n\t\t\tpath (string): The location of the xyz files to be read\n\t\t\tproperties (list of strings): A list of properties in the comment line of the file.\n\n\t\tNotes:\n\t\t\tProperties in the xyz file must be semi-colon delimited and in the same order as specified in\n\t\t\tthe properties variable.\n\n\t\t\tComponents of properties must be comma delimited and are expected in the same order as the atoms\n\t\t\twith ordering of x, y, z if necessary.\n\n\t\t\tCurrently only valid properties are ""name"", ""energy"", ""forces"", ""dipole"", and ""mulliken_charges"".\n\t\t\tOther properties to be added as needed.\n\t\t""""""\n\t\tfrom os import listdir\n\t\tfrom os.path import isfile, join\n\t\tonlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n\t\tfor file in onlyfiles:\n\t\t\tif ( file[-4:]!=\'.xyz\' ):\n\t\t\t\t\tcontinue\n\t\t\tself.mols.append(Mol())\n\t\t\tself.mols[-1].read_xyz_with_properties(path+file, properties, self.center)\n\t\treturn\n\n\tdef ReadXYZUnpacked(self, path=""/Users/johnparkhill/gdb9/"", has_energy=False, has_force=False, has_charge=False, has_mmff94=False):\n\t\t""""""\n\t\tReads XYZs in distinct files in one directory as a molset\n\t\tArgs:\n\t\t\tpath: the directory which contains the .xyz files to be read\n\t\t\thas_energy: switch to turn on reading the energy from the comment line as formatted from the md_dataset on quantum-machine.org\n\t\t\thas_force: switch to turn on reading the force from the comment line as formatted from the md_dataset on quantum-machine.org\n\t\t""""""\n\t\tfrom os import listdir\n\t\tfrom os.path import isfile, join\n\t\t#onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n\t\tonlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n\t\tfor file in onlyfiles:\n\t\t\tif ( file[-4:]!=\'.xyz\' ):\n\t\t\t\t\tcontinue\n\t\t\tself.mols.append(Mol())\n\t\t\tself.mols[-1].ReadGDB9(path+file, file)\n\t\t\tself.mols[-1].properties[""set_name""] = self.name\n\t\t\tif has_force:\n\t\t\t\tself.mols[-1].ForceFromXYZ(path+file)\n\t\t\tif has_energy:\n\t\t\t\tself.mols[-1].EnergyFromXYZ(path+file)\n\t\t\tif has_charge:\n\t\t\t\tself.mols[-1].ChargeFromXYZ(path+file)\n\t\t\tif has_mmff94:\n\t\t\t\tself.mols[-1].MMFF94FromXYZ(path+file)\n\t\tif (self.center):\n\t\t\tself.CenterSet()\n\t\treturn\n\n\tdef ReadXYZ(self,filename = None, xyz_type = \'mol\'):\n\t\t""""""\n\t\tReads XYZs concatenated into a single file separated by \\n\\n as a molset\n\t\t""""""\n\t\tif filename == None:\n\t\t\tfilename = self.name\n\t\tf = open(self.path+filename+"".xyz"",""r"")\n\t\ttxts = f.readlines()\n\t\tfor line in range(len(txts)):\n\t\t\tif (txts[line].strip() and\n\t\t\t\t\tall([x.isdigit() for x in txts[line].split()])):\n\t\t\t\tline0=line\n\t\t\t\tnlines=int(txts[line0])\n\t\t\t\tif xyz_type == \'mol\':\n\t\t\t\t\tself.mols.append(Mol())\n\t\t\t\telif xyz_type == \'frag_of_mol\':\n\t\t\t\t\tself.mols.append(Frag_of_Mol())\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Unknown Type!"")\n\t\t\t\tself.mols[-1].FromXYZString(\'\'.join(txts[line0:line0+nlines+2]))\n\t\t\t\tself.mols[-1].name = str(txts[line0+1])\n\t\t\t\tself.mols[-1].properties[""set_name""] = self.name\n\t\tif (self.center):\n\t\t\tself.CenterSet()\n\t\tLOGGER.debug(""Read ""+str(len(self.mols))+"" molecules from XYZ"")\n\t\treturn\n\n\tdef AppendFromDirectory(self, apath_):\n\t\t""""""\n\t\tAppend all xyz files in apath_ to this set.\n\t\t""""""\n\t\tfor file in os.listdir(apath_):\n\t\t\tif file.endswith("".xyz""):\n\t\t\t\tm = Mol()\n\t\t\t\tm.properties = {""from_file"":file}\n\t\t\t\tf = open(file,\'r\')\n\t\t\t\tfs = f.read()\n\t\t\t\tm.FromXYZString(fs)\n\t\t\t\tself.mols.append(m)\n\t\treturn\n\n\tdef WriteXYZ(self,filename=None):\n\t\tif filename == None:\n\t\t\tfilename = self.name\n\t\tfor mol in self.mols:\n\t\t\tmol.WriteXYZfile(self.path,filename)\n\t\tLOGGER.info(\'Wrote %s \', filename)\n\t\treturn\n\n\tdef pop(self, ntopop):\n\t\tfor i in range(ntopop):\n\t\t\tself.mols.pop()\n\t\treturn\n\n\tdef OnlyWithElements(self, allowed_eles):\n\t\t""""""\n\t\tRemoves molecules with unwanted atoms from a set.\n\t\t""""""\n\t\tmols=[]\n\t\tfor mol in self.mols:\n\t\t\tif set(list(mol.atoms)).issubset(allowed_eles):\n\t\t\t\tmols.append(mol)\n\t\tfor i in allowed_eles:\n\t\t\tself.name += ""_""+str(i)\n\t\tself.mols=mols\n\t\treturn\n\n\tdef OnlyAtoms(self,allowed_eles):\n\t\t""""""\n\t\tRemoves any unwanted atoms from the set.\n\t\t""""""\n\t\tfor mol in self.mols:\n\t\t\tincluded = []\n\t\t\tfor i,atom in enumerate(mol.atoms):\n\t\t\t\tif (atom in allowed_eles):\n\t\t\t\t\tincluded.append(i)\n\t\t\tmol.atoms = mol.atoms[included]\n\t\t\tmol.coords = mol.coords[included]\n\t\treturn\n\n\tdef AppendSet(self, b):\n\t\tif (self.name == None):\n\t\t\tself.name = self.name + b.name\n\t\tself.mols = self.mols+b.mols\n\t\treturn\n\n\tdef rms(self,other_):\n\t\tif (len(self.mols) != len(other_.mols)):\n\t\t\traise Exception(""Bad Comparison"")\n\t\trmss = [self.mols[i].rms_inv(other_.mols[i]) for i in range(len(self.mols))]\n\t\treturn rmss\n\n\tdef Statistics(self):\n\t\t"""""" Return some energy information about the samples we have... """"""\n\t\tprint(""Set Statistics----"")\n\t\tprint(""Nmol: "", len(self.mols))\n\t\tsampfrac = 0.1;\n\t\tnp.random.seed(int(time.time()))\n\t\tord=np.random.permutation(int(len(self.mols)*sampfrac))\n\t\tif len(ord)==0:\n\t\t\treturn\n\t\tens = np.zeros(len(ord))\n\t\trmsd = np.zeros(len(ord))\n\t\tn=0\n\t\tfor j in ord:\n\t\t\tif (""energy"" in self.mols[j].properties.keys()):\n\t\t\t\tens[n] = self.mols[j].properties[""energy""]\n\t\t\telse :\n\t\t\t\tens[n] = self.mols[j].GoEnergy(self.mols[j].coords.flatten())\n\t\t\t\ttmp = MolEmb.Make_DistMat(self.mols[j].coords) - self.mols[j].DistMatrix\n\t\t\t\trmsd[n] = np.sum(tmp*tmp)/len(self.mols[j].coords)\n\t\t\t\tn=n+1\n\t\tprint(""Mean and Std. Energy"", np.average(ens), np.std(ens))\n\t\tprint(""Energy Histogram"", np.histogram(ens, 100))\n\t\tprint(""RMSD Histogram"", np.histogram(rmsd, 100))\n\t\treturn\n\n\tdef Clean_GDB9(self):\n\t\ts = MSet(self.name+""_cleaned"")\n\t\ts.path = self.path\n\t\tfor mol in self.mols:\n\t\t\tif float(\'inf\') in mol.Bonds_Between:\n\t\t\t\tprint(""disconnected atoms in mol.. discard"")\n\t\t\telif -1 in mol.bond_type or 0 in mol.bond_type:\n\t\t\t\tprint(""allowed bond type in mol... discard"")\n\t\t\telse:\n\t\t\t\ts.mols.append(mol)\n\t\treturn s\n\n\tdef Calculate_vdw(self):\n\t\tfor mol in self.mols:\n\t\t\tmol.Calculate_vdw()\n\t\t\tprint(""atomization:"", mol.atomization, "" vdw:"", mol.vdw)\n\t\treturn\n\n\tdef WriteSmiles(self):\n\t\tfor mol in self.mols:\n\t\t\tmol.WriteSmiles()\n\t\treturn\n\n\tdef MakeBonds(self):\n\t\tself.NBonds = 0\n\t\tfor m in self.mols:\n\t\t\tself.NBonds += m.MakeBonds()\n\t\tself.BondTypes = np.unique(np.concatenate([m.bondtypes for m in self.mols],axis=0),axis=0)\n\n\n\n\nclass FragableMSet(MSet):\n\tdef __init__(self, name_ =""NaClH2O"", path_=""./datasets/""):\n\t\tMSet.__init__(self, name_, path_)\n\t\treturn\n\n\tdef ReadGDB9Unpacked(self, path=""/Users/johnparkhill/gdb9/""):\n\t\t"""""" Reads the GDB9 dataset as a pickled list of molecules""""""\n\t\tfrom os import listdir\n\t\tfrom os.path import isfile, join\n\t\t#onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n\t\tonlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n\t\tfor file in onlyfiles:\n\t\t\tif ( file[-4:]!=\'.xyz\' ):\n\t\t\t\tcontinue\n\t\t\tself.mols.append(FragableCluster())\n\t\t\tself.mols[-1].ReadGDB9(path+file, file)\n\t\t\tself.mols[-1].properties[""set_name""] = self.name\n\t\t\tself\n\t\treturn\n\n\tdef ReadXYZ(self,filename, xyz_type = \'mol\'):\n\t\t"""""" Reads XYZs concatenated into a single separated by \\n\\n file as a molset """"""\n\t\tf = open(self.path+filename+"".xyz"",""r"")\n\t\ttxts = f.readlines()\n\t\tfor line in range(len(txts)):\n\t\t\tif (txts[line].count(\'Comment:\')>0):\n\t\t\t\tline0=line-1\n\t\t\t\tnlines=int(txts[line0])\n\t\t\t\tif xyz_type == \'mol\':\n\t\t\t\t\tself.mols.append(FragableCluster())\n\t\t\t\telif xyz_type == \'frag_of_mol\':\n\t\t\t\t\tself.mols.append(Frag_of_Mol())\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Unknown Type!"")\n\t\t\t\tself.mols[-1].FromXYZString(\'\'.join(txts[line0:line0+nlines+2]))\n\t\t\t\tself.mols[-1].name = str(line)\n\t\t\t\tself.mols[-1].properties[""set_name""] = self.name\n\t\treturn\n\n\tdef MBE(self,  atom_group=1, cutoff=10, center_atom=0):\n\t\tfor mol in self.mols:\n\t\t\tmol.MBE(atom_group, cutoff, center_atom)\n\t\treturn\n\n\tdef PySCF_Energy(self):\n\t\tfor mol in self.mols:\n\t\t\tmol.properties[""energy""] = PySCFMP2Energy(mol)\n\t\treturn\n\n\tdef Generate_All_MBE_term(self,  atom_group=1, cutoff=10, center_atom=0, max_case = 1000000):\n\t\tfor mol in self.mols:\n\t\t\tmol.Generate_All_MBE_term(atom_group, cutoff, center_atom, max_case)\n\t\treturn\n\n\tdef Generate_All_MBE_term_General(self, frag_list=[], cutoff=10, center_atom=0):\n\t\tfor mol in self.mols:\n\t\t\tmol.Generate_All_MBE_term_General(frag_list, cutoff, center_atom)\n\t\treturn\n\n\tdef Calculate_All_Frag_Energy(self, method=""pyscf""):\n\t\tfor mol in self.mols:\n\t\t\tmol.Calculate_All_Frag_Energy(method)\n\t\t\t# \tmol.Set_MBE_Energy()\n\t\treturn\n\n\tdef Calculate_All_Frag_Energy_General(self, method=""pyscf""):\n\t\tfor mol in self.mols:\n\t\t\t#print mol.properties\n\t\t\t#print ""Mol set_name"", mol.properties[""set_name""]\n\t\t\tmol.Calculate_All_Frag_Energy_General(method)\n\t\t\t#        mol.Set_MBE_Energy()\n\t\treturn\n\n\tdef Get_All_Qchem_Frag_Energy(self):\n\t\tfor mol in self.mols:\n\t\t\tmol.Get_All_Qchem_Frag_Energy()\n\t\treturn\n\n\tdef Get_All_Qchem_Frag_Energy_General(self):\n\t\tfor mol in self.mols:\n\t\t\tmol.Get_All_Qchem_Frag_Energy_General()\n\t\treturn\n\n\tdef Generate_All_Pairs(self, pair_list=[]):\n\t\tfor mol in self.mols:\n\t\t\tmol.Generate_All_Pairs(pair_list)\n\t\treturn\n\n\tdef Get_Permute_Frags(self, indis=[0]):\n\t\tfor mol in self.mols:\n\t\t\tmol.Get_Permute_Frags(indis)\n\t\treturn\n\n\tdef Set_Qchem_Data_Path(self):\n\t\tfor mol in self.mols:\n\t\t\tmol.Set_Qchem_Data_Path()\n\t\treturn\n\n\n\nclass FragableMSetBF(FragableMSet):\n\tdef __init__(self, name_ =""NaClH2O"", path_=""./datasets/""):\n\t\tMSet.__init__(self, name_, path_)\n\t\treturn\n\n\tdef ReadXYZ(self,filename, xyz_type = \'mol\'):\n\t\t"""""" Reads XYZs concatenated into a single separated by \\n\\n file as a molset """"""\n\t\tf = open(self.path+filename+"".xyz"",""r"")\n\t\ttxts = f.readlines()\n\t\tfor line in range(len(txts)):\n\t\t\tif (txts[line].count(\'Comment:\')>0):\n\t\t\t\tline0=line-1\n\t\t\t\tnlines=int(txts[line0])\n\t\t\t\tif xyz_type == \'mol\':\n\t\t\t\t\tself.mols.append(FragableClusterBF())\n\t\t\t\telif xyz_type == \'frag_of_mol\':\n\t\t\t\t\tself.mols.append(Frag_of_Mol())\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Unknown Type!"")\n\t\t\t\tself.mols[-1].FromXYZString(\'\'.join(txts[line0:line0+nlines+2]))\n\t\t\t\tself.mols[-1].name = str(line)\n\t\t\t\tself.mols[-1].properties[""set_name""] = self.name\n\t\treturn\n\n\tdef Generate_All_MBE_term_General(self, frag_list=[]):\n\t\tfor mol in self.mols:\n\t\t\tmol.Generate_All_MBE_term_General(frag_list)\n\t\treturn\n\n\n\n\nclass GraphSet(MSet):\n\tdef __init__(self, name_ =""gdb9"", path_=""./datasets/"", center_=True):\n\t\tMSet.__init__(self, name_, path_, center_)\n\t\tself.graphs=[]\n\t\tself.path=path_\n\t\tself.name=name_\n\t\tself.suffix="".graph"" #Pickle Database? Poor choice.\n\n\tdef BondTypes(self):\n\t\ttypes = np.array([],dtype=np.uint8)\n\t\tfor m in self.mols:\n\t\t\ttypes = np.union1d(types,m.BondTypes())\n\t\treturn types\n\n\tdef NBonds(self):\n\t\tnbonds=0\n\t\tfor m in self.mols:\n\t\t\tnbonds += m.NBonds()\n\t\treturn nbonds\n\n\tdef MakeGraphs(self):\n\t\tgraphs = map(MolGraph, self.mols)\n\t\treturn graphs\n\n\tdef ReadXYZ(self,filename = None, xyz_type = \'mol\', eqforce=False):\n\t\t"""""" Reads XYZs concatenated into a single file separated by \\n\\n as a molset """"""\n\t\tif filename == None:\n\t\t\tfilename = self.name\n\t\tf = open(self.path+filename+"".xyz"",""r"")\n\t\ttxts = f.readlines()\n\t\tfor line in range(len(txts)):\n\t\t\tif (txts[line].count(\'Comment:\')>0):\n\t\t\t\tline0=line-1\n\t\t\t\tnlines=int(txts[line0])\n\t\t\t\tif xyz_type == \'mol\':\n\t\t\t\t\tself.mols.append(MolGraph())\n\t\t\t\telif xyz_type == \'frag_of_mol\':\n\t\t\t\t\tself.mols.append(Frag_of_Mol())\n\t\t\t\telse:\n\t\t\t\t\traise Exception(""Unknown Type!"")\n\t\t\t\tself.mols[-1].FromXYZString(\'\'.join(txts[line0:line0+nlines+2]))\n\t\t\t\tself.mols[-1].name = str(txts[line0+1])\n\t\t\t\tself.mols[-1].properties[""set_name""] = self.name\n\t\tif (self.center):\n\t\t\tself.CenterSet()\n\t\tif (eqforce):\n\t\t\tself.EQ_forces()\n\t\tLOGGER.debug(""Read ""+str(len(self.mols))+"" molecules from XYZ"")\n\t\treturn\n\n\tdef Save(self):\n\t\tprint(""Saving set to: "", self.path+self.name+self.suffix)\n\t\tf=open(self.path+self.name+self.suffix,""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=1)\n\t\tf.close()\n\t\treturn\n\n\tdef Load(self):\n\t\tf = open(self.path+self.name+self.suffix,""rb"")\n\t\ttmp=pickle.load(f)\n\t\tself.__dict__.update(tmp)\n\t\tf.close()\n\t\tprint(""Loaded, "", len(self.mols), "" molecules "")\n\t\tprint(self.NAtoms(), "" Atoms total"")\n\t\tprint(self.AtomTypes(), "" Types "")\n\t\treturn\n'"
TensorMol/Containers/TensorData.py,4,"b'""""""\n Contains Routines to generate training sets\n Combining a dataset, sampler and an embedding. (CM etc.)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport os, gc\nfrom .Sets import *\nfrom .Digest import *\nfrom ..ForceModifiers.Transformer import *\n#import tables should go to hdf5 soon...\n\nclass TensorData():\n\t""""""\n\tA Training Set is a Molecule set, with a sampler and an embedding\n\tThe sampler chooses points in the molecular volume.\n\tThe embedding turns that into inputs and labels for a network to regress.\n\t""""""\n\tdef __init__(self, MSet_=None, Dig_=None, Name_=None, type_=""atom""):\n\t\t""""""\n\t\tmake a tensordata object\n\t\tSeveral arguments of PARAMS affect this classes behavior\n\n\t\tArgs:\n\t\t\tMSet_: A MoleculeSet\n\t\t\tDig_: A Digester\n\t\t\tName_: A Name\n\t\t""""""\n\t\tself.path = ""./trainsets/""\n\t\tself.suffix = "".pdb""\n\t\tself.set = MSet_\n\t\tself.set_name = None\n\t\tif (self.set != None):\n\t\t\tprint(""loading the set..."")\n\t\t\tself.set_name = MSet_.name # Check to make sure the name can recall the set.\n\t\t\tprint(""finished loading the set.."")\n\t\tself.dig = Dig_\n\t\tself.type = type_\n\t\tself.CurrentElement = None # This is a mode switch for when TensorData provides training data.\n\t\tself.SamplesPerElement = []\n\t\tself.AvailableElements = []\n\t\tself.AvailableDataFiles = []\n\t\tself.NTest = 0  # assgin this value when the data is loaded\n\t\tself.TestRatio = PARAMS[""TestRatio""] # number of cases withheld for testing.\n\t\tself.Random = PARAMS[""RandomizeData""] # Whether to scramble training data (can be disabled for debugging purposes)\n\t\tself.ScratchNCase = 0\n\t\tself.ScratchState=None\n\t\tself.ScratchPointer=0 # for non random batch iteration.\n\t\tself.scratch_inputs=None\n\t\tself.scratch_outputs=None\n\t\tself.scratch_test_inputs=None # These should be partitioned out by LoadElementToScratch\n\t\tself.scratch_test_outputs=None\n\t\tself.Classify=PARAMS[""Classify""] # should be moved to transformer.\n\t\tself.MxTimePerElement=PARAMS[""MxTimePerElement""]\n\t\tself.MxMemPerElement=PARAMS[""MxMemPerElement""]\n\t\tself.ChopTo = PARAMS[""ChopTo""]\n\t\tself.ExpandIsometriesAltogether = False\n\t\tself.ExpandIsometriesBatchwise = False\n\n\t\t# Ordinarily during training batches will be requested repeatedly\n\t\t# for the same element. Introduce some scratch space for that.\n\t\tif (not os.path.isdir(self.path)):\n\t\t\tos.mkdir(self.path)\n\t\tif (Name_!= None):\n\t\t\tself.name = Name_\n\t\t\tself.Load()\n\t\t\treturn\n\t\telif (MSet_==None or Dig_==None):\n\t\t\traise Exception(""I need a set and Digester if you\'re not loading me."")\n\t\tself.name = """"\n\n\tdef CleanScratch(self):\n\t\tself.ScratchState=None\n\t\tself.ScratchPointer=0 # for non random batch iteration.\n\t\tself.scratch_inputs=None\n\t\tself.scratch_outputs=None\n\t\tself.scratch_test_inputs=None # These should be partitioned out by LoadElementToScratch\n\t\tself.scratch_test_outputs=None\n\t\tself.set=None\n\t\treturn\n\n\tdef ReloadSet(self):\n\t\t""""""\n\t\tRecalls the MSet to build training data etc.\n\t\t""""""\n\t\tself.set = MSet(self.set_name)\n\t\tself.set.Load()\n\t\treturn\n\n\tdef PrintStatus(self):\n\t\tprint(""self.ScratchState"",self.ScratchState)\n\t\tprint(""self.ScratchNCase"", self.ScratchNCase)\n\t\tprint(""self.NTrainCasesInScratch()"", self.NTrainCasesInScratch())\n\t\tprint(""self.ScratchPointer"",self.ScratchPointer)\n\t\tif (self.scratch_outputs != None):\n\t\t\tprint(""self.scratch_inputs.shape"",self.scratch_inputs.shape)\n\t\t\tprint(""self.scratch_outputs.shape"",self.scratch_outputs.shape)\n\t\t\tprint(""scratch_test_inputs.shape"",self.scratch_test_inputs.shape)\n\t\t\tprint(""scratch_test_outputs.shape"",self.scratch_test_outputs.shape)\n\n\tdef CheckShapes(self):\n\t\t# Establish case and label shapes.\n\t\ttest_mol = Mol(np.array([1,1],dtype=np.uint8),np.array([[0.0,0.0,0.0],[0.7,0.0,0.0]]))\n\t\ttest_mol.properties[""forces""] = np.zeros((2,3))\n\t\ttest_mol.properties[""mmff94forces""] = np.zeros((2,3))\n\t\ttins,touts = self.dig.TrainDigest(test_mol, 1)\n\t\tprint(""self.dig input shape: "", self.dig.eshape)\n\t\tprint(""self.dig output shape: "", self.dig.lshape)\n\t\tprint(""TrainDigest input shape: "", tins.shape)\n\t\tprint(""TrainDigest output shape: "", touts.shape)\n\t\tif (self.dig.eshape == None or self.dig.lshape ==None):\n\t\t\traise Exception(""Ain\'t got no fucking shape."")\n\n\tdef BuildTrainMolwise(self, name_=""gdb9"", atypes=[], append=False, MakeDebug=False):\n\t\t""""""\n\t\tGenerates inputs for all training data using the chosen digester.\n\t\tThis version builds all the elements at the same time.\n\t\tThe other version builds each element separately\n\t\tIf PESSamples = [] it may use a Go-model (CITE:http://dx.doi.org/10.1016/S0006-3495(02)75308-3)\n\t\t""""""\n\t\tif (((self.dig.name != ""GauInv"" and self.dig.name !=""GauSH"" and self.dig.name !=""ANI1_Sym"")) or (self.dig.OType != ""GoForce"" and self.dig.OType!=""GoForceSphere""\n \t\t\t\t\t\t\t\tand self.dig.OType!=""Force"" and self.dig.OType!=""Del_Force"" and self.dig.OType !=""ForceSphere"" and self.dig.OType !=""ForceMag"")):\n\t\t\traise Exception(""Molwise Embedding not supported"")\n\t\tif (self.set == None):\n\t\t\ttry:\n\t\t\t\tself.ReloadSet()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""TData doesn\'t have a set."", Ex)\n\t\tself.CheckShapes()\n\t\tself.name=name_\n\t\tLOGGER.info(""Generating Train set: %s from mol set %s of size %i molecules"", self.name, self.set.name, len(self.set.mols))\n\t\tif (len(atypes)==0):\n\t\t\tatypes = self.set.AtomTypes()\n\t\tLOGGER.debug(""Will train atoms: ""+str(atypes))\n\t\t# Determine the size of the training set that will be made.\n\t\tnofe = [0 for i in range(MAX_ATOMIC_NUMBER)]\n\t\tfor element in atypes:\n\t\t\tfor m in self.set.mols:\n\t\t\t\tnofe[element] = nofe[element]+m.NumOfAtomsE(element)\n\t\ttruncto = [nofe[i] for i in range(MAX_ATOMIC_NUMBER)]\n\t\tcases_list = [np.zeros(shape=tuple([nofe[element]*self.dig.NTrainSamples]+list(self.dig.eshape)), dtype=np.float64) for element in atypes]\n\t\tlabels_list = [np.zeros(shape=tuple([nofe[element]*self.dig.NTrainSamples]+list(self.dig.lshape)), dtype=np.float64) for element in atypes]\n\t\tcasep_list = [0 for element in atypes]\n\t\tt0 = time.time()\n\t\tord = len(self.set.mols)\n\t\tmols_done = 0\n\t\ttry:\n\t\t\tfor mi in xrange(ord):\n\t\t\t\tm = self.set.mols[mi]\n\t\t\t\tins,outs = self.dig.TrainDigestMolwise(m)\n\t\t\t\tfor i in range(m.NAtoms()):\n\t\t\t\t\t# Route all the inputs and outputs to the appropriate place...\n\t\t\t\t\tai = atypes.tolist().index(m.atoms[i])\n\t\t\t\t\tcases_list[ai][casep_list[ai]] = ins[i]\n\t\t\t\t\tlabels_list[ai][casep_list[ai]] = outs[i]\n\t\t\t\t\tcasep_list[ai] = casep_list[ai]+1\n\t\t\t\tif (mols_done%10000==0 and mols_done>0):\n\t\t\t\t\tprint(mols_done)\n\t\t\t\tif (mols_done==400):\n\t\t\t\t\tprint(""Seconds to process 400 molecules: "", time.time()-t0)\n\t\t\t\tmols_done = mols_done + 1\n\t\texcept Exception as Ex:\n\t\t\t\tprint(""Likely you need to re-install MolEmb."", Ex)\n\t\tfor element in atypes:\n\t\t\t# Write the numpy arrays for this element.\n\t\t\tai = atypes.tolist().index(element)\n\t\t\tinsname = self.path+name_+""_""+self.dig.name+""_""+str(element)+""_in.npy""\n\t\t\toutsname = self.path+name_+""_""+self.dig.name+""_""+str(element)+""_out.npy""\n\t\t\talreadyexists = (os.path.isfile(insname) and os.path.isfile(outsname))\n\t\t\tif (append and alreadyexists):\n\t\t\t\tti=None\n\t\t\t\tto=None\n\t\t\t\tinf = open(insname,""rb"")\n\t\t\t\touf = open(outsname,""rb"")\n\t\t\t\tti = np.load(inf)\n\t\t\t\tto = np.load(ouf)\n\t\t\t\tinf.close()\n\t\t\t\touf.close()\n\t\t\t\ttry:\n\t\t\t\t\tcases = np.concatenate((cases_list[ai][:casep_list[ai]],ti))\n\t\t\t\t\tlabels = np.concatenate((labels_list[ai][:casep_list[ai]],to))\n\t\t\t\texcept Exception as Ex:\n\t\t\t\t\tprint(""Size mismatch with old training data, clear out trainsets"")\n\t\t\t\tinf = open(insname,""wb"")\n\t\t\t\touf = open(outsname,""wb"")\n\t\t\t\tnp.save(inf,cases)\n\t\t\t\tnp.save(ouf,labels)\n\t\t\t\tinf.close()\n\t\t\t\touf.close()\n\t\t\t\tself.AvailableDataFiles.append([insname,outsname])\n\t\t\t\tself.AvailableElements.append(element)\n\t\t\t\tself.SamplesPerElement.append(casep_list[ai]*self.dig.NTrainSamples)\n\t\t\telse:\n\t\t\t\tinf = open(insname,""wb"")\n\t\t\t\touf = open(outsname,""wb"")\n\t\t\t\tnp.save(inf,cases_list[ai][:casep_list[ai]])\n\t\t\t\tnp.save(ouf,labels_list[ai][:casep_list[ai]])\n\t\t\t\tinf.close()\n\t\t\t\touf.close()\n\t\t\t\tself.AvailableDataFiles.append([insname,outsname])\n\t\t\t\tself.AvailableElements.append(element)\n\t\t\t\tself.SamplesPerElement.append(casep_list[ai]*self.dig.NTrainSamples)\n\t\tself.Save() #write a convenience pickle.\n\t\treturn\n\n\tdef BuildTrain(self, name_=""gdb9"", atypes=[], append=False, MakeDebug=False):\n\t\t""""""\n\t\tGenerates probability inputs for all training data using the chosen digester.\n\t\tAll the inputs for a given atom are built separately.\n\t\tNow requires some sort of PES information.\n\t\tIf PESSamples = [] it will use a Go-model (CITE:http://dx.doi.org/10.1016/S0006-3495(02)75308-3)\n\t\tThe code that uses ab-initio samples isn\'t written yet, but should be.\n\t\t""""""\n\t\tif (self.set == None):\n\t\t\ttry:\n\t\t\t\tself.ReloadSet()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""TData doesn\'t have a set."", Ex)\n\t\tself.CheckShapes()\n\t\tself.name=name_\n\t\tprint(""Generating Train set:"", self.name, "" from mol set "", self.set.name, "" of size "", len(self.set.mols),"" molecules"")\n\t\tif (len(atypes)==0):\n\t\t\tatypes = self.set.AtomTypes()\n\t\tprint(""Will train atoms: "", atypes)\n\t\t# Determine the size of the training set that will be made.\n\t\tnofe = [0 for i in range(MAX_ATOMIC_NUMBER)]\n\t\tfor element in atypes:\n\t\t\tfor m in self.set.mols:\n\t\t\t\tnofe[element] = nofe[element]+m.NumOfAtomsE(element)\n\t\treqmem = [nofe[element]*self.dig.NTrainSamples*np.prod(self.dig.eshape)*4/1024.0/1024.0 for element in range(MAX_ATOMIC_NUMBER)]\n\t\ttruncto = [nofe[i] for i in range(MAX_ATOMIC_NUMBER)]\n\t\tfor element in atypes:\n\t\t\tprint(""AN: "", element, "" contributes "", nofe[element]*self.dig.NTrainSamples , "" samples, requiring "", reqmem[element], "" MB of in-core memory. "")\n\t\t\tif (reqmem[element]>self.MxMemPerElement):\n\t\t\t\ttruncto[element]=int(self.MxMemPerElement/reqmem[element]*nofe[element])\n\t\t\t\tprint(""Truncating element "", element, "" to "",truncto[element],"" Samples"")\n\t\tfor element in atypes:\n\t\t\tDebugCases=[]\n\t\t\tprint(""Digesting atom: "", element)\n\t\t\tcases = np.zeros(shape=tuple([truncto[element]*self.dig.NTrainSamples]+list(self.dig.eshape)), dtype=np.float64)\n\t\t\tlabels = np.zeros(shape=tuple([truncto[element]*self.dig.NTrainSamples]+list(self.dig.lshape)), dtype=np.float64)\n\t\t\tcasep = 0\n\t\t\tt0 = time.time()\n\t\t\tfor mi in range(len(self.set.mols)):\n\t\t\t\tm = self.set.mols[mi]\n\t\t\t\tif (mi%100==0):\n\t\t\t\t\tprint(""Digested "", mi ,"" of "",len(self.set.mols))\n\t\t\t\tins,outs=None,None\n\t\t\t\tif (MakeDebug):\n\t\t\t\t\tins,outs,db = self.dig.TrainDigest(self.set.mols[mi],element,True)\n\t\t\t\t\tDebugCases = DebugCases + db\n\t\t\t\telse:\n\t\t\t\t\tins,outs = self.dig.TrainDigest(self.set.mols[mi],element)\n\t\t\t\tGotOut = outs.shape[0]\n\t\t\t\tif (GotOut!=ins.shape[0]):\n\t\t\t\t\traise Exception(""Insane Digest"")\n\t\t\t\tif (truncto[element]<casep+GotOut):\n\t\t\t\t\tprint(""Truncating at "", casep, ""Samples because "",truncto[element], "" is less than "",casep,"" plus "",GotOut)\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tcases[casep:casep+outs.shape[0]] += ins\n\t\t\t\t\tlabels[casep:casep+outs.shape[0]] += outs\n\t\t\t\t\tcasep += outs.shape[0]\n\t\t\t\tif ((time.time()-t0)>self.MxTimePerElement):\n\t\t\t\t\tbreak\n\t\t\t\tif (mi==40 and casep>=40):\n\t\t\t\t\tprint(""Seconds to process 40 molecules: "", time.time()-t0)\n\t\t\t\t\tprint(""Average label: "", np.average(labels[:casep]))\n\t\t\t\t\tif  (not np.isfinite(np.average(labels[:casep]))):\n\t\t\t\t\t\traise Exception(""Bad Labels"")\n\t\t\t\tif (mi%1000):\n\t\t\t\t\tgc.collect()\n\t\t\t\tif (mi%1000==0):\n\t\t\t\t\tprint(mi)\n\t\t\t# Write the numpy arrays for this element.\n\t\t\tinsname = self.path+name_+""_""+self.dig.name+""_""+str(element)+""_in.npy""\n\t\t\toutsname = self.path+name_+""_""+self.dig.name+""_""+str(element)+""_out.npy""\n\t\t\talreadyexists = (os.path.isfile(insname) and os.path.isfile(outsname))\n\t\t\tif (append and alreadyexists):\n\t\t\t\tti=None\n\t\t\t\tto=None\n\t\t\t\tinf = open(insname,""rb"")\n\t\t\t\touf = open(outsname,""rb"")\n\t\t\t\tti = np.load(inf)\n\t\t\t\tto = np.load(ouf)\n\t\t\t\tinf.close()\n\t\t\t\touf.close()\n\t\t\t\tcases = np.concatenate((cases[:casep],ti))\n\t\t\t\tlabels = np.concatenate((labels[:casep],to))\n\t\t\t\tinf = open(insname,""wb"")\n\t\t\t\touf = open(outsname,""wb"")\n\t\t\t\tnp.save(inf,cases)\n\t\t\t\tnp.save(ouf,labels)\n\t\t\t\tinf.close()\n\t\t\t\touf.close()\n\t\t\t\tself.AvailableDataFiles.append([insname,outsname])\n\t\t\t\tself.AvailableElements.append(element)\n\t\t\t\tself.SamplesPerElement.append(casep*self.dig.NTrainSamples)\n\t\t\telse:\n\t\t\t\tinf = open(insname,""wb"")\n\t\t\t\touf = open(outsname,""wb"")\n\t\t\t\tnp.save(inf,cases[:casep])\n\t\t\t\tnp.save(ouf,labels[:casep])\n\t\t\t\tinf.close()\n\t\t\t\touf.close()\n\t\t\t\tself.AvailableDataFiles.append([insname,outsname])\n\t\t\t\tself.AvailableElements.append(element)\n\t\t\t\tself.SamplesPerElement.append(casep*self.dig.NTrainSamples)\n\t\t\tif (MakeDebug):\n\t\t\t\tdbgname = self.path+name_+""_""+self.dig.name+""_""+str(element)+""_dbg.tdt""\n\t\t\t\tf=open(dbgname,""wb"")\n\t\t\t\tpickle.dump(DebugCases, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\t\t\tf.close()\n\t\tself.Save() #write a convenience pickle.\n\t\treturn\n\n\tdef GetTrainBatch(self,ele,ncases=2000,random=True):\n\t\tif (self.ScratchState != ele):\n\t\t\tself.LoadElementToScratch(ele,random)\n\t\tif (ncases>self.NTrainCasesInScratch()):\n\t\t\traise Exception(""Training Data is less than the batchsize... :( "")\n\t\tif (self.ExpandIsometriesBatchwise):\n\t\t\tif ( self.ScratchPointer*GRIDS.NIso()+ncases >= self.NTrainCasesInScratch() ):\n\t\t\t\tself.ScratchPointer = 0 #Sloppy.\n\t\t\tneff = int(ncases/GRIDS.NIso())+1\n\t\t\ttmp=GRIDS.ExpandIsometries(self.scratch_inputs[self.ScratchPointer:self.ScratchPointer+neff], self.scratch_outputs[self.ScratchPointer:self.ScratchPointer+neff])\n\t\t\t#print tmp[0].shape, tmp[1].shape\n\t\t\tself.ScratchPointer += neff\n\t\t\treturn tmp[0][:ncases], tmp[1][:ncases]\n\t\telse:\n\t\t\tif ( self.ScratchPointer+ncases >= self.NTrainCasesInScratch()):\n\t\t\t\tself.ScratchPointer = 0 #Sloppy.\n\t\t\ttmp=(self.scratch_inputs[self.ScratchPointer:self.ScratchPointer+ncases], self.scratch_outputs[self.ScratchPointer:self.ScratchPointer+ncases])\n\t\t\tself.ScratchPointer += ncases\n\t\t\treturn tmp\n\n\tdef GetTestBatch(self,ele,ncases=200, ministep = 0):\n\t\tif (self.ScratchState != ele):\n\t\t\tself.LoadElementToScratch(ele,False)\n\t\tif (ncases>self.scratch_test_inputs.shape[0]):\n\t\t\tprint(""Test Data is less than the batchsize...:("")\n\t\t\ttmpinputs=np.zeros(shape=tuple([ncases]+list(self.dig.eshape)), dtype=np.float64)\n\t\t\ttmpoutputs=np.zeros(shape=tuple([ncases]+list(self.dig.lshape)), dtype=np.float64)\n\t\t\ttmpinputs[0:self.scratch_test_inputs.shape[0]] += self.scratch_test_inputs\n\t\t\ttmpoutputs[0:self.scratch_test_outputs.shape[0]] += self.scratch_test_outputs\n\t\t\treturn (tmpinputs[ncases*(ministep):ncases*(ministep+1)], tmpoutputs[ncases*(ministep):ncases*(ministep+1)])\n\t\treturn (self.scratch_test_inputs[ncases*(ministep):ncases*(ministep+1)], self.scratch_test_outputs[ncases*(ministep):ncases*(ministep+1)])\n\n\tdef EvaluateTestBatch(self, desired, predicted, tformer, Opt=False):\n\t\ttry:\n\t\t\tif (tformer.outnorm != None):\n\t\t\t\tdesired = tformer.UnNormalizeOuts(desired)\n\t\t\t\tpredicted = tformer.UnNormalizeOuts(predicted)\n\t\t\tprint(""Evaluating, "", len(desired), "" predictions... "")\n\t\t\tprint(desired.shape, predicted.shape)\n\t\t\tif (self.dig.OType==""Disp"" or self.dig.OType==""Force"" or self.dig.OType == ""GoForce"" or self.dig.OType == ""Del_Force""):\n\t\t\t\terr = predicted-desired\n\t\t\t\tders = np.linalg.norm(err, axis=1)\n\t\t\t\tfor i in range(50):\n\t\t\t\t\tprint(""Desired: "",i,desired[i,-3:],"" Predicted: "",predicted[i,-3:])\n\t\t\t\tLOGGER.info(""Test displacement errors direct (mean,std) %f,%f"",np.average(ders),np.std(ders))\n\t\t\t\tLOGGER.info(""MAE and Std. Dev.: %f, %f"", np.mean(np.absolute(err)), np.std(np.absolute(err)))\n\t\t\t\tLOGGER.info(""Average learning target: %s, Average output (direct) %s"", str(np.average(desired,axis=0)),str(np.average(predicted,axis=0)))\n\t\t\t\tLOGGER.info(""Fraction of incorrect directions: %f"", np.sum(np.sign(desired[:,-3:])-np.sign(predicted[:,-3:]))/(6.*len(desired)))\n\t\t\telif (self.dig.OType == ""GoForceSphere"" or self.dig.OType == ""ForceSphere""):\n\t\t\t\t# Convert them back to cartesian\n\t\t\t\tdesiredc = SphereToCartV(desired)\n\t\t\t\tpredictedc = SphereToCartV(predicted)\n\t\t\t\terr = predictedc-desiredc\n\t\t\t\tders = np.linalg.norm(err, axis=1)\n\t\t\t\tfor i in range(100):\n\t\t\t\t\tprint(""Desired: "",i,desiredc[i,-3:],"" Predicted: "",predictedc[i,-3:])\n\t\t\t\tLOGGER.info(""Test displacement errors direct (mean,std) %f,%f"",np.average(ders),np.std(ders))\n\t\t\t\tLOGGER.info(""MAE and Std. Dev.: %f, %f"", np.mean(np.absolute(err)), np.std(np.absolute(err)))\n\t\t\t\tLOGGER.info(""Average learning target: %s, Average output (direct) %s"", str(np.average(desiredc[:,-3:],axis=0)),str(np.average(predictedc[:,-3:],axis=0)))\n\t\t\t\tLOGGER.info(""Fraction of incorrect directions: %f"", np.sum(np.sign(desired[:,-3:])-np.sign(predicted[:,-3:]))/(6.*len(desired)))\n\t\t\telif (self.dig.OType == ""ForceMag""):\n\t\t\t\terr = predicted-desired\n\t\t\t\tfor i in range(100):\n\t\t\t\t\tprint(""Desired: "",i,desired[i],"" Predicted: "",predicted[i])\n\t\t\t\tLOGGER.info(""MAE and Std. Dev.: %f, %f"", np.mean(np.absolute(err)), np.std(np.absolute(err)))\n\t\t\t\tLOGGER.info(""Average learning target: %s, Average output (direct) %s"", str(np.average(desired[:],axis=0)),str(np.average(predicted[:],axis=0)))\n\t\t\telif (self.dig.OType==""SmoothP""):\n\t\t\t\tders=np.zeros(len(desired))\n\t\t\t\tiers=np.zeros(len(desired))\n\t\t\t\tcomp=np.zeros(len(desired))\n\t\t\t\tfor i in range(len(desired)):\n\t\t\t\t\t#print ""Direct - desired disp"", desired[i,-3:],"" Pred disp"", predicted[i,-3:]\n\t\t\t\t\tPr = GRIDS.Rasterize(predicted[i,:GRIDS.NGau3])\n\t\t\t\t\tPr /= np.sum(Pr)\n\t\t\t\t\tp=np.dot(GRIDS.MyGrid().T,Pr)\n\t\t\t\t\t#print ""fit disp: "", p\n\t\t\t\t\tders[i] = np.linalg.norm(predicted[i,-3:]-desired[i,-3:])\n\t\t\t\t\tiers[i] = np.linalg.norm(p-desired[i,-3:])\n\t\t\t\t\tcomp[i] = np.linalg.norm(p-predicted[i,-3:])\n\t\t\t\tprint(""Test displacement errors direct (mean,std) "", np.average(ders),np.std(ders), "" indirect "",np.average(iers),np.std(iers), "" Comp "", np.average(comp), np.std(comp))\n\t\t\t\tprint(""Average learning target: "", np.average(desired[:,-3:],axis=0),""Average output (direct)"",np.average(predicted[:,-3:],axis=0))\n\t\t\t\tprint(""Fraction of incorrect directions: "", np.sum(np.sign(desired[:,-3:])-np.sign(predicted[:,-3:]))/(6.*len(desired)))\n\t\t\telif (self.dig.OType==""StoP""):\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\telif (self.dig.OType==""Energy""):\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\telif (self.dig.OType==""GoForce_old_version""): # python version is fine for here\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\telif (self.dig.OType==""HardP""):\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\telse:\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\texcept Exception as Ex:\n\t\t\tprint(""Something went wrong"")\n\t\t\tprint(Ex)\n\t\t\tpass\n\t\tif (Opt):\n\t\t\treturn np.mean(np.absolute(predicted[:,-3:]-desired[:,-3:]))\n\t\treturn\n\n\tdef MergeWith(self,ASet_):\n\t\t\'\'\'\n\t\tAugments my training data with another set, which for example may have been generated on another computer.\n\t\t\'\'\'\n\t\tself.QueryAvailable()\n\t\tASet_.QueryAvailable()\n\t\tprint(""Merging"", self.name, "" with "", ASet_.name)\n\t\tfor ele in ASet_.AvailableElements:\n\t\t\tif (self.AvailableElements.count(ele)==0):\n\t\t\t\traise Exception(""WriteME192837129874"")\n\t\t\telse:\n\t\t\t\tmti,mto = self.LoadElement(ele)\n\t\t\t\tati,ato = ASet_.LoadElement(ele)\n\t\t\t\tlabelshapes = list(mti.shape)[1:]\n\t\t\t\teshapes = list(mto.shape)[1:]\n\t\t\t\tASet_labelshapes = list(ati.shape)[1:]\n\t\t\t\tASet_eshapes = list(ato.shape)[1:]\n\t\t\t\tif (labelshapes != ASet_labelshapes or eshapes != ASet_eshapes):\n\t\t\t\t\traise Exception(""incompatible"")\n\t\t\t\tif (self.dig.name != ASet_.dig.name):\n\t\t\t\t\traise Exception(""incompatible"")\n\t\t\t\tprint(""Merging "", self.name, "" element, "", ele ,"" with "", ASet_.name)\n\t\t\t\tmti=np.concatenate((mti,ati),axis=0)\n\t\t\t\tmto=np.concatenate((mto,ato),axis=0)\n\t\t\t\tprint(""The new element train set will have"", mti.shape[0], "" cases in it"")\n\t\t\t\tinsname = self.path+self.name+""_""+self.dig.name+""_""+str(ele)+""_in.npy""\n\t\t\t\toutsname = self.path+self.name+""_""+self.dig.name+""_""+str(ele)+""_out.npy""\n\t\t\t\tinf = open(insname,""wb"")\n\t\t\t\touf = open(outsname,""wb"")\n\t\t\t\tnp.save(inf,mti)\n\t\t\t\tnp.save(ouf,mto)\n\t\t\t\tinf.close()\n\t\t\t\touf.close()\n\n\tdef Save(self):\n\t\tself.CleanScratch()\n\t\tf=open(self.path+self.name+""_""+self.dig.name+"".tdt"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\n\tdef Load(self):\n\t\tprint(""Unpickling Tensordata"")\n\t\tf = open(self.path+self.name+"".tdt"",""rb"")\n\t\ttmp=pickle.load(f)\n\t\tself.__dict__.update(tmp)\n\t\tf.close()\n\t\tself.CheckShapes()\n\t\tprint(""Training data manager loaded."")\n\t\tif (self.set != None):\n\t\t\tprint(""Based on "", len(self.set.mols), "" molecules "")\n\t\tprint(""Based on files: "",self.AvailableDataFiles)\n\t\tself.QueryAvailable()\n\t\tself.PrintSampleInformation()\n\t\tself.dig.Print()\n\t\treturn\n\n\tdef QueryAvailable(self):\n\t\t""""""\n\t\tIf Tensordata has already been made, this looks for it under a passed name.\n\t\t""""""\n\t\tself.AvailableElements=[]\n\t\tself.SamplesPerElement=[]\n\t\tfor i in range(MAX_ATOMIC_NUMBER):\n\t\t\tif (os.path.isfile(self.path+self.name+""_""+self.dig.name+""_""+str(i)+""_in.npy"") and os.path.isfile(self.path+self.name+""_""+self.dig.name+""_""+str(i)+""_out.npy"")):\n\t\t\t\tself.AvailableElements.append(i)\n\t\t\t\tifname = self.path+self.name+""_""+self.dig.name+""_""+str(i)+""_out.npy""\n\t\t\t\tofname = self.path+self.name+""_""+self.dig.name+""_""+str(i)+""_out.npy""\n\t\t\t\tinf = open(ifname,""rb"")\n\t\t\t\touf = open(ofname,""rb"")\n\t\t\t\tti = np.load(inf)\n\t\t\t\tto = np.load(ouf)\n\t\t\t\tinf.close()\n\t\t\t\touf.close()\n\t\t\t\tif (len(ti)!=len(to)):\n\t\t\t\t\tprint(""...Retrain element i"")\n\t\t\t\telse:\n\t\t\t\t\tif (self.ChopTo!=None):\n\t\t\t\t\t\tself.SamplesPerElement.append(min(len(ti),self.ChopTo))\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.SamplesPerElement.append(len(ti))\n\t\tself.AvailableElements.sort()\n\t\t# It should probably check the sanity of each input/outputfile as well...\n\t\treturn\n\n\tdef LoadElement(self, ele, Random=True, DebugData_=False):\n\t\tinsname = self.path+self.name+""_""+self.dig.name+""_""+str(ele)+""_in.npy""\n\t\toutsname = self.path+self.name+""_""+self.dig.name+""_""+str(ele)+""_out.npy""\n\t\tdbgname = self.path+self.name+""_""+self.dig.name+""_""+str(ele)+""_dbg.tdt""\n\t\ttry:\n\t\t\tinf = open(insname,""rb"")\n\t\t\touf = open(outsname,""rb"")\n\t\t\tti = np.load(inf)\n\t\t\tto = np.load(ouf)\n\t\t\tinf.close()\n\t\t\touf.close()\n\t\texcept Exception as Ex:\n\t\t\tprint(""Failed to read:"",insname, "" or "",outsname)\n\t\t\traise Ex\n\t\tif (ti.shape[0] != to.shape[0]):\n\t\t\traise Exception(""Bad Training Data."")\n\t\tif (self.ChopTo!=None):\n\t\t\tti = ti[:self.ChopTo]\n\t\t\tto = to[:self.ChopTo]\n\t\tif (DebugData_):\n\t\t\tprint(""DEBUGGING, "", len(ti), "" cases.."")\n\t\t\tf = open(dbgname,""rb"")\n\t\t\tdbg=pickle.load(f)\n\t\t\tf.close()\n\t\t\tprint(""Found "", len(dbg), "" pieces of debug information for this element... "")\n\t\t\tfor i in range(len(dbg)):\n\t\t\t\tprint(""CASE:"", i, "" was for ATOM"", dbg[i][1], "" At Point "", dbg[i][2])\n\t\t\t\tds=GRIDS.Rasterize(ti[i])\n\t\t\t\tGridstoRaw(ds, GRIDS.NPts, ""InpCASE""+str(i))\n\t\t\t\tprint(dbg[i][0].coords)\n\t\t\t\tprint(dbg[i][0].atoms)\n\t\t#ti = ti.reshape((ti.shape[0],-1))  # flat data to [ncase, num_per_case]\n\t\t#to = to.reshape((to.shape[0],-1))  # flat labels to [ncase, 1]\n\t\tif (Random):\n\t\t\tidx = np.random.permutation(ti.shape[0])\n\t\t\tti = ti[idx]\n\t\t\tto = to[idx]\n\t\tself.ScratchNCase = to.shape[0]\n\t\treturn ti, to\n\n\tdef LoadElementToScratch(self,ele,tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\t\t""""""\n\t\tti, to = self.LoadElement(ele, self.Random)\n\t\tif (self.dig.name==""SensoryBasis"" and self.dig.OType==""Disp"" and self.ExpandIsometriesAltogether):\n\t\t\tprint(""Expanding the given set over isometries."")\n\t\t\tti,to = GRIDS.ExpandIsometries(ti,to)\n\t\tif (tformer.outnorm != None):\n\t\t\tto = tformer.NormalizeOuts(to)\n\t\tif (tformer.innorm != None):\n\t\t\tti = tformer.NormalizeIns(ti)\n\t\tself.NTest = int(self.TestRatio * ti.shape[0])\n\t\tself.scratch_inputs = ti[:ti.shape[0]-self.NTest]\n\t\tself.scratch_outputs = to[:ti.shape[0]-self.NTest]\n\t\tself.scratch_test_inputs = ti[ti.shape[0]-self.NTest:]\n\t\tself.scratch_test_outputs = to[ti.shape[0]-self.NTest:]\n\t\tself.ScratchState = ele\n\t\tself.ScratchPointer=0\n\t\tLOGGER.debug(""Element ""+str(ele)+"" loaded..."")\n\t\treturn\n\n\tdef NTrainCasesInScratch(self):\n\t\tif (self.ExpandIsometriesBatchwise):\n\t\t\treturn self.scratch_inputs.shape[0]*GRIDS.NIso()\n\t\telse:\n\t\t\treturn self.scratch_inputs.shape[0]\n\n\tdef NTestCasesInScratch(self):\n\t\treturn self.scratch_inputs.shape[0]\n\n\tdef PrintSampleInformation(self):\n\t\tlim = min(len(self.AvailableElements),len(self.SamplesPerElement),len(self.AvailableDataFiles))\n\t\tfor i in range(lim):\n\t\t\tprint(""AN: "", self.AvailableElements[i], "" contributes "", self.SamplesPerElement[i] , "" samples "")\n\t\t\tprint(""From files: "", self.AvailableDataFiles[i])\n\t\treturn\n\nclass TensorDataDirect(TensorData):\n\tdef __init__(self, MSet_=None, Dig_=None, Name_=None, type_=""atom""):\n\t\tTensorData.__init__(self, MSet_, Dig_, Name_, type_)\n\t\tif (MSet_ != None):\n\t\t\tself.MaxNAtoms = np.max([m.NAtoms() for m in self.set.mols])\n\t\t\tself.Nmols = len(self.set.mols)\n\t\t\tself.AvailableElements = self.set.AtomTypes()\n\t\t\tself.name = self.set.name\n\n\tdef LoadData(self):\n\t\tif self.set == None:\n\t\t\tself.ReloadSet()\n\t\trandom.shuffle(self.set.mols)\n\t\txyzs = np.zeros((self.Nmols, self.MaxNAtoms, 3))\n\t\tZs = np.zeros((self.Nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tif (self.dig.OType == ""Force""):\n\t\t\tlabels = np.zeros((self.Nmols,self.MaxNAtoms,3))\n\t\telse:\n\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\tfor i, mol in enumerate(self.set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tlabels[i][:mol.NAtoms()] = mol.properties[""forces""]\n\t\treturn xyzs, Zs, labels\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tself.xyzs, self.Zs, self.labels = self.LoadData()\n\t\tif (tformer.outnorm != None):\n\t\t\tself.labels = tformer.NormalizeOuts(self.labels)\n\t\tself.NTestMols = int(self.TestRatio * self.Zs.shape[0])\n\t\tself.LastTrainMol = int(self.Zs.shape[0]-self.NTestMols)\n\t\tself.NTrain = self.LastTrainMol\n\t\tself.NTest = self.NTestMols\n\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.ScratchPointer = 0\n\t\tself.ScratchState = 1\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\treturn\n\n\tdef GetTrainBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tlabels = self.labels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\treturn [xyzs, Zs, labels]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tlabels = self.labels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\treturn [xyzs, Zs, labels]\n\n\tdef EvaluateTestBatch(self, desired, predicted):\n\t\ttry:\n\t\t\tprint(""Evaluating, "", len(desired), "" predictions... "")\n\t\t\tprint(desired.shape, predicted.shape)\n\t\t\tif (self.dig.OType==""Disp"" or self.dig.OType==""Force"" or self.dig.OType == ""GoForce"" or self.dig.OType == ""Del_Force""):\n\t\t\t\terr = predicted-desired\n\t\t\t\tders = np.linalg.norm(err, axis=1)\n\t\t\t\tfor i in range(20):\n\t\t\t\t\tprint(""Desired: "",i,desired[i,-3:],"" Predicted: "",predicted[i,-3:])\n\t\t\t\tLOGGER.info(""Test displacement errors direct (mean,std) %f,%f"",np.average(ders),np.std(ders))\n\t\t\t\tLOGGER.info(""MAE and Std. Dev.: %f, %f"", np.mean(np.absolute(err)), np.std(np.absolute(err)))\n\t\t\t\tLOGGER.info(""Average learning target: %s, Average output (direct) %s"", str(np.average(desired,axis=0)),str(np.average(predicted,axis=0)))\n\t\t\t\tLOGGER.info(""Fraction of incorrect directions: %f"", np.sum(np.sign(desired[:,-3:])-np.sign(predicted[:,-3:]))/(6.*len(desired)))\n\t\t\telif (self.dig.OType == ""GoForceSphere"" or self.dig.OType == ""ForceSphere""):\n\t\t\t\t# Convert them back to cartesian\n\t\t\t\tdesiredc = SphereToCartV(desired)\n\t\t\t\tpredictedc = SphereToCartV(predicted)\n\t\t\t\terr = predictedc-desiredc\n\t\t\t\tders = np.linalg.norm(err, axis=1)\n\t\t\t\tfor i in range(20):\n\t\t\t\t\tprint(""Desired: "",i,desiredc[i,-3:],"" Predicted: "",predictedc[i,-3:])\n\t\t\t\tLOGGER.info(""Test displacement errors direct (mean,std) %f,%f"",np.average(ders),np.std(ders))\n\t\t\t\tLOGGER.info(""MAE and Std. Dev.: %f, %f"", np.mean(np.absolute(err)), np.std(np.absolute(err)))\n\t\t\t\tLOGGER.info(""Average learning target: %s, Average output (direct) %s"", str(np.average(desiredc[:,-3:],axis=0)),str(np.average(predictedc[:,-3:],axis=0)))\n\t\t\t\tLOGGER.info(""Fraction of incorrect directions: %f"", np.sum(np.sign(desired[:,-3:])-np.sign(predicted[:,-3:]))/(6.*len(desired)))\n\t\t\telif (self.dig.OType == ""ForceMag""):\n\t\t\t\terr = predicted-desired\n\t\t\t\tfor i in range(20):\n\t\t\t\t\tprint(""Desired: "",i,desired[i],"" Predicted: "",predicted[i])\n\t\t\t\tLOGGER.info(""MAE and Std. Dev.: %f, %f"", np.mean(np.absolute(err)), np.std(np.absolute(err)))\n\t\t\t\tLOGGER.info(""Average learning target: %s, Average output (direct) %s"", str(np.average(desired[:],axis=0)),str(np.average(predicted[:],axis=0)))\n\t\t\telif (self.dig.OType==""SmoothP""):\n\t\t\t\tders=np.zeros(len(desired))\n\t\t\t\tiers=np.zeros(len(desired))\n\t\t\t\tcomp=np.zeros(len(desired))\n\t\t\t\tfor i in range(len(desired)):\n\t\t\t\t\t#print ""Direct - desired disp"", desired[i,-3:],"" Pred disp"", predicted[i,-3:]\n\t\t\t\t\tPr = GRIDS.Rasterize(predicted[i,:GRIDS.NGau3])\n\t\t\t\t\tPr /= np.sum(Pr)\n\t\t\t\t\tp=np.dot(GRIDS.MyGrid().T,Pr)\n\t\t\t\t\t#print ""fit disp: "", p\n\t\t\t\t\tders[i] = np.linalg.norm(predicted[i,-3:]-desired[i,-3:])\n\t\t\t\t\tiers[i] = np.linalg.norm(p-desired[i,-3:])\n\t\t\t\t\tcomp[i] = np.linalg.norm(p-predicted[i,-3:])\n\t\t\t\tprint(""Test displacement errors direct (mean,std) "", np.average(ders),np.std(ders), "" indirect "",np.average(iers),np.std(iers), "" Comp "", np.average(comp), np.std(comp))\n\t\t\t\tprint(""Average learning target: "", np.average(desired[:,-3:],axis=0),""Average output (direct)"",np.average(predicted[:,-3:],axis=0))\n\t\t\t\tprint(""Fraction of incorrect directions: "", np.sum(np.sign(desired[:,-3:])-np.sign(predicted[:,-3:]))/(6.*len(desired)))\n\t\t\telif (self.dig.OType==""StoP""):\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\telif (self.dig.OType==""Energy""):\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\telif (self.dig.OType==""GoForce_old_version""): # python version is fine for here\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\telif (self.dig.OType==""HardP""):\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\t\telse:\n\t\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\texcept Exception as Ex:\n\t\t\tprint(""Something went wrong"")\n\t\t\tprint(Ex)\n\t\t\tpass\n\t\treturn\n'"
TensorMol/Containers/TensorMolData.py,8,"b'#\n# Contains Routines to generate training sets\n# Combining a dataset, sampler and an embedding. (CM etc.)\n#\n# These work Moleculewise the versions without the mol prefix work atomwise.\n# but otherwise the behavior of these is the same as Tensordata etc.\n#\n#\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport os, gc\nfrom .Sets import *\nfrom .DigestMol import *\nfrom .TensorData import *\nfrom ..ForceModifiers.Neighbors import *\n#import tables should go to hdf5 soon...\n\nclass TensorMolData(TensorData):\n\t""""""\n\tA Training Set is a Molecule set, with a sampler and an embedding\n\tThe sampler chooses points in the molecular volume.\n\tThe embedding turns that into inputs and labels for a network to regress.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol""):\n\t\t""""""\n\t\tArgs:\n\t\t\tMSet_: A molecule set from which to cull data.\n\t\t\tDig_: A MolDigester object to create embeddings, and evaluate outputs.\n\t\t\tName_: A name for this TensorMolData\n\t\t\t# These parameters should be removed ------------\n\t\t\torder_ : Order of many-body expansion to perform.\n\t\t\tnum_indis_: Number of Indistinguishable Fragments.\n\t\t\ttype_: Whether this TensorMolData is for ""frag"", ""atom"", or ""mol""\n\t\t""""""\n\t\tself.order = order_\n\t\tself.num_indis = num_indis_\n\t\tself.NTrain = 0\n\t\t#self.MaxNAtoms = MSet_.MaxNAtoms()\n\t\tTensorData.__init__(self, MSet_,Dig_,Name_, type_=type_)\n\t\ttry:\n\t\t\tLOGGER.info(""TensorMolData.type: %s"",self.type)\n\t\t\tLOGGER.info(""TensorMolData.dig.name: %s"",self.dig.name)\n\t\t\tLOGGER.info(""NMols in TensorMolData.set: %i"", len(self.set.mols))\n\t\t\tself.raw_it = iter(self.set.mols)\n\t\texcept:\n\t\t\tprint("" do not include MSet"")\n\t\tself.MaxNAtoms = None\n\t\ttry:\n\t\t\tif (MSet_ != None):\n\t\t\t\tself.MaxNAtoms = MSet_.MaxNAtoms()\n\t\texcept:\n\t\t\tprint(""fail to load self.MaxNAtoms"")\n\t\treturn\n\n\tdef QueryAvailable(self):\n\t\t"""""" If Tensordata has already been made, this looks for it under a passed name.""""""\n\t\t# It should probably check the sanity of each input/outputfile as well...\n\t\treturn\n\n\tdef CheckShapes(self):\n\t\t# Establish case and label shapes.\n\t\tif self.type==""frag"":\n\t\t\ttins,touts = self.dig.Emb(test_mol.mbe_frags[self.order][0],False,False)\n\t\telif self.type==""mol"":\n\t\t\tif (self.set != None):\n\t\t\t\ttest_mol = self.set.mols[0]\n\t\t\t\ttins,touts = self.dig.Emb(test_mol,True,False)\n\t\t\telse:\n\t\t\t\treturn\n\t\telse:\n\t\t\traise Exception(""Unknown Type"")\n\t\tprint(""self.dig "", self.dig.name)\n\t\tprint(""self.dig input shape: "", self.dig.eshape)\n\t\tprint(""self.dig output shape: "", self.dig.lshape)\n\t\tif (self.dig.eshape == None or self.dig.lshape ==None):\n\t\t\traise Exception(""Ain\'t got no fucking shape."")\n\n\tdef BuildTrain(self, name_=""gdb9"",  append=False):\n\t\tself.CheckShapes()\n\t\tself.name=name_\n\t\ttotal_case = 0\n\t\tfor mi in range(len(self.set.mols)):\n\t\t\ttotal_case += len(self.set.mols[mi].mbe_frags[self.order])\n\t\tcases = np.zeros(tuple([total_case]+list(self.dig.eshape)))\n\t\tlabels = np.zeros(tuple([total_case]+list(self.dig.lshape)))\n\t\tcasep=0\n\t\tinsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_""+str(self.order)+""_in.npy""\n\t\toutsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_""+str(self.order)+""_out.npy""\n\t\tif self.type==""frag"":\n\t\t\tfor mi in range(len(self.set.mols)):\n\t\t\t\tfor frag in self.set.mols[mi].mbe_frags[self.order]:\n\t\t\t\t\t#print  frag.dist[0], frag.frag_mbe_energy\n\t\t\t\t\tins,outs = self.dig.TrainDigest(frag)\n\t\t\t\t\tcases[casep:casep+1] += ins\n\t\t\t\t\tlabels[casep:casep+1] += outs\n\t\t\t\t\tcasep += 1\n\t\telif self.type==""mol"":\n\t\t\tfor mi in range(len(self.set.mols)):\n\t\t\t\tif (mi%10000==0):\n\t\t\t\t\tLOGGER.debug(""Mol: ""+str(mi))\n\t\t\t\tins,outs = self.dig.TrainDigest(mi)\n\t\t\t\tcases[casep:casep+1] += ins\n\t\t\t\tlabels[casep:casep+1] += outs\n\t\t\t\tcasep += 1\n\t\telse:\n\t\t\traise Exception(""Unknown Type"")\n\t\talreadyexists = (os.path.isfile(insname) and os.path.isfile(outsname))\n\t\tif (append and alreadyexists):\n\t\t\tti=None\n\t\t\tto=None\n\t\t\tinf = open(insname,""rb"")\n\t\t\touf = open(outsname,""rb"")\n\t\t\tti = np.load(inf)\n\t\t\tto = np.load(ouf)\n\t\t\tinf.close()\n\t\t\touf.close()\n\t\t\tcases = np.concatenate((cases[:casep],ti))\n\t\t\tlabels = np.concatenate((labels[:casep],to))\n\t\t\tinf = open(insname,""wb"")\n\t\t\touf = open(outsname,""wb"")\n\t\t\tnp.save(inf,cases)\n\t\t\tnp.save(ouf,labels)\n\t\t\tinf.close()\n\t\t\touf.close()\n\t\t\tself.AvailableDataFiles.append([insname,outsname])\n\t\t\t#self.SamplesPerElement.append(casep*self.dig.NTrainSamples)\n\t\telse:\n\t\t\tinf = open(insname,""wb"")\n\t\t\touf = open(outsname,""wb"")\n\t\t\tnp.save(inf,cases[:casep])\n\t\t\tnp.save(ouf,labels[:casep])\n\t\t\tinf.close()\n\t\t\touf.close()\n\t\t\tself.AvailableDataFiles.append([insname,outsname])\n\t\t\t#self.SamplesPerElement.append(casep*self.dig.NTrainSamples)\n\t\tself.Save() #write a convenience pickle.\n\t\treturn\n\n\tdef RawBatch(self,nmol = 4096):\n\t\t""""""\n\t\t\tShimmy Shimmy Ya Shimmy Ya Shimmy Yay.\n\t\t\tThis type of batch is not built beforehand\n\t\t\tbecause there\'s no real digestion involved.\n\n\t\t\tArgs:\n\t\t\t\tnmol: number of molecules to put in the output.\n\n\t\t\tReturns:\n\t\t\t\tIns: a #atomsX4 tensor (AtNum,x,y,z)\n\t\t\t\tOuts: output of the digester\n\t\t\t\tKeys: (nmol)X(MaxNAtoms) tensor listing each molecule\'s place in the input.\n\t\t""""""\n\t\tndone = 0\n\t\tnatdone = 0\n\t\tself.MaxNAtoms = self.set.MaxNAtoms()\n\t\tIns = np.zeros(tuple([nmol,self.MaxNAtoms,4]))\n\t\tOuts = np.zeros(tuple([nmol,self.MaxNAtoms,3]))\n\t\twhile (ndone<nmol):\n\t\t\ttry:\n\t\t\t\tm = next(self.raw_it)\n#\t\t\t\tprint ""m props"", m.properties.keys()\n#\t\t\t\tprint ""m coords"", m.coords\n\t\t\t\tti, to = self.dig.Emb(m, True, False)\n\t\t\t\tn=ti.shape[0]\n\n\t\t\t\tIns[ndone,:n,:] = ti.copy()\n\t\t\t\tOuts[ndone,:n,:] = to.copy()\n\t\t\t\tndone += 1\n\t\t\t\tnatdone += n\n\t\t\texcept StopIteration:\n\t\t\t\tself.raw_it = iter(self.set.mols)\n\t\treturn Ins,Outs\n\n\tdef GetTrainBatch(self,ncases=1280,random=False):\n\t\tif (self.ScratchState != self.order):\n\t\t\tself.LoadDataToScratch()\n\t\tif (ncases>self.NTrain):\n\t\t\traise Exception(""Training Data is less than the batchsize... :( "")\n\t\tif ( self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0 #Sloppy.\n\t\ttmp=(self.scratch_inputs[self.ScratchPointer:self.ScratchPointer+ncases], self.scratch_outputs[self.ScratchPointer:self.ScratchPointer+ncases])\n\t\tself.ScratchPointer += ncases\n\t\treturn tmp\n\n\tdef GetTestBatch(self,ncases=1280, ministep = 0):\n\t\tif (ncases>self.NTest):\n\t\t\traise Exception(""Test Data is less than the batchsize... :( "")\n\t\treturn (self.scratch_test_inputs[ncases*(ministep):ncases*(ministep+1)], self.scratch_test_outputs[ncases*(ministep):ncases*(ministep+1)])\n\n\tdef Randomize(self, ti, to, group):\n\t\tti = ti.reshape((ti.shape[0]/group, group, -1))\n\t\tto = to.reshape((to.shape[0]/group, group, -1))\n\t\trandom.seed(0)\n\t\tidx = np.random.permutation(ti.shape[0])\n\t\tti = ti[idx]\n\t\tto = to[idx]\n\t\tti = ti.reshape((ti.shape[0]*ti.shape[1],-1))\n\t\tto = to.reshape((to.shape[0]*to.shape[1],-1))\n\t\treturn ti, to\n\n\tdef LoadData(self, random=False):\n\t\tinsname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_""+str(self.order)+""_in.npy""\n\t\toutsname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_""+str(self.order)+""_out.npy""\n\t\tinf = open(insname,""rb"")\n\t\touf = open(outsname,""rb"")\n\t\tti = np.load(inf)\n\t\tto = np.load(ouf)\n\t\tinf.close()\n\t\touf.close()\n\t\tif (ti.shape[0] != to.shape[0]):\n\t\t\traise Exception(""Bad Training Data."")\n\t\tti = ti.reshape((ti.shape[0],-1))  # flat data to [ncase, num_per_case]\n\t\tto = to.reshape((to.shape[0],-1))  # flat labels to [ncase, 1]\n\t\tgroup = 1\n\t\ttmp = 1\n\t\tfor i in range (1, self.order+1):\n\t\t\tgroup = group*i\n\t\tfor i in range (1, self.num_indis+1):\n\t\t\ttmp = tmp*i\n\t\tgroup = group*(tmp**self.order)\n\t\tprint(""randomize group:"", group)\n\t\tif (random):\n\t\t\tti, to = self.Randomize(ti, to, group)\n\t\tself.NTrain = to.shape[0]\n\t\treturn ti, to\n\n\tdef KRR(self):\n\t\tfrom sklearn.kernel_ridge import KernelRidge\n\t\tti, to = self.LoadData(True)\n\t\tprint(""KRR: input shape"", ti.shape, "" output shape"", to.shape)\n\t\t#krr = KernelRidge()\n\t\tkrr = KernelRidge(alpha=0.0001, kernel=\'rbf\')\n\t\ttrainsize = int(ti.shape[0]*0.5)\n\t\tkrr.fit(ti[0:trainsize,:], to[0:trainsize])\n\t\tpredict  = krr.predict(ti[trainsize:, : ])\n\t\tprint(predict.shape)\n\t\tkrr_acc_pred  = np.zeros((predict.shape[0],2))\n\t\tkrr_acc_pred[:,0] = to[trainsize:].reshape(to[trainsize:].shape[0])\n\t\tkrr_acc_pred[:,1] = predict.reshape(predict.shape[0])\n\t\tnp.savetxt(""krr_acc_pred.dat"", krr_acc_pred)\n\t\tprint(""KRR train R^2:"", krr.score(ti[0:trainsize, : ], to[0:trainsize]))\n\t\tprint(""KRR test  R^2:"", krr.score(ti[trainsize:, : ], to[trainsize:]))\n\t\treturn\n\n\tdef LoadDataToScratch(self, tformer, random=True):\n\t\tti, to = self.LoadData( random)\n\t\tif (tformer.innorm != None):\n\t\t\tti = tformer.NormalizeIns(ti)\n\t\tif (tformer.outnorm != None):\n\t\t\tto = tformer.NormalizeOuts(to)\n\t\tself.NTest = int(self.TestRatio * ti.shape[0])\n\t\tself.NTrain = int(ti.shape[0]-self.NTest)\n\t\tself.scratch_inputs = ti[:ti.shape[0]-self.NTest]\n\t\tself.scratch_outputs = to[:ti.shape[0]-self.NTest]\n\t\tself.scratch_test_inputs = ti[ti.shape[0]-self.NTest:]\n\t\tself.scratch_test_outputs = to[ti.shape[0]-self.NTest:]\n\t\tif random==True:\n\t\t\tself.scratch_inputs, self.scratch_outputs = self.Randomize(self.scratch_inputs, self.scratch_outputs, 1)\n\t\t\tself.scratch_test_inputs, self.scratch_test_outputs = self.Randomize(self.scratch_test_inputs, self.scratch_test_outputs, 1)\n\t\tself.ScratchState = self.order\n\t\tself.ScratchPointer=0\n\t\t#\n\t\t# Also get the relevant Normalizations of input, output\n\t\t# and average stoichiometries, etc.\n\t\t#\n\t\treturn\n\n\tdef PrintSampleInformation(self):\n\t\tprint(""From files: "", self.AvailableDataFiles)\n\t\treturn\n\n\tdef Save(self):\n\t\tself.CleanScratch()\n\t\tf=open(self.path+self.name+""_""+self.dig.name+""_""+str(self.order)+"".tdt"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\n\tdef EvaluateTestBatch(self, desired, predicted, tformer, nmols_=100):\n\t\tif (tformer.outnorm != None):\n\t\t\tdesired = tformer.UnNormalizeOuts(desired)\n\t\t\tpredicted = tformer.UnNormalizeOuts(predicted)\n\t\tLOGGER.info(""desired.shape ""+str(desired.shape)+"" predicted.shape ""+str(predicted.shape)+"" nmols ""+str(nmols_))\n\t\tLOGGER.info(""Evaluating, ""+str(len(desired))+"" predictions... "")\n\t\tif (self.dig.OType==""GoEnergy"" or self.dig.OType == ""Energy"" or self.dig.OType == ""AtomizationEnergy""):\n\t\t\tpredicted=predicted.flatten()[:nmols_]\n\t\t\tdesired=desired.flatten()[:nmols_]\n\t\t\tLOGGER.info( ""NCases: ""+str(len(desired)))\n\t\t\t#LOGGER.info( ""Mean Energy ""+str(self.unscld(desired)))\n\t\t\t#LOGGER.info( ""Mean Predicted Energy ""+str(self.unscld(predicted)))\n\t\t\tfor i in range(min(50,nmols_)):\n\t\t\t\tLOGGER.info( ""Desired: ""+str(i)+"" ""+str(desired[i])+"" Predicted ""+str(predicted[i]))\n\t\t\tLOGGER.info(""MAE ""+str(np.average(np.abs(desired-predicted))))\n\t\t\tLOGGER.info(""STD ""+str(np.std(desired-predicted)))\n\t\telse:\n\t\t\traise Exception(""Unknown Digester Output Type."")\n\t\treturn\n\nclass TensorMolDataDirect:\n\t""""""\n\tA tensordata class for direct embeddings evaluated during training\n\n\tArgs:\n\t\tmolecule_set (TensorMol.MSet): A set containing molecules and the properties used for training\n\t\tlearning_target (str): Learning target used as the labels for an instance class\n\n\tNotes:\n\t\tCurrently only implimented with atomization_energy as the learning target.\n\t""""""\n\tdef __init__(self, molecule_set, learning_target):\n\t\tself.molecule_set = molecule_set\n\t\tself.molecule_set_name = self.molecule_set.name\n\t\tself.learning_target = learning_target\n\t\tself.randomize_data = PARAMS[""RandomizeData""]\n\t\tself.test_ratio = PARAMS[""TestRatio""]\n\t\tself.elements = self.molecule_set.AtomTypes()\n\t\tself.max_num_atoms = self.molecule_set.MaxNAtoms() #Used to pad data so that each molecule is the same size\n\t\tself.num_molecules = len(self.molecule_set.mols)\n\t\tself.Ree_cut = PARAMS[""EECutoffOff""]\n\t\treturn\n\n\tdef clean_scratch(self):\n\t\tself.scratch_state = None\n\t\tself.scratch_pointer = 0 # for non random batch iteration.\n\t\tself.scratch_train_inputs = None\n\t\tself.scratch_train_outputs = None\n\t\tself.scratch_test_inputs = None # These should be partitioned out by LoadElementToScratch\n\t\tself.scratch_test_outputs = None\n\t\tself.molecule_set = None\n\t\tself.xyzs = None\n\t\tself.Zs = None\n\t\tself.labels = None\n\t\tself.grads = None\n\t\treturn\n\n\tdef reload_set(self):\n\t\t""""""\n\t\tRecalls the MSet to build training data etc.\n\t\t""""""\n\t\tself.molecule_set = MSet(self.molecule_set_name)\n\t\tself.molecule_set.Load()\n\t\treturn\n\n\tdef load_data(self):\n\t\tif (self.molecule_set == None):\n\t\t\ttry:\n\t\t\t\tself.reload_set()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""TensorData object has no molecule set."", Ex)\n\t\tif self.randomize_data:\n\t\t\trandom.shuffle(self.molecule_set.mols)\n\t\txyzs = np.zeros((self.num_molecules, self.max_num_atoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((self.num_molecules, self.max_num_atoms), dtype = np.int32)\n\t\tnum_atoms = np.zeros((self.num_molecules), dtype = np.int32)\n\t\tif self.learning_target == ""atomization"":\n\t\t\tenergies = np.zeros((self.num_molecules), dtype = np.float64)\n\t\t\tdipoles = np.zeros((self.num_molecules, 3), dtype = np.float64)\n\t\telse:\n\t\t\traise Exception(""TensorMolDataDirect currently only supports atomization energy for learning target"")\n\t\tgradients = np.zeros((self.num_molecules, self.max_num_atoms, 3), dtype=np.float64)\n\t\tfor i, mol in enumerate(self.molecule_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tenergies[i] = mol.properties[""atomization""]\n\t\t\tdipoles[i] = mol.properties[""dipole""]\n\t\t\tnum_atoms[i] = mol.NAtoms()\n\t\t\tgradients[i][:mol.NAtoms()] = mol.properties[""gradients""]\n\t\treturn xyzs, Zs, energies, dipoles, num_atoms, gradients\n\n\tdef load_data_to_scratch(self):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\tself.xyzs, self.Zs, self.energies, self.dipoles, self.num_atoms, self.gradients = self.load_data()\n\t\tself.num_test_cases = int(self.test_ratio * self.num_molecules)\n\t\tself.last_train_case = int(self.num_molecules - self.num_test_cases)\n\t\tself.num_train_cases = self.last_train_case\n\t\tself.test_scratch_pointer = self.last_train_case\n\t\tself.train_scratch_pointer = 0\n\t\tLOGGER.debug(""Number of training cases: %i"", self.num_train_cases)\n\t\tLOGGER.debug(""Number of test cases: %i"", self.num_test_cases)\n\t\treturn\n\n\tdef get_train_batch(self, batch_size):\n\t\tif batch_size > self.num_train_cases:\n\t\t\traise Exception(""Insufficent training data to fill a training batch.\\n""\\\n\t\t\t\t\t+str(self.num_train_cases)+"" cases in dataset with a batch size of ""+str(batch_size))\n\t\tif self.train_scratch_pointer + batch_size >= self.num_train_cases:\n\t\t\tself.train_scratch_pointer = 0\n\t\tself.train_scratch_pointer += batch_size\n\t\txyzs = self.xyzs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]\n\t\tZs = self.Zs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]\n\t\tenergies = self.energies[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]\n\t\tdipoles = self.dipoles[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]\n\t\tnum_atoms = self.num_atoms[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]\n\t\tgradients = self.gradients[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]\n\t\tNLEE = NeighborListSet(xyzs, num_atoms, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(self.Ree_cut)\n\t\treturn [xyzs, Zs, energies, dipoles, gradients, num_atoms, rad_eep]\n\n\tdef get_test_batch(self, batch_size):\n\t\tif batch_size > self.num_test_cases:\n\t\t\traise Exception(""Insufficent training data to fill a test batch.\\n""\\\n\t\t\t\t\t+str(self.num_test_cases)+"" cases in dataset with a batch size of ""+str(num_cases_batch))\n\t\tif self.test_scratch_pointer + batch_size >= self.num_train_cases:\n\t\t\tself.test_scratch_pointer = self.last_train_case\n\t\tself.test_scratch_pointer += batch_size\n\t\txyzs = self.xyzs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\tZs = self.Zs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\tenergies = self.energies[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\tdipoles = self.dipoles[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\tnum_atoms = self.num_atoms[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\tgradients = self.gradients[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\tNLEE = NeighborListSet(xyzs, num_atoms, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(self.Ree_cut)\n\t\treturn [xyzs, Zs, energies, dipoles, gradients, num_atoms, rad_eep]\n\n\tdef save(self):\n\t\tself.clean_scratch()\n\t\tf = open(self.path+self.name+"".tdt"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\nclass TensorMolData_BP(TensorMolData):\n\t""""""\n\t\t\tA tensordata for molecules and Behler-Parinello.\n\t\t\ta Case is an input to the NN.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tself.CaseMetadata=None # case X molecule index X element type (Strictly ascending)\n\t\tself.LastTrainMol=0\n\t\tself.NTestMols=0\n\t\tself.scratch_meta = None\n\t\tself.scratch_test_meta = None\n\t\tself.scratch_grads = None\n\t\tself.scratch_test_grads = None\n\t\tself.HasGrad = WithGrad_ # whether to pass around the gradient.\n\t\tTensorMolData.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_)\n\t\tself.eles = []\n\t\tif (MSet_ != None):\n\t\t\tself.eles = list(MSet_.AtomTypes())\n\t\t\tself.eles.sort()\n\t\tself.MeanStoich=None\n\t\tself.MeanNAtoms=None\n\t\tself.test_mols_done = False\n\t\tself.test_begin_mol  = None\n\t\tself.test_mols = []\n\t\tself.MaxN3 = None # The most coordinates in the set.\n\t\tprint(""TensorMolData_BP.eles"", self.eles)\n\t\tprint(""self.HasGrad:"", self.HasGrad)\n\t\tif (self.HasGrad):\n\t\t\tself.MaxN3 = 3*np.max([m.NAtoms() for m in self.set.mols])\n\t\t\tprint(""TensorMolData_BP.MaxN3"", self.MaxN3)\n\t\treturn\n\n\tdef CleanScratch(self):\n\t\tTensorData.CleanScratch(self)\n\t\tself.raw_it=None\n\t\tself.CaseMetadata=None # case X molecule index , element type , first atom in this mol, last atom in this mol (exclusive)\n\t\tself.scratch_meta = None\n\t\tself.scratch_test_meta = None\n\t\treturn\n\n\tdef BuildTrain(self, name_=""gdb9"",  append=False, max_nmols_=1000000, WithGrad_=False):\n\t\tif (WithGrad_ and self.dig.OType != ""AEAndForce""):\n\t\t\traise Exception(""Use to find forces.... "")\n\t\tself.CheckShapes()\n\t\tself.name=name_\n\t\tLOGGER.info(""TensorMolData, self.type:""+self.type)\n\t\tif self.type==""frag"":\n\t\t\traise Exception(""No BP frags now"")\n\t\tnmols  = len(self.set.mols)\n\t\tnatoms = self.set.NAtoms()\n\t\tLOGGER.info( ""self.dig.eshape""+str(self.dig.eshape)+"" self.dig.lshape""+str(self.dig.lshape))\n\t\tcases = np.zeros(tuple([natoms]+list(self.dig.eshape)))\n\t\tcasesg = None\n\t\tif (WithGrad_):\n\t\t\tcasesg = np.zeros(tuple([natoms]+list(self.dig.eshape)+[self.MaxN3]))\n\t\t\tself.HasGrad = True\n\t\telse:\n\t\t\tself.HasGrad = False\n\t\tLOGGER.info( ""cases:""+str(cases.shape))\n\t\tlabels = np.zeros(tuple([nmols]+list(self.dig.lshape)))\n\t\tif (WithGrad_):\n\t\t\t# Tediously if you have the gradient the lshape can\'t really be general....\n\t\t\t# We should figure out a more universal, differentiable way to do this.\n\t\t\tlabels = np.zeros(tuple([nmols]+[self.MaxN3+1]))\n\t\tself.CaseMetadata = np.zeros((natoms, 4), dtype = np.int)\n\t\tinsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_in.npy""\n\t\tingname = self.path+""Mol_""+name_+""_""+self.dig.name+""_ing.npy""\n\t\toutsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_out.npy""\n\t\tmetasname = self.path+""Mol_""+name_+""_""+self.dig.name+""_meta.npy"" # Used aggregate and properly sum network inputs and outputs.\n\t\tcasep=0\n\t\t# Generate the set in a random order.\n\t\tord=np.random.permutation(len(self.set.mols))\n\t\tmols_done = 0\n\t\tt = time.time()\n\t\tfor mi in ord:\n\t\t\tnat = self.set.mols[mi].NAtoms()\n\t\t\t#print ""casep:"", casep\n\t\t\tif (mols_done%1000==0):\n\t\t\t\tprint(""time cost:"", time.time() -t, "" second"")\n\t\t\t\tLOGGER.info(""Mol:""+str(mols_done))\n\t\t\t\tt = time.time()\n\t\t\tif (WithGrad_):\n\t\t\t\tins, grads, outs = self.dig.TrainDigest(self.set.mols[mi])\n\t\t\telse:\n\t\t\t\tins, outs = self.dig.TrainDigest(self.set.mols[mi])\n\t\t\tif not np.all(np.isfinite(ins)):\n\t\t\t\tprint(""find a bad case, writting down xyz.."")\n\t\t\t\tself.set.mols[mi].WriteXYZfile(fpath=""."", fname=""bad_buildset_cases"")\n\t\t\t#print mi, ins.shape, outs.shape\n\t\t\tcases[casep:casep+nat] = ins\n\t\t\tif (WithGrad_):\n\t\t\t\tif (grads.shape[2]%3 != 0):\n\t\t\t\t\traise Exception(""Bad Deriv."")\n\t\t\t\tcasesg[casep:casep+nat,:,:grads.shape[2]] = grads # grads are atomXdesc dimX R\n\t\t\t\tlabels[mols_done,:outs.shape[0]] = outs\n\t\t\telse:\n\t\t\t\tlabels[mols_done] = outs\n\t\t\tfor j in range(casep,casep+nat):\n\t\t\t\tself.CaseMetadata[j,0] = mols_done\n\t\t\t\tself.CaseMetadata[j,1] = self.set.mols[mi].atoms[j-casep]\n\t\t\t\tself.CaseMetadata[j,2] = casep\n\t\t\t\tself.CaseMetadata[j,3] = casep+nat\n\t\t\tcasep += nat\n\t\t\tmols_done = mols_done + 1\n\t\t\tif (mols_done>=max_nmols_):\n\t\t\t\tbreak\n\t\tinf = open(insname,""wb"")\n\t\touf = open(outsname,""wb"")\n\t\tmef = open(metasname,""wb"")\n\t\tif (WithGrad_):\n\t\t\tingf = open(ingname,""wb"")\n\t\t\tnp.save(ingf,casesg[:casep,:])\n\t\t\tingf.close()\n\t\t\tself.AvailableDataFiles.append([ingname])\n\t\tnp.save(inf,cases[:casep,:])\n\t\tnp.save(ouf,labels[:mols_done,:])\n\t\tnp.save(mef,self.CaseMetadata[:casep,:])\n\t\tinf.close()\n\t\touf.close()\n\t\tmef.close()\n\t\tself.AvailableDataFiles.append([insname,outsname,metasname])\n\t\tself.Save() #write a convenience pickle.\n\t\treturn\n\n\tdef LoadData(self):\n\t\tinsname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_in.npy""\n\t\tingname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_ing.npy""\n\t\toutsname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_out.npy""\n\t\tmetasname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_meta.npy"" # Used aggregate\n\t\tinf = open(insname,""rb"")\n\t\touf = open(outsname,""rb"")\n\t\tmef = open(metasname,""rb"")\n\t\tti = np.load(inf)\n\t\tto = np.load(ouf)\n\t\ttm = np.load(mef)\n\t\tinf.close()\n\t\touf.close()\n\t\tmef.close()\n\t\tto = to.reshape((to.shape[0],-1))  # flat labels to [mol, 1]\n\t\tif (os.path.isfile(ingname) and self.HasGrad):\n\t\t\ting = open(ingname,""rb"")\n\t\t\ttig = np.load(ing)\n\t\t\ting.close()\n\t\t\treturn ti, tig, to, tm\n\t\treturn ti, to, tm\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\ttry:\n\t\t\tself.HasGrad\n\t\texcept:\n\t\t\tself.HasGrad = False\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tif (self.HasGrad):\n\t\t\tti, tig, to, tm = self.LoadData()\n\t\telse:\n\t\t\tti, to, tm = self.LoadData()\n\t\tif (tformer.innorm != None):\n\t\t\tti = tformer.NormalizeIns(ti)\n\t\tif (tformer.outnorm != None):\n\t\t\tto = tformer.NormalizeOuts(to)\n\t\tself.NTestMols = int(self.TestRatio * to.shape[0])\n\t\tself.LastTrainMol = int(to.shape[0]-self.NTestMols)\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\tLOGGER.debug(""Number of molecules in meta:: %i"", tm[-1,0]+1)\n\t\tLastTrainCase=0\n\t\tfor i in range(len(tm)):\n\t\t\tif (tm[i,0] == self.LastTrainMol):\n\t\t\t\tLastTrainCase = tm[i,2] # exclusive\n\t\t\t\tbreak\n\t\tLOGGER.debug(""last train atom: %i"",LastTrainCase)\n\t\tLOGGER.debug(""Num Test atoms: %i"",len(tm)-LastTrainCase)\n\t\tLOGGER.debug(""Num atoms: %i"",len(tm))\n\t\tself.NTrain = LastTrainCase\n\t\tself.NTest = len(tm)-LastTrainCase\n\t\tself.scratch_inputs = ti[:LastTrainCase]\n\t\tself.scratch_outputs = to[:self.LastTrainMol]\n\t\tself.scratch_meta = tm[:LastTrainCase]\n\t\tself.scratch_test_inputs = ti[LastTrainCase:]\n\t\tself.scratch_test_outputs = to[self.LastTrainMol:]\n\t\tif (self.HasGrad):\n\t\t\tself.scratch_grads = tig[:LastTrainCase]\n\t\t\tself.scratch_test_grads = tig[LastTrainCase:]\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t# these columns need to be shifted.\n\t\tself.scratch_test_meta = tm[LastTrainCase:]\n\t\tself.test_begin_mol = self.scratch_test_meta[0,0]\n\t\t#\t\tprint ""before shift case  "", tm[LastTrainCase:LastTrainCase+30], ""real"", self.set.mols[tm[LastTrainCase, 0]].bonds, self.set.mols[self.test_begin_mol].bonds\n\t\tself.scratch_test_meta[:,0] -= self.scratch_test_meta[0,0]\n\t\tself.scratch_test_meta[:,3] -= self.scratch_test_meta[0,2]\n\t\tself.scratch_test_meta[:,2] -= self.scratch_test_meta[0,2]\n\t\tself.ScratchState = 1\n\t\tself.ScratchPointer = 0\n\t\tself.test_ScratchPointer=0\n\t\t# Compute mean Stoichiometry and number of atoms.\n\t\tself.eles = np.unique(tm[:,1]).tolist()\n\t\tatomcount = np.zeros(len(self.eles))\n\t\tself.MeanStoich = np.zeros(len(self.eles))\n\t\tfor j in range(len(self.eles)):\n\t\t\tfor i in range(len(ti)):\n\t\t\t\tif (tm[i,1]==self.eles[j]):\n\t\t\t\t\tatomcount[j]=atomcount[j]+1\n\t\tself.MeanStoich=atomcount/len(to)\n\t\tself.MeanNumAtoms = np.sum(self.MeanStoich)\n\t\treturn\n\n\tdef GetTrainBatch(self,ncases,noutputs):\n\t\t""""""\n\t\tConstruct the data required for a training batch Returns inputs (sorted by element), and indexing matrices and outputs.\n\t\tBehler parinello batches need to have a typical overall stoichiometry.\n\t\tand a constant number of atoms, and must contain an integer number of molecules.\n\t\tBesides making sure all of that takes place this routine makes the summation matrices\n\t\twhich map the cases => molecular energies in the Neural Network output.\n\n\t\tArgs:\n\t\t\tncases: the size of a training cases.\n\t\t\tnoutputs: the maximum number of molecule energies which can be produced.\n\n\t\tReturns:\n\t\t\tA an **ordered** list of length self.eles containing\n\t\t\t\ta list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\t\ta list of (num_of atom type X batchsize) matrices which linearly combines the elements\n\t\t\t\ta list of outputs.\n\t\t""""""\n\t\tstart_time = time.time()\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tinputs = []\n\t\tinputgs = []\n\t\tmatrices = []\n\t\toffsets = []\n\t\t# Get the number of molecules which would be contained in the desired batch size\n\t\t# and the number of element cases.\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\tbmols=np.unique(self.scratch_meta[self.ScratchPointer:self.ScratchPointer+ncases,0])\n\t\tnmols_out=len(bmols[1:-1])\n\t\tif (nmols_out > noutputs):\n\t\t\traise Exception(""Insufficent Padding. ""+str(nmols_out)+"" is greater than ""+str(noutputs))\n\t\tinputpointer = 0\n\t\toutputpointer = 0\n\t\t#currentmol=self.scratch_meta[self.ScratchPointer,0]\n\t\tsto = np.zeros(len(self.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.eles),dtype = np.int32) # output pointers within each element block.\n\t\tdestinations = np.zeros(ncases) # The index in the output of each case in the scratch.\n\t\tignore_first_mol = 0\n\t\tfor i in range(self.ScratchPointer,self.ScratchPointer+ncases):\n\t\t\tif (self.scratch_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\telif (self.scratch_meta[i,0] == bmols[0]):\n\t\t\t\tignore_first_mol += 1\n\t\t\telse:\n\t\t\t\tsto[self.eles.index(self.scratch_meta[i,1])]+=1\n\t\tcurrentmol=self.scratch_meta[self.ScratchPointer+ignore_first_mol,0]\n\t\toutputs = None\n\t\tif (self.HasGrad):\n\t\t\toutputs = np.zeros((noutputs,self.dig.lshape[0]))\n\t\telse:\n\t\t\toutputs = np.zeros((noutputs))\n\t\tfor e in range(len(self.eles)):\n\t\t\tinputs.append(np.zeros((sto[e],np.prod(self.dig.eshape))))\n\t\t\tif (self.HasGrad):\n\t\t\t\tinputgs.append(np.zeros((sto[e],np.prod(self.dig.eshape),self.MaxN3)))\n\t\t\tmatrices.append(np.zeros((sto[e],noutputs)))\n\t\tfor i in range(self.ScratchPointer+ignore_first_mol, self.ScratchPointer+ncases):\n\t\t\tif (self.scratch_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\tif (currentmol != self.scratch_meta[i,0]):\n\t\t\t\toutputpointer = outputpointer+1\n\t\t\t\tcurrentmol = self.scratch_meta[i,0]\n\t\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t\te = (self.scratch_meta[i,1])\n\t\t\tei = self.eles.index(e)\n\t\t\t# The offset for this element should be within the bounds or something is wrong...\n\t\t\tinputs[ei][offsets[ei],:] = self.scratch_inputs[i]\n\t\t\tif (self.HasGrad):\n\t\t\t\tinputgs[ei][offsets[ei],:] = self.scratch_grads[i]\n\t\t\tmatrices[ei][offsets[ei],outputpointer] = 1.0\n\t\t\toutputs[outputpointer] = self.scratch_outputs[self.scratch_meta[i,0]]\n\t\t\toffsets[ei] += 1\n\t\t#print ""inputs"",inputs\n\t\t#print ""bounds"",bounds\n\t\t#print ""matrices"",matrices\n\t\t#print ""outputs"",outputs\n\t\tself.ScratchPointer += ncases\n\t\tif (self.HasGrad):\n\t\t\t#print ""inputs: "", inputs[0].shape, "" inputgs:"", inputgs[0], inputgs[0].shape, "" outputs"", outputs.shape, "" matrices"", matrices.shape\n\t\t\treturn [inputs, inputgs, matrices, outputs]\n\t\telse:\n\t\t\treturn [inputs, matrices, outputs]\n\n\tdef GetTestBatch(self,ncases,noutputs):\n\t\t""""""\n\t\tReturns:\n\t\tA an **ordered** list of length self.eles containing\n\t\t\ta list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\ta list of (num_of atom type X batchsize) matrices which linearly combines the elements\n\t\t\ta list of outputs.\n\t\t\tthe number of output molecules.\n\t\t""""""\n\t\tstart_time = time.time()\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases >= self.NTest):\n\t\t\tself.test_ScratchPointer = 0\n\t\t\tself.test_mols_done = True\n\t\tinputs = []\n\t\tinputgs = []\n\t\tmatrices = []\n\t\toffsets= []\n\t\t# Get the number of molecules which would be contained in the desired batch size\n\t\t# and the number of element cases.\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\tbmols=np.unique(self.scratch_test_meta[self.test_ScratchPointer:self.test_ScratchPointer+ncases,0])\n\t\tnmols_out=len(bmols[1:-1])\n\t\t#print ""batch contains"",nmols_out, ""Molecules in "",ncases\n\t\tif (nmols_out > noutputs):\n\t\t\traise Exception(""Insufficent Padding. ""+str(nmols_out)+"" is greater than ""+str(noutputs))\n\t\tinputpointer = 0\n\t\toutputpointer = 0\n\t\t#currentmol=self.scratch_meta[self.ScratchPointer,0]\n\t\tsto = np.zeros(len(self.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.eles),dtype = np.int32) # output pointers within each element block.\n\t\tdestinations = np.zeros(ncases) # The index in the output of each case in the scratch.\n\t\tignore_first_mol = 0\n\t\tfor i in range(self.test_ScratchPointer,self.test_ScratchPointer+ncases):\n\t\t\tif (self.scratch_test_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\telif (self.scratch_test_meta[i,0] == bmols[0]):\n\t\t\t\tignore_first_mol += 1\n\t\t\telse:\n\t\t\t\tsto[self.eles.index(self.scratch_test_meta[i,1])]+=1\n\t\tcurrentmol=self.scratch_test_meta[self.test_ScratchPointer+ignore_first_mol,0]\n\t\toutputs = None\n\t\tif (self.HasGrad):\n\t\t\toutputs = np.zeros((noutputs,self.dig.lshape[0]))\n\t\telse:\n\t\t\toutputs = np.zeros((noutputs))\n\t\tfor e in range(len(self.eles)):\n\t\t\tinputs.append(np.zeros((sto[e],np.prod(self.dig.eshape))))\n\t\t\tif (self.HasGrad):\n\t\t\t\tinputgs.append(np.zeros((sto[e],np.prod(self.dig.eshape),self.MaxN3)))\n\t\t\tmatrices.append(np.zeros((sto[e],noutputs)))\n\t\tfor i in range(self.test_ScratchPointer+ignore_first_mol, self.test_ScratchPointer+ncases):\n\t\t\tif (self.scratch_test_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\tif (currentmol != self.scratch_test_meta[i,0]):\n\t\t\t\toutputpointer = outputpointer+1\n\t\t\t\tcurrentmol = self.scratch_test_meta[i,0]\n\t\t\tif not self.test_mols_done and self.test_begin_mol+currentmol not in self.test_mols:\n\t\t\t\t\tself.test_mols.append(self.test_begin_mol+currentmol)\n\t\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t\te = (self.scratch_test_meta[i,1])\n\t\t\tei = self.eles.index(e)\n\t\t\t# The offset for this element should be within the bounds or something is wrong...\n\t\t\tinputs[ei][offsets[ei],:] = self.scratch_test_inputs[i]\n\t\t\tif (self.HasGrad):\n\t\t\t\tinputgs[ei][offsets[ei],:] = self.scratch_test_grads[i]\n\t\t\tmatrices[ei][offsets[ei],outputpointer] = 1.0\n\t\t\toutputs[outputpointer] = self.scratch_test_outputs[self.scratch_test_meta[i,0]]\n\t\t\toffsets[ei] += 1\n\t\tself.test_ScratchPointer += ncases\n\t\tif (self.HasGrad):\n\t\t\treturn [inputs, inputgs, matrices, outputs]\n\t\telse:\n\t\t\treturn [inputs, matrices, outputs]\n\n\tdef PrintStatus(self):\n\t\tprint(""self.ScratchState"",self.ScratchState)\n\t\tprint(""self.ScratchPointer"",self.ScratchPointer)\n\t\t#print ""self.test_ScratchPointer"",self.test_ScratchPointer\n\n\n\tdef Init_TraceBack(self):\n\t\tnum_eles = [0 for ele in self.eles]\n\t\tfor mol_index in self.test_mols:\n\t\t\tfor ele in list(self.set.mols[mol_index].atoms):\n\t\t\t\tnum_eles[self.eles.index(ele)] += 1\n\t\tself.test_atom_index = [np.zeros((num_eles[i],2), dtype = np.int) for i in range (0, len(self.eles))]\n\n\t\tpointer = [0 for ele in self.eles]\n\t\tfor mol_index in self.test_mols:\n\t\t\tmol = self.set.mols[mol_index]\n\t\t\tfor i in range (0, mol.atoms.shape[0]):\n\t\t\t\tatom_type = mol.atoms[i]\n\t\t\t\tself.test_atom_index[self.eles.index(atom_type)][pointer[self.eles.index(atom_type)]] = [int(mol_index), i]\n\t\t\t\tpointer[self.eles.index(atom_type)] += 1\n\t\tprint(self.test_atom_index)\n\t\tf  = open(""test_energy_real_atom_index_for_test.dat"",""wb"")\n\t\tpickle.dump(self.test_atom_index, f)\n\t\tf.close()\n\t\treturn\n\n\tdef Save(self):\n\t\tself.CleanScratch()\n\t\tf=open(self.path+self.name+""_""+self.dig.name+"".tdt"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\n\nclass TensorMolData_Bond_BP(TensorMolData_BP):\n\t""""""\n\tA tensordata for molecules and Bond-wise Behler-Parinello.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol""):\n\t\tTensorMolData_BP.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_)\n\t\tself.eles = list(self.set.BondTypes())\n\t\tself.eles.sort()\n\t\t#self. = self.set.\n\t\t#self.bonds = list(self.set.BondTypes())\n\t\t#self.bonds.sort()\n\t\treturn\n\n\tdef CleanScratch(self):\n\t\tTensorData.CleanScratch(self)\n\t\tself.CaseMetadata=None # case X molecule index , element type , first atom in this mol, last atom in this mol (exclusive)\n\t\tself.scratch_meta = None\n\t\tself.scratch_test_meta = None\n\t\treturn\n\n\tdef BuildTrain(self, name_=""gdb9"",  append=False, max_nmols_=1000000):\n\t\tself.CheckShapes()\n\t\tself.name=name_\n\t\tprint(""self.type:"", self.type)\n\t\tif self.type==""frag"":\n\t\t\traise Exception(""No BP frags now"")\n\t\tnmols  = len(self.set.mols)\n\t\tnbonds = self.set.NBonds()\n\t\tprint(""self.dig.eshape"", self.dig.eshape, "" self.dig.lshape"", self.dig.lshape)\n\t\tcases = np.zeros(tuple([nbonds]+list(self.dig.eshape)))\n\t\tprint(""cases:"", cases.shape)\n\t\tlabels = np.zeros(tuple([nmols]+list(self.dig.lshape)))\n\t\tself.CaseMetadata = np.zeros((nbonds, 4), dtype = np.int)\n\t\tinsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_in.npy""\n\t\toutsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_out.npy""\n\t\tmetasname = self.path+""Mol_""+name_+""_""+self.dig.name+""_meta.npy"" # Used aggregate and properly sum network inputs and outputs.\n\t\tcasep=0\n\t\t# Generate the set in a random order.\n\t\t#ord = range (0, len(self.set.mols))  # debug\n\t\tord=np.random.permutation(len(self.set.mols))\n\t\tmols_done = 0\n\t\tfor mi in ord:\n\t\t\tnbo = self.set.mols[mi].NBonds()\n\t\t\tif (mi == 0 or mi == 1):\n\t\t\t\tprint(""name of the first/second mol:"", self.set.mols[mi].name)\n\t\t\t#print ""casep:"", casep\n\t\t\tif (mols_done%1000==0):\n\t\t\t\tprint(""Mol:"", mols_done)\n\t\t\tins,outs = self.dig.TrainDigest(self.set.mols[mi])\n\t\t\t#print mi, ins.shape, outs.shape\n\t\t\tcases[casep:casep+nbo] = ins\n\t\t\t#if (self.set.mols[mi].name == ""Comment: c60""):\n\t\t\t#\tnp.savetxt(""c60_in.dat"", ins)\n\t\t\t#print ""ins:"", ins, "" cases:"", cases[casep:casep+nat]\n\t\t\tlabels[mols_done] = outs\n\t\t\tfor j in range(casep,casep+nbo):\n\t\t\t\tself.CaseMetadata[j,0] = mols_done\n\t\t\t\tself.CaseMetadata[j,1] = self.set.mols[mi].bonds[j-casep, 0]\n\t\t\t\tself.CaseMetadata[j,2] = casep\n\t\t\t\tself.CaseMetadata[j,3] = casep+nbo\n\t\t\tcasep += nbo\n\t\t\tmols_done = mols_done + 1\n\t\t\tif (mols_done>=max_nmols_):\n\t\t\t\tbreak\n\t\tinf = open(insname,""wb"")\n\t\touf = open(outsname,""wb"")\n\t\tmef = open(metasname,""wb"")\n\t\tnp.save(inf,cases[:casep,:])\n\t\tnp.save(ouf,labels[:mols_done,:])\n\t\tnp.save(mef,self.CaseMetadata[:casep,:])\n\t\tinf.close()\n\t\touf.close()\n\t\tmef.close()\n\t\tself.AvailableDataFiles.append([insname,outsname,metasname])\n\t\tself.Save() #write a convenience pickle.\n\t\treturn\n\n\tdef Init_TraceBack(self):\n\t\tnum_eles = [0 for ele in self.eles]\n\t\tfor mol_index in self.test_mols:\n\t\t\tfor ele in list(self.set.mols[mol_index].bonds[:,0]):\n\t\t\t\tnum_eles[self.eles.index(ele)] += 1\n\t\tself.test_atom_index = [np.zeros((num_eles[i],2), dtype = np.int) for i in range (0, len(self.eles))]\n\t\tpointer = [0 for ele in self.eles]\n\t\tfor mol_index in self.test_mols:\n\t\t\tmol = self.set.mols[mol_index]\n\t\t\tfor i in range (0, mol.bonds.shape[0]):\n\t\t\t\tbond_type = mol.bonds[i,0]\n\t\t\t\tself.test_atom_index[self.eles.index(bond_type)][pointer[self.eles.index(bond_type)]] = [int(mol_index), i]\n\t\t\t\tpointer[self.eles.index(bond_type)] += 1\n\t\tprint(self.test_atom_index)\n\t\tf  = open(""test_energy_bond_index_for_test.dat"",""wb"")\n\t\tpickle.dump(self.test_atom_index, f)\n\t\tf.close()\n\t\treturn\n\n\nclass TensorMolData_BP_Update(TensorMolData_BP):\n\t""""""\n\t\t\tA update version of tensordata for molecules and Behler-Parinello.\n\t\t\ta Case is an input to the NN.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_)\n\t\treturn\n\n\n\tdef GetTrainBatch(self,ncases,noutputs):\n\t\t""""""\n\t\tConstruct the data required for a training batch Returns inputs (sorted by element), and indexing matrices and outputs.\n\t\tBehler parinello batches need to have a typical overall stoichiometry.\n\t\tand a constant number of atoms, and must contain an integer number of molecules.\n\t\tBesides making sure all of that takes place this routine makes the summation matrices\n\t\twhich map the cases => molecular energies in the Neural Network output.\n\n\t\tArgs:\n\t\t\tncases: the size of a training cases.\n\t\t\tnoutputs: the maximum number of molecule energies which can be produced.\n\n\t\tReturns:\n\t\t\tA an **ordered** list of length self.eles containing\n\t\t\t\ta list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\t\ta list of (num_of atom type X batchsize) array  which linearly combines the elements\n\t\t\t\ta list of outputs.\n\t\t""""""\n\t\tstart_time = time.time()\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tinputs = []\n\t\tinputgs = []\n\t\tatom_mol_index = [] # mol index of each atom\n\t\toffsets=[]\n\t\t# Get the number of molecules which would be contained in the desired batch size\n\t\t# and the number of element cases.\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\tbmols=np.unique(self.scratch_meta[self.ScratchPointer:self.ScratchPointer+ncases,0])\n\t\tnmols_out=len(bmols[1:-1])\n\t\tif (nmols_out > noutputs):\n\t\t\traise Exception(""Insufficent Padding. ""+str(nmols_out)+"" is greater than ""+str(noutputs))\n\t\tinputpointer = 0\n\t\toutputpointer = 0\n\t\t#currentmol=self.scratch_meta[self.ScratchPointer,0]\n\t\tsto = np.zeros(len(self.eles),dtype = np.int64)\n\t\toffsets = np.zeros(len(self.eles),dtype = np.int64) # output pointers within each element block.\n\t\tdestinations = np.zeros(ncases) # The index in the output of each case in the scratch.\n\t\tignore_first_mol = 0\n\t\tfor i in range(self.ScratchPointer,self.ScratchPointer+ncases):\n\t\t\tif (self.scratch_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\telif (self.scratch_meta[i,0] == bmols[0]):\n\t\t\t\tignore_first_mol += 1\n\t\t\telse:\n\t\t\t\tsto[self.eles.index(self.scratch_meta[i,1])]+=1\n\t\tcurrentmol=self.scratch_meta[self.ScratchPointer+ignore_first_mol,0]\n\t\toutputs = None\n\t\tif (self.HasGrad):\n\t\t\toutputs = np.zeros((noutputs,self.dig.lshape[0]))\n\t\telse:\n\t\t\toutputs = np.zeros((noutputs))\n\t\tfor e in range(len(self.eles)):\n\t\t\tinputs.append(np.zeros((sto[e],np.prod(self.dig.eshape))))\n\t\t\tif (self.HasGrad):\n\t\t\t\tinputgs.append(np.zeros((sto[e],np.prod(self.dig.eshape),self.MaxN3)))\n\t\t\tatom_mol_index.append(np.zeros((sto[e]), dtype=np.int64))\n\t\tfor i in range(self.ScratchPointer+ignore_first_mol, self.ScratchPointer+ncases):\n\t\t\tif (self.scratch_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\tif (currentmol != self.scratch_meta[i,0]):\n\t\t\t\toutputpointer = outputpointer+1\n\t\t\t\tcurrentmol = self.scratch_meta[i,0]\n\t\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t\te = (self.scratch_meta[i,1])\n\t\t\tei = self.eles.index(e)\n\t\t\t# The offset for this element should be within the bounds or something is wrong...\n\t\t\tinputs[ei][offsets[ei],:] = self.scratch_inputs[i]\n\t\t\tif (self.HasGrad):\n\t\t\t\tinputgs[ei][offsets[ei],:] = self.scratch_grads[i]\n\t\t\tatom_mol_index[ei][offsets[ei]] = outputpointer\n\t\t\toutputs[outputpointer] = self.scratch_outputs[self.scratch_meta[i,0]]\n\t\t\toffsets[ei] += 1\n\t\t#print ""inputs"",inputs\n\t\t#print ""bounds"",bounds\n\t\t#print ""matrices"",matrices\n\t\t#print ""outputs"",outputs\n\t\tself.ScratchPointer += ncases\n\t\tif (self.HasGrad):\n\t\t\t#print ""inputs: "", inputs[0].shape, "" inputgs:"", inputgs[0], inputgs[0].shape, "" outputs"", outputs.shape, "" matrices"", matrices.shape\n\t\t\treturn [inputs, inputgs, atom_mol_index, outputs]\n\t\telse:\n\t\t\treturn [inputs, atom_mol_index, outputs]\n\n\tdef GetTestBatch(self,ncases,noutputs):\n\t\t""""""\n\t\tReturns:\n\t\tA an **ordered** list of length self.eles containing\n\t\t\ta list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\ta list of (num_of atom type X batchsize) matrices which linearly combines the elements\n\t\t\ta list of outputs.\n\t\t\tthe number of output molecules.\n\t\t""""""\n\t\tstart_time = time.time()\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases >= self.NTest):\n\t\t\tself.test_ScratchPointer = 0\n\t\t\tself.test_mols_done = True\n\t\tinputs = []\n\t\tinputgs = []\n\t\tatom_mol_index = []#np.zeros((len(self.eles), ncases, noutputs))\n\t\toffsets= []\n\t\t# Get the number of molecules which would be contained in the desired batch size\n\t\t# and the number of element cases.\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\tbmols=np.unique(self.scratch_test_meta[self.test_ScratchPointer:self.test_ScratchPointer+ncases,0])\n\t\tnmols_out=len(bmols[1:-1])\n\t\t#print ""batch contains"",nmols_out, ""Molecules in "",ncases\n\t\tif (nmols_out > noutputs):\n\t\t\traise Exception(""Insufficent Padding. ""+str(nmols_out)+"" is greater than ""+str(noutputs))\n\t\tinputpointer = 0\n\t\toutputpointer = 0\n\t\t#currentmol=self.scratch_meta[self.ScratchPointer,0]\n\t\tsto = np.zeros(len(self.eles),dtype = np.int64)\n\t\toffsets = np.zeros(len(self.eles),dtype = np.int64) # output pointers within each element block.\n\t\tdestinations = np.zeros(ncases) # The index in the output of each case in the scratch.\n\t\tignore_first_mol = 0\n\t\tfor i in range(self.test_ScratchPointer,self.test_ScratchPointer+ncases):\n\t\t\tif (self.scratch_test_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\telif (self.scratch_test_meta[i,0] == bmols[0]):\n\t\t\t\tignore_first_mol += 1\n\t\t\telse:\n\t\t\t\tsto[self.eles.index(self.scratch_test_meta[i,1])]+=1\n\t\tcurrentmol=self.scratch_test_meta[self.test_ScratchPointer+ignore_first_mol,0]\n\t\toutputs = None\n\t\tif (self.HasGrad):\n\t\t\toutputs = np.zeros((noutputs,self.dig.lshape[0]))\n\t\telse:\n\t\t\toutputs = np.zeros((noutputs))\n\t\tfor e in range(len(self.eles)):\n\t\t\tinputs.append(np.zeros((sto[e],np.prod(self.dig.eshape))))\n\t\t\tif (self.HasGrad):\n\t\t\t\tinputgs.append(np.zeros((sto[e],np.prod(self.dig.eshape),self.MaxN3)))\n\t\t\tatom_mol_index.append(np.zeros((sto[e]), dtype=np.int64))\n\t\tfor i in range(self.test_ScratchPointer+ignore_first_mol, self.test_ScratchPointer+ncases):\n\t\t\tif (self.scratch_test_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\tif (currentmol != self.scratch_test_meta[i,0]):\n\t\t\t\toutputpointer = outputpointer+1\n\t\t\t\tcurrentmol = self.scratch_test_meta[i,0]\n\t\t\tif not self.test_mols_done and self.test_begin_mol+currentmol not in self.test_mols:\n\t\t\t\t\tself.test_mols.append(self.test_begin_mol+currentmol)\n\t\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t\te = (self.scratch_test_meta[i,1])\n\t\t\tei = self.eles.index(e)\n\t\t\t# The offset for this element should be within the bounds or something is wrong...\n\t\t\tinputs[ei][offsets[ei],:] = self.scratch_test_inputs[i]\n\t\t\tif (self.HasGrad):\n\t\t\t\tinputgs[ei][offsets[ei],:] = self.scratch_test_grads[i]\n\t\t\tatom_mol_index[ei][offsets[ei]] = outputpointer\n\t\t\toutputs[outputpointer] = self.scratch_test_outputs[self.scratch_test_meta[i,0]]\n\t\t\toffsets[ei] += 1\n\t\tself.test_ScratchPointer += ncases\n\t\tif (self.HasGrad):\n\t\t\treturn [inputs, inputgs, atom_mol_index, outputs]\n\t\telse:\n\t\t\treturn [inputs, atom_mol_index, outputs]\n\nclass TensorMolData_Bond_BP_Update(TensorMolData_BP_Update):\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP_Update.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_)\n\t\tself.eles = list(self.set.BondTypes())\n\t\tself.eles.sort()\n\t\treturn\n\n\tdef CleanScratch(self):\n\t\tTensorData.CleanScratch(self)\n\t\tself.CaseMetadata=None # case X molecule index , element type , first atom in this mol, last atom in this mol (exclusive)\n\t\tself.scratch_meta = None\n\t\tself.scratch_test_meta = None\n\t\treturn\n\n\tdef BuildTrain(self, name_=""gdb9"",  append=False, max_nmols_=1000000):\n\t\tself.CheckShapes()\n\t\tself.name=name_\n\t\tprint(""self.type:"", self.type)\n\t\tif self.type==""frag"":\n\t\t\traise Exception(""No BP frags now"")\n\t\tnmols  = len(self.set.mols)\n\t\tnbonds = self.set.NBonds()\n\t\tprint(""self.dig.eshape"", self.dig.eshape, "" self.dig.lshape"", self.dig.lshape)\n\t\tcases = np.zeros(tuple([nbonds]+list(self.dig.eshape)))\n\t\tprint(""cases:"", cases.shape)\n\t\tlabels = np.zeros(tuple([nmols]+list(self.dig.lshape)))\n\t\tself.CaseMetadata = np.zeros((nbonds, 4), dtype = np.int)\n\t\tinsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_in.npy""\n\t\toutsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_out.npy""\n\t\tmetasname = self.path+""Mol_""+name_+""_""+self.dig.name+""_meta.npy"" # Used aggregate and properly sum network inputs and outputs.\n\t\tcasep=0\n\t\t# Generate the set in a random order.\n\t\t#ord = range (0, len(self.set.mols))  # debug\n\t\tord=np.random.permutation(len(self.set.mols))\n\t\tmols_done = 0\n\t\tfor mi in ord:\n\t\t\tnbo = self.set.mols[mi].NBonds()\n\t\t\tif (mi == 0 or mi == 1):\n\t\t\t\tprint(""name of the first/second mol:"", self.set.mols[mi].name)\n\t\t\t#print ""casep:"", casep\n\t\t\tif (mols_done%1000==0):\n\t\t\t\tprint(""Mol:"", mols_done)\n\t\t\tins,outs = self.dig.TrainDigest(self.set.mols[mi])\n\t\t\t#print mi, ins.shape, outs.shape\n\t\t\tcases[casep:casep+nbo] = ins\n\t\t\t#if (self.set.mols[mi].name == ""Comment: c60""):\n\t\t\t#\tnp.savetxt(""c60_in.dat"", ins)\n\t\t\t#print ""ins:"", ins, "" cases:"", cases[casep:casep+nat]\n\t\t\tlabels[mols_done] = outs\n\t\t\tfor j in range(casep,casep+nbo):\n\t\t\t\tself.CaseMetadata[j,0] = mols_done\n\t\t\t\tself.CaseMetadata[j,1] = self.set.mols[mi].bonds[j-casep, 0]\n\t\t\t\tself.CaseMetadata[j,2] = casep\n\t\t\t\tself.CaseMetadata[j,3] = casep+nbo\n\t\t\tcasep += nbo\n\t\t\tmols_done = mols_done + 1\n\t\t\tif (mols_done>=max_nmols_):\n\t\t\t\tbreak\n\t\tinf = open(insname,""wb"")\n\t\touf = open(outsname,""wb"")\n\t\tmef = open(metasname,""wb"")\n\t\tnp.save(inf,cases[:casep,:])\n\t\tnp.save(ouf,labels[:mols_done,:])\n\t\tnp.save(mef,self.CaseMetadata[:casep,:])\n\t\tinf.close()\n\t\touf.close()\n\t\tmef.close()\n\t\tself.AvailableDataFiles.append([insname,outsname,metasname])\n\t\tself.Save() #write a convenience pickle.\n\t\treturn\n\n\tdef Init_TraceBack(self):\n\t\tnum_eles = [0 for ele in self.eles]\n\t\tfor mol_index in self.test_mols:\n\t\t\tfor ele in list(self.set.mols[mol_index].bonds[:,0]):\n\t\t\t\tnum_eles[self.eles.index(ele)] += 1\n\t\tself.test_atom_index = [np.zeros((num_eles[i],2), dtype = np.int) for i in range (0, len(self.eles))]\n\t\tpointer = [0 for ele in self.eles]\n\t\tfor mol_index in self.test_mols:\n\t\t\tmol = self.set.mols[mol_index]\n\t\t\tfor i in range (0, mol.bonds.shape[0]):\n\t\t\t\tbond_type = mol.bonds[i,0]\n\t\t\t\tself.test_atom_index[self.eles.index(bond_type)][pointer[self.eles.index(bond_type)]] = [int(mol_index), i]\n\t\t\t\tpointer[self.eles.index(bond_type)] += 1\n\t\tprint(self.test_atom_index)\n\t\tf  = open(""test_energy_bond_index_for_test.dat"",""wb"")\n\t\tpickle.dump(self.test_atom_index, f)\n\t\tf.close()\n\t\treturn\n\nclass TensorMolData_BP_Direct(TensorMolData):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tself.HasGrad = WithGrad_ # whether to pass around the gradient.\n\t\tTensorMolData.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_)\n\t\tself.eles = []\n\t\tif (MSet_ != None):\n\t\t\tself.eles = list(MSet_.AtomTypes())\n\t\t\tself.eles.sort()\n\t\t\tself.MaxNAtoms = np.max([m.NAtoms() for m in self.set.mols])\n\t\t\tprint(""self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tself.Nmols = len(self.set.mols)\n\t\tself.MeanStoich=None\n\t\tself.MeanNAtoms=None\n\t\tself.test_mols_done = False\n\t\tself.test_begin_mol  = None\n\t\tself.test_mols = []\n\t\tself.MaxN3 = None # The most coordinates in the set.\n\t\tself.name = self.set.name\n\t\tprint(""TensorMolData_BP.eles"", self.eles)\n\t\tprint(""self.HasGrad:"", self.HasGrad)\n\t\treturn\n\n\tdef CleanScratch(self):\n\t\tTensorData.CleanScratch(self)\n\t\tself.raw_it = None\n\t\tself.xyzs = None\n\t\tself.Zs = None\n\t\tself.labels = None\n\t\tself.grads = None\n\t\treturn\n\n\tdef LoadData(self):\n\t\tif (self.set == None):\n\t\t\ttry:\n\t\t\t\tself.ReloadSet()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""TData doesn\'t have a set."", Ex)\n\t\trandom.shuffle(self.set.mols)\n\t\txyzs = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((self.Nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tif (self.dig.OType == ""AtomizationEnergy""):\n\t\t\tlabels = np.zeros((self.Nmols), dtype = np.float64)\n\t\telse:\n\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\tif (self.HasGrad):\n\t\t\tgrads = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype=np.float64)\n\t\tfor i, mol in enumerate(self.set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tif (self.dig.OType  == ""AtomizationEnergy""):\n\t\t\t\tlabels[i] = mol.properties[""atomization""]\n\t\t\telse:\n\t\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\t\tif (self.HasGrad):\n\t\t\t\tgrads[i][:mol.NAtoms()] = mol.properties[""gradients""]\n\t\tif (self.HasGrad):\n\t\t\treturn xyzs, Zs, labels, grads\n\t\telse:\n\t\t\treturn xyzs, Zs, labels\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\ttry:\n\t\t\tself.HasGrad\n\t\texcept:\n\t\t\tself.HasGrad = False\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tif (self.HasGrad):\n\t\t\tself.xyzs, self.Zs, self.labels, self.grads = self.LoadData()\n\t\telse:\n\t\t\tself.xyzs, self.Zs, self.labels  = self.LoadData()\n\t\tself.NTestMols = int(self.TestRatio * self.Zs.shape[0])\n\t\tself.LastTrainMol = int(self.Zs.shape[0]-self.NTestMols)\n\t\tself.NTrain = self.LastTrainMol\n\t\tself.NTest = self.NTestMols\n\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.ScratchPointer = 0\n\t\tself.ScratchState = 1\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\treturn\n\n\tdef GetTrainBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tlabels = self.labels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, labels, self.grads[self.ScratchPointer-ncases:self.ScratchPointer]]\n\t\telse:\n\t\t\treturn [xyzs, Zs, labels]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tlabels = self.labels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, labels, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer]]\n\t\telse:\n\t\t\treturn [xyzs, Zs, labels]\n\n\tdef PrintStatus(self):\n\t\tprint(""self.ScratchState"",self.ScratchState)\n\t\tprint(""self.ScratchPointer"",self.ScratchPointer)\n\t\t#print ""self.test_ScratchPointer"",self.test_ScratchPointer\n\n\tdef Save(self):\n\t\tself.CleanScratch()\n\t\tf=open(self.path+self.name+""_""+self.dig.name+"".tdt"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\nclass TensorMolData_BPBond_Direct(TensorMolData):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=1, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tself.HasGrad = WithGrad_ # whether to pass around the gradient.\n\t\tTensorMolData.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_)\n\t\tself.eles = []\n\t\tif (MSet_ != None):\n\t\t\tself.eles = list(MSet_.AtomTypes())\n\t\t\tself.eles.sort()\n\t\t\tself.MaxNAtoms = np.max([m.NAtoms() for m in self.set.mols])\n\t\t\tprint(""self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tself.Nmols = len(self.set.mols)\n\t\tself.MeanStoich=None\n\t\tself.MeanNAtoms=None\n\t\tself.test_mols_done = False\n\t\tself.test_begin_mol  = None\n\t\tself.test_mols = []\n\t\tself.MaxN3 = None # The most coordinates in the set.\n\t\tself.name = self.set.name\n\t\tprint(""TensorMolData_BP.eles"", self.eles)\n\t\tprint(""self.HasGrad:"", self.HasGrad)\n\t\treturn\n\n\tdef CleanScratch(self):\n\t\tTensorData.CleanScratch(self)\n\t\tself.raw_it = None\n\t\tself.xyzs = None\n\t\tself.Zs = None\n\t\tself.labels = None\n\t\tself.grads = None\n\t\treturn\n\n\tdef RawBatch(self,nmol = 4096):\n\t\t""""""\n\t\t\tShimmy Shimmy Ya Shimmy Ya Shimmy Yay.\n\t\t\tThis type of batch is not built beforehand\n\t\t\tbecause there\'s no real digestion involved.\n\n\t\t\tArgs:\n\t\t\t\tnmol: number of molecules to put in the output.\n\n\t\t\tReturns:\n\t\t\t\tIns: a #atomsX4 tensor (AtNum,x,y,z)\n\t\t\t\tOuts: output of the digester\n\t\t\t\tKeys: (nmol)X(MaxNAtoms) tensor listing each molecule\'s place in the input.\n\t\t""""""\n\t\tif self.set == None:\n\t\t\tself.ReloadSet()\n\t\tndone = 0\n\t\tnatdone = 0\n\t\tself.MaxNAtoms = self.set.MaxNAtoms()\n\t\tIns = np.zeros(tuple([nmol,self.MaxNAtoms,4]))\n\t\tNAtomsVec = np.zeros((nmol),dtype=np.int32)\n\t\tOuts = np.zeros(tuple([nmol]))\n\t\twhile (ndone<nmol):\n\t\t\ttry:\n\t\t\t\tm = next(self.raw_it)\n\t\t\t\tti, to = self.dig.Emb(m, True, False)\n\t\t\t\tn=ti.shape[0]\n\t\t\t\tIns[ndone,:n,:] = ti\n\t\t\t\tNAtomsVec[ndone] = m.NAtoms()\n\t\t\t\tOuts[ndone] = to\n\t\t\t\tndone += 1\n\t\t\t\tnatdone += n\n\t\t\texcept StopIteration:\n\t\t\t\tself.raw_it = iter(self.set.mols)\n\t\tNL = NeighborListSet(Ins[:,:,1:],NAtomsVec, alg_=0)\n\t\tBondIdxMatrix = NL.buildPairs()\n\t\treturn Ins,BondIdxMatrix,Outs\n\n\tdef LoadData(self):\n\t\tif self.set == None:\n\t\t\tself.ReloadSet()\n\t\trandom.shuffle(self.set.mols)\n\t\txyzs = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((self.Nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tif (self.dig.OType == ""AtomizationEnergy""):\n\t\t\tlabels = np.zeros((self.Nmols), dtype = np.float64)\n\t\telse:\n\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\tif (self.HasGrad):\n\t\t\tgrads = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype=np.float64)\n\t\tfor i, mol in enumerate(self.set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tif (self.dig.OType  == ""AtomizationEnergy""):\n\t\t\t\tlabels[i] = mol.properties[""atomization""]\n\t\t\telse:\n\t\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\t\tif (self.HasGrad):\n\t\t\t\tgrads[i][:mol.NAtoms()] = mol.properties[""gradients""]\n\t\tif (self.HasGrad):\n\t\t\treturn xyzs, Zs, labels, grads\n\t\telse:\n\t\t\treturn xyzs, Zs, labels\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\ttry:\n\t\t\tself.HasGrad\n\t\texcept:\n\t\t\tself.HasGrad = False\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tif (self.HasGrad):\n\t\t\tself.xyzs, self.Zs, self.labels, self.grads = self.LoadData()\n\t\telse:\n\t\t\tself.xyzs, self.Zs, self.labels  = self.LoadData()\n\t\tself.NTestMols = int(self.TestRatio * self.Zs.shape[0])\n\t\tself.LastTrainMol = int(self.Zs.shape[0]-self.NTestMols)\n\t\tself.NTrain = self.LastTrainMol\n\t\tself.NTest = self.NTestMols\n\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.ScratchPointer = 0\n\t\tself.ScratchState = 1\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\treturn\n\n\n\t# def GetTrainBatch(self,ncases):\n\t# \tif (self.ScratchState == 0):\n\t# \t\tself.LoadDataToScratch()\n\t# \treset = False\n\t# \tif (ncases > self.NTrain):\n\t# \t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t# \tif (self.ScratchPointer+ncases >= self.NTrain):\n\t# \t\tself.ScratchPointer = 0\n\t# \tself.ScratchPointer += ncases\n\t# \txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t# \tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t# \tlabels = self.labels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t# \tif (self.HasGrad):\n\t# \t\treturn [xyzs, Zs, labels, self.grads[self.ScratchPointer-ncases:self.ScratchPointer]]\n\t# \telse:\n\t# \t\treturn [xyzs, Zs, labels]\n\t#\n\t# def GetTestBatch(self,ncases):\n\t# \tif (self.ScratchState == 0):\n\t# \t\tself.LoadDataToScratch()\n\t# \treset = False\n\t# \tif (ncases > self.NTest):\n\t# \t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t# \tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t# \t\tself.test_ScratchPointer = self.LastTrainMol\n\t# \tself.test_ScratchPointer += ncases\n\t# \txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t# \tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t# \tlabels = self.labels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t# \tif (self.HasGrad):\n\t# \t\treturn [xyzs, Zs, labels, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer]]\n\t# \telse:\n\t# \t\treturn [xyzs, Zs, labels]\n\n\tdef PrintStatus(self):\n\t\tprint(""self.ScratchState"",self.ScratchState)\n\t\tprint(""self.ScratchPointer"",self.ScratchPointer)\n\t\t#print ""self.test_ScratchPointer"",self.test_ScratchPointer\n\n\tdef Save(self):\n\t\tself.CleanScratch()\n\t\tf=open(self.path+self.name+""_""+self.dig.name+"".tdt"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\nclass TensorMolData_BP_Direct_Linear(TensorMolData_BP_Direct):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP_Direct.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_, WithGrad_)\n\t\tself.Rr_cut = PARAMS[""AN1_r_Rc""]\n\t\tself.Ra_cut = PARAMS[""AN1_a_Rc""]\n\t\treturn\n\n\tdef LoadData(self):\n\t\tif (self.set == None):\n\t\t\ttry:\n\t\t\t\tself.ReloadSet()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""TData doesn\'t have a set."", Ex)\n\t\trandom.shuffle(self.set.mols)\n\t\txyzs = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((self.Nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((self.Nmols), dtype = np.int64)\n\t\tif (self.dig.OType == ""AtomizationEnergy""):\n\t\t\tlabels = np.zeros((self.Nmols), dtype = np.float64)\n\t\telse:\n\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\tif (self.HasGrad):\n\t\t\tgrads = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype=np.float64)\n\t\tfor i, mol in enumerate(self.set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\t\tif (self.dig.OType  == ""AtomizationEnergy""):\n\t\t\t\tlabels[i] = mol.properties[""atomization""]\n\t\t\telse:\n\t\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\t\tif (self.HasGrad):\n\t\t\t\tgrads[i][:mol.NAtoms()] = mol.properties[""gradients""]\n\t\tif (self.HasGrad):\n\t\t\treturn xyzs, Zs, labels, natom, grads\n\t\telse:\n\t\t\treturn xyzs, Zs, labels, natom\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\ttry:\n\t\t\tself.HasGrad\n\t\texcept:\n\t\t\tself.HasGrad = False\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tif (self.HasGrad):\n\t\t\tself.xyzs, self.Zs, self.labels, self.natom, self.grads = self.LoadData()\n\t\telse:\n\t\t\tself.xyzs, self.Zs, self.labels, self.natom  = self.LoadData()\n\t\tself.NTestMols = int(self.TestRatio * self.Zs.shape[0])\n\t\tself.LastTrainMol = int(self.Zs.shape[0]-self.NTestMols)\n\t\tself.NTrain = self.LastTrainMol\n\t\tself.NTest = self.NTestMols\n\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.ScratchPointer = 0\n\t\tself.ScratchState = 1\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\treturn\n\n\tdef GetTrainBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tlabels = self.labels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tnatom = self.natom[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs)\n\t\trad_p, ang_t = NL.buildPairsAndTriples(self.Rr_cut, self.Ra_cut)\n\t\t#print ""Rad_p:"", rad_p[:20]\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, labels, self.grads[self.ScratchPointer-ncases:self.ScratchPointer], rad_p, ang_t]\n\t\telse:\n\t\t\treturn [xyzs, Zs, labels, rad_p, ang_t]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tlabels = self.labels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tnatom = self.natom[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs)\n\t\trad_p, ang_t = NL.buildPairsAndTriples(self.Rr_cut, self.Ra_cut)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, labels, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer], rad_p, ang_t]\n\t\telse:\n\t\t\treturn [xyzs, Zs, labels, rad_p, ang_t]\n\n\n\tdef GetBatch(self, ncases, Train_=True):\n\t\tif Train_:\n\t\t\treturn self.GetTrainBatch(ncases)\n\t\telse:\n\t\t\treturn self.GetTestBatch(ncases)\n\nclass TensorMolData_BP_Direct_EE(TensorMolData_BP_Direct_Linear):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP_Direct_Linear.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_, WithGrad_)\n\t\tself.Ree_cut = PARAMS[""EECutoffOff""]\n\t\treturn\n\n\tdef LoadData(self):\n\t\tif (self.set == None):\n\t\t\ttry:\n\t\t\t\tself.ReloadSet()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""TData doesn\'t have a set."", Ex)\n\t\trandom.shuffle(self.set.mols)\n\t\txyzs = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((self.Nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((self.Nmols), dtype = np.int32)\n\t\tif (self.dig.OType == ""EnergyAndDipole""):\n\t\t\tElabels = np.zeros((self.Nmols), dtype = np.float64)\n\t\t\tDlabels = np.zeros((self.Nmols, 3),  dtype = np.float64)\n\t\telse:\n\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\tif (self.HasGrad):\n\t\t\tgrads = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype=np.float64)\n\t\tfor i, mol in enumerate(self.set.mols):\n\t\t\ttry:\n\t\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\t\tnatom[i] = mol.NAtoms()\n\t\t\texcept Exception as ex:\n\t\t\t\tprint(mol.coords, mol.atoms, mol.coords.shape[0], mol.atoms.shape[0])\n\t\t\t\traise Exception(""Bad data2"")\n\t\t\tif (self.dig.OType  == ""EnergyAndDipole""):\n\t\t\t\tElabels[i] = mol.properties[""atomization""]\n\t\t\t\tDlabels[i] = mol.properties[""dipole""]*AUPERDEBYE\n\t\t\telse:\n\t\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\t\tif (self.HasGrad):\n\t\t\t\tgrads[i][:mol.NAtoms()] = mol.properties[""gradients""]\n\t\tif (self.HasGrad):\n\t\t\treturn xyzs, Zs, Elabels, Dlabels, natom, grads\n\t\telse:\n\t\t\treturn xyzs, Zs, Elabels, Dlabels, natom\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\ttry:\n\t\t\tself.HasGrad\n\t\texcept:\n\t\t\tself.HasGrad = False\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tif (self.HasGrad):\n\t\t\tself.xyzs, self.Zs, self.Elabels, self.Dlabels, self.natom, self.grads = self.LoadData()\n\t\telse:\n\t\t\tself.xyzs, self.Zs, self.Elabels, self.Dlabels, self.natom  = self.LoadData()\n\t\tself.NTestMols = int(self.TestRatio * self.Zs.shape[0])\n\t\tself.LastTrainMol = int(self.Zs.shape[0]-self.NTestMols)\n\t\tself.NTrain = self.LastTrainMol\n\t\tself.NTest = self.NTestMols\n\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.ScratchPointer = 0\n\t\tself.ScratchState = 1\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\treturn\n\n\tdef GetTrainBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tDlabels = self.Dlabels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tElabels = self.Elabels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tnatom = self.natom[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs)\n\t\trad_p, ang_t = NL.buildPairsAndTriples(self.Rr_cut, self.Ra_cut)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(self.Ree_cut)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, self.grads[self.ScratchPointer-ncases:self.ScratchPointer], rad_p, ang_t, rad_eep, 1.0/natom]\n\t\telse:\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, rad_p, ang_t, rad_eep, 1.0/natom]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tElabels = self.Elabels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tDlabels = self.Dlabels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tnatom = self.natom[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs)\n\t\trad_p, ang_t = NL.buildPairsAndTriples(self.Rr_cut, self.Ra_cut)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(self.Ree_cut)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer], rad_p, ang_t, rad_eep, 1.0/natom]\n\t\telse:\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, rad_p, ang_t, rad_eep, 1.0/natom]\n\n\nclass TensorMolData_BP_Direct_EandG_Release(TensorMolData_BP_Direct_Linear):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = True):\n\t\tTensorMolData_BP_Direct_Linear.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_, WithGrad_)\n\t\tself.ele = None #  determine later\n\t\tself.elep = None # determine later\n\t\treturn\n\n\tdef GetTrainBatch(self, ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tElabels = self.labels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tnatom = self.natom[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, Elabels, self.grads[self.ScratchPointer-ncases:self.ScratchPointer], rad_p_ele, ang_t_elep, mil_j, mil_jk, 1.0/natom]\n\t\telse:\n\t\t\treturn [xyzs, Zs, Elabels, rad_p_ele, ang_t_elep, mil_j, mil_jk, 1.0/natom]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tElabels = self.labels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tnatom = self.natom[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, Elabels, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer], rad_p_ele, ang_t_elep, mil_j, mil_jk, 1.0/natom]\n\t\telse:\n\t\t\treturn [xyzs, Zs, Elabels, rad_p_ele, ang_t_elep, mil_j, mil_jk, 1.0/natom]\n\n\nclass TensorMolData_BP_Direct_EE_WithEle(TensorMolData_BP_Direct_EE):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP_Direct_EE.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_, WithGrad_)\n\t\tself.Ree_cut = PARAMS[""EECutoffOff""]\n\t\tself.ele = None #  determine later\n\t\tself.elep = None # determine later\n\t\treturn\n\n\tdef GetTrainBatch(self, ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tDlabels = self.Dlabels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tElabels = self.Elabels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tnatom = self.natom[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(self.Ree_cut)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, self.grads[self.ScratchPointer-ncases:self.ScratchPointer], rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom]\n\t\telse:\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tElabels = self.Elabels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tDlabels = self.Dlabels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tnatom = self.natom[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(self.Ree_cut)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer], rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom]\n\t\telse:\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom]\n\nclass TensorMolData_BP_Direct_EE_WithEle_Release(TensorMolData_BP_Direct_EE):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP_Direct_EE.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_, WithGrad_)\n\t\tself.Ree_cut = PARAMS[""EECutoffOff""]\n\t\tself.ele = None #  determine later\n\t\tself.elep = None # determine later\n\t\treturn\n\n\tdef GetTrainBatch(self, ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tDlabels = self.Dlabels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tElabels = self.Elabels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tnatom = self.natom[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(self.Ree_cut)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, self.grads[self.ScratchPointer-ncases:self.ScratchPointer], rad_p_ele, ang_t_elep, rad_eep, mil_j, mil_jk, 1.0/natom]\n\t\telse:\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, rad_p_ele, ang_t_elep, rad_eep, mil_j, mil_jk, 1.0/natom]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tElabels = self.Elabels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tDlabels = self.Dlabels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tnatom = self.natom[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(self.Ree_cut)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer], rad_p_ele, ang_t_elep, rad_eep, mil_j, mil_jk, 1.0/natom]\n\t\telse:\n\t\t\treturn [xyzs, Zs, Elabels, Dlabels, rad_p_ele, ang_t_elep, rad_eep, mil_j, mil_jk, 1.0/natom]\n\nclass TensorMolData_BP_Direct_WithEle(TensorMolData_BP_Direct_EE):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP_Direct_EE.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_, WithGrad_)\n\t\tself.Ree_cut = PARAMS[""EECutoffOff""]\n\t\tself.ele = None #  determine later\n\t\tself.elep = None # determine later\n\t\treturn\n\n\tdef LoadData(self):\n\t\tif (self.set == None):\n\t\t\ttry:\n\t\t\t\tself.ReloadSet()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""TData doesn\'t have a set."", Ex)\n\t\trandom.shuffle(self.set.mols)\n\t\txyzs = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((self.Nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((self.Nmols), dtype = np.int32)\n\t\tif (self.dig.OType == ""atomization""):\n\t\t\tlabels = np.zeros((self.Nmols), dtype = np.float64)\n\t\telse:\n\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\tif (self.HasGrad):\n\t\t\tgrads = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype=np.float64)\n\t\tfor i, mol in enumerate(self.set.mols):\n\t\t\ttry:\n\t\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\t\tnatom[i] = mol.NAtoms()\n\t\t\texcept Exception as ex:\n\t\t\t\tprint(mol.coords, mol.atoms, mol.coords.shape[0], mol.atoms.shape[0])\n\t\t\t\traise Exception(""Bad data2"")\n\t\t\tif (self.dig.OType  == ""atomization""):\n\t\t\t\tlabels[i] = mol.properties[""atomization""]\n\t\t\telse:\n\t\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\t\tif (self.HasGrad):\n\t\t\t\tgrads[i][:mol.NAtoms()] = -1 * mol.properties[""forces""]\n\t\tif (self.HasGrad):\n\t\t\treturn xyzs, Zs, labels, natom, grads\n\t\telse:\n\t\t\treturn xyzs, Zs, labels, natom\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\ttry:\n\t\t\tself.HasGrad\n\t\texcept:\n\t\t\tself.HasGrad = False\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tif (self.HasGrad):\n\t\t\tself.xyzs, self.Zs, self.labels, self.natom, self.grads = self.LoadData()\n\t\telse:\n\t\t\tself.xyzs, self.Zs, self.labels, self.natom  = self.LoadData()\n\t\tself.NTestMols = int(self.TestRatio * self.Zs.shape[0])\n\t\tself.LastTrainMol = int(self.Zs.shape[0]-self.NTestMols)\n\t\tself.NTrain = self.LastTrainMol\n\t\tself.NTest = self.NTestMols\n\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.ScratchPointer = 0\n\t\tself.ScratchState = 1\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\treturn\n\n\tdef GetTrainBatch(self, ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tlabels = self.labels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tnatom = self.natom[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, labels, self.grads[self.ScratchPointer-ncases:self.ScratchPointer], natom, rad_p_ele, ang_t_elep, mil_jk]\n\t\telse:\n\t\t\treturn [xyzs, Zs, labels, natom, rad_p_ele, ang_t_elep, mil_jk]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tlabels = self.labels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tnatom = self.natom[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, labels, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer], natom, rad_p_ele, ang_t_elep, mil_jk]\n\t\telse:\n\t\t\treturn [xyzs, Zs, labels, natom, rad_p_ele, ang_t_elep, mil_jk]\n\n\nclass TensorMolData_BP_Direct_Charge_WithEle_Release(TensorMolData_BP_Direct_EE):\n\t""""""\n\tThis tensordata serves up batches digested within TensorMol.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP_Direct_EE.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_, WithGrad_)\n\t\tself.Ree_cut = PARAMS[""EECutoffOff""]\n\t\tself.ele = None #  determine later\n\t\tself.elep = None # determine later\n\t\treturn\n\n\tdef GetTrainBatch(self, ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tDlabels = self.Dlabels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tnatom = self.natom[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  Zs)\n\t\trad_eep_e1e2 = NLEE.buildPairsWithBothEleIndex(self.Ree_cut, self.ele, True)\n\t\treturn [xyzs, Zs, Dlabels, rad_p_ele, ang_t_elep, rad_eep_e1e2, mil_j, mil_jk, 1.0/natom]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tElabels = self.Elabels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tDlabels = self.Dlabels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tnatom = self.natom[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(self.Rr_cut, self.Ra_cut, self.ele, self.elep)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  Zs)\n\t\trad_eep_e1e2 = NLEE.buildPairsWithBothEleIndex(self.Ree_cut, self.ele, True)\n\t\treturn [xyzs, Zs, Dlabels, rad_p_ele, ang_t_elep, rad_eep_e1e2, mil_j, mil_jk, 1.0/natom]\n\n'"
TensorMol/Containers/TensorMolDataEE.py,0,"b'#\n# Contains Routines to generate training sets\n# Combining a dataset, sampler and an embedding. (CM etc.)\n#\n# These work Moleculewise the versions without the mol prefix work atomwise.\n# but otherwise the behavior of these is the same as Tensordata etc.\n#\n#\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport os, gc\nfrom .Sets import *\nfrom .DigestMol import *\nfrom .TensorMolData import *\n\nclass TensorMolData_BP_Multipole(TensorMolData_BP):\n\t""""""\n\t\t\tA tensordata for learning the multipole of molecules using Behler-Parinello scheme.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol""):\n\t\tTensorMolData_BP.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_)\n\t\tself.xyzMetadata=None # case X molecule index X element type (Strictly ascending)\n\t\tself.scratch_xyzmeta = None\n\t\tself.scratch_test_xyzmeta = None\n\t\treturn\n\n\tdef CleanScratch(self):\n\t\tTensorMolData_BP.CleanScratch(self)\n\t\tself.xyzMetadata=None # case X molecule index , element type , first atom in this mol, last atom in this mol (exclusive)\n\t\tself.scratch_xyzmeta = None\n\t\tself.scratch_test_xyzmeta = None\n\t\treturn\n\n\tdef LoadData(self):\n\t\tinsname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_in.npy""\n\t\toutsname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_out.npy""\n\t\tmetasname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_meta.npy"" # Used aggregate\n\t\txyzmetasname = self.path+""Mol_""+self.name+""_""+self.dig.name+""_xyzmeta.npy""\n\t\tinf = open(insname,""rb"")\n\t\touf = open(outsname,""rb"")\n\t\tmef = open(metasname,""rb"")\n\t\txyzf = open(xyzmetasname,""rb"")\n\t\tti = np.load(inf)\n\t\tto = np.load(ouf)\n\t\ttm = np.load(mef)\n\t\ttxyzm = np.load(xyzf)\n\t\tinf.close()\n\t\touf.close()\n\t\tmef.close()\n\t\txyzf.close()\n\t\tto = to.reshape((to.shape[0],-1))  # flat labels to [mol, 1]\n\t\treturn ti, to, tm, txyzm\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tti, to, tm, txyzm = self.LoadData()\n\t\tif (tformer.innorm != None):\n\t\t\tti = tformer.NormalizeIns(ti)\n\t\tif (tformer.outnorm != None):\n\t\t\tto = tformer.NormalizeOuts(to)\n\t\tself.NTestMols = int(self.TestRatio * to.shape[0])\n\t\tself.LastTrainMol = int(to.shape[0]-self.NTestMols)\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\tLOGGER.debug(""Number of molecules in meta:: %i"", tm[-1,0]+1)\n\t\tLastTrainCase=0\n\t\t#print tm\n\t\t# Figure out the number of atoms in training and test.\n\t\tfor i in range(len(tm)):\n\t\t\tif (tm[i,0] == self.LastTrainMol):\n\t\t\t\tLastTrainCase = tm[i,2] # exclusive\n\t\t\t\tbreak\n\t\tLOGGER.debug(""last train atom: %i"",LastTrainCase)\n\t\tLOGGER.debug(""Num Test atoms: %i"",len(tm)-LastTrainCase)\n\t\tLOGGER.debug(""Num atoms: %i"",len(tm))\n\t\tself.NTrain = LastTrainCase\n\t\tself.NTest = len(tm)-LastTrainCase\n\t\tself.scratch_inputs = ti[:LastTrainCase]\n\t\tself.scratch_outputs = to[:self.LastTrainMol]\n\t\tself.scratch_meta = tm[:LastTrainCase]\n\t\tself.scratch_xyzmeta = txyzm[:LastTrainCase]\n\t\tself.scratch_test_inputs = ti[LastTrainCase:]\n\t\tself.scratch_test_outputs = to[self.LastTrainMol:]\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t# these columns need to be shifted.\n\t\tself.scratch_test_meta = tm[LastTrainCase:]\n\t\tself.scratch_test_xyzmeta = txyzm[LastTrainCase:]\n\t\tself.test_begin_mol = self.scratch_test_meta[0,0]\n#\t\tprint ""before shift case  "", tm[LastTrainCase:LastTrainCase+30], ""real"", self.set.mols[tm[LastTrainCase, 0]].bonds, self.set.mols[self.test_begin_mol].bonds\n\t\tself.scratch_test_meta[:,0] -= self.scratch_test_meta[0,0]\n\t\tself.scratch_test_meta[:,3] -= self.scratch_test_meta[0,2]\n\t\tself.scratch_test_meta[:,2] -= self.scratch_test_meta[0,2]\n\t\tself.ScratchState = 1\n\t\tself.ScratchPointer = 0\n\t\tself.test_ScratchPointer=0\n\t\t# Compute mean Stoichiometry and number of atoms.\n\t\tself.eles = np.unique(tm[:,1]).tolist()\n\t\tatomcount = np.zeros(len(self.eles))\n\t\tself.MeanStoich = np.zeros(len(self.eles))\n\t\tfor j in range(len(self.eles)):\n\t\t\tfor i in range(len(ti)):\n\t\t\t\tif (tm[i,1]==self.eles[j]):\n\t\t\t\t\tatomcount[j]=atomcount[j]+1\n\t\tself.MeanStoich=atomcount/len(to)\n\t\tself.MeanNumAtoms = np.sum(self.MeanStoich)\n\t\treturn\n\n\n\n\tdef BuildTrain(self, name_=""gdb9"",  append=False, max_nmols_=1000000):\n\t\tself.CheckShapes()\n\t\tself.name=name_\n\t\tLOGGER.info(""TensorMolData_BP_Multipole, self.type:""+self.type)\n\t\tif self.type==""frag"":\n\t\t\traise Exception(""No BP frags now"")\n\t\tnmols  = len(self.set.mols)\n\t\tnatoms = self.set.NAtoms()\n\t\tLOGGER.info( ""self.dig.eshape""+str(self.dig.eshape)+"" self.dig.lshape""+str(self.dig.lshape))\n\t\tcases = np.zeros(tuple([natoms]+list(self.dig.eshape)))\n\t\tLOGGER.info( ""cases:""+str(cases.shape))\n\t\tlabels = np.zeros(tuple([nmols]+list(self.dig.lshape)))\n\t\tself.CaseMetadata = np.zeros((natoms, 4), dtype = np.int)\n\t\tself.xyzMetadata = np.zeros((natoms, 3))\n\t\tinsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_in.npy""\n\t\toutsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_out.npy""\n\t\tmetasname = self.path+""Mol_""+name_+""_""+self.dig.name+""_meta.npy"" # Used aggregate and properly sum network inputs and outputs.\n\t\txyzmetasname = self.path+""Mol_""+name_+""_""+self.dig.name+""_xyzmeta.npy"" # Used aggregate and properly sum network inputs and outputs.\n\t\tcasep=0\n\t\t# Generate the set in a random order.\n\t\tord=np.random.permutation(len(self.set.mols))\n\t\tmols_done = 0\n\t\tfor mi in ord:\n\t\t\tnat = self.set.mols[mi].NAtoms()\n\t\t\t#print ""casep:"", casep\n\t\t\tif (mols_done%1000==0):\n\t\t\t\tLOGGER.info(""Mol:""+str(mols_done))\n\t\t\tins,outs = self.dig.TrainDigest(self.set.mols[mi])\n\t\t\tif not np.all(np.isfinite(ins)):\n\t\t\t\tprint(""find a bad case, writting down xyz.."")\n\t\t\t\tself.set.mols[mi].WriteXYZfile(fpath=""."", fname=""bad_buildset_cases"")\n\t\t\t#print mi, ins.shape, outs.shape\n\t\t\tcases[casep:casep+nat] = ins\n\t\t\tlabels[mols_done] = outs\n\t\t\tcenter_xyz = self.set.mols[mi].coords - np.average(self.set.mols[mi].coords, axis=0)\n\t\t\tfor j in range(casep,casep+nat):\n\t\t\t\tself.CaseMetadata[j,0] = mols_done\n\t\t\t\tself.CaseMetadata[j,1] = self.set.mols[mi].atoms[j-casep]\n\t\t\t\tself.CaseMetadata[j,2] = casep\n\t\t\t\tself.CaseMetadata[j,3] = casep+nat\n\t\t\t\tself.xyzMetadata[j] = center_xyz[j - casep]\n\t\t\tcasep += nat\n\t\t\tmols_done = mols_done + 1\n\t\t\tif (mols_done>=max_nmols_):\n\t\t\t\tbreak\n\t\tinf = open(insname,""wb"")\n\t\touf = open(outsname,""wb"")\n\t\tmef = open(metasname,""wb"")\n\t\txyzf = open(xyzmetasname, ""wb"")\n\t\tnp.save(inf,cases[:casep,:])\n\t\tnp.save(ouf,labels[:mols_done,:])\n\t\tnp.save(mef,self.CaseMetadata[:casep,:])\n\t\tnp.save(xyzf, self.xyzMetadata[:casep,:])\n\t\tinf.close()\n\t\touf.close()\n\t\tmef.close()\n\t\txyzf.close()\n\t\tself.AvailableDataFiles.append([insname,outsname,metasname, xyzmetasname])\n\t\tself.Save() #write a convenience pickle.\n\t\treturn\n\n\tdef GetTrainBatch(self,ncases,noutputs):\n\t\t""""""\n\t\tConstruct the data required for a training batch Returns inputs (sorted by element), and indexing matrices and outputs.\n\t\tBehler parinello batches need to have a typical overall stoichiometry.\n\t\tand a constant number of atoms, and must contain an integer number of molecules.\n\n\t\tBesides making sure all of that takes place this routine makes the summation matrices\n\t\twhich map the cases => molecular energies in the Neural Network output.\n\n\t\tArgs:\n\t\t\tncases: the size of a training cases.\n\t\t\tnoutputs: the maximum number of molecule energies which can be produced.\n\t\tReturns:\n\t\t\tA an **ordered** list of length self.eles containing\n\t\t\t\ta list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\t\ta list of (num_of atom type X batchsize) matrices which linearly combines the elements\n\t\t\t\ta list of outputs.\n\t\t""""""\n\t\tstart_time = time.time()\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tinputs = []#np.zeros((ncases, np.prod(self.dig.eshape)))\n\t\tmatrices = []#np.zeros((len(self.eles), ncases, noutputs))\n\t\toffsets=[]\n\t\tcoords = []\n\t\t# Get the number of molecules which would be contained in the desired batch size\n\t\t# and the number of element cases.\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\tbmols=np.unique(self.scratch_meta[self.ScratchPointer:self.ScratchPointer+ncases,0])\n\t\tnmols_out=len(bmols[1:-1])\n\t\tif (nmols_out > noutputs):\n\t\t\traise Exception(""Insufficent Padding. ""+str(nmols_out)+"" is greater than ""+str(noutputs))\n\t\tinputpointer = 0\n\t\toutputpointer = 0\n\t\t#currentmol=self.scratch_meta[self.ScratchPointer,0]\n\t\tsto = np.zeros(len(self.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.eles),dtype = np.int32) # output pointers within each element block.\n\t\tdestinations = np.zeros(ncases) # The index in the output of each case in the scratch.\n\t\tignore_first_mol = 0\n\t\tfor i in range(self.ScratchPointer,self.ScratchPointer+ncases):\n\t\t\tif (self.scratch_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\telif (self.scratch_meta[i,0] == bmols[0]):\n\t\t\t\tignore_first_mol += 1\n\t\t\telse:\n\t\t\t\tsto[self.eles.index(self.scratch_meta[i,1])]+=1\n\t\tcurrentmol=self.scratch_meta[self.ScratchPointer+ignore_first_mol,0]\n\t\toutputs = np.zeros((noutputs, 4))\n\t\tfor e in range(len(self.eles)):\n\t\t\tinputs.append(np.zeros((sto[e],np.prod(self.dig.eshape))))\n\t\t\tmatrices.append(np.zeros((sto[e],noutputs)))\n\t\t\tcoords.append(np.zeros((sto[e], 3)))\n\t\tfor i in range(self.ScratchPointer+ignore_first_mol, self.ScratchPointer+ncases):\n\t\t\tif (self.scratch_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\tif (currentmol != self.scratch_meta[i,0]):\n\t\t\t\toutputpointer = outputpointer+1\n\t\t\t\tcurrentmol = self.scratch_meta[i,0]\n\t\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t\te = (self.scratch_meta[i,1])\n\t\t\tei = self.eles.index(e)\n\t\t\t# The offset for this element should be within the bounds or something is wrong...\n\t\t\tinputs[ei][offsets[ei],:] = self.scratch_inputs[i]\n\t\t\tmatrices[ei][offsets[ei],outputpointer] = 1.0\n\t\t\tcoords[ei][offsets[ei]] = self.scratch_xyzmeta[i]\n\t\t\toutputs[outputpointer] = self.scratch_outputs[self.scratch_meta[i,0]]\n\t\t\toffsets[ei] += 1\n\t\t#print ""inputs"",inputs\n\t\t#print ""bounds"",bounds\n\t\t#print ""matrices"",matrices\n\t\t#print ""outputs"",outputs\n\t\tself.ScratchPointer += ncases\n\t\treturn [inputs, matrices, coords, outputs]\n\n\tdef GetTestBatch(self,ncases,noutputs):\n\t\t""""""\n\t\t\tReturns:\n\t\t\tA an **ordered** list of length self.eles containing\n\t\t\t\ta list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\t\ta list of (num_of atom type X batchsize) matrices which linearly combines the elements\n\t\t\t\ta list of outputs.\n\t\t\t\tthe number of output molecules.\n\t\t""""""\n\t\tstart_time = time.time()\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases >= self.NTest):\n\t\t\tself.test_ScratchPointer = 0\n\t\t\tself.test_mols_done = True\n\t\tinputs = []#np.zeros((ncases, np.prod(self.dig.eshape)))\n\t\tmatrices = []#np.zeros((len(self.eles), ncases, noutputs))\n\t\tcoords = []\n\t\toffsets= []\n\t\t# Get the number of molecules which would be contained in the desired batch size\n\t\t# and the number of element cases.\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\tbmols=np.unique(self.scratch_test_meta[self.test_ScratchPointer:self.test_ScratchPointer+ncases,0])\n\t\tnmols_out=len(bmols[1:-1])\n\t\t#print ""batch contains"",nmols_out, ""Molecules in "",ncases\n\t\tif (nmols_out > noutputs):\n\t\t\traise Exception(""Insufficent Padding. ""+str(nmols_out)+"" is greater than ""+str(noutputs))\n\t\tinputpointer = 0\n\t\toutputpointer = 0\n\t\t#currentmol=self.scratch_meta[self.ScratchPointer,0]\n\t\tsto = np.zeros(len(self.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.eles),dtype = np.int32) # output pointers within each element block.\n\t\tdestinations = np.zeros(ncases) # The index in the output of each case in the scratch.\n\t\tignore_first_mol = 0\n\t\tfor i in range(self.test_ScratchPointer,self.test_ScratchPointer+ncases):\n\t\t\tif (self.scratch_test_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\telif (self.scratch_test_meta[i,0] == bmols[0]):\n\t\t\t\tignore_first_mol += 1\n\t\t\telse:\n\t\t\t\tsto[self.eles.index(self.scratch_test_meta[i,1])]+=1\n\t\tcurrentmol=self.scratch_test_meta[self.test_ScratchPointer+ignore_first_mol,0]\n\t\toutputs = np.zeros((noutputs, 4))\n\t\tfor e in range(len(self.eles)):\n\t\t\tinputs.append(np.zeros((sto[e],np.prod(self.dig.eshape))))\n\t\t\tmatrices.append(np.zeros((sto[e],noutputs)))\n\t\t\tcoords.append(np.zeros((sto[e], 3)))\n\t\tfor i in range(self.test_ScratchPointer+ignore_first_mol, self.test_ScratchPointer+ncases):\n\t\t\tif (self.scratch_test_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\tif (currentmol != self.scratch_test_meta[i,0]):\n\t\t\t\toutputpointer = outputpointer+1\n\t\t\t\tcurrentmol = self.scratch_test_meta[i,0]\n\t\t\tif not self.test_mols_done and self.test_begin_mol+currentmol not in self.test_mols:\n\t\t\t\t\tself.test_mols.append(self.test_begin_mol+currentmol)\n#\t\t\t\t\tif i < self.test_ScratchPointer+ignore_first_mol + 50:\n#\t\t\t\t\t\tprint ""i "",i, self.set.mols[self.test_mols[-1]].bonds\n\t\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t\te = (self.scratch_test_meta[i,1])\n\t\t\tei = self.eles.index(e)\n\t\t\t# The offset for this element should be within the bounds or something is wrong...\n\t\t\tinputs[ei][offsets[ei],:] = self.scratch_test_inputs[i]\n\t\t\tmatrices[ei][offsets[ei],outputpointer] = 1.0\n\t\t\tcoords[ei][offsets[ei]] = self.scratch_test_xyzmeta[i]\n\t\t\toutputs[outputpointer] = self.scratch_test_outputs[self.scratch_test_meta[i,0]]\n\t\t\toffsets[ei] += 1\n#\t\t\tif i < self.test_ScratchPointer+ignore_first_mol + 50:\n#\t\t\t\tprint ""first 50 meta data :"", i, self.test_ScratchPointer+ignore_first_mol, self.scratch_test_meta[i]\n\t\t#print ""inputs"",inputs\n\t\t#print ""bounds"",bounds\n\t\t#print ""matrices"",matrices\n\t\t#print ""outputs"",outputs\n\t\tself.test_ScratchPointer += ncases\n#\t\tprint ""length of test_mols:"", len(self.test_mols)\n#\t\tprint ""outputpointer:"", outputpointer\n\t\treturn [inputs, matrices, coords, outputs]\n\nclass TensorMolData_BP_Multipole_2(TensorMolData_BP_Multipole):\n\t""""""\n    A tensordata for learning the multipole of molecules using Behler-Parinello scheme.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol""):\n\t\tTensorMolData_BP_Multipole.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_)\n\t\treturn\n\n\tdef GetTrainBatch(self,ncases,noutputs):\n\t\t""""""\n\t\tConstruct the data required for a training batch Returns inputs (sorted by element), and indexing matrices and outputs.\n\t\tBehler parinello batches need to have a typical overall stoichiometry.\n\t\tand a constant number of atoms, and must contain an integer number of molecules.\n\n\t\tBesides making sure all of that takes place this routine makes the summation matrices\n\t\twhich map the cases => molecular energies in the Neural Network output.\n\n\t\tArgs:\n\t\t\tncases: the size of a training cases.\n\t\t\tnoutputs: the maximum number of molecule energies which can be produced.\n\t\tReturns:\n\t\t\tA an **ordered** list of length self.eles containing\n\t\t\t\ta list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\t\ta list of (num_of atom type X batchsize) matrices which linearly combines the elements\n\t\t\t\ta list of outputs.\n\t\t""""""\n\t\tstart_time = time.time()\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tinputs = []#np.zeros((ncases, np.prod(self.dig.eshape)))\n\t\tmatrices = []#np.zeros((len(self.eles), ncases, noutputs))\n\t\toffsets=[]\n\t\tcoords = []\n\t\t# Get the number of molecules which would be contained in the desired batch size\n\t\t# and the number of element cases.\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\tbmols=np.unique(self.scratch_meta[self.ScratchPointer:self.ScratchPointer+ncases,0])\n\t\tnmols_out=len(bmols[1:-1])\n\t\tif (nmols_out > noutputs):\n\t\t\traise Exception(""Insufficent Padding. ""+str(nmols_out)+"" is greater than ""+str(noutputs))\n\t\tinputpointer = 0\n\t\toutputpointer = 0\n\t\t#currentmol=self.scratch_meta[self.ScratchPointer,0]\n\t\tsto = np.zeros(len(self.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.eles),dtype = np.int32) # output pointers within each element block.\n\t\tdestinations = np.zeros(ncases) # The index in the output of each case in the scratch.\n\t\tignore_first_mol = 0\n\t\tfor i in range(self.ScratchPointer,self.ScratchPointer+ncases):\n\t\t\tif (self.scratch_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\telif (self.scratch_meta[i,0] == bmols[0]):\n\t\t\t\tignore_first_mol += 1\n\t\t\telse:\n\t\t\t\tsto[self.eles.index(self.scratch_meta[i,1])]+=1\n\t\tcurrentmol=self.scratch_meta[self.ScratchPointer+ignore_first_mol,0]\n\t\toutputs = np.zeros((noutputs, 3))\n\t\tnatom_in_mol  = np.zeros((noutputs, 1))\n\t\tnatom_in_mol.fill(float(\'inf\'))\n\t\tfor e in range(len(self.eles)):\n\t\t\tinputs.append(np.zeros((sto[e],np.prod(self.dig.eshape))))\n\t\t\tmatrices.append(np.zeros((sto[e],noutputs)))\n\t\t\tcoords.append(np.zeros((sto[e], 3)))\n\t\tfor i in range(self.ScratchPointer+ignore_first_mol, self.ScratchPointer+ncases):\n\t\t\tif (self.scratch_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\tif (currentmol != self.scratch_meta[i,0]):\n\t\t\t\toutputpointer = outputpointer+1\n\t\t\t\tcurrentmol = self.scratch_meta[i,0]\n\t\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t\te = (self.scratch_meta[i,1])\n\t\t\tei = self.eles.index(e)\n\t\t\t# The offset for this element should be within the bounds or something is wrong...\n\t\t\tinputs[ei][offsets[ei],:] = self.scratch_inputs[i]\n\t\t\tmatrices[ei][offsets[ei],outputpointer] = 1.0\n\t\t\tcoords[ei][offsets[ei]] = self.scratch_xyzmeta[i]\n\t\t\toutputs[outputpointer] = self.scratch_outputs[self.scratch_meta[i,0]]\n\t\t\tnatom_in_mol[outputpointer] = self.scratch_meta[i,3] - self.scratch_meta[i,2]\n\t\t\toffsets[ei] += 1\n\t\tself.ScratchPointer += ncases\n\t\treturn [inputs, matrices, coords, 1.0/natom_in_mol, outputs]\n\n\tdef GetTestBatch(self,ncases,noutputs):\n\t\t""""""\n\t\t\tReturns:\n\t\t\tA an **ordered** list of length self.eles containing\n\t\t\t\ta list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\t\ta list of (num_of atom type X batchsize) matrices which linearly combines the elements\n\t\t\t\ta list of outputs.\n\t\t\t\tthe number of output molecules.\n\t\t""""""\n\t\tstart_time = time.time()\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases >= self.NTest):\n\t\t\tself.test_ScratchPointer = 0\n\t\t\tself.test_mols_done = True\n\t\tinputs = []#np.zeros((ncases, np.prod(self.dig.eshape)))\n\t\tmatrices = []#np.zeros((len(self.eles), ncases, noutputs))\n\t\tcoords = []\n\t\toffsets= []\n\t\t# Get the number of molecules which would be contained in the desired batch size\n\t\t# and the number of element cases.\n\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\tbmols=np.unique(self.scratch_test_meta[self.test_ScratchPointer:self.test_ScratchPointer+ncases,0])\n\t\tnmols_out=len(bmols[1:-1])\n\t\t#print ""batch contains"",nmols_out, ""Molecules in "",ncases\n\t\tif (nmols_out > noutputs):\n\t\t\traise Exception(""Insufficent Padding. ""+str(nmols_out)+"" is greater than ""+str(noutputs))\n\t\tinputpointer = 0\n\t\toutputpointer = 0\n\t\t#currentmol=self.scratch_meta[self.ScratchPointer,0]\n\t\tsto = np.zeros(len(self.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.eles),dtype = np.int32) # output pointers within each element block.\n\t\tdestinations = np.zeros(ncases) # The index in the output of each case in the scratch.\n\t\tignore_first_mol = 0\n\t\tfor i in range(self.test_ScratchPointer,self.test_ScratchPointer+ncases):\n\t\t\tif (self.scratch_test_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\telif (self.scratch_test_meta[i,0] == bmols[0]):\n\t\t\t\tignore_first_mol += 1\n\t\t\telse:\n\t\t\t\tsto[self.eles.index(self.scratch_test_meta[i,1])]+=1\n\t\tcurrentmol=self.scratch_test_meta[self.test_ScratchPointer+ignore_first_mol,0]\n\t\toutputs = np.zeros((noutputs, 3))\n\t\tnatom_in_mol = np.zeros((noutputs, 1))\n\t\tnatom_in_mol.fill(float(\'inf\'))\n\t\tfor e in range(len(self.eles)):\n\t\t\tinputs.append(np.zeros((sto[e],np.prod(self.dig.eshape))))\n\t\t\tmatrices.append(np.zeros((sto[e],noutputs)))\n\t\t\tcoords.append(np.zeros((sto[e], 3)))\n\t\tfor i in range(self.test_ScratchPointer+ignore_first_mol, self.test_ScratchPointer+ncases):\n\t\t\tif (self.scratch_test_meta[i,0] == bmols[-1]):\n\t\t\t\tbreak\n\t\t\tif (currentmol != self.scratch_test_meta[i,0]):\n\t\t\t\toutputpointer = outputpointer+1\n\t\t\t\tcurrentmol = self.scratch_test_meta[i,0]\n\t\t\tif not self.test_mols_done and self.test_begin_mol+currentmol not in self.test_mols:\n\t\t\t\t\tself.test_mols.append(self.test_begin_mol+currentmol)\n#\t\t\t\t\tif i < self.test_ScratchPointer+ignore_first_mol + 50:\n#\t\t\t\t\t\tprint ""i "",i, self.set.mols[self.test_mols[-1]].bonds\n\t\t\t# metadata contains: molecule index, atom type, mol start, mol stop\n\t\t\te = (self.scratch_test_meta[i,1])\n\t\t\tei = self.eles.index(e)\n\t\t\t# The offset for this element should be within the bounds or something is wrong...\n\t\t\tinputs[ei][offsets[ei],:] = self.scratch_test_inputs[i]\n\t\t\tmatrices[ei][offsets[ei],outputpointer] = 1.0\n\t\t\tcoords[ei][offsets[ei]] = self.scratch_test_xyzmeta[i]\n\t\t\tnatom_in_mol[outputpointer] = self.scratch_test_meta[i,3] - self.scratch_test_meta[i,2]\n\t\t\toutputs[outputpointer] = self.scratch_test_outputs[self.scratch_test_meta[i,0]]\n\t\t\toffsets[ei] += 1\n#\t\t\tif i < self.test_ScratchPointer+ignore_first_mol + 50:\n#\t\t\t\tprint ""first 50 meta data :"", i, self.test_ScratchPointer+ignore_first_mol, self.scratch_test_meta[i]\n\t\t#print ""inputs"",inputs\n\t\t#print ""bounds"",bounds\n\t\t#print ""matrices"",matrices\n\t\t#print ""outputs"",outputs\n#\t\tself.test_ScratchPointer += ncases\n#\t\tprint ""length of test_mols:"", len(self.test_mols)\n#\t\tprint ""outputpointer:"", outputpointer\n\t\treturn [inputs, matrices, coords, 1.0/natom_in_mol, outputs]\n\n\n\n\tdef BuildTrain(self, name_=""gdb9"",  append=False, max_nmols_=1000000):\n\t\tself.CheckShapes()\n\t\tself.name=name_\n\t\tLOGGER.info(""TensorMolData_BP_Multipole, self.type:""+self.type)\n\t\tif self.type==""frag"":\n\t\t\traise Exception(""No BP frags now"")\n\t\tnmols  = len(self.set.mols)\n\t\tnatoms = self.set.NAtoms()\n\t\tLOGGER.info( ""self.dig.eshape""+str(self.dig.eshape)+"" self.dig.lshape""+str(self.dig.lshape))\n\t\tcases = np.zeros(tuple([natoms]+list(self.dig.eshape)))\n\t\tLOGGER.info( ""cases:""+str(cases.shape))\n\t\tlabels = np.zeros(tuple([nmols]+list(self.dig.lshape)))\n\t\tself.CaseMetadata = np.zeros((natoms, 4), dtype = np.int)\n\t\tself.xyzMetadata = np.zeros((natoms, 3))\n\t\tinsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_in.npy""\n\t\toutsname = self.path+""Mol_""+name_+""_""+self.dig.name+""_out.npy""\n\t\tmetasname = self.path+""Mol_""+name_+""_""+self.dig.name+""_meta.npy"" # Used aggregate and properly sum network inputs and outputs.\n\t\txyzmetasname = self.path+""Mol_""+name_+""_""+self.dig.name+""_xyzmeta.npy"" # Used aggregate and properly sum network inputs and outputs.\n\t\tcasep=0\n\t\t# Generate the set in a random order.\n\t\tord=np.random.permutation(len(self.set.mols))\n\t\tmols_done = 0\n\t\tfor mi in ord:\n\t\t\tnat = self.set.mols[mi].NAtoms()\n\t\t\t#print ""casep:"", casep\n\t\t\tif (mols_done%1000==0):\n\t\t\t\tLOGGER.info(""Mol:""+str(mols_done))\n\t\t\tins,outs = self.dig.TrainDigest(self.set.mols[mi])\n\t\t\tif not np.all(np.isfinite(ins)):\n\t\t\t\tprint(""find a bad case, writting down xyz.."")\n\t\t\t\tself.set.mols[mi].WriteXYZfile(fpath=""."", fname=""bad_buildset_cases"")\n\t\t\t#print mi, ins.shape, outs.shape\n\t\t\tcases[casep:casep+nat] = ins\n\t\t\tlabels[mols_done] = outs\n\t\t\tcenter_xyz = self.set.mols[mi].coords - np.average(self.set.mols[mi].coords, axis=0)\n\t\t\tfor j in range(casep,casep+nat):\n\t\t\t\tself.CaseMetadata[j,0] = mols_done\n\t\t\t\tself.CaseMetadata[j,1] = self.set.mols[mi].atoms[j-casep]\n\t\t\t\tself.CaseMetadata[j,2] = casep\n\t\t\t\tself.CaseMetadata[j,3] = casep+nat\n\t\t\t\tself.xyzMetadata[j] = center_xyz[j - casep]\n\t\t\tcasep += nat\n\t\t\tmols_done = mols_done + 1\n\t\t\tif (mols_done>=max_nmols_):\n\t\t\t\tbreak\n\t\tinf = open(insname,""wb"")\n\t\touf = open(outsname,""wb"")\n\t\tmef = open(metasname,""wb"")\n\t\txyzf = open(xyzmetasname, ""wb"")\n\t\tnp.save(inf,cases[:casep,:])\n\t\tnp.save(ouf,labels[:mols_done,:])\n\t\tnp.save(mef,self.CaseMetadata[:casep,:])\n\t\tnp.save(xyzf, self.xyzMetadata[:casep,:])\n\t\tinf.close()\n\t\touf.close()\n\t\tmef.close()\n\t\txyzf.close()\n\t\tself.AvailableDataFiles.append([insname,outsname,metasname, xyzmetasname])\n\t\tself.Save() #write a convenience pickle.\n\t\treturn\n\nclass TensorMolData_BP_Multipole_2_Direct(TensorMolData_BP_Direct):\n\t""""""\n\tA tensordata for learning the multipole of molecules using Behler-Parinello scheme.\n\t""""""\n\tdef __init__(self, MSet_=None,  Dig_=None, Name_=None, order_=3, num_indis_=1, type_=""mol"", WithGrad_ = False):\n\t\tTensorMolData_BP_Direct.__init__(self, MSet_, Dig_, Name_, order_, num_indis_, type_, WithGrad_)\n\n\tdef LoadData(self):\n\t\tself.ReloadSet()\n\t\trandom.shuffle(self.set.mols)\n\t\txyzs = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((self.Nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((self.Nmols), dtype = np.int32)\n\t\tif (self.dig.OType == ""Multipole2""):\n\t\t\tlabels = np.zeros((self.Nmols, 3), dtype = np.float64)\n\t\telse:\n\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\tif (self.HasGrad):\n\t\t\tgrads = np.zeros((self.Nmols, self.MaxNAtoms, 3), dtype=np.float64)\n\t\tfor i, mol in enumerate(self.set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\t\tif (self.dig.OType  == ""Multipole2""):\n\t\t\t\tlabels[i] = mol.properties[""dipole""]*AUPERDEBYE\n\t\t\telse:\n\t\t\t\traise Exception(""Output Type is not implemented yet"")\n\t\t\tif (self.HasGrad):\n\t\t\t\tgrads[i][:mol.NAtoms()] = mol.properties[""gradients""]\n\t\tif (self.HasGrad):\n\t\t\treturn xyzs, Zs, labels, natom, grads\n\t\telse:\n\t\t\treturn xyzs, Zs, labels, natom\n\n\tdef LoadDataToScratch(self, tformer):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\ttry:\n\t\t\tself.HasGrad\n\t\texcept:\n\t\t\tself.HasGrad = False\n\t\tif (self.ScratchState == 1):\n\t\t\treturn\n\t\tif (self.HasGrad):\n\t\t\tself.xyzs, self.Zs, self.labels, self.natom, self.grads = self.LoadData()\n\t\telse:\n\t\t\tself.xyzs, self.Zs, self.labels, self.natom  = self.LoadData()\n\t\tself.NTestMols = int(self.TestRatio * self.Zs.shape[0])\n\t\tself.LastTrainMol = int(self.Zs.shape[0]-self.NTestMols)\n\t\tself.NTrain = self.LastTrainMol\n\t\tself.NTest = self.NTestMols\n\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.ScratchPointer = 0\n\t\tself.ScratchState = 1\n\t\tLOGGER.debug(""LastTrainMol in TensorMolData: %i"", self.LastTrainMol)\n\t\tLOGGER.debug(""NTestMols in TensorMolData: %i"", self.NTestMols)\n\t\treturn\n\n\tdef GetTrainBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTrain):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTrain)+"" vs ""+str(ncases))\n\t\tif (self.ScratchPointer+ncases >= self.NTrain):\n\t\t\tself.ScratchPointer = 0\n\t\tself.ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tZs = self.Zs[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tlabels = self.labels[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tnatom = self.natom[self.ScratchPointer-ncases:self.ScratchPointer]\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, labels, 1.0/natom, self.grads[self.ScratchPointer-ncases:self.ScratchPointer]]\n\t\telse:\n\t\t\treturn [xyzs, Zs, labels, 1.0/natom]\n\n\tdef GetTestBatch(self,ncases):\n\t\tif (self.ScratchState == 0):\n\t\t\tself.LoadDataToScratch()\n\t\treset = False\n\t\tif (ncases > self.NTest):\n\t\t\traise Exception(""Insufficent training data to fill a batch""+str(self.NTest)+"" vs ""+str(ncases))\n\t\tif (self.test_ScratchPointer+ncases > self.Zs.shape[0]):\n\t\t\tself.test_ScratchPointer = self.LastTrainMol\n\t\tself.test_ScratchPointer += ncases\n\t\txyzs = self.xyzs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tZs = self.Zs[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tlabels = self.labels[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tnatom = self.natom[self.test_ScratchPointer-ncases:self.test_ScratchPointer]\n\t\tif (self.HasGrad):\n\t\t\treturn [xyzs, Zs, labels, 1.0/natom, self.grads[self.test_ScratchPointer-ncases:self.test_ScratchPointer]]\n\t\telse:\n\t\t\treturn [xyzs, Zs, labels, 1.0/natom]\n'"
TensorMol/Containers/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom .Mol import *\nfrom .Sets import *\nfrom .DigestMol import *\nfrom .TensorMolData import * \n'
TensorMol/ForceModels/Electrostatics.py,0,"b'""""""\nRoutines for calculating dipoles quadropoles, etc, and cutoff electrostatic energies in python\nSee also: ElectrostaticsTF for tensorflow implementations of electrostatics.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Util import *\nimport numpy as np\nimport random, math\nimport MolEmb\n\ndef WeightedCoordAverage(x_, q_, center_=None):\n\t"""""" Dipole relative to center of x_ """"""\n\tif (center_== None):\n\t\tcenter_ = np.average(x_,axis=0)\n\treturn np.einsum(""ax,a..."", x_-center_ , q_)\n\ndef DipoleDebug(m_):\n\t\tif (""dipole"" in m_.properties and ""charges"" in m_.properties):\n\t\t\tprint(""Qchem, Calc\'d"", m_.properties[""dipole""]*AUPERDEBYE, WeightedCoordAverage(m_.coords*BOHRPERA, m_.properties[""charges""], m_.Center()))\n\ndef Dipole(x_, q_):\n\t"""""" Arguments are in A, and elementary charges.  """"""\n\treturn WeightedCoordAverage(x_*BOHRPERA, q_)\n\n\ndef Dipole_Naive(x_, q_):\n\t"""""" Arguments are in A, and elementary charges.  """"""\n\tdipole = np.zeros(3)\n\tfor i in  range(0, q_.shape[0]):\n\t\tdipole += q_[i]*x_[i]*BOHRPERA\n\treturn dipole\n\ndef ChargeCharge(m1_, m2_):\n\t""""""calculate  the charge-charge interaction energy between two molecules""""""\n\tcc_energy = 0.0\n\tfor i in range (0, m1_.NAtoms()):\n\t\tfor j in range (0, m2_.NAtoms()):\n\t\t\tdist = (np.sum(np.square(m1_.coords[i] - m2_.coords[j])))**0.5 * BOHRPERA\n\t\t\tcc_energy += m1_.properties[\'atom_charges\'][i]*m2_.properties[\'atom_charges\'][j]/dist\n\treturn cc_energy\n\ndef Dimer_ChargeCharge(m_):\n\t""""""calculate the charge-charge interaction between two monomer in a dimer""""""\n\tcc_energy = 0.0\n\tseperate_index = m_.properties[""natom_each_mono""][0]\n\tif type(m_.DistMatrix) is not np.ndarray:\n\t\tm_.DistMatrix = MolEmb.Make_DistMat(m_.coords)\n\tfor i in range (0, seperate_index):\n\t\tfor j in range (seperate_index, m_.NAtoms()):\n\t\t\tcc_energy += m_.properties[\'atom_charges\'][i]*m_.properties[\'atom_charges\'][j]/(m_.DistMatrix[i][j]*BOHRPERA)\n\treturn cc_energy\n\n\ndef Dimer_Replusive(m_):\n\t""""""calculate the charge-charge interaction between two monomer in a dimer""""""\n\treplu_energy = 0.0\n\tif type(m_.DistMatrix) is not np.ndarray:\n\t\tm_.DistMatrix = MolEmb.Make_DistMat(m_.coords)\n\tseperate_index = m_.properties[""natom_each_mono""][0]\n\tif type(m_.DistMatrix) is not np.ndarray:\n\t\tm_.DistMatrix = MolEmb.Make_DistMat(m_.coords)\n\tfor i in range (0, seperate_index):\n\t\tfor j in range (seperate_index, m_.NAtoms()):\n\t\t\treplu_energy += 0.1/(m_.DistMatrix[i][j])**12\n\treturn replu_energy\n\ndef Dimer_ChargeCharge_Grad(m_):\n\t""""""calculate the gradient of charge-charge interaction between two monomer in a dimer""""""\n\tif type(m_.DistMatrix) is not np.ndarray:\n\t\tm_.DistMatrix = MolEmb.Make_DistMat(m_.coords)\n\tcc_energy_grad = np.zeros((m_.NAtoms(), 3))\n\tseperate_index = m_.properties[""natom_each_mono""][0]\n\tfor i in range (0, m_.NAtoms()):\n\t\tfor j in range (0, seperate_index):\n\t\t\tfor k in range (seperate_index, m_.NAtoms()):\n\t\t\t\tfor q in range (0, 3):\n\t\t\t\t\tif j == i:\n\t\t\t\t\t\tcc_energy_grad[i][q] += (m_.properties[\'atom_charges_grads\'][j][i][q]*m_.properties[\'atom_charges\'][k]+m_.properties[\'atom_charges\'][j]*m_.properties[\'atom_charges_grads\'][k][i][q])/(m_.DistMatrix[j][k]*BOHRPERA) - (m_.properties[\'atom_charges\'][j]*m_.properties[\'atom_charges\'][k]*(m_.coords[j][q]-m_.coords[k][q]))/(m_.DistMatrix[j][k]*m_.DistMatrix[j][k]*m_.DistMatrix[j][k]*BOHRPERA)\n\t\t\t\t\telif k == i:\n\t\t\t\t\t\tcc_energy_grad[i][q] += (m_.properties[\'atom_charges_grads\'][j][i][q]*m_.properties[\'atom_charges\'][k]+m_.properties[\'atom_charges\'][j]*m_.properties[\'atom_charges_grads\'][k][i][q])/(m_.DistMatrix[j][k]*BOHRPERA) - (m_.properties[\'atom_charges\'][j]*m_.properties[\'atom_charges\'][k]*(m_.coords[k][q]-m_.coords[j][q]))/(m_.DistMatrix[j][k]*m_.DistMatrix[j][k]*m_.DistMatrix[j][k]*BOHRPERA)\n\t\t\t\t\telse:\n\t\t\t\t\t\tcc_energy_grad[i][q] += (m_.properties[\'atom_charges_grads\'][j][i][q]*m_.properties[\'atom_charges\'][k]+m_.properties[\'atom_charges\'][j]*m_.properties[\'atom_charges_grads\'][k][i][q])/(m_.DistMatrix[j][k]*BOHRPERA)\n\treturn cc_energy_grad\n\ndef Dimer_Cutoff_Grad(m_, dist_, cutoff_, cutoff_width_):\n\t""""""calculate the gradient of cutoff function: (1+tanh((dist-cutoff)/cutoff_width))/2.0, where dist is the distance between the COM of two monoers""""""\n\tif type(m_.DistMatrix) is not np.ndarray:\n\t\tm_.DistMatrix = MolEmb.Make_DistMat(m_.coords)\n\tcutoff_grad = np.zeros((m_.NAtoms(), 3))\n\tseperate_index = m_.properties[""natom_each_mono""][0]\n\tmass = np.array(map(lambda x: ATOMICMASSES[x-1],m_.atoms))\n\tmass_sum_1 = np.sum(mass[:seperate_index])\n\tmass_sum_2 = np.sum(mass[seperate_index:])\n\tA = 1.0/2.0*(1 - math.tanh((dist_ - cutoff_)/cutoff_width_)**2)/cutoff_width_\n\tfor i in range (0, m_.NAtoms()):\n\t\tfor q in range (0, 3):\n\t\t\tif i < seperate_index:\n\t\t\t\tcutoff_grad[i][q] = A/dist_*(m_.properties[""center""][0][q] - m_.properties[""center""][1][q])*mass[i]/mass_sum_1\n\t\t\telse:\n\t\t\t\tcutoff_grad[i][q] = A/dist_*(m_.properties[""center""][1][q] - m_.properties[""center""][0][q])*mass[i]/mass_sum_2\n\treturn cutoff_grad\n\ndef Dimer_Replusive_Grad(m_):\n\tif type(m_.DistMatrix) is not np.ndarray:\n\t\tm_.DistMatrix = MolEmb.Make_DistMat(m_.coords)\n\treplu_energy_grad = np.zeros((m_.NAtoms(), 3))\n\tseperate_index = m_.properties[""natom_each_mono""][0]\n\tfor i in range (0, m_.NAtoms()):\n\t\tfor j in range (0, seperate_index):\n\t\t\tfor k in range (seperate_index, m_.NAtoms()):\n\t\t\t\tfor q in range (0, 3):\n\t\t\t\t\tif j == i:\n\t\t\t\t\t\treplu_energy_grad[i][q] +=  - 0.1*(m_.coords[j][q]-m_.coords[k][q])/(m_.DistMatrix[j][k]**14)\n\t\t\t\t\telif k == i:\n\t\t\t\t\t\treplu_energy_grad[i][q] +=  - 0.1*(m_.coords[k][q]-m_.coords[j][q])/(m_.DistMatrix[j][k]**14)\n\t\t\t\t\telse:\n\t\t\t\t\t\tcontinue\n\treturn replu_energy_grad\n\ndef ElectricFieldForce(q_,E_):\n\t""""""\n\tBoth are received in atomic units.\n\tThe force should be returned in kg(m/s)^2, but I haven\'t fixed the units yet.\n\t""""""\n\treturn np.einsum(""q,x->qx"",q_,E_)\n\ndef ECoulECutoff(m_):\n\tdm = MolEmb.Make_DistMat(m_.coords)*BOHRPERA\n\tdm += np.eye(len(m_.coords))\n\tidm = 1.0/dm\n\tidm *= (np.ones(len(m_.coords)) - np.eye(len(m_.coords)))\n\tECoul = np.dot(m_.properties[""charges""],np.dot(idm,m_.properties[""charges""]))\n\t# 1/r => 0.5*(Tanh[(x - EECutoff)/EEdr] + 1)/r\n\tOneOverRScreened = 0.5*(np.tanh((dm - PARAMS[""EECutoff""]*BOHRPERA)/(PARAMS[""EEdr""]*BOHRPERA))+1)*idm\n\tECutoff = np.dot(m_.properties[""charges""],np.dot(OneOverRScreened, m_.properties[""charges""]))\n\tprint(ECoul, ECutoff)\n\treturn\n\ndef WriteDerDipoleCorrelationFunction(MuTraj, name_=""MutMu0.txt""):\n\t""""""\n\tArgs:\n\t\ttime, mux, muy, muz ...\n\tReturns: \\sum_i \\langle \\dot{\\mu_i(t)}\\cdot \\dot{\\mu_i(0)} \\rangle\n\t""""""\n\tdt = MuTraj[1,0] - MuTraj[0,0]\n\tdmu = np.diff(MuTraj[:,1:4],axis=0)/dt\n\tn = dmu.shape[0]\n\tnkept = int(n/4.)\n\ttore = np.zeros((nkept,2))\n\ttore[:,0] = np.array(range(nkept))*dt\n\ttore[:nkept,1] = MolEmb.DipoleAutoCorr(dmu)[:nkept,0]\n\tnp.savetxt(""./results/""+name_,tore)\n\treturn tore\n\ndef WriteDerDipoleCorrelationFunctionOld(MuTraj, name_=""MutMu0.txt""):\n\t""""""\n\tArgs:\n\t\ttime, mux, muy, muz ...\n\tReturns: \\sum_i \\langle \\dot{\\mu_i(t)}\\cdot \\dot{\\mu_i(0)} \\rangle\n\t""""""\n\tdt = MuTraj[1,0] - MuTraj[0,0]\n\tt0 = np.zeros((MuTraj.shape[0]-1,4))\n\tfor i in range(MuTraj.shape[0]-1):\n\t\tt0[i,0] = MuTraj[i,0]\n\t\tt0[i,1:4] = (MuTraj[i+1,1:4]- MuTraj[i,1:4])/dt\n\tn = t0.shape[0]\n\ttore = np.zeros((n,2))\n\tfor i in range(int(n/4)): # Can\'t use the whole data....\n\t\ttore[i,0] = t0[i,0]\n\t\ttore[i,1] = 0.0\n\t\tfor j in range(n-i):\n\t\t\ttore[i,1] +=  np.dot(t0[j,1:4],t0[i+j,1:4])\n\t\ttore[i,1] /= 3.*float(n-i)\n\tnp.savetxt(""./results/""+name_,tore)\n\treturn tore\n\ndef WriteVelocityAutocorrelations(muhis,vhis):\n\t""""""\n\tArgs:\n\t\tGenerate velocity autocorrelation functions.\n\t""""""\n\tdt = muhis[0,0] - muhis[1,0]\n\tn = muhis.shape[0]\n\tnatom = vhis.shape[1]\n\tncorr = int(n/4)\n\tfor atom in range(natom):\n\t\ttore = np.zeros((ncorr,2))\n\t\tfor i in range(ncorr):\n\t\t\ttore[i,0] = muhis[i,0]\n\t\t\ttore[i,1] = 0.0\n\t\t\tfor j in range(n-i):\n\t\t\t\ttore[i,1] +=  np.dot(vhis[j,atom],vhis[i+j,atom])\n\t\t\ttore[i,1] /= 3.*float(n-i)\n\t\tnp.savetxt(""./results/""+""VtV0_""+str(atom)+"".txt"",tore)\n\treturn\n\ndef WriteDipoleCorrelationFunction(t0,t1,t2):\n\t""""""\n\tArgs:\n\t\ttime, mux, muy, muz ...\n\tReturns: \\sum_i \\langle \\mu_i(t)\\cdot \\mu_i(0) \\rangle\n\t""""""\n\tdt = t0[0,0] - t0[1,0]\n\tn = t0.shape[0]\n\ttore = np.zeros((n,2))\n\tfor i in range(n):\n\t\ttore[i,0] = t0[i,0]\n\t\ttore[i,1] = 0.0\n\t\tfor j in range(n-i):\n\t\t\ttore[i,1] +=  t0[j,1]*t0[i+j,1]+t1[j,2]*t1[i+j,2]+t2[j,2]*t2[i+j,2]\n\t\ttore[i,1] /= 3.*float(n-i)\n\tnp.savetxt(""./results/""+""MutMu0.txt"",tore)\n\treturn tore\n'"
TensorMol/ForceModels/ElectrostaticsTF.py,210,"b'""""""\nThis file contains routines for calculating electrostatic energies and forces\nusing tensorflow. No training etc. These functions are used as a utility in\nother Instance models. Some of these functions mirror Electrostatics.py\n\nPosition units are Bohr, and energy units are Hartree\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport math, time, os, sys, os.path\nfrom ..Util import *\n\nif (HAS_TF):\n\timport tensorflow as tf\n\ndef TFMatrixPower(mat_,exp_):\n\t""""""\n\tGeneral Matrix Power in Tensorflow.\n\tThis is NOT differentiable as of 1.2.\n\ttf.matrix_inverse and tf.matrix_determinant are though.\n\t""""""\n\ts,u,v = tf.svd(mat_,full_matrices=True,compute_uv=True)\n\treturn tf.transpose(tf.matmul(u,tf.matmul(tf.diag(tf.pow(s,exp_)),tf.transpose(v))))\n\ndef TFMatrixSqrt(mat_):\n\t""""""\n\tUse Denman-Beavers iteration to compute a\n\tMatrix Square root differentiably.\n\t""""""\n\tcond = lambda i,y,z: i<10\n\tbody = lambda i,y,z: [i+1,0.5*(y+tf.matrix_inverse(z)),0.5*(z+tf.matrix_inverse(y))]\n\tinitial = (0,a,tf.eye(tf.shape(a)[0]))\n\tI,Y,Z = tf.while_loop(cond,body,initial)\n\treturn Y,Z\n\ndef TFDistance(A):\n\t""""""\n\tCompute a distance matrix of A, a coordinate matrix\n\tUsing the factorization:\n\tDij = <i|i> - 2<i|j> + <j,j>\n\tArgs:\n\t\tA: a Nx3 matrix\n\tReturns:\n\t\tD: a NxN matrix\n\t""""""\n\tr = tf.reduce_sum(A*A, 1)\n\tr = tf.reshape(r, [-1, 1]) # For the later broadcast.\n\t# Tensorflow can only reverse mode grad the sqrt if all these elements\n\t# are nonzero\n\tD = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r)\n\treturn tf.sqrt(tf.clip_by_value(D,1e-36,1e36))\n\ndef TFDistances(r_):\n\t""""""\n\tReturns distance matrices batched over mols\n\tArgs:\n\t\tr_: Nmol X MaxNAtom X 3 coordinate tensor\n\tReturns\n\t\tD: Nmol X MaxNAtom X MaxNAtom Distance tensor.\n\t""""""\n\trm = tf.einsum(\'ijk,ijk->ij\',r_,r_) # Mols , Rsq.\n\trshp = tf.shape(rm)\n\trmt = tf.tile(rm, [1,rshp[1]])\n\trmt = tf.reshape(rmt,[rshp[0],rshp[1],rshp[1]])\n\trmtt = tf.transpose(rmt,perm=[0,2,1])\n\t# Tensorflow can only reverse mode grad of sqrt if all these elements\n\t# are nonzero\n\tD = rmt - 2*tf.einsum(\'ijk,ilk->ijl\',r_,r_) + rmtt\n\treturn tf.sqrt(tf.clip_by_value(D,1e-36,1e36))\n\ndef TFDistanceLinear(B,NZP):\n\t""""""\n\tCompute a distance vector of B, a coordinate matrix\n\tArgs:\n\t\tB: a Nx3 matrix\n\t\tNZP: a (nonzero pairs X 2) index matrix.\n\tReturns:\n\t\tD: a NZP X 1 tensor of distances.\n\t""""""\n\tIi = tf.slice(NZP,[0,0],[-1,1])\n\tIj = tf.slice(NZP,[0,1],[-1,1])\n\tRi = tf.gather_nd(B,Ii)\n\tRj = tf.gather_nd(B,Ij)\n\tA = Ri - Rj\n\treturn tf.sqrt(tf.clip_by_value(tf.reduce_sum(A*A, 1),1e-36,1e36))\n\ndef TFDistancesLinear(B,NZP):\n\t""""""\n\tReturns distance vector batched over mols\n\tWith these sparse versions I think the mol dimension should be eliminated.\n\n\tArgs:\n\t\tr_: Nmol X MaxNAtom X 3 coordinate tensor\n\t\tNZP: a ( nonzero pairs X 3) index matrix. (mol, i, j)\n\tReturns\n\t\tD: nonzero pairs  Distance vector. (Dij)\n\t\tThe calling routine has to scatter back into mol dimension using NZP if desired.\n\t""""""\n\tIi = tf.slice(NZP,[0,0],[-1,2])\n\tIj = tf.concat([tf.slice(NZP,[0,0],[-1,1]),tf.slice(NZP,[0,2],[-1,1])],1)\n\tRi = tf.gather_nd(B,Ii)\n\tRj = tf.gather_nd(B,Ij)\n\tA = Ri - Rj\n\tD = tf.sqrt(tf.clip_by_value(tf.reduce_sum(A*A, 1),1e-36,1e36))\n\treturn D\n\ndef BumpEnergy(h,w,xyz,x,nbump):\n\t""""""\n\tA -1*potential energy which is just the sum of gaussians\n\twith height h and width w at positions xyz sampled at x.\n\tThis uses distance matrices to maintain rotational invariance.\n\tThe factor of negative 1 is because we only desire the force...\n\n\tArgs:\n\t\th: bump height\n\t\tw: bump width\n\t\txyz: a nbump X N X 3 tensor of bump centers.\n\t\tx: (n X 3) tensor representing the point at which the energy is sampled.\n\t\tnbump: an integer determining the number of nonzero bumps.\n\t""""""\n\txshp = tf.shape(x)\n\tnx = xshp[0]\n\tNzxyz = tf.slice(xyz,[0,0,0],[nbump,nx,3])\n\tDs = TFDistances(Nzxyz) # nbump X MaxNAtom X MaxNAtom Distance tensor.\n\tDx = TFDistance(x) # MaxNAtom X MaxNAtom Distance tensor.\t#sqrt2pi = tf.constant(2.50662827463100,dtype = tf.float64)\n\tw2 = w*w\n\trij = (Ds - tf.tile(tf.reshape(Dx,[1,nx,nx]),[nbump,1,1]))\n\tToExp = tf.einsum(\'ijk,ijk->i\',rij,rij)\n\tToSum = -1.0*h*tf.exp(-0.5*ToExp/w2)\n\treturn tf.reduce_sum(ToSum,axis=0)\n\ndef BumpEnergyMR(h,w,xyz,x,nbump):\n\t""""""\n\tA -1*potential energy which is just the sum of gaussians\n\twith height h and width w at positions xyz sampled at x.\n\tThis uses distance matrices to maintain rotational invariance.\n\tThe factor of negative 1 is because we only desire the force...\n\n\tThis version screens the difference so that distant and bonded atoms do not contribute to the force.\n\n\tArgs:\n\t\th: bump height\n\t\tw: bump width\n\t\txyz: a nbump X N X 3 tensor of bump centers.\n\t\tx: (n X 3) tensor representing the point at which the energy is sampled.\n\t\tnbump: an integer determining the number of nonzero bumps.\n\t""""""\n\txshp = tf.shape(x)\n\tnx = xshp[0]\n\tNzxyz = tf.slice(xyz,[0,0,0],[nbump,nx,3])\n\tDs = TFDistances(Nzxyz) # nbump X MaxNAtom X MaxNAtom Distance tensor.\n\tDx = TFDistance(x) # MaxNAtom X MaxNAtom Distance tensor.\t#sqrt2pi = tf.constant(2.50662827463100,dtype = tf.float64)\n\tw2 = w*w\n\trij = Ds - Dx[tf.newaxis,:,:]\n\tToExp = rij*rij\n\tsigma = 5.0\n\t# The bump is larger in real space the further apart the atoms are.\n\t# Screen local structure, so only long range changes contribute to the difference most important.\n\tScreen = w2*(1e0+(Dx*Dx)[tf.newaxis,:,:])\n\tToProd = tf.exp(-0.5*ToExp/Screen)\n\t# We want the product of the gaussians for each bump within the sensory radius.\n\t# then we want to sum between bumps.\n\tToSum = tf.reduce_prod(ToProd,axis=[1,2])\n\treturn -1.0*h*tf.reduce_sum(ToSum,axis=0)\n\ndef BowlEnergy(BowlK,x):\n\t""""""\n\tA bowl which attracts everything to 0,0,0\n\twith the energy tf.sqrt(x.x)\n\n\tArgs:\n\t\tBowlK: the bowl force constant.\n\t\tx: (n X 3) tensor representing the point at which the energy is sampled.\n\t""""""\n\txshp = tf.shape(x)\n\tnx = xshp[0]\n\tDs = tf.einsum(\'ij,ij->i\',x,x)\n\treturn tf.reduce_sum(-1*BowlK*((Ds)+1e-26),axis=0)\n\ndef MorseKernel(D,Z,Ae,De,Re):\n\t""""""\n\tArgs:\n\t\tD: A square distance matrix (bohr)\n\t\tZ: Atomic Numbers.\n\t\tAe: a matrix of force constants.\n\t\tDe: a matrix of Morse De parameters. (MaxAtomicNumber X MaxAtomicNumber)\n\t\tRe: a matrix of\n\t""""""\n\t# Extract De_ij and Re_ij\n\tZshp = tf.shape(Z)\n\tZr = tf.reshape(Z,[Zshp[0],1])-1 # Indices start at 0 AN\'s start at 1.\n\tZij1 = tf.tile(Zr,[1,Zshp[0]])\n\tZij2 = tf.transpose(Zij1)\n\tZij = tf.stack([Zij1,Zij2],axis=2) # atomXatomX2\n\tZij = tf.reshape(Zij,[Zshp[0]*Zshp[0],2])\n\tAeij = tf.reshape(tf.gather_nd(Ae,Zij),tf.shape(D))\n\tDeij = tf.reshape(tf.gather_nd(De,Zij),tf.shape(D))\n\tReij = tf.reshape(tf.gather_nd(Re,Zij),tf.shape(D))\n\tDt = D + tf.eye(Zshp[0])\n\t# actually compute the kernel.\n\tK = Deij*tf.pow(1.0 - tf.exp(-Aeij*(Dt-Reij)),2.0)\n\tK = tf.subtract(K,tf.diag(tf.diag_part(K)))\n\tK = tf.matrix_band_part(K, 0, -1) # Extract upper triangle\n\treturn K\n\ndef LJKernel(D,Z,Ee,Re):\n\t""""""\n\tA Lennard-Jones Kernel\n\tArgs:\n\t\tD: A square distance matrix (bohr)\n\t\tZ: Atomic Numbers.\n\t\tEe: a matrix of LJ well depths.\n\t\tRe: a matrix of Bond minima.\n\t""""""\n\t# Extract De_ij and Re_ij\n\tZshp = tf.shape(Z)\n\tZr = tf.reshape(Z,[Zshp[0],1])-1 # Indices start at 0 AN\'s start at 1.\n\tZij1 = tf.tile(Zr,[1,Zshp[0]])\n\tZij2 = tf.transpose(Zij1)\n\tZij = tf.stack([Zij1,Zij2],axis=2) # atomXatomX2\n\tZij = tf.reshape(Zij,[Zshp[0]*Zshp[0],2])\n\tEeij = tf.reshape(tf.gather_nd(Ee,Zij),tf.shape(D))\n\tReij = tf.reshape(tf.gather_nd(Re,Zij),tf.shape(D))\n\tReij = tf.Print(Reij,[Reij],""Reij"",10000,1000)\n\tDt = D + tf.eye(Zshp[0])\n\tK = Eeij*(tf.pow(Reij/Dt,12.0)-2.0*tf.pow(Reij/Dt,6.0))\n\tK = tf.subtract(K,tf.diag(tf.diag_part(K)))\n\tK = tf.matrix_band_part(K, 0, -1) # Extract upper triangle\n\treturn K\n\ndef LJKernels(Ds,Zs,Ee,Re):\n\t""""""\n\tBatched over molecules.\n\tArgs:\n\t\tDs: A batch of square distance matrix (bohr)\n\t\tZs: A batch of Atomic Numbers.\n\t\tEe: a matrix of LJ well depths.\n\t\tRe: a matrix of Bond minima.\n\tReturns\n\t\tA #Mols X MaxNAtoms X MaxNAtoms matrix of LJ kernel contributions.\n\t""""""\n\t# Zero distances will be set to 100.0 then masked to zero energy contributions.\n\tones = tf.ones(tf.shape(Ds),dtype = tf.float64)\n\tzeros = tf.zeros(tf.shape(Ds),dtype = tf.float64)\n\tZeroTensor = tf.where(tf.less_equal(Ds,0.000000001),ones,zeros)\n\tDs += ZeroTensor\n\t# Zero atomic numbers will be set to 1 and masked elsewhere\n\tZs = tf.where(tf.equal(Zs,0),tf.ones_like(Zs),Zs)\n\t# Extract De_ij and Re_ij\n\tZshp = tf.shape(Zs)\n\tZr = tf.reshape(Zs,[Zshp[0],Zshp[1],1])-1 # Indices start at 0 AN\'s start at 1.\n\tZij1 = tf.tile(Zr,[1,1,Zshp[1]]) # molXatomXatom\n\tZij2 = tf.transpose(Zij1,perm=[0,2,1])\n\tZij = tf.stack([Zij1,Zij2],axis=3) # molXatomXatomX2\n\t# Gather desired LJ parameters.\n\tZij = tf.reshape(Zij,[Zshp[0]*Zshp[1]*Zshp[1],2])\n\tEeij = tf.reshape(tf.gather_nd(Ee,Zij),[Zshp[0],Zshp[1],Zshp[1]])\n\tReij = tf.reshape(tf.gather_nd(Re,Zij),[Zshp[0],Zshp[1],Zshp[1]])\n\tR = Reij/Ds\n\tK = Eeij*(tf.pow(R,12.0)-2.0*tf.pow(R,6.0))\n\t# Use the ZeroTensors to mask the output for zero dist or AN.\n\tK = tf.where(tf.equal(ZeroTensor,1.0),tf.zeros_like(K),K)\n\tK = tf.where(tf.is_nan(K),tf.zeros_like(K),K)\n\tK = tf.matrix_band_part(K, 0, -1) # Extract upper triangle of each.\n\treturn K\n\ndef LJKernelLinear(Ds,Zs,Ee,Re,NZP):\n\t""""""\n\tLinear Scaling Lennard-Jones Energy for a single Molecule.\n\n\tArgs:\n\t\tDs: Distances Enumerated by NZP (flat)\n\t\tZs: A batch of Atomic Numbers. (maxatom X 1)\n\t\tEe: a matrix of LJ well depths.\n\t\tRe: a matrix of Bond minima.\n\t\tNZP: a list of nonzero atom pairs NNZ X 2 = (i, j).\n\tReturns\n\t\tLJ energy.\n\t""""""\n\tNZP_shape = tf.shape(NZP)\n\tZs_shp = tf.shape(Zs)\n\tNZP_shape = tf.Print(NZP_shape,[NZP_shape])\n\tmaxnpairs = tf.shape(NZP)[0]\n\tIi = tf.slice(NZP,[0,0],[-1,1])\n\tIj = tf.slice(NZP,[0,1],[-1,1])\n\tZi = tf.reshape(tf.gather_nd(Zs,Ii),[tf.shape(Ii)[0],1])\n\tZj = tf.reshape(tf.gather_nd(Zs,Ij),[tf.shape(Ii)[0],1])\n\tZij = tf.concat([Zi,Zj],axis=1)\n\tEeij = tf.gather_nd(Ee,Zij)\n\tReij = tf.gather_nd(Re,Zij)\n\tR = Reij/Ds\n\tK = Eeij*(tf.pow(R,12.0)-2.0*tf.pow(R,6.0))\n\tK = tf.where(tf.is_nan(K),tf.zeros_like(K),K)\n\tK = tf.reduce_sum(K,axis=0)\n\treturn K\n\ndef LJKernelsLinear(Ds,Zs,Ee,Re,NZP):\n\t""""""\n\tBatched over molecules.\n\tArgs:\n\t\tDs: Distances Enumerated by NZP (flat)\n\t\tZs: A batch of Atomic Numbers. (nmol X maxatom X 1)\n\t\tEe: a matrix of LJ well depths.\n\t\tRe: a matrix of Bond minima.\n\t\tNZP: a list of nonzero atom pairs NNZ X (mol, i, j).\n\tReturns\n\t\tA #Mols vector of LJ energies.\n\t""""""\n\tNZP_shape = tf.shape(NZP)\n\tZs_shp = tf.shape(Zs)\n\tmaxnpairs = NZP_shape[0]\n\tnmols = Zs_shp[0]\n\tIi = tf.slice(NZP,[0,0],[-1,2])\n\tIj = tf.concat([tf.slice(NZP,[0,0],[-1,1]),tf.slice(NZP,[0,2],[-1,1])],1)\n\tZi = tf.reshape(tf.gather_nd(Zs,Ii),[maxnpairs])\n\tZj = tf.reshape(tf.gather_nd(Zs,Ij),[maxnpairs])\n\t# Gather desired LJ parameters.\n\tZij = tf.stack([Zi,Zj],axis=1)\n\tEeij = tf.reshape(tf.gather_nd(Ee,Zij),[maxnpairs])\n\tReij = tf.reshape(tf.gather_nd(Re,Zij),[maxnpairs])\n\tR = Reij/tf.reshape(Ds,[maxnpairs])\n\tK = Eeij*(tf.pow(R,12.0)-2.0*tf.pow(R,6.0))\n\tK = tf.where(tf.is_nan(K),tf.zeros_like(K),K)\n\trange_index = tf.reshape(tf.range(tf.cast(maxnpairs, tf.int64), dtype=tf.int64),[maxnpairs,1])\n\tmol_index = tf.reshape(tf.slice(NZP,[0,0],[-1,1]),[maxnpairs,1])\n\tinds = tf.reshape(tf.stack([mol_index,range_index],axis=1),[maxnpairs,2])\n\t# Now use the sparse reduce sum trick to scatter this into mols.\n\tsp_atomoutputs = tf.SparseTensor(inds, tf.reshape(K,[maxnpairs]), dense_shape=[tf.cast(nmols, tf.int64), tf.cast(maxnpairs, tf.int64)])\n\treturn tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\ndef ElectrostaticDampedShiftedLinear(Ds,Qs,NZP,alpha,Rc):\n\t""""""\n\tA tensorflow linear scaling implementation of the Damped Shifted Electrostatic Force\n\thttp://aip.scitation.org.proxy.library.nd.edu/doi/pdf/10.1063/1.2206581\n\tBatched over molecules.\n\n\tArgs:\n\t\tDs: Distances Enumerated by NZP (flat)\n\t\tQs: A batch of Atomic Charges. (nmol X maxatom)\n\t\tNZP: a list of nonzero atom pairs NNZ X (mol, i, j).\n\t\talpha: DSF alpha parameter (~0.2)\n\t\tRc: DSF Rc parameter. (15A)\n\tReturns\n\t\tA #Mols X MaxNAtoms X MaxNAtoms matrix of LJ kernel contributions.\n\t""""""\n\ttwooversqrtpi = tf.constant(1.1283791671,dtype=tf.float64)\n\tNZP_shape = tf.shape(NZP)\n\tZs_shp = tf.shape(Zs)\n\tmaxnpairs = NZP_shape[0]\n\tnmols = Zs_shp[0]\n\tIi = tf.slice(NZP,[0,0],[-1,2])\n\tIj = tf.concat([tf.slice(NZP,[0,0],[-1,1]),tf.slice(NZP,[0,2],[-1,1])],1)\n\tQi = tf.reshape(tf.gather_nd(Qs,Ii),[maxnpairs])\n\tQj = tf.reshape(tf.gather_nd(Qs,Ij),[maxnpairs])\n\t# Gather desired LJ parameters.\n\tQij = Qi*Qj\n\t# This is Dan\'s Equation (18)\n\tXX = alpha*Rc\n\tZZ = tf.erfc(XX)/Rc\n\tYY = twooversqrtpi*alpha*tf.exp(-XX*XX)/Rc\n\tK = Qij*(tf.erfc(alpha*Ds)/Ds - ZZ + (Ds-Rc)*(ZZ/Rc+YY))\n\tK = tf.where(tf.is_nan(K),tf.zeros_like(K),K)\n\trange_index = tf.reshape(tf.range(tf.cast(maxnpairs, tf.int64), dtype=tf.int64),[maxnpairs,1])\n\tmol_index = tf.reshape(tf.slice(NZP,[0,0],[-1,1]),[maxnpairs,1])\n\tinds = tf.reshape(tf.stack([mol_index,range_index],axis=1),[maxnpairs,2])\n\t# Now use the sparse reduce sum trick to scatter this into mols.\n\tsp_atomoutputs = tf.SparseTensor(inds, tf.reshape(K,[maxnpairs]), dense_shape=[tf.cast(nmols, tf.int64), tf.cast(maxnpairs, tf.int64)])\n\treturn tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\ndef LJEnergy_Numpy(XYZ,Z,Ee,Re):\n\t""""""\n\tThe same as the routine below, but\n\tin numpy just to test.\n\t""""""\n\tn = XYZ.shape[0]\n\tD = np.zeros((n,n))\n\tfor i in range(n):\n\t\tD[i,i] = 1.0\n\t\tfor j in range(n):\n\t\t\tif i == j:\n\t\t\t\tcontinue\n\t\t\tD[i,j] = np.linalg.norm(XYZ[i]-XYZ[j])\n\tR = 1.0/D\n\tK = 0.01*(np.power(R,12.0)-2.0*np.power(R,6.0))\n\tEn = 0.0\n\tfor i in range(n):\n\t\tfor j in range(n):\n\t\t\tif j<=i:\n\t\t\t\tK[i,j] = 0.\n\t\t\telse:\n\t\t\t\tEn += K[i,j]\n\treturn En\n\ndef LJEnergy(XYZs_,Zs_,Ee_, Re_):\n\t""""""\n\tReturns LJ Energy of single molecule.\n\tInput can be padded with zeros. That will be\n\tremoved by LJKernels.\n\n\tArgs:\n\t\tXYZs_: maxatom X 3 coordinate tensor.\n\t\tZs_: maxatom X 1 atomic number tensor.\n\t\tEe_: MAX_ATOMIC_NUMBER X MAX_ATOMIC_NUMBER Epsilon parameter matrix.\n\t\tRe_: MAX_ATOMIC_NUMBER X MAX_ATOMIC_NUMBER Re parameter matrix.\n\t""""""\n\tDs = TFDistance(XYZs_)\n\tDs = tf.where(tf.is_nan(Ds), tf.zeros_like(Ds), Ds)\n\tKs = LJKernel(Ds,Zs_,Ee_,Re_)\n\tEns = tf.reduce_sum(Ks)\n\treturn Ens\n\ndef LJEnergies(XYZs_,Zs_,Ee_, Re_):\n\t""""""\n\tReturns LJ Energies batched over molecules.\n\tInput can be padded with zeros. That will be\n\tremoved by LJKernels.\n\n\tArgs:\n\t\tXYZs_: nmols X maxatom X 3 coordinate tensor.\n\t\tZs_: nmols X maxatom X 1 atomic number tensor.\n\t\tEe_: MAX_ATOMIC_NUMBER X MAX_ATOMIC_NUMBER Epsilon parameter matrix.\n\t\tRe_: MAX_ATOMIC_NUMBER X MAX_ATOMIC_NUMBER Re parameter matrix.\n\t""""""\n\tDs = TFDistances(XYZs_)\n\tDs = tf.where(tf.is_nan(Ds), tf.zeros_like(Ds), Ds)\n\tLJe = Ee_*tf.ones([8,8],dtype = tf.float64)\n\tLJr = Re_*tf.ones([8,8],dtype = tf.float64)\n\tKs = LJKernels(Ds,Zs_,LJe,LJr)\n\tEns = tf.reduce_sum(Ks,[1,2])\n\treturn Ens\n\ndef LJEnergyLinear(XYZs_,Zs_,Ee_, Re_, NZP_):\n\t""""""\n\tLinear scaling Lennard-Jones energy.\n\n\tArgs:\n\t\tXYZs_: maxatom X 3 coordinate tensor.\n\t\tZs_: nmols X maxatom X 1 atomic number tensor.\n\t\tEe_: MAX_ATOMIC_NUMBER X MAX_ATOMIC_NUMBER Epsilon parameter matrix.\n\t\tRe_: MAX_ATOMIC_NUMBER X MAX_ATOMIC_NUMBER Re parameter matrix.\n\t\tNZP_: Nonzero Pairs (nnzp X 3) matrix (mol, i, j)\n\t""""""\n\tDs = TFDistanceLinear(XYZs_[0,:,:],NZP_)\n\tLJe = Ee_*tf.ones([8,8],dtype = tf.float64)\n\tLJr = Re_*tf.ones([8,8],dtype = tf.float64)\n\treturn LJKernelLinear(Ds, Zs_, LJe, LJr, NZP_)\n\ndef LJEnergiesLinear(XYZs_,Zs_,Ee_, Re_, NZP_):\n\t""""""\n\tReturns LJ Energies batched over molecules.\n\tInput can be padded with zeros. That will be\n\tremoved by LJKernels. This version is linear scaling with sparse indices NZP\n\n\tArgs:\n\t\tXYZs_: nmols X maxatom X 3 coordinate tensor.\n\t\tZs_: nmols X maxatom X 1 atomic number tensor.\n\t\tEe_: MAX_ATOMIC_NUMBER X MAX_ATOMIC_NUMBER Epsilon parameter matrix.\n\t\tRe_: MAX_ATOMIC_NUMBER X MAX_ATOMIC_NUMBER Re parameter matrix.\n\t\tNZP_: Nonzero Pairs (nnzp X 3) matrix (mol, i, j)\n\t""""""\n\tDs = TFDistancesLinear(XYZs_,NZP_)\n\tLJe = Ee_*tf.ones([8,8],dtype = tf.float64)\n\tLJr = Re_*tf.ones([8,8],dtype = tf.float64)\n\treturn LJKernelsLinear(Ds, Zs_, LJe, LJr, NZP_)\n\ndef HarmKernels(XYZs, Deqs, Keqs):\n\t""""""\n\tArgs:\n\t\tXYZs: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tDeqs: a nmol X maxnatom X maxnatom tensor of Equilibrium distances\n\t\tKeqs: a nmol X maxnatom X maxnatom tensor of Force constants.\n\t""""""\n\tDs = TFDistances(XYZs)\n\ttmp = Ds - tf.cast(Deqs,tf.float64)\n\ttmp -= tf.matrix_diag(tf.matrix_diag_part(tmp))\n\tK = tf.cast(Keqs,tf.float64)*tmp*tmp\n\t#K = tf.Print(K,[K],""Kern"",100)\n\tK = tf.matrix_band_part(K, 0, -1) # Extract upper triangle of each.\n\treturn K\n\ndef CoulombKernel(D):\n\t""""""\n\tArgs:\n\t\tD:  A square distance matrix (bohr)\n\t\tPARAMS[""EESwitchFunc""]: The Kernel type\n\t\t\tNone => 1/r, bare Coulomb\n\t\t\t\'Cos\' => 1/r -> (0.5*(cos(PI*r/EECutoff)+1))/r (if r>Cutoff else 0)\n\t\t\t\'Tanh\' => 1/r => 0.5*(Tanh[(x - EECutoff)/EEdr] + 1)/r\n\t""""""\n\tK = tf.div(tf.cast(1.0, dtype=tf.float64),D)\n\tK = tf.subtract(K,tf.diag(tf.diag_part(K)))\n\tK = tf.matrix_band_part(K, 0, -1) # Extract upper triangle\n\t#K = tf.Print(K,[K],""K Kernel"",-1,1000000)\n\treturn K\n\n\ndef CosKernelLR(D):\n\t""""""\n\t\'Cos\' => 1/r -> (1-0.5*(cos(PI*r/EECutoff)+1))/r (if r>Cutoff else 0)\n\tArgs:\n\t\tD:  A square distance matrix (bohr)\n\t\tLong: Whether long range or short range\n\n\t""""""\n\tones = tf.ones_like(D)\n\tCosScreen = tf.where(tf.greater(D, PARAMS[""EECutoff""]), ones, 0.0*D)\n\tCut = (1.0-0.5*(tf.cos(D*Pi/PARAMS[""EECutoff""])+1))*CosScreen\n\t#Cut = tf.Print(Cut,[Cut],""CosCut"", 10000, 1000 )\n\treturn CoulombKernel(D)*Cut\n\ndef CosKernelSR(D):\n\t""""""\n\t\'Cos\' => 1/r -> (1-0.5*(cos(PI*r/EECutoff)+1))/r (if r>Cutoff else 0)\n\tArgs:\n\t\tD:  A square distance matrix (bohr)\n\t\tLong: Whether long range or short range\n\n\t""""""\n\tones = tf.ones_like(D)\n\tCosScreen = tf.where(tf.greater(D, PARAMS[""EECutoff""]), ones, 0.0*D)\n\tCut = 1.0-(1.0-0.5*(tf.cos(D*Pi/PARAMS[""EECutoff""])+1))*CosScreen\n\t#Cut = tf.Print(Cut,[Cut],""CosCut"", 10000, 1000 )\n\treturn CoulombKernel(D)*Cut\n\ndef TanhKernelLR(D):\n\t""""""\n\tArgs:\n\t\tD:  A square distance matrix (bohr)\n\t\t\'Tanh\' => 1/r => 0.5*(Tanh[(x - EECutoff)/EEdr] + 1)/r\n\t""""""\n\tones = tf.ones_like(D)\n\tScreen = tf.where(tf.greater(D, PARAMS[""EECutoff""]+3.0*PARAMS[""EEdr""]), ones,0.0*D)\n\tTanhOut = 0.5*(tf.tanh((D - PARAMS[""EECutoff""])/PARAMS[""EEdr""]) + 1)\n\tCut = TanhOut*Screen\n\tK = CoulombKernel(D)\n\t#Cut = tf.Print(Cut,[Cut],""Cut"", 10000, 1000 )\n\treturn K*Cut\n\ndef TanhKernelSR(D):\n\t""""""\n\tArgs:\n\t\tD:  A square distance matrix (bohr)\n\t\t\'Tanh\' => 1/r => 0.5*(Tanh[(x - EECutoff)/EEdr] + 1)/r\n\t""""""\n\tones = tf.ones_like(D)\n\tScreen = tf.where(tf.greater(D, PARAMS[""EECutoff""]+3.0*PARAMS[""EEdr""]), ones,0.0*D)\n\tTanhOut = 0.5*(tf.tanh((D - PARAMS[""EECutoff""])/PARAMS[""EEdr""]) + 1)\n\tCut = TanhOut*Screen\n\tK = CoulombKernel(D)\n\tCut = 1.0-Cut\n\t#Cut = tf.Print(Cut,[Cut],""Cut"", 10000, 1000 )\n\treturn K*Cut\n\ndef XyzsToCoulomb(xyz_pl, q_pl, Long = True):\n\t""""""\n\tArgs:\n\t\tThis version is quadratic (inefficient) and should eventually\n\t\tonly be used for training purposes.\n\n\t\txyz_pl: a NMol, Natom X 3 tensor of coordinates.\n\t\tq_pl: an NMol X Natom X 1 tensor of atomic charges.\n\t\tLong: Whether to use long-rage or short-range kernel.\n\n\t\tPARAMS[""EESwitchFunc""]: The Kernel type\n\t\t\tNone => 1/r, bare Coulomb\n\t\t\t\'Cos\' => 1/r -> (1.-0.5*(cos(PI*r/EECutoff)+1))/r (if r<Cutoff else 0)\n\t\t\t\'Tanh\' => 1/r => 0.5*(Tanh[(x - EECutoff)/EEdr] + 1)/r\n\t\tReturns:\n\t\t\tE mol = \\sum_{atom1,atom2,cart} q_1*q_2*Kernel(sqrt(pow(atom1_cart - atom2_cart,2.0)))\n\t""""""\n\tD = TFDistances(xyz_pl)  # Make distance matrices for all mols.\n\t# Compute Kernel of the distances.\n\tK = None\n\tif (PARAMS[""EESwitchFunc""] == None):\n\t\tK = tf.map_fn(CoulombKernel, D)\n\tif (PARAMS[""EESwitchFunc""] == \'CosLR\'):\n\t\tK = tf.map_fn(CosKernelLR, D)\n\tif (PARAMS[""EESwitchFunc""] == \'CosSR\'):\n\t\tK = tf.map_fn(CosKernelSR, D)\n\tif (PARAMS[""EESwitchFunc""] == \'TanhLR\'):\n\t\tK = tf.map_fn(TanhKernelLR, D)\n\tif (PARAMS[""EESwitchFunc""] == \'TanhSR\'):\n\t\tK = tf.map_fn(TanhKernelSR, D)\n\t#Ks = tf.shape(K)\n\t#Kpr = tf.Print(K,[tf.to_float(Ks)],""K Shape"",-1,1000000)\n\tEn1 = tf.einsum(\'aij,ai->aij\', K, q_pl)\n\tEn2 = tf.einsum(\'aij,aj->aij\', En1, q_pl)\n\tEmols = tf.reduce_sum(En2,[1,2])\n\t# dEmols = tf.gradients(Emols,xyz_pl) # This works just fine :)\n\treturn Emols\n\ndef TestCoulomb():\n\txyz_ = tf.Variable([[0.,0.,0.],[10.0,0.,0.],[0.,0.,5.],[0.,0.,2.],[0.,1.,9.],[0.,1.,20.]], dtype=tf.float64)\n\tq_ = tf.Variable([1.,-1.,1.,-1.,0.5,0.5], dtype=tf.float64)\n\tmolis = tf.Variable([[0,1,2],[3,4,5]])\n\txyzs = tf.gather(xyz_,molis)\n\tcharges = tf.gather(q_,molis)\n\tDs = TFDistances(xyzs)\n\tdDs = tf.gradients(Ds,xyz_)\n\tinit = tf.global_variables_initializer()\n\timport sys\n\tsys.stderr = sys.stdout\n\twith tf.Session() as session:\n\t\tsession.run(init)\n\t\tprint(session.run(Ds))\n\t\tprint(session.run(dDs))\n\t\tprint(session.run(charges))\n\t\tPARAMS[""EESwitchFunc""] = None # options are Cosine, and Tanh.\n\t\tprint(session.run(XyzsToCoulomb(xyzs,charges)))\n\t\tPARAMS[""EESwitchFunc""] = ""CosSR"" # options are Cosine, and Tanh.\n\t\tprint(session.run(XyzsToCoulomb(xyzs,charges)))\n\t\tPARAMS[""EESwitchFunc""] = ""CosLR"" # options are Cosine, and Tanh.\n\t\tprint(session.run(XyzsToCoulomb(xyzs,charges)))\n\t\tPARAMS[""EESwitchFunc""] = ""TanhSR"" # options are Cosine, and Tanh.\n\t\tprint(session.run(XyzsToCoulomb(xyzs,charges)))\n\t\tPARAMS[""EESwitchFunc""] = ""TanhLR"" # options are Cosine, and Tanh.\n\t\tprint(session.run(XyzsToCoulomb(xyzs,charges)))\n\treturn\n\ndef TestLJ():\n\txyz_ = tf.Variable([[0.,0.,0.],[10.0,0.,0.],[0.,0.,5.],[0.,0.,2.],[0.,1.,9.],[0.,1.,20.]])\n\tZ_ = tf.Variable([1,2,3,4,5,6])\n\tRe_ = tf.ones([6,6])\n\tEe_ = tf.ones([6,6])\n\tDs = TFDistance(xyz_)\n\n\tinit = tf.global_variables_initializer()\n\timport sys\n\tsys.stderr = sys.stdout\n\twith tf.Session() as session:\n\t\tsession.run(init)\n\t\tprint(session.run(Ds))\n\t\tprint(""LJ Kernel: "", session.run(LJKernel(Ds,Z_,Ee_,Re_)))\n\treturn\n\ndef LJForce(xyz_,Z_,inds_,Ee_, Re_):\n\tXYZs = tf.gather(xyz_,inds_)\n\tZs = tf.gather(Z_,inds_)\n\tEns = LJEnergies(XYZs, Zs, Ee_, Re_)\n\toutput = tf.gradients(Ens, XYZs)\n\treturn output\n\ndef LearnLJ():\n\txyz_ = tf.Variable([[0.,0.,0.],[10.0,0.,0.],[0.,0.,5.],[0.,0.,2.],[0.,1.,9.],[0.,1.,20.]],trainable=False)\n\tZ_ = tf.Variable([1,2,3,4,5,6],dtype = tf.int32,trainable=False)\n\tRe_ = tf.Variable(tf.ones([6,6]),trainable=True)\n\tEe_ = tf.Variable(tf.ones([6,6]),trainable=True)\n\tinds_ = tf.Variable([[0,1,2],[3,4,5]],trainable=False)\n\tfrcs = tf.Variable([[1.0,0.0,0.0],[-1.0,0.0,0.0],[0.,0.,1.],[0.,0.,1.],[0.,0.2,1.],[0.,0.,1.]],trainable=False)\n\n\tdes_frces = tf.gather(frcs, inds_)\n\tloss = tf.nn.l2_loss(LJForce(xyz_, Z_, inds_, Ee_, Re_) - des_frces)\n\toptimizer = tf.train.GradientDescentOptimizer(0.5)\n\ttrain = optimizer.minimize(loss)\n\n\tinit = tf.global_variables_initializer()\n\timport sys\n\tsys.stderr = sys.stdout\n\twith tf.Session() as session:\n\t\tsession.run(init)\n\t\tprint()\n\t\tfor step in range(1000):\n\t\t\tsession.run(train)\n\t\t\tprint(""step"", step, ""Energies:"", session.run(LJEnergies(tf.gather(xyz_,inds_), tf.gather(Z_,inds_), Ee_, Re_)), "" Forces "", session.run(LJForce(xyz_, Z_, inds_, Ee_, Re_)), "" loss "", session.run(loss))\n\treturn\n'"
TensorMol/ForceModels/TFForces.py,92,"b'""""""\n\tTensorflow holders for simple force field ingredients\n\tThat cannot be trained.\n\n\tTODO: some simple BP-Style Traditional force field for atoms in general.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom ..TFNetworks.TFInstance import *\nfrom ..Containers.TensorMolData import *\nfrom ..TFNetworks.TFMolInstance import *\nfrom ..ForceModels.ElectrostaticsTF import *\nfrom ..ForceModifiers.Neighbors import *\nfrom tensorflow.python.client import timeline\nimport threading\n\ndef XInLat(x_, lat_):\n\t""""""\n\tquadratic wall forces which go off into infinity at the sides of a lattice cell. Within the cell they increase linearly with a slope of 10Eh / 0.1 Angstrom. beginning at zero at a skin-depth of self.skinD. This can be used to crush a\n\tsimulation.\n\n\tArgs:\n\t\tx_: points to evaluate this force on.\n\t\tlat_: The Lattice.\n\t""""""\n\tlatmet = TFMatrixPower(tf.matmul(lat_,tf.transpose(lat_)),-0.5)\n\txinlat = tf.matmul(x_,latmet) # The points in the lattice coordinates.\n\treturn xinlat\n\txinlat2 = xinlat*xinlat\n\txinlatm12 = (xinlat-1.0)*(xinlat-1.0)\n\tlower_walls = tf.where(xinlat<0.0, xinlat2, tf.zeros_like(x_))\n\tupper_walls = tf.where(xinlat>1.0, xinlatm12, tf.zeros_like(x_))\n\treturn x_\n\ndef WallEn(x_, lat_):\n\t""""""\n\tquadratic wall forces which go off into infinity at the sides of a lattice cell. Within the cell forces increase linearly with a slope of 10Eh / 0.1 Angstrom. beginning at zero at a skin-depth of self.skinD. This can be used to crush a\n\tsimulation.\n\n\tArgs:\n\t\tx_: points to evaluate this force on.\n\t\tlat_: The Lattice.\n\t""""""\n\tlatmet = TFMatrixPower(tf.matmul(lat_,tf.transpose(lat_)),-0.5)\n\txinlat = tf.matmul(x_,latmet) # The points in the lattice coordinates.\n\txinlat2 = xinlat*xinlat\n\txinlatm12 = (xinlat-1.0)*(xinlat-1.0)\n\tlower_walls = tf.where(xinlat<0.0, xinlat2, tf.zeros_like(x_))\n\tupper_walls = tf.where(xinlat>1.0, xinlatm12, tf.zeros_like(x_))\n\treturn tf.reduce_sum(lower_walls+upper_walls)\n\ndef Elastic(xs_,ks_):\n\t""""""\n\tThis is the energy kernel for an elastic band in many dimensions\n\tIt can be used to construct a conservative NEB lagrangian.\n\tc.f. http://www.inference.vc/my-notes-on-the-numerics-of-gans/\n\n\tArgs:\n\t\txs_: nbeads, bead coordinates\n\t\tks_: n-1 band forces.\n\tReturns:\n\t\tDifferentiable energy kernel of the band.\n\t""""""\n\tnbeads = tf.shape(xs_)[0]\n\tds = xs_[:-2]-xs_[1:]\n\treturn tf.reduce_sum(ds*ds*ks_)\n\ndef BandForce(xs_,ks_,F_):\n\t""""""\n\tDifferentiable Nudged elastic band force including the projection of the spring\n\tforces onto tangents and forces onto perpendiculars.\n\n\tArgs:\n\t\txs_: the beads.\n\t\tks_: the force constants.\n\t\tF_: a routine which returns energies and physical forces on each bead.\n\t""""""\n\treturn\n\nclass ForceHolder:\n\tdef __init__(self,natom_):\n\t\t""""""\n\t\tBase Force holder for an aperiodic force.\n\n\t\targs:\n\t\t\tnatom_: number of atoms the force can be evaluated on.\n\t\t""""""\n\t\tself.natom = natom_\n\t\tself.sess = None\n\t\tself.x_pl = None\n\t\treturn\n\tdef Prepare(self):\n\t\twith tf.Graph().as_default():\n\t\t\tself.x_pl=tf.placeholder(tf.float64, shape=tuple([self.natom,3]))\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\nclass BoxHolder(ForceHolder):\n\tdef __init__(self,natom_):\n\t\t""""""\n\t\tHolds quadratic wall forces which go off into infinity at the sides of a lattice cell. Within the cell they increase linearly with a slope of 10Eh / 0.1 Angstrom.\n\t\tbeginning at zero at a skin-depth of self.skinD. This can be used to crush a\n\t\tsimulation.\n\t\t""""""\n\t\tForceHolder.__init__(self,natom_)\n\t\tself.lat_pl = None # boundary vectors of the cell.\n\t\tself.slope = None\n\t\tself.Prepare()\n\tdef Prepare(self):\n\t\t\tself.x_pl=tf.placeholder(tf.float64, shape=tuple([self.natom,3]))\n\t\t\tself.lat_pl=tf.placeholder(tf.float64, shape=tuple([3,3]))\n\t\t\tself.slope=tf.Variable(10.0,dtype=tf.float64)\n\t\t\tself.SkinD=tf.Variable(0.1,dtype=tf.float64)\n\t\t\tself.Energy = self.slope*WallEn(self.x_pl,self.lat_pl)\n\t\t\tself.Force = tf.gradients(WallEn(self.x_pl,self.lat_pl),self.x_pl)\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\tdef __call__(self,x_,lat_):\n\t\t""""""\n\t\tArgs:\n\t\t\tx_: the coordinates on which to evaluate the force.\n\t\t\tlat_: the lattice boundary vectors.\n\t\tReturns:\n\t\t\tthe Energy and Force (Eh and Eh/ang.) associated with the quadratic walls.\n\t\t""""""\n#\t\tprint(""lat"",lat_)\n#\t\tprint(""Xinlat"",self.sess.run([XInLat(self.x_pl,self.lat_pl)], feed_dict = {self.x_pl:x_, self.lat_pl:lat_}))\n\t\te,f = self.sess.run([self.Energy,self.Force], feed_dict = {self.x_pl:x_, self.lat_pl:lat_})\n\t\t#print(""Min max and lat"", np.min(x_), np.max(x_), lat_, e ,f)\n\t\treturn e, f[0]\n\nclass BumpHolder(ForceHolder):\n\tdef __init__(self,natom_,maxbump_,bowlk_=0.0,h_=0.5,w_=1.0,Type_=""LR""):\n\t\t""""""\n\t\tHolds a bump-function graph to allow for rapid\n\t\tmetadynamics. Can also hold an attractive bump which draws\n\t\tatoms towards 0,0,0\n\t\tArgs:\n\t\t\tnatom_: number of atoms in the molecule to be bumped.\n\t\t\tmaxbump_: maximum size of the coordinate arrays to store old positions.\n\t\t\tType: ""LR"", means that small changes in distant atoms will satisfy the bump. (slow collective bumps.)\n\t\t\t\t  ""MR"", means that on average 5-angstrom differences are most important in the loss. (dihedralish-bumps.)\n\t\t""""""\n\t\tForceHolder.__init__(self, natom_)\n\t\tself.maxbump = maxbump_\n\t\tself.Type=Type_\n\t\tself.nb_pl = None\n\t\tself.h_a = h_\n\t\tself.w_a = w_\n\t\tself.h = None\n\t\tself.w = None\n\t\tself.BowlK = bowlk_\n\t\tself.Prepare()\n\t\treturn\n\n\tdef Prepare(self):\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(tf.float64, shape=tuple([self.maxbump,self.natom,3]))\n\t\t\tself.x_pl=tf.placeholder(tf.float64, shape=tuple([self.natom,3]))\n\t\t\tself.nb_pl=tf.placeholder(tf.int32)\n\t\t\tself.h = tf.Variable(self.h_a,dtype = tf.float64)\n\t\t\tself.w = tf.Variable(self.w_a,dtype = tf.float64)\n\t\t\tself.BowlKv = tf.Variable(self.BowlK,dtype = tf.float64)\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tif (self.Type==""LR""):\n\t\t\t\tself.BE = -1.0*BumpEnergy(self.h, self.w, self.xyzs_pl, self.x_pl, self.nb_pl)\n\t\t\t\tself.BF = tf.gradients(BumpEnergy(self.h, self.w, self.xyzs_pl, self.x_pl, self.nb_pl), self.x_pl)\n\t\t\telse:\n\t\t\t\tself.BE = -1.0*BumpEnergyMR(self.h, self.w, self.xyzs_pl, self.x_pl, self.nb_pl)\n\t\t\t\tself.BF = tf.gradients(BumpEnergyMR(self.h, self.w, self.xyzs_pl, self.x_pl, self.nb_pl), self.x_pl)\n\t\t\tself.BowlE = BowlEnergy(self.BowlKv, self.x_pl)\n\t\t\tself.BowlF = tf.gradients(BowlEnergy(self.BowlKv, self.x_pl), self.x_pl)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef Bump(self, BumpCoords, x_, NBump_):\n\t\t""""""\n\t\tReturns the Bump energy force.\n\t\t""""""\n\t\tif (self.BowlK == 0.0):\n\t\t\treturn self.sess.run([self.BE,self.BF], feed_dict = {self.xyzs_pl:BumpCoords, self.x_pl:x_, self.nb_pl:NBump_})\n\t\telse:\n\t\t\te,f,we,wf = self.sess.run([self.BE,self.BF,self.BowlE,self.BowlF], feed_dict = {self.xyzs_pl:BumpCoords, self.x_pl:x_, self.nb_pl:NBump_})\n\t\t\treturn (e+we), ([f[0]+wf[0]])\n\n\tdef Bowl(self, x_):\n\t\t""""""\n\t\tReturns the Bowl force.\n\t\twhich is a linear attraction to 0.0.0\n\t\t""""""\n\t\treturn self.sess.run([self.BowlE,self.BowlF], feed_dict = {self.x_pl:x_})\n\nclass MolInstance_DirectForce(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tAn instance which can evaluate and optimize some model force field.\n\tThe force routines are in ElectrostaticsTF.py\n\tThe force routines can take some parameters described here.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True, ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""LJForce""\n\t\tself.TData = TData_\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.MaxNAtoms = TData_.MaxNAtoms\n\t\tself.batch_size_output = 4096\n\t\tself.PreparedFor = 0\n\t\tself.inp_pl=None\n\t\tself.nzp_pl=None\n\t\tself.x_pl=None\n\t\tself.z_pl=None\n\t\tself.nzp2_pl=None\n\t\tself.frce_pl=None\n\t\tself.sess = None\n\t\tself.ForceType = ForceType_\n\t\tself.forces = None\n\t\tself.energies = None\n\t\tself.forcesLinear = None\n\t\tself.energiesLinear = None\n\t\tself.forceLinear = None\n\t\tself.energyLinear = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\t\tself.LJe = None\n\t\tself.LJr = None\n\t\tself.Deq = None\n\t\tself.dbg1 = None\n\t\tself.dbg2 = None\n\t\tself.NL = None\n\t\tself.max_checkpoints = 5000\n\t\t# Using multidimensional inputs creates all sorts of issues; for the time being only support flat inputs.\n\n\tdef loss_op(self, output, labels):\n\t\t""""""\n\t\tThe loss operation of this model is complicated\n\t\tBecause you have to construct the electrostatic energy moleculewise,\n\t\tand the mulitpoles.\n\n\t\tEmats and Qmats are constructed to accerate this process...\n\t\t""""""\n\t\toutput = tf.Print(output,[output],""Comp\'d"",1000,1000)\n\t\tlabels = tf.Print(labels,[labels],""Desired"",1000,1000)\n\t\tdiff  = tf.subtract(output, labels)\n\t\t#tf.Print(diff, [diff], message=""This is diff: "",first_n=10000000,summarize=100000000)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef LJFrc(self, inp_pl):\n\t\t""""""\n\t\tCompute forces for a batch of molecules\n\t\twith the current LJe, and LJr.\n\n\t\tArgs:\n\t\t\tinp_pl: placeholder for the NMol X MaxNatom X 4 tensor of Z,x,y,z\n\t\t""""""\n\t\t# separate out the Z from the XYZ.\n\t\tinp_shp = tf.shape(inp_pl)\n\t\tnmol = inp_shp[0]\n\t\tmaxnatom = inp_shp[1]\n\t\tXYZs = tf.slice(inp_pl,[0,0,1],[-1,-1,-1])\n\t\tZs = tf.cast(tf.reshape(tf.slice(inp_pl,[0,0,0],[-1,-1,1]),[nmol,maxnatom,1]),tf.int64)\n\t\t#self.LJe = tf.Print(self.LJe,[self.LJe],""LJe"",1000,1000)\n\t\t#self.LJr = tf.Print(self.LJr,[self.LJr],""LJr"",1000,1000)\n\t\tLJe2 = self.LJe*self.LJe\n\t\tLJr2 = self.LJr*self.LJr\n\t\t#LJe2 = tf.Print(LJe2,[LJe2],""LJe2"",1000,1000)\n\t\t#LJr2 = tf.Print(LJr2,[LJr2],""LJr2"",1000,1000)\n\t\tEns = LJEnergies(XYZs, Zs, LJe2, LJr2)\n\t\t#Ens = tf.Print(Ens,[Ens],""Energies"",5000,5000)\n\t\tfrcs = -1.0*(tf.gradients(Ens, XYZs)[0])\n\t\treturn Ens, frcs\n\n\tdef LJFrcLinear(self, z_pl, x_pl, nzp2_pl):\n\t\t""""""\n\t\tCompute forces\n\t\tArgs:\n\t\t\tinp_pl: placeholder for the NMol X MaxNatom X 4 tensor of Z,x,y,z\n\t\t\tnzp_pl: placeholder for the NMol X 3 tensor of nonzero pairs.\n\t\t""""""\n\t\tLJe2 = 0.116*tf.ones([8,8],dtype=tf.float64)\n\t\tLJr2 = tf.ones([8,8],dtype=tf.float64)\n\t\tx_plshp = tf.shape(x_pl)\n\t\txpl = tf.reshape(x_pl,[1,x_plshp[0],x_plshp[1]])\n\t\tEns = LJEnergyLinear(xpl, z_pl, LJe2, LJr2, nzp2_pl)\n\t\tfrcs = -1.0*(tf.gradients(Ens, xpl)[0])\n\t\treturn Ens, frcs\n\n\tdef LJFrcsLinear(self, inp_pl, nzp_pl):\n\t\t""""""\n\t\tCompute forces for a batch of molecules\n\t\twith the current LJe, and LJr with linear scaling.\n\n\t\tArgs:\n\t\t\tinp_pl: placeholder for the NMol X MaxNatom X 4 tensor of Z,x,y,z\n\t\t\tnzp_pl: placeholder for the NMol X 3 tensor of nonzero pairs.\n\t\t""""""\n\t\t# separate out the Z from the XYZ.\n\t\tinp_shp = tf.shape(inp_pl)\n\t\tnmol = inp_shp[0]\n\t\tmaxnatom = inp_shp[1]\n\t\tXYZs = tf.slice(inp_pl,[0,0,1],[-1,-1,-1])\n\t\tZs = tf.cast(tf.reshape(tf.slice(inp_pl,[0,0,0],[-1,-1,1]),[nmol,maxnatom,1]),tf.int64)\n\t\tLJe2 = 0.116*tf.ones([8,8],dtype=tf.float64)\n\t\tLJr2 = tf.ones([8,8],dtype=tf.float64)\n\t\tEns = LJEnergiesLinear(XYZs, Zs, LJe2, LJr2, nzp_pl)\n\t\tfrcs = -1.0*(tf.gradients(Ens, XYZs)[0])\n\t\treturn Ens, frcs\n\n\tdef HarmFrc(self, inp_pl):\n\t\t""""""\n\t\tCompute Harmonic Forces with equilibrium distance matrix\n\t\tDeqs, and force constant matrix, Keqs\n\n\t\tArgs:\n\t\t\tinp_pl: placeholder for the NMol X MaxNatom X 4 tensor of Z,x,y,z\n\t\t""""""\n\t\t# separate out the Z from the XYZ.\n\t\tinp_shp = tf.shape(inp_pl)\n\t\tnmol = inp_shp[0]\n\t\tmaxnatom = inp_shp[1]\n\t\tXYZs = tf.slice(inp_pl,[0,0,1],[-1,-1,-1])\n\t\tZs = tf.cast(tf.reshape(tf.slice(inp_pl,[0,0,0],[-1,-1,1]),[nmol,maxnatom,1]),tf.int64)\n\t\tZZeroTensor = tf.cast(tf.where(tf.equal(Zs,0),tf.ones_like(Zs),tf.zeros_like(Zs)),tf.float64)\n\t\t# Construct a atomic number masks.\n\t\tZshp = tf.shape(Zs)\n\t\tZzij1 = tf.tile(ZZeroTensor,[1,1,Zshp[1]]) # mol X atom X atom.\n\t\tZzij2 = tf.transpose(Zzij1,perm=[0,2,1]) # mol X atom X atom.\n\t\tDeqs = tf.ones((nmol,maxnatom,maxnatom))\n\t\tKeqs = 0.001*tf.ones((nmol,maxnatom,maxnatom))\n\t\tK = HarmKernels(XYZs, Deqs, Keqs)\n\t\tK = tf.where(tf.equal(Zzij1,1.0),tf.zeros_like(K),K)\n\t\tK = tf.where(tf.equal(Zzij2,1.0),tf.zeros_like(K),K)\n\t\tEns = tf.reduce_sum(K,[1,2])\n\t\tfrcs = -1.0*(tf.gradients(Ens, XYZs)[0])\n\t\t#frcs = tf.Print(frcs,[frcs],""Forces"",1000,1000)\n\t\treturn Ens, frcs\n\n\tdef EvalForce(self,m):\n\t\tIns = self.TData.dig.Emb(m,False,False)\n\t\tIns = Ins.reshape(tuple([1]+list(Ins.shape)))\n\t\tfeeddict = {self.inp_pl:Ins}\n\t\tEn,Frc = self.sess.run([self.energies, self.forces],feed_dict=feeddict)\n\t\treturn En, JOULEPERHARTREE*Frc[0] # Returns energies and forces.\n\tdef CallLinearLJForce(self,z,x,NZ):\n\t\tif (z.shape[0] != self.PreparedFor):\n\t\t\tself.MaxNAtoms = z.shape[0]\n\t\t\tself.Prepare()\n\t\tfeeddict = {self.z_pl:z.astype(np.int64).reshape(z.shape[0],1),self.x_pl:x, self.nzp2_pl:NZ}\n\t\tEn,Frc = self.sess.run([self.energyLinear, self.forceLinear],feed_dict=feeddict)\n\t\treturn En, JOULEPERHARTREE*Frc[0] # Returns energies and forces.\n\tdef EvalForceLinear(self,m):\n\t\tIns = self.TData.dig.Emb(m,False,False)\n\t\tif (Ins.shape[0] != self.PreparedFor):\n\t\t\tself.MaxNAtoms = Ins.shape[0]\n\t\t\tself.Prepare()\n\t\tIns = Ins.reshape(tuple([1]+list(Ins.shape))) # mol X 4\n\t\tif (self.NL==None):\n\t\t\tself.NL = NeighborListSet(Ins[:,:,1:],np.array([m.NAtoms()]))\n\t\tself.NL.Update(Ins[:,:,1:],7.0)\n\t\tfeeddict = {self.inp_pl:Ins, self.nzp_pl:self.NL.pairs}\n\t\tEn,Frc = self.sess.run([self.energiesLinear, self.forcesLinear],feed_dict=feeddict)\n\t\treturn En, JOULEPERHARTREE*Frc[0] # Returns energies and forces.\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  train loss: "", ""%.10f""%(float(loss)/(Ncase)))\n\t\treturn\n\tdef Prepare(self):\n\t\tself.TrainPrepare()\n\t\treturn\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = len(self.TData.set.mols)\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size_output)):\n\t\t\t#print (""ministep: "", ministep, "" Ncase_train:"", Ncase_train, "" self.batch_size"", self.batch_size)\n\t\t\tbatch_data = self.TData.RawBatch()\n\t\t\tif (not np.all(np.isfinite(batch_data[0]))):\n\t\t\t\tprint(""Bad Batch...0 "")\n\t\t\tif (not np.all(np.isfinite(batch_data[1]))):\n\t\t\t\tprint(""Bad Batch...1 "")\n\t\t\tfeeddict={i:d for i,d in zip([self.inp_pl,self.frce_pl],[batch_data[0],batch_data[1]])}\n\t\t\tdump_2, total_loss_value, loss_value = self.sess.run([self.train_op, self.total_loss, self.loss], feed_dict=feeddict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += self.batch_size_output\n\t\t\t#print (""atom_outputs:"", atom_outputs, "" mol outputs:"", mol_output)\n\t\t\t#print (""atom_outputs shape:"", atom_outputs[0].shape, "" mol outputs"", mol_output.shape)\n\t\t#print(""train diff:"", (mol_output[0]-batch_data[2])[:actual_mols], np.sum(np.square((mol_output[0]-batch_data[2])[:actual_mols])))\n\t\t#print (""train_loss:"", train_loss, "" Ncase_train:"", Ncase_train, train_loss/num_of_mols)\n\t\t#print (""diff:"", mol_output - batch_data[2], "" shape:"", mol_output.shape)\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\tself.PreparedFor = self.MaxNAtoms\n\t\twith tf.Graph().as_default():\n\t\t\tself.inp_pl=tf.placeholder(tf.float64, shape=tuple([None,self.MaxNAtoms,4]))\n\t\t\tself.nzp_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.z_pl=tf.placeholder(tf.int64, shape=tuple([self.MaxNAtoms,1]))\n\t\t\tself.x_pl=tf.placeholder(tf.float64, shape=tuple([self.MaxNAtoms,3]))\n\t\t\tself.nzp2_pl=tf.placeholder(tf.int64, shape=tuple([None,2]))\n\t\t\tself.frce_pl = tf.placeholder(tf.float64, shape=tuple([None,self.MaxNAtoms,3])) # Forces.\n\t\t\tif (self.ForceType==""LJ""):\n\t\t\t\tself.LJe = tf.Variable(0.40*tf.ones([8,8],dtype=tf.float64),trainable=True,dtype=tf.float64)\n\t\t\t\tself.LJr = tf.Variable(1.1*tf.ones([8,8],dtype=tf.float64),trainable=True,dtype=tf.float64)\n\t\t\t\t# These are squared later to keep them positive.\n\t\t\t\tself.energies, self.forces = self.LJFrc(self.inp_pl)\n\t\t\t\tself.energyLinear, self.forceLinear = self.LJFrcLinear(self.z_pl,self.x_pl,self.nzp2_pl)\n\t\t\t\tself.energiesLinear, self.forcesLinear = self.LJFrcsLinear(self.inp_pl,self.nzp_pl)\n\t\t\t\tself.total_loss, self.loss = self.loss_op(self.forces, self.frce_pl)\n\t\t\t\tself.train_op = self.training(self.total_loss, PARAMS[""learning_rate""], PARAMS[""momentum""])\n\t\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\telif (self.ForceType==""Harm""):\n\t\t\t\tself.energies, self.forces = self.HarmFrc(self.inp_pl)\n\t\t\telse:\n\t\t\t\traise Exception(""Unknown Kernel"")\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef train(self, mxsteps=10000):\n\t\tself.TrainPrepare()\n\t\tLOGGER.info(""MolInstance_LJForce.train()"")\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_test_loss = float(\'inf\') # some big numbers\n\t\tfor step in  range (0, mxsteps):\n\t\t\tself.train_step(step)\n\t\tself.SaveAndClose()\n\t\treturn\n'"
TensorMol/ForceModels/TFModels.py,39,"b'""""""\n\tTensorflow implementations of simple chemical models to test out learning approaches.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom ..TFNetworks.TFInstance import *\nfrom ..Containers.TensorMolData import *\nfrom ..TFNetworks.TFMolInstance import *\nfrom .ElectrostaticsTF import *\nfrom .TFForces import *\nfrom ..ForceModifiers.Neighbors import *\nfrom . import *\nfrom tensorflow.python.client import timeline\nimport threading\n\n\nclass MorseModel(ForceHolder):\n\tdef __init__(self,natom_=3):\n\t\t""""""\n\t\tsimple morse model for three atoms for a training example.\n\t\t""""""\n\t\tForceHolder.__init__(self,natom_)\n\t\tself.lat_pl = None # boundary vectors of the cell.\n\t\t#self.a = sqrt(self.k/(2.0*self.de))\n\t\tself.Prepare()\n\tdef PorterKarplus(self,x_pl):\n\t\tx1 = x_pl[0] - x_pl[1]\n\t\tx2 = x_pl[2] - x_pl[1]\n\t\tx12 = x_pl[0] - x_pl[2]\n\t\tr1 = tf.norm(x1)\n\t\tr2 = tf.norm(x2)\n\t\tr12 = tf.norm(x12)\n\t\tv1 = 0.7*tf.pow(1.-tf.exp(-(r1-0.7)),2.0)\n\t\tv2 = 0.7*tf.pow(1.-tf.exp(-(r2-0.7)),2.0)\n\t\tv3 = 0.7*tf.pow(1.-tf.exp(-((r12)-0.7)),2.0)\n\t\treturn v1+v2+v3\n\tdef Prepare(self):\n\t\tself.x_pl=tf.placeholder(tf.float64, shape=tuple([self.natom,3]))\n\t\tself.Energy = self.PorterKarplus(self.x_pl)\n\t\tself.Force =tf.gradients(-1.0*self.PorterKarplus(self.x_pl),self.x_pl)\n\t\tinit = tf.global_variables_initializer()\n\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\tself.sess.run(init)\n\tdef __call__(self,x_):\n\t\t""""""\n\t\tArgs:\n\t\t\tx_: the coordinates on which to evaluate the force.\n\t\t\tlat_: the lattice boundary vectors.\n\t\tReturns:\n\t\t\tthe Energy and Force (Eh and Eh/ang.) associated with the quadratic walls.\n\t\t""""""\n#\t\tprint(""lat"",lat_)\n#\t\tprint(""Xinlat"",self.sess.run([XInLat(self.x_pl,self.lat_pl)], feed_dict = {self.x_pl:x_, self.lat_pl:lat_}))\n\t\te,f = self.sess.run([self.Energy,self.Force], feed_dict = {self.x_pl:x_})\n\t\t#print(""Min max and lat"", np.min(x_), np.max(x_), lat_, e ,f)\n\t\treturn e, f[0]\n\nclass QuantumElectrostatic(ForceHolder):\n\tdef __init__(self,natom_=3):\n\t\t""""""\n\t\tThis is a huckle-like model, something like BeH2\n\t\tfour valence charges are exchanged between the atoms\n\t\twhich experience a screened coulomb interaction\n\t\t""""""\n\t\tForceHolder.__init__(self,natom_)\n\t\tself.Prepare()\n\tdef HuckelBeH2(self,x_pl):\n\t\tr = tf.reduce_sum(x_pl*x_pl, 1)\n\t\tr = tf.reshape(r, [-1, 1]) # For the later broadcast.\n\t\t# Tensorflow can only reverse mode grad the sqrt if all these elements\n\t\t# are nonzero\n\t\tD = tf.sqrt(r - 2*tf.matmul(x_pl, tf.transpose(x_pl)) + tf.transpose(r) + tf.cast(1e-26,tf.float64))\n\t\temat = tf.diag(self.en0s)\n\t\tJ = tf.matrix_band_part(-1.0/tf.pow((D + 0.5*0.5*0.5),1.0/3.0), 0, -1)\n\t\temat += J + tf.transpose(J)\n\t\te,v = tf.self_adjoint_eig(emat)\n\t\tpopd = tf.nn.top_k(-1.*e, 2, sorted=True).indices\n\t\t# The lowest two orbitals are populated.\n\t\tEnergy = e[popd[0]]+e[popd[1]]\n\t\tq1=-1.0+v[popd[0],0]*v[popd[0],0]+v[popd[1],0]*v[popd[1],0]\n\t\tq2=-0.5+v[popd[0],1]*v[popd[0],1]+v[popd[1],1]*v[popd[1],1]\n\t\tq3=-0.5+v[popd[0],2]*v[popd[0],2]+v[popd[1],2]*v[popd[1],2]\n\t\t# compute the dipole moment.\n\t\tDipole = (q1*x_pl[0]+q2*x_pl[1]+q3*x_pl[2])/3.0\n\t\treturn Energy, Dipole, [q1,q2,q3]\n\tdef Prepare(self):\n\t\tself.en0s = tf.constant([-1.1,-0.5,-0.5],dtype=tf.float64)\n\t\tself.x_pl=tf.placeholder(tf.float64, shape=tuple([self.natom,3]))\n\t\tself.Energy,self.Dipole,self.Charges = self.HuckelBeH2(self.x_pl)\n\t\tself.Force =tf.gradients(-1.0*self.Energy,self.x_pl)\n\t\tinit = tf.global_variables_initializer()\n\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\tself.sess.run(init)\n\tdef __call__(self,x_):\n\t\t""""""\n\t\tArgs:\n\t\t\tx_: the coordinates on which to evaluate the force.\n\t\tReturns:\n\t\t\tthe Energy and Force (Eh and Eh/ang.) associated with the quadratic walls.\n\t\t""""""\n\t\te,f,d,q = self.sess.run([self.Energy,self.Force,self.Dipole,self.Charges], feed_dict = {self.x_pl:x_})\n\t\t#print(""Min max and lat"", np.min(x_), np.max(x_), lat_, e ,f)\n\t\treturn e, f[0], d, q\n\nclass ExtendedHuckel(ForceHolder):\n\tdef __init__(self,):\n\t\t""""""\n\n\t\t""""""\n\t\tForceHolder.__init__(self,natom_)\n\t\tself.Prepare()\n\tdef HuckelBeH2(self,x_pl):\n\t\tr = tf.reduce_sum(x_pl*x_pl, 1)\n\t\tr = tf.reshape(r, [-1, 1]) # For the later broadcast.\n\t\t# Tensorflow can only reverse mode grad the sqrt if all these elements\n\t\t# are nonzero\n\t\tD = tf.sqrt(r - 2*tf.matmul(x_pl, tf.transpose(x_pl)) + tf.transpose(r) + tf.cast(1e-26,tf.float64))\n\t\temat = tf.diag(self.en0s)\n\t\tJ = tf.matrix_band_part(-1.0/tf.pow((D + 0.5*0.5*0.5),1.0/3.0), 0, -1)\n\t\temat += J + tf.transpose(J)\n\t\te,v = tf.self_adjoint_eig(emat)\n\t\tpopd = tf.nn.top_k(-1.*e, 2, sorted=True).indices\n\t\t# The lowest two orbitals are populated.\n\t\tEnergy = e[popd[0]]+e[popd[1]]\n\t\tq1=-1.0+v[popd[0],0]*v[popd[0],0]+v[popd[1],0]*v[popd[1],0]\n\t\tq2=-0.5+v[popd[0],1]*v[popd[0],1]+v[popd[1],1]*v[popd[1],1]\n\t\tq3=-0.5+v[popd[0],2]*v[popd[0],2]+v[popd[1],2]*v[popd[1],2]\n\t\t# compute the dipole moment.\n\t\tDipole = (q1*x_pl[0]+q2*x_pl[1]+q3*x_pl[2])/3.0\n\t\treturn Energy, Dipole, [q1,q2,q3]\n\tdef Prepare(self):\n\t\tself.en0s = tf.constant([-1.1,-0.5,-0.5],dtype=tf.float64)\n\t\tself.x_pl=tf.placeholder(tf.float64, shape=tuple([self.natom,3]))\n\t\tself.Energy,self.Dipole,self.Charges = self.HuckelBeH2(self.x_pl)\n\t\tself.Force =tf.gradients(-1.0*self.Energy,self.x_pl)\n\t\tinit = tf.global_variables_initializer()\n\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\tself.sess.run(init)\n\tdef __call__(self,x_):\n\t\t""""""\n\t\tArgs:\n\t\t\tx_: the coordinates on which to evaluate the force.\n\t\tReturns:\n\t\t\tthe Energy and Force (Eh and Eh/ang.) associated with the quadratic walls.\n\t\t""""""\n\t\te,f,d,q = self.sess.run([self.Energy,self.Force,self.Dipole,self.Charges], feed_dict = {self.x_pl:x_})\n\t\t#print(""Min max and lat"", np.min(x_), np.max(x_), lat_, e ,f)\n\t\treturn e, f[0], d, q\n'"
TensorMol/ForceModels/TFPeriodicForces.py,97,"b'""""""\n\tThis file provides raw sparse, linear scaling periodic forces.\n\n\tThis is made in two parts, the first part is LinearVoxelBase\n\twhich is a linear scaling pair list using cubic voxels.\n\n\tthe second part is a periodic force evaluator which handles tesselation and\n\tforce evaluation. It can use LinearVoxelBase to generate pair lists, and\n\tvoxel-decomposed coordinate tensors (nvox X maxnatom X 4)\n\n\tDerived classes can inherit both these functionalities.\n\tThe lattice vectors are a part of the periodic object and are thus differentiable.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom ..ForceModifiers.Periodic import *\n# from TensorMol.PeriodicTF import *\n#from TensorMol.TFMolInstanceDirect import *\n\nclass LinearVoxelBase:\n\t""""""\n\tBase class which provides a linear scaling voxel decompoosition and pair list. There is a single set of overlapping\n\tvoxels such that every point lies within the sub-cube with a skin of depth rng. This routine can also create\n\tpair lists for sparse algorithms. First it divides space into cubes then uses the quadratic alg. within each.\n\t""""""\n\tdef __init__(self,rng_):\n\t\t""""""\n\t\tArgs:\n\t\t    rng_: the range of the pairs (only unique)\n\t\t""""""\n\t\tself.rng = rng_\n\t\tself.xv_pl = None\n\t\tself.x0 = None # The origin of the first voxel\n\t\tself.dx = rng_ # the offset between voxels (stride) which is equal to the core length\n\t\tself.dl = rng_ # length to core from surface. The voxel has side-length 2dl+dx\n\t\tself.dxv = None # the offset between voxels (stride) in one cartesian direction.\n\t\tself.nvox = None # number in each direction determined by dx\n\t\tself.sess = None\n\t\tself.VL = None # Computes List of voxels.\n\t\tself.g = tf.Graph()\n\t\treturn\n\tdef Prepare(self):\n\t\twith self.g.as_default():\n\t\t\tself.dx = tf.constant(self.rng,dtype = tf.float64) # Stride\n\t\t\tself.dl = tf.constant(self.rng,dtype = tf.float64)\n\t\t\tself.dxv = tf.Variable([self.rng,self.rng,self.rng],dtype = tf.float64)\n\t\t\tself.x0 = tf.Variable([0.,0.,0.],dtype = tf.float64)\n\t\t\tself.nvox = tf.Variable(0, dtype = tf.int64)\n\t\t\tself.xv_pl = tf.placeholder(tf.float64, shape=(None,3), name=""LVBx_pl"")\n\t\t\tself.nreal_pl = tf.placeholder(tf.int32)\n\t\t\tself.VL = self.VoxelList(self.xv_pl,self.nreal_pl)\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\tdef TFRaster3(self):\n\t\twith self.g.as_default():\n\t\t\trngs1 = tf.tile(tf.range(3,dtype=tf.int64)[:,tf.newaxis,tf.newaxis],[1,3,3])\n\t\t\trngs2 = tf.tile(tf.range(3,dtype=tf.int64)[tf.newaxis,:,tf.newaxis],[3,1,3])\n\t\t\trngs3 = tf.tile(tf.range(3,dtype=tf.int64)[tf.newaxis,tf.newaxis,:],[3,3,1])\n\t\t\trngs = tf.reshape(tf.transpose(tf.stack([rngs1,rngs2,rngs3]),perm=[1,2,3,0]),[3*3*3,3])\n\t\t\treturn rngs\n\tdef TFRasterN(self,n_):\n\t\twith self.g.as_default():\n\t\t\tn=tf.reshape(tf.cast(n_,dtype=tf.int32),())\n\t\t\trngs1 = tf.tile(tf.range(n)[:,tf.newaxis,tf.newaxis],[1,n,n])\n\t\t\trngs2 = tf.tile(tf.range(n)[tf.newaxis,:,tf.newaxis],[n,1,n])\n\t\t\trngs3 = tf.tile(tf.range(n)[tf.newaxis,tf.newaxis,:],[n,n,1])\n\t\t\trngs = tf.reshape(tf.transpose(tf.stack([rngs1,rngs2,rngs3]),perm=[1,2,3,0]),[n*n*n,3])\n\t\t\treturn rngs\n\tdef VoxelIndices(self,pts_,x0_,dx_,nvox_):\n\t\t""""""\n\t\tReturns the xyz core indices of a voxel containing pts_ within their core\n\t\t""""""\n\t\twith self.g.as_default():\n\t\t\treturn tf.floordiv(pts_ - x0_ - dx_, dx_)\n\tdef VoxelCenter(self,ind):\n\t\t""""""\n\t\tReturns the center of a Voxel.\n\t\t""""""\n\t\twith self.g.as_default():\n\t\t\treturn self.x0 + self.dl + (ind+0.5)*(self.dxv)\n\tdef VoxelCenters(self,ind,x0,dxv):\n\t\t""""""\n\t\tReturns the center of a Voxel.\n\t\t""""""\n\t\twith self.g.as_default():\n\t\t\ttmp1 = x0[tf.newaxis,:]\n\t\t\ttmp2 = dxv[tf.newaxis,:]\n\t\t\ttmp4 = 0.5*tf.ones((1,3),dtype=tf.float64)\n\t\t\ttmp3 = tf.cast(ind,tf.float64)\n\t\t\treturn tmp1+tmp2+(tmp3+tmp4)*(self.dxv[tf.newaxis,:])\n\tdef PointsWithin(self, pts_, VoxInd_, rng_):\n\t\t""""""\n\t\tReturns indices of pts_ which are within the core rng of VoxInd_\n\t\t""""""\n\t\twith self.g.as_default():\n\t\t\tnpts = tf.shape(pts_)[0]\n\t\t\treturn vis\n\tdef IndicesToRasters(self, inds_, x0_, nvox_):\n\t\t""""""\n\t\tConvert a list of voxel index triples into\n\t\traster indices.\n\t\t""""""\n\t\twith self.g.as_default():\n\t\t\tnvox = tf.cast(nvox_,dtype=tf.int64)\n\t\t\tinds = tf.cast(inds_,tf.int64)\n\t\t\ttmp=inds[:,:,0]*nvox*nvox+inds[:,:,1]*nvox+inds[:,:,2]\n\t\t\treturn tmp\n\tdef RastersToIndices(self, r_):\n\t\twith self.g.as_default():\n\t\t\tind0 = tf.floordiv(r_,self.nvox*self.nvox)\n\t\t\ttmp0 = r_ - ind0*self.nvox*self.nvox\n\t\t\tind1 = tf.floordiv(tmp0,self.nvox)\n\t\t\tind2 = tmp0 - ind1*self.nvox\n\t\t\treturn tf.concat([ind0[:,tf.newaxis],ind1[:,tf.newaxis],ind2[:,tf.newaxis]],axis = 1)\n\tdef MemberOfVoxels(self, vis_, x0_, dx_, nvox_):\n\t\t""""""\n\t\tReturns the raster indices of the 27 voxels each point belongs to.\n\t\tThe 13th is always the core of the voxel.\n\n\t\tArgs:\n\t\t vis_: index triples of the core voxel.\n\t\tReturns:\n\t\t the raster indices of the surrounding 27 voxels this point can belong to.\n\t\t""""""\n\t\twith self.g.as_default():\n\t\t\tcoreis = vis_[:,tf.newaxis,:]\n\t\t\ttoadd = self.TFRaster3()[tf.newaxis,:,:]-tf.ones(shape=[27,3],dtype=tf.int64)[tf.newaxis,:,:]\n\t\t\tcarts = tf.cast(coreis,tf.int64)+toadd\n\t\t\treturn self.IndicesToRasters(carts, x0_, nvox_)\n\tdef VoxelList(self, x_, nreal_):\n\t\t""""""\n\t\tGiven a monolithic set of coordinates x_\n\t\tThis routine:\n\t\t- Chooses origin of voxel and # of voxels.\n\t\t- The number of voxels is 2x what is required to cover x_\n\t\t- Assigns the voxel indices owned by each atom (real)\n\n\t\tArgs:\n\t\t x_: a nptsX3 coordinate array.\n\n\t\tReturns:\n\t\t voxel origin, nvox, voxel centers, and voxel membership. npts X 27\n\t\t""""""\n\t\twith self.g.as_default():\n\t\t\txmn = tf.cast(tf.reduce_min(x_),tf.float64)\n\t\t\txmx = tf.cast(tf.reduce_max(x_),tf.float64)\n\t\t\tself.x0 = tf.ones(3,dtype=tf.float64)*xmn-2.0*self.dxv\n\t\t\txp = tf.ones(3,dtype=tf.float64)*xmx+2.0*self.dxv\n\t\t\tdiam = tf.reduce_max(xp-self.x0)\n\t\t\tnvox = tf.round(diam/(self.dx))+1\n\t\t\t# Determine the voxel each atom belongs to (core and tesselated)\n\t\t\tvis = self.VoxelIndices(x_, self.x0, self.dx, nvox)\n\t\t\treturn self.x0, nvox, self.MemberOfVoxels(vis, self.x0, self.dx, nvox)\n\t\t\t# Below can be used to test this is working, which it is.\n\t\t\t#return xmn,xmx,x0,nvox,self.VoxelCenters(vis,x0,self.dxv),self.MemberOfVoxels(vis,x0,self.dx,nvox), self.VoxelCenters(self.TFRasterN(nvox),x0,self.dxv), vis\n\tdef MakeVoxelList(self,x_, nreal_):\n\t\tprint(np.min(x_),np.max(x_))\n\t\ttmp = self.sess.run(self.VL, feed_dict={self.xv_pl:x_, self.nreal_pl:nreal_} )\n\t\treturn tmp\n\tdef MakeVoxelMols(self,z_,x_,nreal_):\n\t\t""""""\n\t\tThis routine makes a batch of molecules which can be used to\n\t\tform energies in a linear scaling way.\n\t\tIt also assigns NNZ for each molecule and maxnatom.\n\t\tIe to form an energy with linear scaling you form the atomwise energies\n\t\tthese mols, and only sum up the energy up-to atom nreal.\n\n\t\tArgs:\n\t\t  x_, z_ : a coordinate and atomic number tensor.\n\n\t\tReturns:\n\t\t  mol batches of atoms, coords, NNZ and CoreReal (number of energetic atoms in this mol)\n\t\t""""""\n\t\tx0,nvox_,voxs = self.MakeVoxelList(x_,nreal_) # only the first n atoms are periodically real.\n\t\tprint(""nvox, Voxel assignments: "", nvox_, voxs, np.max(voxs))\n\t\tif (np.max(voxs) > nvox_*nvox_*nvox_):\n\t\t\tprint(""Wrong nvox_"")\n\t\t\traise Exception(""Bad NVox"")\n\t\tnvox = int(nvox_)\n\t\tnvox3 = nvox*nvox*nvox\n\t\tfvox = voxs.flatten()\n\t\tmxvx = np.max(fvox)\n\t\tmnvx = np.min(fvox)\n\t\tmembercounts = np.bincount(fvox,minlength=nvox3)\n\t\tprint(membercounts)\n\t\tcore = voxs[:nreal_,13]\n\t\tcorec = np.bincount(core,minlength=nvox3)\n\t\tvxkey = np.zeros(nvox3,dtype=np.int)\n\t\tvxkey -= 1\n\t\tnnzvx = 0\n\t\tcoremem=[] # only voxels containing core atoms will be generated.\n\t\t# Determine the number of nonzero voxels (which contain real atoms)\n\t\tfor i in range(corec.shape[0]):\n\t\t\tif corec[i] > 0:\n\t\t\t\tvxkey[i] = nnzvx\n\t\t\t\tprint(i,nnzvx)\n\t\t\t\tcoremem.append(membercounts[i])\n\t\t\t\tnnzvx += 1\n\t\tcoremem = np.array(coremem)\n\t\tmaxnatom = np.max(coremem)\n\t\tprint(""Max Number of Atoms: "",coremem,maxnatom)\n\t\tminnatom = np.min(coremem)\n\t\tfilling = np.zeros(nnzvx,dtype=np.int)\n\t\tcoords = np.zeros((nnzvx,maxnatom,3))\n\t\tprint(""Coords.shape"",coords.shape)\n\t\tatoms = np.zeros((nnzvx,maxnatom,1),dtype=np.uint8)\n\t\tabsindex = np.zeros((nnzvx,maxnatom,1),dtype=np.uint8)\n\t\tRealOrImage = np.zeros((nnzvx,maxnatom,1),dtype=np.uint8)\n\t\tfor i in range(x_.shape[0]):\n\t\t\tfor k,vx in enumerate(voxs[i]):\n\t\t\t\tvk = vxkey[vx]\n\t\t\t\tif (vk < 0):\n\t\t\t\t\tcontinue;\n\t\t\t\t#print(i, k , vx, vk, filling[vk], membercounts[vx])\n\t\t\t\tif (k==13):\n\t\t\t\t    RealOrImage[vk,filling[vk],0] = 1\n\t\t\t\tcoords[vk,filling[vk],:] = x_[i]\n\t\t\t\tatoms[vk,filling[vk],:] = z_[i]\n\t\t\t\tabsindex[vk,filling[vk],:] = i\n\t\t\t\tfilling[vk] += 1\n\t\treturn atoms, coords, filling, RealOrImage, absindex\n\nclass TFPeriodic():\n\tdef __init__(self , rcut_ = 16.0, lat_ = np.array(np.eye(3))):\n\t\t""""""\n\t\tArgs:\n\t\t\trcut_: a cutoff radius (A) used for softness.\n\t\t\tlat_: a 3x3 ndarray of initial lattice vectors.\n\t\t""""""\n\t\t# Determine the required tesselations to perform.\n\t\tself.AssignLat(lat_)\n\t\t# or at least checked, but not for now.\n\t\tself.rcut = rcut_\n\t\t# Placeholders....\n\t\tself.x_pl = None\n\t\tself.z_pl = None\n\t\tself.lat_pl = None\n\t\tself.tess_pl = None\n\t\t# Molwise-voxel batch placeholders.\n\t\txs_pl = None\n\t\tzs_pl = None\n\t\t# Graphs of desired outputs produced by __call__\n\t\tself.energy = None\n\t\tself.force = None\n\t\tself.stress = None\n\t\t# Graphs of desired outputs produced by __call__\n\t\tself.PW = None\n\t\tself.energies = None\n\t\tself.forces = None\n\t\tself.stresses = None\n\t\tself.Prepare()\n\t\treturn\n\tdef AssignLat(self,lat_):\n\t\tself.lat = lat_.copy()\n\t\tlatcenter = (lat_[0]+lat_[1]+lat_[2])/2.0\n\t\tvertices = [(lat_[0]+lat_[1])/2.0,(lat_[2]+lat_[1])/2.0,(lat_[0]+lat_[2])/2.0,lat_[0],lat_[1],lat_[2]]\n\t\tdists = [np.linalg.norm(v-latcenter) for v in vertices]\n\t\tself.ntess = max(int((rcut_ -  np.min(dists))/np.min(np.sum(np.square(lat_), axis=1)**0.5)),0) + 1  # this may only work for\n\t\ttessrng = range(-self.ntess,self.ntess+1)\n\t\ttesslist = [[0,0,0]] # Only real atoms are in the first unit cell.\n\t\tfor t1 in tessrng:\n\t\t\tfor t2 in tessrng:\n\t\t\t\tfor t3 in tessrng:\n\t\t\t\t\tif ((t1 != 0) or (t2 != 0) or (t3 != 0) ):\n\t\t\t\t\t\ttesslist.append([t1,t2,t3])\n\t\tself.tess = np.array(tesslist).reshape((pow(len(tessrng),3),3))\n\t\treturn\n\tdef Prepare(self):\n\t\twith self.g.as_default():\n\t\t\tself.z_pl = tf.placeholder(tf.float64, shape=tuple([None]))\n\t\t\tself.x_pl = tf.placeholder(tf.float64, shape=tuple([None,3]))\n\t\t\tself.lat_pl = tf.placeholder(tf.float64, shape=tuple([3,3]))\n\t\t\tself.tess_pl = tf.placeholder(tf.float64, shape=tuple([None,3]))\n\t\t\tself.xs_pl = tf.placeholder(tf.float64, shape=tuple([None, None, 3]))\n\t\t\tself.zs_pl = tf.placeholder(tf.float64, shape=tuple([None, None, 1]))\n\t\t\tself.Rc = tf.Variable(self.rcut,dtype = tf.float64)\n\t\t\tself.PW = self.PeriodicWrapping(self.z_pl, self.x_pl, self.lat_pl, self.tess_pl)\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\tdef GetLatMetrics(self,lat_):\n\t\t""""""\n\t\tReturns <lat_i| lat_j>**-0.5, and **0.5\n\t\tThese are the change of basis matrices into and out-of lattice coordinates.\n\t\t""""""\n\t\treturn TFMatrixSqrt(tf.matmul(lat_, tf.transpose(lat_)))\n\tdef InLat(self,xyz_,lat_):\n\t\t""""""\n\t\tConverts xyz_ into linear combination of the row vectors in lat_\n\t\t""""""\n\t\tlatmet = tf.matrix_inverse(tf.matmul(lat_, tf.transpose(lat_)))\n\t\treturn tf.matmul(xyz_,tf.matmul(tf.transpose(lat_),latmet))\n\tdef FromLat(self,abc_,lat_):\n\t\t""""""\n\t\tConverts abc_ from lattice to cartesian.\n\t\t""""""\n\t\treturn tf.matmul(abc_,lat_)\n\tdef PeriodicWrapping(self,z_, xyz_, lat_, tess_):\n\t\t""""""\n\t\tThis tesselates xyz_ using the lattice vectors lat_\n\t\taccording to tess_, so that lat_ can be differentiable tensorflow varaiables.\n\t\tie: the stress tensor can be calculated. The first block of atoms are\n\t\tthe \'real\' atoms (ie: atoms whose forces will be calculated)\n\t\tthe remainder are images.\n\n\t\tArgs:\n\t\t\tz_: vector of atomic numbers\n\t\t\txyz_: nprimitive atoms X 3 coordinates.\n\t\t\tlat_: 3x3 lattice matrix.\n\t\t\ttess_: an ntess X 3 matrix of tesselations to perform. The identity is the assumed initial state.\n\t\t""""""\n\t\t# Move into the lattice frame and tesselate there.\n\t\tx_lat = self.InLat(xyz_,lat_)\n\t\tntess = tf.shape(tess_)[0]\n\t\ti0 = tf.zeros(1,tf.int32)[0]\n\t\ttess=tf.cast(tess_,dtype=tf.float64)\n\t\tnatold = tf.shape(z_)[0]\n\t\tnatnew = tf.shape(z_)*ntess\n\t\toutz = tf.TensorArray(tf.float64, size=ntess)\n\t\toutx = tf.TensorArray(tf.float64, size=ntess)\n\t\tcond = lambda i,z,x: tf.less(i,ntess)\n\t\tbody = lambda i,z,x: [i+1, outz.write(i,z_) ,outx.write(i,x_lat+tess[i,0])]\n\t\tinital = (0,outz,outx)\n\t\ti,zr,xr = tf.while_loop(cond,body,inital)\n\t\txcart = self.FromLat(xr.concat(),lat_)\n\t\treturn zr.concat(), xcart\n\tdef MakeTess(self, z_, x_, lat_):\n\t\t""""""\n\t\tPerform the tesselation if you want to call this externally.\n\t\tThis is not desireable. Efficient periodic forces should inherit this class\n\t\tand over-ride the call method, so that the tesselation is in the same\n\t\tgraph as the evaluation of the force.\n\t\t""""""\n\t\t# Step 1: Tesselate the cell.\n\t\tfeeddict = {self.z_pl:z_, self.x_pl:x_, self.lat_pl: lat_, self.tess_pl:self.tess}\n\t\tzp, xp = self.sess.run(self.PW,feed_dict=feeddict)\n\t\treturn zp, xp\n\nclass TFPeriodicVoxelForce(LinearVoxelBase, TFPeriodic):\n\tdef __init__(self , rcut_ = 16.0, lat_ = np.array(np.eye(3))):\n\t\t""""""\n\t\tBase periodic local tensorflow evaluator.\n\t\tAnd serves as a prototype for other simple differentiable\n\t\tperiodic forces. properly softens the\n\t\tforce at it\'s cutoff. Check out PeriodicTF.py\n\n\t\tOne key option is self.StressDifferentiable.\n\t\tIf that\'s true, then the stress tensor can be calculated\n\t\tat the cost of a much more complex computational graph.\n\t\tBecause the formation of the voxel molecules must be done\n\t\twithin tensorflow.\n\n\t\tArgs:\n\t\t\trcut_: a cutoff radius (A) used for softness.\n\t\t\tlat_: a 3x3 ndarray of initial lattice vectors.\n\t\t""""""\n\t\t# Determine the required tesselations to perform.\n\t\tself.lat = lat_.copy()\n\t\t# Ensure the number of tesselations cover the physical interactions.\n\t\tlatcenter = (lat_[0]+lat_[1]+lat_[2])/2.0\n\t\tvertices = [(lat_[0]+lat_[1])/2.0,(lat_[2]+lat_[1])/2.0,(lat_[0]+lat_[2])/2.0,lat_[0],lat_[1],lat_[2]]\n\t\tdists = [np.linalg.norm(v-latcenter) for v in vertices]\n\t\t#self.ntess = int(rcut_ / np.min(dists))\n\t\t#print (""mini:"", np.min(np.sum(np.square(lat_), axis=1)**0.5))\n\t\tself.ntess = int((rcut_ -  np.min(dists))/np.min(np.sum(np.square(lat_), axis=1)**0.5)) + 1  # this may only work for cubic cell\n\t\ttessrng = range(-self.ntess,self.ntess+1)\n\t\ttesslist = [[0,0,0]] # Only real atoms are in the first unit cell.\n\t\tfor t1 in tessrng:\n\t\t\tfor t2 in tessrng:\n\t\t\t\tfor t3 in tessrng:\n\t\t\t\t\tif ((t1 != 0) or (t2 != 0) or (t3 != 0) ):\n\t\t\t\t\t\ttesslist.append([t1,t2,t3])\n\t\tself.tess = np.array(tesslist).reshape((pow(len(tessrng),3),3))\n\t\t# The number of tesselations should probably be variable....\n\t\t# or at least checked, but not for now.\n\t\tLinearVoxelBase.__init__(self, rcut_)\n\t\tself.rcut = rcut_\n\t\t# Placeholders....\n\t\tself.x_pl = None\n\t\tself.z_pl = None\n\t\tself.lat_pl = None\n\t\tself.tess_pl = None\n\t\t# Molwise-voxel batch placeholders.\n\t\txs_pl = None\n\t\tzs_pl = None\n\t\t# Graphs of desired outputs produced by __call__\n\t\tself.energy = None\n\t\tself.force = None\n\t\tself.stress = None\n\t\t# Graphs of desired outputs produced by __call__\n\t\tself.PW = None\n\t\tself.energies = None\n\t\tself.forces = None\n\t\tself.stresses = None\n\t\tself.Prepare()\n\t\treturn\n\n\tdef Prepare(self):\n\t\tLinearVoxelBase.Prepare(self)\n\t\twith self.g.as_default():\n\t\t\tself.z_pl = tf.placeholder(tf.float64, shape=tuple([None]))\n\t\t\tself.x_pl = tf.placeholder(tf.float64, shape=tuple([None,3]))\n\t\t\tself.lat_pl = tf.placeholder(tf.float64, shape=tuple([3,3]))\n\t\t\tself.tess_pl = tf.placeholder(tf.float64, shape=tuple([None,3]))\n\t\t\tself.xs_pl = tf.placeholder(tf.float64, shape=tuple([None, None, 3]))\n\t\t\tself.zs_pl = tf.placeholder(tf.float64, shape=tuple([None, None, 1]))\n\t\t\tself.Rc = tf.Variable(self.rcut,dtype = tf.float64)\n\t\t\tself.PW = self.PeriodicWrapping(self.z_pl, self.x_pl, self.lat_pl, self.tess_pl)\n\t\t\tself.energies = self.LJEnergies(self.xs_pl)\n\t\t\tself.forces = tf.gradients(self.energies,self.xs_pl)[0]\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\tdef GetLatMetrics(self,lat_):\n\t\t""""""\n\t\tReturns <lat_i| lat_j>**-0.5, and **0.5\n\t\tThese are the change of basis matrices into and out-of lattice coordinates.\n\t\t""""""\n\t\treturn TFMatrixSqrt(tf.matmul(lat_, tf.transpose(lat_)))\n\tdef InLat(self,xyz_,lat_):\n\t\t""""""\n\t\tConverts xyz_ into linear combination of the row vectors in lat_\n\t\t""""""\n\t\tlatmet = tf.matrix_inverse(tf.matmul(lat_, tf.transpose(lat_)))\n\t\treturn tf.matmul(xyz_,tf.matmul(tf.transpose(lat_),latmet))\n\tdef FromLat(self,abc_,lat_):\n\t\t""""""\n\t\tConverts abc_ from lattice to cartesian.\n\t\t""""""\n\t\treturn tf.matmul(abc_,lat_)\n\tdef PeriodicWrapping(self,z_, xyz_, lat_, tess_):\n\t\t""""""\n\t\tThis tesselates xyz_ using the lattice vectors lat_\n\t\taccording to tess_, so that lat_ can be differentiable tensorflow varaiables.\n\t\tie: the stress tensor can be calculated. The first block of atoms are\n\t\tthe \'real\' atoms (ie: atoms whose forces will be calculated)\n\t\tthe remainder are images.\n\n\t\tArgs:\n\t\t    z_: vector of atomic numbers\n\t\t    xyz_: nprimitive atoms X 3 coordinates.\n\t\t    lat_: 3x3 lattice matrix.\n\t\t    tess_: an ntess X 3 matrix of tesselations to perform. The identity is the assumed initial state.\n\t\t""""""\n\t\t# Move into the lattice frame and tesselate there.\n\t\tx_lat = self.InLat(xyz_,lat_)\n\t\tntess = tf.shape(tess_)[0]\n\t\ti0 = tf.zeros(1,tf.int32)[0]\n\t\ttess=tf.cast(tess_,dtype=tf.float64)\n\t\tnatold = tf.shape(z_)[0]\n\t\tnatnew = tf.shape(z_)*ntess\n\t\toutz = tf.TensorArray(tf.float64, size=ntess)\n\t\toutx = tf.TensorArray(tf.float64, size=ntess)\n\t\tcond = lambda i,z,x: tf.less(i,ntess)\n\t\tbody = lambda i,z,x: [i+1, outz.write(i,z_) ,outx.write(i,x_lat+tess[i,0])]\n\t\tinital = (0,outz,outx)\n\t\ti,zr,xr = tf.while_loop(cond,body,inital)\n\t\txcart = self.FromLat(xr.concat(),lat_)\n\t\treturn zr.concat(), xcart\n\tdef __call__(self, z_, x_, lat_):\n\t\t""""""\n\t\tThis doesn\'t support direct calculation of\n\t\tstress tensor derivatives yet... It\'s really\n\t\tannoying to put the batch creation into tensorflow\n\n\t\tArgs:\n\t\t\tz_: atoms within the unit cell. (numpy array)\n\t\t\tx_: Coordinates within the unit cell.\n\t\t\tlat_: 3x3 matrix of lattice coords.\n\t\t\ttess_: List of non-identity tesselations to perform.\n\n\t\tReturns:\n\t\t\treturns the energy, force within the cell, and stress tensor.\n\t\t""""""\n\t\t# Step 1: Tesselate the cell.\n\t\tfeeddict = {self.z_pl:z_, self.x_pl:x_, self.lat_pl: lat_, self.tess_pl:self.tess}\n\t\tzp, xp = self.sess.run(self.PW,feed_dict=feeddict)\n\t\treturn zp, xp\n\t\tprint(""z,xp.shape after tess: "", zp.shape, xp.shape, np.max(xp),np.min(xp))\n\t\t# Step 2: Make a voxel batch out of that.\n\t\tats, crds, fill, roi, inds = self.MakeVoxelMols(zp,xp,x_.shape[0])\n\t\tprint(""crds.shape after voxelization: "", crds.shape)\n\t\tprint(""inds.shape after voxelization: "", inds.shape)\n\t\t# Step 3: evaluate the force using quadratic algs. over the voxels.\n\t\tfeeddict = {self.zs_pl:ats, self.xs_pl:crds}\n\t\t# This routine returns partial forces partitioned over atoms.\n\t\tEns,Frc = self.sess.run([self.energies, self.forces],feed_dict=feeddict)\n\t\t# Grab back out the desired paritial energy and force due to the real atoms in the unit cell.\n\t\tprint(""Nreal atoms:"", x_.shape, Frc.shape)\n\t\tnrealat = x_.shape[0]\n\t\tFrcToRe = np.zeros(x_.shape)\n\t\tEnToRe = 0.0\n\t\tStrsToRe = np.zeros((3,3))\n\t\tfor m in range(crds.shape[0]):\n\t\t\tfor i in range(crds.shape[1]):\n\t\t\t\tif (inds[m,i]<nrealat and roi[m,i] ==1):\n\t\t\t\t\tFrcToRe[inds[m,i]] = Frc[m,i]\n\t\t\t\t\tEnToRe += Ens[m,i]\n\t\treturn EnToRe, JOULEPERHARTREE*FrcToRe, JOULEPERHARTREE*StrsToRe[0] # Returns energies and forces and stresses\n\tdef LJEnergies(self,XYZs_):\n\t\t""""""\n\t\tReturns LJ Energies batched over molecules.\n\t\tInput can be padded with zeros. That will be\n\t\tremoved by LJKernels.\n\n\t\tArgs:\n\t\t\tXYZs_: nmols X maxatom X 3 coordinate tensor.\n\t\tReturns:\n\t\t\tA vector of partial energies for each atom given as an argument.\n\t\t""""""\n\t\tDs = TFDistances(XYZs_)\n\t\tDs = tf.where(tf.is_nan(Ds), tf.zeros_like(Ds), Ds)\n\t\tones = tf.ones(tf.shape(Ds),dtype = tf.float64)\n\t\tzeros = tf.zeros(tf.shape(Ds),dtype = tf.float64)\n\t\tZeroTensor = tf.where(tf.less_equal(Ds,0.000000001),ones,zeros)\n\t\tDs += ZeroTensor\n\t\tR = 0.5/Ds\n\t\tKs = 0.5*(tf.pow(R,12.0)-2.0*tf.pow(R,6.0))\n\t\t# Use the ZeroTensors to mask the output for zero dist or AN.\n\t\tKs = tf.where(tf.equal(ZeroTensor,1.0),tf.zeros_like(Ks),Ks)\n\t\tKs = tf.where(tf.is_nan(Ks),tf.zeros_like(Ks),Ks)\n\t\tKs = tf.matrix_band_part(Ks, 0, -1)\n\t\tEns = tf.reduce_sum(Ks,[2])\n\t\treturn Ens\n'"
TensorMol/ForceModels/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom .Electrostatics import *\nfrom .ElectrostaticsTF import *\nfrom .TFForces import *\nfrom .TFModels import *\nfrom .TFPeriodicForces import *\n'
TensorMol/ForceModifiers/Accommodator.py,0,"b'""""""\nThis is a dirty dirty hack which makes forces evaluable even when there is\nno trained network for an atom type, etc. does QM/MM Type embeddings etc.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .Periodic import *\n\nclass Force:\n\tdef __init__(self, rng_=1.0, NeedsTriples_=False):\n\t\t""""""\n\t\tThis is a dummy force, which basically nullify\'s an atom\'s contribution to the\n\t\ttotal energy.\n\t\t""""""\n\t\tself.range = rng_\n\t\tself.NeedsTriples = NeedsTriples_\n\t\treturn\n\tdef __call__(self, z, x, NZ, DoForce = True):\n\t\t""""""\n\t\tGeneric call to a linear scaling local force.\n\n\t\tArgs:\n\t\t\tz: atomic number vector\n\t\t\tx: atoms X 3 coordinate vector.\n\t\t\tNZ: pair or triples matrix. (NZP X 2)\n\t\treturns:\n\t\t\tenergy number, and force vector with same shape as x.\n\t\t""""""\n\t\treturn np.zeros(x.shape)\n\nclass DenseForce:\n\tdef __init__(self, rng_=1.0, NeedsTriples_=False):\n\t\t""""""\n\t\tThis is a force which has a local sight radius.\n\t\tand is then broken into overlapping local forces. \n\t\t""""""\n\t\tself.range = rng_\n\t\treturn\n\tdef __call__(self, z, x, NZ, DoForce = True):\n\t\t""""""\n\t\tGeneric call to a linear scaling local force.\n\n\t\tArgs:\n\t\t\tz: atomic number vector\n\t\t\tx: atoms X 3 coordinate vector.\n\t\t\tNZ: pair or triples matrix. (NZP X 2)\n\t\treturns:\n\t\t\tenergy number, and force vector with same shape as x.\n\t\t""""""\n\t\treturn np.zeros(x.shape)\n\nclass ForceAdaptor:\n\tdef __init__(self, m):\n\t\t""""""\n\t\tTo use the ForceAdaptor you bind local forces to the accomodator with lists that\n\t\tspecify what atom-types they can embed, evaluate forces on, and which atoms you\'d like them\n\t\tto apply to. The force accomodator will break up the overall energy into these contributions.\n\t\t""""""\n\t\tself.NL = None\n\t\tself.mol0 = self.lattice.CenteredInLattice(pm_)\n\t\tself.atoms = self.mol0.atoms.copy()\n\t\tself.natoms = self.mol0.NAtoms()\n\t\tself.natomsReal = pm_.NAtoms()\n\t\tself.maxrng = 0.0\n\t\tself.Forces = []\n\t\tself.ForcedAtoms=[] # Numforces list of atoms which experience the force.\n\t\tself.lastx = np.zeros(pm_.coords.shape)\n\t\treturn\n\tdef BindForce(self, lf_, fa_, rng_=15.0):\n\t\t""""""\n\t\tAdds a local force to be computed when the PeriodicForce is called.\n\n\t\tArgs:\n\t\t\tlf_: a function which takes z,x and returns atom energies, atom forces.\n\t\t\trng_: the visibility radius of this force (A)\n\t\t\tfa_: Integer list of atoms in m to which this force applies.\n\t\t""""""\n\t\tself.Forces.append(Force(lf_,rng_))\n\t\tself.ForcedAtoms.append(np.array(fa,dtype=np.uint32))\n\tdef __call__(self,x_):\n\t\t""""""\n\t\tReturns the Energy per unit cell and force on all atoms\n\n\t\tArgs:\n\t\t\tx_: a primitive geometry of atoms matching self.atoms.\n\t\t""""""\n\t\tetore = 0.0\n\t\tftore = np.zeros((self.natomsReal,3))\n\t\tif (self.maxrng == 0.0):\n\t\t\tself.maxrng = max([f.range for f in self.LocalForces])\n\t\tfor i in range(len(self.Forces)):\n\t\t\t# Compute all the atoms within the sensory radius\n\t\t\t# and all forced atoms, and increment the forces.\n'"
TensorMol/ForceModifiers/Neighbors.py,0,"b'""""""\nLinear Scaling Atom Neighbor List Generators.\nsee also: http://scipy-cookbook.readthedocs.io/items/KDTree_example.html\nDepending on cutoffs and density these scale to >20,000 atoms\n\nTODO:\n\tfix bug with generation when there\'s no neighbors...  IE:\n\t\tFile ""/Users/johnparkhill/TensorMol/TensorMol/TFMolManage.py"", line 1288, in EvalBPDirectEEUpdateSinglePeriodic\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexPeriodic(Rr_cut, Ra_cut, self.Instances.eles_np, self.Instances.eles_pairs_np)\n\t\tFile ""/Users/johnparkhill/TensorMol/TensorMol/Neighbors.py"", line 413, in buildPairsAndTriplesWithEleIndexPeriodic\n\t\ttrpE_sorted, trtE_sorted, mil_jk, jk_max = self.buildPairsAndTriplesWithEleIndex(rcut_pairs, rcut_triples, ele, elep)\n\t\tFile ""/Users/johnparkhill/TensorMol/TensorMol/Neighbors.py"", line 367, in buildPairsAndTriplesWithEleIndex\n\t\tprev_l = trtE_sorted[0][4]\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Util import *\nimport numpy as np\nfrom MolEmb import *\nimport time\n\nclass NeighborList:\n\t""""""\n\tTODO: incremental tree and neighborlist updates.\n\t""""""\n\t@TMTiming(""NeighborList::init"")\n\tdef __init__(self, x_, DoTriples_ = False, DoPerms_ = False, ele_ = None, alg_ = None, sort_ = False):\n\t\t""""""\n\t\tBuilds or updates a neighbor list of atoms within rcut_\n\t\tusing n*Log(n) kd-tree.\n\n\t\tArgs:\n\t\t\tx_: coordinate array\n\t\t\trcut_: distance cutoff.\n\t\t\tele_: element types of each atoms.\n\t\t\tsort_: whether sorting the jk in triples by atom index\n\t\t""""""\n\t\tself.natom = x_.shape[0] # includes periodic images.\n\t\tself.x = x_.T.copy()\n\t\tself.pairs = None\n\t\tself.triples = None\n\t\tself.DoTriples = DoTriples_\n\t\tself.DoPerms = DoPerms_\n\t\tself.ele = ele_\n\t\tself.npairs = None\n\t\tself.ntriples = None\n\t\tself.alg = 0 if self.natom < 2000000 else 1\n\t\t#self.alg = 0 if self.natom < 20000 else 1\n\t\tif (alg_ != None):\n\t\t\tself.alg = alg_\n\t\tself.sort = sort_\n\t\treturn\n\n\tdef Update(self, x_, rcut_pairs=5.0, rcut_triples=5.0, molind_ = None, nreal_ = None):\n\t\t""""""\n\t\tIn the future this should only force incremental builds.\n\n\t\tArgs:\n\t\t\tx_: coordinates.\n\t\t\trcut_: cutoff of the force.\n\t\t\tmolind_: possible molecule index if we are doing a set.\n\t\t\tnreal_: only generate pairs for the first nreal_ atoms.\n\t\t""""""\n\t\tself.x = x_.copy()\n\t\tif (self.DoTriples):\n\t\t\tself.pairs, self.triples = self.buildPairsAndTriples(rcut_pairs,rcut_triples, molind_, nreal_=nreal_)\n\t\t\tself.npairs = self.pairs.shape[0]\n\t\t\tself.ntriples = self.triples.shape[0]\n\t\telse:\n\t\t\tself.pairs = self.buildPairs(rcut_pairs, molind_, nreal_=nreal_)\n\t\t\tself.npairs = self.pairs.shape[0]\n\t\treturn\n\n\t@TMTiming(""NeighborList::BuildPairs"")\n\tdef buildPairs(self, rcut=5.0, molind_=None, nreal_=None):\n\t\t""""""\n\t\tReturns the nonzero pairs, triples in self.x within the cutoff.\n\t\tTriples are non-repeating ie: no 1,2,2 or 2,2,2 etc. but unordered\n\n\t\tArgs:\n\t\t\trcut: the cutoff for pairs and triples\n\t\tReturns:\n\t\t\tpair matrix (npair X 2)\n\t\t\ttriples matrix (ntrip X 3)\n\t\t""""""\n\t\tpair = []\n\t\tntodo = self.natom\n\t\tif (nreal_ != None):\n\t\t\tntodo = int(nreal_)\n\t\tpair = None\n\t\tif (self.alg==0):\n\t\t\tpair = Make_NListNaive(self.x,rcut,ntodo,int(self.DoPerms))\n\t\telse:\n\t\t\tpair = Make_NListLinear(self.x,rcut,ntodo,int(self.DoPerms))\n\t\tnpairi = map(len,pair)\n\t\tnpair = sum(npairi)\n\t\tp = None\n\t\tif (molind_!=None):\n\t\t\tp=np.zeros((npair,3),dtype = np.uint64)\n\t\telse:\n\t\t\tp=np.zeros((npair,2),dtype = np.uint64)\n\t\tpp = 0\n\t\tfor i in range(ntodo):\n\t\t\tfor j in pair[i]:\n\t\t\t\tif (molind_!=None):\n\t\t\t\t\tp[pp,0]=molind_\n\t\t\t\t\tp[pp,1]=i\n\t\t\t\t\tp[pp,2]=j\n\t\t\t\telse:\n\t\t\t\t\tp[pp,0]=i\n\t\t\t\t\tp[pp,1]=j\n\t\t\t\tpp = pp+1\n\t\tdel pair\n\t\treturn p\n\n\tdef buildPairsAndTriples(self, rcut_pairs=5.0, rcut_triples=5.0, molind_=None, nreal_=None):\n\t\t""""""\n\t\tReturns the nonzero pairs, triples in self.x within the cutoff.\n\t\tTriples are non-repeating ie: no 1,2,2 or 2,2,2 etc. but unordered\n\n\t\tArgs:\n\t\t\trcut: the cutoff for pairs and triples\n\t\tReturns:\n\t\t\tpair matrix (npair X 2)\n\t\t\ttriples matrix (ntrip X 3)\n\t\t""""""\n\t\tif (self.ele is None):\n\t\t\tprint(""WARNING... need self.ele for angular SymFunc triples... "")\n\t\tpair = []\n\t\ttpair = [] # since these may have different cutoff\n\t\tntodo = self.natom\n\t\tif (nreal_ != None):\n\t\t\tntodo = int(nreal_)\n\t\tpair = None\n\t\ttpair = None\n\n\t\tif (self.alg==0):\n\t\t\tpair = Make_NListNaive(self.x,rcut_pairs,int(ntodo),int(self.DoPerms))\n\t\t\ttpair = Make_NListNaive(self.x,rcut_triples,int(ntodo),int(self.DoPerms))\n\t\telse:\n\t\t\tpair = Make_NListLinear(self.x,rcut_pairs,int(ntodo),int(self.DoPerms))\n\t\t\ttpair = Make_NListLinear(self.x,rcut_triples,int(ntodo),int(self.DoPerms))\n\t\tnpairi = map(len,pair)\n\t\tnpair = sum(npairi)\n\t\tnpairi = map(len,tpair)\n\t\t#ntrip = sum(map(lambda x: x*x if x>0 else 0, npairi))\n\t\tntrip = sum(map(lambda x: int(x*(x-1)/2) if x>0 else 0, npairi))\n\t\t#print (""npair:"", npair, "" ntrip:"", ntrip, "" rcut_triples:"", rcut_triples, "" tpair"", tpair, "" ntodo:"", int(ntodo), "" self.DoPerms"", int(self.DoPerms), "" x:"", self.x[:int(ntodo)], "" pair:"", pair)\n\t\tp = None\n\t\tt = None\n\t\tif (molind_!=None):\n\t\t\tp=np.zeros((npair,3),dtype = np.uint64)\n\t\t\tt=np.zeros((ntrip,4),dtype = np.uint64)\n\t\telse:\n\t\t\tp=np.zeros((npair,2),dtype = np.uint64)\n\t\t\tt=np.zeros((ntrip,3),dtype = np.uint64)\n\t\tpp = 0\n\t\ttp = 0\n\t\tfor i in range(ntodo):\n\t\t\tfor j in pair[i]:\n\t\t\t\tif (molind_!=None):\n\t\t\t\t\tp[pp,0]=molind_\n\t\t\t\t\tp[pp,1]=i\n\t\t\t\t\tp[pp,2]=j\n\t\t\t\telse:\n\t\t\t\t\tp[pp,0]=i\n\t\t\t\t\tp[pp,1]=j\n\t\t\t\tpp = pp+1\n\t\t\tfor j in tpair[i]:\n\t\t\t\tfor k in tpair[i]:\n\t\t\t\t\t#if True:\n\t\t\t\t\tif (k > j): # do not do ijk, ikj permutation\n\t\t\t\t\t#if (k!=j):\n\t\t\t\t\t\tif (molind_!=None):\n\t\t\t\t\t\t\tt[tp,0]=molind_\n\t\t\t\t\t\t\tt[tp,1]=i\n\t\t\t\t\t\t\tif self.ele is not None and self.ele[j] > self.ele[k]:  # atom will smaller element index alway go first\n\t\t\t\t\t\t\t\tt[tp,2]=k\n\t\t\t\t\t\t\t\tt[tp,3]=j\n\t\t\t\t\t\t\telif self.sort and j > k:\n\t\t\t\t\t\t\t\tt[tp,2]=k\n\t\t\t\t\t\t\t\tt[tp,3]=j\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tt[tp,2]=j\n\t\t\t\t\t\t\t\tt[tp,3]=k\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tt[tp,0]=i\n\t\t\t\t\t\t\tif self.ele is not None and self.ele[j] > self.ele[k]:\n\t\t\t\t\t\t\t\tt[tp,1]=k\n\t\t\t\t\t\t\t\tt[tp,2]=j\n\t\t\t\t\t\t\telif self.sort and j > k:\n\t\t\t\t\t\t\t\tt[tp,1]=k\n\t\t\t\t\t\t\t\tt[tp,2]=j\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tt[tp,1]=j\n\t\t\t\t\t\t\t\tt[tp,2]=k\n\t\t\t\t\t\ttp=tp+1\n\t\tdel pair\n\t\tdel tpair\n\t\treturn p,t\n\nclass NeighborListSet:\n\tdef __init__(self, x_, nnz_, DoTriples_=False, DoPerms_=False, ele_=None, alg_ = None, sort_ = False):\n\t\t""""""\n\t\tA neighborlist for a set\n\n\t\tArgs:\n\t\t\tx_: NMol X MaxNAtom X 3 tensor of coordinates.\n\t\t\tnnz_: NMol vector of maximum atoms in each mol.\n\t\t\tele_: element type of each atom.\n\t\t\tsort_: whether sort jk in triples by atom index\n\t\t""""""\n\t\tself.nlist = []\n\t\tself.nmol = x_.shape[0]\n\t\tself.maxnatom = x_.shape[1]\n\t\tself.alg = 0 if self.maxnatom < 2000000 else 1\n\t\t#self.alg = 0 if self.maxnatom < 20000 else 1\n\t\tif (alg_ != None):\n\t\t\tself.alg = alg_\n\t\t# alg=0 naive quadratic.\n\t\t# alg=1 linear scaling\n\t\t# alg=2 PairProvider\n\t\tself.x = x_.copy()\n\t\tself.nnz = nnz_.copy()\n\t\tself.nreal = nnz_.copy() # Over-ridden in the WithImages child of this class.\n\t\tself.ele = ele_\n\t\tself.sort = sort_\n\t\tself.pairs = None\n\t\tself.DoTriples = DoTriples_\n\t\tself.DoPerms = DoPerms_\n\t\tself.triples = None\n\t\tself.UpdateInterval = 1\n\t\tself.UpdateCounter = 0\n\t\tself.PairMaker=None\n\t\tif (self.alg<2):\n\t\t\tif self.ele is None:\n\t\t\t\tfor i in range(self.nmol):\n\t\t\t\t\tself.nlist.append(NeighborList(x_[i,:nnz_[i]],DoTriples_,DoPerms_, None, self.alg, self.sort))\n\t\t\telse:\n\t\t\t\tfor i in range(self.nmol):\n\t\t\t\t\tself.nlist.append(NeighborList(x_[i,:nnz_[i]],DoTriples_,DoPerms_, self.ele[i,:nnz_[i]], self.alg, self.sort))\n\t\telse:\n\t\t\traise Exception(""Using fucked up old code."")\n\t\t\tself.PairMaker = PairProvider(self.nmol,self.maxnatom)\n\t\treturn\n\n\t@TMTiming(""NLSetUpdate"")\n\tdef Update(self, x_, rcut_pairs = 5.0, rcut_triples = 5.0):\n\t\tif (self.UpdateCounter == 0):\n\t\t\tself.UpdateCounter = self.UpdateCounter + 1\n\t\t\tself.x = x_.copy()\n\t\t\tif (self.DoTriples):\n\t\t\t\tself.pairs, self.triples = self.buildPairsAndTriples(rcut_pairs,rcut_triples)\n\t\t\telse:\n\t\t\t\tself.pairs = self.buildPairs(rcut_pairs)\n\t\telif (self.UpdateCounter < self.UpdateInterval):\n\t\t\tself.UpdateCounter = self.UpdateCounter + 1\n\t\telse:\n\t\t\tself.UpdateCounter = 0\n\n\tdef buildPairs(self, rcut=5.0):\n\t\t""""""\n\t\tbuilds nonzero pairs and triples for current x.\n\n\t\tArgs:\n\t\t\trcut_: a cutoff parameter.\n\t\tReturns:\n\t\t\t(nnzero pairs X 3 pair tensor) (mol , I , J)\n\t\t""""""\n\t\tif self.alg < 2:\n\t\t\tfor i,mol in enumerate(self.nlist):\n\t\t\t\tmol.Update(self.x[i,:self.nnz[i]],rcut,rcut,i, self.nreal[i])\n\t\telse:\n\t\t\treturn self.PairMaker(self.x,rcut,self.nnz)\n\t\tnzp = sum([mol.npairs for mol in self.nlist])\n\t\ttrp = np.zeros((nzp,3),dtype=np.uint64)\n\t\tpp = 0\n\t\tfor mol in self.nlist:\n\t\t\ttrp[pp:pp+mol.npairs] = mol.pairs\n\t\t\tpp += mol.npairs\n\t\treturn trp\n\n\t@TMTiming(""SetbuildPairsAndTriples"")\n\tdef buildPairsAndTriples(self, rcut_pairs=5.0, rcut_triples=5.0):\n\t\t""""""\n\t\tbuilds nonzero pairs and triples for current x.\n\n\t\tArgs:\n\t\t\trcut_: a cutoff parameter.\n\t\tReturns:\n\t\t\t(nnzero pairs X 3 pair tensor) (mol , I , J)\n\t\t\t(nnzero X 4 triples tensor) (mol , I , J , K)\n\t\t""""""\n\t\tif (self.alg==2):\n\t\t\ttrp = self.PairMaker(self.x,rcut_pairs,self.nnz)\n\t\t\ttrtmp = self.PairMaker(self.x,rcut_triples,self.nnz)\n\t\t\thack=[[[] for j in range(self.maxnatom)] for i in range(self.nmol)]\n\t\t\ttore = []\n\t\t\tfor p in trtmp:\n\t\t\t\t(hack[p[0]])[p[1]].append(p[2])\n\t\t\tfor i in range(self.nmol):\n\t\t\t\tfor j in range(self.maxnatom):\n\t\t\t\t\tfor k in hack[i][j]:\n\t\t\t\t\t\ttore.append([i,j,k])\n\t\t\treturn trp, np.array(tore)\n\t\telse:\n\t\t\tfor i,mol in enumerate(self.nlist):\n\t\t\t\tmol.Update(self.x[i,:self.nnz[i]], rcut_pairs, rcut_triples, i, nreal_=self.nreal[i])\n\t\t\tnzp = sum([mol.npairs for mol in self.nlist])\n\t\t\tnzt = sum([mol.ntriples for mol in self.nlist])\n\t\t\ttrp = np.zeros((nzp,3),dtype=np.uint64)\n\t\t\ttrt = np.zeros((nzt,4),dtype=np.uint64)\n\t\t\tpp = 0\n\t\t\ttp = 0\n\t\t\tfor mol in self.nlist:\n\t\t\t\ttrp[pp:pp+mol.npairs] = mol.pairs\n\t\t\t\ttrt[tp:tp+mol.ntriples] = mol.triples\n\t\t\t\tpp += mol.npairs\n\t\t\t\ttp += mol.ntriples\n\t\t\treturn trp, trt\n\n\t@TMTiming(""buildPairsWithBothEleIndex"")\n\tdef buildPairsWithBothEleIndex(self, rcut=5.0, ele=None, sort_=False):\n\t\ttrp  = self.buildPairs(rcut)\n\t\tZ1 = self.ele[trp[:, 0], trp[:, 1]]\n\t\tZ2 = self.ele[trp[:, 0], trp[:, 2]]\n\t\tpair_mask1 = np.equal(Z1.reshape(trp.shape[0],1,1), ele.reshape(ele.shape[0],1))\n\t\tpair_mask2 = np.equal(Z2.reshape(trp.shape[0],1,1), ele.reshape(ele.shape[0],1))\n\t\tpair_index1 = np.where(np.all(pair_mask1, axis=-1))[1]\n\t\tpair_index2 = np.where(np.all(pair_mask2, axis=-1))[1]\n\t\ttrpE1E2 = np.concatenate((trp, pair_index1.reshape((-1,1)), pair_index2.reshape((-1,1))), axis=-1)\n\t\tif sort_:\n\t\t\t#print (""before sorted:"", trpE1E2)\n\t\t\tindex1 = np.argsort(trpE1E2[:,3:])\n\t\t\tindex2 = np.zeros((trpE1E2.shape[0],2), dtype=int)\n\t\t\tindex2[:,:] = np.arange(trpE1E2.shape[0]).reshape((trpE1E2.shape[0],1))\n\t\t\ttrpE1E2[:,1:3] = trpE1E2[:,1:3][index2, index1]\n\t\t\ttrpE1E2[:,3:] = trpE1E2[:,3:][index2, index1]\n\t\t#print (""trpE1E2:"", trpE1E2.shape)\n\t\t\t#print (""after sorted:"", trpE1E2)\n\t\treturn trpE1E2\n\n\tdef buildPairsAndTriplesWithEleIndex(self, rcut_pairs=5.0, rcut_triples=5.0, ele=None, elep=None):\n\t\t""""""\n\t\tgenerate sorted pairs and triples with index of correspoding ele or elepair append to it.\n\t\tsorted order: mol, i (center atom), l (ele or elepair index), j (connected atom 1), k (connected atom 2 for triples)\n\n\t\tArgs:\n\t\t\trcut_: a cutoff parameter.\n\t\t\tele: element\n\t\t\telep: element pairs\n\t\tReturns:\n\t\t\t(nnzero pairs X 4 pair tensor) (mol, I, J, L)\n\t\t\t(nnzero triples X 5 triple tensor) (mol, I, J, K, L)\n\t\t""""""\n\n\t\tif not self.sort:\n\t\t\tprint (""Warning! Triples need to be sorted"")\n\t\t# if self.ele == None:\n\t\t# \traise Exception(""Element type of each atom is needed."")\n\t\t#import time\n\t\tt0 = time.time()\n\t\ttrp, trt = self.buildPairsAndTriples(rcut_pairs, rcut_triples)\n\t\t#print (""trp, trt"",trp.shape, trt.shape)\n\t\t#print (""build P and T time:"", time.time()-t0)\n\t\t#print (""trp:"", trp, ""trt:"", trt)\n\t\tt_start = time.time()\n\t\teleps = np.hstack((elep, np.flip(elep, axis=1))).reshape((elep.shape[0], 2, -1))\n\t\tZ = self.ele[trp[:, 0], trp[:, 2]]\n\t\tpair_mask = np.equal(Z.reshape(trp.shape[0],1,1), ele.reshape(ele.shape[0],1))\n\t\tpair_index = np.where(np.all(pair_mask, axis=-1))[1]\n\t\tZ1 = self.ele[trt[:, 0], trt[:, 2]]\n\t\tZ2 = self.ele[trt[:, 0], trt[:, 3]]\n\t\tZ1Z2 = np.transpose(np.vstack((Z1, Z2)))\n\t\ttrip_mask = np.equal(Z1Z2.reshape((trt.shape[0],1,1,2)), eleps.reshape((eleps.shape[0],2,2)))\n\t\ttrip_index = np.where(np.any(np.all(trip_mask, axis=-1),axis=-1))[1]\n\t\ttrpE = np.concatenate((trp, pair_index.reshape((-1,1))), axis=-1)\n\t\ttrtE = np.concatenate((trt, trip_index.reshape((-1,1))), axis=-1)\n\t\tsort_index = np.lexsort((trpE[:,2], trpE[:,3], trpE[:,1], trpE[:,0]))\n\t\ttrpE_sorted = trpE[sort_index]\n\t\tsort_index = np.lexsort((trtE[:,2], trtE[:,3], trtE[:,4], trtE[:,1], trtE[:,0]))\n\t\ttrtE_sorted = trtE[sort_index]\n\t\t#print (""numpy lexsorting time:"", time.time() -t0)\n\t\t#print (""time to append and sort element"", time.time() - t_start)\n\t\tvalance_pair = np.zeros(trt.shape[0])\n\t\tpointer = 0\n\t\tif (len(trtE_sorted)==0):\n\t\t\tmil_jk = np.zeros((trt.shape[0],4))\n\t\t\tjk_max = 0\n\t\t\treturn trpE_sorted, trtE_sorted, mil_jk, jk_max\n\t\tprev_l = trtE_sorted[0][4]\n\t\tprev_atom = trtE_sorted[0][1]\n\t\tprev_mol = trtE_sorted[0][0]\n\t\tfor i in range(0, trt.shape[0]):\n\t\t\tcurrent_l = trtE_sorted[i][4]\n\t\t\tcurrent_atom = trtE_sorted[i][1]\n\t\t\tcurrent_mol = trtE_sorted[i][0]\n\t\t\t#print (""i:       "", i)\n\t\t\t#print (""prev:    "", prev_l)\n\t\t\t#print (""current: "", current_l)\n\t\t\tif current_l == prev_l and current_atom == prev_atom and current_mol == prev_mol:\n\t\t\t\tpointer += 1\n\t\t\t\tif i == trt.shape[0]-1:\n\t\t\t\t\tvalance_pair[i-pointer+1:]=range(0, pointer)\n\t\t\t\telse:\n\t\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tvalance_pair[i-pointer:i]=range(0, pointer)\n\t\t\t\tpointer = 1\n\t\t\t\tprev_l = current_l\n\t\t\t\tprev_atom = current_atom\n\t\t\t\tprev_mol = current_mol\n\t\t#print (""valance_pair:"", valance_pair[:20])\n\t\t#print (""trtE:"", trtE_sorted[:20])\n\t\tmil_jk = np.zeros((trt.shape[0],4))\n\t\tmil_jk[:,[0,1,2]]= trtE_sorted[:,[0,1,4]]\n\t\tmil_jk[:,3] = valance_pair\n\t\t#print (""mil_jk"", mil_jk)\n\t\tjk_max = np.max(valance_pair)\n\t\t#print (""after processing time:"", time.time() - t_start)\n\t\t#print (trpE_sorted, trtE_sorted, jk_max)\n\t\treturn trpE_sorted, trtE_sorted, mil_jk, jk_max\n\n\t@TMTiming(""buildPairsAndTriplesWithEleIndexPeriodic"")\n\tdef buildPairsAndTriplesWithEleIndexPeriodic(self, rcut_pairs=5.0, rcut_triples=5.0, ele=None, elep=None):\n\t\t""""""\n\t\tgenerate sorted pairs and triples with index of correspoding ele or elepair append to it.\n\t\tsorted order: mol, i (center atom), l (ele or elepair index), j (connected atom 1), k (connected atom 2 for triples)\n\n\t\tArgs:\n\t\t\trcut_: a cutoff parameter.\n\t\t\tele: element\n\t\t\telep: element pairs\n\t\tReturns:\n\t\t\t(nnzero pairs X 4 pair tensor) (mol, I, J, L)\n\t\t\t(nnzero triples X 5 triple tensor) (mol, I, J, K, L)\n\t\t""""""\n\t\ttrpE_sorted, trtE_sorted, mil_jk, jk_max = self.buildPairsAndTriplesWithEleIndex(rcut_pairs, rcut_triples, ele, elep)\n\t\tmil_j = np.zeros((trpE_sorted.shape[0], 4))\n\t\tpair_pair = np.zeros(trpE_sorted.shape[0])\n\t\tif (len(trpE_sorted)==0):\n\t\t\treturn trpE_sorted, trtE_sorted, mil_j, mil_jk\n\t\tprev_l = trpE_sorted[0][3]\n\t\tprev_atom = trpE_sorted[0][1]\n\t\tprev_mol = trpE_sorted[0][0]\n\t\tpointer = 0\n\t\tfor i in range(0, trpE_sorted.shape[0]):\n\t\t\tcurrent_l = trpE_sorted[i][3]\n\t\t\tcurrent_atom = trpE_sorted[i][1]\n\t\t\tcurrent_mol = trpE_sorted[i][0]\n\t\t\tif current_l == prev_l and current_atom == prev_atom and current_mol == prev_mol:\n\t\t\t\tpointer += 1\n\t\t\t\tif i == trpE_sorted.shape[0]-1:\n\t\t\t\t\tpair_pair[i-pointer+1:]=range(0, pointer)\n\t\t\t\telse:\n\t\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tpair_pair[i-pointer:i]=range(0, pointer)\n\t\t\t\tpointer = 1\n\t\t\t\tprev_l = current_l\n\t\t\t\tprev_atom = current_atom\n\t\t\t\tprev_mol = current_mol\n\t\tmil_j[:,[0,1,2]] = trpE_sorted[:,[0,1,3]]\n\t\tmil_j[:,3] = pair_pair\n\t\t#print (""trpE_sorted, trtE_sorted"",trpE_sorted.shape, trtE_sorted.shape)\n\t\treturn trpE_sorted, trtE_sorted, mil_j, mil_jk\n\n\tdef buildPairsAndTriplesWithEleIndexLinear(self, rcut_pairs=5.0, rcut_triples=5.0, ele=None, elep=None):\n\t\treturn self.buildPairsAndTriplesWithEleIndexPeriodic(rcut_pairs, rcut_triples, ele, elep)\n\n\nclass NeighborListSetWithImages(NeighborListSet):\n\tdef __init__(self, x_, nnz_, nreal_,  DoTriples_=False, DoPerms_=False, ele_=None, alg_ = None, sort_ = False):\n\t\t""""""\n\t\tA neighborlist for a set which avoids calculating the energy of ""image"" atoms,\n\t\twhich are generated only for the periodic components of a force, and\n\t\tdo not contribute to energy or force.\n\n\t\tArgs:\n\t\t\tx_: NMol X MaxNAtom X 3 tensor of coordinates.\n\t\t\tnnz_: NMol vector of maximum atoms in each mol.\n\t\t\tnreal_: NMol vector of real atoms in each mol.\n\t\t\tele_: element type of each atom.\n\t\t\tsort_: whether sort jk in triples by atom index\n\t\t""""""\n\t\tNeighborListSet.__init__(self, x_,nnz_,DoTriples_,DoPerms_,ele_,alg_ ,sort_)\n\t\tself.nreal = nreal_\n\t\treturn\n\nclass CellList:\n\t""""""\n\tTODO: CellList updates.\n\t- Determine optimal number of Voxels.\n\t""""""\n\tdef __init__(self, x_, cutoff_ = 5.0, ele_ = None, padding_ = 1.0):\n\t\t""""""\n\t\tBuilds or updates a cubic cell list. Each cell size  = 2*cutoff_.\n\t\tArgs:\n\t\t\tx_: coordinate array\n\t\t\tcutoff_: interaction cutoff\n\t\t\tele_: element types of each atoms.\n\t\t\tpadding_: padding of the molecule box.\n\t\t""""""\n\t\tself.natom = x_.shape[0] # includes periodic images.\n\t\tself.x = x_.copy()\n\t\tself.ele = ele_\n\t\tself.cutoff = cutoff_\n\t\tself.Rcore = self.cutoff   # in current implementation, Rcore has to equal Rskin\n\t\tself.Rskin = self.cutoff\n\t\tself.Rcell = self.Rcore + 2*self.Rskin\n\t\tself.padding = padding_\n\t\tfrom itertools import product\n\t\tself.offset = np.asarray(list(product([-1, 0, 1], repeat=3)),dtype=int)\n\t\treturn\n\n\tdef Update(self, x_):\n\t\tself.x = x_.copy()\n\t\tcore_begin_end = np.array([[np.min(self.x[:,0])-self.padding, np.max(self.x[:,0])+self.padding],\\\n\t\t\t[np.min(self.x[:,1])-self.padding, np.max(self.x[:,1])+self.padding],\\\n\t\t\t[np.min(self.x[:,2])-self.padding, np.max(self.x[:,2])+self.padding]])\n\t\t#print (""core_begin_end:"", core_begin_end)\n\t\tcore_size = core_begin_end[:,1] - core_begin_end[:,0]\n\t\tn_core = np.array([core_size[0]/self.Rcore, core_size[1]/self.Rcore, core_size[2]/self.Rcore], dtype=int) + 1\n\t\tn_cell = n_core.copy()\n\t\tcell_begin_end = core_begin_end + np.array([-self.Rskin, self.Rskin])\n\t\t#print (""cell_begin_end:"", cell_begin_end)\n\t\t#print (""n_core:"", n_core)\n\t\tcore_index = [[] for i in range(0, np.prod(n_core))]\n\t\tcell_index = [[] for i in range(0, np.prod(n_cell))]\n\t\tfor i in range (0, self.natom):\n\t\t\tatom_core_index = ((self.x[i]  - core_begin_end[:,0])/self.Rcore).astype(int)\n\t\t\tcore_index[atom_core_index[0]*n_core[1]*n_core[2]+atom_core_index[1]*n_core[2]+atom_core_index[2]].append(i)\n\t\t\ttmp  =  atom_core_index + self.offset\n\t\t\tatom_cell_index = tmp[(tmp[:,0] >= 0) & (tmp[:,0] <  n_cell[0]) & (tmp[:,1] >= 0) & (tmp[:,1] <  n_cell[1]) &  (tmp[:,2] >= 0) & (tmp[:,2] <  n_cell[2])]\n\t\t\tfor j in list(atom_cell_index[:,0]*n_cell[1]*n_cell[2]+atom_cell_index[:,1]*n_cell[2]+atom_cell_index[:,2]):\n\t\t\t\tcell_index[j].append(i)\n\t\t#print (""core_index:"", core_index)\n\t\t#print (""cell_index:"", cell_index)\n\t\treturn core_index, cell_index\n'"
TensorMol/ForceModifiers/NeighborsMB.py,0,"b'""""""\nVery similar to neighbors, but does a many body expansion\nRequires a list of atom fragments, prepared by the user or MSet::ToFragments\nTODO: extend to triples.\n""""""\n\nfrom __future__ import absolute_import\nimport numpy as np\n\nclass MBNeighbors:\n\t""""""\n\tThe purpose of this class is to provide:\n\tself.sings, singz : the monomer part of the set batch, and it\'s atomic numbers.\n\tself.pairs, self.trips (likewise)\n\tself.singC : the coefficient of each in the many-body expansion\n\t""""""\n\tdef __init__(self,x_,z_,frags_):\n\t\t""""""\n\t\tInitialize a Many-Body Neighbor list which\n\t\tcan generate in linear time.\n\t\tterms in a many body expansion up to three.\n\n\t\tArgs:\n\t\t\tx_ : coordinates of all the atoms.\n\t\t\tz_ : atomic numbers of all the atoms.\n\t\t\tfrags_: list of lists containing these atoms.\n\t\t""""""\n\t\t# Coordinates to pass to networks. (unique)\n\t\tself.sings = None\n\t\tself.pairs = None\n\t\tself.trips = None\n\t\t# Atomic numbers to pass to networks. (unique)\n\t\tself.singz = None\n\t\tself.pairz = None\n\t\tself.tripz = None\n\t\t# indices in terms of monomers.\n\t\tself.pairi = None\n\t\tself.tripi = None\n\t\t# Coefficients of the expansion.\n\t\tself.singC = None\n\t\tself.pairC = None\n\t\tself.tripC = None\n\t\tself.x = x_.copy()\n\t\tself.z = z_.copy()\n\t\tself.frags = frags_\n\t\tself.nt = self.x.shape[0]\n\t\tself.nf = len(frags_)\n\t\tself.ntrip = 0\n\t\tself.npair = 0\n\t\tself.maxnatom = 3*max(map(len,frags_))\n\t\tself.sings = np.zeros((self.nf,self.maxnatom,3))\n\t\tself.singz = np.zeros((self.nf,self.maxnatom), dtype=np.uint8)\n\t\tfor i in range(self.nf):\n\t\t\tself.sings[i,:len(self.frags[i]),:] = self.x[self.frags[i]].copy()\n\t\t\tself.singz[i,:len(self.frags[i])] = self.z[self.frags[i]].copy()\n\n\tdef Update(self,x_, R2=10.0, R3=5.0):\n\t\t""""""\n\t\tUpdate the lists of bodies and their coefficients.\n\n\t\tArgs:\n\t\t\tx: A new position vector\n\t\t\tR2: pair cutoff\n\t\t\tR3: triples cutoff\n\t\t""""""\n\t\tif (R2<R3):\n\t\t\traise Exception(""R3<R2 Assumption Violated."")\n\t\tself.x = x_.copy()\n\t\tfor i in range(self.nf):\n\t\t\tself.sings[i,:len(self.frags[i]),:] = self.x[self.frags[i]].copy()\n\t\t\tself.singz[i,:len(self.frags[i])] = self.z[self.frags[i]].copy()\n\t\tcenters = np.zeros((self.nf,3))\n\t\tfor i in range(self.nf):\n\t\t\tcenters[i] = np.average(self.x[self.frags[i]],axis=0)\n\t\tTwoBodyPairs=None\n\t\tThreeBodyPairs=None\n\t\t#print ""Number of Frags"",self.nf\n\t\tif (self.nf < 500):\n\t\t\tTwoBodyPairs = Make_NListNaive(centers,R2,self.nf,int(False))\n\t\t\tThreeBodyPairs = Make_NListNaive(centers,R3,self.nf,int(False))\n\t\telse:\n\t\t\tTwoBodyPairs = Make_NListLinear(centers,R2,self.nf,int(False))\n\t\t\tThreeBodyPairs = Make_NListLinear(centers,R3,self.nf,int(False))\n\t\tself.pairi = set()\n\t\tself.tripi = set()\n\t\t#print TwoBodyPairs\n\t\t#print ThreeBodyPairs\n\t\tfor i in range(self.nf):\n\t\t\tnnt = len(ThreeBodyPairs[i])\n\t\t\tnnp = len(TwoBodyPairs[i])\n\t\t\tif (nnp>0):\n\t\t\t\tfor j in range(nnp):\n\t\t\t\t\tif (j != None):\n\t\t\t\t\t\tself.pairi.add(tuple(sorted([i,TwoBodyPairs[i][j]])))\n\t\t\tif (nnt>=2):\n\t\t\t\tfor j in range(nnt):\n\t\t\t\t\tif (j != None):\n\t\t\t\t\t\tfor k in range(j+1,nnt):\n\t\t\t\t\t\t\tif (k != None):\n\t\t\t\t\t\t\t\tif ThreeBodyPairs[i][k] in ThreeBodyPairs[ThreeBodyPairs[i][j]]:\n\t\t\t\t\t\t\t\t\tself.tripi.add(tuple(sorted([i,ThreeBodyPairs[i][j],ThreeBodyPairs[i][k]])))\n\t\tDistMatrix = Make_DistMat(self.x)\n\t\tself.ntrip = len(self.tripi)\n\t\tself.npair = len(self.pairi)\n\t\t#print ""num pairs"", self.npair\n\t\t#print ""num trips"", self.ntrip\n\t\tself.pairi = map(list,self.pairi)\n\t\tself.tripi = map(list,self.tripi)\n\t\t#print ""Pairs"", self.pairi\n\t\t#print ""Trips"", self.tripi\n\t\t# Now generate the coeffs of each order and sum them.\n\t\tself.singC = np.ones(self.nf)\n\t\tself.pairC = np.ones(self.npair)\n\t\tself.tripC = np.ones(self.ntrip)\n\t\tself.singI = self.frags\n\t\tself.pairI = []\n\t\tself.tripI = []\n\t\tself.pairs = np.zeros((self.npair,self.maxnatom,3))\n\t\tself.trips = np.zeros((self.ntrip,self.maxnatom,3))\n\t\tself.pairz = np.zeros((self.npair,self.maxnatom), dtype=np.uint8)\n\t\tself.tripz = np.zeros((self.ntrip,self.maxnatom), dtype=np.uint8)\n\t\tfor trip_index, trip in enumerate(self.tripi):\n\t\t\ti,j,k = trip[0],trip[1],trip[2]\n\t\t\tself.tripI.append([self.frags[i]+self.frags[j]+self.frags[k]])\n\t\t\tni,nj,nk = len(self.frags[i]),len(self.frags[j]),len(self.frags[k])\n\t\t\tself.trips[trip_index,:ni,:] = self.x[self.frags[i]].copy()\n\t\t\tself.trips[trip_index,ni:(ni+nj),:] = self.x[self.frags[j]].copy()\n\t\t\tself.trips[trip_index,(ni+nj):(ni+nj+nk),:] = self.x[self.frags[k]].copy()\n\t\t\tself.tripz[trip_index,:ni] = self.z[self.frags[i]].copy()\n\t\t\tself.tripz[trip_index,ni:(ni+nj)] = self.z[self.frags[j]].copy()\n\t\t\tself.tripz[trip_index,(ni+nj):(ni+nj+nk)] = self.z[self.frags[k]].copy()\n\t\t\tself.pairC[self.pairi.index(sorted([i,j]))] -= 1\n\t\t\tself.pairC[self.pairi.index(sorted([j,k]))] -= 1\n\t\t\tself.pairC[self.pairi.index(sorted([k,i]))] -= 1\n\t\t\tself.singC[i] += 1\n\t\t\tself.singC[j] += 1\n\t\t\tself.singC[k] += 1\n\t\tfor pair_index, pair in enumerate(self.pairi):\n\t\t\ti,j = pair[0],pair[1]\n\t\t\tself.pairI.append([self.frags[i]+self.frags[j]])\n\t\t\tni,nj = len(self.frags[i]),len(self.frags[j])\n\t\t\tself.pairs[pair_index,:ni,:] = self.x[self.frags[i]].copy()\n\t\t\tself.pairs[pair_index,ni:(ni+nj),:] = self.x[self.frags[j]].copy()\n\t\t\tself.pairz[pair_index,:ni] = self.z[self.frags[i]].copy()\n\t\t\tself.pairz[pair_index,ni:(ni+nj)] = self.z[self.frags[j]].copy()\n\t\t\tself.singC[i] -= 1\n\t\t\tself.singC[j] -= 1\n\t\treturn\n\n\n\nclass MBNeighborsSet:\n\t""""""\n\tThe purpose of this class is to provide:\n\tself.sings_atom_index, self.pairs_atom_index, self.trips_atom_index: keep record of which atoms the frag contains in the format of: num_of_frags X max_num_atoms_of_frag X 2: (mol_index, atom_index_in_mol) example: [[[0, 0], [0, 1], [0, 2]],[[0, 3], [0, 4], [0, 5]]]\n\tself.sings_mol_index, self.pairs_mol_index, self.trips_mol_index: keep record of which mol the frag belongs to in the format of:  num_of_frags: [0, 0]:\n\t""""""\n\tdef __init__(self, x_, nnz_, frags_):\n\t\t""""""\n\t\tInitialize a Many-Body Neighbor list which\n\t\tcan generate in linear time.\n\t\tterms in a many body expansion up to three.\n\n\t\tArgs:\n\t\t\tx_ : coordinates of all the atoms.\n\t\t\tz_ : atomic numbers of all the atoms.\n\t\t\tfrags_: list of lists containing these atoms.\n\t\t""""""\n\n\t\tself.nlist = []\n\t\tself.nmol = x_.shape[0]\n\t\tself.maxnatom = x.shape[1]\n\t\tself.x = x_\n\t\tself.maxnatom = 3*max(len(frag) for frag in frags_)\n\t\tself.frags = frags_\n\t\tself.nnz = nnz_\n\t\tself.UpdateInterval = 1\n\t\tself.UpdateCounter = 0\n\t\tself.sings = []\n\t\tfor i in range (0, self.nmol):\n\t\t\tfor j in range(0, len(self.frags[i])):\n\t\t\t\tself.sings.append(self.x[i, self.frags[i][j]].copy())\n\t\treturn\n\n\tdef Update(self, x_, R2=10.0, R3=5.0):\n\t\t""""""\n\t\tUpdate...\n\n\t\tArgs:\n\t\t\tx: A new position vector\n\t\t\tR2: pair cutoff\n\t\t\tR3: triples cutoff\n\t\t""""""\n\t\tif (R2<R3):\n\t\t\traise Exception(""R3<R2 Assumption Violated."")\n'"
TensorMol/ForceModifiers/Periodic.py,0,"b'""""""\nNo symmetry but general unit cells supported.\nMaintenance of the unit cell, etc. are handled by PeriodicForce.\nOnly linear scaling forces with energy are supported.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..ForceModifiers.Neighbors import *\nfrom ..ForceModels.Electrostatics import *\nfrom ..Simulations.SimpleMD import *\n\nclass Lattice:\n\tdef __init__(self, latvec_):\n\t\t""""""\n\t\tBuild a periodic lattice\n\n\t\tArgs:\n\t\t\tlatvec_: A 3x3 tensor of lattice vectors.\n\t\t""""""\n\t\tself.lattice = latvec_.copy()\n\t\tself.latticeCenter = (self.lattice[0]+self.lattice[1]+self.lattice[2])/2.0\n\t\tjd = np.concatenate([self.latticeCenter[np.newaxis,:],self.LatticeFacePoints()])\n\t\tDistsToCenter = MolEmb.Make_DistMat_ForReal(jd,1)[0,1:]\n\t\tprint(DistsToCenter)\n\t\tself.latticeMinDiameter = 2.0*np.min(DistsToCenter)\n\t\tself.lp = np.array([[0.,0.,0.],self.lattice[0].tolist(),self.lattice[1].tolist(),self.lattice[2].tolist(),(self.lattice[0]+self.lattice[1]).tolist(),(self.lattice[0]+self.lattice[2]).tolist(),(self.lattice[1]+self.lattice[2]).tolist(),(self.lattice[0]+self.lattice[1]+self.lattice[2]).tolist()])\n\t\tself.ntess = 1 # number of shells over which to tesselate.\n\t\tself.facenormals = self.LatticeNormals()\n\t\treturn\n\tdef LatticeFacePoints(self):\n\t\t""""""\n\t\tReturns vertices, Face centers, and axis centers.\n\t\t""""""\n\t\tlfp = np.zeros((14,3))\n\t\tlfp[0] = self.lattice[0]\n\t\tlfp[1] = self.lattice[1]\n\t\tlfp[2] = self.lattice[2]\n\t\tlfp[3] = self.lattice[0]+self.lattice[1]\n\t\tlfp[4] = self.lattice[0]+self.lattice[2]\n\t\tlfp[5] = self.lattice[1]+self.lattice[2]\n\t\tlfp[6] = self.lattice[0]+self.lattice[1]+self.lattice[2]\n\t\tlfp[7] = np.zeros(3)\n\t\tlfp[8] = 0.5*(self.lattice[0]+self.lattice[1])\n\t\tlfp[9] = 0.5*(self.lattice[2]+self.lattice[1])\n\t\tlfp[10] = 0.5*(self.lattice[0]+self.lattice[2])\n\t\tlfp[11] = 0.5*(self.lattice[0]+self.lattice[1])+self.lattice[2]\n\t\tlfp[12] = 0.5*(self.lattice[2]+self.lattice[1])+self.lattice[0]\n\t\tlfp[13] = 0.5*(self.lattice[0]+self.lattice[2])+self.lattice[1]\n\t\treturn lfp\n\tdef LatticeNormals(self):\n\t\tlp = self.lp\n\t\tfn = np.zeros((6,3))\n\t\tfn[0] = np.cross(lp[1]-lp[0],lp[2]-lp[0]) # face 012\n\t\tfn[1] = np.cross(lp[1]-lp[0],lp[3]-lp[0]) # face 013\n\t\tfn[2] = np.cross(lp[2]-lp[0],lp[3]-lp[0]) # face 023\n\t\tfn[3] = np.cross(lp[4]-lp[-1],lp[5]-lp[-1])\n\t\tfn[4] = np.cross(lp[6]-lp[-1],lp[5]-lp[-1])\n\t\tfn[5] = np.cross(lp[6]-lp[-1],lp[4]-lp[-1])\n\t\t# Normalize them.\n\t\tfn /= np.sqrt(np.sum(fn*fn,axis=1))[:,np.newaxis]\n\t\treturn fn\n\tdef InRangeOfLatNormals(self,pt,rng_):\n\t\tfor i in range(6):\n\t\t\tif (i<3):\n\t\t\t\tif (np.abs(np.sum(self.facenormals[i]*(pt - self.lp[0]))) < rng_):\n\t\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tif (np.abs(np.sum(self.facenormals[i]*(pt - self.lp[7]))) < rng_):\n\t\t\t\t\treturn True\n\tdef CenteredInLattice(self, mol):\n\t\tm=Mol(mol.atoms,self.ModuloLattice(mol.coords - mol.Center() + self.latticeCenter))\n\t\tm.properties[""Lattice""] = self.lattice.copy()\n\t\treturn m\n\tdef InLat(self,crds):\n\t\t""""""\n\t\tExpress coordinates (atom X 3 cart)\n\t\tIn units of the lattice vectors.\n\t\t""""""\n\t\tlatmet = MatrixPower(np.dot(self.lattice, self.lattice.T),-1)\n\t\treturn \tnp.dot(crds,np.dot(self.lattice.T,latmet))\n\tdef FromLat(self,crds):\n\t\t""""""\n\t\tExpress coordinates (atom X 3 lat)\n\t\tIn cartesian units.\n\t\t""""""\n\t\treturn np.dot(crds,self.lattice)\n\tdef ModuloLattice(self, crds):\n\t\t""""""\n\t\tTransports all coordinates into the primitive cell.\n\n\t\tArgs:\n\t\t\tcrds: a natom X 3 ndarray of atomic coordinates.\n\t\tReturns:\n\t\t\tcrds modulo the primitive lattice.\n\t\t""""""\n\t\ttmp = self.InLat(crds)\n\t\tfpart = np.fmod(tmp,1.0)\n\t\trevs=np.where(fpart < 0.0)\n\t\tfpart[revs] = 1.0 + fpart[revs]\n\t\treturn self.FromLat(fpart)\n\tdef TessNTimes(self, atoms_, coords_, ntess_):\n\t\t""""""\n\t\tEnlarges a molecule to allow for accurate calculation of a short-ranged force\n\n\t\tArgs:\n\t\t\tmol_: a molecule.\n\t\t\trng_: minimum distance from center covered by the tesselation. (Angstrom)\n\n\t\tReturns:\n\t\t\tAn enlarged molecule where the real coordinates preceed \'fake\' periodic images.\n\t\t""""""\n\t\tntess = ntess_\n\t\tnatom = atoms_.shape[0]\n\t\tnimages = pow(ntess_,3)\n\t\t#print(""Doing"",nimages,""images... of "",natom)\n\t\tnewAtoms = np.zeros(nimages*natom,dtype=np.uint8)\n\t\tnewCoords = np.zeros((nimages*natom,3))\n\t\tnewAtoms[:natom] = atoms_\n\t\tnewCoords[:natom,:3] = coords_\n\t\tind = 1\n\t\tfor i in range(ntess):\n\t\t\tfor j in range(ntess):\n\t\t\t\tfor k in range(ntess):\n\t\t\t\t\tif (i==0 and j==0 and k ==0):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tnewAtoms[ind*natom:(ind+1)*natom] = atoms_\n\t\t\t\t\tnewCoords[ind*natom:(ind+1)*natom,:] = coords_ + i*self.lattice[0] + j*self.lattice[1] + k*self.lattice[2]\n\t\t\t\t\t#print(i,j,k,ind,nimages)\n\t\t\t\t\tind = ind + 1\n\t\treturn newAtoms, newCoords\n\tdef TessLattice(self, atoms_, coords_, rng_):\n\t\t""""""\n\t\tEnlarges a molecule to allow for accurate calculation of a short-ranged force\n\n\t\tArgs:\n\t\t\tatoms_: an integer array of atoms.\n\t\t\tcoords_: a double array of coordinates.\n\t\t\trng_: minimum distance from center covered by the tesselation. (Angstrom)\n\n\t\tReturns:\n\t\t\tAn enlarged molecule where the real coordinates preceed \'fake\' periodic images.\n\t\t""""""\n\t\tif (rng_ > self.latticeMinDiameter):\n\t\t\tself.ntess = int(rng_/self.latticeMinDiameter)+1\n\t\t\t#print(""Doing"",self.ntess,""tesselations..."", rng_,self.latticeMinDiameter)\n\t\telse:\n\t\t\tself.ntess = 1\n\t\t\t#print(""Doing"",self.ntess,""tesselations..."", rng_,self.latticeMinDiameter,atoms_.shape)\n\t\t\t#print(rng_,self.latticeMinDiameter)\n\t\tnatom = atoms_.shape[0]\n\t\tnimages = pow(2*self.ntess+1,3)\n\t\t#print(""Doing"",nimages,""images... of "",natom)\n\t\tnewAtoms = np.zeros(nimages*natom,dtype=np.uint8)\n\t\tnewCoords = np.zeros((nimages*natom,3))\n\t\tnewAtoms[:natom] = atoms_\n\t\tnewCoords[:natom,:3] = coords_\n\t\tind = 1\n\t\tfor i in range(-self.ntess,self.ntess+1):\n\t\t\tfor j in range(-self.ntess,self.ntess+1):\n\t\t\t\tfor k in range(-self.ntess,self.ntess+1):\n\t\t\t\t\tif (i==0 and j==0 and k ==0):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tnewAtoms[ind*natom:(ind+1)*natom] = atoms_\n\t\t\t\t\tnewCoords[ind*natom:(ind+1)*natom,:] = coords_ + i*self.lattice[0] + j*self.lattice[1] + k*self.lattice[2]\n\t\t\t\t\tind = ind + 1\n\t\t# Now pare that down where the images are too far from the edges of the lattice.\n\t\t#print(""Natom:"", newAtoms.shape,newCoords.shape)\n\t\treturn newAtoms, newCoords\n\t\tAtoms = np.zeros(nimages*natom,dtype=np.uint8)\n\t\tCoords = np.zeros((nimages*natom,3))\n\t\tind=natom\n\t\tCoords[:natom] = newCoords[:natom].copy()\n\t\tAtoms[:natom] = newAtoms[:natom].copy()\n\t\tfor j in range(natom,natom*nimages):\n\t\t\tif(self.InRangeOfLatNormals(newCoords[j],rng_)):\n\t\t\t\tCoords[ind] = newCoords[j]\n\t\t\t\tAtoms[ind] = newAtoms[j]\n\t\t\t\tind = ind + 1\n\t\t#print(""tes sparsity"",float(ind)/(natom*nimages))\n\t\treturn Atoms[:ind], Coords[:ind]\n\nclass LocalForce:\n\tdef __init__(self, f_, rng_=5.0, NeedsTriples_=False):\n\t\tself.range = rng_\n\t\tself.func=f_\n\t\tself.NeedsTriples = NeedsTriples_\n\t\treturn\n\tdef __call__(self, z, x, NZ, DoForce = True):\n\t\t""""""\n\t\tGeneric call to a linear scaling local force.\n\n\t\tArgs:\n\t\t\tz: atomic number vector\n\t\t\tx: atoms X 3 coordinate vector.\n\t\t\tNZ: pair or triples matrix. (NZP X 2)\n\t\treturns:\n\t\t\tenergy number, and force vector with same shape as x.\n\t\t""""""\n\t\ttmp = self.func(z, x, NZ, DoForce)\n\t\treturn tmp\n\nclass PeriodicForceWithNeighborList:\n\tdef __init__(self, pm_, lat_):\n\t\t""""""\n\t\tA periodic force evaluator. The force consists of two components\n\t\tShort-Ranged forces, and long-ranged forces. Short ranged forces are\n\t\tevaluated by tesselation. Long-range forces are not supported yet.\n\t\tThis version manages and passes Neighbor lists.\n\n\n\t\tArgs:\n\t\t\tpm_: a molecule.\n\t\t\tlat_: lattice vectors.\n\t\t""""""\n\t\tself.lattice = Lattice(lat_)\n\t\tself.NL = None\n\t\tself.mol0 = self.lattice.CenteredInLattice(pm_)\n\t\tself.atoms = self.mol0.atoms.copy()\n\t\tself.natoms = self.mol0.NAtoms()\n\t\tself.natomsReal = pm_.NAtoms()\n\t\tself.maxrng = 0.0\n\t\tself.LocalForces = []\n\t\tself.lastx = np.zeros(pm_.coords.shape)\n\t\tself.nlthresh = 0.05 #per-atom Threshold for NL rebuild. (A)\n\t\t#self.LongForces = [] Everything is real-space courtesy of DSF.\n\t\treturn\n\tdef AdjustLattice(self, x_, lat_):\n\t\t""""""\n\t\tAdjusts the lattice and rescales the coordinates of m relative to previous lattice.\n\t\t""""""\n\t\til = self.lattice.InLat(m.coords)\n\t\tself.lattice = Lattice(lat_)\n\t\tm.coords = self.lattice.FromLat(il)\n\t\treturn m\n\tdef BindForce(self, lf_, rng_):\n\t\t""""""\n\t\tAdds a local force to be computed when the PeriodicForce is called.\n\n\t\tArgs:\n\t\t\tlf_: a function which takes z,x and returns atom energies, atom forces.\n\t\t""""""\n\t\tself.LocalForces.append(LocalForce(lf_,rng_))\n\tdef __call__(self,x_):\n\t\t""""""\n\t\tReturns the Energy per unit cell and force on all primitive atoms\n\n\t\tArgs:\n\t\t\tx_: a primitive geometry of atoms matching self.atoms.\n\t\t""""""\n\t\t# Compute local energy.\n\t\tetore = 0.0\n\t\tftore = np.zeros((self.natomsReal,3))\n\t\tif (self.maxrng == 0.0):\n\t\t\tself.maxrng = max([f.range for f in self.LocalForces])\n\t\t# Tesselate atoms.\n\t\tz,x = self.lattice.TessLattice(self.atoms,x_, self.maxrng)\n\t\t# Construct NeighborList determine if rebuild is necessary.\n\t\tif (self.NL==None):\n\t\t\tNeedsTriples = any([f.NeedsTriples for f in self.LocalForces])\n\t\t\tself.NL = NeighborList(x, DoTriples_=NeedsTriples, DoPerms_=False, ele_=None, alg_=None, sort_=False)\n\t\t\tself.NL.Update(x, self.maxrng, 0.0, None, self.natomsReal)\n\t\trms = np.mean(np.linalg.norm(x_-self.lastx,axis=1))\n\t\tif (rms > self.nlthresh):\n\t\t\tself.NL.Update(x, self.maxrng, 0.0, None, self.natomsReal)\n\t\t\tself.lastx = x_.copy()\n\t\t\t#print(""NL update"",rms)\n\t\telse:\n\t\t\tprint(""No NL update"",rms)\n\t\t#print(self.NL.pairs)\n\t\t# Compute forces and energies.\n\t\tfor f in self.LocalForces:\n\t\t\teinc, finc = f(z,x,self.NL.pairs)\n\t\t\tetore += np.sum(einc)\n\t\t\tftore += finc[:self.natomsReal]\n\t\treturn etore, ftore\n\nclass PeriodicForce:\n\tdef __init__(self, pm_, lat_):\n\t\t""""""\n\t\tA periodic force evaluator. The force consists of two components\n\t\tShort-Ranged forces, and long-ranged forces. Short ranged forces are\n\t\tevaluated by tesselation. Long-range forces are not supported yet.\n\t\tThis version manages and passes Neighbor lists.\n\n\n\t\tArgs:\n\t\t\tpm_: a molecule.\n\t\t\tlat_: lattice vectors.\n\t\t""""""\n\t\tself.lattice = Lattice(lat_)\n\t\tself.NL = None\n\t\tself.mol0 = self.lattice.CenteredInLattice(pm_)\n\t\tself.atoms = self.mol0.atoms.copy()\n\t\tself.natoms = self.mol0.NAtoms()\n\t\tself.natomsReal = pm_.NAtoms()\n\t\tself.maxrng = 0.0\n\t\tself.LocalForces = []\n\t\tself.lastx = np.zeros(pm_.coords.shape)\n\t\tself.nlthresh = 0.05 #per-atom Threshold for NL rebuild. (A)\n\t\t#self.LongForces = [] Everything is real-space courtesy of DSF.\n\t\treturn\n\tdef ReLattice(self,lat_):\n\t\tself.lattice = Lattice(lat_)\n\t\treturn\n\tdef Density(self):\n\t\t""""""\n\t\tReturns the density in g/cm**3 of the bulk.\n\t\t""""""\n\t\tm = np.array(map(lambda x: ATOMICMASSES[x-1], self.mol0.atoms))*1000.0\n\t\tlatvol = np.linalg.det(self.lattice.lattice) # in A**3\n\t\treturn (np.sum(m)/AVOCONST)/(latvol*pow(10, -24))\n\tdef AdjustLattice(self, x_, lat0_, latp_):\n\t\t""""""\n\t\trescales the coordinates of m relative to previous lattice.\n\t\t""""""\n\t\tlatmet = MatrixPower(np.dot(lat0_, lat0_.T),-1)\n\t\tinlat = np.dot(x_,np.dot(lat0_.T,latmet))\n\t\treturn np.dot(inlat, latp_)\n\tdef LatticeStep(self,x_):\n\t\t""""""\n\t\tDisplace all lattice coordinates by dlat.\n\t\tRelattice if the energy decreases.\n\t\t""""""\n\t\txx = x_.copy()\n\t\te,f = self.__call__(xx)\n\t\tifstepped = True\n\t\tifsteppedoverall = False\n\t\tdlat = PARAMS[""OptLatticeStep""]\n\t\twhile (ifstepped):\n\t\t\tifstepped = False\n\t\t\tfor i in range(3):\n\t\t\t\tfor j in range(3):\n\t\t\t\t\ttmp = self.lattice.lattice.copy()\n\t\t\t\t\ttmp[i,j] += dlat\n\t\t\t\t\tlatt = Lattice(tmp)\n\t\t\t\t\txtmp = latt.ModuloLattice(xx)\n\t\t\t\t\tz,x = latt.TessLattice(self.atoms,xtmp, self.maxrng)\n\t\t\t\t\tet,ft = (self.LocalForces[-1])(z,x,self.natomsReal)\n\t\t\t\t\tif (et < e and abs(e-et) > 0.00001):\n\t\t\t\t\t\te = et\n\t\t\t\t\t\tself.ReLattice(tmp)\n\t\t\t\t\t\txx = xtmp\n\t\t\t\t\t\tifstepped=True\n\t\t\t\t\t\tifsteppedoverall=True\n\t\t\t\t\t\tprint(""LatStep: "",e,self.lattice.lattice)\n\t\t\t\t\t\tMol(z,x).WriteXYZfile(""./results"",""LatOpt"")\n\t\t\t\t\ttmp = self.lattice.lattice.copy()\n\t\t\t\t\ttmp[i,j] -= dlat\n\t\t\t\t\tlatt = Lattice(tmp)\n\t\t\t\t\txtmp = latt.ModuloLattice(xx)\n\t\t\t\t\tz,x = latt.TessLattice(self.atoms, xtmp, self.maxrng)\n\t\t\t\t\tet,ft = (self.LocalForces[-1])(z,x,self.natomsReal)\n\t\t\t\t\tif (et < e and abs(e-et) > 0.00001):\n\t\t\t\t\t\te = et\n\t\t\t\t\t\tself.ReLattice(tmp)\n\t\t\t\t\t\txx = xtmp\n\t\t\t\t\t\tifstepped=True\n\t\t\t\t\t\tifsteppedoverall=True\n\t\t\t\t\t\tprint(""LatStep: "",e,self.lattice.lattice)\n\t\t\t\t\t\tMol(z,x).WriteXYZfile(""./results"",""LatOpt"")\n\t\tif (not ifsteppedoverall and PARAMS[""OptLatticeStep""] > 0.001):\n\t\t\tPARAMS[""OptLatticeStep""] = PARAMS[""OptLatticeStep""]/2.0\n\t\treturn xx\n\tdef Save(self,x_,name_ = ""PMol""):\n\t\tm=Mol(self.atoms,x_)\n\t\tm.properties[""Lattice""] = np.array_str(self.lattice.lattice.flatten())\n\t\tm.WriteXYZfile(""./results/"", name_, \'w\', True)\n\tdef BindForce(self, lf_, rng_):\n\t\t""""""\n\t\tAdds a local force to be computed when the PeriodicForce is called.\n\n\t\tArgs:\n\t\t\tlf_: a function which takes z,x and returns atom energies, atom forces.\n\t\t""""""\n\t\tself.LocalForces.append(LocalForce(lf_,rng_))\n\tdef __call__(self,x_,DoForce = True):\n\t\t""""""\n\t\tReturns the Energy per unit cell and force on all primitive atoms\n\n\t\tArgs:\n\t\t\tx_: a primitive geometry of atoms matching self.atoms.\n\t\t""""""\n\t\t# Compute local energy.\n\t\tetore = 0.0\n\t\tftore = np.zeros((self.natomsReal,3))\n\t\tif (self.maxrng == 0.0):\n\t\t\tself.maxrng = max([f.range for f in self.LocalForces])\n\t\t# Tesselate atoms.\n\t\tz,x = self.lattice.TessLattice(self.atoms,self.lattice.ModuloLattice(x_), self.maxrng)\n\t\t#Mol(z[:self.natomsReal], x[:self.natomsReal]).WriteXYZfile(fpath=""./results"", fname=""untessed_traj"")\n\t\t# Compute forces and energies.\n\t\tfor f in self.LocalForces:\n\t\t\tif (DoForce):\n\t\t\t\teinc, finc = f(z,x,self.natomsReal)\n\t\t\t\tetore += np.sum(einc)\n\t\t\t\tftore += finc[:self.natomsReal]\n\t\t\telse:\n\t\t\t\teinc = f(z,x,self.natomsReal,DoForce)\n\t\t\t\tetore += np.sum(einc)\n\t\treturn etore, ftore\n\tdef TestGradient(self,x_):\n\t\t""""""\n\t\tTravel along a gradient direction.\n\t\tSubsample to examine how integrable the forces are versus\n\t\tthe energy along this path.\n\t\t""""""\n\t\te0,g0 = self.__call__(x_)\n\t\tg0 /= JOULEPERHARTREE\n\t\tefunc = lambda x: self.__call__(x)[0]\n\t\tprint(""Magnitude of g"", np.linalg.norm(g0))\n\t\tprint(""g"",g0)\n\t\t#print(""FDiff g"", FdiffGradient(efunc,x_))\n\t\txt = x_.copy()\n\t\tes = np.zeros(40)\n\t\tgs = np.zeros((40,g0.shape[0],g0.shape[1]))\n\t\tfor i,d in enumerate(range(-20,20)):\n\t\t\tdx = d*0.01*g0\n\t\t\t#print(""dx"", dx)\n\t\t\txt = x_ + dx\n\t\t\tes[i], gs[i] = self.__call__(xt)\n\t\t\tgs[i] /= JOULEPERHARTREE\n\t\t\tprint(""es "", es[i], i, np.sqrt(np.sum(dx*dx)) , np.sum(gs[i]*g0), np.sum(g0*g0))\n\tdef RDF(self,x_,z1=8,z2=8,rng=15.0,dx = 0.02,name_=""RDF.txt""):\n\t\tzt,xt = self.lattice.TessLattice(self.atoms, x_ , rng)\n\t\tni = MolEmb.CountInRange(zt,xt,self.natoms,z1,z2,rng,dx)\n\t\tri = np.arange(0.0,rng,dx)\n\t\tif 0:\n\t\t\tni = np.zeros(ri.shape)\n\t\t\tnav = 0.0\n\t\t\tfor i in range(self.natoms):\n\t\t\t\tif (zt[i]==z1):\n\t\t\t\t\tnav += 1.0\n\t\t\t\t\tfor j in range(zt.shape[0]):\n\t\t\t\t\t\tif i==j:\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\telif (zt[j] == z2):\n\t\t\t\t\t\t\td = np.linalg.norm(xt[j]-xt[i])\n\t\t\t\t\t\t\tni[int(d/dx):] += 1\n\t\t\t# Estimate the effective density by the number contained in the outermost sphere.\n\t\t\tni /= nav\n\t\tdensity = ni[-1]/(4.18879*ri[-1]*ri[-1]*ri[-1])\n\t\t# Now take the derivative by central differences\n\t\tx2gi = np.gradient(ni / (12.56637*density),dx)\n\t\t# finally divide by x**2 and moving average it.\n\t\tgi = np.zeros(ri.shape)\n\t\tgi[1:] = x2gi[1:]/(ri[1:]*ri[1:])\n\t\tgi[0] = 0.0\n\t\tgi = MovingAverage(gi,2)\n\t\treturn gi\n\t\t#np.savetxt(""./results/""+name_+"".txt"",gi)\n\tdef RDF_inC(self,x_,z_,lat_,z1=8,z2=8,rng=10.0,dx = 0.02,name_=""RDF.txt""):\n\t\trdf_index =  GetRDF_Bin(x_, z_, rng, dx, lat_, z1, z2)\n\t\tri = np.arange(0.0,rng,dx)\n\t\tni = np.zeros(ri.shape)\n\t\tfor index in rdf_index:\n\t\t\tni[index:] += 1\n\t\tnav = 0.0\n\t\t#print (""rdf_index:"", len(rdf_index))\n\t\tfor i in range(z_.shape[0]):\n\t\t\tif (z_[i]==z1):\n\t\t\t\tnav += 1.0\n\t\t#np.savetxt(""./results/""+name_+""ni.txt"",ni)\n\t\t# Estimate the effective density by the number contained in the outermost sphere.\n\t\tni /= nav\n\t\tdensity = ni[-1]/(4.18879*ri[-1]*ri[-1]*ri[-1])\n\t\t# Now take the derivative by central differences\n\t\tx2gi = np.gradient(ni / (12.56637*density),dx)\n\t\t# finally divide by x**2 and moving average it.\n\t\tgi = x2gi/(ri*ri)\n\t\tgi[0] = 0.0\n\t\tgi = MovingAverage(gi,2)\n\t\treturn gi\n\t\t#np.savetxt(""./results/""+name_+"".txt"",gi)\n'"
TensorMol/ForceModifiers/Transformer.py,0,"b'from __future__ import absolute_import\nfrom ..Containers.Mol import *\nfrom ..Util import *\nimport os,sys,re\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\nfrom ..Math import LinearOperations\n\nclass Transformer:\n\t""""""\n\tData manipulation routines for normalizing and other transformations to the\n\tembedding and learning targets. TensorData initializes the transformer\n\tautomatically if a .tdt file is loaded for training. The choice of transformation\n\troutines are set by PARAMS[""InNormRoutine""] and PARAMS[""OutNormRoutine""].\n\t""""""\n\tdef __init__(self, InNorm_ = None, OutNorm_ = None, ele_ = None, Emb_ = None, OType_ = None):\n\t\t""""""\n\t\tArgs:\n\t\t\tInNorm_ : Embedding normalization type\n\t\t\tOutNorm_: Learning target normalization type\n\t\t\tele_: Element type for this transformer\n\t\t\tEmb_: Type of digester to reduce molecules to NN inputs.\n\t\t\tOType_: Property of the molecule which will be learned (energy, force, etc)\n\t\t""""""\n\t\tself.Emb = Emb_\n\t\tself.OType = OType_\n\t\tself.innorm = InNorm_\n\t\tself.outnorm = OutNorm_\n\n\n\t\t#Should check that normalization routines match input/output types here\n\n\tdef Print(self):\n\t\tLOGGER.info(""-------------------- "")\n\t\tLOGGER.info(""Transformer Information "")\n\t\tLOGGER.info(""self.innorm: ""+str(self.innorm))\n\t\tLOGGER.info(""self.outnorm: ""+str(self.outnorm))\n\t\tif (self.outnorm == ""MeanStd""):\n\t\t\tLOGGER.info(""self.outmean: ""+str(self.outmean))\n\t\t\tLOGGER.info(""self.outstd: ""+str(self.outstd))\n\t\tLOGGER.info(""-------------------- "")\n\n\tdef NormalizeIns(self, ins, train=True):\n\t\tif (self.innorm == ""Frobenius""):\n\t\t\treturn self.NormInFrobenius(ins)\n\t\telif (self.innorm == ""MeanStd""):\n\t\t\tif (train):\n\t\t\t\tself.AssignInMeanStd(ins)\n\t\t\treturn self.NormInMeanStd(ins)\n\t\telif (self.innorm == ""DeltaMeanStd""):\n\t\t\treturn self.NormInDeltaMeanStd(ins)\n\t\telif (self.innorm == ""MinMax""):\n\t\t\tif (train):\n\t\t\t\tself.AssignInMinMax(ins)\n\t\t\treturn self.NormInMinMax(ins)\n\n\tdef NormalizeOuts(self, outs, train=True):\n\t\tif (self.outnorm == ""MeanStd""):\n\t\t\tif (train):\n\t\t\t\tself.AssignOutMeanStd(outs)\n\t\t\treturn self.NormOutMeanStd(outs)\n\t\telif (self.outnorm == ""Logarithmic""):\n\t\t\treturn self.NormOutLogarithmic(outs)\n\t\telif (self.outnorm == ""Sign""):\n\t\t\treturn self.NormOutSign(outs)\n\n\tdef NormInFrobenius(self, ins):\n\t\tfor i in range(len(ins)):\n\t\t\tins[i] = ins[i]/(np.linalg.norm(ins[i])+1.0E-8)\n\t\treturn ins\n\n\tdef AssignInMeanStd(self, ins):\n\t\tself.inmean = (np.mean(ins, axis=0)).reshape((1,-1))\n\t\tself.instd = (np.std(ins, axis=0)).reshape((1, -1))\n\n\tdef NormInMeanStd(self, ins):\n\t\treturn (ins - self.inmean)/self.instd\n\n\tdef NormInDeltaMeanStd(self, ins):\n\t\tins[:,-3:] = (ins[:,-3:] - self.outmean)/self.outstd\n\t\treturn ins\n\n\tdef AssignInMinMax(self, ins):\n\t\tself.inmin = np.amin(ins)\n\t\tself.inmax = np.amax(ins)\n\n\tdef NormInMinMax(self, ins):\n\t\treturn (ins - self.inmin)/(self.inmax-self.inmin)\n\n\tdef AssignOutMeanStd(self, outs):\n\t\touts = outs[~np.all(np.equal(outs, 0), axis=2)]\n\t\tself.outmean = np.mean(outs)\n\t\tself.outstd = np.std(outs)\n\n\tdef NormOutMeanStd(self, outs):\n\t\treturn (outs - self.outmean)/self.outstd\n\n\tdef NormOutSign(self, outs):\n\t\treturn np.sign(outs)\n\n\tdef UnNormalizeOuts(self, outs):\n\t\tif (self.outnorm == ""MeanStd""):\n\t\t\treturn self.UnNormOutMeanStd(outs)\n\t\telif (self.outnorm == ""Logarithmic""):\n\t\t\treturn self.UnNormOutLogarithmic(outs)\n\n\tdef UnNormOutMeanStd(self, outs):\n\t\treturn outs*self.outstd+self.outmean\n'"
TensorMol/ForceModifiers/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom .Neighbors import *\nfrom .NeighborsMB import *\nfrom .Periodic import *\nfrom .Transformer import * \n'
TensorMol/Interfaces/AbInitio.py,4,"b'""""""\nRoutines for running external Ab-Initio packages to get shit out of mol.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\nimport random, math, subprocess\n\ndef PyscfDft(m_,basis_ = \'6-31g*\',xc_=\'b3lyp\'):\n\tif (not HAS_PYSCF):\n\t\tprint(""Missing PYSCF"")\n\t\treturn 0.0\n\tmol = gto.Mole()\n\tpyscfatomstring=""""\n\tcrds = m_.coords.copy()\n\tcrds[abs(crds)<0.0001] *=0.0\n\tfor j in range(len(m_.atoms)):\n\t\tpyscfatomstring=pyscfatomstring+str(m_.atoms[j])+"" ""+str(crds[j,0])+"" ""+str(crds[j,1])+"" ""+str(crds[j,2])+("";"" if j!= len(m_.atoms)-1 else """")\n\tmol.atom = pyscfatomstring\n\tmol.unit = ""Angstrom""\n\tmol.charge = 0\n\tmol.spin = 0\n\tmol.basis = basis_\n\tmol.verbose = 0\n\tmol.build()\n\tmf = dft.RKS(mol)\n\tmf.xc = xc_\n\te = mf.kernel()\n\treturn e\n\ndef QchemDFT(m_,basis_ = \'6-31g*\',xc_=\'b3lyp\', jobtype_=\'force\', filename_=\'tmp\', path_=\'./qchem/\', threads=False):\n\tistring = \'$molecule\\n0 1 \\n\'\n\tcrds = m_.coords.copy()\n\tcrds[abs(crds)<0.0000] *=0.0\n\tfor j in range(len(m_.atoms)):\n\t\tistring=istring+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\tif jobtype_ == ""dipole"":\n\t\tistring =istring + \'$end\\n\\n$rem\\njobtype sp\\nbasis \'+basis_+\'\\nmethod \'+xc_+\'\\nthresh 11\\nsymmetry false\\nsym_ignore true\\n$end\\n\'\n\telse:\n\t\tistring =istring + \'$end\\n\\n$rem\\njobtype \'+jobtype_+\'\\nbasis \'+basis_+\'\\nmethod \'+xc_+\'\\nthresh 11\\nUNRESTRICTED   true\\nsymmetry false\\nsym_ignore true\\n$end\\n\'\n\twith open(path_+filename_+\'.in\',\'w\') as fin:\n\t\tfin.write(istring)\n\twith open(path_+filename_+\'.out\',\'a\') as fout:\n\t\tif threads:\n\t\t\tproc = subprocess.Popen([\'qchem\', \'-nt\', str(threads), path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\telse:\n\t\t\tproc = subprocess.Popen([\'qchem\', path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\tout, err = proc.communicate()\n\t\tfout.write(out)\n\tlines = out.split(\'\\n\')\n\tif jobtype_ == \'force\':\n\t\tForces = np.zeros((m_.atoms.shape[0],3))\n\t\tfor i, line in enumerate(lines):\n\t\t\tif line.count(\'Convergence criterion met\')>0:\n\t\t\t\tEnergy = float(line.split()[1])\n\t\t\tif line.count(""Gradient of SCF Energy"") > 0:\n\t\t\t\tk = 0\n\t\t\t\tl = 0\n\t\t\t\tfor j in range(1, m_.atoms.shape[0]+1):\n\t\t\t\t\tForces[j-1,:] = float(lines[i+k+2].split()[l+1]), float(lines[i+k+3].split()[l+1]), float(lines[i+k+4].split()[l+1])\n\t\t\t\t\tl += 1\n\t\t\t\t\tif (j % 6) == 0:\n\t\t\t\t\t\tk += 4\n\t\t\t\t\t\tl = 0\n\t\treturn Energy, -Forces*JOULEPERHARTREE*BOHRPERA\n\telif jobtype_ == \'sp\':\n\t\tfor line in lines:\n\t\t\tif line.count(\'Convergence criterion met\')>0:\n\t\t\t\tEnergy = float(line.split()[1])\n\t\treturn Energy\n\telif jobtype_ ==  \'dipole\':\n\t\tfor i, line in enumerate(lines):\n\t\t\tif ""Dipole Moment (Debye)"" in line:\n\t\t\t\ttmp = lines[i+1].split()\n\t\t\t\tdipole = np.asarray([float(tmp[1]),float(tmp[3]),float(tmp[5])])\n\t\t\t\treturn dipole\n\telse:\n\t\traise Exception(""jobtype needs formatted for return variables"")\n\ndef QchemDFT_optimize(m_,basis_ = \'6-31g*\',xc_=\'b3lyp\', filename_=\'tmp\', path_=\'./qchem/\', threads=False):\n\tistring = \'$molecule\\n0 1 \\n\'\n\tcrds = m_.coords.copy()\n\tcrds[abs(crds)<0.0000] *=0.0\n\tatoms = m_.atoms.shape[0]\n\tfor j in range(len(m_.atoms)):\n\t\tistring=istring+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\tistring =istring + \'$end\\n\\n$rem\\njobtype opt\\ngeom_opt_max_cycles 500\\nbasis \'+basis_+\'\\nmethod \'+xc_+\'\\nthresh 11\\nsymmetry false\\nsym_ignore true\\n$end\\n\'\n\twith open(path_+filename_+\'.in\',\'w\') as fin:\n\t\tfin.write(istring)\n\twith open(path_+filename_+\'.out\',\'a\') as fout:\n\t\tif threads:\n\t\t\tproc = subprocess.Popen([\'qchem\', \'-nt\', str(threads), path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\telse:\n\t\t\tproc = subprocess.Popen([\'qchem\', path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\tout, err = proc.communicate()\n\t\tfout.write(out)\n\tlines = out.split(\'\\n\')\n\tfor i, line in enumerate(lines):\n\t\tif ""**  OPTIMIZATION CONVERGED  **"" in line:\n\t\t\twith open(path_+filename_+\'.xyz\',\'a\') as fopt:\n\t\t\t\txyz = []\n\t\t\t\tfor j in range(atoms):\n\t\t\t\t\txyz.append([lines[i+5+j].split()[1],lines[i+5+j].split()[2],lines[i+5+j].split()[3],lines[i+5+j].split()[4]])\n\t\t\t\tfopt.write(str(atoms)+""\\nComment:\\n"")\n\t\t\t\tfor j in range(atoms):\n\t\t\t\t\tfopt.write(xyz[j][0]+""      ""+xyz[j][1]+""      ""+xyz[j][2]+""      ""+xyz[j][3]+""\\n"")\n\t\t\t\tfopt.write(""\\n"")\n\t\t\tbreak\n\ndef QchemRIMP2(m_,basis_ = \'cc-pvtz\', aux_basis_=\'rimp2-cc-pvtz\', jobtype_=\'force\', filename_=\'tmp\', path_=\'./qchem/\', threads=False):\n\tistring = \'$molecule\\n0 1 \\n\'\n\tcrds = m_.coords.copy()\n\tcrds[abs(crds)<0.0000] *=0.0\n\tfor j in range(len(m_.atoms)):\n\t\tistring=istring+itoa[m_.atoms[j]]+\' \'+str(crds[j,0])+\' \'+str(crds[j,1])+\' \'+str(crds[j,2])+\'\\n\'\n\tistring =istring + \'$end\\n\\n$rem\\njobtype \'+jobtype_+\'\\nbasis \'+basis_+\'\\nAUX_BASIS \'+aux_basis_+\'\\nmethod rimp2\\nsymmetry false\\nsym_ignore true\\nmem_total 9999\\nmem_static 2000\\n$end\\n\'\n\t#print istring\n\twith open(path_+filename_+\'.in\',\'w\') as fin:\n\t\tfin.write(istring)\n\twith open(path_+filename_+\'.out\',\'a\') as fout:\n\t\tif threads:\n\t\t\tproc = subprocess.Popen([\'qchem\', \'-nt\', str(threads), path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\telse:\n\t\t\tproc = subprocess.Popen([\'qchem\', path_+filename_+\'.in\'], stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=False)\n\t\tout, err = proc.communicate()\n\t\tfout.write(out)\n\tlines = out.split(\'\\n\')\n\tif jobtype_ == \'force\':\n\t\tForces = np.zeros((m_.atoms.shape[0],3))\n\t\tfor i, line in enumerate(lines):\n\t\t\tif line.count(\'RI-MP2 TOTAL ENERGY\')>0:\n\t\t\t\tEnergy = float(line.split()[4])\n\t\t\tif line.count(""Full Analytical Gradient of MP2 Energy"") > 0:\n\t\t\t\tk = 0\n\t\t\t\tl = 0\n\t\t\t\tfor j in range(1, m_.atoms.shape[0]+1):\n\t\t\t\t\tForces[j-1,:] = float(lines[i+k+2].split()[l+1]), float(lines[i+k+3].split()[l+1]), float(lines[i+k+4].split()[l+1])\n\t\t\t\t\tl += 1\n\t\t\t\t\tif (j % 5) == 0:\n\t\t\t\t\t\tk += 4\n\t\t\t\t\t\tl = 0\n\t\t# return Energy, Forces\n\t\treturn -Forces*JOULEPERHARTREE/BOHRPERA\n\telif jobtype_ == \'sp\':\n\t\tfor line in lines:\n\t\t\tif line.count(\'RI-MP2 TOTAL ENERGY\')>0:\n\t\t\t\tEnergy = float(line.split()[4])\n\t\treturn Energy\n\telse:\n\t\traise Exception(""jobtype needs formatted for return variables"")\n\n\ndef PullFreqData():\n\ta = open(""/media/sdb1/dtoth/qchem_jobs/new/phenol.out"", ""r+"") #Change file name\n\t# each time to read correct output file\n\tf=open(""phenol_freq.dat"", ""w"") #Change file name to whatever you want --\n\t# make sure it\'s different each time\n\tlines = a.readlines()\n\tdata = []\n\tip = 0\n\tfor i, line in enumerate(lines):\n\t\tif line.count(""NAtoms"") > 0:\n\t\t\tatoms = int(lines[i+1].split()[0])\n\t\t\tbreak\n\tnm = np.zeros((3*atoms-6, atoms, 3))\n\tfor i, line in enumerate(lines):\n\t\tif ""Frequency:"" in line:\n\t\t\tfreq = [line.split()[1], line.split()[2],line.split()[3]]\n\t\t\tintens = [lines[i+4].split()[2], lines[i+4].split()[3],lines[i+4].split()[4]]\n\t\t\tf.write(freq[0] + ""   "" + intens[0] + ""\\n"")\n\t\t\tf.write(freq[1] + ""   "" + intens[1] + ""\\n"")\n\t\t\tf.write(freq[2] + ""   "" + intens[2] + ""\\n"")\n\t\tif ""Raman Active"" in line:\n\t\t\tfor j in range(atoms):\n\t\t\t\tit = 0\n\t\t\t\tfor k in range(3):\n\t\t\t\t\tfor l in range(3):\n\t\t\t\t\t\tnm[it+ip,j,l] = float(lines[i+j+2].split()[k*3+l+1])\n\t\t\t\t\tit += 1\n\t\t\tip += 3\n\t\t\t# f.write(nm[0] + ""  "" + nm)\n\tnp.save(""morphine_nm.npy"", nm)\n\tf.close()\n\ndef PySCFMP2Energy(m, basis_=\'cc-pvqz\'):\n\tmol = gto.Mole()\n\tpyscfatomstring=""""\n\tfor j in range(len(m.atoms)):\n\t\ts = m.coords[j]\n\t\tpyscfatomstring=pyscfatomstring+str(m.AtomName(j))+"" ""+str(s[0])+"" ""+str(s[1])+"" ""+str(s[2])+("";"" if j!= len(m.atoms)-1 else """")\n\tmol.atom = pyscfatomstring\n\tmol.basis = basis_\n\tmol.verbose = 0\n\ttry:\n\t\tmol.build()\n\t\tmf=scf.RHF(mol)\n\t\thf_en = mf.kernel()\n\t\tmp2 = mp.MP2(mf)\n\t\tmp2_en = mp2.kernel()\n\t\ten = hf_en + mp2_en[0]\n\t\tm.properties[""energy""] = en\n\t\treturn en\n\texcept Exception as Ex:\n\t\tprint(""PYSCF Calculation error... :"",Ex)\n\t\tprint(""Mol.atom:"", mol.atom)\n\t\tprint(""Pyscf string:"", pyscfatomstring)\n\t\treturn 0.0\n\t\t#raise Ex\n\treturn\n'"
TensorMol/Interfaces/TMIPIinterface.py,0,"b'import socket\nimport numpy as np\nfrom ..PhysicalData import *\n\n\nHDRLEN = 12\nclass TMIPIManger():\n\tdef __init__(self, EnergyForceField=None, TCP_IP=""localhost"", TCP_PORT= 31415):\n\t\tself.EnergyForceField = EnergyForceField\n\t\tself.s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t\tself.hasdata = False\n\t\ttry:\n\t\t\tself.s.connect((TCP_IP, TCP_PORT))\n\t\t\tprint (""Connect to server with address:"", TCP_IP+"" ""+str(TCP_PORT))\n\t\texcept:\n\t\t\tprint (""Fail connect to server with address: "", TCP_IP+"" ""+str(TCP_PORT))\n\n\n\tdef md_run(self):\n\t\twhile (True):\n\t\t\tdata = self.s.recv(HDRLEN)\n\t\t\tif data.strip() == ""STATUS"":\n\t\t\t\tif self.hasdata:\n\t\t\t\t\tprint (""client has data."")\n\t\t\t\t\tself.s.sendall(""HAVEDATA    "")\n\t\t\t\telse:\n\t\t\t\t\tprint (""client is ready to get position from server"")\n\t\t\t\t\tself.s.sendall(""READY       "")\n\t\t\telif data.strip() == ""POSDATA"":\n\t\t\t\t\tprint (""server is sending positon."")\n\t\t\t\t\tbuf_ = self.s.recv(9*8) # cellh np.float64\n\t\t\t\t\tcellh = np.fromstring(buf_, np.float64)/BOHRPERA\n\t\t\t\t\tbuf_ = self.s.recv(9*8) # cellih np.float64\n\t\t\t\t\tcellih = np.fromstring(buf_, np.float64)*BOHRPERA\n\t\t\t\t\tbuf_ = self.s.recv(4) # natom\n\t\t\t\t\tnatom = np.fromstring(buf_, np.int32)[0]\n\t\t\t\t\tbuf_ = self.s.recv(3*natom*8) # position\n\t\t\t\t\tposition = (np.fromstring(buf_, np.float64)/BOHRPERA).reshape((-1, 3))\n\t\t\t\t\tprint (""cellh:"", cellh, ""  cellih:"", cellih, "" natom:"", natom)\n\t\t\t\t\tprint (""position:"", position)\n\t\t\t\t\tprint (""now is running the client to calculate force..."")\n\n\t\t\t\t\tenergy, force=self.EnergyForceField(position)\n\t\t\t\t\tforce = force/JOULEPERHARTREE/BOHRPERA\n\t\t\t\t\t# some dummyy function to calculte the energy, natom,\n\t\t\t\t\tvir = np.zeros((3,3))\n\n\t\t\t\t\tself.hasdata = True\n\n\t\t\telif data.strip() == ""GETFORCE"":\n\t\t\t\t\tprint (""server is ready to get force from client"")\n\t\t\t\t\tself.s.sendall(""FORCEREADY  "")\n\t\t\t\t\tself.s.sendall(np.float64(energy))\n\t\t\t\t\tself.s.sendall(np.int32(natom))\n\t\t\t\t\tself.s.sendall(force)\n\t\t\t\t\tself.s.sendall(vir)\n\t\t\t\t\tself.s.sendall(np.int32(7))\n\t\t\t\t\tself.s.sendall(""nothing"")\n\t\t\t\t\tself.hasdata = False\n\t\t\telse:\n\t\t\t\traise Exception(""wrong message from server"")\n'"
TensorMol/Interfaces/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom .AbInitio import * \n'
TensorMol/MBE/MBE.py,0,b''
TensorMol/MBE/MBEData.py,0,b''
TensorMol/MBE/MBE_Opt.py,0,"b'""""""\nChanges that need to be made:\n - LBFGS needs to be modularized (Solvers.py)?\n - The filename needs to be changed to Opt_MBE\n - This needs to be a child of the ordinary optimizer class.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .NN_MBE import *\n\nclass MBE_Optimizer:\n\tdef __init__(self,nn_mbe_):\n\t\tself.energy_thresh = 1e-9\n\t\tself.force_thresh = 0.00001\n\t\tself.step_size = 0.1\n\t\tself.momentum = 0.9\n\t\tself.momentum_decay = 0.7\n\t\tself.max_opt_step = 10000\n\t\tself.nn_mbe = nn_mbe_\n\t\tself.m_max = 7\n\t\treturn\n\n\tdef MBE_Opt(self, m):\n\t\tcoords_hist = []\n\t\tstep = 0\n\t\tenergy_err = 100\n\t\tforce_err = 100\n\t\tenergy_his = []\n\t\tself.nn_mbe.NN_Energy_Force(m)\n\t\t#self.nn_mbe.NN_Energy(m)\n\t\tforce = np.array(m.properties[""mbe_deri""],copy=True)\n\t\twhile( ( force_err >self.force_thresh or energy_err > self.energy_thresh) and  step < self.max_opt_step ):\n\t\t\tcoords = np.array(m.coords,copy=True)\n\t\t\tforce = np.array(m.properties[""mbe_deri""],copy=True)\n\t\t\tif step==0:\n\t\t\t\told_force =force\n\t\t\tforce = (1-self.momentum)*force + self.momentum*old_force\n\t\tprint(""force:"", force)\n\t\t\told_force =force\n\t\t\tenergy = m.nn_energy\n\t\t\tenergy_his.append(energy)\n\t\t\tcoords_hist.append(coords)\n\t\t\tstep += 1\n\t\t\tm.coords = m.coords - self.step_size*force\n\t\t\tm.Reset_Frags()\n\t\t#m.Generate_All_MBE_term_General([{""atom"":""HOH"", ""charge"":0}])\n\t\tself.nn_mbe.NN_Energy_Force(m)\n\t\t\t#self.nn_mbe.NN_Energy(m)\n\t\t\tenergy_err = abs(m.nn_energy - energy)\n\t\t\tforce_err = (np.absolute(force)).max()\n\t\t\tprint(""\\n\\n"")\n\t\t\tprint(""step:"", step)\n\t\t\tprint(""old_energy:"", energy, ""new_energy:"", m.nn_energy, ""energy_err:"", energy_err)\n\t\t\tif (step%10 == 0):\n\t\t\t\tm.WriteXYZfile(""./results/"", ""OptLog"")\n\t\tnp.savetxt(""gd_opt_no_momentum.dat"", np.asarray(energy_his))\n\t\treturn\n\n\tdef MBE_LBFGS_Opt(self, m):\n\t\tstep = 0\n\t\tenergy_err = 100\n\t\tforce_err = 100\n\t\tself.nn_mbe.NN_Energy(m)\n\t\tforce = np.array(m.properties[""mbe_deri""],copy=True)\n\t\tforce_his = []\n\t\tcoord_his = []\n\t\tenergy_his = []\n\t\twhile( ( force_err >self.force_thresh or energy_err > self.energy_thresh) and  step < self.max_opt_step ):\n\t\t\tcoords = np.array(m.coords,copy=True)\n\t\t\tforce = np.array(m.properties[""mbe_deri""],copy=True)\n\t\t\tif step < self.m_max:\n\t\t\t\tforce_his.append(force.reshape(force.shape[0]*force.shape[1]))\n\t\t\t\tcoord_his.append(coords.reshape(coords.shape[0]*coords.shape[1]))\n\t\t\telse:\n\t\t\t\tforce_his.pop(0)\n\t\t\t\tforce_his.append(force.reshape(force.shape[0]*force.shape[1]))\n\t\t\t\tcoord_his.pop(0)\n\t\t\t\tcoord_his.append(coords.reshape(coords.shape[0]*coords.shape[1]))\n\t\tprint(""force:"", force)\n\t\tq = (force.reshape(force.shape[0]*force.shape[1])).copy()\n\t\tfor i in range (len(force_his)-1, 0, -1):\n\t\t\ts=coord_his[i] - coord_his[i-1]\n\t\t\ty=force_his[i] - force_his[i-1]\n\t\t\trho = 1/y.dot(s)\n\t\t\ta=rho*s.dot(q)\n\t\t\tq = q - a*y\n\t\tif step == 0:\n\t\t\tH=1.0\n\t\telse:\n\t\t\tH = ((coord_his[-1] - coord_his[-2]).dot(force_his[-1] - force_his[-2]))/((force_his[-1] - force_his[-2]).dot(force_his[-1] - force_his[-2]))\n\t\tz = H*q\n\t\tfor i in range (1, len(force_his)):\n\t\t\ts=coord_his[i] - coord_his[i-1]\n\t\t\ty=force_his[i] - force_his[i-1]\n\t\t\trho = 1/y.dot(s)\n\t\t\ta=rho*s.dot(q)\n\t\t\tbeta = rho*(force_his[i] - force_his[i-1]).dot(z)\n\t\t\tz = z + s*(a -beta)\n\t\t\tz = z.reshape((m.NAtoms(), -1))\n\t\t\tprint(""z: "",z)\n\t\t\tenergy = m.nn_energy\n\t\t\tenergy_his.append(energy)\n\t\t\tm.coords = m.coords - self.step_size*z\n\t\t\tm.Reset_Frags()\n\t\t\tself.nn_mbe.NN_Energy(m)\n\t\t\tenergy_err = abs(m.nn_energy - energy)\n\t\t\tforce_err = (np.absolute(force)).max()\n\t\t\tstep += 1\n\t\t\tprint(""\\n"")\n\t\t\tprint(""step:"", step)\n\t\t\tprint(""old_energy:"", energy, ""new_energy:"", m.nn_energy, ""energy_err:"", energy_err)\n\t\t\tif (step%50 == 0):\n\t\t\t\tm.WriteXYZfile(""./datasets/"", ""OptLog"")\n\t\tnp.savetxt(""lbfgs_opt.dat"", np.asarray(energy_his))\n\t\treturn\n'"
TensorMol/MBE/NN_MBE.py,0,"b'#\n# Optimization algorithms\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Containers.Sets import *\nfrom ..TFNetworks.TFMolManage import *\nfrom ..Containers.Mol import *\nfrom ..ForceModels.Electrostatics import *\n\nclass NN_MBE:\n\tdef __init__(self,tfm_=None):\n\t\tself.nn_mbe = dict()\n\t\tif tfm_ != None:\n\t\t\tfor order in tfm_:\n\t\t\t\tprint(tfm_[order])\n\t\t\t\tself.nn_mbe[order] = TFMolManage(tfm_[order], None, False)\n\t\treturn\n\n\n\tdef NN_Energy(self, mol):\n\t\tmol.Generate_All_MBE_term(atom_group=3, cutoff=6, center_atom=0)  # one needs to change the variable here\n\t\tnn_energy = 0.0\n\t\tfor i in range (1, mol.mbe_order+1):\n\t\t\tnn_energy += self.nn_mbe[i].Eval_Mol(mol)\n\t\tmol.Set_MBE_Force()\n\t\tmol.nn_energy = nn_energy\n\t\tprint(""coords of mol:"", mol.coords)\n\t\tprint(""force of mol:"", mol.properties[""mbe_deri""])\n\t\tprint(""energy of mol:"", nn_energy)\n\t\treturn\n\n\nclass NN_MBE_Linear:\n\tdef __init__(self,tfm_=None):\n\t\tself.mbe_order = PARAMS[""MBE_ORDER""]\n\t\tself.nn_mbe = tfm_\n\t\tself.max_num_frags = None\n\t\tself.nnz_frags = None\n\t\treturn\n\n\tdef EnergyForceDipole(self, N_MB):\n\t\teval_set = MSet(""TmpMBESet"")\n\t\tMBE_C = []\n\t\tMBE_Index = []\n\t\tNAtom = []\n\t\tself.max_num_frags = N_MB.nf + N_MB.nf*(N_MB.nf-1)/2 + N_MB.nf*(N_MB.nf-1)*(N_MB.nf-2)/6  # set the max_frag to include all possible dimers and trimers\n\t\tif self.mbe_order >= 1:\n\t\t\tfor i in range (N_MB.nf):\n\t\t\t\tnatom = np.count_nonzero(N_MB.singz[i])\n\t\t\t\tNAtom.append(natom)\n\t\t\t\teval_set.mols.append(Mol(N_MB.singz[i][:natom], N_MB.sings[i][:natom]))\n\t\t\t\tMBE_C.append(N_MB.singC[i])\n\t\t\t\tMBE_Index.append(N_MB.singI[i])\n\t\tif self.mbe_order >= 2:\n\t\t\tfor i in range (N_MB.npair):\n\t\t\t\tnatom = np.count_nonzero(N_MB.pairz[i])\n\t\t\t\tNAtom.append(natom)\n\t\t\t\teval_set.mols.append(Mol(N_MB.pairz[i][:natom], N_MB.pairs[i][:natom]))\n\t\t\t\tMBE_C.append(N_MB.pairC[i])\n\t\t\t\tMBE_Index.append(N_MB.pairI[i])\n\t\tif self.mbe_order >= 3:\n\t\t\tfor i in range (N_MB.ntrip):\n\t\t\t\tnatom = np.count_nonzero(N_MB.tripz[i])\n\t\t\t\tNAtom.append(natom)\n\t\t\t\teval_set.mols.append(Mol(N_MB.tripz[i][:natom], N_MB.trips[i][:natom]))\n\t\t\t\tMBE_C.append(N_MB.tripC[i])\n\t\t\t\tMBE_Index.append(N_MB.tripI[i])\n\t\tif  self.mbe_order >= 4:\n\t\t\traise Exception(""Linear MBE only implemented up to order 3"")\n\t\tMBE_C = np.asarray(MBE_C)\n\t\tself.nnz_frags = MBE_C.shape[0]\n\t\tfor dummy_index in range(self.nnz_frags, self.max_num_frags):\n\t\t\teval_set.mols.append(Mol(np.zeros((1), dtype=np.uint8),np.zeros((1,3),dtype=float)))\n\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = self.nn_mbe.EvalBPDirectEESet(eval_set, PARAMS[""AN1_r_Rc""], PARAMS[""AN1_a_Rc""], PARAMS[""EECutoffOff""])\n\t\tprint((""Etotal:"", Etotal, "" self.nnz_frags:"",self.nnz_frags))\n\t\tE_mbe = np.sum(Etotal[:self.nnz_frags]*MBE_C)\n\t\tgradient_mbe = np.zeros((N_MB.nt,3))\n\t\tatom_charge_mbe = np.zeros((N_MB.nt))\n\t\tfor i, index in enumerate(MBE_Index):\n\t\t\tgradient_mbe[index] += gradient[i][:NAtom[i]]*MBE_C[i]\n\t\t\tatom_charge_mbe[index] += atom_charge[i][:NAtom[i]]*MBE_C[i]\n\t\t#print (""gradient_mbe:"", gradient_mbe/-JOULEPERHARTREE)\n\t\treturn E_mbe, gradient_mbe, atom_charge_mbe\n\n\n\nclass NN_MBE_BF:\n\tdef __init__(self,tfm_=None, dipole_tfm_=None):\n\t\tself.mbe_order = PARAMS[""MBE_ORDER""]\n\t\tself.nn_mbe = tfm_\n\t\tself.nn_dipole_mbe = dipole_tfm_\n\t\treturn\n\n\tdef NN_Energy(self, mol, embed_ = False):\n\t\ts = MSet()\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\ts.mols += mol.mbe_frags[order]\n\t\tenergies =  np.asarray(self.nn_mbe.Eval_BPEnergy(s))\n\t\tpointer = 0\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tmol.frag_energy_sum[order] = np.sum(energies[pointer:pointer+len(mol.mbe_frags[order])])\n\t\t\tif order == 1 or order ==2 :\n\t\t\t\tfor i, mol_frag in enumerate(mol.mbe_frags[order]):\n\t\t\t\t\tmol_frag.properties[""nn_energy""] = energies[pointer+i]\n\t\t\tpointer += len(mol.mbe_frags[order])\n\t\tif embed_:\n\t\t\tmol.MBE_Energy_Embed()\n\t\telse:\n\t\t\tmol.MBE_Energy()\n\t\treturn\n\n\tdef NN_Dipole(self, mol): # unit: Debye\n\t\ts = MSet()\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\ts.mols += mol.mbe_frags[order]\n\t\tdipoles, charges =  self.nn_dipole_mbe.Eval_BPDipole_2(s)\n\t\t#print ""dipole:"", dipoles\n\t\tpointer = 0\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tmol.frag_dipole_sum[order] = np.sum(dipoles[pointer:pointer+len(mol.mbe_frags[order])], axis=0)\n\t\t\tpointer += len(mol.mbe_frags[order])\n\t\t#print ""mol.frag_dipole_sum[order] "", mol.frag_dipole_sum\n\t\tmol.MBE_Dipole()\n\t\tprint(""mbe dipole: "", mol.nn_dipole)\n\t\treturn\n\n\tdef NN_Energy_Force(self, mol, embed_=False):\n\t\ts = MSet()\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\ts.mols += mol.mbe_frags[order]\n\t\tt = time.time()\n\t\tenergies, forces =  self.nn_mbe.Eval_BPForceSet(s)\n\t\tprint(""actual evaluation cost:"", time.time() -t)\n\t\tenergies = np.asarray(energies)\n\t\t#print ""energies: "", energies\n\t\tpointer = 0\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\tmol.frag_force_sum[order] = np.zeros((mol.NAtoms(),3))\n\t\t\tfor i, mol_frag in enumerate(mol.mbe_frags[order]):\n\t\t\t\tmol.frag_force_sum[order][mol_frag.properties[""mbe_atom_index""]] += forces[pointer+i]\n\t\t\t#print ""energy of frags in order"", order\n\t\t\t#print energies[pointer:pointer+len(mol.mbe_frags[order])]\n\t\t\tmol.frag_energy_sum[order] = np.sum(energies[pointer:pointer+len(mol.mbe_frags[order])])\n\t\t\tif order == 1 or order ==2 :\n\t\t\t\tfor i, mol_frag in enumerate(mol.mbe_frags[order]):\n\t\t\t\t\tmol_frag.properties[""nn_energy""] = energies[pointer+i]\n\t\t\t\t\tmol_frag.properties[""nn_energy_grads""] = forces[pointer+i]\n\t\t\t\tpointer += len(mol.mbe_frags[order])\n\t\tif embed_:\n\t\t\tt = time.time()\n\t\t\tmol.MBE_Energy_Embed()\n\t\t\tprint(""MBE_Energy_Embed cost:"", t-time.time())\n\t\t\tt = time.time()\n\t\t\tmol.MBE_Force_Embed()\n\t\t\tprint(""MBE_Force_Embed cost:"", t-time.time())\n\t\telse:\n\t\t\tt = time.time()\n\t\t\tmol.MBE_Energy()\n\t\t\tprint(""MBE_Energy_Embed cost:"", t-time.time())\n\t\t\tt = time.time()\n\t\t\tmol.MBE_Force()\n\t\t\tprint(""MBE_Force_Embed cost:"", t-time.time())\n\t\t#print mol.properties[\'mbe_deri\']\n\t\t#print mol.nn_energy, mol.nn_force\n\t\treturn mol.nn_energy, mol.nn_force\n\n\tdef NN_Charge(self, mol, grads_= False):  # unit: au.  Dipole derived  from this charge has unit of au\n\t\ts = MSet()\n\t\tfor order in range (1, self.mbe_order+1):\n\t\t\ts.mols += mol.mbe_frags[order]\n\t\tif not grads_:\n\t\t\tdipoles, charges =  self.nn_dipole_mbe.Eval_BPDipole_2(s)\n\t\telse:\n\t\t\tdipoles, charges, gradient =  self.nn_dipole_mbe.Eval_BPDipoleGrad_2(s)\n\t\tpointer = 0\n\t\tfor order in range(1, self.mbe_order+1):\n\t\t\tmol.frag_charge_sum[order] = np.zeros(mol.NAtoms())\n\t\t\tcharge_charge_sum = 0.0\n\t\t\tfor i, mol_frag in enumerate(mol.mbe_frags[order]):\n\t\t\t\tmol.frag_charge_sum[order][mol_frag.properties[""mbe_atom_index""]] += charges[pointer+i]\n\t\t\t\tif order == 2:\n\t\t\t\t\tmol_frag.properties[""atom_charges""] = charges[pointer+i]\n\t\t\t\t\tif grads_:\n\t\t\t\t\t\tmol_frag.properties[""atom_charges_grads""] = gradient[pointer+i]\n\t\t\t\t\t\tpointer += len(mol.mbe_frags[order])\n\t\tt = time.time()\n\t\tmol.MBE_Charge()\n\t\tprint(""MBE_Charge cost:"", time.time() -t)\n\t\t#mol.properties[\'embedded_charge\'] =  mol.properties[\'embedded_charge\']\n\t\t#print ""charge dipole: "", Dipole(mol.coords, mol.nn_charge)\n\t\t#for i, mol_frag in enumerate(mol.mbe_frags[1]):\n\t\t#\tmol_frag.properties[""atom_charges""] = np.copy(mol.properties[\'embedded_charge\'][mol_frag.properties[""mbe_atom_index""]])\n\t\t#charge_charge_sum = 0.0\n\t\t#for i in range (0, len(mol.mbe_frags[1])):\n\t\t#\tfor j  in range (i+1, len(mol.mbe_frags[1])):\n\t\t#\t\tcharge_charge_sum += ChargeCharge(mol.mbe_frags[1][i], mol.mbe_frags[1][j])\n\t\treturn\tmol.nn_charge\n'"
TensorMol/Math/BFGS.py,0,"b'""""""\nTODO:\n\tSystematic comparison of BFGS vs CG etc.\n\tConsistent solver organization & interface. (CG,BFGS,DIIS etc. )\n""""""\nfrom __future__ import absolute_import\nfrom ..PhysicalData import *\n\nclass SteepestDescent:\n\tdef __init__(self, ForceAndEnergy_,x0_):\n\t\t""""""\n\t\tThe desired interface for a solver in tensormol.\n\n\t\tArgs:\n\t\t\tForceAndEnergy_: a routine which returns energy, force.\n\t\t\tx0_: a initial vector\n\t\t""""""\n\t\tself.step = 0\n\t\tself.x0=x0_.copy()\n\t\tif (len(self.x0.shape)==2):\n\t\t\tself.natom = self.x0.shape[0]\n\t\telse:\n\t\t\tself.natom = self.x0.shape[0]*self.x0.shape[1]\n\t\tself.EForce = ForceAndEnergy_ # Used for line-search.\n\t\treturn\n\tdef __call__(self, new_vec_):\n\t\t""""""\n\t\tIterate\n\n\t\tArgs:\n\t\t\tnew_vec_: Point at which to minimize gradients\n\t\tReturns:\n\t\t\tNext point, energy, and gradient.\n\t\t""""""\n\t\te,g = self.EForce(new_vec_)\n\t\tself.step += 1\n\t\treturn new_vec_ + PARAMS[""SDStep""]*g, e, g\n\nclass VerletOptimizer:\n\tdef __init__(self, ForceAndEnergy_,x0_):\n\t\t""""""\n\t\tBased on Hankelman\'s momentum optimizer.\n\n\t\tArgs:\n\t\t\tForceAndEnergy_: a routine which returns energy, force.\n\t\t\tx0_: a initial vector\n\t\t""""""\n\t\tself.step = 0\n\t\tself.x0=x0_.copy()\n\t\tself.v=np.zeros(x0_.shape)\n\t\tself.a=np.zeros(x0_.shape)\n\t\tself.dt = 0.1\n\t\tif (len(self.x0.shape)==2):\n\t\t\tself.natom = self.x0.shape[0]\n\t\telse:\n\t\t\tself.natom = self.x0.shape[0]*self.x0.shape[1]\n\t\tself.EForce = ForceAndEnergy_ # Used for line-search.\n\t\treturn\n\tdef __call__(self, x_):\n\t\t""""""\n\t\tIterate\n\n\t\tArgs:\n\t\t\tnew_vec_: Point at which to minimize gradients\n\t\tReturns:\n\t\t\tNext point, energy, and gradient.\n\t\t""""""\n\t\tx = x_ + self.v*self.dt + (1./2.)*self.a*self.dt*self.dt\n\t\te, f_x_ = self.EForce(x)\n\t\t#a = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/m_) # m^2/s^2 => A^2/Fs^2\n\t\tself.v += (1./2.)*(self.a+f_x_)*self.dt\n\t\tif (np.sum(self.v*f_x_)<0):\n\t\t\tself.v *= 0.0\n\t\t\tself.a *= 0.0\n\t\tself.step += 1\n\t\treturn x, e, f_x_\n\nclass BFGS(SteepestDescent):\n\tdef __init__(self, ForceAndEnergy_,x0_):\n\t\t""""""\n\t\tSimplest Possible minimizing BFGS\n\n\t\tArgs:\n\t\t\tForceAndEnergy_: a routine which returns energy, force.\n\t\t\tx0_: a initial vector\n\t\t""""""\n\t\tself.m_max = PARAMS[""MaxBFGS""]\n\t\tself.step = 0\n\t\tself.x0=x0_.copy()\n\t\tif (len(self.x0.shape)==2):\n\t\t\tself.natom = self.x0.shape[0]\n\t\telse:\n\t\t\tself.natom = self.x0.shape[0]*self.x0.shape[1]\n\t\tself.EForce = ForceAndEnergy_ # Used for line-search.\n\t\tself.R_Hist = np.zeros(([self.m_max]+list(self.x0.shape)))\n\t\tself.F_Hist = np.zeros(([self.m_max]+list(self.x0.shape)))\n\t\treturn\n\tdef BFGSstep(self, new_vec_, new_residual_):\n\t\tif self.step < self.m_max:\n\t\t\tself.R_Hist[self.step] = new_vec_.copy()\n\t\t\tself.F_Hist[self.step] = new_residual_.copy()\n\t\telse:\n\t\t\tself.R_Hist = np.roll(self.R_Hist,-1,axis=0)\n\t\t\tself.F_Hist = np.roll(self.F_Hist,-1,axis=0)\n\t\t\tself.R_Hist[-1] = new_vec_.copy()\n\t\t\tself.F_Hist[-1] = new_residual_.copy()\n\t\t# Quasi Newton L-BFGS global step.\n\t\tq = new_residual_.copy()\n\t\ta = np.zeros(min(self.m_max,self.step))\n\t\tfor i in range(min(self.m_max,self.step)-1, 0, -1):\n\t\t\ts = self.R_Hist[i] - self.R_Hist[i-1]\n\t\t\ty = self.F_Hist[i] - self.F_Hist[i-1]\n\t\t\trho = 1.0/np.sum(y*s)\n\t\t\ta[i] = rho * np.sum(s*q)\n\t\t\t#print ""a "",a\n\t\t\tq -= a[i]*y\n\t\tif self.step < 1:\n\t\t\tH=1.0\n\t\telse:\n\t\t\tnum = min(self.m_max-1,self.step)\n\t\t\tv1 = (self.R_Hist[num] - self.R_Hist[num-1])\n\t\t\tv2 = (self.F_Hist[num] - self.F_Hist[num-1])\n\t\t\tH = np.sum(v1*v2)/np.sum(v2*v2)\n\t\t\tif (abs(H)<0.001):  # Switch to SD.\n\t\t\t\tself.step += 1\n\t\t\t\treturn -0.001*new_residual_\n\t\tz = H*q\n\t\tfor i in range(1,min(self.m_max,self.step)):\n\t\t\ts = (self.R_Hist[i] - self.R_Hist[i-1])\n\t\t\ty = (self.F_Hist[i] - self.F_Hist[i-1])\n\t\t\trho = 1.0/np.sum(y*s)\n\t\t\tbeta = rho*np.sum(y*z)\n\t\t\t#print ""a-b: "", (a-beta)\n\t\t\tz += s*(a[i]-beta)\n\t\tself.step += 1\n\t\treturn -1.0*z\n\tdef __call__(self, new_vec_):\n\t\t""""""\n\t\tIterate BFGS\n\n\t\tArgs:\n\t\t\tnew_vec_: Point at which to minimize gradients\n\t\tReturns:\n\t\t\tNext point, energy, and gradient.\n\t\t""""""\n\t\te,g = self.EForce(new_vec_)\n\t\tz = self.BFGSstep(new_vec_, g)\n\t\treturn new_vec_ + 0.005*z, e, g\n\nclass BFGS_WithLinesearch(BFGS):\n\tdef __init__(self, ForceAndEnergy_, x0_ ):\n\t\t""""""\n\t\tSimplest Possible BFGS\n\n\t\tArgs:\n\t\t\tForceAndEnergy_: a routine which returns energy, force.\n\t\t\tx0_: a initial vector\n\t\t""""""\n\t\tBFGS.__init__(self,ForceAndEnergy_,x0_)\n\t\tself.alpha = PARAMS[""GSSearchAlpha""]\n\t\tself.Energy = lambda x: self.EForce(x,False)\n\t\treturn\n\tdef LineSearch(self, x0_, p_, thresh = 0.0001):\n\t\t\'\'\'\n\t\tgolden section search to find the minimum of f on [a,b]\n\n\t\tArgs:\n\t\t\tf_: a function which returns energy.\n\t\t\tx0_: Origin of the search.\n\t\t\tp_: search direction.\n\n\t\tReturns:\n\t\t\tx: coordinates which minimize along this search direction.\n\t\t\'\'\'\n\t\tk=0\n\t\trmsdist = 10.0\n\t\ta = x0_\n\t\tb = x0_ + self.alpha*p_\n\t\tc = b - (b - a) / GOLDENRATIO\n\t\td = a + (b - a) / GOLDENRATIO\n\t\tfa = self.Energy(a)\n\t\tfb = self.Energy(b)\n\t\tfc = self.Energy(c)\n\t\tfd = self.Energy(d)\n\t\twhile (rmsdist > thresh):\n\t\t\tif (fa < fc and fa < fd and fa < fb):\n\t\t\t\t#print fa,fc,fd,fb\n\t\t\t\t#print RmsForce(fpa), RmsForce(fpc), RmsForce(fpd), RmsForce(fpb)\n\t\t\t\tprint(""Line Search: Overstep"")\n\t\t\t\tif (self.alpha > 0.0001):\n\t\t\t\t\tself.alpha /= 1.71\n\t\t\t\telse:\n\t\t\t\t\tprint(""Keeping step"")\n\t\t\t\t\treturn a\n\t\t\t\ta = x0_\n\t\t\t\tb = x0_ + self.alpha*p_\n\t\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\t\tfa = self.Energy(a)\n\t\t\t\tfb = self.Energy(b)\n\t\t\t\tfc = self.Energy(c)\n\t\t\t\tfd = self.Energy(d)\n\t\t\telif (fb < fc and fb < fd and fb < fa):\n\t\t\t\t#print fa,fc,fd,fb\n\t\t\t\t#print RmsForce(fpa), RmsForce(fpc), RmsForce(fpd), RmsForce(fpb)\n\t\t\t\tprint(""Line Search: Understep"")\n\t\t\t\tif (self.alpha < 100.0):\n\t\t\t\t\tself.alpha *= 1.7\n\t\t\t\ta = x0_\n\t\t\t\tb = x0_ + self.alpha*p_\n\t\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\t\tfa = self.Energy(a)\n\t\t\t\tfb = self.Energy(b)\n\t\t\t\tfc = self.Energy(c)\n\t\t\t\tfd = self.Energy(d)\n\t\t\telif fc < fd:\n\t\t\t\tb = d\n\t\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\t\tfb = fd\n\t\t\t\tfc = self.Energy(c)\n\t\t\t\tfd = self.Energy(d)\n\t\t\telse:\n\t\t\t\ta = c\n\t\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\t\tfa = fc\n\t\t\t\tfc = self.Energy(c)\n\t\t\t\tfd = self.Energy(d)\n\t\t\trmsdist = np.sum(np.linalg.norm(a-b,axis=1))/self.natom\n\t\t\tk+=1\n\t\treturn (b + a) / 2\n\tdef __call__(self, new_vec_):\n\t\t""""""\n\t\tIterate BFGS\n\n\t\tArgs:\n\t\t\tnew_vec_: Point at which to minimize gradients\n\t\tReturns:\n\t\t\tNext point, energy, and gradient.\n\t\t""""""\n\t\te,g = self.EForce(new_vec_)\n\t\tz = self.BFGSstep(new_vec_, g)\n\t\tnew_vec = self.LineSearch(new_vec_, z)\n\t\treturn new_vec, e, g\n'"
TensorMol/Math/Basis.py,2,"b'""""""\nLoads and handles basis sets for embeddings\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport os, sys, re, random, math, copy\nimport numpy as np\nimport sys\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\nfrom ..Math import LinearOperations, DigestMol, Digest, Opt, Ipecac\n\nclass Basis:\n\tdef __init__(self, Name_ = None):\n\t\tself.path = ""./basis/""\n\t\tself.name = Name_\n\t\tself.params = None\n\n\tdef Load(self, filename=None):\n\t\tprint(""Unpickling Basis Set"")\n\t\tif filename == None:\n\t\t\tfilename = self.name\n\t\tf = open(self.path+filename+"".tmb"",""rb"")\n\t\ttmp=pickle.load(f)\n\t\tself.__dict__.update(tmp)\n\t\tf.close()\n\t\treturn\n\n\tdef Save(self):\n\t\tif filename == None:\n\t\t\tfilename = self.name\n\t\tf=open(self.path+self.name+""_""+self.dig.name+"".tmb"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\nclass Basis_GauSH(Basis):\n\tdef __init__(self, Name_ = None):\n\t\tBasis.__init__(self, Name_)\n\t\tself.type = ""GauSH""\n\t\tself.RBFS = np.tile(np.array([[0.1, 0.156787], [0.3, 0.3], [0.5, 0.5], [0.7, 0.7], [1.3, 1.3], [2.2, 2.4],\n\t\t\t\t\t\t\t\t\t\t[4.4, 2.4], [6.6, 2.4], [8.8, 2.4], [11., 2.4], [13.2,2.4], [15.4, 2.4]]), (10,1,1))\n\t\treturn\n\n\tdef Orthogonalize(self):\n\t\tfrom TensorMol.LinearOperations import MatrixPower\n\t\tS_Rad = MolEmb.Overlap_RBFS(PARAMS, self.RBFS)\n\t\tself.SRBF = np.zeros((self.RBFS.shape[0],PARAMS[""SH_NRAD""],PARAMS[""SH_NRAD""]))\n\t\tfor i in range(S_Rad.shape[0]):\n\t\t\tself.SRBF[i] = MatrixPower(S_Rad[i],-1./2)\n'"
TensorMol/Math/DIIS.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nclass DIIS:\n\tdef __init__(self, ForceAndEnergy_, x0_=None):\n\t\t""""""\n\t\tSimplest Possible DIIS\n\t\t""""""\n\t\tself.m_max = PARAMS[""DiisSize""]\n\t\tself.n_now = 0\n\t\tself.v_shp = None\n\t\tself.Vs = None\n\t\tself.Rs = None\n\t\tself.M = np.zeros((self.m_max+1,self.m_max+1))\n\t\tself.S = np.zeros((self.m_max+1,self.m_max+1)) # Overlap matrix.\n\t\tself.EForce = ForceAndEnergy_\n\t\treturn\n\tdef __call__(self, new_vec_):\n\t\t""""""\n\t\tIterate BFGS\n\n\t\tArgs:\n\t\t\tnew_vec_: Point at which to minimize gradients\n\t\tReturns:\n\t\t\tNext point, energy, and gradient.\n\t\t""""""\n\t\te,g = self.EForce(new_vec_)\n\t\treturn self.NextStep(new_vec_, g), e, g\n\tdef NextStep(self, new_vec_, new_residual_):\n\t\tif (self.n_now == 0):\n\t\t\tself.v_shp = new_vec_.shape\n\t\t\tself.Vs = np.zeros([self.m_max]+list(self.v_shp))\n\t\t\tself.Rs = np.zeros([self.m_max]+list(self.v_shp))\n\t\tif (self.n_now<self.m_max):\n\t\t\tself.Vs[self.n_now] = new_vec_.copy()\n\t\t\tself.Rs[self.n_now] = new_residual_.copy()\n\t\t\tfor i in range(self.n_now):\n\t\t\t\tself.S[i,self.n_now] = np.dot(self.Rs[i].flatten(),new_residual_.flatten())\n\t\t\t\tself.S[self.n_now,i] = self.S[i,self.n_now]\n\t\t\tself.S[self.n_now,self.n_now] = np.dot(new_residual_.flatten(),new_residual_.flatten())\n\t\t\tself.n_now += 1\n\t\telse:\n\t\t\tself.Vs = np.roll(self.Vs,-1,axis=0)\n\t\t\tself.Rs = np.roll(self.Rs,-1,axis=0)\n\t\t\tself.S = np.roll(np.roll(self.S,-1,axis=0),-1,axis=1)\n\t\t\tself.Vs[-1] = new_vec_.copy()\n\t\t\tself.Rs[-1] = new_residual_.copy()\n\t\t\tfor i in range(self.n_now):\n\t\t\t\tself.S[i,self.n_now] = np.dot(self.Rs[i].flatten(),new_residual_.flatten())\n\t\t\t\tself.S[self.n_now,i] = self.S[i,self.n_now]\n\t\t\tself.S[self.n_now,self.n_now] = np.dot(new_residual_.flatten(),new_residual_.flatten())\n\t\tif (self.n_now<2):\n\t\t\treturn new_vec_ + 0.02*new_residual_\n\t\t#print ""S"", self.S[:self.n_now,:self.n_now]\n\t\t#print ""Vs: "", self.Vs\n\t\t#print ""Rs: "", self.Rs\n\t\t# Build the DIIS matrix which has dimension n_now + 1\n\t\tself.M *= 0.0\n\t\tscale_factor = 1.0#/self.S[0,0]\n\t\tfor i in range(self.n_now):\n\t\t\tfor j in range(self.n_now):\n\t\t\t\tself.M[i,j] = self.S[i,j] * scale_factor\n\t\tself.M[self.n_now,:] = -1.0\n\t\tself.M[:,self.n_now] = -1.0\n\t\tself.M[self.n_now,self.n_now] = 0.0\n\t\t#print self.M[:self.n_now+1,:self.n_now+1]\n\t\t# Solve the DIIS problem.\n\t\ttmp = np.zeros(self.n_now+1)\n\t\ttmp[self.n_now]=-1.0\n\t\tU, s, V = np.linalg.svd(self.M[:self.n_now+1,:self.n_now+1]) #M=u * np.diag(s) * v,\n\t\t# Filter small values.\n\t\t#print ""s"", s\n\t\tfor i in range(len(s)):\n\t\t\tif (s[i]!=0.0 and abs(s[i])>0.0000001):\n\t\t\t\ts[i]=1.0/s[i]\n\t\t\telse:\n\t\t\t\ts[i] = 0.0\n\t\t#print ""Diis inv: "", (U*s*V)\n\t\tnext_coeffs = np.dot(np.dot(np.dot(U,np.diag(s)),V),tmp)\n\t\tprint(""DIIS COEFFs: "", next_coeffs)\n\t\t#next_coeffs = np.dot(np.linalg.inv(self.M[:self.n_now+1,:self.n_now+1]),tmp)\n\t\t#print next_coeffs.shape\n\t\ttore = np.zeros(new_vec_.shape)\n\t\tfor i in range(self.n_now):\n\t\t\ttore += self.Vs[i]*next_coeffs[i]\n\t\treturn tore\n'"
TensorMol/Math/EmbOpt.py,0,"b'""""""\n Embedding Optimizer\n This file contains routines to optimize an embedding\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Mol import *\nfrom ..Util import *\nfrom ..Containers.TensorData import *\nfrom ..TFNetworks.TFInstance import *\nfrom scipy import optimize\nimport os, sys, re, random, math, copy\nimport numpy as np\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\nfrom ..Math.LinearOperations import *\nfrom ..Containers.DigestMol import *\nfrom ..Containers.Digest import *\nfrom ..Simulations.Opt import *\nfrom ..Math.Ipecac import *\n\nclass EmbeddingOptimizer:\n\t""""""\n\tProvides an objective function to optimize an embedding, maximizing the reversibility of the embedding, and the distance the embedding predicts between molecules which are not equivalent in their geometry or stoiciometry.\n\t""""""\n\tdef __init__(self, method_, set_, dig_, OptParam_, OType_ = None, Elements_ = None):\n\t\tprint(""Will produce an objective function to optimize basis parameters, and then optimize it"")\n\t\tself.method = method_\n\t\tself.set = set_\n\t\tself.dig = dig_\n\t\tself.OptParam = OptParam_\n\t\tLOGGER.info(""Optimizing %s part of %s based off of %s"", self.OptParam, self.dig.name, self.method)\n\t\tif self.method == ""Ipecac"":\n\t\t\tself.ip = Ipecac.Ipecac(self.set, self.dig, eles_=[1,6,7,8])\n\t\t\tself.Mols = []\n\t\t\tself.DistortMols=[]\n\t\t\tself.DesiredEmbs=[]\n\t\t\tfor mol in self.set.mols:\n\t\t\t\tself.Mols.append(copy.deepcopy(mol))\n\t\t\t\tself.Mols[-1].BuildDistanceMatrix()\n\t\t\t\t# for i in range(9):\n\t\t\t\t# \tself.Mols.append(copy.deepcopy(mol))\n\t\t\t\t# \tself.Mols[-1].Distort(0.15)\n\t\t\t\t# \tself.Mols[-1].BuildDistanceMatrix()\n\t\t\tfor mol in self.Mols:\n\t\t\t\tself.DistortMols.append(copy.deepcopy(mol))\n\t\t\t\tself.DistortMols[-1].Distort(0.15)\n\t\t\t\tself.DistortMols[-1].BuildDistanceMatrix()\n\t\t\tLOGGER.info(""Using %d unique geometries"", len(self.Mols))\n\t\telif self.method == ""KRR"":\n\t\t\tself.OType = OType_\n\t\t\tif self.OType == None:\n\t\t\t\traise Exception(""KRR optimization requires setting OType_ for the EmbeddingOptimizer."")\n\t\t\tself.elements = Elements_\n\t\t\tif self.elements == None:\n\t\t\t\traise Exception(""KRR optimization requires setting Elements_ for the EmbeddingOptimizer."")\n\t\t\tself.TreatedAtoms = self.set.AtomTypes()\n\t\t\tprint(""Optimizing based off "", self.OType, "" using elements"")\n\t\treturn\n\n\tdef SetBasisParams(self,basisParams_):\n\t\tif self.dig.name == ""GauSH"":\n\t\t\tif self.OptParam == ""radial"":\n\t\t\t\tPARAMS[""RBFS""] = basisParams_[:PARAMS[""SH_NRAD""]*2].reshape(PARAMS[""SH_NRAD""],2).copy()\n\t\t\telif self.OptParam == ""atomic"":\n\t\t\t\tPARAMS[""ANES""][[0,5,6,7]] = basisParams_[[0,1,2,3]].copy()\n\t\telif self.digname == ""ANI1_Sym"":\n\t\t\traise Exception(""Not yet implemented for ANI1"")\n\t\tS_Rad = MolEmb.Overlap_RBF(PARAMS)\n\t\tPARAMS[""SRBF""] = MatrixPower(S_Rad,-1./2)\n\t\tprint(""Eigenvalue Overlap Error: "", (1/np.amin(np.linalg.eigvals(S_Rad)))/1.e6)\n\t\treturn np.abs((1/np.amin(np.linalg.eigvals(S_Rad)))/1.e6)\n\t\t#return 0.0\n\n\tdef Ipecac_Objective(self,basisParams_):\n\t\t""""""\n\t\tResets the parameters. Builds the overlap if neccesary. Resets the desired embeddings. Reverses the distorted set and computes an error.\n\t\t""""""\n\t\tberror = self.SetBasisParams(basisParams_)\n\t\tself.SetEmbeddings()\n\t\tresultmols = []\n\t\tfor i in range(len(self.DistortMols)):\n\t\t\tm = copy.deepcopy(self.DistortMols[i])\n\t\t\temb = self.DesiredEmbs[i]\n\t\t\tresultmols.append(self.ip.ReverseAtomwiseEmbedding(emb, m.atoms, guess_=m.coords, GdDistMatrix=self.Mols[i].DistMatrix))\n\t\t# Compute the various parts of the error.\n\t\tSelfDistances = 0.0\n\t\tOtherDistances = 0.0\n\t\tfor i in range(len(self.DistortMols)):\n\t\t\tSelfDistances += self.Mols[i].rms_inv(resultmols[i])\n\t\t\tprint(SelfDistances)\n\t\t\tfor j in range(len(self.DistortMols)):\n\t\t\t\tif (i != j and len(self.Mols[i].atoms)==len(self.Mols[j].atoms)):\n\t\t\t\t\tOtherDistances += np.exp(-1.0*self.Mols[i].rms_inv(resultmols[j]))\n\t\tLOGGER.info(""Using params_: %s"", basisParams_)\n\t\tLOGGER.info(""Got Error: %.6f"", berror+SelfDistances+OtherDistances)\n\t\treturn berror+SelfDistances+OtherDistances\n\n\tdef KRR_Objective(self, basisParams_):\n\t\t""""""\n\t\tResets the parameters. Builds the overlap if neccesary. Resets the desired embeddings. Reverses the distorted set and computes an error.\n\t\t""""""\n\t\tberror = self.SetBasisParams(basisParams_)\n\t\tsqerr = 0.0\n\t\ttset = TensorData(self.set,self.dig)\n\t\ttset.BuildTrainMolwise(self.set.name+""_BasisOpt"")\n\t\tfor ele in self.elements:\n\t\t\tele_inst = Instance_KRR(tset, ele, None)\n\t\t\tsqerr += (ele_inst.basis_opt_run())**2\n\t\tLOGGER.info(""Basis Params: %s"", basisParams_)\n\t\tLOGGER.info(""SqError: %f"", sqerr+berror)\n\t\treturn sqerr+berror\n\n\tdef PerformOptimization(self):\n\t\tprm0 = PARAMS[""RBFS""][:PARAMS[""SH_NRAD""]].flatten()\n\t\t# prm0 = np.array((PARAMS[""ANES""][0], PARAMS[""ANES""][5], PARAMS[""ANES""][6], PARAMS[""ANES""][7]))\n\t\tprint(prm0)\n\n\t\tprint(""Optimizing RBFS."")\n\t\tif (self.method == ""Ipecac""):\n\t\t\tobj = lambda x: self.Ipecac_Objective(x)\n\t\telif (self.method == ""KRR""):\n\t\t\tobj = lambda x: self.KRR_Objective(x)\n\t\tres=optimize.minimize(obj, prm0, method=\'COBYLA\', tol=1e-8, options={\'disp\':True, \'rhobeg\':0.1})\n\t\tLOGGER.info(""Opt complete: %s"", res.message)\n\t\tLOGGER.info(""Optimal Basis Parameters: %s"", res.x)\n\t\treturn\n\n\tdef BasinHopping(self):\n\t\tprm0 = PARAMS[""RBFS""][:PARAMS[""SH_NRAD""]].flatten()\n\t\t# prm0 = np.array((PARAMS[""ANES""][0], PARAMS[""ANES""][5], PARAMS[""ANES""][6], PARAMS[""ANES""][7]))\n\t\tprint(prm0)\n\t\tprint(""Optimizing RBFS."")\n\t\tif (self.method == ""Ipecac""):\n\t\t\tobj = lambda x: self.Ipecac_Objective(x)\n\t\telif (self.method == ""KRR""):\n\t\t\tobj = lambda x: self.KRR_Objective(x)\n\t\tmin_kwargs = {""method"": ""COBYLA"", ""options"": ""{\'rhobeg\':0.1}""}\n\t\tret=optimize.basinhopping(obj, prm0, minimizer_kwargs=min_kwargs, niter=100)\n\t\tLOGGER.info(""Optimal Basis Parameters: %s"", res.x)\n\t\treturn\n\n\tdef SetEmbeddings(self):\n\t\tself.DesiredEmbs = []\n\t\tfor mol in self.Mols:\n\t\t\tself.DesiredEmbs.append(self.dig.Emb(mol,MakeOutputs=False))\n'"
TensorMol/Math/Grids.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\nimport random\nfrom pyscf import scf\nfrom pyscf import gto\nfrom pyscf import dft\nimport math\nfrom math import pi as Pi\n\n#\n# The H@0 atom is for fitting the potential near equilibrium and it\'s small...\n#\nif (HAS_GRIDS):\n\tATOM_BASIS={\'H@0\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  15.0\t\t\t\t\t1.0\n\t\t\t\'\'\')}\n\tTOTAL_SENSORY_BASIS={\'C\': gto.basis.parse(\'\'\'\n\tC    S\n\t\t  1.0\t\t\t\t\t1.0000000\n\tC    S\n\t\t  0.5\t\t\t\t\t1.0000000\n\tC    S\n\t\t  0.1\t\t\t\t\t1.0000000\n\tC    S\n\t\t  0.02\t\t\t\t\t1.0000000\n\tC    S\n\t\t  0.005\t\t\t\t\t1.0000000\n\tC    P\n\t\t  1.0\t\t\t\t\t1.0000000\n\tC    P\n\t\t  0.5\t\t\t\t\t1.0000000\n\tC    P\n\t\t  0.1\t\t\t\t\t1.0000000\n\tC    P\n\t\t  0.02\t\t\t\t\t1.0000000\n\tC    P\n\t\t  0.005\t\t\t\t\t1.0000000\n\tC    D\n\t\t  0.5\t\t\t\t\t1.0000000\n\tC    D\n\t\t  0.05\t\t\t\t\t1.0000000\n\tC    D\n\t\t  0.01\t\t\t\t\t1.0000000\n\tC    D\n\t\t  0.004\t\t\t\t\t1.0000000\n\tC    F\n\t\t  0.2\t\t\t\t\t1.0000000\n\tC    F\n\t\t  0.02\t\t\t\t\t1.0000000\n\tC    F\n\t\t  0.004\t\t\t\t\t1.0000000\n\tC    G\n\t\t  0.1\t\t\t\t\t1.0000000\n\tC    G\n\t\t  0.02\t\t\t\t\t1.0000000\n\tC    G\n\t\t  0.004\t\t\t\t\t1.0000000\n\tC    H\n\t\t  0.1\t\t\t\t\t1.0000000\n\tC    H\n\t\t  0.02\t\t\t\t\t1.0000000\n\tC    H\n\t\t  0.004\t\t\t\t\t1.0000000\n\tC    I\n\t\t  0.02\t\t\t\t\t1.0000000\n\tC    I\n\t\t  0.004\t\t\t\t\t1.0000000\n\t\t  \'\'\'),\'H@0\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  1.5\t\t\t\t\t1.0\n\t\t\t\'\'\'),\'H@1\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  3              1.0\n\t\t\t\'\'\'),\'H@2\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  2.6\t\t\t\t1.0\n\t\t\t\'\'\'),\'H@3\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  2.34\t\t\t\t\t1.0\n\t\t\t\'\'\'),\'H@4\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  2\t\t\t\t1.0\n\t\t\t\'\'\'),\'H@5\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  1.7\t\t\t\t1.0\n\t\t\t\'\'\'),\'H@6\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  1.3\t\t\t\t\t1.0\n\t\t\t\'\'\'),\'H@7\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  1\t\t\t\t\t1.0\n\t\t\t\'\'\'),\'H@8\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  .6666             1.0\n\t\t\t\'\'\'),\'H@9\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  .3333             1.0\n\t\t\t\'\'\'),\'N@0\': gto.basis.parse(\'\'\'\n\tH    S\n\t\t  1.0             -1.0\n\t\t\t\'\'\')\n\t\t\t}\n\nclass Grids:\n\t""""""\n\t\tPrecomputes and stores orthogonalized grids for embedding molecules and potentials.\n\t\tself,NGau_=6, GridRange_=0.6, NPts_=45 is a good choice for a 1 Angstrom potential fit\n\n\t\tAlso now has a few\n\t""""""\n\tdef __init__(self,NGau_=6, GridRange_=0.8, NPts_=30):\n\t\t# Coulomb Fitting parameters\n\t\tself.Spherical = False\n\t\t# Cartesian Embedding parameters.\n\t\tself.GridRange = GridRange_\n\t\tself.NGau=NGau_ # Number of gaussians in each direction.\n\t\tself.NPts=NPts_ # Number of gaussians in each direction.\n\t\tself.NGau3=NGau_*NGau_*NGau_ # Number of gaussians in each direction.\n\t\tself.NPts3=NPts_*NPts_*NPts_\n\t\tself.OBFs=None # matrix of orthogonal basis functions stored on grid, once.\n\t\tself.GauGrid=None\n\t\tself.Grid=None\n\t\tself.dx = 0.0\n\t\tself.dy = 0.0\n\t\tself.dz = 0.0\n\n\t\tself.SH_S = None\n\t\tself.SH_Sinv = None\n\t\tself.SH_C = None\n\n\t\t#\n\t\t# These are for embedding molecular environments.\n\t\t#\n\t\tself.SenseRange = 3.5\n\t\tself.NSense = self.NGau\n\t\tif (self.Spherical):\n\t\t\tself.SenseRange = 12.0\n\t\t\tself.NSense = None\n\t\tself.SenseGrid = None\n\t\tself.SenseS = None\n\t\tself.SenseSinv = None\n\t\tself.Isometries = None\n\t\tself.InvIsometries = None\n\t\tself.IsometryRelabelings = None\n\t\treturn\n\n\tdef\tMyGrid(self):\n\t\tif (self.Grid==None):\n\t\t\tself.Populate()\n\t\treturn self.Grid\n\n\tdef\tBasisRelabelingUnderTransformation(self,trans,i):\n\t\tv=self.GauGrid[i]\n\t\tvp=np.dot(trans,v)\n\t\tdgr = np.array(map(lambda x: np.linalg.norm(x-vp),self.GauGrid))\n\t\tposs = np.where(dgr<0.000001)[0][0]\n\t\treturn poss\n\n\tdef BuildIsometries(self):\n\t\tself.Isometries = OctahedralOperations()\n\t\tself.InvIsometries = OctahedralOperations()\n\t\tself.IsometryRelabelings = np.zeros(shape=(len(self.Isometries),len(self.GauGrid)),dtype=np.int64)\n\t\tfor i in range(len(self.Isometries)):\n\t\t\top=self.Isometries[i]\n\t\t\ti0=range(len(self.GauGrid))\n\t\t\tself.InvIsometries[i] = np.linalg.inv(op)\n\t\t\tself.IsometryRelabelings[i]=np.array(map(lambda x: self.BasisRelabelingUnderTransformation(op,x), i0))\n\t\t\t# print ip\n\t\t\t# Check that it\'s an isometry.\n\t\t\tif (i0 != sorted(self.IsometryRelabelings[i])):\n\t\t\t\tprint(""Not an isometry :( "", i0,self.IsometryRelabelings[i])\n\t\t\t\traise Exception(""Bad Isometry"")\n\t\treturn\n\n\tdef Populate(self):\n\t\tprint(""Populating Grids... "")\n\t\t#\n\t\t# Populate output Bases\n\t\t#\n\t\tself.GauGrid = MakeUniform([0,0,0],self.GridRange*.8,self.NGau)\n\t\tself.Grid = MakeUniform([0,0,0],self.GridRange,self.NPts)\n\t\tself.dx=(np.max(self.Grid[:,0])-np.min(self.Grid[:,0]))/self.NPts*1.889725989\n\t\tself.dy=(np.max(self.Grid[:,1])-np.min(self.Grid[:,1]))/self.NPts*1.889725989\n\t\tself.dz=(np.max(self.Grid[:,2])-np.min(self.Grid[:,2]))/self.NPts*1.889725989\n\t\treturn\n\n\t\tmol = gto.Mole()\n\t\tmol.atom = \'\'.join([""H@0 ""+str(self.GauGrid[iii,0])+"" ""+str(self.GauGrid[iii,1])+"" ""+str(self.GauGrid[iii,2])+"";"" for iii in range(len(self.GauGrid))])[:-1]\n\t\tif (self.NGau3%2==0):\n\t\t\tmol.spin = 0\n\t\telse:\n\t\t\tmol.spin = 1\n\t\tif (ATOM_BASIS == None):\n\t\t\traise(""missing ATOM_BASIS"")\n\t\tmol.basis = ATOM_BASIS\n\t\ttry:\n\t\t\tmol.build()\n\t\texcept Exception as Ex:\n\t\t\tprint(mol.atom)\n\t\t\traise Ex\n\t\t# All this shit could be Pre-Computed...\n\t\t# Really any grid could be used.\n\t\torbs=gto.eval_gto(\'GTOval_sph\',mol._atm,mol._bas,mol._env,self.Grid*1.889725989)\n\t\tnbas=orbs.shape[1]\n\t\tif (nbas!=self.NGau3):\n\t\t\traise Exception(""insanity"")\n\t\tS=mol.intor(\'cint1e_ovlp_sph\')\n\t\t#S = np.zeros(shape=(nbas,nbas))\n\t\t#for i in range(nbas):\n\t\t#\tfor j in range(nbas):\n\t\t#\t\tS[i,j] += np.sum(orbs[:,i]*orbs[:,j])\n\t\tC = MatrixPower(S,-0.5)\n\t\tif (0):\n\t\t\tfor i in range(nbas):\n\t\t\t\tCM = np.dot(self.Grid.T,orbs[:,i])\n\t\t\t\tprint(""Centers of Mass, i"", np.dot(self.Grid.T,orbs[:,i]*orbs[:,i])*self.dx*self.dy*self.dz, self.GauGrid[i])\n\t\t\t\tRsq = np.array(map(np.linalg.norm,self.Grid-CM))\n\t\t\t\tprint(""Rsq of Mass, i"", np.sqrt(np.dot(Rsq,orbs[:,i]*orbs[:,i]))*self.dx*self.dy*self.dz)\n\t\t\t\tfor j in range(nbas):\n\t\t\t\t\tprint(""Normalization of grid i."", np.sum(orbs[:,i]*orbs[:,j])*self.dx*self.dy*self.dz)\n\t\tself.OBFs = np.zeros(shape=(self.NGau3,self.NPts3))\n\t\tfor i in range(nbas):\n\t\t\tfor j in range(nbas):\n\t\t\t\tself.OBFs[i,:] += (C.T[i,j]*orbs[:,j]).T\n\t\t# Populate Sensory bases.\n\t\tself.PopulateSense()\n\t\tif (not self.Spherical):\n\t\t\tself.BuildIsometries()\n\t\t\tprint(""Using "", len(self.Isometries), "" isometries."")\n\t\tprint(""Grid storage cost: "",self.OBFs.size*64/1024/1024, ""Mb"")\n\t\t#for i in range(nbas):\n\t\t#\tGridstoRaw(orbs[:,i]*orbs[:,i],self.NPts,""BF""+str(i))\n\t\t#\tGridstoRaw(self.OBFs[i,:]*self.OBFs[i,:],self.NPts,""OBF""+str(i))\n\t\t# Quickly check orthonormality.\n\t\t#for i in range(nbas):\n\t\t#\t\tfor j in range(nbas):\n\t\t#\t\t\tprint np.dot(self.OBFs[i],self.OBFs[j])*self.dx*self.dy*self.dz\n\t\t#\t\tprint """"\n\n\tdef PopulateSense(self):\n\t\tmol = gto.Mole()\n\t\tif (self.Spherical):\n\t\t\tmol.atom =""C 0.0 0.0 0.0""\n\t\t\tmol.spin = 0\n\t\telse:\n\t\t\tself.SenseGrid = MakeUniform([0,0,0],self.SenseRange,self.NSense)\n\t\t\t#pyscfatomstring=""C ""+str(p[0])+"" ""+str(p[1])+"" ""+str(p[2])+"";""\n\t\t\tmol.atom = \'\'.join([""H@0 ""+str(self.SenseGrid[iii,0])+"" ""+str(self.SenseGrid[iii,1])+"" ""+str(self.SenseGrid[iii,2])+"";"" for iii in range(len(self.SenseGrid))])\n\t\t\tna = self.NSense\n\t\t\tif (na%2 == 0):\n\t\t\t\tmol.spin = 0\n\t\t\telse:\n\t\t\t\tmol.spin = 1\n\t\tif (TOTAL_SENSORY_BASIS == None):\n\t\t\traise(""missing sensory basis"")\n\t\tmol.basis = TOTAL_SENSORY_BASIS\n\t\tnsaos = 0\n\t\ttry:\n\t\t\tmol.build()\n\t\texcept Exception as Ex:\n\t\t\tprint(mol.atom, mol.basis)\n\t\t\traise Ex\n\t\t#nbas = gto.nao_nr(mol)\n\t\t#print nbas\n\t\tself.SenseS = mol.intor(\'cint1e_ovlp_sph\',shls_slice=(0,mol.nbas,0,mol.nbas))\n\t\tself.NSense = self.SenseS.shape[0]\n\t\tself.SenseSinv = MatrixPower(self.SenseS,-1.0)\n\t\treturn\n\n\tdef Vectorize(self,input, QualityOfFit=False):\n\t\t\'\'\'\n\t\tInput is rasterized volume information,\n\t\toutput is a vector of NGau3 coefficients fitting that volume.\n\n\t\tThe underlying grid is assumed to be MyGrid(), although it can be scaled.\n\t\t\'\'\'\n\t\tif self.Grid==None:\n\t\t\tself.Populate()\n\t\tCM = np.dot(self.Grid.T,input)\n\t\tif ((self.GridRange-np.max(CM))/self.GridRange < 0.2):\n\t\t\tprint(""Warning... GridRange "", ((self.GridRange-np.max(CM))/self.GridRange))\n\t\toutput = np.tensordot(self.OBFs,np.power(input,0.5),axes=[[1],[0]])*self.dx*self.dy*self.dz\n\t\tif (QualityOfFit and np.linalg.norm(input)!=0.0):\n\t\t\tGridstoRaw(input,self.NPts,""Input"")\n\t\t\tprint(""Coefs"", output)\n\t\t\ttmp = self.Rasterize(output)\n\t\t\tGridstoRaw(tmp,self.NPts,""Output"")\n\t\t\tprint(""Sum of Input and reconstruction"", np.sum(input), np.sum(tmp))\n\t\t\tprint(""Average of Input and reconstruction"", np.average(input), np.average(tmp))\n\t\t\tprint(""Max of Input and reconstruction"", np.max(input), np.max(tmp))\n\t\t\tprint(""relative norm of difference:"", np.linalg.norm(tmp-input)/np.linalg.norm(input))\n\t\t\tGridstoRaw(input-tmp,self.NPts,""Diff"")\n\t\t\ttmp /= np.sum(tmp)\n\t\t\tprint(""Centers of Mass, in"", np.dot(self.Grid.T,input),"" and out "", np.dot(self.Grid.T,tmp))\n\t\t\tRsq = np.array(map(np.linalg.norm,self.Grid-CM))\n\t\t\tprint(""Variance of In"", np.dot(Rsq.T,input))\n\t\t\tprint(""Variance of Out"", np.dot(Rsq.T,tmp))\n\t\treturn output\n\n\tdef Rasterize(self,inp):\n\t\tif (self.Spherical):\n\t\t\tgrd = MakeUniform([0,0,0],self.SenseRange, self.NPts)\n\t\t\torbs = self.SenseOnGrid([0.0,0.0,0.0],grd)\n\t\t\tif (len(inp)!=self.NSense):\n\t\t\t\traise Exception(""Bad input dim."")\n\t\t\treturn np.tensordot(inp,orbs,axes=[[0],[1]])\n\t\telse :\n\t\t\tif (len(inp)!=self.NGau3):\n\t\t\t\traise Exception(""Bad input dim."")\n\t\t\treturn np.power(np.tensordot(inp,self.OBFs,axes=[[0],[0]]),2.0)\n\n\tdef CenterOfP(self,POnGrid,AGrid=None):\n\t\tif (len(POnGrid)!=self.NPts3):\n\t\t\traise Exception(""Bad input dim."")\n\t\tif (AGrid==None):\n\t\t\treturn np.array([np.dot(self.MyGrid().T,POnGrid)])[0]\n\t\telse:\n\t\t\treturn np.array([np.dot(AGrid.T,POnGrid)])[0]\n\n\tdef SenseOnGrid(self,p,grd_):\n\t\tmol = gto.Mole()\n\t\tmol.atom =\'\'\n\t\tif (not self.Spherical):\n\t\t\ttmpgrid = self.SenseGrid + p\n\t\t\tmol.atom = \'\'.join([""H@0 ""+str(tmpgrid[iii,0])+"" ""+str(tmpgrid[iii,1])+"" ""+str(tmpgrid[iii,2])+"";"" for iii in range(len(tmpgrid))])\n\t\telse:\n\t\t\tmol.atom = \'\'.join(""C ""+str(p[0])+"" ""+str(p[1])+"" ""+str(p[2])+"";"")\n\t\tmol.spin = 0\n\t\tif (TOTAL_SENSORY_BASIS == None):\n\t\t\traise(""missing sensory basis"")\n\t\tmol.basis = TOTAL_SENSORY_BASIS\n\t\tmol.build()\n\t\treturn gto.eval_gto(\'GTOval_sph\',mol._atm,mol._bas,mol._env,grd_*1.889725989)\n\n\tdef VecToRaw(self,inp,Nm_=""VecToRaw""):\n\t\tGridstoRaw(self.Rasterize(inp),self.NPts,Nm_)\n\n\tdef VdwDensity(self,m,p=[0.0, 0.0, 0.0],ngrid=150,Nm_=""Atoms"",tag=None):\n\t\tsamps, vol = m.SpanningGrid(ngrid,2)\n\t\tprint(""Grid ranges (A):"",np.max(samps[:,0]),np.min(samps[:,0]))\n\t\tprint(""Grid ranges (A):"",np.max(samps[:,1]),np.min(samps[:,1]))\n\t\tprint(""Grid ranges (A):"",np.max(samps[:,2]),np.min(samps[:,2]))\n\t\t# Make the atom densities.\n\t\tPs = self.MolDensity(samps,m,p,tag)\n\t\tGridstoRaw(Ps,ngrid,Nm_)\n\t\treturn samps\n\n\tdef MolDensity(self,samps,m,p=[0.0,0.0,0.0],tag=None):\n\t\tPs = np.zeros(len(samps))\n\t\tmol = gto.Mole()\n\t\tpyscfatomstring=\'\'\n\t\tif (tag==None):\n\t\t\tfor j in range(len(m.atoms)):\n\t\t\t\tpyscfatomstring=pyscfatomstring+""H@""+str(m.atoms[j])+"" ""+str(m.coords[j,0])+"" ""+str(m.coords[j,1])+"" ""+str(m.coords[j,2])+("";"" if j!= len(m.atoms)-1 else """")\n\t\telse:\n\t\t\tfor j in range(len(m.atoms)):\n\t\t\t\tif (j == tag):\n\t\t\t\t\tprint(""Tagging atom"", j)\n\t\t\t\t\tpyscfatomstring=pyscfatomstring+""N@0""+"" ""+str(m.coords[j,0])+"" ""+str(m.coords[j,1])+"" ""+str(m.coords[j,2])+("";"" if j!= len(m.atoms)-1 else """")\n\t\t\t\telse:\n\t\t\t\t\tpyscfatomstring=pyscfatomstring+""H@""+str(m.atoms[j])+"" ""+str(m.coords[j,0])+"" ""+str(m.coords[j,1])+"" ""+str(m.coords[j,2])+("";"" if j!= len(m.atoms)-1 else """")\n\t\tmol.atom = pyscfatomstring\n\t\tmol.basis = TOTAL_SENSORY_BASIS\n\t\tmol.verbose = 0\n\t\tif (len(m.atoms)%2 == 0):\n\t\t\tmol.spin = 0\n\t\telse:\n\t\t\tmol.spin = 1\n\t\ttry:\n\t\t\tmol.build()\n\t\texcept Exception as Ex:\n\t\t\tprint(mol.atom, mol.basis, m.atoms, m.coords)\n\t\t\traise Ex\n\t\treturn np.sum(gto.eval_gto(\'GTOval_sph\',mol._atm,mol._bas,mol._env,samps*1.889725989),axis=1)\n\n\tdef AtomEmbedAtomCentered(self,samps,m,p,i=-1):\n\t\tmol = gto.Mole()\n\t\tMaxEmbedded = 30\n\t\tSensedAtoms = [a for a in m.AtomsWithin(30.0,p) if a != i]\n\t\tif (len(SensedAtoms)>MaxEmbedded):\n\t\t\tSensedAtoms=SensedAtoms[:MaxEmbedded]\n\t\tif (len(SensedAtoms)==0):\n\t\t\traise Exception(""NoAtomsInSensoryRadius"")\n\t\tmol.atom=""C ""+str(p[0])+"" ""+str(p[1])+"" ""+str(p[2])+"";""\n\t\tna=0\n\t\tfor j in SensedAtoms:\n\t\t\tmol.atom=mol.atom+""H@""+str(m.atoms[j])+"" ""+str(m.coords[j,0])+"" ""+str(m.coords[j,1])+"" ""+str(m.coords[j,2])+("";"" if j!=SensedAtoms[-1] else """")\n\t\t\tna=na+1\n\t\t#print mol.atom\n\t\t#print self.atoms\n\t\t#print self.coords\n\t\t#print ""Basis Atom"",[mol.bas_atom(i) for i in range(mol.nbas)]\n\t\tif (na%2 == 0):\n\t\t\tmol.spin = 0\n\t\telse:\n\t\t\tmol.spin = 1\n\t\tif (TOTAL_SENSORY_BASIS == None):\n\t\t\traise(""missing sensory basis"")\n\t\tmol.basis = TOTAL_SENSORY_BASIS\n\t\tnsaos = 0\n\t\ttry:\n\t\t\tmol.build()\n\t\texcept Exception as Ex:\n\t\t\tprint(mol.atom, mol.basis, m.atoms, m.coords, SensedAtoms, p)\n\t\t\traise Ex\n\t\tnsaos = gto.nao_nr_range(mol,0,mol.atom_nshells(0))[1]\n\t\tnbas = gto.nao_nr(mol)\n\t\tprint(""nAtoms: "",m.NAtoms(),"" nsaos: "", nsaos, "" nbas "", nbas)\n\t\tS = mol.intor(\'cint1e_ovlp_sph\',shls_slice=(0,mol.atom_nshells(0),0,mol.atom_nshells(0)))\n\t\tSinv = MatrixPower(S,-1.0)\n\t\tSBFs = gto.eval_gto(\'GTOval_sph\',mol._atm,mol._bas,mol._env,samps*1.889725989,comp=1,shls_slice=(0,mol.atom_nshells(0)))\n\t\tprint(""SBFs.shape"", SBFs.shape)\n\t\tCs = mol.intor(\'cint1e_ovlp_sph\',shls_slice=(0,mol.atom_nshells(0),mol.atom_nshells(0),mol.nbas))\n\t\tprint(""Cs.shape"", Cs.shape)\n\t#\tfor i in range(len(Cs[0])):\n\t#\t\ttmp = np.dot(SBFs,np.dot(Sinv,Cs[:,i]))\n\t#\t\tGridstoRaw(tmp*tmp,150,""Atoms""+str(i))\n\t#\texit(0)\n\t\tSd = np.sum(np.dot(SBFs,np.dot(Sinv,Cs)),axis=1)\n\t\tprint(""Sd.shape"", Sd.shape)\n\t\treturn Sd\n\n\tdef TestGridGauEmbedding(self,samps,m,p,i):\n\t\tmol = gto.Mole()\n\t\tMaxEmbedded = 15\n\t\tSensedAtoms = [a for a in m.AtomsWithin(10.0,p) if a != i]\n\t\tif (len(SensedAtoms)>MaxEmbedded):\n\t\t\tSensedAtoms=SensedAtoms[:MaxEmbedded]\n\t\tif (len(SensedAtoms)==0):\n\t\t\traise Exception(""NoAtomsInSensoryRadius"")\n\n\t\tGauGrid = MakeUniform([0,0,0],3.5,self.NGau)\n\t\t#pyscfatomstring=""C ""+str(p[0])+"" ""+str(p[1])+"" ""+str(p[2])+"";""\n\t\tmol.atom = \'\'.join([""H@0 ""+str(GauGrid[iii,0])+"" ""+str(GauGrid[iii,1])+"" ""+str(GauGrid[iii,2])+"";"" for iii in range(len(GauGrid))])\n\n\t\tna = len(GauGrid)\n\t\t#na=0\n\t\tfor j in SensedAtoms:\n\t\t\tmol.atom=mol.atom+""H@""+str(m.atoms[j])+"" ""+str(m.coords[j,0])+"" ""+str(m.coords[j,1])+"" ""+str(m.coords[j,2])+("";"" if j!=SensedAtoms[-1] else """")\n\t\t\tna=na+1\n\t\t#print mol.atom\n\t\t#print self.atoms\n\t\t#print self.coords\n\t\t#print ""Basis Atom"",[mol.bas_atom(i) for i in range(mol.nbas)]\n\n\t\tif (na%2 == 0):\n\t\t\tmol.spin = 0\n\t\telse:\n\t\t\tmol.spin = 1\n\t\tif (TOTAL_SENSORY_BASIS == None):\n\t\t\traise(""missing sensory basis"")\n\t\tmol.basis = TOTAL_SENSORY_BASIS\n\t\tnsaos = 0\n\n\t\ttry:\n\t\t\tmol.build()\n\t\texcept Exception as Ex:\n\t\t\tprint(mol.atom, mol.basis, m.atoms, m.coords, SensedAtoms, p)\n\t\t\traise Ex\n\n\t\tnsaos = len(GauGrid)\n\t\tnbas = gto.nao_nr(mol)\n\t\tprint(""nAtoms: "",m.NAtoms(),"" nsaos: "", nsaos, "" nbas "", nbas)\n\t\tS = mol.intor(\'cint1e_ovlp_sph\',shls_slice=(0,nsaos,0,nsaos))\n\t\tSinv = MatrixPower(S,-1.0)\n\t\tSBFs = gto.eval_gto(\'GTOval_sph\',mol._atm,mol._bas,mol._env,samps*1.889725989,comp=1,shls_slice=(0,nsaos))\n\t\tprint(""SBFs.shape"", SBFs.shape)\n\t\tCs = mol.intor(\'cint1e_ovlp_sph\',shls_slice=(0,nsaos,nsaos,mol.nbas))\n\t\tprint(""Cs.shape"", Cs.shape)\n\t\tSd = np.sum(np.dot(SBFs,np.dot(Sinv,Cs)),axis=1)\n\t\tprint(""Sd.shape"", Sd.shape)\n\t\treturn Sd\n\n\tdef EmbedAtom(self,m,p,i=-1):\n\t\t\'\'\'\n\t\t\tReturns coefficents embedding the environment of atom i, centered at point p.\n\t\t\'\'\'\n\t\tmol = gto.Mole()\n\t\tMaxEmbedded = 15\n\t\tSensedAtoms = None\n\t\tif (i != -1):\n\t\t\tSensedAtoms = [a for a in m.AtomsWithin(10.0,p) if a != i]\n\t\telse:\n\t\t\tSensedAtoms = [a for a in m.AtomsWithin(10.0,p)]\n\t\tif (len(SensedAtoms)>MaxEmbedded):\n\t\t\tSensedAtoms=SensedAtoms[:MaxEmbedded]\n\t\tif (len(SensedAtoms)==0):\n\t\t\traise Exception(""NoAtomsInSensoryRadius"")\n\t\tmol.atom =\'\'\n\t\tif (not self.Spherical):\n\t\t\ttmpgrid = self.SenseGrid + p\n\t\t\tmol.atom = \'\'.join([""H@0 ""+str(tmpgrid[iii,0])+"" ""+str(tmpgrid[iii,1])+"" ""+str(tmpgrid[iii,2])+"";"" for iii in range(len(tmpgrid))])\n\t\telse:\n\t\t\tmol.atom = \'\'.join(""C ""+str(p[0])+"" ""+str(p[1])+"" ""+str(p[2])+"";"")\n\t\tna = 0\n\t\tif (not self.Spherical):\n\t\t\tna = self.NSense\n\t\tfor j in SensedAtoms:\n\t\t\tmol.atom=mol.atom+""H@""+str(m.atoms[j])+"" ""+str(m.coords[j,0])+"" ""+str(m.coords[j,1])+"" ""+str(m.coords[j,2])+("";"" if j!=SensedAtoms[-1] else """")\n\t\t\tna=na+1\n\t\tif (na%2 == 0):\n\t\t\tmol.spin = 0\n\t\telse:\n\t\t\tmol.spin = 1\n\t\tif (TOTAL_SENSORY_BASIS == None):\n\t\t\traise(""missing sensory basis"")\n\t\tmol.basis = TOTAL_SENSORY_BASIS\n\t\tnsaos = 0\n\t\ttry:\n\t\t\tmol.build()\n\t\texcept Exception as Ex:\n\t\t\tprint(mol.atom, mol.basis, m.atoms, m.coords, SensedAtoms, p)\n\t\t\traise Ex\n\t\tnsaos = len(self.SenseSinv)\n\t\tif (self.Spherical):\n\t\t\tnsaos = mol.atom_nshells(0)\n\t\tCs = mol.intor(\'cint1e_ovlp_sph\',shls_slice=(0,nsaos,nsaos,mol.nbas))\n\t\treturn np.sum(np.dot(self.SenseSinv,Cs),axis=1)\n\n\tdef TestSense(self,m,p=[0.0, 0.0, 0.0],ngrid=150,Nm_=""Atoms""):\n\t\tsamps, vol = m.SpanningGrid(ngrid,2)\n\t\t# Make the atom densities.\n\t\tPs = self.MolDensity(samps,m,p)\n\t\tGridstoRaw(Ps,ngrid,Nm_)\n\t\tfor i in range(m.NAtoms()):\n\t\t\tPe = self.TestGridGauEmbedding(samps,m,p,i)\n\t\t\tGridstoRaw(Pe,ngrid,Nm_+str(i))\n\n\tdef TransformGrid(self,aGrid,ALinearOperator):\n\t\treturn np.array([np.dot(ALinearOperator,pt) for pt in aGrid])\n\n\tdef ExpandIsometries(self,inputs,outputs):\n\t\tncase=inputs.shape[0]\n\t\tniso=self.NIso()\n\t\tif (len(outputs)!=ncase):\n\t\t\traise Exception(""Nonsense inputs"")\n\t\tins=[ncase*niso]+(list(inputs.shape)[1:])\n\t\tous=[ncase*niso]+(list(outputs.shape)[1:])\n\t\tnewins=np.zeros(shape=ins)\n\t\tnewout=np.zeros(shape=ous)\n\t\t#for i in range(ncase):\n\t\t#\tfor j in range(niso):\n\t\t#\t\tnewins[i*niso+j] = inputs[i][self.IsometryRelabelings[j]]\n\t\t#\t\tnewout[i*niso+j] = np.dot(self.InvIsometries[j],outputs[i])\n\t\tfor j in range(niso):\n\t\t\tnewins[j*ncase:(j+1)*ncase]=inputs[:,self.IsometryRelabelings[j]]\n\t\t\tnewout[j*ncase:(j+1)*ncase]=np.tensordot(outputs,self.InvIsometries[j],axes=[[1],[1]])\n\t\t#for i in range(ncase):\n\t\t#\tfor j in range(niso):\n\t\t#\t\tnewins[i*niso+j] = inputs[i][self.IsometryRelabelings[j]]\n\t\t#\t\tnewout[i*niso+j] = np.dot(self.InvIsometries[j],outputs[i])\n\t\treturn newins,newout\n\n\tdef\tNIso(self):\n\t\treturn len(self.Isometries)\n\n\tdef TestIsometries(self,m,p=[0.0, 0.0, 0.0],ngrid=150):\n\t\t\'\'\' Tests that isometries of the basis match up to isometries of the fit. \'\'\'\n\t\tsamps, vol = m.SpanningGrid(ngrid,2)\n\t\t# Make the atom densities.\n\t\tPs = self.MolDensity(samps,m,p)\n\t\tGridstoRaw(Ps,ngrid,""Atoms"")\n\t\tCs = self.EmbedAtom(m,p,-1)\n\t\tPe = self.Rasterize(Cs)\n\t\tGridstoRaw(Pe,self.NPts,""Atoms0"")\n\t\tCoP=self.CenterOfP(Pe)\n\t\tprint(""COM:"",CoP)\n\t\tfor i in range(len(self.IsometryRelabelings)):\n\t\t\tPCs = self.Rasterize(Cs[self.IsometryRelabelings[i]])\n\t\t\tprint(""Transformed"",np.dot(self.InvIsometries[i],CoP))\n\t\t\tprint(""COM:"",self.CenterOfP(PCs))\n\t\t\t#GridstoRaw(PCs,self.NPts,""Atoms""+str(i+1))\n'"
TensorMol/Math/Ipecac.py,0,"b'""""""\n Ipecac: a syrup which causes vomiting.\n This file contains routines which \'attempt\' to reverse embeddings back to the geometry of a molecule.\n Ie: it\'s the inverse of Digest.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .Mol import *\nfrom ..Util import *\nimport os, sys, re, random, math, copy, itertools\nimport numpy as np\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\nfrom .LinearOperations import *\nfrom ..Containers.DigestMol import *\nfrom ..Containers.Digest import *\nfrom ..Simulations.Opt import *\nfrom scipy import optimize\n\nclass Ipecac:\n\tdef __init__(self, set_, dig_, eles_):\n\t\t""""""\n\t\tArgs:\n\t\t\tset_ : an MSet of molecules\n\t\t\tdig_: embedding type for reversal\n\t\t\teles_: list of possible elements for reversal\n\t\t""""""\n\t\tself.set = set_\n\t\tself.dig = dig_\n\t\tself.eles = eles_\n\n\tdef ReverseAtomwiseEmbedding(self, emb_,atoms_, guess_, GdDistMatrix):\n\t\t""""""\n\t\tArgs:\n\t\t\tatoms_: a list of element types for which this routine provides coords.\n\t\t\tdig_: a digester\n\t\t\temb_: the embedding which we will try to construct a mol to match. Because this is atomwise this will actually be a (natoms X embedding shape) tensor.\n\t\tReturns:\n\t\t\tA best-fit version of a molecule which produces an embedding as close to emb_ as possible.\n\t\t""""""\n\t\tnatoms = emb_.shape[0]\n\t\tif atoms_ == None:\n\t\t\tatoms = np.full((natoms),6)\n\t\telse:\n\t\t\tatoms = atoms_\n\t\t# if (guess_==None):\n\t\t# \tcoords = np.zeros((natoms, 3))\n\t\t# \tprint self.EmbAtomwiseErr(Mol(atoms[:1], coords[:1,:]), emb_[:1,:])\n\t\t# \treturn\n\t\t# \tfunc = lambda crds: self.EmbAtomwiseErr(Mol(atoms,crds.reshape(natoms,3)),emb_)\n\t\t# \tmin_kwargs = {""method"": ""BFGS""}\n\t\t# \t# This puts natom into a cube of length 1 so correct the density to be roughly 1atom/angstrom.\n\t\t# \tcoords = np.random.rand(natoms,3)\n\t\t# \tcoords *= natoms\n\t\t# \tret = optimize.basinhopping(func, coords, minimizer_kwargs=min_kwargs, niter=500)\n\t\t# \tmfit = Mol(atoms, coords)\n\t\t# \tatoms_ = self.BruteForceAtoms(mfit, emb_)\n\t\t# \tfunc = lambda crds: self.EmbAtomwiseErr(Mol(atoms_,crds.reshape(natoms,3)),emb_)\n\t\t# \tret = optimize.basinhopping(func, coords, minimizer_kwargs=min_kwargs, niter=500)\n\t\t# \tprint(""global minimum: coords = %s, atoms = %s, f(x0) = %.4f"" % (ret.x, atoms_, ret.fun))\n\t\t# \t# return\n\t\t# \tcoords = ret.x.reshape(natoms, 3)\n\t\t# \tmfit = Mol(atoms_,coords)\n\t\t# \tmfit.WriteXYZfile(""./results/"", ""RevLog"")\n\t\t# \t# Next optimize with an equilibrium distance matrix which is roughly correct for each type of species...\n\t\t# \t# mfit.DistMatrix = np.ones((natoms,3))\n\t\t# \t# np.fill_diagonal(mfit.DistMatrix,0.0)\n\t\t# \t# opt = Optimizer(None)\n\t\t# \t# opt.OptGoForce(mfit)\n\t\t# \t# mfit.WriteXYZfile(""./results/"", ""RevLog"")\n\t\t# else:\n\t\tcoords = guess_\n\t\t# atoms = np.ones(len(atoms_), dtype=np.uint8)\n\t\t# Now shit gets real. Create a function to minimize.\n\t\tobjective = lambda crds: self.EmbAtomwiseErr(Mol(atoms,crds.reshape(natoms,3)),emb_)\n\t\tif (1):\n\t\t\tdef callbk(x_):\n\t\t\t\tmn = Mol(atoms, x_.reshape(natoms,3))\n\t\t\t\tmn.BuildDistanceMatrix()\n\t\t\t\tprint(""Distance error : "", np.sqrt(np.sum((GdDistMatrix-mn.DistMatrix)*(GdDistMatrix-mn.DistMatrix))))\n\t\timport scipy.optimize\n\t\tstep = 0\n\t\tres=optimize.minimize(objective,coords.reshape(natoms*3),method=\'L-BFGS-B\',tol=1.e-12,options={""maxiter"":5000000,""maxfun"":10000000},callback=callbk)\n\t\t# res=scipy.optimize.minimize(objective,coords.reshape(natoms*3),method=\'SLSQP\',tol=0.000001,options={""maxiter"":5000000},callback=callbk)\n\t\t# coords = res.x.reshape(natoms,3)\n\t\t# res=scipy.optimize.minimize(objective,coords.reshape(natoms*3),method=\'Powell\',tol=0.000001,options={""maxiter"":5000000},callback=callbk)\n\t\t# while (self.EmbAtomwiseErr(Mol(atoms_,coords),emb_) > 1.e-5) and (step < 10):\n\t\t# \tstep += 1\n\t\t# \tres=scipy.optimize.minimize(objective,coords.reshape(natoms*3),method=\'L-BFGS-B\',tol=0.000001,options={""maxiter"":5000000,""maxfun"":10000000},callback=callbk)\n\t\t# \tprint ""Reversal complete: "", res.message\n\t\t# \tcoords = res.x.reshape(natoms,3)\n\t\t# \tmfit = Mol(atoms_, coords)\n\t\t# \tatoms_ = self.BruteForceAtoms(mfit, emb_)\n\t\tmfit = Mol(atoms, res.x.reshape(natoms,3))\n\t\tself.DistanceErr(GdDistMatrix, mfit)\n\t\treturn mfit\n\n\tdef BruteForceAtoms(self, mol_, emb_):\n\t\tprint(""Searching for best atom fit"")\n\t\tbestmol = copy.deepcopy(mol_)\n\t\tbesterr = 100.0\n\t\t# posib_stoich = [x for x in itertools.product([1,6,7,8], repeat=len(mol_.atoms))]\n\t\t# for stoich in posib_stoich:\n\t\tfor stoich in itertools.product([1,6,7,8], repeat=len(mol_.atoms)):\n\t\t\ttmpmol = Mol(np.array(stoich), mol_.coords)\n\t\t\ttmperr = self.EmbAtomwiseErr(tmpmol,emb_)\n\t\t\tif tmperr < besterr:\n\t\t\t\tbestmol = copy.deepcopy(tmpmol)\n\t\t\t\tbesterr = tmperr\n\t\t\t\tprint(besterr)\n\t\tprint(bestmol.atoms)\n\t\treturn bestmol.atoms\n\n\tdef EmbAtomwiseErr(self, mol_,emb_):\n\t\tins = self.dig.Emb(mol_,MakeOutputs=False)\n\t\terr = np.sqrt(np.sum((ins-emb_)*(ins-emb_)))\n\t\t# print ""Emb err: "", err\n\t\treturn err\n\n\tdef DistanceErr(self, GdDistMatrix_, mol_):\n\t\tmol_.BuildDistanceMatrix()\n\t\tprint(""Final Distance error : "", np.sqrt(np.sum((GdDistMatrix_-mol_.DistMatrix)*(GdDistMatrix_-mol_.DistMatrix))))\n'"
TensorMol/Math/LinearOperations.py,2,"b'""""""\nLinear algebra operations and coordinate transformations.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\nimport random\nimport math\nfrom math import pi as Pi\n\ndef MovingAverage(a, n=3) :\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\ndef PseudoInverse(mat_):\n\tU, s, V = np.linalg.svd(mat_) #M=u * np.diag(s) * v,\n\tfor i in range(len(s)):\n\t\tif (s[i]!=0.0 and abs(s[i])>0.0000001):\n\t\t\ts[i]=1.0/s[i]\n\t\telse:\n\t\t\ts[i] = 0.0\n\treturn np.dot(np.dot(U,np.diag(s)),V)\n\n# Simple vectorized coordinate transformations.\ndef SphereToCart(arg_):\n\tr = arg_[0]\n\ttheta = arg_[1]\n\tphi = arg_[2]\n\tx = r*np.sin(theta)*np.cos(phi)\n\ty = r*np.sin(theta)*np.sin(phi)\n\tz = r*np.cos(theta)\n\treturn np.array([x,y,z])\n\ndef CartToSphere(arg_):\n\tx = arg_[0]\n\ty = arg_[1]\n\tz = arg_[2]\n\tr = np.sqrt(x*x+y*y+z*z)\n\ttheta = np.arccos(z/r)\n\tphi = np.arctan2(y,x)\n\treturn np.array([r,theta,phi])\n\ndef SchmidtStep(xs,y_):\n\t""""""\n\treturn y - projection of y onto all xs normalized.\n\tArgs:\n\t\txs: orthonormal row vectors\n\t\ty: another row vector.\n\t""""""\n\ty = y_.copy()\n\tfor i in range(xs.shape[0]):\n\t\ty -= np.dot(xs[i],y)*xs[i]/np.dot(xs[i],xs[i])\n\tntmp = np.dot(y,y)\n\treturn ntmp, y\ndef Normalize(x_):\n\treturn x_/np.sqrt(np.dot(x_,x_))\ndef PairOrthogonalize(x,y):\n\t""""""\n\tDoes a Graham-Schmidt\n\tThe assumption here is that y is square and full-rank\n\tand x has smaller and full rank. Returns rank(y)-rank(x) row vectors\n\twhich are all normalized and orthogonal to x.\n\t""""""\n\tny = y.shape[0]\n\tnx = x.shape[0]\n\tdim = y.shape[1]\n\tif (x.shape[1] != y.shape[1]):\n\t\traise Exception(""Dim mismatch"")\n\t# Orthogonalize\n\tOx = np.zeros((nx+ny,dim))\n\tOx[0] = Normalize(x[0])\n\tOrank = 1\n\tfor i in range(1,nx):\n\t\tntmp, tmp = SchmidtStep(Ox[:Orank],x[i])\n\t\tif (ntmp > pow(10.0,-12.0)):\n\t\t\tOx[Orank] = tmp/np.sqrt(ntmp)\n\t\t\tOrank += 1\n\tLastXVec = Orank\n\tfor i in range(ny):\n\t\tntmp, tmp = SchmidtStep(Ox[:Orank],y[i])\n\t\tif (ntmp > pow(10.0,-12.0)):\n\t\t\tOx[Orank] = tmp/np.sqrt(ntmp)\n\t\t\tOrank += 1\n\treturn Ox[LastXVec:Orank]#Ox[:Orank]#\ndef SphereToCartV(arg_):\n\treturn np.array(map(SphereToCart,arg_))\ndef CartToSphereV(arg_):\n\treturn np.array(map(CartToSphere,arg_))\ndef MakeUniform(point,disp,num):\n\t\'\'\' Uniform Grids of dim numxnumxnum around a point\'\'\'\n\tgrids = np.mgrid[-disp:disp:num*1j, -disp:disp:num*1j, -disp:disp:num*1j]\n\tgrids = grids.transpose()\n\tgrids = grids.reshape((grids.shape[0]*grids.shape[1]*grids.shape[2], grids.shape[3]))\n\treturn point+grids\ndef GridstoRaw(grids, ngrids=250, save_name=""mol"", save_path =""./densities/""):\n\t#print ""Writing Grid Mx, Mn, Std, Sum "", np.max(grids),np.min(grids),np.std(grids),np.sum(grids)\n\tmgrids = np.copy(grids)\n\tmgrids *= (254/np.max(grids))\n\tmgrids = np.array(mgrids, dtype=np.uint8)\n\t#print np.bincount(mgrids)\n\t#print ""Writing Grid Mx, Mn, Std, Sum "", np.max(mgrids),np.min(mgrids),np.std(mgrids),np.sum(mgrids)\n\tprint(""Saving density to:"",save_path+save_name+"".raw"")\n\tf = open(save_path+save_name+"".raw"", ""wb"")\n\tf.write(bytes(np.array([ngrids,ngrids,ngrids],dtype=np.uint8).tostring())+bytes(mgrids.tostring()))\n\tf.close()\n\ndef MatrixPower(A,p,PrintCondition=False):\n\t\'\'\' Raise a Hermitian Matrix to a possibly fractional power. \'\'\'\n\tu,s,v = np.linalg.svd(A)\n\tif (PrintCondition):\n\t\tprint(""MatrixPower: Minimal Eigenvalue ="", np.min(s))\n\tfor i in range(len(s)):\n\t\tif (abs(s[i]) < np.power(10.0,-14.0)):\n\t\t\ts[i] = np.power(10.0,-14.0)\n\treturn np.dot(u,np.dot(np.diag(np.power(s,p)),v))\n\ndef RotationMatrix(axis, theta):\n\t""""""\n\tReturn the rotation matrix associated with counterclockwise rotation about\n\tthe given axis by theta radians.\n\t""""""\n\taxis = np.asarray(axis)\n\taxis = axis/np.linalg.norm(axis)\n\ta = math.cos(theta/2.0)\n\tb, c, d = -axis*math.sin(theta/2.0)\n\taa, bb, cc, dd = a*a, b*b, c*c, d*d\n\tbc, ad, ac, ab, bd, cd = b*c, a*d, a*c, a*b, b*d, c*d\n\treturn np.array([[aa+bb-cc-dd, 2*(bc+ad), 2*(bd-ac)],\n\t\t\t\t\t [2*(bc-ad), aa+cc-bb-dd, 2*(cd+ab)],\n\t\t\t\t\t [2*(bd+ac), 2*(cd-ab), aa+dd-bb-cc]])\n\ndef RotationMatrix_v2(randnums=None, deflection=1.0):\n\t""""""\n\tCreates a uniformly random rotation matrix\n\tArgs:\n\t\trandnums: theta, phi, and z for rotation, if None then chosen uniformly random\n\t\tdeflection: magnitude of rotation, 0 is no rotation, 1 is completely random rotation, inbetween are perturbations\n\t""""""\n\tif randnums is None:\n\t\trandnums = np.random.uniform(size=(3,))\n\ttheta, phi, z = randnums[0]*2.0*deflection*np.pi, randnums[1]*2.0*np.pi, randnums[2]*2.0*deflection\n\tr = np.sqrt(z)\n\tv = np.array([np.sin(phi)*r, np.cos(phi)*r, np.sqrt(2.0-z)])\n\tR = np.array(((np.cos(theta),np.sin(theta),0.),(-np.sin(theta),np.cos(theta),0.),(0.,0.,1.)))\n\tM = (np.outer(v,v) - np.eye(3)).dot(R)\n\treturn M\n\ndef ReflectionMatrix(axis1,axis2):\n\taxis1 = np.asarray(axis1)\n\taxis2 = np.asarray(axis2)\n\ta1=axis1/np.linalg.norm(axis1)\n\ta2=axis2/np.linalg.norm(axis2)\n\tunitNormal=np.cross(a1,a2)\n\treturn np.eye(3) - 2.0*np.outer(unitNormal,unitNormal)\n\ndef OctahedralOperations():\n\t\'\'\'\n\t\tTransformation matrices for symmetries of an octahedral shape.\n\t\tFar from the complete set but enough for debugging and seeing if it helps.\n\t\'\'\'\n\tIdent=[np.eye(3)]\n\tFaceRotations=[RotationMatrix([1,0,0], Pi/2.0),RotationMatrix([0,1,0], Pi/2.0),RotationMatrix([0,0,1], Pi/2.0),RotationMatrix([-1,0,0], Pi/2.0),RotationMatrix([0,-1,0], Pi/2.0),RotationMatrix([0,0,-1], Pi/2.0)]\n\tFaceRotations2=[RotationMatrix([1,0,0], Pi),RotationMatrix([0,1,0], Pi),RotationMatrix([0,0,1], Pi),RotationMatrix([-1,0,0], Pi),RotationMatrix([0,-1,0], Pi),RotationMatrix([0,0,-1], Pi)]\n\tFaceRotations3=[RotationMatrix([1,0,0], 3.0*Pi/2.0),RotationMatrix([0,1,0], 3.0*Pi/2.0),RotationMatrix([0,0,1], 3.0*Pi/2.0),RotationMatrix([-1,0,0], 3.0*Pi/2.0),RotationMatrix([0,-1,0], 3.0*Pi/2.0),RotationMatrix([0,0,-1], 3.0*Pi/2.0)]\n\tCornerRotations=[RotationMatrix([1,1,1], 2.0*Pi/3.0),RotationMatrix([-1,1,1], 2.0*Pi/3.0),RotationMatrix([-1,-1,1], 2.0*Pi/3.0),RotationMatrix([-1,-1,-1], 2.0*Pi/3.0),RotationMatrix([-1,1,-1], 2.0*Pi/3.0),RotationMatrix([1,-1,-1], 2.0*Pi/3.0),RotationMatrix([1,1,-1], 2.0*Pi/3.0),RotationMatrix([1,-1,1], 2.0*Pi/3.0)]\n\tCornerRotations2=[RotationMatrix([1,1,1], 4.0*Pi/3.0),RotationMatrix([-1,1,1], 4.0*Pi/3.0),RotationMatrix([-1,-1,1], 4.0*Pi/3.0),RotationMatrix([-1,-1,-1], 4.0*Pi/3.0),RotationMatrix([-1,1,-1], 4.0*Pi/3.0),RotationMatrix([1,-1,-1], 4.0*Pi/3.0),RotationMatrix([1,1,-1], 4.0*Pi/3.0),RotationMatrix([1,-1,1], 4.0*Pi/3.0)]\n\tEdgeRotations=[RotationMatrix([1,1,0], Pi),RotationMatrix([1,0,1], Pi),RotationMatrix([0,1,1], Pi)]\n\tEdgeReflections=[ReflectionMatrix([1,0,0],[0,1,0]),ReflectionMatrix([0,0,1],[0,1,0]),ReflectionMatrix([1,0,0],[0,0,1])]\n\treturn Ident+FaceRotations+FaceRotations2+FaceRotations3+CornerRotations+CornerRotations2+EdgeRotations+EdgeReflections\n'"
TensorMol/Math/QuasiNewtonTools.py,0,"b'""""""\nRoutines which help do differential analysis and Newtonian Mechanics\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..PhysicalData import *\nfrom ..Util import *\nfrom .LinearOperations import *\n\ndef RmsForce(f_):\n\treturn np.mean(np.linalg.norm(f_,axis=1))\ndef CenterOfMass(x_,m_):\n\treturn (np.einsum(""m,mx->x"",m_,x_)/np.sum(m_))\ndef InertiaTensor(x_,m_):\n\tI = np.zeros((3,3))\n\tfor i in range(len(m_)):\n\t\tI[0,0] += m_[i]*(x_[i,1]*x_[i,1]+x_[i,2]*x_[i,2])\n\t\tI[1,1] += m_[i]*(x_[i,0]*x_[i,0]+x_[i,2]*x_[i,2])\n\t\tI[2,2] += m_[i]*(x_[i,1]*x_[i,1]+x_[i,0]*x_[i,0])\n\t\tI[0,1] -= m_[i]*(x_[i,0]*x_[i,1])\n\t\tI[0,2] -= m_[i]*(x_[i,0]*x_[i,2])\n\t\tI[1,2] -= m_[i]*(x_[i,1]*x_[i,2])\n\tI[1,0] = I[0,1]\n\tI[2,0] = I[0,2]\n\tI[2,1] = I[1,2]\n\treturn I\ndef DiagHess(f_,x_,eps_=0.0005):\n\t""""""\n\tArgs:\n\t\tf_ returns -1*gradient.\n\t\tx_ a guess_\n\t""""""\n\ttore=np.zeros(x_.shape)\n\tx_t = x_.copy()\n\tf_x_ = f_(x_)\n\tit = np.nditer(x_, flags=[\'multi_index\'])\n\twhile not it.finished:\n\t\tx_t = x_.copy()\n\t\tx_t[it.multi_index] += eps_\n\t\ttore[it.multi_index] = ((f_(x_t) - f_x_)/eps_)[it.multi_index]\n\t\tit.iternext()\n\treturn tore\ndef FdiffGradient(f_, x_, eps_=0.0001):\n\t""""""\n\tComputes a finite difference gradient of a single or multi-valued function\n\tat x_ for debugging purposes.\n\t""""""\n\tx_t = x_.copy()\n\tf_x_ = f_(x_)\n\toutshape = x_.shape+f_x_.shape\n\ttore=np.zeros(outshape)\n\tit = np.nditer(x_, flags=[\'multi_index\'])\n\twhile not it.finished:\n\t\tx_t = x_.copy()\n\t\tx_t[it.multi_index] += eps_\n\t\ttore[it.multi_index] = ((f_(x_t) - f_x_)/eps_)\n\t\tit.iternext()\n\treturn tore\ndef CoordinateScan(f_, x_, name_="""", eps_=0.03, num_=15):\n\t# Writes a plaintext file containing scans of each coordinate.\n\tsamps = np.logspace(0.0,eps_,num_)-1.0\n\tsamps = np.concatenate((-1*samps[::-1][:-1],samps),axis=0)\n\titi = np.nditer(x_, flags=[\'multi_index\'])\n\ttore = np.zeros(x_.shape+(len(samps),2))\n\tci = 0\n\twhile not iti.finished:\n\t\tfor i,d in enumerate(samps):\n\t\t\tx_t = x_.copy()\n\t\t\tx_t[iti.multi_index] += d\n\t\t\ttore[iti.multi_index][i,0]=d\n\t\t\ttore[iti.multi_index][i,1]=f_(x_t)\n\t\tnp.savetxt(""./results/CoordScan""+name_+str(ci)+"".txt"",tore[iti.multi_index])\n\t\tci += 1\n\t\titi.iternext()\ndef FdiffHessian(f_, x_, eps_=0.001, mode_ = ""forward"", grad_ = None):\n\t""""""\n\tComputes a finite difference hessian of a single or multi-valued function\n\tat x_ for debugging purposes.\n\n\tArgs:\n\t\tf_ : objective function of x_\n\t\tx_: point at which derivative is taken.\n\t\teps_: finite difference step\n\t\tmode_: forward, central, or gradient Differences\n\t\tgrad_: a gradient function if available.\n\t""""""\n\tx_t = x_.copy()\n\tf_x_ = f_(x_)\n\toutshape = x_.shape+x_.shape+f_x_.shape\n\ttore=np.zeros(outshape)\n\tif (mode_ == ""gradient"" and grad_ != None):\n\t\ttmpshape = x_.shape+x_.shape+f_x_.shape\n\t\ttmpp = np.zeros(tmpshape)\n\t\ttmpm = np.zeros(tmpshape)\n\t\titi = np.nditer(x_, flags=[\'multi_index\'])\n\t\twhile not iti.finished:\n\t\t\txi_t = x_.copy()\n\t\t\txi_t[iti.multi_index] += eps_\n\t\t\txmi_t = x_.copy()\n\t\t\txmi_t[iti.multi_index] -= eps_\n\t\t\ttmpp[iti.multi_index]  = grad_(xi_t).copy()\n\t\t\ttmpm[iti.multi_index]  = grad_(xmi_t).copy()\n\t\t\titi.iternext()\n\t\titi = np.nditer(x_, flags=[\'multi_index\'])\n\t\twhile not iti.finished:\n\t\t\titj = np.nditer(x_, flags=[\'multi_index\'])\n\t\t\twhile not itj.finished:\n\t\t\t\tgpjci = tmpp[itj.multi_index][iti.multi_index]\n\t\t\t\tgmjci = tmpm[itj.multi_index][iti.multi_index]\n\t\t\t\tgpicj = tmpp[iti.multi_index][itj.multi_index]\n\t\t\t\tgmicj = tmpm[iti.multi_index][itj.multi_index]\n\t\t\t\ttore[iti.multi_index][itj.multi_index] = ((gpjci-gmjci)/(4.0*eps_))+((gpicj-gmicj)/(4.0*eps_))\n\t\t\t\titj.iternext()\n\t\t\titi.iternext()\n\telif (mode_ == ""forward""):\n\t\ttmpshape = x_.shape+f_x_.shape\n\t\ttmpfs = np.zeros(tmpshape)\n\t\titi = np.nditer(x_, flags=[\'multi_index\'])\n\t\twhile not iti.finished:\n\t\t\txi_t = x_.copy()\n\t\t\txi_t[iti.multi_index] += eps_\n\t\t\ttmpfs[iti.multi_index]  = f_(xi_t).copy()\n\t\t\tprint(iti.multi_index,tmpfs[iti.multi_index])\n\t\t\titi.iternext()\n\t\titi = np.nditer(x_, flags=[\'multi_index\'])\n\t\twhile not iti.finished:\n\t\t\txi_t = x_.copy()\n\t\t\txi_t[iti.multi_index] += eps_\n\t\t\titj = np.nditer(x_, flags=[\'multi_index\'])\n\t\t\twhile not itj.finished:\n\t\t\t\txij_t = xi_t.copy()\n\t\t\t\txij_t[itj.multi_index] += eps_\n\t\t\t\ttore[iti.multi_index][itj.multi_index] = ((f_(xij_t)-tmpfs[iti.multi_index]-tmpfs[itj.multi_index]+f_x_)/eps_/eps_)\n\t\t\t\titj.iternext()\n\t\t\titi.iternext()\n\telif (mode_ == ""central""):\n\t\titi = np.nditer(x_, flags=[\'multi_index\'])\n\t\twhile not iti.finished:\n\t\t\txi_t = x_.copy()\n\t\t\txi_t[iti.multi_index] += eps_\n\t\t\txmi_t = x_.copy()\n\t\t\txmi_t[iti.multi_index] -= eps_\n\t\t\titj = np.nditer(x_, flags=[\'multi_index\'])\n\t\t\twhile not itj.finished:\n\t\t\t\txpipj_t = xi_t.copy()\n\t\t\t\txpipj_t[itj.multi_index] += eps_\n\t\t\t\txpimj_t = xi_t.copy()\n\t\t\t\txpimj_t[itj.multi_index] -= eps_\n\t\t\t\txmipj_t = xmi_t.copy()\n\t\t\t\txmipj_t[itj.multi_index] += eps_\n\t\t\t\txmimj_t = xmi_t.copy()\n\t\t\t\txmimj_t[itj.multi_index] -= eps_\n\t\t\t\ttore[iti.multi_index][itj.multi_index] = (f_(xpipj_t)-f_(xpimj_t)-f_(xmipj_t)+f_(xmimj_t))/(4.0*eps_*eps_)\n\t\t\t\titj.iternext()\n\t\t\titi.iternext()\n\treturn tore\ndef FourPointHessQuad(f):\n\t""""""\n\tf is a 4x4xOutshape\n\tsampling eps*[-2, -1, 1, 2]\n\t""""""\n\tterm1 = -63.0*(f[2,0]+f[3,1]+f[0,2]+f[1,3])\n\tterm2 =  63.0*(f[1,0]+f[0,1]+f[2,3]+f[3,2])\n\tterm3 =  44.0*(f[3,0]+f[0,3]-f[0,0]-f[3,3])\n\tterm4 =  74.0*(f[1,1]+f[2,2]-f[2,1]-f[1,2])\n\treturn (term1+term2+term3+term4)/600.0\ndef DirectedFdiffHessian(f_, x_, dirs_, eps_=0.01):\n\t""""""\n\tFour-Point Hessian quadrature along dirs_ directions.\n\n\tArgs:\n\t\tdirs_ : a set of directions having x_\'s shape\n\tReturns:\n\t\td^2 f/ (d dirs_i, d dirs_j)\n\t""""""\n\tf_x_ = f_(x_)\n\tN = dirs_.shape[0]\n\ttore = np.zeros((N,N))\n\tfor i in range(N):\n\t\tfor j in range(i,N):\n\t\t\tsamps = np.zeros((4,4)+f_x_.shape)\n\t\t\tfor ic,di in enumerate([-2.,-1.,1.,2.]):\n\t\t\t\tfor jc,dj in enumerate([-2.,-1.,1.,2.]):\n\t\t\t\t\tsamps[ic,jc] = f_(x_+dirs_[i]*eps_*di+dirs_[j]*eps_*dj)\n\t\t\ttore[i,j] = FourPointHessQuad(samps)/eps_/eps_\n\t\t\ttore[j,i] = tore[i,j]\n\treturn tore\ndef InternalCoordinates(x_,m):\n\t""""""\n\tGenerates a set of internal (ie: rot-trans free)\n\tvectors spanning coordinates for asymmetric mols.\n\tIf you are doing a diatomic, use a quantum chemistry package\n\t""""""\n\tif (len(m)<=2):\n\t\tprint(m)\n\t\traise Exception(""No Diatomics"")\n\tCOM0 = CenterOfMass(x_,m)\n\txc_  = x_ - COM0\n\tI = InertiaTensor(xc_,m)\n\tIp,X = np.linalg.eig(I)\n\tIp0 = Ip.copy()\n\tn = len(m)\n\tn3 = 3*n\n\t# Generate the 6 rotations and translations.\n\tD = np.zeros((6,n3))\n\tMWC = np.zeros((n3,n3))\n\tfor i in range(n3):\n\t\tif (i%3==0):\n\t\t\tD[0,i] = np.sqrt(m[int(i/3)])\n\t\telif (i%3==1):\n\t\t\tD[1,i] = np.sqrt(m[int(i/3)])\n\t\telif (i%3==2):\n\t\t\tD[2,i] = np.sqrt(m[int(i/3)])\n\tfor i in range(n):\n\t\tPx = np.dot(xc_[i],X[:,0])\n\t\tPy = np.dot(xc_[i],X[:,1])\n\t\tPz = np.dot(xc_[i],X[:,2])\n\t\tfor j in range(3):\n\t\t\tD[3,i*3+j] = (Py*X[2,j]-Pz*X[1,j])/np.sqrt(m[i])\n\t\t\tD[4,i*3+j] = (Pz*X[0,j]-Px*X[2,j])/np.sqrt(m[i])\n\t\t\tD[5,i*3+j] = (Px*X[1,j]-Py*X[0,j])/np.sqrt(m[i])\n\t\t\tMWC[i*3+j,i*3+j] = np.sqrt(m[i])\n\tS = PairOrthogonalize(D,MWC) # Returns normalized Coords.\n\tnint = S.shape[0]\n\tprint(""3N, Number of Internal Coordinates: "", n3 , nint)\n\treturn S\n\ndef HarmonicSpectra(f_, x_, at_, grad_=None, eps_ = 0.001, WriteNM_=False, Mu_ = None):\n\t""""""\n\tPerform a finite difference normal mode analysis\n\tof a molecule. basically implements http://gaussian.com/vib/\n\n\tArgs:\n\t\tf_: Energies in Hartree.\n\t\tx_: Coordinates (A)\n\t\tat_: element type of each atom\n\t\tgrad_: forces in Hartree/angstrom if available. (unused)\n\t\teps_: finite difference step\n\t\tWriteNM_: Whether to write the normal modes to readable files\n\t\tMu_: A dipole field routine for intensities.\n\n\tReturns:\n\t\tFrequencies in wavenumbers and Normal modes (cart)\n\t""""""\n\tLOGGER.info(""Harmonic Analysis"")\n\tn = x_.shape[0]\n\tn3 = 3*n\n\tm_ = np.array(map(lambda x: ATOMICMASSESAMU[x-1]*ELECTRONPERPROTONMASS, at_.tolist()))\n\tprint (""m_:"", m_)\n\tCrds = InternalCoordinates(x_,m_) #invbasis X cart\n\t#Crds=np.eye(n3).reshape((n3,n,3))\n\t#print(""En?"",f_(x_))\n\tif 0:\n\t\tHess = DirectedFdiffHessian(f_, x_, Crds.reshape((len(Crds),n,3)))\n\t\tprint(""Hess (Internal):"", Hess)\n\t\t# Transform the invariant hessian into cartesian coordinates.\n\t\tcHess = np.dot(Crds.T,np.dot(Hess,Crds))\n\telse:\n\t\tcHess = FdiffHessian(f_, x_,0.0005).reshape((n3,n3))\n\tcHess /= (BOHRPERA*BOHRPERA)\n\tprint(""Hess (Cart):"", cHess)\n\t# Mass weight the invariant hessian in cartesian coordinate\n\tfor i,mi in enumerate(m_):\n\t\tcHess[i*3:(i+1)*3, i*3:(i+1)*3] /= np.sqrt(mi*mi)\n\t\tfor j,mj in enumerate(m_):\n\t\t\tif (i != j):\n\t\t\t\tcHess[i*3:(i+1)*3, j*3:(j+1)*3] /= np.sqrt(mi*mj)\n\t# Get the vibrational spectrum and normal modes.\n\tu,s,v = np.linalg.svd(cHess)\n\tfor l in s:\n\t\tprint(""Central Energy (cm**-1): "", np.sign(l)*np.sqrt(l)*WAVENUMBERPERHARTREE)\n\tprint(""--"")\n\t# Get the actual normal modes, for visualization sake.\n\tw,v = np.linalg.eigh(cHess)\n\tv = v.real\n\twave = np.sign(w)*np.sqrt(abs(w))*WAVENUMBERPERHARTREE\n\tprint(""N3, shape v"",n3,v.shape)\n\tif (WriteNM_):\n\t\tfor i in range(3*n):\n\t\t\tnm = np.zeros(3*n)\n\t\t\tfor j,mi in enumerate(m_):\n\t\t\t\tnm[3*j:3*(j+1)] = v[3*j:3*(j+1),i]/np.sqrt(mi/ELECTRONPERPROTONMASS)\n\t\t\t#nm /= np.sqrt(np.sum(nm*nm))\n\t\t\tnm = nm.reshape((n,3))\n\t\t\t# Take finite difference derivative of mu(Q) and return the <dmu/dQ, dmu/dQ>\n\t\t\tstep = 0.01\n\t\t\tdmudq = (Mu_(x_+step*nm)-Mu_(x_))/step\n\t\t\tprint(""|f| (UNITS????) "",np.dot(dmudq,dmudq.T))\n\t\t\tfor alpha in np.append(np.linspace(0.1,-0.1,30),np.linspace(0.1,-0.1,30)):\n\t\t\t\tmdisp = Mol(at_, x_+alpha*nm)\n\t\t\t\t#print(""Mu"",Mu_(x_+alpha*nm))\n\t\t\t\tmdisp.WriteXYZfile(""./results/"",""NormalMode_""+str(i))\n\treturn wave, v\n\ndef HarmonicSpectraWithProjection(f_, x_, at_, grad_=None, eps_ = 0.001, WriteNM_=False, Mu_ = None):\n\t""""""\n\tPerform a finite difference normal mode analysis\n\tof a molecule. basically implements http://gaussian.com/vib/\n\n\tArgs:\n\t\tf_: Energies in Hartree.\n\t\tx_: Coordinates (A)\n\t\tat_: element type of each atom\n\t\tgrad_: forces in Hartree/angstrom if available. (unused)\n\t\teps_: finite difference step\n\t\tWriteNM_: Whether to write the normal modes to readable files\n\t\tMu_: A dipole field routine for intensities.\n\n\tReturns:\n\t\tFrequencies in wavenumbers and Normal modes (cart)\n\t""""""\n\tLOGGER.info(""Harmonic Analysis"")\n\tn = x_.shape[0]\n\tn3 = 3*n\n\tm_ = np.array(map(lambda x: ATOMICMASSESAMU[x-1]*ELECTRONPERPROTONMASS, at_.tolist()))\n\tCrds = InternalCoordinates(x_,m_) #invbasis X cart flatten.\n\tcHess = FdiffHessian(f_, x_,0.0005).reshape((n3,n3))\n\tcHess /= (BOHRPERA*BOHRPERA)\n\tprint(""Hess (Cart):"", cHess)\n\t# Mass weight the invariant hessian in cartesian coordinate\n\tfor i,mi in enumerate(m_):\n\t\tcHess[i*3:(i+1)*3, i*3:(i+1)*3] /= np.sqrt(mi*mi)\n\t\tfor j,mj in enumerate(m_):\n\t\t\tif (i != j):\n\t\t\t\tcHess[i*3:(i+1)*3, j*3:(j+1)*3] /= np.sqrt(mi*mj)\n\t# Get the vibrational spectrum and normal modes.\n\t# pHess = np.einsum(\'ab,cb->ac\',np.einsum(\'ij,jk->ik\',Crds,cHess),Crds)\n\ts,v = np.linalg.eigh(Hess)\n\tfor l in s:\n\t\tprint(""Central Energy (cm**-1): "", np.sign(l)*np.sqrt(l)*WAVENUMBERPERHARTREE)\n\tprint(""--"")\n\t# Get the actual normal modes, for visualization sake.\n\tv = v.real\n\twave = np.sign(s)*np.sqrt(abs(s))*WAVENUMBERPERHARTREE\n\tprint(""N3, shape v"",n3,v.shape)\n\tif (WriteNM_):\n\t\tfor i in range(3*n):\n\t\t\tnm = v[:,i].reshape((n,3))\n\t\t\tnm *= np.sqrt(m_[:,np.newaxis]).T\n\t\t\ttmp = nm.reshape((x_.shape[0],3))\n\t\t\t# Take finite difference derivative of mu(Q) and return the <dmu/dQ, dmu/dQ>\n\t\t\tstep = 0.01\n\t\t\tdmudq = (Mu_(x_+step*tmp)-Mu_(x_))/step\n\t\t\tprint(""|f| (UNITS????) "",np.dot(dmudq,dmudq.T))\n\t\t\tfor alpha in np.append(np.linspace(-.1,.1,30),np.linspace(.1,-.1,30)):\n\t\t\t\tmdisp = Mol(at_, x_+alpha*tmp)\n\t\t\t\tmdisp.WriteXYZfile(""./results/"",""NormalMode_""+str(i))\n\treturn wave, v\n\nclass ConjGradient:\n\tdef __init__(self,f_,x0_,thresh_=0.0001):\n\t\t""""""\n\t\tArgs:\n\t\t\tf_ : an energy, force routine.\n\t\t\tx0_: initial point.\n\t\t\tp_: initial search direction.\n\t\t""""""\n\t\tself.EForce = f_\n\t\tself.Energy = lambda x: self.EForce(x,False)\n\t\tself.x0 = x0_.copy()\n\t\tself.xold = x0_.copy()\n\t\tself.e, self.gold  = self.EForce(x0_)\n\t\tself.s = self.gold.copy()\n\t\tself.thresh = thresh_\n\t\tself.alpha = PARAMS[""GSSearchAlpha""]\n\t\treturn\n\tdef Reset(self,x0_):\n\t\tself.xold = x0_.copy()\n\t\tself.e, self.gold  = self.EForce(x0_)\n\t\tself.s = self.gold.copy()\n\t\tself.thresh = thresh_\n\t\tself.alpha = PARAMS[""GSSearchAlpha""]\n\tdef BetaPR(self,g):\n\t\tbetapr = np.sum((g)*(g - self.gold))/(np.sum(self.gold*self.gold))\n\t\tself.gold = g.copy()\n\t\treturn max(0,betapr)\n\tdef __call__(self,x0):\n\t\t""""""\n\t\tIterate Conjugate Gradient.\n\n\t\tArgs:\n\t\t\tx0: Point at which to minimize gradients\n\t\tReturns:\n\t\t\tNext point, energy, and gradient.\n\t\t""""""\n\t\te,g = self.EForce(x0)\n\t\tbeta_n = self.BetaPR(g)\n\t\tself.s = g + beta_n*self.s\n\t\tself.xold = self.LineSearch(x0,self.s,self.thresh)\n\t\treturn self.xold, e, g\n\tdef LineSearch(self, x0_, p_, thresh = 0.0001):\n\t\t\'\'\'\n\t\tgolden section search to find the minimum of f on [a,b]\n\n\t\tArgs:\n\t\t\tf_: a function which returns energy.\n\t\t\tx0_: Origin of the search.\n\t\t\tp_: search direction.\n\n\t\tReturns:\n\t\t\tx: coordinates which minimize along this search direction.\n\t\t\'\'\'\n\t\tk=0\n\t\trmsdist = 10.0\n\t\ta = x0_\n\t\tb = x0_ + self.alpha*p_\n\t\tc = b - (b - a) / GOLDENRATIO\n\t\td = a + (b - a) / GOLDENRATIO\n\t\tfa = self.Energy(a)\n\t\tfb = self.Energy(b)\n\t\tfc = self.Energy(c)\n\t\tfd = self.Energy(d)\n\t\twhile (rmsdist > thresh):\n\t\t\tif (fa < fc and fa < fd and fa < fb):\n\t\t\t\t#print fa,fc,fd,fb\n\t\t\t\t#print RmsForce(fpa), RmsForce(fpc), RmsForce(fpd), RmsForce(fpb)\n\t\t\t\tprint(""Line Search: Overstep alpha="",self.alpha)\n\t\t\t\tif (self.alpha > 0.00001):\n\t\t\t\t\tself.alpha /= 1.8001\n\t\t\t\telif self.alpha < 0.0001:\n\t\t\t\t\tprint(""ARE YOU SURE FORCE MATCHES ENERGY??? "")\t\t\t\t\t\n\t\t\t\telse:\n\t\t\t\t\tprint(""Keeping step"")\n\t\t\t\t\treturn a\n\t\t\t\ta = x0_\n\t\t\t\tb = x0_ + self.alpha*p_\n\t\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\t\tfa = self.Energy(a)\n\t\t\t\tfb = self.Energy(b)\n\t\t\t\tfc = self.Energy(c)\n\t\t\t\tfd = self.Energy(d)\n\t\t\telif (fb < fc and fb < fd and fb < fa):\n\t\t\t\t#print fa,fc,fd,fb\n\t\t\t\t#print RmsForce(fpa), RmsForce(fpc), RmsForce(fpd), RmsForce(fpb)\n\t\t\t\tprint(""Line Search: Understep alpha="",self.alpha)\n\t\t\t\tif (self.alpha < 100.0):\n\t\t\t\t\tself.alpha *= 1.8\n\t\t\t\ta = x0_\n\t\t\t\tb = x0_ + self.alpha*p_\n\t\t\t\treturn (b + a) / 2 # It\'s okay to return understeps, the force evals arent worth it.\n\t\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\t\tfa = self.Energy(a)\n\t\t\t\tfb = self.Energy(b)\n\t\t\t\tfc = self.Energy(c)\n\t\t\t\tfd = self.Energy(d)\n\t\t\telif fc < fd:\n\t\t\t\tb = d\n\t\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\t\tfb = fd\n\t\t\t\tfc = self.Energy(c)\n\t\t\t\tfd = self.Energy(d)\n\t\t\telse:\n\t\t\t\ta = c\n\t\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\t\tfa = fc\n\t\t\t\tfc = self.Energy(c)\n\t\t\t\tfd = self.Energy(d)\n\t\t\trmsdist = np.sum(np.linalg.norm(a-b,axis=1))/a.shape[0]\n\t\t\tk+=1\n\t\treturn (b + a) / 2\n\ndef LineSearch(f_, x0_, p_, thresh = 0.0001):\n\t\'\'\'\n\tgolden section search to find the minimum of f on [a,b]\n\n\tArgs:\n\t\tf_: a function which returns energy.\n\t\tx0_: Origin of the search.\n\t\tp_: search direction.\n\n\tReturns:\n\t\tx: coordinates which minimize along this search direction.\n\t\'\'\'\n\tk=0\n\trmsdist = 10.0\n\ta = x0_\n\tb = x0_ + PARAMS[""GSSearchAlpha""]*p_\n\tc = b - (b - a) / GOLDENRATIO\n\td = a + (b - a) / GOLDENRATIO\n\tfa = f_(a)\n\tfb = f_(b)\n\tfc = f_(c)\n\tfd = f_(d)\n\twhile (rmsdist > thresh):\n\t\tif (fa < fc and fa < fd and fa < fb):\n\t\t\t#print fa,fc,fd,fb\n\t\t\t#print RmsForce(fpa), RmsForce(fpc), RmsForce(fpd), RmsForce(fpb)\n\t\t\tprint(""Line Search: Overstep"")\n\t\t\tif (PARAMS[""GSSearchAlpha""] > 0.00001):\n\t\t\t\tPARAMS[""GSSearchAlpha""] /= 1.71\n\t\t\telse:\n\t\t\t\tprint(""Keeping step"")\n\t\t\t\treturn a\n\t\t\ta = x0_\n\t\t\tb = x0_ + PARAMS[""GSSearchAlpha""]*p_\n\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\tfa = f_(a)\n\t\t\tfb = f_(b)\n\t\t\tfc = f_(c)\n\t\t\tfd = f_(d)\n\t\telif (fb < fc and fb < fd and fb < fa):\n\t\t\t#print fa,fc,fd,fb\n\t\t\t#print RmsForce(fpa), RmsForce(fpc), RmsForce(fpd), RmsForce(fpb)\n\t\t\tprint(""Line Search: Understep"")\n\t\t\tif (PARAMS[""GSSearchAlpha""] < 10.0):\n\t\t\t\tPARAMS[""GSSearchAlpha""] *= 1.7\n\t\t\ta = x0_\n\t\t\tb = x0_ + PARAMS[""GSSearchAlpha""]*p_\n\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\tfa = f_(a)\n\t\t\tfb = f_(b)\n\t\t\tfc = f_(c)\n\t\t\tfd = f_(d)\n\t\telif fc < fd:\n\t\t\tb = d\n\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\tfb = fd.copy()\n\t\t\tfc = f_(c)\n\t\t\tfd = f_(d)\n\t\telse:\n\t\t\ta = c\n\t\t\tc = b - (b - a) / GOLDENRATIO\n\t\t\td = a + (b - a) / GOLDENRATIO\n\t\t\tfa = fc.copy()\n\t\t\tfc = f_(c)\n\t\t\tfd = f_(d)\n\t\trmsdist = np.sum(np.linalg.norm(a-b,axis=1))/a.shape[0]\n\t\tk+=1\n\treturn (b + a) / 2\n\ndef LineSearchCart(f_, x0_):\n\t"""""" A line search in each cartesian direction. """"""\n\tx_ = x0_.copy()\n\titi = np.nditer(x_, flags=[\'multi_index\'])\n\twhile not iti.finished:\n\t\tx_ = x0_.copy()\n\t\tx_[iti.multi_index] -= 0.05\n\t\tp=np.zeros(x0_.shape)\n\t\tp[iti.multi_index] += 0.05\n\t\tx_= LineSearch(f_,x_,p)\n\t\titi.iternext()\n\treturn x_\n\ndef RemoveInvariantForce(x_,f_,m_):\n\t""""""\n\tRemoves center of mass motion and torque from f_, and returns the invariant bits.\n\t""""""\n\tif (PARAMS[""RemoveInvariant""]==False):\n\t\treturn f_\n\t#print x_, f_ , m_\n\tfnet = np.sum(f_,axis=0)\n\t# Remove COM force.\n\tfnew_ = f_ - (np.einsum(""m,f->mf"",m_,fnet)/np.sum(m_))\n\ttorque = np.sum(np.cross(x_,fnew_),axis=0)\n\t#print torque\n\t# Compute inertia tensor\n\tI = InertiaTensor(x_,m_)\n\tIinv = PseudoInverse(I)\n\t#print ""Inertia tensor"", I\n\t#print ""Inverse Inertia tensor"", Iinv\n\t# Compute angular acceleration  = torque/I\n\tdwdt = np.dot(Iinv,torque)\n\t#print ""Angular acceleration"", dwdt\n\t# Compute the force correction.\n\tfcorr = np.zeros(f_.shape)\n\tfor i in range(len(m_)):\n\t\tfcorr[i,0] += m_[i]*(-1.0*dwdt[2]*x_[i,1] + dwdt[1]*x_[i,2])\n\t\tfcorr[i,1] += m_[i]*(dwdt[2]*x_[i,0] - dwdt[0]*x_[i,2])\n\t\tfcorr[i,2] += m_[i]*(-1.0*dwdt[1]*x_[i,0] + dwdt[0]*x_[i,1])\n\treturn fnew_ - fcorr\n'"
TensorMol/Math/Statistics.py,0,"b'""""""\nRoutines for simple statistical analysis.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nclass OnlineEstimator:\n\t""""""\n\tSimple storage-less Knuth estimator which\n\taccumulates mean and variance.\n\t""""""\n\tdef __init__(self,x_):\n\t\tself.n = 1\n\t\tself.mean = x_*0.\n\t\tself.m2 = x_*0.\n\t\tdelta = x_ - self.mean\n\t\tself.mean += delta / self.n\n\t\tdelta2 = x_ - self.mean\n\t\tself.m2 += delta * delta2\n\tdef __call__(self, x_):\n\t\tself.n += 1\n\t\tdelta = x_ - self.mean\n\t\tself.mean += delta / self.n\n\t\tdelta2 = x_ - self.mean\n\t\tself.m2 += delta * delta2\n\t\treturn self.mean, self.m2/(self.n-1)\n'"
TensorMol/Math/TFMath.py,0,"b'""""""\nRoutines which do elementary math in Tensorflow which is for whatever reason missing in the Tensorflow API.\n""""""\n'"
TensorMol/Math/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom .Statistics import *\nfrom .QuasiNewtonTools import *\nfrom .LinearOperations import *\n'
TensorMol/Simulations/InfraredMD.py,0,"b'""""""\nThe Units chosen are Angstrom, Fs and units derived from those.\nI convert the force outside from kcal/(mol angstrom) to Joules/(mol angstrom)\n\nThis IR trajectory is specially purposed for a force routine which\nproduces energy, forces and charges. It also does all the pre-optimization and stuff\nautomatically.\n\nUsage:\nIRProp(f,m,""TrajectoryName"").Prop()\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .SimpleMD import *\nfrom .Opt import *\n\nclass IRProp(VelocityVerlet):\n\tdef __init__(self,efq_,g0_,name_=str(0)):\n\t\t""""""A specialized sort of dynamics which is appropriate for obtaining IR spectra at\n\t\tZero temperature. Absorption cross section is given by:\n\t\talpha = frac{4pi^2}{hbar c} omega (1 - Exp[-beta hbar omega]) sigma(omega))\n\t\tsigma  = frac{1}{6 pi} mathcal{F} {mu(t)mu(0)}\n\n\t\tArgs:\n\t\t\tefq_: a function which yields the energy, force, and charge\n\t\t\tg0_: an initial geometry.\n\t\t\tPARAMS[""MDAnnealTF""]: The temperature at which the IR will take place.\n\t\t""""""\n\t\tself.EnergyAndForce = lambda x: efq_(x)[:2]\n\t\tself.EnergyForceCharge = lambda x: efq_(x)\n\t\tVelocityVerlet.__init__(self, lambda x: efq_(x)[0], g0_, name_, self.EnergyAndForce)\n\t\tself.Force = lambda x: efq_(x)[0]\n\t\t# Optimize the molecule.\n\t\tLOGGER.info(""Preparing to do an IR. First Optimizing... "")\n\t\tG0Opt = GeomOptimizer(self.EnergyAndForce).Opt(g0_)\n\t\t# Anneal to Target Temperature.\n\t\t# Take the resulting x,v\n\t\tLOGGER.info(""Now Annealing... "")\n\t\tanneal = Annealer(self.EnergyAndForce, None, G0Opt)\n\t\tanneal.Prop()\n\t\tself.x = anneal.x.copy()\n\t\tself.v = anneal.v.copy()\n\t\tself.EPot0 , self.f0, self.q0 = self.EnergyForceCharge(g0_.coords)\n\t\tself.EPot = self.EPot0\n\t\tself.qs = np.ones(self.m.shape)\n\t\tself.Mu0 = np.zeros(3)\n\t\tself.mu_his = None\n\t\tself.qs = self.q0.copy()\n\t\tself.Mu0 = Dipole(self.x, self.qs)\n\t\tprint(self.qs, self.Mu0)\n\t\t# This can help in case you had a bad initial geometry\n\t\tself.MinS = 0\n\t\tself.MinE = 0.0\n\t\tself.Minx = None\n\n\tdef VVStep(self):\n\t\t""""""\n\t\tA Velocity Verlet Step\n\n\t\tReturns:\n\t\t\tx: updated positions\n\t\t\tv: updated Velocities\n\t\t\ta: updated accelerations\n\t\t\te: Energy at midpoint.\n\t\t""""""\n\t\tx = self.x + self.v*self.dt + (1./2.)*self.a*self.dt*self.dt\n\t\te,f_x_, self.qs = self.EnergyForceCharge(x)\n\t\ta = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/self.m) # m^2/s^2 => A^2/Fs^2\n\t\tv = self.v + (1./2.)*(self.a+a)*self.dt\n\t\treturn x,v,a,e\n\n\tdef WriteTrajectory(self):\n\t\tm=Mol(self.atoms,self.x)\n\t\t#m.properties[""Time""]=self.t\n\t\t#m.properties[""KineticEnergy""]=self.KE\n\t\tm.properties[""Energy""]=self.EPot\n\t\t#m.properties[""Charges""]=self.qs\n\t\tm.WriteXYZfile(""./results/"", ""MDTrajectory""+self.name)\n\t\treturn\n\n\tdef Prop(self):\n\t\tself.mu_his = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n\t\tvhis = np.zeros((self.maxstep,)+self.v.shape) # time Dipoles Energy\n\t\tstep = 0\n\t\twhile(step < self.maxstep):\n\t\t\tself.t = step*self.dt\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\n\t\t\tself.Mu = Dipole(self.x, self.qs) - self.Mu0\n\t\t\tself.mu_his[step,0] = self.t\n\t\t\tself.mu_his[step,1:4] = self.Mu.copy()\n\t\t\tself.mu_his[step,4] = self.KE\n\t\t\tself.mu_his[step,5] = self.EPot\n\t\t\tself.mu_his[step,6] = self.KE+self.EPot\n\t\t\tvhis[step] = self.v.copy()\n\n\t\t\tself.x , self.v, self.a, self.EPot = self.VVStep() # Always Unthermostated\n\t\t\tif (step%50==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tstep+=1\n\t\t\tif (step%1000==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.mu_his)\n\t\t\t\tWriteDerDipoleCorrelationFunction(self.mu_his[:step],self.name+""MutMu0.txt"")\n\n\t\t\tLOGGER.info(""%s Step: %i time: %.1f(fs) <KE>(kJ): %.5f <PotE>(Eh): %.5f <ETot>(kJ/mol): %.5f Teff(K): %.5f Mu: (%f,%f,%f)"", self.name, step, self.t, self.KE/1000.0, self.EPot, self.KE/1000.0+(self.EPot-self.EPot0)*KJPERHARTREE, Teff, self.Mu[0], self.Mu[1], self.Mu[2])\n\t\treturn\n'"
TensorMol/Simulations/MetaDynamics.py,0,"b'""""""\nFor enhanced sampling of a PES near its minimum...\nWe could even use poorly trained networks for this.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .SimpleMD import *\nfrom ..ForceModels import TFForces # Note this is actually the RIGHT way to do this.\nfrom ..Math.Statistics import *\n\nclass MetaDynamics(VelocityVerlet):\n\tdef __init__(self,f_,g0_,name_=""MetaMD"",EandF_=None):\n\t\t""""""\n\t\tA trajectory which explores chemical space more rapidly\n\t\tby droppin\' gaussians after a region has been explored for BumpTime\n\t\tRequires a thermostat and currently uses Nose.\n\n\t\tArgs:\n\t\t\tf_: A routine which returns the force.\n\t\t\tg0_: an initial molecule.\n\t\t\tname_: a name for output.\n\t\t\tEandF_: a routine returning the energy and the force.\n\t\t\tPARAMS[""BowlK""] : a force constant of an attractive potential.\n\t\t""""""\n\t\tVelocityVerlet.__init__(self, f_, g0_, name_, EandF_)\n\t\tself.BumpTime = PARAMS[""MetaBumpTime""]\n\t\tself.MaxBumps = PARAMS[""MetaMaxBumps""]\n\t\tself.bump_height = PARAMS[""MetaMDBumpHeight""]\n\t\tself.bump_width = PARAMS[""MetaMDBumpWidth""]\n\t\tself.BumpCoords = np.zeros((self.MaxBumps,self.natoms,3))\n\t\tself.NBump = 0\n\t\tself.DStat = OnlineEstimator(MolEmb.Make_DistMat(self.x))\n\t\tself.BowlK = PARAMS[""MetaBowlK""]\n\t\tif (self.Tstat.name != ""Andersen""):\n\t\t\tLOGGER.info(""I really recommend you use Andersen Thermostat with Meta-Dynamics."")\n\t\tself.Bumper = TFForces.BumpHolder(self.natoms, self.MaxBumps, self.BowlK, self.bump_height, self.bump_width,""MR"")\n\n\tdef BumpForce(self,x_):\n\t\tBE = 0.0\n\t\tBF = np.zeros(x_.shape)\n\t\tif (self.NBump > 0):\n\t\t\tBE, BF = self.Bumper.Bump(self.BumpCoords.astype(np.float32), x_.astype(np.float32), self.NBump%self.MaxBumps)\n\t\tif (self.EnergyAndForce != None):\n\t\t\tself.RealPot, PF = self.EnergyAndForce(x_)\n\t\telse:\n\t\t\tPF = self.ForceFunction(x_)\n\t\tif self.NBump > 0:\n\t\t\tBF[0] *= self.m[:,None]\n\t\tPF += JOULEPERHARTREE*BF[0]\n\t\tPF = RemoveInvariantForce(x_,PF,self.m)\n\t\treturn BE+self.RealPot, PF\n\n\tdef Bump(self):\n\t\tself.BumpCoords[self.NBump%self.MaxBumps] = self.x\n\t\tself.NBump += 1\n\t\tLOGGER.info(""Bump added!"")\n\t\treturn\n\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\t\t""""""\n\t\tstep = 0\n\t\tbumptimer = self.BumpTime\n\t\tself.md_log = np.zeros((self.maxstep, 11)) # time Dipoles Energy\n\t\twhile(step < self.maxstep):\n\t\t\tt = time.time()\n\t\t\tself.t = step*self.dt\n\t\t\tbumptimer -= self.dt\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\t\t\tif (PARAMS[""MDThermostat""]==None):\n\t\t\t\tself.x , self.v, self.a, self.EPot = VelocityVerletStep(self.BumpForce, self.a, self.x, self.v, self.m, self.dt, self.BumpForce)\n\t\t\telse:\n\t\t\t\tself.x , self.v, self.a, self.EPot, self.force = self.Tstat.step(self.BumpForce, self.a, self.x, self.v, self.m, self.dt, self.BumpForce)\n\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,4] = self.KE\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tself.md_log[step,6] = self.KE+(self.EPot-self.EPot0)*JOULEPERHARTREE\n\t\t\tself.md_log[step,7] = self.RealPot\n\n\t\t\t# Add an averager which accumulates RMS distance. Also, wow the width is small.\n\t\t\tEav, Evar = self.EnergyStat(self.RealPot)\n\t\t\tDav, Dvar = self.DStat(MolEmb.Make_DistMat(self.x))\n\t\t\tself.md_log[step,8] = Eav\n\t\t\tself.md_log[step,9] = Evar\n\t\t\tself.md_log[step,10] = np.linalg.norm(Dvar)\n\n\t\t\tif (bumptimer < 0.0):\n\t\t\t\tself.Bump()\n\t\t\t\tbumptimer = self.BumpTime\n\n\t\t\tif (step%10==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\n\t\t\tLOGGER.info(""Step: %i time: %.1f(fs) KE(kJ/mol): %.5f <|a|>(m/s2): %.5f EPot(Eh): %.5f <EPot(Eh)>: %.5f Etot(kJ/mol): %.5f  <d(D_ij)^2>: %.5f Teff(K): %.5f"",\n\t\t\t\t\tstep, self.t, self.KE/1000.0,  np.linalg.norm(self.a) , self.EPot, Eav, self.md_log[step,6]/1000.0, self.md_log[step,10], Teff)\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\t\tstep+=1\n\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\t\treturn\n\nclass BoxingDynamics(VelocityVerlet):\n\tdef __init__(self,f_,g0_,name_=""MetaMD"",EandF_=None,BoxingLat0_=np.eye(3),BoxingLatp_=np.eye(3), BoxingT_ = 500.0):\n\t\t""""""\n\t\tA Trajectory which crushes molecules into a box\n\n\t\tArgs:\n\t\t\tf_: A routine which returns the force.\n\t\t\tg0_: an initial molecule.\n\t\t\tname_: a name for output.\n\t\t\tEandF_: a routine returning the energy and the force.\n\t\t\tBoxingLat0_:  Box to Start From\n\t\t\tBoxingLatp_"": Box to force it into...\n\t\t\tBoxingT: Duration of boxing process (fs)\n\t\t""""""\n\t\tVelocityVerlet.__init__(self, f_, g0_, name_, EandF_)\n\t\tself.BoxingLat0 = BoxingLat0_.copy()\n\t\tself.BoxingLatp = BoxingLatp_.copy()\n\t\tself.boxnow = self.BoxingLat0\n\t\tself.BoxingT = BoxingT_\n\t\tself.Tstat = NoseThermostat(self.m,self.v)\n\t\tself.Boxer = TFForces.BoxHolder(self.natoms)\n\tdef BoxForce(self, x_ ):\n\t\tprint(""self.boxnow"", self.boxnow)\n\t\tBE, BF = self.Boxer(x_, self.boxnow)\n\t\tprint(""Mass Vector"", self.m[:,None])\n\t\tBF *= -500.0*JOULEPERHARTREE*(self.m[:,None]/np.sqrt(np.sum(self.m*self.m)))\n\t\tprint(""Bump Energy and Force: "",BE,BF)\n\t\tPE, PF = self.EnergyAndForce(x_)\n\t\tprint(""Pot Energy and Force: "",PE,PF)\n\t\treturn BE+PE,PF+BF\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\n\t\tmindistance_ is a cut off variable.  The box stops crushing if\n\t\tit has reached its minimum intermolecular distance.\n\t\t""""""\n\t\tstep = 0\n\t\tself.md_log = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n\t\twhile(step < self.maxstep): # || self.BoxingLatp_[0][0] < ?????current distance????):\n\t\t\tt = time.time()\n\t\t\tself.t = step*self.dt\n\n\t\t\tif (self.t>self.BoxingT):\n\t\t\t\tprint(""Exceeded Boxtime\\n"",self.BoxingLatp)\n\t\t\t\tself.boxnow = self.BoxingLatp.copy()\n\t\t\telse:\n\t\t\t\tself.boxnow = ((self.BoxingT-self.t)/(self.BoxingT))*self.BoxingLat0+(1.0-(self.BoxingT-self.t)/(self.BoxingT))*self.BoxingLatp\n\n\t\t\tprint(self.boxnow)\n\t\t\tprint(np.min(self.x),np.max(self.x))\n\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\t\t\tself.x , self.v, self.a, self.EPot, self.force = self.Tstat.step(self.BoxForce, self.a, self.x, self.v, self.m, self.dt, self.BoxForce)\n\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,4] = self.KE\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tself.md_log[step,6] = self.KE+(self.EPot-self.EPot0)*JOULEPERHARTREE\n\n\t\t\tif (step%10==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\n\t\t\tstep+=1\n\t\t\tLOGGER.info(""Step: %i time: %.1f(fs) <KE>(kJ/mol): %.5f <|a|>(m/s2): %.5f <EPot>(Eh): %.5f <Etot>(kJ/mol): %.5f Teff(K): %.5f"", step, self.t, self.KE/1000.0,  np.linalg.norm(self.a) , self.EPot, self.KE/1000.0+self.EPot*KJPERHARTREE, Teff)\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\treturn\n\nclass BoxedMetaDynamics(VelocityVerlet):\n\tdef __init__(self, EandF_, g0_, name_=""MetaMD"", Box_=np.array(10.0*np.eye(3))):\n\t\tVelocityVerlet.__init__(self, None, g0_, name_, EandF_)\n\t\tself.BumpTime = 12000000000000.0 # Fs\n\t\tself.MaxBumps = PARAMS[""MetaMaxBumps""] # think you want this to be >500k\n\t\tself.BumpCoords = np.zeros((self.MaxBumps,self.natoms,3))\n\t\tself.NBump = 0\n\t\tself.Tstat = NoseThermostat(self.m,self.v)\n\t\tself.Boxer = TFForces.BoxHolder(self.natoms)\n\t\tself.Box = Box_.copy()\n\t\tself.BowlK = 0.0\n\t\tself.Bumper = TFForces.BumpHolder(self.natoms, self.MaxBumps, self.BowlK)\n\tdef Bump(self):\n\t\tself.BumpCoords[self.NBump%self.MaxBumps] = self.x\n\t\tself.NBump += 1\n\t\tLOGGER.info(""Bump added!"")\n\t\treturn\n\tdef BoxForce(self, x_ ):\n\t\tBxE, BxF = self.Boxer(x_, self.Box)\n\t\tBxF *= -500.0*JOULEPERHARTREE*(self.m[:,None]/np.sqrt(np.sum(self.m*self.m)))\n\t\tPE, PF = self.EnergyAndForce(x_)\n\t\tBE = 0.0\n\t\tBF = np.zeros(x_.shape)\n\t\tif (self.NBump > 0):\n\t\t\tBE, BF = self.Bumper.Bump(self.BumpCoords.astype(np.float32), x_.astype(np.float32), self.NBump%self.MaxBumps)\n\t\t\tBF[0] *= self.m[:,None]\n\t\treturn BxE+BE+PE,PF+BxF+JOULEPERHARTREE*BF[0]\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\t\t""""""\n\t\tstep = 0\n\t\tbumptimer = self.BumpTime\n\t\tself.md_log = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n\t\twhile(step < self.maxstep):\n\t\t\tt = time.time()\n\t\t\tself.t = step*self.dt\n\t\t\tbumptimer -= self.dt\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\t\t\tif (PARAMS[""MDThermostat""]==None):\n\t\t\t\tself.x , self.v, self.a, self.EPot = VelocityVerletStep(self.BoxForce, self.a, self.x, self.v, self.m, self.dt, self.BoxForce)\n\t\t\telse:\n\t\t\t\tself.x , self.v, self.a, self.EPot, self.force = self.Tstat.step(self.BoxForce, self.a, self.x, self.v, self.m, self.dt, self.BoxForce)\n\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,4] = self.KE\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tself.md_log[step,6] = self.KE+(self.EPot-self.EPot0)*JOULEPERHARTREE\n\n\t\t\tif (bumptimer < 0.0):\n\t\t\t\tself.Bump()\n\t\t\t\tbumptimer = self.BumpTime\n\n\t\t\tif (step%3==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDBoxLog""+self.name+"".txt"",self.md_log)\n\n\t\t\tstep+=1\n\t\t\tLOGGER.info(""Step: %i time: %.1f(fs) <KE>(kJ/mol): %.5f <|a|>(m/s2): %.5f <EPot>(Eh): %.5f <Etot>(kJ/mol): %.5f Teff(K): %.5f"", step, self.t, self.KE/1000.0,  np.linalg.norm(self.a) , self.EPot, self.KE/1000.0+self.EPot*KJPERHARTREE, Teff)\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\treturn\n\nclass NearbyMinima(MetaDynamics):\n\tdef __init__(self,f_,g0_,name_=""MinimaSearch"",EandF_=None):\n\t\t""""""\n\t\tSearches for nearby minima and stores unique new minima.\n\t\toptimizes without the bump every 50fs.\n\n\t\tArgs:\n\t\t\tf_: A routine which returns the force.\n\t\t\tg0_: an initial molecule.\n\t\t\tname_: a name for output.\n\t\t\tEandF_: a routine returning the energy and the force.\n\t\t\tPARAMS[""BowlK""] : a force constant of an attractive potential.\n\t\t""""""\n\t\tVelocityVerlet.__init__(self, f_, g0_, name_, EandF_)\n\t\tself.BumpTime = PARAMS[""MetaBumpTime""]\n\t\tself.MaxBumps = PARAMS[""MetaMaxBumps""]\n\t\tself.bump_height = PARAMS[""MetaMDBumpHeight""]\n\t\tself.bump_width = PARAMS[""MetaMDBumpWidth""]\n\t\tself.BumpCoords = np.zeros((self.MaxBumps,self.natoms,3))\n\t\tself.NBump = 0\n\t\tself.DStat = OnlineEstimator(MolEmb.Make_DistMat(self.x))\n\t\tself.BowlK = PARAMS[""MetaBowlK""]\n\t\tif (self.Tstat.name != ""Andersen""):\n\t\t\tLOGGER.info(""I really recommend you use Andersen Thermostat with Meta-Dynamics."")\n\t\tself.Bumper = TFForces.BumpHolder(self.natoms, self.MaxBumps, self.BowlK, self.bump_height, self.bump_width,""MR"")\n\n\tdef BumpForce(self,x_):\n\t\tBE = 0.0\n\t\tBF = np.zeros(x_.shape)\n\t\tif (self.NBump > 0):\n\t\t\tBE, BF = self.Bumper.Bump(self.BumpCoords.astype(np.float32), x_.astype(np.float32), self.NBump%self.MaxBumps)\n\t\tif (self.EnergyAndForce != None):\n\t\t\tself.RealPot, PF = self.EnergyAndForce(x_)\n\t\telse:\n\t\t\tPF = self.ForceFunction(x_)\n\t\tif self.NBump > 0:\n\t\t\tBF[0] *= self.m[:,None]\n\t\tPF += JOULEPERHARTREE*BF[0]\n\t\tPF = RemoveInvariantForce(x_,PF,self.m)\n\t\treturn BE+self.RealPot, PF\n\n\tdef Bump(self):\n\t\tself.BumpCoords[self.NBump%self.MaxBumps] = self.x\n\t\tself.NBump += 1\n\t\tLOGGER.info(""Bump added!"")\n\t\treturn\n\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\t\t""""""\n\t\tstep = 0\n\t\tbumptimer = self.BumpTime\n\t\tmintimer = 50.0\n\t\tself.md_log = np.zeros((self.maxstep, 11)) # time Dipoles Energy\n\t\tself.configs = []\n\t\twhile(step < self.maxstep):\n\t\t\tt = time.time()\n\t\t\tself.t = step*self.dt\n\t\t\tbumptimer -= self.dt\n\t\t\tmintimer -= self.dt\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\t\t\tif (PARAMS[""MDThermostat""]==None):\n\t\t\t\tself.x , self.v, self.a, self.EPot = VelocityVerletStep(self.BumpForce, self.a, self.x, self.v, self.m, self.dt, self.BumpForce)\n\t\t\telse:\n\t\t\t\tself.x , self.v, self.a, self.EPot, self.force = self.Tstat.step(self.BumpForce, self.a, self.x, self.v, self.m, self.dt, self.BumpForce)\n\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,4] = self.KE\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tself.md_log[step,6] = self.KE+(self.EPot-self.EPot0)*JOULEPERHARTREE\n\t\t\tself.md_log[step,7] = self.RealPot\n\n\t\t\t# Add an averager which accumulates RMS distance. Also, wow the width is small.\n\t\t\tEav, Evar = self.EnergyStat(self.RealPot)\n\t\t\tDav, Dvar = self.DStat(MolEmb.Make_DistMat(self.x))\n\t\t\tself.md_log[step,8] = Eav\n\t\t\tself.md_log[step,9] = Evar\n\t\t\tself.md_log[step,10] = np.linalg.norm(Dvar)\n\n\t\t\tif (bumptimer < 0.0):\n\t\t\t\tself.Bump()\n\t\t\t\tbumptimer = self.BumpTime\n\t\t\tif (mintimer < 0.0):\n\t\t\t\t# optimize off this guess.\n\t\t\t\tOpt()\n\t\t\t\tmintimer=50.0\n\t\t\tif (step%10==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\n\t\t\tLOGGER.info(""Step: %i time: %.1f(fs) KE(kJ/mol): %.5f <|a|>(m/s2): %.5f EPot(Eh): %.5f <EPot(Eh)>: %.5f Etot(kJ/mol): %.5f  <d(D_ij)^2>: %.5f Teff(K): %.5f"",\n\t\t\t\t\tstep, self.t, self.KE/1000.0,  np.linalg.norm(self.a) , self.EPot, Eav, self.md_log[step,6]/1000.0, self.md_log[step,10], Teff)\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\t\tstep+=1\n\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\t\treturn\n'"
TensorMol/Simulations/Neb.py,0,"b'""""""\nChanges that need to be made:\n - do the conservative minimization of nabla L\n - implement Climbing image NEB\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Containers.Sets import *\nfrom ..TFNetworks.TFManage import *\nfrom ..Math.QuasiNewtonTools import *\nfrom ..Math.BFGS import *\nfrom ..Math.DIIS import *\nimport random\nimport time\n\nclass NudgedElasticBand:\n\tdef __init__(self,f_,g0_,g1_,name_=""Neb"",thresh_=None,nbeads_=None):\n\t\t""""""\n\t\tNudged Elastic band. JCP 113 9978\n\n\t\tArgs:\n\t\t\tf_: an energy, force routine (energy Hartree, Force Kcal/ang.)\n\t\t\tg0_: initial molecule.\n\t\t\tg1_: final molecule.\n\n\t\tReturns:\n\t\t\tA reaction path.\n\t\t""""""\n\t\tself.name = name_\n\t\tself.thresh = PARAMS[""OptThresh""]\n\t\tif (thresh_ != None):\n\t\t\tself.thresh= thresh_\n\t\tself.max_opt_step = PARAMS[""OptMaxCycles""]\n\t\tself.nbeads = PARAMS[""NebNumBeads""]\n\t\tif (nbeads_!=None):\n\t\t\tself.nbeads = nbeads_\n\t\tself.k = PARAMS[""NebK""]\n\t\tself.f = f_\n\t\tself.atoms = g0_.atoms.copy()\n\t\tself.natoms = len(self.atoms)\n\t\tself.beads=np.array([(1.-l)*g0_.coords+l*g1_.coords for l in np.linspace(0.,1.,self.nbeads)])\n\t\tself.Fs = np.zeros(self.beads.shape) # Real forces.\n\t\tself.Ss = np.zeros(self.beads.shape) # Spring Forces.\n\t\tself.Ts = np.zeros(self.beads.shape) # Tangents.\n\t\tself.Es = np.zeros(self.nbeads) # As Evaluated.\n\t\tself.Esi = np.zeros(self.nbeads) # Integrated\n\t\tself.Rs = np.zeros(self.nbeads) # Distance between beads.\n\t\tself.Solver=None\n\t\tself.step=0\n\t\tif (PARAMS[""NebSolver""]==""SD""):\n\t\t\tself.Solver = SteepestDescent(self.WrappedEForce,self.beads)\n\t\tif (PARAMS[""NebSolver""]==""Verlet""):\n\t\t\tself.Solver = VerletOptimizer(self.WrappedEForce,self.beads)\n\t\telif (PARAMS[""NebSolver""]==""BFGS""):\n\t\t\tself.Solver = BFGS_WithLinesearch(self.WrappedEForce,self.beads)\n\t\telif (PARAMS[""NebSolver""]==""DIIS""):\n\t\t\tself.Solver = DIIS(self.WrappedEForce, self.beads)\n\t\telif (PARAMS[""NebSolver""]==""CG""):\n\t\t\tself.Solver = ConjGradient(self.WrappedEForce,self.beads)\n\t\telse:\n\t\t\traise Exception(""Missing Neb Solver"")\n\t\tfor i,bead in enumerate(self.beads):\n\t\t\tm=Mol(self.atoms,bead)\n\t\t\tm.WriteXYZfile(""./results/"", ""NebTraj0"")\n\t\treturn\n\tdef Tangent(self,beads_,i):\n\t\tif (i==0 or i==(self.nbeads-1)):\n\t\t\treturn np.zeros(self.beads[0].shape)\n\t\ttm1 = beads_[i] - beads_[i-1]\n\t\ttp1 = beads_[i+1] - beads_[i]\n\t\tt = tm1 + tp1\n\t\tt = t/np.sqrt(np.einsum(\'ia,ia\',t,t))\n\t\treturn t\n\tdef SpringEnergy(self,beads_):\n\t\ttmp = 0.0\n\t\tfor i in range(self.nbeads-1):\n\t\t\ttmp2 = beads_[i+1] - beads_[i]\n\t\t\ttmp += 0.5*self.k*self.nbeads*np.sum(tmp2*tmp2)\n\t\treturn tmp\n\tdef SpringDeriv(self,beads_,i):\n\t\tif (i==0 or i==(self.nbeads-1)):\n\t\t\treturn np.zeros(self.beads[0].shape)\n\t\ttmp = self.k*self.nbeads*(2.0*beads_[i] - beads_[i+1] - beads_[i-1])\n\t\treturn tmp\n\tdef Parallel(self,v_,t_):\n\t\treturn t_*(np.einsum(""ia,ia"",v_,t_))\n\tdef Perpendicular(self,v_,t_):\n\t\treturn (v_ - t_*(np.einsum(""ia,ia"",v_,t_)))\n\tdef BeadAngleCosine(self,beads_,i):\n\t\tv1 = (beads_[i+1] - beads_[i])\n\t\tv2 = (beads_[i-1] - beads_[i])\n\t\treturn np.einsum(\'ia,ia\',v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n\tdef NebForce(self, beads_, i, DoForce = True):\n\t\t""""""\n\t\tThis uses the mixing of Perpendicular spring force\n\t\tto reduce kinks\n\t\t""""""\n\t\tif (i==0 or i==(self.nbeads-1)):\n\t\t\tself.Fs[i] = np.zeros(self.beads[0].shape)\n\t\t\tself.Es[i] = self.f(beads_[i],False)\n\t\telif (DoForce):\n\t\t\tself.Es[i], self.Fs[i] = self.f(beads_[i],DoForce)\n\t\telse:\n\t\t\tself.Es[i] = self.f(beads_[i],DoForce)\n\t\t# Compute the spring part of the energy.\n\t\tif (not DoForce):\n\t\t\treturn self.Es[i]\n\t\tt = self.Tangent(beads_,i)\n\t\tself.Ts[i] = t\n\t\tS = -1.0*self.SpringDeriv(beads_,i)\n\t\tSpara = self.Parallel(S,t)\n\t\tself.Ss[i] = Spara\n\t\tF = self.Fs[i].copy()\n\t\tF = self.Perpendicular(F,t)\n\t\t# Instead use Wales\' DNEB\n\t\tif (0):\n\t\t\tif (np.linalg.norm(F) != 0.0):\n\t\t\t\tFn = F/np.linalg.norm(F)\n\t\t\telse:\n\t\t\t\tFn = F\n\t\t\tSperp = self.Perpendicular(self.Perpendicular(S,t),Fn)\n\t\t\t#Fneb = self.PauliForce(i)+Spara+Sperp+F\n\t\tFneb = Spara+F\n\t\t# If enabled and this is the TS bead, do the climbing image.\n\t\tif (PARAMS[""NebClimbingImage""] and self.step>10 and i==self.TSI):\n\t\t\tprint(""Climbing image i"", i)\n\t\t\tFneb = self.Fs[i] + -2.0*np.sum(self.Fs[i]*self.Ts[i])*self.Ts[i]\n\t\treturn self.Es[i], Fneb\n\tdef WrappedEForce(self, beads_, DoForce=True):\n\t\tF = np.zeros(beads_.shape)\n\t\tif (DoForce):\n\t\t\tfor i,bead in enumerate(beads_):\n\t\t\t\t#print(DoForce,self.NebForce(bead,i,DoForce))\n\t\t\t\tself.Es[i], F[i] = self.NebForce(beads_,i,DoForce)\n\t\t\t\tF[i] = RemoveInvariantForce(bead, F[i], self.atoms)\n\t\t\t\tF[i] /= JOULEPERHARTREE\n\t\t\tTE = np.sum(self.Es)+self.SpringEnergy(beads_)\n\t\t\treturn TE,F\n\t\telse:\n\t\t\tfor i,bead in enumerate(beads_):\n\t\t\t\tself.Es[i] = self.NebForce(beads_,i,DoForce)\n\t\t\tTE = np.sum(self.Es)+self.SpringEnergy(beads_)\n\t\t\treturn TE\n\tdef IntegrateEnergy(self):\n\t\t""""""\n\t\tUse the fundamental theorem of line integrals to calculate an energy.\n\t\tAn interpolated path could improve this a lot.\n\t\t""""""\n\t\tself.Esi[0] = self.Es[0]\n\t\tfor i in range(1,self.nbeads):\n\t\t\tdR = self.beads[i] - self.beads[i-1]\n\t\t\tdV = -1*(self.Fs[i] + self.Fs[i-1])/2. # midpoint rule.\n\t\t\tself.Esi[i] = self.Esi[i-1]+np.einsum(""ia,ia"",dR,dV)\n\tdef HighQualityPES(self,npts_ = 100):\n\t\t""""""\n\t\tDo a high-quality integration of the path and forces.\n\t\t""""""\n\t\tfrom scipy.interpolate import CubicSpline\n\t\tls = np.linspace(0.,1.,self.nbeads)\n\t\tRint = CubicSpline(self.beads)\n\t\tFint = CubicSpline(self.Fs)\n\t\tEs = np.zeros(npts_)\n\t\tEs[0] = 0\n\t\tls = np.linspace(0.,1.,npts_)\n\t\tfor i,l in enumerate(ls):\n\t\t\tif (i==0):\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tEs[i] = Es[i-1] + np.einsum(""ia,ia"", Rint(l) - Rint(ls[i-1]), -1.0*Fint(l))\n\t\t\tm=Mol(self.atoms,Rint(l))\n\t\t\tm.properties[""Energy""] = Es[i]\n\t\t\tm.properties[""Force""] = Fint(l)\n\t\t\tm.WriteXYZfile(""./results/"", ""NebHQTraj"")\n\tdef WriteTrajectory(self,nm_):\n\t\tfor i,bead in enumerate(self.beads):\n\t\t\tm=Mol(self.atoms,bead)\n\t\t\tm.WriteXYZfile(""./results/"", ""Bead""+str(i))\n\t\tfor i,bead in enumerate(self.beads):\n\t\t\tm=Mol(self.atoms,bead)\n\t\t\tm.properties[""bead""] = i\n\t\t\tm.properties[""Energy""] = self.Es[i]\n\t\t\tm.properties[""NormNebForce""]=np.linalg.norm(self.Fs[i])\n\t\t\tm.WriteXYZfile(""./results/"", nm_+""Traj"")\n\t\treturn\n\tdef Opt(self, filename=""Neb"",Debug=False):\n\t\t""""""\n\t\tOptimize the nudged elastic band using the solver that has been selected.\n\t\t""""""\n\t\t# Sweeps one at a time\n\t\tself.step=0\n\t\tself.Fs = np.ones(self.beads.shape)\n\t\tPES = np.zeros((self.max_opt_step, self.nbeads))\n\t\twhile(self.step < self.max_opt_step and np.sqrt(np.mean(self.Fs*self.Fs))>self.thresh):\n\t\t\t# Update the positions of every bead together.\n\t\t\tself.beads, energy, self.Fs = self.Solver(self.beads)\n\t\t\tPES[self.step] = self.Es.copy()\n\t\t\tself.IntegrateEnergy()\n\t\t\tprint(""Rexn Profile: "", self.Es, self.Esi)\n\t\t\tself.TSI = np.argmax(self.Es)\n\t\t\tbeadFs = [np.linalg.norm(x) for x in self.Fs[1:-1]]\n\t\t\tbeadFperp = [np.linalg.norm(self.Perpendicular(self.Fs[i],self.Ts[i])) for i in range(1,self.nbeads-1)]\n\t\t\tbeadRs = [np.linalg.norm(self.beads[x+1]-self.beads[x]) for x in range(self.nbeads-1)]\n\t\t\tbeadCosines = [self.BeadAngleCosine(self.beads,i) for i in range(1,self.nbeads-1)]\n\t\t\tprint(""Frce Profile: "", beadFs)\n\t\t\tprint(""F_|_ Profile: "", beadFperp)\n\t\t\t#print(""SFrc Profile: "", beadSfs)\n\t\t\tprint(""Dist Profile: "", beadRs)\n\t\t\tprint(""BCos Profile: "", beadCosines)\n\t\t\tminforce = np.min(beadFs)\n\t\t\t\t#rmsdisp[i] = np.sum(np.linalg.norm((prev_m.coords-m.coords),axis=1))/m.coords.shape[0]\n\t\t\t\t#maxdisp[i] = np.amax(np.linalg.norm((prev_m.coords - m.coords), axis=1))\n\t\t\tif (self.step%10==0):\n\t\t\t\tself.WriteTrajectory(filename)\n\t\t\tLOGGER.info(self.name+""Step: %i Objective: %.5f RMS Gradient: %.5f  Max Gradient: %.5f |F_perp| : %.5f |F_spring|: %.5f "", self.step, np.sum(PES[self.step]), np.sqrt(np.mean(self.Fs*self.Fs)), np.max(self.Fs),np.mean(beadFperp),np.linalg.norm(self.Ss))\n\t\t\tself.step+=1\n\t\t#self.HighQualityPES()\n\t\tprint(""Activation Energy:"",np.max(self.Es)-np.min(self.Es))\n\t\tnp.savetxt(""./results/NEB_""+filename+""_Energy.txt"",PES)\n\t\treturn self.beads\n'"
TensorMol/Simulations/Opt.py,0,"b'""""""\nWe should get all references to TFManage out of this\nand just pass a EnergyAndForce Field function.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Containers.Sets import *\nfrom ..TFNetworks.TFManage import *\nfrom ..Math.QuasiNewtonTools import *\nfrom ..Math.DIIS import *\nfrom ..Math.BFGS import *\nfrom ..Math.LinearOperations import *\nfrom ..ForceModels import *\nimport random\nimport time\n\nclass GeomOptimizer:\n\tdef __init__(self,f_):\n\t\t""""""\n\t\tGeometry optimizations based on NN-PES\'s etc.\n\n\t\tArgs:\n\t\t\tf_: An EnergyForce routine\n\t\t""""""\n\t\tself.thresh = PARAMS[""OptThresh""]\n\t\tself.maxstep = PARAMS[""OptMaxStep""]\n\t\tself.fscale = PARAMS[""OptStepSize""]\n\t\tself.momentum = PARAMS[""OptMomentum""]\n\t\tself.momentum_decay = PARAMS[""OptMomentumDecay""]\n\t\tself.max_opt_step = PARAMS[""OptMaxCycles""]\n\t\tself.step = self.maxstep\n\t\tself.EnergyAndForce = f_\n\t\tself.m = None\n\t\treturn\n\n\tdef WrappedEForce(self,x_,DoForce=True):\n\t\tif (DoForce):\n\t\t\tenergy, frc = self.EnergyAndForce(x_, DoForce)\n\t\t\tfrc = RemoveInvariantForce(x_, frc, self.m.atoms)\n\t\t\tfrc /= JOULEPERHARTREE\n\t\t\treturn energy, frc\n\t\telse:\n\t\t\tenergy = self.EnergyAndForce(x_,False)\n\t\t\treturn energy\n\n\tdef Opt(self,m_, filename=""OptLog"",Debug=False):\n\t\t""""""\n\t\tOptimize using An EnergyAndForce Function with conjugate gradients.\n\n\t\tArgs:\n\t\t\tm: A distorted molecule to optimize\n\t\t""""""\n\t\tm = Mol(m_.atoms,m_.coords)\n\t\tself.m = m\n\t\trmsdisp = 10.0\n\t\trmsgrad = 10.0\n\t\tstep=0\n\t\tmol_hist = []\n\t\tprev_m = Mol(m.atoms, m.coords)\n\t\tprint(""Orig Mol:\\n"", m)\n\t\tCG = ConjGradient(self.WrappedEForce, m.coords)\n\t\twhile( step < self.max_opt_step and rmsgrad > self.thresh and (rmsdisp > 0.000001 or step<5) ):\n\t\t\tprev_m = Mol(m.atoms, m.coords)\n\t\t\tm.coords, energy, frc = CG(m.coords)\n\t\t\trmsgrad = np.sum(np.linalg.norm(frc,axis=1))/m.coords.shape[0]\n\t\t\trmsdisp = np.sum(np.linalg.norm(m.coords-prev_m.coords,axis=1))/m.coords.shape[0]\n\t\t\tLOGGER.info(filename+""step: %i energy: %0.5f rmsgrad: %0.5f rmsdisp: %0.5f "", step , energy, rmsgrad, rmsdisp)\n\t\t\tmol_hist.append(prev_m)\n\t\t\tprev_m.properties[""Step""] = step\n\t\t\tprev_m.properties[""Energy""] = energy\n\t\t\tprev_m.WriteXYZfile(""./results/"", filename,\'a\',True)\n\t\t\tstep+=1\n\t\t# Checks stability in each cartesian direction.\n\t\tprint(""Final Energy:"", self.EnergyAndForce(prev_m.coords,False))\n\t\treturn prev_m\n\n\tdef BumpForce(self,x_):\n\t\tBE = 0.0\n\t\tBF = np.zeros(x_.shape)\n\t\tif (self.NBump > 0):\n\t\t\tBE, BF = self.Bumper.Bump(self.BumpCoords.astype(np.float32), x_.astype(np.float32), self.NBump%self.MaxBumps)\n\t\tif (self.EnergyAndForce != None):\n\t\t\tself.RealPot, PF = self.EnergyAndForce(x_)\n\t\telse:\n\t\t\tPF = self.ForceFunction(x_)\n\t\tif self.NBump > 0:\n\t\t\tBF[0] *= self.m[:,None]\n\t\tPF += JOULEPERHARTREE*BF[0]\n\t\tPF = RemoveInvariantForce(x_,PF,self.m)\n\t\treturn BE+self.RealPot, PF\n\n\tdef Bump(self):\n\t\tself.BumpCoords[self.NBump%self.MaxBumps] = self.x\n\t\tself.NBump += 1\n\t\tLOGGER.info(""Bump added!"")\n\t\treturn\n\n\tdef Opt_LS(self,m, filename=""OptLog"",Debug=False):\n\t\t""""""\n\t\tOptimize with Steepest Descent + Line search using An EnergyAndForce Function.\n\n\t\tArgs:\n\t\t        m: A distorted molecule to optimize\n\t\t""""""\n\t\t# Sweeps one at a time\n\t\trmsdisp = 10.0\n\t\tmaxdisp = 10.0\n\t\trmsgrad = 10.0\n\t\tstep=0\n\t\tmol_hist = []\n\t\tprev_m = Mol(m.atoms, m.coords)\n\t\tprint(""Orig Coords"", m.coords)\n\t\t#print ""Initial force"", self.tfm.evaluate(m, i), ""Real Force"", m.properties[""forces""][i]\n\t\tveloc=np.zeros(m.coords.shape)\n\t\told_veloc=np.zeros(m.coords.shape)\n\t\tEnergy = lambda x_: self.EnergyAndForce(x_)[0]\n\t\twhile( step < self.max_opt_step and rmsgrad > self.thresh):\n\t\t\tprev_m = Mol(m.atoms, m.coords)\n\t\t\tenergy, frc = self.EnergyAndForce(m.coords)\n\t\t\tfrc = RemoveInvariantForce(m.coords, frc, m.atoms)\n\t\t\tfrc /= JOULEPERHARTREE\n\t\t\trmsgrad = np.sum(np.linalg.norm(frc,axis=1))/frc.shape[0]\n\t\t\tm.coords = LineSearch(Energy, m.coords, frc)\n\t\t\trmsdisp = np.sum(np.linalg.norm(m.coords-prev_m.coords,axis=1))/veloc.shape[0]\n\t\t\tprint(""step: "", step ,"" energy: "", energy, "" rmsgrad "", rmsgrad, "" rmsdisp "", rmsdisp)\n\t\t\tmol_hist.append(prev_m)\n\t\t\tprev_m.WriteXYZfile(""./results/"", filename)\n\t\t\tstep+=1\n\t\t# Checks stability in each cartesian direction.\n\t\t#prev_m.coords = LineSearchCart(Energy, prev_m.coords)\n\t\tprint(""Final Energy:"", self.EnergyAndForce(prev_m.coords,False))\n\t\treturn prev_m\n\n\tdef OptGD(self,m_, filename=""GDOptLog"",Debug=False):\n\t\t""""""\n\t\tOptimize using steepest descent  and an EnergyAndForce Function.\n\n\t\tArgs:\n\t\t        m: A distorted molecule to optimize\n\t\t""""""\n\t\t# Sweeps one at a time\n\t\trmsdisp = 10.0\n\t\trmsgrad = 10.0\n\t\tstep=0\n\t\tndives = 0\n\t\tm = Mol(m_.atoms,m_.coords)\n\t\tmol_hist = []\n\t\tprev_m = Mol(m.atoms, m.coords)\n\t\tprint(""Orig Coords"", m.coords)\n\t\t#print ""Initial force"", self.tfm.evaluate(m, i), ""Real Force"", m.properties[""forces""][i]\n\t\tenergy, old_frc  = self.WrappedEForce(m.coords)\n\t\twhile( step < self.max_opt_step and rmsgrad > self.thresh):\n\t\t\tprev_m = Mol(m.atoms, m.coords)\n\t\t\tif step > 0:\n\t\t\t\told_frc = frc\n\t\t\tenergy, frc = self.WrappedEForce(m.coords)\n\t\t\tif (np.sum(frc*old_frc)<0.0):\n\t\t\t\told_frc *= 0.0\n\t\t\trmsgrad = np.sum(np.linalg.norm(frc,axis=1))/frc.shape[0]\n\t\t\tfrc += self.momentum*old_frc\n\t\t\tm.coords = m.coords + self.fscale*frc\n\t\t\trmsdisp = np.sum(np.linalg.norm(m.coords-prev_m.coords,axis=1))/m.coords.shape[0]\n\t\t\tLOGGER.info(filename+""step: %i energy: %0.5f rmsgrad: %0.5f rmsdisp: %0.5f "", step , energy, rmsgrad, rmsdisp)\n\t\t\tmol_hist.append(prev_m)\n\t\t\tprev_m.WriteXYZfile(""./results/"", filename)\n\t\t\tstep+=1\n\t\t# Checks stability in each cartesian direction.\n\t\t#prev_m.coords = LineSearchCart(Energy, prev_m.coords)\n\t\treturn prev_m\n\nclass MetaOptimizer(GeomOptimizer):\n\tdef __init__(self,f_,m,StopAfter_=20,Box_=False,OnlyHev_=True):\n\t\t""""""\n\t\tA Meta-Optimizer performs nested optimization.\n\t\tThe outer loop has a bump potential to find new initial geometries.\n\t\tthe inner loop digs down to new minima.\n\t\tit saves the record of minima it reaches.\n\t\tThe default parameters are tuned to find nearby reactions.\n\t\tConformational search should probably be done with internal coordinates\n\t\tnot distance matrices.\n\n\t\tArgs:\n\t\t\tf_: An EnergyForce routine\n\t\t\tm: a molecules\n\t\t\tStopAfter_: Look for this many nearby minima.\n\t\t\tBox_: whether to use a box.\n\t\t\tOnlyHev_: whether to only bump heavy atom bonds.\n\t\t""""""\n\t\tGeomOptimizer.__init__(self,f_)\n\t\tself.thresh = PARAMS[""OptThresh""]*5.0\n\t\tself.StopAfter = StopAfter_\n\t\tself.OnlyHev = OnlyHev_\n\t\tself.m = m\n\t\tself.fscale = 0.3\n\t\tself.momentum = 0.3\n\t\tself.thresh = 0.005\n\t\tself.masses = np.array(map(lambda x: ATOMICMASSES[x-1], m.atoms))\n\t\tself.natoms = m.NAtoms()\n\t\tself.MaxBumps = PARAMS[""MetaMaxBumps""] # think you want this to be >500k\n\t\tself.BumpCoords = np.zeros((self.MaxBumps,self.natoms,3))\n\t\tself.MinimaCoords = np.zeros((self.StopAfter,self.natoms,3))\n\t\tself.NMinima = 0\n\t\tself.NBump = 0\n\t\tself.UseBox = Box_\n\t\tself.Boxer = TFForces.BoxHolder(self.natoms)\n\t\tself.lastbumpstep = 0\n\t\t# just put the atoms in a box the size of their max and min coordinates.\n\t\tself.Box =  Box_=np.array((np.max(m.coords)+0.1)*np.eye(3))\n\t\tself.BowlK = 0.01\n\t\t#self.Bumper = TFForces.BumpHolder(self.natoms, self.MaxBumps, self.BowlK, h_=1.0, w_=1.2,Type_=""MR"")\n\t\tself.Bumper = TFForces.BumpHolder(self.natoms, self.MaxBumps, self.BowlK, h_=0.5, w_=0.6,Type_=""MR"")\n\t\treturn\n\n\tdef WrappedBumpedEForce(self, x_ ,DoForce = True, DoBump=True):\n\t\tPE,PF = None, None\n\t\tif (DoForce):\n\t\t\tPE, PF = self.EnergyAndForce(x_, DoForce)\n\t\t\tif (not DoBump):\n\t\t\t\treturn PE,PF\n\t\telse:\n\t\t\tPE = self.EnergyAndForce(x_, DoForce)\n\t\t\tif (not DoBump):\n\t\t\t\treturn PE\n\t\tBxE = 0.0\n\t\tBxF = np.zeros(x_.shape)\n\t\tif (self.UseBox):\n\t\t\tBxE, BxF = self.Boxer(x_, self.Box)\n\t\t\tBxF *= -5.0*JOULEPERHARTREE#*(self.masses[:,None]/np.sqrt(np.sum(self.masses*self.masses)))\n\t\t#print(""Box Force"",np.max(x_),np.max(BxF),BxE)\n\t\tBE = 0.0\n\t\tBF = np.zeros(x_.shape)\n\t\tif (self.NBump > 0):\n\t\t\tBE, BF = self.Bumper.Bump(self.BumpCoords.astype(np.float32), x_.astype(np.float32), self.NBump%self.MaxBumps)\n\t\t\tBF = JOULEPERHARTREE*BF[0]\n\t\t\tif (self.OnlyHev):\n\t\t\t\tfor i in range(self.m.NAtoms()):\n\t\t\t\t\tif (self.m.atoms[i]==1):\n\t\t\t\t\t\tBF[i] *= 0.0\n\t\tif (DoForce):\n\t\t\tfrc = PF+BF+BxE\n\t\t\tfrc = RemoveInvariantForce(x_, frc, self.m.atoms)\n\t\t\tfrc /= JOULEPERHARTREE\n\t\t\trmsgrad = np.sum(np.linalg.norm(PF,axis=1))/PF.shape[0]\n\t\t\trmsgradb = np.sum(np.linalg.norm(BF,axis=1))/PF.shape[0]\n\t\t\tprint(rmsgradb,rmsgrad)\n\n\t\t\treturn BE+PE+BxE,frc\n\t\telse:\n\t\t\treturn BE+PE+BxE\n\n\tdef Bump(self,x_):\n\t\tself.BumpCoords[self.NBump%self.MaxBumps] = x_\n\t\tself.NBump += 1\n\t\tLOGGER.info(""Bump added!"")\n\t\treturn\n\n\tdef MetaOptCG(self,m, filename=""MetaOptLog"",Debug=False, SearchConfs_=False):\n\t\t""""""\n\t\tOptimize using An EnergyAndForce Function with conjugate gradients.\n\n\t\tArgs:\n\t\t\tm: A distorted molecule to optimize\n\t\t""""""\n\t\tself.m = m\n\t\trmsdisp = 10.0\n\t\trmsgrad = 10.0\n\t\tstep=0\n\t\tmol_hist = []\n\t\tndives=0\n\t\tprev_m = Mol(m.atoms, m.coords)\n\t\tprint(""Orig Mol:\\n"", m)\n\t\tBM = m.BondMatrix()\n\t\tCG = ConjGradient(self.WrappedBumpedEForce, m.coords)\n\t\twhile(step < self.max_opt_step):\n\t\t\twhile( step < self.max_opt_step and rmsgrad > self.thresh and (rmsdisp > 0.000001 or step<5) ):\n\t\t\t\tprev_m = Mol(m.atoms, m.coords)\n\t\t\t\tm.coords, energy, frc = CG(m.coords)\n\t\t\t\trmsgrad = np.sum(np.linalg.norm(frc,axis=1))/m.coords.shape[0]\n\t\t\t\trmsdisp = np.sum(np.linalg.norm(m.coords-prev_m.coords,axis=1))/m.coords.shape[0]\n\t\t\t\tLOGGER.info(filename+""step: %i energy: %0.5f rmsgrad: %0.5f rmsdisp: %0.5f "", step , energy, rmsgrad, rmsdisp)\n\t\t\t\tmol_hist.append(prev_m)\n\t\t\t\tprev_m.properties[""Step""] = step\n\t\t\t\tprev_m.properties[""Energy""] = energy\n\t\t\t\tprev_m.WriteXYZfile(""./results/"", filename,\'a\',True)\n\t\t\t\tstep+=1\n\t\t\tself.Bump(m.coords)\n\t\t\tm.Distort(0.01)\n\t\t\tif ((BM != prev_m.BondMatrix()).any() or SearchConfs_):\n\t\t\t\td = self.Opt(prev_m,""Dive""+str(ndives))\n\t\t\t\tBM = prev_m.BondMatrix()\n\t\t\t\tself.AppendIfNew( d )\n\t\t\t\tself.Bump(d.coords)\n\t\t\t\tndives += 1\n\t\t\trmsdisp = 10.0\n\t\t\trmsgrad = 10.0\n\t\t\tstep=0\n\t\t\tPARAMS[""GSSearchAlpha""]=0.1\n\t\t\tCG = ConjGradient(self.WrappedBumpedEForce, m.coords)\n\t\tprint(""Final Energy:"", self.EnergyAndForce(prev_m.coords,False))\n\t\treturn prev_m\n\n\tdef MetaOpt(self,m_=None, filename=""MetaOptLog"",Debug=False, SearchConfs_=False):\n\t\t""""""\n\t\tOptimize using steepest descent  and an EnergyAndForce Function.\n\n\t\tArgs:\n\t\t        m: A distorted molecule to optimize\n\t\t""""""\n\t\t# Sweeps one at a time\n\t\trmsdisp = 10.0\n\t\trmsgrad = 10.0\n\t\tstep=0\n\t\tndives = 0\n\t\tm = Mol(self.m.atoms,self.m.coords)\n\t\tif (m_ != None):\n\t\t\tm = Mol(m_.atoms,m_.coords)\n\t\tmol_hist = []\n\t\tprev_m = Mol(m.atoms, m.coords)\n\t\tprint(""Orig Coords"", m.coords)\n\t\t#print ""Initial force"", self.tfm.evaluate(m, i), ""Real Force"", m.properties[""forces""][i]\n\t\tenergy, old_frc  = self.WrappedBumpedEForce(m.coords)\n\t\tBM = m.BondMatrix()\n\t\twhile(self.NMinima < self.StopAfter):\n\t\t\twhile( step < self.max_opt_step and rmsgrad > self.thresh):\n\t\t\t\tprev_m = Mol(m.atoms, m.coords)\n\t\t\t\tif step > 0:\n\t\t\t\t\told_frc = frc\n\t\t\t\tenergy, frc = self.WrappedBumpedEForce(m.coords)\n\t\t\t\tif (np.sum(frc*old_frc)<0.0):\n\t\t\t\t\told_frc *= 0.0\n\t\t\t\trmsgrad = np.sum(np.linalg.norm(frc,axis=1))/frc.shape[0]\n\t\t\t\tfrc += self.momentum*old_frc\n\t\t\t\tm.coords = m.coords + self.fscale*frc\n\t\t\t\trmsdisp = np.sum(np.linalg.norm(m.coords-prev_m.coords,axis=1))/m.coords.shape[0]\n\t\t\t\tLOGGER.info(filename+""step: %i energy: %0.5f rmsgrad: %0.5f rmsdisp: %0.5f "", step , energy, rmsgrad, rmsdisp)\n\t\t\t\tmol_hist.append(prev_m)\n\t\t\t\tprev_m.WriteXYZfile(""./results/"", filename)\n\t\t\t\tstep+=1\n\t\t\tself.Bump(m.coords)\n\t\t\tm.Distort(0.01)\n\t\t\tif ((BM != prev_m.BondMatrix()).any() or SearchConfs_):\n\t\t\t\td = self.OptGD(prev_m,""Dive""+str(ndives))\n\t\t\t\tBM = prev_m.BondMatrix()\n\t\t\t\tself.AppendIfNew( d )\n\t\t\t\tself.Bump(d.coords)\n\t\t\t\tndives += 1\n\t\t\trmsdisp = 10.0\n\t\t\trmsgrad = 10.0\n\t\t\tstep=0\n\t\t\tPARAMS[""GSSearchAlpha""]=0.1\n\t\t# Checks stability in each cartesian direction.\n\t\t#prev_m.coords = LineSearchCart(Energy, prev_m.coords)\n\t\treturn self.MinimaCoords\n\n\tdef AppendIfNew(self,m):\n\t\toverlaps = []\n\t\tif (self.NMinima==0):\n\t\t\tprint(""New Configuration!"")\n\t\t\tm.WriteXYZfile(""./results/"",""NewMin""+str(self.NMinima))\n\t\t\tself.MinimaCoords[self.NMinima] = m.coords\n\t\t\tself.NMinima += 1\n\t\t\tself.Bump(m.coords)\n\t\t\treturn\n\t\tfor i in range(self.NMinima):\n\t\t\tmdm = MolEmb.Make_DistMat(self.MinimaCoords[i])\n\t\t\todm = MolEmb.Make_DistMat(m.coords)\n\t\t\ttmp = (mdm-odm)\n\t\t\toverlaps.append(np.sqrt(np.sum(tmp*tmp)/(mdm.shape[0]*mdm.shape[0])))\n\t\tif (min(overlaps) > self.thresh):\n\t\t\tprint(""New Configuration!"")\n\t\t\tm.WriteXYZfile(""./results/"",""NewMin""+str(self.NMinima))\n\t\t\tself.MinimaCoords[self.NMinima] = m.coords.copy()\n\t\t\tself.NMinima += 1\n\t\t\tself.Bump(m.coords)\n\t\telse:\n\t\t\tprint(""Overlaps"", overlaps)\n\t\treturn\n'"
TensorMol/Simulations/OptPeriodic.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\nfrom .Opt import *\nfrom ..ForceModifiers.Periodic import *\nimport random\nimport time\n\nclass PeriodicGeomOptimizer(GeomOptimizer):\n\tdef __init__(self,f_):\n\t\t""""""\n\t\tPeriodic Geometry Optimizations.\n\t\tTakes a periodic force f_\n\n\t\tArgs:\n\t\t\tf_: A PeriodicForce object\n\t\t""""""\n\t\tself.thresh = PARAMS[""OptThresh""]\n\t\tself.maxstep = PARAMS[""OptMaxStep""]\n\t\tself.fscale = PARAMS[""OptStepSize""]\n\t\tself.momentum = PARAMS[""OptMomentum""]\n\t\tself.momentum_decay = PARAMS[""OptMomentumDecay""]\n\t\tself.max_opt_step = PARAMS[""OptMaxCycles""]\n\t\tself.step = self.maxstep\n\t\tself.EnergyAndForce = f_\n\t\treturn\n\tdef Opt(self,m, filename=""PdicOptLog"",Debug=False):\n\t\t""""""\n\t\tOptimize using An EnergyAndForce Function.\n\n\t\tArgs:\n\t\t\tm: A distorted molecule to optimize\n\t\t""""""\n\t\t# Sweeps one at a time\n\t\tPARAMS[""OptLatticeStep""] = 0.050\n\t\trmsdisp = 10.0\n\t\tmaxdisp = 10.0\n\t\trmsgrad = 10.0\n\t\tmaxgrad = 10.0\n\t\tstep=0\n\t\tmol_hist = []\n\t\tprev_m = Mol(m.atoms, m.coords)\n\t\tprint(""Orig Mol:\\n"", m)\n\t\tdef WrappedEForce(x_,DoForce=True):\n\t\t\tif (DoForce):\n\t\t\t\tenergy, frc = self.EnergyAndForce(x_, DoForce)\n\t\t\t\tfrc = RemoveInvariantForce(x_, frc, m.atoms)\n\t\t\t\tfrc /= JOULEPERHARTREE\n\t\t\t\treturn energy, frc\n\t\t\telse:\n\t\t\t\tenergy = self.EnergyAndForce(x_,False)\n\t\t\t\treturn energy\n\t\tCG = ConjGradient(WrappedEForce, m.coords)\n\t\tDensity = self.EnergyAndForce.Density()\n\t\twhile( step < self.max_opt_step and rmsgrad > self.thresh and (rmsdisp > 0.0001 or step < 3)):\n\t\t\tprev_m = Mol(m.atoms, m.coords)\n\t\t\tm.coords, energy, frc = CG(m.coords)\n\t\t\trmsgrad = np.sum(np.linalg.norm(frc,axis=1))/m.coords.shape[0]\n\t\t\trmsdisp = np.sum(np.linalg.norm(m.coords-prev_m.coords,axis=1))/m.coords.shape[0]\n\t\t\tm.coords = self.EnergyAndForce.lattice.ModuloLattice(m.coords)\n\t\t\tprint(""step: "", step ,"" energy: "", energy,"" density: "", Density, "" rmsgrad "", rmsgrad, "" rmsdisp "", rmsdisp)\n\t\t\tmol_hist.append(prev_m)\n\t\t\tprev_m.properties[\'Lattice\']=self.EnergyAndForce.lattice.lattice.copy()\n\t\t\tprev_m.WriteXYZfile(""./results/"", filename,\'a\',True)\n\t\t\tMol(*self.EnergyAndForce.lattice.TessNTimes(prev_m.atoms,prev_m.coords,2)).WriteXYZfile(""./results/"", ""Tess""+filename,\'a\',wprop=True)\n\t\t\tstep+=1\n\t\t# Checks stability in each cartesian direction.\n\t\t#prev_m.coords = LineSearchCart(Energy, prev_m.coords)\n\t\tprint(""Final Energy:"", self.EnergyAndForce(prev_m.coords,False))\n\t\tself.EnergyAndForce.Save(prev_m.coords,""FinalPeriodicOpt"")\n\t\tself.EnergyAndForce.mol0.coords = prev_m.coords.copy()\n\t\treturn prev_m\n\n\tdef OptToDensity(self, m, rho_target=1.0, filename=""PdicOptLog"",Debug=False):\n\t\t""""""\n\t\tOptimize using An EnergyAndForce Function.\n\t\tSqueeze the lattice gently such that the atoms achieve a target density.\n\n\t\tArgs:\n\t\t\tm: A distorted molecule to optimize\n\t\t\trho_target: Target density (g/cm**3)\n\t\t""""""\n\t\t# Sweeps one at a time\n\t\tPARAMS[""OptLatticeStep""] = 0.050\n\t\trmsdisp = 10.0\n\t\tmaxdisp = 10.0\n\t\trmsgrad = 10.0\n\t\tmaxgrad = 10.0\n\t\tstep=0\n\t\tmol_hist = []\n\t\tprev_m = Mol(m.atoms, m.coords)\n\t\tprint(""Orig Coords"", m.coords)\n\t\t#print ""Initial force"", self.tfm.evaluate(m, i), ""Real Force"", m.properties[""forces""][i]\n\t\tveloc=np.zeros(m.coords.shape)\n\t\told_veloc=np.zeros(m.coords.shape)\n\t\tEnergy = lambda x_: self.EnergyAndForce(x_)[0]\n\t\tDensity = self.EnergyAndForce.Density()\n\t\tdef WrappedEForce(x_,DoForce=True):\n\t\t\tif (DoForce):\n\t\t\t\tenergy, frc = self.EnergyAndForce(x_, DoForce)\n\t\t\t\tfrc = RemoveInvariantForce(x_, frc, m.atoms)\n\t\t\t\tfrc /= JOULEPERHARTREE\n\t\t\t\treturn energy, frc\n\t\t\telse:\n\t\t\t\tenergy = self.EnergyAndForce(x_,False)\n\t\t\t\treturn energy\n\t\tCG = ConjGradient(WrappedEForce, m.coords)\n\t\twhile (abs(Density-rho_target) > 0.001):\n\t\t\tstep = 0\n\t\t\tDensity = self.EnergyAndForce.Density()\n\t\t\tfac = rho_target/Density\n\t\t\toldlat = self.EnergyAndForce.lattice.lattice.copy()\n\t\t\tnewlat = 0.65*oldlat + 0.35*(oldlat*pow(1.0/fac,1.0/3.))\n\t\t\tm.coords = self.EnergyAndForce.AdjustLattice(m.coords,oldlat,newlat)\n\t\t\tself.EnergyAndForce.ReLattice(newlat)\n\t\t\trmsgrad = 10.0\n\t\t\trmsdisp = 10.0\n\t\t\twhile( step < self.max_opt_step and rmsgrad > self.thresh and rmsdisp > 0.0001 ):\n\t\t\t\tprev_m = Mol(m.atoms, m.coords)\n\t\t\t\tm.coords, energy, frc = CG(m.coords)\n\t\t\t\trmsgrad = np.sum(np.linalg.norm(frc,axis=1))/veloc.shape[0]\n\t\t\t\trmsdisp = np.sum(np.linalg.norm(m.coords-prev_m.coords,axis=1))/veloc.shape[0]\n\t\t\t\tm.coords = self.EnergyAndForce.lattice.ModuloLattice(m.coords)\n\t\t\t\tprint(""step: "", step ,"" energy: "", energy,"" density: "", Density, "" rmsgrad "", rmsgrad, "" rmsdisp "", rmsdisp)\n\t\t\t\tmol_hist.append(prev_m)\n\t\t\t\tprev_m.properties[\'Lattice\']=self.EnergyAndForce.lattice.lattice.copy()\n\t\t\t\tprev_m.WriteXYZfile(""./results/"", filename,\'a\',True)\n\t\t\t\t#Mol(*self.EnergyAndForce.lattice.TessNTimes(prev_m.atoms,prev_m.coords,2)).WriteXYZfile(""./results/"", ""Tess""+filename,\'a\',wprop=True)\n\t\t\t\tstep+=1\n\t\t# Checks stability in each cartesian direction.\n\t\t#prev_m.coords = LineSearchCart(Energy, prev_m.coords)\n\t\tprint(""Final Energy:"", Energy(prev_m.coords))\n\t\tself.EnergyAndForce.Save(prev_m.coords,""FinalPeriodicOpt"")\n\t\tself.EnergyAndForce.mol0.coords = prev_m.coords.copy()\n\t\treturn prev_m\n\ndef Solvate(f_,m1_,m2_,shells_=3,rho_target=0.87):\n\t""""""\n\tTakes a PeriodicForce which includes an initial molecule, and solvates it with\n\tshells_ of M2. and optimized to a desired density.\n\t""""""\n\txmn1 = min(np.min(PF_.mol0.coords),np.min(m2_.coords))\n\txmx1 = max(np.max(PF_.mol0.coords),np.max(m2_.coords))\n\tdx = xmx1-xmn1\n\tlat = Lattice(np.array([[dx,0.,0.],[0.,dx,0.],[0.,0.,dx]]))\n\tm = lat.CenteredInLattice(m2_)\n\tnat = m1_.NAtoms()\n\tnatsolv = m.NAtoms()\n\tsa,sc = lat.TessNTimes(m.atoms,m.coords,2*shells_+1)\n\t# cell index of the center.\n\tccell = shells_*pow(2*shells_+1,2)+shells_*pow(2*shells_+1,1)+shells_\n\t# Replace the center cell with the solute.\n\tsa = np.concatenate([m1_.atoms,sa[:ccell*natsolv],sa[(ccell+1)*natsolv:]])\n\tsc = np.concatenate([m1_.coords-np.mean(m1_,axis=0)[np.newaxis,:],sa[:ccell*natsolv,:],sa[(ccell+1)*natsolv:,:]],axis=0)\n\tm = Mol(sa,sc)\n\tm.WriteXYZfile(""./results"",""solvated"")\n\tm.coords -= np.min(m.coords)\n\txmx = np.max(m.coords)\n\tPF = PeriodicForce(m,np.array([[xmx,0.,0.],[0.,xmx,0.],[0.,0.,xmx]]))\n\tPF.BindForce(f_, 15.0)\n\topt = PeriodicGeomOptimizer(PF)\n\tmsolv = opt.OptToDensity(m,rho_target)\n\treturn msolv\n'"
TensorMol/Simulations/PeriodicMC.py,0,"b'""""""\nPeriodic Monte Carlo.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Math.QuasiNewtonTools import *\nfrom ..Math.Statistics import *\nfrom ..Containers.Sets import *\nfrom ..TFNetworks.TFManage import *\nfrom ..ForceModifiers.Neighbors import *\nfrom ..ForceModels.Electrostatics import *\nfrom ..ForceModifiers.Periodic import *\nfrom .SimpleMD import *\nfrom .PeriodicMD import *\n\nclass PeriodicMonteCarlo(PeriodicVelocityVerlet):\n\tdef __init__(self, Force_, name_ =""PdicMC""):\n\t\t""""""\n\n\t\tArgs:\n\t\t\tForce_: A PERIODIC energy, force CLASS.\n\t\t\tPMol_: initial molecule.\n\t\tReturns:\n\t\t\tNothing.\n\t\t""""""\n\t\tPeriodicVelocityVerlet.__init__(self, Force_,name_)\n\t\te0, f0 = self.PForce(self.PForce.mol0.coords)\n\t\tself.eold = e0\n\t\tself.Estat = OnlineEstimator(e0)\n\t\tself.RDFold = self.PForce.RDF(self.PForce.mol0.coords)\n\t\tself.RDFstat = OnlineEstimator(self.RDFold)\n\t\tself.Xstat = OnlineEstimator(self.x)\n\t\tself.PACCstat = OnlineEstimator(1.0)\n\t\tself.kbt = KAYBEETEE*(PARAMS[""MDTemp""]/300.0) # Hartrees.\n\t\tself.Eav = None\n\t\tself.dE2 = None\n\t\tself.Xav = None\n\t\tself.dX2 = None\n\t\tself.Pacc = 0.0\n\tdef RandomVectorField(self,x_):\n\t\t""""""\n\t\tThis is a random move which is locally continuous\n\t\tso that nearby atoms are pulled together to improve\n\t\tacceptance probability. This is done by interpolating\n\t\trandomly placed vectors of random orientations to the points\n\t\twhere the atoms are. (9 such vectors.)\n\n\t\tTo accomplish rotations, there is also a solenoidal field\n\t\twhich acts as a cross product.\n\n\t\tV(x_j) = \\sum_(pts, i) v_i * exp(-dij^2)\n\t\t""""""\n\t\tmxx = np.max(x_)\n\t\tmnx = np.min(x_)\n\t\tnpts = 4\n\t\tpts = np.random.uniform(mxx-mnx,size=(npts,3))+mnx\n\t\trmagn = np.random.normal(scale = 0.07, size=(npts,1))\n\t\ttheta = np.random.uniform(3.1415, size=(npts,1))\n\t\tphi = np.random.uniform(2.0*3.1415, size=(npts,1))\n\t\trx = np.sin(theta)*np.cos(phi)\n\t\try = np.sin(theta)*np.sin(phi)\n\t\trz = np.cos(theta)\n\t\tmagn = np.concatenate([rmagn*rx,rmagn*ry,rmagn*rz],axis=1)\n\t\tjd = np.concatenate([pts,x_])\n\t\tDMat = MolEmb.Make_DistMat_ForReal(jd,npts)\n\t\tsigma = np.random.uniform(2.2)+0.02\n\t\texpd = (1.0/np.sqrt(6.2831*sigma*sigma))*np.exp(-1.0*DMat[:,npts:]*DMat[:,npts:]/(2*sigma*sigma))\n\t\ttore = np.einsum(\'jk,ji->ik\', magn, expd)\n\t\t# do the solenoidal piece.\n\t\tfor i in range(npts):\n\t\t\tif (np.random.random() < 0.6):\n\t\t\t\tvs = x_ - pts[i]\n\t\t\t\tfor j in range(x_.shape[0]):\n\t\t\t\t\tvs[j] /= np.linalg.norm(vs[j])\n\t\t\t\tsoil = np.cross(vs,4.0*magn[i])*(expd[i,:,np.newaxis])\n\t\t\t\t#print(soil)\n\t\t\t\ttore += soil\n\t\treturn tore\n\tdef MetropolisHastings(self,x_):\n\t\t""""""\n\t\tPerform the Metropolis step.\n\t\t""""""\n\t\tdx = self.RandomVectorField(x_)\n\t\tdx += np.random.uniform(size=x_.shape)*0.0005\n\t\tedx , tmp = self.PForce(x_+dx,DoForce=False)\n\t\tPMove = min(1.0,np.exp(-(edx - self.eold)/self.kbt))\n\t\tif (np.random.random()<PMove):\n\t\t\tself.x = self.PForce.lattice.ModuloLattice(x_ + dx)\n\t\t\tself.eold = edx\n\t\t\tself.RDFold = self.PForce.RDF(self.x)\n\t\t\tself.Pacc,t = self.PACCstat(1.0)\n\t\t\tprint(""accept"")\n\t\telse:\n\t\t\tself.Pacc,t = self.PACCstat(0.0)\n\t\t\tprint(\'reject\',PMove,(edx - self.eold),self.kbt)\n\t\tself.Eav, self.dE2 = self.Estat(self.eold)\n\t\tself.Xav, self.dX2 = self.Xstat(self.x)\n\t\treturn\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate Monte Carlo.\n\t\tMEEETROOOPPOLLIISSS\n\t\t""""""\n\t\tstep = 0\n\t\tself.md_log = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n\t\twhile(step < self.maxstep):\n\t\t\tself.t = step\n\t\t\tt = time.time()\n\t\t\tself.MetropolisHastings(self.x)\n\t\t\trdf, rdf2 = self.RDFstat(self.RDFold)\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tif (step%3==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\t\t\t\tnp.savetxt(""./results/""+""MCRDF""+self.name+"".txt"",rdf)\n\t\t\t\tnp.savetxt(""./results/""+""MCRDF2""+self.name+"".txt"",rdf2)\n\t\t\tstep+=1\n\t\t\tLOGGER.info(""Step: %i <E>(kJ/mol): %.5f sqrt(<dE2>): %.5f sqrt(<dX2>): %.5f Paccept %.5f Rho(g/cm**3): %.5f "", step, self.Eav, np.sqrt(self.dE2), np.sqrt(np.linalg.norm(self.dX2)), self.Pacc,self.Density())\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\treturn\n'"
TensorMol/Simulations/PeriodicMD.py,0,"b'""""""\nA periodic version of SimpleMD\nNo symmetry but general unit cells supported.\n\nMaintenance of the unit cell, etc. are handled by PeriodicForce.\nOnly linear scaling forces with energy are supported.\n\nTODO: Barostat...\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Containers.Sets import *\nfrom ..TFNetworks.TFManage import *\nfrom ..ForceModifiers.Neighbors import *\nfrom ..ForceModels.Electrostatics import *\nfrom ..Math.QuasiNewtonTools import *\nfrom ..ForceModifiers.Periodic import *\nfrom .SimpleMD import *\n\ndef PeriodicVelocityVerletStep(pf_, a_, x_, v_, m_, dt_):\n\t""""""\n\tA Periodic Velocity Verlet Step (just modulo\'s the vectors.)\n\n\tArgs:\n\t\tpf_: The PERIODIC force class (returns Joules/Angstrom)\n\t\ta_: The acceleration at current step. (A^2/fs^2)\n\t\tx_: Current coordinates (A)\n\t\tv_: Velocities (A/fs)\n\t\tm_: the mass vector. (kg)\n\tReturns:\n\t\tx: updated positions\n\t\tv: updated Velocities\n\t\ta: updated accelerations\n\t\te: Energy at midpoint per unit cell.\n\t""""""\n\tx = x_ + v_*dt_ + (1./2.)*a_*dt_*dt_\n\tx = pf_.lattice.ModuloLattice(x)\n\te, f_x_ = pf_(x)\n\ta = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/m_) # m^2/s^2 => A^2/Fs^2\n\tv = v_ + (1./2.)*(a_+a)*dt_\n\treturn x,v,a,e\n\nclass PeriodicNoseThermostat(NoseThermostat):\n\tdef __init__(self,m_,v_):\n\t\tNoseThermostat.__init__(self,m_,v_)\n\t\treturn\n\tdef step(self, pf_, a_, x_, v_, m_, dt_ ):\n\t\t""""""\n\t\tA periodic Nose thermostat velocity verlet step\n\t\thttp://www2.ph.ed.ac.uk/~dmarendu/MVP/MVP03.pdf\n\n\t\tArgs:\n\t\t\tpf_: a Periodic force class.\n\t\t\ta_: acceleration\n\t\t\tx_: coordinates\n\t\t\tv_: velocities\n\t\t\tm_: masses\n\t\t\tdt_: timestep.\n\t\t""""""\n\t\t# Recompute these stepwise in case of variable T.\n\t\tself.kT = IDEALGASR*pow(10.0,-10.0)*self.T # energy units here are kg (A/fs)^2\n\t\tself.tau = 20.0*PARAMS[""MDdt""]*self.N\n\t\tself.Q = self.kT*self.tau*self.tau\n\t\tx = x_ + v_*dt_ + (1./2.)*(a_ - self.eta*v_)*dt_*dt_\n\t\tx = pf_.lattice.ModuloLattice(x)\n\t\tvdto2 = v_ + (1./2.)*(a_ - self.eta*v_)*dt_\n\t\te, f_x_ = pf_(x)\n\t\ta = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/m_) # m^2/s^2 => A^2/Fs^2\n\t\tke = (1./2.)*np.dot(np.einsum(""ia,ia->i"",v_,v_),m_)\n\t\tetadto2 = self.eta + (dt_/(2.*self.Q))*(ke - (((3.*self.N+1)/2.))*self.kT)\n\t\tkedto2 = (1./2.)*np.dot(np.einsum(""ia,ia->i"",vdto2,vdto2),m_)\n\t\tself.eta = etadto2 + (dt_/(2.*self.Q))*(kedto2 - (((3.*self.N+1)/2.))*self.kT)\n\t\tv = (vdto2 + (dt_/2.)*a)/(1 + (dt_/2.)*self.eta)\n\t\treturn x,v,a,e\n\nclass PeriodicVelocityVerlet(VelocityVerlet):\n\tdef __init__(self, Force_, name_ =""PdicMD"", v0_=None):\n\t\t""""""\n\t\tMolecular dynamics\n\n\t\tArgs:\n\t\t\tForce_: A PERIODIC energy, force CLASS.\n\t\t\tPMol_: initial molecule.\n\t\t\tPARAMS[""MDMaxStep""]: Number of steps to take.\n\t\t\tPARAMS[""MDTemp""]: Temperature to initialize or Thermostat to.\n\t\t\tPARAMS[""MDdt""]: Timestep.\n\t\t\tPARAMS[""MDV0""]: Sort of velocity initialization (None, or ""Random"")\n\t\t\tPARAMS[""MDLogTrajectory""]: Write MD Trajectory.\n\t\tReturns:\n\t\t\tNothing.\n\t\t""""""\n\t\tself.PForce = Force_\n\t\tVelocityVerlet.__init__(self, None, self.PForce.mol0, name_, self.PForce.__call__)\n\t\tif v0_ is not None:\n\t\t\tself.v = v0_\n\t\tif (PARAMS[""MDThermostat""]==""Nose""):\n\t\t\tself.Tstat = PeriodicNoseThermostat(self.m,self.v)\n\t\telse:\n\t\t\tprint(""Unthermostated Periodic Velocity Verlet."")\n\t\treturn\n\tdef Density(self):\n\t\t""""""\n\t\tReturns the density in g/cm**3 of the bulk.\n\t\t""""""\n\t\treturn self.PForce.Density()\n\tdef WriteTrajectory(self):\n\t\tm=Mol(self.atoms,self.x)\n\t\tm.properties[""Lattice""]=self.PForce.lattice.lattice.copy()\n\t\tm.properties[""Time""]=self.t\n\t\tm.properties[""KineticEnergy""]=self.KE\n\t\tm.properties[""PotEnergy""]=self.EPot\n\t\tm.WriteXYZfile(""./results/"", ""MDTrajectory""+self.name,\'a\',True)\n\t\treturn\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\t\t""""""\n\t\tprint (""begin Periodic VelocityVerlet"")\n\t\tstep = 0\n\t\tself.md_log = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n\t\twhile(step < self.maxstep):\n\t\t\tt = time.time()\n\t\t\tself.t = step*self.dt\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\t\t\tif (PARAMS[""MDThermostat""]==None):\n\t\t\t\tself.x , self.v, self.a, self.EPot = PeriodicVelocityVerletStep(self.PForce, self.a, self.x, self.v, self.m, self.dt)\n\t\t\telse:\n\t\t\t\tself.x , self.v, self.a, self.EPot = self.Tstat.step(self.PForce, self.a, self.x, self.v, self.m, self.dt)\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,4] = self.KE\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tself.md_log[step,6] = self.KE+(self.EPot-self.EPot0)*JOULEPERHARTREE\n\n\t\t\tif (PARAMS[""PrintTMTimer""]):\n\t\t\t\tPrintTMTIMER()\n\t\t\tif (step%1==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\n\t\t\tstep+=1\n\t\t\tLOGGER.info(""Step: %i time: %.1f(fs) <KE>(kJ/mol): %.5f <|a|>(m/s2): %.5f <EPot>(Eh): %.5f <Etot>(kJ/mol): %.5f Rho(g/cm**3): %.5f Teff(K): %.5f"", step, self.t, self.KE/1000.0,  np.linalg.norm(self.a) , self.EPot, self.KE/1000.0+self.EPot*KJPERHARTREE,self.Density(), Teff)\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\treturn\n\nclass PeriodicBoxingDynamics(PeriodicVelocityVerlet):\n\tdef __init__(self, Force_, BoxingLatp_=np.eye(3), name_ =""PdicBoxMD"", BoxingT_= 400):\n\t\t""""""\n\t\tPeriodically Crushes a molecule by shrinking it\'s lattice to obtain a desired density.\n\n\t\tArgs:\n\t\t\tForce_: A PeriodicForce object\n\t\t\tBoxingLatp_ : Final Lattice.\n\t\t\tBoxingT_: amount of time for the boxing dynamics in fs.\n\t\t\tPARAMS[""MDMaxStep""]: Number of steps to take.\n\t\t\tPARAMS[""MDTemp""]: Temperature to initialize or Thermostat to.\n\t\t\tPARAMS[""MDdt""]: Timestep.\n\t\t\tPARAMS[""MDV0""]: Sort of velocity initialization (None, or ""Random"")\n\t\t\tPARAMS[""MDLogTrajectory""]: Write MD Trajectory.\n\t\tReturns:\n\t\t\tNothing.\n\t\t""""""\n\t\tself.PForce = Force_\n\t\tself.BoxingLat0 = Force_.lattice.lattice.copy()\n\t\tself.BoxingLatp = BoxingLatp_.copy()\n\t\tself.BoxingT = BoxingT_\n\t\tVelocityVerlet.__init__(self, None, self.PForce.mol0, name_, self.PForce.__call__)\n\t\tif (PARAMS[""MDThermostat""]==""Nose""):\n\t\t\tself.Tstat = PeriodicNoseThermostat(self.m,self.v)\n\t\telse:\n\t\t\tprint(""Unthermostated Periodic Velocity Verlet."")\n\t\treturn\n\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\t\t""""""\n\t\tstep = 0\n\t\tself.md_log = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n\t\twhile(step < self.maxstep):\n\t\t\tt = time.time()\n\t\t\tself.t = step*self.dt\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\n\t\t\tif (self.t>self.BoxingT):\n\t\t\t\tprint(""Exceeded Boxtime\\n"",self.BoxingLatp)\n\t\t\telse:\n\t\t\t\tnewlattice = ((self.BoxingT-self.t)/(self.BoxingT))*self.BoxingLat0+(1.0-(self.BoxingT-self.t)/(self.BoxingT))*self.BoxingLatp\n\t\t\t\tself.x = self.PForce.AdjustLattice(self.x, self.PForce.lattice.lattice,newlattice)\n\t\t\t\tself.v = self.PForce.AdjustLattice(self.v, self.PForce.lattice.lattice,newlattice)\n\t\t\t\tself.a = self.PForce.AdjustLattice(self.a, self.PForce.lattice.lattice,newlattice)\n\t\t\t\tself.PForce.ReLattice(newlattice)\n\t\t\tprint(""Density:"", self.Density())\n\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\t\t\tif (PARAMS[""MDThermostat""]==None):\n\t\t\t\tself.x , self.v, self.a, self.EPot = PeriodicVelocityVerletStep(self.PForce, self.a, self.x, self.v, self.m, self.dt)\n\t\t\telse:\n\t\t\t\tself.x , self.v, self.a, self.EPot = self.Tstat.step(self.PForce, self.a, self.x, self.v, self.m, self.dt)\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,4] = self.KE\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tself.md_log[step,6] = self.KE+(self.EPot-self.EPot0)*JOULEPERHARTREE\n\n\t\t\tif (step%3==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\n\t\t\tstep+=1\n\t\t\tLOGGER.info(""Step: %i time: %.1f(fs) <KE>(kJ/mol): %.5f <|a|>(m/s2): %.5f <EPot>(Eh): %.5f <Etot>(kJ/mol): %.5f Teff(K): %.5f"", step, self.t, self.KE/1000.0,  np.linalg.norm(self.a) , self.EPot, self.KE/1000.0+self.EPot*KJPERHARTREE, Teff)\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\treturn\n\n\nclass PeriodicAnnealer(PeriodicVelocityVerlet):\n\tdef __init__(self, Force_, name_ =""PdicAnneal"",AnnealThresh_ = 0.000009):\n\t\t""""""\n\t\tAnneals a periodic molecule.\n\n\t\tArgs:\n\t\t\tForce_: A PeriodicForce object\n\t\t\tPARAMS[""MDMaxStep""]: Number of steps to take.\n\t\t\tPARAMS[""MDTemp""]: Temperature to initialize or Thermostat to.\n\t\t\tPARAMS[""MDdt""]: Timestep.\n\t\t\tPARAMS[""MDV0""]: Sort of velocity initialization (None, or ""Random"")\n\t\t\tPARAMS[""MDLogTrajectory""]: Write MD Trajectory.\n\t\tReturns:\n\t\t\tNothing.\n\t\t""""""\n\t\tPeriodicVelocityVerlet.__init__(self, Force_, name_)\n\t\tself.dt = 0.1\n\t\tself.v *= 0.0\n\t\tself.AnnealT0 = PARAMS[""MDAnnealT0""]\n\t\tself.AnnealSteps = PARAMS[""MDAnnealSteps""]\n\t\tself.MinS = 0\n\t\tself.MinE = 99999999.0\n\t\tself.Minx = None\n\t\tself.AnnealThresh = AnnealThresh_\n\t\tself.Tstat = PeriodicNoseThermostat(self.m,self.v)\n\t\treturn\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\t\t""""""\n\t\tstep = 0\n\t\tself.md_log = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n\t\twhile(step < self.AnnealSteps):\n\t\t\tt = time.time()\n\t\t\tself.t = step*self.dt\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\n\t\t\tAnnealFrac = float(self.AnnealSteps - step)/self.AnnealSteps\n\t\t\tself.Tstat.T = self.AnnealT0*AnnealFrac + PARAMS[""MDAnnealTF""]*(1.0-AnnealFrac) + pow(10.0,-10.0)\n\n\t\t\tself.x , self.v, self.a, self.EPot = self.Tstat.step(self.PForce, self.a, self.x, self.v, self.m, self.dt)\n\n\t\t\tif (self.EPot < self.MinE and abs(self.EPot - self.MinE)>self.AnnealThresh and step>1):\n\t\t\t\tself.MinE = self.EPot\n\t\t\t\tself.Minx = self.x.copy()\n\t\t\t\tself.MinS = step\n\t\t\t\tLOGGER.info(""   -- cycling annealer -- "")\n\t\t\t\tif (PARAMS[""MDAnnealT0""] > PARAMS[""MDAnnealTF""]):\n\t\t\t\t\tself.AnnealT0 = self.Tstat.T+PARAMS[""MDAnnealKickBack""]\n\t\t\t\tprint(self.x)\n\t\t\t\tstep=0\n\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,4] = self.KE\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tself.md_log[step,6] = self.KE+(self.EPot-self.EPot0)*JOULEPERHARTREE\n\n\t\t\tif (step%3==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\n\t\t\tstep+=1\n\t\t\tLOGGER.info(""Step: %i time: %.1f(fs) <KE>(kJ/mol): %.5f <|a|>(m/s2): %.5f <EPot>(Eh): %.5f <Etot>(kJ/mol): %.5f Rho(g/cm3) %.5f Teff(K): %.5f T_target(K): %.5f"", step, self.t, self.KE/1000.0,  np.linalg.norm(self.a) , self.EPot, self.KE/1000.0+self.EPot*KJPERHARTREE, self.Density(),  Teff, self.Tstat.T)\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\tMol(self.atoms,self.Minx).WriteXYZfile(""./results"",""PAnnealMin"",wprop = True)\n\t\treturn\n'"
TensorMol/Simulations/SimpleMD.py,0,"b'""""""\nThe Units chosen are Angstrom, Fs.\nI convert the force outside from kcal/(mol angstrom) to Joules/(mol angstrom)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Containers.Sets import *\nfrom ..TFNetworks.TFManage import *\nfrom ..ForceModels.Electrostatics import *\nfrom ..Math.QuasiNewtonTools import *\nfrom ..Math.Statistics import *\n\ndef VelocityVerletStep(f_, a_, x_, v_, m_, dt_, fande_=None):\n\t""""""\n\tA Velocity Verlet Step\n\n\tArgs:\n\t\tf_: The force function (returns Joules/Angstrom)\n\t\ta_: The acceleration at current step. (A^2/fs^2)\n\t\tx_: Current coordinates (A)\n\t\tv_: Velocities (A/fs)\n\t\tm_: the mass vector. (kg)\n\tReturns:\n\t\tx: updated positions\n\t\tv: updated Velocities\n\t\ta: updated accelerations\n\t\te: Energy at midpoint.\n\t""""""\n\tx = x_ + v_*dt_ + (1./2.)*a_*dt_*dt_\n\te, f_x_ = 0.0, None\n\tif (fande_==None):\n\t\tf_x_ = f_(x)\n\telse:\n\t\te, f_x_ = fande_(x)\n\ta = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/m_) # m^2/s^2 => A^2/Fs^2\n\tv = v_ + (1./2.)*(a_+a)*dt_\n\treturn x,v,a,e\n\ndef KineticEnergy(v_, m_):\n\t""""""\n\tThe KineticEnergy\n\n\tArgs:\n\t\tThe masses are in kg.\n\t\tv_: Velocities (A/fs)\n\t\tm_: the mass vector. (kg/mol)\n\tReturns:\n\t\tThe kinetic energy per atom (J/mol)\n\t""""""\n\treturn (1./2.)*np.dot(np.einsum(""ia,ia->i"",v_,v_)*pow(10.0,10.0),m_)/len(m_)\n\nclass Thermostat:\n\tdef __init__(self,m_,v_):\n\t\t""""""\n\t\tVelocity Verlet step with a Rescaling Thermostat\n\t\t""""""\n\t\tself.N = len(m_)\n\t\tself.m = m_.copy()\n\t\tself.T = PARAMS[""MDTemp""]\n\t\tself.Teff = 0.001\n\t\tself.kT = IDEALGASR*pow(10.0,-10.0)*self.T # energy units here are kg (A/fs)^2\n\t\tself.tau = 30*PARAMS[""MDdt""]\n\t\tself.name = ""Rescaling""\n\t\tprint(""Using "", self.name, "" thermostat at "",self.T, "" degrees Kelvin"")\n\t\tself.Rescale(v_)\n\t\treturn\n\n\tdef step(self,f_, a_, x_, v_, m_, dt_ , fande_=None):\n\t\tx = x_ + v_*dt_ + (1./2.)*a_*dt_*dt_\n\t\te, f_x_ = 0.0, None\n\t\tif (fande_==None):\n\t\t\tf_x_ = f_(x)\n\t\telse:\n\t\t\te, f_x_ = fande_(x)\n\t\ta = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/m_) # m^2/s^2 => A^2/Fs^2\n\t\tv = v_ + (1./2.)*(a_+a)*dt_\n\t\tself.Teff = (2./3.)*KineticEnergy(v,self.m)/IDEALGASR\n\t\tv *= np.sqrt(self.T/self.Teff)\n\t\treturn x,v,a,e\n\n\tdef Rescale(self,v_):\n\t\t# Do this elementwise otherwise H\'s blow off.\n\t\tfor i in range(self.N):\n\t\t\tTeff = (2.0/(3.0*IDEALGASR))*pow(10.0,10.0)*(1./2.)*self.m[i]*np.einsum(""i,i"",v_[i],v_[i])\n\t\t\tif (Teff != 0.0):\n\t\t\t\tv_[i] *= np.sqrt(self.T/(Teff))\n\t\treturn\n\nclass NoseThermostat(Thermostat):\n\tdef __init__(self,m_,v_):\n\t\t""""""\n\t\tVelocity Verlet step with a Nose-Hoover Thermostat.\n\t\t""""""\n\t\tself.m = m_.copy()\n\t\tself.N = len(m_)\n\t\tself.T = PARAMS[""MDTemp""]  # Length of NH chain.\n\t\tself.eta = 0.0\n\t\tself.name = ""Nose""\n\t\tself.Rescale(v_)\n\t\tprint(""Using "", self.name, "" thermostat at "",self.T, "" degrees Kelvin"")\n\t\treturn\n\n\tdef step(self,f_, a_, x_, v_, m_, dt_ , fande_=None, frc_ = True):\n\t\t""""""\n\t\thttp://www2.ph.ed.ac.uk/~dmarendu/MVP/MVP03.pdf\n\t\t""""""\n\t\t# Recompute these stepwise in case of variable T.\n\t\tself.kT = IDEALGASR*pow(10.0,-10.0)*self.T # energy units here are kg (A/fs)^2\n\t\tself.tau = 20.0*PARAMS[""MDdt""]*self.N\n\t\tself.Q = self.kT*self.tau*self.tau\n\n\t\tx = x_ + v_*dt_ + (1./2.)*(a_ - self.eta*v_)*dt_*dt_\n\t\tvdto2 = v_ + (1./2.)*(a_ - self.eta*v_)*dt_\n\t\te, f_x_ = 0.0, None\n\t\tif (fande_==None):\n\t\t\tf_x_ = f_(x)\n\t\telse:\n\t\t\te, f_x_ = fande_(x)\n\t\ta = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/m_) # m^2/s^2 => A^2/Fs^2\n\t\tke = (1./2.)*np.dot(np.einsum(""ia,ia->i"",v_,v_),m_)\n\t\tetadto2 = self.eta + (dt_/(2.*self.Q))*(ke - (((3.*self.N+1)/2.))*self.kT)\n\t\tkedto2 = (1./2.)*np.dot(np.einsum(""ia,ia->i"",vdto2,vdto2),m_)\n\t\tself.eta = etadto2 + (dt_/(2.*self.Q))*(kedto2 - (((3.*self.N+1)/2.))*self.kT)\n\t\tv = (vdto2 + (dt_/2.)*a)/(1 + (dt_/2.)*self.eta)\n\t\tif frc_:\n\t\t\treturn x,v,a,e,f_x_\n\t\telse:\n\t\t\treturn x,v,a,e\n\nclass AndersenThermostat(Thermostat):\n\tdef __init__(self,m_,v_):\n\t\t""""""\n\t\tVelocity Verlet step with a Langevin Thermostat\n\t\t""""""\n\t\tself.m = m_.copy()\n\t\tself.N = len(list(m_))\n\t\tself.T = PARAMS[""MDTemp""]  # Length of NH chain.\n\t\tself.gamma = 1/2.0 # Collision frequency (fs**-1)\n\t\tself.name = ""Andersen""\n\t\tself.Rescale(v_)\n\t\tprint(""Using "", self.name, "" thermostat at "",self.T, "" degrees Kelvin"")\n\t\treturn\n\tdef step(self,f_, a_, x_, v_, m_, dt_ , fande_=None, frc_ = True):\n\t\tx = x_ + v_*dt_ + (1./2.)*a_*dt_*dt_\n\t\te, f_x_ = 0.0, None\n\t\tif (fande_==None):\n\t\t\tf_x_ = f_(x)\n\t\telse:\n\t\t\te, f_x_ = fande_(x)\n\t\ta = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/m_) # m^2/s^2 => A^2/Fs^2\n\t\tv = v_ + (1./2.)*(a_+a)*dt_\n\n\t\t# Andersen velocity randomization.\n\t\tself.kT = IDEALGASR*pow(10.0,-10.0)*self.T # energy units here are kg (A/fs)^2\n\t\ts = np.sqrt(2.0*self.gamma*self.kT/self.m) # Mass is in kg,\n\t\tfor i in range(x_.shape[0]):\n\t\t\tif (np.random.random() < self.gamma*dt_):\n\t\t\t\tv[i] = np.random.normal(0.0,s[i],size = (3))\n\t\tif frc_:\n\t\t\treturn x,v,a,e,f_x_\n\t\telse:\n\t\t\treturn x,v,a,e\n\nclass LangevinThermostat(Thermostat):\n\t""""""\n\tNot Working.\n\t""""""\n\tdef __init__(self,m_,v_):\n\t\t""""""\n\t\tVelocity Verlet step with a Langevin Thermostat\n\t\t""""""\n\t\tself.m = m_.copy()\n\t\tself.N = len(m_)\n\t\tself.T = PARAMS[""MDTemp""]  # Length of NH chain.\n\t\tself.gamma = 0.05 # friction of the thermostat.\n\t\tself.name = ""Langevin""\n\t\tself.Rescale(v_)\n\t\tprint(""Using "", self.name, "" thermostat at "",self.T, "" degrees Kelvin"")\n\t\treturn\n\tdef step(self,f_, a_, x_, v_, m_, dt_ , fande_=None, frc_ = True):\n\t\t""""""\n\t\tarXiv:1212.1244v4\n\t\t""""""\n\t\tself.kT = IDEALGASR*pow(10.0,-10.0)*self.T # energy units here are kg (A/fs)^2\n\t\ts = np.sqrt(2.0*self.gamma*self.kT/dt_) # Mass is in kg,\n\t\tbeta = np.random.normal(0.0,s,size = x_.shape)\n\t\tm = np.tile(self.m[:,np.newaxis],(1,3))\n\t\tprint(""M shape: "",m.shape)\n\t\ta = (1.0-self.gamma*dt_/(2.0*m))/(1.0+self.gamma*dt_/(2.0*m))\n\t\tb = 1.0/(1.0+self.gamma*dt_/(2.0*m))\n\t\tprint(""a,b:"",a,b)\n\t\tx = x_ + b*dt_*v_ + (b*dt_*dt_)/(2.0*m)*(a_*m)  + (b*dt_)/(2.0*m)*beta\n\t\te, f_x_ = 0.0, None\n\t\tif (fande_==None):\n\t\t\tf_x_ = f_(x)\n\t\telse:\n\t\t\te, f_x_ = fande_(x)\n\t\tv = a*v_ + dt_/(2.0*m)*(a*(a_*m)+f_x_) + (b/m)*beta\n\t\ta = f_x_/m\n\t\tif frc_:\n\t\t\treturn x,v,a,e,f_x_\n\t\telse:\n\t\t\treturn x,v,a,e\n\nclass NoseChainThermostat(Thermostat):\n\tdef __init__(self,m_,v_):\n\t\t""""""\n\t\tVelocity Verlet step with a Nose-Hoover Chain Thermostat.\n\t\tBased on Appendix A of martyna 1996\n\t\thttp://dx.doi.org/10.1080/00268979600100761\n\n\t\tArgs:\n\t\t\tx_: an example of system positions.\n\t\t\tm_: system masses.\n\t\t\tPARAMS[""MNHChain""]: depth of the Nose Hoover Chain\n\t\t\tPARAMS[""MDTemp""]: Temperature of the Thermostat.\n\t\t\tPARAMS[""MDdt""]: Timestep of the dynamics.\n\t\t""""""\n\t\tself.M = PARAMS[""MNHChain""] # Depth of NH chain.\n\t\tself.N = len(v_) # Number of particles.\n\t\tself.Nf = len(v_)*3 # Number of Degrees of freedom.\n\t\tself.T = PARAMS[""MDTemp""]  # Length of NH chain.\n\t\tself.kT = IDEALGASR*pow(10.0,-10.0)*self.T # energy units here are kg (A/fs)^2\n\t\tself.GNKT = self.Nf*self.kT\n\t\tself.nc = 2 # nc Number of Trotterizations To be increased if Q is large.\n\t\tself.ny = 3# nys (eq. 29 of Martyna. number of quadrature points within step.)\n\t\tself.wj = np.zeros(self.ny)\n\t\tif (self.ny == 3):\n\t\t\tself.wj[0],self.wj[1],self.wj[2] = (1./(2. - np.power(2.,1./3.))),(1.-2.*(1./(2. - np.power(2.,1./3.)))),(1./(2. - np.power(2.,1./3.)))\n\t\telif (self.ny == 5):\n\t\t\tself.wj[0] = (1./(4. - np.power(4.,1./3.)))\n\t\t\tself.wj[1] = (1./(4. - np.power(4.,1./3.)))\n\t\t\tself.wj[2] = 1.-4.*self.wj[0]\n\t\t\tself.wj[3] = (1./(4. - np.power(4.,1./3.)))\n\t\t\tself.wj[4] = (1./(4. - np.power(4.,1./3.)))\n\t\tself.tau = 80.0*PARAMS[""MDdt""]\n\t\tself.dt = PARAMS[""MDdt""]\n\t\tself.dt2 = self.dt/2.\n\t\tself.dt22 = self.dt*self.dt/2.\n\t\tself.Qs = None # Chain Masses.\n\t\tself.MartynaQs() # assign the chain masses.\n\t\tself.eta = np.zeros(self.M) # Chain positions.\n\t\tself.Veta = np.zeros(self.M) # Chain velocities\n\t\tself.Geta = np.zeros(self.M) # Chain forces\n\t\tself.name = ""NoseHooverChain""\n\t\tself.Rescale(v_)\n\t\tprint(""Using "", self.name, "" thermostat at "",self.T, "" degrees Kelvin"")\n\t\treturn\n\n\tdef MartynaQs(self):\n\t\tif (self.M==0):\n\t\t\treturn\n\t\tself.Qs = np.ones(self.M)*self.kT*self.tau*self.tau\n\t\tself.Qs[0] = 3.*self.N*self.kT*self.tau*self.tau\n\t\treturn\n\n\tdef step(self,f_, a_, x_, v_, m_, dt_ ,fande_=None):\n\t\tv = self.IntegrateChain(a_, x_, v_, m_, dt_) # half step the chain.\n\t\t# Get KE of the chain.\n\t\tprint(""Energies of the system... "", self.ke(v_,m_), "" Teff "", (2./3.)*self.ke(v_,m_)*pow(10.0,10.0)/IDEALGASR/self.N)\n\t\tprint(""Energies along the chain... Desired:"", (3./2.)*self.kT)\n\t\tfor i in range(self.M):\n\t\t\tprint(self.Veta[i]*self.Veta[i]*self.Qs[i]/2.)\n\t\tv = v + self.dt2*a_\n\t\tx = x_ + self.dt*v\n\n\t\te, f_x_ = 0.0, None\n\t\tif (fande_==None):\n\t\t\tf_x_ = f_(x)\n\t\telse:\n\t\t\te, f_x_ = fande_(x)\n\n\t\ta = pow(10.0,-10.0)*np.einsum(""ax,a->ax"", f_x_, 1.0/m_) # m^2/s^2 => A^2/Fs^2\n\t\tv = v + self.dt2*a\n\t\tv = self.IntegrateChain(a, x_, v, m_, dt_) # half step the chain.\n\t\treturn x, v, a, e\n\n\tdef ke(self,v_,m_):\n\t\treturn (1./2.)*np.dot(np.einsum(""ia,ia->i"",v_,v_),m_)\n\n\tdef IntegrateChain(self, a_, x_, v_, m_, dt_ ):\n\t\t""""""\n\t\tThe Nose Hoover chain is twice trotterized in Martyna\'s subroutine\n\t\tSo this evolves the chain a half-step, and updates v_\n\t\t""""""\n\t\tif (self.M==0):\n\t\t\treturn v_\n\t\take = self.ke(v_,m_) # in kg (A^2/Fs^2)\n\t\t# Update thermostat forces.\n\t\tself.Geta[0]  = (2.*ake - self.GNKT) / self.Qs[0]\n\t\tscale = 1.0\n\t\tfor k in range(self.nc):\n\t\t\tfor j in range(self.ny):\n\t\t\t\t# UPDATE THE THERMOSTAT VELOCITIES.\n\t\t\t\twdtj2 = (self.wj[j]*self.dt/self.nc)/2.\n\t\t\t\twdtj4 = wdtj2/2.\n\t\t\t\twdtj8 = wdtj4/2.\n\t\t\t\tself.Veta[-1] += self.Geta[-1]*wdtj4\n\t\t\t\tfor i in range(self.M-1)[::-1]:\n\t\t\t\t\tAA = np.exp(-wdtj8*self.Veta[i+1])\n\t\t\t\t\tself.Veta[i] = self.Veta[i]*AA*AA + wdtj4*self.Geta[i]*AA\n\t\t\t\t# Update the particle velocities.\n\t\t\t\tAA = np.exp(-wdtj2*self.Veta[1])\n\t\t\t\tscale *= AA\n\t\t\t\tself.Geta[0] = (scale*scale*2.0*ake - self.GNKT)/self.Qs[0]\n\t\t\t\t# Update the Thermostat Positions.\n\t\t\t\tfor i in range(self.M):\n\t\t\t\t\tself.eta[i] += self.Veta[i]*wdtj2\n\t\t\t\t# Update the thermostat velocities\n\t\t\t\tfor i in range(self.M-1):\n\t\t\t\t\tAA = np.exp(-wdtj8*self.Veta[i+1])\n\t\t\t\t\tself.Veta[i] = self.Veta[i]*AA*AA + wdtj4*self.Geta[i]*AA\n\t\t\t\t\tself.Geta[i+1] = (self.Qs[i]*self.Veta[i]*self.Veta[i] - self.kT)/self.Qs[i+1]\n\t\t\t\tself.Veta[-1] += self.Geta[-1] * wdtj4\n\t\tprint(""eta"",self.eta)\n\t\tprint(""Meta"",self.Qs)\n\t\tprint(""Veta"",self.Veta)\n\t\tprint(""Geta"",self.Geta)\n\t\treturn v_*scale\n\nclass VelocityVerlet:\n\tdef __init__(self, f_, g0_, name_ ="""", EandF_=None, cellsize_=None):\n\t\t""""""\n\t\tMolecular dynamics\n\n\t\tArgs:\n\t\t\tf_: a force routine (optionally None if EandF_ is nonzero)\n\t\t\tg0_: initial molecule.\n\t\t\tEandF_: An energy,force routine. (optionally None)\n\t\t\tPARAMS[""MDMaxStep""]: Number of steps to take.\n\t\t\tPARAMS[""MDTemp""]: Temperature to initialize or Thermostat to.\n\t\t\tPARAMS[""MDdt""]: Timestep.\n\t\t\tPARAMS[""MDV0""]: Sort of velocity initialization (None, or ""Random"")\n\t\t\tPARAMS[""MDLogTrajectory""]: Write MD Trajectory.\n\t\tReturns:\n\t\t\tNothing.\n\t\t""""""\n\t\tself.name = name_\n\t\tself.cellsize = cellsize_\n\t\tself.maxstep = PARAMS[""MDMaxStep""]\n\t\tself.T = PARAMS[""MDTemp""]\n\t\tself.dt = PARAMS[""MDdt""]\n\t\tself.ForceFunction = f_\n\t\tself.EnergyAndForce = EandF_\n\t\tself.EPot0 = 0.0\n\t\tif (EandF_ != None):\n\t\t\tself.EPot0 , self.f0 = self.EnergyAndForce(g0_.coords)\n\t\tself.EPot = self.EPot0\n\t\tself.EnergyStat = OnlineEstimator(self.EPot0)\n\t\tself.RealPot = 0.0 # The real potential energy.\n\t\tself.t = 0.0\n\t\tself.KE = 0.0\n\t\tself.atoms = g0_.atoms.copy()\n\t\tself.m = np.array(list(map(lambda x: ATOMICMASSES[x-1], self.atoms)))\n\t\tself.natoms = len(self.atoms)\n\t\tself.x = g0_.coords.copy()\n\t\tself.v = np.zeros(self.x.shape)\n\t\tself.a = np.zeros(self.x.shape)\n\t\tself.md_log = None\n\n\t\tif (PARAMS[""MDV0""]==""Random""):\n\t\t\tnp.random.seed()   # reset random seed\n\t\t\tself.v = np.random.randn(*self.x.shape)\n\t\t\tTstat = Thermostat(self.m, self.v) # Will rescale self.v appropriately.\n\t\telif PARAMS[""MDV0""]==""Thermal"":\n\t\t\tself.v = np.random.normal(size=self.x.shape) * np.sqrt(1.38064852e-23 * self.T / self.m)[:,None]\n\t\tself.Tstat = None\n\t\tif (PARAMS[""MDThermostat""]==""Rescaling""):\n\t\t\tself.Tstat = Thermostat(self.m,self.v)\n\t\telif (PARAMS[""MDThermostat""]==""Nose""):\n\t\t\tself.Tstat = NoseThermostat(self.m,self.v)\n\t\telif (PARAMS[""MDThermostat""]==""Andersen""):\n\t\t\tself.Tstat = AndersenThermostat(self.m,self.v)\n\t\telif (PARAMS[""MDThermostat""]==""Langevin""):\n\t\t\tself.Tstat = LangevinThermostat(self.m,self.v)\n\t\telif (PARAMS[""MDThermostat""]==""NoseHooverChain""):\n\t\t\tself.Tstat = NoseChainThermostat(self.m, self.v)\n\t\telse:\n\t\t\tpass #print(""Unthermostated Velocity Verlet."")\n\t\treturn\n\n\tdef WriteTrajectory(self):\n\t\tm=Mol(self.atoms,self.x)\n\t\tm.properties[""Time""]=self.t\n\t\tm.properties[""KineticEnergy""]=self.KE\n\t\tm.properties[""PotEnergy""]=self.EPot\n\t\tm.WriteXYZfile(""./results/"", ""MDTrajectory""+self.name)\n\t\treturn\n\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\t\t""""""\n\t\tstep = 0\n\t\tself.md_log = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n\t\twhile(step < self.maxstep):\n\t\t\tt = time.time()\n\t\t\tself.t = step*self.dt\n\t\t\t#self.KE = KineticEnergy(self.v,self.m)\n\t\t\t#Teff = (2./3.)*self.KE/IDEALGASR\n\t\t\tif (PARAMS[""MDThermostat""]==None):\n\t\t\t\tself.x , self.v, self.a, self.EPot = VelocityVerletStep(self.ForceFunction, self.a, self.x, self.v, self.m, self.dt, self.EnergyAndForce)\n\t\t\telse:\n\t\t\t\tself.x , self.v, self.a, self.EPot, self.force = self.Tstat.step(self.ForceFunction, self.a, self.x, self.v, self.m, self.dt, self.EnergyAndForce)\n\t\t\tif self.cellsize != None:\n\t\t\t\tself.x  = np.mod(self.x, self.cellsize)\n\t\t\tself.md_log[step,0] = self.t\n\t\t\tself.md_log[step,4] = self.KE\n\t\t\tself.md_log[step,5] = self.EPot\n\t\t\tself.md_log[step,6] = self.KE+(self.EPot-self.EPot0)*JOULEPERHARTREE\n\t\t\tavE, Evar = self.EnergyStat(self.EPot) # I should log these.\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\n\t\t\tif (step%3==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tif (step%500==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.md_log)\n\n\t\t\tstep+=1\n\t\t\tLOGGER.info(""%s Step: %i time: %.1f(fs) KE(kJ): %.5f PotE(Eh): %.5f ETot(kJ/mol): %.5f Teff(K): %.5f"", self.name, step, self.t, self.KE*len(self.m)/1000.0, self.EPot, self.KE*len(self.m)/1000.0+(self.EPot)*KJPERHARTREE, Teff)\n\t\t\t#LOGGER.info(""Step: %i time: %.1f(fs) <KE>(kJ/mol): %.5f <|a|>(m/s2): %.5f <EPot>(Eh): %.5f <Etot>(kJ/mol): %.5f Teff(K): %.5f"", step, self.t, self.KE/1000.0,  np.linalg.norm(self.a) , self.EPot, self.KE/1000.0+self.EPot*KJPERHARTREE, Teff)\n\t\t\tprint((""per step cost:"", time.time() -t ))\n\t\treturn\n\nclass IRTrajectory(VelocityVerlet):\n\tdef __init__(self,f_,q_,g0_,name_=str(0),v0_=None):\n\t\t""""""A specialized sort of dynamics which is appropriate for obtaining IR spectra at\n\t\tZero temperature. Absorption cross section is given by:\n\t\talpha = frac{4pi^2}{hbar c} omega (1 - Exp[-beta hbar omega]) sigma(omega))\n\t\tsigma  = frac{1}{6 pi} mathcal{F} {mu(t)mu(0)}\n\n\t\tArgs:\n\t\t\tf_: a function which yields the energy, force\n\t\t\tq_: a function which yields the charge.\n\t\t\tg0_: an initial geometry.\n\t\t\tPARAMS[""MDFieldVec""]\n\t\t\tPARAMS[""MDFieldAmp""]\n\t\t\tPARAMS[""MDFieldT0""] = 3.0\n\t\t\tPARAMS[""MDFieldTau""] = 1.2 #1.2 fs pulse.\n\t\t\tPARAMS[""MDFieldFreq""] = 1/1.2 # 700nm light is about 1/1.2 fs.\n\t\t""""""\n\t\tVelocityVerlet.__init__(self, f_, g0_, name_, f_)\n\t\tif (v0_ is not None):\n\t\t\tself.v = v0_.copy()\n\t\tself.EField = np.zeros(3)\n\t\tself.IsOn = False\n\t\tself.FieldVec = PARAMS[""MDFieldVec""]\n\t\tself.FieldAmp = PARAMS[""MDFieldAmp""]\n\t\tself.FieldFreq = PARAMS[""MDFieldFreq""]\n\t\tself.Tau = PARAMS[""MDFieldTau""]\n\t\tself.TOn = PARAMS[""MDFieldT0""]\n\t\tself.UpdateCharges = PARAMS[""MDUpdateCharges""]\n\t\tself.EnergyAndForce = f_\n\t\tself.EPot0 , self.f0 = self.EnergyAndForce(g0_.coords)\n\t\tself.EPot = self.EPot0\n\t\tself.ChargeFunction = None\n\t\tself.q0 = 0*self.m\n\t\tself.qs = np.ones(self.m.shape)\n\t\tself.Mu0 = np.zeros(3)\n\t\tself.mu_his = None\n\t\tif (q_ != None):\n\t\t\tself.ChargeFunction = q_\n\t\t\tself.q0 = self.ChargeFunction(self.x)\n\t\t\tself.qs = self.q0.copy()\n\t\t\tself.Mu0 = Dipole_Naive(self.x, self.ChargeFunction(self.x))\n\t\telse:\n\t\t\tself.UpdateCharges = False\n\t\t# This can help in case you had a bad initial geometry\n\t\tself.MinS = 0\n\t\tself.MinE = 0.0\n\t\tself.Minx = None\n\n\tdef Pulse(self,t_):\n\t\t""""""\n\t\tdelta pulse of duration\n\t\t""""""\n\t\tsin_part = (np.sin(2.0*3.1415*self.FieldFreq*t_))\n\t\texp_part = (1.0/np.sqrt(2.0*3.1415*self.Tau*self.Tau))*(np.exp(-1.0*np.power(t_-self.TOn,2.0)/(2.0*self.Tau*self.Tau)))\n\t\tamp = self.FieldAmp*sin_part*exp_part\n\t\tif (np.abs(amp) > np.power(10.0,-12.0)):\n\t\t\treturn self.FieldVec*amp, True\n\t\telse:\n\t\t\treturn np.zeros(3), False\n\n\tdef ForcesWithCharge(self,x_):\n\t\te, FFForce = self.EnergyAndForce(x_)\n\t\tif (self.IsOn):\n\t\t\t# ElectricFieldForce Is in units of Hartree/angstrom.\n\t\t\t# and must be converted to kg*Angstrom/(Fs^2)\n\t\t\tElecForce = 4184.0*ElectricFieldForce(self.qs, self.EField)\n\t\t\tprint(""Field Free Force"", FFForce)\n\t\t\tprint(""ElecForce Force"", ElecForce)\n\t\t\treturn e, RemoveInvariantForce(x_, FFForce + ElecForce, self.m)\n\t\telse:\n\t\t\treturn e, RemoveInvariantForce(x_, FFForce, self.m)\n\n\tdef WriteTrajectory(self):\n\t\tm=Mol(self.atoms,self.x)\n\t\t#m.properties[""Time""]=self.t\n\t\t#m.properties[""KineticEnergy""]=self.KE\n\t\tm.properties[""Energy""]=self.EPot\n\t\t#m.properties[""Charges""]=self.qs\n\t\tm.WriteXYZfile(""./results/"", ""MDTrajectory""+self.name)\n\t\treturn\n\n\tdef Prop(self):\n\t\tself.mu_his = np.zeros((self.maxstep, 7)) # time Dipoles Energy\n#HACK\n\t\tvhis = np.zeros((self.maxstep,)+self.v.shape) # time Dipoles Energy\n\t\tstep = 0\n\t\twhile(step < self.maxstep):\n\t\t\tself.t = step*self.dt\n\t\t\t#self.KE = KineticEnergy(self.v,self.m)\n\t\t\t#Teff = (2./3.)*self.KE/IDEALGASR\n\n\t\t\tself.EField, self.IsOn = self.Pulse(self.t)\n\t\t\tif (self.UpdateCharges and not self.IsOn):\n\t\t\t\tself.qs = self.ChargeFunction(self.x)\n\t\t\telse:\n\t\t\t\tself.qs = self.q0\n\t\t\tself.Mu = Dipole_Naive(self.x, self.qs) - self.Mu0\n\t\t\tself.mu_his[step,0] = self.t\n\t\t\tself.mu_his[step,1:4] = self.Mu.copy()\n\t\t\tself.mu_his[step,4] = self.KE\n\t\t\tself.mu_his[step,5] = self.EPot\n\t\t\tself.mu_his[step,6] = self.KE+self.EPot\n\t\t\tvhis[step] = self.v.copy()\n\n\t\t\tif (PARAMS[""MDThermostat""]==None):\n\t\t\t\tself.x , self.v, self.a, self.EPot = VelocityVerletStep(None, self.a, self.x, self.v, self.m, self.dt,self.ForcesWithCharge)\n\t\t\telse:\n\t\t\t\tself.x , self.v, self.a, self.EPot, self.force = self.Tstat.step(None, self.a, self.x, self.v, self.m, self.dt,self.ForcesWithCharge)\n\n\t\t\tif (PARAMS[""MDIrForceMin""] and self.EPot < self.MinE and abs(self.EPot - self.MinE)>0.00005):\n\t\t\t\tself.MinE = self.EPot\n\t\t\t\tself.Minx = self.x.copy()\n\t\t\t\tself.MinS = step\n\t\t\t\tLOGGER.info("" -- You didn\'t start from the global minimum -- "")\n\t\t\t\tLOGGER.info(""   -- I\'mma set you back to the beginning -- "")\n\t\t\t\tprint(self.x)\n\t\t\t\tself.Mu0 = Dipole_Naive(self.x, self.qs)\n\t\t\t\tstep=0\n\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\n\t\t\tif (step%50==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tstep+=1\n\t\t\tif (step%1000==0):\n\t\t\t\tnp.savetxt(""./results/""+""MDLog""+self.name+"".txt"",self.mu_his)\n\t\t\tLOGGER.info(""%s Step: %i time: %.1f(fs) <KE>(kJ): %.5f <PotE>(Eh): %.5f <ETot>(kJ/mol): %.5f Teff(K): %.5f Mu: (%f,%f,%f)"", self.name, step, self.t, self.KE*len(self.m)/1000.0, self.EPot, self.KE*len(self.m)/1000.0+(self.EPot-self.EPot0)*KJPERHARTREE, Teff, self.Mu[0], self.Mu[1], self.Mu[2])\n\t\t#WriteVelocityAutocorrelations(self.mu_his,vhis)\n\t\treturn\n\nclass Annealer(IRTrajectory):\n\tdef __init__(self,f_,q_,g0_,name_=""anneal"",AnnealThresh_ = 0.000009):\n\t\tPARAMS[""MDThermostat""] = None\n\t\t#PARAMS[""MDV0""] = None\n\t\tIRTrajectory.__init__(self, f_, q_, g0_, name_)\n\t\t#self.dt = 0.2\n\t\t#self.v *= 0.0\n\t\tself.AnnealT0 = PARAMS[""MDAnnealT0""]\n\t\tself.AnnealSteps = PARAMS[""MDAnnealSteps""]\n\t\tself.MinS = 0\n\t\tself.MinE = 0.0\n\t\tself.Minx = None\n\t\tself.AnnealThresh = AnnealThresh_\n\t\tself.Tstat = NoseThermostat(self.m,self.v)\n\t\t# The annealing program is 1K => 0K in 500 steps.\n\t\treturn\n\n\tdef Prop(self):\n\t\t""""""\n\t\tPropagate VelocityVerlet\n\t\t""""""\n\t\tstep = 0\n\t\tself.Tstat.T = self.AnnealT0*float(self.AnnealSteps - step)/self.AnnealSteps + pow(10.0,-10.0)\n\t\tTeff = PARAMS[""MDAnnealT0""]\n\t\tprint (""Teff"", Teff, "" MDAnnealTF:"", PARAMS[""MDAnnealTF""])\n\t\twhile(step < self.AnnealSteps):\n\t\t\tself.t = step*self.dt\n\t\t\t#self.KE = KineticEnergy(self.v,self.m)\n\t\t\t#Teff = (2./3.)*self.KE/IDEALGASR\n\n\t\t\tself.EField, self.IsOn = self.Pulse(self.t)\n\t\t\tif (self.UpdateCharges and not self.IsOn):\n\t\t\t\tself.qs = self.ChargeFunction(self.x)\n\t\t\telse:\n\t\t\t\tself.qs = self.q0\n\t\t\tself.Mu = Dipole(self.x, self.qs) - self.Mu0\n\t\t\t# avoid the thermostat blowing up.\n\t\t\tAnnealFrac = float(self.AnnealSteps - step)/self.AnnealSteps\n\t\t\tself.Tstat.T = max(0.1, self.AnnealT0*AnnealFrac + PARAMS[""MDAnnealTF""]*(1.0-AnnealFrac) + pow(10.0,-10.0))\n\t\t\t# First 50 steps without any thermostat.\n\t\t\tself.x , self.v, self.a, self.EPot, self.force = self.Tstat.step(self.ForceFunction, self.a, self.x, self.v, self.m, self.dt, self.EnergyAndForce)\n\t\t\tif (self.EPot < self.MinE and abs(self.EPot - self.MinE)>self.AnnealThresh):\n\t\t\t\tself.MinE = self.EPot\n\t\t\t\tself.Minx = self.x.copy()\n\t\t\t\tself.MinS = step\n\t\t\t\tLOGGER.info(""   -- cycling annealer -- "")\n\t\t\t\tif (PARAMS[""MDAnnealT0""] > PARAMS[""MDAnnealTF""]):\n\t\t\t\t\tself.AnnealT0 = min(PARAMS[""MDAnnealT0""], self.Tstat.T+PARAMS[""MDAnnealKickBack""])\n\t\t\t\tprint(self.x)\n\t\t\t\tself.Mu0 = Dipole(self.x, self.qs)\n\t\t\t\tstep=0\n\t\t\tself.KE = KineticEnergy(self.v,self.m)\n\t\t\tTeff = (2./3.)*self.KE/IDEALGASR\n\t\t\tif (PARAMS[""PrintTMTimer""]):\n\t\t\t\tPrintTMTIMER()\n\t\t\tif (step%7==0 and PARAMS[""MDLogTrajectory""]):\n\t\t\t\tself.WriteTrajectory()\n\t\t\tstep+=1\n\t\t\tLOGGER.info(""%s Step: %i time: %.1f(fs) <KE>(kJ): %.5f <PotE>(Eh): %.5f <ETot>(kJ/mol): %.5f T_eff(K): %.5f T_target(K): %.5f"", self.name, step, self.t, self.KE*len(self.m)/1000.0, self.EPot, self.KE*len(self.m)/1000.0+(self.EPot-self.EPot)*2625.5, Teff, self.Tstat.T)\n\t\t#self.x = self.Minx.copy()\n\t\tprint(""Achieved Minimum energy "", self.MinE, "" at step "", step)\n\t\treturn\n'"
TensorMol/Simulations/Web.py,0,"b'""""""\n\tThis is a crazy type of simulation\n\twhich finds minimum energy paths between\n\tmultiple geometries using repeated NEB calculations.\n\tThis could be used for example to make better training data.\n\tfor reactive species.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .Opt import *\nfrom .Neb import *\nimport random\nimport time\n\nclass LocalReactions:\n\tdef __init__(self,f_, g0_, Nstat_ = 20):\n\t\t""""""\n\t\tIn this case the bumps are pretty severe\n\t\tand the metadynamics is designed to sample reactions rapidly.\n\n\t\tArgs:\n\t\t\tf_: an energy, force routine (energy Hartree, Force Kcal/ang.)\n\t\t\tg0_: initial molecule.\n\t\t""""""\n\t\tself.NStat = Nstat_\n\t\tLOGGER.info(""Finding reactions between %i local minima... "", self.NStat)\n\t\tMOpt = MetaOptimizer(f_,g0_,StopAfter_=self.NStat)\n\t\tmins = MOpt.MetaOpt()\n\t\tLOGGER.info(""---------------------------------"")\n\t\tLOGGER.info(""--------- Nudged Bands ----------"")\n\t\tLOGGER.info(""---------------------------------"")\n\t\tnbead = 15\n\t\tself.path = np.zeros(shape = ((self.NStat-1)*nbead,g0_.NAtoms(),3))\n\t\tfor i in range(self.NStat-1):\n\t\t\tg0 = Mol(g0_.atoms,mins[i])\n\t\t\tg1 = Mol(g0_.atoms,mins[i+1])\n\t\t\tneb = NudgedElasticBand(f_,g0,g1,name_ = ""Neb""+str(i),thresh_=0.004,nbeads_=nbead)\n\t\t\tself.path[i*nbead:(i+1)*nbead] = neb.Opt()\n\t\t# finally write the whole trajectory.\n\t\tfor i in range(self.path.shape[0]):\n\t\t\tm = Mol(g0_.atoms, self.path[i])\n\t\t\tm.WriteXYZfile(""./results/"",""WebPath"")\n\t\treturn\n'"
TensorMol/Simulations/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom .Opt import *\nfrom .Neb import *\nfrom .Web import *\nfrom .SimpleMD import *\nfrom .MetaDynamics import *\nfrom .InfraredMD import *\nfrom .OptPeriodic import *\nfrom .PeriodicMD import *\n'
TensorMol/SystemBuilders/MolBuilders.py,0,"b'""""""\nRoutines to build molecular systems.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Util import *\nfrom .Mol import *\nfrom .LinearOperations import *\n\ndef FillWithGas(l=10.0):\n\t""""""\n\tFills a box of length l with H2, CH4, N2, H2O, CO\n\t""""""\n\treturn \n'"
TensorMol/TFDescriptors/PairProviderTF.py,21,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom ..ForceModels.ElectrostaticsTF import *\nfrom .RawSH import *\nfrom .RawSymFunc import *\n\nclass PairProvider:\n\tdef __init__(self,nmol_,natom_):\n\t\t""""""\n\t\tReturns pairs within a cutoff.\n\n\t\tArgs:\n\t\t\tnatom: number of atoms in a molecule.\n\t\t\tnmol: number of molecules in the batch.\n\t\t""""""\n\t\tself.nmol = nmol_\n\t\tself.natom = natom_\n\t\tself.sess = None\n\t\tself.xyzs_pl = None\n\t\tself.nnz_pl = None\n\t\tself.rng_pl = None\n\t\tself.Prepare()\n\t\treturn\n\n\tdef PairsWithinCutoff(self,xyzs_pl,rng_pl,nnz_pl):\n\t\t""""""\n\t\tIt\'s up to whatever routine calls this to chop out\n\t\tall the indices which are larger than the number of atoms in this molecule.\n\t\t""""""\n\t\tnrawpairs = self.nmol*self.natom*self.natom\n\t\tDs = TFDistances(xyzs_pl)\n\t\tDs = tf.matrix_band_part(Ds, 0, -1) # Extract upper triangle\n\t\tDs = tf.where(tf.equal(Ds,0.0),10000.0*tf.ones_like(Ds),Ds)\n\t\tinds = tf.reshape(AllDoublesSet(tf.tile(tf.reshape(tf.range(self.natom),[1,self.natom]),[self.nmol,1])),[self.nmol*self.natom*self.natom,3])\n\t\t# inds has shape nmol X natom X natom X 3\n\t\tnnzs = tf.reshape(tf.tile(tf.reshape(nnz_pl,[self.nmol,1,1]),[1,self.natom,self.natom]),[self.nmol*self.natom*self.natom,1])\n\t\ta1 = tf.slice(inds,[0,1],[-1,1])\n\t\ta2 = tf.slice(inds,[0,2],[-1,1])\n\t\tprs = tf.reshape(tf.less(Ds,rng_pl),[self.nmol*self.natom*self.natom,1]) # Shape nmol X natom X natom X 1\n\t\tmsk0 = tf.reshape(tf.logical_and(tf.logical_and(tf.less(a1,nnzs),tf.less(a2,nnzs)),prs),[nrawpairs])\n\t\tinds = tf.boolean_mask(inds,msk0)\n\t\ta1 = tf.slice(inds,[0,1],[-1,1])\n\t\ta2 = tf.slice(inds,[0,2],[-1,1])\n\t\tnodiag = tf.not_equal(a1,a2)\n\t\tmsk = tf.reshape(nodiag,[tf.shape(inds)[0]])\n\t\treturn tf.boolean_mask(inds,msk)\n\n\tdef Prepare(self):\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(tf.float64, shape=tuple([None,self.natom,3]))\n\t\t\tself.rng_pl=tf.placeholder(tf.float64, shape=())\n\t\t\tself.nnz_pl=tf.placeholder(tf.int32, shape=(None))\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.GetPairs = self.PairsWithinCutoff(self.xyzs_pl,self.rng_pl,self.nnz_pl)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\t#self.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef __call__(self, xpl, rngpl, nnz):\n\t\t""""""\n\t\tReturns the nonzero pairs.\n\t\t""""""\n\t\ttmp = self.sess.run([self.GetPairs], feed_dict = {self.xyzs_pl:xpl,self.rng_pl:rngpl,self.nnz_pl: nnz})\n\t\treturn tmp[0]\n'"
TensorMol/TFDescriptors/RawSH.py,584,"b'""""""\nRaw => various descriptors in Tensorflow code.\n\nThe Raw format is a batch of rank three tensors.\nmol X MaxNAtoms X 4\nThe final dim is atomic number, x,y,z (Angstrom)\n\nhttps://www.youtube.com/watch?v=h2zgB93KANE\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom ..ForceModifiers.Neighbors import *\nfrom ..Containers.TensorData import *\nfrom ..ForceModels.ElectrostaticsTF import * # Why is this imported here?\nfrom tensorflow.python.client import timeline\nimport numpy as np\nfrom tensorflow.python.framework import function\nif (HAS_TF):\n\timport tensorflow as tf\n\ndef tf_pairs_list(xyzs, Zs, r_cutoff, element_pairs):\n\tdxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\n\tdist_tensor = tf.norm(dxyzs,axis=3)\n\tpadding_mask = tf.not_equal(Zs, 0)\n\tpair_indices = tf.where(tf.logical_and(tf.logical_and(tf.less(dist_tensor, r_cutoff),\n\t\t\t\t\ttf.expand_dims(padding_mask, axis=1)), tf.expand_dims(padding_mask, axis=-1)))\n\tpermutation_identity_mask = tf.where(tf.less(pair_indices[:,1], pair_indices[:,2]))\n\tpair_indices = tf.cast(tf.squeeze(tf.gather(pair_indices, permutation_identity_mask)), tf.int32)\n\tpair_distances = tf.expand_dims(tf.gather_nd(dist_tensor, pair_indices), axis=1)\n\tpair_elements = tf.stack([tf.gather_nd(Zs, pair_indices[:,0:2]), tf.gather_nd(Zs, pair_indices[:,0:3:2])], axis=-1)\n\telement_pair_mask = tf.cast(tf.where(tf.logical_or(tf.reduce_all(tf.equal(tf.expand_dims(pair_elements, axis=1), tf.expand_dims(element_pairs, axis=0)), axis=2),\n\t\t\t\t\t\ttf.reduce_all(tf.equal(tf.expand_dims(pair_elements, axis=1), tf.expand_dims(element_pairs[:,::-1], axis=0)), axis=2))), tf.int32)\n\tnum_element_pairs = element_pairs.get_shape().as_list()[0]\n\telement_pair_distances = tf.dynamic_partition(pair_distances, element_pair_mask[:,1], num_element_pairs)\n\tmol_indices = tf.dynamic_partition(pair_indices[:,0], element_pair_mask[:,1], num_element_pairs)\n\treturn element_pair_distances, mol_indices\n\ndef tf_triples_list(xyzs, Zs, r_cutoff, element_triples):\n\tnum_mols = Zs.get_shape().as_list()[0]\n\tdxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\n\tdist_tensor = tf.norm(dxyzs,axis=3)\n\tpadding_mask = tf.not_equal(Zs, 0)\n\tpair_indices = tf.where(tf.logical_and(tf.logical_and(tf.less(dist_tensor, r_cutoff),\n\t\t\t\t\ttf.expand_dims(padding_mask, axis=1)), tf.expand_dims(padding_mask, axis=-1)))\n\tpermutation_identity_mask = tf.where(tf.less(pair_indices[:,1], pair_indices[:,2]))\n\tpair_indices = tf.cast(tf.squeeze(tf.gather(pair_indices, permutation_identity_mask)), tf.int32)\n\tmol_pair_indices = tf.dynamic_partition(pair_indices, pair_indices[:,0], num_mols)\n\tmol_triples_indices = []\n\ttmp = []\n\tfor i in xrange(num_mols):\n\t\tmol_common_atom_indices = tf.where(tf.reduce_all(tf.equal(tf.expand_dims(mol_pair_indices[i][:,0:2], axis=0), tf.expand_dims(mol_pair_indices[i][:,0:2], axis=1)), axis=2))\n\t\tpermutation_pairs_mask = tf.where(tf.less(mol_common_atom_indices[:,0], mol_common_atom_indices[:,1]))\n\t\tmol_common_atom_indices = tf.squeeze(tf.gather(mol_common_atom_indices, permutation_pairs_mask), axis=1)\n\t\ttmp.append(mol_common_atom_indices)\n\t\tmol_triples_indices.append(tf.concat([tf.gather(mol_pair_indices[i], mol_common_atom_indices[:,0]), tf.expand_dims(tf.gather(mol_pair_indices[i], mol_common_atom_indices[:,1])[:,2], axis=1)], axis=1))\n\ttriples_indices = tf.concat(mol_triples_indices, axis=0)\n\ttriples_distances = tf.stack([tf.gather_nd(dist_tensor, triples_indices[:,:3]),\n\t\t\t\t\t\ttf.gather_nd(dist_tensor, tf.concat([triples_indices[:,:2], triples_indices[:,3:]], axis=1)),\n\t\t\t\t\t\ttf.gather_nd(dist_tensor, tf.concat([triples_indices[:,0:1], triples_indices[:,2:]], axis=1))], axis=-1)\n\tcos_thetas = tf.stack([(tf.square(triples_distances[:,0]) + tf.square(triples_distances[:,1]) - tf.square(triples_distances[:,2])) \\\n\t\t\t\t\t\t\t/ (2 * triples_distances[:,0] * triples_distances[:,1]),\n\t\t\t\t\t\t(tf.square(triples_distances[:,0]) - tf.square(triples_distances[:,1]) + tf.square(triples_distances[:,2])) \\\n\t\t\t\t\t\t\t/ (2 * triples_distances[:,0] * triples_distances[:,2]),\n\t\t\t\t\t\t(-tf.square(triples_distances[:,0]) + tf.square(triples_distances[:,1]) + tf.square(triples_distances[:,2])) \\\n\t\t\t\t\t\t\t/ (2 * triples_distances[:,1] * triples_distances[:,2])], axis=-1)\n\tcos_thetas = tf.where(tf.greater_equal(cos_thetas, 1.0), tf.ones_like(cos_thetas) * (1.0 - 1.0e-24), cos_thetas)\n\tcos_thetas = tf.where(tf.less_equal(cos_thetas, -1.0), -1.0 * tf.ones_like(cos_thetas) * (1.0 - 1.0e-24), cos_thetas)\n\ttriples_angles = tf.acos(cos_thetas)\n\ttriples_distances_angles = tf.concat([triples_distances, triples_angles], axis=1)\n\ttriples_elements = tf.stack([tf.gather_nd(Zs, triples_indices[:,0:2]), tf.gather_nd(Zs, triples_indices[:,0:3:2]), tf.gather_nd(Zs, triples_indices[:,0:4:3])], axis=-1)\n\tsorted_triples_elements, _ = tf.nn.top_k(triples_elements, k=3)\n\telement_triples_mask = tf.cast(tf.where(tf.reduce_all(tf.equal(tf.expand_dims(sorted_triples_elements, axis=1), tf.expand_dims(element_triples, axis=0)), axis=2)), tf.int32)\n\tnum_element_triples = element_triples.get_shape().as_list()[0]\n\telement_triples_distances_angles = tf.dynamic_partition(triples_distances_angles, element_triples_mask[:,1], num_element_triples)\n\tmol_indices = tf.dynamic_partition(triples_indices[:,0], element_triples_mask[:,1], num_element_triples)\n\treturn element_triples_distances_angles, mol_indices\n\n\ndef matrix_power(matrix, power):\n\t""""""\n\tRaise a Hermitian Matrix to a possibly fractional power.\n\n\tArgs:\n\t\tmatrix (tf.float): Diagonalizable matrix\n\t\tpower (tf.float): power to raise the matrix to\n\n\tReturns:\n\t\tmatrix_to_power (tf.float): matrix raised to the power\n\n\tNote:\n\t\tAs of tensorflow v1.3, tf.svd() does not have gradients implimented\n\t""""""\n\ts, U, V = tf.svd(matrix)\n\ts = tf.maximum(s, tf.pow(10.0, -14.0))\n\treturn tf.matmul(U, tf.matmul(tf.diag(tf.pow(s, power)), tf.transpose(V)))\n\ndef matrix_power2(matrix, power):\n\t""""""\n\tRaises a matrix to a possibly fractional power\n\n\tArgs:\n\t\tmatrix (tf.float): Diagonalizable matrix\n\t\tpower (tf.float): power to raise the matrix to\n\n\tReturns:\n\t\tmatrix_to_power (tf.float): matrix raised to the power\n\t""""""\n\tmatrix_eigenvals, matrix_eigenvecs = tf.self_adjoint_eig(matrix)\n\tmatrix_to_power = tf.matmul(matrix_eigenvecs, tf.matmul(tf.matrix_diag(tf.pow(matrix_eigenvals, power)), tf.transpose(matrix_eigenvecs)))\n\treturn matrix_to_power\n\ndef tf_gauss_overlap(gauss_params):\n\tr_nought = gauss_params[:,0]\n\tsigma = gauss_params[:,1]\n\tscaling_factor = tf.cast(tf.sqrt(np.pi / 2), eval(PARAMS[""tf_prec""]))\n\texponential_factor = tf.exp(-tf.square(tf.expand_dims(r_nought, axis=0) - tf.expand_dims(r_nought, axis=1))\n\t/ (2.0 * (tf.square(tf.expand_dims(sigma, axis=0)) + tf.square(tf.expand_dims(sigma, axis=1)))))\n\troot_inverse_sigma_sum = tf.sqrt((1.0 / tf.expand_dims(tf.square(sigma), axis=0)) + (1.0 / tf.expand_dims(tf.square(sigma), axis=1)))\n\terf_numerator = (tf.expand_dims(r_nought, axis=0) * tf.expand_dims(tf.square(sigma), axis=1)\n\t\t\t\t+ tf.expand_dims(r_nought, axis=1) * tf.expand_dims(tf.square(sigma), axis=0))\n\terf_denominator = (tf.sqrt(tf.cast(2.0, eval(PARAMS[""tf_prec""]))) * tf.expand_dims(tf.square(sigma), axis=0) * tf.expand_dims(tf.square(sigma), axis=1)\n\t\t\t\t* root_inverse_sigma_sum)\n\terf_factor = 1 + tf.erf(erf_numerator / erf_denominator)\n\toverlap_matrix = scaling_factor * exponential_factor * erf_factor / root_inverse_sigma_sum\n\treturn overlap_matrix\n\ndef tf_sparse_gauss(dist_tensor, gauss_params):\n\texponent = ((tf.square(tf.expand_dims(dist_tensor, axis=-1) - tf.expand_dims(gauss_params[:,0], axis=0)))\n\t\t\t\t/ (-2.0 * (gauss_params[:,1] ** 2)))\n\tgaussian_embed = tf.where(tf.greater(exponent, -25.0), tf.exp(exponent), tf.zeros_like(exponent))\n\txi = (dist_tensor - 6.0) / (7.0 - 6.0)\n\tcutoff_factor = 1 - 3 * tf.square(xi) + 2 * tf.pow(xi, 3.0)\n\tcutoff_factor = tf.where(tf.greater(dist_tensor, 7.0), tf.zeros_like(cutoff_factor), cutoff_factor)\n\tcutoff_factor = tf.where(tf.less(dist_tensor, 6.0), tf.ones_like(cutoff_factor), cutoff_factor)\n\treturn gaussian_embed * tf.expand_dims(cutoff_factor, axis=-1)\n\ndef tf_gauss(dist_tensor, gauss_params):\n\texponent = (tf.square(tf.expand_dims(dist_tensor, axis=-1) - tf.expand_dims(tf.expand_dims(gauss_params[:,0], axis=0), axis=1))) \\\n\t\t\t\t/ (-2.0 * (gauss_params[:,1] ** 2))\n\tgaussian_embed = tf.where(tf.greater(exponent, -25.0), tf.exp(exponent), tf.zeros_like(exponent))\n\tidentity_mask = tf.matrix_set_diag(tf.ones_like(gaussian_embed[:,:,0]), tf.zeros_like(gaussian_embed[0,:,0]))\n\txi = (dist_tensor - 6.0) / (7.0 - 6.0)\n\tcutoff_factor = 1 - 3 * tf.square(xi) + 2 * tf.pow(xi, 3.0)\n\tcutoff_factor = tf.where(tf.greater(dist_tensor, 7.0), tf.zeros_like(cutoff_factor), cutoff_factor)\n\tcutoff_factor = tf.where(tf.less(dist_tensor, 6.0), tf.ones_like(cutoff_factor), cutoff_factor)\n\treturn gaussian_embed * tf.expand_dims(identity_mask, axis=-1) * tf.expand_dims(cutoff_factor, axis=-1)\n\ndef tf_spherical_harmonics_0(inv_dist_tensor):\n\treturn tf.fill(tf.shape(inv_dist_tensor), tf.constant(0.28209479177387814, dtype=eval(PARAMS[""tf_prec""])))\n\ndef tf_spherical_harmonics_1(dxyzs, inv_dist_tensor):\n\tlower_order_harmonics = tf_spherical_harmonics_0(tf.expand_dims(inv_dist_tensor, axis=-1))\n\tl1_harmonics = 0.4886025119029199 * tf.stack([dxyzs[...,1], dxyzs[...,2], dxyzs[...,0]],\n\t\t\t\t\t\t\t\t\t\taxis=-1) * tf.expand_dims(inv_dist_tensor, axis=-1)\n\tif PARAMS[""SH_rot_invar""]:\n\t\treturn tf.concat([lower_order_harmonics, tf.norm(l1_harmonics+1.e-16, axis=-1, keep_dims=True)], axis=-1)\n\telse:\n\t\treturn tf.concat([lower_order_harmonics, l1_harmonics], axis=-1)\n\ndef tf_spherical_harmonics_2(dxyzs, inv_dist_tensor):\n\tlower_order_harmonics = tf_spherical_harmonics_1(dxyzs, inv_dist_tensor)\n\tl2_harmonics = tf.stack([(-1.0925484305920792 * dxyzs[...,0] * dxyzs[...,1]),\n\t\t\t(1.0925484305920792 * dxyzs[...,1] * dxyzs[...,2]),\n\t\t\t(-0.31539156525252005 * (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 2. * tf.square(dxyzs[...,2]))),\n\t\t\t(1.0925484305920792 * dxyzs[...,0] * dxyzs[...,2]),\n\t\t\t(0.5462742152960396 * (tf.square(dxyzs[...,0]) - 1. * tf.square(dxyzs[...,1])))], axis=-1) \\\n\t\t\t* tf.expand_dims(tf.square(inv_dist_tensor),axis=-1)\n\tif PARAMS[""SH_rot_invar""]:\n\t\treturn tf.concat([lower_order_harmonics, tf.norm(l2_harmonics+1.e-16, axis=-1, keep_dims=True)], axis=-1)\n\telse:\n\t\treturn tf.concat([lower_order_harmonics, l2_harmonics], axis=-1)\n\ndef tf_spherical_harmonics_3(dxyzs, inv_dist_tensor):\n\tlower_order_harmonics = tf_spherical_harmonics_2(dxyzs, inv_dist_tensor)\n\tl3_harmonics = tf.stack([(-0.5900435899266435 * dxyzs[...,1] * (-3. * tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]))),\n\t\t\t(-2.890611442640554 * dxyzs[...,0] * dxyzs[...,1] * dxyzs[...,2]),\n\t\t\t(-0.4570457994644658 * dxyzs[...,1] * (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 4. \\\n\t\t\t\t* tf.square(dxyzs[...,2]))),\n\t\t\t(0.3731763325901154 * dxyzs[...,2] * (-3. * tf.square(dxyzs[...,0]) - 3. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ 2. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.4570457994644658 * dxyzs[...,0] * (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 4. \\\n\t\t\t\t* tf.square(dxyzs[...,2]))),\n\t\t\t(1.445305721320277 * (tf.square(dxyzs[...,0]) - 1. * tf.square(dxyzs[...,1])) * dxyzs[...,2]),\n\t\t\t(0.5900435899266435 * dxyzs[...,0] * (tf.square(dxyzs[...,0]) - 3. * tf.square(dxyzs[...,1])))], axis=-1) \\\n\t\t\t\t* tf.expand_dims(tf.pow(inv_dist_tensor,3),axis=-1)\n\tif PARAMS[""SH_rot_invar""]:\n\t\treturn tf.concat([lower_order_harmonics, tf.norm(l3_harmonics+1.e-16, axis=-1, keep_dims=True)], axis=-1)\n\telse:\n\t\treturn tf.concat([lower_order_harmonics, l3_harmonics], axis=-1)\n\ndef tf_spherical_harmonics_4(dxyzs, inv_dist_tensor):\n\tlower_order_harmonics = tf_spherical_harmonics_3(dxyzs, inv_dist_tensor)\n\tl4_harmonics = tf.stack([(2.5033429417967046 * dxyzs[...,0] * dxyzs[...,1] * (-1. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t+ tf.square(dxyzs[...,1]))),\n\t\t\t(-1.7701307697799304 * dxyzs[...,1] * (-3. * tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1])) * dxyzs[...,2]),\n\t\t\t(0.9461746957575601 * dxyzs[...,0] * dxyzs[...,1] * (tf.square(dxyzs[...,0]) \\\n\t\t\t\t+ tf.square(dxyzs[...,1]) - 6. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.6690465435572892 * dxyzs[...,1] * dxyzs[...,2] * (3. * tf.square(dxyzs[...,0]) + 3. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) - 4. * tf.square(dxyzs[...,2]))),\n\t\t\t(0.10578554691520431 * (3. * tf.pow(dxyzs[...,0], 4) + 3. * tf.pow(dxyzs[...,1], 4) - 24. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 8. * tf.pow(dxyzs[...,2], 4) + 6. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 4. * tf.square(dxyzs[...,2])))),\n\t\t\t(-0.6690465435572892 * dxyzs[...,0] * dxyzs[...,2] * (3. * tf.square(dxyzs[...,0]) + 3.\n\t\t\t\t* tf.square(dxyzs[...,1]) - 4. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.47308734787878004 * (tf.square(dxyzs[...,0]) - 1. * tf.square(dxyzs[...,1])) \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 6. * tf.square(dxyzs[...,2]))),\n\t\t\t(1.7701307697799304 * dxyzs[...,0] * (tf.square(dxyzs[...,0]) - 3. * tf.square(dxyzs[...,1])) * dxyzs[...,2]),\n\t\t\t(0.6258357354491761 * (tf.pow(dxyzs[...,0], 4) - 6. * tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ tf.pow(dxyzs[...,1], 4)))], axis=-1) \\\n\t\t\t* tf.expand_dims(tf.pow(inv_dist_tensor,4),axis=-1)\n\tif PARAMS[""SH_rot_invar""]:\n\t\treturn tf.concat([lower_order_harmonics, tf.norm(l4_harmonics+1.e-16, axis=-1, keep_dims=True)], axis=-1)\n\telse:\n\t\treturn tf.concat([lower_order_harmonics, l4_harmonics], axis=-1)\n\ndef tf_spherical_harmonics_5(dxyzs, inv_dist_tensor):\n\tlower_order_harmonics = tf_spherical_harmonics_4(dxyzs, inv_dist_tensor)\n\tl5_harmonics = tf.stack([(0.6563820568401701 * dxyzs[...,1] * (5. * tf.pow(dxyzs[...,0], 4) - 10. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) + tf.pow(dxyzs[...,1], 4))),\n\t\t\t(8.302649259524166 * dxyzs[...,0] * dxyzs[...,1] * (-1. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t+ tf.square(dxyzs[...,1])) * dxyzs[...,2]),\n\t\t\t(0.4892382994352504 * dxyzs[...,1] * (-3. * tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1])) \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2]))),\n\t\t\t(4.793536784973324 * dxyzs[...,0] * dxyzs[...,1] * dxyzs[...,2] \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 2. * tf.square(dxyzs[...,2]))),\n\t\t\t(0.45294665119569694 * dxyzs[...,1] * (tf.pow(dxyzs[...,0], 4) + tf.pow(dxyzs[...,1], 4) - 12. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 8. * tf.pow(dxyzs[...,2], 4) + 2. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 6. * tf.square(dxyzs[...,2])))),\n\t\t\t(0.1169503224534236 * dxyzs[...,2] * (15. * tf.pow(dxyzs[...,0], 4) + 15. * tf.pow(dxyzs[...,1], 4) \\\n\t\t\t\t- 40. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 8. * tf.pow(dxyzs[...,2], 4) + 10. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (3. * tf.square(dxyzs[...,1]) - 4. * tf.square(dxyzs[...,2])))),\n\t\t\t(0.45294665119569694 * dxyzs[...,0] * (tf.pow(dxyzs[...,0], 4) + tf.pow(dxyzs[...,1], 4) - 12. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 8. * tf.pow(dxyzs[...,2], 4) + 2. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 6. * tf.square(dxyzs[...,2])))),\n\t\t\t(-2.396768392486662 * (tf.square(dxyzs[...,0]) - 1. * tf.square(dxyzs[...,1])) * dxyzs[...,2] \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 2. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.4892382994352504 * dxyzs[...,0] * (tf.square(dxyzs[...,0]) - 3. * tf.square(dxyzs[...,1])) \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2]))),\n\t\t\t(2.0756623148810416 * (tf.pow(dxyzs[...,0], 4) - 6. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) + tf.pow(dxyzs[...,1], 4)) * dxyzs[...,2]),\n\t\t\t(0.6563820568401701 * dxyzs[...,0] * (tf.pow(dxyzs[...,0], 4) - 10. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) + 5. * tf.pow(dxyzs[...,1], 4)))], axis=-1) \\\n\t\t\t* tf.expand_dims(tf.pow(inv_dist_tensor,5),axis=-1)\n\tif PARAMS[""SH_rot_invar""]:\n\t\treturn tf.concat([lower_order_harmonics, tf.norm(l5_harmonics+1.e-16, axis=-1, keep_dims=True)], axis=-1)\n\telse:\n\t\treturn tf.concat([lower_order_harmonics, l5_harmonics], axis=-1)\n\ndef tf_spherical_harmonics_6(dxyzs, inv_dist_tensor):\n\tlower_order_harmonics = tf_spherical_harmonics_5(dxyzs, inv_dist_tensor)\n\tl6_harmonics = tf.stack([(-1.3663682103838286 * dxyzs[...,0] * dxyzs[...,1] * (3. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t- 10. * tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) + 3. * tf.pow(dxyzs[...,1], 4))),\n\t\t\t(2.366619162231752 * dxyzs[...,1] * (5. * tf.pow(dxyzs[...,0], 4) - 10. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) + tf.pow(dxyzs[...,1], 4)) * dxyzs[...,2]),\n\t\t\t(2.0182596029148967 * dxyzs[...,0] * dxyzs[...,1] * (tf.square(dxyzs[...,0]) - 1. * tf.square(dxyzs[...,1])) \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 10. * tf.square(dxyzs[...,2]))),\n\t\t\t(0.9212052595149236 * dxyzs[...,1] * (-3. * tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1])) \\\n\t\t\t\t* dxyzs[...,2] * (3. * tf.square(dxyzs[...,0]) + 3. * tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.9212052595149236 * dxyzs[...,0] * dxyzs[...,1] * (tf.pow(dxyzs[...,0], 4) + tf.pow(dxyzs[...,1], 4) \\\n\t\t\t\t- 16. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 16. * tf.pow(dxyzs[...,2], 4) \\\n\t\t\t\t+ 2. * tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2])))),\n\t\t\t(0.5826213625187314 * dxyzs[...,1] * dxyzs[...,2] * (5. * tf.pow(dxyzs[...,0], 4) + 5. * tf.pow(dxyzs[...,1], 4) \\\n\t\t\t\t- 20. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 8. * tf.pow(dxyzs[...,2], 4) \\\n\t\t\t\t+ 10. * tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 2. * tf.square(dxyzs[...,2])))),\n\t\t\t(-0.06356920226762842 * (5. * tf.pow(dxyzs[...,0], 6) + 5. * tf.pow(dxyzs[...,1], 6) - 90. \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 4) * tf.square(dxyzs[...,2]) + 120. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) - 16. * tf.pow(dxyzs[...,2], 6) + 15. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t* (tf.square(dxyzs[...,1]) - 6. * tf.square(dxyzs[...,2])) + 15. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* (tf.pow(dxyzs[...,1], 4) - 12. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 8. * tf.pow(dxyzs[...,2], 4)))),\n\t\t\t(0.5826213625187314 * dxyzs[...,0] * dxyzs[...,2] * (5. * tf.pow(dxyzs[...,0], 4) + 5. \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 4) - 20. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 8. \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) + 10. * tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 2. \\\n\t\t\t\t* tf.square(dxyzs[...,2])))),\n\t\t\t(0.4606026297574618 * (tf.square(dxyzs[...,0]) - 1. * tf.square(dxyzs[...,1])) * (tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t+ tf.pow(dxyzs[...,1], 4) - 16. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 16. \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) + 2. * tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 8. \\\n\t\t\t\t* tf.square(dxyzs[...,2])))),\n\t\t\t(-0.9212052595149236 * dxyzs[...,0] * (tf.square(dxyzs[...,0]) - 3. * tf.square(dxyzs[...,1])) * dxyzs[...,2] \\\n\t\t\t\t* (3. * tf.square(dxyzs[...,0]) + 3. * tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.5045649007287242 * (tf.pow(dxyzs[...,0], 4) - 6. * tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ tf.pow(dxyzs[...,1], 4)) * (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 10. * tf.square(dxyzs[...,2]))),\n\t\t\t(2.366619162231752 * dxyzs[...,0] * (tf.pow(dxyzs[...,0], 4) - 10. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) + 5. * tf.pow(dxyzs[...,1], 4)) * dxyzs[...,2]),\n\t\t\t(0.6831841051919143 * (tf.pow(dxyzs[...,0], 6) - 15. * tf.pow(dxyzs[...,0], 4) * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ 15. * tf.square(dxyzs[...,0]) * tf.pow(dxyzs[...,1], 4) - 1. * tf.pow(dxyzs[...,1], 6)))], axis=-1) \\\n\t\t\t* tf.expand_dims(tf.pow(inv_dist_tensor,6),axis=-1)\n\tif PARAMS[""SH_rot_invar""]:\n\t\treturn tf.concat([lower_order_harmonics, tf.norm(l6_harmonics+1.e-16, axis=-1, keep_dims=True)], axis=-1)\n\telse:\n\t\treturn tf.concat([lower_order_harmonics, l6_harmonics], axis=-1)\n\ndef tf_spherical_harmonics_7(dxyzs, inv_dist_tensor):\n\tlower_order_harmonics = tf_spherical_harmonics_6(dxyzs, inv_dist_tensor)\n\tl7_harmonics = tf.stack([(-0.7071627325245962 * dxyzs[...,1] * (-7. * tf.pow(dxyzs[...,0], 6) + 35. \\\n\t\t\t\t* tf.pow(dxyzs[...,0], 4) * tf.square(dxyzs[...,1]) - 21. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 4) + tf.pow(dxyzs[...,1], 6))),\n\t\t\t(-5.291921323603801 * dxyzs[...,0] * dxyzs[...,1] * (3. * tf.pow(dxyzs[...,0], 4) - 10. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) + 3. * tf.pow(dxyzs[...,1], 4)) * dxyzs[...,2]),\n\t\t\t(-0.5189155787202604 * dxyzs[...,1] * (5. * tf.pow(dxyzs[...,0], 4) - 10. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) + tf.pow(dxyzs[...,1], 4)) \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 12. * tf.square(dxyzs[...,2]))),\n\t\t\t(4.151324629762083 * dxyzs[...,0] * dxyzs[...,1] * (tf.square(dxyzs[...,0]) - 1. \\\n\t\t\t\t* tf.square(dxyzs[...,1])) * dxyzs[...,2] * (3. * tf.square(dxyzs[...,0]) + 3. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) - 10. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.15645893386229404 * dxyzs[...,1] * (-3. * tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1])) \\\n\t\t\t\t* (3. * tf.pow(dxyzs[...,0], 4) + 3. * tf.pow(dxyzs[...,1], 4) - 60. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.square(dxyzs[...,2]) + 80. * tf.pow(dxyzs[...,2], 4) + 6. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* (tf.square(dxyzs[...,1]) - 10. * tf.square(dxyzs[...,2])))),\n\t\t\t(-0.4425326924449826 * dxyzs[...,0] * dxyzs[...,1] * dxyzs[...,2] * (15. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t+ 15. * tf.pow(dxyzs[...,1], 4) - 80. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 48. \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) + 10. * tf.square(dxyzs[...,0]) * (3. * tf.square(dxyzs[...,1]) - 8. \\\n\t\t\t\t* tf.square(dxyzs[...,2])))),\n\t\t\t(-0.0903316075825173 * dxyzs[...,1] * (5. * tf.pow(dxyzs[...,0], 6) + 5. * tf.pow(dxyzs[...,1], 6) - 120. \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 4) * tf.square(dxyzs[...,2]) + 240. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) - 64. * tf.pow(dxyzs[...,2], 6) + 15. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t* (tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2])) + 15. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* (tf.pow(dxyzs[...,1], 4) - 16. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 16. \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4)))),\n\t\t\t(0.06828427691200495 * dxyzs[...,2] * (-35. * tf.pow(dxyzs[...,0], 6) - 35. * tf.pow(dxyzs[...,1], 6) \\\n\t\t\t\t+ 210. * tf.pow(dxyzs[...,1], 4) * tf.square(dxyzs[...,2]) - 168. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) + 16. * tf.pow(dxyzs[...,2], 6) - 105. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t* (tf.square(dxyzs[...,1]) - 2. * tf.square(dxyzs[...,2])) - 21. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* (5. * tf.pow(dxyzs[...,1], 4) - 20. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) \\\n\t\t\t\t+ 8. * tf.pow(dxyzs[...,2], 4)))),\n\t\t\t(-0.0903316075825173 * dxyzs[...,0] * (5. * tf.pow(dxyzs[...,0], 6) + 5. * tf.pow(dxyzs[...,1], 6) \\\n\t\t\t\t- 120. * tf.pow(dxyzs[...,1], 4) * tf.square(dxyzs[...,2]) + 240. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) - 64. * tf.pow(dxyzs[...,2], 6) + 15. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t* (tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2])) + 15. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* (tf.pow(dxyzs[...,1], 4) - 16. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 16. \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4)))),\n\t\t\t(0.2212663462224913 * (tf.square(dxyzs[...,0]) - 1. * tf.square(dxyzs[...,1])) * dxyzs[...,2] \\\n\t\t\t\t* (15. * tf.pow(dxyzs[...,0], 4) + 15. * tf.pow(dxyzs[...,1], 4) - 80. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.square(dxyzs[...,2]) + 48. * tf.pow(dxyzs[...,2], 4) + 10. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* (3. * tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2])))),\n\t\t\t(0.15645893386229404 * dxyzs[...,0] * (tf.square(dxyzs[...,0]) - 3. * tf.square(dxyzs[...,1])) \\\n\t\t\t\t* (3. * tf.pow(dxyzs[...,0], 4) + 3. * tf.pow(dxyzs[...,1], 4) - 60. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.square(dxyzs[...,2]) + 80. * tf.pow(dxyzs[...,2], 4) + 6. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* (tf.square(dxyzs[...,1]) - 10. * tf.square(dxyzs[...,2])))),\n\t\t\t(-1.0378311574405208 * (tf.pow(dxyzs[...,0], 4) - 6. * tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ tf.pow(dxyzs[...,1], 4)) * dxyzs[...,2] * (3. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t+ 3. * tf.square(dxyzs[...,1]) - 10. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.5189155787202604 * dxyzs[...,0] * (tf.pow(dxyzs[...,0], 4) - 10. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) + 5. * tf.pow(dxyzs[...,1], 4)) * (tf.square(dxyzs[...,0]) \\\n\t\t\t\t+ tf.square(dxyzs[...,1]) - 12. * tf.square(dxyzs[...,2]))),\n\t\t\t(2.6459606618019005 * (tf.pow(dxyzs[...,0], 6) - 15. * tf.pow(dxyzs[...,0], 4) * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ 15. * tf.square(dxyzs[...,0]) * tf.pow(dxyzs[...,1], 4) - 1. * tf.pow(dxyzs[...,1], 6)) * dxyzs[...,2]),\n\t\t\t(0.7071627325245962 * dxyzs[...,0] * (tf.pow(dxyzs[...,0], 6) - 21. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) + 35. * tf.square(dxyzs[...,0]) * tf.pow(dxyzs[...,1], 4) - 7. \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 6)))], axis=-1) \\\n\t\t\t* tf.expand_dims(tf.pow(inv_dist_tensor,7),axis=-1)\n\tif PARAMS[""SH_rot_invar""]:\n\t\treturn tf.concat([lower_order_harmonics, tf.norm(l7_harmonics+1.e-16, axis=-1, keep_dims=True)], axis=-1)\n\telse:\n\t\treturn tf.concat([lower_order_harmonics, l7_harmonics], axis=-1)\n\ndef tf_spherical_harmonics_8(dxyzs, inv_dist_tensor):\n\tlower_order_harmonics = tf_spherical_harmonics_7(dxyzs, inv_dist_tensor)\n\tl8_harmonics = tf.stack([(-5.831413281398639 * dxyzs[...,0] * dxyzs[...,1] * (tf.pow(dxyzs[...,0], 6) \\\n\t\t\t\t- 7. * tf.pow(dxyzs[...,0], 4) * tf.square(dxyzs[...,1]) + 7. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 4) - 1. * tf.pow(dxyzs[...,1], 6))),\n\t\t\t(-2.9157066406993195 * dxyzs[...,1] * (-7. * tf.pow(dxyzs[...,0], 6) + 35. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) - 21. * tf.square(dxyzs[...,0]) * tf.pow(dxyzs[...,1], 4) \\\n\t\t\t\t+ tf.pow(dxyzs[...,1], 6)) * dxyzs[...,2]),\n\t\t\t(1.0646655321190852 * dxyzs[...,0] * dxyzs[...,1] * (3. * tf.pow(dxyzs[...,0], 4) - 10. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) + 3. * tf.pow(dxyzs[...,1], 4)) \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 14. * tf.square(dxyzs[...,2]))),\n\t\t\t(-3.449910622098108 * dxyzs[...,1] * (5. * tf.pow(dxyzs[...,0], 4) - 10. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) + tf.pow(dxyzs[...,1], 4)) * dxyzs[...,2] * (tf.square(dxyzs[...,0]) \\\n\t\t\t\t+ tf.square(dxyzs[...,1]) - 4. * tf.square(dxyzs[...,2]))),\n\t\t\t(-1.9136660990373227 * dxyzs[...,0] * dxyzs[...,1] * (tf.square(dxyzs[...,0]) - 1. \\\n\t\t\t\t* tf.square(dxyzs[...,1])) * (tf.pow(dxyzs[...,0], 4) + tf.pow(dxyzs[...,1], 4) - 24. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 40. * tf.pow(dxyzs[...,2], 4) + 2. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 12. * tf.square(dxyzs[...,2])))),\n\t\t\t(-1.2352661552955442 * dxyzs[...,1] * (-3. * tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1])) \\\n\t\t\t\t* dxyzs[...,2] * (3. * tf.pow(dxyzs[...,0], 4) + 3. * tf.pow(dxyzs[...,1], 4) - 20. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 16. * tf.pow(dxyzs[...,2], 4) \\\n\t\t\t\t+ tf.square(dxyzs[...,0]) * (6. * tf.square(dxyzs[...,1]) - 20. * tf.square(dxyzs[...,2])))),\n\t\t\t(0.912304516869819 * dxyzs[...,0] * dxyzs[...,1] * (tf.pow(dxyzs[...,0], 6) + tf.pow(dxyzs[...,1], 6) \\\n\t\t\t\t- 30. * tf.pow(dxyzs[...,1], 4) * tf.square(dxyzs[...,2]) + 80. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) - 32. * tf.pow(dxyzs[...,2], 6) + 3. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t* (tf.square(dxyzs[...,1]) - 10. * tf.square(dxyzs[...,2])) + tf.square(dxyzs[...,0]) \\\n\t\t\t\t* (3. * tf.pow(dxyzs[...,1], 4) - 60. * tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 80. \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4)))),\n\t\t\t(-0.10904124589877995 * dxyzs[...,1] * dxyzs[...,2] * (35. * tf.pow(dxyzs[...,0], 6) + 35. \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 6) - 280. * tf.pow(dxyzs[...,1], 4) * tf.square(dxyzs[...,2]) + 336. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.pow(dxyzs[...,2], 4) - 64. * tf.pow(dxyzs[...,2], 6) + 35. \\\n\t\t\t\t* tf.pow(dxyzs[...,0], 4) * (3. * tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2])) + 7. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (15. * tf.pow(dxyzs[...,1], 4) - 80. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.square(dxyzs[...,2]) + 48. * tf.pow(dxyzs[...,2], 4)))),\n\t\t\t(0.009086770491564996 * (35. * tf.pow(dxyzs[...,0], 8) + 35. * tf.pow(dxyzs[...,1], 8) - 1120. \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 6) * tf.square(dxyzs[...,2]) + 3360. * tf.pow(dxyzs[...,1], 4) \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 4) - 1792. * tf.square(dxyzs[...,1]) * tf.pow(dxyzs[...,2], 6) + 128. \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 8) + 140. * tf.pow(dxyzs[...,0], 6) * (tf.square(dxyzs[...,1]) - 8. \\\n\t\t\t\t* tf.square(dxyzs[...,2])) + 210. * tf.pow(dxyzs[...,0], 4) * (tf.pow(dxyzs[...,1], 4) - 16. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 16. * tf.pow(dxyzs[...,2], 4)) + 28. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (5. * tf.pow(dxyzs[...,1], 6) - 120. * tf.pow(dxyzs[...,1], 4) \\\n\t\t\t\t* tf.square(dxyzs[...,2]) + 240. * tf.square(dxyzs[...,1]) * tf.pow(dxyzs[...,2], 4) - 64. \\\n\t\t\t\t* tf.pow(dxyzs[...,2], 6)))),\n\t\t\t(-0.10904124589877995 * dxyzs[...,0] * dxyzs[...,2] * (35. * tf.pow(dxyzs[...,0], 6) + 35. \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 6) - 280. * tf.pow(dxyzs[...,1], 4) * tf.square(dxyzs[...,2]) + 336. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.pow(dxyzs[...,2], 4) - 64. * tf.pow(dxyzs[...,2], 6) + 35. \\\n\t\t\t\t* tf.pow(dxyzs[...,0], 4) * (3. * tf.square(dxyzs[...,1]) - 8. * tf.square(dxyzs[...,2])) + 7. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (15. * tf.pow(dxyzs[...,1], 4) - 80. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.square(dxyzs[...,2]) + 48. * tf.pow(dxyzs[...,2], 4)))),\n\t\t\t(-0.4561522584349095 * (tf.square(dxyzs[...,0]) - 1. * tf.square(dxyzs[...,1])) * (tf.pow(dxyzs[...,0], 6) \\\n\t\t\t\t+ tf.pow(dxyzs[...,1], 6) - 30. * tf.pow(dxyzs[...,1], 4) * tf.square(dxyzs[...,2]) + 80. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.pow(dxyzs[...,2], 4) - 32. * tf.pow(dxyzs[...,2], 6) + 3. \\\n\t\t\t\t* tf.pow(dxyzs[...,0], 4) * (tf.square(dxyzs[...,1]) - 10. * tf.square(dxyzs[...,2])) \\\n\t\t\t\t+ tf.square(dxyzs[...,0]) * (3. * tf.pow(dxyzs[...,1], 4) - 60. * tf.square(dxyzs[...,1]) \\\n\t\t\t\t* tf.square(dxyzs[...,2]) + 80. * tf.pow(dxyzs[...,2], 4)))),\n\t\t\t(1.2352661552955442 * dxyzs[...,0] * (tf.square(dxyzs[...,0]) - 3. * tf.square(dxyzs[...,1])) \\\n\t\t\t\t* dxyzs[...,2] * (3. * tf.pow(dxyzs[...,0], 4) + 3. * tf.pow(dxyzs[...,1], 4) - 20. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 16. * tf.pow(dxyzs[...,2], 4) \\\n\t\t\t\t+ tf.square(dxyzs[...,0]) * (6. * tf.square(dxyzs[...,1]) - 20. * tf.square(dxyzs[...,2])))),\n\t\t\t(0.47841652475933066 * (tf.pow(dxyzs[...,0], 4) - 6. * tf.square(dxyzs[...,0]) * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ tf.pow(dxyzs[...,1], 4)) * (tf.pow(dxyzs[...,0], 4) + tf.pow(dxyzs[...,1], 4) - 24. \\\n\t\t\t\t* tf.square(dxyzs[...,1]) * tf.square(dxyzs[...,2]) + 40. * tf.pow(dxyzs[...,2], 4) + 2. \\\n\t\t\t\t* tf.square(dxyzs[...,0]) * (tf.square(dxyzs[...,1]) - 12. * tf.square(dxyzs[...,2])))),\n\t\t\t(-3.449910622098108 * dxyzs[...,0] * (tf.pow(dxyzs[...,0], 4) - 10. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) + 5. * tf.pow(dxyzs[...,1], 4)) * dxyzs[...,2] * (tf.square(dxyzs[...,0]) \\\n\t\t\t\t+ tf.square(dxyzs[...,1]) - 4. * tf.square(dxyzs[...,2]))),\n\t\t\t(-0.5323327660595426 * (tf.pow(dxyzs[...,0], 6) - 15. * tf.pow(dxyzs[...,0], 4) * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ 15. * tf.square(dxyzs[...,0]) * tf.pow(dxyzs[...,1], 4) - 1. * tf.pow(dxyzs[...,1], 6)) \\\n\t\t\t\t* (tf.square(dxyzs[...,0]) + tf.square(dxyzs[...,1]) - 14. * tf.square(dxyzs[...,2]))),\n\t\t\t(2.9157066406993195 * dxyzs[...,0] * (tf.pow(dxyzs[...,0], 6) - 21. * tf.pow(dxyzs[...,0], 4) \\\n\t\t\t\t* tf.square(dxyzs[...,1]) + 35. * tf.square(dxyzs[...,0]) * tf.pow(dxyzs[...,1], 4) - 7. \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 6)) * dxyzs[...,2]),\n\t\t\t(0.7289266601748299 * (tf.pow(dxyzs[...,0], 8) - 28. * tf.pow(dxyzs[...,0], 6) * tf.square(dxyzs[...,1]) \\\n\t\t\t\t+ 70. * tf.pow(dxyzs[...,0], 4) * tf.pow(dxyzs[...,1], 4) - 28. * tf.square(dxyzs[...,0]) \\\n\t\t\t\t* tf.pow(dxyzs[...,1], 6) + tf.pow(dxyzs[...,1], 8)))], axis=-1) \\\n\t\t\t* tf.expand_dims(tf.pow(inv_dist_tensor,8),axis=-1)\n\tif PARAMS[""SH_rot_invar""]:\n\t\treturn tf.concat([lower_order_harmonics, tf.norm(l8_harmonics+1.e-16, axis=-1, keep_dims=True)], axis=-1)\n\telse:\n\t\treturn tf.concat([lower_order_harmonics, l8_harmonics], axis=-1)\n\ndef tf_spherical_harmonics(dxyzs, dist_tensor, max_l):\n\tinv_dist_tensor = tf.where(tf.greater(dist_tensor, 1.e-9), tf.reciprocal(dist_tensor), tf.zeros_like(dist_tensor))\n\tif max_l == 8:\n\t\tharmonics = tf_spherical_harmonics_8(dxyzs, inv_dist_tensor)\n\telif max_l == 7:\n\t\tharmonics = tf_spherical_harmonics_7(dxyzs, inv_dist_tensor)\n\telif max_l == 6:\n\t\tharmonics = tf_spherical_harmonics_6(dxyzs, inv_dist_tensor)\n\telif max_l == 5:\n\t\tharmonics = tf_spherical_harmonics_5(dxyzs, inv_dist_tensor)\n\telif max_l == 4:\n\t\tharmonics = tf_spherical_harmonics_4(dxyzs, inv_dist_tensor)\n\telif max_l == 3:\n\t\tharmonics = tf_spherical_harmonics_3(dxyzs, inv_dist_tensor)\n\telif max_l == 2:\n\t\tharmonics = tf_spherical_harmonics_2(dxyzs, inv_dist_tensor)\n\telif max_l == 1:\n\t\tharmonics = tf_spherical_harmonics_1(dxyzs, inv_dist_tensor)\n\telif max_l == 0:\n\t\tharmonics = tf_spherical_harmonics_0(inv_dist_tensor)\n\telse:\n\t\traise Exception(""Spherical Harmonics only implemented up to l=8. Choose a lower order"")\n\treturn harmonics\n\ndef tf_gaussian_spherical_harmonics_element(xyzs, Zs, element, gauss_params, atomic_embed_factors, l_max, labels=None):\n\t""""""\n\tEncodes atoms into a gaussians and spherical harmonics embedding\n\n\tArgs:\n\t\txyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor\n\t\tZs (tf.int32): NMol x MaxNAtoms atomic number tensor\n\t\telement (int): element to return embedding/labels for\n\t\tgauss_params (tf.float): NGaussians x 2 tensor of gaussian parameters\n\t\tatomic_embed_factors (tf.float): MaxElementNumber tensor of scaling factors for elements\n\t\tl_max (tf.int32): Scalar for the highest order spherical harmonics to use (needs implemented)\n\t\tlabels (tf.Tensor): NMol x MaxNAtoms x label shape tensor of learning targets\n\n\tReturns:\n\t\tembedding (tf.float): atom embeddings for element\n\t\tlabels (tf.float): atom labels for element\n\t""""""\n\tdxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\n\telement_indices = tf.cast(tf.where(tf.equal(Zs, element)), tf.int32)\n\tnum_batch_elements = tf.shape(element_indices)[0]\n\telement_dxyzs = tf.gather_nd(dxyzs, element_indices)\n\telement_Zs = tf.gather(Zs, element_indices[:,0])\n\tdist_tensor = tf.norm(element_dxyzs+1.e-16,axis=2)\n\tatom_scaled_gauss, min_eigenval = tf_gauss(tf.expand_dims(dist_tensor, axis=-1), element_Zs, gauss_params, atomic_embed_factors)\n\tspherical_harmonics = tf_spherical_harmonics(element_dxyzs, dist_tensor, l_max)\n\telement_embedding = tf.reshape(tf.einsum(\'jkg,jkl->jgl\', atom_scaled_gauss, spherical_harmonics),\n\t\t\t\t\t\t\t[num_batch_elements, tf.shape(gauss_params)[0] * (l_max + 1) ** 2])\n\tif labels != None:\n\t\telement_labels = tf.gather_nd(labels, element_indices)\n\t\treturn element_embedding, element_labels, element_indices, min_eigenval\n\telse:\n\t\treturn element_embedding, element_indices, min_eigenval\n\ndef tf_gaussian_spherical_harmonics(xyzs, Zs, elements, gauss_params, atomic_embed_factors, l_max):\n\t""""""\n\tEncodes atoms into a gaussians and spherical harmonics embedding\n\n\tArgs:\n\t\txyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor\n\t\tZs (tf.int32): NMol x MaxNAtoms atomic number tensor\n\t\telement (int): element to return embedding/labels for\n\t\tgauss_params (tf.float): NGaussians x 2 tensor of gaussian parameters\n\t\tatomic_embed_factors (tf.float): MaxElementNumber tensor of scaling factors for elements\n\t\tl_max (tf.int32): Scalar for the highest order spherical harmonics to use (needs implemented)\n\t\tlabels (tf.Tensor): NMol x MaxNAtoms x label shape tensor of learning targets\n\n\tReturns:\n\t\tembedding (tf.float): atom embeddings for element\n\t\tlabels (tf.float): atom labels for element\n\t""""""\n\tnum_elements = elements.get_shape().as_list()[0]\n\tdxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\n\telement_indices = tf.cast(tf.where(tf.equal(tf.expand_dims(Zs, axis=-1), tf.reshape(elements, [1, 1, tf.shape(elements)[0]]))), tf.int32)\n\tdist_tensor = tf.norm(dxyzs+1.e-16,axis=3)\n\tatom_scaled_gauss = tf_gauss(dist_tensor, Zs, gauss_params, atomic_embed_factors, orthogonalize)\n\tspherical_harmonics = tf_spherical_harmonics(dxyzs, dist_tensor, l_max)\n\tembeddings = tf.reshape(tf.einsum(\'ijkg,ijkl->ijgl\', atom_scaled_gauss, spherical_harmonics),\n\t\t\t\t\t\t\t[tf.shape(Zs)[0], tf.shape(Zs)[1], tf.shape(gauss_params)[0] * (l_max + 1) ** 2])\n\tembeddings = tf.gather_nd(embeddings, element_indices[:,0:2])\n\telement_embeddings = tf.dynamic_partition(embeddings, element_indices[:,2], num_elements)\n\tmolecule_indices = tf.dynamic_partition(element_indices[:,0:2], element_indices[:,2], num_elements)\n\treturn element_embeddings, molecule_indices\n\ndef tf_gauss_harmonics_echannel(xyzs, Zs, elements, gauss_params, l_max):\n\t""""""\n\tEncodes atoms into a gaussians * spherical harmonics embedding\n\tWorks on a batch of molecules. This is the embedding routine used\n\tin BehlerParinelloDirectGauSH.\n\n\tArgs:\n\t\txyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor\n\t\tZs (tf.int32): NMol x MaxNAtoms atomic number tensor\n\t\telement (int): element to return embedding/labels for\n\t\tgauss_params (tf.float): NGaussians x 2 tensor of gaussian parameters\n\t\tl_max (tf.int32): Scalar for the highest order spherical harmonics to use (needs implemented)\n\t\tlabels (tf.Tensor): NMol x MaxNAtoms x label shape tensor of learning targets\n\n\tReturns:\n\t\tembedding (tf.float): atom embeddings for element\n\t\tmolecule_indices (tf.float): mapping between atoms and molecules.\n\t""""""\n\tnum_elements = elements.get_shape().as_list()[0]\n\tnum_mols = Zs.get_shape().as_list()[0]\n\tdxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\n\tatom_idx = tf.where(tf.not_equal(Zs, 0))\n\tdxyzs = tf.gather_nd(dxyzs, atom_idx)\n\tdist_tensor = tf.norm(dxyzs+1.e-16,axis=2)\n\tgauss = tf_gauss(dist_tensor, gauss_params)\n\tharmonics = tf_spherical_harmonics(dxyzs, dist_tensor, l_max)\n\tchannel_scatter_bool = tf.gather(tf.equal(tf.expand_dims(Zs, axis=1),\n\t\t\t\t\t\ttf.reshape(elements, [1, num_elements, 1])), atom_idx[:,0])\n\tchannel_scatter = tf.where(channel_scatter_bool, tf.ones_like(channel_scatter_bool, dtype=eval(PARAMS[""tf_prec""])),\n\t\t\t\t\ttf.zeros_like(channel_scatter_bool, dtype=eval(PARAMS[""tf_prec""])))\n\tchannel_gauss = tf.expand_dims(gauss, axis=1) * tf.expand_dims(channel_scatter, axis=-1)\n\tchannel_harmonics = tf.expand_dims(harmonics, axis=1) * tf.expand_dims(channel_scatter, axis=-1)\n\tembeds = tf.reshape(tf.einsum(\'ijkg,ijkl->ijgl\', channel_gauss, channel_harmonics),\n\t[tf.shape(atom_idx)[0], -1])\n\n\tpartition_idx = tf.cast(tf.where(tf.equal(tf.expand_dims(tf.gather_nd(Zs, atom_idx), axis=-1),\n\t\t\t\t\t\ttf.expand_dims(elements, axis=0)))[:,1], tf.int32)\n\tembeds = tf.dynamic_partition(embeds, partition_idx, num_elements)\n\tmol_idx = tf.dynamic_partition(atom_idx, partition_idx, num_elements)\n\treturn embeds, mol_idx\n\ndef tf_sparse_gauss_harmonics_echannel(xyzs, Zs, num_atoms, elements, gauss_params, l_max, r_cutoff):\n\t""""""\n\tEncodes atoms into a gaussians * spherical harmonics embedding\n\tWorks on a batch of molecules. This is the embedding routine used\n\tin BehlerParinelloDirectGauSH.\n\n\tArgs:\n\t\txyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor\n\t\tZs (tf.int32): NMol x MaxNAtoms atomic number tensor\n\t\telement (int): element to return embedding/labels for\n\t\tgauss_params (tf.float): NGaussians x 2 tensor of gaussian parameters\n\t\tl_max (tf.int32): Scalar for the highest order spherical harmonics to use\n\t\tlabels (tf.Tensor): NMol x MaxNAtoms x label shape tensor of learning targets\n\n\tReturns:\n\t\tembedding (tf.float): atom embeddings for element\n\t\tmolecule_indices (tf.float): mapping between atoms and molecules.\n\t""""""\n\tnum_elements = elements.get_shape().as_list()[0]\n\tnum_mols = Zs.get_shape().as_list()[0]\n\tmax_atoms = tf.reduce_max(num_atoms)\n\tpair_idx = tf_neighbor_list_sort(xyzs, Zs, num_atoms, elements, r_cutoff)\n\tk_max = tf.reduce_max(pair_idx[:,3])\n\n\tpair_dxyz = tf.gather_nd(xyzs, pair_idx[:,:2]) - tf.gather_nd(xyzs, pair_idx[:,::4])\n\tpair_dist = tf.norm(pair_dxyz+1.e-16, axis=1)\n\tgauss = tf.reshape(tf_sparse_gauss(pair_dist, gauss_params),\n\t\t\t[tf.shape(pair_dist)[0], tf.shape(gauss_params)[0]])\n\tharmonics = tf.reshape(tf_spherical_harmonics(pair_dxyz, pair_dist, l_max),\n\t\t\t\t[tf.shape(pair_dist)[0], tf.square(l_max + 1)])\n\tpair_embed = tf.reshape(tf.einsum(\'ij,ik->ijk\', gauss, harmonics),\n\t\t\t\t[-1, tf.shape(gauss_params)[0] * tf.square(l_max + 1)])\n\tscatter_embed = tf.reshape(tf.reduce_sum(tf.scatter_nd(pair_idx[:,:-1], pair_embed, [num_mols, max_atoms,\n\t\t\t\t\tnum_elements, k_max, tf.shape(gauss_params)[0] * tf.square(l_max + 1)]), axis=3),\n\t\t\t\t\t[num_mols, max_atoms, -1])\n\tatom_idxs = tf.where(tf.not_equal(Zs, 0))\n\tembeds = tf.gather_nd(scatter_embed, atom_idxs)\n\telement_part_idx = tf.cast(tf.where(tf.equal(tf.expand_dims(tf.gather_nd(Zs, atom_idxs), axis=1),\n\t\t\t\t\t\ttf.expand_dims(elements, axis=0)))[:,1], tf.int32)\n\tembeds = tf.dynamic_partition(embeds, element_part_idx, num_elements)\n\tmol_idx = tf.dynamic_partition(atom_idxs, element_part_idx, num_elements)\n\treturn embeds, mol_idx\n\ndef tf_random_rotate(xyzs, rot_params, labels = None, return_matrix = False):\n\t""""""\n\tRotates molecules and optionally labels in a uniformly random fashion\n\n\tArgs:\n\t\txyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor\n\t\tlabels (tf.float, optional): NMol x MaxNAtoms x label shape tensor of learning targets\n\t\treturn_matrix (bool): Returns rotation tensor if True\n\n\tReturns:\n\t\tnew_xyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor of randomly rotated molecules\n\t\tnew_labels (tf.float): NMol x MaxNAtoms x label shape tensor of randomly rotated learning targets\n\t""""""\n\tr = tf.sqrt(rot_params[...,2])\n\tv = tf.stack([tf.sin(rot_params[...,1]) * r, tf.cos(rot_params[...,1]) * r, tf.sqrt(2.0 - rot_params[...,2])], axis=-1)\n\tzero_tensor = tf.zeros_like(rot_params[...,1])\n\n\tR1 = tf.stack([tf.cos(rot_params[...,0]), tf.sin(rot_params[...,0]), zero_tensor], axis=-1)\n\tR2 = tf.stack([-tf.sin(rot_params[...,0]), tf.cos(rot_params[...,0]), zero_tensor], axis=-1)\n\tR3 = tf.stack([zero_tensor, zero_tensor, tf.ones_like(rot_params[...,1])], axis=-1)\n\tR = tf.stack([R1, R2, R3], axis=-2)\n\tM = tf.matmul((tf.expand_dims(v, axis=-2) * tf.expand_dims(v, axis=-1)) - tf.eye(3, dtype=eval(PARAMS[""tf_prec""])), R)\n\tnew_xyzs = tf.einsum(""lij,lkj->lki"", M, xyzs)\n\tif labels != None:\n\t\tnew_labels = tf.einsum(""lij,lkj->lki"",M, (xyzs + labels)) - new_xyzs\n\t\tif return_matrix:\n\t\t\treturn new_xyzs, new_labels, M\n\t\telse:\n\t\t\treturn new_xyzs, new_labels\n\telif return_matrix:\n\t\treturn new_xyzs, M\n\telse:\n\t\treturn new_xyzs\n\ndef tf_symmetry_functions(xyzs, Zs, elements, element_pairs, radial_cutoff, angular_cutoff, radial_rs, angular_rs, theta_s, zeta, eta):\n\t""""""\n\tEncodes atoms into the symmetry function embedding as implemented in the ANI-1 Neural Network (doi: 10.1039/C6SC05720A)\n\n\tArgs:\n\t\txyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor\n\t\tZs (tf.int32): NMol x MaxNAtoms atomic number tensor\n\t\tnum_atoms (tf.int32): NMol number of atoms numpy array\n\t\telements (tf.int32): NElements tensor containing sorted unique atomic numbers present\n\t\telement_pairs (tf.int32): NElementPairs x 2 tensor containing sorted unique pairs of atomic numbers present\n\t\tradial_cutoff (tf.float): scalar tensor with the cutoff for radial pairs\n\t\tangular_cutoff (tf.float): scalar tensor with the cutoff for the angular triples\n\t\tradial_rs (tf.float): NRadialGridPoints tensor with R_s values for the radial grid\n\t\tangular_rs (tf.float): NAngularGridPoints tensor with the R_s values for the radial part of the angular grid\n\t\ttheta_s (tf.float): NAngularGridPoints tensor with the theta_s values for the angular grid\n\t\tzeta (tf.float): scalar tensor with the zeta parameter for the symmetry functions\n\t\teta (tf.float): scalar tensor with the eta parameter for the symmetry functions\n\n\tReturns:\n\t\telement_embeddings (list of tf.floats): List of NAtoms x (NRadial_rs x NElements + NAngular_rs x NTheta_s x NElementPairs)\n\t\t\t\ttensors of the same element type\n\t\tmol_indices (list of tf.int32s): List of NAtoms of the same element types with the molecule index of each atom\n\t""""""\n\tnum_molecules = Zs.get_shape().as_list()[0]\n\tnum_elements = elements.get_shape().as_list()[0]\n\tnum_element_pairs = element_pairs.get_shape().as_list()[0]\n\n\tradial_embedding, pair_indices, pair_elements = tf_symmetry_functions_radial_grid(xyzs, Zs, radial_cutoff, radial_rs, eta)\n\tangular_embedding, triples_indices, triples_element, triples_element_pairs = tf_symmetry_function_angular_grid(xyzs, Zs, angular_cutoff, angular_rs, theta_s, zeta, eta)\n\n\tpair_element_indices = tf.cast(tf.where(tf.equal(tf.expand_dims(pair_elements[:,1], axis=-1),\n\t\t\t\t\t\t\ttf.expand_dims(elements, axis=0))), tf.int32)[:,1]\n\ttriples_elements_indices = tf.cast(tf.where(tf.reduce_all(tf.equal(tf.expand_dims(triples_element_pairs, axis=-2),\n\t\t\t\t\t\t\t\t\telement_pairs), axis=-1)), tf.int32)[:,1]\n\tradial_scatter_indices = tf.concat([pair_indices, tf.expand_dims(pair_element_indices, axis=1)], axis=1)\n\tangular_scatter_indices = tf.concat([triples_indices, tf.expand_dims(triples_elements_indices, axis=1)], axis=1)\n\n\tradial_molecule_embeddings = tf.dynamic_partition(radial_embedding, pair_indices[:,0], num_molecules)\n\tradial_atom_indices = tf.dynamic_partition(radial_scatter_indices[:,1:], pair_indices[:,0], num_molecules)\n\tangular_molecule_embeddings = tf.dynamic_partition(angular_embedding, triples_indices[:,0], num_molecules)\n\tangular_atom_indices = tf.dynamic_partition(angular_scatter_indices[:,1:], triples_indices[:,0], num_molecules)\n\n\tembeddings = []\n\tmol_atom_indices = []\n\tfor molecule in range(num_molecules):\n\t\tatom_indices = tf.cast(tf.where(tf.not_equal(Zs[molecule], 0)), tf.int32)\n\t\tmolecule_atom_elements = tf.gather_nd(Zs[molecule], atom_indices)\n\t\tnum_atoms = tf.shape(molecule_atom_elements)[0]\n\t\tradial_atom_embeddings = tf.reshape(tf.reduce_sum(tf.scatter_nd(radial_atom_indices[molecule], radial_molecule_embeddings[molecule],\n\t\t\t\t\t\t\t\t[num_atoms, num_atoms, num_elements, tf.shape(radial_rs)[0]]), axis=1), [num_atoms, -1])\n\t\tangular_atom_embeddings = tf.reshape(tf.reduce_sum(tf.scatter_nd(angular_atom_indices[molecule], angular_molecule_embeddings[molecule],\n\t\t\t\t\t\t\t\t\t[num_atoms, num_atoms, num_atoms, num_element_pairs, tf.shape(angular_rs)[0] * tf.shape(theta_s)[0]]),\n\t\t\t\t\t\t\t\t\taxis=[1,2]), [num_atoms, -1])\n\t\tembeddings.append(tf.concat([radial_atom_embeddings, angular_atom_embeddings], axis=1))\n\t\tmol_atom_indices.append(tf.concat([tf.fill([num_atoms, 1], molecule), atom_indices], axis=1))\n\n\tembeddings = tf.concat(embeddings, axis=0)\n\tmol_atom_indices = tf.concat(mol_atom_indices, axis=0)\n\tatom_Zs = tf.gather_nd(Zs, tf.where(tf.not_equal(Zs, 0)))\n\tatom_Z_indices = tf.cast(tf.where(tf.equal(tf.expand_dims(atom_Zs, axis=1), tf.expand_dims(elements, axis=0)))[:,1], tf.int32)\n\n\telement_embeddings = tf.dynamic_partition(embeddings, atom_Z_indices, num_elements)\n\tmol_indices = tf.dynamic_partition(mol_atom_indices, atom_Z_indices, num_elements)\n\treturn element_embeddings, mol_indices\n\ndef tf_symmetry_functions_radial_grid(xyzs, Zs, radial_cutoff, radial_rs, eta, prec=tf.float64):\n\t""""""\n\tEncodes the radial grid portion of the symmetry functions. Should be called by tf_symmetry_functions_2()\n\n\tArgs:\n\t\txyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor\n\t\tZs (tf.int32): NMol x MaxNAtoms atomic number tensor\n\t\tnum_atoms (np.int32): NMol number of atoms numpy array\n\t\tradial_cutoff (tf.float): scalar tensor with the cutoff for radial pairs\n\t\tradial_rs (tf.float): NRadialGridPoints tensor with R_s values for the radial grid\n\t\teta (tf.float): scalar tensor with the eta parameter for the symmetry functions\n\n\tReturns:\n\t\tradial_embedding (tf.float): tensor of radial embeddings for all atom pairs within the radial_cutoff\n\t\tpair_indices (tf.int32): tensor of the molecule, atom, and pair atom indices\n\t\tpair_elements (tf.int32): tensor of the atomic numbers for the atom and its pair atom\n\t""""""\n\tdxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\n\tdist_tensor = tf.norm(dxyzs + 1.e-16,axis=3)\n\tpadding_mask = tf.not_equal(Zs, 0)\n\tpair_indices = tf.where(tf.logical_and(tf.logical_and(tf.less(dist_tensor, radial_cutoff),\n\t\t\t\t\ttf.expand_dims(padding_mask, axis=1)), tf.expand_dims(padding_mask, axis=-1)))\n\tidentity_mask = tf.where(tf.not_equal(pair_indices[:,1], pair_indices[:,2]))\n\tpair_indices = tf.cast(tf.squeeze(tf.gather(pair_indices, identity_mask)), tf.int32)\n\tpair_distances = tf.gather_nd(dist_tensor, pair_indices)\n\tpair_elements = tf.stack([tf.gather_nd(Zs, pair_indices[:,0:2]), tf.gather_nd(Zs, pair_indices[:,0:3:2])], axis=-1)\n\tgaussian_factor = tf.exp(-eta * tf.square(tf.expand_dims(pair_distances, axis=-1) - tf.expand_dims(radial_rs, axis=0)))\n\tcutoff_factor = tf.expand_dims(0.5 * (tf.cos(3.14159265359 * pair_distances / radial_cutoff) + 1.0), axis=-1)\n\tradial_embedding = gaussian_factor * cutoff_factor\n\treturn radial_embedding, pair_indices, pair_elements\n\ndef tf_symmetry_function_angular_grid(xyzs, Zs, angular_cutoff, angular_rs, theta_s, zeta, eta):\n\t""""""\n\tEncodes the radial grid portion of the symmetry functions. Should be called by tf_symmetry_functions_2()\n\n\tArgs:\n\t\txyzs (tf.float): NMol x MaxNAtoms x 3 coordinates tensor\n\t\tZs (tf.int32): NMol x MaxNAtoms atomic number tensor\n\t\tangular_cutoff (tf.float): scalar tensor with the cutoff for the angular triples\n\t\tangular_rs (tf.float): NAngularGridPoints tensor with the R_s values for the radial part of the angular grid\n\t\ttheta_s (tf.float): NAngularGridPoints tensor with the theta_s values for the angular grid\n\t\tzeta (tf.float): scalar tensor with the zeta parameter for the symmetry functions\n\t\teta (tf.float): scalar tensor with the eta parameter for the symmetry functions\n\n\tReturns:\n\t\tangular_embedding (tf.float): tensor of radial embeddings for all atom pairs within the radial_cutoff\n\t\ttriples_indices (tf.int32): tensor of the molecule, atom, and triples atom indices\n\t\ttriples_elements (tf.int32): tensor of the atomic numbers for the atom\n\t\tsorted_triples_element_pairs (tf.int32): sorted tensor of the atomic numbers of triples atoms\n\t""""""\n\tnum_mols = Zs.get_shape().as_list()[0]\n\tdxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\n\tdist_tensor = tf.norm(dxyzs + 1.e-16,axis=3)\n\tpadding_mask = tf.not_equal(Zs, 0)\n\tpair_indices = tf.cast(tf.where(tf.logical_and(tf.logical_and(tf.less(dist_tensor, angular_cutoff),\n\t\t\t\t\ttf.expand_dims(padding_mask, axis=1)), tf.expand_dims(padding_mask, axis=-1))), tf.int32)\n\tidentity_mask = tf.where(tf.not_equal(pair_indices[:,1], pair_indices[:,2]))\n\tpair_indices = tf.cast(tf.squeeze(tf.gather(pair_indices, identity_mask)), tf.int32)\n\tmol_pair_indices = tf.dynamic_partition(pair_indices, pair_indices[:,0], num_mols)\n\ttriples_indices = []\n\ttmp = []\n\tfor i in xrange(num_mols):\n\t\tmol_common_pair_indices = tf.where(tf.equal(tf.expand_dims(mol_pair_indices[i][:,1], axis=1),\n\t\t\t\t\t\t\t\t\ttf.expand_dims(mol_pair_indices[i][:,1], axis=0)))\n\t\tmol_triples_indices = tf.concat([tf.gather(mol_pair_indices[i], mol_common_pair_indices[:,0]),\n\t\t\t\t\t\t\t\ttf.gather(mol_pair_indices[i], mol_common_pair_indices[:,1])[:,-1:]], axis=1)\n\t\tpermutation_identity_pairs_mask = tf.where(tf.less(mol_triples_indices[:,2], mol_triples_indices[:,3]))\n\t\tmol_triples_indices = tf.squeeze(tf.gather(mol_triples_indices, permutation_identity_pairs_mask))\n\t\ttriples_indices.append(mol_triples_indices)\n\ttriples_indices = tf.concat(triples_indices, axis=0)\n\n\ttriples_elements = tf.gather_nd(Zs, triples_indices[:,0:2])\n\ttriples_element_pairs, _ = tf.nn.top_k(tf.stack([tf.gather_nd(Zs, triples_indices[:,0:3:2]),\n\t\t\t\t\t\t\ttf.gather_nd(Zs, triples_indices[:,0:4:3])], axis=-1), k=2)\n\tsorted_triples_element_pairs = tf.reverse(triples_element_pairs, axis=[-1])\n\n\ttriples_distances = tf.stack([tf.gather_nd(dist_tensor, triples_indices[:,:3]), tf.gather_nd(dist_tensor,\n\t\t\t\t\t\ttf.concat([triples_indices[:,:2], triples_indices[:,3:]], axis=1))], axis=1)\n\tr_ijk_s = tf.square(tf.expand_dims(tf.reduce_sum(triples_distances, axis=1) / 2.0, axis=-1) - tf.expand_dims(angular_rs, axis=0))\n\texponential_factor = tf.exp(-eta * r_ijk_s)\n\n\txyz_ij_ik = tf.reduce_sum(tf.gather_nd(dxyzs, triples_indices[:,:3]) * tf.gather_nd(dxyzs,\n\t\t\t\t\t\ttf.concat([triples_indices[:,:2], triples_indices[:,3:]], axis=1)), axis=1)\n\tcos_theta = xyz_ij_ik / (triples_distances[:,0] * triples_distances[:,1])\n\tcos_theta = tf.where(tf.greater_equal(cos_theta, 1.0), tf.ones_like(cos_theta) - 1.0e-16, cos_theta)\n\tcos_theta = tf.where(tf.less_equal(cos_theta, -1.0), -1.0 * tf.ones_like(cos_theta) - 1.0e-16, cos_theta)\n\ttriples_angle = tf.acos(cos_theta)\n\ttheta_ijk_s = tf.expand_dims(triples_angle, axis=-1) - tf.expand_dims(theta_s, axis=0)\n\tcos_factor = tf.pow((1 + tf.cos(theta_ijk_s)), zeta)\n\n\tcutoff_factor = 0.5 * (tf.cos(3.14159265359 * triples_distances / angular_cutoff) + 1.0)\n\tscalar_factor = tf.pow(tf.cast(2.0, eval(PARAMS[""tf_prec""])), 1.0-zeta)\n\n\tangular_embedding = tf.reshape(scalar_factor * tf.expand_dims(cos_factor * tf.expand_dims(cutoff_factor[:,0] * cutoff_factor[:,1], axis=-1), axis=-1) \\\n\t\t\t\t\t\t* tf.expand_dims(exponential_factor, axis=-2), [tf.shape(triples_indices)[0], tf.shape(theta_s)[0] * tf.shape(angular_rs)[0]])\n\treturn angular_embedding, triples_indices, triples_elements, sorted_triples_element_pairs\n\ndef tf_coulomb_dsf_elu(xyzs, charges, Radpair, elu_width, dsf_alpha, cutoff_dist):\n\t""""""\n\tA tensorflow linear scaling implementation of the Damped Shifted Electrostatic Force with short range cutoff with elu function (const at short range).\n\thttp://aip.scitation.org.proxy.library.nd.edu/doi/pdf/10.1063/1.2206581\n\tBatched over molecules.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tQs : nmol X maxnatom X 1 tensor of atomic charges.\n\t\tR_srcut: Short Range Erf Cutoff\n\t\tR_lrcut: Long Range DSF Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\talpha: DSF alpha parameter (~0.2)\n\tReturns\n\t\tEnergy of  Mols\n\t""""""\n\txyzs *= BOHRPERA\n\telu_width *= BOHRPERA\n\tdsf_alpha /= BOHRPERA\n\tcutoff_dist *= BOHRPERA\n\tinp_shp = tf.shape(xyzs)\n\tnum_mol = tf.cast(tf.shape(xyzs)[0], dtype=tf.int64)\n\tnum_pairs = tf.cast(tf.shape(Radpair)[0], tf.int64)\n\telu_shift, elu_alpha = tf_dsf_potential(elu_width, cutoff_dist, dsf_alpha, return_grad=True)\n\n\tRij = DifferenceVectorsLinear(xyzs + 1e-16, Radpair)\n\tRijRij2 = tf.norm(Rij, axis=1)\n\tqij = tf.gather_nd(charges, Radpair[:,:2]) * tf.gather_nd(charges, Radpair[:,::2])\n\n\tcoulomb_potential = qij * tf.where(tf.greater(RijRij2, elu_width), tf_dsf_potential(RijRij2, cutoff_dist, dsf_alpha),\n\t\t\t\t\t\telu_alpha * (tf.exp(RijRij2 - elu_width) - 1.0) + elu_shift)\n\n\trange_index = tf.range(num_pairs, dtype=tf.int64)\n\tmol_index = tf.cast(Radpair[:,0], dtype=tf.int64)\n\tsparse_index = tf.cast(tf.stack([mol_index, range_index], axis=1), tf.int64)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, coulomb_potential, [num_mol, num_pairs])\n\treturn tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\ndef tf_dsf_potential(dists, cutoff_dist, dsf_alpha, return_grad=False):\n\tdsf_potential = tf.erfc(dsf_alpha * dists) / dists - tf.erfc(dsf_alpha * cutoff_dist) / cutoff_dist \\\n\t\t\t\t\t+ (dists - cutoff_dist) * (tf.erfc(dsf_alpha * cutoff_dist) / tf.square(cutoff_dist) \\\n\t\t\t\t\t+ 2.0 * dsf_alpha * tf.exp(-tf.square(dsf_alpha * cutoff_dist)) / (tf.sqrt(np.pi) * cutoff_dist))\n\tdsf_potential = tf.where(tf.greater(dists, cutoff_dist), tf.zeros_like(dsf_potential), dsf_potential)\n\tif return_grad:\n\t\tdsf_gradient = -(tf.erfc(dsf_alpha * dists) / tf.square(dists) - tf.erfc(dsf_alpha * cutoff_dist) / tf.square(cutoff_dist) \\\n\t \t\t\t\t\t+ 2.0 * dsf_alpha / tf.sqrt(np.pi) * (tf.exp(-tf.square(dsf_alpha * dists)) / dists \\\n\t\t\t\t\t\t- tf.exp(-tf.square(dsf_alpha * cutoff_dist)) / tf.sqrt(np.pi)))\n\t\treturn dsf_potential, dsf_gradient\n\telse:\n\t\treturn dsf_potential\n\ndef tf_neighbor_list(xyzs, Zs, cutoff):\n\twith tf.device(""/cpu:0""):\n\t\tnum_mols = Zs.get_shape().as_list()[0]\n\t\tpadding_mask = tf.cast(tf.where(tf.not_equal(Zs, 0)), tf.int32)\n\t\tmol_coords = tf.dynamic_partition(tf.gather_nd(xyzs, padding_mask), padding_mask[:,0], num_mols)\n\t\tpair_idx = []\n\t\tpair_dist = []\n\t\tpair_dxyz = []\n\t\tfor i, mol in enumerate(mol_coords):\n\t\t\tdx = tf.abs(tf.expand_dims(mol[...,0], axis=-1) - tf.expand_dims(mol[...,0], axis=-2))\n\t\t\tmol_pair_idx = tf.where(tf.less(dx, cutoff))\n\t\t\tidentity_mask = tf.where(tf.not_equal(mol_pair_idx[:,0], mol_pair_idx[:,1]))\n\t\t\tmol_pair_idx = tf.gather_nd(mol_pair_idx, identity_mask)\n\t\t\tmol_pair_dxyz = tf.gather(mol, mol_pair_idx[:,0]) - tf.gather(mol, mol_pair_idx[:,1])\n\t\t\tmol_pair_dist = tf.norm(mol_pair_dxyz + 1.e-16, axis=1)\n\t\t\tcutoff_pairs = tf.where(tf.less(mol_pair_dist, cutoff))\n\t\t\tmol_pair_idx = tf.gather_nd(mol_pair_idx, cutoff_pairs)\n\t\t\tmol_pair_dist = tf.gather_nd(mol_pair_dist, cutoff_pairs)\n\t\t\tmol_pair_dxyz = tf.gather_nd(mol_pair_dxyz, cutoff_pairs)\n\t\t\tpair_idx.append(tf.concat([tf.fill([tf.shape(mol_pair_idx)[0], 1], i), tf.cast(mol_pair_idx, tf.int32)], axis=-1))\n\t\t\tpair_dist.append(mol_pair_dist)\n\t\t\tpair_dxyz.append(mol_pair_dxyz)\n\t\treturn tf.concat(pair_idx, axis=0), tf.concat(pair_dist, axis=0), tf.concat(pair_dxyz, axis=0)\n\ndef tf_neighbor_list_sort(xyzs, Zs, num_atoms, elements, cutoff):\n\twith tf.device(""/cpu:0""):\n\t\tnum_mols = Zs.get_shape().as_list()[0]\n\t\tpadding_mask = tf.cast(tf.where(tf.not_equal(Zs, 0)), tf.int32)\n\t\tmol_coords = tf.dynamic_partition(tf.gather_nd(xyzs, padding_mask), padding_mask[:,0], num_mols)\n\t\tpair_idxs = []\n\t\tfor i, mol in enumerate(mol_coords):\n\t\t\tdx = tf.abs(tf.expand_dims(mol[...,0], axis=-1) - tf.expand_dims(mol[...,0], axis=-2))\n\t\t\tmol_pair_idx = tf.where(tf.less(dx, cutoff))\n\t\t\tidentity_mask = tf.where(tf.not_equal(mol_pair_idx[:,0], mol_pair_idx[:,1]))\n\t\t\tmol_pair_idx = tf.gather_nd(mol_pair_idx, identity_mask)\n\t\t\tmol_pair_dxyz = tf.gather(mol, mol_pair_idx[:,0]) - tf.gather(mol, mol_pair_idx[:,1])\n\t\t\tmol_pair_dist = tf.norm(mol_pair_dxyz + 1.e-16, axis=1)\n\t\t\tcutoff_pairs = tf.where(tf.less(mol_pair_dist, cutoff))\n\t\t\tmol_pair_idx = tf.cast(tf.gather_nd(mol_pair_idx, cutoff_pairs), tf.int32)\n\t\t\tidx_arr = tf.TensorArray(tf.int32, size=num_atoms[i], infer_shape=False)\n\n\t\t\tdef cond(j, idx_arr):\n\t \t\t\treturn tf.less(j, num_atoms[i])\n\n\t\t\tdef body(j, idx_arr):\n\t\t\t\tatom_pairs = tf.gather_nd(mol_pair_idx[:,1], tf.where(tf.equal(mol_pair_idx[:,0], j)))\n\t\t\t\tpair_element = tf.gather(Zs[i], atom_pairs)\n\t\t\t\tpair_element_idx = tf.cast(tf.where(tf.equal(tf.expand_dims(pair_element, axis=-1),\n\t\t\t\t\t\t\t\ttf.expand_dims(elements, axis=0)))[:,1], tf.int32)\n\t\t\t\tsort_element, sort_idx = tf.nn.top_k(pair_element_idx, k=tf.shape(pair_element_idx)[0])\n\t\t\t\tsort_pair = tf.gather(atom_pairs, sort_idx)\n\t\t\t\tunique, _, count = tf.unique_with_counts(sort_element)\n\t\t\t\tk_idx_arr = tf.TensorArray(tf.int32, size=tf.shape(unique)[0], infer_shape=False)\n\n\t\t\t\tdef inner_cond(k, inarr):\n\t\t\t\t\treturn tf.less(k, tf.shape(unique)[0])\n\n\t\t\t\tdef inner_body(k, inarr):\n\t\t\t\t\tk_idx = tf.range(count[k])\n\t\t\t\t\treturn (tf.add(k, 1), inarr.write(k, k_idx))\n\n\t\t\t\t_, k_idx_arr = tf.while_loop(inner_cond, inner_body, (tf.constant(0), k_idx_arr))\n\t\t\t\tatom_idx = tf.concat([tf.fill([tf.shape(atom_pairs)[0], 1], i), tf.fill([tf.shape(atom_pairs)[0], 1], j),\n\t\t\t\t\ttf.expand_dims(sort_element, axis=1), tf.expand_dims(k_idx_arr.concat(), axis=1),\n\t\t\t\t\ttf.expand_dims(sort_pair, axis=1)], axis=1)\n\t\t\t\treturn (tf.add(j, 1), idx_arr.write(j, atom_idx))#, dist_arr.write(j, sort_dist), dxyz_arr.write(j, sort_dxyz))\n\n\t\t\t_, idx_arr = tf.while_loop(cond, body, (tf.constant(0), idx_arr)) #, dist_arr, dxyz_arr))\n\t\t\tpair_idxs.append(idx_arr.concat())\n\n\t\tpair_idxs = tf.concat(pair_idxs, axis=0)\n\t\treturn pair_idxs\n'"
TensorMol/TFDescriptors/RawSymFunc.py,1096,"b'""""""\nRaw => various descriptors in Tensorflow code.\n\nThe Raw format is a batch of rank three tensors.\nmol X MaxNAtoms X 4\nThe final dim is atomic number, x,y,z (Angstrom)\n\nhttps://www.youtube.com/watch?v=h2zgB93KANE\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom ..ForceModifiers.Neighbors import *\nfrom ..Containers.TensorData import *\nfrom ..ForceModels.ElectrostaticsTF import * # Why is this imported here?\nfrom tensorflow.python.client import timeline\nimport numpy as np\nimport time\nfrom tensorflow.python.framework import function\nif (HAS_TF):\n\timport tensorflow as tf\n\ndef AllTriples(rng):\n\t""""""Returns all possible triples of an input list.\n\n\tArgs:\n\t\trng: a 1D integer tensor to be triply outer product\'d\n\tReturns:\n\t\tA natom X natom X natom X 3 tensor of all triples of entries from rng.\n\t""""""\n\trshp = tf.shape(rng)\n\tnatom = rshp[0]\n\tv1 = tf.tile(tf.reshape(rng,[natom,1]),[1,natom])\n\tv2 = tf.tile(tf.reshape(rng,[1,natom]),[natom,1])\n\tv3 = tf.transpose(tf.stack([v1,v2],0),perm=[1,2,0])\n\t# V3 is now all pairs (nat x nat x 2). now do the same with another to make nat X 3\n\tv4 = tf.tile(tf.reshape(v3,[natom,natom,1,2]),[1,1,natom,1])\n\tv5 = tf.tile(tf.reshape(rng,[1,1,natom,1]),[natom,natom,1,1])\n\tv6 = tf.concat([v4,v5], axis = 3) # All triples in the range.\n\treturn v6\n\ndef AllTriplesSet(rng, prec=tf.int32):\n\t""""""Returns all possible triples of integers between zero and natom.\n\n\tArgs:\n\t\trng: a 1D integer tensor to be triply outer product\'d\n\tReturns:\n\t\tA Nmol X natom X natom X natom X 4 tensor of all triples.\n\t""""""\n\tnatom = tf.shape(rng)[1]\n\tnmol = tf.shape(rng)[0]\n\tv1 = tf.tile(tf.reshape(rng,[nmol,natom,1]),[1,1,natom])\n\tv2 = tf.tile(tf.reshape(rng,[nmol,1,natom]),[1,natom,1])\n\tv3 = tf.transpose(tf.stack([v1,v2],1),perm=[0,2,3,1])\n\t# V3 is now all pairs (nat x nat x 2). now do the same with another to make nat X 3\n\tv4 = tf.tile(tf.reshape(v3,[nmol,natom,natom,1,2]),[1,1,1,natom,1])\n\tv5 = tf.tile(tf.reshape(rng,[nmol,1,1,natom,1]),[1,natom,natom,1,1])\n\tv6 = tf.concat([v4,v5], axis = 4) # All triples in the range.\n\tv7 = tf.cast(tf.tile(tf.reshape(tf.range(nmol),[nmol,1,1,1,1]),[1,natom,natom,natom,1]), dtype=prec)\n\tv8 = tf.concat([v7,v6], axis = -1)\n\treturn v8\n\ndef AllDoublesSet(rng, prec=tf.int32):\n\t""""""Returns all possible doubles of integers between zero and natom.\n\n\tArgs:\n\t\tnatom: max integer\n\tReturns:\n\t\tA nmol X natom X natom X 3 tensor of all doubles.\n\t""""""\n\tnatom = tf.shape(rng)[1]\n\tnmol = tf.shape(rng)[0]\n\tv1 = tf.tile(tf.reshape(rng,[nmol,natom,1]),[1,1,natom])\n\tv2 = tf.tile(tf.reshape(rng,[nmol,1,natom]),[1,natom,1])\n\tv3 = tf.transpose(tf.stack([v1,v2],1),perm=[0,2,3,1])\n\tv4 = tf.cast(tf.tile(tf.reshape(tf.range(nmol),[nmol,1,1,1]),[1,natom,natom,1]),dtype=prec)\n\tv5 = tf.concat([v4,v3], axis = -1)\n\treturn v5\n\ndef AllSinglesSet(rng, prec=tf.int32):\n\t""""""Returns all possible triples of integers between zero and natom.\n\n\tArgs:\n\t\tnatom: max integer\n\tReturns:\n\t\tA nmol X natom X 2 tensor of all doubles.\n\t""""""\n\tnatom = tf.shape(rng)[1]\n\tnmol = tf.shape(rng)[0]\n\tv1 = tf.reshape(rng,[nmol,natom,1])\n\tv2 = tf.cast(tf.tile(tf.reshape(tf.range(nmol),[nmol,1,1]),[1,natom,1]), dtype=prec)\n\tv3 = tf.concat([v2,v1], axis = -1)\n\treturn v3\n\ndef ZouterSet(Z):\n\t""""""\n\tReturns the outer product of atomic numbers for all molecules.\n\n\tArgs:\n\t\tZ: nMol X MaxNAtom X 1 Z tensor\n\tReturns\n\t\tZ1Z2: nMol X MaxNAtom X MaxNAtom X 2 Z1Z2 tensor.\n\t""""""\n\tzshp = tf.shape(Z)\n\tZs = tf.reshape(Z,[zshp[0],zshp[1],1])\n\tz1 = tf.tile(Zs, [1,1,zshp[1]])\n\tz2 = tf.transpose(z1,perm=[0,2,1])\n\treturn tf.transpose(tf.stack([z1,z2],axis=1),perm=[0,2,3,1])\n\ndef DifferenceVectorsSet(r_,prec = tf.float64):\n\t""""""\n\tGiven a nmol X maxnatom X 3 tensor of coordinates this\n\treturns a nmol X maxnatom X maxnatom X 3 tensor of Rij\n\t""""""\n\tnatom = tf.shape(r_)[1]\n\tnmol = tf.shape(r_)[0]\n\t#ri = tf.tile(tf.reshape(r_,[nmol,1,natom,3]),[1,natom,1,1])\n\tri = tf.tile(tf.reshape(tf.cast(r_,prec),[nmol,1,natom*3]),[1,natom,1])\n\tri = tf.reshape(ri, [nmol, natom, natom, 3])\n\trj = tf.transpose(ri,perm=(0,2,1,3))\n\treturn (ri-rj)\n\ndef DifferenceVectorsLinear(B, NZP):\n\t""""""\n\tB: Nmol X NmaxNAtom X 3 coordinate tensor\n\tNZP: a index matrix (nzp X 3)\n\t""""""\n\tIi = tf.slice(NZP,[0,0],[-1,2])\n\tIj = tf.concat([tf.slice(NZP,[0,0],[-1,1]),tf.slice(NZP,[0,2],[-1,1])],1)\n\tRi = tf.gather_nd(B,Ii)\n\tRj = tf.gather_nd(B,Ij)\n\tA = Ri - Rj\n\treturn A\n\ndef TFSymASet(R, Zs, eleps_, SFPs_, R_cut, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teleps_: a nelepairs X 2 tensor of element pairs present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 4 X nzeta X neta X thetas X nRs. For example, SFPs_[0,0,0,0,0]\n\t\tis the first zeta parameter. SFPs_[3,0,0,0,1] is the second R parameter.\n\t\tR_cut: Radial Cutoff\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnatom3 = natom*natom2\n\tnelep = tf.shape(eleps_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnzeta = pshape[1]\n\tneta = pshape[2]\n\tntheta = pshape[3]\n\tnr = pshape[4]\n\tnsym = nzeta*neta*ntheta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tonescalar = 1.0 - 0.0000000000000001\n\n\t# atom triples.\n\tats = AllTriplesSet(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]))\n\t# before performing any computation reduce this to desired pairs.\n\t# Construct the angle triples acos(<Rij,Rik>/|Rij||Rik|) and mask them onto the correct output\n\t# Get Rij, Rik...\n\tRm_inds = tf.slice(ats,[0,0,0,0,0],[nmol,natom,natom,natom,1])\n\tRi_inds = tf.slice(ats,[0,0,0,0,1],[nmol,natom,natom,natom,1])\n\tRj_inds = tf.slice(ats,[0,0,0,0,2],[nmol,natom,natom,natom,1])\n\tRk_inds = tf.slice(ats,[0,0,0,0,3],[nmol,natom,natom,natom,1])\n\tRjk_inds = tf.reshape(tf.concat([Rm_inds,Rj_inds,Rk_inds],axis=4),[nmol,natom3,3])\n\tZ1Z2 = ZouterSet(Zs)\n\tZPairs = tf.gather_nd(Z1Z2,Rjk_inds) # should have shape nmol X natom3 X 2\n\tElemReduceMask = tf.reduce_all(tf.equal(tf.reshape(ZPairs,[nmol,natom3,1,2]),tf.reshape(eleps_,[1,1,nelep,2])),axis=-1) # nmol X natom3 X nelep\n\t# Zero out the diagonal contributions (i==j or i==k)\n\tIdentMask = tf.tile(tf.reshape(tf.logical_and(tf.not_equal(Ri_inds,Rj_inds),tf.not_equal(Ri_inds,Rk_inds)),[nmol,natom3,1]),[1,1,nelep])\n\tMask = tf.logical_and(ElemReduceMask,IdentMask) # nmol X natom3 X nelep\n\t# Mask is true if atoms ijk => pair_l and many triples are unused.\n\t# So we create a final index tensor, which is only nonzero m,ijk,l\n\tpinds = tf.range(nelep)\n\tats = tf.tile(tf.reshape(ats,[nmol,natom3,1,4]),[1,1,nelep,1])\n\tps = tf.tile(tf.reshape(pinds,[1,1,nelep,1]),[nmol,natom3,1,1])\n\tToMask = tf.concat([ats,ps],axis=3)\n\tGoodInds = tf.boolean_mask(ToMask,Mask)\n\tnnz = tf.shape(GoodInds)[0]\n\t# Good Inds has shape << nmol * natom3 * nelep X 5 (mol, i,j,k,l=element pair.)\n\t# and contains all the indices we actually want to compute, Now we just slice, gather and compute.\n\tmijs = tf.slice(GoodInds,[0,0],[nnz,3])\n\tmiks = tf.concat([tf.slice(GoodInds,[0,0],[nnz,2]),tf.slice(GoodInds,[0,3],[nnz,1])],axis=-1)\n\tRij = DifferenceVectorsSet(R,prec) # nmol X atom X atom X 3\n\tA = tf.gather_nd(Rij,mijs)\n\tB = tf.gather_nd(Rij,miks)\n\tRijRik = tf.reduce_sum(A*B,axis=1)\n\tRijRij = tf.sqrt(tf.reduce_sum(A*A,axis=1)+infinitesimal)\n\tRikRik = tf.sqrt(tf.reduce_sum(B*B,axis=1)+infinitesimal)\n\tdenom = RijRij*RikRik+infinitesimal\n\t# Mask any troublesome entries.\n\tToACos = RijRik/denom\n\tToACos = tf.where(tf.greater_equal(ToACos,1.0),tf.ones_like(ToACos, dtype=prec)*onescalar,ToACos)\n\tToACos = tf.where(tf.less_equal(ToACos,-1.0),-1.0*tf.ones_like(ToACos, dtype=prec)*onescalar,ToACos)\n\tThetaijk = tf.acos(ToACos)\n\tzetatmp = tf.cast(tf.reshape(SFPs_[0],[1,nzeta,neta,ntheta,nr]),prec)\n\tthetatmp = tf.cast(tf.tile(tf.reshape(SFPs_[2],[1,nzeta,neta,ntheta,nr]),[nnz,1,1,1,1]),prec)\n\t# Broadcast the thetas and ToCos together\n\ttct = tf.tile(tf.reshape(Thetaijk,[nnz,1,1,1,1]),[1,nzeta,neta,ntheta,nr])\n\tToCos = tct-thetatmp\n\tTijk = tf.cos(ToCos) # shape: natom3 X ...\n\t# complete factor 1\n\tfac1 = tf.pow(tf.cast(2.0, prec),1.0-zetatmp)*tf.pow((1.0+Tijk),zetatmp)\n\tetmp = tf.cast(tf.reshape(SFPs_[1],[1,nzeta,neta,ntheta,nr]),prec) # ijk X zeta X eta ....\n\trtmp = tf.cast(tf.reshape(SFPs_[3],[1,nzeta,neta,ntheta,nr]),prec) # ijk X zeta X eta ....\n\tToExp = ((RijRij+RikRik)/2.0)\n\ttet = tf.tile(tf.reshape(ToExp,[nnz,1,1,1,1]),[1,nzeta,neta,ntheta,nr]) - rtmp\n\tToExp2 = etmp*tet*tet\n\tToExp3 = tf.where(tf.greater(ToExp2,30),-30.0*tf.ones_like(ToExp2),-1.0*ToExp2)\n\tfac2 = tf.exp(ToExp3)\n\t# And finally the last two factors\n\tfac3 = tf.where(tf.greater_equal(RijRij,R_cut),tf.zeros_like(RijRij, dtype=prec),0.5*(tf.cos(3.14159265359*RijRij/R_cut)+1.0))\n\tfac4 = tf.where(tf.greater_equal(RikRik,R_cut),tf.zeros_like(RikRik, dtype=prec),0.5*(tf.cos(3.14159265359*RikRik/R_cut)+1.0))\n\t# assemble the full symmetry function for all triples.\n\tfac34t =  tf.tile(tf.reshape(fac3*fac4,[nnz,1,1,1,1]),[1,nzeta,neta,ntheta,nr])\n\tGm = tf.reshape(fac1*fac2*fac34t,[nnz*nzeta*neta*ntheta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t# Finally scatter out the symmetry functions where they belong.\n\tjk2 = tf.add(tf.multiply(tf.slice(GoodInds,[0,2],[nnz,1]), natom), tf.slice(GoodInds,[0,3],[nnz, 1]))\n\tmil_jk2 = tf.concat([tf.slice(GoodInds,[0,0],[nnz,2]),tf.slice(GoodInds,[0,4],[nnz,1]),tf.reshape(jk2,[nnz,1])],axis=-1)\n\tmil_jk_Outer2 = tf.tile(tf.reshape(mil_jk2,[nnz,1,4]),[1,nsym,1])\n\t# So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tp1_2 = tf.tile(tf.reshape(tf.multiply(tf.range(nzeta), neta*ntheta*nr),[nzeta,1]),[1,neta])\n\tp2_2 = tf.tile(tf.reshape(tf.concat([p1_2,tf.tile(tf.reshape(tf.multiply(tf.range(neta),ntheta*nr),[1,neta]),[nzeta,1])],axis=-1),[nzeta,neta,1,2]),[1,1,ntheta,1])\n\tp3_2 = tf.tile(tf.reshape(tf.concat([p2_2,tf.tile(tf.reshape(tf.multiply(tf.range(ntheta),nr),[1,1,ntheta,1]),[nzeta,neta,1,1])],axis=-1),[nzeta,neta,ntheta,1,3]),[1,1,1,nr,1])\n\tp4_2 = tf.reshape(tf.concat([p3_2,tf.tile(tf.reshape(tf.range(nr),[1,1,1,nr,1]),[nzeta,neta,ntheta,1,1])],axis=-1),[1,nzeta,neta,ntheta,nr,4])\n\tp5_2 = tf.reshape(tf.reduce_sum(p4_2,axis=-1),[1,nsym,1]) # scatter_nd only supports upto rank 5... so gotta smush this...\n\tp6_2 = tf.tile(p5_2,[nnz,1,1]) # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_jk_Outer2,p6_2],axis=-1),[nnz*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\tto_reduce2 = tf.scatter_nd(ind2,Gm,[nmol,natom,nelep,natom2,nsym])\n\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\n\ndef TFSymASet_Update(R, Zs, eleps_, SFPs_, R_cut, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teleps_: a nelepairs X 2 tensor of element pairs present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 4 X nzeta X neta X thetas X nRs. For example, SFPs_[0,0,0,0,0]\n\t\tis the first zeta parameter. SFPs_[3,0,0,0,1] is the second R parameter.\n\t\tR_cut: Radial Cutoff\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnatom3 = natom*natom2\n\tnelep = tf.shape(eleps_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnzeta = pshape[1]\n\tneta = pshape[2]\n\tntheta = pshape[3]\n\tnr = pshape[4]\n\tnsym = nzeta*neta*ntheta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tonescalar = 1.0 - 0.0000000000000001\n\n\t# atom triples.\n\tats = AllTriplesSet(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]))\n\t# before performing any computation reduce this to desired pairs.\n\t# Construct the angle triples acos(<Rij,Rik>/|Rij||Rik|) and mask them onto the correct output\n\t# Get Rij, Rik...\n\tRm_inds = tf.slice(ats,[0,0,0,0,0],[nmol,natom,natom,natom,1])\n\tRi_inds = tf.slice(ats,[0,0,0,0,1],[nmol,natom,natom,natom,1])\n\tRj_inds = tf.slice(ats,[0,0,0,0,2],[nmol,natom,natom,natom,1])\n\tRk_inds = tf.slice(ats,[0,0,0,0,3],[nmol,natom,natom,natom,1])\n\tRjk_inds = tf.reshape(tf.concat([Rm_inds,Rj_inds,Rk_inds],axis=4),[nmol,natom3,3])\n\tZ1Z2 = ZouterSet(Zs)\n\tZPairs = tf.gather_nd(Z1Z2,Rjk_inds) # should have shape nmol X natom3 X 2\n\tElemReduceMask = tf.reduce_all(tf.equal(tf.reshape(ZPairs,[nmol,natom3,1,2]),tf.reshape(eleps_,[1,1,nelep,2])),axis=-1) # nmol X natom3 X nelep\n\t# Zero out the diagonal contributions (i==j or i==k)\n\tIdentMask = tf.tile(tf.reshape(tf.logical_and(tf.not_equal(Ri_inds,Rj_inds),tf.not_equal(Ri_inds,Rk_inds)),[nmol,natom3,1]),[1,1,nelep])\n\tMask = tf.logical_and(ElemReduceMask,IdentMask) # nmol X natom3 X nelep\n\t# Mask is true if atoms ijk => pair_l and many triples are unused.\n\t# So we create a final index tensor, which is only nonzero m,ijk,l\n\tpinds = tf.range(nelep)\n\tats = tf.tile(tf.reshape(ats,[nmol,natom3,1,4]),[1,1,nelep,1])\n\tps = tf.tile(tf.reshape(pinds,[1,1,nelep,1]),[nmol,natom3,1,1])\n\tToMask = tf.concat([ats,ps],axis=3)\n\tGoodInds = tf.boolean_mask(ToMask,Mask)\n\tnnz = tf.shape(GoodInds)[0]\n\t# Good Inds has shape << nmol * natom3 * nelep X 5 (mol, i,j,k,l=element pair.)\n\t# and contains all the indices we actually want to compute, Now we just slice, gather and compute.\n\tmijs = tf.slice(GoodInds,[0,0],[nnz,3])\n\tmiks = tf.concat([tf.slice(GoodInds,[0,0],[nnz,2]),tf.slice(GoodInds,[0,3],[nnz,1])],axis=-1)\n\tRij = DifferenceVectorsSet(R,prec) # nmol X atom X atom X 3\n\tA = tf.gather_nd(Rij,mijs)\n\tB = tf.gather_nd(Rij,miks)\n\tRijRik = tf.reduce_sum(A*B,axis=1)\n\tRijRij = tf.sqrt(tf.reduce_sum(A*A,axis=1)+infinitesimal)\n\tRikRik = tf.sqrt(tf.reduce_sum(B*B,axis=1)+infinitesimal)\n\n\tMaskDist1 = tf.where(tf.greater_equal(RijRij,R_cut),tf.zeros([nnz], dtype=tf.bool), tf.ones([nnz], dtype=tf.bool))\n\tMaskDist2 = tf.where(tf.greater_equal(RikRik,R_cut),tf.zeros([nnz], dtype=tf.bool), tf.ones([nnz], dtype=tf.bool))\n\tMaskDist12 = tf.logical_and(MaskDist1, MaskDist2) # nmol X natom3 X nelep\n\tGoodInds2 = tf.boolean_mask(GoodInds, MaskDist12)\n\tnnz2 = tf.shape(GoodInds2)[0]\n\tmijs2 = tf.slice(GoodInds2,[0,0],[nnz2,3])\n\tmiks2 = tf.concat([tf.slice(GoodInds2,[0,0],[nnz2,2]),tf.slice(GoodInds2,[0,3],[nnz2,1])],axis=-1)\n\tA2 = tf.gather_nd(Rij,mijs2)\n\tB2 = tf.gather_nd(Rij,miks2)\n\tRijRik2 = tf.reduce_sum(A2*B2,axis=1)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(A2*A2,axis=1)+infinitesimal)\n\tRikRik2 = tf.sqrt(tf.reduce_sum(B2*B2,axis=1)+infinitesimal)\n\n\tdenom = RijRij2*RikRik2\n\t# Mask any troublesome entries.\n\tToACos = RijRik2/denom\n\tToACos = tf.where(tf.greater_equal(ToACos,1.0),tf.ones_like(ToACos, dtype=prec)*onescalar,ToACos)\n\tToACos = tf.where(tf.less_equal(ToACos,-1.0),-1.0*tf.ones_like(ToACos, dtype=prec)*onescalar,ToACos)\n\tThetaijk = tf.acos(ToACos)\n\tzetatmp = tf.cast(tf.reshape(SFPs_[0],[1,nzeta,neta,ntheta,nr]),prec)\n\tthetatmp = tf.cast(tf.tile(tf.reshape(SFPs_[2],[1,nzeta,neta,ntheta,nr]),[nnz2,1,1,1,1]),prec)\n\t# Broadcast the thetas and ToCos together\n\ttct = tf.tile(tf.reshape(Thetaijk,[nnz2,1,1,1,1]),[1,nzeta,neta,ntheta,nr], name=""tct"")\n\tToCos = tct-thetatmp\n\tTijk = tf.cos(ToCos) # shape: natom3 X ...\n\t# complete factor 1\n\tfac1 = tf.pow(tf.cast(2.0, prec),1.0-zetatmp)*tf.pow((1.0+Tijk),zetatmp)\n\tetmp = tf.cast(tf.reshape(SFPs_[1],[1,nzeta,neta,ntheta,nr]),prec) # ijk X zeta X eta ....\n\trtmp = tf.cast(tf.reshape(SFPs_[3],[1,nzeta,neta,ntheta,nr]),prec) # ijk X zeta X eta ....\n\tToExp = ((RijRij2+RikRik2)/2.0)\n\ttet = tf.tile(tf.reshape(ToExp,[nnz2,1,1,1,1]),[1,nzeta,neta,ntheta,nr], name=""tet"") - rtmp\n\tfac2 = tf.exp(-etmp*tet*tet)\n\t# And finally the last two factors\n\tfac3 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac4 = 0.5*(tf.cos(3.14159265359*RikRik2/R_cut)+1.0)\n\t# assemble the full symmetry function for all triples.\n\tfac34t =  tf.tile(tf.reshape(fac3*fac4,[nnz2,1,1,1,1]),[1,nzeta,neta,ntheta,nr], name=""fac34t"")\n\tGm = tf.reshape(fac1*fac2*fac34t,[nnz2*nzeta*neta*ntheta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t# Finally scatter out the symmetry functions where they belong.\n\tjk2 = tf.add(tf.multiply(tf.slice(GoodInds2,[0,2],[nnz2,1]), natom), tf.slice(GoodInds2,[0,3],[nnz2, 1]))\n\tmil_jk2 = tf.concat([tf.slice(GoodInds2,[0,0],[nnz2,2]),tf.slice(GoodInds2,[0,4],[nnz2,1]),tf.reshape(jk2,[nnz2,1])],axis=-1)\n\tmil_jk_Outer2 = tf.tile(tf.reshape(mil_jk2,[nnz2,1,4]),[1,nsym,1], name=""mil_jk_Outer2"")\n\t# So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tp1_2 = tf.tile(tf.reshape(tf.multiply(tf.range(nzeta), neta*ntheta*nr),[nzeta,1]),[1,neta])\n\tp2_2 = tf.tile(tf.reshape(tf.concat([p1_2,tf.tile(tf.reshape(tf.multiply(tf.range(neta),ntheta*nr),[1,neta]),[nzeta,1])],axis=-1),[nzeta,neta,1,2]),[1,1,ntheta,1])\n\tp3_2 = tf.tile(tf.reshape(tf.concat([p2_2,tf.tile(tf.reshape(tf.multiply(tf.range(ntheta),nr),[1,1,ntheta,1]),[nzeta,neta,1,1])],axis=-1),[nzeta,neta,ntheta,1,3]),[1,1,1,nr,1])\n\tp4_2 = tf.reshape(tf.concat([p3_2,tf.tile(tf.reshape(tf.range(nr),[1,1,1,nr,1]),[nzeta,neta,ntheta,1,1])],axis=-1),[1,nzeta,neta,ntheta,nr,4])\n\tp5_2 = tf.reshape(tf.reduce_sum(p4_2,axis=-1),[1,nsym,1]) # scatter_nd only supports upto rank 5... so gotta smush this...\n\tp6_2 = tf.tile(p5_2,[nnz2,1,1], name=""p6_tile"") # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_jk_Outer2,p6_2],axis=-1),[nnz2*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\tto_reduce2 = tf.scatter_nd(ind2,Gm,[nmol,natom,nelep,natom2,nsym])\n\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\n\ndef TFSymRSet(R, Zs, eles_, SFPs_, R_cut, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tneta = pshape[1]\n\tnr = pshape[2]\n\tnsym = neta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\n\t# atom triples.\n\tats = AllDoublesSet(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]))\n\t# before performing any computation reduce this to desired pairs.\n\t# Construct the angle triples acos(<Rij,Rik>/|Rij||Rik|) and mask them onto the correct output\n\t# Get Rij, Rik...\n\tRm_inds = tf.slice(ats,[0,0,0,0],[nmol,natom,natom,1])\n\tRi_inds = tf.slice(ats,[0,0,0,1],[nmol,natom,natom,1])\n\tRj_inds = tf.slice(ats,[0,0,0,2],[nmol,natom,natom,1])\n\t#Rjk_inds = tf.reshape(tf.concat([Rm_inds,Rj_inds,Rk_inds],axis=4),[nmol,natom3,3])\n\tZAll = AllDoublesSet(Zs)\n\tZPairs = tf.slice(ZAll,[0,0,0,2],[nmol,natom,natom,1]) # should have shape nmol X natom X natom X 1\n\tElemReduceMask = tf.reduce_all(tf.equal(tf.reshape(ZPairs,[nmol,natom2,1,1]),tf.reshape(eles_,[1,1,nele,1])),axis=-1) # nmol X natom3 X nelep\n\t# Zero out the diagonal contributions (i==j or i==k)\n\tIdentMask = tf.tile(tf.reshape(tf.not_equal(Ri_inds,Rj_inds),[nmol,natom2,1]),[1,1,nele])\n\tMask = tf.logical_and(ElemReduceMask,IdentMask) # nmol X natom3 X nelep\n\t# Mask is true if atoms ijk => pair_l and many triples are unused.\n\t# So we create a final index tensor, which is only nonzero m,ijk,l\n\tpinds = tf.range(nele)\n\tats = tf.tile(tf.reshape(ats,[nmol,natom2,1,3]),[1,1,nele,1])\n\tps = tf.tile(tf.reshape(pinds,[1,1,nele,1]),[nmol,natom2,1,1])\n\tToMask = tf.concat([ats,ps],axis=3)\n\tGoodInds = tf.boolean_mask(ToMask,Mask)\n\tnnz = tf.shape(GoodInds)[0]\n\t# Good Inds has shape << nmol * natom2 * nele X 4 (mol, i, j, l=element pair.)\n\t# and contains all the indices we actually want to compute, Now we just slice, gather and compute.\n\tmijs = tf.slice(GoodInds,[0,0],[nnz,3])\n\tRij = DifferenceVectorsSet(R,prec) # nmol X atom X atom X 3\n\tA = tf.gather_nd(Rij,mijs)\n\tRijRij = tf.sqrt(tf.reduce_sum(A*A,axis=1)+infinitesimal)\n\t# Mask any troublesome entries.\n\tetmp = tf.cast(tf.reshape(SFPs_[0],[1,neta,nr]),prec) # ijk X zeta X eta ....\n\trtmp = tf.cast(tf.reshape(SFPs_[1],[1,neta,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij,[nnz,1,1]),[1,neta,nr]) - rtmp\n\tfac1 = tf.exp(-etmp*tet*tet)\n\t# And finally the last two factors\n\tfac2 = tf.where(tf.greater_equal(RijRij,R_cut),tf.zeros_like(RijRij, dtype=prec),0.5*(tf.cos(3.14159265359*RijRij/R_cut)+1.0))\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz,1,1]),[1,neta,nr])\n\t# assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t,[nnz*neta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t# Finally scatter out the symmetry functions where they belong.\n\tmil_j = tf.concat([tf.slice(GoodInds,[0,0],[nnz,2]),tf.slice(GoodInds,[0,3],[nnz,1]),tf.slice(GoodInds,[0,2],[nnz,1])],axis=-1)\n\tmil_j_Outer = tf.tile(tf.reshape(mil_j,[nnz,1,4]),[1,nsym,1])\n\t# So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tp1_2 = tf.tile(tf.reshape(tf.multiply(tf.range(neta), nr),[neta,1,1]),[1,nr,1])\n\tp2_2 = tf.reshape(tf.concat([p1_2,tf.tile(tf.reshape(tf.range(nr),[1,nr,1]),[neta,1,1])],axis=-1),[1,neta,nr,2])\n\tp3_2 = tf.reshape(tf.reduce_sum(p2_2,axis=-1),[1,nsym,1]) # scatter_nd only supports up to rank 5... so gotta smush this...\n\tp4_2 = tf.tile(p3_2,[nnz,1,1]) # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_j_Outer,p4_2],axis=-1),[nnz*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\tto_reduce2 = tf.scatter_nd(ind2,Gm,[nmol,natom,nele,natom,nsym])\n\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\n\ndef TFSymRSet_Update(R, Zs, eles_, SFPs_, R_cut, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tneta = pshape[1]\n\tnr = pshape[2]\n\tnsym = neta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\n\t# atom triples.\n\tats = AllDoublesSet(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]))\n\t# before performing any computation reduce this to desired pairs.\n\t# Construct the angle triples acos(<Rij,Rik>/|Rij||Rik|) and mask them onto the correct output\n\t# Get Rij, Rik...\n\tRm_inds = tf.slice(ats,[0,0,0,0],[nmol,natom,natom,1])\n\tRi_inds = tf.slice(ats,[0,0,0,1],[nmol,natom,natom,1])\n\tRj_inds = tf.slice(ats,[0,0,0,2],[nmol,natom,natom,1])\n\t#Rjk_inds = tf.reshape(tf.concat([Rm_inds,Rj_inds,Rk_inds],axis=4),[nmol,natom3,3])\n\tZAll = AllDoublesSet(Zs)\n\tZPairs = tf.slice(ZAll,[0,0,0,2],[nmol,natom,natom,1]) # should have shape nmol X natom X natom X 1\n\tElemReduceMask = tf.reduce_all(tf.equal(tf.reshape(ZPairs,[nmol,natom2,1,1]),tf.reshape(eles_,[1,1,nele,1])),axis=-1) # nmol X natom3 X nelep\n\t# Zero out the diagonal contributions (i==j or i==k)\n\tIdentMask = tf.tile(tf.reshape(tf.not_equal(Ri_inds,Rj_inds),[nmol,natom2,1]),[1,1,nele])\n\tMask = tf.logical_and(ElemReduceMask,IdentMask) # nmol X natom3 X nelep\n\t# Mask is true if atoms ijk => pair_l and many triples are unused.\n\t# So we create a final index tensor, which is only nonzero m,ijk,l\n\tpinds = tf.range(nele)\n\tats = tf.tile(tf.reshape(ats,[nmol,natom2,1,3]),[1,1,nele,1])\n\tps = tf.tile(tf.reshape(pinds,[1,1,nele,1]),[nmol,natom2,1,1])\n\tToMask = tf.concat([ats,ps],axis=3)\n\tGoodInds = tf.boolean_mask(ToMask,Mask)\n\tnnz = tf.shape(GoodInds)[0]\n\t# Good Inds has shape << nmol * natom2 * nele X 4 (mol, i, j, l=element pair.)\n\t# and contains all the indices we actually want to compute, Now we just slice, gather and compute.\n\tmijs = tf.slice(GoodInds,[0,0],[nnz,3])\n\tRij = DifferenceVectorsSet(R,prec) # nmol X atom X atom X 3\n\tA = tf.gather_nd(Rij,mijs)\n\tRijRij = tf.sqrt(tf.reduce_sum(A*A,axis=1)+infinitesimal)\n\n\tMaskDist = tf.where(tf.greater_equal(RijRij,R_cut),tf.zeros([nnz], dtype=tf.bool), tf.ones([nnz], dtype=tf.bool))\n\tGoodInds2 = tf.boolean_mask(GoodInds, MaskDist)\n\tnnz2 = tf.shape(GoodInds2)[0]\n\tmijs2 = tf.slice(GoodInds2,[0,0],[nnz2,3])\n\tA2 = tf.gather_nd(Rij,mijs2)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(A2*A2,axis=1)+infinitesimal)\n\n\t# Mask any troublesome entries.\n\tetmp = tf.cast(tf.reshape(SFPs_[0],[1,neta,nr]),prec) # ijk X zeta X eta ....\n\trtmp = tf.cast(tf.reshape(SFPs_[1],[1,neta,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij2,[nnz2,1,1]),[1,neta,nr]) - rtmp\n\tfac1 = tf.exp(-etmp*tet*tet)\n\t# And finally the last two factors\n\tfac2 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz2,1,1]),[1,neta,nr])\n\t# assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t,[nnz2*neta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t# Finally scatter out the symmetry functions where they belong.\n\tmil_j = tf.concat([tf.slice(GoodInds2,[0,0],[nnz2,2]),tf.slice(GoodInds2,[0,3],[nnz2,1]),tf.slice(GoodInds2,[0,2],[nnz2,1])],axis=-1)\n\tmil_j_Outer = tf.tile(tf.reshape(mil_j,[nnz2,1,4]),[1,nsym,1])\n\t# So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tp1_2 = tf.tile(tf.reshape(tf.multiply(tf.range(neta), nr),[neta,1,1]),[1,nr,1])\n\tp2_2 = tf.reshape(tf.concat([p1_2,tf.tile(tf.reshape(tf.range(nr),[1,nr,1]),[neta,1,1])],axis=-1),[1,neta,nr,2])\n\tp3_2 = tf.reshape(tf.reduce_sum(p2_2,axis=-1),[1,nsym,1]) # scatter_nd only supports up to rank 5... so gotta smush this...\n\tp4_2 = tf.tile(p3_2,[nnz2,1,1]) # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_j_Outer,p4_2],axis=-1),[nnz2*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\tto_reduce2 = tf.scatter_nd(ind2,Gm,[nmol,natom,nele,natom,nsym])\n\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\n\ndef TFSymASet_Update2(R, Zs, eleps_, SFPs_, zeta, eta, R_cut, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teleps_: a nelepairs X 2 tensor of element pairs present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 4 X nzeta X neta X thetas X nRs. For example, SFPs_[0,0,0,0,0]\n\t\tis the first zeta parameter. SFPs_[3,0,0,0,1] is the second R parameter.\n\t\tR_cut: Radial Cutoff\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnatom3 = natom*natom2\n\tnelep = tf.shape(eleps_)[0]\n\tpshape = tf.shape(SFPs_)\n\tntheta = pshape[1]\n\tnr = pshape[2]\n\tnsym = ntheta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tonescalar = 1.0 - 0.0000000000000001\n\n\t# atom triples.\n\tats = AllTriplesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]), dtype=tf.int64), prec=tf.int64)\n\t# before performing any computation reduce this to desired pairs.\n\t# Construct the angle triples acos(<Rij,Rik>/|Rij||Rik|) and mask them onto the correct output\n\t# Get Rij, Rik...\n\tRm_inds = tf.slice(ats,[0,0,0,0,0],[nmol,natom,natom,natom,1])\n\tRi_inds = tf.slice(ats,[0,0,0,0,1],[nmol,natom,natom,natom,1])\n\tRj_inds = tf.slice(ats,[0,0,0,0,2],[nmol,natom,natom,natom,1])\n\tRk_inds = tf.slice(ats,[0,0,0,0,3],[nmol,natom,natom,natom,1])\n\tRjk_inds = tf.reshape(tf.concat([Rm_inds,Rj_inds,Rk_inds],axis=4),[nmol,natom3,3])\n\tZ1Z2 = ZouterSet(Zs)\n\tZPairs = tf.gather_nd(Z1Z2,Rjk_inds) # should have shape nmol X natom3 X 2\n\tElemReduceMask = tf.reduce_all(tf.equal(tf.reshape(ZPairs,[nmol,natom3,1,2]),tf.reshape(eleps_,[1,1,nelep,2])),axis=-1) # nmol X natom3 X nelep\n\t# Zero out the diagonal contributions (i==j or i==k)\n\tIdentMask = tf.tile(tf.reshape(tf.logical_and(tf.not_equal(Ri_inds,Rj_inds),tf.not_equal(Ri_inds,Rk_inds)),[nmol,natom3,1]),[1,1,nelep])\n\tMask = tf.logical_and(ElemReduceMask,IdentMask) # nmol X natom3 X nelep\n\t# Mask is true if atoms ijk => pair_l and many triples are unused.\n\t# So we create a final index tensor, which is only nonzero m,ijk,l\n\tpinds = tf.cast(tf.range(nelep),dtype=tf.int64)\n\tats = tf.tile(tf.reshape(ats,[nmol,natom3,1,4]),[1,1,nelep,1])\n\tps = tf.tile(tf.reshape(pinds,[1,1,nelep,1]),[nmol,natom3,1,1])\n\tToMask = tf.concat([ats,ps],axis=3)\n\tGoodInds = tf.boolean_mask(ToMask,Mask)\n\tnnz = tf.shape(GoodInds)[0]\n\t# Good Inds has shape << nmol * natom3 * nelep X 5 (mol, i,j,k,l=element pair.)\n\t# and contains all the indices we actually want to compute, Now we just slice, gather and compute.\n\tmijs = tf.slice(GoodInds,[0,0],[nnz,3])\n\tmiks = tf.concat([tf.slice(GoodInds,[0,0],[nnz,2]),tf.slice(GoodInds,[0,3],[nnz,1])],axis=-1)\n\tRij = DifferenceVectorsSet(R,prec) # nmol X atom X atom X 3\n\tA = tf.gather_nd(Rij,mijs)\n\tB = tf.gather_nd(Rij,miks)\n\tRijRik = tf.reduce_sum(A*B,axis=1)\n\tRijRij = tf.sqrt(tf.reduce_sum(A*A,axis=1)+infinitesimal)\n\tRikRik = tf.sqrt(tf.reduce_sum(B*B,axis=1)+infinitesimal)\n\n\tMaskDist1 = tf.where(tf.greater_equal(RijRij,R_cut),tf.zeros([nnz], dtype=tf.bool), tf.ones([nnz], dtype=tf.bool))\n\tMaskDist2 = tf.where(tf.greater_equal(RikRik,R_cut),tf.zeros([nnz], dtype=tf.bool), tf.ones([nnz], dtype=tf.bool))\n\tMaskDist12 = tf.logical_and(MaskDist1, MaskDist2) # nmol X natom3 X nelep\n\tGoodInds2 = tf.boolean_mask(GoodInds, MaskDist12)\n\tnnz2 = tf.shape(GoodInds2)[0]\n\tmijs2 = tf.slice(GoodInds2,[0,0],[nnz2,3])\n\tmiks2 = tf.concat([tf.slice(GoodInds2,[0,0],[nnz2,2]),tf.slice(GoodInds2,[0,3],[nnz2,1])],axis=-1)\n\tA2 = tf.gather_nd(Rij,mijs2)\n\tB2 = tf.gather_nd(Rij,miks2)\n\tRijRik2 = tf.reduce_sum(A2*B2,axis=1)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(A2*A2,axis=1)+infinitesimal)\n\tRikRik2 = tf.sqrt(tf.reduce_sum(B2*B2,axis=1)+infinitesimal)\n\n\tdenom = RijRij2*RikRik2\n\t#Mask any troublesome entries.\n\tToACos = RijRik2/denom\n\tToACos = tf.where(tf.greater_equal(ToACos,1.0),tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\tToACos = tf.where(tf.less_equal(ToACos,-1.0),-1.0*tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\tThetaijk = tf.acos(ToACos)\n\tthetatmp = tf.cast(tf.tile(tf.reshape(SFPs_[0],[1,ntheta,nr]),[nnz2,1,1]),prec)\n\t# Broadcast the thetas and ToCos together\n\ttct = tf.tile(tf.reshape(Thetaijk,[nnz2,1,1]),[1,ntheta,nr])\n\tToCos = tct-thetatmp\n\tTijk = tf.cos(ToCos) # shape: natom3 X ...\n\t# complete factor 1\n\tfac1 = tf.pow(tf.cast(2.0, prec),1.0-zeta)*tf.pow((1.0+Tijk),zeta)\n\trtmp = tf.cast(tf.reshape(SFPs_[1],[1,ntheta,nr]),prec) # ijk X zeta X eta ....\n\tToExp = ((RijRij2+RikRik2)/2.0)\n\ttet = tf.tile(tf.reshape(ToExp,[nnz2,1,1]),[1,ntheta,nr]) - rtmp\n\tfac2 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac3 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac4 = 0.5*(tf.cos(3.14159265359*RikRik2/R_cut)+1.0)\n\t# assemble the full symmetry function for all triples.\n\tfac34t =  tf.tile(tf.reshape(fac3*fac4,[nnz2,1,1]),[1,ntheta,nr])\n\t#Gm = tf.reshape(fac2*fac34t,[nnz2*ntheta*nr]) # nnz X nzeta X neta X ntheta X nr\n\tGm = tf.reshape(fac1*fac2*fac34t,[nnz2*ntheta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t# Finally scatter out the symmetry functions where they belong.\n\tjk2 = tf.add(tf.multiply(tf.slice(GoodInds2,[0,2],[nnz2,1]), tf.cast(natom, dtype=tf.int64)), tf.slice(GoodInds2,[0,3],[nnz2, 1]))\n\tmil_jk2 = tf.concat([tf.slice(GoodInds2,[0,0],[nnz2,2]),tf.slice(GoodInds2,[0,4],[nnz2,1]),tf.reshape(jk2,[nnz2,1])],axis=-1)\n\tmil_jk_Outer2 = tf.tile(tf.reshape(mil_jk2,[nnz2,1,4]),[1,nsym,1])\n\t# So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\n\tp1_2 = tf.tile(tf.reshape(tf.multiply(tf.cast(tf.range(ntheta), dtype=tf.int64), tf.cast(nr, dtype=tf.int64)),[ntheta,1,1]),[1,nr,1])\n\tp2_2 = tf.reshape(tf.concat([p1_2,tf.tile(tf.reshape(tf.cast(tf.range(nr), dtype=tf.int64),[1,nr,1]),[ntheta,1,1])],axis=-1),[1,ntheta,nr,2])\n\tp3_2 = tf.reshape(tf.reduce_sum(p2_2,axis=-1),[1,nsym,1]) # scatter_nd only supports up to rank 5... so gotta smush this...\n\tp6_2 = tf.tile(p3_2,[nnz2,1,1]) # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_jk_Outer2,p6_2],axis=-1),[nnz2*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\tto_reduce2 = tf.scatter_nd(ind2,Gm,tf.cast([nmol,natom,nelep,natom2,nsym], dtype=tf.int64))\n\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\n\n\ndef TFSymRSet_Update2(R, Zs, eles_, SFPs_, eta, R_cut, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnr = pshape[1]\n\tnsym = nr\n\tinfinitesimal = 0.000000000000000000000000001\n\n\t# atom triples.\n\tats = AllDoublesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]), dtype=tf.int64), prec=tf.int64)\n\t# before performing any computation reduce this to desired pairs.\n\t# Construct the angle triples acos(<Rij,Rik>/|Rij||Rik|) and mask them onto the correct output\n\t# Get Rij, Rik...\n\tRm_inds = tf.slice(ats,[0,0,0,0],[nmol,natom,natom,1])\n\tRi_inds = tf.slice(ats,[0,0,0,1],[nmol,natom,natom,1])\n\tRj_inds = tf.slice(ats,[0,0,0,2],[nmol,natom,natom,1])\n\t#Rjk_inds = tf.reshape(tf.concat([Rm_inds,Rj_inds,Rk_inds],axis=4),[nmol,natom3,3])\n\tZAll = AllDoublesSet(Zs, prec=tf.int64)\n\tZPairs = tf.slice(ZAll,[0,0,0,2],[nmol,natom,natom,1]) # should have shape nmol X natom X natom X 1\n\tElemReduceMask = tf.reduce_all(tf.equal(tf.reshape(ZPairs,[nmol,natom2,1,1]),tf.reshape(eles_,[1,1,nele,1])),axis=-1) # nmol X natom3 X nelep\n\t# Zero out the diagonal contributions (i==j or i==k)\n\tIdentMask = tf.tile(tf.reshape(tf.not_equal(Ri_inds,Rj_inds),[nmol,natom2,1]),[1,1,nele])\n\tMask = tf.logical_and(ElemReduceMask,IdentMask) # nmol X natom3 X nelep\n\t# Mask is true if atoms ijk => pair_l and many triples are unused.\n\t# So we create a final index tensor, which is only nonzero m,ijk,l\n\tpinds = tf.cast(tf.range(nele), dtype=tf.int64)\n\tats = tf.tile(tf.reshape(ats,[nmol,natom2,1,3]),[1,1,nele,1])\n\tps = tf.tile(tf.reshape(pinds,[1,1,nele,1]),[nmol,natom2,1,1])\n\tToMask = tf.concat([ats,ps],axis=3)\n\tGoodInds = tf.boolean_mask(ToMask,Mask)\n\tnnz = tf.shape(GoodInds)[0]\n\t# Good Inds has shape << nmol * natom2 * nele X 4 (mol, i, j, l=element pair.)\n\t# and contains all the indices we actually want to compute, Now we just slice, gather and compute.\n\tmijs = tf.slice(GoodInds,[0,0],[nnz,3])\n\tRij = DifferenceVectorsSet(R,prec) # nmol X atom X atom X 3\n\tA = tf.gather_nd(Rij,mijs)\n\tRijRij = tf.sqrt(tf.reduce_sum(A*A,axis=1)+infinitesimal)\n\n\tMaskDist = tf.where(tf.greater_equal(RijRij,R_cut),tf.zeros([nnz], dtype=tf.bool), tf.ones([nnz], dtype=tf.bool))\n\tGoodInds2 = tf.boolean_mask(GoodInds, MaskDist)\n\tnnz2 = tf.shape(GoodInds2)[0]\n\tmijs2 = tf.slice(GoodInds2,[0,0],[nnz2,3])\n\tA2 = tf.gather_nd(Rij,mijs2)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(A2*A2,axis=1)+infinitesimal)\n\n\t# Mask any troublesome entries.\n\trtmp = tf.cast(tf.reshape(SFPs_[0],[1,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij2,[nnz2,1]),[1,nr]) - rtmp\n\tfac1 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac2 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz2,1]),[1,nr])\n\t# assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t,[nnz2*nr]) # nnz X nzeta X neta X ntheta X nr\n\t# Finally scatter out the symmetry functions where they belong.\n\tmil_j = tf.concat([tf.slice(GoodInds2,[0,0],[nnz2,2]),tf.slice(GoodInds2,[0,3],[nnz2,1]),tf.slice(GoodInds2,[0,2],[nnz2,1])],axis=-1)\n\tmil_j_Outer = tf.tile(tf.reshape(mil_j,[nnz2,1,4]),[1,nsym,1])\n\t# So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tp2_2 = tf.reshape(tf.reshape(tf.cast(tf.range(nr), dtype=tf.int64),[nr,1]),[1,nr,1])\n\tp4_2 = tf.tile(p2_2,[nnz2,1,1]) # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_j_Outer,p4_2],axis=-1),[nnz2*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\tto_reduce2 = tf.scatter_nd(ind2,Gm,tf.cast([nmol,natom,nele,natom,nsym], dtype=tf.int64))\n\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\n\ndef TFSymASet_Linear(R, Zs, eleps_, SFPs_, zeta, eta, R_cut, Angtri, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teleps_: a nelepairs X 2 tensor of element pairs present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 4 X nzeta X neta X thetas X nRs. For example, SFPs_[0,0,0,0,0]\n\t\tis the first zeta parameter. SFPs_[3,0,0,0,1] is the second R parameter.\n\t\tR_cut: Radial Cutoff\n\t\tAngtri: angular triples within the cutoff.\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnatom3 = natom*natom2\n\tnelep = tf.shape(eleps_)[0]\n\tpshape = tf.shape(SFPs_)\n\tntheta = pshape[1]\n\tnr = pshape[2]\n\tnsym = ntheta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tonescalar = 1.0 - 0.0000000000000001\n\tnnzt = tf.shape(Angtri)[0]\n\n\tZ1Z2 = ZouterSet(Zs)\n\n\tRij_inds = tf.slice(Angtri,[0,0],[nnzt,3])\n\tRik_inds = tf.concat([tf.slice(Angtri,[0,0],[nnzt,2]), tf.slice(Angtri,[0,3],[nnzt,1])],axis=-1)\n\tRjk_inds = tf.concat([tf.slice(Angtri,[0,0],[nnzt,1]), tf.slice(Angtri,[0,2],[nnzt,2])],axis=-1)\n\tZPairs = tf.gather_nd(Z1Z2, Rjk_inds)\n\tEleIndex = tf.slice(tf.where(tf.reduce_all(tf.equal(tf.reshape(ZPairs,[nnzt,1,2]), tf.reshape(eleps_,[1, nelep, 2])),axis=-1)),[0,1],[nnzt,1])\n\tGoodInds2 = tf.concat([Angtri,EleIndex],axis=-1)\n\n\tRij = DifferenceVectorsLinear(R, Rij_inds)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\tRik = DifferenceVectorsLinear(R, Rik_inds)\n\tRikRik2 = tf.sqrt(tf.reduce_sum(Rik*Rik,axis=1)+infinitesimal)\n\tRijRik2 = tf.reduce_sum(Rij*Rik, axis=1)\n\tdenom = RijRij2*RikRik2\n\t#Mask any troublesome entries.\n\tToACos = RijRik2/denom\n\tToACos = tf.where(tf.greater_equal(ToACos,1.0),tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\tToACos = tf.where(tf.less_equal(ToACos,-1.0),-1.0*tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\tThetaijk = tf.acos(ToACos)\n\tthetatmp = tf.cast(tf.tile(tf.reshape(SFPs_[0],[1,ntheta,nr]),[nnzt,1,1]),prec)\n\t# Broadcast the thetas and ToCos together\n\ttct = tf.tile(tf.reshape(Thetaijk,[nnzt,1,1]),[1,ntheta,nr])\n\tToCos = tct-thetatmp\n\tTijk = tf.cos(ToCos) # shape: natom3 X ...\n\t# complete factor 1\n\tfac1 = tf.pow(tf.cast(2.0, prec),1.0-zeta)*tf.pow((1.0+Tijk),zeta)\n\trtmp = tf.cast(tf.reshape(SFPs_[1],[1,ntheta,nr]),prec) # ijk X zeta X eta ....\n\tToExp = ((RijRij2+RikRik2)/2.0)\n\ttet = tf.reshape(ToExp,[nnzt,1,1]) - rtmp\n\t#tet = tf.tile(tf.reshape(ToExp,[nnzt,1,1]),[1,ntheta,nr]) - rtmp\n\tfac2 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac3 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac4 = 0.5*(tf.cos(3.14159265359*RikRik2/R_cut)+1.0)\n\t## assemble the full symmetry function for all triples.\n\tfac34t =  tf.tile(tf.reshape(fac3*fac4,[nnzt,1,1]),[1,ntheta,nr])\n\tGm = tf.reshape(fac1*fac2*fac34t,[nnzt*ntheta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t## Finally scatter out the symmetry functions where they belong.\n\tjk2 = tf.add(tf.multiply(tf.slice(GoodInds2,[0,2],[nnzt,1]), tf.cast(natom, dtype=tf.int64)), tf.slice(GoodInds2,[0,3],[nnzt, 1]))\n\tmil_jk2 = tf.concat([tf.slice(GoodInds2,[0,0],[nnzt,2]),tf.slice(GoodInds2,[0,4],[nnzt,1]),tf.reshape(jk2,[nnzt,1])],axis=-1)\n\tmil_jk_Outer2 = tf.tile(tf.reshape(mil_jk2,[nnzt,1,4]),[1,nsym,1])\n\t## So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tp1_2 = tf.tile(tf.reshape(tf.multiply(tf.cast(tf.range(ntheta), dtype=tf.int64), tf.cast(nr, dtype=tf.int64)),[ntheta,1,1]),[1,nr,1])\n\tp2_2 = tf.reshape(tf.concat([p1_2,tf.tile(tf.reshape(tf.cast(tf.range(nr), dtype=tf.int64),[1,nr,1]),[ntheta,1,1])],axis=-1),[1,ntheta,nr,2])\n\tp3_2 = tf.reshape(tf.reduce_sum(p2_2,axis=-1),[1,nsym,1]) # scatter_nd only supports up to rank 5... so gotta smush this...\n\tp6_2 = tf.tile(p3_2,[nnzt,1,1]) # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_jk_Outer2,p6_2],axis=-1),[nnzt*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\t#to_reduce2 = tf.scatter_nd(ind2,Gm,tf.cast([nmol,natom,nelep,natom2,nsym], dtype=tf.int64))  # scatter_nd way to do it\n\tto_reduce2 = tf.SparseTensor(ind2, Gm, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(natom, tf.int64), tf.cast(nelep, tf.int64), tf.cast(natom2, tf.int64), tf.cast(nsym, tf.int64)])\n\t#to_reduce2_reorder = tf.sparse_reorder(to_reduce2)\n\treduced2 = tf.sparse_reduce_sum_sparse(to_reduce2, axis=3)\n\t#to_reduce2_dense = tf.sparse_tensor_to_dense(to_reduce2, validate_indices=False)\n\t#return tf.sparse_reduce_sum(to_reduce2, axis=3)\n\t#return tf.reduce_sum(to_reduce2_dense, axis=3)\n\treturn tf.sparse_tensor_to_dense(reduced2)\n\ndef TFSymASet_Linear_WithEle(R, Zs, eleps_, SFPs_, zeta, eta, R_cut, AngtriEle, mil_jk2, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves append ele pair index at the end of triples with\n\tsorted order: m, i, l, j, k\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teleps_: a nelepairs X 2 tensor of element pairs present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 4 X nzeta X neta X thetas X nRs. For example, SFPs_[0,0,0,0,0]\n\t\tis the first zeta parameter. SFPs_[3,0,0,0,1] is the second R parameter.\n\t\tR_cut: Radial Cutoff\n\t\tAngtriEle: angular triples within the cutoff. m, i, j, k, l\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnatom3 = natom*natom2\n\tnelep = tf.shape(eleps_)[0]\n\tpshape = tf.shape(SFPs_)\n\tntheta = pshape[1]\n\tnr = pshape[2]\n\tnsym = ntheta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tonescalar = 1.0 - 0.0000000000000001\n\tnnzt = tf.shape(AngtriEle)[0]\n\n\n\tRij_inds = tf.slice(AngtriEle,[0,0],[nnzt,3])\n\tRik_inds = tf.concat([tf.slice(AngtriEle,[0,0],[nnzt,2]), tf.slice(AngtriEle,[0,3],[nnzt,1])],axis=-1)\n\tRjk_inds = tf.concat([tf.slice(AngtriEle,[0,0],[nnzt,1]), tf.slice(AngtriEle,[0,2],[nnzt,2])],axis=-1)\n\n\tRij = DifferenceVectorsLinear(R, Rij_inds)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\tRik = DifferenceVectorsLinear(R, Rik_inds)\n\tRikRik2 = tf.sqrt(tf.reduce_sum(Rik*Rik,axis=1)+infinitesimal)\n\tRijRik2 = tf.reduce_sum(Rij*Rik, axis=1)\n\tdenom = RijRij2*RikRik2\n\t#Mask any troublesome entries.\n\tToACos = RijRik2/denom\n\tToACos = tf.where(tf.greater_equal(ToACos,1.0),tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\tToACos = tf.where(tf.less_equal(ToACos,-1.0),-1.0*tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\tThetaijk = tf.acos(ToACos)\n\t#thetatmp = tf.cast(tf.tile(tf.reshape(SFPs_[0],[1,ntheta,nr]),[nnzt,1,1]),prec)\n\t# Broadcast the thetas and ToCos together\n\t#tct = tf.tile(tf.reshape(Thetaijk,[nnzt,1,1]),[1,ntheta,nr])\n\tthetatmp = tf.cast(tf.expand_dims(SFPs_[0], axis=0),prec)\n\ttct = tf.expand_dims(tf.expand_dims(Thetaijk, axis=1), axis=-1)\n\tToCos = tct-thetatmp\n\tTijk = tf.cos(ToCos) # shape: natom3 X ...\n\t# complete factor 1\n\tfac1 = tf.pow(tf.cast(2.0, prec),1.0-zeta)*tf.pow((1.0+Tijk),zeta)\n\trtmp = tf.cast(tf.reshape(SFPs_[1],[1,ntheta,nr]),prec) # ijk X zeta X eta ....\n\tToExp = ((RijRij2+RikRik2)/2.0)\n\ttet = tf.tile(tf.reshape(ToExp,[nnzt,1,1]),[1,ntheta,nr]) - rtmp\n\tfac2 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac3 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac4 = 0.5*(tf.cos(3.14159265359*RikRik2/R_cut)+1.0)\n\t## assemble the full symmetry function for all triples.\n\tfac34t =  tf.tile(tf.reshape(fac3*fac4,[nnzt,1,1]),[1,ntheta,nr])\n\tGm = tf.reshape(fac1*fac2*fac34t,[nnzt*ntheta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t## Finally scatter out the symmetry functions where they belong.\n\t#jk2 = tf.add(tf.multiply(tf.slice(AngtriEle,[0,2],[nnzt,1]), tf.cast(natom, dtype=tf.int64)), tf.slice(AngtriEle,[0,3],[nnzt, 1]))\n\t#mil_jk2 = tf.concat([tf.slice(AngtriEle,[0,0],[nnzt,2]),tf.slice(AngtriEle,[0,4],[nnzt,1]),tf.reshape(jk2,[nnzt,1])],axis=-1)\n\tjk_max = tf.reduce_max(tf.slice(mil_jk2,[0,3], [nnzt, 1])) + 1\n\n\tGm2= tf.reshape(Gm, [nnzt, nsym])\n\tto_reduce2 = tf.scatter_nd(mil_jk2, Gm2, tf.cast([nmol,natom, nelep, tf.cast(jk_max, tf.int32), nsym], dtype=tf.int64))\n#\tmil_jk_Outer2 = tf.tile(tf.reshape(mil_jk2,[nnzt,1,4]),[1,nsym,1])\n#\t## So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n#\tp1_2 = tf.tile(tf.reshape(tf.multiply(tf.cast(tf.range(ntheta), dtype=tf.int64), tf.cast(nr, dtype=tf.int64)),[ntheta,1,1]),[1,nr,1])\n#\tp2_2 = tf.reshape(tf.concat([p1_2,tf.tile(tf.reshape(tf.cast(tf.range(nr), dtype=tf.int64),[1,nr,1]),[ntheta,1,1])],axis=-1),[1,ntheta,nr,2])\n#\tp3_2 = tf.reshape(tf.reduce_sum(p2_2,axis=-1),[1,nsym,1]) # scatter_nd only supports up to rank 5... so gotta smush this...\n#\tp6_2 = tf.tile(p3_2,[nnzt,1,1]) # should be nnz X nsym\n#\tind2 = tf.reshape(tf.concat([mil_jk_Outer2,p6_2],axis=-1),[nnzt*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n#\tto_reduce2 = tf.scatter_nd(ind2,Gm,tf.cast([nmol,natom, nelep, tf.cast(jk_max, tf.int32), nsym], dtype=tf.int64))  # scatter_nd way to do it\n#\t#to_reduce2 = tf.SparseTensor(ind2, Gm, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(natom, tf.int64), tf.cast(nelep, tf.int64), tf.cast(jk_max, tf.int64), tf.cast(nsym, tf.int64)])\n#\t#to_reduce2_reorder = tf.sparse_reorder(to_reduce2)\n#\t#reduced2 = tf.sparse_reduce_sum_sparse(to_reduce2, axis=3)\n#\t#to_reduce2_dense = tf.sparse_tensor_to_dense(to_reduce2, validate_indices=True)\n#\t#to_reduce2_dense = tf.sparse_to_dense(ind2, [tf.cast(nmol, tf.int64), tf.cast(natom, tf.int64), tf.cast(nelep, tf.int64), tf.cast(jk_max, tf.int64), tf.cast(nsym, tf.int64)], Gm)\n#\t#to_reduce2_dense = tf.sparse_to_dense(ind2, [tf.cast(nmol, tf.int64), tf.cast(natom, tf.int64), tf.cast(nelep, tf.int64), tf.cast(natom2, tf.int64), tf.cast(nsym, tf.int64)], Gm, validate_indices=True)\n#\t#return tf.sparse_reduce_sum(to_reduce2, axis=3)\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\t#return tf.sparse_tensor_to_dense(reduced2), ind2\n\n\ndef TFSymASet_Linear_WithEle_UsingList(R, Zs, eleps_, SFPs_, zeta, eta, R_cut, AngtriEle, mil_jk2, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves append ele pair index at the end of triples with\n\tsorted order: m, i, l, j, k\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teleps_: a nelepairs X 2 tensor of element pairs present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 4 X nzeta X neta X thetas X nRs. For example, SFPs_[0,0,0,0,0]\n\t\tis the first zeta parameter. SFPs_[3,0,0,0,1] is the second R parameter.\n\t\tR_cut: Radial Cutoff\n\t\tAngtriEle: angular triples within the cutoff. m, i, j, k, l\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnatom3 = natom*natom2\n\tnelep = tf.shape(eleps_)[0]\n\tpshape = tf.shape(SFPs_)\n\tntheta = pshape[1]\n\tnr = pshape[2]\n\tnsym = ntheta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tonescalar = 1.0 - 0.0000000000000001\n\tnnzt = tf.shape(AngtriEle)[0]\n\n\tnum_elep, num_dim = eleps_.get_shape().as_list()\n\telep_range = tf.cast(tf.range(nelep),dtype=tf.int64)\n\n\tAsym_ByElep = []\n\tfor e in range(num_elep):\n\t\ttomask = tf.equal(AngtriEle[:,4], tf.reshape(elep_range[e], [1,1]))\n\t\tAngtriEle_sub = tf.reshape(tf.boolean_mask(AngtriEle, tf.tile(tf.reshape(tomask,[-1,1]),[1,5])),[-1,5])\n\n\t\ttomask1 = tf.equal(mil_jk2[:,2], tf.reshape(elep_range[e], [1,1]))\n\t\tmil_jk2_sub = tf.reshape(tf.boolean_mask(mil_jk2, tf.tile(tf.reshape(tomask1,[-1,1]),[1,4])),[-1,4])\n\t\tmi_jk2_sub = tf.concat([mil_jk2_sub[:,0:2],  mil_jk2_sub[:,3:]], axis=-1)\n\n\t\tnnzt_sub = tf.shape(AngtriEle_sub)[0]\n\t\tRij_inds = tf.slice(AngtriEle_sub,[0,0],[nnzt_sub,3])\n\t\tRik_inds = tf.concat([tf.slice(AngtriEle_sub,[0,0],[nnzt_sub,2]), tf.slice(AngtriEle_sub,[0,3],[nnzt_sub,1])],axis=-1)\n\t\tRjk_inds = tf.concat([tf.slice(AngtriEle_sub,[0,0],[nnzt_sub,1]), tf.slice(AngtriEle_sub,[0,2],[nnzt_sub,2])],axis=-1)\n\n\t\tRij = DifferenceVectorsLinear(R, Rij_inds)\n\t\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\t\tRik = DifferenceVectorsLinear(R, Rik_inds)\n\t\tRikRik2 = tf.sqrt(tf.reduce_sum(Rik*Rik,axis=1)+infinitesimal)\n\t\tRijRik2 = tf.reduce_sum(Rij*Rik, axis=1)\n\t\tdenom = RijRij2*RikRik2\n\t\t#Mask any troublesome entries.\n\t\tToACos = RijRik2/denom\n\t\tToACos = tf.where(tf.greater_equal(ToACos,1.0),tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\t\tToACos = tf.where(tf.less_equal(ToACos,-1.0),-1.0*tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\t\tThetaijk = tf.acos(ToACos)\n\t\tthetatmp = tf.cast(tf.expand_dims(SFPs_[0], axis=0),prec)\n\t\ttct = tf.expand_dims(tf.expand_dims(Thetaijk, axis=1), axis=-1)\n\t\tToCos = tct-thetatmp\n\t\tTijk = tf.cos(ToCos) # shape: natom3 X ...\n\t\t# complete factor 1\n\t\tfac1 = tf.pow(tf.cast(2.0, prec),1.0-zeta)*tf.pow((1.0+Tijk),zeta)\n\t\trtmp = tf.cast(tf.reshape(SFPs_[1],[1,ntheta,nr]),prec) # ijk X zeta X eta ....\n\t\tToExp = ((RijRij2+RikRik2)/2.0)\n\t\ttet = tf.tile(tf.reshape(ToExp,[nnzt_sub,1,1]),[1,ntheta,nr]) - rtmp\n\t\tfac2 = tf.exp(-eta*tet*tet)\n\t\t# And finally the last two factors\n\t\tfac3 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\t\tfac4 = 0.5*(tf.cos(3.14159265359*RikRik2/R_cut)+1.0)\n\t\t## assemble the full symmetry function for all triples.\n\t\tfac34t =  tf.tile(tf.reshape(fac3*fac4,[nnzt_sub,1,1]),[1,ntheta,nr])\n\t\tGm = tf.reshape(fac1*fac2*fac34t,[nnzt_sub*ntheta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t\tjk_max = tf.reduce_max(tf.slice(mil_jk2_sub,[0,3], [nnzt_sub, 1])) + 1\n\t\tGm2= tf.reshape(Gm, [nnzt_sub, nsym])\n\t\tto_reduce2 = tf.scatter_nd(mi_jk2_sub, Gm2, tf.cast([nmol,natom, tf.cast(jk_max, tf.int32), nsym], dtype=tf.int64))\n\t\tAsym_ByElep.append(tf.reduce_sum(to_reduce2, axis=2))\n\treturn tf.stack(Asym_ByElep, axis=2)\n\n\ndef TFSymASet_Linear_WithElePeriodic(R, Zs, eleps_, SFPs_, zeta, eta, R_cut, AngtriEle, mil_jk2, nreal, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves append ele pair index at the end of triples with\n\tsorted order: m, i, l, j, k\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teleps_: a nelepairs X 2 tensor of element pairs present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 4 X nzeta X neta X thetas X nRs. For example, SFPs_[0,0,0,0,0]\n\t\tis the first zeta parameter. SFPs_[3,0,0,0,1] is the second R parameter.\n\t\tR_cut: Radial Cutoff\n\t\tAngtriEle: angular triples within the cutoff. m, i, j, k, l\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnatom3 = natom*natom2\n\tnelep = tf.shape(eleps_)[0]\n\tpshape = tf.shape(SFPs_)\n\tntheta = pshape[1]\n\tnr = pshape[2]\n\tnsym = ntheta*nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tonescalar = 1.0 - 0.0000000000000001\n\tnnzt = tf.shape(AngtriEle)[0]\n\n\n\tRij_inds = tf.slice(AngtriEle,[0,0],[nnzt,3])\n\tRik_inds = tf.concat([tf.slice(AngtriEle,[0,0],[nnzt,2]), tf.slice(AngtriEle,[0,3],[nnzt,1])],axis=-1)\n\tRjk_inds = tf.concat([tf.slice(AngtriEle,[0,0],[nnzt,1]), tf.slice(AngtriEle,[0,2],[nnzt,2])],axis=-1)\n\n\tRij = DifferenceVectorsLinear(R, Rij_inds)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\tRik = DifferenceVectorsLinear(R, Rik_inds)\n\tRikRik2 = tf.sqrt(tf.reduce_sum(Rik*Rik,axis=1)+infinitesimal)\n\tRijRik2 = tf.reduce_sum(Rij*Rik, axis=1)\n\tdenom = RijRij2*RikRik2\n\t#Mask any troublesome entries.\n\tToACos = RijRik2/denom\n\tToACos = tf.where(tf.greater_equal(ToACos,1.0),tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\tToACos = tf.where(tf.less_equal(ToACos,-1.0),-1.0*tf.ones_like(ToACos, dtype=prec)*onescalar, ToACos)\n\tThetaijk = tf.acos(ToACos)\n\t#thetatmp = tf.cast(tf.tile(tf.reshape(SFPs_[0],[1,ntheta,nr]),[nnzt,1,1]),prec)\n\t# Broadcast the thetas and ToCos together\n\t#tct = tf.tile(tf.reshape(Thetaijk,[nnzt,1,1]),[1,ntheta,nr])\n\tthetatmp = tf.cast(tf.expand_dims(SFPs_[0], axis=0),prec)\n\ttct = tf.expand_dims(tf.expand_dims(Thetaijk, axis=1), axis=-1)\n\tToCos = tct-thetatmp\n\tTijk = tf.cos(ToCos) # shape: natom3 X ...\n\t# complete factor 1\n\tfac1 = tf.pow(tf.cast(2.0, prec),1.0-zeta)*tf.pow((1.0+Tijk),zeta)\n\trtmp = tf.cast(tf.reshape(SFPs_[1],[1,ntheta,nr]),prec) # ijk X zeta X eta ....\n\tToExp = ((RijRij2+RikRik2)/2.0)\n\ttet = tf.reshape(ToExp,[nnzt,1,1]) - rtmp\n\t#tet = tf.tile(tf.reshape(ToExp,[nnzt,1,1]),[1,ntheta,nr]) - rtmp\n\tfac2 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac3 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac4 = 0.5*(tf.cos(3.14159265359*RikRik2/R_cut)+1.0)\n\t## assemble the full symmetry function for all triples.\n\tfac34t = tf.reshape(fac3*fac4,[nnzt,1,1])\n\tGm = tf.reshape(fac1*fac2*fac34t,[nnzt, nsym]) # nnz X nzeta X neta X ntheta X nr\n\t#fac34t =  tf.tile(tf.reshape(fac3*fac4,[nnzt,1,1]),[1,ntheta,nr])\n\t#Gm = tf.reshape(fac1*fac2*fac34t,[nnzt*ntheta*nr]) # nnz X nzeta X neta X ntheta X nr\n\t## Finally scatter out the symmetry functions where they belong.\n\t#jk2 = tf.add(tf.multiply(tf.slice(AngtriEle,[0,2],[nnzt,1]), tf.cast(natom, dtype=tf.int64)), tf.slice(AngtriEle,[0,3],[nnzt, 1]))\n\t#mil_jk2 = tf.concat([tf.slice(AngtriEle,[0,0],[nnzt,2]),tf.slice(AngtriEle,[0,4],[nnzt,1]),tf.reshape(jk2,[nnzt,1])],axis=-1)\n\tjk_max = tf.reduce_max(tf.slice(mil_jk2,[0,3], [nnzt, 1])) + 1\n\n\tGm2= tf.reshape(Gm, [nnzt, nsym])\n\tto_reduce2 = tf.scatter_nd(mil_jk2, Gm2, tf.cast([nmol,tf.cast(nreal, tf.int32), nelep, tf.cast(jk_max, tf.int32), nsym], dtype=tf.int64))\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\ndef TFCoulomb(R, Qs, R_cut, Radpair, prec=tf.float64):\n\t""""""\n\tTensorflow implementation of sparse-coulomb\n\tMadelung energy build.\n\n\tArgs:\n\t    R: a nmol X maxnatom X 3 tensor of coordinates.\n\t    Qs : nmol X maxnatom X 1 tensor of atomic charges.\n\t    R_cut: Radial Cutoff\n\t    Radpair: None zero pairs X 3 tensor (mol, i, j)\n\t    prec: a precision.\n\tReturns:\n\t    Digested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=-1)+infinitesimal)\n\t# Grab the Q\'s.\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\t# Finish the Kernel.\n\tKern = Qi*Qj/RijRij2\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tsparse_index =tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, Kern, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\tE_ee = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\treturn E_ee\n\n\ndef TFCoulombCosLR(R, Qs, R_cut, Radpair, prec=tf.float64):\n\t""""""\n\tTensorflow implementation of long range cutoff sparse-coulomb\n\tMadelung energy build.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tQs : nmol X maxnatom X 1 tensor of atomic charges.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\t# Generate LR cutoff Matrix\n\tCut = (1.0-0.5*(tf.cos(RijRij2*Pi/R_cut)+1.0))\n\t# Grab the Q\'s.\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\t# Finish the Kernel.\n\tKern = Qi*Qj/RijRij2*Cut\n\t# Scatter Back\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tsparse_index =tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, Kern, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\tE_ee = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\treturn E_ee\n\n\ndef TFCoulombPolyLR(R, Qs, R_cut, Radpair, prec=tf.float64):\n\t""""""\n\tTensorflow implementation of short range cutoff sparse-coulomb\n\tMadelung energy build. Using switch function 1+x^2(2x-3) in http://pubs.acs.org/doi/ipdf/10.1021/ct501131j\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tQs : nmol X maxnatom X 1 tensor of atomic charges.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tR_width = PARAMS[""Poly_Width""]*BOHRPERA\n\tR_begin = R_cut\n\tR_end =  R_cut+R_width\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\tt = (RijRij2 - R_begin)/R_width\n\tCut_step1  = tf.where(tf.greater(t, 0.0), -t*t*(2.0*t-3.0), tf.zeros_like(t))\n\tCut = tf.where(tf.greater(t, 1.0), tf.ones_like(t), Cut_step1)\n\t# Grab the Q\'s.\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\t# Finish the Kernel.\n\tKern = Qi*Qj/RijRij2*Cut\n\t# Scatter Back\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tsparse_index =tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, Kern, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\tE_ee = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\treturn E_ee\n\n\ndef TFCoulombPolyLRSR(R, Qs, R_cut, Radpair, prec=tf.float64):\n\t""""""\n\tTensorflow implementation of short range and long range cutoff sparse-coulomb\n\tMadelung energy build. Using switch function 1+x^2(2x-3) in http://pubs.acs.org/doi/ipdf/10.1021/ct501131j\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tQs : nmol X maxnatom X 1 tensor of atomic charges.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tR_width = PARAMS[""Poly_Width""]*BOHRPERA\n\tR_begin = R_cut\n\tR_end =  R_cut+R_width\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\tt = (RijRij2 - R_begin)/R_width\n\tCut_step1  = tf.where(tf.greater(t, 0.0), -t*t*(2.0*t-3.0), tf.zeros_like(t))\n\tCut = tf.where(tf.greater(t, 1.0), tf.ones_like(t), Cut_step1)\n\n\tR_off = PARAMS[""EECutoffOff""]*BOHRPERA\n\tt_off = (RijRij2 - (R_off-R_width))/R_width\n\tCut_off_step1  = tf.where(tf.greater(t_off, 0.0), 1+t_off*t_off*(2.0*t_off-3.0), tf.ones_like(t_off))\n\tCut_off  = tf.where(tf.greater(t_off, 1.0), tf.zeros_like(t), Cut_off_step1)\n\t# Grab the Q\'s.\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\t# Finish the Kernel.\n\tKern = Qi*Qj/RijRij2*Cut*Cut_off\n\t# Scatter Back\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tsparse_index =tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, Kern, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\tE_ee = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\treturn E_ee\n\ndef TFCoulombEluSRDSFLR(R, Qs, R_cut, Radpair, alpha, elu_a, elu_shift, prec=tf.float64):\n\t""""""\n\tA tensorflow linear scaling implementation of the Damped Shifted Electrostatic Force with short range cutoff with elu function (const at short range).\n\thttp://aip.scitation.org.proxy.library.nd.edu/doi/pdf/10.1063/1.2206581\n\tBatched over molecules.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tQs : nmol X maxnatom X 1 tensor of atomic charges.\n\t\tR_srcut: Short Range Erf Cutoff\n\t\tR_lrcut: Long Range DSF Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\talpha: DSF alpha parameter (~0.2)\n\tReturns\n\t\tEnergy of  Mols\n\t""""""\n\talpha = alpha/BOHRPERA\n\tR_lrcut = PARAMS[""EECutoffOff""]*BOHRPERA\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\n\tSR_sub = tf.where(tf.greater(RijRij2, R_cut), elu_a*(RijRij2-R_cut)+elu_shift, elu_a*(tf.exp(RijRij2-R_cut)-1.0)+elu_shift)\n\n\ttwooversqrtpi = tf.constant(1.1283791671,dtype=tf.float64)\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\t# Gather desired LJ parameters.\n\tQij = Qi*Qj\n\t# This is Dan\'s Equation (18)\n\tXX = alpha*R_lrcut\n\tZZ = tf.erfc(XX)/R_lrcut\n\tYY = twooversqrtpi*alpha*tf.exp(-XX*XX)/R_lrcut\n\tLR = Qij*(tf.erfc(alpha*RijRij2)/RijRij2 - ZZ + (RijRij2-R_lrcut)*(ZZ/R_lrcut+YY))\n\tLR= tf.where(tf.is_nan(LR), tf.zeros_like(LR), LR)\n\tLR = tf.where(tf.greater(RijRij2,R_lrcut), tf.zeros_like(LR), LR)\n\n\tSR = Qij*SR_sub\n\n\tK = tf.where(tf.greater(RijRij2, R_cut), LR, SR)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\tsparse_index = tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, K, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\t# Now use the sparse reduce sum trick to scatter this into mols.\n\treturn tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\ndef TFVdwPolyLR(R, Zs, eles, c6, R_vdw, R_cut, Radpair, prec=tf.float64):\n\t""""""\n\tTensorflow implementation of short range cutoff sparse-coulomb\n\tMadelung energy build. Using switch function 1+x^2(2x-3) in http://pubs.acs.org/doi/ipdf/10.1021/ct501131j\n\tdamping function in http://pubs.rsc.org/en/content/articlepdf/2008/cp/b810189b is used.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tc6 : nele. Grimmer C6 coff in a.u.\n\t\tR_vdw: nele. Grimmer vdw radius in a.u.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tR = tf.multiply(R, BOHRPERA)\n\tR_width = PARAMS[""Poly_Width""]*BOHRPERA\n\tR_begin = R_cut\n\tR_end =  R_cut+R_width\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles)[0]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\n\tt = (RijRij2 - R_begin)/R_width\n\tCut_step1  = tf.where(tf.greater(t, 0.0), -t*t*(2.0*t-3.0), tf.zeros_like(t))\n\tCut = tf.where(tf.greater(t, 1.0), tf.ones_like(t), Cut_step1)\n\n\tZAll = AllDoublesSet(Zs, prec=tf.int64)\n\tZPairs1 = tf.slice(ZAll,[0,0,0,1],[nmol,natom,natom,1])\n\tZPairs2 = tf.slice(ZAll,[0,0,0,2],[nmol,natom,natom,1])\n\tRi=tf.gather_nd(ZPairs1, Radpair)\n\tRl=tf.gather_nd(ZPairs2, Radpair)\n\tElemIndex_i = tf.slice(tf.where(tf.equal(Ri, tf.reshape(eles, [1,nele]))),[0,1],[nnz,1])\n\tElemIndex_j = tf.slice(tf.where(tf.equal(Rl, tf.reshape(eles, [1,nele]))),[0,1],[nnz,1])\n\n\tc6_i=tf.gather_nd(c6, ElemIndex_i)\n\tc6_j=tf.gather_nd(c6, ElemIndex_j)\n\tRvdw_i = tf.gather_nd(R_vdw, ElemIndex_i)\n\tRvdw_j = tf.gather_nd(R_vdw, ElemIndex_j)\n\tKern = -Cut*tf.sqrt(c6_i*c6_j)/tf.pow(RijRij2,6.0)*1.0/(1.0+6.0*tf.pow(RijRij2/(Rvdw_i+Rvdw_j),-12.0))\n\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tsparse_index =tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, Kern, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\tE_vdw = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\treturn E_vdw\n\ndef TFVdwPolyLRWithEle(R, Zs, eles, c6, R_vdw, R_cut, Radpair_E1E2, prec=tf.float64):\n\t""""""\n\tTensorflow implementation of short range cutoff sparse-coulomb\n\tMadelung energy build. Using switch function 1+x^2(2x-3) in http://pubs.acs.org/doi/ipdf/10.1021/ct501131j\n\tdamping function in http://pubs.rsc.org/en/content/articlepdf/2008/cp/b810189b is used.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tc6 : nele. Grimmer C6 coff in a.u.\n\t\tR_vdw: nele. Grimmer vdw radius in a.u.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tRadpair = Radpair_E1E2[:,:3]\n\tR = tf.multiply(R, BOHRPERA)\n\tR_width = PARAMS[""Poly_Width""]*BOHRPERA\n\tR_begin = R_cut\n\tR_end =  R_cut+R_width\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles)[0]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\n\tt = (RijRij2 - R_begin)/R_width\n\tCut_step1  = tf.where(tf.greater(t, 0.0), -t*t*(2.0*t-3.0), tf.zeros_like(t))\n\tCut = tf.where(tf.greater(t, 1.0), tf.ones_like(t), Cut_step1)\n\n\tElemIndex_i = tf.reshape(Radpair_E1E2[:,3],[nnz, 1])\n\tElemIndex_j = tf.reshape(Radpair_E1E2[:,4],[nnz, 1])\n\n\tc6_i=tf.gather_nd(c6, ElemIndex_i)\n\tc6_j=tf.gather_nd(c6, ElemIndex_j)\n\tRvdw_i = tf.gather_nd(R_vdw, ElemIndex_i)\n\tRvdw_j = tf.gather_nd(R_vdw, ElemIndex_j)\n\tKern = -Cut*tf.sqrt(c6_i*c6_j)/tf.pow(RijRij2,6.0)*1.0/(1.0+6.0*tf.pow(RijRij2/(Rvdw_i+Rvdw_j),-12.0))\n\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tsparse_index =tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, Kern, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\tE_vdw = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\treturn E_vdw\n\ndef PolynomialRangeSepCoulomb(R,Qs,Radpair,SRRc,LRRc,dx):\n\t""""""\n\tA tensorflow linear scaling implementation of a short-range and long range cutoff\n\tcoulomb kernel. The cutoff functions are polynomials subject to the constraint\n\tthat 1/r is brought to 0 twice-differentiably at SR and LR+dx cutoffs.\n\n\tThe SR cutoff polynomial is 4th order, and the LR is fifth.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tQs : nmol X maxnatom X 1 tensor of atomic charges.\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tSRRc: Distance where SR polynomial ends.\n\t\tLRRc: Distance where LR polynomial begins.\n\t\tdx: Small interval after which the kernel is zero.\n\tReturns\n\t\tA #Mols X MaxNAtoms X MaxNAtoms matrix of LJ kernel contributions.\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tDs = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\ttwooversqrtpi = tf.constant(1.1283791671,dtype=tf.float64)\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\tQij = Qi*Qj\n\tD2 = Ds*Ds\n\tD3 = D2*Ds\n\tD4 = D3*Ds\n\tD5 = D4*Ds\n\n\tasr = -5./(3.*tf.pow(SRRc,4.0))\n\tdsr = 5./(3.*SRRc)\n\tcsr = 1./(tf.pow(SRRc,5.0))\n\n\tx0 = LRRc\n\tx02 = x0*x0\n\tx03 = x02*x0\n\tx04 = x03*x0\n\tx05 = x04*x0\n\n\tdx2 = dx*dx\n\tdx3 = dx2*dx\n\tdx4 = dx3*dx\n\tdx5 = dx4*dx\n\n\talr = -((3.*(dx4+2.*dx3*x0-4.*dx2*x02+10.*dx*x03+20.*x04))/(dx5*x03))\n\tblr = -((-dx5-9*dx4*x0+8.*dx2*x03-60.0*dx*x04-60.0*x05)/(dx5*x03))\n\tclr = (3.*(dx3-dx2*x0+10.*x03))/(dx5*x03)\n\tdlr = -((3.*(dx5+3.*dx4*x0-2.*dx3*x02+dx2*x03+15.*dx*x04+10.*x05))/(dx5*x02))\n\telr = (3.*(dx5+dx4*x0-dx3*x02+dx2*x03+4.*dx*x04+2.*x05))/(dx5*x0)\n\tflr = -((dx2-3.*dx*x0+6.*x02)/(dx5*x03))\n\n\tCK = (Qij/Ds)\n\tSRK = Qij*(asr*D3+csr*D4+dsr)\n\tLRK = Qij*(alr*D3 + blr*D2 + dlr*Ds + clr*D4 + elr + flr*D5)\n\tZK = tf.zeros_like(Ds)\n\n\tK0 = tf.where(tf.less_equal(Ds,SRRc),SRK,CK)\n\tK1 = tf.where(tf.greater_equal(Ds,LRRc),LRK,K0)\n\tK = tf.where(tf.greater_equal(Ds,LRRc+dx),ZK,K1)\n\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\tsparse_index = tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, K, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\t# Now use the sparse reduce sum trick to scatter this into mols.\n\treturn tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\ndef TFCoulombErfLR(R, Qs, R_cut,  Radpair, prec=tf.float64):\n\t""""""\n\tTensorflow implementation of long range cutoff sparse-Erf\n\tMadelung energy build.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tQs : nmol X maxnatom X 1 tensor of atomic charges.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tR_width = PARAMS[""Erf_Width""]*BOHRPERA\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\t# Generate LR cutoff Matrix\n\tCut = (1.0 + tf.erf((RijRij2 - R_cut)/R_width))*0.5\n\t# Grab the Q\'s.\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\t# Finish the Kernel.\n\tKern = Qi*Qj/RijRij2*Cut\n\t# Scatter Back\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tsparse_index =tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, Kern, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\tE_ee = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\treturn E_ee\n\n\ndef TFCoulombErfSRDSFLR(R, Qs, R_srcut, R_lrcut, Radpair, alpha, prec=tf.float64):\n\t""""""\n\tA tensorflow linear scaling implementation of the Damped Shifted Electrostatic Force with short range cutoff\n\thttp://aip.scitation.org.proxy.library.nd.edu/doi/pdf/10.1063/1.2206581\n\tBatched over molecules.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tQs : nmol X maxnatom X 1 tensor of atomic charges.\n\t\tR_srcut: Short Range Erf Cutoff\n\t\tR_lrcut: Long Range DSF Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\talpha: DSF alpha parameter (~0.2)\n\tReturns\n\t\tEnergy of  Mols\n\t""""""\n\talpha = alpha/BOHRPERA\n\tR_width = PARAMS[""Erf_Width""]*BOHRPERA\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\tCut = (1.0 + tf.erf((RijRij2 - R_srcut)/R_width))*0.5\n\n\ttwooversqrtpi = tf.constant(1.1283791671,dtype=tf.float64)\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\t# Gather desired LJ parameters.\n\tQij = Qi*Qj\n\t# This is Dan\'s Equation (18)\n\tXX = alpha*R_lrcut\n\tZZ = tf.erfc(XX)/R_lrcut\n\tYY = twooversqrtpi*alpha*tf.exp(-XX*XX)/R_lrcut\n\tK = Qij*(tf.erfc(alpha*RijRij2)/RijRij2 - ZZ + (RijRij2-R_lrcut)*(ZZ/R_lrcut+YY))*Cut\n\tK = tf.where(tf.is_nan(K),tf.zeros_like(K),K)\n\trange_index = tf.range(tf.cast(nnz, tf.int64), dtype=tf.int64)\n\tmol_index = tf.cast(tf.reshape(tf.slice(Radpair,[0,0],[-1,1]),[nnz]), dtype=tf.int64)\n\tsparse_index = tf.stack([mol_index, range_index], axis=1)\n\tsp_atomoutputs = tf.SparseTensor(sparse_index, K, dense_shape=[tf.cast(nmol, tf.int64), tf.cast(nnz, tf.int64)])\n\t# Now use the sparse reduce sum trick to scatter this into mols.\n\treturn tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\n\ndef TFSymRSet_Linear(R, Zs, eles_, SFPs_, eta, R_cut, Radpair, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnr = pshape[1]\n\tnsym = nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\t#Rtmp = tf.concat([tf.slice(Radpair,[0,0],[nnz,1]), tf.slice(Radpair,[0,2],[nnz,1])], axis=-1)\n\t#Rreverse = tf.concat([Rtmp, tf.slice(Radpair,[0,1],[nnz,1])], axis=-1)\n\t#Rboth = tf.concat([Radpair, Rreverse], axis=0)\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\tZAll = AllDoublesSet(Zs, prec=tf.int64)\n\tZPairs = tf.slice(ZAll,[0,0,0,2],[nmol,natom,natom,1])\n\tRl=tf.gather_nd(ZPairs, Radpair)\n\tElemIndex = tf.slice(tf.where(tf.equal(Rl, tf.reshape(eles_,[1,nele]))),[0,1],[nnz,1])\n\tGoodInds2 = tf.concat([Radpair, ElemIndex], axis=-1)\n\n\trtmp = tf.cast(tf.reshape(SFPs_[0],[1,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij2,[nnz,1]),[1,nr]) - rtmp\n\tfac1 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac2 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz,1]),[1,nr])\n\t## assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t,[nnz*nr]) # nnz X nzeta X neta X ntheta X nr\n\t## Finally scatter out the symmetry functions where they belong.\n\tmil_j = tf.concat([tf.slice(GoodInds2,[0,0],[nnz,2]),tf.slice(GoodInds2,[0,3],[nnz,1]),tf.slice(GoodInds2,[0,2],[nnz,1])],axis=-1)\n\tmil_j_Outer = tf.tile(tf.reshape(mil_j,[nnz,1,4]),[1,nsym,1])\n\t## So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tp2_2 = tf.reshape(tf.reshape(tf.cast(tf.range(nr), dtype=tf.int64),[nr,1]),[1,nr,1])\n\tp4_2 = tf.tile(p2_2,[nnz,1,1]) # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_j_Outer,p4_2],axis=-1),[nnz*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\tto_reduce2 = tf.scatter_nd(ind2,Gm,tf.cast([nmol,natom,nele,natom,nsym], dtype=tf.int64))\n\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\n\ndef TFSymRSet_Linear_WithEle(R, Zs, eles_, SFPs_, eta, R_cut, RadpairEle, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version appends the element type (by its index in eles_) in RadpairEle, and it is sorted by m,i,l,j\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tRadpairEle: None zero pairs X 4 tensor (mol, i, j, l)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnr = pshape[1]\n\tnsym = nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(RadpairEle)[0]\n\t#Rtmp = tf.concat([tf.slice(Radpair,[0,0],[nnz,1]), tf.slice(Radpair,[0,2],[nnz,1])], axis=-1)\n\t#Rreverse = tf.concat([Rtmp, tf.slice(Radpair,[0,1],[nnz,1])], axis=-1)\n\t#Rboth = tf.concat([Radpair, Rreverse], axis=0)\n\tRij = DifferenceVectorsLinear(R, tf.slice(RadpairEle,[0, 0],[nnz, 3]))\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\t#GoodInds2 = tf.concat([Radpair, ElemIndex], axis=-1)\n\n\trtmp = tf.cast(tf.reshape(SFPs_[0],[1,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij2,[nnz,1]),[1,nr]) - rtmp\n\tfac1 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac2 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz,1]),[1,nr])\n\t## assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t,[nnz*nr]) # nnz X nzeta X neta X ntheta X nr\n\tGm2 = tf.reshape(Gm, [nnz, nr])\n\t## Finally scatter out the symmetry functions where they belong.\n\tmil_j = tf.concat([tf.slice(RadpairEle,[0,0],[nnz,2]),tf.slice(RadpairEle,[0,3],[nnz,1]),tf.slice(RadpairEle,[0,2],[nnz,1])],axis=-1)\n\t#mil_j_Outer = tf.tile(tf.reshape(mil_j,[nnz,1,4]),[1,nsym,1])\n\n\tto_reduce2 = tf.scatter_nd(mil_j, Gm2, tf.cast([nmol,natom,nele,natom,nsym], dtype=tf.int64))\n#\t## So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n#\tp2_2 = tf.reshape(tf.reshape(tf.cast(tf.range(nr), dtype=tf.int64),[nr,1]),[1,nr,1])\n#\tp4_2 = tf.tile(p2_2,[nnz,1,1]) # should be nnz X nsym\n#\tind2 = tf.reshape(tf.concat([mil_j_Outer,p4_2],axis=-1),[nnz*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n#\tto_reduce2 = tf.scatter_nd(ind2,Gm,tf.cast([nmol,natom,nele,natom,nsym], dtype=tf.int64))\n#\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\ndef TFSymRSet_Linear_WithEle_Release(R, Zs, eles_, SFPs_, eta, R_cut, RadpairEle, mil_j, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version appends the element type (by its index in eles_) in RadpairEle, and it is sorted by m,i,l,j\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tRadpairEle: None zero pairs X 4 tensor (mol, i, j, l)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnr = pshape[1]\n\tnsym = nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(RadpairEle)[0]\n\t#Rtmp = tf.concat([tf.slice(Radpair,[0,0],[nnz,1]), tf.slice(Radpair,[0,2],[nnz,1])], axis=-1)\n\t#Rreverse = tf.concat([Rtmp, tf.slice(Radpair,[0,1],[nnz,1])], axis=-1)\n\t#Rboth = tf.concat([Radpair, Rreverse], axis=0)\n\tRij = DifferenceVectorsLinear(R, tf.slice(RadpairEle,[0, 0],[nnz, 3]))\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\t#GoodInds2 = tf.concat([Radpair, ElemIndex], axis=-1)\n\n\trtmp = tf.cast(tf.reshape(SFPs_[0],[1,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij2,[nnz,1]),[1,nr]) - rtmp\n\tfac1 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac2 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz,1]),[1,nr])\n\t## assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t,[nnz*nr]) # nnz X nzeta X neta X ntheta X nr\n\tGm2 = tf.reshape(Gm, [nnz, nr])\n\t## Finally scatter out the symmetry functions where they belong.\n\tj_max = tf.reduce_max(tf.slice(mil_j, [0,3], [nnz, 1])) + 1\n\tto_reduce2 = tf.scatter_nd(mil_j, Gm2, tf.cast([nmol, tf.cast(natom, tf.int32), nele, tf.cast(j_max, tf.int32), nsym], dtype=tf.int64))\n\t#mil_j = tf.concat([tf.slice(RadpairEle,[0,0],[nnz,2]),tf.slice(RadpairEle,[0,3],[nnz,1]),tf.slice(RadpairEle,[0,2],[nnz,1])],axis=-1)\n\t#to_reduce2 = tf.scatter_nd(mil_j, Gm2, tf.cast([nmol,natom,nele,natom,nsym], dtype=tf.int64))\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\n\ndef TFSymRSet_Linear_WithElePeriodic(R, Zs, eles_, SFPs_, eta, R_cut, RadpairEle, mil_j, nreal, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version appends the element type (by its index in eles_) in RadpairEle, and it is sorted by m,i,l,j\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tRadpairEle: None zero pairs X 4 tensor (mol, i, j, l)\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnr = pshape[1]\n\tnsym = nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(RadpairEle)[0]\n\t#Rtmp = tf.concat([tf.slice(Radpair,[0,0],[nnz,1]), tf.slice(Radpair,[0,2],[nnz,1])], axis=-1)\n\t#Rreverse = tf.concat([Rtmp, tf.slice(Radpair,[0,1],[nnz,1])], axis=-1)\n\t#Rboth = tf.concat([Radpair, Rreverse], axis=0)\n\tRij = DifferenceVectorsLinear(R, tf.slice(RadpairEle,[0, 0],[nnz, 3]))\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\t#GoodInds2 = tf.concat([Radpair, ElemIndex], axis=-1)\n\n\trtmp = tf.cast(tf.reshape(SFPs_[0],[1,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij2,[nnz,1]),[1,nr]) - rtmp\n\tfac1 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac2 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz,1]),[1,nr])\n\t## assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t,[nnz*nr]) # nnz X nzeta X neta X ntheta X nr\n\tGm2 = tf.reshape(Gm, [nnz, nr])\n\t## Finally scatter out the symmetry functions where they belong.\n\tj_max = tf.reduce_max(tf.slice(mil_j, [0,3], [nnz, 1])) + 1\n\t#mil_j_Outer = tf.tile(tf.reshape(mil_j,[nnz,1,4]),[1,nsym,1])\n\tto_reduce2 = tf.scatter_nd(mil_j, Gm2, tf.cast([nmol, tf.cast(nreal, tf.int32), nele, tf.cast(j_max, tf.int32), nsym], dtype=tf.int64))\n\treturn tf.reduce_sum(to_reduce2, axis=3)\n\ndef TFSymRSet_Linear_Qs(R, Zs, eles_, SFPs_, eta, R_cut, Radpair, Qs, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tQs: charge of each atom. nmol X maxnatom\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnatom2 = natom*natom\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnr = pshape[1]\n\tnsym = nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\t#Rtmp = tf.concat([tf.slice(Radpair,[0,0],[nnz,1]), tf.slice(Radpair,[0,2],[nnz,1])], axis=-1)\n\t#Rreverse = tf.concat([Rtmp, tf.slice(Radpair,[0,1],[nnz,1])], axis=-1)\n\t#Rboth = tf.concat([Radpair, Rreverse], axis=0)\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\tZAll = AllDoublesSet(Zs, prec=tf.int64)\n\tZPairs = tf.slice(ZAll,[0,0,0,2],[nmol,natom,natom,1])\n\tRl=tf.gather_nd(ZPairs, Radpair)\n\tElemIndex = tf.slice(tf.where(tf.equal(Rl, tf.reshape(eles_,[1,nele]))),[0,1],[nnz,1])\n\tGoodInds2 = tf.concat([Radpair, ElemIndex], axis=-1)\n\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\tQit = tf.tile(tf.reshape(Qi,[nnz,1]),[1, nr])\n\tQjt = tf.tile(tf.reshape(Qj,[nnz,1]),[1, nr])\n\n\trtmp = tf.cast(tf.reshape(SFPs_[0],[1,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij2,[nnz,1]),[1,nr]) - rtmp\n\tfac1 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac2 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz,1]),[1,nr])\n\t## assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t*Qit*Qjt,[nnz*nr]) # nnz X nzeta X neta X ntheta X nr\n\n\t## Finally scatter out the symmetry functions where they belong.\n\tmil_j = tf.concat([tf.slice(GoodInds2,[0,0],[nnz,2]),tf.slice(GoodInds2,[0,3],[nnz,1]),tf.slice(GoodInds2,[0,2],[nnz,1])],axis=-1)\n\tmil_j_Outer = tf.tile(tf.reshape(mil_j,[nnz,1,4]),[1,nsym,1])\n\t## So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tp2_2 = tf.reshape(tf.reshape(tf.cast(tf.range(nr), dtype=tf.int64),[nr,1]),[1,nr,1])\n\tp4_2 = tf.tile(p2_2,[nnz,1,1]) # should be nnz X nsym\n\tind2 = tf.reshape(tf.concat([mil_j_Outer,p4_2],axis=-1),[nnz*nsym,5]) # This is now nnz*nzeta*neta*ntheta*nr X 8 -  m,i,l,jk,zeta,eta,theta,r\n\tto_reduce2 = tf.scatter_nd(ind2,Gm,tf.cast([nmol,natom,nele,natom,nsym], dtype=tf.int64))\n\t#to_reduce2 = tf.sparse_to_dense(ind2, tf.convert_to_tensor([nmol, natom, nelep, natom2, nsym]), Gm)\n\t#to_reduce_sparse = tf.SparseTensor(ind2,[nmol, natom, nelep, natom2, nzeta, neta, ntheta, nr])\n\treturn tf.reduce_sum(tf.reduce_sum(to_reduce2, axis=3), axis=2)\n\n\n\ndef TFSymRSet_Linear_Qs_Periodic(R, Zs, eles_, SFPs_, eta, R_cut, Radpair, Qs, mil_j, nreal, prec=tf.float64):\n\t""""""\n\tA tensorflow implementation of the angular AN1 symmetry function for a single input molecule.\n\tHere j,k are all other atoms, but implicitly the output\n\tis separated across elements as well. eleps_ is a list of element pairs\n\tG = 2**(1-zeta) \\sum_{j,k \\neq i} (Angular triple) (radial triple) f_c(R_{ij}) f_c(R_{ik})\n\ta-la MolEmb.cpp. Also depends on PARAMS for zeta, eta, theta_s r_s\n\tThis version improves on the previous by avoiding some\n\theavy tiles.\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a nelepairs X 1 tensor of elements present in the data.\n\t\tSFP: A symmetry function parameter tensor having the number of elements\n\t\tas the SF output. 2 X neta  X nRs.\n\t\tR_cut: Radial Cutoff\n\t\tRadpair: None zero pairs X 3 tensor (mol, i, j)\n\t\tQs: charge of each atom. nmol X maxnatom\n\t\tprec: a precision.\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X nelepairs X nZeta X nEta X nThetas X nRs\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnele = tf.shape(eles_)[0]\n\tpshape = tf.shape(SFPs_)\n\tnr = pshape[1]\n\tnsym = nr\n\tinfinitesimal = 0.000000000000000000000000001\n\tnnz = tf.shape(Radpair)[0]\n\t#Rtmp = tf.concat([tf.slice(Radpair,[0,0],[nnz,1]), tf.slice(Radpair,[0,2],[nnz,1])], axis=-1)\n\t#Rreverse = tf.concat([Rtmp, tf.slice(Radpair,[0,1],[nnz,1])], axis=-1)\n\t#Rboth = tf.concat([Radpair, Rreverse], axis=0)\n\tRij = DifferenceVectorsLinear(R, Radpair)\n\tRijRij2 = tf.sqrt(tf.reduce_sum(Rij*Rij,axis=1)+infinitesimal)\n\n\tQii = tf.slice(Radpair,[0,0],[-1,2])\n\tQji = tf.concat([tf.slice(Radpair,[0,0],[-1,1]),tf.slice(Radpair,[0,2],[-1,1])], axis=-1)\n\tQi = tf.gather_nd(Qs,Qii)\n\tQj = tf.gather_nd(Qs,Qji)\n\tQit = tf.tile(tf.reshape(Qi,[nnz,1]),[1, nr])\n\tQjt = tf.tile(tf.reshape(Qj,[nnz,1]),[1, nr])\n\n\trtmp = tf.cast(tf.reshape(SFPs_[0],[1,nr]),prec) # ijk X zeta X eta ....\n\ttet = tf.tile(tf.reshape(RijRij2,[nnz,1]),[1,nr]) - rtmp\n\tfac1 = tf.exp(-eta*tet*tet)\n\t# And finally the last two factors\n\tfac2 = 0.5*(tf.cos(3.14159265359*RijRij2/R_cut)+1.0)\n\tfac2t = tf.tile(tf.reshape(fac2,[nnz,1]),[1,nr])\n\t## assemble the full symmetry function for all triples.\n\tGm = tf.reshape(fac1*fac2t*Qit*Qjt,[nnz, nr]) # nnz X nzeta X neta X ntheta X nr\n\t## Finally scatter out the symmetry functions where they belong.\n\t## So the above is Mol, i, l... now must outer nzeta,neta,ntheta,nr to finish the indices.\n\tj_max = tf.reduce_max(tf.slice(mil_j, [0,3], [nnz, 1])) + 1\n\tto_reduce2 = tf.scatter_nd(mil_j, Gm, tf.cast([nmol, tf.cast(nreal, tf.int32), nele, tf.cast(j_max, tf.int32), nsym], dtype=tf.int64))\n\treturn tf.reduce_sum(tf.reduce_sum(to_reduce2, axis=3), axis=2)\n\n\ndef TFSymSet(R, Zs, eles_, SFPsR_, Rr_cut, eleps_, SFPsA_, Ra_cut):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet(R, Zs, eles_, SFPsR_, Rr_cut),[nmol, natom, -1])\n\tGMA = tf.reshape(TFSymASet(R, Zs, eleps_, SFPsA_, Ra_cut),[nmol, natom, -1])\n\tGM = tf.concat([GMR, GMA], axis=2)\n\treturn GM\n\ndef TFSymSet_Scattered(R, Zs, eles_, SFPsR_, Rr_cut, eleps_, SFPsA_, Ra_cut):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet(R, Zs, eles_, SFPsR_, Rr_cut),[nmol, natom, -1])\n\tGMA = tf.reshape(TFSymASet(R, Zs, eleps_, SFPsA_, Ra_cut),[nmol, natom, -1])\n\tGM = tf.concat([GMR, GMA], axis=2)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]))\n\tToMask = AllSinglesSet(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]))\n\tIndexList = []\n\tSymList=[]\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tSymList.append(tf.gather_nd(GM, GatherList[-1]))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tIndexList.append(tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle]))\n\treturn SymList, IndexList\n\ndef TFSymSet_Scattered_Update(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_, SFPsA_, Ra_cut):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet_Update(R, Zs, eles_, SFPsR_, Rr_cut), [nmol, natom, -1])\n\tGMA = tf.reshape(TFSymASet_Update(R, Zs, eleps_, SFPsA_, Ra_cut), [nmol, natom, -1])\n\tGM = tf.concat([GMR, GMA], axis=2)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]))\n\tToMask = AllSinglesSet(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]))\n\tIndexList = []\n\tSymList=[]\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tSymList.append(tf.gather_nd(GM, GatherList[-1]))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tIndexList.append(tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle]))\n\treturn SymList, IndexList\n\n\ndef TFSymSet_Scattered_Update2(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_, SFPsA_, zeta, eta, Ra_cut):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet_Update2(R, Zs, eles_, SFPsR_, eta, Rr_cut), [nmol, natom, -1])\n\tGMA = tf.reshape(TFSymASet_Update2(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut), [nmol, natom, -1])\n\tGM = tf.concat([GMR, GMA], axis=2)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]))\n\tToMask = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tIndexList = []\n\tSymList=[]\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tSymList.append(tf.gather_nd(GM, GatherList[-1]))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tIndexList.append(tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle]))\n\treturn SymList, IndexList\n\ndef TFSymSet_Scattered_Update_Scatter(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_, SFPsA_, zeta, eta, Ra_cut):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tThis also selects out which of the atoms will contribute to the BP energy on the\n\tbasis of whether the atomic number is treated in the \'element types to do list.\'\n\taccording to kun? (Trusted source?)\n\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet_Update2(R, Zs, eles_, SFPsR_, eta, Rr_cut), [nmol, natom, -1])\n\tGMA = tf.reshape(TFSymASet_Update2(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut), [nmol, natom, -1])\n\tGM = tf.concat([GMR, GMA], axis=2)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]))\n\tToMask1 = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tv = tf.cast(tf.reshape(tf.range(nmol*natom), [nmol, natom, 1]), dtype=tf.int64)\n\tToMask = tf.concat([ToMask1, v], axis = -1)\n\tIndexList = []\n\tSymList= []\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tSymList.append(tf.gather_nd(GM, tf.slice(GatherList[-1],[0,0],[NAtomOfEle,2])))\n\t\tmol_index = tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tatom_index = tf.reshape(tf.slice(GatherList[-1],[0,2],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tIndexList.append(tf.concat([mol_index, atom_index], axis = -1))\n\treturn SymList, IndexList\n\ndef TFSymSet_Scattered_Linear(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_, SFPsA_, zeta, eta, Ra_cut, Radp, Angt):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet_Linear(R, Zs, eles_, SFPsR_, eta, Rr_cut, Radp),[nmol, natom,-1], name=""FinishGMR"")\n\tGMA = tf.reshape(TFSymASet_Linear(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  Angt), [nmol, natom,-1], name=""FinishGMA"")\n\tGM = tf.concat([GMR, GMA], axis=2, name=""ConcatRadAng"")\n\t#GM = tf.identity(GMA)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]), name=""FormEleMask"")\n\tToMask1 = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tv = tf.cast(tf.reshape(tf.range(nmol*natom), [nmol, natom, 1]), dtype=tf.int64, name=""FormIndices"")\n\tToMask = tf.concat([ToMask1, v], axis = -1)\n\tIndexList = []\n\tSymList= []\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tSymList.append(tf.gather_nd(GM, tf.slice(GatherList[-1],[0,0],[NAtomOfEle,2])))\n\t\tmol_index = tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tatom_index = tf.reshape(tf.slice(GatherList[-1],[0,2],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tIndexList.append(tf.concat([mol_index, atom_index], axis = -1))\n\treturn SymList, IndexList\n\n\ndef TFSymSet_Scattered_Linear_WithEle(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_, SFPsA_, zeta, eta, Ra_cut, RadpEle, AngtEle, mil_jk):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet_Linear_WithEle(R, Zs, eles_, SFPsR_, eta, Rr_cut, RadpEle),[nmol, natom,-1], name=""FinishGMR"")\n\tGMA = tf.reshape(TFSymASet_Linear_WithEle(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  AngtEle, mil_jk),[nmol, natom,-1], name=""FinishGMA"")\n\t#return GMR, R_index, GMA, A_index\n\t#GMR = tf.reshape(TFSymRSet_Linear_WithEle(R, Zs, eles_, SFPsR_, eta, Rr_cut, RadpEle),[nmol, natom,-1], name=""FinishGMR"")\n\t#GMA = tf.reshape(TFSymASet_Linear_WithEle(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  AngtEle), [nmol, natom,-1], name=""FinishGMA"")\n\tGM = tf.concat([GMR, GMA], axis=2, name=""ConcatRadAng"")\n\t#GM = tf.identity(GMA)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]), name=""FormEleMask"")\n\tToMask1 = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tv = tf.cast(tf.reshape(tf.range(nmol*natom), [nmol, natom, 1]), dtype=tf.int64, name=""FormIndices"")\n\tToMask = tf.concat([ToMask1, v], axis = -1)\n\tIndexList = []\n\tSymList= []\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tSymList.append(tf.gather_nd(GM, tf.slice(GatherList[-1],[0,0],[NAtomOfEle,2])))\n\t\tmol_index = tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tatom_index = tf.reshape(tf.slice(GatherList[-1],[0,2],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tIndexList.append(tf.concat([mol_index, atom_index], axis = -1))\n\treturn SymList, IndexList\n\ndef TFSymSet_Scattered_Linear_WithEle_Release(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_, SFPsA_, zeta, eta, Ra_cut, RadpEle, AngtEle, mil_j, mil_jk):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet_Linear_WithEle_Release(R, Zs, eles_, SFPsR_, eta, Rr_cut, RadpEle, mil_j),[nmol, natom,-1], name=""FinishGMR"")\n\tGMA = tf.reshape(TFSymASet_Linear_WithEle(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  AngtEle, mil_jk),[nmol, natom,-1], name=""FinishGMA"")\n\t#return GMR, R_index, GMA, A_index\n\t#GMR = tf.reshape(TFSymRSet_Linear_WithEle(R, Zs, eles_, SFPsR_, eta, Rr_cut, RadpEle),[nmol, natom,-1], name=""FinishGMR"")\n\t#GMA = tf.reshape(TFSymASet_Linear_WithEle(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  AngtEle), [nmol, natom,-1], name=""FinishGMA"")\n\tGM = tf.concat([GMR, GMA], axis=2, name=""ConcatRadAng"")\n\t#GM = tf.identity(GMA)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]), name=""FormEleMask"")\n\tToMask1 = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tv = tf.cast(tf.reshape(tf.range(nmol*natom), [nmol, natom, 1]), dtype=tf.int64, name=""FormIndices"")\n\tToMask = tf.concat([ToMask1, v], axis = -1)\n\tIndexList = []\n\tSymList= []\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tSymList.append(tf.gather_nd(GM, tf.slice(GatherList[-1],[0,0],[NAtomOfEle,2])))\n\t\tmol_index = tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tatom_index = tf.reshape(tf.slice(GatherList[-1],[0,2],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tIndexList.append(tf.concat([mol_index, atom_index], axis = -1))\n\treturn SymList, IndexList\n\ndef TFSymSet_Scattered_Linear_WithEle_UsingList(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_, SFPsA_, zeta, eta, Ra_cut, RadpEle, AngtEle, mil_jk):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet_Linear_WithEle(R, Zs, eles_, SFPsR_, eta, Rr_cut, RadpEle),[nmol, natom,-1], name=""FinishGMR"")\n\tGMA = tf.reshape(TFSymASet_Linear_WithEle_UsingList(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  AngtEle, mil_jk),[nmol, natom,-1], name=""FinishGMA"")\n\t#return GMR, R_index, GMA, A_index\n\t#GMR = tf.reshape(TFSymRSet_Linear_WithEle(R, Zs, eles_, SFPsR_, eta, Rr_cut, RadpEle),[nmol, natom,-1], name=""FinishGMR"")\n\t#GMA = tf.reshape(TFSymASet_Linear_WithEle(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  AngtEle), [nmol, natom,-1], name=""FinishGMA"")\n\tGM = tf.concat([GMR, GMA], axis=2, name=""ConcatRadAng"")\n\t#GM = tf.identity(GMA)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]), name=""FormEleMask"")\n\tToMask1 = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tv = tf.cast(tf.reshape(tf.range(nmol*natom), [nmol, natom, 1]), dtype=tf.int64, name=""FormIndices"")\n\tToMask = tf.concat([ToMask1, v], axis = -1)\n\tIndexList = []\n\tSymList= []\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tSymList.append(tf.gather_nd(GM, tf.slice(GatherList[-1],[0,0],[NAtomOfEle,2])))\n\t\tmol_index = tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tatom_index = tf.reshape(tf.slice(GatherList[-1],[0,2],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tIndexList.append(tf.concat([mol_index, atom_index], axis = -1))\n\treturn SymList, IndexList\n\ndef TFSymSet_Scattered_Linear_WithEle_Periodic(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_, SFPsA_, zeta, eta, Ra_cut, RadpEle, AngtEle, mil_j, mil_jk, nreal):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGMR = tf.reshape(TFSymRSet_Linear_WithElePeriodic(R, Zs, eles_, SFPsR_, eta, Rr_cut, RadpEle, mil_j, nreal),[nmol, nreal, -1], name=""FinishGMR"")\n\tGMA = tf.reshape(TFSymASet_Linear_WithElePeriodic(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  AngtEle, mil_jk, nreal),[nmol, nreal, -1], name=""FinishGMA"")\n\t#return GMR, R_index, GMA, A_index\n\t#GMR = tf.reshape(TFSymRSet_Linear_WithEle(R, Zs, eles_, SFPsR_, eta, Rr_cut, RadpEle),[nmol, natom,-1], name=""FinishGMR"")\n\t#GMA = tf.reshape(TFSymASet_Linear_WithEle(R, Zs, eleps_, SFPsA_, zeta,  eta, Ra_cut,  AngtEle), [nmol, natom,-1], name=""FinishGMA"")\n\tGM = tf.concat([GMR, GMA], axis=2, name=""ConcatRadAng"")\n\t#GM = tf.identity(GMA)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tZs_real = Zs[:,:nreal]\n\tMaskAll = tf.equal(tf.reshape(Zs_real,[nmol,nreal,1]),tf.reshape(eles_,[1,1,nele]), name=""FormEleMask"")\n\tToMask1 = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(nreal),[1,nreal]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tv = tf.cast(tf.reshape(tf.range(nmol*nreal), [nmol, nreal, 1]), dtype=tf.int64, name=""FormIndices"")\n\tToMask = tf.concat([ToMask1, v], axis = -1)\n\tIndexList = []\n\tSymList= []\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,nreal,1]),[nmol, nreal])))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tSymList.append(tf.gather_nd(GM, tf.slice(GatherList[-1],[0,0],[NAtomOfEle,2])))\n\t\tmol_index = tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tatom_index = tf.reshape(tf.slice(GatherList[-1],[0,2],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tIndexList.append(tf.concat([mol_index, atom_index], axis = -1))\n\treturn SymList, IndexList\n\ndef TFSymSet_Radius_Scattered_Linear_Qs(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_,  eta,  Radp, Qs):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGM = tf.reshape(TFSymRSet_Linear_Qs(R, Zs, eles_, SFPsR_, eta, Rr_cut, Radp, Qs),[nmol, natom,-1])\n\t#GM = tf.identity(GMA)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]))\n\tToMask1 = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tv = tf.cast(tf.reshape(tf.range(nmol*natom), [nmol, natom, 1]), dtype=tf.int64)\n\tToMask = tf.concat([ToMask1, v], axis = -1)\n\tIndexList = []\n\tSymList= []\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tSymList.append(tf.gather_nd(GM, tf.slice(GatherList[-1],[0,0],[NAtomOfEle,2])))\n\t\tmol_index = tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tatom_index = tf.reshape(tf.slice(GatherList[-1],[0,2],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tIndexList.append(tf.concat([mol_index, atom_index], axis = -1))\n\treturn SymList, IndexList\n\ndef TFSymSet_Radius_Scattered_Linear_Qs_Periodic(R, Zs, eles_, SFPsR_, Rr_cut,  eleps_,  eta,  Radp, Qs, mil_j, nreal):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom X 1 tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\tSFPsR_: A symmetry function parameter of radius part\n\t\tRr_cut: Radial Cutoff of radius part\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tSFPsA_: A symmetry function parameter of angular part\n\t\tRA_cut: Radial Cutoff of angular part\n\tReturns:\n\t\tDigested Mol. In the shape nmol X maxnatom X (Dimension of radius part + Dimension of angular part)\n\t""""""\n\tinp_shp = tf.shape(R)\n\tnmol = inp_shp[0]\n\tnatom = inp_shp[1]\n\tnele = tf.shape(eles_)[0]\n\tnelep = tf.shape(eleps_)[0]\n\tGM = tf.reshape(TFSymRSet_Linear_Qs_Periodic(R, Zs, eles_, SFPsR_, eta, Rr_cut, Radp, Qs, mil_j, nreal),[nmol, nreal,-1])\n\t#GM = tf.identity(GMA)\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tZs_real = Zs[:,:nreal]\n\tMaskAll = tf.equal(tf.reshape(Zs_real,[nmol,nreal,1]),tf.reshape(eles_,[1,1,nele]))\n\tToMask1 = AllSinglesSet(tf.cast(tf.tile(tf.reshape(tf.range(nreal),[1,nreal]),[nmol,1]),dtype=tf.int64), prec=tf.int64)\n\tv = tf.cast(tf.reshape(tf.range(nmol*nreal), [nmol, nreal, 1]), dtype=tf.int64)\n\tToMask = tf.concat([ToMask1, v], axis = -1)\n\tIndexList = []\n\tSymList= []\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,nreal,1]),[nmol, nreal])))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tSymList.append(tf.gather_nd(GM, tf.slice(GatherList[-1],[0,0],[NAtomOfEle,2])))\n\t\tmol_index = tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tatom_index = tf.reshape(tf.slice(GatherList[-1],[0,2],[NAtomOfEle,1]),[NAtomOfEle, 1])\n\t\tIndexList.append(tf.concat([mol_index, atom_index], axis = -1))\n\treturn SymList, IndexList\n\ndef NNInterface(R, Zs, eles_, GM):\n\t""""""\n\tA tensorflow implementation of the AN1 symmetry function for a set of molecule.\n\tArgs:\n\t\tR: a nmol X maxnatom X 3 tensor of coordinates.\n\t\tZs : nmol X maxnatom  tensor of atomic numbers.\n\t\teles_: a neles X 1 tensor of elements present in the data.\n\t\teleps_: a nelepairs X 2 X 12tensor of elements pairs present in the data.\n\t\tGM: Unscattered ANI1 sym Func: nmol X natom X nele X Dim\n\n\n\tReturns:\n\t\tList of ANI SymFunc of each atom by element type.\n\t\tList of Mol index of each atom by element type.\n\t""""""\n\tnele = tf.shape(eles_)[0]\n\tnum_ele, num_dim = eles_.get_shape().as_list()\n\tR_shp = tf.shape(R)\n\tnmol = R_shp[0]\n\tnatom = R_shp[1]\n\tMaskAll = tf.equal(tf.reshape(Zs,[nmol,natom,1]),tf.reshape(eles_,[1,1,nele]))\n\tToMask = AllSinglesSet(tf.tile(tf.reshape(tf.range(natom),[1,natom]),[nmol,1]))\n\tIndexList = []\n\tSymList=[]\n\tGatherList = []\n\tfor e in range(num_ele):\n\t\tGatherList.append(tf.boolean_mask(ToMask,tf.reshape(tf.slice(MaskAll,[0,0,e],[nmol,natom,1]),[nmol, natom])))\n\t\tSymList.append(tf.gather_nd(GM, GatherList[-1]))\n\t\tNAtomOfEle=tf.shape(GatherList[-1])[0]\n\t\tIndexList.append(tf.reshape(tf.slice(GatherList[-1],[0,0],[NAtomOfEle,1]),[NAtomOfEle]))\n\treturn SymList, IndexList\n\nclass ANISym:\n\tdef __init__(self, mset_):\n\t\tself.set = mset_\n\t\tself.MaxAtoms = self.set.MaxNAtoms()\n\t\tself.nmol = len(self.set.mols)\n\t\tself.MolPerBatch = 10000\n\t\tself.SymOutput = None\n\t\tself.xyz_pl= None\n\t\tself.Z_pl = None\n\t\tself.SFPa = None\n\t\tself.SFPr = None\n\t\tself.SymGrads = None\n\t\tself.TDSSet = None\n\t\tself.vdw_R = np.zeros(2)\n\t\tself.C6 = np.zeros(2)\n\t\tfor i, ele in enumerate([1,8]):\n\t\t\tself.C6[i] = C6_coff[ele]* (BOHRPERA*10.0)**6.0 / JOULEPERHARTREE # convert into a.u.\n\t\t\tself.vdw_R[i] = atomic_vdw_radius[ele]*BOHRPERA\n\n\tdef SetANI1Param(self):\n\t\tzetas = np.array([[8.0]], dtype = np.float64)\n\t\tetas = np.array([[4.0]], dtype = np.float64)\n\t\tself.zeta = 8.0\n\t\tself.eta = 4.0\n\t\tAN1_num_a_As = 8\n\t\tthetas = np.array([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype = np.float64)\n\t\tAN1_num_a_Rs = 8\n\t\tAN1_a_Rc = 3.1\n\t\tself.Ra_cut = 3.1\n\t\tself.Rr_cut = 4.6\n\t\trs =  np.array([ AN1_a_Rc*i/AN1_num_a_Rs for i in range (0, AN1_num_a_Rs)], dtype = np.float64)\n\t\tRa_cut = AN1_a_Rc\n\t\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\t\tp1 = np.tile(np.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp3 = np.tile(np.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_a_Rs,1])\n\t\tp4 = np.tile(np.reshape(rs,[1,1,1,AN1_num_a_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\t\tSFPa = np.concatenate([p1,p2,p3,p4],axis=4)\n\t\tself.SFPa = np.transpose(SFPa, [4,0,1,2,3])\n\t\t#self.P5 = Tile_P5(1, 1, AN1_num_a_As, AN1_num_a_Rs)\n\n\t\t# Create a parameter tensor. 2 x ntheta X nr\n\t\tp1 = np.tile(np.reshape(thetas,[AN1_num_a_As,1,1]),[1,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(rs,[1,AN1_num_a_Rs,1]),[AN1_num_a_As,1,1])\n\t\tSFPa2 = np.concatenate([p1,p2],axis=2)\n\t\tself.SFPa2 = np.transpose(SFPa2, [2,0,1])\n\n\t\tetas_R = np.array([[4.0]], dtype = np.float64)\n\t\tAN1_num_r_Rs = 32\n\t\tAN1_r_Rc = 4.6\n\t\trs_R =  np.array([ AN1_r_Rc*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype = np.float64)\n\t\tRr_cut = AN1_r_Rc\n\t\t# Create a parameter tensor. 2 x  neta X nr\n\t\tp1_R = np.tile(np.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\t\tp2_R = np.tile(np.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\t\tSFPr = np.concatenate([p1_R,p2_R],axis=2)\n\t\tself.SFPr = np.transpose(SFPr, [2,0,1])\n\t\t# Create a parameter tensor. 1  X nr\n\t\tp1_new = np.reshape(rs_R,[AN1_num_r_Rs,1])\n\t\tself.SFPr2 = np.transpose(p1_new, [1,0])\n\t\t#self.P3 = Tile_P3(1,  AN1_num_r_Rs)\n\t\t#self.TDSSet = [AllTriplesSet_Np(self.MolPerBatch, self.MaxAtoms), AllDoublesSet_Np(self.MolPerBatch, self.MaxAtoms), AllSinglesSet_Np(self.MolPerBatch, self.MaxAtoms)]\n\n\n\tdef Prepare(self):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t        continue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyz_pl=tf.placeholder(tf.float64, shape=tuple([self.MolPerBatch, self.MaxAtoms,3]))\n\t\t\tself.Z_pl=tf.placeholder(tf.int64, shape=tuple([self.MolPerBatch, self.MaxAtoms]))\n\t\t\tself.Radp_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.Angt_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.RadpEle_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.AngtEle_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_j_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_jk_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\t#self.nreal = tf.Variable(self.Num_Real, dtype = tf.int32)\n\t\t\tself.Qs_pl=tf.placeholder(tf.float64, shape=tuple([self.MolPerBatch, self.MaxAtoms]))\n\t\t\tEle = tf.Variable([[1],[8]], dtype = tf.int64)\n\t\t\tElep = tf.Variable([[1,1],[1,8],[8,8]], dtype = tf.int64)\n\t\t\tC6 = tf.Variable(self.C6, tf.float64)\n\t\t\tR_vdw = tf.Variable(self.vdw_R, tf.float64)\n\t\t\t#zetas = tf.Variable([[8.0]], dtype = tf.float64)\n\t\t\t#etas = tf.Variable([[4.0]], dtype = tf.float64)\n\t\t\tSFPa = tf.Variable(self.SFPa, tf.float64)\n\t\t\tSFPr = tf.Variable(self.SFPr, tf.float64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, tf.float64)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, tf.float64)\n\t\t\t#P3 = tf.Variable(self.P3, tf.int32)\n\t\t\t#P5 = tf.Variable(self.P5, tf.int32)\n\t\t\tRa_cut = 3.1\n\t\t\tRr_cut = 4.6\n\t\t\tRee_on = 0.0\n\t\t\tself.Radp_pl = self.RadpEle_pl[:,:3]\n\t\t\t#self.A, self.B, self.C, self.D, self.E, self.F, self.G, self.H, self.I = TFVdwPolyLR(self.xyz_pl,  self.Z_pl, Ele, C6, R_vdw, Ree_on, self.Radp_pl)\n\t\t\t#self.Scatter_Sym, self.Sym_Index = TFSymSet_Scattered(self.xyz_pl, self.Z_pl, Ele, SFPr, Rr_cut, Elep, SFPa, Ra_cut)\n\t\t\t#self.Scatter_Sym_Update, self.Sym_Index_Update = TFSymSet_Scattered_Update(self.xyz_pl, self.Z_pl, Ele, SFPr, Rr_cut, Elep, SFPa, Ra_cut)\n\t\t\t#self.Scatter_Sym_Update2, self.Sym_Index_Update2 = TFSymSet_Scattered_Update2(self.xyz_pl, self.Z_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, self.zeta, self.eta, Ra_cut)\n\t\t\t#self.Scatter_Sym_Update, self.Sym_Index_Update = TFSymSet_Scattered_Update_Scatter(self.xyz_pl, self.Z_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, self.zeta, self.eta, Ra_cut)\n\t\t\tself.Scatter_Sym_Linear_Qs, self.Sym_Index_Linear_Qs = TFSymSet_Radius_Scattered_Linear_Qs(self.xyz_pl, self.Z_pl, Ele, SFPr2, Rr_cut, Elep,  self.eta,  self.Radp_pl, self.Qs_pl)\n\n\t\t\tself.Scatter_Sym_Linear_Qs_Periodic, self.Sym_Index_Linear_Qs_Periodic = TFSymSet_Radius_Scattered_Linear_Qs_Periodic(self.xyz_pl, self.Z_pl, Ele, SFPr2, Rr_cut, Elep,  self.eta,  self.Radp_pl, self.Qs_pl, self.mil_j_pl, self.nreal)\n\t\t\t#self.Scatter_Sym_Linear, self.Sym_Index_Linear = TFSymSet_Scattered_Linear(self.xyz_pl, self.Z_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, self.zeta, self.eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\n\t\t\tself.Scatter_Sym_Linear_Ele, self.Sym_Index_Linear_Ele = TFSymSet_Scattered_Linear_WithEle(self.xyz_pl, self.Z_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, self.zeta, self.eta, Ra_cut, self.RadpEle_pl, self.AngtEle_pl, self.mil_jk_pl)\n\t\t\tself.Scatter_Sym_Linear_Ele_Periodic, self.Sym_Index_Linear_Ele_Periodic = TFSymSet_Scattered_Linear_WithEle_Periodic(self.xyz_pl, self.Z_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, self.zeta, self.eta, Ra_cut, self.RadpEle_pl, self.AngtEle_pl, self.mil_j_pl, self.mil_jk_pl, self.nreal)\n\t\t\t#self.Scatter_Sym_Linear_tmp, self.Sym_Index_Linear_tmp = TFSymSet_Scattered_Linear_tmp(self.xyz_pl, self.Z_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, self.zeta, self.eta, Ra_cut, self.RadpEle_pl, self.AngtEle_pl, self.mil_jk_pl)\n\t\t\t#self.Eee, self.Kern, self.index = TFCoulombCosLR(self.xyz_pl, tf.cast(self.Z_pl, dtype=tf.float64), Rr_cut, self.Radp_pl)\n\t\t\t#self.gradient = tf.gradients(self.Scatter_Sym, self.xyz_pl)\n\t\t\t#self.gradient_update2 = tf.gradients(self.Scatter_Sym_Update2, self.xyz_pl)\n\t\t\t#self.gradient = tf.gradients(self.Scatter_Sym_Update, self.xyz_pl)\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.sess.run(init)\n\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\tself.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\t#def fill_feed_dict(self, batch_data, coord_pl, atom_pl, radp_pl,  angt_pl, Qs_pl, radpEle_pl,  angtEle_pl, mil_j_pl, mil_jk_pl):\n\t#\treturn {coord_pl: batch_data[0], atom_pl: batch_data[1], radp_pl:batch_data[2], angt_pl:batch_data[3], Qs_pl:batch_data[4], radpEle_pl:batch_data[5], angtEle_pl:batch_data[6], mil_j_pl:batch_data[7], mil_jk_pl:batch_data[8]}\n\n\n\tdef fill_feed_dict(self, batch_data, coord_pl, atom_pl, radpEle_pl,  angtEle_pl, mil_j_pl, mil_jk_pl, Qs_pl):\n\t\treturn {coord_pl: batch_data[0], atom_pl: batch_data[1], radpEle_pl:batch_data[2], angtEle_pl:batch_data[3], mil_j_pl:batch_data[4], mil_jk_pl:batch_data[5], Qs_pl:batch_data[6]}\n\n\tdef TestPeriodic(self):\n\t\t""""""\n\t\tTests a Simple Periodic optimization.\n\t\tTrying to find the HCP minimum for the LJ crystal.\n\t\t""""""\n\t\ta=MSet(""water_tiny"", center_=False)\n\t\ta.ReadXYZ(""water_tiny"")\n\t\tm = a.mols[-1]\n\t\tm.coords = m.coords - np.min(m.coords)\n\t\tprint(""Original coords:"", m.coords)\n\t\t# Generate a Periodic Force field.\n\t\tcellsize = 9.3215\n\t\tlat = cellsize*np.eye(3)\n\t\tself.MolPerBatch = 1\n\t\tPF = TensorMol.TFPeriodicForces.TFPeriodicVoxelForce(15.0,lat)\n\t\t#zp, xp = PF(m.atoms,m.coords,lat)  # tessilation in TFPeriodic seems broken\n\n\t\tzp = np.zeros(m.NAtoms()*PF.tess.shape[0], dtype=np.int32)\n\t\txp = np.zeros((m.NAtoms()*PF.tess.shape[0], 3))\n\t\tfor i in range(0, PF.tess.shape[0]):\n\t\t\tzp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.atoms\n\t\t\txp[i*m.NAtoms():(i+1)*m.NAtoms()] = m.coords + cellsize*PF.tess[i]\n\n\t\tself.MaxAtoms = xp.shape[0]\n\t\tself.nreal = m.NAtoms()\n\n\t\tqs = np.zeros((1, self.MaxAtoms))\n\t\tfor j in range (0, m.NAtoms()):\n\t\t\tif m.atoms[j] == 1:\n\t\t\t\tqs[0][j] = 0.5\n\t\t\telse:\n\t\t\t\tqs[0][j] = -1.0\n\n\n\n\t\t#self.Num_Real = m.NAtoms()\n\t\tself.SetANI1Param()\n\t\tself.Prepare()\n\t\tt_total = time.time()\n\t\tEle = np.asarray([1,8])\n\t\tElep = np.asarray([[1,1],[1,8],[8,8]])\n\n\t\tt0 = time.time()\n\t\tNL = NeighborListSetWithImages(xp.reshape((1,-1,3)), np.array([zp.shape[0]]), np.array([m.NAtoms()]), True, True, zp.reshape((1,-1)), sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexPeriodic(4.6, 3.1, np.array([1,8]), np.array([[1,1],[1,8],[8,8]]))\n\t\tprint (""python time cost:"", time.time() - t0)\n\t\tbatch_data = [xp.reshape((1,-1,3)), zp.reshape((1,-1)), rad_p_ele, ang_t_elep, mil_j, mil_jk, qs]\n\t\tfeed_dict = self.fill_feed_dict(batch_data, self.xyz_pl, self.Z_pl, self.RadpEle_pl, self.AngtEle_pl, self.mil_j_pl, self.mil_jk_pl, self.Qs_pl)\n\n\t\tA, B, C, D = self.sess.run([self.Scatter_Sym_Linear_Qs, self.Sym_Index_Linear_Qs, self.Scatter_Sym_Linear_Qs_Periodic, self.Sym_Index_Linear_Qs_Periodic], feed_dict = feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\tprint (""A:\\n"",A[0].shape, len(A))\n\t\tprint (""B:\\n"",B[0].shape)\n\t\tprint (""C:\\n"",C[0].shape, len(C))\n\t\tprint (""D:\\n"",D[0].shape)\n\t\t#print (""A==C:"", A[0]==C[0])\n\t\t#print (""B==D"", B[0]==D[0])\n\t\tprint (""total time cost:"", time.time() - t0)\n\t\tnp.set_printoptions(threshold=np.nan)\n\t\tprint (A[0][54],A[0][53])\n\t\tfor i in range(0, 54):\n\t\t\tprint (np.array_equal(A[0][i], C[0][i]))\n\n\t\tfor i in range(0, 27):\n\t\t\tprint (np.array_equal(A[1][i], C[1][i]))\n\t\t#print (""B:"",B[0][:200])\n\n\tdef Generate_ANISYM(self):\n\t\txyzs = np.zeros((self.nmol, self.MaxAtoms, 3),dtype=np.float64)\n\t\tqs = np.zeros((self.nmol, self.MaxAtoms),dtype=np.float64)\n\t\tZs = np.zeros((self.nmol, self.MaxAtoms), dtype=np.int64)\n\t\tnnz_atom = np.zeros((self.nmol), dtype=np.int64)\n\t\t#random.shuffle(self.set.mols)\n\t\tfor i, mol in enumerate(self.set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnnz_atom[i] = mol.NAtoms()\n\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\tif Zs[i][j] == 1:\n\t\t\t\t\tqs[i][j] = 0.5\n\t\t\t\telse:\n\t\t\t\t\tqs[i][j] = -1.0\n\t\tself.SetANI1Param()\n\t\tself.Prepare()\n\t\tt_total = time.time()\n\t\tEle = np.asarray([1,8])\n\t\tElep = np.asarray([[1,1],[1,8],[8,8]])\n\t\tm = self.set.mols[0]\n\t\t#vdw = 0.0\n\t\t#for i in range(0, m.NAtoms()):\n\t\t#\tfor j in range(i+1, m.NAtoms()):\n\t\t#\t\tdist = np.sum(np.square(m.coords[i]-m.coords[j]))**0.5\n\t\t#\t\tc6_i = C6_coff[m.atoms[i]]*(10.0)**6.0 / JOULEPERHARTREE\n\t\t#\t\tc6_j = C6_coff[m.atoms[j]]*(10.0)**6.0 / JOULEPERHARTREE\n\t\t#\t\tRi = atomic_vdw_radius[m.atoms[i]]\n\t\t#\t\tRj = atomic_vdw_radius[m.atoms[j]]\n\t\t#\t\tif dist > 4.6:\n\t\t#\t\t\tcut = 1.0\n\t\t#\t\telse:\n\t\t#\t\t\tt = dist/4.6\n\t\t#\t\t\tcut = -t*t*(2.0*t-3.0)\n\t\t#\t\tprint (dist*BOHRPERA, -(c6_i*c6_j)**0.5/dist**6 /(1.0+6*(dist/(Ri+Rj))**-12)*cut, cut, 1.0/(1.0+6*(dist/(Ri+Rj))**-12))\n\t\t#\t\tvdw += -(c6_i*c6_j)**0.5/dist**6 /(1.0+6*(dist/(Ri+Rj))**-12)*cut\n\t\t#print (""vdw:"", vdw,""\\n\\n\\n"")\n\n\n\t\tfor i in range (0, int(self.nmol/self.MolPerBatch-1)):\n\t\t\tt = time.time()\n\t\t\tNL = NeighborListSet(xyzs[i*self.MolPerBatch: (i+1)*self.MolPerBatch], nnz_atom[i*self.MolPerBatch: (i+1)*self.MolPerBatch], True, True, Zs[i*self.MolPerBatch: (i+1)*self.MolPerBatch], sort_=True)\n\t\t\trad_p, ang_t = NL.buildPairsAndTriples(self.Rr_cut, self.Ra_cut)\n\t\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(self.Rr_cut, self.Ra_cut, Ele, Elep)\n\n\t\t\tNLEE = NeighborListSet(xyzs[i*self.MolPerBatch: (i+1)*self.MolPerBatch], nnz_atom[i*self.MolPerBatch: (i+1)*self.MolPerBatch], False, False,  None)\n\t\t\trad_p = NLEE.buildPairs(self.Rr_cut)\n\t\t\tprint (""rad_p:"", rad_p[:20])\n\t\t\t#print (""time to build pairs:"", time.time() - t)\n\t\t\t#print (""rad_p_ele:\\n"", rad_p_ele, ""\\nang_t_elep:\\n"", ang_t_elep)\n\t\t\t#raise Exception(""Debug.."")\n\t\t\tt = time.time()\n\t\t\tbatch_data = [xyzs[i*self.MolPerBatch: (i+1)*self.MolPerBatch], Zs[i*self.MolPerBatch: (i+1)*self.MolPerBatch], rad_p,  ang_t, qs[i*self.MolPerBatch: (i+1)*self.MolPerBatch], rad_p_ele, ang_t_elep, mil_jk]\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.xyz_pl, self.Z_pl, self.Radp_pl, self.Angt_pl, self.Qs_pl, self.RadpEle_pl, self.AngtEle_pl, self.mil_jk_pl)\n\t\t\tt = time.time()\n\t\t\t#sym_output, grad = self.sess.run([self.SymOutput, self.SymGrads], feed_dict = feed_dict)\n\t\t\t#sym_output_update2, sym_index_update2, sym_output, sym_index, gradient_update2 = self.sess.run([self.Scatter_Sym_Update2, self.Sym_Index_Update2, self.Scatter_Sym_Update, self.Sym_Index_Update, self.gradient_update2], feed_dict = feed_dict)\n\t\t\t#sym_output_update, sym_index_update, sym_output, sym_index, gradient, gradient_update = self.sess.run([self.Scatter_Sym_Update, self.Sym_Index_Update, self.Scatter_Sym, self.Sym_Index, self.gradient, self.gradient_update], feed_dict = feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\t\t#sym_output, sym_index  = self.sess.run([self.Scatter_Sym_Update2, self.Sym_Index_Update2], feed_dict = feed_dict)\n\t\t\t#sym_output, sym_index  = self.sess.run([self.Scatter_Sym, self.Sym_Index], feed_dict = feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\t\t#A, B  = self.sess.run([self.Scatter_Sym_Update, self.Sym_Index_Update], feed_dict = feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\t\t#A, B, C, D  = self.sess.run([self.Scatter_Sym_Linear, self.Sym_Index_Linear, self.Scatter_Sym_Linear_Qs, self.Sym_Index_Linear_Qs], feed_dict = feed_dict)\n\n\t\t\t#A, B  = self.sess.run([self.Scatter_Sym_Linear, self.Sym_Index_Linear], feed_dict = feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\t\t#C, D  = self.sess.run([self.Scatter_Sym_Linear_Ele, self.Sym_Index_Linear_Ele], feed_dict = feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\t\tA, B, C, D, E, F, H, G, I = self.sess.run([self.A, self.B, self.C, self.D, self.E, self.F, self.H, self.G, self.I], feed_dict = feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\t\t#A, B, C, D  = self.sess.run([self.Scatter_Sym_Linear, self.Sym_Index_Linear, self.Scatter_Sym_Linear_Ele, self.Sym_Index_Linear_Ele], feed_dict = feed_dict)\n\t\t\t#print (""A:\\n"", A[0].shape, ""\\nC:\\n"", C[0].shape)\n\t\t\t#np.set_printoptions(threshold=np.nan)\n\t\t\tprint (""A:"",A, ""B:"",B)\n\t\t\tprint (""C:"",C, ""D:"",D)\n\t\t\tprint (""E:"",E, "" F:"",F)\n\t\t\tprint (""H:"", H, ""G:"", G)\n\t\t\tprint (""I:"", I)\n\t\t\treturn\n\t\t\t#np.savetxt(""rad_sym.dat"", A[0][0][:64])\n\t\t\t#np.savetxt(""Zs_sym.dat"", C[0][0])\n\t\t\t#np.savetxt(""rad_ang_sym.dat"", A[0][0])\n\t\t\t#raise Exception(""End Here"")\n\t\t\t#print (""i: "", i,  ""sym_ouotput: "", len(sym_output),"" time:"", time.time() - t, "" second"", ""gpu time:"", time.time()-t1, sym_index)\n\t\t\t#print (""sym_output_update:"", np.array_equal(sym_output_update2[0], sym_output[0]))\n\t\t\t#print (""sym_output_update:"", np.sum(np.abs(sym_output_update2[0]-sym_output[0])))\n\t\t\t#print (""gradient_update:"", np.sum(np.abs(gradient[0]-gradient_update[0])))\n\t\t\t#print (""sym_index_update:"", np.array_equal(sym_index_update[0], sym_index[0]))\n\t\t\t#print (""gradient:"", gradient[0].shape)\n\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\twith open(\'timeline_step_%d_new.json\' % i, \'w\') as f:\n\t\t\t\tf.write(chrome_trace)\n\t\t\tprint (""inference time:"", time.time() - t)\n'"
TensorMol/TFDescriptors/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom .RawSH import *\nfrom .RawSymFunc import *\n'
TensorMol/TFNetworks/TFBehlerParinello.py,237,"b'""""""\nThis version of the Behler-Parinello is aperiodic,non-sparse.\nIt\'s being developed to explore alternatives to symmetry functions. It\'s not for production use.\n\nJohn: Do you think they really have to be kept separate?\nCant the descriptor just be conditionally switched in Prepare?\nThat would seem to make more sense to me.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport random\nimport sys\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\n\n\nfrom ..Containers.TensorMolData import *\nfrom ..TFDescriptors.RawSH import *\nfrom ..TFDescriptors.RawSymFunc import *\nfrom tensorflow.python.client import timeline\n\nclass BehlerParinelloNetwork(object):\n\t""""""\n\tBase class for Behler-Parinello network using embedding from RawEmbeddings.py\n\tDo not use directly, only for inheritance to derived classes\n\talso has sparse evaluation using an updated version of the\n\tneighbor list, and a polynomial cutoff coulomb interaction.\n\t""""""\n\tdef __init__(self, mol_set_name=None, name=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tmol_set (TensorMol.MSet object): a class which holds the training data\n\t\t\tname (str): a name used to recall this network\n\n\t\tNotes:\n\t\t\tif name != None, attempts to load a previously saved network, otherwise assumes a new network\n\t\t""""""\n\t\tself.tf_precision = eval(PARAMS[""tf_prec""])\n\t\tself.hidden_layers = PARAMS[""HiddenLayers""]\n\t\tself.learning_rate = PARAMS[""learning_rate""]\n\t\tself.weight_decay = PARAMS[""weight_decay""]\n\t\tself.momentum = PARAMS[""momentum""]\n\t\tself.max_steps = PARAMS[""max_steps""]\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tself.max_checkpoints = PARAMS[""max_checkpoints""]\n\t\tself.path = PARAMS[""networks_directory""]\n\t\tself.train_gradients = PARAMS[""train_gradients""]\n\t\tself.train_dipole = PARAMS[""train_dipole""]\n\t\tself.train_quadrupole = PARAMS[""train_quadrupole""]\n\t\tself.train_rotation = PARAMS[""train_rotation""]\n\t\tself.profiling = PARAMS[""Profiling""]\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.randomize_data = PARAMS[""RandomizeData""]\n\t\tself.test_ratio = PARAMS[""TestRatio""]\n\t\tself.assign_activation()\n\n\t\tself.coulomb_cutoff = PARAMS[""EECutoffOff""]\n\t\tself.dsf_alpha = PARAMS[""DSFAlpha""]\n\t\tself.elu_width = PARAMS[""Elu_Width""]\n\n\t\t#Reloads a previous network if name variable is not None\n\t\tif name != None:\n\t\t\tself.name = name\n\t\t\tself.load_network()\n\t\t\tLOGGER.info(""Reloaded network from %s"", self.network_directory)\n\t\t\treturn\n\n\t\t#Data parameters\n\t\tself.mol_set_name = mol_set_name\n\t\tself.mol_set = MSet(self.mol_set_name)\n\t\tself.mol_set.Load()\n\t\tself.elements = self.mol_set.AtomTypes()\n\t\tself.max_num_atoms = self.mol_set.MaxNAtoms()\n\t\tself.num_molecules = len(self.mol_set.mols)\n\n\t\tLOGGER.info(""learning rate: %f"", self.learning_rate)\n\t\tLOGGER.info(""batch size:    %d"", self.batch_size)\n\t\tLOGGER.info(""max steps:     %d"", self.max_steps)\n\t\treturn\n\n\tdef __getstate__(self):\n\t\tstate = self.__dict__.copy()\n\t\tremove_vars = [""mol_set"", ""activation_function"", ""xyz_data"", ""Z_data"", ""energy_data"", ""dipole_data"",\n\t\t\t\t\t\t""num_atoms_data"", ""gradient_data""]\n\t\tfor var in remove_vars:\n\t\t\ttry:\n\t\t\t\tdel state[var]\n\t\t\texcept:\n\t\t\t\tpass\n\t\treturn state\n\n\tdef assign_activation(self):\n\t\tLOGGER.debug(""Assigning Activation Function: %s"", PARAMS[""NeuronType""])\n\t\ttry:\n\t\t\tif self.activation_function_type == ""relu"":\n\t\t\t\tself.activation_function = tf.nn.relu\n\t\t\telif self.activation_function_type == ""elu"":\n\t\t\t\tself.activation_function = tf.nn.elu\n\t\t\telif self.activation_function_type == ""selu"":\n\t\t\t\tself.activation_function = self.selu\n\t\t\telif self.activation_function_type == ""softplus"":\n\t\t\t\tself.activation_function = tf.nn.softplus\n\t\t\telif self.activation_function_type == ""shifted_softplus"":\n\t\t\t\tself.activation_function = self.shifted_softplus\n\t\t\telif self.activation_function_type == ""tanh"":\n\t\t\t\tself.activation_function = tf.tanh\n\t\t\telif self.activation_function_type == ""sigmoid"":\n\t\t\t\tself.activation_function = tf.sigmoid\n\t\t\telif self.activation_function_type == ""sigmoid_with_param"":\n\t\t\t\tself.activation_function = self.sigmoid_with_param\n\t\t\telse:\n\t\t\t\tprint (""unknown activation function, set to relu"")\n\t\t\t\tself.activation_function = tf.nn.relu\n\t\texcept Exception as Ex:\n\t\t\tprint(Ex)\n\t\t\tprint (""activation function not assigned, set to relu"")\n\t\t\tself.activation_function = tf.nn.relu\n\t\treturn\n\n\tdef shifted_softplus(self, x):\n\t\treturn tf.nn.softplus(x) - tf.cast(tf.log(2.0), self.tf_precision)\n\n\tdef sigmoid_with_param(self, x):\n\t\treturn tf.log(1.0+tf.exp(tf.multiply(tf.cast(PARAMS[""sigmoid_alpha""], dtype=self.tf_precision), x)))/tf.cast(PARAMS[""sigmoid_alpha""], dtype=self.tf_precision)\n\n\tdef start_training(self):\n\t\tself.load_data_to_scratch()\n\t\tself.compute_normalization()\n\t\tself.save_network()\n\t\tself.train_prepare()\n\t\tself.train()\n\n\tdef restart_training(self):\n\t\tself.reload_set()\n\t\tself.load_data_to_scratch()\n\t\tself.train_prepare(restart=True)\n\t\tself.train()\n\n\tdef train(self):\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tif self.train_dipole:\n\t\t\tmini_test_loss = 1e10\n\t\t\tfor step in range(1, 51):\n\t\t\t\tself.dipole_train_step(step)\n\t\t\t\tif step%test_freq==0:\n\t\t\t\t\ttest_loss = self.dipole_test_step(step)\n\t\t\t\t\tif (test_loss < mini_test_loss):\n\t\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\t\tself.save_checkpoint(step)\n\t\t\tLOGGER.info(""Continue training dipole until new best checkpoint found."")\n\t\t\ttrain_energy_flag = False\n\t\t\tstep += 1\n\t\t\twhile train_energy_flag == False:\n\t\t\t\tself.dipole_train_step(step)\n\t\t\t\ttest_loss = self.dipole_test_step(step)\n\t\t\t\tif (test_loss < mini_test_loss):\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_checkpoint(step)\n\t\t\t\t\ttrain_energy_flag=True\n\t\t\t\t\tLOGGER.info(""New best checkpoint found. Starting energy network training."")\n\t\t\t\tstep += 1\n\t\tmini_test_loss = 1e10\n\t\tfor step in range(1, self.max_steps+1):\n\t\t\tself.energy_train_step(step)\n\t\t\tif step%test_freq==0:\n\t\t\t\ttest_loss = self.energy_test_step(step)\n\t\t\t\tif (test_loss < mini_test_loss):\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_checkpoint(step)\n\t\tself.sess.close()\n\t\tself.save_network()\n\t\treturn\n\n\tdef save_checkpoint(self, step):\n\t\tcheckpoint_file = os.path.join(self.network_directory,self.name+\'-checkpoint\')\n\t\tLOGGER.info(""Saving checkpoint file %s"", checkpoint_file)\n\t\tself.saver.save(self.sess, checkpoint_file, global_step=step)\n\t\treturn\n\n\tdef save_network(self):\n\t\tprint(""Saving TFInstance"")\n\t\tf = open(self.network_directory+"".tfn"",""wb"")\n\t\tpickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\n\tdef load_network(self):\n\t\tLOGGER.info(""Loading TFInstance"")\n\t\t# import TensorMol.PickleTM\n\t\t# network_member_variables = TensorMol.PickleTM.UnPickleTM(f)\n\t\tnetwork = pickle.load(open(self.path+""/""+self.name+"".tfn"",""rb""))\n\t\tself.__dict__.update(network.__dict__)\n\t\treturn\n\n\tdef reload_set(self):\n\t\t""""""\n\t\tRecalls the MSet to build training data etc.\n\t\t""""""\n\t\tself.mol_set = MSet(self.mol_set_name)\n\t\tself.mol_set.Load()\n\t\treturn\n\n\tdef load_data(self):\n\t\tif (self.mol_set == None):\n\t\t\ttry:\n\t\t\t\tself.reload_set()\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""TensorData object has no molecule set."", Ex)\n\t\tself.xyz_data = np.zeros((self.num_molecules, self.max_num_atoms, 3), dtype = np.float64)\n\t\tself.Z_data = np.zeros((self.num_molecules, self.max_num_atoms), dtype = np.int32)\n\t\t# self.mulliken_charges_data = np.zeros((self.num_molecules, self.max_num_atoms), dtype = np.float64)\n\t\tself.num_atoms_data = np.zeros((self.num_molecules), dtype = np.int32)\n\t\tself.energy_data = np.zeros((self.num_molecules), dtype = np.float64)\n\t\tif self.train_dipole:\n\t\t\tself.dipole_data = np.zeros((self.num_molecules, 3), dtype = np.float64)\n\t\tif self.train_quadrupole:\n\t\t\tself.quadrupole_data = np.zeros((self.num_molecules, 2, 3), dtype = np.float64)\n\t\tself.gradient_data = np.zeros((self.num_molecules, self.max_num_atoms, 3), dtype=np.float64)\n\t\tfor i, mol in enumerate(self.mol_set.mols):\n\t\t\tself.xyz_data[i][:mol.NAtoms()] = mol.coords\n\t\t\tself.Z_data[i][:mol.NAtoms()] = mol.atoms\n\t\t\t# self.mulliken_charges_data[i][:mol.NAtoms()] = mol.properties[""mulliken_charges""]\n\t\t\tself.energy_data[i] = mol.properties[""atomization""]\n\t\t\tif self.train_dipole:\n\t\t\t\tself.dipole_data[i] = mol.properties[""dipole""]\n\t\t\tif self.train_quadrupole:\n\t\t\t\tself.quadrupole_data[i] = mol.properties[""quadrupole""]\n\t\t\tself.num_atoms_data[i] = mol.NAtoms()\n\t\t\tself.gradient_data[i][:mol.NAtoms()] = mol.properties[""gradients""]\n\t\treturn\n\n\tdef load_data_to_scratch(self):\n\t\t""""""\n\t\tReads built training data off disk into scratch space.\n\t\tDivides training and test data.\n\t\tNormalizes inputs and outputs.\n\t\tnote that modifies my MolDigester to incorporate the normalization\n\t\tInitializes pointers used to provide training batches.\n\n\t\tArgs:\n\t\t\trandom: Not yet implemented randomization of the read data.\n\n\t\tNote:\n\t\t\tAlso determines mean stoichiometry\n\t\t""""""\n\t\tself.load_data()\n\t\tself.num_test_cases = int(self.test_ratio * self.num_molecules)\n\t\tself.num_train_cases = int(self.num_molecules - self.num_test_cases)\n\t\tcase_idxs = np.arange(int(self.num_molecules))\n\t\tnp.random.shuffle(case_idxs)\n\t\tself.train_idxs = case_idxs[:int(self.num_molecules - self.num_test_cases)]\n\t\tself.test_idxs = case_idxs[int(self.num_molecules - self.num_test_cases):]\n\t\tself.train_scratch_pointer, self.test_scratch_pointer = 0, 0\n\t\tif self.batch_size > self.num_train_cases:\n\t\t\traise Exception(""Insufficent training data to fill a training batch.\\n""\\\n\t\t\t\t\t+str(self.num_train_cases)+"" cases in dataset with a batch size of ""+str(self.batch_size))\n\t\tif self.batch_size > self.num_test_cases:\n\t\t\traise Exception(""Insufficent testing data to fill a test batch.\\n""\\\n\t\t\t\t\t+str(self.num_test_cases)+"" cases in dataset with a batch size of ""+str(self.batch_size))\n\t\tLOGGER.debug(""Number of training cases: %i"", self.num_train_cases)\n\t\tLOGGER.debug(""Number of test cases: %i"", self.num_test_cases)\n\t\treturn\n\n\tdef get_dipole_train_batch(self, batch_size):\n\t\tif self.train_scratch_pointer + batch_size >= self.num_train_cases:\n\t\t\tnp.random.shuffle(self.train_idxs)\n\t\t\tself.train_scratch_pointer = 0\n\t\tself.train_scratch_pointer += batch_size\n\t\txyzs = self.xyz_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tZs = self.Z_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tenergies = self.energy_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tdipoles = self.dipole_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tquadrupoles = self.quadrupole_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tgradients = self.gradient_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tnum_atoms = self.num_atoms_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\treturn [xyzs, Zs, dipoles, quadrupoles, gradients, num_atoms]\n\n\tdef get_energy_train_batch(self, batch_size):\n\t\tif self.train_scratch_pointer + batch_size >= self.num_train_cases:\n\t\t\tnp.random.shuffle(self.train_idxs)\n\t\t\tself.train_scratch_pointer = 0\n\t\tself.train_scratch_pointer += batch_size\n\t\txyzs = self.xyz_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tZs = self.Z_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tenergies = self.energy_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\t# dipoles = self.dipole_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tnum_atoms = self.num_atoms_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\t\tgradients = self.gradient_data[self.train_idxs[self.train_scratch_pointer - batch_size:self.train_scratch_pointer]]\n\n\t\t# NLEE = NeighborListSet(xyzs, num_atoms, False, False, None)\n\t\t# rad_eep = NLEE.buildPairs(self.coulomb_cutoff)\n\t\treturn [xyzs, Zs, energies, gradients, num_atoms]\n\n\tdef get_dipole_test_batch(self, batch_size):\n\t\tif self.test_scratch_pointer + batch_size >= self.num_test_cases:\n\t\t\tself.test_scratch_pointer = 0\n\t\tself.test_scratch_pointer += batch_size\n\t\txyzs = self.xyz_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tZs = self.Z_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tenergies = self.energy_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tdipoles = self.dipole_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tquadrupoles = self.quadrupole_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tgradients = self.gradient_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tnum_atoms = self.num_atoms_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tmulliken_charges = self.mulliken_charges_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\treturn [xyzs, Zs, dipoles, quadrupoles, gradients, num_atoms, mulliken_charges]\n\n\tdef get_energy_test_batch(self, batch_size):\n\t\tif self.test_scratch_pointer + batch_size >= self.num_test_cases:\n\t\t\tself.test_scratch_pointer = 0\n\t\tself.test_scratch_pointer += batch_size\n\t\txyzs = self.xyz_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tZs = self.Z_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tenergies = self.energy_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\t# dipoles = self.dipole_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tgradients = self.gradient_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\t\tnum_atoms = self.num_atoms_data[self.test_idxs[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]]\n\n\t\t# xyzs = self.xyz_data[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\t# Zs = self.Z_data[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\t# energies = self.energy_data[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\t# dipoles = self.dipole_data[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\t# num_atoms = self.num_atoms_data[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\t# gradients = self.gradient_data[self.test_scratch_pointer - batch_size:self.test_scratch_pointer]\n\t\t# NLEE = NeighborListSet(xyzs, num_atoms, False, False, None)\n\t\t# rad_eep = NLEE.buildPairs(self.coulomb_cutoff)\n\t\treturn [xyzs, Zs, energies, gradients, num_atoms]#, rad_eep]\n\n\tdef variable_summaries(self, var):\n\t\t""""""\n\t\tOPTIONALLY --- Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n\t\tThese actually take dozens of seconds to initialize.\n\t\t""""""\n\t\twith tf.name_scope(\'summaries\'):\n\t\t\tmean = tf.reduce_mean(var)\n\t\t\ttf.summary.scalar(\'mean\', mean)\n\t\twith tf.name_scope(\'stddev\'):\n\t\t\tstddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n\t\t\ttf.summary.scalar(\'stddev\', stddev)\n\t\t\ttf.summary.scalar(\'max\', tf.reduce_max(var))\n\t\t\ttf.summary.scalar(\'min\', tf.reduce_min(var))\n\t\t\ttf.summary.histogram(\'histogram\', var)\n\n\tdef variable_with_weight_decay(self, shape, stddev, weight_decay, name = None):\n\t\t""""""\n\t\tCreates random tensorflow variable from a truncated normal distribution with weight decay\n\n\t\tArgs:\n\t\t\tname: name of the variable\n\t\t\tshape: list of ints\n\t\t\tstddev: standard deviation of a truncated Gaussian\n\t\t\twd: add L2Loss weight decay multiplied by this float. If None, weight\n\t\t\tdecay is not added for this Variable.\n\n\t\tReturns:\n\t\t\tVariable Tensor\n\n\t\tNotes:\n\t\t\tNote that the Variable is initialized with a truncated normal distribution.\n\t\t\tA weight decay is added only if one is specified.\n\t\t""""""\n\t\tvariable = tf.Variable(tf.truncated_normal(shape, stddev = stddev, dtype = self.tf_precision), name = name)\n\t\tif weight_decay is not None:\n\t\t\tweightdecay = tf.multiply(tf.nn.l2_loss(variable), weight_decay, name=\'weight_loss\')\n\t\t\ttf.add_to_collection(\'losses\', weightdecay)\n\t\treturn variable\n\n\tdef fill_dipole_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl, self.Zs_pl, self.dipole_pl, self.quadrupole_pl, self.gradients_pl, self.num_atoms_pl], batch_data)}\n\t\treturn feed_dict\n\n\tdef fill_energy_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl, self.Zs_pl, self.energy_pl, self.gradients_pl, self.num_atoms_pl], batch_data)}\n\t\treturn feed_dict\n\n\tdef energy_inference(self, inp, indexs):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\tbranches=[]\n\t\tvariables=[]\n\t\toutput = tf.zeros([self.batch_size, self.max_num_atoms], dtype=self.tf_precision)\n\t\twith tf.name_scope(""energy_network""):\n\t\t\tfor e in range(len(self.elements)):\n\t\t\t\tbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tindex = indexs[e]\n\t\t\t\tfor i in range(len(self.hidden_layers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.elements[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self.variable_with_weight_decay(shape=[self.embedding_shape, self.hidden_layers[i]],\n\t\t\t\t\t\t\t\t\tstddev=math.sqrt(2.0 / float(self.embedding_shape)), weight_decay=self.weight_decay, name=""weights"")\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.hidden_layers[i]], dtype=self.tf_precision), name=\'biases\')\n\t\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\t\t\tvariables.append(weights)\n\t\t\t\t\t\t\tvariables.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.elements[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self.variable_with_weight_decay(shape=[self.hidden_layers[i-1], self.hidden_layers[i]],\n\t\t\t\t\t\t\t\t\tstddev=math.sqrt(2.0 / float(self.hidden_layers[i-1])), weight_decay=self.weight_decay, name=""weights"")\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.hidden_layers[i]], dtype=self.tf_precision), name=\'biases\')\n\t\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tvariables.append(weights)\n\t\t\t\t\t\t\tvariables.append(biases)\n\t\t\t\twith tf.name_scope(str(self.elements[e])+\'_regression_linear\'):\n\t\t\t\t\tweights = self.variable_with_weight_decay(shape=[self.hidden_layers[-1], 1],\n\t\t\t\t\t\t\tstddev=math.sqrt(2.0 / float(self.hidden_layers[-1])), weight_decay=self.weight_decay, name=""weights"")\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_precision), name=\'biases\')\n\t\t\t\t\tbranches[-1].append(tf.squeeze(tf.matmul(branches[-1][-1], weights) + biases, axis=1))\n\t\t\t\t\tvariables.append(weights)\n\t\t\t\t\tvariables.append(biases)\n\t\t\t\t\toutput += tf.scatter_nd(index, branches[-1][-1], [self.batch_size, self.max_num_atoms])\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\treturn tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size]), variables\n\n\tdef dipole_inference(self, inp, indexs, xyzs, natom):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\txyzs *= BOHRPERA\n\t\tbranches=[]\n\t\tvariables=[]\n\t\tatom_outputs_charge = []\n\t\toutput = tf.zeros([self.batch_size, self.max_num_atoms], dtype=self.tf_precision)\n\t\twith tf.name_scope(""dipole_network""):\n\t\t\tfor e in range(len(self.elements)):\n\t\t\t\tbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tindex = indexs[e]\n\t\t\t\tfor i in range(len(self.hidden_layers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.elements[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self.variable_with_weight_decay(shape=[self.embedding_shape, self.hidden_layers[i]],\n\t\t\t\t\t\t\t\t\tstddev=math.sqrt(2.0 / float(self.embedding_shape)), weight_decay=self.weight_decay, name=""weights"")\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.hidden_layers[i]], dtype=self.tf_precision), name=\'biases\')\n\t\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\t\t\tvariables.append(weights)\n\t\t\t\t\t\t\tvariables.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.elements[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self.variable_with_weight_decay(shape=[self.hidden_layers[i-1], self.hidden_layers[i]],\n\t\t\t\t\t\t\t\t\tstddev=math.sqrt(2.0 / float(self.hidden_layers[i-1])), weight_decay=self.weight_decay, name=""weights"")\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.hidden_layers[i]], dtype=self.tf_precision), name=\'biases\')\n\t\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tvariables.append(weights)\n\t\t\t\t\t\t\tvariables.append(biases)\n\t\t\t\twith tf.name_scope(str(self.elements[e])+\'_regression_linear\'):\n\t\t\t\t\tweights = self.variable_with_weight_decay(shape=[self.hidden_layers[-1], 1],\n\t\t\t\t\t\t\tstddev=math.sqrt(2.0 / float(self.hidden_layers[-1])), weight_decay=self.weight_decay, name=""weights"")\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_precision), name=\'biases\')\n\t\t\t\t\tbranches[-1].append(tf.squeeze(tf.matmul(branches[-1][-1], weights) + biases, axis=1))\n\t\t\t\t\tvariables.append(weights)\n\t\t\t\t\tvariables.append(biases)\n\t\t\t\t\toutput += tf.scatter_nd(index, branches[-1][-1], [self.batch_size, self.max_num_atoms])\n\n\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tnet_charge = tf.reduce_sum(output, axis=1)\n\t\t\tdelta_charge = net_charge / tf.cast(natom, self.tf_precision)\n\t\t\tcharges = output - tf.expand_dims(delta_charge, axis=1)\n\t\t\tdipole = tf.reduce_sum(xyzs * tf.expand_dims(charges, axis=-1), axis=1)\n\t\t\tif (PARAMS[""train_quadrupole""]):\n\t\t\t\tquadrupole_coords = 3 * tf.stack([tf.stack([tf.square(xyzs[...,0]), xyzs[...,0] * xyzs[...,1], tf.square(xyzs[...,1])], axis=-1),\n\t\t\t\t\t\t\t\t\ttf.stack([xyzs[...,0] * xyzs[...,2], xyzs[...,1] * xyzs[...,2], tf.square(xyzs[...,2])], axis=-1)], axis=-2)\n\t\t\t\tquadrupole_coords -= tf.expand_dims(tf.reduce_sum(tf.square(xyzs), axis=-1, keepdims=True), axis=-1)\n\t\t\t\tquadrupole = tf.reduce_sum(quadrupole_coords * tf.expand_dims(tf.expand_dims(charges, axis=-1), axis=-1), axis=1)\n\t\treturn dipole, quadrupole, charges, net_charge, variables\n\n\tdef optimizer(self, loss, learning_rate, momentum, variables):\n\t\t""""""\n\t\tSets up the training Ops.\n\t\tCreates a summarizer to track the loss over time in TensorBoard.\n\t\tCreates an optimizer and applies the gradients to all trainable variables.\n\t\tThe Op returned by this function is what must be passed to the\n\t\t`sess.run()` call to cause the model to train.\n\n\t\tArgs:\n\t\t\tloss: Loss tensor, from loss().\n\t\t\tlearning_rate: the learning rate to use for gradient descent.\n\n\t\tReturns:\n\t\t\ttrain_op: the tensorflow operation to call for training.\n\t\t""""""\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate)\n\t\tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t\ttrain_op = optimizer.minimize(loss, global_step=global_step, var_list=variables)\n\t\treturn train_op\n\n\tdef loss_op(self, error):\n\t\tloss = tf.nn.l2_loss(error)\n\t\treturn loss\n\n\tdef dipole_train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.num_train_cases\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_quadrupole_loss = 0.0\n\t\t# train_charge_loss = 0.0\n\t\ttrain_gradient_loss = 0.0\n\t\tnum_mols = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.get_dipole_train_batch(self.batch_size)\n\t\t\tfeed_dict = self.fill_dipole_feed_dict(batch_data)\n\t\t\t_, total_loss, dipole_loss, quadrupole_loss = self.sess.run([self.dipole_train_op, self.dipole_losses, self.dipole_loss, self.quadrupole_loss], feed_dict=feed_dict)\n\t\t\ttrain_loss += total_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\ttrain_quadrupole_loss += quadrupole_loss\n\t\t\t# train_charge_loss += charge_loss\n\t\t\tnum_mols += self.batch_size\n\t\tduration = time.time() - start_time\n\t\tLOGGER.info(""step: %5d    duration: %10.5f  train loss: %13.10f  dipole loss: %13.10f  quadrupole loss: %13.10f"", step, duration,\n\t\t\ttrain_loss / num_mols, train_dipole_loss / num_mols, train_quadrupole_loss / num_mols)\n\t\treturn\n\n\tdef energy_train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.num_train_cases\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_gradient_loss = 0.0\n\t\ttrain_rotation_loss = 0.0\n\t\tnum_mols = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.get_energy_train_batch(self.batch_size)\n\t\t\tfeed_dict = self.fill_energy_feed_dict(batch_data)\n\t\t\tif self.train_gradients and self.train_rotation:\n\t\t\t\t_, summaries, total_loss, energy_loss, gradient_loss, rotation_loss = self.sess.run([self.energy_train_op,\n\t\t\t\tself.summary_op, self.energy_losses, self.energy_loss, self.gradient_loss, self.rotation_loss], feed_dict=feed_dict)\n\t\t\t\ttrain_gradient_loss += gradient_loss\n\t\t\t\ttrain_rotation_loss += rotation_loss\n\t\t\telif self.train_gradients:\n\t\t\t\t_, summaries, total_loss, energy_loss, gradient_loss = self.sess.run([self.energy_train_op,\n\t\t\t\tself.summary_op, self.energy_losses, self.energy_loss, self.gradient_loss], feed_dict=feed_dict)\n\t\t\t\ttrain_gradient_loss += gradient_loss\n\t\t\telif self.train_rotation:\n\t\t\t\t_, summaries, total_loss, energy_loss, rotation_loss = self.sess.run([self.energy_train_op,\n\t\t\t\tself.summary_op, self.energy_losses, self.energy_loss, self.rotation_loss], feed_dict=feed_dict)\n\t\t\t\ttrain_rotation_loss += rotation_loss\n\t\t\telse:\n\t\t\t\tif self.profiling:\n\t\t\t\t\t_, summaries, total_loss, energy_loss = self.sess.run([self.energy_train_op,\n\t\t\t\t\tself.summary_op, self.energy_losses, self.energy_loss], feed_dict=feed_dict,\n\t\t\t\t\toptions=self.options, run_metadata=self.run_metadata)\n\t\t\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t\t\twith open(\'timeline_step_%d.json\' % ministep, \'w\') as f:\n\t\t\t\t\t\tf.write(chrome_trace)\n\t\t\t\telse:\n\t\t\t\t\t_, summaries, total_loss, energy_loss = self.sess.run([self.energy_train_op,\n\t\t\t\t\tself.summary_op, self.energy_losses, self.energy_loss], feed_dict=feed_dict)\n\t\t\ttrain_loss += total_loss\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\tnum_mols += self.batch_size\n\t\t\tself.summary_writer.add_summary(summaries, step * int(Ncase_train/self.batch_size) + ministep)\n\t\tduration = time.time() - start_time\n\t\tself.print_epoch(step, duration, train_loss, train_energy_loss, train_gradient_loss, train_rotation_loss, num_mols)\n\t\treturn\n\n\tdef dipole_test_step(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tprint( ""testing..."")\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.num_test_cases\n\t\tnum_mols = 0\n\t\ttest_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_quadrupole_loss = 0.0\n\t\t# test_charge_loss = 0.0\n\t\ttest_epoch_dipole_labels, test_epoch_dipole_outputs = [], []\n\t\ttest_epoch_quadrupole_labels, test_epoch_quadrupole_outputs = [], []\n\t\ttest_net_charges = []\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data = self.get_dipole_test_batch(self.batch_size)\n\t\t\tfeed_dict = self.fill_dipole_feed_dict(batch_data[:-1])\n\t\t\tdipoles, dipole_labels, total_loss, dipole_loss, quadrupoles, quadrupole_labels, quadrupole_loss, charges = self.sess.run([self.dipoles, self.dipole_labels,\n\t\t\tself.dipole_losses, self.dipole_loss, self.quadrupoles, self.quadrupole_pl, self.quadrupole_loss, self.charges],  feed_dict=feed_dict)\n\t\t\tnum_mols += self.batch_size\n\t\t\ttest_loss += total_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\ttest_quadrupole_loss += quadrupole_loss\n\t\t\t# test_charge_loss += charge_loss\n\t\t\ttest_epoch_dipole_labels.append(dipole_labels)\n\t\t\ttest_epoch_dipole_outputs.append(dipoles)\n\t\t\ttest_epoch_quadrupole_labels.append(quadrupole_labels)\n\t\t\ttest_epoch_quadrupole_outputs.append(quadrupoles)\n\t\t\t# test_net_charges.append(net_charges)\n\t\ttest_epoch_dipole_labels = np.concatenate(test_epoch_dipole_labels)\n\t\ttest_epoch_dipole_outputs = np.concatenate(test_epoch_dipole_outputs)\n\t\ttest_epoch_dipole_errors = test_epoch_dipole_labels - test_epoch_dipole_outputs\n\t\ttest_epoch_quadrupole_labels = np.concatenate(test_epoch_quadrupole_labels)\n\t\ttest_epoch_quadrupole_outputs = np.concatenate(test_epoch_quadrupole_outputs)\n\t\ttest_epoch_quadrupole_errors = test_epoch_quadrupole_labels - test_epoch_quadrupole_outputs\n\t\t# test_net_charges = np.concatenate(test_net_charges)\n\t\tduration = time.time() - start_time\n\t\t# for i in [random.randint(0, self.batch_size - 1) for _ in xrange(20)]:\n\t\t# \tLOGGER.info(""Net Charges: %11.8f"", test_net_charges[i])\n\t\tfor i in [random.randint(0, self.batch_size - 1) for _ in xrange(20)]:\n\t\t\tLOGGER.info(""Dipole label: %s  Dipole output: %s"", test_epoch_dipole_labels[i], test_epoch_dipole_outputs[i])\n\t\tfor i in [random.randint(0, self.batch_size - 1) for _ in xrange(20)]:\n\t\t\tLOGGER.info(""Quadrupole label: %s  Quadrupole output: %s"", test_epoch_quadrupole_labels[i], test_epoch_quadrupole_outputs[i])\n\t\t# LOGGER.info(""MAE  Dipole: %11.8f  Net Charge: %11.8f"", np.mean(np.abs(test_epoch_dipole_errors)), np.mean(np.abs(test_net_charges)))\n\t\t# LOGGER.info(""MSE  Dipole: %11.8f  Net Charge: %11.8f"", np.mean(test_epoch_dipole_errors), np.mean(test_net_charges))\n\t\t# LOGGER.info(""RMSE Dipole: %11.8f  Net Charge: %11.8f"", np.sqrt(np.mean(np.square(test_epoch_dipole_errors))), np.sqrt(np.mean(np.square(test_net_charges))))\n\t\tLOGGER.info(""MAE  Dipole: %11.8f  MAE  Quadrupole: %11.8f"", np.mean(np.abs(test_epoch_dipole_errors)), np.mean(np.abs(test_epoch_quadrupole_errors)))\n\t\tLOGGER.info(""MSE  Dipole: %11.8f  MSE  Quadrupole: %11.8f"", np.mean(test_epoch_dipole_errors), np.mean(test_epoch_quadrupole_errors))\n\t\tLOGGER.info(""RMSE Dipole: %11.8f  RMSE Quadrupole: %11.8f"", np.sqrt(np.mean(np.square(test_epoch_dipole_errors))), np.sqrt(np.mean(np.square(test_epoch_quadrupole_errors))))\n\t\tfor i in range(10):\n\t\t\tprint(""Charge     "" + str(i) + "": "" + str(charges[i]))\n\t\t\tprint(""Batch      "" + str(i) + "": "" + str(batch_data[-1][i]))\n\t\t\tprint(""Net Charge "" + str(i) + "": "" + str(np.sum(charges[i])))\n\t\tLOGGER.info(""step: %5d    duration: %10.5f   test loss: %13.10f  dipole loss: %13.10f  quadrupole loss: %13.10f"", step, duration,\n\t\t\ttest_loss / num_mols, test_dipole_loss / num_mols, test_quadrupole_loss / num_mols)\n\t\treturn test_loss\n\n\tdef energy_test_step(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tprint( ""testing..."")\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.num_test_cases\n\t\tnum_mols = 0\n\t\ttest_energy_loss = 0.0\n\t\ttest_gradient_loss = 0.0\n\t\ttest_rotation_loss = 0.0\n\t\ttest_charge_loss = 0.0\n\t\ttest_epoch_energy_labels, test_epoch_energy_outputs = [], []\n\t\ttest_epoch_force_labels, test_epoch_force_outputs = [], []\n\t\tnum_atoms_epoch = []\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data = self.get_energy_test_batch(self.batch_size)\n\t\t\tfeed_dict = self.fill_energy_feed_dict(batch_data)\n\t\t\ttotal_energies, energy_labels, gradients, gradient_labels, total_loss, energy_loss, gradient_loss, rotation_loss, num_atoms, gaussian_params = self.sess.run([self.total_energy,\n\t\t\tself.energy_pl, self.gradients, self.gradient_labels, self.energy_losses, self.energy_loss,\n\t\t\tself.gradient_loss, self.rotation_loss, self.num_atoms_pl, self.gaussian_params],  feed_dict=feed_dict)\n\t\t\ttest_loss += total_loss\n\t\t\tnum_mols += self.batch_size\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_gradient_loss += gradient_loss\n\t\t\ttest_rotation_loss += rotation_loss\n\t\t\ttest_epoch_energy_labels.append(energy_labels)\n\t\t\ttest_epoch_energy_outputs.append(total_energies)\n\t\t\ttest_epoch_force_labels.append(-1.0 * gradient_labels)\n\t\t\ttest_epoch_force_outputs.append(-1.0 * gradients)\n\t\t\tnum_atoms_epoch.append(num_atoms)\n\t\ttest_epoch_energy_labels = np.concatenate(test_epoch_energy_labels)\n\t\ttest_epoch_energy_outputs = np.concatenate(test_epoch_energy_outputs)\n\t\ttest_epoch_energy_errors = test_epoch_energy_labels - test_epoch_energy_outputs\n\t\ttest_epoch_force_labels = np.concatenate(test_epoch_force_labels)\n\t\ttest_epoch_force_outputs = np.concatenate(test_epoch_force_outputs)\n\t\ttest_epoch_force_errors = test_epoch_force_labels - test_epoch_force_outputs\n\t\tnum_atoms_epoch = np.sum(np.concatenate(num_atoms_epoch))\n\t\tduration = time.time() - start_time\n\t\tfor i in [random.randint(0, self.batch_size - 1) for _ in xrange(20)]:\n\t\t\tLOGGER.info(""Energy label: %11.8f  Energy output: %11.8f"", test_epoch_energy_labels[i], test_epoch_energy_outputs[i])\n\t\tfor i in [random.randint(0, num_atoms_epoch - 1) for _ in xrange(20)]:\n\t\t\tLOGGER.info(""Forces label: %s  Forces output: %s"", test_epoch_force_labels[i], test_epoch_force_outputs[i])\n\t\tLOGGER.info(""MAE  Energy: %11.8f  Forces: %11.8f"", np.mean(np.abs(test_epoch_energy_errors)),\n\t\tnp.mean(np.abs(test_epoch_force_errors)))\n\t\tLOGGER.info(""MSE  Energy: %11.8f  Forces: %11.8f"", np.mean(test_epoch_energy_errors),\n\t\tnp.mean(test_epoch_force_errors))\n\t\tLOGGER.info(""RMSE Energy: %11.8f  Forces: %11.8f"", np.sqrt(np.mean(np.square(test_epoch_energy_errors))),\n\t\tnp.sqrt(np.mean(np.square(test_epoch_force_errors))))\n\t\tLOGGER.info(""Gaussian paramaters: %s"", gaussian_params)\n\t\tself.print_epoch(step, duration, test_loss, test_energy_loss, test_gradient_loss, test_rotation_loss, num_mols, testing=True)\n\t\treturn test_loss\n\n\nclass BehlerParinelloSymFunc(BehlerParinelloNetwork):\n\t""""""\n\tBehler-Parinello network using symmetry function embedding from RawEmbeddings.py\n\talso has sparse evaluation using an updated version of the\n\tneighbor list, and a polynomial cutoff coulomb interaction.\n\t""""""\n\tdef __init__(self, mol_set=None, name=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tmol_set (TensorMol.MSet object): a class which holds the training data\n\t\t\tname (str): a name used to recall this network\n\n\t\tNotes:\n\t\t\tif name != None, attempts to load a previously saved network, otherwise assumes a new network\n\t\t""""""\n\t\tBehlerParinelloNetwork.__init__(self, mol_set, name)\n\t\tif name == None:\n\t\t\tself.network_type = ""BPSymFunc""\n\t\t\tself.name = self.network_type+""_""+self.mol_set_name+""_""+time.strftime(""%a_%b_%d_%H.%M.%S_%Y"")\n\t\t\tself.network_directory = PARAMS[""networks_directory""]+self.name\n\t\t\tself.set_symmetry_function_params()\n\t\treturn\n\n\tdef set_symmetry_function_params(self):\n\t\tself.element_pairs = np.array([[self.elements[i], self.elements[j]] for i in range(len(self.elements)) for j in range(i, len(self.elements))])\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\n\t\t#Define radial grid parameters\n\t\tnum_radial_rs = PARAMS[""AN1_num_r_Rs""]\n\t\tself.radial_cutoff = PARAMS[""AN1_r_Rc""]\n\t\tself.radial_rs = self.radial_cutoff * np.linspace(0, (num_radial_rs - 1.0) / num_radial_rs, num_radial_rs)\n\n\t\t#Define angular grid parameters\n\t\tnum_angular_rs = PARAMS[""AN1_num_a_Rs""]\n\t\tnum_angular_theta_s = PARAMS[""AN1_num_a_As""]\n\t\tself.angular_cutoff = PARAMS[""AN1_a_Rc""]\n\t\tself.theta_s = 2.0 * np.pi * np.linspace(0, (num_angular_theta_s - 1.0) / num_angular_theta_s, num_angular_theta_s)\n\t\tself.angular_rs = self.angular_cutoff * np.linspace(0, (num_angular_rs - 1.0) / num_angular_rs, num_angular_rs)\n\t\treturn\n\n\tdef compute_normalization(self):\n\t\telements = tf.constant(self.elements, dtype = tf.int32)\n\t\telement_pairs = tf.constant(self.element_pairs, dtype = tf.int32)\n\t\tradial_rs = tf.constant(self.radial_rs, dtype = self.tf_precision)\n\t\tangular_rs = tf.constant(self.angular_rs, dtype = self.tf_precision)\n\t\ttheta_s = tf.constant(self.theta_s, dtype = self.tf_precision)\n\t\tradial_cutoff = tf.constant(self.radial_cutoff, dtype = self.tf_precision)\n\t\tangular_cutoff = tf.constant(self.angular_cutoff, dtype = self.tf_precision)\n\t\tzeta = tf.constant(self.zeta, dtype = self.tf_precision)\n\t\teta = tf.constant(self.eta, dtype = self.tf_precision)\n\t\txyzs_pl = tf.placeholder(self.tf_precision, shape=tuple([self.batch_size, self.max_num_atoms, 3]))\n\t\tZs_pl = tf.placeholder(tf.int32, shape=tuple([self.batch_size, self.max_num_atoms]))\n\t\tembeddings, molecule_indices = tf_symmetry_functions(xyzs_pl, Zs_pl, elements, element_pairs, radial_cutoff,\n\t\t\t\t\t\t\t\t\t\tangular_cutoff, radial_rs, angular_rs, theta_s, zeta, eta)\n\t\tembeddings_list = [[], [], [], []]\n\t\tlabels_list = []\n\n\t\tself.embeddings_max = []\n\t\tsess = tf.Session()\n\t\tsess.run(tf.global_variables_initializer())\n\t\tfor ministep in range (0, max(2, int(0.1 * self.num_train_cases/self.batch_size))):\n\t\t\tbatch_data = self.get_energy_train_batch(self.batch_size)\n\t\t\tlabels_list.append(batch_data[2])\n\t\t\tembedding, molecule_index = sess.run([embeddings, molecule_indices], feed_dict = {xyzs_pl:batch_data[0], Zs_pl:batch_data[1]})\n\t\t\tfor element in range(len(self.elements)):\n\t\t\t\tembeddings_list[element].append(embedding[element])\n\t\tsess.close()\n\t\tfor element in range(len(self.elements)):\n\t\t\tself.embeddings_max.append(np.amax(np.concatenate(embeddings_list[element])))\n\t\tlabels = np.concatenate(labels_list)\n\t\tself.labels_mean = np.mean(labels)\n\t\tself.labels_stddev = np.std(labels)\n\t\tself.train_scratch_pointer = 0\n\n\t\t#Set the embedding and label shape\n\t\tself.embedding_shape = embedding[0].shape[1]\n\t\tself.label_shape = labels[0].shape\n\t\treturn\n\n\tdef train_prepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, self.max_num_atoms, 3])\n\t\t\tself.Zs_pl = tf.placeholder(tf.int32, shape=[self.batch_size, self.max_num_atoms])\n\t\t\tself.energy_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size])\n\t\t\tself.dipole_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, 3])\n\t\t\tself.quadrupole_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, 2, 3])\n\t\t\tself.gradients_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, self.max_num_atoms, 3])\n\t\t\tself.num_atoms_pl = tf.placeholder(tf.int32, shape=[self.batch_size])\n\t\t\tself.Reep_pl = tf.placeholder(tf.int32, shape=[None, 3])\n\n\t\t\tself.dipole_labels = self.dipole_pl\n\t\t\tself.quadrupole_labels = self.quadrupole_pl\n\n\t\t\telements = tf.constant(self.elements, dtype = tf.int32)\n\t\t\telement_pairs = tf.constant(self.element_pairs, dtype = tf.int32)\n\t\t\tradial_rs = tf.Variable(self.radial_rs, trainable=False, dtype = self.tf_precision)\n\t\t\tangular_rs = tf.Variable(self.angular_rs, trainable=False, dtype = self.tf_precision)\n\t\t\ttheta_s = tf.Variable(self.theta_s, trainable=False, dtype = self.tf_precision)\n\t\t\tradial_cutoff = tf.Variable(self.radial_cutoff, trainable=False, dtype = self.tf_precision)\n\t\t\tangular_cutoff = tf.Variable(self.angular_cutoff, trainable=False, dtype = self.tf_precision)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_precision)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_precision)\n\t\t\telu_width = tf.Variable(self.elu_width * BOHRPERA, trainable=False, dtype = self.tf_precision)\n\t\t\tdsf_alpha = tf.Variable(self.dsf_alpha, trainable=False, dtype = self.tf_precision)\n\t\t\tcoulomb_cutoff = tf.Variable(self.coulomb_cutoff, trainable=False, dtype = self.tf_precision)\n\n\t\t\tembeddings_max = tf.constant(self.embeddings_max, dtype = self.tf_precision)\n\t\t\tlabels_mean = tf.constant(self.labels_mean, dtype = self.tf_precision)\n\t\t\tlabels_stddev = tf.constant(self.labels_stddev, dtype = self.tf_precision)\n\t\t\tnum_atoms_batch = tf.reduce_sum(self.num_atoms_pl)\n\n\t\t\tembeddings, mol_idx = tf_symmetry_functions(self.xyzs_pl, self.Zs_pl, elements,\n\t\t\t\t\telement_pairs, radial_cutoff, angular_cutoff, radial_rs, angular_rs, theta_s, zeta, eta)\n\t\t\tfor element in range(len(self.elements)):\n\t\t\t\tembeddings[element] /= embeddings_max[element]\n\t\t\tnorm_bp_energy, energy_variables = self.energy_inference(embeddings, mol_idx)\n\t\t\tself.bp_energy = (norm_bp_energy * self.labels_stddev) + self.labels_mean\n\n\t\t\tif self.train_dipole:\n\t\t\t\tself.dipoles, self.quadrupoles, self.charges, self.net_charge, dipole_variables = self.dipole_inference(embeddings, mol_idx, self.xyzs_pl, self.num_atoms_pl)\n\t\t\t\tif (PARAMS[""OPR12""]==""DSF""):\n\t\t\t\t\tself.coulomb_energy = tf_coulomb_dsf_elu(rotated_xyzs, self.charges, self.Reep_pl, elu_width, dsf_alpha, coulomb_cutoff)\n\t\t\t\telif (PARAMS[""OPR12""]==""Poly""):\n\t\t\t\t\tself.coulomb_energy = PolynomialRangeSepCoulomb(self.xyzs_pl, self.charges, self.Reep_pl, 5.0, 12.0, 5.0)\n\t\t\t\tself.total_energy = self.bp_energy + self.coulomb_energy\n\t\t\t\tself.dipole_loss = self.loss_op(self.dipoles - self.dipole_pl)\n\t\t\t\tself.quadrupole_loss = self.loss_op(self.quadrupoles - self.quadrupole_pl)\n\t\t\t\ttf.add_to_collection(\'dipole_losses\', self.dipole_loss)\n\t\t\t\ttf.add_to_collection(\'quadrupole_losses\', self.quadrupole_loss)\n\t\t\t\tself.dipole_losses = tf.add_n(tf.get_collection(\'dipole_losses\'))\n\t\t\t\ttf.summary.scalar(""dipole losses"", self.dipole_losses)\n\t\t\t\tself.dipole_train_op = self.optimizer(self.dipole_losses, self.learning_rate, self.momentum, dipole_variables)\n\t\t\telse:\n\t\t\t\tself.total_energy = self.bp_energy\n\n\t\t\tself.gradients = tf.gather_nd(tf.gradients(self.bp_energy, self.xyzs_pl)[0], tf.where(tf.not_equal(self.Zs_pl, 0)))\n\t\t\tself.gradient_labels = tf.gather_nd(self.gradients_pl, tf.where(tf.not_equal(self.Zs_pl, 0)))\n\t\t\tself.energy_loss = self.loss_op(self.total_energy - self.energy_pl)\n\t\t\ttf.summary.scalar(""energy loss"", self.energy_loss)\n\t\t\ttf.add_to_collection(\'energy_losses\', self.energy_loss)\n\t\t\tself.gradient_loss = self.loss_op(self.gradients - self.gradient_labels) / tf.cast(tf.reduce_sum(self.num_atoms_pl), self.tf_precision)\n\t\t\tif self.train_gradients:\n\t\t\t\ttf.add_to_collection(\'energy_losses\', self.gradient_loss)\n\t\t\t\ttf.summary.scalar(""gradient loss"", self.gradient_loss)\n\n\t\t\tself.energy_losses = tf.add_n(tf.get_collection(\'energy_losses\'))\n\t\t\ttf.summary.scalar(""energy losses"", self.energy_losses)\n\n\t\t\tself.energy_train_op = self.optimizer(self.energy_losses, self.learning_rate, self.momentum, energy_variables)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.network_directory, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\tif self.profiling:\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\tdef print_epoch(self, step, duration, loss, energy_loss, gradient_loss, num_mols, testing=False):\n\t\tif testing:\n\t\t\tLOGGER.info(""step: %5d  duration: %.3f  test loss: %.10f  energy loss: %.10f  gradient loss: %.10f"",\n\t\t\tstep, duration, loss / num_mols, energy_loss / num_mols, gradient_loss / num_mols)\n\t\telse:\n\t\t\tLOGGER.info(""step: %5d  duration: %.3f  train loss: %.10f  energy loss: %.10f  gradient loss: %.10f"",\n\t\t\tstep, duration, loss / num_mols, energy_loss / num_mols, gradient_loss / num_mols)\n\t\treturn\n\n\tdef evaluate_prepare(self):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\t#Define the placeholders to be fed in for each batch\n\t\t\tself.xyzs_pl = tf.placeholder(self.tf_precision, shape=tuple([self.batch_size, self.max_num_atoms, 3]))\n\t\t\tself.Zs_pl = tf.placeholder(tf.int32, shape=tuple([self.batch_size, self.max_num_atoms]))\n\n\t\t\t#Define the constants/Variables for the symmetry function basis\n\t\t\telements = tf.constant(self.elements, dtype = tf.int32)\n\t\t\telement_pairs = tf.constant(self.element_pairs, dtype = tf.int32)\n\t\t\tradial_rs = tf.Variable(self.radial_rs, trainable=False, dtype = self.tf_precision)\n\t\t\tangular_rs = tf.Variable(self.angular_rs, trainable=False, dtype = self.tf_precision)\n\t\t\ttheta_s = tf.Variable(self.theta_s, trainable=False, dtype = self.tf_precision)\n\t\t\tradial_cutoff = tf.constant(self.radial_cutoff, dtype = self.tf_precision)\n\t\t\tangular_cutoff = tf.constant(self.angular_cutoff, dtype = self.tf_precision)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_precision)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_precision)\n\n\t\t\t#Define normalization constants\n\t\t\tembeddings_max = tf.constant(self.embeddings_max, dtype = self.tf_precision)\n\t\t\tlabels_mean = tf.constant(self.labels_mean, dtype = self.tf_precision)\n\t\t\tlabels_stddev = tf.constant(self.labels_stddev, dtype = self.tf_precision)\n\n\t\t\t#Define the graph for computing the embedding, feeding through the network, and evaluating the loss\n\t\t\telement_embeddings, mol_indices = tf_symmetry_functions(self.xyzs_pl, self.Zs_pl, elements,\n\t\t\t\t\telement_pairs, radial_cutoff, angular_cutoff, radial_rs, angular_rs, theta_s, zeta, eta)\n\t\t\tfor element in range(len(self.elements)):\n\t\t\t\telement_embeddings[element] /= embeddings_max[element]\n\t\t\tself.normalized_output = self.inference(element_embeddings, mol_indices)\n\t\t\tself.bp_energy = (self.normalized_output * self.labels_stddev) + self.labels_mean\n\t\t\tself.gradients = tf.gradients(self.bp_energy, self.xyzs_pl)[0]\n\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.network_directory))\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.network_directory, self.sess.graph)\n\t\treturn\n\n\tdef evaluate_fill_feed_dict(self, xyzs, Zs):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl, self.Zs_pl], [xyzs, Zs])}\n\t\treturn feed_dict\n\n\tdef evaluate_mol(self, mol, eval_forces=True):\n\t\t""""""\n\t\tTakes coordinates and atomic numbers from a manager and feeds them into the network\n\t\tfor evaluation of the forces\n\n\t\tArgs:\n\t\t\txyzs (np.float): numpy array of atomic coordinates\n\t\t\tZs (np.int32): numpy array of atomic numbers\n\t\t""""""\n\t\tif not self.sess:\n\t\t\tprint(self.latest_checkpoint_file)\n\t\t\tprint(""loading the session.."")\n\t\t\tself.batch_size = 1\n\t\t\tself.num_atoms = mol.NAtoms()\n\t\t\tself.assign_activation()\n\t\t\tself.evaluate_prepare()\n\t\tatomization_energy = 0.0\n\t\tfor atom in mol.atoms:\n\t\t\tif atom in ele_U:\n\t\t\t\tatomization_energy += ele_U[atom]\n\t\txyzs = np.expand_dims(mol.coords, axis=0)\n\t\tZs = np.expand_dims(mol.atoms, axis=0)\n\t\tfeed_dict=self.evaluate_fill_feed_dict(xyzs, Zs)\n\t\tif eval_forces:\n\t\t\tenergy, gradients = self.sess.run([self.bp_energy, self.gradients], feed_dict=feed_dict)\n\t\t\treturn energy + atomization_energy, -gradients[0]\n\t\telse:\n\t\t\tenergy = self.sess.run(self.bp_energy, feed_dict=feed_dict)\n\t\t\treturn energy + atomization_energy\n\n\tdef evaluate_batch(self, mols, eval_forces=True):\n\t\t""""""\n\t\tTakes coordinates and atomic numbers from a manager and feeds them into the network\n\t\tfor evaluation of the forces\n\n\t\tArgs:\n\t\t\txyzs (np.float): numpy array of atomic coordinates\n\t\t\tZs (np.int32): numpy array of atomic numbers\n\t\t""""""\n\t\tif not self.sess:\n\t\t\tprint(self.latest_checkpoint_file)\n\t\t\tprint(""loading the session.."")\n\t\t\tself.batch_size = len(mols)\n\t\t\tself.max_num_atoms = max([mol.NAtoms() for mol in mols])\n\t\t\tself.assign_activation()\n\t\t\tself.evaluate_prepare()\n\t\t# if (max([mol.NAtoms() for mol in mols]) > self.max_num_atoms) or (len(mols) > self.batch_size):\n \t# \t\tself.max_num_atoms = max([mol.NAtoms() for mol in mols])\n\t\t# \tself.batch_size = len(mols)\n\t\t# \tself.evaluate_prepare()\n\t\txyzs = np.zeros((self.batch_size, self.max_num_atoms, 3))\n\t\tZs = np.zeros((self.batch_size, self.max_num_atoms))\n\t\tfor i, mol in enumerate(mols):\n\t\t\txyzs[i, :mol.NAtoms()] = mol.coords\n\t\t\tZs[i, :mol.NAtoms()] = mol.atoms\n\t\tfeed_dict=self.evaluate_fill_feed_dict(xyzs, Zs)\n\t\tenergy = self.sess.run(self.bp_energy, feed_dict=feed_dict)\n\t\treturn energy[:len(mols)]\n\n\nclass BehlerParinelloGauSH(BehlerParinelloNetwork):\n\t""""""\n\tBehler-Parinello network using symmetry function embedding from RawEmbeddings.py\n\talso has sparse evaluation using an updated version of the\n\tneighbor list, and a polynomial cutoff coulomb interaction.\n\t""""""\n\tdef __init__(self, mol_set=None, name=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tmol_set (TensorMol.MSet object): a class which holds the training data\n\t\t\tname (str): a name used to recall this network\n\n\t\tNotes:\n\t\t\tif name != None, attempts to load a previously saved network, otherwise assumes a new network\n\t\t""""""\n\t\tBehlerParinelloNetwork.__init__(self, mol_set, name)\n\t\tif name == None:\n\t\t\tself.network_type = ""BPGauSH""\n\t\t\tself.name = self.network_type+""_""+self.mol_set_name+""_""+time.strftime(""%a_%b_%d_%H.%M.%S_%Y"")\n\t\t\tself.network_directory = PARAMS[""networks_directory""]+self.name\n\t\t\tself.l_max = PARAMS[""SH_LMAX""]\n\t\t\tself.gaussian_params = PARAMS[""RBFS""]\n\t\t\tself.atomic_embed_factors = PARAMS[""ANES""]\n\t\treturn\n\n\tdef compute_normalization(self):\n\t\txyzs_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, self.max_num_atoms, 3])\n\t\tZs_pl = tf.placeholder(tf.int32, shape=[self.batch_size, self.max_num_atoms])\n\t\tnum_atoms_pl = tf.placeholder(tf.int32, shape=[self.batch_size])\n\t\tgaussian_params = tf.Variable(self.gaussian_params, trainable=False, dtype=self.tf_precision)\n\t\telements = tf.constant(self.elements, dtype = tf.int32)\n\n\t\trotation_params = tf.stack([np.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_precision),\n\t\t\t\tnp.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_precision),\n\t\t\t\ttf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_precision)], axis=-1, name=""rotation_params"")\n\t\trotated_xyzs = tf_random_rotate(xyzs_pl, rotation_params)\n\t\tembeddings, molecule_indices = tf_gauss_harmonics_echannel(rotated_xyzs, Zs_pl, elements, gaussian_params, self.l_max)\n\n\t\tembeddings_list = []\n\t\tfor element in range(len(self.elements)):\n\t\t\tembeddings_list.append([])\n\t\tlabels_list = []\n\t\tself.embeddings_mean = []\n\t\tself.embeddings_stddev = []\n\n\t\tsess = tf.Session()\n\t\tsess.run(tf.global_variables_initializer())\n\t\tfor ministep in range (0, max(2, int(0.1 * self.num_train_cases/self.batch_size))):\n\t\t\tbatch_data = self.get_energy_train_batch(self.batch_size)\n\t\t\tlabels_list.append(batch_data[2])\n\t\t\tembedding, molecule_index = sess.run([embeddings, molecule_indices],\n\t\t\t\t\t\t\t\t\tfeed_dict = {xyzs_pl:batch_data[0], Zs_pl:batch_data[1], num_atoms_pl:batch_data[4]})\n\t\t\tfor element in range(len(self.elements)):\n\t\t\t\tembeddings_list[element].append(embedding[element])\n\t\tsess.close()\n\t\tfor element in range(len(self.elements)):\n\t\t\tself.embeddings_mean.append(np.mean(np.concatenate(embeddings_list[element]), axis=0))\n\t\t\tself.embeddings_stddev.append(np.std(np.concatenate(embeddings_list[element]), axis=0))\n\t\tself.embeddings_mean = np.stack(self.embeddings_mean)\n\t\tself.embeddings_stddev = np.stack(self.embeddings_stddev)\n\t\tlabels = np.concatenate(labels_list)\n\t\tself.labels_mean = np.mean(labels)\n\t\tself.labels_stddev = np.std(labels)\n\t\tself.train_scratch_pointer = 0\n\n\t\t#Set the embedding and label shape\n\t\tself.embedding_shape = embedding[0].shape[1]\n\t\tself.label_shape = labels[0].shape\n\t\treturn\n\n\tdef train_prepare(self, restart=False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, self.max_num_atoms, 3])\n\t\t\tself.Zs_pl = tf.placeholder(tf.int32, shape=[self.batch_size, self.max_num_atoms])\n\t\t\tself.energy_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size])\n\t\t\tself.dipole_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, 3])\n\t\t\tself.quadrupole_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, 3])\n\t\t\tself.gradients_pl = tf.placeholder(self.tf_precision, shape=[self.batch_size, self.max_num_atoms, 3])\n\t\t\tself.num_atoms_pl = tf.placeholder(tf.int32, shape=[self.batch_size])\n\t\t\tself.Reep_pl = tf.placeholder(tf.int32, shape=[None, 3])\n\n\t\t\tself.gaussian_params = tf.Variable(self.gaussian_params, trainable=True, dtype=self.tf_precision)\n\t\t\telements = tf.Variable(self.elements, trainable=False, dtype = tf.int32)\n\t\t\tembeddings_mean = tf.Variable(self.embeddings_mean, trainable=False, dtype = self.tf_precision)\n\t\t\tembeddings_stddev = tf.Variable(self.embeddings_stddev, trainable=False, dtype = self.tf_precision)\n\t\t\tlabels_mean = tf.Variable(self.labels_mean, trainable=False, dtype = self.tf_precision)\n\t\t\tlabels_stddev = tf.Variable(self.labels_stddev, trainable=False, dtype = self.tf_precision)\n\t\t\telu_width = tf.Variable(self.elu_width * BOHRPERA, trainable=False, dtype = self.tf_precision)\n\t\t\tdsf_alpha = tf.Variable(self.dsf_alpha, trainable=False, dtype = self.tf_precision)\n\t\t\tcoulomb_cutoff = tf.Variable(self.coulomb_cutoff, trainable=False, dtype = self.tf_precision)\n\n\t\t\trotation_params = tf.stack([np.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_precision),\n\t\t\t\t\tnp.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_precision),\n\t\t\t\t\ttf.random_uniform([self.batch_size], minval=0.1, maxval=1.9, dtype=self.tf_precision)], axis=-1)\n\t\t\trotated_xyzs, rotated_gradients = tf_random_rotate(self.xyzs_pl, rotation_params, self.gradients_pl)\n\t\t\tself.dipole_labels = tf.squeeze(tf_random_rotate(tf.expand_dims(self.dipole_pl, axis=1), rotation_params))\n\t\t\tembeddings, molecule_indices = tf_gauss_harmonics_echannel(rotated_xyzs, self.Zs_pl,\n\t\t\t\t\t\t\t\t\t\t\telements, self.gaussian_params, self.l_max)\n\t\t\tfor element in range(len(self.elements)):\n\t\t\t\tembeddings[element] -= embeddings_mean[element]\n\t\t\t\tembeddings[element] /= embeddings_stddev[element]\n\t\t\tnorm_bp_energy, energy_variables = self.energy_inference(embeddings, molecule_indices)\n\t\t\tself.bp_energy = (norm_bp_energy * labels_stddev) + labels_mean\n\t\t\tif self.train_dipole:\n\t\t\t\tself.dipoles, self.charges, self.net_charge, dipole_variables = self.dipole_inference(embeddings, molecule_indices, rotated_xyzs, self.num_atoms_pl)\n\t\t\t\tif (PARAMS[""OPR12""]==""DSF""):\n\t\t\t\t\tself.coulomb_energy = tf_coulomb_dsf_elu(rotated_xyzs, self.charges, self.Reep_pl, elu_width, dsf_alpha, coulomb_cutoff)\n\t\t\t\telif (PARAMS[""OPR12""]==""Poly""):\n\t\t\t\t\tself.coulomb_energy = PolynomialRangeSepCoulomb(rotated_xyzs, self.charges, self.Reep_pl, 5.0, 12.0, 5.0)\n\t\t\t\tself.total_energy = self.bp_energy + self.coulomb_energy\n\t\t\t\tself.dipole_loss = self.loss_op(self.dipoles - self.dipole_labels)\n\t\t\t\ttf.add_to_collection(\'dipole_losses\', self.dipole_loss)\n\t\t\t\tself.dipole_losses = tf.add_n(tf.get_collection(\'dipole_losses\'))\n\t\t\t\ttf.summary.scalar(""dipole losses"", self.dipole_losses)\n\t\t\t\tself.dipole_train_op = self.optimizer(self.dipole_losses, self.learning_rate, self.momentum, dipole_variables)\n\t\t\telse:\n\t\t\t\tself.total_energy = self.bp_energy\n\n\t\t\txyz_grad, rot_grad = tf.gradients(self.total_energy, [rotated_xyzs, rotation_params])\n\t\t\tself.gradients = tf.gather_nd(xyz_grad, tf.where(tf.not_equal(self.Zs_pl, 0)))\n\t\t\tself.gradient_labels = tf.gather_nd(rotated_gradients, tf.where(tf.not_equal(self.Zs_pl, 0)))\n\n\t\t\tself.energy_loss = self.loss_op(self.total_energy - self.energy_pl)\n\t\t\ttf.summary.scalar(""energy loss"", self.energy_loss)\n\t\t\ttf.add_to_collection(\'energy_losses\', self.energy_loss)\n\t\t\tself.gradient_loss = self.loss_op(self.gradients - self.gradient_labels) / tf.cast(tf.reduce_sum(self.num_atoms_pl), self.tf_precision)\n\t\t\tself.rotation_loss = self.loss_op(rot_grad) / 500.0\n\t\t\tif self.train_gradients:\n\t\t\t\ttf.add_to_collection(\'energy_losses\', self.gradient_loss)\n\t\t\t\ttf.summary.scalar(""gradient loss"", self.gradient_loss)\n\t\t\tif self.train_rotation:\n\t\t\t\ttf.add_to_collection(\'energy_losses\', self.rotation_loss)\n\t\t\t\ttf.summary.scalar(""rotational loss"", self.rotation_loss)\n\n\t\t\t# barrier_function = 1.e5 * tf.concat([tf.pow((0.15 - self.gaussian_params), 3.0), tf.expand_dims(tf.pow((self.gaussian_params[:,0] - 6.1), 3.0), axis=-1),\n\t\t\t# \t\t\ttf.expand_dims(tf.pow((self.gaussian_params[:,1] - 3.6), 3.0), axis=-1)], axis=1)\n\t\t\t# truncated_barrier_function = tf.reduce_sum(tf.where(tf.greater(barrier_function, 0.0),\n\t\t\t# \t\t\t\t\tbarrier_function, tf.zeros_like(barrier_function)))\n\t\t\t# gaussian_overlap_loss = tf.square(0.001 / tf.reduce_min(tf.self_adjoint_eig(tf_gaussian_overlap(self.gaussian_params))[0]))\n\t\t\t# tf.add_to_collection(\'dipole_losses\', truncated_barrier_function)\n\t\t\t# tf.add_to_collection(\'dipole_losses\', gaussian_overlap_loss)\n\n\t\t\tself.energy_losses = tf.add_n(tf.get_collection(\'energy_losses\'))\n\t\t\ttf.summary.scalar(""energy losses"", self.energy_losses)\n\n\t\t\tself.energy_train_op = self.optimizer(self.energy_losses, self.learning_rate, self.momentum, energy_variables)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.network_directory, self.sess.graph)\n\t\t\tif restart:\n\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.network_directory))\n\t\t\telse:\n\t\t\t\tinit = tf.global_variables_initializer()\n\t\t\t\tself.sess.run(init)\n\t\t\tif self.profiling:\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\tdef print_epoch(self, step, duration, loss, energy_loss, gradient_loss, rotation_loss, num_mols, testing=False):\n\t\tif testing:\n\t\t\tLOGGER.info(""step: %5d  duration: %.3f  test loss: %.10f  energy loss: %.10f  gradient loss: %.10f  rotation loss: %.10f"",\n\t\t\tstep, duration, loss / num_mols, energy_loss / num_mols, gradient_loss / num_mols, rotation_loss / num_mols)\n\t\telse:\n\t\t\tLOGGER.info(""step: %5d  duration: %.3f  train loss: %.10f  energy loss: %.10f  gradient loss: %.10f  rotation loss: %.10f"",\n\t\t\tstep, duration, loss / num_mols, energy_loss / num_mols, gradient_loss / num_mols, rotation_loss / num_mols)\n\t\treturn\n\n\tdef evaluate_prepare(self):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl = tf.placeholder(self.tf_precision, shape=tuple([None, self.max_num_atoms, 3]))\n\t\t\tself.Zs_pl = tf.placeholder(tf.int32, shape=tuple([None, self.max_num_atoms]))\n\t\t\tself.num_atoms_pl = tf.placeholder(tf.int32, shape=[None])\n\t\t\tself.Reep_pl = tf.placeholder(tf.int32, shape=[None,3])\n\n\t\t\tself.gaussian_params = tf.Variable(self.gaussian_params, trainable=False, dtype=self.tf_precision)\n\t\t\telements = tf.Variable(self.elements, trainable=False, dtype = tf.int32)\n\t\t\tembeddings_mean = tf.Variable(self.embeddings_mean, trainable=False, dtype = self.tf_precision)\n\t\t\tembeddings_stddev = tf.Variable(self.embeddings_stddev, trainable=False, dtype = self.tf_precision)\n\t\t\tlabels_mean = tf.Variable(self.labels_mean, trainable=False, dtype = self.tf_precision)\n\t\t\tlabels_stddev = tf.Variable(self.labels_stddev, trainable=False, dtype = self.tf_precision)\n\t\t\telu_width = tf.Variable(self.elu_width * BOHRPERA, trainable=False, dtype = self.tf_precision)\n\t\t\tdsf_alpha = tf.Variable(self.dsf_alpha, trainable=False, dtype = self.tf_precision)\n\t\t\tcoulomb_cutoff = tf.Variable(self.coulomb_cutoff, trainable=False, dtype = self.tf_precision)\n\n\t\t\t# self.tiled_xyzs = tf.tile(tf.expand_dims(self.xyzs_pl, axis=1), [1, 100, 1, 1])\n\t\t\t# self.tiled_Zs = tf.tile(tf.expand_dims(self.Zs_pl, axis=1), [1, 100, 1])\n\t\t\t# self.rotation_params = tf.tile(tf.expand_dims(tf.concat([np.pi * tf.expand_dims(tf.tile(tf.linspace(0.0, 2.0, 5), [20]), axis=1),\n\t\t\t# \t\tnp.pi * tf.reshape(tf.tile(tf.expand_dims(tf.linspace(0.0, 2.0, 5), axis=1), [1,20]), [100,1]),\n\t\t\t# \t\ttf.reshape(tf.tile(tf.expand_dims(tf.expand_dims(tf.linspace(0.1, 1.9, 4), axis=1),\n\t\t\t# \t\taxis=2), [5,1,5]), [100,1])], axis=1), axis=0), [self.batch_size, 1, 1])\n\t\t\t# self.rotated_xyzs = tf_random_rotate(self.tiled_xyzs, self.rotation_params)\n\t\t\t# self.rotated_xyzs = tf.reshape(self.rotated_xyzs, [-1, self.num_atoms, 3])\n\t\t\t# # self.rotated_gradients = tf.reshape(rotated_gradients, [-1, self.max_num_atoms, 3])\n\t\t\t# self.tiled_Zs = tf.reshape(self.tiled_Zs, [-1, self.num_atoms])\n\n\t\t\t# tiled_xyzs = tf.tile(self.xyzs_pl, [self.batch_size, 1, 1])\n\t\t\t# tiled_Zs = tf.tile(self.Zs_pl, [self.batch_size, 1])\n\t\t\t# rotation_params = tf.concat([np.pi * tf.expand_dims(tf.tile(tf.linspace(0.001, 1.999, 5), [20]), axis=1),\n\t\t\t# \t\tnp.pi * tf.reshape(tf.tile(tf.expand_dims(tf.linspace(0.001, 1.999, 5), axis=1), [1,20]), [100,1]),\n\t\t\t# \t\ttf.reshape(tf.tile(tf.expand_dims(tf.expand_dims(tf.linspace(0.001, 1.999, 4), axis=1),\n\t\t\t# \t\taxis=2), [5,1,5]), [100,1])], axis=1)\n\t\t\t# rotation_params = tf.stack([np.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_precision),\n\t\t\t# \t\tnp.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_precision),\n\t\t\t# \t\ttf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_precision)], axis=-1, name=""rotation_params"")\n\t\t\t# rotated_xyzs = tf_random_rotate(tiled_xyzs, rotation_params)\n\t\t\tembeddings, molecule_indices = tf_gaussian_spherical_harmonics_channel(self.xyzs_pl,\n\t\t\t\t\t\t\t\t\t\t\tself.Zs_pl, elements, self.gaussian_params, self.l_max)\n\t\t\tfor element in range(len(self.elements)):\n\t\t\t\tembeddings[element] -= embeddings_mean[element]\n\t\t\t\tembeddings[element] /= embeddings_stddev[element]\n\t\t\tself.dipoles, self.charges, self.net_charge, dipole_variables = self.dipole_inference(embeddings, molecule_indices, self.xyzs_pl, self.num_atoms_pl)\n\t\t\tself.coulomb_energy = tf_coulomb_dsf_elu(self.xyzs_pl, self.charges, self.Reep_pl, elu_width, dsf_alpha, coulomb_cutoff)\n\t\t\tnorm_bp_energy, energy_variables = self.energy_inference(embeddings, molecule_indices)\n\t\t\tself.bp_energy = (norm_bp_energy * labels_stddev) + labels_mean\n\t\t\tself.total_energy = self.bp_energy + self.coulomb_energy\n\t\t\tself.gradients = tf.gradients(self.total_energy, self.xyzs_pl)[0]\n\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver()\n\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.network_directory))\n\t\treturn\n\n\tdef evaluate_fill_feed_dict(self, xyzs, Zs, num_atoms, Reep):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl, self.Zs_pl, self.num_atoms_pl, self.Reep_pl], [xyzs, Zs, num_atoms, Reep])}\n\t\treturn feed_dict\n\n\tdef evaluate_mol(self, mol, eval_forces=True):\n\t\t""""""\n\t\tTakes coordinates and atomic numbers from a manager and feeds them into the network\n\t\tfor evaluation of the forces\n\n\t\tArgs:\n\t\t\txyzs (np.float): numpy array of atomic coordinates\n\t\t\tZs (np.int32): numpy array of atomic numbers\n\t\t""""""\n\t\tif not self.sess:\n\t\t\tprint(""loading the session.."")\n\t\t\tself.gaussian_params = PARAMS[""RBFS""][:self.number_radial]\n\t\t\tself.assign_activation()\n\t\t\tself.max_num_atoms = mol.NAtoms()\n\t\t\tself.batch_size = 1\n\t\t\tself.evaluate_prepare()\n\t\tif mol.NAtoms() > self.max_num_atoms:\n\t\t\tprint(""Atoms and max atoms"", mol.NAtoms(), self.max_num_atoms)\n\t\t\tself.sess.close()\n\t\t\ttf.reset_default_graph()\n\t\t\tself.max_num_atoms = mol.NAtoms()\n\t\t\tself.evaluate_prepare()\n\t\txyzs_feed = np.zeros((1,self.max_num_atoms, 3))\n\t\txyzs_feed[0,:mol.NAtoms()] = mol.coords\n\t\tZs_feed = np.zeros((1,self.max_num_atoms), dtype=np.int32)\n\t\tZs_feed[0,:mol.NAtoms()] = mol.atoms\n\t\tnum_atoms_feed = np.zeros((1), dtype=np.int32)\n\t\tnum_atoms_feed[0] = mol.NAtoms()\n\t\tNLEE = NeighborListSet(xyzs_feed, num_atoms_feed, False, False, None)\n\t\trad_eep = NLEE.buildPairs(self.coulomb_cutoff)\n\t\tfeed_dict=self.evaluate_fill_feed_dict(xyzs_feed, Zs_feed, num_atoms_feed, rad_eep)\n\t\tatomization_energy = 0.0\n\t\tfor atom in mol.atoms:\n\t\t\tif atom in ele_U:\n\t\t\t\tatomization_energy += ele_U[atom]\n\t\tif eval_forces:\n\t\t\tenergy, gradients = self.sess.run([self.total_energy, self.gradients], feed_dict=feed_dict)\n\t\t\tforces = -gradients[0,:mol.NAtoms()]\n\t\t\treturn energy + atomization_energy, forces\n\t\telse:\n\t\t\tenergy = self.sess.run(self.total_energy, feed_dict=feed_dict)\n\t\t\treturn energy + atomization_energy\n'"
TensorMol/TFNetworks/TFBehlerParinelloSymEE.py,576,"b'""""""\nThese instances are re-writes of the convoluted instances found in TFMolInstanceDirect.\n\nI would still like the following changes:\n- Independence from any manager.\n- Inheritance from a re-written instance base class.\n- Removal of any dependence on TensorMolData\n- Removal of any dependence on TFInstance.\n\nBut at least these are a first step.  JAP 12/2017.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .TFInstance import *\nfrom ..Containers.TensorMolData import *\nfrom .TFMolInstance import *\nfrom ..ForceModels.ElectrostaticsTF import *\nfrom ..ForceModifiers.Neighbors import *\nfrom ..TFDescriptors.RawSymFunc import *\nfrom tensorflow.python.client import timeline\nimport time\nimport threading\n\nclass MolInstance_DirectBP_EandG_SymFunction(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tBehler Parinello Scheme with energy and gradient training.\n\tNO Electrostatic embedding.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t\tTrainable_: True for training, False for evalution\n\t\t\tForceType_: Deprecated\n\t\t""""""\n\t\tself.SFPa = None\n\t\tself.SFPr = None\n\t\tself.Ra_cut = None\n\t\tself.Rr_cut = None\n\t\tself.HasANI1PARAMS = False\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.eles_np = np.asarray(self.eles).reshape((self.n_eles,1))\n\t\tself.eles_pairs = []\n\t\tfor i in range (len(self.eles)):\n\t\t\tfor j in range(i, len(self.eles)):\n\t\t\t\tself.eles_pairs.append([self.eles[i], self.eles[j]])\n\t\tself.eles_pairs_np = np.asarray(self.eles_pairs)\n\t\tif not self.HasANI1PARAMS:\n\t\t\tself.SetANI1Param()\n\t\tself.HiddenLayers = PARAMS[""HiddenLayers""]\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tprint (""self.activation_function_type: "", self.activation_function_type)\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\tself.xyzs_pl = None\n\t\tself.Zs_pl = None\n\t\tself.label_pl = None\n\t\tself.grads_pl = None\n\t\tself.sess = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\t\tself.learning_rate = PARAMS[""learning_rate""]\n\t\tself.suffix = PARAMS[""NetNameSuffix""]\n\t\tself.SetANI1Param()\n\t\tself.run_metadata = None\n\n\t\tself.GradScalar = PARAMS[""GradScalar""]\n\t\tself.EnergyScalar = PARAMS[""EnergyScalar""]\n\t\tself.TData.ele = self.eles_np\n\t\tself.TData.elep = self.eles_pairs_np\n\n\t\tself.NetType = ""RawBP_EandG""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+self.suffix\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.keep_prob = np.asarray(PARAMS[""KeepProb""])\n\t\tself.nlayer = len(PARAMS[""KeepProb""]) - 1\n\t\tself.monitor_mset =  PARAMS[""MonitorSet""]\n\n\tdef SetANI1Param(self, prec=np.float64):\n\t\t""""""\n\t\tGenerate ANI1 symmetry function parameter tensor.\n\t\t""""""\n\t\tself.Ra_cut = PARAMS[""AN1_a_Rc""]\n\t\tself.Rr_cut = PARAMS[""AN1_r_Rc""]\n\t\tzetas = np.array([[PARAMS[""AN1_zeta""]]], dtype = prec)\n\t\tetas = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_a_As = PARAMS[""AN1_num_a_As""]\n\t\tAN1_num_a_Rs = PARAMS[""AN1_num_a_Rs""]\n\t\tthetas = np.array([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype = prec)\n\t\trs =  np.array([ self.Ra_cut*i/AN1_num_a_Rs for i in range (0, AN1_num_a_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\t\tp1 = np.tile(np.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp3 = np.tile(np.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_a_Rs,1])\n\t\tp4 = np.tile(np.reshape(rs,[1,1,1,AN1_num_a_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\t\tSFPa = np.concatenate([p1,p2,p3,p4],axis=4)\n\t\tself.SFPa = np.transpose(SFPa, [4,0,1,2,3])\n\t\tetas_R = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_r_Rs = PARAMS[""AN1_num_r_Rs""]\n\t\trs_R =  np.array([ self.Rr_cut*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 2 x  neta X nr\n\t\tp1_R = np.tile(np.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\t\tp2_R = np.tile(np.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\t\tSFPr = np.concatenate([p1_R,p2_R],axis=2)\n\t\tself.SFPr = np.transpose(SFPr, [2,0,1])\n\t\tself.inshape = int(len(self.eles)*AN1_num_r_Rs + len(self.eles_pairs)*AN1_num_a_Rs*AN1_num_a_As)\n\t\tself.inshape_withencode = int(self.inshape + AN1_num_r_Rs)\n\t\t#self.inshape = int(len(self.eles)*AN1_num_r_Rs)\n\t\tp1 = np.tile(np.reshape(thetas,[AN1_num_a_As,1,1]),[1,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(rs,[1,AN1_num_a_Rs,1]),[AN1_num_a_As,1,1])\n\t\tSFPa2 = np.concatenate([p1,p2],axis=2)\n\t\tself.SFPa2 = np.transpose(SFPa2, [2,0,1])\n\t\tp1_new = np.reshape(rs_R,[AN1_num_r_Rs,1])\n\t\tself.SFPr2 = np.transpose(p1_new, [1,0])\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\t\tself.HasANI1PARAMS = True\n\t\tprint (""self.inshape:"", self.inshape)\n\n\tdef Clean(self):\n\t\t""""""\n\t\tClean Instance for pickle saving.\n\t\t""""""\n\t\tInstance.Clean(self)\n\t\t#self.tf_prec = None\n\t\tself.xyzs_pl, self.Zs_pl, self.label_pl, self.grads_pl, self.natom_pl = None, None, None, None, None\n\t\tself.check, self.options, self.run_metadata = None, None, None\n\t\tself.atom_outputs = None\n\t\tself.energy_loss = None\n\t\tself.Scatter_Sym, self.Sym_Index = None, None\n\t\tself.Radp_pl, self.Angt_pl = None, None\n\t\tself.Elabel_pl = None\n\t\tself.Etotal, self.Ebp, self.Ebp_atom = None, None, None\n\t\tself.gradient = None\n\t\tself.total_loss_dipole, self.energy_loss, self.grads_loss = None, None, None\n\t\tself.train_op_dipole, self.train_op_EandG = None, None\n\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG = None, None, None, None\n\t\tself.Radp_Ele_pl, self.Angt_Elep_pl = None, None\n\t\tself.mil_jk_pl, self.mil_j_pl = None, None\n\t\tself.keep_prob_pl = None\n\t\treturn\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tDefine Tensorflow graph for training.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Release(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl)\n\t\t\tself.Etotal, self.Ebp, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.keep_prob_pl)\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss = self.loss_op(self.Etotal, self.gradient, self.Elabel_pl, self.grads_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\traise Exception(""Please check your inputs"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.mil_j_pl] + [self.mil_jk_pl] + [self.natom_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\tdef energy_inference(self, inp, indexs, xyzs, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph for calculating energy.\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements.\n\t\t\txyzs: xyz coordinates of atoms.\n\t\t\tkeep_prob: dropout prob of each layer.\n\t\tReturns:\n\t\t\tThe BP graph energy output\n\t\t""""""\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.identity(bp_energy)\n\t\treturn total_energy, bp_energy, output\n\n\n\tdef loss_op(self, energy, energy_grads, Elabels, grads, natom):\n\t\t""""""\n\t\tlosss function that includes dipole loss, energy loss and gradient loss.\n\t\t""""""\n\t\tmaxatom=tf.cast(tf.shape(energy_grads)[2], self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels,name=""EnDiff""), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff,name=""EnL2"")\n\t\tgrads_diff = tf.multiply(tf.subtract(energy_grads, grads,name=""GradDiff""), tf.reshape(natom*maxatom, [1, self.batch_size, 1, 1]))\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff,name=""GradL2"")\n\t\tEandG_loss = tf.add(tf.multiply(energy_loss, self.EnergyScalar), tf.multiply(grads_loss, self.GradScalar),name=""MulLoss"")\n\t\tloss = tf.identity(EandG_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss\n\n\tdef training(self, loss, learning_rate, momentum):\n\t\t""""""Sets up the training Ops.\n\t\tCreates a summarizer to track the loss over time in TensorBoard.\n\t\tCreates an optimizer and applies the gradients to all trainable variables.\n\t\tThe Op returned by this function is what must be passed to the\n\t\t`sess.run()` call to cause the model to train.\n\t\tArgs:\n\t\tloss: Loss tensor, from loss().\n\t\tlearning_rate: The learning rate to use for gradient descent.\n\t\tReturns:\n\t\ttrain_op: The Op for training.\n\t\t""""""\n\t\ttf.summary.scalar(loss.op.name, loss)\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate,name=""Adam"")\n\t\tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t\ttrain_op = optimizer.minimize(loss, global_step=global_step, name=""trainop"")\n\t\treturn train_op\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size.\n\t\tTraining object including dipole, energy and gradient\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss, Etotal = self.sess.run([self.train_op, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.Etotal], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size) + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss, Etotal = self.sess.run([self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.Etotal], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, num_of_mols, duration, False)\n\t\treturn test_loss\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\t""""""\n\t\tThis the training loop for the united model.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(continue_training)\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_dipole_test_loss = float(\'inf\') # some big numbers\n\t\tmini_energy_test_loss = float(\'inf\')\n\t\tmini_test_loss = float(\'inf\')\n\t\tfor step in  range (0, mxsteps):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, step=step)\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\n\tdef profile_step(self, step):\n\t\t""""""\n\t\tPerform a single profiling step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, Etotal = self.sess.run([self.train_op, self.Etotal], feed_dict=self.fill_feed_dict(batch_data), options=self.options, run_metadata=self.run_metadata)\n\t\t\tprint (""inference time:"", time.time() - t)\n\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, \'minstep%d\' % ministep)\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\twith open(\'timeline_step_%d.json\' % ministep, \'w\') as f:\n\t\t\t\tf.write(chrome_trace)\n\t\treturn\n\n\tdef profile(self):\n\t\t""""""\n\t\tThis profiles a training step.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(False)\n\t\tself.profile_step(1)\n\t\treturn\n\n\tdef InTrainEval(self, mol_set, Rr_cut, Ra_cut, step=0):\n\t\t""""""\n\t\tEvaluted the network during training.\n\t\t""""""\n\t\tnmols = len(mol_set.mols)\n\t\tfor i in range(nmols, self.batch_size):\n\t\t\tmol_set.mols.append(mol_set.mols[-1])\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\txyzs = np.zeros((nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(Rr_cut, Ra_cut, self.eles_np, self.eles_pairs_np)\n\t\tbatch_data = [xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, mil_j, mil_jk, 1.0/natom]\n\t\tfeed_dict=self.fill_feed_dict(batch_data + [np.ones(self.nlayer+1)])\n\t\tEtotal, Ebp, Ebp_atom, gradient= self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.gradient], feed_dict=feed_dict)\n\t\tmonitor_data = [Etotal, Ebp, Ebp_atom, gradient]\n\t\tf = open(self.name+""_monitor_""+str(step)+"".dat"",""wb"")\n\t\tpickle.dump(monitor_data, f)\n\t\tf.close()\n\t\tprint (""calculating monitoring set.."")\n\t\treturn Etotal, Ebp, Ebp_atom, gradient\n\n\tdef print_training(self, step, loss, energy_loss, grads_loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f  energy_loss: %.10f  grad_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f energy_loss: %.10f  grad_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)))\n\t\treturn\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.AssignActivation()\n\t\t#print (""self.activation_function:\\n\\n"", self.activation_function)\n\t\tprint (""self.batch_size:"", self.batch_size, "" nmol:"", nmol)\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms or self.batch_size != nmol):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[np.ones(self.nlayer+1)])\n\t\tEtotal, Ebp, Ebp_atom, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.gradient], feed_dict=feed_dict)\n\t\treturn Etotal, Ebp, Ebp_atom, gradient\n\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGenerate Tensorflow graph of evalution.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Release(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl)\n\t\t\tself.Etotal, self.Ebp, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.keep_prob_pl)\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef energy_inference_periodic(self, inp, indexs, xyzs, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph for calculating the energy of periodic system\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.nreal], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.nreal]),[self.batch_size, self.nreal])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.identity(bp_energy, cc_energy)\n\t\treturn total_energy, bp_energy, output\n\n\n\tdef fill_feed_dict_periodic(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl]  + [self.mil_j_pl]  + [self.mil_jk_pl] + [self.natom_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\t@TMTiming(""EvalPeriodic"")\n\tdef evaluate_periodic(self, batch_data, nreal, DoForce = True):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph for a periodic system.\n\t\t""""""\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.nreal = nreal\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.AssignActivation()\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Periodic()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Periodic()\n\t\tt0 = time.time()\n\t\tfeed_dict=self.fill_feed_dict_periodic(batch_data+[np.ones(self.nlayer+1)])\n\t\tif (DoForce):\n\t\t\tEtotal, Ebp, Ebp_atom, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.gradient], feed_dict=feed_dict)\n\t\t\treturn Etotal, Ebp, Ebp_atom, gradient\n\t\telse:\n\t\t\tEtotal = self.sess.run(self.Etotal, feed_dict=feed_dict)\n\t\t\treturn Etotal\n\n\tdef EvalPrepare_Periodic(self,  continue_training =False):\n\t\t""""""\n\t\tGenerate Tensorlfow graph for evalution of periodic system.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Periodic(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl, self.nreal)\n\t\t\tself.Etotal, self.Ebp, self.Ebp_atom = self.energy_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.keep_prob_pl)\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\nclass MolInstance_DirectBP_EE_SymFunction(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t\tTrainable_: True for training, False for evalution\n\t\t\tForceType_: Deprecated\n\t\t""""""\n\t\tself.SFPa = None\n\t\tself.SFPr = None\n\t\tself.Ra_cut = None\n\t\tself.Rr_cut = None\n\t\tself.HasANI1PARAMS = False\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.eles_np = np.asarray(self.eles).reshape((self.n_eles,1))\n\t\tself.eles_pairs = []\n\t\tfor i in range (len(self.eles)):\n\t\t\tfor j in range(i, len(self.eles)):\n\t\t\t\tself.eles_pairs.append([self.eles[i], self.eles[j]])\n\t\tself.eles_pairs_np = np.asarray(self.eles_pairs)\n\t\tif not self.HasANI1PARAMS:\n\t\t\tself.SetANI1Param()\n\t\tself.HiddenLayers = PARAMS[""HiddenLayers""]\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tprint (""self.activation_function_type: "", self.activation_function_type)\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\tself.xyzs_pl = None\n\t\tself.Zs_pl = None\n\t\tself.label_pl = None\n\t\tself.grads_pl = None\n\t\tself.sess = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\n\t\tself.GradScalar = PARAMS[""GradScalar""]\n\t\tself.EnergyScalar = PARAMS[""EnergyScalar""]\n\t\tself.DipoleScalar = PARAMS[""DipoleScalar""]\n\t\tself.Ree_on  = PARAMS[""EECutoffOn""]\n\t\tself.Ree_off  = PARAMS[""EECutoffOff""]\n\t\tself.DSFAlpha = PARAMS[""DSFAlpha""]\n\t\tself.learning_rate_dipole = PARAMS[""learning_rate_dipole""]\n\t\tself.learning_rate_energy = PARAMS[""learning_rate_energy""]\n\t\tself.suffix = PARAMS[""NetNameSuffix""]\n\t\tself.SetANI1Param()\n\t\tself.run_metadata = None\n\n\t\tself.Training_Traget = ""Dipole""\n\t\tself.TData.ele = self.eles_np\n\t\tself.TData.elep = self.eles_pairs_np\n\n\t\tself.Training_Traget = ""Dipole""\n\t\tself.vdw_R = np.zeros(self.n_eles)\n\t\tself.C6 = np.zeros(self.n_eles)\n\t\tfor i, ele in enumerate(self.eles):\n\t\t\tself.C6[i] = C6_coff[ele]* (BOHRPERA*10.0)**6.0 / JOULEPERHARTREE # convert into a.u.\n\t\t\tself.vdw_R[i] = atomic_vdw_radius[ele]*BOHRPERA\n\n\t\tif self.Ree_on != 0.0:\n\t\t\traise Exception(""EECutoffOn should equal to zero in DSF_elu"")\n\t\tself.elu_width = PARAMS[""Elu_Width""]\n\t\tself.elu_shift = DSF(self.elu_width*BOHRPERA, self.Ree_off*BOHRPERA, self.DSFAlpha/BOHRPERA)\n\t\tself.elu_alpha = DSF_Gradient(self.elu_width*BOHRPERA, self.Ree_off*BOHRPERA, self.DSFAlpha/BOHRPERA)\n\n\t\tself.NetType = ""RawBP_EE_SymFunction""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+self.suffix\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.keep_prob = np.asarray(PARAMS[""KeepProb""])\n\t\tself.nlayer = len(PARAMS[""KeepProb""]) - 1\n\t\tself.monitor_mset =  PARAMS[""MonitorSet""]\n\n\tdef SetANI1Param(self, prec=np.float64):\n\t\t""""""\n\t\tGenerate ANI1 symmetry function paramter tensor.\n\t\t""""""\n\t\tself.Ra_cut = PARAMS[""AN1_a_Rc""]\n\t\tself.Rr_cut = PARAMS[""AN1_r_Rc""]\n\t\tzetas = np.array([[PARAMS[""AN1_zeta""]]], dtype = prec)\n\t\tetas = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_a_As = PARAMS[""AN1_num_a_As""]\n\t\tAN1_num_a_Rs = PARAMS[""AN1_num_a_Rs""]\n\t\tthetas = np.array([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype = prec)\n\t\trs =  np.array([ self.Ra_cut*i/AN1_num_a_Rs for i in range (0, AN1_num_a_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\t\tp1 = np.tile(np.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp3 = np.tile(np.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_a_Rs,1])\n\t\tp4 = np.tile(np.reshape(rs,[1,1,1,AN1_num_a_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\t\tSFPa = np.concatenate([p1,p2,p3,p4],axis=4)\n\t\tself.SFPa = np.transpose(SFPa, [4,0,1,2,3])\n\t\tetas_R = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_r_Rs = PARAMS[""AN1_num_r_Rs""]\n\t\trs_R =  np.array([ self.Rr_cut*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 2 x  neta X nr\n\t\tp1_R = np.tile(np.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\t\tp2_R = np.tile(np.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\t\tSFPr = np.concatenate([p1_R,p2_R],axis=2)\n\t\tself.SFPr = np.transpose(SFPr, [2,0,1])\n\t\tself.inshape = int(len(self.eles)*AN1_num_r_Rs + len(self.eles_pairs)*AN1_num_a_Rs*AN1_num_a_As)\n\t\tself.inshape_withencode = int(self.inshape + AN1_num_r_Rs)\n\t\t#self.inshape = int(len(self.eles)*AN1_num_r_Rs)\n\t\tp1 = np.tile(np.reshape(thetas,[AN1_num_a_As,1,1]),[1,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(rs,[1,AN1_num_a_Rs,1]),[AN1_num_a_As,1,1])\n\t\tSFPa2 = np.concatenate([p1,p2],axis=2)\n\t\tself.SFPa2 = np.transpose(SFPa2, [2,0,1])\n\t\tp1_new = np.reshape(rs_R,[AN1_num_r_Rs,1])\n\t\tself.SFPr2 = np.transpose(p1_new, [1,0])\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\t\tself.HasANI1PARAMS = True\n\t\tprint (""self.inshape:"", self.inshape)\n\n\tdef Clean(self):\n\t\t""""""\n\t\tClean Instance for pickle saving.\n\t\t""""""\n\t\tInstance.Clean(self)\n\t\t#self.tf_prec = None\n\t\tself.xyzs_pl, self.Zs_pl, self.label_pl, self.grads_pl = None, None, None, None\n\t\tself.check, self.options, self.run_metadata = None, None, None\n\t\tself.atom_outputs = None\n\t\tself.energy_loss, self.grads_loss, self.dipole_loss = None, None, None\n\t\tself.Scatter_Sym, self.Sym_Index = None, None\n\t\tself.Radp_pl, self.Angt_pl = None, None\n\t\tself.Elabel_pl, self.Dlabel_pl = None, None\n\t\tself.Reep_pl, self.natom_pl, self.AddEcc_pl = None, None, None\n\t\tself.Etotal, self.Ebp, self.Ecc, self.Ebp_atom, self.Evdw  = None, None, None, None, None\n\t\tself.dipole, self.charge = None, None\n\t\tself.energy_wb, self.dipole_wb = None, None\n\t\tself.gradient = None\n\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = None, None, None, None, None\n\t\tself.train_op_dipole, self.train_op_EandG = None, None\n\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = None, None, None, None, None\n\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = None, None\n\t\tself.Radp_Ele_pl, self.Angt_Elep_pl = None, None\n\t\tself.mil_jk_pl, self.mil_j_pl = None, None\n\t\tself.elu_width, self.elu_shift, self.elu_alpha = None, None, None\n\t\tself.keep_prob_pl = None\n\t\treturn\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tDefine Tensorflow graph for training.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Release(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\traise Exception(""Please check your inputs"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_pl] + [self.mil_j_pl] + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\tdef energy_inference(self, inp, indexs,  cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep, EE_cuton, EE_cutoff, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph for calculating energy.\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements.\n\t\t\tcc_energy: System Coulomb energy.\n\t\t\txyzs: xyz coordinates of atoms.\n\t\t\tZs: atomic number of atoms.\n\t\t\teles: list of element type.\n\t\t\tc6: Grimmer C6 coefficient.\n\t\t\tR_vdw: Van der waals cutoff.\n\t\t\tReep: Atom index of vdw pairs.\n\t\t\tEE_cuton: Where Coulomb is turned on.\n\t\t\tEE_cutoff: Where Coulomb is turned off.\n\t\t\tkeep_prob: dropout prob of each layer.\n\t\tReturns:\n\t\t\tThe BP graph energy output\n\t\t""""""\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLR(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep)\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output\n\n\tdef dipole_inference(self, inp, indexs, xyzs, natom, Elu_Width, EE_cutoff, Reep, AddEcc, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph for calculating dipole.\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements.\n\t\t\txyzs: xyz coordinates of atoms.\n\t\t\tnatom: 1/(max of number of atoms in the set).\n\t\t\tElu_Width: Width of the elu version of the Coulomb interaction.\n\t\t\tEE_cutoff: Where Coulomb is turned off\n\t\t\tReep: Atom index of vdw pairs.\n\t\t\tAddEcc: Whether add Coulomb energy to the total energy\n\t\t\tkeep_prob: dropout prob of each layer.\n\t\tReturns:\n\t\t\tThe BP graph charge and dipole  output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(charge_inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.MaxNAtoms])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzsInBohr,[self.batch_size*self.MaxNAtoms, 3]), tf.reshape(scaled_charge,[self.batch_size*self.MaxNAtoms, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.MaxNAtoms, 3]), axis=1)\n\t\tdef f1(): return TFCoulombEluSRDSFLR(xyzsInBohr, scaled_charge, Elu_Width*BOHRPERA, Reep, tf.cast(self.DSFAlpha, self.tf_prec), tf.cast(self.elu_alpha,self.tf_prec), tf.cast(self.elu_shift,self.tf_prec))\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)\n\t\treturn  cc_energy, dipole, scaled_charge, dipole_wb\n\n\tdef loss_op(self, energy, energy_grads, dipole, Elabels, grads, Dlabels, natom):\n\t\t""""""\n\t\tlosss function that includes dipole loss, energy loss and gradient loss.\n\t\t""""""\n\t\tmaxatom=tf.cast(tf.shape(energy_grads)[2], self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels,name=""EnDiff""), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff,name=""EnL2"")\n\t\tgrads_diff = tf.multiply(tf.subtract(energy_grads, grads,name=""GradDiff""), tf.reshape(natom*maxatom, [1, self.batch_size, 1, 1]))\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff,name=""GradL2"")\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels,name=""DipoleDiff""), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff,name=""DipL2"")\n\t\tEandG_loss = tf.add(tf.multiply(energy_loss, self.EnergyScalar), tf.multiply(grads_loss, self.GradScalar),name=""MulLoss"")\n\t\tloss = tf.add(EandG_loss, tf.multiply(dipole_loss, self.DipoleScalar))\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\tdef loss_op_dipole(self, energy, energy_grads, dipole, Elabels, grads, Dlabels, natom):\n\t\t""""""\n\t\tlosss function that includes dipole loss.\n\t\t""""""\n\t\tmaxatom=tf.cast(tf.shape(energy_grads)[2], self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.multiply(tf.subtract(energy_grads, grads), tf.reshape(natom*maxatom, [1, self.batch_size, 1, 1]))\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\tEandG_loss = tf.add(tf.multiply(energy_loss, self.EnergyScalar), tf.multiply(grads_loss, self.GradScalar))\n\t\tloss = tf.identity(dipole_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\tdef loss_op_EandG(self, energy, energy_grads, dipole, Elabels, grads, Dlabels, natom):\n\t\t""""""\n\t\tlosss function that includes energy loss and gradient loss.\n\t\t""""""\n\t\tmaxatom=tf.cast(tf.shape(energy_grads)[2], self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.multiply(tf.subtract(energy_grads, grads), tf.reshape(natom*maxatom, [1, self.batch_size, 1, 1]))\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\tEandG_loss = tf.add(tf.multiply(energy_loss, self.EnergyScalar), tf.multiply(grads_loss, self.GradScalar))\n\t\tloss = tf.identity(EandG_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\tdef training(self, loss, learning_rate, momentum, update_var=None):\n\t\t""""""Sets up the training Ops.\n\t\tCreates a summarizer to track the loss over time in TensorBoard.\n\t\tCreates an optimizer and applies the gradients to all trainable variables.\n\t\tThe Op returned by this function is what must be passed to the\n\t\t`sess.run()` call to cause the model to train.\n\t\tArgs:\n\t\tloss: Loss tensor, from loss().\n\t\tlearning_rate: The learning rate to use for gradient descent.\n\t\tReturns:\n\t\ttrain_op: The Op for training.\n\t\t""""""\n\t\ttf.summary.scalar(loss.op.name, loss)\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate,name=""Adam"")\n\t\t#optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n\t\tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t\tif update_var == None:\n\t\t\ttrain_op = optimizer.minimize(loss, global_step=global_step, name=""trainop"")\n\t\telse:\n\t\t\ttrain_op = optimizer.minimize(loss, global_step=global_step, var_list=update_var, name=""trainop"")\n\t\treturn train_op\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size.\n\t\tTraining object including dipole, energy and gradient\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+[PARAMS[""AddEcc""]] + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\treturn\n\n\n\tdef train_step_EandG(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\t\tTraining object including energy and dipole.\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_grads_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+[PARAMS[""AddEcc""]] + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, Evdw, mol_dipole, atom_charge = self.sess.run([self.train_op_EandG, self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.Evdw,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_grads_loss += grads_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""ministep... time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini, "" grads_loss:"", print_grads_loss/print_per_mini, "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_grads_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\treturn\n\n\n\tdef train_step_dipole(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\t\tTraining object including dipole.\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_grads_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [False] + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op_dipole, self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_grads_loss += grads_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""ministep... time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini, "" grads_loss:"", print_grads_loss/print_per_mini, "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_grads_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[PARAMS[""AddEcc""]] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration, False)\n\t\treturn test_loss\n\n\tdef test_dipole(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[False] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration, False)\n\t\treturn  test_loss\n\n\tdef test_EandG(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[PARAMS[""AddEcc""]] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration, False)\n\t\treturn  test_loss\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\t""""""\n\t\tThis the training loop for the united model.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(continue_training)\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_dipole_test_loss = float(\'inf\') # some big numbers\n\t\tmini_energy_test_loss = float(\'inf\')\n\t\tmini_test_loss = float(\'inf\')\n\t\tfor step in  range (0, mxsteps):\n\t\t\tif self.Training_Traget == ""EandG"":\n\t\t\t\tself.train_step_EandG(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_energy_loss = self.test_EandG(step)\n\t\t\t\t\tif test_energy_loss < mini_energy_test_loss:\n\t\t\t\t\t\tmini_energy_test_loss = test_energy_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\t\telif self.Training_Traget == ""Dipole"":\n\t\t\t\tself.train_step_dipole(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_dipole_loss = self.test_dipole(step)\n\t\t\t\t\tif test_dipole_loss < mini_dipole_test_loss:\n\t\t\t\t\t\tmini_dipole_test_loss = test_dipole_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\t\t\t\t\tif step >= PARAMS[""SwitchEpoch""]:\n\t\t\t\t\t\t\tself.Training_Traget = ""EandG""\n\t\t\t\t\t\t\tprint (""Switching to Energy and Gradient Learning..."")\n\t\t\telse:\n\t\t\t\tself.train_step(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_loss = self.test(step)\n\t\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef profile_step(self, step):\n\t\t""""""\n\t\tPerform a single profiling step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [PARAMS[""AddEcc""]] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op_dipole, self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data), options=self.options, run_metadata=self.run_metadata)\n\t\t\tprint (""inference time:"", time.time() - t)\n\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, \'minstep%d\' % ministep)\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\twith open(\'timeline_step_%d.json\' % ministep, \'w\') as f:\n\t\t\t\tf.write(chrome_trace)\n\t\treturn\n\n\tdef profile(self):\n\t\t""""""\n\t\tThis profiles a training step.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(False)\n\t\tself.profile_step(1)\n\t\treturn\n\n\tdef InTrainEval(self, mol_set, Rr_cut, Ra_cut, Ree_cut, step=0):\n\t\t""""""\n\t\tEvaluted the network during training.\n\t\t""""""\n\t\tnmols = len(mol_set.mols)\n\t\tfor i in range(nmols, self.batch_size):\n\t\t\tmol_set.mols.append(mol_set.mols[-1])\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\txyzs = np.zeros((nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(Rr_cut, Ra_cut, self.eles_np, self.eles_pairs_np)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(Ree_cut)\n\t\tbatch_data = [xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom]\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]]+[np.ones(self.nlayer+1)])\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient= self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\tmonitor_data = [Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient]\n\t\tf = open(self.name+""_monitor_""+str(step)+"".dat"",""wb"")\n\t\tpickle.dump(monitor_data, f)\n\t\tf.close()\n\t\tprint (""calculating monitoring set.."")\n\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\n\tdef print_training(self, step, loss, energy_loss, grads_loss, dipole_loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f  energy_loss: %.10f  grad_loss: %.10f, dipole_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)), (float(dipole_loss)/(Ncase)))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f energy_loss: %.10f  grad_loss: %.10f, dipole_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)), (float(dipole_loss)/(Ncase)))\n\t\treturn\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.AssignActivation()\n\t\t#print (""self.activation_function:\\n\\n"", self.activation_function)\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms or self.batch_size != nmol):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]]+[np.ones(self.nlayer+1)])\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGenerate Tensorflow graph of evalution.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Release(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.bp_gradient  = tf.gradients(self.Ebp, self.xyzs_pl, name=""BPGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef energy_inference_periodic(self, inp, indexs, cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep_e1e2, EE_cuton, EE_cutoff, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph for calculating the energy of periodic system\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.nreal], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.nreal]),[self.batch_size, self.nreal])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLRWithEle(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep_e1e2)/2.0\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output\n\n\tdef dipole_inference_periodic(self, inp, indexs, xyzs, natom, Elu_Width, EE_cutoff, Reep, AddEcc, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph for calculating the atom charges of periodic system.\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\txyzs_real = xyzsInBohr[:,:self.nreal]\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.nreal], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(charge_inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.nreal]),[self.batch_size, self.nreal])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.nreal])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzs_real,[self.batch_size*self.nreal, 3]), tf.reshape(scaled_charge,[self.batch_size*self.nreal, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.nreal, 3]), axis=1)\n\n\t\tntess = tf.cast(tf.div(self.MaxNAtoms, self.nreal), dtype=tf.int32)\n\t\tscaled_charge_all = tf.tile(scaled_charge, [1, ntess])\n\t\tdef f1(): return TFCoulombEluSRDSFLR(xyzsInBohr, scaled_charge_all, Elu_Width*BOHRPERA, Reep, tf.cast(self.DSFAlpha, self.tf_prec), tf.cast(self.elu_alpha,self.tf_prec), tf.cast(self.elu_shift,self.tf_prec))\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)/2.0\n\t\treturn  cc_energy, dipole, scaled_charge_all, dipole_wb\n\n\tdef fill_feed_dict_periodic(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_e1e2_pl] + [self.mil_j_pl]  + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\t@TMTiming(""EvalPeriodic"")\n\tdef evaluate_periodic(self, batch_data, nreal,DoForce = True):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph for a periodic system.\n\t\t""""""\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.nreal = nreal\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.AssignActivation()\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Periodic()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Periodic()\n\t\tt0 = time.time()\n\t\tfeed_dict=self.fill_feed_dict_periodic(batch_data+[PARAMS[""AddEcc""]]+[np.ones(self.nlayer+1)])\n\t\tif (DoForce):\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\t\telse:\n\t\t\tEtotal = self.sess.run(self.Etotal, feed_dict=feed_dict)\n\t\t\treturn Etotal\n\n\tdef EvalPrepare_Periodic(self,  continue_training =False):\n\t\t""""""\n\t\tGenerate Tensorlfow graph for evalution of periodic system.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_e1e2_pl=tf.placeholder(tf.int64, shape=tuple([None,5]),name=""RadialElectros"")\n\t\t\tself.Reep_pl = self.Reep_e1e2_pl[:,:3]\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Periodic(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl, self.nreal)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_e1e2_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\nclass MolInstance_DirectBP_Charge_SymFunction(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t\tTrainable_: True for training, False for evalution\n\t\t\tForceType_: Deprecated\n\t\t""""""\n\t\tself.SFPa = None\n\t\tself.SFPr = None\n\t\tself.Ra_cut = None\n\t\tself.Rr_cut = None\n\t\tself.HasANI1PARAMS = False\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.eles_np = np.asarray(self.eles).reshape((self.n_eles,1))\n\t\tself.eles_pairs = []\n\t\tfor i in range (len(self.eles)):\n\t\t\tfor j in range(i, len(self.eles)):\n\t\t\t\tself.eles_pairs.append([self.eles[i], self.eles[j]])\n\t\tself.eles_pairs_np = np.asarray(self.eles_pairs)\n\t\tif not self.HasANI1PARAMS:\n\t\t\tself.SetANI1Param()\n\t\tself.HiddenLayers = PARAMS[""HiddenLayers""]\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tprint (""self.activation_function_type: "", self.activation_function_type)\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\tself.xyzs_pl = None\n\t\tself.Zs_pl = None\n\t\tself.sess = None\n\t\tself.total_loss = None\n\t\tself.dipole_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\t\tself.learning_rate_dipole = PARAMS[""learning_rate_dipole""]\n\t\tself.suffix = PARAMS[""NetNameSuffix""]\n\t\tself.Ree_off  = PARAMS[""EECutoffOff""]\n\t\tself.SetANI1Param()\n\t\tself.run_metadata = None\n\t\tself.Training_Traget = ""Dipole""\n\t\tself.TData.ele = self.eles_np\n\t\tself.TData.elep = self.eles_pairs_np\n\t\tself.Training_Traget = ""Dipole""\n\n\t\tself.NetType = ""RawBP_Charge_SymFunction""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+self.suffix\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.keep_prob = np.asarray(PARAMS[""KeepProb""])\n\t\tself.nlayer = len(PARAMS[""KeepProb""]) - 1\n\t\tself.monitor_mset =  PARAMS[""MonitorSet""]\n\n\t\tprint (""self.eles_pairs:"",self.eles_pairs)\n\n\n\tdef SetANI1Param(self, prec=np.float64):\n\t\t""""""\n\t\tGenerate ANI1 symmetry function paramter tensor.\n\t\t""""""\n\t\tself.Ra_cut = PARAMS[""AN1_a_Rc""]\n\t\tself.Rr_cut = PARAMS[""AN1_r_Rc""]\n\t\tzetas = np.array([[PARAMS[""AN1_zeta""]]], dtype = prec)\n\t\tetas = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_a_As = PARAMS[""AN1_num_a_As""]\n\t\tAN1_num_a_Rs = PARAMS[""AN1_num_a_Rs""]\n\t\tthetas = np.array([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype = prec)\n\t\trs =  np.array([ self.Ra_cut*i/AN1_num_a_Rs for i in range (0, AN1_num_a_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\t\tp1 = np.tile(np.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp3 = np.tile(np.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_a_Rs,1])\n\t\tp4 = np.tile(np.reshape(rs,[1,1,1,AN1_num_a_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\t\tSFPa = np.concatenate([p1,p2,p3,p4],axis=4)\n\t\tself.SFPa = np.transpose(SFPa, [4,0,1,2,3])\n\t\tetas_R = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_r_Rs = PARAMS[""AN1_num_r_Rs""]\n\t\trs_R =  np.array([ self.Rr_cut*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 2 x  neta X nr\n\t\tp1_R = np.tile(np.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\t\tp2_R = np.tile(np.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\t\tSFPr = np.concatenate([p1_R,p2_R],axis=2)\n\t\tself.SFPr = np.transpose(SFPr, [2,0,1])\n\t\tself.inshape = int(len(self.eles)*AN1_num_r_Rs + len(self.eles_pairs)*AN1_num_a_Rs*AN1_num_a_As)\n\t\tself.inshape_withencode = int(self.inshape + AN1_num_r_Rs)\n\t\t#self.inshape = int(len(self.eles)*AN1_num_r_Rs)\n\t\tp1 = np.tile(np.reshape(thetas,[AN1_num_a_As,1,1]),[1,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(rs,[1,AN1_num_a_Rs,1]),[AN1_num_a_As,1,1])\n\t\tSFPa2 = np.concatenate([p1,p2],axis=2)\n\t\tself.SFPa2 = np.transpose(SFPa2, [2,0,1])\n\t\tp1_new = np.reshape(rs_R,[AN1_num_r_Rs,1])\n\t\tself.SFPr2 = np.transpose(p1_new, [1,0])\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\t\tself.HasANI1PARAMS = True\n\t\tprint (""self.inshape:"", self.inshape)\n\n\tdef Clean(self):\n\t\t""""""\n\t\tClean Instance for pickle saving.\n\t\t""""""\n\t\tInstance.Clean(self)\n\t\t#self.tf_prec = None\n\t\tself.xyzs_pl, self.Zs_pl = None, None\n\t\tself.check, self.options, self.run_metadata = None, None, None\n\t\tself.atom_outputs = None\n\t\tself.dipole_loss = None\n\t\tself.Scatter_Sym, self.Sym_Index = None, None\n\t\tself.Radp_pl, self.Angt_pl = None, None\n\t\tself.Dlabel_pl = None\n\t\tself.Reep_pl, self.Reep_e1e2_pl, self.natom_pl = None, None, None\n\t\tself.dipole, self.charge = None, None\n\t\tself.dipole_wb = None, None\n\t\tself.total_loss_dipole, self.loss_dipole = None, None\n\t\tself.train_op_dipole = None\n\t\tself.Radp_Ele_pl, self.Angt_Elep_pl = None, None\n\t\tself.mil_jk_pl, self.mil_j_pl = None, None\n\t\tself.keep_prob_pl = None\n\t\tself.eleneg, self.ini_charge, self.debug1  = None, None, None\n\t\treturn\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tDefine Tensorflow graph for training.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_e1e2_pl=tf.placeholder(tf.int64, shape=tuple([None,5]),name=""RadialElectros"")\n\t\t\tself.Reep_pl = self.Reep_e1e2_pl[:,:3]\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Release(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl)\n\t\t\t#self.dipole, self.charge = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, self.Reep_e1e2_pl,  self.keep_prob_pl)\n\t\t\t#self.eleneg = self.eleneg_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, self.keep_prob_pl)\n\t\t\tself.eleneg = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\t\tself.ini_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\t\tself.dipole, self.charge, self.debug1 = self.pairwise_charge_exchange(self.Sym_Index, self.xyzs_pl, self.Zs_pl, self.eleneg, self.ini_charge, Ele, Elep, self.Reep_e1e2_pl, self.keep_prob_pl)\n\t\t\tself.total_loss, self.dipole_loss = self.loss_op(self.dipole, self.Dlabel_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.total_loss)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl] + [self.Zs_pl] + [self.Dlabel_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_e1e2_pl] + [self.mil_j_pl] + [self.mil_jk_pl] + [self.natom_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\n\tdef dipole_inference(self, inp, indexs, xyzs, natom, Reep_e1e2,  keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph for calculating dipole.\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements.\n\t\t\txyzs: xyz coordinates of atoms.\n\t\t\tnatom: 1/(max of number of atoms in the set).\n\t\t\tElu_Width: Width of the elu version of the Coulomb interaction.\n\t\t\tEE_cutoff: Where Coulomb is turned off\n\t\t\tReep: Atom index of vdw pairs.\n\t\t\tAddEcc: Whether add Coulomb energy to the total energy\n\t\t\tkeep_prob: dropout prob of each layer.\n\t\tReturns:\n\t\t\tThe BP graph charge and dipole  output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\t#biases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\t#Dbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(charge_inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(charge_inputs, keep_prob[i]), weights)))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\t#dipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\t#biases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\t#Dbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[i]), weights)))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\t#dipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\t#biases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t#dipole_wb.append(biases)\n\t\t\t\t\t#Dbranches[-1].append(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[-1]), weights))\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.MaxNAtoms])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzsInBohr,[self.batch_size*self.MaxNAtoms, 3]), tf.reshape(scaled_charge,[self.batch_size*self.MaxNAtoms, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.MaxNAtoms, 3]), axis=1)\n\n\t\treturn  dipole, scaled_charge\n\n\tdef eleneg_inference(self, inp, indexs, xyzs, natom, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph for calculating electronic negativity for atoms.\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements.\n\t\t\txyzs: xyz coordinates of atoms.\n\t\t\tkeep_prob: dropout prob of each layer.\n\t\tReturns:\n\t\t\tThe BP graph electronic negativity  output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tDbranches=[]\n\t\toutput_eleneg = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\twith tf.name_scope(""EleNegNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\teleneg_inputs = inp[e]\n\t\t\t\teleneg_shp_in = tf.shape(eleneg_inputs)\n\t\t\t\teleneg_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_eleneg\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(eleneg_inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_eleneg""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_eleneg\'):\n\t\t\t\t\teleneg_shp = tf.shape(eleneg_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tDbranches[-1].append(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(eleneg_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput_eleneg = tf.add(output_eleneg, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_eleneg,""Nan in output!!!"")\n\t\tscaled_output_eleneg = tf.sigmoid(output_eleneg)\n\t\treturn  scaled_output_eleneg\n\n\n\tdef pairwise_charge_exchange(self, indexs, xyzs, Zs, eleneg, charge, Ele, Elep, Reep_e1e2, keep_prob):\n\t\t""""""\n\t\tPairwise charge exchange model.\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements.\n\t\t\txyzs: xyz coordinates of atoms.\n\t\t\tini_charge: initial charge\n\t\t\teleneg: electronic negativity determined by the BP network.\n\t\t\tkeep_prob: dropout prob of each layer.\n\t\tReturns:\n\t\t\tThe BP graph electronic negativity  output\n\t\t""""""\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tCbranches = []\n\t\tpair_indexs = []\n\t\tnpair, pair_dim = Reep_e1e2.get_shape().as_list()\n\t\tele1 = tf.gather_nd(tf.reshape(Ele,[2]), tf.reshape(Reep_e1e2[:,3],[-1,1]))\n\t\tele2 = tf.gather_nd(tf.reshape(Ele,[2]), tf.reshape(Reep_e1e2[:,4],[-1,1]))\n\t\tele12 = tf.transpose(tf.stack([ele1, ele2]))\n\n\t\tfor loop_num in range(0, 5):\n\t\t\tmj = tf.transpose(tf.stack([Reep_e1e2[:,0], Reep_e1e2[:,2]]))\n\t\t\tmji = tf.transpose(tf.stack([Reep_e1e2[:,0], Reep_e1e2[:,2], Reep_e1e2[:,1]]))\n\t\t\tRi = tf.gather_nd(xyzsInBohr, Reep_e1e2[:,:2])\n\t\t\tRj = tf.gather_nd(xyzsInBohr, mj)\n\t\t\tD = tf.sqrt(tf.reduce_sum((Ri-Rj)*(Ri-Rj), 1))\n\t\t\tqi = tf.gather_nd(charge, Reep_e1e2[:,:2])\n\t\t\tqj = tf.gather_nd(charge, mj)\n\t\t\tEjoni = tf.reshape(qj,[-1,1])*(Ri-Rj)/tf.reshape(D*D,[-1,1])\n\t\t\tEionj = tf.reshape(qi,[-1,1])*(Rj-Ri)/tf.reshape(D*D,[-1,1])\n\t\t\tscattered_Ejoni = tf.scatter_nd(Reep_e1e2[:,:3], Ejoni, [self.batch_size, self.MaxNAtoms, self.MaxNAtoms, 3]) # this needs to be replaced if linear scaling is needed\n\t\t\tscattered_Eionj = tf.scatter_nd(mji, Eionj, [self.batch_size, self.MaxNAtoms, self.MaxNAtoms, 3]) # this needs to be replaced if linear scaling is needed\n\t\t\tEjoni_sum = tf.reduce_sum(scattered_Ejoni,2)\n\t\t\tEionj_sum = tf.reduce_sum(scattered_Eionj,2)\n\t\t\tE_sum = Ejoni_sum + Eionj_sum\n\t\t\tif loop_num == 0:\n\t\t\t\treuse_flag = False\n\t\t\telse:\n\t\t\t\treuse_flag = True\n\t\t\twith tf.variable_scope(""ChargeNet"", reuse=reuse_flag):\n\t\t\t\tdelta_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\t\t\tfor pair in range(len(self.eles_pairs)):\n\t\t\t\t\tmask = tf.reduce_all(tf.equal(Elep[pair], ele12), 1)\n\t\t\t\t\tmasked  = tf.boolean_mask(Reep_e1e2, mask)\n\t\t\t\t\tneg1 = tf.gather_nd(eleneg, masked[:,:2])\n\t\t\t\t\tneg2 = tf.gather_nd(eleneg, tf.transpose(tf.stack([masked[:,0], masked[:,2]])))\n\t\t\t\t\tcharge1 = tf.gather_nd(charge, masked[:,:2])\n\t\t\t\t\tcharge2 = tf.gather_nd(charge, tf.transpose(tf.stack([masked[:,0], masked[:,2]])))\n\t\t\t\t\tRi = tf.gather_nd(xyzsInBohr, masked[:,:2])\n\t\t\t\t\tRj = tf.gather_nd(xyzsInBohr, tf.transpose(tf.stack([masked[:,0], masked[:,2]])))\n\t\t\t\t\tdist  = tf.sqrt(tf.reduce_sum((Ri-Rj)*(Ri-Rj), 1))\n\t\t\t\t\tE1 = tf.gather_nd(E_sum, masked[:,:2])\n\t\t\t\t\tE2 = tf.gather_nd(E_sum, tf.transpose(tf.stack([masked[:,0], masked[:,2]])))\n\t\t\t\t\tE1_proj = tf.reduce_sum(E1*(Rj-Ri),-1)/dist\n\t\t\t\t\tE2_proj = tf.reduce_sum(E2*(Ri-Rj),-1)/dist\n\t\t\t\t\tpair_indexs.append(masked)\n\t\t\t\t\tcharge_inputs = tf.transpose(tf.stack([charge1, E1_proj, charge2, E2_proj, 1.0/dist]))\n\t\t\t\t\t#charge_inputs = tf.transpose(tf.stack([neg1, charge1, E1_proj, neg2, charge2, E2_proj, 1.0/dist]))\n\t\t\t\t\tCbranches.append([])\n\t\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\n\t\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\t\tif i == 0:\n\t\t\t\t\t\t\twith tf.variable_scope(str(pair)+\'_hidden1_charge\',reuse=reuse_flag):\n\t\t\t\t\t\t\t\tweights = self._get_variable_with_weight_decay(var_name=str(pair)+str(1)+\'weights\', var_shape=[5, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(5.0)), var_wd=0.001)\n\t\t\t\t\t\t\t\tbiases = tf.get_variable(str(pair)+str(1)+""biases"", [self.HiddenLayers[i]], self.tf_prec, initializer=tf.constant_initializer(0.0, dtype=self.tf_prec))\n\t\t\t\t\t\t\t\tCbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(charge_inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\twith tf.variable_scope(str(pair)+\'_hidden\'+str(i+1)+""_charge"", reuse=reuse_flag):\n\t\t\t\t\t\t\t\tweights = self._get_variable_with_weight_decay(var_name=str(pair)+str(i+1)+\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\t\tbiases = tf.get_variable(str(pair)+str(i+1)+""biases"", [self.HiddenLayers[i]], self.tf_prec, initializer=tf.constant_initializer(0.0, dtype=self.tf_prec))\n\t\t\t\t\t\t\t\tCbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Cbranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\twith tf.variable_scope(str(pair)+\'_regression_linear_charge\', reuse=reuse_flag):\n\t\t\t\t\t\tweights = self._get_variable_with_weight_decay(var_name=str(pair)+\'linear_reg_weights_1\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\t\tbiases = tf.get_variable(str(pair)+""linear_reg_biases_1"", [1], self.tf_prec, initializer=tf.constant_initializer(0.0, dtype=self.tf_prec))\n\t\t\t\t\t\tCbranches[-1].append(tf.matmul(tf.nn.dropout(Cbranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\t\tshp_out = tf.shape(Cbranches[-1][-1])\n\t\t\t\t\t\trshp = tf.reshape(Cbranches[-1][-1],[-1])\n\t\t\t\t\t\tdelta_charge += tf.scatter_nd(masked[:,:2], rshp, [self.batch_size, self.MaxNAtoms])\n\t\t\t\t\t\tdelta_charge -= tf.scatter_nd(tf.transpose(tf.stack([masked[:,0], masked[:,2]])), rshp, [self.batch_size, self.MaxNAtoms])\n\t\t\t\tcharge += delta_charge\n\n\n\t\tdipole = tf.reduce_sum(tf.multiply(xyzsInBohr, tf.reshape(charge,[self.batch_size,self.MaxNAtoms,1])), axis=1)\n\t\treturn  dipole, charge, E_sum\n\n\n\tdef loss_op(self, dipole, Dlabels, natom):\n\t\t""""""\n\t\tlosss function that includes dipole loss, energy loss and gradient loss.\n\t\t""""""\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels,name=""DipoleDiff""), tf.reshape(natom*100.0,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff,name=""DipL2"")\n\t\ttf.add_to_collection(\'losses\', dipole_loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), dipole_loss\n\n\n\tdef training(self, loss, learning_rate, momentum):\n\t\t""""""Sets up the training Ops.\n\t\tCreates a summarizer to track the loss over time in TensorBoard.\n\t\tCreates an optimizer and applies the gradients to all trainable variables.\n\t\tThe Op returned by this function is what must be passed to the\n\t\t`sess.run()` call to cause the model to train.\n\t\tArgs:\n\t\tloss: Loss tensor, from loss().\n\t\tlearning_rate: The learning rate to use for gradient descent.\n\t\tReturns:\n\t\ttrain_op: The Op for training.\n\t\t""""""\n\t\ttf.summary.scalar(loss.op.name, loss)\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate,name=""Adam"")\n\t\t#optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n\t\tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t\ttrain_op = optimizer.minimize(loss, global_step=global_step, name=""trainop"")\n\t\treturn train_op\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size.\n\t\tTraining object including dipole, energy and gradient\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+ [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, dipole_loss, dipole, charge, debug1 = self.sess.run([self.train_op, self.total_loss, self.dipole_loss, self.dipole, self.charge, self.debug1], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss = train_loss + dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#print (""dipole:"", dipole, dipole.shape, dipole[0], np.sum(dipole[0]))\n\t\t\t#print (""charge:"", charge, charge.shape, charge[-1], np.sum(charge[-1]))\n\t\t\t#print (""debug1:"", debug1, debug1.shape, debug1[-1], np.sum(debug1[-1]))\n\n\t\t\t#xyz = batch_data[0][-1]*BOHRPERA\n\t\t\t#natom = int(1/ batch_data[-2][-1])\n\t\t\t#charge = charge[-1]\n\t\t\t#field = np.zeros(3)\n\t\t\t#for i in range(0, natom-1):\n\t\t\t#\tfield += charge[i]*(xyz[natom-1] - xyz[i])/np.linalg.norm(xyz[natom-1] - xyz[i])**2\n\t\t\t#print (""xyz:"", xyz,""charge:"", charge, ""field:"", field)\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size) + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, dipole_loss, mol_dipole, atom_charge = self.sess.run([self.total_loss, self.dipole_loss, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttest_loss = test_loss + dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tprint (""testing..."")\n\t\tprint (""charge:"", atom_charge, atom_charge.shape, atom_charge[0], np.sum(atom_charge[0]))\n\t\tself.print_training(step, test_loss, num_of_mols, duration, False)\n\t\treturn test_loss\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\t""""""\n\t\tThis the training loop for the united model.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(continue_training)\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_dipole_test_loss = float(\'inf\') # some big numbers\n\t\tmini_energy_test_loss = float(\'inf\')\n\t\tmini_test_loss = float(\'inf\')\n\t\tfor step in  range (0, mxsteps):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\n\tdef print_training(self, step, dipole_loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  dipole_loss: %.10f"", step, duration,  (float(dipole_loss)/(Ncase)))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  dipole_loss: %.10f"", step, duration,  (float(dipole_loss)/(Ncase)))\n\t\treturn\n\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.AssignActivation()\n\t\t#print (""self.activation_function:\\n\\n"", self.activation_function)\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms or self.batch_size != nmol):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[np.ones(self.nlayer+1)])\n\t\tmol_dipole, atom_charge, eleneg = self.sess.run([self.dipole, self.charge, self.eleneg], feed_dict=feed_dict)\n\t\t#print (""eleneg:"", eleneg)\n\t\treturn mol_dipole, atom_charge\n\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGenerate Tensorflow graph of evalution.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_e1e2_pl=tf.placeholder(tf.int64, shape=tuple([None,5]),name=""RadialElectros"")\n\t\t\tself.Reep_pl = self.Reep_e1e2_pl[:,:3]\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Release(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl)\n\t\t\t#self.dipole, self.charge = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, self.Reep_e1e2_pl,  self.keep_prob_pl)\n\t\t\tself.eleneg = self.eleneg_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, self.keep_prob_pl)\n\t\t\tself.ini_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\t\tself.dipole, self.charge, self.debug1 = self.pairwise_charge_exchange(self.Sym_Index, self.xyzs_pl, self.Zs_pl, self.eleneg, self.ini_charge, Ele, Elep, self.Reep_e1e2_pl, self.keep_prob_pl)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n'"
TensorMol/TFNetworks/TFInstance.py,312,"b'""""""\nFor the sake of modularity, all direct access to dig\nneeds to be phased out...\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom ..Containers.TensorData import *\nfrom ..ForceModifiers.Transformer import *\nfrom ..TFDescriptors.RawSymFunc import *\nfrom ..Util import *\nimport numpy as np\nimport math\nimport time\nimport os\nimport sys\nimport numbers\nimport random\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\nimport os.path\nif (HAS_TF):\n\timport tensorflow as tf\n\nclass Instance:\n\t""""""\n\tManages a persistent training network instance\n\t""""""\n\tdef __init__(self, TData_, ele_ = 1 , Name_=None, NetType_=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: a TensorData\n\t\t\tele_: an element type for this instance.\n\t\t\tName_ : a name for this instance, attempts to load from checkpoint.\n\t\t""""""\n\t\t# The tensorflow objects go up here.\n\t\tself.inshape = None\n\t\tself.outshape = None\n\t\tself.sess = None\n\t\tself.loss = None\n\t\tself.output = None\n\t\tself.train_op = None\n\t\tself.total_loss = None\n\t\tself.embeds_placeholder = None\n\t\tself.labels_placeholder = None\n\t\tself.saver = None\n\t\tself.gradient =None\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\t\t# The parameters below belong to tensorflow and its graph\n\t\t# all tensorflow variables cannot be pickled they are populated by Prepare\n\t\tself.PreparedFor=0\n\t\ttry:\n\t\t\tself.tf_prec\n\t\texcept:\n\t\t\tself.tf_prec = eval(PARAMS[""tf_prec""])\n\t\tself.HiddenLayers = PARAMS[""HiddenLayers""]\n\t\tself.hidden1 = PARAMS[""hidden1""]\n\t\tself.hidden2 = PARAMS[""hidden2""]\n\t\tself.hidden3 = PARAMS[""hidden3""]\n\t\tself.learning_rate = PARAMS[""learning_rate""]\n\t\tself.momentum = PARAMS[""momentum""]\n\t\tself.max_steps = PARAMS[""max_steps""]\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tself.max_checkpoints = PARAMS[""max_checkpoints""]\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.activation_function = None\n\t\tself.profiling = PARAMS[""Profiling""]\n\t\tself.AssignActivation()\n\t\tself.path=PARAMS[""networks_directory""]\n\t\tif (Name_ !=  None):\n\t\t\tself.name = Name_\n\t\t\t#self.QueryAvailable() # Should be a sanity check on the data files.\n\t\t\tself.Load() # Network still cannot be used until it is prepared.\n\t\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\t\tLOGGER.info(""raised network: ""+ self.train_dir)\n\t\t\treturn\n\t\tself.element = ele_\n\t\tself.TData = TData_\n\t\tself.tformer = Transformer(PARAMS[""InNormRoutine""], PARAMS[""OutNormRoutine""], self.element, self.TData.dig.name, self.TData.dig.OType)\n\t\tif (not os.path.isdir(self.path)):\n\t\t\tos.mkdir(self.path)\n\t\tself.chk_file = \'\'\n\n\t\tLOGGER.info(""self.learning_rate: ""+str(self.learning_rate))\n\t\tLOGGER.info(""self.batch_size: ""+str(self.batch_size))\n\t\tLOGGER.info(""self.max_steps: ""+str(self.max_steps))\n\n\t\tself.NetType = ""None""\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.element)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\t# if (self.element != 0):\n\t\t\t# self.TData.LoadElementToScratch(self.element, self.tformer)\n\t\t\t# self.tformer.Print()\n\t\t\t# self.TData.PrintStatus()\n\t\t\t# self.inshape = self.TData.dig.eshape\n\t\t\t# self.outshape = self.TData.dig.lshape\n\t\treturn\n\n\tdef __del__(self):\n\t\tif (self.sess != None):\n\t\t\tself.sess.close()\n\t\tself.Clean()\n\n\tdef AssignActivation(self):\n\t\tLOGGER.debug(""Assigning Activation... %s"", PARAMS[""NeuronType""])\n\t\ttry:\n\t\t\tif self.activation_function_type == ""relu"":\n\t\t\t\tself.activation_function = tf.nn.relu\n\t\t\telif self.activation_function_type == ""elu"":\n\t\t\t\tself.activation_function = tf.nn.elu\n\t\t\telif self.activation_function_type == ""selu"":\n\t\t\t\tself.activation_function = self.selu\n\t\t\telif self.activation_function_type == ""softplus"":\n\t\t\t\tself.activation_function = tf.nn.softplus\n\t\t\telif self.activation_function_type == ""tanh"":\n\t\t\t\tself.activation_function = tf.tanh\n\t\t\telif self.activation_function_type == ""sigmoid"":\n\t\t\t\tself.activation_function = tf.sigmoid\n\t\t\telif self.activation_function_type == ""sigmoid_with_param"":\n\t\t\t\tself.activation_function = sigmoid_with_param\n\t\t\telif self.activation_function_type == ""gaussian"":\n\t\t\t\tself.activation_function = guassian_act\n\t\t\telif self.activation_function_type == ""gaussian_rev_tozero"":\n\t\t\t\tself.activation_function = guassian_rev_tozero\n\t\t\telif self.activation_function_type == ""gaussian_rev_tozero_tolinear"":\n\t\t\t\tself.activation_function = guassian_rev_tozero_tolinear\n\t\t\telif self.activation_function_type == ""square_tozero_tolinear"":\n\t\t\t\tself.activation_function = square_tozero_tolinear\n\t\t\telse:\n\t\t\t\tprint (""unknown activation function, set to relu"")\n\t\t\t\tself.activation_function = tf.nn.relu\n\t\texcept Exception as Ex:\n\t\t\tprint(Ex)\n\t\t\tprint (""activation function not assigned, set to relu"")\n\t\t\tself.activation_function = tf.nn.relu\n\t\treturn\n\n\tdef evaluate(self, eval_input):\n\t\t# Check sanity of input\n\t\tif (not np.all(np.isfinite(eval_input))):\n\t\t\tLOGGER.error(""WTF, you trying to feed me, garbage?"")\n\t\t\traise Exception(""bad digest."")\n\t\tif (self.PreparedFor < eval_input.shape[0]):\n\t\t\tself.Prepare(eval_input, eval_input.shape[0])\n\t\treturn\n\n\tdef Prepare(self, eval_input, Ncase=1250):\n\t\t""""""\n\t\tCalled if only evaluations are being done, by evaluate()\n\t\t""""""\n\t\tself.Clean()\n\t\tself.AssignActivation()\n\t\t# Always prepare for at least 125,000 cases which is a 50x50x50 grid.\n\t\teval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default():\n\t\t\tself.embeds_placeholder, self.labels_placeholder = self.placeholder_inputs(Ncase)\n\t\t\tself.output = self.inference(self.embeds_placeholder)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n\t\t\tif (len(metafiles)>0):\n\t\t\t\tmost_recent_meta_file=metafiles[0]\n\t\t\t\tLOGGER.debug(""Restoring training from Meta file: ""+most_recent_meta_file)\n\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n\t\t\t\tself.sess = tf.Session(config=config)\n\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n\t\tself.PreparedFor = Ncase\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t"""""" Builds the graphs by calling inference """"""\n\t\twith tf.Graph().as_default():\n\t\t\tself.embeds_placeholder, self.labels_placeholder = self.placeholder_inputs(self.batch_size)\n\t\t\tself.output = self.inference(self.embeds_placeholder)\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.labels_placeholder)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.sess.run(init)\n\t\t\ttry:\n\t\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n\t\t\t\tif (len(metafiles)>0):\n\t\t\t\t\tmost_recent_meta_file=metafiles[0]\n\t\t\t\t\tLOGGER.info(""Restoring training from Metafile: ""+most_recent_meta_file)\n\t\t\t\t\t#Set config to allow soft device placement for temporary fix to known issue with Tensorflow up to version 0.12 atleast - JEH\n\t\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n\t\t\t\t\tself.sess = tf.Session(config=config)\n\t\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n\t\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n\t\t\texcept Exception as Ex:\n\t\t\t\tLOGGER.error(""Restore Failed"")\n\t\t\t\tpass\n\t\t\tself.summary_writer =  tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\treturn\n\n\tdef Clean(self):\n\t\tif (self.sess != None):\n\t\t\tself.sess.close()\n\t\tself.sess = None\n\t\tself.loss = None\n\t\tself.output = None\n\t\tself.total_loss = None\n\t\tself.train_op = None\n\t\tself.embeds_placeholder = None\n\t\tself.labels_placeholder = None\n\t\tself.saver = None\n\t\tself.gradient =None\n\t\tself.summary_writer = None\n\t\tself.PreparedFor = 0\n\t\tself.summary_op = None\n\t\tself.activation_function = None\n\t\tself.options = None\n\t\tself.run_metadata = None\n\t\treturn\n\n\tdef SaveAndClose(self):\n\t\tprint(""Saving TFInstance..."")\n\t\tif (self.TData!=None):\n\t\t\tself.TData.CleanScratch()\n\t\tself.Clean()\n\t\t#print(""Going to pickle...\\n"",[(attr,type(ins)) for attr,ins in self.__dict__.items()])\n\t\tf=open(self.path+self.name+"".tfn"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\n\tdef variable_summaries(self, var):\n\t\t""""""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n\t\twith tf.name_scope(\'summaries\'):\n\t\t\tmean = tf.reduce_mean(var)\n\t\t\ttf.summary.scalar(\'mean\', mean)\n\t\twith tf.name_scope(\'stddev\'):\n\t\t\tstddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n\t\t\ttf.summary.scalar(\'stddev\', stddev)\n\t\t\ttf.summary.scalar(\'max\', tf.reduce_max(var))\n\t\t\ttf.summary.scalar(\'min\', tf.reduce_min(var))\n\t\t\ttf.summary.histogram(\'histogram\', var)\n\n\tdef save_chk(self,  step):  # this can be included in the Instance\n\t\tcheckpoint_file_mini = os.path.join(self.train_dir,self.name+\'-chk-\'+str(step))\n\t\tLOGGER.info(""Saving Checkpoint file, ""+checkpoint_file_mini)\n\t\tself.saver.save(self.sess, checkpoint_file_mini)\n\t\treturn\n\n\tdef FindLastCheckpoint(self):\n\t\tchkfiles = [x for x in os.listdir(self.train_dir) if (x.count(\'chk\')>0 and x.count(\'meta\')>0)]\n\t\tif (len(chkfiles)==0):\n\t\t\treturn False\n\t\tchknums = sorted([int(chkfile.replace(self.name+\'-chk-\',\'\').replace(\'.meta\',\'\')) for chkfile in chkfiles])\n\t\tlastchkfile = os.path.join(self.train_dir,self.name+\'-chk-\'+str(chknums[-1]))\n\t\tprint(""Found Last Checkpoint file: "",lastchkfile)\n\t\treturn lastchkfile\n\n\t#this isn\'t really the correct way to load()\n\t# only the local class members (not any TF objects should be unpickled.)\n\tdef Load(self):\n\t\tLOGGER.info(""Unpickling TFInstance..."")\n\t\tfrom ..Containers.PickleTM import UnPickleTM as UnPickleTM\n\t\ttmp = UnPickleTM(self.path+self.name+"".tfn"")\n\t\t# All this shit should be deleteable after re-training.\n\t\tself.__dict__.update(tmp)\n\t\tchkfiles = [x for x in os.listdir(self.train_dir) if (x.count(\'chk\')>0 and x.count(\'meta\')==0)]\n\t\t# if (len(chkfiles)>0):\n\t\t# \tself.chk_file = chkfiles[0]\n\t\t# else:\n\t\t# \tLOGGER.error(""Network not found... Traindir:""+self.train_dir)\n\t\t# \tLOGGER.error(""Traindir contents: ""+str(os.listdir(self.train_dir)))\n\t\treturn\n\n\tdef _variable_with_weight_decay(self, var_name, var_shape, var_stddev, var_wd):\n\t\t""""""Helper to create an initialized Variable with weight decay.\n\n\t\tNote that the Variable is initialized with a truncated normal distribution.\n\t\tA weight decay is added only if one is specified.\n\n\t\tArgs:\n\t\tname: name of the variable\n\t\tshape: list of ints\n\t\tstddev: standard deviation of a truncated Gaussian\n\t\twd: add L2Loss weight decay multiplied by this float. If None, weight\n\t\tdecay is not added for this Variable.\n\n\t\tReturns:\n\t\tVariable Tensor\n\t\t""""""\n\t\tvar = tf.Variable(tf.truncated_normal(var_shape, stddev=var_stddev, dtype=self.tf_prec), name=var_name)\n\t\tif var_wd is not None:\n\t\t\tweight_decay = tf.multiply(tf.nn.l2_loss(var), var_wd, name=\'weight_loss\')\n\t\t\ttf.add_to_collection(\'losses\', weight_decay)\n\t\treturn var\n\n\n\tdef _get_weight_variable(self, name, shape):\n\t\treturn tf.get_variable(name, shape, self.tf_prec, tf.truncated_normal_initializer(stddev=0.01))\n\n\tdef _get_bias_variable(self, name, shape):\n\t\treturn tf.get_variable(name, shape, self.tf_prec, tf.constant_initializer(0.01, dtype=self.tf_prec))\n\n\n\tdef _get_variable_with_weight_decay(self, var_name, var_shape, var_stddev, var_wd):\n\t\t""""""Helper to create an initialized Variable with weight decay for sharing weights.\n\n\t\tNote that the Variable is initialized with a truncated normal distribution.\n\t\tA weight decay is added only if one is specified.\n\n\t\tArgs:\n\t\tname: name of the variable\n\t\tshape: list of ints\n\t\tstddev: standard deviation of a truncated Gaussian\n\t\twd: add L2Loss weight decay multiplied by this float. If None, weight\n\t\tdecay is not added for this Variable.\n\n\t\tReturns:\n\t\tVariable Tensor\n\t\t""""""\n\t\tvar = tf.get_variable(var_name, var_shape, self.tf_prec, tf.truncated_normal_initializer(stddev=var_stddev))\n\t\tif var_wd is not None:\n\t\t\tweight_decay = tf.multiply(tf.nn.l2_loss(var), var_wd, name=\'weight_loss\')\n\t\t\ttf.add_to_collection(\'losses\', weight_decay)\n\t\treturn var\n\n\tdef dropout_selu(self, x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, noise_shape=None, seed=None, name=None, training=False):\n\t\t""""""Dropout to a value with rescaling.""""""\n\t\tdef dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n\t\t\tkeep_prob = 1.0 - rate\n\t\t\tx = tf.convert_to_tensor(x, name=""x"")\n\t\t\tif isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n\t\t\t\traise ValueError(""keep_prob must be a scalar tensor or a float in the ""\n\t\t\t\t\t\t\t\t""range (0, 1], got %g"" % keep_prob)\n\t\t\tkeep_prob = tf.convert_to_tensor(keep_prob, dtype=x.dtype, name=""keep_prob"")\n\t\t\tkeep_prob.get_shape().assert_is_compatible_with([])\n\n\t\t\talpha = tf.convert_to_tensor(alpha, dtype=x.dtype, name=""alpha"")\n\t\t\tkeep_prob.get_shape().assert_is_compatible_with([])\n\n\t\t\tif tf.contrib.util.constant_value(keep_prob) == 1:\n\t\t\t\treturn x\n\n\t\t\tnoise_shape = noise_shape if noise_shape is not None else tf.shape(x)\n\t\t\trandom_tensor = keep_prob\n\t\t\trandom_tensor += tf.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n\t\t\tbinary_tensor = tf.floor(random_tensor)\n\t\t\tret = x * binary_tensor + alpha * (1-binary_tensor)\n\n\t\t\ta = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n\n\t\t\tb = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n\t\t\tret = a * ret + b\n\t\t\tret.set_shape(x.get_shape())\n\t\t\treturn ret\n\n\t\twith tf.name_scope(name, ""dropout"", [x]) as name:\n\t\t\t# return dropout_selu_impl(x, rate, alpha, noise_shape, seed, name) if training else array_ops.identity(x)\n\t\t\treturn tf.cond(training,\n\t\t\t\tlambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n\t\t\t\tlambda: tf.identity(x))\n\n\tdef selu(self, x):\n\t\twith tf.name_scope(\'elu\') as scope:\n\t\t\talpha = 1.6732632423543772848170429916717\n\t\t\tscale = 1.0507009873554804934193349852946\n\t\t\treturn scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n\n\tdef placeholder_inputs(self, batch_size):\n\t\traise(""Populate placeholder_inputs"")\n\t\treturn\n\n\tdef fill_feed_dict(self, batch_data, embeds_pl, labels_pl):\n\t\t""""""Fills the feed_dict for training the given step.\n\t\tA feed_dict takes the form of:\n\t\tfeed_dict = {\n\t\t<placeholder>: <tensor of values to be passed for placeholder>,\n\t\t....\n\t\t}\n\t\tArgs:\n\t\tdata_set: The set of images and labels, from input_data.read_data_sets()\n\t\tembeds_pl: The images placeholder, from placeholder_inputs().\n\t\tlabels_pl: The labels placeholder, from placeholder_inputs().\n\t\tReturns:\n\t\tfeed_dict: The feed dictionary mapping from placeholders to values.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[0]))):\n\t\t\tLOGGER.error(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tif (not np.all(np.isfinite(batch_data[1]))):\n\t\t\tLOGGER.error(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict = {embeds_pl: batch_data[0], labels_pl: batch_data[1],}\n\t\treturn feed_dict\n\n\tdef inference(self, inputs):\n\t\t""""""Builds the network architecture. Number of hidden layers and nodes in each layer defined in TMParams ""HiddenLayers"".\n\t\tArgs:\n\t\t\tinputs: input placeholder for training data from Digester.\n\t\tReturns:\n\t\t\toutput: scalar or vector of OType from Digester.\n\t\t""""""\n\n\t\thiddens = []\n\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\tif i == 0:\n\t\t\t\twith tf.name_scope(\'hidden1\'):\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\',\n\t\t\t\t\t\t\t\t\tvar_shape=([self.inshape, self.HiddenLayers[i]]),\n\t\t\t\t\t\t\t\t\tvar_stddev= 1.0 / math.sqrt(float(self.inshape)), var_wd= 0.00)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\thiddens.append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\t# tf.scalar_summary(\'min/\' + weights.name, tf.reduce_min(weights))\n\t\t\t\t\t# tf.histogram_summary(weights.name, weights)\n\t\t\telse:\n\t\t\t\twith tf.name_scope(\'hidden\'+str(i+1)):\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\',\n\t\t\t\t\t\t\t\t\tvar_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]],\n\t\t\t\t\t\t\t\t\tvar_stddev= 1.0 / math.sqrt(float(self.HiddenLayers[i-1])), var_wd= 0.00)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec),name=\'biases\')\n\t\t\t\t\thiddens.append(self.activation_function(tf.matmul(hiddens[-1], weights) + biases))\n\t\twith tf.name_scope(\'regression_linear\'):\n\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\',\n\t\t\t\t\t\t\tvar_shape=[self.HiddenLayers[-1], self.outshape],\n\t\t\t\t\t\t\tvar_stddev= 1.0 / math.sqrt(float(self.HiddenLayers[-1])), var_wd= 0.00)\n\t\t\tbiases = tf.Variable(tf.zeros(self.outshape, dtype=self.tf_prec), name=\'biases\')\n\t\t\toutput = tf.matmul(hiddens[-1], weights) + biases\n\t\treturn output\n\n\tdef loss_op(self, output, labels):\n\t\t""""""\n\t\tCalculates the loss from the logits and the labels.\n\t\tArgs:\n\t\tlogits: Logits tensor, float - [batch_size, NUM_CLASSES].\n\t\tlabels: Labels tensor, int32 - [batch_size].\n\t\tReturns:\n\t\tloss: Loss tensor of type float.\n\t\t""""""\n\t\traise Exception(""Base Loss."")\n\t\treturn\n\n\tdef training(self, loss, learning_rate, momentum):\n\t\t""""""Sets up the training Ops.\n\t\tCreates a summarizer to track the loss over time in TensorBoard.\n\t\tCreates an optimizer and applies the gradients to all trainable variables.\n\t\tThe Op returned by this function is what must be passed to the\n\t\t`sess.run()` call to cause the model to train.\n\t\tArgs:\n\t\tloss: Loss tensor, from loss().\n\t\tlearning_rate: The learning rate to use for gradient descent.\n\t\tReturns:\n\t\ttrain_op: The Op for training.\n\t\t""""""\n\t\ttf.summary.scalar(loss.op.name, loss)\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate)\n\t\tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t\ttrain_op = optimizer.minimize(loss, global_step=global_step)\n\t\treturn train_op\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\tself.TrainPrepare(continue_training)\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_test_loss = 100000000 # some big numbers\n\t\tfor step in range(1, mxsteps+1):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif (test_loss < mini_test_loss):\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef train_step(self,step):\n\t\traise Exception(""Cannot Train base..."")\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""Train for a number of steps.""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.embeds_placeholder, self.labels_placeholder = self.placeholder_inputs(self.batch_size)\n\t\t\tself.output = self.inference(self.embeds_placeholder)\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.labels_placeholder)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.sess.run(init)\n\t\t\ttry: # I think this may be broken\n\t\t\t\tchkfiles = [x for x in os.listdir(self.train_dir) if (x.count(\'chk\')>0 and x.count(\'meta\')==0)]\n\t\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n\t\t\t\tif (len(metafiles)>0):\n\t\t\t\t\tmost_recent_meta_file=metafiles[0]\n\t\t\t\t\tprint(""Restoring training from Metafile: "",most_recent_meta_file)\n\t\t\t\t\t#Set config to allow soft device placement for temporary fix to known issue with Tensorflow up to version 0.12 atleast - JEH\n\t\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n\t\t\t\t\tself.sess = tf.Session(config=config)\n\t\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n\t\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""Restore Failed 2341325"",Ex)\n\t\t\t\tpass\n\t\t\tself.summary_writer =  tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\treturn\n\n\tdef test(self,step):\n\t\traise Exception(""Base Test"")\n\t\treturn\n\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tdenom = max((int(Ncase/self.batch_size)),1)\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f train loss: %.10f"", step, duration,(float(loss)/(denom*self.batch_size)))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f test loss: %.10f"", step, duration,(float(loss)/(denom*self.batch_size)))\n\t\treturn\n\nclass Instance_fc_classify(Instance):\n\tdef __init__(self, TData_, ele_ = 1 , Name_=None):\n\t\tInstance.__init__(self, TData_, ele_, Name_)\n\t\tself.NetType = ""fc_classify""\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.element)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.prob = None\n\t\t#\t\tself.inshape = self.TData.scratch_inputs.shape[1]\n\t\tself.correct = None\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\n\tdef n_correct(self, output, labels):\n\t\t# For a classifier model, we can use the in_top_k Op.\n\t\t# It returns a bool tensor with shape [batch_size] that is true for\n\t\t# the examples where the label is in the top k (here k=1)\n\t\t# of all logits for that example.\n\t\tlabels = tf.to_int64(labels)\n\t\tcorrect = tf.nn.in_top_k(output, labels, 1)\n\t\t# Return the number of true entries.\n\t\treturn tf.reduce_sum(tf.cast(correct, tf.int32))\n\n\tdef evaluate(self, eval_input):\n\t\t# Check sanity of input\n\t\tInstance.evaluate(self, eval_input)\n\t\teval_input_ = eval_input\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\teval_input_ =np.copy(eval_input)\n\t\t\teval_input_.resize((self.PreparedFor,eval_input.shape[1]))\n\t\t\t# pad with zeros\n\t\teval_labels = np.zeros(self.PreparedFor)  # dummy labels\n\t\tbatch_data = [eval_input_, eval_labels]\n\t\t#embeds_placeholder, labels_placeholder = self.placeholder_inputs(Ncase) Made by Prepare()\n\t\tfeed_dict = self.fill_feed_dict(batch_data,self.embeds_placeholder,self.labels_placeholder)\n\t\ttmp = (np.array(self.sess.run([self.prob], feed_dict=feed_dict))[0,:eval_input.shape[0],1])\n\t\tif (not np.all(np.isfinite(tmp))):\n\t\t\tLOGGER.error(""TFsession returned garbage"")\n\t\t\tLOGGER.error(""TFInputs: ""+str(eval_input) ) #If it\'s still a problem here use tf.Print version of the graph.\n\t\t\traise Exception(""Garbage..."")\n\t\tif (self.PreparedFor > eval_input.shape[0]):\n\t\t\treturn tmp[:eval_input.shape[0]]\n\t\treturn tmp\n\n\tdef Prepare(self, eval_input, Ncase=1250):\n\t\tInstance.Prepare(self)\n\t\tLOGGER.info(""Preparing a ""+self.NetType+""Instance"")\n\t\tself.prob = None\n\t\tself.correct = None\n\t\t# Always prepare for at least 125,000 cases which is a 50x50x50 grid.\n\t\teval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default():\n\t\t\tself.embeds_placeholder, self.labels_placeholder = self.placeholder_inputs(Ncase)\n\t\t\tself.output = self.inference(self.embeds_placeholder)\n\t\t\tself.correct = self.n_correct(self.output, self.labels_placeholder)\n\t\t\tself.prob = self.justpreds(self.output)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tchkfiles = [x for x in os.listdir(self.train_dir) if (x.count(\'chk\')>0 and x.count(\'meta\')==0)]\n\t\t\tif (len(chkfiles)>0):\n\t\t\t\tmost_recent_chk_file=chkfiles[0]\n\t\t\t\tLOGGER.info(""Restoring training from Checkpoint: ""+most_recent_chk_file)\n\t\t\t\tself.saver.restore(self.sess, self.train_dir+\'/\'+most_recent_chk_file)\n\t\tself.PreparedFor = Ncase\n\t\treturn\n\n\tdef Save(self):\n\t\tself.prob = None\n\t\tself.correct = None\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\t\tInstance.Save(self)\n\t\treturn\n\n\tdef placeholder_inputs(self, batch_size):\n\t\t""""""Generate placeholder variables to represent the input tensors.\n\t\tThese placeholders are used as inputs by the rest of the model building\n\t\tcode and will be fed from the downloaded data in the .run() loop, below.\n\t\tArgs:\n\t\tbatch_size: The batch size will be baked into both placeholders.\n\t\tReturns:\n\t\tembeds_placeholder: Images placeholder.\n\t\tlabels_placeholder: Labels placeholder.\n\t\t""""""\n\t\t# Note that the shapes of the placeholders match the shapes of the full\n\t\t# image and label tensors, except the first dimension is now batch_size\n\t\t# rather than the full size of the train or test data sets.\n\t\tinputs_pl = tf.placeholder(self.tf_prec, shape=(batch_size,self.inshape)) # JAP : Careful about the shapes... should be flat for now.\n\t\toutputs_pl = tf.placeholder(self.tf_prec, shape=(batch_size))\n\t\treturn inputs_pl, outputs_pl\n\n\tdef justpreds(self, output):\n\t\t""""""Calculates the loss from the logits and the labels.\n\t\tArgs:\n\t\tlogits: Logits tensor, float - [batch_size, NUM_CLASSES].\n\t\tlabels: Labels tensor, int32 - [batch_size].\n\t\tReturns:\n\t\tloss: Loss tensor of type float.\n\t\t""""""\n\t\tprob = tf.nn.softmax(output)\n\t\treturn prob\n\n\tdef loss_op(self, output, labels):\n\t\t""""""Calculates the loss from the logits and the labels.\n\t\tArgs:\n\t\tlogits: Logits tensor, float - [batch_size, NUM_CLASSES].\n\t\tlabels: Labels tensor, int32 - [batch_size].\n\t\tReturns:\n\t\tloss: Loss tensor of type float.\n\t\t""""""\n\t\tprob = tf.nn.softmax(output)\n\t\tlabels = tf.to_int64(labels)\n\t\tcross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(output, labels, name=\'xentropy\')\n\t\tcross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n\t\ttf.add_to_collection(\'losses\', cross_entropy_mean)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), cross_entropy_mean, prob\n\n\tdef print_training(self, step, loss, total_correct, Ncase, duration):\n\t\tdenom=max(int(Ncase/self.batch_size),1)\n\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  train loss: "", ""%.10f""%(float(loss)/denom),""accu:  %.5f""%(float(total_correct)/(denom*self.batch_size)))\n\t\treturn\n\n\tdef train_step(self,step):\n\t\tNcase_train = self.TData.NTrainCasesInScratch()\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttotal_correct = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTrainBatch(self.element,  self.batch_size) #advances the case pointer in TData...\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\t_, total_loss_value, loss_value, prob_value, correct_num  = self.sess.run([self.train_op, self.total_loss, self.loss, self.prob, self.correct], feed_dict=feed_dict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttotal_correct = total_correct + correct_num\n\t\tduration = time.time() - start_time\n\t\t#self.print_training(step, train_loss, total_correct, Ncase_train, duration)\n\t\tself.print_training(step, train_loss, Ncase_train, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\tNcase_test = self.TData.NTest\n\t\ttest_loss =  0.0\n\t\ttest_correct = 0.\n\t\ttest_start_time = time.time()\n\t\ttest_loss = None\n\t\tfeed_dict = None\n\t\tfor  ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.element,  self.batch_size, ministep)\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\tloss_value, prob_value, test_correct_num = self.sess.run([ self.loss, self.prob, self.correct],  feed_dict=feed_dict)\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_correct = test_correct + test_correct_num\n\t\t\tduration = time.time() - test_start_time\n\t\t\tLOGGER.info(""testing..."")\n\t\t\tself.print_training(step, test_loss, test_correct, Ncase_test, duration)\n\t\treturn test_loss, feed_dict\n\nclass Instance_fc_sqdiff(Instance):\n\tdef __init__(self, TData_, ele_ = 1 , Name_=None):\n\t\tInstance.__init__(self, TData_, ele_, Name_)\n\t\tself.NetType = ""fc_sqdiff""\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.element)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\tdef evaluate(self, eval_input):\n\t\t# Check sanity of input\n\t\tInstance.evaluate(self, eval_input)\n\t\tgiven_cases = eval_input.shape[0]\n\t\t#print(""given_cases:"", given_cases)\n\t\teis = list(eval_input.shape)\n\t\teval_input_ = eval_input.copy()\n\t\tif (self.PreparedFor > given_cases):\n\t\t\teval_input_.resize(([self.PreparedFor]+eis[1:]))\n\t\t\t# pad with zeros\n\t\teval_labels = np.zeros(tuple([self.PreparedFor]+list(self.outshape)))  # dummy labels\n\t\tbatch_data = [eval_input_, eval_labels]\n\t\t#embeds_placeholder, labels_placeholder = self.placeholder_inputs(Ncase) Made by Prepare()\n\t\tfeed_dict = self.fill_feed_dict(batch_data,self.embeds_placeholder, self.labels_placeholder)\n\t\ttmp = np.array(self.sess.run([self.output], feed_dict=feed_dict))\n\t\tif (not np.all(np.isfinite(tmp))):\n\t\t\tLOGGER.error(""TFsession returned garbage"")\n\t\t\tLOGGER.error(""TFInputs""+str(eval_input) ) #If it\'s still a problem here use tf.Print version of the graph.\n\t\treturn tmp[0,:given_cases]\n\n\tdef Save(self):\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\t\tInstance.Save(self)\n\t\treturn\n\n\tdef placeholder_inputs(self, batch_size):\n\t\t""""""Generate placeholder variables to represent the input tensors.\n\t\tThese placeholders are used as inputs by the rest of the model building\n\t\tcode and will be fed from the downloaded data in the .run() loop, below.\n\t\tArgs:\n\t\tbatch_size: The batch size will be baked into both placeholders.\n\t\tReturns:\n\t\tembeds_placeholder: Images placeholder.\n\t\tlabels_placeholder: Labels placeholder.\n\t\t""""""\n\t\t# Note that the shapes of the placeholders match the shapes of the full\n\t\t# image and label tensors, except the first dimension is now batch_size\n\t\t# rather than the full size of the train or test data sets.\n\t\tinputs_pl = tf.placeholder(self.tf_prec, shape=tuple([batch_size]+list(self.inshape)))\n\t\toutputs_pl = tf.placeholder(self.tf_prec, shape=tuple([batch_size]+list(self.outshape)))\n\t\treturn inputs_pl, outputs_pl\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef train_step(self,step):\n\t\tNcase_train = self.TData.NTrainCasesInScratch()\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttotal_correct = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTrainBatch(self.element,  self.batch_size) #advances the case pointer in TData...\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\t_, total_loss_value, loss_value = self.sess.run([self.train_op, self.total_loss, self.loss], feed_dict=feed_dict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\tduration = time.time() - start_time\n\t\t#self.print_training(step, train_loss, total_correct, Ncase_train, duration)\n\t\tself.print_training(step, train_loss, Ncase_train, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\tNcase_test = self.TData.NTestCasesInScratch()\n\t\ttest_loss =  0.0\n\t\ttest_start_time = time.time()\n\t\t#for ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\tbatch_data=self.TData.GetTestBatch(self.element,  self.batch_size)#, ministep)\n\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\tpreds, total_loss_value, loss_value  = self.sess.run([self.output, self.total_loss,  self.loss],  feed_dict=feed_dict)\n\t\tself.TData.EvaluateTestBatch(batch_data[1],preds, self.tformer)\n\t\ttest_loss = test_loss + loss_value\n\t\tduration = time.time() - test_start_time\n\t\tprint(""testing..."")\n\t\tself.print_training(step, test_loss,  Ncase_test, duration, Train=False)\n\t\treturn test_loss, feed_dict\n\n\tdef PrepareData(self, batch_data):\n\t\tif (batch_data[0].shape[0]==self.batch_size):\n\t\t\tbatch_data=[batch_data[0], batch_data[1].reshape((batch_data[1].shape[0],1))]\n\t\telif (batch_data[0].shape[0] < self.batch_size):\n\t\t\tbatch_data=[batch_data[0], batch_data[1].reshape((batch_data[1].shape[0],1))]\n\t\t\ttmp_input = np.copy(batch_data[0])\n\t\t\ttmp_output = np.copy(batch_data[1])\n\t\t\ttmp_input.resize((self.batch_size,  batch_data[0].shape[1]))\n\t\t\ttmp_output.resize((self.batch_size,  batch_data[1].shape[1]))\n\t\t\tbatch_data=[ tmp_input, tmp_output]\n\t\treturn batch_data\n\nclass Instance_fc_sqdiff_GauSH_direct(Instance):\n\tdef __init__(self, TData=None, elements=None, trainable=True, name=None):\n\t\tInstance.__init__(self, TData, elements, name)\n\t\tself.number_radial = PARAMS[""SH_NRAD""]\n\t\tself.l_max = PARAMS[""SH_LMAX""]\n\t\tself.gaussian_params = PARAMS[""RBFS""][:self.number_radial]\n\t\tself.atomic_embed_factors = PARAMS[""ANES""]\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.inshape =  self.number_radial * (self.l_max + 1) ** 2\n\t\tself.outshape = 3\n\t\tTensorMol.TFDescriptors.RawSH.data_precision = self.tf_prec\n\t\tif name != None:\n\t\t\tself.path = PARAMS[""networks_directory""]\n\t\t\tself.name = name\n\t\t\tself.Load()\n\t\t\tself.gaussian_params = PARAMS[""RBFS""][:self.number_radial]\n\t\t\tself.atomic_embed_factors = PARAMS[""ANES""]\n\t\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\t\tself.inshape =  self.number_radial * (self.l_max + 1) ** 2\n\t\t\tself.outshape = 3\n\t\t\tself.AssignActivation()\n\n\t\t\treturn\n\t\tself.NetType = ""fc_sqdiff_GauSH_direct""\n\t\tself.name = self.TData.name+""_""+self.NetType+""_""+str(self.element)+""_""+time.strftime(""%a_%b_%d_%H.%M.%S_%Y"")\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.trainable = trainable\n\t\tself.orthogonalize = True\n\t\tif (self.trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\n\tdef compute_normalization_constants(self):\n\t\tbatch_data = self.TData.GetTrainBatch(20 * self.batch_size)\n\t\tself.TData.ScratchPointer = 0\n\t\txyzs, Zs, labels = tf.convert_to_tensor(batch_data[0], dtype=self.tf_prec), tf.convert_to_tensor(batch_data[1]), tf.convert_to_tensor(batch_data[2], dtype=self.tf_prec)\n\t\tnum_mols = tf.shape(xyzs)[0]\n\t\trotation_params = tf.stack([np.pi * tf.random_uniform([num_mols], maxval=2.0, dtype=self.tf_prec),\n\t\t\t\tnp.pi * tf.random_uniform([num_mols], maxval=2.0, dtype=self.tf_prec),\n\t\t\t\ttf.random_uniform([num_mols], maxval=2.0, dtype=self.tf_prec)], axis=-1)\n\t\trotated_xyzs, rotated_labels = tf_random_rotate(xyzs, rotation_params, labels)\n\t\tembedding, labels, _, _ = tf_gaussian_spherical_harmonics_element(rotated_xyzs, Zs, rotated_labels,\n\t\t\t\t\t\t\t\t\t\t\tself.element, tf.Variable(self.gaussian_params, dtype=self.tf_prec),\n\t\t\t\t\t\t\t\t\t\t\ttf.Variable(self.atomic_embed_factors, trainable=False, dtype=self.tf_prec),\n\t\t\t\t\t\t\t\t\t\t\tself.l_max, self.orthogonalize)\n\t\twith tf.Session() as sess:\n\t\t\tsess.run(tf.global_variables_initializer())\n\t\t\tembed, label = sess.run([embedding, labels])\n\t\tself.inmean, self.instd = np.mean(embed, axis=0), np.std(embed, axis=0)\n\t\tself.outmean, self.outstd = np.mean(label), np.std(label)\n\t\treturn\n\n\tdef TrainPrepare(self):\n\t\t"""""" Builds the graphs by calling inference """"""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl = tf.placeholder(self.tf_prec, shape=tuple([None, self.MaxNAtoms, 3]))\n\t\t\tself.Zs_pl = tf.placeholder(tf.int32, shape=tuple([None, self.MaxNAtoms]))\n\t\t\tself.labels_pl = tf.placeholder(self.tf_prec, shape=tuple([None, self.MaxNAtoms, 3]))\n\t\t\tself.gaussian_params = tf.Variable(self.gaussian_params, trainable=False, dtype=self.tf_prec)\n\t\t\tself.atomic_embed_factors = tf.Variable(self.atomic_embed_factors, trainable=False, dtype=self.tf_prec)\n\t\t\telement = tf.constant(self.element, dtype=tf.int32)\n\t\t\tinmean = tf.constant(self.inmean, dtype=self.tf_prec)\n\t\t\tinstd = tf.constant(self.instd, dtype=self.tf_prec)\n\t\t\toutmean = tf.constant(self.outmean, dtype=self.tf_prec)\n\t\t\toutstd = tf.constant(self.outstd, dtype=self.tf_prec)\n\t\t\trotation_params = tf.stack([np.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_prec),\n\t\t\t\t\tnp.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_prec),\n\t\t\t\t\ttf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_prec)], axis=-1, name=""rotation_params"")\n\t\t\trotated_xyzs, rotated_labels = tf_random_rotate(self.xyzs_pl, rotation_params, self.labels_pl)\n\t\t\tself.embedding, self.labels, _, min_eigenval = tf_gaussian_spherical_harmonics_element(rotated_xyzs, self.Zs_pl, element,\n\t\t\t\t\tself.gaussian_params, self.atomic_embed_factors, self.l_max, rotated_labels, orthogonalize=self.orthogonalize)\n\t\t\tself.norm_embedding = (self.embedding - inmean) / instd\n\t\t\tself.norm_labels = (self.labels - outmean) / outstd\n\t\t\tself.norm_output = self.inference(self.norm_embedding)\n\t\t\tself.output = (self.norm_output * outstd) + outmean\n\t\t\tself.n_atoms_batch = tf.shape(self.output)[0]\n\t\t\tself.total_loss, self.loss = self.loss_op(self.norm_output, self.norm_labels)\n\t\t\tbarrier_function = -1000.0 * tf.log(tf.concat([self.gaussian_params + 0.9, tf.expand_dims(6.5 - self.gaussian_params[:,0], axis=-1), tf.expand_dims(1.75 - self.gaussian_params[:,1], axis=-1)], axis=1))\n\t\t\ttruncated_barrier_function = tf.reduce_sum(tf.where(tf.greater(barrier_function, 0.0), barrier_function, tf.zeros_like(barrier_function)))\n\t\t\tgaussian_overlap_constraint = tf.square(0.001 / min_eigenval)\n\t\t\tloss_and_constraint = self.total_loss + truncated_barrier_function + gaussian_overlap_constraint\n\t\t\tself.train_op = self.training(loss_and_constraint, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif self.profiling:\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\treturn\n\n\tdef inference(self, inputs):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinputs: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\tlayers=[]\n\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\tif i == 0:\n\t\t\t\twith tf.name_scope(\'hidden1\'):\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvar_stddev = math.sqrt(2.0 / float(self.inshape)), var_wd=0.00)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tlayers.append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\telse:\n\t\t\t\twith tf.name_scope(\'hidden\'+str(i+1)):\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1],self.HiddenLayers[i]],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvar_stddev = math.sqrt(2.0 / float(self.HiddenLayers[i-1])), var_wd=0.00)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tlayers.append(self.activation_function(tf.matmul(layers[-1], weights) + biases))\n\t\twith tf.name_scope(\'regression_linear\'):\n\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], self.outshape],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tvar_stddev = math.sqrt(2.0 / float(self.HiddenLayers[-1])), var_wd=None)\n\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\toutputs = tf.matmul(layers[-1], weights) + biases\n\t\ttf.verify_tensor_all_finite(outputs,""Nan in output!!!"")\n\t\treturn outputs\n\n\tdef evaluate(self, eval_input):\n\t\t# Check sanity of input\n\t\tInstance.evaluate(self, eval_input)\n\t\tgiven_cases = eval_input.shape[0]\n\t\t#print(""given_cases:"", given_cases)\n\t\teis = list(eval_input.shape)\n\t\teval_input_ = eval_input.copy()\n\t\tif (self.PreparedFor > given_cases):\n\t\t\teval_input_.resize(([self.PreparedFor]+eis[1:]))\n\t\t\t# pad with zeros\n\t\teval_labels = np.zeros(tuple([self.PreparedFor]+list(self.outshape)))  # dummy labels\n\t\tbatch_data = [eval_input_, eval_labels]\n\t\t#embeds_placeholder, labels_placeholder = self.placeholder_inputs(Ncase) Made by Prepare()\n\t\tfeed_dict = self.fill_feed_dict(batch_data,self.embeds_placeholder, self.labels_placeholder)\n\t\ttmp = np.array(self.sess.run([self.output], feed_dict=feed_dict))\n\t\tif (not np.all(np.isfinite(tmp))):\n\t\t\tLOGGER.error(""TFsession returned garbage"")\n\t\t\tLOGGER.error(""TFInputs""+str(eval_input) ) #If it\'s still a problem here use tf.Print version of the graph.\n\t\treturn tmp[0,:given_cases]\n\n\tdef Clean(self):\n\t\tif (self.sess != None):\n\t\t\tself.sess.close()\n\t\tself.sess = None\n\t\tself.loss = None\n\t\tself.output = None\n\t\tself.total_loss = None\n\t\tself.train_op = None\n\t\tself.saver = None\n\t\tself.gradient = None\n\t\tself.summary_writer = None\n\t\tself.PreparedFor = 0\n\t\tself.summary_op = None\n\t\tself.activation_function = None\n\t\tself.atomic_embed_factors = None\n\t\tself.gaussian_params = None\n\t\tself.labels = None\n\t\tself.norm_output = None\n\t\tself.norm_labels = None\n\t\tself.norm_embedding = None\n\t\tself.labels_pl = None\n\t\tself.Zs_pl = None\n\t\tself.xyzs_pl = None\n\t\tself.output = None\n\t\tself.embedding = None\n\t\tself.n_atoms_batch = None\n\t\tself.tf_prec = None\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl] + [self.Zs_pl] + [self.labels_pl], [batch_data[0]] + [batch_data[1]] + [batch_data[2]])}\n\t\treturn feed_dict\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\tself.compute_normalization_constants()\n\t\tself.TrainPrepare()\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_test_loss = 1.e16\n\t\tfor step in range(1, mxsteps+1):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0:\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif (test_loss < mini_test_loss):\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.summary_writer.close()\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef train_step(self,step):\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss, n_atoms_epoch = 0.0, 0.0\n\t\tfor ministep in xrange(0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) #advances the case pointer in TData...\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data)\n\t\t\tif self.profiling:\n\t\t\t\t_, total_loss_value, loss_value, n_atoms_batch = self.sess.run([self.train_op, self.total_loss, self.loss, self.n_atoms_batch], feed_dict=feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t\twith open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t\t\tf.write(chrome_trace)\n\t\t\telse:\n\t\t\t\t_, total_loss_value, loss_value, n_atoms_batch = self.sess.run([self.train_op, self.total_loss, self.loss, self.n_atoms_batch], feed_dict=feed_dict)\n\t\t\ttrain_loss += total_loss_value\n\t\t\tn_atoms_epoch += n_atoms_batch\n\t\tduration = time.time() - start_time\n\t\tself.print_training(step, train_loss, n_atoms_epoch, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\tprint(""testing..."")\n\t\tNcase_test = self.TData.NTest\n\t\ttest_loss, n_atoms_epoch = 0.0, 0.0\n\t\ttest_start_time = time.time()\n\t\tmean_test_error, std_dev_test_error = 0.0, 0.0\n\t\ttest_epoch_labels, test_epoch_outputs = [], []\n\t\tfor ministep in xrange(0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size)\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data)\n\t\t\toutput, labels, total_loss_value, loss_value, n_atoms_batch, gaussian_params, atomic_embed_factors = self.sess.run([self.output, self.labels, self.total_loss, self.loss, self.n_atoms_batch, self.gaussian_params, self.atomic_embed_factors],  feed_dict=feed_dict)\n\t\t\ttest_loss += total_loss_value\n\t\t\tn_atoms_epoch += n_atoms_batch\n\t\t\ttest_epoch_labels.append(labels)\n\t\t\ttest_epoch_outputs.append(output)\n\t\ttest_epoch_labels = np.concatenate(test_epoch_labels)\n\t\ttest_epoch_outputs = np.concatenate(test_epoch_outputs)\n\t\ttest_epoch_errors = test_epoch_labels - test_epoch_outputs\n\t\tduration = time.time() - test_start_time\n\t\tfor i in [random.randint(0, self.batch_size) for j in xrange(20)]:\n\t\t\tLOGGER.info(""Label: %s  Output: %s"", test_epoch_labels[i], test_epoch_outputs[i])\n\t\tLOGGER.info(""MAE: %f"", np.mean(np.abs(test_epoch_errors)))\n\t\tLOGGER.info(""MSE: %f"", np.mean(test_epoch_errors))\n\t\tLOGGER.info(""RMSE: %f"", np.sqrt(np.mean(np.square(test_epoch_errors))))\n\t\tLOGGER.info(""Std. Dev.: %f"", np.std(test_epoch_errors))\n\t\tLOGGER.info(""Gaussian paramaters: %s"", gaussian_params)\n\t\tLOGGER.info(""Atomic embedding factors: %s"", atomic_embed_factors)\n\t\tself.print_testing(step, test_loss, n_atoms_epoch, duration)\n\t\treturn test_loss\n\n\tdef print_training(self, step, loss, n_cases, duration):\n\t\tLOGGER.info(""step: %7d  duration: %.5f train loss: %.10f"", step, duration,(loss / float(n_cases)))\n\t\treturn\n\n\tdef print_testing(self, step, loss, n_cases, duration):\n\t\tLOGGER.info(""step: %7d  duration: %.5f test loss: %.10f"", step, duration,(loss / float(n_cases)))\n\t\treturn\n\n\tdef PrepareData(self, batch_data):\n\t\tif (batch_data[0].shape[0]==self.batch_size):\n\t\t\tbatch_data=[batch_data[0], batch_data[1].reshape((batch_data[1].shape[0],1))]\n\t\telif (batch_data[0].shape[0] < self.batch_size):\n\t\t\tbatch_data=[batch_data[0], batch_data[1].reshape((batch_data[1].shape[0],1))]\n\t\t\ttmp_input = np.copy(batch_data[0])\n\t\t\ttmp_output = np.copy(batch_data[1])\n\t\t\ttmp_input.resize((self.batch_size,  batch_data[0].shape[1]))\n\t\t\ttmp_output.resize((self.batch_size,  batch_data[1].shape[1]))\n\t\t\tbatch_data=[ tmp_input, tmp_output]\n\t\treturn batch_data\n\n\tdef evaluate_prepare(self):\n\t\t"""""" Builds the graphs by calling inference """"""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl = tf.placeholder(self.tf_prec, shape=tuple([None, 465, 3]))\n\t\t\tself.Zs_pl = tf.placeholder(tf.int32, shape=tuple([None, 465]))\n\t\t\tself.labels_pl = tf.placeholder(self.tf_prec, shape=tuple([None, 465, 3]))\n\t\t\tself.gaussian_params = tf.Variable(self.gaussian_params, trainable=False, dtype=self.tf_prec)\n\t\t\tself.atomic_embed_factors = tf.Variable(self.atomic_embed_factors, trainable=False, dtype=self.tf_prec)\n\t\t\telement = tf.constant(self.element, dtype=tf.int32)\n\t\t\tinmean = tf.constant(self.inmean, dtype=self.tf_prec)\n\t\t\tinstd = tf.constant(self.instd, dtype=self.tf_prec)\n\t\t\toutmean = tf.constant(self.outmean, dtype=self.tf_prec)\n\t\t\toutstd = tf.constant(self.outstd, dtype=self.tf_prec)\n\t\t\t# rotation_params = tf.stack([np.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_prec),\n\t\t\t# \t\tnp.pi * tf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_prec),\n\t\t\t# \t\ttf.random_uniform([self.batch_size], maxval=2.0, dtype=self.tf_prec)], axis=-1, name=""rotation_params"")\n\t\t\t# rotated_xyzs, rotated_labels = tf_random_rotate(self.xyzs_pl, rotation_params, self.labels_pl)\n\t\t\tself.embedding, self.atom_indices, min_eigenval = tf_gaussian_spherical_harmonics_element(self.xyzs_pl, self.Zs_pl, element,\n\t\t\t\t\tself.gaussian_params, self.atomic_embed_factors, self.l_max, orthogonalize=self.orthogonalize)\n\t\t\tself.norm_embedding = (self.embedding - inmean) / instd\n\t\t\t# self.norm_labels = (self.labels - outmean) / outstd\n\t\t\tself.norm_output = self.inference(self.norm_embedding)\n\t\t\tself.output = (self.norm_output * outstd) + outmean\n\t\t\tself.n_atoms_batch = tf.shape(self.output)[0]\n\t\t\t# self.total_loss, self.loss = self.loss_op(self.norm_output, self.norm_labels)\n\t\t\t# barrier_function = -1000.0 * tf.log(tf.concat([self.gaussian_params + 0.9, tf.expand_dims(6.5 - self.gaussian_params[:,0], axis=-1), tf.expand_dims(1.75 - self.gaussian_params[:,1], axis=-1)], axis=1))\n\t\t\t# truncated_barrier_function = tf.reduce_sum(tf.where(tf.greater(barrier_function, 0.0), barrier_function, tf.zeros_like(barrier_function)))\n\t\t\t# gaussian_overlap_constraint = tf.square(0.001 / min_eigenval)\n\t\t\t# loss_and_constraint = self.total_loss + truncated_barrier_function + gaussian_overlap_constraint\n\t\t\t# self.train_op = self.training(loss_and_constraint, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\t# init = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\t# self.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\t# if self.profiling:\n\t\t\t# \tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t# \tself.run_metadata = tf.RunMetadata()\n\t\t\treturn\n\n\tdef evaluate_fill_feed_dict(self, xyzs, Zs):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl, self.Zs_pl], [xyzs, Zs])}\n\t\treturn feed_dict\n\n\tdef evaluate(self, xyzs, Zs):\n\t\t""""""\n\t\tTakes coordinates and atomic numbers from a manager and feeds them into the network\n\t\tfor evaluation of the forces\n\n\t\tArgs:\n\t\t\txyzs (np.float): numpy array of atomic coordinates\n\t\t\tZs (np.int32): numpy array of atomic numbers\n\t\t""""""\n\t\tif not self.sess:\n\t\t\tprint(""loading the session.."")\n\t\t\tself.chk_file = self.FindLastCheckpoint()\n\t\t\tself.evaluate_prepare()\n\t\tnew_xyzs = np.zeros((1, 465,3))\n\t\tnew_xyzs[0,:np.shape(xyzs)[0]] = xyzs\n\t\tnew_Zs = np.zeros((1, 465), dtype=np.int32)\n\t\tnew_Zs[0,:np.shape(Zs)[0]] = Zs\n\t\tfeed_dict=self.evaluate_fill_feed_dict(new_xyzs, new_Zs)\n\t\tforces, atom_indices = self.sess.run([self.output, self.atom_indices], feed_dict=feed_dict)\n\t\treturn -forces, atom_indices\n\nclass FCGauSHDirectRotationInvariant(Instance_fc_sqdiff_GauSH_direct):\n\tdef __init__(self, TData_, elements_ , Trainable_ = True, Name_ = None):\n\t\tInstance.__init__(self, TData_, elements_, Name_)\n\t\tself.NetType = ""fc_sqdiff_GauSH_direct""\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.element)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.number_radial = PARAMS[""SH_NRAD""]\n\t\tself.l_max = PARAMS[""SH_LMAX""]\n\t\tself.gaussian_params = PARAMS[""RBFS""][:self.number_radial]\n\t\tself.atomic_embed_factors = PARAMS[""ANES""]\n\t\tTensorMol.TFDescriptors.RawSH.data_precision = self.tf_prec\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.inshape =  self.number_radial * (self.l_max + 1) ** 2\n\t\tself.outshape = 3\n\t\tself.Trainable = Trainable_\n\t\tself.orthogonalize = True\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\n\tdef compute_normalization_constants(self):\n\t\tbatch_data = self.TData.GetTrainBatch(20 * self.batch_size)\n\t\tself.TData.ScratchPointer = 0\n\t\txyzs, Zs, labels = tf.convert_to_tensor(batch_data[0], dtype=self.tf_prec), tf.convert_to_tensor(batch_data[1]), tf.convert_to_tensor(batch_data[2], dtype=self.tf_prec)\n\t\tnum_mols = tf.shape(xyzs)[0]\n\t\trotation_params = tf.stack([np.pi * tf.random_uniform([num_mols], maxval=2.0, dtype=self.tf_prec),\n\t\t\t\tnp.pi * tf.random_uniform([num_mols], maxval=2.0, dtype=self.tf_prec),\n\t\t\t\ttf.random_uniform([num_mols], maxval=2.0, dtype=self.tf_prec)], axis=-1)\n\t\trotated_xyzs, rotated_labels = tf_random_rotate(xyzs, rotation_params, labels)\n\t\tembedding, labels, _, _ = tf_gaussian_spherical_harmonics_element(rotated_xyzs, Zs, rotated_labels,\n\t\t\t\t\t\t\t\t\t\t\tself.element, tf.Variable(self.gaussian_params, dtype=self.tf_prec),\n\t\t\t\t\t\t\t\t\t\t\ttf.Variable(self.atomic_embed_factors, trainable=False, dtype=self.tf_prec),\n\t\t\t\t\t\t\t\t\t\t\tself.l_max, self.orthogonalize)\n\t\twith tf.Session() as sess:\n\t\t\tsess.run(tf.global_variables_initializer())\n\t\t\tembed, label = sess.run([embedding, labels])\n\t\tself.inmean, self.instd = np.mean(embed, axis=0), np.std(embed, axis=0)\n\t\tself.outmean, self.outstd = np.mean(label), np.std(label)\n\t\treturn\n\n\tdef TrainPrepare(self):\n\t\t"""""" Builds the graphs by calling inference """"""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl = tf.placeholder(self.tf_prec, shape=tuple([None, self.MaxNAtoms, 3]), name=""xyzs_pl"")\n\t\t\tself.Zs_pl = tf.placeholder(tf.int32, shape=tuple([None, self.MaxNAtoms]), name=""Zs_pl"")\n\t\t\tself.labels_pl = tf.placeholder(self.tf_prec, shape=tuple([None, self.MaxNAtoms, 3]), name=""labels_pl"")\n\t\t\tself.gaussian_params = tf.Variable(self.gaussian_params, trainable=False, dtype=self.tf_prec)\n\t\t\tself.atomic_embed_factors = tf.Variable(self.atomic_embed_factors, trainable=False, dtype=self.tf_prec)\n\t\t\tnum_mols = tf.shape(self.xyzs_pl)[0]\n\t\t\telement = tf.constant(self.element, dtype=tf.int32)\n\t\t\tinmean = tf.constant(self.inmean, dtype=self.tf_prec)\n\t\t\tinstd = tf.constant(self.instd, dtype=self.tf_prec)\n\t\t\toutmean = tf.constant(self.outmean, dtype=self.tf_prec)\n\t\t\toutstd = tf.constant(self.outstd, dtype=self.tf_prec)\n\t\t\twith tf.name_scope(""Rotation""):\n\t\t\t\trotation_params = tf.stack([np.pi * tf.random_uniform([num_mols], minval=0.1, maxval=1.9, dtype=self.tf_prec),\n\t\t\t\t\t\tnp.pi * tf.random_uniform([num_mols], minval=0.1, maxval=1.9, dtype=self.tf_prec),\n\t\t\t\t\t\ttf.random_uniform([num_mols], minval=0.1, maxval=1.9, dtype=self.tf_prec)], axis=-1, name=""rotation_params"")\n\t\t\t\trotated_xyzs, rotation_matrix = tf_random_rotate(self.xyzs_pl, rotation_params, return_matrix=True)\n\t\t\twith tf.name_scope(""Embedding_Normalization""):\n\t\t\t\tself.embedding, self.labels, mol_atom_indices, min_eigenval = tf_gaussian_spherical_harmonics_element(rotated_xyzs,\n\t\t\t\t\t\tself.Zs_pl, self.labels_pl, element, self.gaussian_params, self.atomic_embed_factors, self.l_max, orthogonalize=self.orthogonalize)\n\t\t\t\tself.norm_embedding = (self.embedding - inmean) / instd\n\t\t\t\tself.norm_labels = (self.labels - outmean) / outstd\n\t\t\tself.norm_output = self.inference(self.norm_embedding)\n\t\t\twith tf.name_scope(""Inverse_Rotation""):\n\t\t\t\tinverse_rotation_matrix = tf.matrix_inverse(tf.gather(rotation_matrix, mol_atom_indices[:,0]))\n\t\t\t\telement_xyzs, rotated_element_xyzs = tf.gather_nd(self.xyzs_pl, mol_atom_indices), tf.gather_nd(rotated_xyzs, mol_atom_indices)\n\t\t\t\tunrotated_norm_output = tf.squeeze(tf.einsum(""lij,lkj->lki"", inverse_rotation_matrix,\n\t\t\t\t\t\ttf.expand_dims(rotated_element_xyzs + self.norm_output, axis=1))) - element_xyzs\n\t\t\tself.output = (unrotated_norm_output * outstd) + outmean\n\t\t\tself.n_atoms_batch = tf.cast(tf.shape(self.output)[0], self.tf_prec)\n\t\t\tself.total_loss, self.loss = self.loss_op(unrotated_norm_output, self.norm_labels)\n\t\t\tself.total_loss /= self.n_atoms_batch\n\t\t\t# barrier_function = -1000.0 * tf.log(tf.concat([self.gaussian_params + 0.9, tf.expand_dims(6.5 - self.gaussian_params[:,0], axis=-1), tf.expand_dims(1.75 - self.gaussian_params[:,1], axis=-1)], axis=1))\n\t\t\t# truncated_barrier_function = tf.reduce_sum(tf.where(tf.greater(barrier_function, 0.0), barrier_function, tf.zeros_like(barrier_function)))\n\t\t\t# gaussian_overlap_constraint = tf.square(0.001 / min_eigenval)\n\t\t\tself.rotation_constraint = tf.reduce_sum(tf.square(tf.clip_by_value(tf.gradients(self.output, rotation_params), -1, 1))) / self.n_atoms_batch\n\t\t\tloss_and_constraint = self.total_loss + self.rotation_constraint\n\t\t\tself.train_op = self.training(loss_and_constraint, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif self.profiling:\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\treturn\n\n\t# def training(self, loss, learning_rate, momentum):\n\t# \t""""""Sets up the training Ops.\n\t# \tCreates a summarizer to track the loss over time in TensorBoard.\n\t# \tCreates an optimizer and applies the gradients to all trainable variables.\n\t# \tThe Op returned by this function is what must be passed to the\n\t# \t`sess.run()` call to cause the model to train.\n\t# \tArgs:\n\t# \tloss: Loss tensor, from loss().\n\t# \tlearning_rate: The learning rate to use for gradient descent.\n\t# \tReturns:\n\t# \ttrain_op: The Op for training.\n\t# \t""""""\n\t# \ttf.summary.scalar(loss.op.name, loss)\n\t# \toptimizer = tf.train.AdamOptimizer(learning_rate)\n\t# \tgrads_and_vars = optimizer.compute_gradients(loss)\n\t# \tcapped_grads_and_vars = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in grads_and_vars]\n\t# \tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t# \ttrain_op = optimizer.apply_gradients(capped_grads_and_vars)\n\t# \treturn train_op\n\n\t# def training_op(self, loss, learning_rate, momentum):\n\t# \t""""""Sets up the training Ops.\n\t# \tCreates a summarizer to track the loss over time in TensorBoard.\n\t# \tCreates an optimizer and applies the gradients to all trainable variables.\n\t# \tThe Op returned by this function is what must be passed to the\n\t# \t`sess.run()` call to cause the model to train.\n\t# \tArgs:\n\t# \tloss: Loss tensor, from loss().\n\t# \tlearning_rate: The learning rate to use for gradient descent.\n\t# \tReturns:\n\t# \ttrain_op: The Op for training.\n\t# \t""""""\n\t# \ttf.summary.scalar(loss.op.name, loss)\n\t# \toptimizer = tf.train.AdamOptimizer(learning_rate)\n\t# \tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t# \tgradients_vars = optimizer.compute_gradients(loss)\n\t# \tfor gradient, var in gradients_vars:\n\t#\n\t# \ttrain_op = optimizer.minimize(loss, global_step=global_step)\n\t# \treturn train_op\n\n\tdef Clean(self):\n\t\tif (self.sess != None):\n\t\t\tself.sess.close()\n\t\tself.sess = None\n\t\tself.loss = None\n\t\tself.output = None\n\t\tself.total_loss = None\n\t\tself.train_op = None\n\t\tself.saver = None\n\t\tself.gradient = None\n\t\tself.summary_writer = None\n\t\tself.PreparedFor = 0\n\t\tself.summary_op = None\n\t\tself.activation_function = None\n\t\tself.atomic_embed_factors = None\n\t\tself.gaussian_params = None\n\t\tself.labels = None\n\t\tself.norm_output = None\n\t\tself.norm_labels = None\n\t\tself.norm_embedding = None\n\t\tself.labels_pl = None\n\t\tself.Zs_pl = None\n\t\tself.xyzs_pl = None\n\t\tself.output = None\n\t\tself.embedding = None\n\t\tself.n_atoms_batch = None\n\t\tself.rotation_constraint = None\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef train_step(self,step):\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss, n_atoms_epoch = 0.0, 0.0\n\t\trotation_loss = 0.0\n\t\tministeps = int(Ncase_train/self.batch_size)\n\t\tfor ministep in xrange(ministeps):\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) #advances the case pointer in TData...\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data)\n\t\t\tif self.profiling:\n\t\t\t\t_, total_loss_value, loss_value, n_atoms_batch = self.sess.run([self.train_op, self.total_loss, self.loss,\n\t\t\t\t\t\tself.n_atoms_batch], feed_dict=feed_dict, options=self.options, run_metadata=self.run_metadata)\n\t\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t\twith open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t\t\tf.write(chrome_trace)\n\t\t\telse:\n\t\t\t\t_, total_loss_value, loss_value, n_atoms_batch, rotation_constraint = self.sess.run([self.train_op, self.total_loss,\n\t\t\t\t\t\tself.loss, self.n_atoms_batch, self.rotation_constraint], feed_dict=feed_dict)\n\t\t\ttrain_loss += total_loss_value\n\t\t\tn_atoms_epoch += n_atoms_batch\n\t\t\trotation_loss += rotation_constraint\n\t\ttrain_loss /= ministeps\n\t\trotation_loss /= ministeps\n\t\tduration = time.time() - start_time\n\t\tself.print_training(step, train_loss, n_atoms_epoch, duration, rotation_loss)\n\t\treturn\n\n\tdef test(self, step):\n\t\tprint(""testing..."")\n\t\tNcase_test = self.TData.NTest\n\t\ttest_loss, n_atoms_epoch = 0.0, 0.0\n\t\ttest_start_time = time.time()\n\t\tmean_test_error, std_dev_test_error = 0.0, 0.0\n\t\ttest_epoch_labels, test_epoch_outputs = [], []\n\t\trotation_loss = 0.0\n\t\tministeps = int(Ncase_test/self.batch_size)\n\t\tfor ministep in xrange(ministeps):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size)\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data)\n\t\t\toutput, labels, total_loss_value, loss_value, n_atoms_batch, rotation_constraint, gaussian_params, atomic_embed_factors = self.sess.run([self.output,\n\t\t\t\t\tself.labels, self.total_loss, self.loss, self.n_atoms_batch, self.rotation_constraint, self.gaussian_params, self.atomic_embed_factors],  feed_dict=feed_dict)\n\t\t\ttest_loss += total_loss_value\n\t\t\tn_atoms_epoch += n_atoms_batch\n\t\t\trotation_loss += rotation_constraint\n\t\t\ttest_epoch_labels.append(labels)\n\t\t\ttest_epoch_outputs.append(output)\n\t\ttest_loss /= ministeps\n\t\trotation_loss /= ministeps\n\t\ttest_epoch_labels = np.concatenate(test_epoch_labels)\n\t\ttest_epoch_outputs = np.concatenate(test_epoch_outputs)\n\t\ttest_epoch_errors = test_epoch_labels - test_epoch_outputs\n\t\tduration = time.time() - test_start_time\n\t\tfor i in [random.randint(0, self.batch_size) for j in xrange(20)]:\n\t\t\tLOGGER.info(""Label: %s  Output: %s"", test_epoch_labels[i], test_epoch_outputs[i])\n\t\tLOGGER.info(""MAE: %f"", np.mean(np.abs(test_epoch_errors)))\n\t\tLOGGER.info(""MSE: %f"", np.mean(test_epoch_errors))\n\t\tLOGGER.info(""RMSE: %f"", np.sqrt(np.mean(np.square(test_epoch_errors))))\n\t\tLOGGER.info(""Std. Dev.: %f"", np.std(test_epoch_errors))\n\t\t# LOGGER.info(""Gaussian paramaters: %s"", gaussian_params)\n\t\t# LOGGER.info(""Atomic embedding factors: %s"", atomic_embed_factors)\n\t\tself.print_testing(step, test_loss, n_atoms_epoch, duration, rotation_loss)\n\t\treturn test_loss\n\n\tdef print_training(self, step, loss, n_cases, duration, rotation_loss):\n\t\tLOGGER.info(""step: %7d  duration: %.5f train loss: %.10f rotation loss: %0.10f"", step, duration, loss, rotation_loss)\n\t\treturn\n\n\tdef print_testing(self, step, loss, n_cases, duration, rotation_loss):\n\t\tLOGGER.info(""step: %7d  duration: %.5f test loss: %.10f rotation loss: %0.10f"", step, duration, loss, rotation_loss)\n\t\treturn\n\nclass Instance_del_fc_sqdiff(Instance_fc_sqdiff):\n\tdef __init__(self, TData_, ele_=1, Name_=None):\n\t\tInstance.__init__(self, TData_, ele_, Name_)\n\t\tself.NetType = ""del_fc_sqdiff""\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.element)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\tdef inference(self, inputs, bleep, bloop, blop):\n\t\t""""""Build the MNIST model up to where it may be used for inference.\n\t\tArgs:\n\t\timages: Images placeholder, from inputs().\n\t\thidden1_units: Size of the first hidden layer.\n\t\thidden2_units: Size of the second hidden layer.\n\t\tReturns:\n\t\tsoftmax_linear: Output tensor with the computed logits.\n\t\t""""""\n\t\thidden1_units = PARAMS[""hidden1""]\n\t\thidden2_units = PARAMS[""hidden2""]\n\t\thidden3_units = PARAMS[""hidden3""]\n\t\tLOGGER.debug(""hidden1_units: ""+str(hidden1_units))\n\t\tLOGGER.debug(""hidden2_units: ""+str(hidden2_units))\n\t\tLOGGER.debug(""hidden3_units: ""+str(hidden3_units))\n\t\t# Hidden 1\n\t\twith tf.name_scope(\'hidden1\'):\n\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=list(self.inshape)+[hidden1_units], var_stddev= 0.4 / math.sqrt(float(self.inshape[0])), var_wd= 0.00)\n\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\thidden1 = tf.nn.relu(tf.matmul(inputs[:-3], weights) + biases)\n\t\t\t#tf.summary.scalar(\'min/\' + weights.name, tf.reduce_min(weights))\n\t\t\t#tf.summary.histogram(weights.name, weights)\n\t\t# Hidden 2\n\t\twith tf.name_scope(\'hidden2\'):\n\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev= 0.4 / math.sqrt(float(hidden1_units)), var_wd= 0.00)\n\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec),name=\'biases\')\n\t\t\thidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n\n\t\t# Hidden 3\n\t\twith tf.name_scope(\'hidden3\'):\n\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev= 0.4 / math.sqrt(float(hidden2_units)), var_wd= 0.00)\n\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec),name=\'biases\')\n\t\t\thidden3 = tf.nn.relu(tf.matmul(hidden2, weights) + biases)\n\t\t#Delta Layer\n\t\twith tf.name_scope(\'delta_layer\'):\n\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units]+ list(2*self.outshape), var_stddev= 0.4 / math.sqrt(float(hidden3_units)), var_wd= 0.00)\n\t\t\tbiases = tf.Variable(tf.zeros(self.outshape, dtype=self.tf_prec), name=\'biases\')\n\t\t\tdelta = tf.matmul(hidden3, weights) + biases\n\t\t# Linear\n\t\twith tf.name_scope(\'regression_linear\'):\n\t\t\tdelta_out = tf.multiply(tf.slice(delta,[self.outshape],[self.outshape]),inputs[-3:])\n\t\t\toutput = tf.add(tf.slice(delta,[0],[self.outshape]),delta_out)\n\t\treturn output\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\nclass Instance_conv2d_sqdiff(Instance):\n\tdef __init__(self, TData_, ele_ = 1 , Name_=None):\n\t\tInstance.__init__(self, TData_, ele_, Name_)\n\t\tself.NetType = ""conv2d_sqdiff""\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.element)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\tdef placeholder_inputs(self, batch_size):\n\t\t""""""Generate placeholder variables to represent the input tensors.\n\t\tThese placeholders are used as inputs by the rest of the model building\n\t\tcode and will be fed from the downloaded data in the .run() loop, below.\n\t\tArgs:\n\t\tbatch_size: The batch size will be baked into both placeholders.\n\t\tReturns:\n\t\tembeds_placeholder: Images placeholder.\n\t\tlabels_placeholder: Labels placeholder.\n\t\t""""""\n\t\t# Note that the shapes of the placeholders match the shapes of the full\n\t\t# image and label tensors, except the first dimension is now batch_size\n\t\t# rather than the full size of the train or test data sets.\n\t\tinputs_pl = tf.placeholder(self.tf_prec, shape=tuple([batch_size,self.inshape]))\n\t\toutputs_pl = tf.placeholder(self.tf_prec, shape=tuple([batch_size, self.outshape]))\n\t\treturn inputs_pl, outputs_pl\n\n\tdef _weight_variable(self, name, shape):\n\t\treturn tf.get_variable(name, shape, self.tf_prec, tf.truncated_normal_initializer(stddev=0.01))\n\n\tdef _bias_variable(self, name, shape):\n\t\treturn tf.get_variable(name, shape, self.tf_prec, tf.constant_initializer(0.01, dtype=self.tf_prec))\n\n\tdef conv2d(self, x, W, b, strides=1):\n\t\t""""""\n\t\t2D Convolution wrapper with bias and relu activation\n\t\t""""""\n\t\tx = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=\'SAME\')\n\t\tx = tf.nn.bias_add(x, b)\n\t\treturn tf.nn.relu(x)\n\n\tdef inference(self, input):\n\t\tFC_SIZE = 512\n\t\twith tf.variable_scope(\'conv1\') as scope:\n\t\t\tin_filters = 1\n\t\t\tout_filters = 8\n\t\t\tkernel = self._weight_variable(\'weights\', [2, 2, 2, in_filters, out_filters])\n\t\t\tconv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding=\'SAME\') # third arg. is the strides case,xstride,ystride,zstride,channel stride\n\t\t\tbiases = self._bias_variable(\'biases\', [out_filters])\n\t\t\tbias = tf.nn.bias_add(conv, biases)\n\t\t\tconv1 = tf.nn.relu(bias, name=scope.name)\n\t\t\tprev_layer = conv1\n\t\t\tin_filters = out_filters\n\n\t\t# pool1 = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding=\'SAME\')\n\t\t#norm1 = pool1  # tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta = 0.75, name=\'norm1\')\n\t\t#prev_layer = norm1\n\n\t\twith tf.variable_scope(\'conv2\') as scope:\n\t\t\tout_filters = 16\n\t\t\tkernel = self._weight_variable(\'weights\', [2, 2, 2, in_filters, out_filters])\n\t\t\tconv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding=\'SAME\')\n\t\t\tbiases = self._bias_variable(\'biases\', [out_filters])\n\t\t\tbias = tf.nn.bias_add(conv, biases)\n\t\t\tconv2 = tf.nn.relu(bias, name=scope.name)\n\t\t\tprev_layer = conv2\n\t\t\tin_filters = out_filters\n\n\t\t# normalize prev_layer here\n\t\t# prev_layer = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding=\'SAME\')\n\n\t\twith tf.variable_scope(\'local1\') as scope:\n\t\t\tdim = np.prod(prev_layer.get_shape().as_list()[1:])\n\t\t\tprev_layer_flat = tf.reshape(prev_layer, [-1, dim])\n\t\t\tweights = self._weight_variable(\'weights\', [dim, FC_SIZE])\n\t\t\tbiases = self._bias_variable(\'biases\', [FC_SIZE])\n\t\t\tlocal1 = tf.nn.relu(tf.matmul(prev_layer_flat, weights) + biases, name=scope.name)\n\t\t\tprev_layer = local1\n\n\t\twith tf.variable_scope(\'local2\') as scope:\n\t\t\tdim = np.prod(prev_layer.get_shape().as_list()[1:])\n\t\t\tprev_layer_flat = tf.reshape(prev_layer, [-1, dim])\n\t\t\tweights = self._weight_variable(\'weights\', [dim, FC_SIZE])\n\t\t\tbiases = self._bias_variable(\'biases\', [FC_SIZE])\n\t\t\tlocal2 = tf.nn.relu(tf.matmul(prev_layer_flat, weights) + biases, name=scope.name)\n\t\t\tprev_layer = local2\n\n\t\twith tf.variable_scope(\'regression_linear\') as scope:\n\t\t\tdim = np.prod(prev_layer.get_shape().as_list()[1:])\n\t\t\tweights = self._weight_variable(\'weights\', [dim]+list(self.outshape))\n\t\t\tbiases = self._bias_variable(\'biases\', self.outshape)\n\t\t\toutput = tf.add(tf.matmul(prev_layer, weights), biases, name=scope.name)\n\t\treturn output\n\n\tdef evaluate(self, eval_input):\n\t\t# Check sanity of input\n\t\tInstance.evaluate(self, eval_input)\n\t\teval_input_ = eval_input\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\teval_input_ =np.copy(eval_input)\n\t\t\teval_input_.resize(([self.PreparedFor]+self.inshape))\n\t\t# pad with zeros\n\t\teval_labels = np.zeros(tuple([self.PreparedFor]+list(self.outshape)))  # dummy labels\n\t\tbatch_data = self.PrepareData([eval_input_, eval_labels])\n\t\t#embeds_placeholder, labels_placeholder = self.placeholder_inputs(Ncase) Made by Prepare()\n\t\tfeed_dict = self.fill_feed_dict(batch_data,self.embeds_placeholder, self.labels_placeholder)\n\t\ttmp = np.array(self.sess.run([self.output], feed_dict=feed_dict))\n\t\tif (not np.all(np.isfinite(tmp))):\n\t\t\tLOGGER.error(""TFsession returned garbage"")\n\t\t\tLOGGER.error(""TFInputs""+str(eval_input)) #If it\'s still a problem here use tf.Print version of the graph.\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\treturn tmp[:eval_input.shape[0]]\n\t\treturn tmp\n\n\tdef Save(self):\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\t\tInstance.Save(self)\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.slice(tf.sub(output, labels),[0,self.outshape[0]-3],[-1,-1])\n\t\t# this only compares direct displacement predictions.\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef train_step(self,step):\n\t\tNcase_train = self.TData.NTrainCasesInScratch()\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttotal_correct = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data=self.PrepareData(self.TData.GetTrainBatch(self.element,  self.batch_size)) #advances the case pointer in TData...\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\t_, total_loss_value, loss_value = self.sess.run([self.train_op, self.total_loss, self.loss], feed_dict=feed_dict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\tduration = time.time() - start_time\n\t\t#self.print_training(step, train_loss, total_correct, Ncase_train, duration)\n\t\tself.print_training(step, train_loss, Ncase_train, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\tNcase_test = self.TData.NTestCasesInScratch()\n\t\ttest_loss =  0.0\n\t\ttest_start_time = time.time()\n\t\t#for ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\tbatch_data=self.PrepareData(self.TData.GetTestBatch(self.element,  self.batch_size))#, ministep)\n\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\tpreds, total_loss_value, loss_value  = self.sess.run([self.output, self.total_loss,  self.loss],  feed_dict=feed_dict)\n\t\tself.TData.EvaluateTestBatch(batch_data[1],preds, self.tformer)\n\t\ttest_loss = test_loss + loss_value\n\t\tduration = time.time() - test_start_time\n\t\tLOGGER.info(""testing..."")\n\t\tself.print_training(step, test_loss,  Ncase_test, duration)\n\t\treturn test_loss, feed_dict\n\n\tdef PrepareData(self, batch_data):\n\n\t\t#for i in range(self.batch_size):\n\t\t#\tds=GRIDS.Rasterize(batch_data[0][i])\n\t\t#\tGridstoRaw(ds, GRIDS.NPts, ""Inp""+str(i))\n\n\t\tif (batch_data[0].shape[0]==self.batch_size):\n\t\t\tbatch_data=[batch_data[0].reshape(batch_data[0].shape[0],GRIDS.NGau,GRIDS.NGau,GRIDS.NGau,1), batch_data[1]]\n\t\telif (batch_data[0].shape[0] < self.batch_size):\n\t\t\tLOGGER.info(""Resizing... "")\n\t\t\tbatch_data=[batch_data[0].resize(self.batch_size,GRIDS.NGau,GRIDS.NGau,GRIDS.NGau,1), batch_data[1].resize((self.batch_size,  batch_data[1].shape[1]))]\n#\t\t\tbatch_data=[batch_data[0], batch_data[1].reshape((batch_data[1].shape[0],1))]\n#\t\t\ttmp_input = np.copy(batch_data[0])\n#\t\t\ttmp_output = np.copy(batch_data[1])\n#\t\t\ttmp_input.resize((self.batch_size,  batch_data[0].shape[1]))\n#\t\t\ttmp_output.resize((self.batch_size,  batch_data[1].shape[1]))\n#\t\t\tbatch_data=[ tmp_input, tmp_output]\n\t\treturn batch_data\n\nclass Instance_3dconv_sqdiff(Instance):\n\t\'\'\' Let\'s see if a 3d-convolutional network improves the learning rate on the Gaussian grids. \'\'\'\n\tdef __init__(self, TData_, ele_ = 1 , Name_=None):\n\t\tInstance.__init__(self, TData_, ele_, Name_)\n\t\tself.NetType = ""3conv_sqdiff""\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.element)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\tdef placeholder_inputs(self, batch_size):\n\t\t""""""Generate placeholder variables to represent the input tensors.\n\t\tThese placeholders are used as inputs by the rest of the model building\n\t\tcode and will be fed from the downloaded data in the .run() loop, below.\n\t\tArgs:\n\t\tbatch_size: The batch size will be baked into both placeholders.\n\t\tReturns:\n\t\tembeds_placeholder: Images placeholder.\n\t\tlabels_placeholder: Labels placeholder.\n\t\t""""""\n\t\t# Note that the shapes of the placeholders match the shapes of the full\n\t\t# image and label tensors, except the first dimension is now batch_size\n\t\t# rather than the full size of the train or test data sets.\n\t\tif (self.inshape[0]!=GRIDS.NGau3):\n\t\t\tprint(""Bad inputs... "", self.inshape)\n\t\t\traise Exception(""Nonsquare"")\n\t\tinputs_pl = tf.placeholder(self.tf_prec, shape=tuple([batch_size,GRIDS.NGau,GRIDS.NGau,GRIDS.NGau,1]))\n\t\toutputs_pl = tf.placeholder(self.tf_prec, shape=tuple([batch_size]+list(self.outshape)))\n\t\treturn inputs_pl, outputs_pl\n\n\tdef _weight_variable(self, name, shape):\n\t\treturn tf.get_variable(name, shape, self.tf_prec, tf.truncated_normal_initializer(stddev=0.01))\n\n\tdef _bias_variable(self, name, shape):\n\t\treturn tf.get_variable(name, shape, self.tf_prec, tf.constant_initializer(0.01, dtype=self.tf_prec))\n\n\tdef inference(self, input):\n\t\tFC_SIZE = 512\n\t\twith tf.variable_scope(\'conv1\') as scope:\n\t\t\tin_filters = 1\n\t\t\tout_filters = 8\n\t\t\tkernel = self._weight_variable(\'weights\', [2, 2, 2, in_filters, out_filters])\n\t\t\tconv = tf.nn.conv3d(input, kernel, [1, 1, 1, 1, 1], padding=\'SAME\') # third arg. is the strides case,xstride,ystride,zstride,channel stride\n\t\t\tbiases = self._bias_variable(\'biases\', [out_filters])\n\t\t\tbias = tf.nn.bias_add(conv, biases)\n\t\t\tconv1 = tf.nn.relu(bias, name=scope.name)\n\t\t\tprev_layer = conv1\n\t\t\tin_filters = out_filters\n\n\t\t# pool1 = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding=\'SAME\')\n\t\t#norm1 = pool1  # tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta = 0.75, name=\'norm1\')\n\t\t#prev_layer = norm1\n\n\t\twith tf.variable_scope(\'conv2\') as scope:\n\t\t\tout_filters = 16\n\t\t\tkernel = self._weight_variable(\'weights\', [2, 2, 2, in_filters, out_filters])\n\t\t\tconv = tf.nn.conv3d(prev_layer, kernel, [1, 1, 1, 1, 1], padding=\'SAME\')\n\t\t\tbiases = self._bias_variable(\'biases\', [out_filters])\n\t\t\tbias = tf.nn.bias_add(conv, biases)\n\t\t\tconv2 = tf.nn.relu(bias, name=scope.name)\n\t\t\tprev_layer = conv2\n\t\t\tin_filters = out_filters\n\n\t\t# normalize prev_layer here\n\t\t# prev_layer = tf.nn.max_pool3d(prev_layer, ksize=[1, 3, 3, 3, 1], strides=[1, 2, 2, 2, 1], padding=\'SAME\')\n\n\t\twith tf.variable_scope(\'local1\') as scope:\n\t\t\tdim = np.prod(prev_layer.get_shape().as_list()[1:])\n\t\t\tprev_layer_flat = tf.reshape(prev_layer, [-1, dim])\n\t\t\tweights = self._weight_variable(\'weights\', [dim, FC_SIZE])\n\t\t\tbiases = self._bias_variable(\'biases\', [FC_SIZE])\n\t\t\tlocal1 = tf.nn.relu(tf.matmul(prev_layer_flat, weights) + biases, name=scope.name)\n\t\t\tprev_layer = local1\n\n\t\twith tf.variable_scope(\'local2\') as scope:\n\t\t\tdim = np.prod(prev_layer.get_shape().as_list()[1:])\n\t\t\tprev_layer_flat = tf.reshape(prev_layer, [-1, dim])\n\t\t\tweights = self._weight_variable(\'weights\', [dim, FC_SIZE])\n\t\t\tbiases = self._bias_variable(\'biases\', [FC_SIZE])\n\t\t\tlocal2 = tf.nn.relu(tf.matmul(prev_layer_flat, weights) + biases, name=scope.name)\n\t\t\tprev_layer = local2\n\n\t\twith tf.variable_scope(\'regression_linear\') as scope:\n\t\t\tdim = np.prod(prev_layer.get_shape().as_list()[1:])\n\t\t\tweights = self._weight_variable(\'weights\', [dim]+list(self.outshape))\n\t\t\tbiases = self._bias_variable(\'biases\', self.outshape)\n\t\t\toutput = tf.add(tf.matmul(prev_layer, weights), biases, name=scope.name)\n\t\treturn output\n\n\tdef evaluate(self, eval_input):\n\t\t# Check sanity of input\n\t\tInstance.evaluate(self, eval_input)\n\t\teval_input_ = eval_input\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\teval_input_ =np.copy(eval_input)\n\t\t\teval_input_.resize(([self.PreparedFor]+self.inshape))\n\t\t# pad with zeros\n\t\teval_labels = np.zeros(tuple([self.PreparedFor]+list(self.outshape)))  # dummy labels\n\t\tbatch_data = self.PrepareData([eval_input_, eval_labels])\n\t\t#embeds_placeholder, labels_placeholder = self.placeholder_inputs(Ncase) Made by Prepare()\n\t\tfeed_dict = self.fill_feed_dict(batch_data,self.embeds_placeholder, self.labels_placeholder)\n\t\ttmp = np.array(self.sess.run([self.output], feed_dict=feed_dict))\n\t\tif (not np.all(np.isfinite(tmp))):\n\t\t\tLOGGER.error(""TFsession returned garbage"")\n\t\t\tLOGGER.error(""TFInputs""+str(eval_input)) #If it\'s still a problem here use tf.Print version of the graph.\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\treturn tmp[:eval_input.shape[0]]\n\t\treturn tmp\n\n\tdef Save(self):\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\t\tInstance.Save(self)\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.slice(tf.sub(output, labels),[0,self.outshape[0]-3],[-1,-1])\n\t\t# this only compares direct displacement predictions.\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef train_step(self,step):\n\t\tNcase_train = self.TData.NTrainCasesInScratch()\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttotal_correct = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data=self.PrepareData(self.TData.GetTrainBatch(self.element,  self.batch_size)) #advances the case pointer in TData...\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\t_, total_loss_value, loss_value = self.sess.run([self.train_op, self.total_loss, self.loss], feed_dict=feed_dict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\tduration = time.time() - start_time\n\t\t#self.print_training(step, train_loss, total_correct, Ncase_train, duration)\n\t\tself.print_training(step, train_loss, Ncase_train, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\tNcase_test = self.TData.NTestCasesInScratch()\n\t\ttest_loss =  0.0\n\t\ttest_start_time = time.time()\n\t\t#for ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\tbatch_data=self.PrepareData(self.TData.GetTestBatch(self.element,  self.batch_size))#, ministep)\n\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\tpreds, total_loss_value, loss_value  = self.sess.run([self.output, self.total_loss,  self.loss],  feed_dict=feed_dict)\n\t\tself.TData.EvaluateTestBatch(batch_data[1],preds, self.tformer)\n\t\ttest_loss = test_loss + loss_value\n\t\tduration = time.time() - test_start_time\n\t\tLOGGER.info(""testing..."")\n\t\tself.print_training(step, test_loss,  Ncase_test, duration)\n\t\treturn test_loss, feed_dict\n\n\tdef PrepareData(self, batch_data):\n\n\t\t#for i in range(self.batch_size):\n\t\t#\tds=GRIDS.Rasterize(batch_data[0][i])\n\t\t#\tGridstoRaw(ds, GRIDS.NPts, ""Inp""+str(i))\n\n\t\tif (batch_data[0].shape[0]==self.batch_size):\n\t\t\tbatch_data=[batch_data[0].reshape(batch_data[0].shape[0],GRIDS.NGau,GRIDS.NGau,GRIDS.NGau,1), batch_data[1]]\n\t\telif (batch_data[0].shape[0] < self.batch_size):\n\t\t\tLOGGER.info(""Resizing... "")\n\t\t\tbatch_data=[batch_data[0].resize(self.batch_size,GRIDS.NGau,GRIDS.NGau,GRIDS.NGau,1), batch_data[1].resize((self.batch_size,  batch_data[1].shape[1]))]\n#\t\t\tbatch_data=[batch_data[0], batch_data[1].reshape((batch_data[1].shape[0],1))]\n#\t\t\ttmp_input = np.copy(batch_data[0])\n#\t\t\ttmp_output = np.copy(batch_data[1])\n#\t\t\ttmp_input.resize((self.batch_size,  batch_data[0].shape[1]))\n#\t\t\ttmp_output.resize((self.batch_size,  batch_data[1].shape[1]))\n#\t\t\tbatch_data=[ tmp_input, tmp_output]\n\t\treturn batch_data\n\n\nclass Instance_KRR(Instance):\n\tdef __init__(self, TData_, ele_ = 1 , Name_=None):\n\t\tInstance.__init__(self, TData_, ele_, Name_)\n\t\tself.NetType = ""KRR""\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.element)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.krr = None\n\t\treturn\n\n\tdef evaluate(self, eval_input):\n\t\treturn self.krr.predict(eval_input)\n\n\tdef Save(self):\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\t\treturn\n\n\tdef train(self,n_step):\n\t\tfrom sklearn.kernel_ridge import KernelRidge\n\t\tself.krr = KernelRidge(alpha=0.001, kernel=\'rbf\')\n\t\t# Here we should use as much data as the kernel method can actually take.\n\t\t# probly on the order of 100k cases.\n\t\tti,to = self.TData.GetTrainBatch(self.element,  10000)\n\t\tself.krr.fit(ti,to)\n\t\tself.test(0)\n\t\treturn\n\n\tdef test(self, step):\n\t\tNcase_test = self.TData.NTestCasesInScratch()\n\t\ttest_loss =  0.0\n\t\tti,to = self.TData.GetTestBatch(self.element,  self.batch_size)\n\t\tpreds  = self.krr.predict(ti)\n\t\tself.TData.EvaluateTestBatch(to,preds, self.tformer)\n\t\treturn None, None\n\n\tdef basis_opt_run(self):\n\t\tfrom sklearn.kernel_ridge import KernelRidge\n\t\tself.krr = KernelRidge(alpha=0.001, kernel=\'rbf\')\n\t\t# Here we should use as much data as the kernel method can actually take.\n\t\t# probly on the order of 100k cases.\n\t\tti,to = self.TData.GetTrainBatch(self.element,  10000)\n\t\tself.krr.fit(ti,to)\n\t\tNcase_test = self.TData.NTestCasesInScratch()\n\t\ttest_loss =  0.0\n\t\tti,to = self.TData.GetTestBatch(self.element,  10000)\n\t\tpreds = self.krr.predict(ti)\n\t\treturn self.TData.EvaluateTestBatch(to,preds, self.tformer, Opt=True)\n\n\tdef PrepareData(self, batch_data):\n\t\traise Exception(""NYI"")\n\t\treturn\n'"
TensorMol/TFNetworks/TFManage.py,1,"b'""""""\n Either trains, tests, evaluates or provides an interface for optimization.\n\n TODO: Unify evaluation interface, clean up TensorMolData etc.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Containers.TensorData import *\nfrom .TFInstance import *\nimport numpy as np\nimport gc\n\nclass TFManage:\n\t""""""\n\t\tA manager of tensorflow instances which perform atom-wise predictions\n\t\tand parent of the molecular instance mangager.\n\t""""""\n\tdef __init__(self, Name_="""", TData_=None, Train_=True, NetType_=""fc_sqdiff"", RandomTData_=True, Trainable_ = True):\n\t\t""""""\n\t\t\tArgs:\n\t\t\t\tName_: If not blank, will try to load a network with that name using Prepare()\n\t\t\t\tTData_: A TensorData instance to provide and process data.\n\t\t\t\tTrain_: Whether to train the instances raised.\n\t\t\t\tNetType_: Choices of Various network architectures.\n\t\t\t\tRandomTData_: Modifes the preparation of training batches.\n\t\t\t\tntrain_: Number of steps to train an element.\n\t\t""""""\n\t\tself.path = PARAMS[""networks_directory""]\n\t\tself.TData = TData_\n\t\tself.Trainable = Trainable_\n\t\tself.NetType = NetType_\n\t\tself.n_train = PARAMS[""max_steps""]\n\t\tif (Name_ != """"):\n\t\t\t# This will unpickle and instantiate TData...\n\t\t\tself.name = Name_\n\t\t\tself.Prepare()\n\t\t\treturn\n\t\tif (RandomTData_==False):\n\t\t\tself.TData.Randomize=False\n\t\t# All done if you\'re doing molecular calculations\n\t\tprint(self.TData.AvailableElements)\n\t\tprint(self.TData.AvailableDataFiles)\n\t\tprint(self.TData.SamplesPerElement)\n\t\tself.name = self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+time.strftime(""%a_%b_%d_%H.%M.%S_%Y"")\n\t\tprint(""--- TF will be fed by ---"", self.TData.name)\n\t\tself.TrainedAtoms=[] # In order of the elements in TData\n\t\tself.TrainedNetworks=[] # In order of the elements in TData\n\t\tself.Instances=[None for i in range(MAX_ATOMIC_NUMBER)] # In order of the elements in TData\n\t\tif (Train_):\n\t\t\tself.Train()\n\t\t\treturn\n\t\treturn\n\n\tdef Print(self):\n\t\tprint(""-- TensorMol, Tensorflow Manager Status--"")\n\t\treturn\n\n\tdef Train(self):\n\t\tprint(""Will train a NNetwork for each element in: "", self.TData.name)\n\t\tfor i in range(len(self.TData.AvailableElements)):\n\t\t\tself.TrainElement(self.TData.AvailableElements[i])\n\t\treturn\n\n\tdef Save(self):\n\t\tprint(""Saving TFManager:"",self.path+self.name+"".tfm"")\n\t\tself.TData.CleanScratch()\n\t\tf=open(self.path+self.name+"".tfm"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\n\tdef Load(self):\n\t\tprint(""Unpickling TFManager..."")\n\t\tfrom ..Containers import PickleTM\n\t\ttmp = PickleTM.UnPickleTM(self.path+self.name+"".tfm"")\n\t\tself.__dict__.update(tmp)\n\t\tprint(""TFManager Metadata Loaded, Reviving Networks."")\n\t\tself.Print()\n\t\treturn\n\n\tdef TrainElement(self, ele):\n\t\tprint(""Training Element:"", ele)\n\t\t# if (self.TData.dig.eshape==None):\n\t\t# \traise Exception(""Must Have Digester"")\n\t\t# It\'s up the TensorData to provide the batches and input output shapes.\n\t\tif (self.NetType == ""fc_classify"" or PARAMS[""Classify""]):\n\t\t\tself.Instances[ele] = Instance_fc_classify(self.TData, ele, None)\n\t\telif (self.NetType == ""fc_sqdiff""):\n\t\t\tself.Instances[ele] = Instance_fc_sqdiff(self.TData, ele, None)\n\t\telif (self.NetType == ""fc_sqdiff_GauSH_direct""):\n\t\t\tself.Instances[ele] = Instance_fc_sqdiff_GauSH_direct(self.TData, ele, True)\n\t\telif (self.NetType == ""fc_sqdiff_GauSH_direct_constrain_rotation""):\n\t\t\tself.Instances[ele] = FCGauSHDirectRotationInvariant(self.TData, ele, True)\n\t\telif (self.NetType == ""del_fc_sqdiff""):\n\t\t\tself.Instances[ele] = Instance_del_fc_sqdiff(self.TData, ele, None)\n\t\telif (self.NetType == ""3conv_sqdiff""):\n\t\t\tself.Instances[ele] = Instance_3dconv_sqdiff(self.TData, ele, None)\n\t\telif (self.NetType == ""KRR_sqdiff""):\n\t\t\tself.Instances[ele] = Instance_KRR(self.TData, ele, None)\n\t\telif (self.NetType == ""fc_sqdiff_queue""):\n\t\t\tself.Instances[ele] = Queue_Instance(self.TData, ele, None)\n\t\telse:\n\t\t\traise Exception(""Unknown Network Type!"")\n\t\tself.Instances[ele].train(self.n_train) # Just for the sake of debugging.\n\t\tnm = self.Instances[ele].name\n\t\tif self.TrainedNetworks.count(nm)==0:\n\t\t\tself.TrainedNetworks.append(nm)\n\t\tif self.TrainedAtoms.count(ele)==0:\n\t\t\tself.TrainedAtoms.append(ele)\n\t\tself.Save()\n\t\tgc.collect()\n\t\treturn\n\n\tdef EvalElement(self, ele, test_input):\n\t\treturn self.Instances[ele].evaluate(test_input)    # for debugging\n\n\tdef Prepare(self):\n\t\tself.Load()\n\t\tself.Instances=[None for i in range(MAX_ATOMIC_NUMBER)] # In order of the elements in TData\n\t\tfor i in range (0, len(self.TrainedAtoms)):\n\t\t\tif (self.NetType == ""fc_classify""):\n\t\t\t\tself.Instances[self.TrainedAtoms[i]] = Instance_fc_classify(None, self.TrainedAtoms[i], self.TrainedNetworks[i])\n\t\t\telif (self.NetType == ""fc_sqdiff""):\n\t\t\t\tself.Instances[self.TrainedAtoms[i]] = Instance_fc_sqdiff(None, self.TrainedAtoms[i], self.TrainedNetworks[i])\n\t\t\telif (self.NetType == ""3conv_sqdiff""):\n\t\t\t\tself.Instances[self.TrainedAtoms[i]] = Instance_3dconv_sqdiff(None, self.TrainedAtoms[i], self.TrainedNetworks[i])\n\t\t\telif (self.NetType == ""fc_sqdiff_GauSH_direct""):\n\t\t\t\tself.Instances[self.TrainedAtoms[i]] = Instance_fc_sqdiff_GauSH_direct(name=self.TrainedNetworks[i], trainable=False)\n\t\t\telse:\n\t\t\t\traise Exception(""Unknown Network Type!"")\n\t\t# Raise TF instances for each atom which have already been trained.\n\t\treturn\n\n\tdef SampleAtomGrid(self, mol, atom, maxstep, ngrid):\n\t\t# use TF instances for each atom.\n\t\treturn  self.TData.dig.UniformDigest(mol,atom,maxstep,ngrid)\n\n\tdef SmoothPOneAtom(self, mol_, atom_):\n\t\t""""""\n\t\t\tUses smooth fitting of Go probability to pick the next point as predicted by the network\n\t\t\tThis should eventually be made faster by doing all atoms of an element type at once.\n\t\t""""""\n\t\tinputs = self.TData.dig.Emb(mol_, atom_, [mol_.coords[atom_]],False)\n\t\toutput = self.Instances[mol_.atoms[atom_]].evaluate(inputs)[0,0]\n\t\tif(not np.all(np.isfinite(output))):\n\t\t\tprint(output)\n\t\t\traise Exception(""BadTFOutput"")\n\t\tp = mol_.UseGoProb(atom_, output)\n\t\treturn p\n\n\tdef EvalRotAvForceOld(self, mol, RotAv=10, Debug=False):\n\t\t""""""\n\t\tGoes without saying we should do this in batches for each element,\n\t\tif it actually improves accuracy. And improve rotational sampling.\n\t\tBut for the time being I\'m doing this sloppily.\n\t\t""""""\n\t\tif(self.TData.dig.name != ""GauSH""):\n\t\t\traise Exception(""Don\'t average this..."")\n\t\tp = np.zeros((mol.NAtoms(),3))\n\t\tpi = np.zeros((3,RotAv,mol.NAtoms(),3))\n\t\tfor atom in range(mol.NAtoms()):\n\t\t\tinputs = np.zeros((3*RotAv,PARAMS[""SH_NRAD""]*(PARAMS[""SH_LMAX""]+1)*(PARAMS[""SH_LMAX""]+1)))\n\t\t\tfor ax in range(3):\n\t\t\t\taxis = [0,0,0]\n\t\t\t\taxis[ax] = 1\n\t\t\t\tfor i, theta in enumerate(np.linspace(-Pi, Pi, RotAv)):\n\t\t\t\t\tmol_t = Mol(mol.atoms, mol.coords)\n\t\t\t\t\tmol_t.Rotate(axis, theta, mol.coords[atom])\n\t\t\t\t\tinputs[ax*RotAv+i] = self.TData.dig.Emb(mol_t, atom, mol_t.coords[atom],False)\n\t\t\touts = self.Instances[mol_t.atoms[atom]].evaluate(inputs)\n\t\t\tfor ax in range(3):\n\t\t\t\taxis = [0,0,0]\n\t\t\t\taxis[ax] = 1\n\t\t\t\tfor i, theta in enumerate(np.linspace(-Pi, Pi, RotAv)):\n\t\t\t\t\tpi[ax,i,atom] = np.dot(RotationMatrix(axis, -1.0*theta),outs[0,ax*RotAv+i].T).reshape(3)\n\t\tp[atom] += pi[ax,i,atom]\n\t\tif (Debug):\n\t\t\tprint(""Checking Rotations... "")\n\t\t\tfor atom in range(mol.NAtoms()):\n\t\t\t\tprint(""Atom "", atom, "" mean: "", np.mean(pi[:,:,atom],axis=(0,1)), "" std "",np.std(pi[:,:,atom],axis=(0,1)))\n\t\t\t\tfor ax in range(3):\n\t\t\t\t\tfor i, theta in enumerate(np.linspace(-Pi, Pi, RotAv)):\n\t\t\t\t\t\tprint(atom,ax,theta,"":"",pi[ax,i,atom])\n\t\treturn p/(3.0*RotAv)\n\n\tdef EvalRotAvForce(self, mol, RotAv=10, Debug=False):\n\t\t""""""\n\t\tRewritten for optimal performance with rotational averaging and atom ordering.\n\t\t""""""\n\t\tif(self.TData.dig.name != ""GauSH""):\n\t\t    raise Exception(""Don\'t average this..."")\n\t\tp = np.zeros((mol.NAtoms(),3)) # Forces to output\n\t\teles = mol.AtomTypes().tolist()\n\t\ttransfs = np.zeros((3*RotAv,3,3))\n\t\titransfs = np.zeros((3*RotAv,3,3))\n\t\tind = 0\n\t\tfor ax in range(3):\n\t\t\taxis = [0,0,0]\n\t\t\taxis[ax] = 1\n\t\t\tfor i, theta in enumerate(np.linspace(-Pi, Pi, RotAv)):\n\t\t\t\ttransfs[ind] = RotationMatrix(axis, theta)\n\t\t\t\titransfs[ind] = RotationMatrix(axis, -1.0*theta)\n\t\t\t\tind = ind+1\n\t\tfor ele in eles:\n\t\t\teats = [i for i in range(mol.NAtoms()) if mol.atoms[i] == ele]\n\t\t\tna = len(eats)\n\t\t\tinputs = np.zeros((na*3*RotAv,PARAMS[""SH_NRAD""]*(PARAMS[""SH_LMAX""]+1)*(PARAMS[""SH_LMAX""]+1)))\n\t\t\t# Create an index matrix to contract these down and increment the right places.\n\t\t\tfor i,ei in enumerate(eats):\n\t\t\t\tinputs[i*3*RotAv:(i+1)*3*RotAv] = self.TData.dig.Emb(mol, ei, mol.coords[ei], False, False, transfs)\n\t\t\t#print inputs.shape, na*3*RotAv , na, 3*RotAv, 3\n\t\t\touts = self.Instances[ele].evaluate(inputs)\n\t\t\t#print inputs.shape, outs.shape, na, 3*RotAv, 3\n\t\t\touts = outs.reshape(na,3*RotAv,3)\n\t\t\tou = np.einsum(""txy,aty->ax"",itransfs,outs)/(3.*RotAv)\n\t\t\tfor i,ei in enumerate(eats):\n\t\t\t\tp[ei] = ou[i].copy()\n\t\treturn p\n\n\tdef EvalOctAvForce(self, mol, Debug=False):\n\t\t""""""\n\t\tGoes without saying we should do this in batches for each element,\n\t\tif it actually improves accuracy. And improve rotational sampling.\n\t\tBut for the time being I\'m doing this sloppily.\n\t\t""""""\n\t\tif(self.TData.dig.name != ""GauSH""):\n\t\t    raise Exception(""Don\'t average this..."")\n\t\tops = OctahedralOperations()\n\t\tinvops = map(np.linalg.inv,ops)\n\t\tpi = np.zeros((mol.NAtoms(),len(ops),3))\n\t\tp = np.zeros((mol.NAtoms(),3))\n\t\tfor atom in range(mol.NAtoms()):\n\t\t\tinputs = np.zeros((len(ops),PARAMS[""SH_NRAD""]*(PARAMS[""SH_LMAX""]+1)*(PARAMS[""SH_LMAX""]+1)))\n\t\t\tfor i in range(len(ops)):\n\t\t\t\top = ops[i]\n\t\t\t\tmol_t = Mol(mol.atoms, mol.coords)\n\t\t\t\tmol_t.Transform(op, mol.coords[atom])\n\t\t\t\tinputs[i] = self.TData.dig.Emb(mol_t, atom, mol_t.coords[atom],False)\n\t\t\t\tif (Debug):\n\t\t\t\t\tprint(inputs[i])\n\t\t\touts = self.Instances[mol_t.atoms[atom]].evaluate(inputs)[0]\n\t\t\tfor i in range(len(ops)):\n\t\t\t\tpi[atom,i] = np.dot(invops[i],outs[i].T).reshape(3)\n\t\t\t\tp[atom] += np.sum(pi[atom,i], axis=0)\n\t\tif (Debug):\n\t\t\tprint(""Checking Rotations... "")\n\t\t\tfor atom in range(mol.NAtoms()):\n\t\t\t\tprint(""Atom "", atom, "" mean: "", np.mean(pi[atom,:],axis=0), "" std "",np.std(pi[atom,:],axis=0))\n\t\t\t\tfor i in range(len(ops)):\n\t\t\t\t\tprint(atom, i, pi[atom,i])\n\t\treturn p/(len(ops))\n\n\tdef evaluate(self, mol, atom):\n\t\tinputs = self.TData.dig.Emb(mol, atom, mol.coords[atom],False)\n\t\tif (self.Instances[mol.atoms[atom]].tformer.innorm != None):\n\t\t\tinputs = self.Instances[mol.atoms[atom]].tformer.NormalizeIns(inputs, train=False)\n\t\touts = self.Instances[mol.atoms[atom]].evaluate(inputs)\n\t\tif (self.Instances[mol.atoms[atom]].tformer.outnorm != None):\n\t\t\touts = self.Instances[mol.atoms[atom]].tformer.UnNormalizeOuts(outs)\n\t\treturn outs[0]\n\n\tdef EvalOneAtom(self, mol, atom, maxstep = 0.2, ngrid = 50):\n\t\txyz, inputs = self.SampleAtomGrid( mol, atom, maxstep, ngrid)\n\t\tp = self.Instances[mol.atoms[atom]].evaluate(inputs)\n\t\tif (np.sum(p**2)**0.5 != 0):\n\t\t\tp = p/(np.sum(p**2))**0.5\n\t\telse:\n\t\t\tp.fill(1.0)\n\t\t#Check finite-ness or throw\n\t\tif(not np.all(np.isfinite(p))):\n\t\t\tprint(p)\n\t\t\traise Exception(""BadTFOutput"")\n\t\treturn xyz, p\n\n\tdef EvalAllAtoms(self, mol, maxstep = 1.5, ngrid = 50):\n\t\tXYZ=[]\n\t\tP=[]\n\t\tfor i in range (0, mol.atoms.shape[0]):\n\t\t\tprint((""Evaluating atom: "", mol.atoms[i]))\n\t\t\txyz, p = self.EvalOneAtom(mol, i, maxstep, ngrid)\n\t\t\tXYZ.append(xyz)\n\t\t\tP.append(p)\n\t\tXYZ = np.asarray(XYZ)\n\t\tP = np.asarray(P)\n\t\treturn XYZ, P\n\n\tdef EvalOneAtomMB(self, mol, atom, maxstep=0.2, ngrid=50):\n\t\t# This version samples all the other atoms as well and supposes the prob of move is the product of all.\n\t\tsatoms=mol.AtomsWithin(self.TData.dig.SensRadius,mol.coords[atom])\n\t\tele = mol.atoms[atom]\n\t\txyz, inputs = self.SampleAtomGrid( mol, atom, maxstep, ngrid)\n\t\tplist = []\n\t\tp = self.Instances[ele].evaluate(inputs.reshape((inputs.shape[0], -1)))\n\t\tprint(""p: "",p)\n\t\tif (np.sum(p**2)**0.5 != 0):\n\t\t\tp = p/(np.sum(p**2))**0.5\n\t\telse:\n\t\t\tp.fill(1.0)\n\t\tfor i in satoms:\n\t\t\tif (i == atom ):\n\t\t#\t\t\tif (i == atom or mol.atoms[i] == 1):  # we ignore the hyrodgen here\n\t\t\t\tcontinue\n\t\t\tele = mol.atoms[i]\n\t\t\t# The conditional digest moves the coordinates of catom to all points in the grid, and then evaluates\n\t\t\t# the \'put-back probability\' of atom i to it\'s place\n\t\t\t#print xyz.shape\n\t\t\tMB_inputs = self.TData.dig.ConditionDigest(mol, atom, i, xyz)\n\t\t\ttmp_p = self.Instances[ele].evaluate(MB_inputs.reshape((MB_inputs.shape[0], -1)))\n\t\t\tprint(""tmp_p"",tmp_p)\n\t\t\tif (np.sum(tmp_p**2)**0.5 != 0):\n\t\t\t\ttmp_p = tmp_p/(np.sum(tmp_p**2))**0.5  #just normlized it...\n\t\t\telse:\n\t\t\t\ttmp_p.fill(1.0)\n\t\t#\t\t\ttmp_p = np.log10(p)  # take the log..\n\t\t\tp *=  tmp_p  # small p is what we want\n\t\tp = np.absolute(p)\n\t\treturn xyz, p\n\n\tdef EvalAllAtomsMB(self, mol, maxstep = 0.2, ngrid = 50):\n\t\tXYZ=[]\n\t\tP=[]\n\t\tfor i in range (0, mol.atoms.shape[0]):\n\t\t\t\tprint((""Evaluating atom: "", mol.atoms[i]))\n\t\t\t\txyz, p = self.EvalOneAtomMB(mol, i, maxstep, ngrid)\n\t\t\t\tXYZ.append(xyz)\n\t\t\t\tP.append(p)\n\t\tXYZ = np.asarray(XYZ)\n\t\tP = np.asarray(P)\n\t\treturn XYZ, P\n\n\tdef EvalMol(self, mol):\n\t\tP=1.0\n\t\tfor i in range (0, mol.atoms.shape[0]):\n\t\t\tinputs = (self.TData.dig.Emb(i, mol.atoms, mol.coords,  ((mol.coords)[i]).reshape((1,3))))[0]\n\t\t\tinputs = np.array(inputs)\n\t\t\tinputs = inputs.reshape((1,-1))\n\t\t\tp = float(self.Instances[mol.atoms[i]].evaluate(inputs))\n\t\t\tprint((""p:"", p))\n\t\t\tP  *= p\n\t\treturn P\n\n\tdef EvalMol_v2(self, mol):\n\t\tlogP=0.0\n\t\tfor i in range (1, mol.NAtoms()):\n\t\t\ttmp_mol = Mol(mol.atoms[0:i+1], mol.coords[0:i+1])\n\t\t\tinputs = (self.TData.dig.Emb(i, tmp_mol.atoms, tmp_mol.coords,  ((tmp_mol.coords)[i]).reshape((1,3))))[0]\n\t\t\tinputs = np.array(inputs)\n\t\t\tinputs = inputs.reshape((1,-1))\n\t\t\tp = float(self.Instances[tmp_mol.atoms[i]].evaluate(inputs))\n\t\t\tprint((""i:"",i, ""p:"", p, ""logp:"", math.log10(p)))\n\t\t\tlogP += math.log10(p)\n\t\tprint((""logP:"", logP))\n\t\treturn logP\n\n\tdef append_instances(self, manager):\n\t\t""""""\n\t\tAppends the instances from another manager into this manager if their isn\'t already\n\t\ta corresponding instance for the same element. Useful for distributing training of\n\t\telements across different GPUs before combining them into the same manager.\n\n\t\tArgs:\n\t\t\tmanager (TensorMol.TFManage): the manager object for which you would like to append\n\t\t\t\t\tits instances to the current manager\n\t\t""""""\n\t\tfor i, atom in enumerate(manager.TrainedAtoms):\n\t\t\tif atom not in self.TrainedAtoms:\n\t\t\t\tself.TrainedAtoms.append(atom)\n\t\t\t\tself.TrainedNetworks.append(manager2.TrainedNetworks[i])\n\t\tself.Save()\n\n\tdef evaluate_mol_forces_direct(self, mol):\n\t\t""""""\n\t\tEvaluates the forces on a molecule from a network with direct embedding\n\n\t\tArgs:\n\t\t\tmol (TensorMol.Mol): a TensorMol Mol object with n atoms and nx3 coordinates\n\n\t\tReturns:\n\t\t\tforces (np.float): an nx3 numpy array of atomic forces\n\t\t""""""\n\t\tatom_types = mol.AtomTypes()\n\t\tfor element in atom_types:\n\t\t\tif self.Instances[element] == None:\n\t\t\t\traise Exception(""Molecule contains an element which manager has no network for"")\n\t\tforces = np.zeros((mol.NAtoms(), 3))\n\t\tfor element in atom_types:\n\t\t\telement_forces, element_indices = self.Instances[element].evaluate(mol.coords, mol.atoms)\n\t\t\tforces[element_indices[:,1]] = element_forces\n\t\treturn forces\n'"
TensorMol/TFNetworks/TFMolInstance.py,259,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .TFInstance import *\nfrom ..Containers.TensorMolData import *\nimport numpy as np\nimport math\nimport time\nimport os.path\nif (HAS_TF):\n\timport tensorflow as tf\nimport os\nimport sys\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\n\n#\n# These work Moleculewise the versions without the mol prefix work atomwise.\n# but otherwise the behavior of these is the same as TFInstance etc.\n#\n\nclass MolInstance(Instance):\n\tdef __init__(self, TData_,  Name_=None, Trainable_=True):\n\t\tInstance.__init__(self, TData_, 0, Name_)\n\t\tself.AssignActivation()\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+str(self.TData.order)+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.Trainable = Trainable_\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\tself.TData.PrintStatus()\n\t\tself.inshape = self.TData.dig.eshape  # use the flatted version\n\t\tself.outshape = self.TData.dig.lshape    # use the flatted version\n\t\tLOGGER.info(""MolInstance.inshape %s MolInstance.outshape %s"", str(self.inshape) , str(self.outshape))\n\t\treturn\n\n\tdef inference(self, inputs):\n\t\t""""""Builds the network architecture. Number of hidden layers and nodes in each layer defined in TMParams ""HiddenLayers"".\n\t\tArgs:\n\t\t\tinputs: input placeholder for training data from Digester.\n\t\tReturns:\n\t\t\toutput: scalar or vector of OType from Digester.\n\t\t""""""\n\t\thiddens = []\n\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\tif i == 0:\n\t\t\t\twith tf.name_scope(\'hidden1\'):\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev= 1.0 / math.sqrt(float(self.inshape[0])), var_wd= 0.00)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\thiddens.append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\ttf.scalar_summary(\'min/\' + weights.name, tf.reduce_min(weights))\n\t\t\t\t\ttf.histogram_summary(weights.name, weights)\n\t\t\telse:\n\t\t\t\twith tf.name_scope(\'hidden\'+str(i+1)):\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev= 1.0 / math.sqrt(float(self.HiddenLayers[i-1])), var_wd= 0.00)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec),name=\'biases\')\n\t\t\t\t\thiddens.append(self.activation_function(tf.matmul(hiddens[-1], weights) + biases))\n\t\twith tf.name_scope(\'regression_linear\'):\n\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], self.outshape], var_stddev= 1.0 / math.sqrt(float(self.HiddenLayers[-1])), var_wd= 0.00)\n\t\t\tbiases = tf.Variable(tf.zeros(self.outshape, dtype=self.tf_prec), name=\'biases\')\n\t\t\toutput = tf.matmul(hiddens[-1], weights) + biases\n\t\treturn output\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(continue_training)\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_test_loss = float(\'inf\') # some big numbers\n\t\tfor step in  range (0, mxsteps):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef train_step(self,step):\n\t\t"""""" I don\'t think the base class should be train-able. Remove? JAP """"""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttotal_correct = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTrainBatch( self.batch_size) #advances the case pointer in TData...\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\t_, total_loss_value, loss_value, prob_value, correct_num  = self.sess.run([self.train_op, self.total_loss, self.loss, self.prob, self.correct], feed_dict=feed_dict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttotal_correct = total_correct + correct_num\n\t\tduration = time.time() - start_time\n\t\tself.print_training(step, train_loss, total_correct, Ncase_train, duration)\n\t\treturn\n\n\tdef save_chk(self, step):  # We need to merge this with the one in TFInstance\n\t\tself.chk_file = os.path.join(self.train_dir,self.name+\'-chk-\'+str(step))\n\t\tLOGGER.info(""Saving Checkpoint file, in the TFMoInstance %s"", self.chk_file)\n\t\tself.saver.save(self.sess,  self.chk_file)\n\t\treturn\n\n\tdef Load(self):\n\t\tprint (""Unpickling TFInstance..."")\n\t\tfrom ..Containers.PickleTM import UnPickleTM as UnPickleTM\n\t\ttmp = UnPickleTM(self.path+self.name+"".tfn"")\n\t\tself.Clean()\n\t\tself.__dict__.update(tmp)\n\t\t# Simple hack to fix checkpoint path.\n\t\tself.chk_file=self.chk_file.replace(""./networks/"",PARAMS[""networks_directory""])\n\t\tprint(""self.chk_file:"", self.chk_file)\n\t\treturn\n\n\tdef SaveAndClose(self):\n\t\tif (self.TData!=None):\n\t\t\tself.TData.CleanScratch()\n\t\tprint(""Saving TFInstance..."")\n\t\tself.Clean()\n\t\t#print(""Going to pickle...\\n"",[(attr,type(ins)) for attr,ins in self.__dict__.items()])\n\t\tf=open(self.path+self.name+"".tfn"",""wb"")\n\t\tpickle.dump(self.__dict__, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\nclass MolInstance_fc_classify(MolInstance):\n\tdef __init__(self, TData_,  Name_=None, Trainable_=True):\n\t\t""""""\n\t\tTranslation of the outputs to meaningful numbers is handled by the digester and Tensordata\n\t\t""""""\n\t\tself.NetType = ""fc_classify""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+str(self.TData.order)+""_""+self.NetType\n\t\tLOGGER.debug(""Instance.__init__: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.prob = None\n\t\tself.correct = None\n\n\tdef n_correct(self, output, labels):\n\t\t""""""\n\t\tThis should average over the classifier output.\n\t\t""""""\n\t\t# For a classifier model, we can use the in_top_k Op.\n\t\t# It returns a bool tensor with shape [batch_size] that is true for\n\t\t# the examples where the label is in the top k (here k=1)\n\t\t# of all logits for that example.\n\t\tlabels = tf.to_int64(labels)\n\t\tcorrect = tf.nn.in_top_k(output, labels, 1)\n\t\t# Return the number of true entries.\n\t\treturn tf.reduce_sum(tf.cast(correct, tf.int32))\n\n\tdef evaluate(self, eval_input):\n\t\t# Check sanity of input\n\t\tMolInstance.evaluate(self, eval_input)\n\t\teval_input_ = eval_input\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\teval_input_ =np.copy(eval_input)\n\t\t\teval_input_.resize((self.PreparedFor,eval_input.shape[1]))\n\t\t\t# pad with zeros\n\t\teval_labels = np.zeros(self.PreparedFor)  # dummy labels\n\t\tbatch_data = [eval_input_, eval_labels]\n\t\t#images_placeholder, labels_placeholder = self.placeholder_inputs(Ncase) Made by Prepare()\n\t\tfeed_dict = self.fill_feed_dict(batch_data,self.embeds_placeholder,self.labels_placeholder)\n\t\ttmp = (np.array(self.sess.run([self.prob], feed_dict=feed_dict))[0,:eval_input.shape[0],1])\n\t\tif (not np.all(np.isfinite(tmp))):\n\t\t\tprint(""TFsession returned garbage"")\n\t\t\tprint(""TFInputs"",eval_input) #If it\'s still a problem here use tf.Print version of the graph.\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\treturn tmp[:eval_input.shape[0]]\n\t\treturn tmp\n\n\tdef Prepare(self, eval_input, Ncase=125000):\n\t\tself.Clean()\n\t\tprint(""Preparing a "",self.NetType,""MolInstance"")\n\t\tself.prob = None\n\t\tself.correct = None\n\t\t# Always prepare for at least 125,000 cases which is a 50x50x50 grid.\n\t\teval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:0\'):\n\t\t\tself.embeds_placeholder, self.labels_placeholder = self.placeholder_inputs(Ncase)\n\t\t\tself.output = self.inference(self.embeds_placeholder, self.hidden1, self.hidden2, self.hidden3)\n\t\t\tself.correct = self.n_correct(self.output, self.labels_placeholder)\n\t\t\tself.prob = self.justpreds(self.output)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\tself.PreparedFor = Ncase\n\t\treturn\n\n\tdef SaveAndClose(self):\n\t\tself.prob = None\n\t\tself.correct = None\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\t\tMolInstance.SaveAndClose(self)\n\t\treturn\n\n\tdef placeholder_inputs(self, batch_size):\n\t\t""""""\n\t\tGenerate placeholder variables to represent the input tensors.\n\t\tThese placeholders are used as inputs by the rest of the model building\n\t\tcode and will be fed from the downloaded data in the .run() loop.\n\n\t\tArgs:\n\t\t\tbatch_size: The batch size will be baked into both placeholders.\n\t\tReturns:\n\t\t\timages_placeholder: Images placeholder.\n\t\t\tlabels_placeholder: Labels placeholder.\n\t\t""""""\n\t\t# Note that the shapes of the placeholders match the shapes of the full\n\t\t# image and label tensors, except the first dimension is now batch_size\n\t\t# rather than the full size of the train or test data sets.\n\t\tinputs_pl = tf.placeholder(self.tf_prec, shape=(batch_size,self.inshape)) # JAP : Careful about the shapes... should be flat for now.\n\t\toutputs_pl = tf.placeholder(self.tf_prec, shape=(batch_size))\n\t\treturn inputs_pl, outputs_pl\n\n\tdef justpreds(self, output):\n\t\t""""""\n\t\tCalculates the loss from the logits and the labels.\n\n\t\tArgs:\n\t\t\tlogits: Logits tensor, float - [batch_size, NUM_CLASSES].\n\t\t\tlabels: Labels tensor, int32 - [batch_size].\n\n\t\tReturns:\n\t\t\tloss: Loss tensor of type float.\n\t\t""""""\n\t\tprob = tf.nn.softmax(output)\n\t\treturn prob\n\n\tdef loss_op(self, output, labels):\n\t\t""""""\n\t\tCalculates the loss from the logits and the labels.\n\n\t\tArgs:\n\t\t\tlogits: Logits tensor, float - [batch_size, NUM_CLASSES].\n\t\t\tlabels: Labels tensor, int32 - [batch_size].\n\n\t\tReturns:\n\t\t\tloss: Loss tensor of type float.\n\t\t""""""\n\t\tprob = tf.nn.softmax(output)\n\t\tlabels = tf.to_int64(labels)\n\t\tcross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(output, labels, name=\'xentropy\')\n\t\tcross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n\t\ttf.add_to_collection(\'losses\', cross_entropy_mean)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), cross_entropy_mean, prob\n\n\tdef print_training(self, step, loss, total_correct, Ncase, duration):\n\t\tdenom=max(int(Ncase/self.batch_size),1)\n\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  train loss: "", ""%.10f""%(float(loss)/denom),""accu:  %.5f""%(float(total_correct)/(denom*self.batch_size)))\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""Train for a number of steps.""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.embeds_placeholder, self.labels_placeholder = self.placeholder_inputs(self.batch_size)\n\t\t\tself.output = self.inference(self.embeds_placeholder, self.hidden1, self.hidden2, self.hidden3)\n\t\t\tself.total_loss, self.loss, self.prob = self.loss_op(self.output, self.labels_placeholder)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.merge_all_summaries()\n\t\t\tinit = tf.initialize_all_variables()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\ttry: # I think this may be broken\n\t\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n\t\t\t\tif (len(metafiles)>0):\n\t\t\t\t\tmost_recent_meta_file=metafiles[0]\n\t\t\t\t\tLOGGER.info(""Restoring training from Metafile: ""+most_recent_meta_file)\n\t\t\t\t\t#Set config to allow soft device placement for temporary fix to known issue with Tensorflow up to version 0.12 atleast - JEH\n\t\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n\t\t\t\t\tself.sess = tf.Session(config=config)\n\t\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n\t\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""Restore Failed"",Ex)\n\t\t\t\tpass\n\t\t\tself.summary_writer = tf.train.SummaryWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef test(self, step):\n\t\tNcase_test = self.TData.NTest\n\t\ttest_loss =  0.0\n\t\ttest_correct = 0.\n\t\ttest_start_time = time.time()\n\t\ttest_loss = None\n\t\tfeed_dict = None\n\t\tfor  ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(  self.batch_size, ministep)\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\tloss_value, prob_value, test_correct_num = self.sess.run([ self.loss, self.prob, self.correct],  feed_dict=feed_dict)\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_correct = test_correct + test_correct_num\n\t\t\tduration = time.time() - test_start_time\n\t\t\tprint(""testing..."")\n\t\t\tself.print_training(step, test_loss, test_correct, Ncase_test, duration)\n\t\treturn test_loss\n\nclass MolInstance_fc_sqdiff(MolInstance):\n\tdef __init__(self, TData_,  Name_=None, Trainable_=True):\n\t\tself.NetType = ""fc_sqdiff""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+str(self.TData.order)+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.hidden1 = PARAMS[""hidden1""]\n\t\tself.hidden2 = PARAMS[""hidden2""]\n\t\tself.hidden3 = PARAMS[""hidden3""]\n\t\tself.inshape = np.prod(self.TData.dig.eshape)\n\t\tself.outshape = np.prod(self.TData.dig.lshape)\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\n\tdef evaluate(self, eval_input):\n\t\t# Check sanity of input\n\t\tMolInstance.evaluate(self, eval_input)\n\t\teval_input_ = eval_input\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\teval_input_ =np.copy(eval_input)\n\t\t\teval_input_.resize((self.PreparedFor,eval_input.shape[1]))\n\t\t\t# pad with zeros\n\t\teval_labels = np.zeros((self.PreparedFor,1))  # dummy labels\n\t\tbatch_data = [eval_input_, eval_labels]\n\t\t#images_placeholder, labels_placeholder = self.placeholder_inputs(Ncase) Made by Prepare()\n\t\tfeed_dict = self.fill_feed_dict(batch_data,self.embeds_placeholder,self.labels_placeholder)\n\t\ttmp, gradient =  (self.sess.run([self.output, self.gradient], feed_dict=feed_dict))\n\t\tif (not np.all(np.isfinite(tmp))):\n\t\t\tprint(""TFsession returned garbage"")\n\t\t\tprint(""TFInputs"",eval_input) #If it\'s still a problem here use tf.Print version of the graph.\n\t\tif (self.PreparedFor>eval_input.shape[0]):\n\t\t\treturn tmp[:eval_input.shape[0]], gradient[:eval_input.shape[0]]\n\t\treturn tmp, gradient\n\n\tdef Prepare(self, eval_input, Ncase=125000):\n\t\tself.Clean()\n\t\t# Always prepare for at least 125,000 cases which is a 50x50x50 grid.\n\t\teval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default():\n\t\t\t\tself.embeds_placeholder, self.labels_placeholder = self.placeholder_inputs(Ncase)\n\t\t\t\tself.output = self.inference(self.embeds_placeholder, self.hidden1, self.hidden2, self.hidden3)\n\t\t\t\tprint (""type of self.embeds_placeholder:"", type(self.embeds_placeholder))\n\t\t\t\tself.gradient = tf.gradients(self.output, self.embeds_placeholder)[0]\n\t\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\tself.PreparedFor = Ncase\n\t\treturn\n\n\tdef SaveAndClose(self):\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\t\tself.check=None\n\t\tself.label_pl = None\n\t\tself.mats_pl = None\n\t\tself.inp_pl = None\n\t\tMolInstance.SaveAndClose(self)\n\t\treturn\n\n\tdef placeholder_inputs(self, batch_size):\n\t\t""""""\n\t\tGenerate placeholder variables to represent the input tensors.\n\n\t\tArgs:\n\t\t\tbatch_size: The batch size will be baked into both placeholders.\n\t\tReturns:\n\t\t\tinputs_pl: Input placeholder.\n\t\t\toutputs_pl: Outputs placeholder.\n\t\t""""""\n\t\t# Note that the shapes of the placeholders match the shapes of the full\n\t\t# image and label tensors, except the first dimension is now batch_size\n\t\t# rather than the full size of the train or test data sets.\n\t\tinputs_pl = tf.placeholder(self.tf_prec, shape=(batch_size,self.inshape)) # JAP : Careful about the shapes... should be flat for now.\n\t\toutputs_pl = tf.placeholder(self.tf_prec, shape=(batch_size, self.outshape))\n\t\treturn inputs_pl, outputs_pl\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef test(self, step):\n\t\tNcase_test = self.TData.NTest\n\t\ttest_loss =  0.0\n\t\ttest_correct = 0.\n\t\ttest_start_time = time.time()\n\t\tfor  ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch( self.batch_size, ministep)\n\t\t\tbatch_data=self.PrepareData(batch_data)\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\ttotal_loss_value, loss_value, output_value  = self.sess.run([self.total_loss,  self.loss, self.output],  feed_dict=feed_dict)\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\tduration = time.time() - test_start_time\n\t\tprint(""testing..."")\n\t\tself.print_training(step, test_loss,  Ncase_test, duration)\n\t\treturn test_loss\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""Train for a number of steps.""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.embeds_placeholder, self.labels_placeholder = self.placeholder_inputs(self.batch_size)\n\t\t\tself.output = self.inference(self.embeds_placeholder, self.hidden1, self.hidden2, self.hidden3)\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.labels_placeholder)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.merge_all_summaries()\n\t\t\tinit = tf.initialize_all_variables()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\ttry: # I think this may be broken\n\t\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n\t\t\t\tif (len(metafiles)>0):\n\t\t\t\t\tmost_recent_meta_file=metafiles[0]\n\t\t\t\t\tLOGGER.info(""Restoring training from Metafile: ""+most_recent_meta_file)\n\t\t\t\t\t#Set config to allow soft device placement for temporary fix to known issue with Tensorflow up to version 0.12 atleast - JEH\n\t\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n\t\t\t\t\tself.sess = tf.Session(config=config)\n\t\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n\t\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""Restore Failed"",Ex)\n\t\t\t\tpass\n\t\t\tself.summary_writer = tf.train.SummaryWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\treturn\n\n\tdef PrepareData(self, batch_data):\n\t\tif (batch_data[0].shape[0]==self.batch_size):\n\t\t\tbatch_data=[batch_data[0], batch_data[1].reshape((batch_data[1].shape[0],self.outshape))]\n\t\telif (batch_data[0].shape[0] < self.batch_size):\n\t\t\tbatch_data=[batch_data[0], batch_data[1].reshape((batch_data[1].shape[0],self.outshape))]\n\t\t\ttmp_input = np.copy(batch_data[0])\n\t\t\ttmp_output = np.copy(batch_data[1])\n\t\t\ttmp_input.resize((self.batch_size,  batch_data[0].shape[1]))\n\t\t\ttmp_output.resize((self.batch_size,  batch_data[1].shape[1]))\n\t\t\tbatch_data=[ tmp_input, tmp_output]\n\t\treturn batch_data\n\n\tdef train_step(self,step):\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTrainBatch( self.batch_size) #advances the case pointer in TData...\n\t\t\tbatch_data=self.PrepareData(batch_data)\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data, self.embeds_placeholder, self.labels_placeholder)\n\t\t\t_, total_loss_value, loss_value  = self.sess.run([self.train_op, self.total_loss, self.loss], feed_dict=feed_dict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\tself.print_training(step, train_loss, Ncase_train, duration)\n\t\treturn\n\nclass MolInstance_fc_sqdiff_BP(MolInstance_fc_sqdiff):\n\t""""""\n\t\tAn instance of A fully connected Behler-Parinello network.\n\t\tWhich requires a TensorMolData to train/execute.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True):\n\t\t""""""\n\t\tRaise a Behler-Parinello TensorFlow instance.\n\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""fc_sqdiff_BP""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+str(self.TData.order)+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\t# Using multidimensional inputs creates all sorts of issues; for the time being only support flat inputs.\n\t\tself.inshape = np.prod(self.TData.dig.eshape)\n\t\tLOGGER.info(""MolInstance_fc_sqdiff_BP.inshape: %s"",str(self.inshape))\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tLOGGER.info(""MolInstance_fc_sqdiff_BP.eles: %s"",str(self.eles))\n\t\tLOGGER.info(""MolInstance_fc_sqdiff_BP.inshape.n_eles: %i"",self.n_eles)\n\t\tself.MeanStoich = self.TData.MeanStoich # Average stoichiometry of a molecule.\n\t\tself.MeanNumAtoms = np.sum(self.MeanStoich)\n\t\tself.inp_pl=None\n\t\tself.mats_pl=None\n\t\tself.label_pl=None\n\t\tself.batch_size_output = 0\n\n\tdef Clean(self):\n\t\tInstance.Clean(self)\n\t\tself.inp_pl=None\n\t\tself.check = None\n\t\tself.mats_pl=None\n\t\tself.label_pl=None\n\t\tself.atom_outputs = None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tLOGGER.info(""self.MeanNumAtoms: %i"",self.MeanNumAtoms)\n\t\t# allow for 120% of required output space, since it\'s cheaper than input space to be padded by zeros.\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\t#self.TData.CheckBPBatchsizes(self.batch_size, self.batch_size_output)\n\t\tLOGGER.info(""Assigned batch input size: %i"",self.batch_size)\n\t\tLOGGER.info(""Assigned batch output size: %i"",self.batch_size_output) #Number of molecules.\n\t\twith tf.Graph().as_default():\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.batch_size_output])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output]))\n\t\t\tself.output, self.atom_outputs = self.inference(self.inp_pl, self.mats_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\ttry: # I think this may be broken\n\t\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n\t\t\t\tif (len(metafiles)>0):\n\t\t\t\t\tmost_recent_meta_file=metafiles[0]\n\t\t\t\t\tLOGGER.info(""Restoring training from Metafile: ""+most_recent_meta_file)\n\t\t\t\t\t#Set config to allow soft device placement for temporary fix to known issue with Tensorflow up to version 0.12 atleast - JEH\n\t\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n\t\t\t\t\tself.sess = tf.Session(config=config)\n\t\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n\t\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""Restore Failed"",Ex)\n\t\t\t\tpass\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\t#tf.Print(diff, [diff], message=""This is diff: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(labels, [labels], message=""This is labels: "",first_n=10000000,summarize=100000000)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef inference(self, inp_pl, mats_pl):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp_pl: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tmats_pl: a list of (num_of atom type X batchsize) matrices which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\t\toutput = tf.zeros([self.batch_size_output], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tprint(""Norms:"", nrm1,nrm2,nrm3)\n\t\tLOGGER.info(""Layer initial Norms: %f %f %f"", nrm1,nrm2,nrm3)\n\t\t#print(inp_pl)\n\t\t#tf.Print(inp_pl, [inp_pl], message=""This is input: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(bnds_pl, [bnds_pl], message=""bnds_pl: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(mats_pl, [mats_pl], message=""mats_pl: "",first_n=10000000,summarize=100000000)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp_pl[e]\n\t\t\tmats = mats_pl[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tmats_shape = tf.shape(mats)\n\t\t\t\ttf.Print(tf.to_float(mats_shape), [tf.to_float(mats_shape)], message=""Element ""+str(e)+""mats shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t#tf.Print(branches[-1], [branches[-1]], message=""This is layer 2: "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t#tf.Print(tf.to_float(shp_out), [tf.to_float(shp_out)], message=""This is outshape: "",first_n=10000000,summarize=100000000)\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\ttmp = tf.matmul(rshp,mats)\n\t\t\t\toutput = tf.add(output,tmp)\n\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn output, atom_outputs\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tfor e in range(len(self.eles)):\n\t\t\tif (not np.all(np.isfinite(batch_data[0][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit1"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\t\tif (not np.all(np.isfinite(batch_data[1][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit3"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit4"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip(self.inp_pl+self.mats_pl+[self.label_pl], batch_data[0]+batch_data[1]+[batch_data[2]])}\n\t\treturn feed_dict\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep: "", ministep, "" Ncase_train:"", Ncase_train, "" self.batch_size"", self.batch_size)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size,self.batch_size_output)\n\t\t\tactual_mols  = np.count_nonzero(batch_data[2])\n\t\t\tdump_, dump_2, total_loss_value, loss_value, mol_output = self.sess.run([self.check, self.train_op, self.total_loss, self.loss, self.output], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#print (""atom_outputs:"", atom_outputs, "" mol outputs:"", mol_output)\n\t\t\t#print (""atom_outputs shape:"", atom_outputs[0].shape, "" mol outputs"", mol_output.shape)\n\t\t#print(""train diff:"", (mol_output[0]-batch_data[2])[:actual_mols], np.sum(np.square((mol_output[0]-batch_data[2])[:actual_mols])))\n\t\t#print (""train_loss:"", train_loss, "" Ncase_train:"", Ncase_train, train_loss/num_of_mols)\n\t\t#print (""diff:"", mol_output - batch_data[2], "" shape:"", mol_output.shape)\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\n\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size,self.batch_size_output)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = np.count_nonzero(batch_data[2])\n\t\t\tpreds, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.output, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\n\t\t#print(""preds:"", preds[0][:actual_mols], "" accurate:"", batch_data[2][:actual_mols])\n\t\tduration = time.time() - start_time\n\t\t#print (""preds:"", preds, "" label:"", batch_data[2])\n\t\t#print (""diff:"", preds - batch_data[2])\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\t#self.TData.dig.EvaluateTestOutputs(batch_data[2],preds)\n\t\treturn test_loss\n\n\tdef test_after_training(self, step):   # testing in the training\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\tall_atoms = []\n\t\tbond_length = []\n\t\tfor i in range (0, len(self.eles)):\n\t\t\tall_atoms.append([])\n\t\t\tbond_length.append([])\n\t\tall_mols_nn = []\n\t\tall_mols_acc = []\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size,self.batch_size_output)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = np.count_nonzero(batch_data[2])\n\t\t\tpreds, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.output, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\n\t\t\tprint (""actual_mols:"", actual_mols)\n\t\t\tall_mols_nn += list(preds[np.nonzero(preds)])\n\t\t\tall_mols_acc += list(batch_data[2][np.nonzero(batch_data[2])])\n\t\t\t#print (""length:"", len(atom_outputs))\n\t\t\tfor atom_index in range (0,len(self.eles)):\n\t\t\t\tall_atoms[atom_index] += list(atom_outputs[atom_index][0])\n\t\t\t\tbond_length[atom_index] += list(1.0/batch_data[0][atom_index][:,-1])\n\t\t\t\t#print (""atom_index:"", atom_index, len(atom_outputs[atom_index][0]))\n\t\ttest_result = dict()\n\t\ttest_result[\'atoms\'] = all_atoms\n\t\ttest_result[\'nn\'] = all_mols_nn\n\t\ttest_result[\'acc\'] = all_mols_acc\n\t\ttest_result[\'length\'] = bond_length\n\t\t#f = open(""test_result_energy_cleaned_connectedbond_angle_for_test_writting_all_mol.dat"",""wb"")\n\t\t#pickle.dump(test_result, f)\n\t\t#f.close()\n\t\t#print(""preds:"", preds[0][:actual_mols], "" accurate:"", batch_data[2][:actual_mols])\n\t\tduration = time.time() - start_time\n\t\t#print (""preds:"", preds, "" label:"", batch_data[2])\n\t\t#print (""diff:"", preds - batch_data[2])\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\t#self.TData.dig.EvaluateTestOutputs(batch_data[2],preds)\n\t\treturn test_loss\n\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f"", step, duration, (float(loss)/(Ncase)))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f"", step, duration, (float(loss)/(Ncase)))\n\t\treturn\n\n\tdef continue_training(self, mxsteps):\n\t\tself.EvalPrepare()\n\t\ttest_loss = self.test(-1)\n\t\ttest_freq = 1\n\t\tmini_test_loss = test_loss\n\t\tfor step in  range (0, mxsteps+1):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef evaluate(self, batch_data, IfGrad=True):   #this need to be modified\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size_output = nmol\n\t\tif not self.sess:\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\tif (IfGrad):\n\t\t\tmol_output, total_loss_value, loss_value, atom_outputs, gradient = self.sess.run([self.output,self.total_loss, self.loss, self.atom_outputs, self.gradient],  feed_dict=feed_dict)\n\t\t\t#print (""atom_outputs:"", atom_outputs)\n\t\t\treturn mol_output, atom_outputs, gradient\n\t\telse:\n\t\t\tmol_output, total_loss_value, loss_value, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\treturn mol_output, atom_outputs\n\n\tdef EvalPrepare(self):\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None, self.batch_size_output])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output]))\n\t\t\tself.output, self.atom_outputs = self.inference(self.inp_pl, self.mats_pl)\n\t\t\t#self.gradient = tf.gradients(self.atom_outputs, self.inp_pl)\n\t\t\tself.gradient = tf.gradients(self.output, self.inp_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\treturn\n\n\tdef Prepare(self):\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\tprint(""I am pretty sure this is depreciated and should be removed. "")\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.batch_size_output])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output]))\n\t\t\tself.output, self.atom_outputs = self.inference(self.inp_pl, self.mats_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\treturn\n\nclass MolInstance_fc_sqdiff_BP_WithGrad(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tAn instance of A fully connected Behler-Parinello network.\n\tWhich requires a TensorMolData_BP to train/execute.\n\tThis simultaneously constrains the gradient\n\n\tEnergy Inputs have dimension\n\t[eles][atom case][Descriptor Dimension]\n\tThe inference is done elementwise.\n\n\tGradient inputs have dimension:\n\t[eles][atom case][Descriptor Dimension][Max(n3)]\n\n\tThe desired outputs have dimension:\n\tmax(3n)+1 (energy and all derivatives, where max n3 is determined by training data.)\n\n\tthe molecular gradient is constructed by breaking up dE/dRx\n\tdE/dRx  = \\sum_atoms dE_atom/dRx = \\sum_atoms dE_atom/dRx\n\tthe sum over atoms is done with the index matrices\n\tafter dE_atom/dRx is made elementwise in this way:\n\tdE_atom/dRy = dE_atom/dD_i * dD_i/dRy\n\n\tSo dE_atom/dRy has dimension MaxN3\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True):\n\t\t""""""\n\t\tRaise a Behler-Parinello TensorFlow instance.\n\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_fc_sqdiff_BP.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""fc_sqdiff_BP_WithGrad""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+str(self.TData.order)+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.grad_pl = None\n\t\tself.MaxN3 = None\n\t\tself.GradWeight = PARAMS[""GradWeight""]\n\t\tif (TData_ != None):\n\t\t\tself.MaxN3 = TData_.MaxN3 # This is only a barrier for training.\n\t\t\tself.GradWeight *= 1.0/self.MaxN3 # relative weight of the nuclear gradient loss.\n\t\t\tLOGGER.info(""MolInstance_fc_sqdiff_BP_WithGrad.MaxN3: %i"", self.MaxN3)\n\t\tLOGGER.info(""MolInstance_fc_sqdiff_BP_WithGrad.GradWeight: %f"", self.GradWeight)\n\n\tdef Clean(self):\n\t\tMolInstance_fc_sqdiff_BP.Clean(self)\n\t\tself.grad_pl=None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tself.MaxN3 = self.TData.MaxN3\n\t\tprint(""self.MeanNumAtoms: "",self.MeanNumAtoms)\n\t\t# allow for 120% of required output space, since it\'s cheaper than input space to be padded by zeros.\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\t#self.TData.CheckBPBatchsizes(self.batch_size, self.batch_size_output)\n\t\tLOGGER.debug(""Assigned batch input size: %i"",self.batch_size)\n\t\tLOGGER.debug(""Assigned batch output size: %i"",self.batch_size_output)\n\t\tLOGGER.debug(""Inshape: %i"",self.inshape)\n\t\tLOGGER.debug(""Gradshape: %i %i"",self.inshape,self.MaxN3)\n\t\twith tf.Graph().as_default():\n\t\t\tself.inp_pl=[]\n\t\t\tself.grad_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tfor e in range(self.n_eles):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.grad_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape,self.MaxN3])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.batch_size_output])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output,self.MaxN3+1]))\n\t\t\tself.output, self.atom_outputs, self.grads = self.inference()\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.grads, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t#self.summary_op = tf.summary.merge_all()\n\t\t\t#init = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\ttry: # I think this may be broken\n\t\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n\t\t\t\tif (len(metafiles)>0):\n\t\t\t\t\tmost_recent_meta_file=metafiles[0]\n\t\t\t\t\tLOGGER.info(""Restoring training from Metafile: ""+most_recent_meta_file)\n\t\t\t\t\t#Set config to allow soft device placement for temporary fix to known issue with Tensorflow up to version 0.12 atleast - JEH\n\t\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n\t\t\t\t\tself.sess = tf.Session(config=config)\n\t\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n\t\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""Restore Failed"",Ex)\n\t\t\t\tpass\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef inference(self):\n\t\t""""""\n\t\tA separate inference routine should be made for\n\t\tevaluation purposes because of MaxN3. This one is for training, specifically.\n\t\t""""""\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\t\toutput = tf.zeros([self.batch_size_output], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tgrads = tf.zeros([self.batch_size_output, self.MaxN3], dtype=self.tf_prec)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = self.inp_pl[e]\n\t\t\tmats = self.mats_pl[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t#tf.Print(branches[-1], [branches[-1]], message=""This is layer 2: "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t#tf.Print(tf.to_float(shp_out), [tf.to_float(shp_out)], message=""This is outshape: "",first_n=10000000,summarize=100000000)\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tdrshp = tf.gradients(rshp,inputs)\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\ttmp = tf.matmul(rshp,mats)\n\t\t\t\toutput = tf.add(output,tmp)\n\t\t\t\t# This loop calculates the force on each atom and sums it to the force on the molecule.\n\t\t\t\t#  dE_atom/dRy = dE_atom/dD_i * dD_i/dRy, as a  padded with zeros up to MaxN3\n\t\t\t\t#  dE_atom/dD_i\n\t\t\t\t#drshp = tf.Print(drshp, [tf.to_float(tf.shape(drshp))], message=""Element ""+str(e)+""dE_atom/dD_i shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tdAtomdRy = tf.tensordot(drshp, self.grad_pl[e],axes=[[1],[1]]) # => Atoms X Grad\n\t\t\t\t#dAtomdRy = tf.Print(dAtomdRy, [tf.to_float(tf.shape(dAtomdRy))], message=""Element ""+str(e)+""dAtomdRy shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tdMoldRy = tf.tensordot(dAtomdRy,mats,axes=[[0],[0]]) #  => Grad X Mols\n\t\t\t\t#dMoldRy = tf.Print(dMoldRy, [tf.to_float(tf.shape(tmp))], message=""Element ""+str(e)+""dE_atom/dRy "",first_n=10000000,summarize=100000000)\n\t\t\t\tdtmp = tf.transpose(dMoldRy) # we want to sum over atoms and end up with (mol X cart)\n\t\t\t\tgrads = tf.add(grads,dtmp) # Sum over element types.\n\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn output, atom_outputs, grads\n\n\tdef loss_op(self, output, grads, labels):\n\t\t""""""\n\t\tArgs:\n\t\t\toutput: energies of molecules.\n\t\t\tgrads: gradients of molecules (MaxN3)\n\t\t\tlabels: energy, gradients.\n\t\tReturns:\n\t\t\tl2 loss on the energies + self.GradWeight*l2 loss on gradients.\n\t\t""""""\n\t\tEnlabels = tf.slice(labels,[0,0],[-1,1])\n\t\tGradlabels = tf.slice(labels,[0,1],[-1,-1])\n\t\tEdiff  = tf.subtract(output, Enlabels)\n\t\tGdiff  = tf.subtract(grads, Gradlabels)\n\t\t#tf.Print(diff, [diff], message=""This is diff: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(labels, [labels], message=""This is labels: "",first_n=10000000,summarize=100000000)\n\t\tEloss = tf.nn.l2_loss(Ediff)\n\t\tGloss = tf.scalar_mul(self.GradWeight,tf.nn.l2_loss(Gdiff))\n\t\tloss = tf.add(Eloss,Gloss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef train_step(self, step):\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep: "", ministep, "" Ncase_train:"", Ncase_train, "" self.batch_size"", self.batch_size)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size,self.batch_size_output)\n\t\t\tactual_mols  = np.count_nonzero(batch_data[3])\n\t\t\tfeeddict={i:d for i,d in zip(self.inp_pl+self.grad_pl+self.mats_pl+[self.label_pl],batch_data[0]+batch_data[1]+batch_data[2]+[batch_data[3]])}\n\t\t\tdump_2, total_loss_value, loss_value, mol_output = self.sess.run([self.train_op, self.total_loss, self.loss, self.output], feed_dict=feeddict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size,self.batch_size_output)\n\t\t\tfeeddict={i:d for i,d in zip(self.inp_pl+self.grad_pl+self.mats_pl+[self.label_pl],batch_data[0]+batch_data[1]+batch_data[2]+[batch_data[3]])}\n\t\t\tactual_mols  = np.count_nonzero(batch_data[3])\n\t\t\tpreds, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.output, self.atom_outputs],  feed_dict=feeddict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\t\t#print(""preds:"", preds[0][:actual_mols], "" accurate:"", batch_data[2][:actual_mols])\n\t\tduration = time.time() - start_time\n\t\t#print (""preds:"", preds, "" label:"", batch_data[2])\n\t\t#print (""diff:"", preds - batch_data[2])\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\t#self.TData.dig.EvaluateTestOutputs(batch_data[2],preds)\n\t\treturn test_loss\n\n\tdef EvalPrepare(self):\n\t\traise Exception(""NYI"")\n\n\nclass MolInstance_fc_sqdiff_BP_Update(MolInstance_fc_sqdiff_BP):\n\t""""""\n\t\tAn instance of A updated version of fully connected Behler-Parinello network.\n\t\tWhat Kun means is that this version doesn\'t need an index matrix just an index list.\n\t\tWhich requires a TensorMolData to train/execute.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True):\n\t\t""""""\n\t\tRaise a Behler-Parinello TensorFlow instance.\n\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""fc_sqdiff_BP_Update""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+str(self.TData.order)+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\t# Using multidimensional inputs creates all sorts of issues; for the time being only support flat inputs.\n\t\tself.inshape = np.prod(self.TData.dig.eshape)\n\t\tLOGGER.info(""MolInstance_fc_sqdiff_BP_Update.inshape: %s"",str(self.inshape))\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tLOGGER.info(""MolInstance_fc_sqdiff_BP_Update.eles: %s"",str(self.eles))\n\t\tLOGGER.info(""MolInstance_fc_sqdiff_BP_Update.inshape.n_eles: %i"",self.n_eles)\n\t\tself.MeanStoich = self.TData.MeanStoich # Average stoichiometry of a molecule.\n\t\tself.MeanNumAtoms = np.sum(self.MeanStoich)\n\t\tself.inp_pl=None\n\t\tself.index_pl=None\n\t\tself.label_pl=None\n\t\tself.batch_size_output = 0\n\t\tself.gradient = None\n\n\tdef Clean(self):\n\t\tInstance.Clean(self)\n\t\tself.inp_pl=None\n\t\tself.check = None\n\t\tself.index_pl=None\n\t\tself.label_pl=None\n\t\tself.atom_outputs = None\n\t\tself.gradient = None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tLOGGER.info(""self.MeanNumAtoms: %i"",self.MeanNumAtoms)\n\t\t# allow for 120% of required output space, since it\'s cheaper than input space to be padded by zeros.\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\t#self.TData.CheckBPBatchsizes(self.batch_size, self.batch_size_output)\n\t\tLOGGER.info(""Assigned batch input size: %i"",self.batch_size)\n\t\tLOGGER.info(""Assigned batch output size: %i"",self.batch_size_output) #Number of molecules.\n\t\twith tf.Graph().as_default():\n\t\t\tself.inp_pl=[]\n\t\t\tself.index_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.index_pl.append(tf.placeholder(tf.int64, shape=tuple([None])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output]))\n\t\t\tself.output, self.atom_outputs = self.inference(self.inp_pl, self.index_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\t#tf.Print(diff, [diff], message=""This is diff: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(labels, [labels], message=""This is labels: "",first_n=10000000,summarize=100000000)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef inference(self, inp_pl, index_pl):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp_pl: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex_pl: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\t\toutput = tf.zeros([self.batch_size_output], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tprint(""Norms:"", nrm1,nrm2,nrm3)\n\t\tLOGGER.info(""Layer initial Norms: %f %f %f"", nrm1,nrm2,nrm3)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp_pl[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tindex = index_pl[e]\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tindex_shape = tf.shape(index)\n\t\t\t\ttf.Print(tf.to_float(index_shape), [tf.to_float(index_shape)], message=""Element ""+str(e)+""index shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t#tf.Print(branches[-1], [branches[-1]], message=""This is layer 2: "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t#tf.Print(tf.to_float(shp_out), [tf.to_float(shp_out)], message=""This is outshape: "",first_n=10000000,summarize=100000000)\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\trange_index = tf.range(tf.cast(shp_out[0], tf.int64), dtype=tf.int64)\n\t\t\t\tsparse_index =tf.stack([index, range_index], axis=1)\n\t\t\t\tsp_atomoutputs = tf.SparseTensor(sparse_index, rshpflat, dense_shape=[tf.cast(self.batch_size_output, tf.int64), tf.cast(shp_out[0], tf.int64)])\n\t\t\t\tmol_tmp = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\t\t\t\toutput = tf.add(output, mol_tmp)\n\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn output, atom_outputs\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tfor e in range(len(self.eles)):\n\t\t\tif (not np.all(np.isfinite(batch_data[0][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit1"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit4"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip(self.inp_pl+self.index_pl+[self.label_pl], batch_data[0]+batch_data[1]+[batch_data[2]])}\n\t\treturn feed_dict\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size_output),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep: "", ministep, "" Ncase_train:"", Ncase_train, "" self.batch_size"", self.batch_size)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size,self.batch_size_output)\n\t\t\tactual_mols  = np.count_nonzero(batch_data[2])\n\t\t\t#print (""index:"", batch_data[1][0][:40], batch_data[1][1][:20])\n\t\t\tdump_, dump_2, total_loss_value, loss_value, mol_output, atom_outputs  = self.sess.run([self.check, self.train_op, self.total_loss, self.loss, self.output,  self.atom_outputs], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#np.set_printoptions(threshold=np.nan)\n\t\t\t#print (""self.atom_outputs"", atom_outputs[0][0][:40], ""\\n"", atom_outputs[1][0][:20], atom_outputs[1].shape, ""\\n  mol_outputs:"", mol_output[:20], mol_output.shape)\n\t\t\t#print (""self.gradient:"", gradient)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#print (""atom_outputs:"", atom_outputs, "" mol outputs:"", mol_output)\n\t\t\t#print (""atom_outputs shape:"", atom_outputs[0].shape, "" mol outputs"", mol_output.shape)\n\t\t#print(""train diff:"", (mol_output[0]-batch_data[2])[:actual_mols], np.sum(np.square((mol_output[0]-batch_data[2])[:actual_mols])))\n\t\t#print (""train_loss:"", train_loss, "" Ncase_train:"", Ncase_train, train_loss/num_of_mols)\n\t\t#print (""diff:"", mol_output - batch_data[2], "" shape:"", mol_output.shape)\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size,self.batch_size_output)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = np.count_nonzero(batch_data[2])\n\t\t\tpreds, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.output, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\n\t\t#print(""preds:"", preds[0][:actual_mols], "" accurate:"", batch_data[2][:actual_mols])\n\t\tduration = time.time() - start_time\n\t\t#print (""preds:"", preds, "" label:"", batch_data[2])\n\t\t#print (""diff:"", preds - batch_data[2])\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\t#self.TData.dig.EvaluateTestOutputs(batch_data[2],preds)\n\t\treturn test_loss\n\n\tdef test_after_training(self, step):   # testing in the training\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\tall_atoms = []\n\t\tbond_length = []\n\t\tfor i in range (0, len(self.eles)):\n\t\t\tall_atoms.append([])\n\t\t\tbond_length.append([])\n\t\tall_mols_nn = []\n\t\tall_mols_acc = []\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size,self.batch_size_output)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = np.count_nonzero(batch_data[2])\n\t\t\tpreds, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.output, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\n\t\t\tprint (""actual_mols:"", actual_mols)\n\t\t\tall_mols_nn += list(preds[np.nonzero(preds)])\n\t\t\tall_mols_acc += list(batch_data[2][np.nonzero(batch_data[2])])\n\t\t\t#print (""length:"", len(atom_outputs))\n\t\t\tfor atom_index in range (0,len(self.eles)):\n\t\t\t\tall_atoms[atom_index] += list(atom_outputs[atom_index][0])\n\t\t\t\tbond_length[atom_index] += list(1.0/batch_data[0][atom_index][:,-1])\n\t\t\t\t#print (""atom_index:"", atom_index, len(atom_outputs[atom_index][0]))\n\t\ttest_result = dict()\n\t\ttest_result[\'atoms\'] = all_atoms\n\t\ttest_result[\'nn\'] = all_mols_nn\n\t\ttest_result[\'acc\'] = all_mols_acc\n\t\ttest_result[\'length\'] = bond_length\n\t\t#f = open(""test_result_energy_cleaned_connectedbond_angle_for_test_writting_all_mol.dat"",""wb"")\n\t\t#pickle.dump(test_result, f)\n\t\t#f.close()\n\t\t#print(""preds:"", preds[0][:actual_mols], "" accurate:"", batch_data[2][:actual_mols])\n\t\tduration = time.time() - start_time\n\t\t#print (""preds:"", preds, "" label:"", batch_data[2])\n\t\t#print (""diff:"", preds - batch_data[2])\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\t#self.TData.dig.EvaluateTestOutputs(batch_data[2],preds)\n\t\treturn test_loss\n\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f"", step, duration, (float(loss)/(Ncase)))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f"", step, duration, (float(loss)/(Ncase)))\n\t\treturn\n\n\tdef continue_training(self, mxsteps):\n\t\tself.EvalPrepare()\n\t\ttest_loss = self.test(-1)\n\t\ttest_freq = 1\n\t\tmini_test_loss = test_loss\n\t\tfor step in  range (0, mxsteps+1):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef evaluate(self, batch_data, IfGrad=True):   #this need to be modified\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size_output = nmol\n\t\tif not self.sess:\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\tif (IfGrad):\n\t\t\tmol_output, total_loss_value, loss_value, atom_outputs, gradient = self.sess.run([self.output,self.total_loss, self.loss, self.atom_outputs, self.gradient],  feed_dict=feed_dict)\n\t\t\t#print (""atom_outputs:"", atom_outputs)\n\t\t\treturn mol_output, atom_outputs, gradient\n\t\telse:\n\t\t\tmol_output, total_loss_value, loss_value, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\treturn mol_output, atom_outputs\n\n\tdef EvalPrepare(self):\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.inp_pl=[]\n\t\t\tself.index_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.index_pl.append(tf.placeholder(tf.int64, shape=tuple([None])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output]))\n\t\t\tself.output, self.atom_outputs = self.inference(self.inp_pl, self.index_pl)\n\t\t\t#self.gradient = tf.gradients(self.atom_outputs, self.inp_pl)\n\t\t\tself.gradient = tf.gradients(self.output, self.inp_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\treturn\n\n\tdef Prepare(self):\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\tprint(""I am pretty sure this is depreciated and should be removed. "")\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.inp_pl=[]\n\t\t\tself.index_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.index_pl.append(tf.placeholder(tf.int64, shape=tuple([None])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output]))\n\t\t\tself.output, self.atom_outputs = self.inference(self.inp_pl, self.index_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\treturn\n'"
TensorMol/TFNetworks/TFMolInstanceDirect.py,2236,"b'""""""\n\tThese instances work directly on raw coordinate, atomic number data.\n\tThey either generate their own descriptor or physical model.\n\tThey are also simplified relative to the usual MolInstance.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .TFInstance import *\nfrom ..Containers.TensorMolData import *\nfrom .TFMolInstance import *\nfrom ..ForceModels.ElectrostaticsTF import *\nfrom ..ForceModifiers.Neighbors import *\nfrom ..TFDescriptors.RawSymFunc import *\nfrom tensorflow.python.client import timeline\nimport time\nimport threading\n\nclass MolInstance_DirectForce_tmp(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tAn instance which can evaluate and optimize some model force field.\n\tThe force routines are in ElectrostaticsTF.py\n\tThe force routines can take some parameters described here.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True, ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\t\tTData_: A TensorMolData instance.\n\t\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""LJE""\n\t\tself.TData = TData_\n\t\tself.MaxNAtoms = TData_.MaxNAtoms\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.batch_size_output = 10000\n\t\tself.inp_pl=None\n\t\tself.frce_pl=None\n\t\tself.sess = None\n\t\tself.ForceType = ForceType_\n\t\tself.forces = None\n\t\tself.energies = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\t\tself.LJe = None\n\t\tself.LJr = None\n\t\tself.Deq = None\n\t\tself.dbg1 = None\n\t\tself.dbg2 = None\n\t\tself.batch_data = self.TData.RawBatch(nmol=30000)\n\t\t# Using multidimensional inputs creates all sorts of issues; for the time being only support flat inputs.\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t        continue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.LJe = tf.placeholder(tf.float64, shape=(),name=""Ee_pl"")\n\t\t\tself.LJr = tf.placeholder(tf.float64, shape=(),name=""Re_pl"")\n\t\t\t# self.Ee_pl = tf.constant(0.316, dtype=tf.float32)\n\t\t\t# self.Re_pl = tf.constant(1.0, dtype=tf.float32)\n\t\t\tself.inp_shp = tf.shape(self.batch_data[0])\n\t\t\tself.nmol = self.inp_shp[0]\n\t\t\tself.maxnatom = self.inp_shp[1]\n\t\t\tself.XYZs = tf.to_float(tf.slice(self.batch_data[0],[0,0,1],[-1,-1,-1]))\n\t\t\tself.REns = tf.convert_to_tensor(self.batch_data[1][:,0,0],dtype=tf.float64)\n\t\t\tself.Zs = tf.cast(tf.reshape(tf.slice(self.batch_data[0],[0,0,0],[-1,-1,1]),[self.nmol,self.maxnatom,1]),tf.int64)\n\t\t\tself.Ens = LJEnergies(self.XYZs, self.Zs, self.LJe, self.LJr)\n\t\t\tself.mae = tf.reduce_mean(tf.abs(tf.subtract(self.Ens, self.REns)))\n\t\t\t# params = (XYZs, Zs, REns)\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.sess.run(init)\n\tdef LJER(self,inp_pl,E_pl,R_pl):\n\t\t""""""\n\t\twith the current LJe, and LJr.\n\n\t\tArgs:\n\t\t\tinp_pl: placeholder for the NMol X MaxNatom X 4 tensor of Z,x,y,z\n\t\t""""""\n\t\t# separate out the Z from the XYZ.\n\t\tinp_shp = tf.shape(inp_pl)\n\t\tnmol = inp_shp[0]\n\t\tmaxnatom = inp_shp[1]\n\t\tXYZs = tf.slice(inp_pl,[0,0,1],[-1,-1,-1])\n\t\tZs = tf.cast(tf.reshape(tf.slice(inp_pl,[0,0,0],[-1,-1,1]),[nmol,maxnatom,1]),tf.int64)\n\t\t#self.LJe = tf.Print(self.LJe,[self.LJe],""LJe"",1000,1000)\n\t\t#self.LJr = tf.Print(self.LJr,[self.LJr],""LJr"",1000,1000)\n\t\tLJe2 = E_pl*E_pl\n\t\tLJr2 = R_pl*R_pl\n\t\t#LJe2 = tf.Print(LJe2,[LJe2],""LJe2"",1000,1000)\n\t\t#LJr2 = tf.Print(LJr2,[LJr2],""LJr2"",1000,1000)\n\t\tEns = LJEnergies(XYZs, Zs, LJe2, LJr2)\n\t\t#Ens = tf.Print(Ens,[Ens],""Energies"",5000,5000)\n\t\treturn Ens\n\tdef LJFrc(self, params):\n\t\t""""""\n\t\tCompute forces for a batch of molecules\n\t\twith the current LJe, and LJr.\n\n\t\tArgs:\n\t\t\tinp_pl: placeholder for the NMol X MaxNatom X 4 tensor of Z,x,y,z\n\t\t""""""\n\t\tfeeddict = {self.LJe:params[0], self.LJr:params[1]}\n\t\tresult = self.sess.run(self.mae,feed_dict=feeddict)\n\t\treturn result\n\nclass MolInstance_DirectBP(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tAn Instance which does a direct Behler Parinello\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True, ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP""\n\t\tself.TData = TData_\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.MaxNAtoms = TData_.MaxNAtoms\n\t\tself.batch_size_output = 4096\n\t\tself.inp_pl=None\n\t\tself.frce_pl=None\n\t\tself.sess = None\n\t\tself.ForceType = ForceType_\n\t\tself.forces = None\n\t\tself.energies = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\t#with tf.Graph().as_default():\n\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\t""""""\n\t\tThe loss operation of this model is complicated\n\t\tBecause you have to construct the electrostatic energy moleculewise,\n\t\tand the mulitpoles.\n\n\t\tEmats and Qmats are constructed to accerate this process...\n\t\t""""""\n\t\toutput = tf.Print(output,[output],""Comp\'d"",1000,1000)\n\t\tlabels = tf.Print(labels,[labels],""Desired"",1000,1000)\n\t\tdiff  = tf.subtract(output, labels)\n\t\t#tf.Print(diff, [diff], message=""This is diff: "",first_n=10000000,summarize=100000000)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef LJFrc(self, inp_pl):\n\t\t""""""\n\t\tCompute forces for a batch of molecules\n\t\twith the current LJe, and LJr.\n\n\t\tArgs:\n\t\t\tinp_pl: placeholder for the NMol X MaxNatom X 4 tensor of Z,x,y,z\n\t\t""""""\n\t\t# separate out the Z from the XYZ.\n\t\tinp_shp = tf.shape(inp_pl)\n\t\tnmol = inp_shp[0]\n\t\tmaxnatom = inp_shp[1]\n\t\tXYZs = tf.slice(inp_pl,[0,0,1],[-1,-1,-1])\n\t\tZs = tf.cast(tf.reshape(tf.slice(inp_pl,[0,0,0],[-1,-1,1]),[nmol,maxnatom,1]),tf.int64)\n\t\t#self.LJe = tf.Print(self.LJe,[self.LJe],""LJe"",1000,1000)\n\t\t#self.LJr = tf.Print(self.LJr,[self.LJr],""LJr"",1000,1000)\n\t\tLJe2 = self.LJe*self.LJe\n\t\tLJr2 = self.LJr*self.LJr\n\t\t#LJe2 = tf.Print(LJe2,[LJe2],""LJe2"",1000,1000)\n\t\t#LJr2 = tf.Print(LJr2,[LJr2],""LJr2"",1000,1000)\n\t\tEns = LJEnergies(XYZs, Zs, LJe2, LJr2)\n\t\t#Ens = tf.Print(Ens,[Ens],""Energies"",5000,5000)\n\t\tfrcs = -1.0*(tf.gradients(Ens, XYZs)[0])\n\t\treturn Ens, frcs\n\n\tdef HarmFrc(self, inp_pl):\n\t\t""""""\n\t\tCompute Harmonic Forces with equilibrium distance matrix\n\t\tDeqs, and force constant matrix, Keqs\n\n\t\tArgs:\n\t\t\tinp_pl: placeholder for the NMol X MaxNatom X 4 tensor of Z,x,y,z\n\t\t""""""\n\t\t# separate out the Z from the XYZ.\n\t\tinp_shp = tf.shape(inp_pl)\n\t\tnmol = inp_shp[0]\n\t\tmaxnatom = inp_shp[1]\n\t\tXYZs = tf.slice(inp_pl,[0,0,1],[-1,-1,-1])\n\t\tZs = tf.cast(tf.reshape(tf.slice(inp_pl,[0,0,0],[-1,-1,1]),[nmol,maxnatom,1]),tf.int64)\n\t\tZZeroTensor = tf.cast(tf.where(tf.equal(Zs,0),tf.ones_like(Zs),tf.zeros_like(Zs)),self.tf_prec)\n\t\t# Construct a atomic number masks.\n\t\tZshp = tf.shape(Zs)\n\t\tZzij1 = tf.tile(ZZeroTensor,[1,1,Zshp[1]]) # mol X atom X atom.\n\t\tZzij2 = tf.transpose(Zzij1,perm=[0,2,1]) # mol X atom X atom.\n\t\tDeqs = tf.ones((nmol,maxnatom,maxnatom),dtype=self.tf_prec)\n\t\tKeqs = 0.001*tf.ones((nmol,maxnatom,maxnatom),dtype=self.tf_prec)\n\t\tK = HarmKernels(XYZs, Deqs, Keqs)\n\t\tK = tf.where(tf.equal(Zzij1,1.0),tf.zeros_like(K,dtype=self.tf_prec),K)\n\t\tK = tf.where(tf.equal(Zzij2,1.0),tf.zeros_like(K,dtype=self.tf_prec),K)\n\t\tEns = tf.reduce_sum(K,[1,2])\n\t\tfrcs = -1.0*(tf.gradients(Ens, XYZs)[0])\n\t\t#frcs = tf.Print(frcs,[frcs],""Forces"",1000,1000)\n\t\treturn Ens, frcs\n\n\tdef EvalForce(self,m):\n\t\tIns = self.TData.dig.Emb(m,False,False)\n\t\tIns = Ins.reshape(tuple([1]+list(Ins.shape)))\n\t\tfeeddict = {self.inp_pl:Ins}\n\t\tEn,Frc = self.sess.run([self.energies, self.forces],feed_dict=feeddict)\n\t\treturn En, JOULEPERHARTREE*Frc[0] # Returns energies and forces.\n\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  train loss: "", ""%.10f""%(float(loss)/(Ncase)))\n\t\treturn\n\n\tdef Prepare(self):\n\t\tself.TrainPrepare()\n\t\treturn\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = len(self.TData.set.mols)\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size_output)):\n\t\t\t#print (""ministep: "", ministep, "" Ncase_train:"", Ncase_train, "" self.batch_size"", self.batch_size)\n\t\t\tbatch_data = self.TData.RawBatch()\n\t\t\tif (not np.all(np.isfinite(batch_data[0]))):\n\t\t\t\tprint(""Bad Batch...0 "")\n\t\t\tif (not np.all(np.isfinite(batch_data[1]))):\n\t\t\t\tprint(""Bad Batch...1 "")\n\t\t\tfeeddict={i:d for i,d in zip([self.inp_pl,self.frce_pl],[batch_data[0],batch_data[1]])}\n\t\t\tdump_2, total_loss_value, loss_value = self.sess.run([self.train_op, self.total_loss, self.loss], feed_dict=feeddict)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += self.batch_size_output\n\t\t\t#print (""atom_outputs:"", atom_outputs, "" mol outputs:"", mol_output)\n\t\t\t#print (""atom_outputs shape:"", atom_outputs[0].shape, "" mol outputs"", mol_output.shape)\n\t\t#print(""train diff:"", (mol_output[0]-batch_data[2])[:actual_mols], np.sum(np.square((mol_output[0]-batch_data[2])[:actual_mols])))\n\t\t#print (""train_loss:"", train_loss, "" Ncase_train:"", Ncase_train, train_loss/num_of_mols)\n\t\t#print (""diff:"", mol_output - batch_data[2], "" shape:"", mol_output.shape)\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef train(self, mxsteps=10000):\n\t\tself.TrainPrepare()\n\t\tLOGGER.info(""MolInstance_LJForce.train()"")\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_test_loss = float(\'inf\') # some big numbers\n\t\tfor step in  range (0, mxsteps):\n\t\t\tself.train_step(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\nclass MolInstance_DirectBP_NoGrad(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tAn Instance which does a direct Behler Parinello\n\tDo not use gradient in training\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_noGrad""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\t#if (Name_ != None):\n\t\t#\treturn\n\t\tself.SFPa = None\n\t\tself.SFPr = None\n\t\tself.Ra_cut = None\n\t\tself.Rr_cut = None\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.eles_np = np.asarray(self.eles).reshape((self.n_eles,1))\n\t\tself.eles_pairs = []\n\t\tfor i in range (len(self.eles)):\n\t\t\tfor j in range(i, len(self.eles)):\n\t\t\t\tself.eles_pairs.append([self.eles[i], self.eles[j]])\n\t\tself.eles_pairs_np = np.asarray(self.eles_pairs)\n\t\tself.SetANI1Param()\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tself.NetType = ""RawBP_noGrad""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\tself.xyzs_pl = None\n\t\tself.Zs_pl = None\n\t\tself.label_pl = None\n\t\tself.sess = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\n\tdef SetANI1Param(self, prec=np.float64):\n\t\tself.Ra_cut = PARAMS[""AN1_a_Rc""]\n\t\tself.Rr_cut = PARAMS[""AN1_r_Rc""]\n\t\tzetas = np.array([[PARAMS[""AN1_zeta""]]], dtype = prec)\n\t\tetas = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_a_As = PARAMS[""AN1_num_a_As""]\n\t\tAN1_num_a_Rs = PARAMS[""AN1_num_a_Rs""]\n\t\tthetas = np.array([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype = prec)\n\t\trs =  np.array([ self.Ra_cut*i/AN1_num_a_Rs for i in range (0, AN1_num_a_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\t\tp1 = np.tile(np.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp3 = np.tile(np.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_a_Rs,1])\n\t\tp4 = np.tile(np.reshape(rs,[1,1,1,AN1_num_a_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\t\tSFPa = np.concatenate([p1,p2,p3,p4],axis=4)\n\t\tself.SFPa = np.transpose(SFPa, [4,0,1,2,3])\n\t\tetas_R = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_r_Rs = PARAMS[""AN1_num_r_Rs""]\n\t\trs_R =  np.array([ self.Rr_cut*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 2 x  neta X nr\n\t\tp1_R = np.tile(np.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\t\tp2_R = np.tile(np.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\t\tSFPr = np.concatenate([p1_R,p2_R],axis=2)\n\t\tself.SFPr = np.transpose(SFPr, [2,0,1])\n\t\tself.inshape = int(len(self.eles)*AN1_num_r_Rs + len(self.eles_pairs)*AN1_num_a_Rs*AN1_num_a_As)\n\t\tp1 = np.tile(np.reshape(thetas,[AN1_num_a_As,1,1]),[1,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(rs,[1,AN1_num_a_Rs,1]),[AN1_num_a_As,1,1])\n\t\tSFPa2 = np.concatenate([p1,p2],axis=2)\n\t\tself.SFPa2 = np.transpose(SFPa2, [2,0,1])\n\t\tp1_new = np.reshape(rs_R,[AN1_num_r_Rs,1])\n\t\tself.SFPr2 = np.transpose(p1_new, [1,0])\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\n\t\tprint (""self.inshape:"", self.inshape)\n\n\tdef Clean(self):\n\t\tInstance.Clean(self)\n\t\tself.xyzs_pl=None\n\t\tself.check = None\n\t\tself.Zs_pl=None\n\t\tself.label_pl=None\n\t\tself.atom_outputs = None\n\t\tself.Scatter_Sym = None\n\t\tself.Sym_Index = None\n\t\tself.options = None\n\t\tself.run_metadata = None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update2(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2, self.Rr_cut, Elep, self.SFPa2,self.zeta, self.eta, self.Ra_cut)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut, Elep, self.SFPa, self.Ra_cut)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut, Elep, self.SFPa, self.Ra_cut)\n\t\t\t#self.Rr_cut_tf = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Ra_cut_tf = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut_tf, Elep, self.SFPa, self.Ra_cut_tf)\n\t\t\t#tf.verify_tensor_all_finite(self.Scatter_Sym[0], ""Nan in output!!! 0 "")\n\t\t\t#tf.verify_tensor_all_finite(self.Scatter_Sym[1], ""Nan in output!!! 1"")\n\t\t\tself.output, self.atom_outputs = self.inference(self.Scatter_Sym, self.Sym_Index)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\t#self.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t#self.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef inference(self, inp, indexs):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\n\t\toutput = tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tprint(""Norms:"", nrm1,nrm2,nrm3)\n\t\tLOGGER.info(""Layer initial Norms: %f %f %f"", nrm1,nrm2,nrm3)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tindex_shape = tf.shape(index)\n\t\t\t\ttf.Print(tf.to_float(index_shape), [tf.to_float(index_shape)], message=""Element ""+str(e)+""index shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t#tf.Print(branches[-1], [branches[-1]], message=""This is layer 2: "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\trange_index = tf.range(tf.cast(shp_out[0], tf.int64), dtype=tf.int64)\n\t\t\t\tsparse_index =tf.stack([index, range_index], axis=1)\n\t\t\t\tsp_atomoutputs = tf.SparseTensor(sparse_index, rshpflat, dense_shape=[tf.cast(self.batch_size, tf.int64), tf.cast(shp_out[0], tf.int64)])\n\t\t\t\tmol_tmp = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\t\t\t\toutput = tf.add(output, mol_tmp)\n\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn output, atom_outputs\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.label_pl], [batch_data[0]]+[batch_data[1]]+[batch_data[2]])}\n\t\treturn feed_dict\n\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  train loss: "", ""%.10f""%(float(loss)/(Ncase)))\n\t\treturn\n\n\tdef Prepare(self):\n\t\tself.TrainPrepare()\n\t\treturn\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t        step: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, mol_output, atom_outputs, gradient = self.sess.run([ self.train_op, self.total_loss, self.loss, self.output,  self.atom_outputs, self.gradient], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#dump_, dump_2, total_loss_value, loss_value, mol_output, atom_outputs, gradient = self.sess.run([self.check, self.train_op, self.total_loss, self.loss, self.output,  self.atom_outputs, self.gradient], feed_dict=self.fill_feed_dict(batch_data), options=self.options, run_metadata=self.run_metadata)\n\t\t\t#print (""gradient:"", gradient[0][:4])\n\n\t\t\t#print (""gradient:"", np.sum(gradient[0]))\n\t\t\t#print (""gradient:"", np.sum(np.isinf(gradient[0])))\n\t\t\t#print (""gradient:"", np.where(np.isinf(gradient[0]) == True))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tpreds, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.output, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\t\tduration = time.time() - start_time\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\treturn test_loss\n\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f"", step, duration, (float(loss)/(Ncase)))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f"", step, duration, (float(loss)/(Ncase)))\n\t\treturn\n\n\tdef evaluate(self, batch_data, IfGrad=True):   #this need to be modified\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t#mol_output, total_loss_value, loss_value, atom_outputs, gradient = self.sess.run([self.output,self.total_loss, self.loss, self.atom_outputs, self.gradient],  feed_dict=feed_dict)\n\t\t#for i in range (0, batch_data[0][-1][-1].shape[0]):\n\t\t#        print(""i:"", i)\n\t\t#        import copy\n\t\t#        new_batch_data=copy.deepcopy(batch_data)\n\t\t#        #new_batch_data = list(batch_data)\n\t\t#        new_batch_data[0][-1][-1][i] += 0.01\n\t\t#        feed_dict=self.fill_feed_dict(new_batch_data)\n\t\t#       new_mol_output, total_loss_value, loss_value, new_atom_outputs, new_gradient = self.sess.run([self.output,self.total_loss, self.loss, self.atom_outputs, self.gradient],  feed_dict=feed_dict)\n\t\t#        print (""new_charge_gradient: "", gradient[-1][-1][i],  new_gradient[-1][-1][i], "" numerical: "", (new_atom_outputs[-1][-1][-1]- atom_outputs[-1][-1][-1])/0.01)\n\t\tif (IfGrad):\n\t\t\tmol_output, total_loss_value, loss_value, atom_outputs, gradient = self.sess.run([self.output,self.total_loss, self.loss, self.atom_outputs, self.gradient],  feed_dict=feed_dict)\n\t\t\t#print (""atom_outputs:"", atom_outputs)\n\t\t\treturn mol_output, atom_outputs, gradient\n\t\telse:\n\t\t\tmol_output, total_loss_value, loss_value, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\treturn mol_output, atom_outputs\n\n\tdef EvalPrepare(self):\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update2(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2, self.Rr_cut, Elep, self.SFPa2,self.zeta, self.eta, self.Ra_cut)\n\t\t\t#elf.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut, Elep, self.SFPa, self.Ra_cut)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut, Elep, self.SFPa, self.Ra_cut)\n\t\t\t#self.Rr_cut_tf = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Ra_cut_tf = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut_tf, Elep, self.SFPa, self.Ra_cut_tf)\n\t\t\t#tf.verify_tensor_all_finite(self.Scatter_Sym[0], ""Nan in output!!! 0 "")\n\t\t\t#tf.verify_tensor_all_finite(self.Scatter_Sym[1], ""Nan in output!!! 1"")\n\t\t\tself.output, self.atom_outputs = self.inference(self.Scatter_Sym, self.Sym_Index)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\t#self.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t#self.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\tdef continue_training(self, mxsteps):\n\t\tself.EvalPrepare()\n\t\ttest_loss = self.test(-1)\n\t\ttest_freq = 1\n\t\tmini_test_loss = test_loss\n\t\tfor step in  range (0, mxsteps+1):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\nclass MolInstance_DirectBPBond_NoGrad(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tAn Instance which does a direct Behler Parinello\n\tDo not use gradient in training\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""BPPairPotential""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\t# if (Name_ != None):\n\t\t\t# return\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.eles_np = np.asarray(self.eles).reshape((self.n_eles,1))\n\t\tself.eles_pairs = []\n\t\tfor i in range (len(self.eles)):\n\t\t\tfor j in range(i, len(self.eles)):\n\t\t\t\tself.eles_pairs.append([self.eles[i], self.eles[j]])\n\t\tself.eles_pairs_np = np.asarray(self.eles_pairs)\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tself.NetType = ""RawBPBond_noGrad""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\tself.xyzs_pl = None\n\t\tself.Zs_pl = None\n\t\tself.label_pl = None\n\t\tself.sess = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\t\tself.profiling = PARAMS[""Profiling""]\n\n\tdef Clean(self):\n\t\tInstance.Clean(self)\n\t\tself.Zxyzs_pl=None\n\t\tself.check = None\n\t\tself.BondIdxMatrix_pl=None\n\t\tself.RList=None\n\t\tself.label_pl=None\n\t\tself.atom_outputs = None\n\t\tself.Scatter_Sym = None\n\t\tself.Sym_Index = None\n\t\tself.options = None\n\t\tself.run_metadata = None\n\t\treturn\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.Zxyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,4]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.BondIdxMatrix_pl = tf.placeholder(tf.int32, shape=tuple([None,3]))\n\t\t\tElemPairs = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int32)\n\t\t\tself.RList, MolIdxList = TFBond(self.Zxyzs_pl, self.BondIdxMatrix_pl, ElemPairs)\n\t\t\tself.output, self.atom_outputs = self.inference(self.RList, MolIdxList)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\tif self.profiling:\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef inference(self, inp, indexs):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\n\t\toutput = tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tprint(""Norms:"", nrm1,nrm2,nrm3)\n\t\tLOGGER.info(""Layer initial Norms: %f %f %f"", nrm1,nrm2,nrm3)\n\t\tfor e in range(len(self.eles_pairs)):\n\t\t\tbranches.append([])\n\t\t\tinputs = tf.reshape(inp[e], [tf.shape(inp[e])[0],1])\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tindex_shape = tf.shape(index)\n\t\t\t\ttf.Print(tf.to_float(index_shape), [tf.to_float(index_shape)], message=""Element ""+str(e)+""index shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles_pairs[e][0])+str(self.eles_pairs[e][1])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles_pairs[e][0])+str(self.eles_pairs[e][1])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles_pairs[e][0])+str(self.eles_pairs[e][1])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t#tf.Print(branches[-1], [branches[-1]], message=""This is layer 2: "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles_pairs[e][0])+str(self.eles_pairs[e][1])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\trange_index = tf.range(tf.cast(shp_out[0], tf.int64), dtype=tf.int64)\n\t\t\t\tsparse_index = tf.stack([index[:,0], range_index], axis=1)\n\t\t\t\tsp_atomoutputs = tf.SparseTensor(sparse_index, rshpflat, dense_shape=[tf.cast(self.batch_size, tf.int64), tf.cast(shp_out[0], tf.int64)])\n\t\t\t\tmol_tmp = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\t\t\t\toutput = tf.add(output, mol_tmp)\n\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn output, atom_outputs\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.Zxyzs_pl]+[self.BondIdxMatrix_pl]+[self.label_pl], [batch_data[0]]+[batch_data[1]]+[batch_data[2]])}\n\t\treturn feed_dict\n\n\tdef Prepare(self):\n\t\tself.TrainPrepare()\n\t\treturn\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.RawBatch(nmol=self.batch_size)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tif self.profiling:\n\t\t\t\tdump_2, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.train_op, self.total_loss, self.loss, self.output,  self.atom_outputs], feed_dict=self.fill_feed_dict(batch_data), options=self.options, run_metadata=self.run_metadata)\n\t\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t\twith open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t\t\tf.write(chrome_trace)\n\t\t\telse:\n\t\t\t\tdump_2, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.train_op, self.total_loss, self.loss, self.output,  self.atom_outputs], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tself.PrintTrain(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tprint( ""testing..."")\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data = self.TData.RawBatch(nmol=self.batch_size)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tdump_2, total_loss_value, loss_value, mol_output, atom_outputs, labels = self.sess.run([self.train_op, self.total_loss, self.loss, self.output,  self.atom_outputs, self.label_pl], feed_dict=feed_dict)\n\t\t\t# preds, total_loss_value, loss_value, mol_output, atom_outputs = self.sess.run([self.output,self.total_loss, self.loss, self.output, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\t\tduration = time.time() - start_time\n\t\tself.PrintTest(mol_output, labels, test_loss, num_of_mols, duration)\n\t\treturn test_loss\n\n\tdef PrintTest(self, output, labels, loss, Ncase, duration):\n\t\tfor i in range(50):\n\t\t\tLOGGER.info(""Label: %.5f   Prediction: %.5f"", labels[i], output[i])\n\t\tLOGGER.info(""Duration: %.5f  Test Loss: %.10f"", duration, (float(loss)/(Ncase)))\n\t\treturn\n\n\tdef PrintTrain(self, step, loss, Ncase, duration, Train=True):\n\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f"", step, duration, (float(loss)/(Ncase)))\n\t\treturn\n\n\tdef Evaluate(self):   #this need to be modified\n\t\tif not self.sess:\n\t\t\tprint(""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tself.TData.ReloadSet()\n\t\tself.TData.raw_it = iter(self.TData.set.mols)\n\t\tNcase_train = self.TData.NTrain\n\t\tAtomOutputs = []\n\t\tfor ministep in xrange(0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.TData.RawBatch(nmol=self.batch_size)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tmol_output, atom_outputs, RList = self.sess.run([self.output, self.atom_outputs, self.RList], feed_dict=feed_dict)\n\t\t\tenergy_distance = []\n\t\t\tfor i in range(len(atom_outputs)):\n\t\t\t\tenergy_distance.append(np.stack([atom_outputs[i][0,:], RList[i]], axis=1))\n\t\t\tif ministep == 0:\n\t\t\t\tAtomOutputs = energy_distance\n\t\t\telse:\n\t\t\t\tfor i in range(len(AtomOutputs)):\n\t\t\t\t\tAtomOutputs[i] = np.append(AtomOutputs[i], energy_distance[i],axis=0)\n\t\treturn AtomOutputs\n\n\tdef EvalPrepare(self):\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.Zxyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.TData.set.MaxNAtoms(),4]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.BondIdxMatrix_pl = tf.placeholder(tf.int32, shape=tuple([None,3]))\n\t\t\tElemPairs = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int32)\n\t\t\tself.RList, MolIdxList = TFBond(self.Zxyzs_pl, self.BondIdxMatrix_pl, ElemPairs)\n\t\t\tself.output, self.atom_outputs = self.inference(self.RList, MolIdxList)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\t#self.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t#self.run_metadata = tf.RunMetadata()\n\t\treturn\n\nclass MolPairsTriples(MolInstance):\n\t""""""\n\tAn Instance which does a direct Behler Parinello\n\tDo not use gradient in training\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""BPPairsTriples""\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.elements = np.asarray(self.TData.eles)\n\t\tself.element_pairs = []\n\t\tself.element_triples = []\n\t\tfor i in range (len(self.TData.eles)):\n\t\t\tfor j in range(i, len(self.TData.eles)):\n\t\t\t\tself.element_pairs.append([self.TData.eles[i], self.TData.eles[j]])\n\t\t\t\tfor k in range(j, len(self.TData.eles)):\n\t\t\t\t\tself.element_triples.append([self.TData.eles[i], self.TData.eles[j], self.TData.eles[k]])\n\t\tself.element_pairs = np.asarray(self.element_pairs)\n\t\tself.element_triples = np.asarray(self.element_triples)\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\n\tdef Clean(self):\n\t\tInstance.Clean(self)\n\t\tself.labels_pl = None\n\t\tself.Zs_pl = None\n\t\tself.xyzs_pl = None\n\t\tself.mol_outputs = None\n\t\treturn\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms, 3]))\n\t\t\tself.Zs_pl = tf.placeholder(tf.int32, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.labels_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tpairs_distance_cutoff = tf.constant(5.0, dtype=self.tf_prec)\n\t\t\ttriples_distance_cutoff = tf.constant(5.0, dtype=self.tf_prec)\n\t\t\telement_pairs = tf.Variable(self.element_pairs, trainable=False, dtype = tf.int32)\n\t\t\telement_triples, _ = tf.nn.top_k(tf.Variable(self.element_triples, trainable=False, dtype = tf.int32), k=3)\n\t\t\tself.element_pairs_embedding, element_pairs_indices = tf_pairs_list(self.xyzs_pl, self.Zs_pl, pairs_distance_cutoff, element_pairs)\n\t\t\tself.element_triples_embedding, element_triples_indices = tf_triples_list(self.xyzs_pl, self.Zs_pl, triples_distance_cutoff, element_triples)\n\t\t\tmol_pairs_outputs = self.pairs_inference(self.element_pairs_embedding, element_pairs_indices)\n\t\t\tmol_triples_outputs = self.triples_inference(self.element_triples_embedding, element_triples_indices)\n\t\t\tself.mol_outputs = mol_pairs_outputs + mol_triples_outputs\n\t\t\tself.total_loss, self.loss = self.loss_op(self.mol_outputs, self.labels_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\tif self.profiling:\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\tdef loss_op(self, output, labels):\n\t\tdiff  = tf.subtract(output, labels)\n\t\tloss = tf.nn.l2_loss(diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef pairs_inference(self, inputs, indices):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\tbranches = []\n\t\tpair_outputs = []\n\t\tmol_pairs = [[] for mol in xrange(self.batch_size)]\n\t\tfor i in range(len(inputs)):\n\t\t\tbranches.append([])\n\t\t\tembedding = inputs[i]\n\t\t\tmol_indices = indices[i]\n\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\tif i == 0:\n\t\t\t\t\twith tf.name_scope(str(self.element_pairs[i][0])+str(self.element_pairs[i][1])+\'_hidden1\'):\n\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[1, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(1.0)), var_wd=0.001)\n\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(embedding, weights) + biases))\n\t\t\t\telse:\n\t\t\t\t\twith tf.name_scope(str(self.element_pairs[i][0])+str(self.element_pairs[i][1])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.element_pairs[i][0])+str(self.element_pairs[i][1])+\'_regression_linear\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tpair_outputs.append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tmol_pair_list = tf.dynamic_partition(pair_outputs[-1], mol_indices, self.batch_size)\n\t\t\t\tfor mol in xrange(self.batch_size):\n\t\t\t\t\tmol_pairs[mol].append(tf.reduce_sum(mol_pair_list[mol]))\n\t\tmol_pairs_sum = tf.stack([tf.add_n(mol_element_pairs) for mol_element_pairs in mol_pairs])\n\t\ttf.verify_tensor_all_finite(mol_pairs_sum,""Nan in output!!!"")\n\t\treturn mol_pairs_sum\n\n\tdef triples_inference(self, inputs, indices):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\tbranches = []\n\t\ttriples_outputs = []\n\t\tmol_triples = [[] for mol in xrange(self.batch_size)]\n\t\tfor i in range(len(inputs)):\n\t\t\tbranches.append([])\n\t\t\tembedding = inputs[i]\n\t\t\tmol_indices = indices[i]\n\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\tif i == 0:\n\t\t\t\t\twith tf.name_scope(str(self.element_triples[i][0])+str(self.element_triples[i][1])+str(self.element_triples[i][2])+\'_hidden1\'):\n\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[6, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(6.0)), var_wd=0.001)\n\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(embedding, weights) + biases))\n\t\t\t\telse:\n\t\t\t\t\twith tf.name_scope(str(self.element_triples[i][0])+str(self.element_triples[i][1])+str(self.element_triples[i][2])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.element_triples[i][0])+str(self.element_triples[i][1])+str(self.element_triples[i][2])+\'_regression_linear\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\ttriples_outputs.append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tmol_triples_list = tf.dynamic_partition(triples_outputs[-1], mol_indices, self.batch_size)\n\t\t\t\tfor mol in xrange(self.batch_size):\n\t\t\t\t\tmol_triples[mol].append(tf.reduce_sum(mol_triples_list[mol]))\n\t\tmol_triples_sum = tf.stack([tf.add_n(mol_element_triples) for mol_element_triples in mol_triples])\n\t\ttf.verify_tensor_all_finite(mol_triples_sum,""Nan in output!!!"")\n\t\treturn mol_triples_sum\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl] + [self.Zs_pl] + [self.labels_pl], [batch_data[0]] + [batch_data[1]] + [batch_data[2]])}\n\t\treturn feed_dict\n\n\tdef Prepare(self):\n\t\tself.TrainPrepare()\n\t\treturn\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tnum_train_cases = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in xrange (0, int(num_train_cases / self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)\n\t\t\tif self.profiling:\n\t\t\t\t_, total_loss_value, loss_value = self.sess.run([self.train_op, self.total_loss, self.loss], feed_dict=self.fill_feed_dict(batch_data), options=self.options, run_metadata=self.run_metadata)\n\t\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t\twith open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t\t\tf.write(chrome_trace)\n\t\t\telse:\n\t\t\t\t_, total_loss_value, loss_value = self.sess.run([self.train_op, self.total_loss, self.loss], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss += total_loss_value\n\t\tduration = time.time() - start_time\n\t\tself.print_training(step, train_loss, num_train_cases, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tprint( ""testing..."")\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tnum_test_cases = self.TData.NTest\n\t\ttest_epoch_labels, test_epoch_outputs = [], []\n\t\tfor ministep in xrange (0, int(num_test_cases / self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)\n\t\t\tfeed_dict = self.fill_feed_dict(batch_data)\n\t\t\t_, total_loss_value, loss_value, mol_outputs, labels = self.sess.run([self.train_op, self.total_loss, self.loss, self.mol_outputs, self.labels_pl], feed_dict=feed_dict)\n\t\t\ttest_loss += total_loss_value\n\t\t\ttest_epoch_labels.append(labels)\n\t\t\ttest_epoch_outputs.append(mol_outputs)\n\t\ttest_epoch_labels = np.concatenate(test_epoch_labels)\n\t\ttest_epoch_outputs = np.concatenate(test_epoch_outputs)\n\t\ttest_epoch_errors = test_epoch_labels - test_epoch_outputs\n\t\tfor i in xrange(20):\n\t\t\tLOGGER.info(""Label: %.5f   Prediction: %.5f"", test_epoch_labels[i], test_epoch_outputs[i])\n\t\tLOGGER.info(""MAE: %f"", np.mean(np.abs(test_epoch_errors)))\n\t\tLOGGER.info(""MSE: %f"", np.mean(test_epoch_errors))\n\t\tLOGGER.info(""RMSE: %f"", np.sqrt(np.mean(np.square(test_epoch_errors))))\n\t\tLOGGER.info(""Std. Dev.: %f"", np.std(test_epoch_errors))\n\t\tduration = time.time() - start_time\n\t\tself.print_testing(mol_outputs, labels, test_loss, num_test_cases, duration)\n\t\treturn test_loss\n\n\tdef print_testing(self, output, labels, loss, num_cases, duration):\n\t\tLOGGER.info(""Duration: %.5f  Test Loss: %.10f"", duration, (float(loss)/(num_cases)))\n\t\treturn\n\n\tdef print_training(self, step, loss, Ncase, duration):\n\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f"", step, duration, (float(loss)/(Ncase)))\n\t\treturn\n\n\tdef Evaluate(self):   #this need to be modified\n\t\tif not self.sess:\n\t\t\tprint(""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tself.TData.ReloadSet()\n\t\tself.TData.raw_it = iter(self.TData.set.mols)\n\t\tNcase_train = self.TData.NTrain\n\t\tAtomOutputs = []\n\t\tfor ministep in xrange(0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.TData.RawBatch(nmol=self.batch_size)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tmol_output, atom_outputs, RList = self.sess.run([self.output, self.atom_outputs, self.RList], feed_dict=feed_dict)\n\t\t\tenergy_distance = []\n\t\t\tfor i in range(len(atom_outputs)):\n\t\t\t\tenergy_distance.append(np.stack([atom_outputs[i][0,:], RList[i]], axis=1))\n\t\t\tif ministep == 0:\n\t\t\t\tAtomOutputs = energy_distance\n\t\t\telse:\n\t\t\t\tfor i in range(len(AtomOutputs)):\n\t\t\t\t\tAtomOutputs[i] = np.append(AtomOutputs[i], energy_distance[i],axis=0)\n\t\treturn AtomOutputs\n\n\tdef EvalPrepare(self):\n\t\twith tf.Graph().as_default():\n\t\t\tself.Zxyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.TData.set.MaxNAtoms(),4]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.BondIdxMatrix_pl = tf.placeholder(tf.int32, shape=tuple([None,3]))\n\t\t\tElemPairs = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int32)\n\t\t\tself.RList, MolIdxList = TFBond(self.Zxyzs_pl, self.BondIdxMatrix_pl, ElemPairs)\n\t\t\tself.output, self.atom_outputs = self.inference(self.RList, MolIdxList)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\treturn\n\nclass MolInstance_DirectBP_Grad(MolInstance_fc_sqdiff_BP):\n\t""""""\n\tAn Instance which does a direct Behler Parinello\n\tDo not use gradient in training\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True, ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_Grad""\n\t\tself.SFPa = None\n\t\tself.SFPr = None\n\t\tself.Ra_cut = None\n\t\tself.Rr_cut = None\n\t\tself.HasANI1PARAMS = False\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\t#if (Name_ != None):\n\t\t#\treturn\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.eles_np = np.asarray(self.eles).reshape((self.n_eles,1))\n\t\tself.eles_pairs = []\n\t\tfor i in range (len(self.eles)):\n\t\t\tfor j in range(i, len(self.eles)):\n\t\t\t\tself.eles_pairs.append([self.eles[i], self.eles[j]])\n\t\tself.eles_pairs_np = np.asarray(self.eles_pairs)\n\t\tif not self.HasANI1PARAMS:\n\t\t\tself.SetANI1Param()\n\t\tself.HiddenLayers = PARAMS[""HiddenLayers""]\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tself.GradScalar = PARAMS[""GradScalar""]\n\t\tself.NetType = ""RawBP_Grad""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tprint (""self.activation_function_type: "", self.activation_function_type)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\tself.xyzs_pl = None\n\t\tself.Zs_pl = None\n\t\tself.label_pl = None\n\t\tself.grads_pl = None\n\t\tself.sess = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\t\tprint (""self.hidden1:"",self.hidden1, "" self.hidden2:"", self.hidden2, "" self.hidden3:"", self.hidden3)\n\n\tdef SetANI1Param(self, prec=np.float64):\n\t\tself.Ra_cut = PARAMS[""AN1_a_Rc""]\n\t\tself.Rr_cut = PARAMS[""AN1_r_Rc""]\n\t\tzetas = np.array([[PARAMS[""AN1_zeta""]]], dtype = prec)\n\t\tetas = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_a_As = PARAMS[""AN1_num_a_As""]\n\t\tAN1_num_a_Rs = PARAMS[""AN1_num_a_Rs""]\n\t\tthetas = np.array([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype = prec)\n\t\trs =  np.array([ self.Ra_cut*i/AN1_num_a_Rs for i in range (0, AN1_num_a_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\t\tp1 = np.tile(np.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp3 = np.tile(np.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_a_Rs,1])\n\t\tp4 = np.tile(np.reshape(rs,[1,1,1,AN1_num_a_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\t\tSFPa = np.concatenate([p1,p2,p3,p4],axis=4)\n\t\tself.SFPa = np.transpose(SFPa, [4,0,1,2,3])\n\t\tetas_R = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_r_Rs = PARAMS[""AN1_num_r_Rs""]\n\t\trs_R =  np.array([ self.Rr_cut*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 2 x  neta X nr\n\t\tp1_R = np.tile(np.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\t\tp2_R = np.tile(np.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\t\tSFPr = np.concatenate([p1_R,p2_R],axis=2)\n\t\tself.SFPr = np.transpose(SFPr, [2,0,1])\n\t\tself.inshape = int(len(self.eles)*AN1_num_r_Rs + len(self.eles_pairs)*AN1_num_a_Rs*AN1_num_a_As)\n\t\t#self.inshape = int(len(self.eles)*AN1_num_r_Rs)\n\t\tp1 = np.tile(np.reshape(thetas,[AN1_num_a_As,1,1]),[1,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(rs,[1,AN1_num_a_Rs,1]),[AN1_num_a_As,1,1])\n\t\tSFPa2 = np.concatenate([p1,p2],axis=2)\n\t\tself.SFPa2 = np.transpose(SFPa2, [2,0,1])\n\t\tp1_new = np.reshape(rs_R,[AN1_num_r_Rs,1])\n\t\tself.SFPr2 = np.transpose(p1_new, [1,0])\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\t\tself.HasANI1PARAMS = True\n\t\tprint (""self.inshape:"", self.inshape)\n\n\tdef Clean(self):\n\t\tInstance.Clean(self)\n\t\tself.xyzs_pl=None\n\t\tself.check = None\n\t\tself.Zs_pl=None\n\t\tself.label_pl=None\n\t\tself.grads_pl = None\n\t\tself.atom_outputs = None\n\t\tself.energy_loss = None\n\t\tself.grads_loss = None\n\t\tself.Scatter_Sym = None\n\t\tself.Sym_Index = None\n\t\tself.options = None\n\t\tself.run_metadata = None\n\t\treturn\n\n\tdef loss_op(self, output, nn_grads, labels, grads):\n\t\tenergy_diff  = tf.subtract(output, labels)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.subtract(nn_grads, grads)\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tloss = tf.add(energy_loss, tf.multiply(grads_loss, self.GradScalar))\n\t\t#loss = tf.identity(energy_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss\n\n\n\tdef inference(self, inp, indexs):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\n\t\toutput = tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tprint(""Norms:"", nrm1,nrm2,nrm3)\n\t\tLOGGER.info(""Layer initial Norms: %f %f %f"", nrm1,nrm2,nrm3)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tindex_shape = tf.shape(index)\n\t\t\t\ttf.Print(tf.to_float(index_shape), [tf.to_float(index_shape)], message=""Element ""+str(e)+""index shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t#tf.Print(branches[-1], [branches[-1]], message=""This is layer 2: "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\trange_index = tf.range(tf.cast(shp_out[0], tf.int64), dtype=tf.int64)\n\t\t\t\tsparse_index =tf.stack([index, range_index], axis=1)\n\t\t\t\tsp_atomoutputs = tf.SparseTensor(sparse_index, rshpflat, dense_shape=[tf.cast(self.batch_size, tf.int64), tf.cast(shp_out[0], tf.int64)])\n\t\t\t\tmol_tmp = tf.sparse_reduce_sum(sp_atomoutputs, axis=1)\n\t\t\t\toutput = tf.add(output, mol_tmp)\n\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn output, atom_outputs\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.label_pl] + [self.grads_pl], [batch_data[0]]+[batch_data[1]]+[batch_data[2]] + [batch_data[3]])}\n\t\treturn feed_dict\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\t#dump_, total_loss_value, loss_value, energy_loss, grads_loss,  mol_output, atom_outputs   = self.sess.run([self.train_op, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.output,  self.atom_outputs], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#dump_, dump_2, total_loss_value, loss_value, energy_loss, grads_loss,  mol_output, atom_outputs   = self.sess.run([self.check, self.train_op, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.output,  self.atom_outputs], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  mol_output, atom_outputs = self.sess.run([self.train_op, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.output,  self.atom_outputs], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tprint (""loss_value:"", loss_value, ""grads_loss:"", grads_loss, ""energy_loss:"", energy_loss)\n\t\t\t#print (""all time:"", time.time() - t0, "" get batch time:"", batchtime)\n\t\t\t#print (""loss_value:"", loss_value, ""grads_loss:"", grads_loss, ""energy_loss:"", energy_loss)\n\t\t\t#print (""SFPr2:"", SFPr2_vary, ""\\n SFPa2:"", SFPa2_vary)\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\ttest_energy_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tpreds, total_loss_value, loss_value, energy_loss, grads_loss, mol_output, atom_outputs = self.sess.run([self.output, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.output, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\tduration = time.time() - start_time\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, num_of_mols, duration)\n\t\treturn test_loss\n\n\tdef print_training(self, step, loss, energy_loss, grads_loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f  energy_loss: %.10f  grad_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f energy_loss: %.10f  grad_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)))\n\t\treturn\n\n\tdef evaluate(self, batch_data, IfGrad=True):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\tmol_output, atom_outputs, gradient = self.sess.run([self.output, self.atom_outputs, self.gradient],  feed_dict=feed_dict)\n\t\treturn mol_output, atom_outputs, gradient\n\n\tdef Prepare(self):\n\t\tself.TrainPrepare()\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.SFPr2_vary = tf.Variable(self.SFPr2, trainable= True, dtype = self.tf_prec)\n\t\t\tRr_cut   = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut   = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta   = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta   = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update2(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update2(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut, Elep, self.SFPa, self.Ra_cut)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut, Elep, self.SFPa, self.Ra_cut)\n\t\t\t#self.Rr_cut_tf = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Ra_cut_tf = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr, self.Rr_cut_tf, Elep, self.SFPa, self.Ra_cut_tf)\n\t\t\t#tf.verify_tensor_all_finite(self.Scatter_Sym[0], ""Nan in output!!! 0 "")\n\t\t\t#tf.verify_tensor_all_finite(self.Scatter_Sym[1], ""Nan in output!!! 1"")\n\t\t\tself.output, self.atom_outputs = self.inference(self.Scatter_Sym, self.Sym_Index)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss = self.loss_op(self.output, self.gradient, self.label_pl, self.grads_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\tself.sess.graph.finalize()\n\t\t\t#self.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t#self.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\tdef EvalPrepare(self):\n\t\t""""""\n\t\tDoesn\'t generate the training operations or losses.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable=False, dtype = self.tf_prec)\n\t\t\tRr_cut   = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut   = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta   = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta   = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update2(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut)\n\t\t\tself.output, self.atom_outputs = self.inference(self.Scatter_Sym, self.Sym_Index)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\tprint(""Prepared for Evaluation..."")\n\t\treturn\n\n\tdef continue_training(self, mxsteps):\n\t\tself.EvalPrepare()\n\t\ttest_loss = self.test(-1)\n\t\ttest_freq = 1\n\t\tmini_test_loss = test_loss\n\t\tfor step in  range (0, mxsteps+1):\n\t\t\tSFPr2_vary, SFPra_vary = self.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\nclass MolInstance_DirectBP_Grad_noGradTrain(MolInstance_DirectBP_Grad):\n\t""""""\n\tAn Instance which does a direct Behler Parinello\n\tDo not use gradient in training\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_Grad_noGradTrain""\n\t\tMolInstance_DirectBP_Grad.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_Grad_noGradTrain""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\tdef loss_op(self, output, nn_grads, labels, grads):\n\t\tenergy_diff  = tf.subtract(output, labels)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.subtract(nn_grads, grads)\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\t#loss = tf.add(energy_loss, grads_loss)\n\t\tloss = tf.identity(energy_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss\n\nclass MolInstance_DirectBP_Grad_NewIndex(MolInstance_DirectBP_Grad):\n\t""""""\n\tAn Update version of Instance which does a direct Behler Parinello\n\tindex_pl holds both the index of molecule and the index of each atom\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_Grad_Update""\n\t\tMolInstance_DirectBP_Grad.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_Grad_Update""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\tdef inference(self, inp, indexs):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tprint(""Norms:"", nrm1,nrm2,nrm3)\n\t\tLOGGER.info(""Layer initial Norms: %f %f %f"", nrm1,nrm2,nrm3)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tindex_shape = tf.shape(index)\n\t\t\t\ttf.Print(tf.to_float(index_shape), [tf.to_float(index_shape)], message=""Element ""+str(e)+""index shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t#tf.Print(branches[-1], [branches[-1]], message=""This is layer 2: "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size]), atom_outputs\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.SFPr2_vary = tf.Variable(self.SFPr2, trainable= True, dtype = self.tf_prec)\n\t\t\tRr_cut   = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut   = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta   = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta   = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update_Scatter(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut)\n\t\t\tself.output, self.atom_outputs = self.inference(self.Scatter_Sym, self.Sym_Index)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss = self.loss_op(self.output, self.gradient, self.label_pl, self.grads_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\t#self.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t#self.run_metadata = tf.RunMetadata()\n\t\treturn\n\nclass MolInstance_DirectBP_Grad_Linear(MolInstance_DirectBP_Grad):\n\t""""""\n\tAn Update version of Instance which does a direct Behler Parinello\n\tindex_pl holds both the index of molecule and the index of each atom\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_Grad_Linear""\n\t\tMolInstance_DirectBP_Grad.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_Grad_Linear""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_Grad.Clean(self)\n\t\tself.Radp_pl = None\n\t\tself.Angt_pl = None\n\t\treturn\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.label_pl] + [self.grads_pl] + [self.Radp_pl] + [self.Angt_pl], [batch_data[0]]+[batch_data[1]]+[batch_data[2]] + [batch_data[3]] + [batch_data[4]] + [batch_data[5]])}\n\t\treturn feed_dict\n\n\tdef inference(self, inp, indexs):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\tif i == 0:\n\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\telse:\n\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size]), atom_outputs\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\n\t\tArgs:\n\t\t\tbatch_data: a list containing\n\t\t\tXYZ,Z,radial pairs, angular triples (all set format Mol X MaxNAtoms... )\n\t\t""""""\n\t\t# Check sanity of input\n\t\txf = batch_data[0].copy()\n\t\tzf = batch_data[1].copy()\n\t\tMustPrepare = not self.sess\n\t\tif (batch_data[0].shape[1] > self.MaxNAtoms or self.batch_size > batch_data[0].shape[0]):\n\t\t\tprint(""Natoms Match?"", batch_data[0].shape[1] , self.MaxNAtoms)\n\t\t\tprint(""BatchSizes Match?"", self.batch_size , batch_data[0].shape[0])\n\t\t\tself.batch_size = batch_data[0].shape[0]\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tMustPrepare = True\n\t\t\t# Create tensors with the right shape, and sub-fill them.\n\t\telif (batch_data[0].shape[1] != self.MaxNAtoms or self.batch_size != batch_data[0].shape[0]):\n\t\t\txf = np.zeros((self.batch_size,self.MaxNAtoms,3))\n\t\t\tzf = np.zeros((self.batch_size,self.MaxNAtoms))\n\t\t\txf[:batch_data[0].shape[0],:batch_data[0].shape[1],:] = batch_data[0]\n\t\t\tzf[:batch_data[1].shape[0],:batch_data[1].shape[1]] = batch_data[1]\n\t\tLOGGER.debug(""Batch_Size: %i"", self.batch_size)\n\t\tif MustPrepare:\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tprint (""batch_data:  "", batch_data)\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Radp_pl]+[self.Angt_pl], [xf]+[zf]+[batch_data[2]]+[batch_data[3]])}\n\t\tmol_output, atom_outputs, gradient = self.sess.run([self.output, self.atom_outputs, self.gradient],  feed_dict=feed_dict)\n\t\treturn mol_output, atom_outputs, gradient\n\n\tdef EvalPrepare(self):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.SetANI1Param()\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.Angt_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.output, self.atom_outputs = self.inference(self.Scatter_Sym, self.Sym_Index)\n\t\t\tself.gradient  = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.Angt_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut   = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut   = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta   = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta   = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.output, self.atom_outputs = self.inference(self.Scatter_Sym, self.Sym_Index)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss = self.loss_op(self.output, self.gradient, self.label_pl, self.grads_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\tif (self.FindLastCheckpoint() != False):\n\t\t\t\tself.saver.restore(self.sess, self.FindLastCheckpoint())\n\t\treturn\n\nclass MolInstance_DirectBP_Grad_Linear_EmbOpt(MolInstance_DirectBP_Grad):\n\t""""""\n\tAn Update version of Instance which does a direct Behler Parinello\n\tindex_pl holds both the index of molecule and the index of each atom\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_Grad_Linear""\n\t\tMolInstance_DirectBP_Grad.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_Grad_Linear""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.TData.ele = self.eles_np\n\t\tself.TData.elep = self.eles_pairs_np\n\n\tdef compute_normalization_constants(self):\n\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)\n\t\tself.TData.ScratchPointer = 0\n\t\txyzs, Zs, rad_p_ele, ang_t_elep, mil_jk = tf.Variable(batch_data[0], dtype=self.tf_prec), \\\n\t\t\t\t\ttf.Variable(batch_data[1], dtype=tf.int32), tf.Variable(batch_data[5], dtype=tf.int32), \\\n\t\t\t\t\ttf.Variable(batch_data[6], dtype=tf.int32), tf.Variable(batch_data[7], dtype=tf.int32)\n\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int32)\n\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int32)\n\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\telement_factors = tf.Variable(np.array([2.20, 2.55, 3.04, 3.44]), trainable=True, dtype=tf.float64)\n\t\telement_pair_factors = tf.Variable([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], trainable=True, dtype=tf.float64)\n\t\tScatter_Sym, Sym_Index = TFSymSet_Linear_channel(xyzs, Zs, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, rad_p_ele, ang_t_elep, mil_jk, element_factors, element_pair_factors)\n\t\twith tf.Session() as sess:\n\t\t\tsess.run(tf.global_variables_initializer())\n\t\t\tembed, _ = sess.run([Scatter_Sym, Sym_Index])\n\t\tself.inmean, self.instd = np.mean(np.concatenate(embed), axis=0), np.std(np.concatenate(embed), axis=0)\n\t\tself.outmean, self.outstd = np.mean(batch_data[2]), np.std(batch_data[2])\n\t\tself.gradmean, self.gradstd = np.mean(batch_data[3]), np.std(batch_data[3])\n\t\treturn\n\n\tdef SetANI1Param(self, prec=np.float64):\n\t\tself.Ra_cut = PARAMS[""AN1_a_Rc""]\n\t\tself.Rr_cut = PARAMS[""AN1_r_Rc""]\n\t\tzetas = np.array([[PARAMS[""AN1_zeta""]]], dtype = prec)\n\t\tetas = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_a_As = PARAMS[""AN1_num_a_As""]\n\t\tAN1_num_a_Rs = PARAMS[""AN1_num_a_Rs""]\n\t\tthetas = np.array([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype = prec)\n\t\trs =  np.array([ self.Ra_cut*i/AN1_num_a_Rs for i in range (0, AN1_num_a_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\t\tp1 = np.tile(np.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp3 = np.tile(np.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_a_Rs,1])\n\t\tp4 = np.tile(np.reshape(rs,[1,1,1,AN1_num_a_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\t\tSFPa = np.concatenate([p1,p2,p3,p4],axis=4)\n\t\tself.SFPa = np.transpose(SFPa, [4,0,1,2,3])\n\t\tetas_R = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_r_Rs = PARAMS[""AN1_num_r_Rs""]\n\t\trs_R =  np.array([ self.Rr_cut*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 2 x  neta X nr\n\t\tp1_R = np.tile(np.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\t\tp2_R = np.tile(np.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\t\tSFPr = np.concatenate([p1_R,p2_R],axis=2)\n\t\tself.SFPr = np.transpose(SFPr, [2,0,1])\n\t\tself.inshape = int(AN1_num_r_Rs + AN1_num_a_Rs*AN1_num_a_As)\n\t\tself.inshape_withencode = int(self.inshape + AN1_num_r_Rs)\n\t\t#self.inshape = int(len(self.eles)*AN1_num_r_Rs)\n\t\tp1 = np.tile(np.reshape(thetas,[AN1_num_a_As,1,1]),[1,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(rs,[1,AN1_num_a_Rs,1]),[AN1_num_a_As,1,1])\n\t\tSFPa2 = np.concatenate([p1,p2],axis=2)\n\t\tself.SFPa2 = np.transpose(SFPa2, [2,0,1])\n\t\tp1_new = np.reshape(rs_R,[AN1_num_r_Rs,1])\n\t\tself.SFPr2 = np.transpose(p1_new, [1,0])\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\t\tself.HasANI1PARAMS = True\n\t\tprint (""self.inshape:"", self.inshape)\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_Grad.Clean(self)\n\t\tself.Radp_pl = None\n\t\tself.Angt_pl = None\n\t\treturn\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.label_pl] + [self.grads_pl] + [self.n_atoms] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.mil_jk_pl], batch_data)}\n\t\treturn feed_dict\n\n\tdef inference(self, inp, indexs):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\tif i == 0:\n\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\telse:\n\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\treturn tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size]), atom_outputs\n\n\tdef loss_op(self, output, nn_grads, labels, grads, n_atoms):\n\t\tenergy_diff  = tf.subtract(output, labels)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.subtract(nn_grads, grads)\n\t\tnonzero_grads_diff = tf.gather_nd(grads_diff, tf.where(tf.not_equal(grads_diff, 0)))\n\t\tgrads_loss = tf.nn.l2_loss(nonzero_grads_diff) / tf.reduce_sum(n_atoms) * self.batch_size\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\t# loss = tf.add(energy_loss, tf.multiply(grads_loss, self.GradScalar))\n\t\tloss = energy_loss + grads_loss\n\t\t#loss = tf.identity(energy_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\t_, total_loss_value, loss_value, energy_loss, grads_loss, mol_output, atom_outputs = self.sess.run([self.train_op, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.output, self.atom_outputs], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\ttest_energy_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tpreds, total_loss_value, loss_value, energy_loss, grads_loss, mol_output, atom_outputs, element_factors, element_pair_factors = self.sess.run([self.output, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.output, self.atom_outputs, self.element_factors, self.element_pair_factors],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\tduration = time.time() - start_time\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, num_of_mols, duration)\n\t\tLOGGER.info(""Element factors: %s"", element_factors)\n\t\tLOGGER.info(""Element pair factors: %s"", element_pair_factors)\n\t\treturn test_loss\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\tself.compute_normalization_constants()\n\t\tself.TrainPrepare(continue_training)\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_test_loss = 100000000 # some big numbers\n\t\tfor step in range(1, mxsteps+1):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif (test_loss < mini_test_loss):\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef save_chk(self, step):  # We need to merge this with the one in TFInstance\n\t\tself.chk_file = os.path.join(self.train_dir,self.name+\'-chk-\'+str(step))\n\t\tLOGGER.info(""Saving Checkpoint file in the TFMoInstance"")\n\t\tself.saver.save(self.sess,  self.chk_file)\n\t\treturn\n\n\tdef print_training(self, step, loss, energy_loss, grads_loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f  energy_loss: %.10f  grad_loss: %.10f"", step, duration, float(loss)/(Ncase), float(energy_loss)/(Ncase), float(grads_loss)/(Ncase))\n\t\telse:\n\t\t\tLOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f energy_loss: %.10f  grad_loss: %.10f"", step, duration, float(loss)/(Ncase), float(energy_loss)/(Ncase), float(grads_loss)/(Ncase))\n\t\treturn\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\n\t\tArgs:\n\t\t\tbatch_data: a list containing\n\t\t\tXYZ,Z,radial pairs, angular triples (all set format Mol X MaxNAtoms... )\n\t\t""""""\n\t\t# Check sanity of input\n\t\txf = batch_data[0].copy()\n\t\tzf = batch_data[1].copy()\n\t\tMustPrepare = not self.sess\n\t\tif (batch_data[0].shape[1] > self.MaxNAtoms or self.batch_size > batch_data[0].shape[0]):\n\t\t\tprint(""Natoms Match?"", batch_data[0].shape[1] , self.MaxNAtoms)\n\t\t\tprint(""BatchSizes Match?"", self.batch_size , batch_data[0].shape[0])\n\t\t\tself.batch_size = batch_data[0].shape[0]\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tMustPrepare = True\n\t\t\t# Create tensors with the right shape, and sub-fill them.\n\t\telif (batch_data[0].shape[1] != self.MaxNAtoms or self.batch_size != batch_data[0].shape[0]):\n\t\t\txf = np.zeros((self.batch_size,self.MaxNAtoms,3))\n\t\t\tzf = np.zeros((self.batch_size,self.MaxNAtoms))\n\t\t\txf[:batch_data[0].shape[0],:batch_data[0].shape[1],:] = batch_data[0]\n\t\t\tzf[:batch_data[1].shape[0],:batch_data[1].shape[1]] = batch_data[1]\n\t\tLOGGER.debug(""Batch_Size: %i"", self.batch_size)\n\t\tif MustPrepare:\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Radp_pl]+[self.Angt_pl], [xf]+[zf]+[batch_data[2]]+[batch_data[3]])}\n\t\tmol_output, atom_outputs, gradient = self.sess.run([self.output, self.atom_outputs, self.gradient],  feed_dict=feed_dict)\n\t\treturn mol_output, atom_outputs, gradient\n\n\tdef EvalPrepare(self):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.SetANI1Param()\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int32, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int32, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int32, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int32, shape=tuple([None,4]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int32)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int32)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\telement_factors = tf.Variable(np.array([2.20, 2.55, 3.04, 3.44]), trainable=False, dtype=tf.float64)\n\t\t\telement_pair_factors = tf.Variable([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], trainable=False, dtype=tf.float64)\n\t\t\tself.Scatter_Sym, self.Sym_Index = TFSymSet_Linear_channel(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl, mil_jkt, element_factors, element_pair_factors )\n\t\t\tself.output, self.atom_outputs = self.inference(self.Scatter_Sym, self.Sym_Index)\n\t\t\tself.gradient = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int32, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int32, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int32, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int32, shape=tuple([None,4]))\n\t\t\tself.n_atoms = tf.placeholder(tf.float64, shape=tuple([self.batch_size]))\n\t\t\tinmean = tf.constant(self.inmean, dtype=self.tf_prec)\n\t\t\tinstd = tf.constant(self.instd, dtype=self.tf_prec)\n\t\t\toutmean = tf.constant(self.outmean, dtype=self.tf_prec)\n\t\t\toutstd = tf.constant(self.outstd, dtype=self.tf_prec)\n\t\t\tgradmean = tf.constant(self.gradmean, dtype=self.tf_prec)\n\t\t\tgradstd = tf.constant(self.gradstd, dtype=self.tf_prec)\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int32)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int32)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.element_factors = tf.Variable(np.array([2.20, 2.55, 3.04, 3.44]), trainable=True, dtype=tf.float64)\n\t\t\tself.element_pair_factors = tf.Variable([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], trainable=True, dtype=tf.float64)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.Scatter_Sym, self.Sym_Index = TFSymSet_Linear_channel(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl, self.element_factors, self.element_pair_factors)\n\t\t\tself.norm_embedding_list = []\n\t\t\tfor embedding in self.Scatter_Sym:\n\t\t\t\tself.norm_embedding_list.append((embedding - inmean) / instd)\n\t\t\tself.norm_output, self.atom_outputs = self.inference(self.norm_embedding_list, self.Sym_Index)\n\t\t\tself.output = (self.norm_output * outstd) - outmean\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.norm_gradient = tf.gradients(self.output, self.xyzs_pl)\n\t\t\tself.gradient = (self.norm_gradient * gradstd) - gradmean\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss = self.loss_op(self.output, self.gradient, self.label_pl, self.grads_pl, self.n_atoms)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\nclass MolInstance_DirectBP_EE(MolInstance_DirectBP_Grad_Linear):\n\t""""""\n\tElectrostatic embedding Behler-Parinello scheme\n\t""""""\n\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_EE""\n\t\tMolInstance_DirectBP_Grad.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE""\n\t\tself.GradScalar = PARAMS[""GradScalar""]\n\t\tself.EnergyScalar = PARAMS[""EnergyScalar""]\n\t\tself.DipoleScalar = PARAMS[""DipoleScalar""]\n\t\tself.Ree_on  = PARAMS[""EECutoffOn""]\n\t\tself.Ree_off  = PARAMS[""EECutoffOff""]\n\t\tself.DSFAlpha = PARAMS[""DSFAlpha""]\n\t\tself.learning_rate_dipole = PARAMS[""learning_rate_dipole""]\n\t\tself.learning_rate_energy = PARAMS[""learning_rate_energy""]\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.Training_Traget = ""Dipole""\n\t\tself.suffix = PARAMS[""NetNameSuffix""]\n\t\tself.SetANI1Param()\n\t\tself.run_metadata = None\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_Grad_Linear.Clean(self)\n\t\tself.Elabel_pl = None\n\t\tself.Dlabel_pl = None\n\t\tself.Radp_pl = None\n\t\tself.Angt_pl = None\n\t\tself.Reep_pl = None\n\t\tself.natom_pl = None\n\t\tself.AddEcc_pl = None\n\t\tself.Etotal = None\n\t\tself.Ebp = None\n\t\tself.Ecc = None\n\t\tself.dipole = None\n\t\tself.charge = None\n\t\tself.energy_wb = None\n\t\tself.dipole_wb = None\n\t\tself.dipole_loss = None\n\t\tself.gradient = None\n\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = None, None, None, None, None\n\t\tself.train_op_dipole, self.train_op_EandG = None, None\n\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = None, None, None, None, None\n\t\tself.run_metadata = None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.Angt_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.Etotal, self.Ebp, self.Ecc, self.dipole, self.charge, self.energy_wb, self.dipole_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl,name=""BPEnGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl,name=""BPEnGrad"", colocate_gradients_with_ops=True)\n\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\n\t\t\t#self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG = self.loss_op_EandG_test(self.Etotal, self.gradient, self.Elabel_pl, self.grads_pl)\n\t\t\t#self.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\n\t\t\tself.sess.graph.finalize()\n\t\treturn\n\n\tdef loss_op(self, energy, energy_grads, dipole, Elabels, grads, Dlabels):\n\t\tenergy_diff  = tf.subtract(energy, Elabels,name=""EnDiff"")\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff,name=""EnL2"")\n\t\tgrads_diff = tf.subtract(energy_grads, grads,name=""GradDiff"")\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff,name=""GradL2"")\n\t\tdipole_diff = tf.subtract(dipole, Dlabels,name=""DipoleDiff"")\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff,name=""DipL2"")\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tEandG_loss = tf.add(energy_loss, tf.multiply(grads_loss, self.GradScalar),name=""MulLoss"")\n\t\tloss = tf.add(EandG_loss, tf.multiply(dipole_loss, self.DipoleScalar))\n\t\t#loss = tf.identity(dipole_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\tdef loss_op_dipole(self, energy, energy_grads, dipole, Elabels, grads, Dlabels):\n\t\tenergy_diff  = tf.subtract(energy, Elabels)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.subtract(energy_grads, grads)\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\tdipole_diff = tf.subtract(dipole, Dlabels)\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tEandG_loss = tf.add(energy_loss, tf.multiply(grads_loss, self.GradScalar))\n\t\tloss = tf.identity(dipole_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\tdef loss_op_EandG(self, energy, energy_grads, dipole, Elabels, grads, Dlabels):\n\t\tenergy_diff  = tf.subtract(energy, Elabels)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.subtract(energy_grads, grads)\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\tdipole_diff = tf.subtract(dipole, Dlabels)\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tEandG_loss = tf.add(energy_loss, tf.multiply(grads_loss, self.GradScalar))\n\t\t#loss = tf.add(EandG_loss, tf.multiply(dipole_loss, self.DipoleScalar))\n\t\tloss = tf.identity(EandG_loss)\n\t\t#loss = tf.identity(energy_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\tdef loss_op_EandG_test(self, energy, energy_grads, Elabels, grads):\n\t\tenergy_diff  = tf.subtract(energy, Elabels)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.subtract(energy_grads, grads)\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\tEandG_loss = tf.add(energy_loss, tf.multiply(grads_loss, self.GradScalar))\n\t\tloss = tf.identity(EandG_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss\n\n\tdef inference(self, inp, indexs, xyzs, natom, EE_cuton, EE_cutoff, Reep, AddEcc):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tEbranches=[]\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(Ebranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(charge_inputs, weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(Dbranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.MaxNAtoms])\n\t\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzsInBohr,[self.batch_size*self.MaxNAtoms, 3]), tf.reshape(scaled_charge,[self.batch_size*self.MaxNAtoms, 1]))\n\t\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.MaxNAtoms, 3]), axis=1)\n\t\t#cc_energy = TFCoulombErfLR(xyzsInBohr, scaled_charge, EE_cuton, Reep)\n\t\t#cc_energy =tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tdef f1(): return TFCoulombPolyLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, Reep)\n\t\t#def f1(): return TFCoulombErfLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, Reep)\n\t\t#def f1(): return  TFCoulombErfSRDSFLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, EE_cutoff*BOHRPERA, Reep, self.DSFAlpha)\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)\n\t\t#dipole_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""DipoleNet"")\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\treturn total_energy, bp_energy, cc_energy, dipole, scaled_charge, energy_vars, dipole_wb\n\n\n\tdef training(self, loss, learning_rate, momentum, update_var=None):\n\t\t""""""Sets up the training Ops.\n\t\tCreates a summarizer to track the loss over time in TensorBoard.\n\t\tCreates an optimizer and applies the gradients to all trainable variables.\n\t\tThe Op returned by this function is what must be passed to the\n\t\t`sess.run()` call to cause the model to train.\n\t\tArgs:\n\t\tloss: Loss tensor, from loss().\n\t\tlearning_rate: The learning rate to use for gradient descent.\n\t\tReturns:\n\t\ttrain_op: The Op for training.\n\t\t""""""\n\t\ttf.summary.scalar(loss.op.name, loss)\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate,name=""Adam"")\n\t\t#optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n\t\tglobal_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\t\tif update_var == None:\n\t\t\ttrain_op = optimizer.minimize(loss, global_step=global_step, name=""trainop"")\n\t\telse:\n\t\t\ttrain_op = optimizer.minimize(loss, global_step=global_step, var_list=update_var, name=""trainop"")\n\t\treturn train_op\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_pl] + [self.Angt_pl] + [self.Reep_pl] + [self.natom_pl] + [self.AddEcc_pl], batch_data)}\n\t\treturn feed_dict\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+[PARAMS[""AddEcc""]]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op, self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Etotal:"", Etotal, "" Ecc:"", Ecc)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[PARAMS[""AddEcc""]]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn test_loss\n\n\tdef train_step_dipole(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [False]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op_dipole, self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""ministep:  "", ministep, ""mini step time dipole:"", time.time() - t_mini )\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#LOGGER.debug(""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#max_index = np.argmax(np.sum(abs(batch_data[3]-mol_dipole),axis=1))\n\t\t\t#LOGGER.debug(""real dipole:\\n"", batch_data[3][max_index], ""\\nmol_dipole:\\n"", mol_dipole[max_index], ""\\n xyz:"", batch_data[0][max_index], batch_data[1][max_index])\n\t\t\t#print (""Etotal:"", Etotal[:20], "" Ecc:"", Ecc[:20])\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\n\tdef test_dipole(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[False]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn  test_loss\n\n\tdef train_step_EandG(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+[PARAMS[""AddEcc""]]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op_EandG, self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""ministep:  "", ministep, ""mini step time EandG:"", time.time() - t_mini)\n\t\t\t##print (""Ecc:"", Ecc[:20])\n\t\t\t#for k, ecc in enumerate(list(Ecc)):\n\t\t\t#\tif ecc > 0.05:\n\t\t\t#\t\tprint (""Ecc:"", ecc)\n\t\t\t#\t\tnp.savetxt(""test_charge.dat"", atom_charge[k])\n\t\t\t#\t\tnp.savetxt(""test_xyz.dat"", batch_data[0][k])\n\t\t\t#\t\traise Exception(""end now"")\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Etotal:"", Etotal, "" Ecc:"", Ecc)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\tdef test_EandG(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[PARAMS[""AddEcc""]]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration)\n\t\treturn  test_loss\n\n\tdef print_training(self, step, loss, energy_loss, grads_loss, dipole_loss, Ncase, duration, Train=True):\n\t    if Train:\n\t        LOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f  energy_loss: %.10f  grad_loss: %.10f, dipole_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)), (float(dipole_loss)/(Ncase)))\n\t    else:\n\t        LOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f energy_loss: %.10f  grad_loss: %.10f, dipole_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)), (float(dipole_loss)/(Ncase)))\n\t    return\n\n\tdef EvalPrepare(self):\n\t\t""""""\n\t\tLoad pretrained network and build graph for evaluation\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.Angt_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut   = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut   = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.Etotal, self.Ebp, self.Ecc, self.dipole, self.charge, self.energy_wb, self.dipole_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, ""TotalEnGrad"")\n\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\tprint(""Prepared for Evaluation..."")\n\t\treturn\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]])\n\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ecc, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\treturn Etotal, Ebp, Ecc, mol_dipole, atom_charge, gradient\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\t""""""\n\t\tThis the training loop for the united model.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(continue_training)\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_dipole_test_loss = float(\'inf\') # some big numbers\n\t\tmini_energy_test_loss = float(\'inf\')\n\t\tmini_test_loss = float(\'inf\')\n\t\tfor step in  range (0, mxsteps):\n\t\t\tif self.Training_Traget == ""EandG"":\n\t\t\t\tself.train_step_EandG(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\ttest_energy_loss = self.test_EandG(step)\n\t\t\t\t\tif test_energy_loss < mini_energy_test_loss:\n\t\t\t\t\t\tmini_energy_test_loss = test_energy_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\t\telif self.Training_Traget == ""Dipole"":\n\t\t\t\tself.train_step_dipole(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\ttest_dipole_loss = self.test_dipole(step)\n\t\t\t\t\tif test_dipole_loss < mini_dipole_test_loss:\n\t\t\t\t\t\tmini_dipole_test_loss = test_dipole_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\t\t\t\t\tif step >= PARAMS[""SwitchEpoch""]:\n\t\t\t\t\t\t\tself.Training_Traget = ""EandG""\n\t\t\t\t\t\t\tprint (""Switching to Energy and Gradient Learning..."")\n\t\t\telse:\n\t\t\t\tself.train_step(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\ttest_loss = self.test(step)\n\t\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef profile_step(self, step):\n\t\t""""""\n\t\tPerform a single profiling step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+[False]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op_EandG, self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data), options=self.options, run_metadata=self.run_metadata)\n\t\t\t#print (""Ecc:"", Ecc[:20])\n\t\t\t#for k, ecc in enumerate(list(Ecc)):\n\t\t\t#\tif ecc > 0.05:\n\t\t\t#\t\tprint (""Ecc:"", ecc)\n\t\t\t#\t\tnp.savetxt(""test_charge.dat"", atom_charge[k])\n\t\t\t#\t\tnp.savetxt(""test_xyz.dat"", batch_data[0][k])\n\t\t\t#\t\traise Exception(""end now"")\n\t\t\tprint (""inference time:"", time.time() - t)\n\t\t\tprint (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Etotal:"", Etotal, "" Ecc:"", Ecc)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, \'minstep%d\' % ministep)\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\tfetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\tchrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\twith open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t\tf.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\tdef profile(self):\n\t\t""""""\n\t\tThis profiles a training step.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(False)\n\t\tself.profile_step(1)\n\t\treturn\n\n\tdef continue_training(self, mxsteps):\n\t\tself.EvalPrepare()\n\t\t#test_loss = self.test(-1)\n\t\tself.test(-1)\n\t\ttest_loss = float(\'inf\')\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_test_loss = test_loss\n\t\tfor step in  range (0, mxsteps+1):\n\t\t\tself.train_step(step)\n\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\ttest_loss = self.test(step)\n\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\nclass MolInstance_DirectBP_EE_ChargeEncode(MolInstance_DirectBP_EE):\n\t""""""\n\tElectrostatic embedding Behler Parinello\n\t""""""\n\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_EE_ChargeEnCode""\n\t\tMolInstance_DirectBP_EE.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.Training_Traget = ""Dipole""\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_EE.Clean(self)\n\t\tself.Radius_Qs_Encode = None\n\t\tself.Radius_Qs_Encode_Index = None\n\n\tdef SetANI1Param(self, prec=np.float64):\n\t\tself.Ra_cut = PARAMS[""AN1_a_Rc""]\n\t\tself.Rr_cut = PARAMS[""AN1_r_Rc""]\n\t\tzetas = np.array([[PARAMS[""AN1_zeta""]]], dtype = prec)\n\t\tetas = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_a_As = PARAMS[""AN1_num_a_As""]\n\t\tAN1_num_a_Rs = PARAMS[""AN1_num_a_Rs""]\n\t\tthetas = np.array([ 2.0*Pi*i/AN1_num_a_As for i in range (0, AN1_num_a_As)], dtype = prec)\n\t\trs =  np.array([ self.Ra_cut*i/AN1_num_a_Rs for i in range (0, AN1_num_a_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 4 x nzeta X neta X ntheta X nr\n\t\tp1 = np.tile(np.reshape(zetas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(etas,[1,1,1,1,1]),[1,1,AN1_num_a_As,AN1_num_a_Rs,1])\n\t\tp3 = np.tile(np.reshape(thetas,[1,1,AN1_num_a_As,1,1]),[1,1,1,AN1_num_a_Rs,1])\n\t\tp4 = np.tile(np.reshape(rs,[1,1,1,AN1_num_a_Rs,1]),[1,1,AN1_num_a_As,1,1])\n\t\tSFPa = np.concatenate([p1,p2,p3,p4],axis=4)\n\t\tself.SFPa = np.transpose(SFPa, [4,0,1,2,3])\n\t\tetas_R = np.array([[PARAMS[""AN1_eta""]]], dtype = prec)\n\t\tAN1_num_r_Rs = PARAMS[""AN1_num_r_Rs""]\n\t\trs_R =  np.array([ self.Rr_cut*i/AN1_num_r_Rs for i in range (0, AN1_num_r_Rs)], dtype = prec)\n\t\t# Create a parameter tensor. 2 x  neta X nr\n\t\tp1_R = np.tile(np.reshape(etas_R,[1,1,1]),[1,AN1_num_r_Rs,1])\n\t\tp2_R = np.tile(np.reshape(rs_R,[1,AN1_num_r_Rs,1]),[1,1,1])\n\t\tSFPr = np.concatenate([p1_R,p2_R],axis=2)\n\t\tself.SFPr = np.transpose(SFPr, [2,0,1])\n\t\tself.inshape = int(len(self.eles)*AN1_num_r_Rs + len(self.eles_pairs)*AN1_num_a_Rs*AN1_num_a_As)\n\t\tself.inshape_withencode = int(self.inshape + AN1_num_r_Rs)\n\t\t#self.inshape = int(len(self.eles)*AN1_num_r_Rs)\n\t\tp1 = np.tile(np.reshape(thetas,[AN1_num_a_As,1,1]),[1,AN1_num_a_Rs,1])\n\t\tp2 = np.tile(np.reshape(rs,[1,AN1_num_a_Rs,1]),[AN1_num_a_As,1,1])\n\t\tSFPa2 = np.concatenate([p1,p2],axis=2)\n\t\tself.SFPa2 = np.transpose(SFPa2, [2,0,1])\n\t\tp1_new = np.reshape(rs_R,[AN1_num_r_Rs,1])\n\t\tself.SFPr2 = np.transpose(p1_new, [1,0])\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\t\tself.HasANI1PARAMS = True\n\t\tprint (""self.inshape:"", self.inshape)\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialPairs"")\n\t\t\tself.Angt_pl=tf.placeholder(tf.int64, shape=tuple([None,4]),name=""AngularTriples"")\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta,  self.Radp_pl, self.charge)\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp,  self.energy_wb = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\n\t\t\tself.sess.graph.finalize()\n\n\tdef dipole_inference(self, inp, indexs, xyzs, natom, EE_cuton, EE_cutoff, Reep, AddEcc):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(charge_inputs, weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(Dbranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.MaxNAtoms])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzsInBohr,[self.batch_size*self.MaxNAtoms, 3]), tf.reshape(scaled_charge,[self.batch_size*self.MaxNAtoms, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.MaxNAtoms, 3]), axis=1)\n\n\t\tdef f1(): return TFCoulombPolyLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, Reep)\n\t\t#def f1(): return TFCoulombErfLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, Reep)\n\t\t#def f1(): return  TFCoulombErfSRDSFLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, EE_cutoff*BOHRPERA, Reep, self.DSFAlpha)\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)\n\t\t#dipole_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""DipoleNet"")\n\t\treturn  cc_energy, dipole, scaled_charge, dipole_wb\n\n\n\tdef energy_inference(self, inp, indexs, charge_encode, cc_energy):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\traw_inputs = inp[e]\n\t\t\t\tencode_inputs = charge_encode[e]\n\t\t\t\tinputs = tf.concat([encode_inputs, raw_inputs], axis=1)\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape_withencode, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape_withencode))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(Ebranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\treturn total_energy, bp_energy, energy_vars\n\n\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.Angt_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta,  self.Radp_pl, self.charge)\n\t\t\tself.Etotal, self.Ebp,  self.energy_wb = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl)\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\n\t\t\t#self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG = self.loss_op_EandG_test(self.Etotal, self.gradient, self.Elabel_pl, self.grads_pl)\n\t\t\t#self.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\tprint(""Prepared for Evaluation..."")\n\n\nclass MolInstance_DirectBP_EE_Update(MolInstance_DirectBP_EE):\n\t""""""\n\tElectrostatic embedding Behler-Parinello scheme.\n\tThis version prebuild the mijkl and mil_jk in python.\n\t""""""\n\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_Update""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.TData.ele = self.eles_np\n\t\tself.TData.elep = self.eles_pairs_np\n\t\tself.SetANI1Param()\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_Grad_Linear.Clean(self)\n\t\tself.Elabel_pl = None\n\t\tself.Dlabel_pl = None\n\t\tself.Radp_Ele_pl = None\n\t\tself.Angt_Elep_pl = None\n\t\tself.mil_jk_pl = None\n\t\tself.Reep_pl = None\n\t\tself.natom_pl = None\n\t\tself.AddEcc_pl = None\n\t\tself.Etotal = None\n\t\tself.Ebp = None\n\t\tself.Ecc = None\n\t\tself.dipole = None\n\t\tself.charge = None\n\t\tself.energy_wb = None\n\t\tself.dipole_wb = None\n\t\tself.dipole_loss = None\n\t\tself.gradient = None\n\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = None, None, None, None, None\n\t\tself.train_op_dipole, self.train_op_EandG = None, None\n\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = None, None, None, None, None\n\t\tself.run_metadata = None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_tmp(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Etotal, self.Ebp, self.Ecc, self.dipole, self.charge, self.energy_wb, self.dipole_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl,name=""BPEnGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl,name=""BPEnGrad"", colocate_gradients_with_ops=True)\n\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\n\t\t\t#self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG = self.loss_op_EandG_test(self.Etotal, self.gradient, self.Elabel_pl, self.grads_pl)\n\t\t\t#self.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tconfig=tf.ConfigProto(allow_soft_placement=True)\n\t\t\t#config=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\t#config.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\n\t\t\tself.sess.graph.finalize()\n\t\treturn\n\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_pl] + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl], batch_data)}\n\t\treturn feed_dict\n\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]])\n\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ecc, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\treturn Etotal, Ebp, Ecc, mol_dipole, atom_charge, gradient\n\n\n\tdef EvalPrepare(self):\n\t\t""""""\n\t\tLoad pretrained network and build graph for evaluation\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]))\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_tmp(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Etotal, self.Ebp, self.Ecc, self.dipole, self.charge, self.energy_wb, self.dipole_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl,name=""BPEnGrad"")\n\t\t\t#self.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\t#self.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\n\t\t\t#self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\t#self.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\n\t\t\t#self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\t#self.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\tprint(""Prepared for Evaluation..."")\n\t\treturn\n\n\nclass MolInstance_DirectBP_EE_ChargeEncode_Update(MolInstance_DirectBP_EE_ChargeEncode):\n\t""""""\n\tElectrostatic embedding Behler Parinello\n\t""""""\n\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.Training_Traget = ""Dipole""\n\t\tself.TData.ele = self.eles_np\n\t\tself.TData.elep = self.eles_pairs_np\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_EE_ChargeEncode.Clean(self)\n\t\tself.Radp_Ele_pl = None\n\t\tself.Angt_Elep_pl = None\n\t\tself.mil_jk_pl = None\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta,  self.Radp_pl, self.charge)\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp,  self.energy_wb = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_pl] + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl], batch_data)}\n\t\treturn feed_dict\n\nclass MolInstance_DirectBP_EE_ChargeEncode_Update_vdw(MolInstance_DirectBP_EE_ChargeEncode_Update):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update_vdw""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.Training_Traget = ""Dipole""\n\t\tself.vdw_R = np.zeros(self.n_eles)\n\t\tself.C6 = np.zeros(self.n_eles)\n\t\tfor i, ele in enumerate(self.eles):\n\t\t\tself.C6[i] = C6_coff[ele]* (BOHRPERA*10.0)**6.0 / JOULEPERHARTREE # convert into a.u.\n\t\t\tself.vdw_R[i] = atomic_vdw_radius[ele]*BOHRPERA\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta,  self.Radp_pl, self.charge)\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef energy_inference(self, inp, indexs, charge_encode, cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep, EE_cuton, EE_cutoff):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\traw_inputs = inp[e]\n\t\t\t\tencode_inputs = charge_encode[e]\n\t\t\t\tinputs = tf.concat([encode_inputs, raw_inputs], axis=1)\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape_withencode, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape_withencode))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(Ebranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLR(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep)\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update.Clean(self)\n\t\tself.Ebp_atom = None\n\t\tself.Evdw = None\n\n\tdef train_step_EandG(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_grads_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+[PARAMS[""AddEcc""]]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, Evdw, mol_dipole, atom_charge = self.sess.run([self.train_op_EandG, self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.Evdw,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_grads_loss += grads_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Ecc:"", Ecc, "" Etotal:"", Etotal)\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini, "" grads_loss:"", print_grads_loss/print_per_mini, "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_grads_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\n\t\t\t\t#print (""Etotal:"", Etotal, "" Ecc:"", Ecc, ""Evdw:"", Evdw)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\n\tdef train_step_dipole(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\n\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_grads_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [False]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op_dipole, self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""ministep:  "", ministep, ""mini step time dipole:"", time.time() - t_mini )\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Ecc:"", Ecc, "" Etotal:"", Etotal)\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_grads_loss += grads_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini, "" grads_loss:"", print_grads_loss/print_per_mini, "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_grads_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\t\t\t#LOGGER.debug(""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#max_index = np.argmax(np.sum(abs(batch_data[3]-mol_dipole),axis=1))\n\t\t\t#LOGGER.debug(""real dipole:\\n"", batch_data[3][max_index], ""\\nmol_dipole:\\n"", mol_dipole[max_index], ""\\n xyz:"", batch_data[0][max_index], batch_data[1][max_index])\n\t\t\t#print (""Etotal:"", Etotal[:20], "" Ecc:"", Ecc[:20])\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]])\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient= self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta,  self.Radp_pl, self.charge)\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\t\tprint(""Prepared for Evaluation..."")\n\n\tdef fill_feed_dict_periodic(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_e1e2_pl] + [self.mil_j_pl]  + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl], batch_data)}\n\t\treturn feed_dict\n\n\t@TMTiming(""EvalPeriodic"")\n\tdef evaluate_periodic(self, batch_data, nreal,DoForce = True):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.nreal = nreal\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Periodic()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Periodic()\n\t\tt0 = time.time()\n\t\tfeed_dict=self.fill_feed_dict_periodic(batch_data+[PARAMS[""AddEcc""]])\n\t\tif (DoForce):\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient, gradient_bp, gradient_cc, scatter_sym = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient, self.gradient_bp, self.gradient_cc, self.Scatter_Sym], feed_dict=feed_dict)\n\t\t\t#print (""atom_charge:"", atom_charge, "" Etotal:"", Etotal, "" Ebp:"", Ebp, "" Ecc:"", Ecc, "" Evdw:"", Evdw)\n\t\t\t#print (""gradient_bp:"", gradient_bp, ""nzz:"", np.count_nonzero(gradient_bp))\n\t\t\t#print (""gradient_cc:"", gradient_cc, ""nzz:"", np.count_nonzero(gradient_cc), ""shape:"", gradient_cc[0].shape)\n\t\t\t#print (""Scatter_Sym:"", Scatter_Sym[0][0])\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_evalutaion.json\', \'w\') as f:\n\t\t\t#\tf.write(chrome_trace)\n\t\t\t#print (""evaluation time:"", time.time() - t0)\n\t\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\t\telse:\n\t\t\tEtotal = self.sess.run(self.Etotal, feed_dict=feed_dict)\n\t\t\treturn Etotal\n\n\n\tdef EvalPrepare_Periodic(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_e1e2_pl=tf.placeholder(tf.int64, shape=tuple([None,5]),name=""RadialElectros"")\n\t\t\tself.Reep_pl = self.Reep_e1e2_pl[:,:3]\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Periodic(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl, self.nreal)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs_Periodic(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta, self.Radp_pl, self.charge, self.mil_j_pl,  self.nreal)\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_e1e2_pl, Ree_on, Ree_off)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient_sym = tf.gradients(self.Scatter_Sym[0][0], self.xyzs_pl, name=""SymGrad"")\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.gradient_bp  = tf.gradients(self.Ebp, self.xyzs_pl, name=""BPGrad"")\n\t\t\tself.gradient_cc  = tf.gradients(self.Ecc, self.xyzs_pl, name=""CCGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\t#self.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t#self.run_metadata = tf.RunMetadata()\n\t\t\tself.sess.graph.finalize()\n\t\tprint(""Prepared for Evaluation..."")\n\n\tdef dipole_inference_periodic(self, inp, indexs, xyzs, natom, EE_cuton, EE_cutoff, Reep, AddEcc):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\txyzs_real = xyzsInBohr[:,:self.nreal]\n\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.nreal], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(charge_inputs, weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(Dbranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.nreal]),[self.batch_size, self.nreal])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.nreal])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzs_real,[self.batch_size*self.nreal, 3]), tf.reshape(scaled_charge,[self.batch_size*self.nreal, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.nreal, 3]), axis=1)\n\n\t\tntess = tf.cast(tf.div(self.MaxNAtoms, self.nreal), dtype=tf.int32)\n\t\tscaled_charge_all = tf.tile(scaled_charge, [1, ntess])\n\t\tdef f1(): return TFCoulombPolyLRSR(xyzsInBohr, scaled_charge_all, EE_cuton*BOHRPERA, Reep)\n\t\t#def f1(): return TFCoulombErfLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, Reep)\n\t\t#def f1(): return  TFCoulombErfSRDSFLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, EE_cutoff*BOHRPERA, Reep, self.DSFAlpha)\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)\n\t\t#dipole_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""DipoleNet"")\n\t\treturn  cc_energy, dipole, scaled_charge_all, dipole_wb\n\n\n\tdef energy_inference_periodic(self, inp, indexs, charge_encode, cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep_e1e2, EE_cuton, EE_cutoff):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.nreal], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\traw_inputs = inp[e]\n\t\t\t\tencode_inputs = charge_encode[e]\n\t\t\t\tinputs = tf.concat([encode_inputs, raw_inputs], axis=1)\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape_withencode, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape_withencode))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(Ebranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.nreal]),[self.batch_size, self.nreal])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLRWithEle(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep_e1e2)\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output\n\n\nclass MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu(MolInstance_DirectBP_EE_ChargeEncode_Update_vdw):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update_vdw_DSF_elu""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tif self.Ree_on != 0.0:\n\t\t\traise Exception(""EECutoffOn should equal to zero in DSF_elu"")\n\t\tself.elu_width = PARAMS[""Elu_Width""]\n\t\t#self.elu_shift = DSF(self.elu_width*BOHRPERA, self.Ree_off*BOHRPERA, self.DSFAlpha)\n\t\t#self.elu_alpha = DSF_Gradient(self.elu_width*BOHRPERA, self.Ree_off*BOHRPERA, self.DSFAlpha)\n\t\tself.elu_shift = DSF(self.elu_width*BOHRPERA, self.Ree_off*BOHRPERA, self.DSFAlpha/BOHRPERA)\n\t\tself.elu_alpha = DSF_Gradient(self.elu_width*BOHRPERA, self.Ree_off*BOHRPERA, self.DSFAlpha/BOHRPERA)\n\t\tprint (""self.elu_shift: "",self.elu_shift)\n\t\tprint (""self.elu_alpha: "",self.elu_alpha)\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw.Clean(self)\n\t\tself.elu_width = None\n\t\tself.elu_shift = None\n\t\tself.elu_alpha = None\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta,  self.Radp_pl, self.charge)\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef dipole_inference(self, inp, indexs, xyzs, natom, Elu_Width, EE_cutoff, Reep, AddEcc):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(charge_inputs, weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(Dbranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.MaxNAtoms])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzsInBohr,[self.batch_size*self.MaxNAtoms, 3]), tf.reshape(scaled_charge,[self.batch_size*self.MaxNAtoms, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.MaxNAtoms, 3]), axis=1)\n\n\t\tdef f1(): return TFCoulombEluSRDSFLR(xyzsInBohr, scaled_charge, Elu_Width*BOHRPERA, Reep, tf.cast(self.DSFAlpha, self.tf_prec), tf.cast(self.elu_alpha,self.tf_prec), tf.cast(self.elu_shift,self.tf_prec))\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)\n\t\t#dipole_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""DipoleNet"")\n\t\treturn  cc_energy, dipole, scaled_charge, dipole_wb\n\n\tdef dipole_inference_periodic(self, inp, indexs, xyzs, natom, Elu_Width, EE_cutoff, Reep, AddEcc):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\txyzs_real = xyzsInBohr[:,:self.nreal]\n\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.nreal], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(charge_inputs, weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(Dbranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.nreal]),[self.batch_size, self.nreal])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.nreal])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzs_real,[self.batch_size*self.nreal, 3]), tf.reshape(scaled_charge,[self.batch_size*self.nreal, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.nreal, 3]), axis=1)\n\n\t\tntess = tf.cast(tf.div(self.MaxNAtoms, self.nreal), dtype=tf.int32)\n\t\tscaled_charge_all = tf.tile(scaled_charge, [1, ntess])\n\t\tdef f1(): return TFCoulombEluSRDSFLR(xyzsInBohr, scaled_charge_all, Elu_Width*BOHRPERA, Reep, tf.cast(self.DSFAlpha, self.tf_prec), tf.cast(self.elu_alpha,self.tf_prec), tf.cast(self.elu_shift,self.tf_prec))\n\t\t#def f1(): return TFCoulombPolyLRSR(xyzsInBohr, scaled_charge_all, EE_cuton*BOHRPERA, Reep)\n\t\t#def f1(): return TFCoulombErfLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, Reep)\n\t\t#def f1(): return  TFCoulombErfSRDSFLR(xyzsInBohr, scaled_charge, EE_cuton*BOHRPERA, EE_cutoff*BOHRPERA, Reep, self.DSFAlpha)\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)\n\t\t#dipole_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""DipoleNet"")\n\t\treturn  cc_energy, dipole, scaled_charge_all, dipole_wb\n\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta,  self.Radp_pl, self.charge)\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\t\tprint(""Prepared for Evaluation..."")\n\n\n\tdef EvalPrepare_Periodic(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_e1e2_pl=tf.placeholder(tf.int64, shape=tuple([None,5]),name=""RadialElectros"")\n\t\t\tself.Reep_pl = self.Reep_e1e2_pl[:,:3]\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Periodic(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl, self.nreal)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n\t\t\tself.Radius_Qs_Encode, self.Radius_Qs_Encode_Index = TFSymSet_Radius_Scattered_Linear_Qs_Periodic(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, eta, self.Radp_pl, self.charge, self.mil_j_pl,  self.nreal)\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.Radius_Qs_Encode, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_e1e2_pl, Ree_on, Ree_off)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient_sym = tf.gradients(self.Scatter_Sym[0][0], self.xyzs_pl, name=""SymGrad"")\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.gradient_bp  = tf.gradients(self.Ebp, self.xyzs_pl, name=""BPGrad"")\n\t\t\tself.gradient_cc  = tf.gradients(self.Ecc, self.xyzs_pl, name=""CCGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\t#self.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t#self.run_metadata = tf.RunMetadata()\n\t\t\tself.sess.graph.finalize()\n\t\tprint(""Prepared for Evaluation..."")\n\n\nclass MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize(MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw.Clean(self)\n\t\tself.elu_width = None\n\t\tself.elu_shift = None\n\t\tself.elu_alpha = None\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\n\tdef energy_inference(self, inp, indexs,  cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep, EE_cuton, EE_cutoff):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(Ebranches[-1][-1], weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLR(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep)\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output\n\n\tdef loss_op(self, energy, energy_grads, dipole, Elabels, grads, Dlabels, natom):\n\t\tmaxatom=tf.cast(tf.shape(energy_grads)[2], self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels,name=""EnDiff""), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff,name=""EnL2"")\n\t\tgrads_diff = tf.multiply(tf.subtract(energy_grads, grads,name=""GradDiff""), tf.reshape(natom*maxatom, [1, self.batch_size, 1, 1]))\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff,name=""GradL2"")\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels,name=""DipoleDiff""), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff,name=""DipL2"")\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tEandG_loss = tf.add(tf.multiply(energy_loss, self.EnergyScalar), tf.multiply(grads_loss, self.GradScalar),name=""MulLoss"")\n\t\tloss = tf.add(EandG_loss, tf.multiply(dipole_loss, self.DipoleScalar))\n\t\t#loss = tf.identity(dipole_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\tdef loss_op_dipole(self, energy, energy_grads, dipole, Elabels, grads, Dlabels, natom):\n\t\tmaxatom=tf.cast(tf.shape(energy_grads)[2], self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.multiply(tf.subtract(energy_grads, grads), tf.reshape(natom*maxatom, [1, self.batch_size, 1, 1]))\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tEandG_loss = tf.add(tf.multiply(energy_loss, self.EnergyScalar), tf.multiply(grads_loss, self.GradScalar))\n\t\tloss = tf.identity(dipole_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\tdef loss_op_EandG(self, energy, energy_grads, dipole, Elabels, grads, Dlabels, natom):\n\t\tmaxatom=tf.cast(tf.shape(energy_grads)[2], self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tgrads_diff = tf.multiply(tf.subtract(energy_grads, grads), tf.reshape(natom*maxatom, [1, self.batch_size, 1, 1]))\n\t\tgrads_loss = tf.nn.l2_loss(grads_diff)\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tEandG_loss = tf.add(tf.multiply(energy_loss, self.EnergyScalar), tf.multiply(grads_loss, self.GradScalar))\n\t\t#loss = tf.add(EandG_loss, tf.multiply(dipole_loss, self.DipoleScalar))\n\t\tloss = tf.identity(EandG_loss)\n\t\t#loss = tf.identity(energy_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, grads_loss, dipole_loss\n\n\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\nclass MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout(MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+self.suffix\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.keep_prob = np.asarray(PARAMS[""KeepProb""])\n\t\tself.nlayer = len(PARAMS[""KeepProb""]) - 1\n\t\tself.monitor_mset =  PARAMS[""MonitorSet""]\n\t\t#self.tf_precision = eval(""tf.float64"")\n\t\t#self.set_symmetry_function_params()\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize.Clean(self)\n\t\tself.keep_prob_pl = None\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef TrainPrepare_Johns(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\t#Elep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\telements = tf.constant(self.elements, dtype = tf.int64)\n\t\t\telement_pairs = tf.constant(self.element_pairs, dtype = tf.int64)\n\t\t\tradial_rs = tf.Variable(self.radial_rs, trainable=False, dtype = self.tf_precision)\n\t\t\tangular_rs = tf.Variable(self.angular_rs, trainable=False, dtype = self.tf_precision)\n\t\t\ttheta_s = tf.Variable(self.theta_s, trainable=False, dtype = self.tf_precision)\n\t\t\tradial_cutoff = tf.constant(self.radial_cutoff, dtype = self.tf_precision)\n\t\t\tangular_cutoff = tf.constant(self.angular_cutoff, dtype = self.tf_precision)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_precision)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_precision)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = tf_symmetry_functions_2(self.xyzs_pl, self.Zs_pl, elements, element_pairs, radial_cutoff, angular_cutoff, radial_rs, angular_rs, theta_s, zeta, eta)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_pl] + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\tdef energy_inference(self, inp, indexs,  cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep, EE_cuton, EE_cutoff, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Ebranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Ebranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLR(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep)\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output\n\n\tdef dipole_inference(self, inp, indexs, xyzs, natom, Elu_Width, EE_cutoff, Reep, AddEcc, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(charge_inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Dbranches[-1].append(self.activation_function(tf.matmul(charge_inputs, weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Dbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.MaxNAtoms])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzsInBohr,[self.batch_size*self.MaxNAtoms, 3]), tf.reshape(scaled_charge,[self.batch_size*self.MaxNAtoms, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.MaxNAtoms, 3]), axis=1)\n\n\t\tdef f1(): return TFCoulombEluSRDSFLR(xyzsInBohr, scaled_charge, Elu_Width*BOHRPERA, Reep, tf.cast(self.DSFAlpha, self.tf_prec), tf.cast(self.elu_alpha,self.tf_prec), tf.cast(self.elu_shift,self.tf_prec))\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)\n\t\t#dipole_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""DipoleNet"")\n\t\treturn  cc_energy, dipole, scaled_charge, dipole_wb\n\n\n\tdef train_step_EandG(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_grads_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+[PARAMS[""AddEcc""]] + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, Evdw, mol_dipole, atom_charge = self.sess.run([self.train_op_EandG, self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.Evdw,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_grads_loss += grads_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Ecc:"", Ecc, "" Etotal:"", Etotal)\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini, "" grads_loss:"", print_grads_loss/print_per_mini, "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_grads_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\n\t\t\t\t#print (""Etotal:"", Etotal, "" Ecc:"", Ecc, ""Evdw:"", Evdw)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\n\tdef train_step_dipole(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\n\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_grads_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [False] + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op_dipole, self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""ministep:  "", ministep, ""mini step time dipole:"", time.time() - t_mini )\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Ecc:"", Ecc, "" Etotal:"", Etotal)\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_grads_loss += grads_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini, "" grads_loss:"", print_grads_loss/print_per_mini, "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_grads_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\t\t\t#LOGGER.debug(""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#max_index = np.argmax(np.sum(abs(batch_data[3]-mol_dipole),axis=1))\n\t\t\t#LOGGER.debug(""real dipole:\\n"", batch_data[3][max_index], ""\\nmol_dipole:\\n"", mol_dipole[max_index], ""\\n xyz:"", batch_data[0][max_index], batch_data[1][max_index])\n\t\t\t#print (""Etotal:"", Etotal[:20], "" Ecc:"", Ecc[:20])\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\n\tdef test_dipole(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[False] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn  test_loss\n\n\tdef test_EandG(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[PARAMS[""AddEcc""]] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_grads_loss += grads_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration)\n\t\treturn  test_loss\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\t""""""\n\t\tThis the training loop for the united model.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.TrainPrepare(continue_training)\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_dipole_test_loss = float(\'inf\') # some big numbers\n\t\tmini_energy_test_loss = float(\'inf\')\n\t\tmini_test_loss = float(\'inf\')\n\t\tfor step in  range (0, mxsteps):\n\t\t\tif self.Training_Traget == ""EandG"":\n\t\t\t\tself.train_step_EandG(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_energy_loss = self.test_EandG(step)\n\t\t\t\t\tif test_energy_loss < mini_energy_test_loss:\n\t\t\t\t\t\tmini_energy_test_loss = test_energy_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\t\telif self.Training_Traget == ""Dipole"":\n\t\t\t\tself.train_step_dipole(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_dipole_loss = self.test_dipole(step)\n\t\t\t\t\tif test_dipole_loss < mini_dipole_test_loss:\n\t\t\t\t\t\tmini_dipole_test_loss = test_dipole_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\t\t\t\t\tif step >= PARAMS[""SwitchEpoch""]:\n\t\t\t\t\t\t\tself.Training_Traget = ""EandG""\n\t\t\t\t\t\t\tprint (""Switching to Energy and Gradient Learning..."")\n\t\t\telse:\n\t\t\t\tself.train_step(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_loss = self.test(step)\n\t\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef InTrainEval(self, mol_set, Rr_cut, Ra_cut, Ree_cut, step=0):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE.\n\t\t""""""\n\t\tnmols = len(mol_set.mols)\n\t\tfor i in range(nmols, self.batch_size):\n\t\t\tmol_set.mols.append(mol_set.mols[-1])\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\txyzs = np.zeros((nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(Rr_cut, Ra_cut, self.eles_np, self.eles_pairs_np)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(Ree_cut)\n\t\tbatch_data = [xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom]\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]]+[np.ones(self.nlayer+1)])\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient= self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\tmonitor_data = [Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient]\n\t\tf = open(self.name+""_monitor_""+str(step)+"".dat"",""wb"")\n\t\tpickle.dump(monitor_data, f)\n\t\tf.close()\n\t\tprint (""calculating monitoring set.."")\n\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\n\tdef print_training(self, step, loss, energy_loss, grads_loss, dipole_loss, Ncase, duration, Train=True):\n\t    if Train:\n\t        LOGGER.info(""step: %7d  duration: %.5f  train loss: %.10f  energy_loss: %.10f  grad_loss: %.10f, dipole_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)), (float(dipole_loss)/(Ncase)))\n\t    else:\n\t        LOGGER.info(""step: %7d  duration: %.5f  test loss: %.10f energy_loss: %.10f  grad_loss: %.10f, dipole_loss: %.10f"", step, duration, (float(loss)/(Ncase)), (float(energy_loss)/(Ncase)), (float(grads_loss)/(Ncase)), (float(dipole_loss)/(Ncase)))\n\t    return\n\n\tdef set_symmetry_function_params(self, prec=np.float64):\n\t\tself.elements = np.asarray(self.TData.eles)\n\t\tself.element_pairs = np.array([[self.elements[i], self.elements[j]] for i in range(len(self.elements)) for j in range(i, len(self.elements))])\n\t\tself.radial_cutoff = PARAMS[""AN1_r_Rc""]\n\t\tself.angular_cutoff = PARAMS[""AN1_a_Rc""]\n\t\tself.zeta = PARAMS[""AN1_zeta""]\n\t\tself.eta = PARAMS[""AN1_eta""]\n\n\t\t#Define radial grid parameters\n\t\tnum_radial_rs = PARAMS[""AN1_num_r_Rs""]\n\t\tself.radial_rs = self.radial_cutoff * np.linspace(0, (num_radial_rs - 1.0) / num_radial_rs, num_radial_rs)\n\n\t\t#Define angular grid parameters\n\t\tnum_angular_rs = PARAMS[""AN1_num_a_Rs""]\n\t\tnum_angular_theta_s = PARAMS[""AN1_num_a_As""]\n\t\tself.theta_s = 2.0 * np.pi * np.linspace(0, (num_angular_theta_s - 1.0) / num_angular_theta_s, num_angular_theta_s)\n\t\tself.angular_rs = self.angular_cutoff * np.linspace(0, (num_angular_rs - 1.0) / num_angular_rs, num_angular_rs)\n\t\treturn\n\n\tdef evaluate_update(self, batch_data):\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.AssignActivation()\n\t\tself.tf_precision = eval(""tf.float64"")\n\t\tself.set_symmetry_function_params()\n\t\tprint (""running john\'s symfunction\\n..."")\n\t\t#print (""self.activation_function:\\n\\n"", self.activation_function)\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Update()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Update()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]]+[np.ones(self.nlayer+1)])\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\n\tdef EvalPrepare_Update(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\t#Elep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\telements = tf.constant(self.elements, dtype = tf.int64)\n\t\t\telement_pairs = tf.constant(self.element_pairs, dtype = tf.int64)\n\t\t\tradial_rs = tf.Variable(self.radial_rs, trainable=False, dtype = self.tf_precision)\n\t\t\tangular_rs = tf.Variable(self.angular_rs, trainable=False, dtype = self.tf_precision)\n\t\t\ttheta_s = tf.Variable(self.theta_s, trainable=False, dtype = self.tf_precision)\n\t\t\tradial_cutoff = tf.constant(self.radial_cutoff, dtype = self.tf_precision)\n\t\t\tangular_cutoff = tf.constant(self.angular_cutoff, dtype = self.tf_precision)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_precision)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_precision)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = tf_symmetry_functions_2(self.xyzs_pl, self.Zs_pl, elements, element_pairs, radial_cutoff, angular_cutoff, radial_rs, angular_rs, theta_s, zeta, eta)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.bp_gradient  = tf.gradients(self.Ebp, self.xyzs_pl, name=""BPGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.AssignActivation()\n\t\t#print (""self.activation_function:\\n\\n"", self.activation_function)\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms or self.batch_size != nmol):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]]+[np.ones(self.nlayer+1)])\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\t#Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient, bp_gradient, syms= self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient, self.bp_gradient, self.Scatter_Sym], feed_dict=feed_dict)\n\t\t#print (""Etotal:"", Etotal, "" bp_gradient"", bp_gradient)\n\t\t#return Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient, bp_gradient, syms\n\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.bp_gradient  = tf.gradients(self.Ebp, self.xyzs_pl, name=""BPGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\tdef energy_inference_periodic(self, inp, indexs, cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep_e1e2, EE_cuton, EE_cutoff, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.nreal], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Ebranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Ebranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.nreal]),[self.batch_size, self.nreal])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLRWithEle(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep_e1e2)/2.0\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output\n\n\tdef dipole_inference_periodic(self, inp, indexs, xyzs, natom, Elu_Width, EE_cutoff, Reep, AddEcc, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\txyzs_real = xyzsInBohr[:,:self.nreal]\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.nreal], dtype=self.tf_prec)\n\t\tdipole_wb = []\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(charge_inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Dbranches[-1].append(self.activation_function(tf.matmul(charge_inputs, weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Dbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tdipole_wb.append(weights)\n\t\t\t\t\tdipole_wb.append(biases)\n\t\t\t\t\tDbranches[-1].append(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob[-1]), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.nreal]),[self.batch_size, self.nreal])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.nreal])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzs_real,[self.batch_size*self.nreal, 3]), tf.reshape(scaled_charge,[self.batch_size*self.nreal, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.nreal, 3]), axis=1)\n\n\t\tntess = tf.cast(tf.div(self.MaxNAtoms, self.nreal), dtype=tf.int32)\n\t\tscaled_charge_all = tf.tile(scaled_charge, [1, ntess])\n\t\tdef f1(): return TFCoulombEluSRDSFLR(xyzsInBohr, scaled_charge_all, Elu_Width*BOHRPERA, Reep, tf.cast(self.DSFAlpha, self.tf_prec), tf.cast(self.elu_alpha,self.tf_prec), tf.cast(self.elu_shift,self.tf_prec))\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)/2.0\n\t\t#dipole_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""DipoleNet"")\n\t\treturn  cc_energy, dipole, scaled_charge_all, dipole_wb\n\n\tdef fill_feed_dict_periodic(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_e1e2_pl] + [self.mil_j_pl]  + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\t@TMTiming(""EvalPeriodic"")\n\tdef evaluate_periodic(self, batch_data, nreal,DoForce = True):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.nreal = nreal\n\t\tself.activation_function_type = PARAMS[""NeuronType""]\n\t\tself.AssignActivation()\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Periodic()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare_Periodic()\n\t\tt0 = time.time()\n\t\tfeed_dict=self.fill_feed_dict_periodic(batch_data+[PARAMS[""AddEcc""]]+[np.ones(self.nlayer+1)])\n\t\tif (DoForce):\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient], feed_dict=feed_dict)\n\t\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\t\telse:\n\t\t\tEtotal = self.sess.run(self.Etotal, feed_dict=feed_dict)\n\t\t\treturn Etotal\n\n\tdef EvalPrepare_Periodic(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.mil_j_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_e1e2_pl=tf.placeholder(tf.int64, shape=tuple([None,5]),name=""RadialElectros"")\n\t\t\tself.Reep_pl = self.Reep_e1e2_pl[:,:3]\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle_Periodic(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_j_pl, self.mil_jk_pl, self.nreal)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference_periodic(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_e1e2_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\nclass MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_AvgPool(MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_AvgPool""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+self.suffix\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.avg_window_size = PARAMS[""AvgWindowSize""]\n\t\tself.chop_out = PARAMS[""ChopPadding""]\n\t\tself.Emax =  0.15\n\t\tself.Emin = -0.15\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout.Clean(self)\n\t\tself.Energy_Prob = None\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom, self.Energy_Prob = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\t@TMTiming(""energy_inference"")\n\tdef energy_inference(self, inp, indexs,  cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep, EE_cuton, EE_cutoff, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\tprob_list = []\n\t\tE_range = tf.reshape(tf.range(self.Emin, self.Emax, (self.Emax-self.Emin)/self.HiddenLayers[-1], dtype=tf.float64),[1,-1])\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1\'):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.inshape))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(inputs, keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Ebranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biaseslayer\'+str(i))\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob[i]), weights) + biases))\n\t\t\t\t\t\t\t#Ebranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_prob_sum\'):\n\t\t\t\t\tprob = tf.divide(tf.exp(Ebranches[-1][-1]), tf.reshape(tf.reduce_sum(tf.exp(Ebranches[-1][-1]), axis=-1),[-1,1]))\n\t\t\t\t\tprob_list.append(prob)\n\t\t\t\t\tprob_sqrt_tmp = tf.sqrt(prob)\n\t\t\t\t\tprob = tf.divide(prob_sqrt_tmp,tf.reshape(tf.reduce_sum(prob_sqrt_tmp,axis=1),[-1, 1]))\n\t\t\t\t\t#prob = tf.nn.softmax(Ebranches[-1][-1])\n\t\t\t\t\tweighted_energy = tf.reshape(tf.multiply(E_range, prob),[shp_in[0],-1,1,1])\n\t\t\t\t\tavg_energy = tf.cast(tf.reshape(tf.nn.avg_pool(tf.cast(weighted_energy, dtype=tf.float32),[1,1,self.avg_window_size,1], [1,1,1,1], ""SAME""),[shp_in[0],-1]), dtype=self.tf_prec)  # this is wrong, the average should be on prob instead of energy\n\t\t\t\t\tatom_energy = tf.reduce_sum(avg_energy[:,self.chop_out:-self.chop_out],axis=-1)\n\t\t\t\t\t#atom_energy = tf.reduce_sum(tf.reshape(weighted_energy[:,self.chop_out:-self.chop_out],[shp_in[0],-1]),axis=-1)\n\t\t\t\t\tEbranches[-1].append(atom_energy)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\t#cut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(Ebranches[-1][-1],[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(Ebranches[-1][-1],[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLR(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep)\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output, prob_list\n\n\tdef evaluate(self, batch_data):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tif (batch_data[0].shape[1] != self.MaxNAtoms):\n\t\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\t\tself.batch_size = nmol\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""self.batch_size:"", self.batch_size, ""  self.MaxNAtoms:"", self.MaxNAtoms)\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data+[PARAMS[""AddEcc""]]+[np.ones(self.nlayer+1)])\n\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient, energy_prob = self.sess.run([self.Etotal, self.Ebp, self.Ebp_atom, self.Ecc, self.Evdw, self.dipole, self.charge, self.gradient, self.Energy_Prob], feed_dict=feed_dict)\n\t\tprint (""Ebp_atom:"", Ebp_atom)\n\t\tprint (""energy_porb:"", energy_prob, np.savetxt(""energy_prob_H.dat"",energy_prob[0]), np.savetxt(""energy_prob_O.dat"",energy_prob[1]))\n\t\treturn Etotal, Ebp, Ebp_atom, Ecc, Evdw, mol_dipole, atom_charge, gradient\n\n\t@TMTiming(""EvalPrepare"")\n\tdef EvalPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom, self.Energy_Prob = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.summary_writer = tf.summary.FileWriter(\'./networks/PROFILE\', self.sess.graph)\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\nclass MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_InputNorm(MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_InputNorm""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+self.suffix\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.input_avg = []\n\t\tself.input_std = []\n\t\tself.Scatter_Sym_Normalize = []\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout.Clean(self)\n\t\tself.Scatter_Sym_Normalize = None\n\n\n\tdef GetAvgPrepare(self):\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tfor i in range(0, len(self.Scatter_Sym)):\n\t\t\t\tself.Scatter_Sym_Normalize.append(tf.divide(tf.subtract(self.Scatter_Sym[i], self.input_avg[i]), self.input_std[i]))\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym_Normalize, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym_Normalize, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"")\n\t\t\t#self.gradient  = tf.gradients(self.Etotal, self.xyzs_pl, name=""BPEGrad"", colocate_gradients_with_ops=True)\n#\t\t\twith tf.name_scope(""losses""):\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.grads_loss, self.dipole_loss = self.loss_op(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.grads_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.gradient, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\n\tdef get_std_avg(self, max_mini=300):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\tall_syms = [ np.array([]).reshape(0, self.inshape) for e in range(len(self.eles))]\n\t\tprint (""generating sym.."")\n\t\tfor ministep in range (0, max_mini):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tt_start = time.time()\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[False] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tprint (""sym time:"", t - t_start)\n\t\t\tscatter_sym = self.sess.run([self.Scatter_Sym], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tfor i, sym in enumerate(scatter_sym[0]):\n\t\t\t\tall_syms[i] = np.concatenate((all_syms[i], sym), axis=0)\n\t\tfor sym in all_syms:\n\t\t\tself.input_std.append(np.std(sym, axis=0))\n\t\t\tself.input_avg.append(np.mean(sym, axis=0))\n\t\tstd_avg = [self.input_std, self.input_avg]\n\t\tpickle.dump(std_avg, open(self.name+""_std_avg.dat"",""wb""))\n\t\t#print (""self.input_std:"", self.input_std)\n\t\t#print (""self.input_avg:"", self.input_avg)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\n\tdef train(self, mxsteps, continue_training= False):\n\t\t""""""\n\t\tThis the training loop for the united model.\n\t\t""""""\n\t\tLOGGER.info(""running the TFMolInstance.train()"")\n\t\tself.GetAvgPrepare()\n\t\ttest_freq = PARAMS[""test_freq""]\n\t\tmini_dipole_test_loss = float(\'inf\') # some big numbers\n\t\tmini_energy_test_loss = float(\'inf\')\n\t\tmini_test_loss = float(\'inf\')\n\t\tself.get_std_avg()\n\t\tself.TrainPrepare(continue_training)\n\t\tfor step in  range (0, mxsteps):\n\t\t\tif self.Training_Traget == ""EandG"":\n\t\t\t\tself.train_step_EandG(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_energy_loss = self.test_EandG(step)\n\t\t\t\t\tif test_energy_loss < mini_energy_test_loss:\n\t\t\t\t\t\tmini_energy_test_loss = test_energy_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\t\telif self.Training_Traget == ""Dipole"":\n\t\t\t\tself.train_step_dipole(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_dipole_loss = self.test_dipole(step)\n\t\t\t\t\tif test_dipole_loss < mini_dipole_test_loss:\n\t\t\t\t\t\tmini_dipole_test_loss = test_dipole_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\t\t\t\t\tif step >= PARAMS[""SwitchEpoch""]:\n\t\t\t\t\t\t\tself.Training_Traget = ""EandG""\n\t\t\t\t\t\t\tprint (""Switching to Energy and Gradient Learning..."")\n\t\t\telse:\n\t\t\t\tself.train_step(step)\n\t\t\t\tif step%test_freq==0 and step!=0 :\n\t\t\t\t\tif self.monitor_mset != None:\n\t\t\t\t\t\tself.InTrainEval(self.monitor_mset, self.Rr_cut, self.Ra_cut, self.Ree_off, step=step)\n\t\t\t\t\ttest_loss = self.test(step)\n\t\t\t\t\tif test_loss < mini_test_loss:\n\t\t\t\t\t\tmini_test_loss = test_loss\n\t\t\t\t\t\tself.save_chk(step)\n\t\tself.SaveAndClose()\n\t\treturn\n\n\tdef train_step_dipole(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\n\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_grads_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [False] + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, grads_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge, sym_normalize = self.sess.run([self.train_op_dipole, self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.grads_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc,  self.dipole, self.charge, self.Scatter_Sym_Normalize], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_grads_loss += grads_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini, "" grads_loss:"", print_grads_loss/print_per_mini, "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_grads_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\t\t\t#LOGGER.debug(""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#max_index = np.argmax(np.sum(abs(batch_data[3]-mol_dipole),axis=1))\n\t\t\t#LOGGER.debug(""real dipole:\\n"", batch_data[3][max_index], ""\\nmol_dipole:\\n"", mol_dipole[max_index], ""\\n xyz:"", batch_data[0][max_index], batch_data[1][max_index])\n\t\t\t#print (""Etotal:"", Etotal[:20], "" Ecc:"", Ecc[:20])\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_grads_loss += grads_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\nclass MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_Conv(MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_Conv""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.filters = PARAMS[""ConvFilter""]\n\t\tself.kernel_size = PARAMS[""ConvKernelSize""]\n\t\tself.strides = PARAMS[""ConvStrides""]\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_pl] + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\t@TMTiming(""energy_inference"")\n\tdef energy_inference(self, inp, indexs,  cc_energy, xyzs, Zs, eles, c6, R_vdw, Reep, EE_cuton, EE_cutoff, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tEbranches=[]\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tatom_outputs = []\n\t\twith tf.name_scope(""EnergyNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tEbranches.append([])\n\t\t\t\tinputs = inp[e]\n\t\t\t\tshp_in = tf.shape(inputs)\n\t\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.filters)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_conv_hidden1_energy\'):\n\t\t\t\t\t\t\tconv = tf.layers.conv2d(tf.reshape(tf.cast(inputs, dtype=tf.float32),[-1, self.inshape, 1, 1]), filters=self.filters[i], kernel_size=self.kernel_size[i], strides=self.strides[i], padding=""valid"", activation=tf.nn.relu)\n\t\t\t\t\t\t\tEbranches[-1].append(conv)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_conv_hidden\'+str(i+1)+""_energy""):\n\t\t\t\t\t\t\tconv = tf.layers.conv2d(Ebranches[-1][-1], filters=self.filters[i], kernel_size=self.kernel_size[i], strides=self.strides[i], padding=""valid"", activation=tf.nn.relu)\n\t\t\t\t\t\t\tEbranches[-1].append(conv)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_energy\'):\n\t\t\t\t\t\t\tEbranches[-1][-1] = tf.reshape(tf.cast(Ebranches[-1][-1], dtype=tf.float64), [shp_in[0], -1])\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[512 , self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(512.0))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_energy""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tEbranches[-1].append(self.activation_function(tf.matmul(Ebranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tEbranches[-1].append(tf.matmul(tf.nn.dropout(Ebranches[-1][-1], keep_prob), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Ebranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Ebranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tbp_energy = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\ttotal_energy = tf.add(bp_energy, cc_energy)\n\t\tvdw_energy = TFVdwPolyLR(xyzsInBohr, Zs, eles, c6, R_vdw, EE_cuton*BOHRPERA, Reep)\n\t\ttotal_energy_with_vdw = tf.add(total_energy, vdw_energy)\n\t\tenergy_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""EnergyNet"")\n\t\treturn total_energy_with_vdw, bp_energy, vdw_energy, energy_vars, output\n\n\t@TMTiming(""dipole_inference"")\n\tdef dipole_inference(self, inp, indexs, xyzs, natom, Elu_Width, EE_cutoff, Reep, AddEcc, keep_prob):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\txyzsInBohr = tf.multiply(xyzs,BOHRPERA)\n\t\tDbranches=[]\n\t\tatom_outputs_charge = []\n\t\toutput_charge = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\twith tf.name_scope(""DipoleNet""):\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tDbranches.append([])\n\t\t\t\tcharge_inputs = inp[e]\n\t\t\t\tcharge_shp_in = tf.shape(charge_inputs)\n\t\t\t\tcharge_index = tf.cast(indexs[e], tf.int64)\n\t\t\t\tfor i in range(len(self.filters)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_conv_hidden1_charge\'):\n\t\t\t\t\t\t\tconv = tf.layers.conv2d(tf.reshape(tf.cast(charge_inputs, dtype=tf.float32),[-1, self.inshape, 1, 1]), filters=self.filters[i], kernel_size=self.kernel_size[i], strides=self.strides[i], padding=""valid"", activation=tf.nn.relu)\n\t\t\t\t\t\t\tDbranches[-1].append(conv)\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_conv_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tconv = tf.layers.conv2d(Dbranches[-1][-1], filters=self.filters[i], kernel_size=self.kernel_size[i], strides=self.strides[i], padding=""valid"", activation=tf.nn.relu)\n\t\t\t\t\t\t\tDbranches[-1].append(conv)\n\t\t\t\tfor i in range(len(self.HiddenLayers)):\n\t\t\t\t\tif i == 0:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden1_charge\'):\n\t\t\t\t\t\t\tDbranches[-1][-1] = tf.reshape(tf.cast(Dbranches[-1][-1], dtype=tf.float64), [charge_shp_in[0], -1])\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[512 , self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(512.0)), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\t\telse:\n\t\t\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden\'+str(i+1)+""_charge""):\n\t\t\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[i-1], self.HiddenLayers[i]], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[i-1]))), var_wd=0.001)\n\t\t\t\t\t\t\tbiases = tf.Variable(tf.zeros([self.HiddenLayers[i]], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\t\t\tDbranches[-1].append(self.activation_function(tf.matmul(Dbranches[-1][-1], weights) + biases))\n\t\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear_charge\'):\n\t\t\t\t\tcharge_shp = tf.shape(charge_inputs)\n\t\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.HiddenLayers[-1], 1], var_stddev=1.0/(10+math.sqrt(float(self.HiddenLayers[-1]))), var_wd=None)\n\t\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\t\tDbranches[-1].append(tf.matmul(tf.nn.dropout(Dbranches[-1][-1], keep_prob), weights) + biases)\n\t\t\t\t\tshp_out = tf.shape(Dbranches[-1][-1])\n\t\t\t\t\tcut = tf.slice(Dbranches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\t\tatom_outputs_charge.append(rshp)\n\t\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\t\tatom_indice = tf.slice(charge_index, [0,1], [shp_out[0],1])\n\t\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\t\toutput_charge = tf.add(output_charge, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output_charge,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output_charge, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.MaxNAtoms])\n\t\t\tscaled_charge =  tf.subtract(output_charge, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzsInBohr,[self.batch_size*self.MaxNAtoms, 3]), tf.reshape(scaled_charge,[self.batch_size*self.MaxNAtoms, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.MaxNAtoms, 3]), axis=1)\n\n\t\tdef f1(): return TFCoulombEluSRDSFLR(xyzsInBohr, scaled_charge, Elu_Width*BOHRPERA, Reep, tf.cast(self.DSFAlpha, self.tf_prec), tf.cast(self.elu_alpha,self.tf_prec), tf.cast(self.elu_shift,self.tf_prec))\n\t\tdef f2(): return  tf.zeros([self.batch_size], dtype=self.tf_prec)\n\t\tcc_energy = tf.cond(AddEcc, f1, f2)/2.0\n\t\tdipole_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=""DipoleNet"")\n\t\treturn  cc_energy, dipole, scaled_charge, dipole_vars\n\n\nclass MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_NoGradTrain(MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout):\n\t""""""\n\tElectrostatic embedding Behler Parinello with van der waals interaction implemented with Grimme C6 scheme.\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tMolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.NetType = ""RawBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_NoGradTrain""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+self.suffix\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""InputCoords"")\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]),name=""InputZs"")\n\t\t\tself.Elabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]),name=""DesEnergy"")\n\t\t\tself.Dlabel_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]),name=""DesDipoles"")\n\t\t\tself.grads_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]),name=""DesGrads"")\n\t\t\tself.Radp_Ele_pl=tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Angt_Elep_pl=tf.placeholder(tf.int64, shape=tuple([None,5]))\n\t\t\tself.mil_jk_pl = tf.placeholder(tf.int64, shape=tuple([None,4]))\n\t\t\tself.Reep_pl=tf.placeholder(tf.int64, shape=tuple([None,3]),name=""RadialElectros"")\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tself.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=tuple([self.nlayer+1]))\n\t\t\t#self.keep_prob_pl =  tf.placeholder(self.tf_prec, shape=())\n\t\t\tself.AddEcc_pl = tf.placeholder(tf.bool, shape=())\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\t#SFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\t#SFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable= False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable= False, dtype = self.tf_prec)\n\t\t\tRr_cut = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_on = tf.Variable(self.Ree_on, trainable=False, dtype = self.tf_prec)\n\t\t\telu_width  = tf.Variable(self.elu_width, trainable=False, dtype = self.tf_prec)\n\t\t\tRee_off = tf.Variable(self.Ree_off, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tC6 = tf.Variable(self.C6,trainable=False, dtype = self.tf_prec)\n\t\t\tvdw_R = tf.Variable(self.vdw_R,trainable=False, dtype = self.tf_prec)\n\t\t\t#self.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear(self.xyzs_pl, self.Zs_pl, Ele, self.SFPr2_vary, Rr_cut, Elep, self.SFPa2_vary, zeta, eta, Ra_cut, self.Radp_pl, self.Angt_pl)\n#\t\t\twith tf.name_scope(""MakeDescriptors""):\n\t\t\t#with tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\t#with tf.device(\'/cpu:0\'):\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Linear_WithEle(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut, self.Radp_Ele_pl, self.Angt_Elep_pl, self.mil_jk_pl)\n\t\t\tself.Ecc, self.dipole, self.charge, self.dipole_wb = self.dipole_inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, elu_width, Ree_off, self.Reep_pl, self.AddEcc_pl, self.keep_prob_pl)\n\t\t\tself.Radp_pl  = self.Radp_Ele_pl[:,:3]\n#\t\t\twith tf.name_scope(""behler""):\n\t\t\tself.Etotal, self.Ebp, self.Evdw,  self.energy_wb, self.Ebp_atom = self.energy_inference(self.Scatter_Sym, self.Sym_Index, self.Ecc, self.xyzs_pl, self.Zs_pl, Ele, C6, vdw_R, self.Reep_pl, Ree_on, Ree_off, self.keep_prob_pl)\n\t\t\t#self.Etotal,  self.energy_wb = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl, Ree_on, Ree_off, self.Reep_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss, self.energy_loss, self.dipole_loss = self.loss_op(self.Etotal, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.dipole_loss_dipole = self.loss_op_dipole(self.Etotal, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\tself.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.dipole_loss_EandG = self.loss_op_EandG(self.Etotal, self.dipole, self.Elabel_pl, self.grads_pl, self.Dlabel_pl, self.natom_pl)\n\t\t\ttf.summary.scalar(""loss"", self.loss)\n\t\t\ttf.summary.scalar(""loss_dip"", self.loss_dipole)\n\t\t\ttf.summary.scalar(""loss_EG"", self.loss_EandG)\n#\t\t\twith tf.name_scope(""training""):\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum, )\n\t\t\tself.train_op_dipole = self.training(self.total_loss_dipole, self.learning_rate_dipole, self.momentum, self.dipole_wb)\n\t\t\tself.train_op_EandG = self.training(self.total_loss_EandG, self.learning_rate_energy, self.momentum, self.energy_wb)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t# please do not use the totality of the GPU memory\n\t\t\tconfig=tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.90\n\t\t\tself.sess = tf.Session(config=config)\n\t\t\tself.saver = tf.train.Saver(max_to_keep = self.max_checkpoints)\n\t\t\tself.sess.run(init)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tif (PARAMS[""Profiling""]>0):\n\t\t\t\tprint(""logging with FULL TRACE"")\n\t\t\t\tself.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t\tself.run_metadata = tf.RunMetadata()\n\t\t\t\tself.summary_writer.add_run_metadata(self.run_metadata, ""init"", global_step=None)\n\t\t\tself.sess.graph.finalize()\n\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(batch_data[2]),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.Elabel_pl] + [self.Dlabel_pl] + [self.grads_pl] + [self.Radp_Ele_pl] + [self.Angt_Elep_pl] + [self.Reep_pl] + [self.mil_jk_pl] + [self.natom_pl] + [self.AddEcc_pl] + [self.keep_prob_pl], batch_data)}\n\t\treturn feed_dict\n\n\n\tdef train_step_EandG(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)+[PARAMS[""AddEcc""]] + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss, dipole_loss,  Etotal, Ecc, Evdw, mol_dipole, atom_charge = self.sess.run([self.train_op_EandG, self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.Evdw,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Ecc:"", Ecc, "" Etotal:"", Etotal)\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini, "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\n\t\t\t\t#print (""Etotal:"", Etotal, "" Ecc:"", Ecc, ""Evdw:"", Evdw)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\n\tdef train_step_dipole(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_dipole_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\n\n\t\tprint_per_mini = 100\n\t\tprint_loss = 0.0\n\t\tprint_energy_loss = 0.0\n\t\tprint_dipole_loss = 0.0\n\t\tprint_grads_loss = 0.0\n\t\tprint_time = 0.0\n\t\ttime_print_mini = time.time()\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tt_mini = time.time()\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size) + [False] + [self.keep_prob]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_2, total_loss_value, loss_value, energy_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.train_op_dipole, self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole, self.dipole_loss_dipole, self.Etotal, self.Ecc,  self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""ministep:  "", ministep, ""mini step time dipole:"", time.time() - t_mini )\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""Ecc:"", Ecc, "" Etotal:"", Etotal)\n\t\t\tprint_loss += loss_value\n\t\t\tprint_energy_loss += energy_loss\n\t\t\tprint_dipole_loss += dipole_loss\n\t\t\tif (ministep%print_per_mini == 0 and ministep!=0):\n\t\t\t\tprint (""time:"", (time.time() - time_print_mini)/print_per_mini ,  "" loss_value: "",  print_loss/print_per_mini, "" energy_loss:"", print_energy_loss/print_per_mini,  "" dipole_loss:"", print_dipole_loss/print_per_mini)\n\t\t\t\tprint_loss = 0.0\n\t\t\t\tprint_energy_loss = 0.0\n\t\t\t\tprint_dipole_loss = 0.0\n\t\t\t\tprint_time = 0.0\n\t\t\t\ttime_print_mini = time.time()\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\ttrain_energy_loss += energy_loss\n\t\t\ttrain_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\tself.print_training(step, train_loss, train_energy_loss, train_grads_loss, train_dipole_loss, num_of_mols, duration)\n\t\treturn\n\n\n\tdef test_dipole(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\ttest_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[False] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss,  dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss_dipole, self.loss_dipole, self.energy_loss_dipole,  self.dipole_loss_dipole, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration)\n\t\t#self.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn  test_loss\n\n\tdef test_EandG(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_test = self.TData.NTest\n\t\tstart_time = time.time()\n\t\ttest_loss =  0.0\n\t\ttest_energy_loss = 0.0\n\t\ttest_dipole_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTestBatch(self.batch_size)+[PARAMS[""AddEcc""]] + [np.ones(self.nlayer+1)]\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\ttotal_loss_value, loss_value, energy_loss, dipole_loss,  Etotal, Ecc, mol_dipole, atom_charge = self.sess.run([self.total_loss_EandG, self.loss_EandG, self.energy_loss_EandG, self.dipole_loss_EandG, self.Etotal, self.Ecc, self.dipole, self.charge], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value: "", loss_value, "" energy_loss:"", energy_loss, "" grads_loss:"", grads_loss, "" dipole_loss:"", dipole_loss)\n\t\t\t#print (""energy_wb[1]:"", energy_wb[1], ""\\ndipole_wb[1]"", dipole_wb[1])\n\t\t\t#print (""charge:"", atom_charge )\n\t\t\ttest_loss = test_loss + loss_value\n\t\t\ttest_energy_loss += energy_loss\n\t\t\ttest_dipole_loss += dipole_loss\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\tprint (""testing..."")\n\t\tself.print_training(step, test_loss, test_energy_loss, test_grads_loss, test_dipole_loss, num_of_mols, duration)\n\t\treturn  test_loss\n\n\tdef loss_op(self, energy, dipole, Elabels, grads, Dlabels, natom):\n\t\tmaxatom=tf.cast(self.MaxNAtoms, self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels,name=""EnDiff""), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff,name=""EnL2"")\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels,name=""DipoleDiff""), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff,name=""DipL2"")\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tloss = tf.add(energy_loss, tf.multiply(dipole_loss, self.DipoleScalar))\n\t\t#loss = tf.identity(dipole_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, dipole_loss\n\n\tdef loss_op_dipole(self, energy,  dipole, Elabels, grads, Dlabels, natom):\n\t\tmaxatom=tf.cast(self.MaxNAtoms, self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\t#loss = tf.multiply(grads_loss, energy_loss)\n\t\tloss = tf.identity(dipole_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, dipole_loss\n\n\tdef loss_op_EandG(self, energy, dipole, Elabels, grads, Dlabels, natom):\n\t\tmaxatom=tf.cast(self.MaxNAtoms, self.tf_prec)\n\t\tenergy_diff  = tf.multiply(tf.subtract(energy, Elabels), natom*maxatom)\n\t\tenergy_loss = tf.nn.l2_loss(energy_diff)\n\t\tdipole_diff = tf.multiply(tf.subtract(dipole, Dlabels), tf.reshape(natom*maxatom,[self.batch_size,1]))\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\tloss = tf.identity(energy_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss, energy_loss, dipole_loss\n'"
TensorMol/TFNetworks/TFMolInstanceEE.py,265,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .TFInstance import *\nfrom ..Containers.TensorMolData import *\nfrom .TFMolInstance import *\nfrom ..ForceModels.ElectrostaticsTF import *\nfrom .TFMolInstanceDirect import *\n\nclass MolInstance_BP_Dipole(MolInstance_fc_sqdiff_BP):\n\t""""""\n\t\tCalculate the Dipole of Molecules\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True):\n\t\t""""""\n\t\tRaise a Behler-Parinello TensorFlow instance.\n\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""fc_sqdiff_BP""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+str(self.TData.order)+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tself.learning_rate = 0.0001\n\t\t#self.learning_rate = 0.00001\n\t\tself.momentum = 0.95\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\t\t# Using multidimensional inputs creates all sorts of issues; for the time being only support flat inputs.\n\t\t\tself.inshape = np.prod(self.TData.dig.eshape)\n\t\t# HACK something was up with the shapes in kun\'s saved network...\n\t\t#I do not need that..\n\t\t#self.inshape = self.inshape[0]\n\t\tprint(""MolInstance_BP_Dipole.inshape: "",self.inshape)\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.MeanStoich = self.TData.MeanStoich # Average stoichiometry of a molecule.\n\t\tself.MeanNumAtoms = np.sum(self.MeanStoich)\n\t\tself.AtomBranchNames=[] # a list of the layers named in each atom branch\n\n\t\tself.netcharge_output = None\n\t\tself.dipole_output = None\n\t\tself.inp_pl=None\n\t\tself.mats_pl=None\n\t\tself.coords = None\n\t\tself.label_pl=None\n\n\t\t# self.batch_size is still the number of inputs in a batch.\n\t\tself.batch_size = 10000\n\t\tself.batch_size_output = 0\n\t\tself.hidden1 = 100\n\t\tself.hidden2 = 100\n\t\tself.hidden3 = 100\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\n\tdef Clean(self):\n\t\tMolInstance_fc_sqdiff_BP.Clean(self)\n\t\tself.coords_pl = None\n\t\tself.netcharge_output = None\n\t\tself.dipole_output = None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tprint(""self.MeanNumAtoms: "",self.MeanNumAtoms)\n\t\t# allow for 120% of required output space, since it\'s cheaper than input space to be padded by zeros.\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\t#self.TData.CheckBPBatchsizes(self.batch_size, self.batch_size_output)\n\t\tprint(""Assigned batch input size: "",self.batch_size)\n\t\tprint(""Assigned batch output size in BP_Dipole:"",self.batch_size_output)\n\t\twith tf.Graph().as_default():\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tself.coords_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.batch_size_output])))\n\t\t\t\tself.coords_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None, 3])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 4]))\n\t\t\tself.netcharge_output, self.dipole_output, self.atom_outputs = self.inference(self.inp_pl, self.mats_pl, self.coords_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.netcharge_output, self.dipole_output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\t#self.summary_op = tf.summary.merge_all()\n\t\t\t#init = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver()\n\t\t\ttry: # I think this may be broken\n\t\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n\t\t\t\tif (len(metafiles)>0):\n\t\t\t\t\tmost_recent_meta_file=metafiles[0]\n\t\t\t\t\tLOGGER.info(""Restoring training from Metafile: ""+most_recent_meta_file)\n\t\t\t\t\t#Set config to allow soft device placement for temporary fix to known issue with Tensorflow up to version 0.12 atleast - JEH\n\t\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n\t\t\t\t\tself.sess = tf.Session(config=config)\n\t\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n\t\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n\t\t\texcept Exception as Ex:\n\t\t\t\tprint(""Restore Failed"",Ex)\n\t\t\t\tpass\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef loss_op(self, netcharge_output, dipole_output, labels):\n\t\t""""""\n\t\ttotal_loss = scaler*l2(netcharge) + l2(dipole)\n\t\t""""""\n\t\tcharge_labels = tf.slice(labels, [0, 0], [self.batch_size_output,1])\n\t\tdipole_labels = tf.slice(labels, [0, 1], [self.batch_size_output,3])\n\t\tcharge_diff  = tf.subtract(netcharge_output, charge_labels)\n\t\tdipole_diff  = tf.subtract(dipole_output, dipole_labels)\n\t\tcharge_loss = tf.nn.l2_loss(charge_diff)\n\t\tdipole_loss = tf.nn.l2_loss(dipole_diff)\n\t\tloss = tf.add(charge_loss, dipole_loss)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef inference(self, inp_pl, mats_pl, coords_pl):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph which also matches monopole,3-dipole, and 9-quadropole elements.\n\n\t\t- It has the shape of two energy networks in parallel.\n\t\t- One network produces the energy, the other produces the charge on an atom.\n\t\t- The charges are constrained to reproduce the molecular multipoles.\n\t\t- The energy and charge are together constrained to produce the molecular energy.\n\t\t- The same atomic linear transformation is used to produce the charges as the energy.\n\t\t- All multipoles have the form of a dot product with atomic charges. That\'s pre-computed externally\n\t\tThe attenuated coulomb energy has the form of a per-molecule vector-matrix-vector product.\n\t\tAnd it is generated as well by this routine.\n\n\t\tArgs:\n\t\t\t\tinp_pl: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\t\tmats_pl: a list of (num_of atom type X batchsize) matrices which linearly combines the elements to give molecular outputs.\n\t\t\t\tmul_pl: Multipole inputs (see Mol::GenerateMultipoleInputs)\n\t\tReturns:\n\t\t\tAtom BP Energy, Atom Charges\n\t\t\tI\'m thinking about doing the contractions for the multipoles and electrostatic energy loss in loss_op... haven\'t settled on it yet.\n\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\t\tnetcharge_output = tf.zeros([self.batch_size_output, 1], dtype=self.tf_prec)\n\t\tdipole_output = tf.zeros([self.batch_size_output, 3], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tLOGGER.info(""Norms: %f,%f,%f"", nrm1,nrm2,nrm3)\n\t\t#print(inp_pl)\n\t\t#tf.Print(inp_pl, [inp_pl], message=""This is input: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(bnds_pl, [bnds_pl], message=""bnds_pl: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(mats_pl, [mats_pl], message=""mats_pl: "",first_n=10000000,summarize=100000000)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp_pl[e]\n\t\t\tmats = mats_pl[e]\n\t\t\tcoords = coords_pl[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tshp_coords = tf.shape(coords)\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tmats_shape = tf.shape(mats)\n\t\t\t\ttf.Print(tf.to_float(mats_shape), [tf.to_float(mats_shape)], message=""Element ""+str(e)+""mats shape "",first_n=10000000,summarize=100000000)\n\t\t\t\ttf.Print(tf.to_float(shp_coords), [tf.to_float(shp_coords)], message=""Element ""+str(e)+""coords shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units],dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.nn.relu(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units],dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.nn.relu(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units],dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.nn.relu(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1],dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\tcoords_rshp = tf.transpose(coords)\n\t\t\t\tcoords_rshp_shape = tf.shape(coords_rshp)\n\n\t\t\t\tdipole_tmp = tf.multiply(rshp, coords_rshp)\n\t\t\t\tdipole_tmp = tf.reshape(dipole_tmp,[3, shp_out[0]])\n\t\t\t\tnetcharge = tf.matmul(rshp,mats)\n\t\t\t\tdipole = tf.matmul(dipole_tmp, mats)\n\t\t\t\tnetcharge = tf.transpose(netcharge)\n\t\t\t\tdipole = tf.transpose(dipole)\n\t\t\t\tnetcharge_output = tf.add(netcharge_output, netcharge)\n\t\t\t\tdipole_output = tf.add(dipole_output, dipole)\n\t\ttf.verify_tensor_all_finite(netcharge_output,""Nan in output!!!"")\n\t\ttf.verify_tensor_all_finite(dipole_output,""Nan in output!!!"")\n\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\treturn netcharge_output, dipole_output, atom_outputs\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tfor e in range(len(self.eles)):\n\t\t\tif (not np.all(np.isfinite(batch_data[0][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit1"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\t\tif (not np.all(np.isfinite(batch_data[1][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit3"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\t\tif (not np.all(np.isfinite(batch_data[2][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit3"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\tif (not np.all(np.isfinite(batch_data[3]),axis=(0,1))):\n\t\t\tprint(""I was fed shit4"")\n\t\t\traise Exception(""DontEatShit"")\n\t\t#feed_dict={i: d for i, d in zip(self.inp_pl+self.mats_pl + self.coords_pl, batch_data[0]+batch_data[1] +  batch_data[2])}\n\t\tfeed_dict={i: d for i, d in zip(self.inp_pl+self.mats_pl+self.coords_pl+[self.label_pl], batch_data[0]+batch_data[1]+ batch_data[2] + [batch_data[3]])}\n\t\treturn feed_dict\n\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep: "", ministep, "" Ncase_train:"", Ncase_train, "" self.batch_size"", self.batch_size)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size,self.batch_size_output)\n\t\t\t#new_coords = np.zeros((batch_data[4].shape[0], batch_data[4].shape[1]+1))\n\t\t\t#new_coords[:,0] = 0\n\t\t\t#new_coords[:,1:] = batch_data[4]\n\t\t\t#batch_data[4] = new_coords\n\t\t\t#batch_data = batch_data[:3] + [batch_data[4]]\n\t\t\t#print (""checking shape:"", batch_data[2][0].shape, batch_data[2][1].shape, batch_data[2][2].shape, batch_data[2][3].shape)\n\t\t\t#print (""checking shape, input:"", batch_data[0][0].shape, batch_data[0][1].shape, batch_data[0][2].shape, batch_data[0][3].shape)\n\t\t\tactual_mols  = np.count_nonzero(np.any(batch_data[3][1:], axis=1))\n\t\t\tdump_, dump_2, total_loss_value, loss_value, netcharge_output, dipole_output = self.sess.run([self.check, self.train_op, self.total_loss, self.loss, self.netcharge_output, self.dipole_output], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#print (""atom_outputs:"", atom_outputs, "" mol outputs:"", mol_output)\n\t\t\t#print (""atom_outputs shape:"", atom_outputs[0].shape, "" mol outputs"", mol_output.shape)\n\t\t#print(""train diff:"", (mol_output[0]-batch_data[2])[:actual_mols], np.sum(np.square((mol_output[0]-batch_data[2])[:actual_mols])))\n\t\t#print (""train_loss:"", train_loss, "" Ncase_train:"", Ncase_train, train_loss/num_of_mols)\n\t\t#print (""diff:"", mol_output - batch_data[2], "" shape:"", mol_output.shape)\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size,self.batch_size_output)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = np.count_nonzero(np.any(batch_data[3][1:], axis=1))\n\t\t\ttotal_loss_value, loss_value, netcharge_output, dipole_output, atom_outputs = self.sess.run([self.total_loss, self.loss, self.netcharge_output, self.dipole_output, self.atom_outputs],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\t\tprint (""testing result:"")\n\t\tprint (""acurrate charge, dipole:"", batch_data[3][:20])\n\t\tprint (""predict dipole"", dipole_output[:20])\n\t\t#print (""charge sum:"", netcharge_output)\n\t\t#print (""charges: "",  atom_outputs)\n\t\tduration = time.time() - start_time\n\t\t#print( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\t#self.TData.dig.EvaluateTestOutputs(batch_data[2],preds)\n\t\treturn test_loss, feed_dict\n\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  train loss: "", ""%.10f""%(float(loss)/(Ncase)))\n\t\telse:\n\t\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  test loss: "", ""%.10f""%(float(loss)/(NCase)))\n\t\treturn\n\n\tdef evaluate(self, batch_data):   #this need to be modified\n\t\t# Check sanity of input\n\t\tnmol = batch_data[3].shape[0]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[3].shape[0])\n\t\tself.batch_size_output = nmol\n\t\tif not self.sess:\n\t\t\tLOGGER.info(""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\tnetcharge, dipole, total_loss_value, loss_value,  atom_outputs = self.sess.run([self.netcharge_output, self.dipole_output, self.total_loss, self.loss, self.atom_outputs],  feed_dict=feed_dict)\n\t\treturn netcharge, dipole/AUPERDEBYE, atom_outputs\n\n\tdef EvalPrepare(self):\n\t\tif (isinstance(self.inshape,tuple)):\n\t\t\tif (len(self.inshape)>1):\n\t\t\t\traise Exception(""My input should be flat"")\n\t\t\telse:\n\t\t\t\tself.inshape = self.inshape[0]\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tself.coords_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None, self.batch_size_output])))\n\t\t\t\tself.coords_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None, 3])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 4]))\n\t\t\tself.netcharge_output, self.dipole_output, self.atom_outputs = self.inference(self.inp_pl, self.mats_pl, self.coords_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.netcharge_output, self.dipole_output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\treturn\n\n\tdef Prepare(self):\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tself.coords = []\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.batch_size_output])))\n\t\t\t\tself.coords_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None, 3])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 4]))\n\t\t\tself.netcharge_output, self.dipole_output, self.atom_outputs = self.inference(self.inp_pl, self.mats_pl, self.coords_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.netcharge_output, self.dipole_out, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\treturn\n\n\n\n\nclass MolInstance_BP_Dipole_2(MolInstance_BP_Dipole):\n\t""""""\n\t\tCalculate the Dipole of Molecules\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True):\n\t\t""""""\n\t\tRaise a Behler-Parinello TensorFlow instance.\n\n\t\tArgs:\n\t\t\tTData_: A TensorMolData instance.\n\t\t\tName_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""fc_sqdiff_BP""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+str(self.TData.order)+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\t#self.learning_rate = 0.0001\n\t\tself.momentum = 0.95\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\t\t# Using multidimensional inputs creates all sorts of issues; for the time being only support flat inputs.\n\t\t\tself.inshape = np.prod(self.TData.dig.eshape)\n\t\t# HACK something was up with the shapes in kun\'s saved network...\n\t\t#I do not need that..\n\t\t#self.inshape = self.inshape[0]\n\t\tprint(""MolInstance_BP_Dipole.inshape: "",self.inshape)\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.MeanStoich = self.TData.MeanStoich # Average stoichiometry of a molecule.\n\t\tself.MeanNumAtoms = np.sum(self.MeanStoich)\n\t\tself.AtomBranchNames=[] # a list of the layers named in each atom branch\n\n\t\tself.netcharge_output = None\n\t\tself.dipole_output = None\n\t\tself.inp_pl=None\n\t\tself.mats_pl=None\n\t\tself.coords = None\n\t\tself.label_pl=None\n\t\tself.natom_pl = None\n\t\tself.charge_gradient = None\n\t\tself.output_list = None\n\t\tself.unscaled_atom_outputs = None\n\n\t\t# self.batch_size is still the number of inputs in a batch.\n\t\tself.batch_size = 10000\n\t\tself.batch_size_output = 0\n\t\t#self.hidden1 = 500\n\t\t#self.hidden2 = 500\n\t\t#self.hidden3 = 500\n\t\tself.summary_op =None\n\t\tself.summary_writer=None\n\n\n\tdef Clean(self):\n\t\tMolInstance_BP_Dipole.Clean(self)\n\t\tself.natom_pl = None\n\t\tself.net_charge = None\n\t\tself.charge_gradient = None\n\t\tself.unscaled_atom_outputs = None\n\t\tself.output_list = None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tprint(""self.MeanNumAtoms: "",self.MeanNumAtoms)\n\t\t# allow for 120% of required output space, since it\'s cheaper than input space to be padded by zeros.\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\t#self.TData.CheckBPBatchsizes(self.batch_size, self.batch_size_output)\n\t\tprint(""Assigned batch input size: "",self.batch_size)\n\t\tprint(""Assigned batch output size in BP_Dipole:"",self.batch_size_output)\n\t\twith tf.Graph().as_default():\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tself.coords_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.batch_size_output])))\n\t\t\t\tself.coords_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None, 3])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 1]))\n\t\t\tself.dipole_output, self.atom_outputs, self.unscaled_atom_outputs, self.net_charge  = self.inference(self.inp_pl, self.mats_pl, self.coords_pl, self.natom_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.dipole_output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver()\n#\t\t\ttry: # I think this may be broken\n#\t\t\t\tmetafiles = [x for x in os.listdir(self.train_dir) if (x.count(\'meta\')>0)]\n#\t\t\t\tif (len(metafiles)>0):\n#\t\t\t\t\tmost_recent_meta_file=metafiles[0]\n#\t\t\t\t\tLOGGER.info(""Restoring training from Metafile: ""+most_recent_meta_file)\n#\t\t\t\t\t#Set config to allow soft device placement for temporary fix to known issue with Tensorflow up to version 0.12 atleast - JEH\n#\t\t\t\t\tconfig = tf.ConfigProto(allow_soft_placement=True)\n#\t\t\t\t\tself.sess = tf.Session(config=config)\n#\t\t\t\t\tself.saver = tf.train.import_meta_graph(self.train_dir+\'/\'+most_recent_meta_file)\n#\t\t\t\t\tself.saver.restore(self.sess, tf.train.latest_checkpoint(self.train_dir))\n#\t\t\texcept Exception as Ex:\n#\t\t\t\tprint(""Restore Failed"",Ex)\n#\t\t\t\tpass\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\treturn\n\n\tdef loss_op(self, dipole_output, labels):\n\t\t""""""\n\t\ttotal_loss =  l2(dipole)\n\t\t""""""\n\t\tdipole_diff  = tf.subtract(dipole_output, labels)\n\t\tloss = tf.nn.l2_loss(dipole_diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\tdef inference(self, inp_pl, mats_pl, coords_pl, natom_pl):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph which also matches monopole,3-dipole, and 9-quadropole elements.\n\n\t\t- It has the shape of two energy networks in parallel.\n\t\t- One network produces the energy, the other produces the charge on an atom.\n\t\t- The charges are constrained to reproduce the molecular multipoles.\n\t\t- The energy and charge are together constrained to produce the molecular energy.\n\t\t- The same atomic linear transformation is used to produce the charges as the energy.\n\t\t- All multipoles have the form of a dot product with atomic charges. That\'s pre-computed externally\n\t\tThe attenuated coulomb energy has the form of a per-molecule vector-matrix-vector product.\n\t\tAnd it is generated as well by this routine.\n\n\t\tArgs:\n\t\t\t\tinp_pl: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\t\tmats_pl: a list of (num_of atom type X batchsize) matrices which linearly combines the elements to give molecular outputs.\n\t\t\t\tmul_pl: Multipole inputs (see Mol::GenerateMultipoleInputs)\n\t\tReturns:\n\t\t\tAtom BP Energy, Atom Charges\n\t\t\tI\'m thinking about doing the contractions for the multipoles and electrostatic energy loss in loss_op... haven\'t settled on it yet.\n\n\t\t""""""\n\t\t# convert the index matrix from bool to float\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\t\tnetcharge_output = tf.zeros([self.batch_size_output, 1],dtype=self.tf_prec)\n\t\tscaled_netcharge_output = tf.zeros([1, self.batch_size_output],dtype=self.tf_prec)\n\t\tdipole_output = tf.zeros([self.batch_size_output, 3],dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tLOGGER.info(""Norms: %f,%f,%f"", nrm1,nrm2,nrm3)\n\t\t#print(inp_pl)\n\t\t#tf.Print(inp_pl, [inp_pl], message=""This is input: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(bnds_pl, [bnds_pl], message=""bnds_pl: "",first_n=10000000,summarize=100000000)\n\t\t#tf.Print(mats_pl, [mats_pl], message=""mats_pl: "",first_n=10000000,summarize=100000000)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp_pl[e]\n\t\t\tmats = mats_pl[e]\n\t\t\tcoords = coords_pl[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tshp_coords = tf.shape(coords)\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tmats_shape = tf.shape(mats)\n\t\t\t\ttf.Print(tf.to_float(mats_shape), [tf.to_float(mats_shape)], message=""Element ""+str(e)+""mats shape "",first_n=10000000,summarize=100000000)\n\t\t\t\ttf.Print(tf.to_float(shp_coords), [tf.to_float(shp_coords)], message=""Element ""+str(e)+""coords shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units],dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.nn.relu(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units],dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.nn.relu(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units],dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.nn.relu(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1],dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\tnetcharge = tf.matmul(rshp,mats)\n\t\t\t\tnetcharge = tf.transpose(netcharge)\n\t\t\t\tnetcharge_output = tf.add(netcharge_output, netcharge)\n\n\t\tdelta_charge = tf.multiply(netcharge_output, natom_pl)\n\t\tdelta_charge = tf.transpose(delta_charge)\n\t\t#total_atom = 0\n\t\tscaled_charge_list = []\n\t\t#for e in range(len(self.eles)):\n\t\t#\ttotal_atom = total_atom + tf.shape(atom_outputs[e])[1]\n\t\t#scaled_charge_list = tf.zeros([total_atom])\n\t\t#pointer = 0\n\t\tfor e in range(len(self.eles)):\n\t\t\tmats = mats_pl[e]\n\t\t\tshp_out = tf.shape(atom_outputs[e])\n\t\t\tcoords = coords_pl[e]\n\t\t\ttrans_mats = tf.transpose(mats)\n\t\t\tele_delta_charge = tf.matmul(delta_charge, trans_mats)\n\t\t\tscaled_charge = tf.subtract(atom_outputs[e], ele_delta_charge)\n\t\t\t#num_rows, natom =scaled_charge.get_shape().as_list()\n\t\t\t#scaled_charge_rshp = tf.reshape(scaled_charge, [shp_out[1]])\n\t\t\t#indices = range(pointer, pointer+num_rows)\n\t\t\t#scaled_charge_list = tf.scatter_update(scaled_charge_list, indices, scaled_charge_rshp)\n\t\t\tscaled_charge_list.append(scaled_charge)\n\t\t\tscaled_netcharge = tf.matmul(scaled_charge,mats)\n\t\t\tscaled_netcharge_output = tf.add(scaled_netcharge_output, scaled_netcharge)\n\t\t\tcoords_rshp = tf.transpose(coords)\n\t\t\tdipole_tmp = tf.multiply(scaled_charge, coords_rshp)\n\t\t\tdipole_tmp = tf.reshape(dipole_tmp, [3, shp_out[1]])\n\t\t\tdipole = tf.matmul(dipole_tmp, mats)\n\t\t\tdipole = tf.transpose(dipole)\n\t\t\tdipole_output = tf.add(dipole_output, dipole)\n\t\t\t#pointer = pointer + natom\n\t\ttf.verify_tensor_all_finite(netcharge_output,""Nan in output!!!"")\n\t\ttf.verify_tensor_all_finite(dipole_output,""Nan in output!!!"")\n\t\t#tf.Print(output, [output], message=""This is output: "",first_n=10000000,summarize=100000000)\n\t\t#return  dipole_output, scaled_charge_list,  scaled_netcharge_output#atom_outputs\n\t\treturn  dipole_output, scaled_charge_list, atom_outputs, scaled_netcharge_output#atom_outputs\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tfor e in range(len(self.eles)):\n\t\t\tif (not np.all(np.isfinite(batch_data[0][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit1"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\t\tif (not np.all(np.isfinite(batch_data[1][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit3"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\t\tif (not np.all(np.isfinite(batch_data[2][e]),axis=(0,1))):\n\t\t\t\tprint(""I was fed shit3"")\n\t\t\t\traise Exception(""DontEatShit"")\n\t\t#if (not np.all(np.isfinite(batch_data[3]),axis=(0,1))):\n\t\t#\tprint(""I was fed shit4"")\n\t\t#\traise Exception(""DontEatShit"")\n\t\tif (not np.all(np.isfinite(batch_data[4]),axis=(0,1))):\n\t\t\tprint(""I was fed shit5"")\n\t\t\traise Exception(""DontEatShit"")\n\t\t#feed_dict={i: d for i, d in zip(self.inp_pl+self.mats_pl + self.coords_pl, batch_data[0]+batch_data[1] +  batch_data[2])}\n\t\tfeed_dict={i: d for i, d in zip(self.inp_pl+self.mats_pl+self.coords_pl+[self.natom_pl]+[self.label_pl], batch_data[0]+batch_data[1]+ batch_data[2] + [batch_data[3]] + [batch_data[4]])}\n\t\t#print (""batch_data"", batch_data)\n\t\treturn feed_dict\n\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep: "", ministep, "" Ncase_train:"", Ncase_train, "" self.batch_size"", self.batch_size)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size,self.batch_size_output)\n\t\t\t#print (batch_data)\n\t\t\t#print (""checking shape:"", batch_data[2][0].shape, batch_data[2][1].shape, batch_data[2][2].shape, batch_data[2][3].shape)\n\t\t\t#print (""checking shape, input:"", batch_data[0][0].shape, batch_data[0][1].shape, batch_data[0][2].shape, batch_data[0][3].shape)\n\t\t\tactual_mols  = np.count_nonzero(np.any(batch_data[3][1:], axis=1))\n\t\t\tdump_, dump_2, total_loss_value, loss_value,  dipole_output, atom_outputs, net_charge = self.sess.run([self.check, self.train_op, self.total_loss, self.loss, self.dipole_output, self.atom_outputs, self.net_charge],  feed_dict=self.fill_feed_dict(batch_data))\n\n\n\t\t\t#dump_2, total_loss_value, loss_value, dipole_output = self.sess.run([self.train_op, self.total_loss, self.loss,  self.dipole_output], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#print (""atom_outputs:"", atom_outputs, "" mol outputs:"", mol_output)\n\t\t\t#print (""atom_outputs shape:"", atom_outputs[0].shape, "" mol outputs"", mol_output.shape)\n\t\t#print(""train diff:"", (mol_output[0]-batch_data[2])[:actual_mols], np.sum(np.square((mol_output[0]-batch_data[2])[:actual_mols])))\n\t\t#print (""train_loss:"", train_loss, "" Ncase_train:"", Ncase_train, train_loss/num_of_mols)\n\t\t#print (""diff:"", mol_output - batch_data[2], "" shape:"", mol_output.shape)\n\t\tself.print_training(step, train_loss, num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size,self.batch_size_output)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = np.count_nonzero(np.any(batch_data[3][1:], axis=1))\n\t\t\ttotal_loss_value, loss_value,  dipole_output, atom_outputs, net_charge  = self.sess.run([self.total_loss, self.loss, self.dipole_output, self.atom_outputs, self.net_charge],  feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\t\t#print (""net charge:"", net_charge)\n\t\t#print (""predict charge:"", atom_outputs[0])\n\t\tprint (""acurrate charge, dipole:"", batch_data[4][:20], "" dipole shape:"", batch_data[4].shape)\n\t\tprint (""predict dipole"", dipole_output[:20])\n\t\t#print (""charge sum:"", netcharge_output)\n\t\t#print (""charges: "",  atom_outputs)\n\t\tduration = time.time() - start_time\n\t\t#print( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\t#self.TData.dig.EvaluateTestOutputs(batch_data[2],preds)\n\t\treturn test_loss, feed_dict\n\n\tdef print_training(self, step, loss, Ncase, duration, Train=True):\n\t\tif Train:\n\t\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  train loss: "", ""%.10f""%(float(loss)/(Ncase)))\n\t\telse:\n\t\t\tprint(""step: "", ""%7d""%step, ""  duration: "", ""%.5f""%duration,  ""  test loss: "", ""%.10f""%(float(loss)/(NCase)))\n\t\treturn\n\n\tdef evaluate(self, batch_data, IfChargeGrad =  False):   #this need to be modified\n\t\t# Check sanity of input\n\t\tnmol = batch_data[4].shape[0]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[4].shape[0])\n\t\tself.batch_size_output = nmol\n\t\tif not self.sess:\n\t\t\tLOGGER.info(""loading the session.."")\n\t\t\tself.EvalPrepare()\n\n\t\t#feed_dict=self.fill_feed_dict(batch_data)\n\t\t#output_list, charge_gradient = self.sess.run([  self.output_list, self.charge_gradient],  feed_dict=feed_dict)\n\n\t\t#for i in range (0, batch_data[0][-1][-1].shape[0]):\n\t\t#\tprint(""i:"", i)\n\t\t#\timport copy\n\t\t#\tnew_batch_data=copy.deepcopy(batch_data)\n\t\t#\t#new_batch_data = list(batch_data)\n\t\t#\tnew_batch_data[0][-1][-1][i] += 0.01\n\t\t#\tfeed_dict=self.fill_feed_dict(new_batch_data)\n\t\t#\tnew_output_list, new_charge_gradient = self.sess.run([  self.output_list, self.charge_gradient],  feed_dict=feed_dict)\n\t\t#\tprint (""new_charge_gradient: "", charge_gradient[-1][-1][i],  new_charge_gradient[-1][-1][i], "" numerical: "", (new_output_list[2][-1][-1][-1]-output_list[2][-1][-1][-1])/0.01)\n\n\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\tif not IfChargeGrad:\n\t\t\toutput_list  = self.sess.run( [self.output_list],  feed_dict=feed_dict)\n\t\t\treturn   output_list[0][0]/AUPERDEBYE, output_list[0][1]\n\t\telse:\n\t\t\t#dipole, total_loss_value, loss_value,  atom_outputs, charge_gradient = self.sess.run([ self.dipole_output, self.total_loss, self.loss, self.atom_outputs, self.charge_gradient],  feed_dict=feed_dict)\n\t\t\toutput_list, charge_gradient = self.sess.run([  self.output_list, self.charge_gradient],  feed_dict=feed_dict)\n\t\t\t#print (""unscaled_charge:\\n"", output_list[2],""\\n"")\n\t\t\t#print (""scaled_atom_outputs:"", output_list[1], ""unscaled_atom_outputs:"", output_list[2], "" charge_gradient:"", charge_gradient, ""length of charge_gradient:"", len(charge_gradient))\n\t\t\treturn   output_list[0]/AUPERDEBYE, output_list[1], charge_gradient\n\n\tdef EvalPrepare(self):\n\t\tif (isinstance(self.inshape,tuple)):\n\t\t\tif (len(self.inshape)>1):\n\t\t\t\traise Exception(""My input should be flat"")\n\t\t\telse:\n\t\t\t\tself.inshape = self.inshape[0]\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tself.coords_pl=[]\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.batch_size_output])))\n\t\t\t\tself.coords_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None, 3])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 1]))\n\t\t\tself.output_list   = self.inference(self.inp_pl, self.mats_pl, self.coords_pl, self.natom_pl)\n\t\t\tself.charge_gradient = tf.gradients(self.output_list[2], self.inp_pl)  # gradient of unscaled_charge respect to input\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\t#self.total_loss, self.loss = self.loss_op(self.dipole_output, self.label_pl)\n\t\t\t#self.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\treturn\n\n\tdef Prepare(self):\n\t\t#eval_labels = np.zeros(Ncase)  # dummy labels\n\t\tself.MeanNumAtoms = self.TData.MeanNumAtoms\n\t\tself.batch_size_output = int(1.5*self.batch_size/self.MeanNumAtoms)\n\t\twith tf.Graph().as_default(), tf.device(\'/job:localhost/replica:0/task:0/gpu:1\'):\n\t\t\tself.inp_pl=[]\n\t\t\tself.mats_pl=[]\n\t\t\tself.coords = []\n\t\t\tfor e in range(len(self.eles)):\n\t\t\t\tself.inp_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.inshape])))\n\t\t\t\tself.mats_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None,self.batch_size_output])))\n\t\t\t\tself.coords_pl.append(tf.placeholder(self.tf_prec, shape=tuple([None, 3])))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size_output, 1]))\n\t\t\tself.dipole_output, self.atom_outputs, self.unscaled_atom_outputs, self.net_charge = self.inference(self.inp_pl, self.mats_pl, self.coords_pl, self.natom_pl)\n\t\t\t#self.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.dipole_out, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tself.summary_op = tf.summary.merge_all()\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.saver = tf.train.Saver()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\treturn\n\n\n\nclass MolInstance_BP_Dipole_2_Direct(MolInstance_DirectBP_NoGrad):\n\t""""""\n\t\tCalculate the Dipole of Molecules\n\t""""""\n\tdef __init__(self, TData_, Name_=None, Trainable_=True,ForceType_=""LJ""):\n\t\t""""""\n\t\tArgs:\n\t\t        TData_: A TensorMolData instance.\n\t\t        Name_: A name for this instance.\n\t\t""""""\n\t\tself.NetType = ""RawBP_Dipole""\n\t\tMolInstance.__init__(self, TData_,  Name_, Trainable_)\n\t\t#if (Name_ != None):\n\t\t#       return\n\t\tself.SFPa = None\n\t\tself.SFPr = None\n\t\tself.Ra_cut = None\n\t\tself.Rr_cut = None\n\t\tself.MaxNAtoms = self.TData.MaxNAtoms\n\t\tself.eles = self.TData.eles\n\t\tself.n_eles = len(self.eles)\n\t\tself.eles_np = np.asarray(self.eles).reshape((self.n_eles,1))\n\t\tself.eles_pairs = []\n\t\tfor i in range (len(self.eles)):\n\t\t\tfor j in range(i, len(self.eles)):\n\t\t\t\tself.eles_pairs.append([self.eles[i], self.eles[j]])\n\t\tself.eles_pairs_np = np.asarray(self.eles_pairs)\n\t\tself.SetANI1Param()\n\t\tself.batch_size = PARAMS[""batch_size""]\n\t\tself.NetType = ""RawBP_Dipole""\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType\n\t\tLOGGER.debug(""Raised Instance: ""+self.name)\n\t\tself.train_dir = PARAMS[""networks_directory""]+self.name\n\t\tif (self.Trainable):\n\t\t\tself.TData.LoadDataToScratch(self.tformer)\n\t\tself.xyzs_pl = None\n\t\tself.Zs_pl = None\n\t\tself.label_pl = None\n\t\tself.sess = None\n\t\tself.total_loss = None\n\t\tself.loss = None\n\t\tself.train_op = None\n\t\tself.summary_op = None\n\t\tself.saver = None\n\t\tself.summary_writer = None\n\n\t\tself.netcharge_output = None\n\t\tself.dipole_output = None\n\t\tself.natom_pl = None\n\t\tself.charge_gradient = None\n\t\tself.output_list = None\n\t\tself.unscaled_atom_outputs = None\n\n\tdef Clean(self):\n\t\tMolInstance_DirectBP_NoGrad.Clean(self)\n\t\tself.unscaled_atom_outputs = None\n\t\tself.netcharge_output = None\n\t\tself.dipole_output = None\n\t\tself.natom_pl = None\n\t\tself.output_list = None\n\t\treturn\n\n\tdef TrainPrepare(self,  continue_training =False):\n\t\t""""""\n\t\tGet placeholders, graph and losses in order to begin training.\n\t\tAlso assigns the desired padding.\n\n\t\tArgs:\n\t\t\tcontinue_training: should read the graph variables from a saved checkpoint.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.SFPr2_vary = tf.Variable(self.SFPr2, trainable= True, dtype = self.tf_prec)\n\t\t\tRr_cut   = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut   = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta   = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta   = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update_Scatter(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut)\n\t\t\tself.dipole_output, self.atom_outputs, self.unscaled_atom_outputs  = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.total_loss, self.loss = self.loss_op(self.dipole_output, self.label_pl)\n\t\t\tself.train_op = self.training(self.total_loss, self.learning_rate, self.momentum)\n\t\t\tinit = tf.global_variables_initializer()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver()\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\t\tself.sess.run(init)\n\t\t\t#self.options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\t\t\t#self.run_metadata = tf.RunMetadata()\n\t\treturn\n\n\tdef loss_op(self, dipole_output, labels):\n\t\t""""""\n\t\ttotal_loss =  l2(dipole)\n\t\t""""""\n\t\tdipole_diff  = tf.subtract(dipole_output, labels)\n\t\tloss = tf.nn.l2_loss(dipole_diff)\n\t\ttf.add_to_collection(\'losses\', loss)\n\t\treturn tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\'), loss\n\n\n\tdef inference(self, inp, indexs, xyzs, natom):\n\t\t""""""\n\t\tBuilds a Behler-Parinello graph\n\n\t\tArgs:\n\t\t\tinp: a list of (num_of atom type X flattened input shape) matrix of input cases.\n\t\t\tindex: a list of (num_of atom type X batchsize) array which linearly combines the elements\n\t\tReturns:\n\t\t\tThe BP graph output\n\t\t""""""\n\t\tbranches=[]\n\t\tatom_outputs = []\n\t\thidden1_units=self.hidden1\n\t\thidden2_units=self.hidden2\n\t\thidden3_units=self.hidden3\n\t\toutput = tf.zeros([self.batch_size, self.MaxNAtoms], dtype=self.tf_prec)\n\t\tnrm1=1.0/(10+math.sqrt(float(self.inshape)))\n\t\tnrm2=1.0/(10+math.sqrt(float(hidden1_units)))\n\t\tnrm3=1.0/(10+math.sqrt(float(hidden2_units)))\n\t\tnrm4=1.0/(10+math.sqrt(float(hidden3_units)))\n\t\tprint(""Norms:"", nrm1,nrm2,nrm3)\n\t\tLOGGER.info(""Layer initial Norms: %f %f %f"", nrm1,nrm2,nrm3)\n\t\tfor e in range(len(self.eles)):\n\t\t\tbranches.append([])\n\t\t\tinputs = inp[e]\n\t\t\tshp_in = tf.shape(inputs)\n\t\t\tindex = tf.cast(indexs[e], tf.int64)\n\t\t\tif (PARAMS[""CheckLevel""]>2):\n\t\t\t\ttf.Print(tf.to_float(shp_in), [tf.to_float(shp_in)], message=""Element ""+str(e)+""input shape "",first_n=10000000,summarize=100000000)\n\t\t\t\tindex_shape = tf.shape(index)\n\t\t\t\ttf.Print(tf.to_float(index_shape), [tf.to_float(index_shape)], message=""Element ""+str(e)+""index shape "",first_n=10000000,summarize=100000000)\n\t\t\tif (PARAMS[""CheckLevel""]>3):\n\t\t\t\ttf.Print(tf.to_float(inputs), [tf.to_float(inputs)], message=""This is input shape "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_1\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[self.inshape, hidden1_units], var_stddev=nrm1, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden1_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(inputs, weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_2\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden1_units, hidden2_units], var_stddev=nrm2, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden2_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_hidden_3\'):\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden2_units, hidden3_units], var_stddev=nrm3, var_wd=0.001)\n\t\t\t\tbiases = tf.Variable(tf.zeros([hidden3_units], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(self.activation_function(tf.matmul(branches[-1][-1], weights) + biases))\n\t\t\t\t#tf.Print(branches[-1], [branches[-1]], message=""This is layer 2: "",first_n=10000000,summarize=100000000)\n\t\t\twith tf.name_scope(str(self.eles[e])+\'_regression_linear\'):\n\t\t\t\tshp = tf.shape(inputs)\n\t\t\t\tweights = self._variable_with_weight_decay(var_name=\'weights\', var_shape=[hidden3_units, 1], var_stddev=nrm4, var_wd=None)\n\t\t\t\tbiases = tf.Variable(tf.zeros([1], dtype=self.tf_prec), name=\'biases\')\n\t\t\t\tbranches[-1].append(tf.matmul(branches[-1][-1], weights) + biases)\n\t\t\t\tshp_out = tf.shape(branches[-1][-1])\n\t\t\t\tcut = tf.slice(branches[-1][-1],[0,0],[shp_out[0],1])\n\t\t\t\trshp = tf.reshape(cut,[1,shp_out[0]])\n\t\t\t\tatom_outputs.append(rshp)\n\t\t\t\trshpflat = tf.reshape(cut,[shp_out[0]])\n\t\t\t\tatom_indice = tf.slice(index, [0,1], [shp_out[0],1])\n\t\t\t\tToAdd = tf.reshape(tf.scatter_nd(atom_indice, rshpflat, [self.batch_size*self.MaxNAtoms]),[self.batch_size, self.MaxNAtoms])\n\t\t\t\toutput = tf.add(output, ToAdd)\n\t\t\ttf.verify_tensor_all_finite(output,""Nan in output!!!"")\n\t\t\tnetcharge = tf.reshape(tf.reduce_sum(output, axis=1), [self.batch_size])\n\t\t\tdelta_charge = tf.multiply(netcharge, natom)\n\t\t\tdelta_charge_tile = tf.tile(tf.reshape(delta_charge,[self.batch_size,1]),[1, self.MaxNAtoms])\n\t\t\tscaled_charge = tf.subtract(output, delta_charge_tile)\n\t\t\tflat_dipole = tf.multiply(tf.reshape(xyzs,[self.batch_size*self.MaxNAtoms, 3]), tf.reshape(scaled_charge,[self.batch_size*self.MaxNAtoms, 1]))\n\t\t\tdipole = tf.reduce_sum(tf.reshape(flat_dipole,[self.batch_size, self.MaxNAtoms, 3]), axis=1)\n\t\treturn  dipole, scaled_charge, output\n\n\tdef fill_feed_dict(self, batch_data):\n\t\t""""""\n\t\tFill the tensorflow feed dictionary.\n\n\t\tArgs:\n\t\t\tbatch_data: a list of numpy arrays containing inputs, bounds, matrices and desired energies in that order.\n\t\t\tand placeholders to be assigned. (it can be longer than that c.f. TensorMolData_BP)\n\n\t\tReturns:\n\t\t\tFilled feed dictionary.\n\t\t""""""\n\t\t# Don\'t eat shit.\n\t\tif (not np.all(np.isfinite(np.sum(batch_data[2], axis=1)),axis=(0))):\n\t\t\tprint(""I was fed shit"")\n\t\t\traise Exception(""DontEatShit"")\n\t\tfeed_dict={i: d for i, d in zip([self.xyzs_pl]+[self.Zs_pl]+[self.label_pl] + [self.natom_pl], [batch_data[0]]+[batch_data[1]]+[batch_data[2]] + [batch_data[3]])}\n\t\treturn feed_dict\n\n\tdef train_step(self, step):\n\t\t""""""\n\t\tPerform a single training step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t        step: the index of this step.\n\t\t""""""\n\t\tNcase_train = self.TData.NTrain\n\t\tstart_time = time.time()\n\t\ttrain_loss =  0.0\n\t\ttrain_energy_loss = 0.0\n\t\ttrain_grads_loss = 0.0\n\t\tnum_of_mols = 0\n\t\tpre_output = np.zeros((self.batch_size),dtype=np.float64)\n\t\tfor ministep in range (0, int(Ncase_train/self.batch_size)):\n\t\t\t#print (""ministep:"", ministep)\n\t\t\tbatch_data = self.TData.GetTrainBatch(self.batch_size)\n\t\t\tactual_mols  = self.batch_size\n\t\t\tt = time.time()\n\t\t\tdump_, dump_2, total_loss_value, loss_value,  dipole_output, atom_outputs = self.sess.run([self.check, self.train_op, self.total_loss, self.loss, self.dipole_output, self.atom_outputs], feed_dict=self.fill_feed_dict(batch_data))\n\t\t\t#print (""loss_value:"", loss_value, ""grads_loss:"", grads_loss, ""energy_loss:"", energy_loss, ""\\n self.SFPr2_vary :"", SFPr2_vary )\n\t\t\t#print (""loss_value:"", loss_value)\n\t\t\t#print ("" dipole_output:"", dipole_output, "" atom_outputs:"", list(atom_outputs[:5]))\n\t\t\ttrain_loss = train_loss + loss_value\n\t\t\tduration = time.time() - start_time\n\t\t\tnum_of_mols += actual_mols\n\t\t\t#fetched_timeline = timeline.Timeline(self.run_metadata.step_stats)\n\t\t\t#chrome_trace = fetched_timeline.generate_chrome_trace_format()\n\t\t\t#with open(\'timeline_step_%d_tm_nocheck_h2o.json\' % ministep, \'w\') as f:\n\t\t\t#       f.write(chrome_trace)\n\t\t#print (""gradients:"", gradients)\n\t\t#print (""labels:"", batch_data[2], ""\\n"", ""predcits:"",mol_output)\n\t\t#self.print_training(step, train_loss, train_energy_loss, train_grads_loss, num_of_mols, duration)\n\t\tself.print_training(step, train_loss,  num_of_mols, duration)\n\t\treturn\n\n\tdef test(self, step):\n\t\t""""""\n\t\tPerform a single test step (complete processing of all input), using minibatches of size self.batch_size\n\n\t\tArgs:\n\t\t\tstep: the index of this step.\n\t\t""""""\n\t\ttest_loss =  0.0\n\t\tstart_time = time.time()\n\t\tNcase_test = self.TData.NTest\n\t\tnum_of_mols = 0\n\t\tfor ministep in range (0, int(Ncase_test/self.batch_size)):\n\t\t\tbatch_data=self.TData.GetTestBatch(self.batch_size)\n\t\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\t\tactual_mols  = self.batch_size\n\t\t\ttotal_loss_value, loss_value, dipole_output, atom_outputs = self.sess.run([self.total_loss, self.loss, self.dipole_output, self.atom_outputs], feed_dict=feed_dict)\n\t\t\ttest_loss += loss_value\n\t\t\tnum_of_mols += actual_mols\n\t\tduration = time.time() - start_time\n\t\tprint( ""testing..."")\n\t\tself.print_training(step, test_loss, num_of_mols, duration)\n\t\treturn test_loss, feed_dict\n\n\tdef evaluate(self, batch_data, IfGrad=True):\n\t\t""""""\n\t\tEvaluate the energy, atom energies, and IfGrad = True the gradients\n\t\tof this Direct Behler-Parinello graph.\n\t\t""""""\n\t\t# Check sanity of input\n\t\tnmol = batch_data[2].shape[0]\n\t\tself.MaxNAtoms = batch_data[0].shape[1]\n\t\tLOGGER.debug(""nmol: %i"", batch_data[2].shape[0])\n\t\tself.batch_size = nmol\n\t\tif not self.sess:\n\t\t\tprint (""loading the session.."")\n\t\t\tself.EvalPrepare()\n\t\tfeed_dict=self.fill_feed_dict(batch_data)\n\t\tdipole_output, atom_outputs = self.sess.run([self.dipole_output, self.atom_outputs],  feed_dict=feed_dict)\n\t\treturn dipole_output, atom_outputs\n\n\tdef Prepare(self):\n\t\tself.TrainPrepare()\n\t\treturn\n\n\tdef EvalPrepare(self):\n\t\t""""""\n\t\tDoesn\'t generate the training operations or losses.\n\t\t""""""\n\t\twith tf.Graph().as_default():\n\t\t\tself.xyzs_pl=tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, self.MaxNAtoms,3]))\n\t\t\tself.Zs_pl=tf.placeholder(tf.int64, shape=tuple([self.batch_size, self.MaxNAtoms]))\n\t\t\tself.label_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size, 3]))\n\t\t\tself.natom_pl = tf.placeholder(self.tf_prec, shape=tuple([self.batch_size]))\n\t\t\tEle = tf.Variable(self.eles_np, trainable=False, dtype = tf.int64)\n\t\t\tElep = tf.Variable(self.eles_pairs_np, trainable=False, dtype = tf.int64)\n\t\t\tSFPa = tf.Variable(self.SFPa, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr = tf.Variable(self.SFPr, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPa2 = tf.Variable(self.SFPa2, trainable=False, dtype = self.tf_prec)\n\t\t\tSFPr2 = tf.Variable(self.SFPr2, trainable=False, dtype = self.tf_prec)\n\t\t\t#self.SFPr2_vary = tf.Variable(self.SFPr2, trainable= True, dtype = self.tf_prec)\n\t\t\tRr_cut   = tf.Variable(self.Rr_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tRa_cut   = tf.Variable(self.Ra_cut, trainable=False, dtype = self.tf_prec)\n\t\t\tzeta   = tf.Variable(self.zeta, trainable=False, dtype = self.tf_prec)\n\t\t\teta   = tf.Variable(self.eta, trainable=False, dtype = self.tf_prec)\n\t\t\tself.Scatter_Sym, self.Sym_Index  = TFSymSet_Scattered_Update_Scatter(self.xyzs_pl, self.Zs_pl, Ele, SFPr2, Rr_cut, Elep, SFPa2, zeta, eta, Ra_cut)\n\t\t\tself.dipole_output, self.atom_outputs, self.unscaled_atom_outputs  = self.inference(self.Scatter_Sym, self.Sym_Index, self.xyzs_pl, self.natom_pl)\n\t\t\tself.check = tf.add_check_numerics_ops()\n\t\t\tself.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\t\t\tself.saver = tf.train.Saver()\n\t\t\tself.saver.restore(self.sess, self.chk_file)\n\t\t\tself.summary_writer = tf.summary.FileWriter(self.train_dir, self.sess.graph)\n\t\tprint(""Prepared for Evaluation..."")\n\t\treturn\n'"
TensorMol/TFNetworks/TFMolManage.py,1,"b'\'\'\'\n These work Moleculewise the versions without the mol prefix work atomwise.\n but otherwise the behavior of these is the same as TFManage etc.\n\nTODO: The interface to evaluation needs to be completely re-done or done away with.\nActually I think the better thing is to eliminate any dependence on TFManage whatsoever.\n\'\'\'\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom ..Containers.Sets import *\nfrom .TFManage import *\nfrom ..Containers.TensorMolData import *\nfrom .TFMolInstance import *\nfrom .TFMolInstanceDirect import *\nfrom .TFBehlerParinello import *\nfrom .TFBehlerParinelloSymEE import *\nfrom .TFMolInstanceEE import *\nfrom .TFMolInstanceDirect import *\nfrom ..Math.QuasiNewtonTools import *\n\nimport numpy as np\nimport gc\nimport time\nimport sys\nif sys.version_info[0] < 3:\n\timport cPickle as pickle\nelse:\n\timport _pickle as pickle\n\n\nmol_set=MSet()\n\nclass TFMolManage(TFManage):\n\t""""""\n\t\tA manager of tensorflow instances which perform molecule-wise predictions\n\t\tincluding Many Body and Behler-Parinello\n\t""""""\n\tdef __init__(self, Name_="""", TData_=None, Train_=False, NetType_=""fc_sqdiff"", RandomTData_=True, Trainable_ = True):\n\t\t""""""\n\t\t\tArgs:\n\t\t\t\tName_: If not blank, will try to load a network with that name using Prepare()\n\t\t\t\tTData_: A TensorMolData instance to provide and process data.\n\t\t\t\tTrain_: Whether to train the instances raised.\n\t\t\t\tNetType_: Choices of Various network architectures.\n\t\t\t\tRandomTData_: Modifes the preparation of training batches.\n\t\t""""""\n\t\tself.path = PARAMS[""networks_directory""]\n\t\tself.Trainable = Trainable_\n\t\tif (Name_!=""""):\n\t\t\tself.name = Name_\n\t\t\tself.Prepare()\n\t\t\treturn\n\t\tTFManage.__init__(self, Name_, TData_, False, NetType_, RandomTData_, Trainable_)\n\t\tself.suffix = PARAMS[""NetNameSuffix""]\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+self.suffix\n\t\t#self.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.TData.order)\n\t\tself.TrainedAtoms=[] # In order of the elements in TData\n\t\tself.TrainedNetworks=[] # In order of the elements in TData\n\t\tself.Instances=None # In order of the elements in TData\n\t\tif (Train_):\n\t\t\tself.Train()\n\t\t\treturn\n\t\treturn\n\n\tdef Train(self, maxstep=3000):\n\t\t""""""\n\t\tInstantiates and trains a Molecular network.\n\t\tThis routine is an abomination. (JAP 12/2017)\n\n\t\tArgs:\n\t\t\tmaxstep: The number of training steps.\n\t\t""""""\n\t\t#if (self.TData.dig.eshape==None):\n\t\t#\traise Exception(""Must Have Digester Shape."")\n\t\t# It\'s up the TensorData to provide the batches and input output shapes.\n\t\tif (self.NetType == ""fc_classify""):\n\t\t\tself.Instances = MolInstance_fc_classify(self.TData, None)\n\t\telif (self.NetType == ""fc_sqdiff""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff(self.TData, None)\n\t\telif (self.NetType == ""fc_sqdiff_BP""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff_BP(self.TData)\n\t\telif self.NetType == ""BehlerParinelloDirect"":\n\t\t\tself.Instances = BehlerParinelloDirect(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_WithGrad""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff_BP_WithGrad(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Update""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff_BP_Update(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct""):\n\t\t\tself.Instances = MolInstance_DirectBP_NoGrad(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BPBond_Direct""):\n\t\t\tself.Instances = MolInstance_DirectBPBond_NoGrad(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BPBond_DirectQueue""):\n\t\t\tself.Instances = MolInstance_DirectBPBond_NoGrad_Queue(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad_noGradTrain""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad_noGradTrain(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad_NewIndex""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad_NewIndex(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad_Linear""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad_Linear(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad_Linear_EmbOpt""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad_Linear_EmbOpt(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_Update""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_Update(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_NoGradTrain""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_NoGradTrain(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_AvgPool""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_AvgPool(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_InputNorm""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_InputNorm(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_Conv""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_Conv(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_SymFunction""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_SymFunction(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EandG_SymFunction""):\n\t\t\tself.Instances = MolInstance_DirectBP_EandG_SymFunction(self.TData)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Charge_SymFunction""):\n\t\t\tself.Instances = MolInstance_DirectBP_Charge_SymFunction(self.TData)\n\t\telif (self.NetType == ""Dipole_BP""):\n\t\t\tself.Instances = MolInstance_BP_Dipole(self.TData)\n\t\telif (self.NetType == ""Dipole_BP_2""):\n\t\t\tself.Instances = MolInstance_BP_Dipole_2(self.TData)\n\t\telif (self.NetType == ""Dipole_BP_2_Direct""):\n\t\t\tself.Instances = MolInstance_BP_Dipole_2_Direct(self.TData)\n\t\telif (self.NetType == ""LJForce""):\n\t\t\tself.Instances = MolInstance_LJForce(self.TData)\n\t\telif (self.NetType == ""pairs_triples""):\n\t\t\tself.Instances = MolPairsTriples(self.TData)\n\t\telse:\n\t\t\traise Exception(""Unknown Network Type!"")\n\t\tif (PARAMS[""Profiling""]>0):\n\t\t\tself.Instances.profile()\n\t\t\treturn\n\t\tself.n_train = PARAMS[""max_steps""]\n\t\tself.Instances.train(self.n_train)\n\t\tnm = self.Instances.name\n\t\t# Here we should print some summary of the pupil\'s progress as well, maybe.\n\t\tif self.TrainedNetworks.count(nm)==0:\n\t\t\tself.TrainedNetworks.append(nm)\n\t\tself.Save()\n\t\tgc.collect()\n\t\treturn\n\n\tdef Continue_Training(self, maxsteps=3000):   # test a pretrained network\n\t\tself.n_train = PARAMS[""max_steps""]\n\t\tself.Instances.TData = self.TData\n\t\tself.Instances.TData.LoadDataToScratch(self.Instances.tformer)\n\t\tself.Instances.Prepare()\n\t\tself.Instances.continue_training(self.n_train)\n\t\tself.Save()\n\t\treturn\n\n\tdef Eval(self, inputs):\n\t\tif (self.Instances[mol_t.atoms[atom]].tformer.innorm != None):\n\t\t\tinputs = self.Instances[mol_t.atoms[atom]].tformer.NormalizeIns(inputs, train=False)\n\t\toutputs = self.Instances.evaluate(inputs)\n\t\tif (self.Instances[mol_t.atoms[atom]].tformer.outnorm != None):\n\t\t\toutputs = self.Instances[mol_t.atoms[atom]].tformer.UnNormalizeOuts(outputs)\n\t\treturn outputs\n\n\tdef Eval_BPEnergy(self, mol_set, total_energy =True):\n\t\tnmols = len(mol_set.mols)\n\t\tnatoms = mol_set.NAtoms()\n\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\tcasep = 0\n\t\tmols_done = 0\n\t\tt = time.time()\n\t\tfor mol in mol_set.mols:\n\t\t\tins = self.TData.dig.EvalDigest(mol, False)\n\t\t\tnat = mol.NAtoms()\n\t\t\tcases[casep:casep+nat] = ins\n\t\t\tfor i in range (casep, casep+nat):\n\t\t\t\tmeta[i, 0] = mols_done\n\t\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\t\tmeta[i, 2] = casep\n\t\t\t\tmeta[i, 3] = casep + nat\n\t\t\tcasep += nat\n\t\t\tmols_done += 1\n\t\tsto = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\tinputs = []\n\t\tmatrices = []\n\t\toutputpointer = 0\n\t\tfor i in range (0, natoms):\n\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\tcurrentmol = 0\n\t\tfor e in range (len(self.TData.eles)):\n\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\tfor i in range (0, natoms):\n\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\toutputpointer += 1\n\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\te = meta[i, 1]\n\t\t\tei = self.TData.eles.index(e)\n\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\toffsets[ei] += 1\n\t\tt = time.time()\n\t\tpointers = [0 for ele in self.TData.eles]\n\t\tmol_out, atom_out, nn_gradient = self.Instances.evaluate([inputs, matrices, dummy_outputs])\n\t\ttotal_list = []\n\t\tfor i in range (0, nmols):\n\t\t\ttotal = mol_out[0][i]\n\t\t\tmol = mol_set.mols[i]\n\t\t\tif total_energy:\n\t\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\t\ttotal += ele_U[mol.atoms[j]]\n\t\t\ttotal_list.append(total)\n\t\treturn total_list\n\n\tdef Eval_BPEnergySingle(self, mol):\n\t\t""""""\n\t\tArgs:\n\t\t\tmol: a Mol.\n\t\tReturns:\n\t\t\tEnergy in Hartree\n\t\t""""""\n\t\tnmols = 1\n\t\tnatoms = mol.NAtoms()\n\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\tcasep = 0\n\t\tmols_done = 0\n\t\tt = time.time()\n\t\t#\n\t\tins = self.TData.dig.EvalDigest(mol, False)\n\t\tnat = mol.NAtoms()\n\t\tcases[casep:casep+nat] = ins\n\t\tfor i in range (casep, casep+nat):\n\t\t\tmeta[i, 0] = mols_done\n\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\tmeta[i, 2] = casep\n\t\t\tmeta[i, 3] = casep + nat\n\t\tcasep += nat\n\t\tmols_done += 1\n\t\t#\n\t\tsto = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\tinputs = []\n\t\tmatrices = []\n\t\toutputpointer = 0\n\t\tfor i in range (0, natoms):\n\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\tcurrentmol = 0\n\t\tfor e in range (len(self.TData.eles)):\n\t\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\tfor i in range (0, natoms):\n\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\toutputpointer += 1\n\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\te = meta[i, 1]\n\t\t\tei = self.TData.eles.index(e)\n\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\toffsets[ei] += 1\n\t\tt = time.time()\n\t\tpointers = [0 for ele in self.TData.eles]\n\t\tmol_out, atom_out = self.Instances.evaluate([inputs, matrices, dummy_outputs],IfGrad=False)\n\t\ttotal = mol_out[0][0]\n\t\tfor j in range (0, mol.NAtoms()):\n\t\t\ttotal += ele_U[mol.atoms[j]]\n\t\treturn total\n\n\tdef Eval_BPForceSet(self, mol_set, total_energy = False):\n\t\t""""""\n\t\tArgs:\n\t\t\tmol_set: a MSet\n\t\t\ttotal_energy: whether to also return the energy as a first argument.\n\t\tReturns:\n\t\t\t(if total_energy == True): Energy in Hartree\n\t\t\tand Forces (J/mol)\n\t\t""""""\n\t\tnmols = len(mol_set.mols)\n\t\tnatoms = mol_set.NAtoms()\n\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\tcases_grads = []\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\tcasep = 0\n\t\tmols_done = 0\n\t\tt = time.time()\n\t\tfor mol in mol_set.mols:\n\t\t\tins, grads = self.TData.dig.EvalDigest(mol,True)\n\t\t\t#print ""ins, grads"", ins.shape, grads.shape\n\t\t\tnat = mol.NAtoms()\n\t\t\tcases[casep:casep+nat] = ins\n\t\t\tcases_grads += list(grads)\n\t\t\tfor i in range (casep, casep+nat):\n\t\t\t\tmeta[i, 0] = mols_done\n\t\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\t\tmeta[i, 2] = casep\n\t\t\t\tmeta[i, 3] = casep + nat\n\t\t\tcasep += nat\n\t\t\tmols_done += 1\n\t\tsto = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\tinputs = []\n\t\tmatrices = []\n\t\tinputs_grads = [[] for i in range (len(self.TData.eles))]\n\t\toutputpointer = 0\n\t\tfor i in range (0, natoms):\n\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\t\tcurrentmol = 0\n\t\t\tfor e in range (len(self.TData.eles)):\n\t\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\t\t\tatom_index_in_mol = [[] for i in range (len(self.TData.eles))]\n\t\t\t\tfor i in range (0, natoms):\n\t\t\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\t\t\toutputpointer += 1\n\t\t\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\t\t\te = meta[i, 1]\n\t\t\t\t\tei = self.TData.eles.index(e)\n\t\t\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\tinputs_grads[ei].append(cases_grads[i])\n\t\t#inputs_grads[ei][offsets[ei], :]  = cases_grads[i]\n\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\tatom_index_in_mol[ei].append(currentmol)\n\t\toffsets[ei] += 1\n\t\tprint((""data prepare cost:"", time.time() -t))\n\t\tt = time.time()\n\t\tpointers = [0 for ele in self.TData.eles]\n\t\tmol_out, atom_out, nn_gradient = self.Instances.evaluate([inputs, matrices, dummy_outputs],IfGrad=True)\n\t\tprint((""actual evaluation cost:"", time.time() -t))\n\t\tt = time.time()\n\t\ttotal_gradient_list = []\n\t\ttotal_energy_list = []\n\t\tfor i in range (0, nmols):\n\t\t\ttotal = mol_out[0][i]\n\t\t\tmol = mol_set.mols[i]\n\t\t\ttotal_gradient = np.zeros((mol.NAtoms()*3))\n\t\t\tfor j, ele in enumerate(self.TData.eles):\n\t\t\t\tele_index = [k for k, tmp_index in enumerate(atom_index_in_mol[j]) if tmp_index == i]\n\t\t\t\tele_desp_grads = np.asarray([ tmp_array for k, tmp_array in enumerate(inputs_grads[j]) if k in ele_index])\n\t\t\t\tele_nn_grads = np.asarray([ tmp_array for k, tmp_array in enumerate(nn_gradient[j]) if k in ele_index])\n\t\t\t\ttotal_gradient += np.einsum(""ad,adx->x"", ele_nn_grads, ele_desp_grads) # Chain rule.\n\t\ttotal_gradient_list.append(-JOULEPERHARTREE*total_gradient.reshape((-1,3)))\n\t\t#total_gradient_list.append(-total_gradient.reshape((-1,3)))\n\t\tif total_energy:\n\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\ttotal += ele_U[mol.atoms[j]]\n\t\t\ttotal_energy_list.append(total)\n\t\telse:\n\t\t\ttotal_energy_list.append(total)\n\t\tprint((""recombine molecule cost:"", time.time() -t))\n\t\treturn total_energy_list, total_gradient_list\n\n\tdef Eval_BPForceSingle(self, mol, total_energy = False):\n\t\t""""""\n\t\tArgs:\n\t\t\tmol: a Mol.\n\t\t\ttotal_energy: whether to also return the energy as a first argument.\n\t\tReturns:\n\t\t\t(if total_energy == True): Energy in Hartree\n\t\t\tand Forces (J/mol)\n\t\t""""""\n\t\tnmols = 1\n\t\tnatoms = mol.NAtoms()\n\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\tcases_grads = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)+list([3*natoms])))\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\tcasep = 0\n\t\tmols_done = 0\n\t\tt = time.time()\n\t\t# Fictitious set loop.\n\t\tins, grads = self.TData.dig.EvalDigest(mol)\n\t\tnat = mol.NAtoms()\n\t\tcases[casep:casep+nat] = ins\n\t\tcases_grads[casep:casep+nat] = grads\n\t\tfor i in range (casep, casep+nat):\n\t\t\tmeta[i, 0] = mols_done\n\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\tmeta[i, 2] = casep\n\t\t\tmeta[i, 3] = casep + nat\n\t\t#casep += nat\n\t\t#mols_done += 1\n\t\t# End fictitious set loop.\n\t\tsto = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\tinputs = []\n\t\tinputs_grads = []\n\t\tmatrices = []\n\t\toutputpointer = 0\n\t\tfor i in range (0, natoms):\n\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\tcurrentmol = 0\n\t\tfor e in range (len(self.TData.eles)):\n\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\tinputs_grads.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape), 3*natoms)))\n\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\tfor i in range (0, natoms):\n\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\toutputpointer += 1\n\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\te = meta[i, 1]\n\t\t\tei = self.TData.eles.index(e)\n\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\tinputs_grads[ei][offsets[ei], :]  = cases_grads[i]\n\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\toffsets[ei] += 1\n\t\tt = time.time()\n\t\tmol_out, atom_out, nn_gradient = self.Instances.evaluate([inputs, matrices, dummy_outputs],IfGrad=True)\n\t\ttotal_gradient = np.zeros((natoms*3))\n\t\t#print (""atom_out\\n:"", atom_out)\n\t\tfor i in range (0, len(nn_gradient)): # Loop over element types.\n\t\t\ttotal_gradient += np.einsum(""ad,adx->x"",nn_gradient[i],inputs_grads[i]) # Chain rule.\n\t\t\t#print ""atom_grads: \\n"", np.einsum(""ad,adx->ax"",nn_gradient[i],inputs_grads[i])\n\t\tif (total_energy):\n\t\t\ttotal = mol_out[0][0]\n\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\ttotal += ele_U[mol.atoms[j]]\n\t\t\t#return total,total_gradient.reshape((-1,3))\n\t\t\treturn  total, (-JOULEPERHARTREE*total_gradient.reshape((-1,3)))\n\t\telse:\n\t\t\t#return total_gradient.reshape((-1,3))\n\t\t\treturn  (-JOULEPERHARTREE*total_gradient.reshape((-1,3)))\n\n\tdef Eval_BPForceHalfNumerical(self, mol, total_energy = False):\n\t\t""""""\n\t\tThis version uses a half-numerical gradient.\n\t\tIt was written for debugging purposes.\n\t\tArgs:\n\t\t\tmol: a Mol.\n\t\t\ttotal_energy: whether to also return the energy as a first argument.\n\t\tReturns:\n\t\t\t(if total_energy == True): Energy in Hartree\n\t\t\tand Forces (kcal/mol)\n\t\t""""""\n\t\tnmols = 1\n\t\tnatoms = mol.NAtoms()\n\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\tcasep = 0\n\t\tmols_done = 0\n\t\tt = time.time()\n\t\t# Fictitious set loop.\n\t\tins = self.TData.dig.EvalDigest(mol,False)\n\t\tnat = mol.NAtoms()\n\t\tcases[casep:casep+nat] = ins\n\t\tfor i in range (casep, casep+nat):\n\t\t\tmeta[i, 0] = mols_done\n\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\tmeta[i, 2] = casep\n\t\t\tmeta[i, 3] = casep + nat\n\t\tcasep += nat\n\t\tmols_done += 1\n\t\t# End fictitious set loop.\n\t\tsto = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\tinputs = []\n\t\tmatrices = []\n\t\toutputpointer = 0\n\t\tfor i in range (0, natoms):\n\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\tcurrentmol = 0\n\t\tfor e in range (len(self.TData.eles)):\n\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\tfor i in range (0, natoms):\n\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\toutputpointer += 1\n\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\te = meta[i, 1]\n\t\t\tei = self.TData.eles.index(e)\n\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\toffsets[ei] += 1\n\t\tt = time.time()\n\t\tmol_out, atom_out, nn_gradient = self.Instances.evaluate([inputs, matrices, dummy_outputs],IfGrad=True)\n\t\ttotal_gradient = np.zeros((natoms*3))\n\t\tfor i in range (0, len(nn_gradient)): # Loop over element types.\n\t\t\tEval_Input = lambda x_: self.Eval_Input(Mol(mol.atoms,x_.reshape((-1,3))))[i]\n\t\t\tinput_grad = np.transpose(FdiffGradient(Eval_Input, mol.coords.flatten(), 0.00001),(1,2,0))\n\t\t\ttotal_gradient += np.einsum(""ad,adx->x"",nn_gradient[i],input_grad) # Chain rule.\n\t\tif (total_energy):\n\t\t\ttotal = mol_out[0][0]\n\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\ttotal += ele_U[mol.atoms[j]]\n\t\t\treturn  total, (-JOULEPERHARTREE*total_gradient.reshape((-1,3)))\n\t\telse:\n\t\t\treturn  (-JOULEPERHARTREE*total_gradient.reshape((-1,3)))\n\n\tdef Eval_Input(self, mol):\n\t\t""""""\n\t\tArgs:\n\t\t\tmol: a Mol.\n\t\t\ttotal_energy: whether to also return the energy as a first argument.\n\t\tReturns:\n\t\t\t(if total_energy == True): Energy in Hartree\n\t\t\tand Forces (kcal/mol)\n\t\t""""""\n\t\tnmols = 1\n\t\tnatoms = mol.NAtoms()\n\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\tcases_grads = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)+list([3*natoms])))\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\tcasep = 0\n\t\tmols_done = 0\n\t\tt = time.time()\n\t\t# Fictitious set loop.\n\t\tins, grads = self.TData.dig.EvalDigest(mol)\n\t\tnat = mol.NAtoms()\n\t\tcases[casep:casep+nat] = ins\n\t\tcases_grads[casep:casep+nat] = grads\n\t\tfor i in range (casep, casep+nat):\n\t\t\tmeta[i, 0] = mols_done\n\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\tmeta[i, 2] = casep\n\t\t\tmeta[i, 3] = casep + nat\n\t\tcasep += nat\n\t\tmols_done += 1\n\t\t# End fictitious set loop.\n\t\tsto = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\tinputs = []\n\t\tinputs_grads = []\n\t\toutputpointer = 0\n\t\tfor i in range (0, natoms):\n\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\tcurrentmol = 0\n\t\tfor e in range (len(self.TData.eles)):\n\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\tfor i in range (0, natoms):\n\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\toutputpointer += 1\n\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\te = meta[i, 1]\n\t\t\tei = self.TData.eles.index(e)\n\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\toffsets[ei] += 1\n\t\treturn inputs\n\n\tdef Eval_InputGrad(self, mol):\n\t\t""""""\n\t\tArgs:\n\t\t\tmol: a Mol.\n\t\t\ttotal_energy: whether to also return the energy as a first argument.\n\t\tReturns:\n\t\t\t(if total_energy == True): Energy in Hartree\n\t\t\tand Forces (kcal/mol)\n\t\t""""""\n\t\tnmols = 1\n\t\tnatoms = mol.NAtoms()\n\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\tcases_grads = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)+list([3*natoms])))\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\tcasep = 0\n\t\tmols_done = 0\n\t\tt = time.time()\n\t\t# Fictitious set loop.\n\t\tins, grads = self.TData.dig.EvalDigest(mol)\n\t\tnat = mol.NAtoms()\n\t\tcases[casep:casep+nat] = ins\n\t\tcases_grads[casep:casep+nat] = grads\n\t\tfor i in range (casep, casep+nat):\n\t\t\tmeta[i, 0] = mols_done\n\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\tmeta[i, 2] = casep\n\t\t\tmeta[i, 3] = casep + nat\n\t\tcasep += nat\n\t\tmols_done += 1\n\t\t# End fictitious set loop.\n\t\tsto = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\tinputs = []\n\t\tinputs_grads = []\n\t\tmatrices = []\n\t\toutputpointer = 0\n\t\tfor i in range (0, natoms):\n\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\tcurrentmol = 0\n\t\tfor e in range (len(self.TData.eles)):\n\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\tinputs_grads.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape), 3*natoms)))\n\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\tfor i in range (0, natoms):\n\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\toutputpointer += 1\n\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\te = meta[i, 1]\n\t\t\tei = self.TData.eles.index(e)\n\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\tinputs_grads[ei][offsets[ei], :]  = cases_grads[i]\n\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\toffsets[ei] += 1\n\t\treturn inputs_grads\n\n\tdef Test_BPGrad(self,mol):\n\t\t""""""\n\t\tComputes the gradient a couple different ways. Compares them.\n\t\t""""""\n\t\tEnergyFunction = lambda x_: JOULEPERHARTREE*self.Eval_BPEnergySingle(Mol(mol.atoms,x_))\n\t\tNumForce0 = FdiffGradient(EnergyFunction, mol.coords, 0.1)\n\t\tNumForce1 = FdiffGradient(EnergyFunction, mol.coords, 0.01)\n\t\tNumForce2 = FdiffGradient(EnergyFunction, mol.coords, 0.001)\n\t\tNumForce3 = FdiffGradient(EnergyFunction, mol.coords, 0.0001)\n\t\tprint(""Force Differences"", RmsForce(NumForce1-NumForce0))\n\t\tprint(""Force Differences"", RmsForce(NumForce2-NumForce1))\n\t\tprint(""Force Differences"", RmsForce(NumForce3-NumForce2))\n\t\tprint(""Force Differences"", RmsForce(NumForce3-NumForce1))\n\t\tAnalForce = self.Eval_BPForceSingle( mol, total_energy = False)\n\t\tHalfAnalForce = self.Eval_BPForceHalfNumerical( mol, total_energy = False)\n\t\tprint(""Force Differences2"", RmsForce(NumForce0-AnalForce))\n\t\tprint(""Force Differences2"", RmsForce(NumForce1-AnalForce))\n\t\tprint(""Force Differences2"", RmsForce(NumForce2-AnalForce))\n\t\tprint(""Force Differences2"", RmsForce(NumForce3-AnalForce))\n\t\tprint(""Force Differences3"", RmsForce(NumForce0-HalfAnalForce))\n\t\tprint(""Force Differences3"", RmsForce(NumForce1-HalfAnalForce))\n\t\tprint(""Force Differences3"", RmsForce(NumForce2-HalfAnalForce))\n\t\tprint(""Force Differences3"", RmsForce(NumForce3-HalfAnalForce))\n\t\tprint(""Force Differences4"", RmsForce(AnalForce-HalfAnalForce))\n\t\tprint(""Numerical force 0 / Analytical force"", NumForce0/AnalForce)\n\t\tprint(""Numerical force 1 / Analytical force"", NumForce1/AnalForce)\n\t\tprint(""HalfAnalForce / Analytical force"", HalfAnalForce/AnalForce)\n\t\tif (0):\n\t\t\tprint(""Testing chain rule components... "")\n\t\t\ttmp = self.Eval_InputGrad(mol)\n\t\t\tfor ele in range(len(tmp)):\n\t\t\t\tEval_Input = lambda x_: self.Eval_Input(Mol(mol.atoms,x_.reshape((-1,3))))[ele]\n\t\t\t\tAnalyticaldgdr = self.Eval_InputGrad(mol)[ele]\n\t\t\t\tNumericaldgdr0 = np.transpose(FdiffGradient(Eval_Input, mol.coords.flatten(), 0.01),(1,2,0))\n\t\t\t\tNumericaldgdr1 = np.transpose(FdiffGradient(Eval_Input, mol.coords.flatten(), 0.001),(1,2,0))\n\t\t\t\tNumericaldgdr2 = np.transpose(FdiffGradient(Eval_Input, mol.coords.flatten(), 0.0001),(1,2,0))\n\t\t\t\tNumericaldgdr3 = np.transpose(FdiffGradient(Eval_Input, mol.coords.flatten(), 0.00001),(1,2,0))\n\t\t\t\tprint(""Shapes"", Analyticaldgdr.shape, Numericaldgdr1.shape)\n\t\t\t\tfor i in range(Analyticaldgdr.shape[0]):\n\t\t\t\t\tfor j in range(Analyticaldgdr.shape[1]):\n\t\t\t\t\t\tfor k in range(Analyticaldgdr.shape[2]):\n\t\t\t\t\t\t\tif (abs(Analyticaldgdr[i,j,k])>0.0000000001):\n\t\t\t\t\t\t\t\tif (abs((Analyticaldgdr[i,j,k]/Numericaldgdr2[i,j,k])-1.)>0.05):\n\t\t\t\t\t\t\t\t\tprint(ele,i,j,k,"" :: "",Analyticaldgdr[i,j,k],"" "", Numericaldgdr0[i,j,k],"" "", Numericaldgdr1[i,j,k],"" "", Numericaldgdr2[i,j,k])\n\n\tdef Eval_BPDipole(self, mol_set,  ScaleCharge_ = False):\n\t\t""""""\n\t\tcan take either a single mol or mol set\n\t\treturn netcharge, dipole, atomcharge\n\t\tDipole has unit in debye\n\t\t""""""\n\t\teles = self.Instances.eles\n\t\tif isinstance(mol_set, Mol):\n\t\t\ttmp = MSet()\n\t\t\ttmp.mols = [mol_set]\n\t\t\tmol_set = tmp\n\t\t\tnmols = len(mol_set.mols)\n\t\t\tnatoms = mol_set.NAtoms()\n\t\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\t\tdummy_outputs = np.zeros((nmols, 4))\n\t\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\t\txyzmeta = np.zeros((natoms, 3))\n\t\t\tcasep = 0\n\t\t\tmols_done = 0\n\t\t\tt = time.time()\n\t\t\tfor mol in mol_set.mols:\n\t\t\t\tins, grads = self.TData.dig.EvalDigest(mol)\n\t\t\t\tnat = mol.NAtoms()\n\t\t\t\txyz_centered = mol.coords - np.average(mol.coords, axis=0)\n\t\t\t\tcases[casep:casep+nat] = ins\n\t\t\t\tfor i in range (casep, casep+nat):\n\t\t\t\t\tmeta[i, 0] = mols_done\n\t\t\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\t\t\tmeta[i, 2] = casep\n\t\t\t\t\tmeta[i, 3] = casep + nat\n\t\t\t\t\txyzmeta[i] = xyz_centered[i - casep]\n\t\t\t\tcasep += nat\n\t\t\t\tmols_done += 1\n\t\t\t\tsto = np.zeros(len(eles),dtype = np.int32)\n\t\t\t\toffsets = np.zeros(len(eles),dtype = np.int32)\n\t\t\t\tinputs = []\n\t\t\t\tmatrices = []\n\t\t\t\txyz = []\n\t\t\t\toutputpointer = 0\n\t\t\t\tfor i in range (0, natoms):\n\t\t\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\t\t\tcurrentmol = 0\n\t\t\t\tfor e in range (len(eles)):\n\t\t\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\t\t\t\txyz.append(np.zeros((sto[e], 3)))\n\t\t\t\tfor i in range (0, natoms):\n\t\t\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\t\t\toutputpointer += 1\n\t\t\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\t\t\te = meta[i, 1]\n\t\t\t\t\tei = eles.index(e)\n\t\t\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\t\t\txyz[ei][offsets[ei]] = xyzmeta[i]\n\t\t\t\t\toffsets[ei] += 1\n\t\t\t\tt = time.time()\n\t\t\t\tnetcharge, dipole, atomcharge = self.Instances.evaluate([inputs, matrices, xyz, dummy_outputs])\n\t\tmolatomcharge = []\n\t\tpointers = [0 for ele in eles]\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\ttmp_atomcharge = np.zeros(mol.NAtoms())\n\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\tatom_type = mol.atoms[j]\n\t\t\t\tatom_index = eles.index(atom_type)\n\t\t\t\ttmp_atomcharge[j] = atomcharge[atom_index][0][pointers[atom_index]]\n\t\t\t\tpointers[atom_index] +=1\n\t\t\tmolatomcharge.append(tmp_atomcharge)\n\t\tif ScaleCharge_:\n\t\t\tsdipole = np.zeros((dipole.shape[0], 3))\n\t\t\tsmolatomcharge = []\n\t\t\tpointers = [0 for ele in eles]\n\t\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\t\ttmp_atomcharge = molatomcharge[i]\n\t\t\t\ttmp_atomcharge = tmp_atomcharge - netcharge[i]/mol.NAtoms()\n\t\t\t\tsmolatomcharge.append(tmp_atomcharge)\n\t\t\t\tcenter_ = np.average(mol.coords,axis=0)\n\t\t\t\tsdipole[i] = np.einsum(""ax,a"", mol.coords - center_, tmp_atomcharge)/AUPERDEBYE\n\t\t\treturn  np.zeros(nmols), sdipole, smolatomcharge\n\t\treturn netcharge, dipole, molatomcharge\n\n\n\tdef Eval_BPDipole_2(self, mol_set,  ScaleCharge_ = False):\n\t\t""""""\n\t\tcan take either a single mol or mol set\n\t\treturn dipole, atomcharge\n\t\tDipole has unit in debye\n\t\t""""""\n\t\teles = self.Instances.eles\n\t\tif isinstance(mol_set, Mol):\n\t\t\ttmp = MSet()\n\t\t\ttmp.mols = [mol_set]\n\t\t\tmol_set = tmp\n\t\t\tnmols = len(mol_set.mols)\n\t\t\tnatoms = mol_set.NAtoms()\n\t\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\t\tdummy_outputs = np.zeros((nmols, 3))\n\t\t\tnatom_in_mol = np.zeros((nmols, 1))\n\t\t\tnatom_in_mol.fill(float(\'inf\'))\n\t\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\t\txyzmeta = np.zeros((natoms, 3))\n\t\t\tcasep = 0\n\t\t\tmols_done = 0\n\t\t\tt = time.time()\n\t\t\tfor mol in mol_set.mols:\n\t\t\t\tins, grads = self.TData.dig.EvalDigest(mol)\n\t\t\t\tnat = mol.NAtoms()\n\t\t\t\txyz_centered = mol.coords - np.average(mol.coords, axis=0)\n\t\t\t\tcases[casep:casep+nat] = ins\n\t\t\t\tfor i in range (casep, casep+nat):\n\t\t\t\t\tmeta[i, 0] = mols_done\n\t\t\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\t\t\tmeta[i, 2] = casep\n\t\t\t\t\tmeta[i, 3] = casep + nat\n\t\t\t\t\txyzmeta[i] = xyz_centered[i - casep]\n\t\t\t\tcasep += nat\n\t\t\t\tmols_done += 1\n\t\t\t\tsto = np.zeros(len(eles),dtype = np.int32)\n\t\t\t\toffsets = np.zeros(len(eles),dtype = np.int32)\n\t\t\t\tinputs = []\n\t\t\t\tmatrices = []\n\t\t\t\txyz = []\n\t\t\t\tnatom = []\n\t\t\t\toutputpointer = 0\n\t\t\t\tfor i in range (0, natoms):\n\t\t\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\t\t\tcurrentmol = 0\n\t\t\t\tfor e in range (len(eles)):\n\t\t\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\t\t\t\txyz.append(np.zeros((sto[e], 3)))\n\t\t\t\tfor i in range (0, natoms):\n\t\t\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\t\t\toutputpointer += 1\n\t\t\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\t\t\te = meta[i, 1]\n\t\t\t\t\tei = eles.index(e)\n\t\t\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\t\t\txyz[ei][offsets[ei]] = xyzmeta[i]\n\t\t\t\t\tnatom_in_mol[outputpointer] = meta[i,3] - meta[i,2]\n\t\t\t\t\toffsets[ei] += 1\n\t\t\t\tt = time.time()\n\t\t\t\tdipole, atomcharge = self.Instances.evaluate([inputs, matrices, xyz, 1.0/natom_in_mol, dummy_outputs])\n\t\telif (mol_set, MSet):\n\t\t\tnmols = len(mol_set.mols)\n\t\t\tnatoms = mol_set.NAtoms()\n\t\t\tprint(""number of molecules in the set:"", nmols)\n\t\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\t\tdummy_outputs = np.zeros((nmols, 3))\n\t\t\tnatom_in_mol = np.zeros((nmols, 1))\n\t\t\tnatom_in_mol.fill(float(\'inf\'))\n\t\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\t\txyzmeta = np.zeros((natoms, 3))\n\t\t\tcasep = 0\n\t\t\tmols_done = 0\n\t\t\tt = time.time()\n\t\t\tfor mol in mol_set.mols:\n\t\t\t\tins, grads = self.TData.dig.EvalDigest(mol)\n\t\t\t\tnat = mol.NAtoms()\n\t\t\t\txyz_centered = mol.coords - np.average(mol.coords, axis=0)\n\t\t\t\tcases[casep:casep+nat] = ins\n\t\t\t\tfor i in range (casep, casep+nat):\n\t\t\t\t\tmeta[i, 0] = mols_done\n\t\t\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\t\t\tmeta[i, 2] = casep\n\t\t\t\t\tmeta[i, 3] = casep + nat\n\t\t\t\t\txyzmeta[i] = xyz_centered[i - casep]\n\t\t\t\tcasep += nat\n\t\t\t\tmols_done += 1\n\t\t\tsto = np.zeros(len(eles),dtype = np.int32)\n\t\t\toffsets = np.zeros(len(eles),dtype = np.int32)\n\t\t\tinputs = []\n\t\t\tmatrices = []\n\t\t\txyz = []\n\t\t\tnatom = []\n\t\t\toutputpointer = 0\n\t\t\tfor i in range (0, natoms):\n\t\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\t\tcurrentmol = 0\n\t\t\tfor e in range (len(eles)):\n\t\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\t\t\txyz.append(np.zeros((sto[e], 3)))\n\t\t\tfor i in range (0, natoms):\n\t\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\t\toutputpointer += 1\n\t\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\t\te = meta[i, 1]\n\t\t\t\tei = eles.index(e)\n\t\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\t\txyz[ei][offsets[ei]] = xyzmeta[i]\n\t\t\t\tnatom_in_mol[outputpointer] = meta[i,3] - meta[i,2]\n\t\t\t\toffsets[ei] += 1\n\t\t\tt = time.time()\n\t\t\tdipole, atomcharge  = self.Instances.evaluate([inputs, matrices, xyz, 1.0/natom_in_mol, dummy_outputs], False)\n\t\t\t#print dipole, atomcharge\n\t\t\t#print  atomcharge\n\t\telse:\n\t\t\traise Exception(""wrong input"")\n\t\tmolatomcharge = []\n\t\tpointers = [0 for ele in eles]\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\ttmp_atomcharge = np.zeros(mol.NAtoms())\n\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\tatom_type = mol.atoms[j]\n\t\t\t\tatom_index = eles.index(atom_type)\n\t\t\t\ttmp_atomcharge[j] = atomcharge[atom_index][0][pointers[atom_index]]/BOHRPERA  # hacky way to do\n\t\t\t\tpointers[atom_index] +=1\n\t\t\tmolatomcharge.append(tmp_atomcharge)\n\t\treturn dipole, molatomcharge\n\n\tdef Eval_BPDipoleGrad_2(self, mol_set,  ScaleCharge_ = False):\n\t\t""""""\n\t\tcan take either a single mol or mol set\n\t\treturn dipole, atomcharge, gradient of the atomcharge\n\t\tDipole has unit in debye\n\t\t""""""\n\t\teles = self.Instances.eles\n\t\tif isinstance(mol_set, Mol):\n\t\t\ttmp = MSet()\n\t\t\ttmp.mols = [mol_set]\n\t\t\tmol_set = tmp\n\t\t\tnmols = len(mol_set.mols)\n\t\t\tnatoms = mol_set.NAtoms()\n\t\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\t\tcases_grads = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)+list([3*natoms])))\n\t\t\tdummy_outputs = np.zeros((nmols, 3))\n\t\t\tnatom_in_mol = np.zeros((nmols, 1))\n\t\t\tnatom_in_mol.fill(float(\'inf\'))\n\t\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\t\txyzmeta = np.zeros((natoms, 3))\n\t\t\tcasep = 0\n\t\t\tmols_done = 0\n\t\t\tfor mol in mol_set.mols:\n\t\t\t\tins, grads = self.TData.dig.EvalDigest(mol)\n\t\t\t\tnat = mol.NAtoms()\n\t\t\t\txyz_centered = mol.coords - np.average(mol.coords, axis=0)\n\t\t\t\tcases[casep:casep+nat] = ins\n\t\t\t\tcases_grads[casep:casep+nat] = grads\n\t\t\t\tfor i in range (casep, casep+nat):\n\t\t\t\t\tmeta[i, 0] = mols_done\n\t\t\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\t\t\tmeta[i, 2] = casep\n\t\t\t\t\tmeta[i, 3] = casep + nat\n\t\t\t\t\txyzmeta[i] = xyz_centered[i - casep]\n\t\t\t\tcasep += nat\n\t\t\t\tmols_done += 1\n\t\t\t\tsto = np.zeros(len(eles),dtype = np.int32)\n\t\t\t\toffsets = np.zeros(len(eles),dtype = np.int32)\n\t\t\t\tinputs = []\n\t\t\t\tinputs_grads = []\n\t\t\t\tmatrices = []\n\t\t\t\txyz = []\n\t\t\t\tnatom = []\n\t\t\t\toutputpointer = 0\n\t\t\t\tfor i in range (0, natoms):\n\t\t\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\t\t\tcurrentmol = 0\n\t\t\t\tfor e in range (len(eles)):\n\t\t\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\t\t\tinputs_grads.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape), 3*natoms)))\n\t\t\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\t\t\t\txyz.append(np.zeros((sto[e], 3)))\n\t\t\t\tfor i in range (0, natoms):\n\t\t\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\t\t\toutputpointer += 1\n\t\t\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\t\t\te = meta[i, 1]\n\t\t\t\t\tei = eles.index(e)\n\t\t\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\t\t\tinputs_grads[ei][offsets[ei], :]  = cases_grads[i]\n\t\t\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\t\t\txyz[ei][offsets[ei]] = xyzmeta[i]\n\t\t\t\t\tnatom_in_mol[outputpointer] = meta[i,3] - meta[i,2]\n\t\t\t\t\toffsets[ei] += 1\n\t\t\t\tt = time.time()\n\t\t\t\tdipole, atomcharge, charge_gradients = self.Instances.evaluate([inputs, matrices, xyz, 1.0/natom_in_mol, dummy_outputs], True)\n\t\t\t\ttotal_unscaled_gradient = []\n\t\t\t\tfor i in range (0, len(charge_gradients)): # Loop over element types.\n\t\t\t\t\ttotal_unscaled_gradient += list(np.einsum(""aij,ai->aj"", inputs_grads[i],  charge_gradients[i])) # Chain rule.\n\t\t\t\ttotal_unscaled_gradient  = np.asarray(total_unscaled_gradient)/BOHRPERA  # hacky way to do\n\t\t\t\ttotal_scaled_gradient =  total_unscaled_gradient - np.sum(total_unscaled_gradient, axis=0)/total_unscaled_gradient.shape[0]\n\t\t\t\ttotal_scaled_gradient_list = []\n\t\t\t\tele_pointer = 0\n\t\t\t\tfor i in range (0, len(charge_gradients)):\n\t\t\t\t\ttotal_scaled_gradient_list.append(total_scaled_gradient[ele_pointer:ele_pointer+charge_gradients[i].shape[0]])\n\t\t\t\t\tele_pointer += charge_gradients[i].shape[0]\n\n\t\telif (mol_set, MSet):\n\t\t\tnmols = len(mol_set.mols)\n\t\t\tnatoms = mol_set.NAtoms()\n\t\t\tcases = np.zeros(tuple([natoms]+list(self.TData.dig.eshape)))\n\t\t\tcases_grads = []\n\t\t\tdummy_outputs = np.zeros((nmols, 3))\n\t\t\tnatom_in_mol = np.zeros((nmols, 1))\n\t\t\tnatom_in_mol.fill(float(\'inf\'))\n\t\t\tmeta = np.zeros((natoms, 4), dtype = np.int)\n\t\t\txyzmeta = np.zeros((natoms, 3))\n\t\t\tcasep = 0\n\t\t\tmols_done = 0\n\t\t\tt = time.time()\n\t\t\tfor mol in mol_set.mols:\n\t\t\t\tins, grads = self.TData.dig.EvalDigest(mol)\n\t\t\t\tnat = mol.NAtoms()\n\t\t\t\txyz_centered = mol.coords - np.average(mol.coords, axis=0)\n\t\t\t\tcases[casep:casep+nat] = ins\n\t\t\t\tcases_grads += list(grads)\n\t\t\t\tfor i in range (casep, casep+nat):\n\t\t\t\t\tmeta[i, 0] = mols_done\n\t\t\t\t\tmeta[i, 1] = mol.atoms[i - casep]\n\t\t\t\t\tmeta[i, 2] = casep\n\t\t\t\t\tmeta[i, 3] = casep + nat\n\t\t\t\t\txyzmeta[i] = xyz_centered[i - casep]\n\t\t\t\tcasep += nat\n\t\t\t\tmols_done += 1\n\t\t\tsto = np.zeros(len(eles),dtype = np.int32)\n\t\t\toffsets = np.zeros(len(eles),dtype = np.int32)\n\t\t\tinputs = []\n\t\t\tinputs_grads = [[] for i in range (len(self.TData.eles))]\n\t\t\tmatrices = []\n\t\t\txyz = []\n\t\t\tnatom = []\n\t\t\toutputpointer = 0\n\t\t\tfor i in range (0, natoms):\n\t\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\t\tcurrentmol = 0\n\t\t\tfor e in range (len(eles)):\n\t\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\t\t\txyz.append(np.zeros((sto[e], 3)))\n\t\t\tatom_index_in_mol = [[] for i in range (len(self.TData.eles))]\n\t\t\tfor i in range (0, natoms):\n\t\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\t\toutputpointer += 1\n\t\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\t\te = meta[i, 1]\n\t\t\t\tei = eles.index(e)\n\t\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\t\tinputs_grads[ei].append(cases_grads[i])\n\t\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\t\txyz[ei][offsets[ei]] = xyzmeta[i]\n\t\t\t\tnatom_in_mol[outputpointer] = meta[i,3] - meta[i,2]\n\t\t\t\tatom_index_in_mol[ei].append(currentmol)\n\t\t\t\toffsets[ei] += 1\n\t\t\tt = time.time()\n\t\t\tdipole, atomcharge, charge_gradients  = self.Instances.evaluate([inputs, matrices, xyz, 1.0/natom_in_mol, dummy_outputs], True)\n\t\t\ttotal_scaled_gradient_list = []\n\t\t\tfor i in range (0, nmols):\n\t\t\t\tmol = mol_set.mols[i]\n\t\t\t\ttotal_unscaled_gradient=[]\n\t\t\t\tn_ele_in_mol = []\n\t\t\t\tfor j, ele in enumerate(self.TData.eles):\n\t\t\t\t\tele_index = [k for k, tmp_index in enumerate(atom_index_in_mol[j]) if tmp_index == i]\n\t\t\t\t\tn_ele_in_mol.append(len(ele_index))\n\t\t\t\t\tele_desp_grads = np.asarray([ tmp_array for k, tmp_array in enumerate(inputs_grads[j]) if k in ele_index])\n\t\t\t\t\tele_nn_grads = np.asarray([ tmp_array for k, tmp_array in enumerate(charge_gradients[j]) if k in ele_index])\n\t\t\t\t\ttotal_unscaled_gradient += list(np.einsum(""ad,adx->ax"", ele_nn_grads, ele_desp_grads))\n\t\t\t\t\ttotal_unscaled_gradient  = np.asarray(total_unscaled_gradient)/BOHRPERA  #hack way to do\n\t\t\t\t\ttotal_scaled_gradient =  total_unscaled_gradient - np.sum(total_unscaled_gradient, axis=0)/total_unscaled_gradient.shape[0]\n\t\t\t\t\ttotal_scaled_gradient_tmp = []\n\t\t\t\t\tele_pointer = 0\n\t\t\t\tfor j in range (0, len(n_ele_in_mol)):\n\t\t\t\t\ttotal_scaled_gradient_tmp.append(total_scaled_gradient[ele_pointer:ele_pointer+n_ele_in_mol[j]])\n\t\t\t\t\tele_pointer += n_ele_in_mol[j]\n\t\t\ttotal_scaled_gradient_list.append(total_scaled_gradient_tmp)\n\t\t\ttotal_scaled_gradient_list_tmp = [[] for i in range (len(self.TData.eles))]\n\t\t\tfor tmp in total_scaled_gradient_list:\n\t\t\t\tfor e in range (len(eles)):\n\t\t\t\t\ttotal_scaled_gradient_list_tmp[e] += list(tmp[e])\n\t\t\t\t\ttotal_scaled_gradient_list = total_scaled_gradient_list_tmp\n\t\telse:\n\t\t\traise Exception(""wrong input"")\n\t\tmolatomcharge = []\n\t\tmolatomcharge_gradient  = []\n\t\tpointers = [0 for ele in eles]\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\ttmp_atomcharge = np.zeros(mol.NAtoms())\n\t\t\ttmp_atomcharge_gradient = np.zeros((mol.NAtoms(), mol.NAtoms(), 3))\n\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\tatom_type = mol.atoms[j]\n\t\t\t\tatom_index = eles.index(atom_type)\n\t\t\t\ttmp_atomcharge[j] = atomcharge[atom_index][0][pointers[atom_index]]/BOHRPERA  #hacky way to do\n\t\t\t\ttmp_atomcharge_gradient[j] = total_scaled_gradient_list[atom_index][pointers[atom_index]].reshape((-1,3))\n\t\t\t\tpointers[atom_index] +=1\n\t\t\tmolatomcharge.append(tmp_atomcharge)\n\t\t\tmolatomcharge_gradient.append(tmp_atomcharge_gradient)\n\t\treturn dipole, molatomcharge, molatomcharge_gradient\n\n\n\tdef Eval_Bond_BP(self, mol_set, total_energy = False):\n\t\tnmols = len(mol_set.mols)\n\t\tnbonds = mol_set.NBonds()\n\t\tcases = np.zeros(tuple([nbonds]+list(self.TData.dig.eshape)))\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tmeta = np.zeros((nbonds, 4), dtype = np.int)\n\t\tcasep = 0\n\t\tmols_done = 0\n\t\tfor mol in mol_set.mols:\n\t\t\tins = self.TData.dig.EvalDigest(mol)\n\t\t\tnbo = mol.NBonds()\n\t\t\tcases[casep:casep+nbo] = ins\n\t\t\tfor i in range (casep, casep+nbo):\n\t\t\t\tmeta[i, 0] = mols_done\n\t\t\t\tmeta[i, 1] = mol.bonds[i - casep,0]\n\t\t\t\tmeta[i, 2] = casep\n\t\t\t\tmeta[i, 3] = casep + nbo\n\t\t\tcasep += nbo\n\t\t\tmols_done += 1\n\t\tsto = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\toffsets = np.zeros(len(self.TData.eles),dtype = np.int32)\n\t\tinputs = []\n\t\tmatrices = []\n\t\toutputpointer = 0\n\t\tfor i in range (0, nbonds):\n\t\t\tsto[self.TData.eles.index(meta[i, 1])] += 1\n\t\tcurrentmol = 0\n\t\tfor e in range (len(self.TData.eles)):\n\t\t\tinputs.append(np.zeros((sto[e], np.prod(self.TData.dig.eshape))))\n\t\t\tmatrices.append(np.zeros((sto[e], nmols)))\n\t\tfor i in range (0, nbonds):\n\t\t\tif currentmol != meta[i, 0]:\n\t\t\t\toutputpointer += 1\n\t\t\t\tcurrentmol = meta[i, 0]\n\t\t\te = meta[i, 1]\n\t\t\tei = self.TData.eles.index(e)\n\t\t\tinputs[ei][offsets[ei], :] = cases[i]\n\t\t\tmatrices[ei][offsets[ei], outputpointer] = 1.0\n\t\t\toffsets[ei] += 1\n\t\t#print ""[inputs, matrices, dummy_outputs]"", [inputs, matrices, dummy_outputs]\n\t\tmol_out, atom_out = self.Instances.evaluate([inputs, matrices, dummy_outputs])\n\n\t\tpointers = [0 for ele in self.TData.eles]\n\t\tdiff = 0\n\t\tfor i in range (0, nmols):\n\t\t\tmol = mol_set.mols[i]\n\t\t\tprint(""for mol :"", mol.name,"" energy:"", mol.energy)\n\t\t\tprint(""total atomization energy:"", mol_out[0][i])\n\t\t\t#diff += abs(mol.energy - mol_out[0][i])\n\t\t\tif total_energy:\n\t\t\t\ttotal = mol_out[0][i]\n\t\t\t\tfor j in range (0, mol.NAtoms()):\n\t\t\t\t\ttotal += ele_U[mol.atoms[j]]\n\t\t\t\tprint(""total electronic energy:"", total)\n\t\t\tfor j in range (0, mol.bonds.shape[0]):\n\t\t\t\tbond_type = mol.bonds[j, 0]\n\t\t\t\tbond_index = self.TData.eles.index(bond_type)\n\t\t\t\tprint(""bond: "", mol.bonds[j], "" energy:"", atom_out[bond_index][0][pointers[bond_index]])\n\t\t\t\tpointers[bond_index] += 1\n\t\t#print ""mol out:"", mol_out, "" atom_out"", atom_out\n\t\t#return\tdiff / nmols\n\t\treturn\n\n\tdef EvalBPPairPotential(self):\n\t\treturn self.Instances.Evaluate()\n\n\n\tdef Eval_Mol(self, mol):\n\t\ttotal_case = len(mol.mbe_frags[self.TData.order])\n\t\tif total_case == 0:\n\t\t\treturn 0.0\n\t\tnatom = mol.mbe_frags[self.TData.order][0].NAtoms()\n\t\tcases = np.zeros((total_case, self.TData.dig.eshape))\n\t\tcases_deri = np.zeros((total_case, natom, natom, 6)) # x1,y1,z1,x2,y2,z2\n\t\tcasep = 0\n\t\tfor frag in mol.mbe_frags[self.TData.order]:\n\t\t\tins, embed_deri =  self.TData.dig.EvalDigest(frag)\n\t\t\tcases[casep:casep+1] += ins\n\t\t\tcases_deri[casep:casep+1]=embed_deri\n\t\t\tcasep += 1\n\t\tprint(""evaluating order:"", self.TData.order)\n\t\tnn, nn_deri=self.Eval(cases)\n\t\t#print ""nn:"",nn, ""nn_deri:"",nn_deri, ""cm_deri:"", cases_deri, ""cases:"",cases, ""coord:"", mol.coords\n\t\tmol.Set_Frag_Force_with_Order(cases_deri, nn_deri, self.TData.order)\n\t\treturn nn.sum()\n\n\tdef Eval_BPEnergy_Direct(self, mol_set):\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_outputs = np.zeros((nmols))\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\tmol_out, atom_out,gradient = self.Instances.evaluate([xyzs, Zs, dummy_outputs], True)\n\t\treturn mol_out, atom_out, gradient\n\n\tdef Eval_BPEnergy_Direct_Grad(self, mol, Grad=True, Energy=True):\n\t\tmol_set = MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\tdummy_outputs = np.zeros((nmols))\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\tmol_out, atom_out,gradient = self.Instances.evaluate([xyzs, Zs, dummy_outputs, dummy_grads])\n\t\tif Grad and Energy:\n\t\t\treturn mol_out[0], -JOULEPERHARTREE*gradient[0][0][:mol.NAtoms()]\n\t\telif Energy and not Grad:\n\t\t\treturn mol_out[0]\n\t\telse:\n\t\t\treturn -JOULEPERHARTREE*gradient[0][0][:mol.NAtoms()]\n\n\tdef Eval_BPEnergy_Direct_Grad_Linear(self, mol, Grad=True, Energy=True):\n\t\t""""""\n\t\tTHIS IS THE only working LINEAR evaluate routine, so far.\n\t\tAlso generates angular pairs, triples. etc.\n\t\t""""""\n\t\tmol_set = MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\tNLs = NeighborListSet(xyzs, np.array([mol.NAtoms()]), True, True, Zs)\n\t\tNLs.Update(xyzs,PARAMS[""AN1_r_Rc""],PARAMS[""AN1_a_Rc""])\n\t\t#print NLs.pairs\n\t\t#print NLs.triples\n\t\tmol_out, atom_out, gradient = self.Instances.evaluate([xyzs, Zs, NLs.pairs, NLs.triples])\n\t\tif Grad and Energy:\n\t\t\treturn mol_out[0], -JOULEPERHARTREE*gradient[0][0][:mol.NAtoms()]\n\t\telif Energy and not Grad:\n\t\t\treturn mol_out[0]\n\t\telse:\n\t\t\treturn -JOULEPERHARTREE*gradient[0][0][:mol.NAtoms()]\n\n\tdef EvalBPDirectSingleEnergyWGrad(self, mol):\n\t\t""""""\n\t\tThe energy and force routine for Kun\'s new direct BPs.\n\t\t""""""\n\t\tmol_set=MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_outputs = np.zeros((nmols))\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\tmol_out, atom_out, gradient = self.Instances.evaluate([xyzs, Zs, dummy_outputs, dummy_grads], True)\n\t\treturn mol_out[0], -JOULEPERHARTREE*gradient[0][0][:mol.NAtoms()]\n\n\tdef EvalBPDirectEESingle(self, mol, Rr_cut, Ra_cut, Ree_cut):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE.\n\t\t""""""\n\t\tmol_set=MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs)\n\t\trad_p, ang_t = NL.buildPairsAndTriples(Rr_cut, Ra_cut)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(Ree_cut)\n\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient  = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p, ang_t, rad_eep, 1.0/natom])\n\t\treturn Etotal, Ebp, Ecc, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0]\n\n\tdef EvalBPDirectEESet(self, mol_set, Rr_cut=PARAMS[""AN1_r_Rc""], Ra_cut=PARAMS[""AN1_a_Rc""], Ree_cut=PARAMS[""EECutoffOff""]):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE.\n\n\t\tReturns:\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient\n\t\t""""""\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\tself.TData.MaxNAtoms = mol_set.MaxNAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs)\n\t\trad_p, ang_t = NL.buildPairsAndTriples(Rr_cut, Ra_cut)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(Ree_cut)\n\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient  = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p, ang_t, rad_eep, 1.0/natom])\n\t\treturn Etotal, Ebp, Ecc, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0]\n\n\tdef EvalBPDirectEEUpdateSet(self, mol_set, Rr_cut, Ra_cut, Ree_cut, HasVdw = False):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE. Evaluate a Set\n\t\t""""""\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\tself.TData.MaxNAtoms = mol_set.MaxNAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(Rr_cut, Ra_cut, self.Instances.eles_np, self.Instances.eles_pairs_np)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(Ree_cut)\n\t\tif not HasVdw:\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient  = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom])\n\t\t\treturn Etotal, Ebp, Ecc, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0]\n\t\telse:\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw,  mol_dipole, atom_charge, gradient = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom])\n\t\t\t#print (""Etotal:"", Etotal)\n\t\t\t#Etotal, Ebp, Ebp_atom, Ecc, Evdw,  mol_dipole, atom_charge, gradient, bp_gradient, syms  = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom])\n\t\t\treturn Etotal, Ebp, Ebp_atom ,Ecc, Evdw, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0]\n\t\t\t#return Etotal, Ebp, Ebp_atom ,Ecc, Evdw, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0], bp_gradient, syms\n\n\t@TMTiming(""EvalBPDirectEEUpdateSingle"")\n\tdef EvalBPDirectEEUpdateSingle(self, mol, Rr_cut, Ra_cut, Ree_cut, HasVdw = False):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE.\n\t\t""""""\n\t\tmol_set=MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_jk, jk_max = NL.buildPairsAndTriplesWithEleIndex(Rr_cut, Ra_cut, self.Instances.eles_np, self.Instances.eles_pairs_np)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(Ree_cut)\n\t\tif not HasVdw:\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient  = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom])\n\t\t\treturn Etotal, Ebp, Ecc, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0]\n\t\telse:\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw,  mol_dipole, atom_charge, gradient = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom])\n\t\t\t#print (""Etotal:"", Etotal)\n\t\t\t#Etotal, Ebp, Ebp_atom, Ecc, Evdw,  mol_dipole, atom_charge, gradient, bp_gradient, syms  = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom])\n\t\t\treturn Etotal, Ebp, Ebp_atom ,Ecc, Evdw, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0]\n\t\t\t#return Etotal, Ebp, Ebp_atom ,Ecc, Evdw, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0], bp_gradient, syms\n\n\t@TMTiming(""EvalBPDirectEEUpdateSinglePeriodic"")\n\tdef EvalBPDirectEEUpdateSinglePeriodic(self, mol, Rr_cut, Ra_cut, Ree_cut, nreal, HasVdw = True, DoForce=True, DoCharge=False):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE.\n\t\t""""""\n\t\tt = time.time()\n\t\tmol_set=MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = nreal   # this is hacky.. K.Y.\n\t\tNL = NeighborListSetWithImages(xyzs, np.array([mol.NAtoms()]), np.array([nreal]),True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexPeriodic(Rr_cut, Ra_cut, self.Instances.eles_np, self.Instances.eles_pairs_np)\n\t\tNLEE = NeighborListSetWithImages(xyzs, np.array([mol.NAtoms()]), np.array([nreal]), False, True,  Zs)\n\t\trad_eep_e1e2 = NLEE.buildPairsWithBothEleIndex(Ree_cut, self.Instances.eles_np)\n\t\t#print (""python code time:"", time.time() - t)\n\t\tt = time.time()\n\t\tif (DoForce):\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw,  mol_dipole, atom_charge, gradient  = self.Instances.evaluate_periodic([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep_e1e2, mil_j, mil_jk, 1.0/natom], nreal)\n\t\t\t#print (""tf code time:"", time.time() - t)\n\t\t\tif not DoCharge:\n\t\t\t\treturn Etotal, -JOULEPERHARTREE*gradient[0][0][:nreal].reshape(1, nreal, 3)  # be consist with old code\n\t\t\telse:\n\t\t\t\treturn Etotal, -JOULEPERHARTREE*gradient[0][0][:nreal].reshape(1, nreal, 3), atom_charge[0][:nreal].reshape(1, nreal)\n\t\telse:\n\t\t\tEtotal = self.Instances.evaluate_periodic([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep_e1e2, mil_j, mil_jk, 1.0/natom], nreal, False)\n\t\t\treturn Etotal\n\n\tdef EvalBPDirectEandGLinearSingle(self, mol, Rr_cut, Ra_cut):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE.\n\t\t""""""\n\t\tmol_set=MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(Rr_cut, Ra_cut, self.Instances.eles_np, self.Instances.eles_pairs_np)\n\t\tEtotal, Ebp, Ebp_atom, gradient = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_grads, rad_p_ele, ang_t_elep,  mil_j, mil_jk, 1.0/natom])\n\t\treturn Etotal, Ebp, Ebp_atom, -JOULEPERHARTREE*gradient[0]\n\n\tdef EvalBPDirectEandGLinearSinglePeriodic(self, mol, Rr_cut, Ra_cut, nreal, DoForce=True):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE.\n\t\t""""""\n\t\tt = time.time()\n\t\tmol_set=MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = nreal   # this is hacky.. K.Y.\n\t\tNL = NeighborListSetWithImages(xyzs, np.array([mol.NAtoms()]), np.array([nreal]),True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexPeriodic(Rr_cut, Ra_cut, self.Instances.eles_np, self.Instances.eles_pairs_np)\n\t\tt = time.time()\n\t\tif (DoForce):\n\t\t\tEtotal, Ebp, Ebp_atom, gradient  = self.Instances.evaluate_periodic([xyzs, Zs, dummy_energy, dummy_grads, rad_p_ele, ang_t_elep, mil_j, mil_jk, 1.0/natom], nreal)\n\t\t\t#print (""tf code time:"", time.time() - t)\n\t\telse:\n\t\t\tEtotal = self.Instances.evaluate_periodic([xyzs, Zs, dummy_energy, dummy_grads, rad_p_ele, ang_t_elep,  mil_j, mil_jk, 1.0/natom], nreal, False)\n\t\t\treturn Etotal\n\n\tdef EvalBPDirectEELinearSingle(self, mol, Rr_cut, Ra_cut, Ree_cut, HasVdw = False):\n\t\t""""""\n\t\tThe energy, force and dipole routine for BPs_EE.\n\t\t""""""\n\t\tmol_set=MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_energy = np.zeros((nmols))\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tdummy_grads = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(Rr_cut, Ra_cut, self.Instances.eles_np, self.Instances.eles_pairs_np)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  None)\n\t\trad_eep = NLEE.buildPairs(Ree_cut)\n\t\tif not HasVdw:\n\t\t\tEtotal, Ebp, Ecc, mol_dipole, atom_charge, gradient  = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_j, mil_jk, 1.0/natom])\n\t\t\treturn Etotal, Ebp, Ecc, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0]\n\t\telse:\n\t\t\tEtotal, Ebp, Ebp_atom, Ecc, Evdw,  mol_dipole, atom_charge, gradient = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_j, mil_jk, 1.0/natom])\n\t\t\t#print (""Etotal:"", Etotal)\n\t\t\t#Etotal, Ebp, Ebp_atom, Ecc, Evdw,  mol_dipole, atom_charge, gradient, bp_gradient, syms  = self.Instances.evaluate([xyzs, Zs, dummy_energy, dummy_dipole, dummy_grads, rad_p_ele, ang_t_elep, rad_eep, mil_jk, 1.0/natom])\n\t\t\treturn Etotal, Ebp, Ebp_atom ,Ecc, Evdw, mol_dipole, atom_charge, -JOULEPERHARTREE*gradient[0]\n\n\tdef EvalBPDirectEELinearSinglePeriodic(self, mol, Rr_cut, Ra_cut, Ree_cut, nreal, HasVdw = True, DoForce=True, DoCharge=False):\n\t\treturn self.EvalBPDirectEEUpdateSinglePeriodic( mol, Rr_cut, Ra_cut, Ree_cut, nreal, HasVdw, DoForce, DoCharge)\n\n\tdef EvalBPDirectChargeSingle(self, mol, Rr_cut, Ra_cut, Ree_cut, HasVdw = False):\n\t\t""""""\n\t\tThe dipole routine for charge.\n\t\t""""""\n\t\tmol_set=MSet()\n\t\tmol_set.mols.append(mol)\n\t\tnmols = len(mol_set.mols)\n\t\tdummy_dipole = np.zeros((nmols, 3))\n\t\tself.TData.MaxNAtoms = mol.NAtoms()\n\t\txyzs = np.zeros((nmols, self.TData.MaxNAtoms, 3), dtype = np.float64)\n\t\tZs = np.zeros((nmols, self.TData.MaxNAtoms), dtype = np.int32)\n\t\tnatom = np.zeros((nmols), dtype = np.int32)\n\t\tfor i, mol in enumerate(mol_set.mols):\n\t\t\txyzs[i][:mol.NAtoms()] = mol.coords\n\t\t\tZs[i][:mol.NAtoms()] = mol.atoms\n\t\t\tnatom[i] = mol.NAtoms()\n\t\tNL = NeighborListSet(xyzs, natom, True, True, Zs, sort_=True)\n\t\trad_p_ele, ang_t_elep, mil_j, mil_jk = NL.buildPairsAndTriplesWithEleIndexLinear(Rr_cut, Ra_cut, self.Instances.eles_np, self.Instances.eles_pairs_np)\n\t\tNLEE = NeighborListSet(xyzs, natom, False, False,  Zs)\n\t\trad_eep_e1e2 = NLEE.buildPairsWithBothEleIndex(Ree_cut, self.Instances.eles_np, True)\n\t\tmol_dipole, atom_charge = self.Instances.evaluate([xyzs, Zs, dummy_dipole, rad_p_ele, ang_t_elep, rad_eep_e1e2, mil_j, mil_jk, 1.0/natom])\n\t\treturn mol_dipole, atom_charge\n\n\t@TMTiming(""TFMolMangePrepare"")\n\tdef Prepare(self):\n\t\tself.Load()\n\t\tself.Instances= None # In order of the elements in TData\n\t\tif (self.NetType == ""fc_classify""):\n\t\t\tself.Instances = MolInstance_fc_classify(None,  self.TrainedNetworks[0], None, Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff(None, self.TrainedNetworks[0], None, Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff_BP(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Update""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff_BP_Update(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct""):\n\t\t\tself.Instances = MolInstance_DirectBP_NoGrad(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BPBond_Direct""):\n\t\t\tself.Instances = MolInstance_DirectBPBond_NoGrad(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad_noGradTrain""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad_noGradTrain(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad_NewIndex""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad_NewIndex(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad_Linear""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad_Linear(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Grad_Linear_EmbOpt""):\n\t\t\tself.Instances = MolInstance_DirectBP_Grad_Linear_EmbOpt(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_Update""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_Update(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_NoGradTrain""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_NoGradTrain(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_AvgPool""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_AvgPool(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_InputNorm""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_InputNorm(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_Conv""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_ChargeEncode_Update_vdw_DSF_elu_Normalize_Dropout_Conv(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EE_SymFunction""):\n\t\t\tself.Instances = MolInstance_DirectBP_EE_SymFunction(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_EandG_SymFunction""):\n\t\t\tself.Instances = MolInstance_DirectBP_EandG_SymFunction(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""fc_sqdiff_BP_Direct_Charge_SymFunction""):\n\t\t\tself.Instances = MolInstance_DirectBP_Charge_SymFunction(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""Dipole_BP""):\n\t\t\tself.Instances = MolInstance_BP_Dipole(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""Dipole_BP_2""):\n\t\t\tself.Instances = MolInstance_BP_Dipole_2(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telif (self.NetType == ""Dipole_BP_2_Direct""):\n\t\t\tself.Instances = MolInstance_BP_Dipole_2_Direct(None,self.TrainedNetworks[0], Trainable_ = self.Trainable)\n\t\telse:\n\t\t\traise Exception(""Unknown Network Type!"")\n\t\t# Raise TF instances for each atom which have already been trained.\n\t\treturn\n\n# This has to be totally re-written to be more like the\n# testing in TFInstance.\n\n\tdef Test(self, save_file=""mbe_test.dat""):\n\t\tti, to = self.TData.LoadData( True)\n\t\tNTest = int(self.TData.TestRatio * ti.shape[0])\n\t\tti= ti[ti.shape[0]-NTest:]\n\t\tto = to[to.shape[0]-NTest:]\n\t\tacc_nn = np.zeros((to.shape[0],2))\n\t\tnn, gradient=self.Eval(ti)\n\t\tacc_nn[:,0]=acc.reshape(acc.shape[0])\n\t\tacc_nn[:,1]=nn.reshape(nn.shape[0])\n\t\tmean, std = self.TData.Get_Mean_Std()\n\t\tacc_nn = acc_nn*std+mean\n\t\tnp.savetxt(save_file,acc_nn)\n\t\tnp.savetxt(""dist_2b.dat"", ti[:,1])\n\t\treturn\n\nclass TFMolManageDirect:\n\tdef __init__(self, mol_set_name=None, name=None, train=True, network_type=""BehlerParinelloDirectSymFunc""):\n\t\t""""""\n\t\t\tArgs:\n\t\t\t\tName_: If not blank, will try to load a network with that name using Prepare()\n\t\t\t\tTData_: A TensorData instance to provide and process data.\n\t\t\t\tTrain_: Whether to train the instances raised.\n\t\t\t\tNetType_: Choices of Various network architectures.\n\t\t\t\tntrain_: Number of steps to train an element.\n\t\t""""""\n\t\tself.path = ""./networks/""\n\n\t\tif (name != None):\n\t\t\tself.name = name\n\t\t\tself.prepare()\n\t\t\treturn\n\t\tself.mol_set_name = mol_set_name\n\t\tself.network_type = network_type\n\t\tself.name = self.network_type+""_""+self.mol_set_name+""_""+time.strftime(""%a_%b_%d_%H.%M.%S_%Y"")\n\t\tif (train):\n\t\t\tself.train()\n\t\t\treturn\n\t\treturn\n\n\tdef __getstate__(self):\n\t\tstate = self.__dict__.copy()\n\t\tdel state[""network""]\n\t\treturn state\n\n\tdef train(self, maxstep=3000):\n\t\t""""""\n\t\tInstantiates and trains a Molecular network.\n\n\t\tArgs:\n\t\t\tmaxstep: The number of training steps.\n\t\t""""""\n\t\tself.init_network()\n\t\tself.network_name = self.network.name\n\t\tself.save()\n\t\tself.network.start_training()\n\t\treturn\n\n\tdef restart_training(self):\n\t\tprint(""Loading previous network to continue training..."")\n\t\tself.network.restart_training()\n\n\tdef init_network(self):\n\t\tif self.network_type == ""BPSymFunc"":\n\t\t\tself.network = BehlerParinelloSymFunc(self.mol_set_name)\n\t\telif self.network_type == ""BPGauSH"":\n\t\t\tself.network = BehlerParinelloGauSH(self.mol_set_name)\n\t\telse:\n\t\t\traise Exception(""Unknown Network Type!"")\n\t\treturn\n\n\tdef save(self):\n\t\tprint(""Saving TFManager:"",self.path+self.name+"".tfm"")\n\t\tf = open(self.path+self.name+"".tfm"",""wb"")\n\t\tpickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n\t\tf.close()\n\t\treturn\n\n\tdef load(self):\n\t\tprint(""Loading TFManager..."")\n\t\t# from ..Containers.PickleTM import UnPickleTM as UnPickleTM\n\t\ttmp = pickle.load(open(self.path+self.name+"".tfm"", ""rb""))\n\t\t# tmp = UnPickleTM(self.path+self.name+"".tfm"")\n\t\tself.__dict__.update(tmp.__dict__)\n\t\tprint(""TFManager Loaded, Reviving Networks."")\n\t\treturn\n\n\tdef prepare(self):\n\t\tself.load()\n\t\tif self.network_type == ""BehlerParinelloDirectSymFunc"":\n\t\t\tself.network = BehlerParinelloDirectSymFunc(name=self.network_name)\n\t\telif (self.network_type == ""BehlerParinelloDirectGauSH""):\n\t\t\tself.network = BehlerParinelloDirectGauSH(name=self.network_name)\n\t\telse:\n\t\t\traise Exception(""Unknown Network Type!"")\n\t\treturn\n\n\tdef evaluate_mol(self, mol, eval_forces=True):\n\t\t""""""\n\t\tEvaluates the energies on a molecule from a network with direct embedding\n\n\t\tArgs:\n\t\t\tmol (TensorMol.Mol): a TensorMol Mol object with n atoms and nx3 coordinates\n\n\t\tReturns:\n\t\t\tenergy (np.float): a numpy float of molecular energy\n\t\t""""""\n\t\tif eval_forces:\n\t\t\tenergy, forces = self.network.evaluate_mol(mol, True)\n\t\t\treturn energy, forces\n\t\telse:\n\t\t\tenergy = self.network.evaluate_mol(mol, False)\n\t\t\treturn energy\n\n\tdef evaluate_batch(self, mols, eval_forces=True):\n\t\t""""""\n\t\tEvaluates the energies on a batch of molecules from a network with direct embedding\n\n\t\tArgs:\n\t\t\tmol (list): a list of TensorMol Mol object with n atoms and nx3 coordinates\n\n\t\tReturns:\n\t\t\tenergy (np.float): a numpy float of molecular energy\n\t\t""""""\n\t\tif eval_forces:\n\t\t\tenergy, forces = self.network.evaluate_batch(mols, True)\n\t\t\treturn energy, forces\n\t\telse:\n\t\t\tenergy = self.network.evaluate_batch(mols, False)\n\t\t\treturn energy\n'"
TensorMol/TFNetworks/TFMolManageQ.py,1,"b'""""""\nAn asynchronous re-write of TFMolManage.\nThe manager will now direct the training set to generate batches\nand manage a random queue to accept those batches.\n THIS WHOLE THING IS BASICALLY NOT WORKING AND UNAPPROACHED\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom .TFManage import *\nfrom ..Containers.TensorMolData import *\nfrom .TFMolInstance import *\nimport numpy as np\nimport gc\n\nclass TFMolQManage(TFMolManage):\n\t""""""\n\tAs of now only Behler-Parinello will be supported here.\n\t""""""\n\tdef __init__(self, Name_="""", TData_=None, Train_=False, NetType_=""fc_sqdiff"", RandomTData_=True):\n\t\t""""""\n\t\t\tArgs:\n\t\t\t\tName_: If not blank, will try to load a network with that name using Prepare()\n\t\t\t\tTData_: A TensorMolData instance to provide and process data.\n\t\t\t\tTrain_: Whether to train the instances raised.\n\t\t\t\tNetType_: Choices of Various network architectures.\n\t\t\t\tRandomTData_: Modifes the preparation of training batches.\n\t\t""""""\n\t\tTFManage.__init__(self, """", TData_, False, NetType_, RandomTData_)\n\t\tself.name = ""Mol_""+self.TData.name+""_""+self.TData.dig.name+""_""+self.NetType+""_""+str(self.TData.order)\n\t\tif (Name_!=""""):\n\t\t\tself.name = Name_\n\t\t\tself.Prepare()\n\t\t\treturn\n\t\tself.TrainedAtoms=[] # In order of the elements in TData\n\t\tself.TrainedNetworks=[] # In order of the elements in TData\n\t\tself.Instances=None # In order of the elements in TData\n\t\t#capacity, min_after_dequeue, dtypes, shapes=None, names=None, seed=None, shared_name=None, name=\'random_shuffle_queue\'\n\t\tself.BatchCapacity = 200\n\t\tself.BatchQueue = None\n\t\tif (Train_):\n\t\t\tself.Train()\n\t\t\treturn\n\t\treturn\n\n\tdef Train(self, maxstep=3000):\n\t\t""""""\n\t\tInstantiates and trains a Molecular network.\n\n\t\tArgs:\n\t\t\tmaxstep: The number of training steps.\n\t\t""""""\n\t\tif (self.TData.dig.eshape==None):\n\t\t\traise Exception(""Must Have Digester"")\n\t\t# It\'s up the TensorData to provide the batches and input output shapes.\n\t\tif (self.NetType == ""fc_classify""):\n\t\t\tself.Instances = MolInstance_fc_classify(self.TData, None)\n\t\telif (self.NetType == ""fc_sqdiff""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff(self.TData, None)\n\t\telif (self.NetType == ""fc_sqdiff_BP""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff_BP(self.TData)\n\t\telse:\n\t\t\traise Exception(""Unknown Network Type!"")\n\t\tself.Instances.train(maxstep) # Just for the sake of debugging.\n\t\tnm = self.Instances.name\n\t\t# Here we should print some summary of the pupil\'s progress as well, maybe.\n\t\tif self.TrainedNetworks.count(nm)==0:\n\t\t\tself.TrainedNetworks.append(nm)\n\t\tself.Save()\n\t\tgc.collect()\n\t\treturn\n\n\tdef Eval(self, test_input):\n\t\treturn self.Instances.evaluate(test_input)\n\n\tdef Eval_Mol(self, mol):\n\t\ttotal_case = len(mol.mbe_frags[self.TData.order])\n\t\tif total_case == 0:\n\t\t\treturn 0.0\n\t\tnatom = mol.mbe_frags[self.TData.order][0].NAtoms()\n\t\tcases = np.zeros((total_case, self.TData.dig.eshape))\n\t\tcases_deri = np.zeros((total_case, natom, natom, 6)) # x1,y1,z1,x2,y2,z2\n\t\tcasep = 0\n\t\tfor frag in mol.mbe_frags[self.TData.order]:\n\t\t\tins, embed_deri =  self.TData.dig.EvalDigest(frag)\n\t\t\tcases[casep:casep+1] += ins\n\t\t\tcases_deri[casep:casep+1]=embed_deri\n\t\t\tcasep += 1\n\t\tprint(""evaluating order:"", self.TData.order)\n\t\tnn, nn_deri=self.Eval(cases)\n\t\tmean, std = self.TData.Get_Mean_Std()\n\t\tnn = nn*std+mean\n\t\tnn_deri = nn_deri*std\n\t\t#print ""nn:"",nn, ""nn_deri:"",nn_deri, ""cm_deri:"", cases_deri, ""cases:"",cases, ""coord:"", mol.coords\n\t\tmol.Set_Frag_Force_with_Order(cases_deri, nn_deri, self.TData.order)\n\t\treturn nn.sum()\n\n\tdef Prepare(self):\n\t\tself.Load()\n\t\tself.Instances= None # In order of the elements in TData\n\t\tif (self.NetType == ""fc_classify""):\n\t\t\tself.Instances = MolInstance_fc_classify(None,  self.TrainedNetworks[0], None)\n\t\telif (self.NetType == ""fc_sqdiff""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff(None, self.TrainedNetworks[0], None)\n\t\telif (self.NetType == ""fc_sqdiff_BP""):\n\t\t\tself.Instances = MolInstance_fc_sqdiff_BP(None,self.TrainedNetworks[0],None)\n\t\telse:\n\t\t\traise Exception(""Unknown Network Type!"")\n\n\t\tself.BatchQueue = tf.RandomShuffleQueue(capacity, min_after_dequeue, dtypes, shapes=None, names=None, seed=None, shared_name=None, name=\'random_shuffle_queue\')\n\n\t\treturn\n\n# This has to be totally re-written to be more like the\n# testing in TFInstance.\n\n\tdef Test(self, save_file=""mbe_test.dat""):\n\t\tti, to = self.TData.LoadData( True)\n\t\tNTest = int(self.TData.TestRatio * ti.shape[0])\n\t\tti= ti[ti.shape[0]-NTest:]\n\t\tto = to[to.shape[0]-NTest:]\n\t\tacc_nn = np.zeros((to.shape[0],2))\n\t\tnn, gradient=self.Eval(ti)\n\t\tacc_nn[:,0]=acc.reshape(acc.shape[0])\n\t\tacc_nn[:,1]=nn.reshape(nn.shape[0])\n\t\tmean, std = self.TData.Get_Mean_Std()\n\t\tacc_nn = acc_nn*std+mean\n\t\tnp.savetxt(save_file,acc_nn)\n\t\tnp.savetxt(""dist_2b.dat"", ti[:,1])\n\t\treturn\n'"
TensorMol/TFNetworks/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom .TFMolManage import *\nfrom .TFMolInstanceDirect import *\nfrom .TFMolInstanceEE import *\n'
