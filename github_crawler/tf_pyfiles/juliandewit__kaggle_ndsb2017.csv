file_path,api_count,code
helpers.py,0,"b'import settings\r\nimport glob\r\nimport datetime\r\nimport os\r\nimport sys\r\nimport numpy\r\nimport cv2\r\nfrom collections import defaultdict\r\nfrom skimage.segmentation import clear_border\r\nfrom skimage.measure import label, regionprops\r\nfrom skimage.morphology import disk, dilation, binary_erosion, binary_closing\r\nfrom skimage.filters import roberts, sobel\r\nfrom scipy import ndimage as ndi\r\nimport math\r\nimport pandas\r\n\r\ndef compute_dice(label_img, pred_img, p_threshold=0.5):\r\n    p = pred_img.astype(numpy.float)\r\n    l = label_img.astype(numpy.float)\r\n    if p.max() > 127:\r\n        p /= 255.\r\n    if l.max() > 127:\r\n        l /= 255.\r\n\r\n    p = numpy.clip(p, 0, 1.0)\r\n    l = numpy.clip(l, 0, 1.0)\r\n    p[p > 0.5] = 1.0\r\n    p[p < 0.5] = 0.0\r\n    l[l > 0.5] = 1.0\r\n    l[l < 0.5] = 0.0\r\n    product = numpy.dot(l.flatten(), p.flatten())\r\n    dice_num = 2 * product + 1\r\n    pred_sum = p.sum()\r\n    label_sum = l.sum()\r\n    dice_den = pred_sum + label_sum + 1\r\n    dice_val = dice_num / dice_den\r\n    return dice_val\r\n\r\n\r\nclass Stopwatch(object):\r\n\r\n    def start(self):\r\n        self.start_time = Stopwatch.get_time()\r\n\r\n    def get_elapsed_time(self):\r\n        current_time = Stopwatch.get_time()\r\n        res = current_time - self.start_time\r\n        return res\r\n\r\n    def get_elapsed_seconds(self):\r\n        elapsed_time = self.get_elapsed_time()\r\n        res = elapsed_time.total_seconds()\r\n        return res\r\n\r\n    @staticmethod\r\n    def get_time():\r\n        res = datetime.datetime.now()\r\n        return res\r\n\r\n    @staticmethod\r\n    def start_new():\r\n        res = Stopwatch()\r\n        res.start()\r\n        return res\r\n\r\n\r\ndef load_patient_images(patient_id, base_dir=None, wildcard=""*.*"", exclude_wildcards=[]):\r\n    if base_dir == None:\r\n        base_dir = settings.LUNA_16_TRAIN_DIR\r\n    src_dir = base_dir + patient_id + ""/""\r\n    src_img_paths = glob.glob(src_dir + wildcard)\r\n    for exclude_wildcard in exclude_wildcards:\r\n        exclude_img_paths = glob.glob(src_dir + exclude_wildcard)\r\n        src_img_paths = [im for im in src_img_paths if im not in exclude_img_paths]\r\n    src_img_paths.sort()\r\n    images = [cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) for img_path in src_img_paths]\r\n    images = [im.reshape((1, ) + im.shape) for im in images]\r\n    res = numpy.vstack(images)\r\n    return res\r\n\r\n\r\ndef save_cube_img(target_path, cube_img, rows, cols):\r\n    assert rows * cols == cube_img.shape[0]\r\n    img_height = cube_img.shape[1]\r\n    img_width = cube_img.shape[1]\r\n    res_img = numpy.zeros((rows * img_height, cols * img_width), dtype=numpy.uint8)\r\n\r\n    for row in range(rows):\r\n        for col in range(cols):\r\n            target_y = row * img_height\r\n            target_x = col * img_width\r\n            res_img[target_y:target_y + img_height, target_x:target_x + img_width] = cube_img[row * cols + col]\r\n\r\n    cv2.imwrite(target_path, res_img)\r\n\r\n\r\ndef load_cube_img(src_path, rows, cols, size):\r\n    img = cv2.imread(src_path, cv2.IMREAD_GRAYSCALE)\r\n    # assert rows * size == cube_img.shape[0]\r\n    # assert cols * size == cube_img.shape[1]\r\n    res = numpy.zeros((rows * cols, size, size))\r\n\r\n    img_height = size\r\n    img_width = size\r\n\r\n    for row in range(rows):\r\n        for col in range(cols):\r\n            src_y = row * img_height\r\n            src_x = col * img_width\r\n            res[row * cols + col] = img[src_y:src_y + img_height, src_x:src_x + img_width]\r\n\r\n    return res\r\n\r\n\r\ndef get_normalized_img_unit8(img):\r\n    img = img.astype(numpy.float)\r\n    min = img.min()\r\n    max = img.max()\r\n    img -= min\r\n    img /= max - min\r\n    img *= 255\r\n    res = img.astype(numpy.uint8)\r\n    return res\r\n\r\n\r\ndef normalize_hu(image):\r\n    MIN_BOUND = -1000.0\r\n    MAX_BOUND = 400.0\r\n    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\r\n    image[image > 1] = 1.\r\n    image[image < 0] = 0.\r\n    return image\r\n\r\n\r\ndef rescale_patient_images(images_zyx, org_spacing_xyz, target_voxel_mm, is_mask_image=False, verbose=False):\r\n    if verbose:\r\n        print(""Spacing: "", org_spacing_xyz)\r\n        print(""Shape: "", images_zyx.shape)\r\n\r\n    # print ""Resizing dim z""\r\n    resize_x = 1.0\r\n    resize_y = float(org_spacing_xyz[2]) / float(target_voxel_mm)\r\n    interpolation = cv2.INTER_NEAREST if is_mask_image else cv2.INTER_LINEAR\r\n    res = cv2.resize(images_zyx, dsize=None, fx=resize_x, fy=resize_y, interpolation=interpolation)  # opencv assumes y, x, channels umpy array, so y = z pfff\r\n    # print ""Shape is now : "", res.shape\r\n\r\n    res = res.swapaxes(0, 2)\r\n    res = res.swapaxes(0, 1)\r\n    # print ""Shape: "", res.shape\r\n    resize_x = float(org_spacing_xyz[0]) / float(target_voxel_mm)\r\n    resize_y = float(org_spacing_xyz[1]) / float(target_voxel_mm)\r\n\r\n    # cv2 can handle max 512 channels..\r\n    if res.shape[2] > 512:\r\n        res = res.swapaxes(0, 2)\r\n        res1 = res[:256]\r\n        res2 = res[256:]\r\n        res1 = res1.swapaxes(0, 2)\r\n        res2 = res2.swapaxes(0, 2)\r\n        res1 = cv2.resize(res1, dsize=None, fx=resize_x, fy=resize_y, interpolation=interpolation)\r\n        res2 = cv2.resize(res2, dsize=None, fx=resize_x, fy=resize_y, interpolation=interpolation)\r\n        res1 = res1.swapaxes(0, 2)\r\n        res2 = res2.swapaxes(0, 2)\r\n        res = numpy.vstack([res1, res2])\r\n        res = res.swapaxes(0, 2)\r\n    else:\r\n        res = cv2.resize(res, dsize=None, fx=resize_x, fy=resize_y, interpolation=interpolation)\r\n\r\n    # channels = cv2.split(res)\r\n    # resized_channels = []\r\n    # for channel in  channels:\r\n    #     channel = cv2.resize(channel, dsize=None, fx=resize_x, fy=resize_y)\r\n    #     resized_channels.append(channel)\r\n    # res = cv2.merge(resized_channels)\r\n    # print ""Shape after resize: "", res.shape\r\n    res = res.swapaxes(0, 2)\r\n    res = res.swapaxes(2, 1)\r\n    if verbose:\r\n        print(""Shape after: "", res.shape)\r\n    return res\r\n\r\n\r\ndef rescale_patient_images2(images_zyx, target_shape, verbose=False):\r\n    if verbose:\r\n        print(""Target: "", target_shape)\r\n        print(""Shape: "", images_zyx.shape)\r\n\r\n    # print ""Resizing dim z""\r\n    resize_x = 1.0\r\n    interpolation = cv2.INTER_NEAREST if False else cv2.INTER_LINEAR\r\n    res = cv2.resize(images_zyx, dsize=(target_shape[1], target_shape[0]), interpolation=interpolation)  # opencv assumes y, x, channels umpy array, so y = z pfff\r\n    # print ""Shape is now : "", res.shape\r\n\r\n    res = res.swapaxes(0, 2)\r\n    res = res.swapaxes(0, 1)\r\n\r\n    # cv2 can handle max 512 channels..\r\n    if res.shape[2] > 512:\r\n        res = res.swapaxes(0, 2)\r\n        res1 = res[:256]\r\n        res2 = res[256:]\r\n        res1 = res1.swapaxes(0, 2)\r\n        res2 = res2.swapaxes(0, 2)\r\n        res1 = cv2.resize(res1, dsize=(target_shape[2], target_shape[1]), interpolation=interpolation)\r\n        res2 = cv2.resize(res2, dsize=(target_shape[2], target_shape[1]), interpolation=interpolation)\r\n        res1 = res1.swapaxes(0, 2)\r\n        res2 = res2.swapaxes(0, 2)\r\n        res = numpy.vstack([res1, res2])\r\n        res = res.swapaxes(0, 2)\r\n    else:\r\n        res = cv2.resize(res, dsize=(target_shape[2], target_shape[1]), interpolation=interpolation)\r\n\r\n    res = res.swapaxes(0, 2)\r\n    res = res.swapaxes(2, 1)\r\n    if verbose:\r\n        print(""Shape after: "", res.shape)\r\n    return res\r\n\r\n\r\ndef print_global_vars(globs, names):\r\n    # globs = globals()\r\n    print(""-- GLOBALS --"")\r\n    for key in globs.keys():\r\n        if key in names:\r\n            print(key, "": "", globs[key])\r\n    print("""")\r\n\r\n\r\nPRINT_TAB_MAP = defaultdict(lambda: [])\r\ndef print_tabbed(value_list, justifications=None, map_id=None, show_map_idx=True):\r\n    map_entries = None\r\n    if map_id is not None:\r\n        map_entries = PRINT_TAB_MAP[map_id]\r\n\r\n    if map_entries is not None and show_map_idx:\r\n        idx = str(len(map_entries))\r\n        if idx == ""0"":\r\n            idx = ""idx""\r\n        value_list.insert(0, idx)\r\n        if justifications is not None:\r\n            justifications.insert(0, 6)\r\n\r\n    value_list = [str(v) for v in value_list]\r\n    if justifications is not None:\r\n        new_list = []\r\n        assert(len(value_list) == len(justifications))\r\n        for idx, value in enumerate(value_list):\r\n            str_value = str(value)\r\n            just = justifications[idx]\r\n            if just > 0:\r\n                new_value = str_value.ljust(just)\r\n            else:\r\n                new_value = str_value.rjust(just)\r\n            new_list.append(new_value)\r\n\r\n        value_list = new_list\r\n\r\n    line = ""\\t"".join(value_list)\r\n    if map_entries is not None:\r\n        map_entries.append(line)\r\n    print(line)\r\n\r\n\r\ndef get_segmented_lungs(im, plot=False):\r\n    # Step 1: Convert into a binary image.\r\n    binary = im < -400\r\n    # Step 2: Remove the blobs connected to the border of the image.\r\n    cleared = clear_border(binary)\r\n    # Step 3: Label the image.\r\n    label_image = label(cleared)\r\n    # Step 4: Keep the labels with 2 largest areas.\r\n    areas = [r.area for r in regionprops(label_image)]\r\n    areas.sort()\r\n    if len(areas) > 2:\r\n        for region in regionprops(label_image):\r\n            if region.area < areas[-2]:\r\n                for coordinates in region.coords:\r\n                       label_image[coordinates[0], coordinates[1]] = 0\r\n    binary = label_image > 0\r\n    # Step 5: Erosion operation with a disk of radius 2. This operation is seperate the lung nodules attached to the blood vessels.\r\n    selem = disk(2)\r\n    binary = binary_erosion(binary, selem)\r\n    # Step 6: Closure operation with a disk of radius 10. This operation is    to keep nodules attached to the lung wall.\r\n    selem = disk(10) # CHANGE BACK TO 10\r\n    binary = binary_closing(binary, selem)\r\n    # Step 7: Fill in the small holes inside the binary mask of lungs.\r\n    edges = roberts(binary)\r\n    binary = ndi.binary_fill_holes(edges)\r\n    # Step 8: Superimpose the binary mask on the input image.\r\n    get_high_vals = binary == 0\r\n    im[get_high_vals] = -2000\r\n    return im, binary\r\n\r\n\r\ndef prepare_image_for_net3D(img, mean_value=None):\r\n    img = img.astype(numpy.float32)\r\n    if mean_value is not None:\r\n        img -= mean_value\r\n    img /= 255.\r\n    img = img.reshape(1, img.shape[0], img.shape[1], img.shape[2], 1)\r\n    return img\r\n\r\n\r\ndef get_distance(df_row1, df_row2):\r\n    dist = math.sqrt(math.pow(df_row1[""coord_x""] - df_row2[""coord_x""], 2) + math.pow(df_row1[""coord_y""] - df_row2[""coord_y""], 2) + math.pow(df_row1[""coord_y""] - df_row2[""coord_y""], 2))\r\n    return dist\r\n\r\n\r\ndef percentage_to_pixels(x_perc, y_perc, z_perc, cube_image):\r\n    res_x = int(round(x_perc * cube_image.shape[2]))\r\n    res_y = int(round(y_perc * cube_image.shape[1]))\r\n    res_z = int(round(z_perc * cube_image.shape[0]))\r\n    return res_x, res_y, res_z\r\n\r\n\r\nPATIENT_LIST = None\r\ndef get_patient_fold(patient_id, submission_set_neg=False):\r\n    global PATIENT_LIST\r\n    if PATIENT_LIST is None:\r\n        df = pandas.read_csv(""resources/stage1_labels.csv"")\r\n        PATIENT_LIST = df[""id""].tolist()\r\n\r\n    if submission_set_neg:\r\n        if patient_id not in PATIENT_LIST:\r\n            return -1\r\n\r\n    res = PATIENT_LIST.index(patient_id)\r\n    res %= 6\r\n    return res\r\n\r\n\r\n\r\n'"
settings.py,0,"b'import os\r\nCOMPUTER_NAME = os.environ[\'COMPUTERNAME\']\r\nprint(""Computer: "", COMPUTER_NAME)\r\n\r\nWORKER_POOL_SIZE = 8\r\n\r\nTARGET_VOXEL_MM = 1.00\r\nMEAN_PIXEL_VALUE_NODULE = 41\r\nLUNA_SUBSET_START_INDEX = 0\r\nSEGMENTER_IMG_SIZE = 320\r\n\r\nBASE_DIR_SSD = ""C:/werkdata/kaggle/ndsb3/""\r\nBASE_DIR = ""D:/werkdata/kaggle/ndsb3/""\r\nEXTRA_DATA_DIR = ""resources/""\r\nNDSB3_RAW_SRC_DIR = BASE_DIR + ""ndsb_raw/stage12/""\r\nLUNA16_RAW_SRC_DIR = BASE_DIR + ""luna_raw/""\r\n\r\nNDSB3_EXTRACTED_IMAGE_DIR = BASE_DIR_SSD + ""ndsb3_extracted_images/""\r\nLUNA16_EXTRACTED_IMAGE_DIR = BASE_DIR_SSD + ""luna16_extracted_images/""\r\nNDSB3_NODULE_DETECTION_DIR = BASE_DIR_SSD + ""ndsb3_nodule_predictions/""\r\n\r\n'"
step1_preprocess_luna16.py,0,"b'import settings\r\nimport helpers\r\nimport SimpleITK  # conda install -c https://conda.anaconda.org/simpleitk SimpleITK\r\nimport numpy\r\nimport pandas\r\nimport ntpath\r\nimport cv2  # conda install -c https://conda.anaconda.org/menpo opencv3\r\nimport shutil\r\nimport random\r\nimport math\r\nimport multiprocessing\r\nfrom bs4 import BeautifulSoup #  conda install beautifulsoup4, coda install lxml\r\nimport os\r\nimport glob\r\n\r\nrandom.seed(1321)\r\nnumpy.random.seed(1321)\r\n\r\n\r\ndef find_mhd_file(patient_id):\r\n    for subject_no in range(settings.LUNA_SUBSET_START_INDEX, 10):\r\n        src_dir = settings.LUNA16_RAW_SRC_DIR + ""subset"" + str(subject_no) + ""/""\r\n        for src_path in glob.glob(src_dir + ""*.mhd""):\r\n            if patient_id in src_path:\r\n                return src_path\r\n    return None\r\n\r\n\r\ndef load_lidc_xml(xml_path, agreement_threshold=0, only_patient=None, save_nodules=False):\r\n    pos_lines = []\r\n    neg_lines = []\r\n    extended_lines = []\r\n    with open(xml_path, \'r\') as xml_file:\r\n        markup = xml_file.read()\r\n    xml = BeautifulSoup(markup, features=""xml"")\r\n    if xml.LidcReadMessage is None:\r\n        return None, None, None\r\n    patient_id = xml.LidcReadMessage.ResponseHeader.SeriesInstanceUid.text\r\n\r\n    if only_patient is not None:\r\n        if only_patient != patient_id:\r\n            return None, None, None\r\n\r\n    src_path = find_mhd_file(patient_id)\r\n    if src_path is None:\r\n        return None, None, None\r\n\r\n    print(patient_id)\r\n    itk_img = SimpleITK.ReadImage(src_path)\r\n    img_array = SimpleITK.GetArrayFromImage(itk_img)\r\n    num_z, height, width = img_array.shape        #heightXwidth constitute the transverse plane\r\n    origin = numpy.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)\r\n    spacing = numpy.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)\r\n    rescale = spacing / settings.TARGET_VOXEL_MM\r\n\r\n    reading_sessions = xml.LidcReadMessage.find_all(""readingSession"")\r\n    for reading_session in reading_sessions:\r\n        # print(""Sesion"")\r\n        nodules = reading_session.find_all(""unblindedReadNodule"")\r\n        for nodule in nodules:\r\n            nodule_id = nodule.noduleID.text\r\n            # print(""  "", nodule.noduleID)\r\n            rois = nodule.find_all(""roi"")\r\n            x_min = y_min = z_min = 999999\r\n            x_max = y_max = z_max = -999999\r\n            if len(rois) < 2:\r\n                continue\r\n\r\n            for roi in rois:\r\n                z_pos = float(roi.imageZposition.text)\r\n                z_min = min(z_min, z_pos)\r\n                z_max = max(z_max, z_pos)\r\n                edge_maps = roi.find_all(""edgeMap"")\r\n                for edge_map in edge_maps:\r\n                    x = int(edge_map.xCoord.text)\r\n                    y = int(edge_map.yCoord.text)\r\n                    x_min = min(x_min, x)\r\n                    y_min = min(y_min, y)\r\n                    x_max = max(x_max, x)\r\n                    y_max = max(y_max, y)\r\n                if x_max == x_min:\r\n                    continue\r\n                if y_max == y_min:\r\n                    continue\r\n\r\n            x_diameter = x_max - x_min\r\n            x_center = x_min + x_diameter / 2\r\n            y_diameter = y_max - y_min\r\n            y_center = y_min + y_diameter / 2\r\n            z_diameter = z_max - z_min\r\n            z_center = z_min + z_diameter / 2\r\n            z_center -= origin[2]\r\n            z_center /= spacing[2]\r\n\r\n            x_center_perc = round(x_center / img_array.shape[2], 4)\r\n            y_center_perc = round(y_center / img_array.shape[1], 4)\r\n            z_center_perc = round(z_center / img_array.shape[0], 4)\r\n            diameter = max(x_diameter , y_diameter)\r\n            diameter_perc = round(max(x_diameter / img_array.shape[2], y_diameter / img_array.shape[1]), 4)\r\n\r\n            if nodule.characteristics is None:\r\n                print(""!!!!Nodule:"", nodule_id, "" has no charecteristics"")\r\n                continue\r\n            if nodule.characteristics.malignancy is None:\r\n                print(""!!!!Nodule:"", nodule_id, "" has no malignacy"")\r\n                continue\r\n\r\n            malignacy = nodule.characteristics.malignancy.text\r\n            sphericiy = nodule.characteristics.sphericity.text\r\n            margin = nodule.characteristics.margin.text\r\n            spiculation = nodule.characteristics.spiculation.text\r\n            texture = nodule.characteristics.texture.text\r\n            calcification = nodule.characteristics.calcification.text\r\n            internal_structure = nodule.characteristics.internalStructure.text\r\n            lobulation = nodule.characteristics.lobulation.text\r\n            subtlety = nodule.characteristics.subtlety.text\r\n\r\n            line = [nodule_id, x_center_perc, y_center_perc, z_center_perc, diameter_perc, malignacy]\r\n            extended_line = [patient_id, nodule_id, x_center_perc, y_center_perc, z_center_perc, diameter_perc, malignacy, sphericiy, margin, spiculation, texture, calcification, internal_structure, lobulation, subtlety ]\r\n            pos_lines.append(line)\r\n            extended_lines.append(extended_line)\r\n\r\n        nonNodules = reading_session.find_all(""nonNodule"")\r\n        for nonNodule in nonNodules:\r\n            z_center = float(nonNodule.imageZposition.text)\r\n            z_center -= origin[2]\r\n            z_center /= spacing[2]\r\n            x_center = int(nonNodule.locus.xCoord.text)\r\n            y_center = int(nonNodule.locus.yCoord.text)\r\n            nodule_id = nonNodule.nonNoduleID.text\r\n            x_center_perc = round(x_center / img_array.shape[2], 4)\r\n            y_center_perc = round(y_center / img_array.shape[1], 4)\r\n            z_center_perc = round(z_center / img_array.shape[0], 4)\r\n            diameter_perc = round(max(6 / img_array.shape[2], 6 / img_array.shape[1]), 4)\r\n            # print(""Non nodule!"", z_center)\r\n            line = [nodule_id, x_center_perc, y_center_perc, z_center_perc, diameter_perc, 0]\r\n            neg_lines.append(line)\r\n\r\n    if agreement_threshold > 1:\r\n        filtered_lines = []\r\n        for pos_line1 in pos_lines:\r\n            id1 = pos_line1[0]\r\n            x1 = pos_line1[1]\r\n            y1 = pos_line1[2]\r\n            z1 = pos_line1[3]\r\n            d1 = pos_line1[4]\r\n            overlaps = 0\r\n            for pos_line2 in pos_lines:\r\n                id2 = pos_line2[0]\r\n                if id1 == id2:\r\n                    continue\r\n                x2 = pos_line2[1]\r\n                y2 = pos_line2[2]\r\n                z2 = pos_line2[3]\r\n                d2 = pos_line1[4]\r\n                dist = math.sqrt(math.pow(x1 - x2, 2) + math.pow(y1 - y2, 2) + math.pow(z1 - z2, 2))\r\n                if dist < d1 or dist < d2:\r\n                    overlaps += 1\r\n            if overlaps >= agreement_threshold:\r\n                filtered_lines.append(pos_line1)\r\n            # else:\r\n            #     print(""Too few overlaps"")\r\n        pos_lines = filtered_lines\r\n\r\n    df_annos = pandas.DataFrame(pos_lines, columns=[""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""malscore""])\r\n    df_annos.to_csv(settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/"" + patient_id + ""_annos_pos_lidc.csv"", index=False)\r\n    df_neg_annos = pandas.DataFrame(neg_lines, columns=[""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""malscore""])\r\n    df_neg_annos.to_csv(settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/"" + patient_id + ""_annos_neg_lidc.csv"", index=False)\r\n\r\n    # return [patient_id, spacing[0], spacing[1], spacing[2]]\r\n    return pos_lines, neg_lines, extended_lines\r\n\r\n\r\ndef normalize(image):\r\n    MIN_BOUND = -1000.0\r\n    MAX_BOUND = 400.0\r\n    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\r\n    image[image > 1] = 1.\r\n    image[image < 0] = 0.\r\n    return image\r\n\r\n\r\ndef process_image(src_path):\r\n    patient_id = ntpath.basename(src_path).replace("".mhd"", """")\r\n    print(""Patient: "", patient_id)\r\n\r\n    dst_dir = settings.LUNA16_EXTRACTED_IMAGE_DIR + patient_id + ""/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n    itk_img = SimpleITK.ReadImage(src_path)\r\n    img_array = SimpleITK.GetArrayFromImage(itk_img)\r\n    print(""Img array: "", img_array.shape)\r\n\r\n    origin = numpy.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Origin (x,y,z): "", origin)\r\n\r\n    direction = numpy.array(itk_img.GetDirection())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Direction: "", direction)\r\n\r\n\r\n    spacing = numpy.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)\r\n    print(""Spacing (x,y,z): "", spacing)\r\n    rescale = spacing / settings.TARGET_VOXEL_MM\r\n    print(""Rescale: "", rescale)\r\n\r\n    img_array = helpers.rescale_patient_images(img_array, spacing, settings.TARGET_VOXEL_MM)\r\n\r\n    img_list = []\r\n    for i in range(img_array.shape[0]):\r\n        img = img_array[i]\r\n        seg_img, mask = helpers.get_segmented_lungs(img.copy())\r\n        img_list.append(seg_img)\r\n        img = normalize(img)\r\n        cv2.imwrite(dst_dir + ""img_"" + str(i).rjust(4, \'0\') + ""_i.png"", img * 255)\r\n        cv2.imwrite(dst_dir + ""img_"" + str(i).rjust(4, \'0\') + ""_m.png"", mask * 255)\r\n\r\n\r\ndef process_pos_annotations_patient(src_path, patient_id):\r\n    df_node = pandas.read_csv(""resources/luna16_annotations/annotations.csv"")\r\n    dst_dir = settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n    dst_dir = dst_dir + patient_id + ""/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n    itk_img = SimpleITK.ReadImage(src_path)\r\n    img_array = SimpleITK.GetArrayFromImage(itk_img)\r\n    print(""Img array: "", img_array.shape)\r\n    df_patient = df_node[df_node[""seriesuid""] == patient_id]\r\n    print(""Annos: "", len(df_patient))\r\n\r\n    num_z, height, width = img_array.shape        #heightXwidth constitute the transverse plane\r\n    origin = numpy.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Origin (x,y,z): "", origin)\r\n    spacing = numpy.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)\r\n    print(""Spacing (x,y,z): "", spacing)\r\n    rescale = spacing /settings.TARGET_VOXEL_MM\r\n    print(""Rescale: "", rescale)\r\n\r\n    direction = numpy.array(itk_img.GetDirection())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Direction: "", direction)\r\n    flip_direction_x = False\r\n    flip_direction_y = False\r\n    if round(direction[0]) == -1:\r\n        origin[0] *= -1\r\n        direction[0] = 1\r\n        flip_direction_x = True\r\n        print(""Swappint x origin"")\r\n    if round(direction[4]) == -1:\r\n        origin[1] *= -1\r\n        direction[4] = 1\r\n        flip_direction_y = True\r\n        print(""Swappint y origin"")\r\n    print(""Direction: "", direction)\r\n    assert abs(sum(direction) - 3) < 0.01\r\n\r\n    patient_imgs = helpers.load_patient_images(patient_id, settings.LUNA16_EXTRACTED_IMAGE_DIR, ""*_i.png"")\r\n\r\n    pos_annos = []\r\n    df_patient = df_node[df_node[""seriesuid""] == patient_id]\r\n    anno_index = 0\r\n    for index, annotation in df_patient.iterrows():\r\n        node_x = annotation[""coordX""]\r\n        if flip_direction_x:\r\n            node_x *= -1\r\n        node_y = annotation[""coordY""]\r\n        if flip_direction_y:\r\n            node_y *= -1\r\n        node_z = annotation[""coordZ""]\r\n        diam_mm = annotation[""diameter_mm""]\r\n        print(""Node org (x,y,z,diam): "", (round(node_x, 2), round(node_y, 2), round(node_z, 2), round(diam_mm, 2)))\r\n        center_float = numpy.array([node_x, node_y, node_z])\r\n        center_int = numpy.rint((center_float-origin) / spacing)\r\n        # center_int = numpy.rint((center_float - origin) )\r\n        print(""Node tra (x,y,z,diam): "", (center_int[0], center_int[1], center_int[2]))\r\n        # center_int_rescaled = numpy.rint(((center_float-origin) / spacing) * rescale)\r\n        center_float_rescaled = (center_float - origin) / settings.TARGET_VOXEL_MM\r\n        center_float_percent = center_float_rescaled / patient_imgs.swapaxes(0, 2).shape\r\n        # center_int = numpy.rint((center_float - origin) )\r\n        print(""Node sca (x,y,z,diam): "", (center_float_rescaled[0], center_float_rescaled[1], center_float_rescaled[2]))\r\n        diameter_pixels = diam_mm / settings.TARGET_VOXEL_MM\r\n        diameter_percent = diameter_pixels / float(patient_imgs.shape[1])\r\n\r\n        pos_annos.append([anno_index, round(center_float_percent[0], 4), round(center_float_percent[1], 4), round(center_float_percent[2], 4), round(diameter_percent, 4), 1])\r\n        anno_index += 1\r\n\r\n    df_annos = pandas.DataFrame(pos_annos, columns=[""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""malscore""])\r\n    df_annos.to_csv(settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/"" + patient_id + ""_annos_pos.csv"", index=False)\r\n    return [patient_id, spacing[0], spacing[1], spacing[2]]\r\n\r\n\r\ndef process_excluded_annotations_patient(src_path, patient_id):\r\n    df_node = pandas.read_csv(""resources/luna16_annotations/annotations_excluded.csv"")\r\n    dst_dir = settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n    dst_dir = dst_dir + patient_id + ""/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n    # pos_annos_df = pandas.read_csv(TRAIN_DIR + ""metadata/"" + patient_id + ""_annos_pos_lidc.csv"")\r\n    pos_annos_df = pandas.read_csv(settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/"" + patient_id + ""_annos_pos.csv"")\r\n    pos_annos_manual = None\r\n    manual_path = settings.EXTRA_DATA_DIR + ""luna16_manual_labels/"" + patient_id + "".csv""\r\n    if os.path.exists(manual_path):\r\n        pos_annos_manual = pandas.read_csv(manual_path)\r\n        dmm = pos_annos_manual[""dmm""]  # check\r\n\r\n    itk_img = SimpleITK.ReadImage(src_path)\r\n    img_array = SimpleITK.GetArrayFromImage(itk_img)\r\n    print(""Img array: "", img_array.shape)\r\n    df_patient = df_node[df_node[""seriesuid""] == patient_id]\r\n    print(""Annos: "", len(df_patient))\r\n\r\n    num_z, height, width = img_array.shape        #heightXwidth constitute the transverse plane\r\n    origin = numpy.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Origin (x,y,z): "", origin)\r\n    spacing = numpy.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)\r\n    print(""Spacing (x,y,z): "", spacing)\r\n    rescale = spacing / settings.TARGET_VOXEL_MM\r\n    print(""Rescale: "", rescale)\r\n\r\n    direction = numpy.array(itk_img.GetDirection())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Direction: "", direction)\r\n    flip_direction_x = False\r\n    flip_direction_y = False\r\n    if round(direction[0]) == -1:\r\n        origin[0] *= -1\r\n        direction[0] = 1\r\n        flip_direction_x = True\r\n        print(""Swappint x origin"")\r\n    if round(direction[4]) == -1:\r\n        origin[1] *= -1\r\n        direction[4] = 1\r\n        flip_direction_y = True\r\n        print(""Swappint y origin"")\r\n    print(""Direction: "", direction)\r\n    assert abs(sum(direction) - 3) < 0.01\r\n\r\n    patient_imgs = helpers.load_patient_images(patient_id, settings.LUNA16_EXTRACTED_IMAGE_DIR, ""*_i.png"")\r\n\r\n    neg_annos = []\r\n    df_patient = df_node[df_node[""seriesuid""] == patient_id]\r\n    anno_index = 0\r\n    for index, annotation in df_patient.iterrows():\r\n        node_x = annotation[""coordX""]\r\n        if flip_direction_x:\r\n            node_x *= -1\r\n        node_y = annotation[""coordY""]\r\n        if flip_direction_y:\r\n            node_y *= -1\r\n        node_z = annotation[""coordZ""]\r\n        center_float = numpy.array([node_x, node_y, node_z])\r\n        center_int = numpy.rint((center_float-origin) / spacing)\r\n        center_float_rescaled = (center_float - origin) / settings.TARGET_VOXEL_MM\r\n        center_float_percent = center_float_rescaled / patient_imgs.swapaxes(0, 2).shape\r\n        # center_int = numpy.rint((center_float - origin) )\r\n        # print(""Node sca (x,y,z,diam): "", (center_float_rescaled[0], center_float_rescaled[1], center_float_rescaled[2]))\r\n        diameter_pixels = 6 / settings.TARGET_VOXEL_MM\r\n        diameter_percent = diameter_pixels / float(patient_imgs.shape[1])\r\n\r\n        ok = True\r\n\r\n        for index, row in pos_annos_df.iterrows():\r\n            pos_coord_x = row[""coord_x""] * patient_imgs.shape[2]\r\n            pos_coord_y = row[""coord_y""] * patient_imgs.shape[1]\r\n            pos_coord_z = row[""coord_z""] * patient_imgs.shape[0]\r\n            diameter = row[""diameter""] * patient_imgs.shape[2]\r\n            print((pos_coord_x, pos_coord_y, pos_coord_z))\r\n            print(center_float_rescaled)\r\n            dist = math.sqrt(math.pow(pos_coord_x - center_float_rescaled[0], 2) + math.pow(pos_coord_y - center_float_rescaled[1], 2) + math.pow(pos_coord_z - center_float_rescaled[2], 2))\r\n            if dist < (diameter + 64):  #  make sure we have a big margin\r\n                ok = False\r\n                print(""################### Too close"", center_float_rescaled)\r\n                break\r\n\r\n        if pos_annos_manual is not None and ok:\r\n            for index, row in pos_annos_manual.iterrows():\r\n                pos_coord_x = row[""x""] * patient_imgs.shape[2]\r\n                pos_coord_y = row[""y""] * patient_imgs.shape[1]\r\n                pos_coord_z = row[""z""] * patient_imgs.shape[0]\r\n                diameter = row[""d""] * patient_imgs.shape[2]\r\n                print((pos_coord_x, pos_coord_y, pos_coord_z))\r\n                print(center_float_rescaled)\r\n                dist = math.sqrt(math.pow(pos_coord_x - center_float_rescaled[0], 2) + math.pow(pos_coord_y - center_float_rescaled[1], 2) + math.pow(pos_coord_z - center_float_rescaled[2], 2))\r\n                if dist < (diameter + 72):  #  make sure we have a big margin\r\n                    ok = False\r\n                    print(""################### Too close"", center_float_rescaled)\r\n                    break\r\n\r\n        if not ok:\r\n            continue\r\n\r\n        neg_annos.append([anno_index, round(center_float_percent[0], 4), round(center_float_percent[1], 4), round(center_float_percent[2], 4), round(diameter_percent, 4), 1])\r\n        anno_index += 1\r\n\r\n    df_annos = pandas.DataFrame(neg_annos, columns=[""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""malscore""])\r\n    df_annos.to_csv(settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/"" + patient_id + ""_annos_excluded.csv"", index=False)\r\n    return [patient_id, spacing[0], spacing[1], spacing[2]]\r\n\r\n\r\ndef process_luna_candidates_patient(src_path, patient_id):\r\n    dst_dir = settings.LUNA16_EXTRACTED_IMAGE_DIR + ""/_labels/""\r\n    img_dir = dst_dir + patient_id + ""/""\r\n    df_pos_annos = pandas.read_csv(dst_dir + patient_id + ""_annos_pos_lidc.csv"")\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n    pos_annos_manual = None\r\n    manual_path = settings.EXTRA_DATA_DIR + ""luna16_manual_labels/"" + patient_id + "".csv""\r\n    if os.path.exists(manual_path):\r\n        pos_annos_manual = pandas.read_csv(manual_path)\r\n\r\n    itk_img = SimpleITK.ReadImage(src_path)\r\n    img_array = SimpleITK.GetArrayFromImage(itk_img)\r\n    print(""Img array: "", img_array.shape)\r\n    print(""Pos annos: "", len(df_pos_annos))\r\n\r\n    num_z, height, width = img_array.shape        #heightXwidth constitute the transverse plane\r\n    origin = numpy.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Origin (x,y,z): "", origin)\r\n    spacing = numpy.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)\r\n    print(""Spacing (x,y,z): "", spacing)\r\n    rescale = spacing / settings.TARGET_VOXEL_MM\r\n    print(""Rescale: "", rescale)\r\n\r\n    direction = numpy.array(itk_img.GetDirection())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Direction: "", direction)\r\n    flip_direction_x = False\r\n    flip_direction_y = False\r\n    if round(direction[0]) == -1:\r\n        origin[0] *= -1\r\n        direction[0] = 1\r\n        flip_direction_x = True\r\n        print(""Swappint x origin"")\r\n    if round(direction[4]) == -1:\r\n        origin[1] *= -1\r\n        direction[4] = 1\r\n        flip_direction_y = True\r\n        print(""Swappint y origin"")\r\n    print(""Direction: "", direction)\r\n    assert abs(sum(direction) - 3) < 0.01\r\n\r\n    src_df = pandas.read_csv(""resources/luna16_annotations/"" + ""candidates_V2.csv"")\r\n    src_df = src_df[src_df[""seriesuid""] == patient_id]\r\n    src_df = src_df[src_df[""class""] == 0]\r\n    patient_imgs = helpers.load_patient_images(patient_id, settings.LUNA16_EXTRACTED_IMAGE_DIR, ""*_i.png"")\r\n    candidate_list = []\r\n\r\n    for df_index, candiate_row in src_df.iterrows():\r\n        node_x = candiate_row[""coordX""]\r\n        if flip_direction_x:\r\n            node_x *= -1\r\n        node_y = candiate_row[""coordY""]\r\n        if flip_direction_y:\r\n            node_y *= -1\r\n        node_z = candiate_row[""coordZ""]\r\n        candidate_diameter = 6\r\n        # print(""Node org (x,y,z,diam): "", (round(node_x, 2), round(node_y, 2), round(node_z, 2), round(candidate_diameter, 2)))\r\n        center_float = numpy.array([node_x, node_y, node_z])\r\n        center_int = numpy.rint((center_float-origin) / spacing)\r\n        # center_int = numpy.rint((center_float - origin) )\r\n        # print(""Node tra (x,y,z,diam): "", (center_int[0], center_int[1], center_int[2]))\r\n        # center_int_rescaled = numpy.rint(((center_float-origin) / spacing) * rescale)\r\n        center_float_rescaled = (center_float - origin) / settings.TARGET_VOXEL_MM\r\n        center_float_percent = center_float_rescaled / patient_imgs.swapaxes(0, 2).shape\r\n        # center_int = numpy.rint((center_float - origin) )\r\n        # print(""Node sca (x,y,z,diam): "", (center_float_rescaled[0], center_float_rescaled[1], center_float_rescaled[2]))\r\n        coord_x = center_float_rescaled[0]\r\n        coord_y = center_float_rescaled[1]\r\n        coord_z = center_float_rescaled[2]\r\n\r\n        ok = True\r\n\r\n        for index, row in df_pos_annos.iterrows():\r\n            pos_coord_x = row[""coord_x""] * patient_imgs.shape[2]\r\n            pos_coord_y = row[""coord_y""] * patient_imgs.shape[1]\r\n            pos_coord_z = row[""coord_z""] * patient_imgs.shape[0]\r\n            diameter = row[""diameter""] * patient_imgs.shape[2]\r\n            dist = math.sqrt(math.pow(pos_coord_x - coord_x, 2) + math.pow(pos_coord_y - coord_y, 2) + math.pow(pos_coord_z - coord_z, 2))\r\n            if dist < (diameter + 64):  #  make sure we have a big margin\r\n                ok = False\r\n                print(""################### Too close"", (coord_x, coord_y, coord_z))\r\n                break\r\n\r\n        if pos_annos_manual is not None and ok:\r\n            for index, row in pos_annos_manual.iterrows():\r\n                pos_coord_x = row[""x""] * patient_imgs.shape[2]\r\n                pos_coord_y = row[""y""] * patient_imgs.shape[1]\r\n                pos_coord_z = row[""z""] * patient_imgs.shape[0]\r\n                diameter = row[""d""] * patient_imgs.shape[2]\r\n                print((pos_coord_x, pos_coord_y, pos_coord_z))\r\n                print(center_float_rescaled)\r\n                dist = math.sqrt(math.pow(pos_coord_x - center_float_rescaled[0], 2) + math.pow(pos_coord_y - center_float_rescaled[1], 2) + math.pow(pos_coord_z - center_float_rescaled[2], 2))\r\n                if dist < (diameter + 72):  #  make sure we have a big margin\r\n                    ok = False\r\n                    print(""################### Too close"", center_float_rescaled)\r\n                    break\r\n\r\n        if not ok:\r\n            continue\r\n\r\n        candidate_list.append([len(candidate_list), round(center_float_percent[0], 4), round(center_float_percent[1], 4), round(center_float_percent[2], 4), round(candidate_diameter / patient_imgs.shape[0], 4), 0])\r\n\r\n    df_candidates = pandas.DataFrame(candidate_list, columns=[""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""malscore""])\r\n    df_candidates.to_csv(dst_dir + patient_id + ""_candidates_luna.csv"", index=False)\r\n\r\n\r\ndef process_auto_candidates_patient(src_path, patient_id, sample_count=1000, candidate_type=""white""):\r\n    dst_dir = settings.LUNA16_EXTRACTED_IMAGE_DIR + ""/_labels/""\r\n    img_dir = settings.LUNA16_EXTRACTED_IMAGE_DIR + patient_id + ""/""\r\n    df_pos_annos = pandas.read_csv(dst_dir + patient_id + ""_annos_pos_lidc.csv"")\r\n\r\n    pos_annos_manual = None\r\n    manual_path = settings.EXTRA_DATA_DIR + ""luna16_manual_labels/"" + patient_id + "".csv""\r\n    if os.path.exists(manual_path):\r\n        pos_annos_manual = pandas.read_csv(manual_path)\r\n\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n    itk_img = SimpleITK.ReadImage(src_path)\r\n    img_array = SimpleITK.GetArrayFromImage(itk_img)\r\n    print(""Img array: "", img_array.shape)\r\n    print(""Pos annos: "", len(df_pos_annos))\r\n\r\n    num_z, height, width = img_array.shape        #heightXwidth constitute the transverse plane\r\n    origin = numpy.array(itk_img.GetOrigin())      # x,y,z  Origin in world coordinates (mm)\r\n    print(""Origin (x,y,z): "", origin)\r\n    spacing = numpy.array(itk_img.GetSpacing())    # spacing of voxels in world coor. (mm)\r\n    print(""Spacing (x,y,z): "", spacing)\r\n    rescale = spacing / settings.TARGET_VOXEL_MM\r\n    print(""Rescale: "", rescale)\r\n\r\n    if candidate_type == ""white"":\r\n        wildcard = ""*_c.png""\r\n    else:\r\n        wildcard = ""*_m.png""\r\n\r\n    src_files = glob.glob(img_dir + wildcard)\r\n    src_files.sort()\r\n    src_candidate_maps = [cv2.imread(src_file, cv2.IMREAD_GRAYSCALE) for src_file in src_files]\r\n\r\n    candidate_list = []\r\n    tries = 0\r\n    while len(candidate_list) < sample_count and tries < 10000:\r\n        tries += 1\r\n        coord_z = int(numpy.random.normal(len(src_files) / 2, len(src_files) / 6))\r\n        coord_z = max(coord_z, 0)\r\n        coord_z = min(coord_z, len(src_files) - 1)\r\n        candidate_map = src_candidate_maps[coord_z]\r\n        if candidate_type == ""edge"":\r\n            candidate_map = cv2.Canny(candidate_map.copy(), 100, 200)\r\n\r\n        non_zero_indices = numpy.nonzero(candidate_map)\r\n        if len(non_zero_indices[0]) == 0:\r\n            continue\r\n        nonzero_index = random.randint(0, len(non_zero_indices[0]) - 1)\r\n        coord_y = non_zero_indices[0][nonzero_index]\r\n        coord_x = non_zero_indices[1][nonzero_index]\r\n        ok = True\r\n        candidate_diameter = 6\r\n        for index, row in df_pos_annos.iterrows():\r\n            pos_coord_x = row[""coord_x""] * src_candidate_maps[0].shape[1]\r\n            pos_coord_y = row[""coord_y""] * src_candidate_maps[0].shape[0]\r\n            pos_coord_z = row[""coord_z""] * len(src_files)\r\n            diameter = row[""diameter""] * src_candidate_maps[0].shape[1]\r\n            dist = math.sqrt(math.pow(pos_coord_x - coord_x, 2) + math.pow(pos_coord_y - coord_y, 2) + math.pow(pos_coord_z - coord_z, 2))\r\n            if dist < (diameter + 48): #  make sure we have a big margin\r\n                ok = False\r\n                print(""# Too close"", (coord_x, coord_y, coord_z))\r\n                break\r\n\r\n        if pos_annos_manual is not None:\r\n            for index, row in pos_annos_manual.iterrows():\r\n                pos_coord_x = row[""x""] * src_candidate_maps[0].shape[1]\r\n                pos_coord_y = row[""y""] * src_candidate_maps[0].shape[0]\r\n                pos_coord_z = row[""z""] * len(src_files)\r\n                diameter = row[""d""] * src_candidate_maps[0].shape[1]\r\n                # print((pos_coord_x, pos_coord_y, pos_coord_z))\r\n                # print(center_float_rescaled)\r\n                dist = math.sqrt(math.pow(pos_coord_x - coord_x, 2) + math.pow(pos_coord_y - coord_y, 2) + math.pow(pos_coord_z - coord_z, 2))\r\n                if dist < (diameter + 72):  #  make sure we have a big margin\r\n                    ok = False\r\n                    print(""#Too close"",  (coord_x, coord_y, coord_z))\r\n                    break\r\n\r\n        if not ok:\r\n            continue\r\n\r\n\r\n        perc_x = round(coord_x / src_candidate_maps[coord_z].shape[1], 4)\r\n        perc_y = round(coord_y / src_candidate_maps[coord_z].shape[0], 4)\r\n        perc_z = round(coord_z / len(src_files), 4)\r\n        candidate_list.append([len(candidate_list), perc_x, perc_y, perc_z, round(candidate_diameter / src_candidate_maps[coord_z].shape[1], 4), 0])\r\n\r\n    if tries > 9999:\r\n        print(""****** WARING!! TOO MANY TRIES ************************************"")\r\n    df_candidates = pandas.DataFrame(candidate_list, columns=[""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""malscore""])\r\n    df_candidates.to_csv(dst_dir + patient_id + ""_candidates_"" + candidate_type + "".csv"", index=False)\r\n\r\n\r\ndef process_images(delete_existing=False, only_process_patient=None):\r\n    if delete_existing and os.path.exists(settings.LUNA16_EXTRACTED_IMAGE_DIR):\r\n        print(""Removing old stuff.."")\r\n        if os.path.exists(settings.LUNA16_EXTRACTED_IMAGE_DIR):\r\n            shutil.rmtree(settings.LUNA16_EXTRACTED_IMAGE_DIR)\r\n\r\n    if not os.path.exists(settings.LUNA16_EXTRACTED_IMAGE_DIR):\r\n        os.mkdir(settings.LUNA16_EXTRACTED_IMAGE_DIR)\r\n        os.mkdir(settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/"")\r\n\r\n    for subject_no in range(settings.LUNA_SUBSET_START_INDEX, 10):\r\n        src_dir = settings.LUNA16_RAW_SRC_DIR + ""subset"" + str(subject_no) + ""/""\r\n        src_paths = glob.glob(src_dir + ""*.mhd"")\r\n\r\n        if only_process_patient is None and True:\r\n            pool = multiprocessing.Pool(settings.WORKER_POOL_SIZE)\r\n            pool.map(process_image, src_paths)\r\n        else:\r\n            for src_path in src_paths:\r\n                print(src_path)\r\n                if only_process_patient is not None:\r\n                    if only_process_patient not in src_path:\r\n                        continue\r\n                process_image(src_path)\r\n\r\n\r\ndef process_pos_annotations_patient2():\r\n    candidate_index = 0\r\n    only_patient = ""197063290812663596858124411210""\r\n    only_patient = None\r\n    for subject_no in range(settings.LUNA_SUBSET_START_INDEX, 10):\r\n        src_dir = settings.LUNA16_RAW_SRC_DIR + ""subset"" + str(subject_no) + ""/""\r\n        for src_path in glob.glob(src_dir + ""*.mhd""):\r\n            if only_patient is not None and only_patient not in src_path:\r\n                continue\r\n            patient_id = ntpath.basename(src_path).replace("".mhd"", """")\r\n            print(candidate_index, "" patient: "", patient_id)\r\n            process_pos_annotations_patient(src_path, patient_id)\r\n            candidate_index += 1\r\n\r\n\r\ndef process_excluded_annotations_patients(only_patient=None):\r\n    candidate_index = 0\r\n    for subject_no in range(settings.LUNA_SUBSET_START_INDEX, 10):\r\n        src_dir = settings.LUNA16_RAW_SRC_DIR + ""subset"" + str(subject_no) + ""/""\r\n        for src_path in glob.glob(src_dir + ""*.mhd""):\r\n            if only_patient is not None and only_patient not in src_path:\r\n                continue\r\n            patient_id = ntpath.basename(src_path).replace("".mhd"", """")\r\n            print(candidate_index, "" patient: "", patient_id)\r\n            process_excluded_annotations_patient(src_path, patient_id)\r\n            candidate_index += 1\r\n\r\n\r\ndef process_auto_candidates_patients():\r\n    for subject_no in range(settings.LUNA_SUBSET_START_INDEX, 10):\r\n        src_dir = settings.LUNA16_RAW_SRC_DIR + ""subset"" + str(subject_no) + ""/""\r\n        for patient_index, src_path in enumerate(glob.glob(src_dir + ""*.mhd"")):\r\n            # if not ""100621383016233746780170740405"" in src_path:\r\n            #     continue\r\n            patient_id = ntpath.basename(src_path).replace("".mhd"", """")\r\n            print(""Patient: "", patient_index, "" "", patient_id)\r\n            # process_auto_candidates_patient(src_path, patient_id, sample_count=500, candidate_type=""white"")\r\n            process_auto_candidates_patient(src_path, patient_id, sample_count=200, candidate_type=""edge"")\r\n\r\n\r\ndef process_luna_candidates_patients(only_patient_id=None):\r\n    for subject_no in range(settings.LUNA_SUBSET_START_INDEX, 10):\r\n        src_dir = settings.LUNA16_RAW_SRC_DIR + ""subset"" + str(subject_no) + ""/""\r\n        for patient_index, src_path in enumerate(glob.glob(src_dir + ""*.mhd"")):\r\n            # if not ""100621383016233746780170740405"" in src_path:\r\n            #     continue\r\n            patient_id = ntpath.basename(src_path).replace("".mhd"", """")\r\n            if only_patient_id is not None and patient_id != only_patient_id:\r\n                continue\r\n            print(""Patient: "", patient_index, "" "", patient_id)\r\n            process_luna_candidates_patient(src_path, patient_id)\r\n\r\n\r\ndef process_lidc_annotations(only_patient=None, agreement_threshold=0):\r\n    # lines.append("","".join())\r\n    file_no = 0\r\n    pos_count = 0\r\n    neg_count = 0\r\n    all_lines = []\r\n    for anno_dir in [d for d in glob.glob(""resources/luna16_annotations/*"") if os.path.isdir(d)]:\r\n        xml_paths = glob.glob(anno_dir + ""/*.xml"")\r\n        for xml_path in xml_paths:\r\n            print(file_no, "": "",  xml_path)\r\n            pos, neg, extended = load_lidc_xml(xml_path=xml_path, only_patient=only_patient, agreement_threshold=agreement_threshold)\r\n            if pos is not None:\r\n                pos_count += len(pos)\r\n                neg_count += len(neg)\r\n                print(""Pos: "", pos_count, "" Neg: "", neg_count)\r\n                file_no += 1\r\n                all_lines += extended\r\n            # if file_no > 10:\r\n            #     break\r\n\r\n            # extended_line = [nodule_id, x_center_perc, y_center_perc, z_center_perc, diameter_perc, malignacy, sphericiy, margin, spiculation, texture, calcification, internal_structure, lobulation, subtlety ]\r\n    df_annos = pandas.DataFrame(all_lines, columns=[""patient_id"", ""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""malscore"", ""sphericiy"", ""margin"", ""spiculation"", ""texture"", ""calcification"", ""internal_structure"", ""lobulation"", ""subtlety""])\r\n    df_annos.to_csv(settings.BASE_DIR + ""lidc_annotations.csv"", index=False)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    if True:\r\n        only_process_patient = None\r\n        process_images(delete_existing=False, only_process_patient=only_process_patient)\r\n\r\n    if True:\r\n        process_lidc_annotations(only_patient=None, agreement_threshold=0)\r\n\r\n    if True:\r\n        process_pos_annotations_patient2()\r\n        process_excluded_annotations_patients(only_patient=None)\r\n\r\n    if True:\r\n        process_luna_candidates_patients(only_patient_id=None)\r\n    if True:\r\n        process_auto_candidates_patients()\r\n'"
step1_preprocess_ndsb.py,0,"b'import settings\r\nimport helpers\r\nimport glob\r\nimport os\r\nimport cv2  # conda install -c https://conda.anaconda.org/menpo opencv3\r\nimport scipy.misc\r\nimport dicom  # pip install pydicom\r\nimport numpy\r\nimport math\r\nfrom multiprocessing import Pool\r\n\r\n\r\ndef load_patient(src_dir):\r\n    slices = [dicom.read_file(src_dir + \'/\' + s) for s in os.listdir(src_dir)]\r\n    slices.sort(key=lambda x: int(x.InstanceNumber))\r\n    try:\r\n        slice_thickness = numpy.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\r\n    except:\r\n        slice_thickness = numpy.abs(slices[0].SliceLocation - slices[1].SliceLocation)\r\n\r\n    for s in slices:\r\n        s.SliceThickness = slice_thickness\r\n    return slices\r\n\r\n\r\ndef get_pixels_hu(slices):\r\n    image = numpy.stack([s.pixel_array for s in slices])\r\n    image = image.astype(numpy.int16)\r\n    image[image == -2000] = 0\r\n    for slice_number in range(len(slices)):\r\n        intercept = slices[slice_number].RescaleIntercept\r\n        slope = slices[slice_number].RescaleSlope\r\n        if slope != 1:\r\n            image[slice_number] = slope * image[slice_number].astype(numpy.float64)\r\n            image[slice_number] = image[slice_number].astype(numpy.int16)\r\n        image[slice_number] += numpy.int16(intercept)\r\n\r\n    return numpy.array(image, dtype=numpy.int16)\r\n\r\n\r\ndef resample(image, scan, new_spacing=[1, 1, 1]):\r\n    spacing = map(float, ([scan[0].SliceThickness] + scan[0].PixelSpacing))\r\n    spacing = numpy.array(list(spacing))\r\n    resize_factor = spacing / new_spacing\r\n    new_real_shape = image.shape * resize_factor\r\n    new_shape = numpy.round(new_real_shape)\r\n    real_resize_factor = new_shape / image.shape\r\n    new_spacing = spacing / real_resize_factor\r\n    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\r\n    return image, new_spacing\r\n\r\ndef cv_flip(img,cols,rows,degree):\r\n    M = cv2.getRotationMatrix2D((cols / 2, rows /2), degree, 1.0)\r\n    dst = cv2.warpAffine(img, M, (cols, rows))\r\n    return dst\r\n\r\ndef extract_dicom_images_patient(src_dir):\r\n    target_dir = settings.NDSB3_EXTRACTED_IMAGE_DIR\r\n    print(""Dir: "", src_dir)\r\n    dir_path = settings.NDSB3_RAW_SRC_DIR + src_dir + ""/""\r\n    patient_id = src_dir\r\n    slices = load_patient(dir_path)\r\n    print(len(slices), ""\\t"", slices[0].SliceThickness, ""\\t"", slices[0].PixelSpacing)\r\n    print(""Orientation: "", slices[0].ImageOrientationPatient)\r\n    #assert slices[0].ImageOrientationPatient == [1.000000, 0.000000, 0.000000, 0.000000, 1.000000, 0.000000]\r\n    cos_value = (slices[0].ImageOrientationPatient[0])\r\n    cos_degree = round(math.degrees(math.acos(cos_value)),2)\r\n    \r\n    pixels = get_pixels_hu(slices)\r\n    image = pixels\r\n    print(image.shape)\r\n\r\n    invert_order = slices[1].ImagePositionPatient[2] > slices[0].ImagePositionPatient[2]\r\n    print(""Invert order: "", invert_order, "" - "", slices[1].ImagePositionPatient[2], "","", slices[0].ImagePositionPatient[2])\r\n\r\n    pixel_spacing = slices[0].PixelSpacing\r\n    pixel_spacing.append(slices[0].SliceThickness)\r\n    image = helpers.rescale_patient_images(image, pixel_spacing, settings.TARGET_VOXEL_MM)\r\n    if not invert_order:\r\n        image = numpy.flipud(image)\r\n\r\n    for i in range(image.shape[0]):\r\n        patient_dir = target_dir + patient_id + ""/""\r\n        if not os.path.exists(patient_dir):\r\n            os.mkdir(patient_dir)\r\n        img_path = patient_dir + ""img_"" + str(i).rjust(4, \'0\') + ""_i.png""\r\n        org_img = image[i]\r\n        # if there exists slope,rotation image with corresponding degree\r\n        if cos_degree>0.0:\r\n            org_img = cv_flip(org_img,org_img.shape[1],org_img.shape[0],cos_degree)\r\n        img, mask = helpers.get_segmented_lungs(org_img.copy())\r\n        org_img = helpers.normalize_hu(org_img)\r\n        cv2.imwrite(img_path, org_img * 255)\r\n        cv2.imwrite(img_path.replace(""_i.png"", ""_m.png""), mask * 255)\r\n\r\n\r\ndef extract_dicom_images(clean_targetdir_first=False, only_patient_id=None):\r\n    print(""Extracting images"")\r\n\r\n    target_dir = settings.NDSB3_EXTRACTED_IMAGE_DIR\r\n    if clean_targetdir_first and only_patient_id is not None:\r\n        print(""Cleaning target dir"")\r\n        for file_path in glob.glob(target_dir + ""*.*""):\r\n            os.remove(file_path)\r\n\r\n    if only_patient_id is None:\r\n        dirs = os.listdir(settings.NDSB3_RAW_SRC_DIR)\r\n        if True:\r\n            pool = Pool(settings.WORKER_POOL_SIZE)\r\n            pool.map(extract_dicom_images_patient, dirs)\r\n        else:\r\n            for dir_entry in dirs:\r\n                extract_dicom_images_patient(dir_entry)\r\n    else:\r\n        extract_dicom_images_patient(only_patient_id)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    extract_dicom_images(clean_targetdir_first=False, only_patient_id=None)\r\n'"
step1b_preprocess_make_train_cubes.py,0,"b'import settings\r\nimport helpers\r\nimport glob\r\nimport pandas\r\nimport ntpath\r\nimport numpy\r\nimport cv2\r\nimport os\r\n\r\nCUBE_IMGTYPE_SRC = ""_i""\r\n\r\n\r\ndef save_cube_img(target_path, cube_img, rows, cols):\r\n    assert rows * cols == cube_img.shape[0]\r\n    img_height = cube_img.shape[1]\r\n    img_width = cube_img.shape[1]\r\n    res_img = numpy.zeros((rows * img_height, cols * img_width), dtype=numpy.uint8)\r\n\r\n    for row in range(rows):\r\n        for col in range(cols):\r\n            target_y = row * img_height\r\n            target_x = col * img_width\r\n            res_img[target_y:target_y + img_height, target_x:target_x + img_width] = cube_img[row * cols + col]\r\n\r\n    cv2.imwrite(target_path, res_img)\r\n\r\n\r\ndef get_cube_from_img(img3d, center_x, center_y, center_z, block_size):\r\n    start_x = max(center_x - block_size / 2, 0)\r\n    if start_x + block_size > img3d.shape[2]:\r\n        start_x = img3d.shape[2] - block_size\r\n\r\n    start_y = max(center_y - block_size / 2, 0)\r\n    start_z = max(center_z - block_size / 2, 0)\r\n    if start_z + block_size > img3d.shape[0]:\r\n        start_z = img3d.shape[0] - block_size\r\n    start_z = int(start_z)\r\n    start_y = int(start_y)\r\n    start_x = int(start_x)\r\n    res = img3d[start_z:start_z + block_size, start_y:start_y + block_size, start_x:start_x + block_size]\r\n    return res\r\n\r\n\r\ndef make_pos_annotation_images():\r\n    src_dir = settings.LUNA_16_TRAIN_DIR2D2 + ""metadata/""\r\n    dst_dir = settings.BASE_DIR_SSD + ""luna16_train_cubes_pos/""\r\n    for file_path in glob.glob(dst_dir + ""*.*""):\r\n        os.remove(file_path)\r\n\r\n    for patient_index, csv_file in enumerate(glob.glob(src_dir + ""*_annos_pos.csv"")):\r\n        patient_id = ntpath.basename(csv_file).replace(""_annos_pos.csv"", """")\r\n        # print(patient_id)\r\n        # if not ""148229375703208214308676934766"" in patient_id:\r\n        #     continue\r\n        df_annos = pandas.read_csv(csv_file)\r\n        if len(df_annos) == 0:\r\n            continue\r\n        images = helpers.load_patient_images(patient_id, settings.LUNA_16_TRAIN_DIR2D2, ""*"" + CUBE_IMGTYPE_SRC + "".png"")\r\n\r\n        for index, row in df_annos.iterrows():\r\n            coord_x = int(row[""coord_x""] * images.shape[2])\r\n            coord_y = int(row[""coord_y""] * images.shape[1])\r\n            coord_z = int(row[""coord_z""] * images.shape[0])\r\n            diam_mm = int(row[""diameter""] * images.shape[2])\r\n            anno_index = int(row[""anno_index""])\r\n            cube_img = get_cube_from_img(images, coord_x, coord_y, coord_z, 64)\r\n            if cube_img.sum() < 5:\r\n                print("" ***** Skipping "", coord_x, coord_y, coord_z)\r\n                continue\r\n\r\n            if cube_img.mean() < 10:\r\n                print("" ***** Suspicious "", coord_x, coord_y, coord_z)\r\n\r\n            save_cube_img(dst_dir + patient_id + ""_"" + str(anno_index) + ""_"" + str(diam_mm) + ""_1_"" + ""pos.png"", cube_img, 8, 8)\r\n        helpers.print_tabbed([patient_index, patient_id, len(df_annos)], [5, 64, 8])\r\n\r\n\r\ndef make_annotation_images_lidc():\r\n    src_dir = settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/""\r\n\r\n    dst_dir = settings.BASE_DIR_SSD + ""generated_traindata/luna16_train_cubes_lidc/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n    for file_path in glob.glob(dst_dir + ""*.*""):\r\n        os.remove(file_path)\r\n\r\n    for patient_index, csv_file in enumerate(glob.glob(src_dir + ""*_annos_pos_lidc.csv"")):\r\n        patient_id = ntpath.basename(csv_file).replace(""_annos_pos_lidc.csv"", """")\r\n        df_annos = pandas.read_csv(csv_file)\r\n        if len(df_annos) == 0:\r\n            continue\r\n        images = helpers.load_patient_images(patient_id, settings.LUNA16_EXTRACTED_IMAGE_DIR, ""*"" + CUBE_IMGTYPE_SRC + "".png"")\r\n\r\n        for index, row in df_annos.iterrows():\r\n            coord_x = int(row[""coord_x""] * images.shape[2])\r\n            coord_y = int(row[""coord_y""] * images.shape[1])\r\n            coord_z = int(row[""coord_z""] * images.shape[0])\r\n            malscore = int(row[""malscore""])\r\n            anno_index = row[""anno_index""]\r\n            anno_index = str(anno_index).replace("" "", ""xspacex"").replace(""."", ""xpointx"").replace(""_"", ""xunderscorex"")\r\n            cube_img = get_cube_from_img(images, coord_x, coord_y, coord_z, 64)\r\n            if cube_img.sum() < 5:\r\n                print("" ***** Skipping "", coord_x, coord_y, coord_z)\r\n                continue\r\n\r\n            if cube_img.mean() < 10:\r\n                print("" ***** Suspicious "", coord_x, coord_y, coord_z)\r\n\r\n            if cube_img.shape != (64, 64, 64):\r\n                print("" ***** incorrect shape !!! "", str(anno_index), "" - "",(coord_x, coord_y, coord_z))\r\n                continue\r\n\r\n            save_cube_img(dst_dir + patient_id + ""_"" + str(anno_index) + ""_"" + str(malscore * malscore) + ""_1_pos.png"", cube_img, 8, 8)\r\n        helpers.print_tabbed([patient_index, patient_id, len(df_annos)], [5, 64, 8])\r\n\r\n\r\ndef make_pos_annotation_images_manual():\r\n    src_dir = ""resources/luna16_manual_labels/""\r\n\r\n    dst_dir = settings.BASE_DIR_SSD + ""generated_traindata/luna16_train_cubes_manual/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n    for file_path in glob.glob(dst_dir + ""*_manual.*""):\r\n        os.remove(file_path)\r\n\r\n    for patient_index, csv_file in enumerate(glob.glob(src_dir + ""*.csv"")):\r\n        patient_id = ntpath.basename(csv_file).replace("".csv"", """")\r\n        if ""1.3.6.1.4"" not in patient_id:\r\n            continue\r\n\r\n        print(patient_id)\r\n        # if not ""172845185165807139298420209778"" in patient_id:\r\n        #     continue\r\n        df_annos = pandas.read_csv(csv_file)\r\n        if len(df_annos) == 0:\r\n            continue\r\n        images = helpers.load_patient_images(patient_id, settings.LUNA16_EXTRACTED_IMAGE_DIR, ""*"" + CUBE_IMGTYPE_SRC + "".png"")\r\n\r\n        for index, row in df_annos.iterrows():\r\n            coord_x = int(row[""x""] * images.shape[2])\r\n            coord_y = int(row[""y""] * images.shape[1])\r\n            coord_z = int(row[""z""] * images.shape[0])\r\n            diameter = int(row[""d""] * images.shape[2])\r\n            node_type = int(row[""id""])\r\n            malscore = int(diameter)\r\n            malscore = min(25, malscore)\r\n            malscore = max(16, malscore)\r\n            anno_index = index\r\n            cube_img = get_cube_from_img(images, coord_x, coord_y, coord_z, 64)\r\n            if cube_img.sum() < 5:\r\n                print("" ***** Skipping "", coord_x, coord_y, coord_z)\r\n                continue\r\n\r\n            if cube_img.mean() < 10:\r\n                print("" ***** Suspicious "", coord_x, coord_y, coord_z)\r\n\r\n            if cube_img.shape != (64, 64, 64):\r\n                print("" ***** incorrect shape !!! "", str(anno_index), "" - "",(coord_x, coord_y, coord_z))\r\n                continue\r\n\r\n            save_cube_img(dst_dir + patient_id + ""_"" + str(anno_index) + ""_"" + str(malscore) + ""_1_"" + (""pos"" if node_type == 0 else ""neg"") + "".png"", cube_img, 8, 8)\r\n        helpers.print_tabbed([patient_index, patient_id, len(df_annos)], [5, 64, 8])\r\n\r\n\r\ndef make_candidate_auto_images(candidate_types=[]):\r\n    dst_dir = settings.BASE_DIR_SSD + ""generated_traindata/luna16_train_cubes_auto/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n    for candidate_type in candidate_types:\r\n        for file_path in glob.glob(dst_dir + ""*_"" + candidate_type + "".png""):\r\n            os.remove(file_path)\r\n\r\n    for candidate_type in candidate_types:\r\n        if candidate_type == ""falsepos"":\r\n            src_dir = ""resources/luna16_falsepos_labels/""\r\n        else:\r\n            src_dir = settings.LUNA16_EXTRACTED_IMAGE_DIR + ""_labels/""\r\n\r\n        for index, csv_file in enumerate(glob.glob(src_dir + ""*_candidates_"" + candidate_type + "".csv"")):\r\n            patient_id = ntpath.basename(csv_file).replace(""_candidates_"" + candidate_type + "".csv"", """")\r\n            print(index, "",patient: "", patient_id, "" type:"", candidate_type)\r\n            # if not ""148229375703208214308676934766"" in patient_id:\r\n            #     continue\r\n            df_annos = pandas.read_csv(csv_file)\r\n            if len(df_annos) == 0:\r\n                continue\r\n            images = helpers.load_patient_images(patient_id, settings.LUNA16_EXTRACTED_IMAGE_DIR, ""*"" + CUBE_IMGTYPE_SRC + "".png"", exclude_wildcards=[])\r\n\r\n            row_no = 0\r\n            for index, row in df_annos.iterrows():\r\n                coord_x = int(row[""coord_x""] * images.shape[2])\r\n                coord_y = int(row[""coord_y""] * images.shape[1])\r\n                coord_z = int(row[""coord_z""] * images.shape[0])\r\n                anno_index = int(row[""anno_index""])\r\n                cube_img = get_cube_from_img(images, coord_x, coord_y, coord_z, 48)\r\n                if cube_img.sum() < 10:\r\n                    print(""Skipping "", coord_x, coord_y, coord_z)\r\n                    continue\r\n                # print(cube_img.sum())\r\n                try:\r\n                    save_cube_img(dst_dir + patient_id + ""_"" + str(anno_index) + ""_0_"" + candidate_type + "".png"", cube_img, 6, 8)\r\n                except Exception as ex:\r\n                    print(ex)\r\n\r\n                row_no += 1\r\n                max_item = 240 if candidate_type == ""white"" else 200\r\n                if candidate_type == ""luna"":\r\n                    max_item = 500\r\n                if row_no > max_item:\r\n                    break\r\n\r\n\r\ndef make_pos_annotation_images_manual_ndsb3():\r\n    src_dir = ""resources/ndsb3_manual_labels/""\r\n    dst_dir = settings.BASE_DIR_SSD + ""generated_traindata/ndsb3_train_cubes_manual/""\r\n    if not os.path.exists(dst_dir):\r\n        os.mkdir(dst_dir)\r\n\r\n\r\n    train_label_df = pandas.read_csv(""resources/stage1_labels.csv"")\r\n    train_label_df.set_index([""id""], inplace=True)\r\n    for file_path in glob.glob(dst_dir + ""*.*""):\r\n        os.remove(file_path)\r\n\r\n    for patient_index, csv_file in enumerate(glob.glob(src_dir + ""*.csv"")):\r\n        patient_id = ntpath.basename(csv_file).replace("".csv"", """")\r\n        if ""1.3.6.1.4.1"" in patient_id:\r\n            continue\r\n\r\n        cancer_label = train_label_df.loc[patient_id][""cancer""]\r\n        df_annos = pandas.read_csv(csv_file)\r\n        if len(df_annos) == 0:\r\n            continue\r\n        images = helpers.load_patient_images(patient_id, settings.NDSB3_EXTRACTED_IMAGE_DIR, ""*"" + CUBE_IMGTYPE_SRC + "".png"")\r\n\r\n        anno_index = 0\r\n        for index, row in df_annos.iterrows():\r\n            pos_neg = ""pos"" if row[""id""] == 0 else ""neg""\r\n            coord_x = int(row[""x""] * images.shape[2])\r\n            coord_y = int(row[""y""] * images.shape[1])\r\n            coord_z = int(row[""z""] * images.shape[0])\r\n            malscore = int(round(row[""dmm""]))\r\n            anno_index += 1\r\n            cube_img = get_cube_from_img(images, coord_x, coord_y, coord_z, 64)\r\n            if cube_img.sum() < 5:\r\n                print("" ***** Skipping "", coord_x, coord_y, coord_z)\r\n                continue\r\n\r\n            if cube_img.mean() < 10:\r\n                print("" ***** Suspicious "", coord_x, coord_y, coord_z)\r\n\r\n            if cube_img.shape != (64, 64, 64):\r\n                print("" ***** incorrect shape !!! "", str(anno_index), "" - "",(coord_x, coord_y, coord_z))\r\n                continue\r\n            print(patient_id)\r\n            assert malscore > 0 or pos_neg == ""neg""\r\n            save_cube_img(dst_dir + ""ndsb3manual_"" + patient_id + ""_"" + str(anno_index) + ""_"" + pos_neg + ""_"" + str(cancer_label) + ""_"" + str(malscore) + ""_1_pn.png"", cube_img, 8, 8)\r\n        helpers.print_tabbed([patient_index, patient_id, len(df_annos)], [5, 64, 8])\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    if not os.path.exists(settings.BASE_DIR_SSD + ""generated_traindata/""):\r\n        os.mkdir(settings.BASE_DIR_SSD + ""generated_traindata/"")\r\n\r\n    if True:\r\n        make_annotation_images_lidc()\r\n    if True:\r\n        make_pos_annotation_images_manual()\r\n    # if False:\r\n    #     make_pos_annotation_images()  # not used anymore\r\n    if True:\r\n        make_candidate_auto_images([""falsepos"", ""luna"", ""edge""])\r\n    if True:\r\n        make_pos_annotation_images_manual_ndsb3()  # for second model\r\n\r\n\r\n\r\n\r\n'"
step2_train_mass_segmenter.py,0,"b'import settings\r\nimport helpers\r\n\r\nimport os\r\nimport glob\r\nimport random\r\nimport ntpath\r\nimport cv2\r\nimport numpy\r\nfrom typing import List, Tuple\r\nfrom keras.optimizers import Adam, SGD\r\nfrom keras.layers import Input, Convolution2D, MaxPooling2D, UpSampling2D, merge, BatchNormalization, SpatialDropout2D\r\nfrom keras.models import Model\r\nfrom keras import backend as K\r\nfrom keras.callbacks import ModelCheckpoint, Callback\r\nfrom scipy.ndimage.interpolation import map_coordinates\r\nfrom scipy.ndimage.filters import gaussian_filter\r\nimport pandas\r\nimport shutil\r\n\r\nMEAN_FRAME_COUNT = 1\r\nCHANNEL_COUNT = 1\r\n\r\n\r\ndef random_scale_img(img, xy_range, lock_xy=False):\r\n    if random.random() > xy_range.chance:\r\n        return img\r\n\r\n    if not isinstance(img, list):\r\n        img = [img]\r\n\r\n    import cv2\r\n    scale_x = random.uniform(xy_range.x_min, xy_range.x_max)\r\n    scale_y = random.uniform(xy_range.y_min, xy_range.y_max)\r\n    if lock_xy:\r\n        scale_y = scale_x\r\n\r\n    org_height, org_width = img[0].shape[:2]\r\n    xy_range.last_x = scale_x\r\n    xy_range.last_y = scale_y\r\n\r\n    res = []\r\n    for img_inst in img:\r\n        scaled_width = int(org_width * scale_x)\r\n        scaled_height = int(org_height * scale_y)\r\n        scaled_img = cv2.resize(img_inst, (scaled_width, scaled_height), interpolation=cv2.INTER_CUBIC)\r\n        if scaled_width < org_width:\r\n            extend_left = (org_width - scaled_width) / 2\r\n            extend_right = org_width - extend_left - scaled_width\r\n            scaled_img = cv2.copyMakeBorder(scaled_img, 0, 0, extend_left, extend_right, borderType=cv2.BORDER_CONSTANT)\r\n            scaled_width = org_width\r\n\r\n        if scaled_height < org_height:\r\n            extend_top = (org_height - scaled_height) / 2\r\n            extend_bottom = org_height - extend_top - scaled_height\r\n            scaled_img = cv2.copyMakeBorder(scaled_img, extend_top, extend_bottom, 0, 0,  borderType=cv2.BORDER_CONSTANT)\r\n            scaled_height = org_height\r\n\r\n        start_x = (scaled_width - org_width) / 2\r\n        start_y = (scaled_height - org_height) / 2\r\n        tmp = scaled_img[start_y: start_y + org_height, start_x: start_x + org_width]\r\n        res.append(tmp)\r\n\r\n    return res\r\n\r\n\r\nclass XYRange:\r\n    def __init__(self, x_min, x_max, y_min, y_max, chance=1.0):\r\n        self.chance = chance\r\n        self.x_min = x_min\r\n        self.x_max = x_max\r\n        self.y_min = y_min\r\n        self.y_max = y_max\r\n        self.last_x = 0\r\n        self.last_y = 0\r\n\r\n    def get_last_xy_txt(self):\r\n        res = ""x_"" + str(int(self.last_x * 100)).replace(""-"", ""m"") + ""-"" + ""y_"" + str(int(self.last_y * 100)).replace(""-"", ""m"")\r\n        return res\r\n\r\n\r\ndef random_translate_img(img, xy_range, border_mode=""constant""):\r\n    if random.random() > xy_range.chance:\r\n        return img\r\n    import cv2\r\n    if not isinstance(img, list):\r\n        img = [img]\r\n\r\n    org_height, org_width = img[0].shape[:2]\r\n    translate_x = random.randint(xy_range.x_min, xy_range.x_max)\r\n    translate_y = random.randint(xy_range.y_min, xy_range.y_max)\r\n    trans_matrix = numpy.float32([[1, 0, translate_x], [0, 1, translate_y]])\r\n\r\n    border_const = cv2.BORDER_CONSTANT\r\n    if border_mode == ""reflect"":\r\n        border_const = cv2.BORDER_REFLECT\r\n\r\n    res = []\r\n    for img_inst in img:\r\n        img_inst = cv2.warpAffine(img_inst, trans_matrix, (org_width, org_height), borderMode=border_const)\r\n        res.append(img_inst)\r\n    if len(res) == 1:\r\n        res = res[0]\r\n    xy_range.last_x = translate_x\r\n    xy_range.last_y = translate_y\r\n    return res\r\n\r\n\r\ndef random_rotate_img(img, chance, min_angle, max_angle):\r\n    import cv2\r\n    if random.random() > chance:\r\n        return img\r\n    if not isinstance(img, list):\r\n        img = [img]\r\n\r\n    angle = random.randint(min_angle, max_angle)\r\n    center = (img[0].shape[0] / 2, img[0].shape[1] / 2)\r\n    rot_matrix = cv2.getRotationMatrix2D(center, angle, scale=1.0)\r\n\r\n    res = []\r\n    for img_inst in img:\r\n        img_inst = cv2.warpAffine(img_inst, rot_matrix, dsize=img_inst.shape[:2], borderMode=cv2.BORDER_CONSTANT)\r\n        res.append(img_inst)\r\n    if len(res) == 0:\r\n        res = res[0]\r\n    return res\r\n\r\n\r\ndef random_flip_img(img, horizontal_chance=0, vertical_chance=0):\r\n    import cv2\r\n    flip_horizontal = False\r\n    if random.random() < horizontal_chance:\r\n        flip_horizontal = True\r\n\r\n    flip_vertical = False\r\n    if random.random() < vertical_chance:\r\n        flip_vertical = True\r\n\r\n    if not flip_horizontal and not flip_vertical:\r\n        return img\r\n\r\n    flip_val = 1\r\n    if flip_vertical:\r\n        flip_val = -1 if flip_horizontal else 0\r\n\r\n    if not isinstance(img, list):\r\n        res = cv2.flip(img, flip_val) # 0 = X axis, 1 = Y axis,  -1 = both\r\n    else:\r\n        res = []\r\n        for img_item in img:\r\n            img_flip = cv2.flip(img_item, flip_val)\r\n            res.append(img_flip)\r\n    return res\r\n\r\n\r\nELASTIC_INDICES = None  # needed to make it faster to fix elastic deformation per epoch.\r\ndef elastic_transform(image, alpha, sigma, random_state=None):\r\n    global ELASTIC_INDICES\r\n    shape = image.shape\r\n\r\n    if ELASTIC_INDICES == None:\r\n        if random_state is None:\r\n            random_state = numpy.random.RandomState(1301)\r\n\r\n        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=""constant"", cval=0) * alpha\r\n        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=""constant"", cval=0) * alpha\r\n        x, y = numpy.meshgrid(numpy.arange(shape[0]), numpy.arange(shape[1]))\r\n        ELASTIC_INDICES = numpy.reshape(y + dy, (-1, 1)), numpy.reshape(x + dx, (-1, 1))\r\n    return map_coordinates(image, ELASTIC_INDICES, order=1).reshape(shape)\r\n\r\n\r\ndef prepare_image_for_net(img):\r\n    img = img.astype(numpy.float)\r\n    img /= 255.\r\n    if len(img.shape) == 3:\r\n        img = img.reshape(img.shape[-3], img.shape[-2], img.shape[-1])\r\n    else:\r\n        img = img.reshape(1, img.shape[-2], img.shape[-1], 1)\r\n    return img\r\n\r\n\r\ndef get_train_holdout_files(model_type, holdout, train_percentage=80, frame_count=8):\r\n    print(""Get train/holdout files."")\r\n    file_paths = glob.glob(""resources/segmenter_traindata/"" + ""*_1.png"")\r\n    file_paths.sort()\r\n    train_res = []\r\n    holdout_res = []\r\n    for index, file_path in enumerate(file_paths):\r\n        file_name = ntpath.basename(file_path)\r\n        overlay_path = file_path.replace(""_1.png"", ""_o.png"")\r\n        train_set = False\r\n        if ""1.3.6.1.4"" in file_name or ""spie"" in file_name or ""TIME"" in file_name:\r\n            train_set = True\r\n        else:\r\n            patient_id = file_name.split(""_"")[0]\r\n            if helpers.get_patient_fold(patient_id) % 3 != holdout:\r\n                train_set = True\r\n\r\n        if train_set:\r\n            train_res.append((file_path, overlay_path))\r\n        else:\r\n            holdout_res.append((file_path, overlay_path))\r\n    print(""Train count: "", len(train_res), "", holdout count: "", len(holdout_res))\r\n    return train_res, holdout_res\r\n\r\n\r\ndef dice_coef(y_true, y_pred):\r\n    y_true_f = K.flatten(y_true)\r\n    y_pred_f = K.flatten(y_pred)\r\n    intersection = K.sum(y_true_f * y_pred_f)\r\n    return (2. * intersection + 100) / (K.sum(y_true_f) + K.sum(y_pred_f) + 100)\r\n\r\n\r\ndef dice_coef_np(y_true, y_pred):\r\n    y_true_f = y_true.flatten()\r\n    y_pred_f = y_pred.flatten()\r\n    intersection = numpy.sum(y_true_f * y_pred_f)\r\n    return (2. * intersection + 100) / (numpy.sum(y_true_f) + numpy.sum(y_pred_f) + 100)\r\n\r\n\r\ndef dice_coef_loss(y_true, y_pred):\r\n    return -dice_coef(y_true, y_pred)\r\n\r\n\r\nclass DumpPredictions(Callback):\r\n\r\n    def __init__(self, dump_filelist : List[Tuple[str, str]], model_type):\r\n        super(DumpPredictions, self).__init__()\r\n        self.dump_filelist = dump_filelist\r\n        self.batch_count = 0\r\n        if not os.path.exists(""workdir/segmenter/""):\r\n            os.mkdir(""workdir/segmenter/"")\r\n        for file_path in glob.glob(""workdir/segmenter/*.*""):\r\n            os.remove(file_path)\r\n        self.model_type = model_type\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        model = self.model  # type: Model\r\n        generator = image_generator(self.dump_filelist, 1, train_set=False, model_type=self.model_type)\r\n        for i in range(0, 10):\r\n            x, y = next(generator)\r\n            y_pred = model.predict(x, batch_size=1)\r\n\r\n            x = x.swapaxes(0, 3)\r\n            x = x[0]\r\n            # print(x.shape, y.shape, y_pred.shape)\r\n            x *= 255.\r\n            x = x.reshape((x.shape[0], x.shape[0])).astype(numpy.uint8)\r\n            y *= 255.\r\n            y = y.reshape((y.shape[1], y.shape[2])).astype(numpy.uint8)\r\n            y_pred *= 255.\r\n            y_pred = y_pred.reshape((y_pred.shape[1], y_pred.shape[2])).astype(numpy.uint8)\r\n            # cv2.imwrite(""workdir/segmenter/img_{0:03d}_{1:02d}_i.png"".format(epoch, i), x)\r\n            # cv2.imwrite(""workdit/segmenter/img_{0:03d}_{1:02d}_o.png"".format(epoch, i), y)\r\n            # cv2.imwrite(""workdit/segmenter/img_{0:03d}_{1:02d}_p.png"".format(epoch, i), y_pred)\r\n\r\n\r\ndef image_generator(batch_files, batch_size, train_set, model_type):\r\n    global ELASTIC_INDICES\r\n    while True:\r\n        if train_set:\r\n            random.shuffle(batch_files)\r\n\r\n        img_list = []\r\n        overlay_list = []\r\n        ELASTIC_INDICES = None\r\n        for batch_file_idx, batch_file in enumerate(batch_files):\r\n            images = []\r\n            img = cv2.imread(batch_file[0], cv2.IMREAD_GRAYSCALE)\r\n            images.append(img)\r\n            overlay = cv2.imread(batch_file[1], cv2.IMREAD_GRAYSCALE)\r\n\r\n            if train_set:\r\n                if random.randint(0, 100) > 50:\r\n                    for img_index, img in enumerate(images):\r\n                        images[img_index] = elastic_transform(img, 128, 15)\r\n                    overlay = elastic_transform(overlay, 128, 15)\r\n\r\n                if True:\r\n                    augmented = images + [overlay]\r\n                    augmented = random_rotate_img(augmented, 0.8, -20, 20)\r\n                    augmented = random_flip_img(augmented, 0.5, 0.5)\r\n\r\n                    # processed = helpers_augmentation.random_flip_img(processed, horizontal_chance=0.5, vertical_chance=0)\r\n                    # processed = helpers_augmentation.random_scale_img(processed, xy_range=helpers_augmentation.XYRange(x_min=0.8, x_max=1.2, y_min=0.8, y_max=1.2, chance=1.0))\r\n                    augmented = random_translate_img(augmented, XYRange(-30, 30, -30, 30, 0.8))\r\n                    images = augmented[:-1]\r\n                    overlay = augmented[-1]\r\n\r\n            for index, img in enumerate(images):\r\n                # img = img[crop_y: crop_y + settings.TRAIN_IMG_HEIGHT3D, crop_x: crop_x + settings.TRAIN_IMG_WIDTH3D]\r\n                img = prepare_image_for_net(img)\r\n                images[index] = img\r\n\r\n            # helpers_augmentation.dump_augmented_image(img, mean_img=None, target_path=""c:\\\\tmp\\\\"" + batch_file[0])\r\n            # overlay = overlay[crop_y: crop_y + settings.TRAIN_IMG_HEIGHT3D, crop_x: crop_x + settings.TRAIN_IMG_WIDTH3D]\r\n            overlay = prepare_image_for_net(overlay)\r\n            # overlay = overlay.reshape(1, overlay.shape[-3] * overlay.shape[-2])\r\n            # overlay *= settings.OVERLAY_MULTIPLIER\r\n            images3d = numpy.vstack(images)\r\n            images3d = images3d.swapaxes(0, 3)\r\n\r\n            img_list.append(images3d)\r\n            overlay_list.append(overlay)\r\n            if len(img_list) >= batch_size:\r\n                x = numpy.vstack(img_list)\r\n                y = numpy.vstack(overlay_list)\r\n                # if len(img_list) >= batch_size:\r\n                yield x, y\r\n                img_list = []\r\n                overlay_list = []\r\n\r\n\r\ndef get_unet(learn_rate, load_weights_path=None) -> Model:\r\n    inputs = Input((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE, CHANNEL_COUNT))\r\n    filter_size = 32\r\n    growth_step = 32\r\n    x = BatchNormalization()(inputs)\r\n    conv1 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(x)\r\n    conv1 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(conv1)\r\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n\r\n    pool1 = BatchNormalization()(pool1)\r\n    filter_size += growth_step\r\n    conv2 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(pool1)\r\n    conv2 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(conv2)\r\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n    pool2 = BatchNormalization()(pool2)\r\n\r\n    filter_size += growth_step\r\n    conv3 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(pool2)\r\n    conv3 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(conv3)\r\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n    pool3 = BatchNormalization()(pool3)\r\n\r\n    filter_size += growth_step\r\n    conv4 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(pool3)\r\n    conv4 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(conv4)\r\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\r\n    pool4 = BatchNormalization()(pool4)\r\n\r\n    conv5 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(pool4)\r\n    conv5 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\', name=""conv5b"")(conv5)\r\n    pool5 = MaxPooling2D(pool_size=(2, 2), name=""pool5"")(conv5)\r\n    pool5 = BatchNormalization()(pool5)\r\n\r\n    conv6 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(pool5)\r\n    conv6 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\', name=""conv6b"")(conv6)\r\n\r\n    up6 = UpSampling2D(size=(2, 2), name=""up6"")(conv6)\r\n    up6 = merge([up6, conv5], mode=\'concat\', concat_axis=3)\r\n    up6 = BatchNormalization()(up6)\r\n\r\n    # up6 = SpatialDropout2D(0.1)(up6)\r\n    filter_size -= growth_step\r\n    conv66 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(up6)\r\n    conv66 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(conv66)\r\n\r\n    up7 = merge([UpSampling2D(size=(2, 2))(conv66), conv4], mode=\'concat\', concat_axis=3)\r\n    up7 = BatchNormalization()(up7)\r\n    # up7 = SpatialDropout2D(0.1)(up7)\r\n\r\n    filter_size -= growth_step\r\n    conv7 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(up7)\r\n    conv7 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(conv7)\r\n\r\n    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv3], mode=\'concat\', concat_axis=3)\r\n    up8 = BatchNormalization()(up8)\r\n    filter_size -= growth_step\r\n    conv8 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(up8)\r\n    conv8 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(conv8)\r\n\r\n\r\n    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv2], mode=\'concat\', concat_axis=3)\r\n    up9 = BatchNormalization()(up9)\r\n    conv9 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(up9)\r\n    conv9 = Convolution2D(filter_size, 3, 3, activation=\'relu\', border_mode=\'same\')(conv9)\r\n    # conv9 = BatchNormalization()(conv9)\r\n\r\n    up10 = UpSampling2D(size=(2, 2))(conv9)\r\n    conv10 = Convolution2D(1, 1, 1, activation=\'sigmoid\')(up10)\r\n\r\n    model = Model(input=inputs, output=conv10)\r\n    # model.load_weights(load_weights_path)\r\n    # model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])\r\n    model.compile(optimizer=SGD(lr=learn_rate, momentum=0.9, nesterov=True), loss=dice_coef_loss, metrics=[dice_coef])\r\n\r\n    model.summary()\r\n    return model\r\n\r\n\r\ndef train_model(holdout, model_type, continue_from=None):\r\n    batch_size = 4\r\n    train_percentage = 80 if model_type == ""masses"" else 90\r\n    train_files, holdout_files = get_train_holdout_files( model_type, holdout, train_percentage, frame_count=CHANNEL_COUNT)\r\n    # train_files = train_files[:100]\r\n    # holdout_files = train_files[:10]\r\n\r\n    tmp_gen = image_generator(train_files[:2], 2, True, model_type)\r\n    for i in range(10):\r\n        x = next(tmp_gen)\r\n        img = x[0][0].reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\r\n        img *= 255\r\n        # cv2.imwrite(""c:/tmp/img_"" + str(i).rjust(3, \'0\') + ""i.png"", img)\r\n        img = x[1][0].reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\r\n        img *= 255\r\n        # cv2.imwrite(""c:/tmp/img_"" + str(i).rjust(3, \'0\') + ""o.png"", img)\r\n        # print(x.shape)\r\n\r\n    train_gen = image_generator(train_files, batch_size, True, model_type)\r\n    holdout_gen = image_generator(holdout_files, batch_size, False, model_type)\r\n\r\n    if continue_from is None:\r\n        model = get_unet(0.001)\r\n    else:\r\n        model = get_unet(0.0001)\r\n        model.load_weights(continue_from)\r\n\r\n    checkpoint1 = ModelCheckpoint(""workdir/"" + model_type +""_model_h"" + str(holdout) + ""_{epoch:02d}-{val_loss:.2f}.hd5"", monitor=\'val_loss\', verbose=1, save_best_only=True, save_weights_only=False, mode=\'auto\', period=1)\r\n    checkpoint2 = ModelCheckpoint(""workdir/"" + model_type +""_model_h"" + str(holdout) + ""_best.hd5"", monitor=\'val_loss\', verbose=1, save_best_only=True, save_weights_only=False, mode=\'auto\', period=1)\r\n    files = []\r\n    idx = 0\r\n    while (idx < (len(holdout_files))):\r\n        files.append(holdout_files[idx])\r\n        idx += 5\r\n    dumper = DumpPredictions(holdout_files[::10], model_type)\r\n    epoch_div = 1\r\n    epoch_count = 200 if model_type == ""masses"" else 50\r\n    model.fit_generator(train_gen, len(train_files) / epoch_div, epoch_count, validation_data=holdout_gen, nb_val_samples=len(holdout_files) / epoch_div, callbacks=[checkpoint1, checkpoint2, dumper])\r\n    if not os.path.exists(""models""):\r\n        os.mkdir(""models"")\r\n    shutil.copy(""workdir/"" + model_type +""_model_h"" + str(holdout) + ""_best.hd5"", ""models/"" + model_type +""_model_h"" + str(holdout) + ""_best.hd5"")\r\n\r\ndef predict_patients(patients_dir, model_path, holdout, patient_predictions, model_type):\r\n    model = get_unet(0.001)\r\n    model.load_weights(model_path)\r\n    for item_name in os.listdir(patients_dir):\r\n        if not os.path.isdir(patients_dir + item_name):\r\n            continue\r\n        patient_id = item_name\r\n\r\n        if holdout >= 0:\r\n            patient_fold = helpers.get_patient_fold(patient_id, submission_set_neg=True)\r\n            if patient_fold < 0:\r\n                if holdout != 0:\r\n                    continue\r\n            else:\r\n                patient_fold %= 3\r\n                if patient_fold != holdout:\r\n                    continue\r\n\r\n        # if ""100953483028192176989979435275"" not in patient_id:\r\n        #     continue\r\n        print(patient_id)\r\n        patient_dir = patients_dir + patient_id + ""/""\r\n        mass = 0\r\n        img_type = ""_i"" if model_type == ""masses"" else ""_c""\r\n        slices = glob.glob(patient_dir + ""*"" + img_type + "".png"")\r\n        if model_type == ""emphysema"":\r\n            slices = slices[int(len(slices) / 2):]\r\n        for img_path in slices:\r\n            src_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\r\n            src_img = cv2.resize(src_img, dsize=(settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\r\n            src_img = prepare_image_for_net(src_img)\r\n            p = model.predict(src_img, batch_size=1)\r\n            p[p < 0.5] = 0\r\n            mass += p.sum()\r\n            p = p[0, :, :, 0] * 255\r\n            # cv2.imwrite(img_path.replace(""_i.png"", ""_mass.png""), p)\r\n            src_img = src_img.reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\r\n            src_img *= 255\r\n            # src_img = cv2.cvtColor(src_img.astype(numpy.uint8), cv2.COLOR_GRAY2BGR)\r\n            # p = cv2.cvtColor(p.astype(numpy.uint8), cv2.COLOR_GRAY2BGRA)\r\n            src_img = cv2.addWeighted(p.astype(numpy.uint8), 0.2, src_img.astype(numpy.uint8), 1 - 0.2, 0)\r\n            cv2.imwrite(img_path.replace(img_type + "".png"", ""_"" + model_type + ""o.png""), src_img)\r\n\r\n        if mass > 1:\r\n            print(model_type + "": "", mass)\r\n        patient_predictions.append((patient_id, mass))\r\n        df = pandas.DataFrame(patient_predictions, columns=[""patient_id"", ""prediction""])\r\n        df.to_csv(settings.BASE_DIR + model_type + ""_predictions.csv"", index=False)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    continue_from = None\r\n    if True:\r\n        for model_type_name in [""masses""]:\r\n            train_model(holdout=0, model_type=model_type_name, continue_from=continue_from)\r\n            train_model(holdout=1, model_type=model_type_name, continue_from=continue_from)\r\n            train_model(holdout=2, model_type=model_type_name, continue_from=continue_from)\r\n\r\n    if True:\r\n        for model_type_name in [""masses""]:\r\n            patient_predictions_global = []\r\n            for holdout_no in [0, 1, 2]:\r\n                patient_base_dir = settings.NDSB3_EXTRACTED_IMAGE_DIR\r\n                predict_patients(patients_dir=patient_base_dir, model_path=""models/"" + model_type_name + ""_model_h"" + str(holdout_no) + ""_best.hd5"", holdout=holdout_no, patient_predictions=patient_predictions_global, model_type=model_type_name)\r\n\r\n\r\n'"
step2_train_nodule_detector.py,2,"b'import settings\r\nimport helpers\r\nimport sys\r\nimport os\r\nimport glob\r\nimport random\r\nimport pandas\r\nimport ntpath\r\nimport cv2\r\nimport numpy\r\nfrom typing import List, Tuple\r\nfrom keras.optimizers import Adam, SGD\r\nfrom keras.layers import Input, Convolution2D, MaxPooling2D, UpSampling2D, merge, Convolution3D, MaxPooling3D, UpSampling3D, LeakyReLU, BatchNormalization, Flatten, Dense, Dropout, ZeroPadding3D, AveragePooling3D, Activation\r\nfrom keras.models import Model, load_model, model_from_json\r\nfrom keras.metrics import binary_accuracy, binary_crossentropy, mean_squared_error, mean_absolute_error\r\nfrom keras import backend as K\r\nfrom keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler\r\nfrom scipy.ndimage.interpolation import map_coordinates\r\nfrom scipy.ndimage.filters import gaussian_filter\r\nimport math\r\nimport shutil\r\n\r\n\r\n# limit memory usage..\r\nimport tensorflow as tf\r\nfrom keras.backend.tensorflow_backend import set_session\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5\r\nset_session(tf.Session(config=config))\r\n\r\n# zonder aug, 10:1 99 train, 97 test, 0.27 cross entropy, before commit 573\r\n# 3 pools istead of 4 gives (bigger end layer) gives much worse validation accuray + logloss .. strange ?\r\n# 32 x 32 x 32 lijkt het beter te doen dan 48 x 48 x 48..\r\n\r\nK.set_image_dim_ordering(""tf"")\r\nCUBE_SIZE = 32\r\nMEAN_PIXEL_VALUE = settings.MEAN_PIXEL_VALUE_NODULE\r\nPOS_WEIGHT = 2\r\nNEGS_PER_POS = 20\r\nP_TH = 0.6\r\n# POS_IMG_DIR = ""luna16_train_cubes_pos""\r\nLEARN_RATE = 0.001\r\n\r\nUSE_DROPOUT = False\r\n\r\ndef prepare_image_for_net3D(img):\r\n    img = img.astype(numpy.float32)\r\n    img -= MEAN_PIXEL_VALUE\r\n    img /= 255.\r\n    img = img.reshape(1, img.shape[0], img.shape[1], img.shape[2], 1)\r\n    return img\r\n\r\n\r\ndef get_train_holdout_files(fold_count, train_percentage=80, logreg=True, ndsb3_holdout=0, manual_labels=True, full_luna_set=False):\r\n    print(""Get train/holdout files."")\r\n    # pos_samples = glob.glob(settings.BASE_DIR_SSD + ""luna16_train_cubes_pos/*.png"")\r\n    pos_samples = glob.glob(settings.BASE_DIR_SSD + ""generated_traindata/luna16_train_cubes_lidc/*.png"")\r\n    print(""Pos samples: "", len(pos_samples))\r\n\r\n    pos_samples_manual = glob.glob(settings.BASE_DIR_SSD + ""generated_traindata/luna16_train_cubes_manual/*_pos.png"")\r\n    print(""Pos samples manual: "", len(pos_samples_manual))\r\n    pos_samples += pos_samples_manual\r\n\r\n    random.shuffle(pos_samples)\r\n    train_pos_count = int((len(pos_samples) * train_percentage) / 100)\r\n    pos_samples_train = pos_samples[:train_pos_count]\r\n    pos_samples_holdout = pos_samples[train_pos_count:]\r\n    if full_luna_set:\r\n        pos_samples_train += pos_samples_holdout\r\n        if manual_labels:\r\n            pos_samples_holdout = []\r\n\r\n\r\n    ndsb3_list = glob.glob(settings.BASE_DIR_SSD + ""generated_traindata/ndsb3_train_cubes_manual/*.png"")\r\n    print(""Ndsb3 samples: "", len(ndsb3_list))\r\n\r\n    pos_samples_ndsb3_fold = []\r\n    pos_samples_ndsb3_holdout = []\r\n    ndsb3_pos = 0\r\n    ndsb3_neg = 0\r\n    ndsb3_pos_holdout = 0\r\n    ndsb3_neg_holdout = 0\r\n    if manual_labels:\r\n        for file_path in ndsb3_list:\r\n            file_name = ntpath.basename(file_path)\r\n\r\n            parts = file_name.split(""_"")\r\n            if int(parts[4]) == 0 and parts[3] != ""neg"":  # skip positive non-cancer-cases\r\n                continue\r\n\r\n            if fold_count == 3:\r\n                if parts[3] == ""neg"":  # skip negative cases\r\n                    continue\r\n\r\n\r\n            patient_id = parts[1]\r\n            patient_fold = helpers.get_patient_fold(patient_id) % fold_count\r\n            if patient_fold == ndsb3_holdout:\r\n                pos_samples_ndsb3_holdout.append(file_path)\r\n                if parts[3] == ""neg"":\r\n                    ndsb3_neg_holdout += 1\r\n                else:\r\n                    ndsb3_pos_holdout += 1\r\n            else:\r\n                pos_samples_ndsb3_fold.append(file_path)\r\n                print(""In fold: "", patient_id)\r\n                if parts[3] == ""neg"":\r\n                    ndsb3_neg += 1\r\n                else:\r\n                    ndsb3_pos += 1\r\n\r\n    print(ndsb3_pos, "" ndsb3 pos labels train"")\r\n    print(ndsb3_neg, "" ndsb3 neg labels train"")\r\n    print(ndsb3_pos_holdout, "" ndsb3 pos labels holdout"")\r\n    print(ndsb3_neg_holdout, "" ndsb3 neg labels holdout"")\r\n\r\n\r\n    if manual_labels:\r\n        for times_ndsb3 in range(4):  # make ndsb labels count 4 times just like in LIDC when 4 doctors annotated a nodule\r\n            pos_samples_train += pos_samples_ndsb3_fold\r\n            pos_samples_holdout += pos_samples_ndsb3_holdout\r\n\r\n    neg_samples_edge = glob.glob(settings.BASE_DIR_SSD + ""generated_traindata/luna16_train_cubes_auto/*_edge.png"")\r\n    print(""Edge samples: "", len(neg_samples_edge))\r\n\r\n    # neg_samples_white = glob.glob(settings.BASE_DIR_SSD + ""luna16_train_cubes_auto/*_white.png"")\r\n    neg_samples_luna = glob.glob(settings.BASE_DIR_SSD + ""generated_traindata/luna16_train_cubes_auto/*_luna.png"")\r\n    print(""Luna samples: "", len(neg_samples_luna))\r\n\r\n    # neg_samples = neg_samples_edge + neg_samples_white\r\n    neg_samples = neg_samples_edge + neg_samples_luna\r\n    random.shuffle(neg_samples)\r\n\r\n    train_neg_count = int((len(neg_samples) * train_percentage) / 100)\r\n\r\n    neg_samples_falsepos = []\r\n    for file_path in glob.glob(settings.BASE_DIR_SSD + ""generated_traindata/luna16_train_cubes_auto/*_falsepos.png""):\r\n        neg_samples_falsepos.append(file_path)\r\n    print(""Falsepos LUNA count: "", len(neg_samples_falsepos))\r\n\r\n    neg_samples_train = neg_samples[:train_neg_count]\r\n    neg_samples_train += neg_samples_falsepos + neg_samples_falsepos + neg_samples_falsepos\r\n    neg_samples_holdout = neg_samples[train_neg_count:]\r\n    if full_luna_set:\r\n        neg_samples_train += neg_samples_holdout\r\n\r\n    train_res = []\r\n    holdout_res = []\r\n    sets = [(train_res, pos_samples_train, neg_samples_train), (holdout_res, pos_samples_holdout, neg_samples_holdout)]\r\n    for set_item in sets:\r\n        pos_idx = 0\r\n        negs_per_pos = NEGS_PER_POS\r\n        res = set_item[0]\r\n        neg_samples = set_item[2]\r\n        pos_samples = set_item[1]\r\n        print(""Pos"", len(pos_samples))\r\n        ndsb3_pos = 0\r\n        ndsb3_neg = 0\r\n        for index, neg_sample_path in enumerate(neg_samples):\r\n            # res.append(sample_path + ""/"")\r\n            res.append((neg_sample_path, 0, 0))\r\n            if index % negs_per_pos == 0:\r\n                pos_sample_path = pos_samples[pos_idx]\r\n                file_name = ntpath.basename(pos_sample_path)\r\n                parts = file_name.split(""_"")\r\n                if parts[0].startswith(""ndsb3manual""):\r\n                    if parts[3] == ""pos"":\r\n                        class_label = 1  # only take positive examples where we know there was a cancer..\r\n                        cancer_label = int(parts[4])\r\n                        assert cancer_label == 1\r\n                        size_label = int(parts[5])\r\n                        # print(parts[1], size_label)\r\n                        assert class_label == 1\r\n                        if size_label < 1:\r\n                            print(""huh ?"")\r\n                        assert size_label >= 1\r\n                        ndsb3_pos += 1\r\n                    else:\r\n                        class_label = 0\r\n                        size_label = 0\r\n                        ndsb3_neg += 1\r\n                else:\r\n                    class_label = int(parts[-2])\r\n                    size_label = int(parts[-3])\r\n                    assert class_label == 1\r\n                    assert parts[-1] == ""pos.png""\r\n                    assert size_label >= 1\r\n\r\n                res.append((pos_sample_path, class_label, size_label))\r\n                pos_idx += 1\r\n                pos_idx %= len(pos_samples)\r\n\r\n        print(""ndsb2 pos: "", ndsb3_pos)\r\n        print(""ndsb2 neg: "", ndsb3_neg)\r\n\r\n    print(""Train count: "", len(train_res), "", holdout count: "", len(holdout_res))\r\n    return train_res, holdout_res\r\n\r\n\r\ndef data_generator(batch_size, record_list, train_set):\r\n    batch_idx = 0\r\n    means = []\r\n    random_state = numpy.random.RandomState(1301)\r\n    while True:\r\n        img_list = []\r\n        class_list = []\r\n        size_list = []\r\n        if train_set:\r\n            random.shuffle(record_list)\r\n        CROP_SIZE = CUBE_SIZE\r\n        # CROP_SIZE = 48\r\n        for record_idx, record_item in enumerate(record_list):\r\n            #rint patient_dir\r\n            class_label = record_item[1]\r\n            size_label = record_item[2]\r\n            if class_label == 0:\r\n                cube_image = helpers.load_cube_img(record_item[0], 6, 8, 48)\r\n                # if train_set:\r\n                #     # helpers.save_cube_img(""c:/tmp/pre.png"", cube_image, 8, 8)\r\n                #     cube_image = random_rotate_cube_img(cube_image, 0.99, -180, 180)\r\n                #\r\n                # if train_set:\r\n                #     if random.randint(0, 100) > 0.1:\r\n                #         # cube_image = numpy.flipud(cube_image)\r\n                #         cube_image = elastic_transform48(cube_image, 64, 8, random_state)\r\n                wiggle = 48 - CROP_SIZE - 1\r\n                indent_x = 0\r\n                indent_y = 0\r\n                indent_z = 0\r\n                if wiggle > 0:\r\n                    indent_x = random.randint(0, wiggle)\r\n                    indent_y = random.randint(0, wiggle)\r\n                    indent_z = random.randint(0, wiggle)\r\n                cube_image = cube_image[indent_z:indent_z + CROP_SIZE, indent_y:indent_y + CROP_SIZE, indent_x:indent_x + CROP_SIZE]\r\n\r\n                if train_set:\r\n                    if random.randint(0, 100) > 50:\r\n                        cube_image = numpy.fliplr(cube_image)\r\n                    if random.randint(0, 100) > 50:\r\n                        cube_image = numpy.flipud(cube_image)\r\n                    if random.randint(0, 100) > 50:\r\n                        cube_image = cube_image[:, :, ::-1]\r\n                    if random.randint(0, 100) > 50:\r\n                        cube_image = cube_image[:, ::-1, :]\r\n\r\n                if CROP_SIZE != CUBE_SIZE:\r\n                    cube_image = helpers.rescale_patient_images2(cube_image, (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE))\r\n                assert cube_image.shape == (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE)\r\n            else:\r\n                cube_image = helpers.load_cube_img(record_item[0], 8, 8, 64)\r\n\r\n                if train_set:\r\n                    pass\r\n\r\n                current_cube_size = cube_image.shape[0]\r\n                indent_x = (current_cube_size - CROP_SIZE) / 2\r\n                indent_y = (current_cube_size - CROP_SIZE) / 2\r\n                indent_z = (current_cube_size - CROP_SIZE) / 2\r\n                wiggle_indent = 0\r\n                wiggle = current_cube_size - CROP_SIZE - 1\r\n                if wiggle > (CROP_SIZE / 2):\r\n                    wiggle_indent = CROP_SIZE / 4\r\n                    wiggle = current_cube_size - CROP_SIZE - CROP_SIZE / 2 - 1\r\n                if train_set:\r\n                    indent_x = wiggle_indent + random.randint(0, wiggle)\r\n                    indent_y = wiggle_indent + random.randint(0, wiggle)\r\n                    indent_z = wiggle_indent + random.randint(0, wiggle)\r\n\r\n                indent_x = int(indent_x)\r\n                indent_y = int(indent_y)\r\n                indent_z = int(indent_z)\r\n                cube_image = cube_image[indent_z:indent_z + CROP_SIZE, indent_y:indent_y + CROP_SIZE, indent_x:indent_x + CROP_SIZE]\r\n                if CROP_SIZE != CUBE_SIZE:\r\n                    cube_image = helpers.rescale_patient_images2(cube_image, (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE))\r\n                assert cube_image.shape == (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE)\r\n\r\n                if train_set:\r\n                    if random.randint(0, 100) > 50:\r\n                        cube_image = numpy.fliplr(cube_image)\r\n                    if random.randint(0, 100) > 50:\r\n                        cube_image = numpy.flipud(cube_image)\r\n                    if random.randint(0, 100) > 50:\r\n                        cube_image = cube_image[:, :, ::-1]\r\n                    if random.randint(0, 100) > 50:\r\n                        cube_image = cube_image[:, ::-1, :]\r\n\r\n\r\n            means.append(cube_image.mean())\r\n            img3d = prepare_image_for_net3D(cube_image)\r\n            if train_set:\r\n                if len(means) % 1000000 == 0:\r\n                    print(""Mean: "", sum(means) / len(means))\r\n            img_list.append(img3d)\r\n            class_list.append(class_label)\r\n            size_list.append(size_label)\r\n\r\n            batch_idx += 1\r\n            if batch_idx >= batch_size:\r\n                x = numpy.vstack(img_list)\r\n                y_class = numpy.vstack(class_list)\r\n                y_size = numpy.vstack(size_list)\r\n                yield x, {""out_class"": y_class, ""out_malignancy"": y_size}\r\n                img_list = []\r\n                class_list = []\r\n                size_list = []\r\n                batch_idx = 0\r\n\r\n\r\ndef get_net(input_shape=(CUBE_SIZE, CUBE_SIZE, CUBE_SIZE, 1), load_weight_path=None, features=False, mal=False) -> Model:\r\n    inputs = Input(shape=input_shape, name=""input_1"")\r\n    x = inputs\r\n    x = AveragePooling3D(pool_size=(2, 1, 1), strides=(2, 1, 1), border_mode=""same"")(x)\r\n    x = Convolution3D(64, 3, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'conv1\', subsample=(1, 1, 1))(x)\r\n    x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), border_mode=\'valid\', name=\'pool1\')(x)\r\n\r\n    # 2nd layer group\r\n    x = Convolution3D(128, 3, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'conv2\', subsample=(1, 1, 1))(x)\r\n    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode=\'valid\', name=\'pool2\')(x)\r\n    if USE_DROPOUT:\r\n        x = Dropout(p=0.3)(x)\r\n\r\n    # 3rd layer group\r\n    x = Convolution3D(256, 3, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'conv3a\', subsample=(1, 1, 1))(x)\r\n    x = Convolution3D(256, 3, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'conv3b\', subsample=(1, 1, 1))(x)\r\n    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode=\'valid\', name=\'pool3\')(x)\r\n    if USE_DROPOUT:\r\n        x = Dropout(p=0.4)(x)\r\n\r\n    # 4th layer group\r\n    x = Convolution3D(512, 3, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'conv4a\', subsample=(1, 1, 1))(x)\r\n    x = Convolution3D(512, 3, 3, 3, activation=\'relu\', border_mode=\'same\', name=\'conv4b\', subsample=(1, 1, 1),)(x)\r\n    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode=\'valid\', name=\'pool4\')(x)\r\n    if USE_DROPOUT:\r\n        x = Dropout(p=0.5)(x)\r\n\r\n    last64 = Convolution3D(64, 2, 2, 2, activation=""relu"", name=""last_64"")(x)\r\n    out_class = Convolution3D(1, 1, 1, 1, activation=""sigmoid"", name=""out_class_last"")(last64)\r\n    out_class = Flatten(name=""out_class"")(out_class)\r\n\r\n    out_malignancy = Convolution3D(1, 1, 1, 1, activation=None, name=""out_malignancy_last"")(last64)\r\n    out_malignancy = Flatten(name=""out_malignancy"")(out_malignancy)\r\n\r\n    model = Model(input=inputs, output=[out_class, out_malignancy])\r\n    if load_weight_path is not None:\r\n        model.load_weights(load_weight_path, by_name=False)\r\n    model.compile(optimizer=SGD(lr=LEARN_RATE, momentum=0.9, nesterov=True), loss={""out_class"": ""binary_crossentropy"", ""out_malignancy"": mean_absolute_error}, metrics={""out_class"": [binary_accuracy, binary_crossentropy], ""out_malignancy"": mean_absolute_error})\r\n\r\n    if features:\r\n        model = Model(input=inputs, output=[last64])\r\n    model.summary(line_length=140)\r\n\r\n    return model\r\n\r\n\r\ndef step_decay(epoch):\r\n    res = 0.001\r\n    if epoch > 5:\r\n        res = 0.0001\r\n    print(""learnrate: "", res, "" epoch: "", epoch)\r\n    return res\r\n\r\n\r\ndef train(model_name, fold_count, train_full_set=False, load_weights_path=None, ndsb3_holdout=0, manual_labels=True):\r\n    batch_size = 16\r\n    train_files, holdout_files = get_train_holdout_files(train_percentage=80, ndsb3_holdout=ndsb3_holdout, manual_labels=manual_labels, full_luna_set=train_full_set, fold_count=fold_count)\r\n\r\n    # train_files = train_files[:100]\r\n    # holdout_files = train_files[:10]\r\n    train_gen = data_generator(batch_size, train_files, True)\r\n    holdout_gen = data_generator(batch_size, holdout_files, False)\r\n    for i in range(0, 10):\r\n        tmp = next(holdout_gen)\r\n        cube_img = tmp[0][0].reshape(CUBE_SIZE, CUBE_SIZE, CUBE_SIZE, 1)\r\n        cube_img = cube_img[:, :, :, 0]\r\n        cube_img *= 255.\r\n        cube_img += MEAN_PIXEL_VALUE\r\n        # helpers.save_cube_img(""c:/tmp/img_"" + str(i) + "".png"", cube_img, 4, 8)\r\n        # print(tmp)\r\n\r\n    learnrate_scheduler = LearningRateScheduler(step_decay)\r\n    model = get_net(load_weight_path=load_weights_path)\r\n    holdout_txt = ""_h"" + str(ndsb3_holdout) if manual_labels else """"\r\n    if train_full_set:\r\n        holdout_txt = ""_fs"" + holdout_txt\r\n    checkpoint = ModelCheckpoint(""workdir/model_"" + model_name + ""_"" + holdout_txt + ""_e"" + ""{epoch:02d}-{val_loss:.4f}.hd5"", monitor=\'val_loss\', verbose=1, save_best_only=not train_full_set, save_weights_only=False, mode=\'auto\', period=1)\r\n    checkpoint_fixed_name = ModelCheckpoint(""workdir/model_"" + model_name + ""_"" + holdout_txt + ""_best.hd5"", monitor=\'val_loss\', verbose=1, save_best_only=True, save_weights_only=False, mode=\'auto\', period=1)\r\n    model.fit_generator(train_gen, len(train_files) / 1, 12, validation_data=holdout_gen, nb_val_samples=len(holdout_files) / 1, callbacks=[checkpoint, checkpoint_fixed_name, learnrate_scheduler])\r\n    model.save(""workdir/model_"" + model_name + ""_"" + holdout_txt + ""_end.hd5"")\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    if True:\r\n        # model 1 on luna16 annotations. full set 1 versions for blending\r\n        train(train_full_set=True, load_weights_path=None, model_name=""luna16_full"", fold_count=-1, manual_labels=False)\r\n        if not os.path.exists(""models/""):\r\n            os.mkdir(""models"")\r\n        shutil.copy(""workdir/model_luna16_full__fs_best.hd5"", ""models/model_luna16_full__fs_best.hd5"")\r\n\r\n    # model 2 on luna16 annotations + ndsb pos annotations. 3 folds (1st half, 2nd half of ndsb patients) 2 versions for blending\r\n    if True:\r\n        train(train_full_set=True, load_weights_path=None, ndsb3_holdout=0, manual_labels=True, model_name=""luna_posnegndsb_v1"", fold_count=2)\r\n        train(train_full_set=True, load_weights_path=None, ndsb3_holdout=1, manual_labels=True, model_name=""luna_posnegndsb_v1"", fold_count=2)\r\n        shutil.copy(""workdir/model_luna_posnegndsb_v1__fs_h0_end.hd5"", ""models/model_luna_posnegndsb_v1__fs_h0_end.hd5"")\r\n        shutil.copy(""workdir/model_luna_posnegndsb_v1__fs_h1_end.hd5"", ""models/model_luna_posnegndsb_v1__fs_h1_end.hd5"")\r\n\r\n    if True:\r\n        train(train_full_set=True, load_weights_path=None, ndsb3_holdout=0, manual_labels=True, model_name=""luna_posnegndsb_v2"", fold_count=2)\r\n        train(train_full_set=True, load_weights_path=None, ndsb3_holdout=1, manual_labels=True, model_name=""luna_posnegndsb_v2"", fold_count=2)\r\n        shutil.copy(""workdir/model_luna_posnegndsb_v2__fs_h0_end.hd5"", ""models/model_luna_posnegndsb_v2__fs_h0_end.hd5"")\r\n        shutil.copy(""workdir/model_luna_posnegndsb_v2__fs_h1_end.hd5"", ""models/model_luna_posnegndsb_v2__fs_h1_end.hd5"")\r\n\r\n'"
step3_predict_nodules.py,2,"b'import settings\r\nimport helpers\r\nimport sys\r\nimport os\r\nimport glob\r\nimport random\r\nimport pandas\r\nimport ntpath\r\nimport cv2\r\nimport numpy\r\nfrom typing import List, Tuple\r\nfrom keras.optimizers import Adam, SGD\r\nfrom keras.layers import Input, Convolution2D, MaxPooling2D, UpSampling2D, merge, Convolution3D, MaxPooling3D, UpSampling3D, LeakyReLU, BatchNormalization, Flatten, Dense, Dropout, ZeroPadding3D, AveragePooling3D, Activation\r\nfrom keras.models import Model, load_model, model_from_json\r\nfrom keras.metrics import binary_accuracy, binary_crossentropy, mean_squared_error, mean_absolute_error\r\nfrom keras import backend as K\r\nfrom keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler\r\nfrom scipy.ndimage.interpolation import map_coordinates\r\nfrom scipy.ndimage.filters import gaussian_filter\r\nimport math\r\n\r\n\r\n# limit memory usage..\r\nimport tensorflow as tf\r\nfrom keras.backend.tensorflow_backend import set_session\r\nimport step2_train_nodule_detector\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5\r\nset_session(tf.Session(config=config))\r\n\r\n# zonder aug, 10:1 99 train, 97 test, 0.27 cross entropy, before commit 573\r\n# 3 pools istead of 4 gives (bigger end layer) gives much worse validation accuray + logloss .. strange ?\r\n# 32 x 32 x 32 lijkt het beter te doen dan 48 x 48 x 48..\r\n\r\nK.set_image_dim_ordering(""tf"")\r\nCUBE_SIZE = step2_train_nodule_detector.CUBE_SIZE\r\nMEAN_PIXEL_VALUE = settings.MEAN_PIXEL_VALUE_NODULE\r\nNEGS_PER_POS = 20\r\nP_TH = 0.6\r\n\r\nPREDICT_STEP = 12\r\nUSE_DROPOUT = False\r\n\r\n\r\ndef prepare_image_for_net3D(img):\r\n    img = img.astype(numpy.float32)\r\n    img -= MEAN_PIXEL_VALUE\r\n    img /= 255.\r\n    img = img.reshape(1, img.shape[0], img.shape[1], img.shape[2], 1)\r\n    return img\r\n\r\n\r\ndef filter_patient_nodules_predictions(df_nodule_predictions: pandas.DataFrame, patient_id, view_size, luna16=False):\r\n    src_dir = settings.LUNA_16_TRAIN_DIR2D2 if luna16 else settings.NDSB3_EXTRACTED_IMAGE_DIR\r\n    patient_mask = helpers.load_patient_images(patient_id, src_dir, ""*_m.png"")\r\n    delete_indices = []\r\n    for index, row in df_nodule_predictions.iterrows():\r\n        z_perc = row[""coord_z""]\r\n        y_perc = row[""coord_y""]\r\n        center_x = int(round(row[""coord_x""] * patient_mask.shape[2]))\r\n        center_y = int(round(y_perc * patient_mask.shape[1]))\r\n        center_z = int(round(z_perc * patient_mask.shape[0]))\r\n\r\n        mal_score = row[""diameter_mm""]\r\n        start_y = center_y - view_size / 2\r\n        start_x = center_x - view_size / 2\r\n        nodule_in_mask = False\r\n        for z_index in [-1, 0, 1]:\r\n            img = patient_mask[z_index + center_z]\r\n            start_x = int(start_x)\r\n            start_y = int(start_y)\r\n            view_size = int(view_size)\r\n            img_roi = img[start_y:start_y+view_size, start_x:start_x + view_size]\r\n            if img_roi.sum() > 255:  # more than 1 pixel of mask.\r\n                nodule_in_mask = True\r\n\r\n        if not nodule_in_mask:\r\n            print(""Nodule not in mask: "", (center_x, center_y, center_z))\r\n            if mal_score > 0:\r\n                mal_score *= -1\r\n            df_nodule_predictions.loc[index, ""diameter_mm""] = mal_score\r\n        else:\r\n            if center_z < 30:\r\n                print(""Z < 30: "", patient_id, "" center z:"", center_z, "" y_perc: "",  y_perc)\r\n                if mal_score > 0:\r\n                    mal_score *= -1\r\n                df_nodule_predictions.loc[index, ""diameter_mm""] = mal_score\r\n\r\n\r\n            if (z_perc > 0.75 or z_perc < 0.25) and y_perc > 0.85:\r\n                print(""SUSPICIOUS FALSEPOSITIVE: "", patient_id, "" center z:"", center_z, "" y_perc: "",  y_perc)\r\n\r\n            if center_z < 50 and y_perc < 0.30:\r\n                print(""SUSPICIOUS FALSEPOSITIVE OUT OF RANGE: "", patient_id, "" center z:"", center_z, "" y_perc: "",  y_perc)\r\n\r\n    df_nodule_predictions.drop(df_nodule_predictions.index[delete_indices], inplace=True)\r\n    return df_nodule_predictions\r\n\r\n\r\ndef filter_nodule_predictions(only_patient_id=None):\r\n    src_dir = settings.NDSB3_NODULE_DETECTION_DIR\r\n    for csv_index, csv_path in enumerate(glob.glob(src_dir + ""*.csv"")):\r\n        file_name = ntpath.basename(csv_path)\r\n        patient_id = file_name.replace("".csv"", """")\r\n        print(csv_index, "": "", patient_id)\r\n        if only_patient_id is not None and patient_id != only_patient_id:\r\n            continue\r\n        df_nodule_predictions = pandas.read_csv(csv_path)\r\n        filter_patient_nodules_predictions(df_nodule_predictions, patient_id, CUBE_SIZE)\r\n        df_nodule_predictions.to_csv(csv_path, index=False)\r\n\r\n\r\ndef make_negative_train_data_based_on_predicted_luna_nodules():\r\n    src_dir = settings.LUNA_NODULE_DETECTION_DIR\r\n    pos_labels_dir = settings.LUNA_NODULE_LABELS_DIR\r\n    keep_dist = CUBE_SIZE + CUBE_SIZE / 2\r\n    total_false_pos = 0\r\n    for csv_index, csv_path in enumerate(glob.glob(src_dir + ""*.csv"")):\r\n        file_name = ntpath.basename(csv_path)\r\n        patient_id = file_name.replace("".csv"", """")\r\n        # if not ""273525289046256012743471155680"" in patient_id:\r\n        #     continue\r\n        df_nodule_predictions = pandas.read_csv(csv_path)\r\n        pos_annos_manual = None\r\n        manual_path = settings.MANUAL_ANNOTATIONS_LABELS_DIR + patient_id + "".csv""\r\n        if os.path.exists(manual_path):\r\n            pos_annos_manual = pandas.read_csv(manual_path)\r\n\r\n        filter_patient_nodules_predictions(df_nodule_predictions, patient_id, CUBE_SIZE, luna16=True)\r\n        pos_labels = pandas.read_csv(pos_labels_dir + patient_id + ""_annos_pos_lidc.csv"")\r\n        print(csv_index, "": "", patient_id, "", pos"", len(pos_labels))\r\n        patient_imgs = helpers.load_patient_images(patient_id, settings.LUNA_16_TRAIN_DIR2D2, ""*_m.png"")\r\n        for nod_pred_index, nod_pred_row in df_nodule_predictions.iterrows():\r\n            if nod_pred_row[""diameter_mm""] < 0:\r\n                continue\r\n            nx, ny, nz = helpers.percentage_to_pixels(nod_pred_row[""coord_x""], nod_pred_row[""coord_y""], nod_pred_row[""coord_z""], patient_imgs)\r\n            diam_mm = nod_pred_row[""diameter_mm""]\r\n            for label_index, label_row in pos_labels.iterrows():\r\n                px, py, pz = helpers.percentage_to_pixels(label_row[""coord_x""], label_row[""coord_y""], label_row[""coord_z""], patient_imgs)\r\n                dist = math.sqrt(math.pow(nx - px, 2) + math.pow(ny - py, 2) + math.pow(nz- pz, 2))\r\n                if dist < keep_dist:\r\n                    if diam_mm >= 0:\r\n                        diam_mm *= -1\r\n                    df_nodule_predictions.loc[nod_pred_index, ""diameter_mm""] = diam_mm\r\n                    break\r\n\r\n            if pos_annos_manual is not None:\r\n                for index, label_row in pos_annos_manual.iterrows():\r\n                    px, py, pz = helpers.percentage_to_pixels(label_row[""x""], label_row[""y""], label_row[""z""], patient_imgs)\r\n                    diameter = label_row[""d""] * patient_imgs[0].shape[1]\r\n                    # print((pos_coord_x, pos_coord_y, pos_coord_z))\r\n                    # print(center_float_rescaled)\r\n                    dist = math.sqrt(math.pow(px - nx, 2) + math.pow(py - ny, 2) + math.pow(pz - nz, 2))\r\n                    if dist < (diameter + 72):  #  make sure we have a big margin\r\n                        if diam_mm >= 0:\r\n                            diam_mm *= -1\r\n                        df_nodule_predictions.loc[nod_pred_index, ""diameter_mm""] = diam_mm\r\n                        print(""#Too close"",  (nx, ny, nz))\r\n                        break\r\n\r\n        df_nodule_predictions.to_csv(csv_path, index=False)\r\n        df_nodule_predictions = df_nodule_predictions[df_nodule_predictions[""diameter_mm""] >= 0]\r\n        df_nodule_predictions.to_csv(pos_labels_dir + patient_id + ""_candidates_falsepos.csv"", index=False)\r\n        total_false_pos += len(df_nodule_predictions)\r\n    print(""Total false pos:"", total_false_pos)\r\n\r\n\r\ndef predict_cubes(model_path, continue_job, only_patient_id=None, luna16=False, magnification=1, flip=False, train_data=True, holdout_no=-1, ext_name="""", fold_count=2):\r\n    if luna16:\r\n        dst_dir = settings.LUNA_NODULE_DETECTION_DIR\r\n    else:\r\n        dst_dir = settings.NDSB3_NODULE_DETECTION_DIR\r\n    if not os.path.exists(dst_dir):\r\n        os.makedirs(dst_dir)\r\n\r\n    holdout_ext = """"\r\n    # if holdout_no is not None:\r\n    #     holdout_ext = ""_h"" + str(holdout_no) if holdout_no >= 0 else """"\r\n    flip_ext = """"\r\n    if flip:\r\n        flip_ext = ""_flip""\r\n\r\n    dst_dir += ""predictions"" + str(int(magnification * 10)) + holdout_ext + flip_ext + ""_"" + ext_name + ""/""\r\n    if not os.path.exists(dst_dir):\r\n        os.makedirs(dst_dir)\r\n\r\n    sw = helpers.Stopwatch.start_new()\r\n    model = step2_train_nodule_detector.get_net(input_shape=(CUBE_SIZE, CUBE_SIZE, CUBE_SIZE, 1), load_weight_path=model_path)\r\n    if not luna16:\r\n        if train_data:\r\n            labels_df = pandas.read_csv(""resources/stage1_labels.csv"")\r\n            labels_df.set_index([""id""], inplace=True)\r\n        else:\r\n            labels_df = pandas.read_csv(""resources/stage2_sample_submission.csv"")\r\n            labels_df.set_index([""id""], inplace=True)\r\n\r\n    patient_ids = []\r\n    for file_name in os.listdir(settings.NDSB3_EXTRACTED_IMAGE_DIR):\r\n        if not os.path.isdir(settings.NDSB3_EXTRACTED_IMAGE_DIR + file_name):\r\n            continue\r\n        patient_ids.append(file_name)\r\n\r\n    all_predictions_csv = []\r\n    for patient_index, patient_id in enumerate(reversed(patient_ids)):\r\n        if not luna16:\r\n            if patient_id not in labels_df.index:\r\n                continue\r\n        if ""metadata"" in patient_id:\r\n            continue\r\n        if only_patient_id is not None and only_patient_id != patient_id:\r\n            continue\r\n\r\n        if holdout_no is not None and train_data:\r\n            patient_fold = helpers.get_patient_fold(patient_id)\r\n            patient_fold %= fold_count\r\n            if patient_fold != holdout_no:\r\n                continue\r\n\r\n        print(patient_index, "": "", patient_id)\r\n        csv_target_path = dst_dir + patient_id + "".csv""\r\n        if continue_job and only_patient_id is None:\r\n            if os.path.exists(csv_target_path):\r\n                continue\r\n\r\n        patient_img = helpers.load_patient_images(patient_id, settings.NDSB3_EXTRACTED_IMAGE_DIR, ""*_i.png"", [])\r\n        if magnification != 1:\r\n            patient_img = helpers.rescale_patient_images(patient_img, (1, 1, 1), magnification)\r\n\r\n        patient_mask = helpers.load_patient_images(patient_id, settings.NDSB3_EXTRACTED_IMAGE_DIR, ""*_m.png"", [])\r\n        if magnification != 1:\r\n            patient_mask = helpers.rescale_patient_images(patient_mask, (1, 1, 1), magnification, is_mask_image=True)\r\n\r\n            # patient_img = patient_img[:, ::-1, :]\r\n            # patient_mask = patient_mask[:, ::-1, :]\r\n\r\n        step = PREDICT_STEP\r\n        CROP_SIZE = CUBE_SIZE\r\n        # CROP_SIZE = 48\r\n\r\n        predict_volume_shape_list = [0, 0, 0]\r\n        for dim in range(3):\r\n            dim_indent = 0\r\n            while dim_indent + CROP_SIZE < patient_img.shape[dim]:\r\n                predict_volume_shape_list[dim] += 1\r\n                dim_indent += step\r\n\r\n        predict_volume_shape = (predict_volume_shape_list[0], predict_volume_shape_list[1], predict_volume_shape_list[2])\r\n        predict_volume = numpy.zeros(shape=predict_volume_shape, dtype=float)\r\n        print(""Predict volume shape: "", predict_volume.shape)\r\n        done_count = 0\r\n        skipped_count = 0\r\n        batch_size = 128\r\n        batch_list = []\r\n        batch_list_coords = []\r\n        patient_predictions_csv = []\r\n        cube_img = None\r\n        annotation_index = 0\r\n\r\n        for z in range(0, predict_volume_shape[0]):\r\n            for y in range(0, predict_volume_shape[1]):\r\n                for x in range(0, predict_volume_shape[2]):\r\n                    #if cube_img is None:\r\n                    cube_img = patient_img[z * step:z * step+CROP_SIZE, y * step:y * step + CROP_SIZE, x * step:x * step+CROP_SIZE]\r\n                    cube_mask = patient_mask[z * step:z * step+CROP_SIZE, y * step:y * step + CROP_SIZE, x * step:x * step+CROP_SIZE]\r\n\r\n                    if cube_mask.sum() < 2000:\r\n                        skipped_count += 1\r\n                    else:\r\n                        if flip:\r\n                            cube_img = cube_img[:, :, ::-1]\r\n\r\n                        if CROP_SIZE != CUBE_SIZE:\r\n                            cube_img = helpers.rescale_patient_images2(cube_img, (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE))\r\n                            # helpers.save_cube_img(""c:/tmp/cube.png"", cube_img, 8, 4)\r\n                            # cube_mask = helpers.rescale_patient_images2(cube_mask, (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE))\r\n\r\n                        img_prep = prepare_image_for_net3D(cube_img)\r\n                        batch_list.append(img_prep)\r\n                        batch_list_coords.append((z, y, x))\r\n                        if len(batch_list) % batch_size == 0:\r\n                            batch_data = numpy.vstack(batch_list)\r\n                            p = model.predict(batch_data, batch_size=batch_size)\r\n                            for i in range(len(p[0])):\r\n                                p_z = batch_list_coords[i][0]\r\n                                p_y = batch_list_coords[i][1]\r\n                                p_x = batch_list_coords[i][2]\r\n                                nodule_chance = p[0][i][0]\r\n                                predict_volume[p_z, p_y, p_x] = nodule_chance\r\n                                if nodule_chance > P_TH:\r\n                                    p_z = p_z * step + CROP_SIZE / 2\r\n                                    p_y = p_y * step + CROP_SIZE / 2\r\n                                    p_x = p_x * step + CROP_SIZE / 2\r\n\r\n                                    p_z_perc = round(p_z / patient_img.shape[0], 4)\r\n                                    p_y_perc = round(p_y / patient_img.shape[1], 4)\r\n                                    p_x_perc = round(p_x / patient_img.shape[2], 4)\r\n                                    diameter_mm = round(p[1][i][0], 4)\r\n                                    # diameter_perc = round(2 * step / patient_img.shape[2], 4)\r\n                                    diameter_perc = round(2 * step / patient_img.shape[2], 4)\r\n                                    diameter_perc = round(diameter_mm / patient_img.shape[2], 4)\r\n                                    nodule_chance = round(nodule_chance, 4)\r\n                                    patient_predictions_csv_line = [annotation_index, p_x_perc, p_y_perc, p_z_perc, diameter_perc, nodule_chance, diameter_mm]\r\n                                    patient_predictions_csv.append(patient_predictions_csv_line)\r\n                                    all_predictions_csv.append([patient_id] + patient_predictions_csv_line)\r\n                                    annotation_index += 1\r\n\r\n                            batch_list = []\r\n                            batch_list_coords = []\r\n                    done_count += 1\r\n                    if done_count % 10000 == 0:\r\n                        print(""Done: "", done_count, "" skipped:"", skipped_count)\r\n\r\n        df = pandas.DataFrame(patient_predictions_csv, columns=[""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""nodule_chance"", ""diameter_mm""])\r\n        filter_patient_nodules_predictions(df, patient_id, CROP_SIZE * magnification)\r\n        df.to_csv(csv_target_path, index=False)\r\n\r\n        # cols = [""anno_index"", ""nodule_chance"", ""diamete_mm""] + [""f"" + str(i) for i in range(64)]\r\n        # df_features = pandas.DataFrame(patient_features_csv, columns=cols)\r\n        # for index, row in df.iterrows():\r\n        #     if row[""diameter_mm""] < 0:\r\n        #         print(""Dropping"")\r\n        #         anno_index = row[""anno_index""]\r\n        #         df_features.drop(df_features[df_features[""anno_index""] == anno_index].index, inplace=True)\r\n        #\r\n        # df_features.to_csv(csv_target_path_features, index=False)\r\n\r\n        # df = pandas.DataFrame(all_predictions_csv, columns=[""patient_id"", ""anno_index"", ""coord_x"", ""coord_y"", ""coord_z"", ""diameter"", ""nodule_chance"", ""diameter_mm""])\r\n        # df.to_csv(""c:/tmp/tmp2.csv"", index=False)\r\n\r\n        print(predict_volume.mean())\r\n        print(""Done in : "", sw.get_elapsed_seconds(), "" seconds"")\r\n\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    CONTINUE_JOB = True\r\n    only_patient_id = None  # ""ebd601d40a18634b100c92e7db39f585""\r\n\r\n    if not CONTINUE_JOB or only_patient_id is not None:\r\n        for file_path in glob.glob(""c:/tmp/*.*""):\r\n            if not os.path.isdir(file_path):\r\n                remove_file = True\r\n                if only_patient_id is not None:\r\n                    if only_patient_id not in file_path:\r\n                        remove_file = False\r\n                        remove_file = False\r\n\r\n                if remove_file:\r\n                    os.remove(file_path)\r\n\r\n    if True:\r\n        for magnification in [1, 1.5, 2]:  #\r\n            predict_cubes(""models/model_luna16_full__fs_best.hd5"", CONTINUE_JOB, only_patient_id=only_patient_id, magnification=magnification, flip=False, train_data=True, holdout_no=None, ext_name=""luna16_fs"")\r\n            predict_cubes(""models/model_luna16_full__fs_best.hd5"", CONTINUE_JOB, only_patient_id=only_patient_id, magnification=magnification, flip=False, train_data=False, holdout_no=None, ext_name=""luna16_fs"")\r\n\r\n    if True:\r\n        for version in [2, 1]:\r\n            for holdout in [0, 1]:\r\n                for magnification in [1, 1.5, 2]:  #\r\n                    predict_cubes(""models/model_luna_posnegndsb_v"" + str(version) + ""__fs_h"" + str(holdout) + ""_end.hd5"", CONTINUE_JOB, only_patient_id=only_patient_id, magnification=magnification, flip=False, train_data=True, holdout_no=holdout, ext_name=""luna_posnegndsb_v"" + str(version), fold_count=2)\r\n                    if holdout == 0:\r\n                        predict_cubes(""models/model_luna_posnegndsb_v"" + str(version) + ""__fs_h"" + str(holdout) + ""_end.hd5"", CONTINUE_JOB, only_patient_id=only_patient_id, magnification=magnification, flip=False, train_data=False, holdout_no=holdout, ext_name=""luna_posnegndsb_v"" + str(version), fold_count=2)\r\n\r\n'"
step4_train_submissions.py,0,"b'import settings\r\nimport helpers\r\nimport sys\r\nimport os\r\nfrom collections import defaultdict\r\nimport glob\r\nimport random\r\nimport pandas\r\nimport ntpath\r\nimport numpy\r\nfrom sklearn import cross_validation\r\nimport xgboost\r\nfrom sklearn.metrics import log_loss\r\n\r\n\r\ndef combine_nodule_predictions(dirs, train_set=True, nodule_th=0.5, extensions=[""""]):\r\n    print(""Combining nodule predictions: "", ""Train"" if train_set else ""Submission"")\r\n    if train_set:\r\n        labels_df = pandas.read_csv(""resources/stage1_labels.csv"")\r\n    else:\r\n        labels_df = pandas.read_csv(""resources/stage2_sample_submission.csv"")\r\n\r\n    mass_df = pandas.read_csv(settings.BASE_DIR + ""masses_predictions.csv"")\r\n    mass_df.set_index([""patient_id""], inplace=True)\r\n\r\n    # meta_df = pandas.read_csv(settings.BASE_DIR + ""patient_metadata.csv"")\r\n    # meta_df.set_index([""patient_id""], inplace=True)\r\n\r\n    data_rows = []\r\n    for index, row in labels_df.iterrows():\r\n        patient_id = row[""id""]\r\n        # mask = helpers.load_patient_images(patient_id, settings.EXTRACTED_IMAGE_DIR, ""*_m.png"")\r\n        print(len(data_rows), "" : "", patient_id)\r\n        # if len(data_rows) > 19:\r\n        #     break\r\n        cancer_label = row[""cancer""]\r\n        mass_pred = int(mass_df.loc[patient_id][""prediction""])\r\n        # meta_row = meta_df.loc[patient_id]\r\n        # z_scale = meta_row[""slice_thickness""]\r\n        # x_scale = meta_row[""spacingx""]\r\n        # vendor_low = 1 if ""1.2.276.0.28.3.145667764438817.42.13928"" in meta_row[""instance_id""] else 0\r\n        # vendor_high = 1 if ""1.3.6.1.4.1.14519.5.2.1.3983.1600"" in meta_row[""instance_id""] else 0\r\n        #         row_items = [cancer_label, 0, mass_pred, x_scale, z_scale, vendor_low, vendor_high] # mask.sum()\r\n\r\n        row_items = [cancer_label, 0, mass_pred] # mask.sum()\r\n\r\n        for magnification in [1, 1.5, 2]:\r\n            pred_df_list = []\r\n            for extension in extensions:\r\n                src_dir = settings.NDSB3_NODULE_DETECTION_DIR + ""predictions"" + str(int(magnification * 10)) + extension + ""/""\r\n                pred_nodules_df = pandas.read_csv(src_dir + patient_id + "".csv"")\r\n                pred_nodules_df = pred_nodules_df[pred_nodules_df[""diameter_mm""] > 0]\r\n                pred_nodules_df = pred_nodules_df[pred_nodules_df[""nodule_chance""] > nodule_th]\r\n                pred_df_list.append(pred_nodules_df)\r\n\r\n            pred_nodules_df = pandas.concat(pred_df_list, ignore_index=True)\r\n\r\n            nodule_count = len(pred_nodules_df)\r\n            nodule_max = 0\r\n            nodule_median = 0\r\n            nodule_chance = 0\r\n            nodule_sum = 0\r\n            coord_z = 0\r\n            second_largest = 0\r\n            nodule_wmax = 0\r\n\r\n            count_rows = []\r\n            coord_y = 0\r\n            coord_x = 0\r\n\r\n            if len(pred_nodules_df) > 0:\r\n                max_index = pred_nodules_df[""diameter_mm""].argmax\r\n                max_row = pred_nodules_df.loc[max_index]\r\n                nodule_max = round(max_row[""diameter_mm""], 2)\r\n                nodule_chance = round(max_row[""nodule_chance""], 2)\r\n                nodule_median = round(pred_nodules_df[""diameter_mm""].median(), 2)\r\n                nodule_wmax = round(nodule_max * nodule_chance, 2)\r\n                coord_z = max_row[""coord_z""]\r\n                coord_y = max_row[""coord_y""]\r\n                coord_x = max_row[""coord_x""]\r\n\r\n\r\n                rows = []\r\n                for row_index, row in pred_nodules_df.iterrows():\r\n                    dist = helpers.get_distance(max_row, row)\r\n                    if dist > 0.2:\r\n                        nodule_mal = row[""diameter_mm""]\r\n                        if nodule_mal > second_largest:\r\n                            second_largest = nodule_mal\r\n                    rows.append(row)\r\n\r\n                count_rows = []\r\n                for row in rows:\r\n                    ok = True\r\n                    for count_row in count_rows:\r\n                        dist = helpers.get_distance(count_row, row)\r\n                        if dist < 0.2:\r\n                            ok = False\r\n                    if ok:\r\n                        count_rows.append(row)\r\n            nodule_count = len(count_rows)\r\n            row_items += [nodule_max, nodule_chance, nodule_count, nodule_median, nodule_wmax, coord_z, second_largest, coord_y, coord_x]\r\n\r\n        row_items.append(patient_id)\r\n        data_rows.append(row_items)\r\n\r\n    # , ""x_scale"", ""z_scale"", ""vendor_low"", ""vendor_high""\r\n    columns = [""cancer_label"", ""mask_size"", ""mass""]\r\n    for magnification in [1, 1.5, 2]:\r\n        str_mag = str(int(magnification * 10))\r\n        columns.append(""mx_"" + str_mag)\r\n        columns.append(""ch_"" + str_mag)\r\n        columns.append(""cnt_"" + str_mag)\r\n        columns.append(""med_"" + str_mag)\r\n        columns.append(""wmx_"" + str_mag)\r\n        columns.append(""crdz_"" + str_mag)\r\n        columns.append(""mx2_"" + str_mag)\r\n        columns.append(""crdy_"" + str_mag)\r\n        columns.append(""crdx_"" + str_mag)\r\n\r\n    columns.append(""patient_id"")\r\n    res_df = pandas.DataFrame(data_rows, columns=columns)\r\n\r\n    if not os.path.exists(settings.BASE_DIR + ""xgboost_trainsets/""):\r\n        os.mkdir(settings.BASE_DIR + ""xgboost_trainsets/"")\r\n    target_path = settings.BASE_DIR + ""xgboost_trainsets/"" ""train"" + extension + "".csv"" if train_set else settings.BASE_DIR + ""xgboost_trainsets/"" + ""submission"" + extension + "".csv""\r\n    res_df.to_csv(target_path, index=False)\r\n\r\n\r\n\r\ndef train_xgboost_on_combined_nodules_ensembletest(fixed_holdout=False, submission_is_fixed_holdout=False, ensemble_lists=[]):\r\n    train_cols = [""mass"", ""mx_10"", ""mx_20"", ""mx_15"", ""crdz_10"", ""crdz_15"", ""crdz_20""]\r\n    runs = 5 if fixed_holdout else 1000\r\n    test_size = 0.1\r\n    record_count = 0\r\n    seed = random.randint(0, 500) if fixed_holdout else 4242\r\n\r\n    variants = []\r\n    x_variants = dict()\r\n    y_variants = dict()\r\n    for ensemble in ensemble_lists:\r\n        for variant in ensemble:\r\n            variants.append(variant)\r\n            df_train = pandas.read_csv(settings.BASE_DIR + ""xgboost_trainsets/"" + ""train"" + variant + "".csv"")\r\n\r\n            y = df_train[""cancer_label""].as_matrix()\r\n            y = y.reshape(y.shape[0], 1)\r\n\r\n            cols = df_train.columns.values.tolist()\r\n            cols.remove(""cancer_label"")\r\n            cols.remove(""patient_id"")\r\n            x = df_train[train_cols].as_matrix()\r\n\r\n            x_variants[variant] = x\r\n            record_count = len(x)\r\n            y_variants[variant] = y\r\n\r\n    scores = defaultdict(lambda: [])\r\n    ensemble_scores = []\r\n    for i in range(runs):\r\n        submission_preds_list = defaultdict(lambda: [])\r\n        train_preds_list = defaultdict(lambda: [])\r\n        holdout_preds_list = defaultdict(lambda: [])\r\n\r\n        train_test_mask = numpy.random.choice([True, False], record_count, p=[0.8, 0.2])\r\n        for variant in variants:\r\n            x = x_variants[variant]\r\n            y = y_variants[variant]\r\n            x_train = x[train_test_mask]\r\n            y_train = y[train_test_mask]\r\n            x_holdout = x[~train_test_mask]\r\n            y_holdout = y[~train_test_mask]\r\n            if fixed_holdout:\r\n                x_train = x[300:]\r\n                y_train = y[300:]\r\n                x_holdout = x[:300]\r\n                y_holdout = y[:300]\r\n\r\n            if True:\r\n                clf = xgboost.XGBRegressor(max_depth=4,\r\n                                           n_estimators=80, #50\r\n                                           learning_rate=0.05,\r\n                                           min_child_weight=60,\r\n                                           nthread=8,\r\n                                           subsample=0.95, #95\r\n                                           colsample_bytree=0.95, # 95\r\n                                           # subsample=1.00,\r\n                                           # colsample_bytree=1.00,\r\n                                           seed=seed)\r\n                #\r\n                clf.fit(x_train, y_train, verbose=fixed_holdout and False, eval_set=[(x_train, y_train), (x_holdout, y_holdout)], eval_metric=""logloss"", early_stopping_rounds=5, )\r\n                holdout_preds = clf.predict(x_holdout)\r\n\r\n            holdout_preds = numpy.clip(holdout_preds, 0.001, 0.999)\r\n            # holdout_preds *= 0.93\r\n            holdout_preds_list[variant].append(holdout_preds)\r\n            train_preds_list[variant].append(holdout_preds.mean())\r\n            score = log_loss(y_holdout, holdout_preds, normalize=True)\r\n            print(score, ""\\tbest:\\t"", clf.best_score, ""\\titer\\t"", clf.best_iteration, ""\\tmean:\\t"", train_preds_list[-1], ""\\thomean:\\t"", y_holdout.mean(), "" variant:"", variant)\r\n            scores[variant].append(score)\r\n\r\n        total_predictions = []\r\n        for ensemble in ensemble_lists:\r\n            ensemble_predictions = []\r\n            for variant in ensemble:\r\n                variant_predictions = numpy.array(holdout_preds_list[variant], dtype=numpy.float)\r\n                ensemble_predictions.append(variant_predictions.swapaxes(0, 1))\r\n            ensemble_predictions_np = numpy.hstack(ensemble_predictions)\r\n            ensemble_predictions_np = ensemble_predictions_np.mean(axis=1)\r\n            score = log_loss(y_holdout, ensemble_predictions_np, normalize=True)\r\n            print(score)\r\n            total_predictions.append(ensemble_predictions_np.reshape(ensemble_predictions_np.shape[0], 1))\r\n        total_predictions_np = numpy.hstack(total_predictions)\r\n        total_predictions_np = total_predictions_np.mean(axis=1)\r\n        score = log_loss(y_holdout, total_predictions_np, normalize=True)\r\n        print(""Total: "", score)\r\n        ensemble_scores.append(score)\r\n\r\n    print(""Average score: "", sum(ensemble_scores) / len(ensemble_scores))\r\n\r\n\r\ndef train_xgboost_on_combined_nodules(extension, fixed_holdout=False, submission=False, submission_is_fixed_holdout=False):\r\n    df_train = pandas.read_csv(settings.BASE_DIR + ""xgboost_trainsets/"" + ""train"" + extension + "".csv"")\r\n    if submission:\r\n        df_submission = pandas.read_csv(settings.BASE_DIR + ""xgboost_trainsets/"" + ""submission"" + extension + "".csv"")\r\n        submission_y = numpy.zeros((len(df_submission), 1))\r\n\r\n    if submission_is_fixed_holdout:\r\n        df_submission = df_train[:300]\r\n        df_train = df_train[300:]\r\n        submission_y = df_submission[""cancer_label""].as_matrix()\r\n        submission_y = submission_y.reshape(submission_y.shape[0], 1)\r\n\r\n    y = df_train[""cancer_label""].as_matrix()\r\n    y = y.reshape(y.shape[0], 1)\r\n    # print(""Mean y: "", y.mean())\r\n\r\n    cols = df_train.columns.values.tolist()\r\n    cols.remove(""cancer_label"")\r\n    cols.remove(""patient_id"")\r\n\r\n    train_cols = [""mass"", ""mx_10"", ""mx_20"", ""mx_15"", ""crdz_10"", ""crdz_15"", ""crdz_20""]\r\n    x = df_train[train_cols].as_matrix()\r\n    if submission:\r\n        x_submission = df_submission[train_cols].as_matrix()\r\n\r\n    if submission_is_fixed_holdout:\r\n        x_submission = df_submission[train_cols].as_matrix()\r\n\r\n    runs = 20 if fixed_holdout else 1000\r\n    scores = []\r\n    submission_preds_list = []\r\n    train_preds_list = []\r\n    holdout_preds_list = []\r\n    for i in range(runs):\r\n        test_size = 0.1 if submission else 0.1\r\n        # stratify=y,\r\n        x_train, x_holdout, y_train, y_holdout = cross_validation.train_test_split(x, y,  test_size=test_size)\r\n        # print(y_holdout.mean())\r\n        if fixed_holdout:\r\n            x_train = x[300:]\r\n            y_train = y[300:]\r\n            x_holdout = x[:300]\r\n            y_holdout = y[:300]\r\n\r\n        seed = random.randint(0, 500) if fixed_holdout else 4242\r\n        if True:\r\n            clf = xgboost.XGBRegressor(max_depth=4,\r\n                                       n_estimators=80, #55\r\n                                       learning_rate=0.05,\r\n                                       min_child_weight=60,\r\n                                       nthread=8,\r\n                                       subsample=0.95, #95\r\n                                       colsample_bytree=0.95, # 95\r\n                                       # subsample=1.00,\r\n                                       # colsample_bytree=1.00,\r\n                                       seed=seed)\r\n            #\r\n            clf.fit(x_train, y_train, verbose=fixed_holdout and False, eval_set=[(x_train, y_train), (x_holdout, y_holdout)], eval_metric=""logloss"", early_stopping_rounds=5, )\r\n            holdout_preds = clf.predict(x_holdout)\r\n\r\n        holdout_preds = numpy.clip(holdout_preds, 0.001, 0.999)\r\n        # holdout_preds *= 0.93\r\n        holdout_preds_list.append(holdout_preds)\r\n        train_preds_list.append(holdout_preds.mean())\r\n        score = log_loss(y_holdout, holdout_preds, normalize=True)\r\n\r\n        print(score, ""\\tbest:\\t"", clf.best_score, ""\\titer\\t"", clf.best_iteration, ""\\tmean:\\t"", train_preds_list[-1], ""\\thomean:\\t"", y_holdout.mean())\r\n        scores.append(score)\r\n\r\n        if submission_is_fixed_holdout:\r\n            submission_preds = clf.predict(x_submission)\r\n            submission_preds_list.append(submission_preds)\r\n\r\n        if submission:\r\n            submission_preds = clf.predict(x_submission)\r\n            submission_preds_list.append(submission_preds)\r\n\r\n    if fixed_holdout:\r\n        all_preds = numpy.vstack(holdout_preds_list)\r\n        avg_preds = numpy.average(all_preds, axis=0)\r\n        avg_preds[avg_preds < 0.001] = 0.001\r\n        avg_preds[avg_preds > 0.999] = 0.999\r\n        deltas = numpy.abs(avg_preds.reshape(300) - y_holdout.reshape(300))\r\n        df_train = df_train[:300]\r\n        df_train[""deltas""] = deltas\r\n        # df_train.to_csv(""c:/tmp/deltas.csv"")\r\n        loss = log_loss(y_holdout, avg_preds)\r\n        print(""Fixed holout avg score: "", loss)\r\n        # print(""Fixed holout mean: "", y_holdout.mean())\r\n\r\n    if submission:\r\n        all_preds = numpy.vstack(submission_preds_list)\r\n        avg_preds = numpy.average(all_preds, axis=0)\r\n        avg_preds[avg_preds < 0.01] = 0.01\r\n        avg_preds[avg_preds > 0.99] = 0.99\r\n        submission_preds_list = avg_preds.tolist()\r\n        df_submission[""id""] = df_submission[""patient_id""]\r\n        df_submission[""cancer""] = submission_preds_list\r\n        df_submission = df_submission[[""id"", ""cancer""]]\r\n        if not os.path.exists(""submission/""):\r\n            os.mkdir(""submission/"")\r\n        if not os.path.exists(""submission/level1/""):\r\n            os.mkdir(""submission/level1/"")\r\n\r\n        df_submission.to_csv(""submission/level1/s"" + extension + "".csv"", index=False)\r\n        # print(""Submission mean chance: "", avg_preds.mean())\r\n\r\n    if submission_is_fixed_holdout:\r\n        all_preds = numpy.vstack(submission_preds_list)\r\n        avg_preds = numpy.average(all_preds, axis=0)\r\n        avg_preds[avg_preds < 0.01] = 0.01\r\n        avg_preds[avg_preds > 0.99] = 0.99\r\n        submission_preds_list = avg_preds.tolist()\r\n        loss = log_loss(submission_y, submission_preds_list)\r\n        # print(""First 300 patients : "", loss)\r\n    if submission_is_fixed_holdout:\r\n        print(""First 300 patients score: "", sum(scores) / len(scores), "" mean chance: "", sum(train_preds_list) / len(train_preds_list))\r\n    else:\r\n        print(""Average score: "", sum(scores) / len(scores), "" mean chance: "", sum(train_preds_list) / len(train_preds_list))\r\n\r\n\r\ndef combine_submissions(level, model_type=None):\r\n    print(""Combine submissions.. level: "", level, "" model_type: "", model_type)\r\n    src_dir = ""submission/level"" + str(level) + ""/""\r\n\r\n    dst_dir = ""submission/""\r\n    if level == 1:\r\n        dst_dir += ""level2/""\r\n    if not os.path.exists(""submission/level2/""):\r\n        os.mkdir(""submission/level2/"")\r\n\r\n    submission_df = pandas.read_csv(""resources/stage2_sample_submission.csv"")\r\n    submission_df[""id2""] = submission_df[""id""]\r\n    submission_df.set_index([""id2""], inplace=True)\r\n    search_expr = ""*.csv"" if model_type is None else ""*"" + model_type + ""*.csv""\r\n    csvs = glob.glob(src_dir + search_expr)\r\n    print(len(csvs), "" found.."")\r\n    for submission_idx, submission_path in enumerate(csvs):\r\n        print(ntpath.basename(submission_path))\r\n        column_name = ""s"" + str(submission_idx)\r\n        submission_df[column_name] = 0\r\n        sub_df = pandas.read_csv(submission_path)\r\n        for index, row in sub_df.iterrows():\r\n            patient_id = row[""id""]\r\n            cancer = row[""cancer""]\r\n            submission_df.loc[patient_id, column_name] = cancer\r\n\r\n    submission_df[""cancer""] = 0\r\n    for i in range(len(csvs)):\r\n        submission_df[""cancer""] += submission_df[""s"" + str(i)]\r\n    submission_df[""cancer""] /= len(csvs)\r\n\r\n    if not os.path.exists(dst_dir + ""debug/""):\r\n        os.mkdir(dst_dir + ""debug/"")\r\n    if level == 2:\r\n        target_path = dst_dir + ""final_submission.csv""\r\n        target_path_allcols = dst_dir + ""debug/final_submission.csv""\r\n    else:\r\n        target_path_allcols = dst_dir + ""debug/"" + ""combined_submission_"" + model_type + "".csv""\r\n        target_path = dst_dir + ""combined_submission_"" + model_type + "".csv""\r\n\r\n    submission_df.to_csv(target_path_allcols, index=False)\r\n    submission_df[[""id"", ""cancer""]].to_csv(target_path, index=False)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    if True:\r\n        for model_variant in [""_luna16_fs"", ""_luna_posnegndsb_v1"", ""_luna_posnegndsb_v2""]:\r\n            print(""Variant: "", model_variant)\r\n            if True:\r\n                combine_nodule_predictions(None, train_set=True, nodule_th=0.7, extensions=[model_variant])\r\n                combine_nodule_predictions(None, train_set=False, nodule_th=0.7, extensions=[model_variant])\r\n            if True:\r\n                train_xgboost_on_combined_nodules(fixed_holdout=False, submission=True, submission_is_fixed_holdout=False, extension=model_variant)\r\n                train_xgboost_on_combined_nodules(fixed_holdout=True, extension=model_variant)\r\n\r\n    combine_submissions(level=1, model_type=""luna_posnegndsb"")\r\n    combine_submissions(level=1, model_type=""luna16_fs"")\r\n    combine_submissions(level=1, model_type=""daniel"")\r\n    combine_submissions(level=2)\r\n'"
