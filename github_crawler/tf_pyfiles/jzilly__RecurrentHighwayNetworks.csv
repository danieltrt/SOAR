file_path,api_count,code
brainstorm_rhn.py,0,"b'#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import division, print_function, unicode_literals\n\nfrom collections import OrderedDict\n\nfrom brainstorm.layers.base_layer import Layer\nfrom brainstorm.structure.buffer_structure import (BufferStructure,\n                                                   StructureTemplate)\nfrom brainstorm.structure.construction import ConstructionWrapper\nfrom brainstorm.utils import LayerValidationError, flatten_time, \\\n    flatten_time_and_features\n\n\ndef HighwayRNNCoupledGates(size, activation=\'tanh\', name=None, recurrence_depth=1):\n    """"""Create a Simple Recurrent layer.""""""\n    return ConstructionWrapper.create(HighwayRNNCoupledGatesLayerImpl, size=size,\n                                      name=name, activation=activation,\n                                      recurrence_depth=recurrence_depth)\n\n\nclass HighwayRNNCoupledGatesLayerImpl(Layer):\n\n    expected_inputs = {\'default\': StructureTemplate(\'T\', \'B\', \'...\')}\n    expected_kwargs = {\'size\', \'activation\', \'recurrence_depth\', \'block_size\', \'sizes_list\'}\n\n    def setup(self, kwargs, in_shapes):\n        self.activation = kwargs.get(\'activation\', \'tanh\')\n        self.size = kwargs.get(\'size\', self.in_shapes[\'default\'].feature_size)\n        self.recurrence_depth = kwargs.get(\'recurrence_depth\', 1)\n        if not isinstance(self.size, int):\n            raise LayerValidationError(\'size must be int but was {}\'.\n                                       format(self.size))\n        if not isinstance(self.recurrence_depth, int):\n            raise LayerValidationError(\'recurrence_depth must be int but was {}\'.\n                                       format(self.recurrence_depth))\n        in_size = self.in_shapes[\'default\'].feature_size\n\n        outputs = OrderedDict()\n        outputs[\'default\'] = BufferStructure(\'T\', \'B\', self.size,\n                                             context_size=1)\n        parameters = OrderedDict()\n        parameters[\'W_H\'] = BufferStructure(self.size, in_size)\n        parameters[\'W_T\'] = BufferStructure(self.size, in_size)\n        parameters[\'R_T\'] = BufferStructure(self.recurrence_depth, self.size, self.size)\n        parameters[\'bias_T\'] = BufferStructure(self.recurrence_depth, self.size)\n        parameters[\'R_H\'] = (BufferStructure(self.recurrence_depth, self.size, self.size))\n        parameters[\'bias_H\'] = BufferStructure(self.recurrence_depth, self.size)\n\n        internals = OrderedDict()\n        for i in range(self.recurrence_depth):\n            internals[\'H_{}\'.format(i)] = BufferStructure(\'T\', \'B\', self.size, context_size=1)\n            internals[\'T_{}\'.format(i)] = BufferStructure(\'T\', \'B\', self.size, context_size=1)\n            internals[\'Y_{}\'.format(i)] = BufferStructure(\'T\', \'B\', self.size, context_size=1)\n            internals[\'dH_{}\'.format(i)] = BufferStructure(\'T\', \'B\', self.size, context_size=1,\n                                                           is_backward_only=True)\n            internals[\'dT_{}\'.format(i)] = BufferStructure(\'T\', \'B\', self.size, context_size=1,\n                                                           is_backward_only=True)\n            internals[\'dY_{}\'.format(i)] = BufferStructure(\'T\', \'B\', self.size, context_size=1,\n                                                           is_backward_only=True)\n\n        return outputs, parameters, internals\n    \n    def forward_pass(self, buffers, training_pass=True):\n        # prepare\n        _h = self.handler\n        W_H, W_T, R_T, bias_T, R_H, bias_H = buffers.parameters\n\n        inputs = buffers.inputs.default\n        outputs = buffers.outputs.default\n\n        H_list = []\n        T_list = []\n        Y_list = []\n\n        for i in range(self.recurrence_depth):\n            H_list.append(buffers.internals[\'H_{}\'.format(i)])\n            T_list.append(buffers.internals[\'T_{}\'.format(i)])\n            Y_list.append(buffers.internals[\'Y_{}\'.format(i)])\n\n        flat_inputs = flatten_time_and_features(inputs)\n\n        flat_H = flatten_time(H_list[0][:-1])\n        flat_T = flatten_time(T_list[0][:-1])\n\n        _h.dot_mm(flat_inputs, W_H, flat_H, transb=True)\n        _h.dot_mm(flat_inputs, W_T, flat_T, transb=True)\n\n        for t in range(inputs.shape[0]):\n            for i in range(self.recurrence_depth):\n                if i == 0:\n                    x = outputs[t-1]\n                    _h.dot_add_mm(x, R_T[i], T_list[i][t], transb=True)\n                    _h.add_mv(T_list[i][t], bias_T[i].reshape((1, self.size)), T_list[i][t])\n                    _h.inplace_act_func[\'sigmoid\'](T_list[i][t])\n                    _h.dot_add_mm(x, R_H[i], H_list[i][t], transb=True)\n                    _h.add_mv(H_list[i][t], bias_H[i].reshape((1, self.size)), H_list[i][t])\n                    _h.inplace_act_func[self.activation](H_list[i][t])\n                else:\n                    x = Y_list[i-1][t]\n                    _h.dot_mm(x, R_T[i], T_list[i][t], transb=True)\n                    _h.add_mv(T_list[i][t], bias_T[i].reshape((1, self.size)), T_list[i][t])\n                    _h.inplace_act_func[\'sigmoid\'](T_list[i][t])\n                    _h.dot_mm(x, R_H[i], H_list[i][t], transb=True)\n                    _h.add_mv(H_list[i][t], bias_H[i].reshape((1, self.size)), H_list[i][t])\n                    _h.inplace_act_func[self.activation](H_list[i][t])\n\n                if i == 0:\n                    _h.mult_tt(T_list[i][t], H_list[i][t], out=Y_list[i][t])\n                    tmp = _h.ones(H_list[i][t].shape)\n                    _h.subtract_tt(tmp, T_list[i][t], tmp)\n                    _h.mult_add_tt(tmp, outputs[t-1], out=Y_list[i][t])\n                else:\n                    _h.mult_tt(T_list[i][t], H_list[i][t], out=Y_list[i][t])\n                    tmp = _h.ones(H_list[i][t].shape)\n                    _h.subtract_tt(tmp, T_list[i][t], tmp)\n                    _h.mult_add_tt(tmp, Y_list[i-1][t], out=Y_list[i][t])\n            _h.copy_to(Y_list[self.recurrence_depth-1][t], outputs[t])\n\n    def backward_pass(self, buffers):\n        # prepare\n        _h = self.handler\n\n        W_H, W_T, R_T, bias_T, R_H, bias_H = buffers.parameters\n        dW_H, dW_T, dR_T, dbias_T, dR_H, dbias_H = buffers.gradients\n\n        inputs = buffers.inputs.default\n        outputs = buffers.outputs.default\n        dinputs = buffers.input_deltas.default\n        doutputs = buffers.output_deltas.default\n\n        H_list = []\n        T_list = []\n        Y_list = []\n        dH_list = []\n        dT_list = []\n        dY_list = []\n\n        for i in range(self.recurrence_depth):\n            H_list.append(buffers.internals[\'H_{}\'.format(i)])\n            T_list.append(buffers.internals[\'T_{}\'.format(i)])\n            Y_list.append(buffers.internals[\'Y_{}\'.format(i)])\n            dH_list.append(buffers.internals[\'dH_{}\'.format(i)])\n            dT_list.append(buffers.internals[\'dT_{}\'.format(i)])\n            dY_list.append(buffers.internals[\'dY_{}\'.format(i)])\n\n        t = inputs.shape[0] - 1\n        _h.copy_to(doutputs[t], dY_list[self.recurrence_depth-1][t])\n\n        for i in range(self.recurrence_depth-1, -1, -1):\n                if i == 0:\n                    _h.mult_tt(dY_list[i][t], T_list[i][t], dH_list[i][t])\n                    tmp = _h.ones(dH_list[i][t].shape)\n                    _h.subtract_tt(H_list[i][t], outputs[t-1], tmp)\n                    _h.mult_tt(dY_list[i][t], tmp, dT_list[i][t])\n\n                    _h.inplace_act_func_deriv[\'sigmoid\'](T_list[i][t], dT_list[i][t])\n                    _h.inplace_act_func_deriv[self.activation](H_list[i][t], dH_list[i][t])\n                else:\n                    _h.mult_tt(dY_list[i][t], T_list[i][t], dH_list[i][t])\n                    tmp = _h.ones(dH_list[i][t].shape)\n                    _h.subtract_tt(tmp, T_list[i][t], tmp)\n                    _h.mult_tt(dY_list[i][t], tmp, dY_list[i-1][t])\n\n                    _h.subtract_tt(H_list[i][t], Y_list[i-1][t], tmp)\n                    _h.mult_tt(dY_list[i][t], tmp, dT_list[i][t])\n\n                    _h.inplace_act_func_deriv[\'sigmoid\'](T_list[i][t], dT_list[i][t])\n                    _h.inplace_act_func_deriv[self.activation](H_list[i][t], dH_list[i][t])\n                    _h.dot_add_mm(dT_list[i][t], R_T[i], dY_list[i-1][t])\n                    _h.dot_add_mm(dH_list[i][t], R_H[i], dY_list[i-1][t])\n\n        for t in range(inputs.shape[0] - 2, -1, -1):\n            _h.dot_add_mm(dT_list[0][t + 1], R_T[0], doutputs[t])\n            _h.dot_add_mm(dH_list[0][t + 1], R_H[0], doutputs[t])\n            tmp = _h.ones(dH_list[0][t + 1].shape)\n            _h.subtract_tt(tmp, T_list[0][t + 1], tmp)\n            _h.mult_add_tt(dY_list[0][t + 1], tmp, doutputs[t])\n            _h.copy_to(doutputs[t], dY_list[self.recurrence_depth-1][t])\n\n            for i in range(self.recurrence_depth-1, -1, -1):\n                    if i == 0:\n                        _h.mult_tt(dY_list[i][t], T_list[i][t], dH_list[i][t])\n                        tmp = _h.ones(dH_list[i][t].shape)\n                        _h.subtract_tt(H_list[i][t], outputs[t-1], tmp)\n                        _h.mult_tt(dY_list[i][t], tmp, dT_list[i][t])\n\n                        _h.inplace_act_func_deriv[\'sigmoid\'](T_list[i][t], dT_list[i][t])\n                        _h.inplace_act_func_deriv[self.activation](H_list[i][t], dH_list[i][t])\n                    else:\n                        _h.mult_tt(dY_list[i][t], T_list[i][t], dH_list[i][t])\n                        tmp = _h.ones(dH_list[i][t].shape)\n                        _h.subtract_tt(tmp, T_list[i][t], tmp)\n                        _h.mult_tt(dY_list[i][t], tmp, dY_list[i-1][t])\n\n                        _h.subtract_tt(H_list[i][t], Y_list[i-1][t], tmp)\n                        _h.mult_tt(dY_list[i][t], tmp, dT_list[i][t])\n\n                        _h.inplace_act_func_deriv[\'sigmoid\'](T_list[i][t], dT_list[i][t])\n                        _h.inplace_act_func_deriv[self.activation](H_list[i][t], dH_list[i][t])\n                        _h.dot_add_mm(dT_list[i][t], R_T[i], dY_list[i-1][t])\n                        _h.dot_add_mm(dH_list[i][t], R_H[i], dY_list[i-1][t])\n\n        flat_inputs = flatten_time_and_features(inputs)\n        flat_dinputs = flatten_time_and_features(dinputs)\n        flat_dH = flatten_time(dH_list[0][:-1])\n        flat_dT = flatten_time(dT_list[0][:-1])\n\n        # calculate in_deltas and gradients\n        _h.dot_add_mm(flat_dH, W_H, flat_dinputs)\n        _h.dot_add_mm(flat_dH, flat_inputs, dW_H, transa=True)\n        _h.dot_add_mm(flat_dT, W_T, flat_dinputs)\n        _h.dot_add_mm(flat_dT, flat_inputs, dW_T, transa=True)\n\n        for i in range(self.recurrence_depth):\n            dbias_tmp = _h.allocate(dbias_H[i].shape)\n            flat_dH = flatten_time(dH_list[i][:-1])\n            flat_dT = flatten_time(dT_list[i][:-1])\n            _h.sum_t(flat_dT, axis=0, out=dbias_tmp)\n            _h.add_tt(dbias_T[i], dbias_tmp, dbias_T[i])\n            _h.sum_t(flat_dH, axis=0, out=dbias_tmp)\n            _h.add_tt(dbias_H[i], dbias_tmp, dbias_H[i])\n\n        for i in range(self.recurrence_depth):\n            if i == 0:\n                flat_outputs = flatten_time(outputs[:-2])\n                flat_dH = flatten_time(dH_list[i][1:-1])\n                flat_dT = flatten_time(dT_list[i][1:-1])\n                _h.dot_add_mm(flat_dT, flat_outputs, dR_T[i], transa=True)\n                _h.dot_add_mm(dT_list[i][0], outputs[-1], dR_T[i], transa=True)\n\n                _h.dot_add_mm(flat_dH, flat_outputs, dR_H[i], transa=True)\n                _h.dot_add_mm(dH_list[i][0], outputs[-1], dR_H[i], transa=True)\n            else:\n                flat_outputs = flatten_time(Y_list[i-1][:-1])\n                flat_dH = flatten_time(dH_list[i][:-1])\n                flat_dT = flatten_time(dT_list[i][:-1])\n                _h.dot_add_mm(flat_dT, flat_outputs, dR_T[i], transa=True)\n                _h.dot_add_mm(flat_dH, flat_outputs, dR_H[i], transa=True)\n'"
rhn.py,34,"b'from __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import math_ops, array_ops\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.ops.nn import rnn_cell\n\nRNNCell = rnn_cell.RNNCell\n\n\nclass Model(object):\n  """"""A Variational RHN model.""""""\n\n  def __init__(self, is_training, config):\n    self.batch_size = batch_size = config.batch_size\n    self.num_steps = num_steps = config.num_steps\n    self.depth = depth = config.depth\n    self.size = size = config.hidden_size\n    self.num_layers = num_layers = config.num_layers\n    vocab_size = config.vocab_size\n    if vocab_size < self.size and not config.tied:\n      in_size = vocab_size\n    else:\n      in_size = self.size\n    self.in_size = in_size\n    self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n    self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n    self._noise_x = tf.placeholder(tf.float32, [batch_size, num_steps, 1])\n    self._noise_i = tf.placeholder(tf.float32, [batch_size, in_size, num_layers])\n    self._noise_h = tf.placeholder(tf.float32, [batch_size, size, num_layers])\n    self._noise_o = tf.placeholder(tf.float32, [batch_size, 1, size])\n\n    with tf.device(""/cpu:0""):\n      embedding = tf.get_variable(""embedding"", [vocab_size, in_size])\n      inputs = tf.nn.embedding_lookup(embedding, self._input_data) * self._noise_x\n\n    outputs = []\n    self._initial_state = [0] * self.num_layers\n    state = [0] * self.num_layers\n    self._final_state = [0] * self.num_layers\n    for l in range(config.num_layers):\n      with tf.variable_scope(\'RHN\' + str(l)):\n        cell = RHNCell(size, in_size, is_training, depth=depth, forget_bias=config.init_bias)\n        self._initial_state[l] = cell.zero_state(batch_size, tf.float32)\n        state[l] = [self._initial_state[l], self._noise_i[:, :, l], self._noise_h[:, :, l]]\n        for time_step in range(num_steps):\n          if time_step > 0:\n            tf.get_variable_scope().reuse_variables()\n          (cell_output, state[l]) = cell(inputs[:, time_step, :], state[l])\n          outputs.append(cell_output)\n        inputs = tf.pack(outputs, axis=1)\n        outputs = []\n\n    output = tf.reshape(inputs * self._noise_o, [-1, size])\n    softmax_w = tf.transpose(embedding) if config.tied else tf.get_variable(""softmax_w"", [size, vocab_size])\n    softmax_b = tf.get_variable(""softmax_b"", [vocab_size])\n    logits = tf.matmul(output, softmax_w) + softmax_b\n    loss = tf.nn.seq2seq.sequence_loss_by_example(\n      [logits],\n      [tf.reshape(self._targets, [-1])],\n      [tf.ones([batch_size * num_steps])])\n    self._final_state = [s[0] for s in state]\n    pred_loss = tf.reduce_sum(loss) / batch_size\n    self._cost = cost = pred_loss\n    if not is_training:\n      return\n    tvars = tf.trainable_variables()\n    l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tvars])\n    self._cost = cost = pred_loss + config.weight_decay * l2_loss\n\n    self._lr = tf.Variable(0.0, trainable=False)\n    self._nvars = np.prod(tvars[0].get_shape().as_list())\n    print(tvars[0].name, tvars[0].get_shape().as_list())\n    for var in tvars[1:]:\n      sh = var.get_shape().as_list()\n      print(var.name, sh)\n      self._nvars += np.prod(sh)\n    print(self._nvars, \'total variables\')\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                      config.max_grad_norm)\n    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n  def assign_lr(self, session, lr_value):\n    session.run(tf.assign(self.lr, lr_value))\n\n  @property\n  def input_data(self):\n    return self._input_data\n\n  @property\n  def targets(self):\n    return self._targets\n\n  @property\n  def noise_x(self):\n    return self._noise_x\n\n  @property\n  def noise_i(self):\n    return self._noise_i\n\n  @property\n  def noise_h(self):\n    return self._noise_h\n\n  @property\n  def noise_o(self):\n    return self._noise_o\n\n  @property\n  def initial_state(self):\n    return self._initial_state\n\n  @property\n  def cost(self):\n    return self._cost\n\n  @property\n  def final_state(self):\n    return self._final_state\n\n  @property\n  def lr(self):\n    return self._lr\n\n  @property\n  def train_op(self):\n    return self._train_op\n\n  @property\n  def nvars(self):\n    return self._nvars\n\n\nclass RHNCell(RNNCell):\n  """"""Variational Recurrent Highway Layer\n\n  Reference: https://arxiv.org/abs/1607.03474\n  """"""\n\n  def __init__(self, num_units, in_size, is_training, depth=3, forget_bias=None):\n    self._num_units = num_units\n    self._in_size = in_size\n    self.is_training = is_training\n    self.depth = depth\n    self.forget_bias = forget_bias\n\n  @property\n  def input_size(self):\n    return self._in_size\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    current_state = state[0]\n    noise_i = state[1]\n    noise_h = state[2]\n    for i in range(self.depth):\n      with tf.variable_scope(\'h_\'+str(i)):\n        if i == 0:\n          h = tf.tanh(linear([inputs * noise_i, current_state * noise_h], self._num_units, True))\n        else:\n          h = tf.tanh(linear([current_state * noise_h], self._num_units, True))\n      with tf.variable_scope(\'t_\'+str(i)):\n        if i == 0:\n          t = tf.sigmoid(linear([inputs * noise_i, current_state * noise_h], self._num_units, True, self.forget_bias))\n        else:\n          t = tf.sigmoid(linear([current_state * noise_h], self._num_units, True, self.forget_bias))\n      current_state = (h - current_state)* t + current_state\n\n    return current_state, [current_state, noise_i, noise_h]\n\n\ndef linear(args, output_size, bias, bias_start=None, scope=None):\n  """"""\n  This is a slightly modified version of _linear used by Tensorflow rnn.\n  The only change is that we have allowed bias_start=None.\n\n  Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to ""Linear"".\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  """"""\n  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(""`args` must be specified"")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(""Linear is expecting 2D arguments: %s"" % str(shapes))\n    if not shape[1]:\n      raise ValueError(""Linear expects shape[1] of arguments: %s"" % str(shapes))\n    else:\n      total_arg_size += shape[1]\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  with vs.variable_scope(scope or ""Linear""):\n    matrix = vs.get_variable(\n        ""Matrix"", [total_arg_size, output_size], dtype=dtype)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], matrix)\n    else:\n      res = math_ops.matmul(array_ops.concat(1, args), matrix)\n    if not bias:\n      return res\n    elif bias_start is None:\n      bias_term = vs.get_variable(""Bias"", [output_size], dtype=dtype)\n    else:\n      bias_term = vs.get_variable(""Bias"", [output_size], dtype=dtype,\n                                  initializer=tf.constant_initializer(bias_start, dtype=dtype))\n  return res + bias_term\n'"
rhn_train.py,29,"b'""""""Word/Symbol level next step prediction using Recurrent Highway Networks.\n\nTo run:\n$ python rhn_train.py\n\n""""""\nfrom __future__ import absolute_import, division, print_function\nfrom copy import deepcopy\nimport time\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom sacred import Experiment\nfrom rhn import Model\nfrom data.reader import data_iterator\n\nex = Experiment(\'rhn_prediction\')\nlogging = tf.logging\n\nclass Config:\n  pass\nC = Config()\n\n\n@ex.config\ndef hyperparameters():\n  data_path = \'data\'\n  dataset = \'ptb\'\n  init_scale = 0.04\n  init_bias = -2.0\n  num_layers = 1\n  depth = 4  #  the recurrence depth\n  learning_rate = 0.2\n  lr_decay = 1.02\n  weight_decay = 1e-7\n  max_grad_norm = 10\n  num_steps = 35\n  hidden_size = 1000\n  max_epoch = 20\n  max_max_epoch = 500\n  batch_size = 20\n  drop_x = 0.25\n  drop_i = 0.75\n  drop_h = 0.25\n  drop_o = 0.75\n  tied = True\n  load_model = \'\'\n  mc_steps = 0\n  if dataset == \'ptb\':\n    vocab_size = 10000\n  elif dataset == \'enwik8\':\n    vocab_size = 205\n  elif dataset == \'text8\':\n    vocab_size = 27\n  else:\n    raise AssertionError(""Unsupported dataset! Only \'ptb\',"",\n                         ""\'enwik8\' and \'text8\' are currently supported."")\n\n\n@ex.named_config\ndef ptb_sota():\n  data_path = \'data\'\n  dataset = \'ptb\'\n  init_scale = 0.04\n  init_bias = -2.0\n  num_layers = 1\n  depth = 10\n  learning_rate = 0.2\n  lr_decay = 1.02\n  weight_decay = 1e-7\n  max_grad_norm = 10\n  num_steps = 35\n  hidden_size = 830\n  max_epoch = 20\n  max_max_epoch = 500\n  batch_size = 20\n  drop_x = 0.25\n  drop_i = 0.75\n  drop_h = 0.25\n  drop_o = 0.75\n  tied = True\n  vocab_size = 10000\n\n\n@ex.named_config\ndef enwik8_sota():\n  # test BPC 1.27\n  data_path = \'data\'\n  dataset = \'enwik8\'\n  init_scale = 0.04\n  init_bias = -4.0\n  num_layers = 1\n  depth = 10\n  learning_rate = 0.2\n  lr_decay = 1.03\n  weight_decay = 1e-7\n  max_grad_norm = 10\n  num_steps = 50\n  hidden_size = 1500\n  max_epoch = 5\n  max_max_epoch = 500\n  batch_size = 128\n  drop_x = 0.10\n  drop_i = 0.40\n  drop_h = 0.10\n  drop_o = 0.40\n  tied = False\n  vocab_size = 205\n\n@ex.named_config\ndef text8_sota():\n  # test BPC 1.27\n  data_path = \'data\'\n  dataset = \'text8\'\n  init_scale = 0.04\n  init_bias = -4.0\n  num_layers = 1\n  depth = 10\n  learning_rate = 0.2\n  lr_decay = 1.03\n  weight_decay = 1e-7\n  max_grad_norm = 10\n  num_steps = 50\n  hidden_size = 1500\n  max_epoch = 5\n  max_max_epoch = 500\n  batch_size = 128\n  drop_x = 0.10\n  drop_i = 0.40\n  drop_h = 0.10\n  drop_o = 0.40\n  tied = False\n  vocab_size = 27\n\n\n@ex.capture\ndef get_config(_config):\n\n  C.__dict__ = dict(_config)\n  return C\n\n\ndef get_data(data_path, dataset):\n  if dataset == \'ptb\':\n    from tensorflow.models.rnn.ptb import reader\n    raw_data = reader.ptb_raw_data(data_path)\n  elif dataset == \'enwik8\':\n    from data import reader\n    raw_data = reader.enwik8_raw_data(data_path)\n  elif dataset == \'text8\':\n    from data import reader\n    raw_data = reader.text8_raw_data(data_path)\n  return reader, raw_data\n\n\ndef get_noise(x, m, drop_x, drop_i, drop_h, drop_o):\n  keep_x, keep_i, keep_h, keep_o = 1.0 - drop_x, 1.0 - drop_i, 1.0 - drop_h, 1.0 - drop_o\n  if keep_x < 1.0:\n    noise_x = (np.random.random_sample((m.batch_size, m.num_steps, 1)) < keep_x).astype(np.float32) / keep_x\n    for b in range(m.batch_size):\n      for n1 in range(m.num_steps):\n        for n2 in range(n1 + 1, m.num_steps):\n          if x[b][n2] == x[b][n1]:\n            noise_x[b][n2][0] = noise_x[b][n1][0]\n            break\n  else:\n    noise_x = np.ones((m.batch_size, m.num_steps, 1), dtype=np.float32)\n\n  if keep_i < 1.0:\n    noise_i = (np.random.random_sample((m.batch_size, m.in_size, m.num_layers)) < keep_i).astype(np.float32) / keep_i\n  else:\n    noise_i = np.ones((m.batch_size, m.in_size, m.num_layers), dtype=np.float32)\n  if keep_h < 1.0:\n    noise_h = (np.random.random_sample((m.batch_size, m.size, m.num_layers)) < keep_h).astype(np.float32) / keep_h\n  else:\n    noise_h = np.ones((m.batch_size, m.size, m.num_layers), dtype=np.float32)\n  if keep_o < 1.0:\n    noise_o = (np.random.random_sample((m.batch_size, 1, m.size)) < keep_o).astype(np.float32) / keep_o\n  else:\n    noise_o = np.ones((m.batch_size, 1, m.size), dtype=np.float32)\n  return noise_x, noise_i, noise_h, noise_o\n\n\ndef run_epoch(session, m, data, eval_op, config, verbose=False):\n  """"""Run the model on the given data.""""""\n  epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n  start_time = time.time()\n  costs = 0.0\n  iters = 0\n  state = [x.eval() for x in m.initial_state]\n  for step, (x, y) in enumerate(data_iterator(data, m.batch_size, m.num_steps)):\n    noise_x, noise_i, noise_h, noise_o = get_noise(x, m, config.drop_x, config.drop_i, config.drop_h, config.drop_o)\n    feed_dict = {m.input_data: x, m.targets: y,\n                 m.noise_x: noise_x, m.noise_i: noise_i, m.noise_h: noise_h, m.noise_o: noise_o}\n    feed_dict.update({m.initial_state[i]: state[i] for i in range(m.num_layers)})\n    cost, state, _ = session.run([m.cost, m.final_state, eval_op], feed_dict)\n    costs += cost\n    iters += m.num_steps\n\n    if verbose and step % (epoch_size // 10) == 10:\n      print(""%.3f perplexity: %.3f speed: %.0f wps"" % (step * 1.0 / epoch_size, np.exp(costs / iters),\n                                                       iters * m.batch_size / (time.time() - start_time)))\n\n  return np.exp(costs / iters)\n\n\n@ex.command\ndef evaluate(data_path, dataset, load_model):\n  """"""Evaluate the model on the given data.""""""\n  ex.commands[""print_config""]()\n  print(""Evaluating model:"", load_model)\n  reader, (train_data, valid_data, test_data, _) = get_data(data_path, dataset)\n\n  config = get_config()\n  val_config = deepcopy(config)\n  test_config = deepcopy(config)\n  val_config.drop_x = test_config.drop_x = 0.0\n  val_config.drop_i = test_config.drop_i = 0.0\n  val_config.drop_h = test_config.drop_h = 0.0\n  val_config.drop_o = test_config.drop_o = 0.0\n  test_config.batch_size = test_config.num_steps = 1\n\n  with tf.Session() as session:\n    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n    with tf.variable_scope(""model"", reuse=None, initializer=initializer):\n      _ = Model(is_training=True, config=config)\n    with tf.variable_scope(""model"", reuse=True, initializer=initializer):\n      mvalid = Model(is_training=False, config=val_config)\n      mtest = Model(is_training=False, config=test_config)\n    tf.global_variables_initializer().run()\n    saver = tf.train.Saver()\n    saver.restore(session, load_model)\n\n    print(""Testing on batched Valid ..."")\n    valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op(), config=val_config)\n    print(""Valid Perplexity (batched): %.3f, Bits: %.3f"" % (valid_perplexity, np.log2(valid_perplexity)))\n\n    print(""Testing on non-batched Valid ..."")\n    valid_perplexity = run_epoch(session, mtest, valid_data, tf.no_op(), config=test_config, verbose=True)\n    print(""Full Valid Perplexity: %.3f, Bits: %.3f"" % (valid_perplexity, np.log2(valid_perplexity)))\n\n    print(""Testing on non-batched Test ..."")\n    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op(), config=test_config, verbose=True)\n    print(""Full Test Perplexity: %.3f, Bits: %.3f"" % (test_perplexity, np.log2(test_perplexity)))\n\n\ndef run_mc_epoch(seed, session, m, data, eval_op, config, mc_steps, verbose=False):\n  """"""Run the model with noise on the given data multiple times for MC evaluation.""""""\n  n_steps = len(data)\n  all_probs = np.array([0.0]*n_steps)\n  sum_probs = np.array([0.0]*n_steps)\n  mc_i = 1\n  print(""Total MC steps to do:"", mc_steps)\n  if not os.path.isdir(\'./probs\'):\n    print(\'Creating probs directory\')\n    os.mkdir(\'./probs\')\n  while mc_i <= mc_steps:\n    print(""MC sample number:"", mc_i)\n    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n    start_time = time.time()\n    costs = 0.0\n    iters = 0\n    state = [x.eval() for x in m.initial_state]\n\n    for step, (x, y) in enumerate(data_iterator(data, m.batch_size, m.num_steps)):\n      if step == 0:\n        noise_x, noise_i, noise_h, noise_o = get_noise(x, m, config.drop_x, config.drop_i, config.drop_h, config.drop_o)\n      feed_dict = {m.input_data: x, m.targets: y,\n                   m.noise_x: noise_x, m.noise_i: noise_i, m.noise_h: noise_h, m.noise_o: noise_o}\n      feed_dict.update({m.initial_state[i]: state[i] for i in range(m.num_layers)})\n      cost, state, _ = session.run([m.cost, m.final_state, eval_op], feed_dict)\n      costs += cost\n      iters += m.num_steps\n      all_probs[step] = np.exp(-cost)\n      if verbose and step % (epoch_size // 10) == 10:\n        print(""%.3f perplexity: %.3f speed: %.0f wps"" % (step * 1.0 / epoch_size, np.exp(costs / iters),\n                                                         iters * m.batch_size / (time.time() - start_time)))\n    perplexity = np.exp(costs / iters)\n    print(""Perplexity:"", perplexity)\n    if perplexity < 500:\n      savefile = \'probs/\' + str(seed) + \'_\' + str(mc_i)\n      print(""Accepted. Saving to:"", savefile)\n      np.save(savefile, all_probs)\n      sum_probs += all_probs\n      mc_i += 1\n\n  return np.exp(np.mean(-np.log(np.clip(sum_probs/mc_steps, 1e-10, 1-1e-10))))\n\n\n@ex.command\ndef evaluate_mc(data_path, dataset, load_model, mc_steps, seed):\n  """"""Evaluate the model on the given data using MC averaging.""""""\n  ex.commands[\'print_config\']()\n  print(""MC Evaluation of model:"", load_model)\n  assert mc_steps > 0\n  reader, (train_data, valid_data, test_data, _) = get_data(data_path, dataset)\n\n  config = get_config()\n  val_config = deepcopy(config)\n  test_config = deepcopy(config)\n  test_config.batch_size = test_config.num_steps = 1\n  with tf.Session() as session:\n    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n    with tf.variable_scope(""model"", reuse=None, initializer=initializer):\n      _ = Model(is_training=True, config=config)\n    with tf.variable_scope(""model"", reuse=True, initializer=initializer):\n      _ = Model(is_training=False, config=val_config)\n      mtest = Model(is_training=False, config=test_config)\n    tf.initialize_all_variables()\n    saver = tf.train.Saver()\n    saver.restore(session, load_model)\n\n    print(""Testing on non-batched Test ..."")\n    test_perplexity = run_mc_epoch(seed, session, mtest, test_data, tf.no_op(), test_config, mc_steps, verbose=True)\n    print(""Full Test Perplexity: %.3f, Bits: %.3f"" % (test_perplexity, np.log2(test_perplexity)))\n\n\n@ex.automain\ndef main(data_path, dataset, seed, _run):\n  ex.commands[\'print_config\']()\n  np.random.seed(seed)\n  reader, (train_data, valid_data, test_data, _) = get_data(data_path, dataset)\n\n  config = get_config()\n  val_config = deepcopy(config)\n  test_config = deepcopy(config)\n  val_config.drop_x = test_config.drop_x = 0.0\n  val_config.drop_i = test_config.drop_i = 0.0\n  val_config.drop_h = test_config.drop_h = 0.0\n  val_config.drop_o = test_config.drop_o = 0.0\n  test_config.batch_size = test_config.num_steps = 1\n\n  with tf.Graph().as_default(), tf.Session() as session:\n    tf.set_random_seed(seed)\n    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n    with tf.variable_scope(""model"", reuse=None, initializer=initializer):\n      mtrain = Model(is_training=True, config=config)\n    with tf.variable_scope(""model"", reuse=True, initializer=initializer):\n      mvalid = Model(is_training=False, config=val_config)\n      mtest = Model(is_training=False, config=test_config)\n\n    tf.global_variables_initializer().run()\n\n    saver = tf.train.Saver()\n    trains, vals, tests, best_val = [np.inf], [np.inf], [np.inf], np.inf\n\n    for i in range(config.max_max_epoch):\n      lr_decay = config.lr_decay ** max(i - config.max_epoch + 1, 0.0)\n      mtrain.assign_lr(session, config.learning_rate / lr_decay)\n\n      print(""Epoch: %d Learning rate: %.3f"" % (i + 1, session.run(mtrain.lr)))\n      train_perplexity = run_epoch(session, mtrain, train_data, mtrain.train_op, config=config,\n                                   verbose=True)\n      print(""Epoch: %d Train Perplexity: %.3f, Bits: %.3f"" % (i + 1, train_perplexity, np.log2(train_perplexity)))\n\n      valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op(), config=val_config)\n      print(""Epoch: %d Valid Perplexity (batched): %.3f, Bits: %.3f"" % (i + 1, valid_perplexity, np.log2(valid_perplexity)))\n\n      test_perplexity = run_epoch(session, mvalid, test_data, tf.no_op(), config=val_config)\n      print(""Epoch: %d Test Perplexity (batched): %.3f, Bits: %.3f"" % (i + 1, test_perplexity, np.log2(test_perplexity)))\n\n      trains.append(train_perplexity)\n      vals.append(valid_perplexity)\n      tests.append(test_perplexity)\n\n      if valid_perplexity < best_val:\n        best_val = valid_perplexity\n        print(""Best Batched Valid Perplexity improved to %.03f"" % best_val)\n        save_path = saver.save(session, \'./\' + dataset + ""_"" + str(seed) + ""_best_model.ckpt"")\n        print(""Saved to:"", save_path)\n\n      _run.info[\'epoch_nr\'] = i + 1\n      _run.info[\'nr_parameters\'] = mtrain.nvars.item()\n      _run.info[\'logs\'] = {\'train_perplexity\': trains, \'valid_perplexity\': vals, \'test_perplexity\': tests}\n\n\n    print(""Training is over."")\n    best_val_epoch = np.argmin(vals)\n    print(""Best Batched Validation Perplexity %.03f (Bits: %.3f) was at Epoch %d"" %\n          (vals[best_val_epoch], np.log2(vals[best_val_epoch]), best_val_epoch))\n    print(""Training Perplexity at this Epoch was %.03f, Bits: %.3f"" %\n          (trains[best_val_epoch], np.log2(trains[best_val_epoch])))\n    print(""Batched Test Perplexity at this Epoch was %.03f, Bits: %.3f"" %\n          (tests[best_val_epoch], np.log2(tests[best_val_epoch])))\n\n    _run.info[\'best_val_epoch\'] = best_val_epoch\n    _run.info[\'best_valid_perplexity\'] = vals[best_val_epoch]\n\n    with tf.Session() as sess:\n      saver.restore(sess, \'./\'  + dataset + ""_"" + str(seed) + ""_best_model.ckpt"")\n\n      print(""Testing on non-batched Valid ..."")\n      valid_perplexity = run_epoch(sess, mtest, valid_data, tf.no_op(), config=test_config, verbose=True)\n      print(""Full Valid Perplexity: %.3f, Bits: %.3f"" % (valid_perplexity, np.log2(valid_perplexity)))\n\n      print(""Testing on non-batched Test ..."")\n      test_perplexity = run_epoch(sess, mtest, test_data, tf.no_op(), config=test_config, verbose=True)\n      print(""Full Test Perplexity: %.3f, Bits: %.3f"" % (test_perplexity, np.log2(test_perplexity)))\n\n      _run.info[\'full_best_valid_perplexity\'] = valid_perplexity\n      _run.info[\'full_test_perplexity\'] = test_perplexity\n\n  return vals[best_val_epoch]\n'"
test_probs.py,0,"b'""""""A simple script to load and average MC testing predictions""""""\nimport numpy as np\nimport os\n\nsum_p = None\ni = 0\ncount = 0\n\nfor i, probfile in enumerate(os.listdir(\'probs\')):\n  prob = np.load(\'probs/\' + probfile)\n  perp = np.exp(np.mean(-np.log(np.clip(prob, 1e-10, 1 - 1e-10))))\n  if perp > 500:\n    continue\n  if i == 0:\n    sum_p = prob\n  else:\n    sum_p += prob\n  count += 1\n  mean = np.exp(np.mean(-np.log(np.clip(sum_p/count, 1e-10, 1-1e-10)))) if count > 0 else 0\n  print(count, probfile, perp, mean)\n'"
theano_data.py,0,"b'# This file is adapted from the tool provided with Tensorflow for\n# reading the Penn Treebank dataset. The original copyright notice is\n# provided below.\n#\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Utilities for training on the Hutter Prize and PTB datasets for the Theano implementation.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nimport numpy as np\n\n\ndef _read_symbols(filename):\n  with open(filename, ""r"") as f:\n    return f.read()\n\n\ndef _read_words(filename):\n  with open(filename, ""r"") as f:\n    return f.read().decode(""utf-8"").replace(""\\n"", ""<eos>"").split()\n\n\ndef _build_vocab(filename):\n  data = _read_words(filename)\n\n  counter = collections.Counter(data)\n  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n  words, _ = list(zip(*count_pairs))\n  word_to_id = dict(zip(words, range(len(words))))\n\n  return word_to_id\n\n\ndef _file_to_word_ids(filename, word_to_id):\n  data = _read_words(filename)\n  return [word_to_id[word] for word in data if word in word_to_id]\n\n\ndef hutter_raw_data(data_path=None, num_test_symbols=5000000):\n  """"""Load raw data from data directory ""data_path"".\n\n  The raw Hutter prize data is at:\n  http://mattmahoney.net/dc/enwik8.zip\n  The extracted enwik8 file should be placed in the data directory.\n\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n    num_test_symbols: number of symbols at the end that make up the test set\n\n  Returns:\n    tuple (train_data, valid_data, test_data, unique)\n    where each of the data objects can be passed to hutter_iterator.\n  """"""\n\n  data_path = os.path.join(data_path, ""enwik8"")\n\n  raw_data = _read_symbols(data_path)\n  raw_data = np.fromstring(raw_data, dtype=np.uint8)\n  unique, data = np.unique(raw_data, return_inverse=True)\n  train_data = data[: -2 * num_test_symbols]\n  valid_data = data[-2 * num_test_symbols: -num_test_symbols]\n  test_data = data[-num_test_symbols:]\n  return train_data, valid_data, test_data, unique\n\n\ndef ptb_raw_data(data_path=None):\n  """"""Load PTB raw data from data directory ""data_path"".\n\n  Reads PTB text files, converts strings to integer ids,\n  and performs mini-batching of the inputs.\n\n  The PTB dataset comes from Tomas Mikolov\'s webpage:\n  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n  The extracted PTB files are present in the data directory.\n\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n\n  Returns:\n    tuple (train_data, valid_data, test_data, vocabulary)\n    where each of the data objects can be passed to PTBIterator.\n  """"""\n\n  train_path = os.path.join(data_path, ""ptb.train.txt"")\n  valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n  test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n  word_to_id = _build_vocab(train_path)\n  train_data = _file_to_word_ids(train_path, word_to_id)\n  valid_data = _file_to_word_ids(valid_path, word_to_id)\n  test_data = _file_to_word_ids(test_path, word_to_id)\n  vocabulary = len(word_to_id)\n  return train_data, valid_data, test_data, vocabulary\n\n\ndef data_iterator(raw_data, batch_size, num_steps):\n  """"""Iterate on the raw Hutter prize data or the raw PTB data.\n\n  This generates batch_size pointers into the given raw data, and allows\n  minibatch iteration along these pointers.\n\n  Args:\n    raw_data: one of the raw data outputs from hutter_raw_data or ptb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n\n  Yields:\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n    The second element of the tuple is the same data time-shifted to the\n    right by one.\n\n  Raises:\n    ValueError: if batch_size or num_steps are too high.\n  """"""\n  raw_data = np.array(raw_data, dtype=np.int32)\n\n  data_len = len(raw_data)\n  batch_len = data_len // batch_size\n  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n  for i in range(batch_size):\n    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n\n  epoch_size = (batch_len - 1) // num_steps\n\n  if epoch_size == 0:\n    raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n  for i in range(epoch_size):\n    x = data[:, i*num_steps:(i+1)*num_steps]\n    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n    yield (x, y)\n'"
theano_rhn.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport numbers\nimport cPickle\n\nimport numpy as np\n\nimport theano\nimport theano.tensor as tt\nfrom theano.ifelse import ifelse\nfrom theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n\n\nfloatX = theano.config.floatX\n\ndef cast_floatX(n):\n  return np.asarray(n, dtype=floatX)\n\n\nclass Model(object):\n  \n  def __init__(self, config):\n\n    self._params = []                                             # shared variables for learned parameters\n    self._sticky_hidden_states = []                               # shared variables which are reset before each epoch\n    self._np_rng = np.random.RandomState(config.seed // 2 + 123)\n    self._theano_rng = RandomStreams(config.seed // 2 + 321)      # generates random numbers directly on GPU\n    self._init_scale = config.init_scale\n    self._is_training = tt.iscalar(\'is_training\')\n    self._lr = theano.shared(cast_floatX(config.learning_rate), \'lr\')\n\n    input_data = tt.imatrix(\'input_data\')     # (batch_size, num_steps)\n    targets = tt.imatrix(\'targets\')           # (batch_size, num_steps)\n    noise_x = tt.matrix(\'noise_x\')            # (batch_size, num_steps)\n\n    # Embed input words and apply variational dropout (for each sample, the embedding of\n    # a dropped word-type consists of all zeros at all occurrences of word-type in sample).\n    embedding = self.make_param((config.vocab_size, config.hidden_size), \'uniform\')\n    inputs = embedding[input_data.T]          # (num_steps, batch_size, hidden_size)\n    inputs = self.apply_dropout(inputs, tt.shape_padright(noise_x.T))\n\n    rhn_updates = []\n    for _ in range(config.num_layers):\n      # y shape: (num_steps, batch_size, hidden_size)\n      y, sticky_state_updates = self.RHNLayer(\n        inputs,\n        config.depth, config.batch_size, config.hidden_size,\n        config.drop_i, config.drop_s,\n        config.init_T_bias, config.init_other_bias,\n        config.tied_noise)\n      rhn_updates += sticky_state_updates\n      inputs = y\n\n    noise_o = self.get_dropout_noise((config.batch_size, config.hidden_size), config.drop_o)\n    outputs = self.apply_dropout(y, tt.shape_padleft(noise_o))               # (num_steps, batch_size, hidden_size)\n      \n    # logits\n    softmax_w = embedding.T if config.tied_embeddings else self.make_param((config.hidden_size, config.vocab_size), \'uniform\')\n    softmax_b = self.make_param((config.vocab_size,), config.init_other_bias)\n    logits = tt.dot(outputs, softmax_w) + softmax_b                          # (num_steps, batch_size, vocab_size)\n\n    # probabilities and prediction loss\n    flat_logits = logits.reshape((config.batch_size * config.num_steps, config.vocab_size))\n    flat_probs = tt.nnet.softmax(flat_logits)\n    flat_targets = targets.T.flatten()                                       # (batch_size * num_steps,)\n    xentropies = tt.nnet.categorical_crossentropy(flat_probs, flat_targets)  # (batch_size * num_steps,)\n    pred_loss = xentropies.sum() / config.batch_size\n\n    # weight decay\n    l2_loss = 0.5 * tt.sum(tt.stack([tt.sum(p**2) for p in self._params]))\n\n    loss = pred_loss + config.weight_decay * l2_loss\n    grads = theano.grad(loss, self._params)\n\n    # gradient clipping\n    global_grad_norm = tt.sqrt(tt.sum(tt.stack([tt.sum(g**2) for g in grads])))\n    clip_factor = ifelse(global_grad_norm < config.max_grad_norm,\n      cast_floatX(1),\n      tt.cast(config.max_grad_norm / global_grad_norm, floatX))\n\n    param_updates = [(p, p - self._lr * clip_factor * g) for p, g in zip(self._params, grads)]\n\n    self.train = theano.function(\n      [input_data, targets, noise_x],\n      loss,\n      givens = {self._is_training: np.int32(1)},\n      updates = rhn_updates + param_updates)\n\n    self.evaluate = theano.function(\n      [input_data, targets],\n      loss,\n      # Note that noise_x is unused in computation graph of this function since _is_training is false.\n      givens = {self._is_training: np.int32(0), noise_x: tt.zeros((config.batch_size, config.num_steps))},\n      updates = rhn_updates)\n\n    self._num_params = np.sum([param.get_value().size for param in self._params])\n\n    if config.load_model:\n      self.load(config.load_model)\n\n\n  @property\n  def lr(self):\n    return self._lr.get_value()\n\n  @property\n  def num_params(self):\n    return self._num_params\n\n\n  def make_param(self, shape, init_scheme):\n    """"""Create Theano shared variables, which are used as trainable model parameters.""""""\n    if isinstance(init_scheme, numbers.Number):\n      init_value = np.full(shape, init_scheme, floatX)\n    elif init_scheme == \'uniform\':\n      init_value = self._np_rng.uniform(low=-self._init_scale, high=self._init_scale, size=shape).astype(floatX)\n    else:\n      raise AssertionError(\'unsupported init_scheme\')\n    p = theano.shared(init_value)\n    self._params.append(p)\n    return p\n\n  def apply_dropout(self, x, noise):\n    return ifelse(self._is_training, noise * x, x)\n\n  def get_dropout_noise(self, shape, dropout_p):\n    keep_p = 1 - dropout_p\n    noise = cast_floatX(1. / keep_p) * self._theano_rng.binomial(size=shape, p=keep_p, n=1, dtype=floatX)\n    return noise\n    \n  def assign_lr(self, lr):\n    self._lr.set_value(cast_floatX(lr))\n\n  def reset_hidden_state(self):\n    for sticky_hidden_state in self._sticky_hidden_states:\n      sticky_hidden_state.set_value(np.zeros_like(sticky_hidden_state.get_value()))\n\n  def save(self, save_path):\n    with open(save_path, \'wb\') as f:\n      for p in self._params:\n        cPickle.dump(p.get_value(), f, protocol=cPickle.HIGHEST_PROTOCOL)\n    \n  def load(self, load_path):\n    with open(load_path, \'rb\') as f:\n      for p in self._params:\n        p.set_value(cPickle.load(f))\n\n\n  def linear(self, x, in_size, out_size, bias, bias_init=None):\n    assert bias == (bias_init is not None)\n    w = self.make_param((in_size, out_size), \'uniform\')\n    y = tt.dot(x, w)\n    if bias:\n      b = self.make_param((out_size,), bias_init)\n      y += b\n    return y\n\n\n  def RHNLayer(self, inputs, depth, batch_size, hidden_size, drop_i, drop_s, init_T_bias, init_H_bias, tied_noise):\n    """"""Variational Recurrent Highway Layer (Theano implementation).\n\n    References:\n      Zilly, J, Srivastava, R, Koutnik, J, Schmidhuber, J., ""Recurrent Highway Networks"", 2016\n    Args:\n      inputs: Theano variable, shape (num_steps, batch_size, hidden_size).\n      depth: int, the number of RHN inner layers i.e. the number of micro-timesteps per timestep.\n      drop_i: float, probability of dropout over inputs.\n      drop_s: float, probability of dropout over recurrent hidden state.\n      init_T_bias: a valid bias_init argument for linear(), initialization of bias of transform gate T.\n      init_H_bias: a valid bias_init argument for linear(), initialization of bias of non-linearity H.\n      tied_noise: boolean, whether to use the same dropout masks when calculating H and when calculating T.\n    Returns:\n      y: Theano variable, recurrent hidden states at each timestep. Shape (num_steps, batch_size, hidden_size).\n      sticky_state_updates: a list of (shared variable, new shared variable value).\n    """"""\n    # We first compute the linear transformation of the inputs over all timesteps.\n    # This is done outside of scan() in order to speed up computation.\n    # The result is then fed into scan()\'s step function, one timestep at a time.\n    noise_i_for_H = self.get_dropout_noise((batch_size, hidden_size), drop_i)\n    noise_i_for_T = self.get_dropout_noise((batch_size, hidden_size), drop_i) if not tied_noise else noise_i_for_H\n\n    i_for_H = self.apply_dropout(inputs, noise_i_for_H)\n    i_for_T = self.apply_dropout(inputs, noise_i_for_T)\n\n    i_for_H = self.linear(i_for_H, in_size=hidden_size, out_size=hidden_size, bias=True, bias_init=init_H_bias)\n    i_for_T = self.linear(i_for_T, in_size=hidden_size, out_size=hidden_size, bias=True, bias_init=init_T_bias)\n\n    # Dropout noise for recurrent hidden state.\n    noise_s = self.get_dropout_noise((batch_size, hidden_size), drop_s)\n    if not tied_noise:\n      noise_s = tt.stack(noise_s, self.get_dropout_noise((batch_size, hidden_size), drop_s))\n\n\n    def step_fn(i_for_H_t, i_for_T_t, y_tm1, noise_s):\n      """"""\n      Args:\n        Elements of sequences given to scan():\n          i_for_H_t: linear trans. of inputs for calculating non-linearity H at timestep t. Shape (batch_size, hidden_size).\n          i_for_T_t: linear trans. of inputs for calculating transform gate T at timestep t. Shape (batch_size, hidden_size).\n        Result of previous step function invocation (equals the outputs_info given to scan() on first timestep):\n          y_tm1: Shape (batch_size, hidden_size).\n        Non-sequences given to scan() (these are the same at all timesteps):\n          noise_s: (batch_size, hidden_size) or (2, batch_size, hidden_size), depending on value of tied_noise.\n      """"""\n      tanh, sigm = tt.tanh, tt.nnet.sigmoid\n      noise_s_for_H = noise_s if tied_noise else noise_s[0]\n      noise_s_for_T = noise_s if tied_noise else noise_s[1]\n\n      s_lm1 = y_tm1\n      for l in range(depth):\n        s_lm1_for_H = self.apply_dropout(s_lm1, noise_s_for_H)\n        s_lm1_for_T = self.apply_dropout(s_lm1, noise_s_for_T)\n        if l == 0:\n          # On the first micro-timestep of each timestep we already have bias\n          # terms summed into i_for_H_t and into i_for_T_t.\n          H = tanh(i_for_H_t + self.linear(s_lm1_for_H, in_size=hidden_size, out_size=hidden_size, bias=False))\n          T = sigm(i_for_T_t + self.linear(s_lm1_for_T, in_size=hidden_size, out_size=hidden_size, bias=False))\n        else:\n          H = tanh(self.linear(s_lm1_for_H, in_size=hidden_size, out_size=hidden_size, bias=True, bias_init=init_H_bias))\n          T = sigm(self.linear(s_lm1_for_T, in_size=hidden_size, out_size=hidden_size, bias=True, bias_init=init_T_bias))\n        s_l = (H - s_lm1) * T + s_lm1\n        s_lm1 = s_l\n\n      y_t = s_l\n      return y_t\n\n    # The recurrent hidden state of the RHN is sticky (the last hidden state of one batch is carried over to the next batch,\n    # to be used as an initial hidden state).  These states are kept in shared variables and are reset before every epoch.\n    y_0 = theano.shared(np.zeros((batch_size, hidden_size), floatX))\n    self._sticky_hidden_states.append(y_0)\n\n    y, _ = theano.scan(step_fn,\n      sequences = [i_for_H, i_for_T],\n      outputs_info = [y_0],\n      non_sequences = [noise_s])\n\n    y_last = y[-1]\n    sticky_state_updates = [(y_0, y_last)]\n\n    return y, sticky_state_updates\n\n'"
theano_rhn_train.py,0,"b'""""""Word/Symbol level next step prediction using Recurrent Highway Networks - Theano implementation.\n\nTo run:\n$ python theano_rhn_train.py\n\nReferences:\n[1] Zilly, J, Srivastava, R, Koutnik, J, Schmidhuber, J., ""Recurrent Highway Networks"", 2016\n[2] Gal, Y, ""A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"", 2015.\n[3] Zaremba, W, Sutskever, I, Vinyals, O, ""Recurrent neural network regularization"", 2014.\n[4] Press, O, Wolf, L, ""Using the Output Embedding to Improve Language Models"", 2016.\n\nImplementation: Shimi Salant\n""""""\nfrom __future__ import absolute_import, division, print_function\n\nfrom copy import deepcopy\nimport time\nimport sys\nimport logging\n\nimport numpy as np\n\nfrom sacred import Experiment\nfrom theano_data import data_iterator, hutter_raw_data, ptb_raw_data\nfrom theano_rhn import Model\n\n\nLOG_FORMAT = \'%(asctime)s - %(message)s\'\nLOG_LEVEL = logging.INFO\n\n\nlog = logging.getLogger(\'custom_logger\')\nlog.setLevel(LOG_LEVEL)\nconsole_handler = logging.StreamHandler(sys.stdout)\nconsole_handler.setFormatter(logging.Formatter(LOG_FORMAT))\nconsole_handler.setLevel(LOG_LEVEL)\nlog.addHandler(console_handler)\n\nex = Experiment(\'theano_rhn_prediction\')\nex.logger = log\n\n\n# When running with a @named_config: values specified in @named_config override those specified in @config.\n\n@ex.config\ndef hyperparameters():\n  data_path = \'data\'\n  dataset = \'ptb\'\n  if dataset not in [\'ptb\', \'enwik8\']:\n    raise AssertionError(""Unsupported dataset! Only \'ptb\' and \'enwik8\' are currently supported."")\n  init_scale = 0.04            # uniform weight initialization values are sampled from U[-init_scale, init_scale]\n  init_T_bias = -2.0           # init scheme for the bias of the T non-linearity: \'uniform\' (random) or a fixed number\n  init_other_bias = \'uniform\'  # init scheme for all other biases (in rhn_train.py there\'s uniform initialization)\n  num_layers = 1               # number of stacked RHN layers\n  depth = 10                   # the recurrence depth within each RHN layer, i.e. number of micro-timesteps per timestep\n  learning_rate = 0.2\n  lr_decay = 1.02\n  weight_decay = 1e-7\n  max_grad_norm = 10\n  num_steps = 35\n  hidden_size = 830\n  max_epoch = 20               # number of epochs after which learning decay starts\n  max_max_epoch = 300          # total number of epochs to train for\n  batch_size = 20\n  drop_x = 0.25                # variational dropout rate over input word embeddings\n  drop_i = 0.75                # variational dropout rate over inputs of RHN layers(s), applied seperately in each RHN layer\n  drop_s = 0.25                # variational dropout rate over recurrent state\n  drop_o = 0.75                # variational dropout rate over outputs of RHN layer(s), applied before classification layer\n  tied_embeddings = True       # whether to use same embedding matrix for both input and output word embeddings\n  tied_noise = True            # whether to use same dropout masks for the T and H non-linearites (tied in rhn_train.py)\n  load_model = \'\'\n  vocab_size = 10000\n\n\n@ex.named_config\ndef ptb_sota():\n  pass\n\n\n@ex.named_config\ndef enwik8_sota():\n  dataset = \'enwik8\'\n  init_T_bias = -4.0\n  lr_decay = 1.03\n  num_steps = 50\n  hidden_size = 1500\n  max_epoch = 5\n  max_max_epoch = 500\n  batch_size = 128\n  drop_x = 0.10\n  drop_i = 0.40\n  drop_s = 0.10\n  drop_o = 0.40\n  tied_embeddings = False\n  vocab_size = 205\n\n\nclass Config:\n  pass\nC = Config()\n\n\n@ex.capture\ndef get_config(_config):\n  C.__dict__ = dict(_config)\n  return C\n\n\n@ex.capture\ndef get_logger(_log, dataset, seed):\n  """"""Returns experiment\'s logger, with an added file handler, for logging to a file as well as to console.""""""\n  file_handler = logging.FileHandler(\'./theano_rhn_\' + dataset + \'_\' + str(seed) + \'.log\')\n  file_handler.setFormatter(logging.Formatter(LOG_FORMAT))\n  file_handler.setLevel(LOG_LEVEL)\n  _log.addHandler(file_handler)\n  return _log\n\n\ndef get_raw_data(data_path, dataset):\n  if dataset == \'ptb\':\n    raw_data = ptb_raw_data(data_path)\n  elif dataset == \'enwik8\':\n    raw_data = hutter_raw_data(data_path)\n  return raw_data\n\n\ndef get_noise_x(x, drop_x):\n  """"""Get a random (variational) dropout noise matrix for input words.\n  Return value is generated by the CPU (rather than directly on the GPU, as is done for other noise matrices).\n  """"""\n  batch_size, num_steps = x.shape\n  keep_x = 1.0 - drop_x\n  if keep_x < 1.0:\n    noise_x = (np.random.random_sample((batch_size, num_steps)) < keep_x).astype(np.float32) / keep_x\n    for b in range(batch_size):\n      for n1 in range(num_steps):\n        for n2 in range(n1 + 1, num_steps):\n          if x[b][n2] == x[b][n1]:\n            noise_x[b][n2] = noise_x[b][n1]\n            break\n  else:\n    noise_x = np.ones((config.batch_size, config.num_steps), dtype=np.float32)\n  return noise_x\n\n\ndef run_epoch(m, data, config, is_train, verbose=False, log=None):\n  """"""Run the model on the given data.""""""\n  epoch_size = ((len(data) // config.batch_size) - 1) // config.num_steps\n  start_time = time.time()\n  costs = 0.0\n  iters = 0\n  m.reset_hidden_state()\n  for step, (x, y) in enumerate(data_iterator(data, config.batch_size, config.num_steps)):\n    if is_train:\n      noise_x = get_noise_x(x, config.drop_x)\n      cost = m.train(x, y, noise_x)\n    else:\n      cost = m.evaluate(x, y)\n    costs += cost\n    iters += config.num_steps\n    if verbose and step % (epoch_size // 10) == 10:\n      log.info(""%.3f perplexity: %.3f speed: %.0f wps"" % (step * 1.0 / epoch_size, np.exp(costs / iters),\n                                                       iters * config.batch_size / (time.time() - start_time)))\n  return np.exp(costs / iters)\n\n\n@ex.automain\ndef main(_run):\n\n  config = get_config()\n  log = get_logger()\n\n  from sacred.commands import _format_config  # brittle: get a string of what ex.commands[\'print_config\']() prints.\n  config_str = _format_config(_run.config, _run.config_modifications)\n  log.info(config_str)\n\n  train_data, valid_data, test_data, _ = get_raw_data(config.data_path, config.dataset)\n\n  log.info(\'Compiling (batched) model...\')\n  m = Model(config)\n  log.info(\'Done. Number of parameters: %d\' % m.num_params)\n\n  trains, vals, tests, best_val, save_path = [np.inf], [np.inf], [np.inf], np.inf, None\n\n  for i in range(config.max_max_epoch):\n    lr_decay = config.lr_decay ** max(i - config.max_epoch + 1, 0.0)\n    m.assign_lr(config.learning_rate / lr_decay)\n\n    log.info(""Epoch: %d Learning rate: %.3f"" % (i + 1, m.lr))\n\n    train_perplexity = run_epoch(m, train_data, config, is_train=True, verbose=True, log=log)\n    log.info(""Epoch: %d Train Perplexity: %.3f, Bits: %.3f"" % (i + 1, train_perplexity, np.log2(train_perplexity)))\n\n    valid_perplexity = run_epoch(m, valid_data, config, is_train=False)\n    log.info(""Epoch: %d Valid Perplexity (batched): %.3f, Bits: %.3f"" % (i + 1, valid_perplexity, np.log2(valid_perplexity)))\n\n    test_perplexity = run_epoch(m, test_data, config, is_train=False)\n    log.info(""Epoch: %d Test Perplexity (batched): %.3f, Bits: %.3f"" % (i + 1, test_perplexity, np.log2(test_perplexity)))\n\n    trains.append(train_perplexity)\n    vals.append(valid_perplexity)\n    tests.append(test_perplexity)\n\n    if valid_perplexity < best_val:\n      best_val = valid_perplexity\n      log.info(""Best Batched Valid Perplexity improved to %.03f"" % best_val)\n      save_path = \'./theano_rhn_\' + config.dataset + \'_\' + str(config.seed) + \'_best_model.pkl\'\n      m.save(save_path)\n      log.info(""Saved to: %s"" % save_path)\n\n  log.info(""Training is over."")\n  best_val_epoch = np.argmin(vals)\n  log.info(""Best Batched Validation Perplexity %.03f (Bits: %.3f) was at Epoch %d"" %\n        (vals[best_val_epoch], np.log2(vals[best_val_epoch]), best_val_epoch))\n  log.info(""Training Perplexity at this Epoch was %.03f, Bits: %.3f"" %\n        (trains[best_val_epoch], np.log2(trains[best_val_epoch])))\n  log.info(""Batched Test Perplexity at this Epoch was %.03f, Bits: %.3f"" %\n        (tests[best_val_epoch], np.log2(tests[best_val_epoch])))\n\n  non_batched_config = deepcopy(config)\n  non_batched_config.batch_size = 1\n  non_batched_config.load_model = save_path\n\n  log.info(\'Compiling (non-batched) model...\')\n  m_non_batched = Model(non_batched_config)\n  log.info(\'Done. Number of parameters: %d\' % m_non_batched.num_params)\n\n  log.info(""Testing on non-batched Valid ..."")\n  valid_perplexity = run_epoch(m_non_batched, valid_data, non_batched_config, is_train=False, verbose=True, log=log)\n  log.info(""Full Valid Perplexity: %.3f, Bits: %.3f"" % (valid_perplexity, np.log2(valid_perplexity)))\n\n  log.info(""Testing on non-batched Test ..."")\n  test_perplexity = run_epoch(m_non_batched, test_data, non_batched_config, is_train=False, verbose=True, log=log)\n  log.info(""Full Test Perplexity: %.3f, Bits: %.3f"" % (test_perplexity, np.log2(test_perplexity)))\n\n  return vals[best_val_epoch]\n\n'"
data/__init__.py,0,b''
data/create_enwik8.py,0,"b'#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import division, print_function, unicode_literals\nfrom six.moves.urllib.request import urlretrieve\nimport numpy as np\nimport zipfile\nimport h5py\nimport os\n\n\ndef convert_to_batches(serial_data, length, bs):\n    assert serial_data.size % length == 0\n    num_sequences = serial_data.size // length\n    assert num_sequences % bs == 0\n    num_batches = num_sequences // bs\n    serial_data = serial_data.reshape((bs, num_batches * length))\n    serial_data = np.vstack(np.hsplit(serial_data, num_batches)).T[:, :, None]\n    return serial_data\n\nbatch_size = 128\n# Batch size which will be used for training.\n# Needed to maintain continuity of data across batches.\nseq_len = 50\n# Number of characters in each sub-sequence.\n# Limits the number of time-steps that the gradient is back-propagated.\nnum_test_chars = 5000000\n# Number of characters which will be used for testing.\n# An equal number of characters will be used for validation.\n\nbs_data_dir = os.environ.get(\'BRAINSTORM_DATA_DIR\', \'.\')\nurl = \'http://mattmahoney.net/dc/enwik8.zip\'\nhutter_file = os.path.join(bs_data_dir, \'enwik8.zip\')\nhdf_file = os.path.join(bs_data_dir, \'HutterPrize_Torch.hdf5\')\n\nprint(""Using data directory:"", bs_data_dir)\nif not os.path.exists(hutter_file):\n    print(""Downloading Hutter Prize data ..."")\n    urlretrieve(url, hutter_file)\n    print(""Done."")\n\nprint(""Extracting Hutter Prize data ..."")\nraw_data = zipfile.ZipFile(hutter_file).read(\'enwik8\')\nprint(""Done."")\n\nprint(""Preparing data for Brainstorm ..."")\nraw_data = np.fromstring(raw_data, dtype=np.uint8)\nunique, data = np.unique(raw_data, return_inverse=True)\n\nprint(""Vocabulary size:"", unique.shape)\ntrain_data = data[: -2 * num_test_chars]\nvalid_data = data[-2 * num_test_chars: -num_test_chars]\ntest_data = data[-num_test_chars:]\n\nprint(""Done."")\n\nprint(""Creating Hutter Prize character-level HDF5 dataset ..."")\nf = h5py.File(hdf_file, \'w\')\ndescription = """"""\nThe Hutter Prize Wikipedia dataset, prepared for character-level language\nmodeling.\n\nThe data was obtained from the link:\nhttp://mattmahoney.net/dc/enwik8.zip\n\nAttributes\n==========\n\ndescription: This description.\n\nunique: A 1-D array of unique characters (0-255 ASCII values) in the dataset.\nThe index of each character was used as the class ID for preparing the data.\n\nVariants\n========\n\nsplit: Split into \'training\', \'validation\' and \'test\' tests of size 90, 5 and\n5 million characters respectively. Each sequence is {} characters long. The\ndataset has been prepared expecting minibatches of {} sequences.\n"""""".format(seq_len, batch_size)\nf.attrs[\'description\'] = description\nf.attrs[\'unique\'] = unique\n\nvariant = f.create_group(\'split\')\ngroup = variant.create_group(\'training\')\ngroup.create_dataset(name=\'default\', data=train_data, compression=\'gzip\')\n\ngroup = variant.create_group(\'validation\')\ngroup.create_dataset(name=\'default\', data=valid_data, compression=\'gzip\')\n\ngroup = variant.create_group(\'test\')\ngroup.create_dataset(name=\'default\', data=test_data, compression=\'gzip\')\n\nf.close()\nprint(""Done."")\n'"
data/create_hutter_enwik8.py,0,"b'#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import division, print_function, unicode_literals\nfrom six.moves.urllib.request import urlretrieve\nimport numpy as np\nimport zipfile\nimport h5py\nimport os\n\n\ndef convert_to_batches(serial_data, length, bs):\n    assert serial_data.size % length == 0\n    num_sequences = serial_data.size // length\n    assert num_sequences % bs == 0\n    num_batches = num_sequences // bs\n    serial_data = serial_data.reshape((bs, num_batches * length))\n    serial_data = np.vstack(np.hsplit(serial_data, num_batches)).T[:, :, None]\n    return serial_data\n\nbatch_size = 100\n# Batch size which will be used for training.\n# Needed to maintain continuity of data across batches.\nseq_len = 50\n# Number of characters in each sub-sequence.\n# Limits the number of time-steps that the gradient is back-propagated.\nnum_test_chars = 5000000\n# Number of characters which will be used for testing.\n# An equal number of characters will be used for validation.\n\nbs_data_dir = os.environ.get(\'BRAINSTORM_DATA_DIR\', \'.\')\nurl = \'http://mattmahoney.net/dc/enwik8.zip\'\nhutter_file = os.path.join(bs_data_dir, \'enwik8.zip\')\nhdf_file = os.path.join(bs_data_dir, \'HutterPrize_Torch.hdf5\')\n\nprint(""Using data directory:"", bs_data_dir)\nif not os.path.exists(hutter_file):\n    print(""Downloading Hutter Prize data ..."")\n    urlretrieve(url, hutter_file)\n    print(""Done."")\n\nprint(""Extracting Hutter Prize data ..."")\nraw_data = zipfile.ZipFile(hutter_file).read(\'enwik8\')\nprint(""Done."")\n\nprint(""Preparing data for Brainstorm ..."")\nraw_data = np.fromstring(raw_data, dtype=np.uint8)\nunique, data = np.unique(raw_data, return_inverse=True)\n\nprint(""Vocabulary size:"", unique.shape)\ntrain_data = data[: -2 * num_test_chars]\nvalid_data = data[-2 * num_test_chars: -num_test_chars]\ntest_data = data[-num_test_chars:]\n\nprint(""Done."")\n\nprint(""Creating Hutter Prize character-level HDF5 dataset ..."")\nf = h5py.File(hdf_file, \'w\')\ndescription = """"""\nThe Hutter Prize Wikipedia dataset, prepared for character-level language\nmodeling.\n\nThe data was obtained from the link:\nhttp://mattmahoney.net/dc/enwik8.zip\n\nAttributes\n==========\n\ndescription: This description.\n\nunique: A 1-D array of unique characters (0-255 ASCII values) in the dataset.\nThe index of each character was used as the class ID for preparing the data.\n\nVariants\n========\n\nsplit: Split into \'training\', \'validation\' and \'test\' tests of size 90, 5 and\n5 million characters respectively. Each sequence is {} characters long. The\ndataset has been prepared expecting minibatches of {} sequences.\n"""""".format(seq_len, batch_size)\nf.attrs[\'description\'] = description\nf.attrs[\'unique\'] = unique\n\nvariant = f.create_group(\'split\')\ngroup = variant.create_group(\'training\')\ngroup.create_dataset(name=\'default\', data=train_data, compression=\'gzip\')\n\ngroup = variant.create_group(\'validation\')\ngroup.create_dataset(name=\'default\', data=valid_data, compression=\'gzip\')\n\ngroup = variant.create_group(\'test\')\ngroup.create_dataset(name=\'default\', data=test_data, compression=\'gzip\')\n\nf.close()\nprint(""Done."")\n'"
data/create_text8.py,0,"b'#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import division, print_function, unicode_literals\nfrom six.moves.urllib.request import urlretrieve\nimport numpy as np\nimport zipfile\nimport h5py\nimport os\n\n\ndef convert_to_batches(serial_data, length, bs):\n    assert serial_data.size % length == 0\n    num_sequences = serial_data.size // length\n    assert num_sequences % bs == 0\n    num_batches = num_sequences // bs\n    serial_data = serial_data.reshape((bs, num_batches * length))\n    serial_data = np.vstack(np.hsplit(serial_data, num_batches)).T[:, :, None]\n    return serial_data\n\nbatch_size = 128\n# Batch size which will be used for training.\n# Needed to maintain continuity of data across batches.\nseq_len = 50\n# Number of characters in each sub-sequence.\n# Limits the number of time-steps that the gradient is back-propagated.\nnum_test_chars = 5000000\n# Number of characters which will be used for testing.\n# An equal number of characters will be used for validation.\n\nbs_data_dir = os.environ.get(\'BRAINSTORM_DATA_DIR\', \'.\')\nurl = \'http://mattmahoney.net/dc/text8.zip\'\ntext8_file = os.path.join(bs_data_dir, \'text8.zip\')\nhdf_file = os.path.join(bs_data_dir, \'Text8_Torch.hdf5\')\n\nprint(""Using data directory:"", bs_data_dir)\nif not os.path.exists(text8_file):\n    print(""Downloading Text8 data ..."")\n    urlretrieve(url, text8_file)\n    print(""Done."")\n\nprint(""Extracting Text8 data ..."")\nraw_data = zipfile.ZipFile(text8_file).read(\'text8\')\nprint(""Done."")\n\nprint(""Preparing data for Brainstorm ..."")\nraw_data = np.fromstring(raw_data, dtype=np.uint8)\nunique, data = np.unique(raw_data, return_inverse=True)\n\nprint(""Vocabulary size:"", unique.shape)\ntrain_data = data[: -2 * num_test_chars]\nvalid_data = data[-2 * num_test_chars: -num_test_chars]\ntest_data = data[-num_test_chars:]\n\nprint(""Done."")\n\nprint(""Creating Text8 character-level HDF5 dataset ..."")\nf = h5py.File(hdf_file, \'w\')\ndescription = """"""\nThe Text8 Wikipedia dataset, prepared for character-level language\nmodeling.\n\nThe data was obtained from the link:\nhttp://mattmahoney.net/dc/text8.zip\n\nVariants\n========\n\nsplit: Split into \'training\', \'validation\' and \'test\' tests of size 90, 5 and\n5 million characters respectively. Each sequence is {} characters long. The\ndataset has been prepared expecting minibatches of {} sequences.\n"""""".format(seq_len, batch_size)\nf.attrs[\'description\'] = description\nf.attrs[\'unique\'] = unique\n\nvariant = f.create_group(\'split\')\ngroup = variant.create_group(\'training\')\ngroup.create_dataset(name=\'default\', data=train_data, compression=\'gzip\')\n\ngroup = variant.create_group(\'validation\')\ngroup.create_dataset(name=\'default\', data=valid_data, compression=\'gzip\')\n\ngroup = variant.create_group(\'test\')\ngroup.create_dataset(name=\'default\', data=test_data, compression=\'gzip\')\n\nf.close()\nprint(""Done."")\n'"
data/reader.py,1,"b'# This file is adapted from the tool provided with Tensorflow for\n# reading the Penn Treebank dataset. The original copyright notice is\n# provided below.\n#\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Utilities for training on the Hutter Prize dataset.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef _read_symbols(filename):\n  with tf.gfile.GFile(filename, ""r"") as f:\n    return f.read()\n\n\ndef enwik8_raw_data(data_path=None, num_test_symbols=5000000):\n  """"""Load raw data from data directory ""data_path"".\n\n  The raw Hutter prize data is at:\n  http://mattmahoney.net/dc/enwik8.zip\n\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n    num_test_symbols: number of symbols at the end that make up the test set\n\n  Returns:\n    tuple (train_data, valid_data, test_data, unique)\n    where each of the data objects can be passed to hutter_iterator.\n  """"""\n\n  data_path = os.path.join(data_path, ""enwik8"")\n\n  raw_data = _read_symbols(data_path)\n  raw_data = np.fromstring(raw_data, dtype=np.uint8)\n  unique, data = np.unique(raw_data, return_inverse=True)\n  train_data = data[: -2 * num_test_symbols]\n  valid_data = data[-2 * num_test_symbols: -num_test_symbols]\n  test_data = data[-num_test_symbols:]\n  return train_data, valid_data, test_data, unique\n\n\ndef text8_raw_data(data_path=None, num_test_symbols=5000000):\n  """"""Load raw data from data directory ""data_path"".\n\n  The raw text8 data is at:\n  http://mattmahoney.net/dc/text8.zip\n\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n    num_test_symbols: number of symbols at the end that make up the test set\n\n  Returns:\n    tuple (train_data, valid_data, test_data, unique)\n    where each of the data objects can be passed to text8_iterator.\n  """"""\n\n  data_path = os.path.join(data_path, ""text8"")\n\n  raw_data = _read_symbols(data_path)\n  raw_data = np.fromstring(raw_data, dtype=np.uint8)\n  unique, data = np.unique(raw_data, return_inverse=True)\n  train_data = data[: -2 * num_test_symbols]\n  valid_data = data[-2 * num_test_symbols: -num_test_symbols]\n  test_data = data[-num_test_symbols:]\n  return train_data, valid_data, test_data, unique\n\n\ndef data_iterator(raw_data, batch_size, num_steps):\n  """"""Iterate on the raw Hutter prize data.\n\n  This generates batch_size pointers into the raw Hutter Prize data, and allows\n  minibatch iteration along these pointers.\n\n  Args:\n    raw_data: one of the raw data outputs from ptb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n\n  Yields:\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n    The second element of the tuple is the same data time-shifted to the\n    right by one.\n\n  Raises:\n    ValueError: if batch_size or num_steps are too high.\n  """"""\n  raw_data = np.array(raw_data, dtype=np.int32)\n\n  data_len = len(raw_data)\n  batch_len = data_len // batch_size\n  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n  for i in range(batch_size):\n    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n\n  epoch_size = (batch_len - 1) // num_steps\n\n  if epoch_size == 0:\n    raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n  for i in range(epoch_size):\n    x = data[:, i*num_steps:(i+1)*num_steps]\n    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n    yield (x, y)\n'"
