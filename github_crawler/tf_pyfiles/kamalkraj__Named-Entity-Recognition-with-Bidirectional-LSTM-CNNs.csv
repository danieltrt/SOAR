file_path,api_count,code
ner.py,0,"b'import numpy as np\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\nfrom keras.models import load_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk import word_tokenize\n\nclass Parser:\n\n    def __init__(self):\n        # ::Hard coded char lookup ::\n        self.char2Idx = {""PADDING"":0, ""UNKNOWN"":1}\n        for c in "" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#\'\\""/\\\\%$`&=*+@^~|"":\n            self.char2Idx[c] = len(self.char2Idx)\n        # :: Hard coded case lookup ::\n        self.case2Idx = {\'numeric\': 0, \'allLower\':1, \'allUpper\':2, \'initialUpper\':3, \'other\':4, \'mainly_numeric\':5, \'contains_digit\': 6, \'PADDING_TOKEN\':7}\n\n    def load_models(self, loc=None):\n        if not loc:\n            loc = os.path.join(os.path.expanduser(\'~\'), \'.ner_model\')\n        self.model = load_model(os.path.join(loc,""model.h5""))\n        # loading word2Idx\n        self.word2Idx = np.load(os.path.join(loc,""word2Idx.npy"")).item()\n        # loading idx2Label\n        self.idx2Label = np.load(os.path.join(loc,""idx2Label.npy"")).item()\n\n    def getCasing(self,word, caseLookup):   \n        casing = \'other\'\n        \n        numDigits = 0\n        for char in word:\n            if char.isdigit():\n                numDigits += 1\n                \n        digitFraction = numDigits / float(len(word))\n        \n        if word.isdigit(): #Is a digit\n            casing = \'numeric\'\n        elif digitFraction > 0.5:\n            casing = \'mainly_numeric\'\n        elif word.islower(): #All lower case\n            casing = \'allLower\'\n        elif word.isupper(): #All upper case\n            casing = \'allUpper\'\n        elif word[0].isupper(): #is a title, initial char upper, then all lower\n            casing = \'initialUpper\'\n        elif numDigits > 0:\n            casing = \'contains_digit\'  \n        return caseLookup[casing]\n\n    def createTensor(self,sentence, word2Idx,case2Idx,char2Idx):\n        unknownIdx = word2Idx[\'UNKNOWN_TOKEN\']\n    \n        wordIndices = []    \n        caseIndices = []\n        charIndices = []\n            \n        for word,char in sentence:  \n            word = str(word)\n            if word in word2Idx:\n                wordIdx = word2Idx[word]\n            elif word.lower() in word2Idx:\n                wordIdx = word2Idx[word.lower()]                 \n            else:\n                wordIdx = unknownIdx\n            charIdx = []\n            for x in char:\n                if x in char2Idx.keys():\n                    charIdx.append(char2Idx[x])\n                else:\n                    charIdx.append(char2Idx[\'UNKNOWN\'])   \n            wordIndices.append(wordIdx)\n            caseIndices.append(self.getCasing(word, case2Idx))\n            charIndices.append(charIdx)\n            \n        return [wordIndices, caseIndices, charIndices]\n\n    def addCharInformation(self, sentence):\n        return [[word, list(str(word))] for word in sentence]\n\n    def padding(self,Sentence):\n        Sentence[2] = pad_sequences(Sentence[2],52,padding=\'post\')\n        return Sentence\n\n    def predict(self,Sentence):\n        Sentence = words =  word_tokenize(Sentence)\n        Sentence = self.addCharInformation(Sentence)\n        Sentence = self.padding(self.createTensor(Sentence,self.word2Idx,self.case2Idx,self.char2Idx))\n        tokens, casing,char = Sentence\n        tokens = np.asarray([tokens])     \n        casing = np.asarray([casing])\n        char = np.asarray([char])\n        pred = self.model.predict([tokens, casing,char], verbose=False)[0]   \n        pred = pred.argmax(axis=-1)\n        pred = [self.idx2Label[x].strip() for x in pred]\n        return list(zip(words,pred))'"
nn.py,0,"b'import numpy as np \nfrom validation import compute_f1\nfrom keras.models import Model\nfrom keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\nfrom prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\nfrom keras.utils import Progbar\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import RandomUniform\n\nepochs = 50\n\ndef tag_dataset(dataset):\n    correctLabels = []\n    predLabels = []\n    b = Progbar(len(dataset))\n    for i,data in enumerate(dataset):    \n        tokens, casing,char, labels = data\n        tokens = np.asarray([tokens])     \n        casing = np.asarray([casing])\n        char = np.asarray([char])\n        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n        pred = pred.argmax(axis=-1) #Predict the classes            \n        correctLabels.append(labels)\n        predLabels.append(pred)\n        b.update(i)\n    b.update(i+1)\n    return predLabels, correctLabels\n\n\ntrainSentences = readfile(""data/train.txt"")\ndevSentences = readfile(""data/valid.txt"")\ntestSentences = readfile(""data/test.txt"")\n\ntrainSentences = addCharInformatioin(trainSentences)\ndevSentences = addCharInformatioin(devSentences)\ntestSentences = addCharInformatioin(testSentences)\n\nlabelSet = set()\nwords = {}\n\nfor dataset in [trainSentences, devSentences, testSentences]:\n    for sentence in dataset:\n        for token,char,label in sentence:\n            labelSet.add(label)\n            words[token.lower()] = True\n\n# :: Create a mapping for the labels ::\nlabel2Idx = {}\nfor label in labelSet:\n    label2Idx[label] = len(label2Idx)\n\n# :: Hard coded case lookup ::\ncase2Idx = {\'numeric\': 0, \'allLower\':1, \'allUpper\':2, \'initialUpper\':3, \'other\':4, \'mainly_numeric\':5, \'contains_digit\': 6, \'PADDING_TOKEN\':7}\ncaseEmbeddings = np.identity(len(case2Idx), dtype=\'float32\')\n\n\n# :: Read in word embeddings ::\nword2Idx = {}\nwordEmbeddings = []\n\nfEmbeddings = open(""embeddings/glove.6B.100d.txt"", encoding=""utf-8"")\n\nfor line in fEmbeddings:\n    split = line.strip().split("" "")\n    word = split[0]\n    \n    if len(word2Idx) == 0: #Add padding+unknown\n        word2Idx[""PADDING_TOKEN""] = len(word2Idx)\n        vector = np.zeros(len(split)-1) #Zero vector vor \'PADDING\' word\n        wordEmbeddings.append(vector)\n        \n        word2Idx[""UNKNOWN_TOKEN""] = len(word2Idx)\n        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n        wordEmbeddings.append(vector)\n\n    if split[0].lower() in words:\n        vector = np.array([float(num) for num in split[1:]])\n        wordEmbeddings.append(vector)\n        word2Idx[split[0]] = len(word2Idx)\n        \nwordEmbeddings = np.array(wordEmbeddings)\n\nchar2Idx = {""PADDING"":0, ""UNKNOWN"":1}\nfor c in "" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#\'\\""/\\\\%$`&=*+@^~|"":\n    char2Idx[c] = len(char2Idx)\n\ntrain_set = padding(createMatrices(trainSentences,word2Idx,  label2Idx, case2Idx,char2Idx))\ndev_set = padding(createMatrices(devSentences,word2Idx, label2Idx, case2Idx,char2Idx))\ntest_set = padding(createMatrices(testSentences, word2Idx, label2Idx, case2Idx,char2Idx))\n\nidx2Label = {v: k for k, v in label2Idx.items()}\nnp.save(""models/idx2Label.npy"",idx2Label)\nnp.save(""models/word2Idx.npy"",word2Idx)\n\ntrain_batch,train_batch_len = createBatches(train_set)\ndev_batch,dev_batch_len = createBatches(dev_set)\ntest_batch,test_batch_len = createBatches(test_set)\n\n\nwords_input = Input(shape=(None,),dtype=\'int32\',name=\'words_input\')\nwords = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)\ncasing_input = Input(shape=(None,), dtype=\'int32\', name=\'casing_input\')\ncasing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\ncharacter_input=Input(shape=(None,52,),name=\'char_input\')\nembed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\'char_embedding\')(character_input)\ndropout= Dropout(0.5)(embed_char_out)\nconv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding=\'same\',activation=\'tanh\', strides=1))(dropout)\nmaxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\nchar = TimeDistributed(Flatten())(maxpool_out)\nchar = Dropout(0.5)(char)\noutput = concatenate([words, casing,char])\noutput = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\noutput = TimeDistributed(Dense(len(label2Idx), activation=\'softmax\'))(output)\nmodel = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\nmodel.compile(loss=\'sparse_categorical_crossentropy\', optimizer=\'nadam\')\nmodel.summary()\n# plot_model(model, to_file=\'model.png\')\n\n\nfor epoch in range(epochs):    \n    print(""Epoch %d/%d""%(epoch,epochs))\n    a = Progbar(len(train_batch_len))\n    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n        labels, tokens, casing,char = batch       \n        model.train_on_batch([tokens, casing,char], labels)\n        a.update(i)\n    a.update(i+1)\n    print(\' \')\n\nmodel.save(""models/model.h5"")\n\n#   Performance on dev dataset        \npredLabels, correctLabels = tag_dataset(dev_batch)        \npre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, idx2Label)\nprint(""Dev-Data: Prec: %.3f, Rec: %.3f, F1: %.3f"" % (pre_dev, rec_dev, f1_dev))\n    \n#   Performance on test dataset       \npredLabels, correctLabels = tag_dataset(test_batch)        \npre_test, rec_test, f1_test= compute_f1(predLabels, correctLabels, idx2Label)\nprint(""Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f"" % (pre_test, rec_test, f1_test))'"
prepro.py,0,"b'import numpy as np\nimport random\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef readfile(filename):\n    \'\'\'\n    read file\n    return format :\n    [ [\'EU\', \'B-ORG\'], [\'rejects\', \'O\'], [\'German\', \'B-MISC\'], [\'call\', \'O\'], [\'to\', \'O\'], [\'boycott\', \'O\'], [\'British\', \'B-MISC\'], [\'lamb\', \'O\'], [\'.\', \'O\'] ]\n    \'\'\'\n    f = open(filename)\n    sentences = []\n    sentence = []\n    for line in f:\n        if len(line)==0 or line.startswith(\'-DOCSTART\') or line[0]==""\\n"":\n            if len(sentence) > 0:\n                sentences.append(sentence)\n                sentence = []\n            continue\n        splits = line.split(\' \')\n        sentence.append([splits[0],splits[-1]])\n\n    if len(sentence) >0:\n        sentences.append(sentence)\n        sentence = []\n    return sentences\n\ndef getCasing(word, caseLookup):   \n    casing = \'other\'\n    \n    numDigits = 0\n    for char in word:\n        if char.isdigit():\n            numDigits += 1\n            \n    digitFraction = numDigits / float(len(word))\n    \n    if word.isdigit(): #Is a digit\n        casing = \'numeric\'\n    elif digitFraction > 0.5:\n        casing = \'mainly_numeric\'\n    elif word.islower(): #All lower case\n        casing = \'allLower\'\n    elif word.isupper(): #All upper case\n        casing = \'allUpper\'\n    elif word[0].isupper(): #is a title, initial char upper, then all lower\n        casing = \'initialUpper\'\n    elif numDigits > 0:\n        casing = \'contains_digit\'\n    \n   \n    return caseLookup[casing]\n    \n\ndef createBatches(data):\n    l = []\n    for i in data:\n        l.append(len(i[0]))\n    l = set(l)\n    batches = []\n    batch_len = []\n    z = 0\n    for i in l:\n        for batch in data:\n            if len(batch[0]) == i:\n                batches.append(batch)\n                z += 1\n        batch_len.append(z)\n    return batches,batch_len\n\ndef createBatches(data):\n    l = []\n    for i in data:\n        l.append(len(i[0]))\n    l = set(l)\n    batches = []\n    batch_len = []\n    z = 0\n    for i in l:\n        for batch in data:\n            if len(batch[0]) == i:\n                batches.append(batch)\n                z += 1\n        batch_len.append(z)\n    return batches,batch_len\n\ndef createMatrices(sentences, word2Idx, label2Idx, case2Idx,char2Idx):\n    unknownIdx = word2Idx[\'UNKNOWN_TOKEN\']\n    paddingIdx = word2Idx[\'PADDING_TOKEN\']    \n        \n    dataset = []\n    \n    wordCount = 0\n    unknownWordCount = 0\n    \n    for sentence in sentences:\n        wordIndices = []    \n        caseIndices = []\n        charIndices = []\n        labelIndices = []\n        \n        for word,char,label in sentence:  \n            wordCount += 1\n            if word in word2Idx:\n                wordIdx = word2Idx[word]\n            elif word.lower() in word2Idx:\n                wordIdx = word2Idx[word.lower()]                 \n            else:\n                wordIdx = unknownIdx\n                unknownWordCount += 1\n            charIdx = []\n            for x in char:\n                charIdx.append(char2Idx[x])\n            #Get the label and map to int            \n            wordIndices.append(wordIdx)\n            caseIndices.append(getCasing(word, case2Idx))\n            charIndices.append(charIdx)\n            labelIndices.append(label2Idx[label])\n           \n        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n        \n    return dataset\n\ndef iterate_minibatches(dataset,batch_len): \n    start = 0\n    for i in batch_len:\n        tokens = []\n        caseing = []\n        char = []\n        labels = []\n        data = dataset[start:i]\n        start = i\n        for dt in data:\n            t,c,ch,l = dt\n            l = np.expand_dims(l,-1)\n            tokens.append(t)\n            caseing.append(c)\n            char.append(ch)\n            labels.append(l)\n        yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)\n\ndef addCharInformatioin(Sentences):\n    for i,sentence in enumerate(Sentences):\n        for j,data in enumerate(sentence):\n            chars = [c for c in data[0]]\n            Sentences[i][j] = [data[0],chars,data[1]]\n    return Sentences\n\ndef padding(Sentences):\n    maxlen = 52\n    for sentence in Sentences:\n        char = sentence[2]\n        for x in char:\n            maxlen = max(maxlen,len(x))\n    for i,sentence in enumerate(Sentences):\n        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding=\'post\')\n    return Sentences\n'"
validation.py,0,"b""import numpy as np\n\n\n#Method to compute the accruarcy. Call predict_labels to get the labels for the dataset\ndef compute_f1(predictions, correct, idx2Label): \n    label_pred = []    \n    for sentence in predictions:\n        label_pred.append([idx2Label[element] for element in sentence])\n        \n    label_correct = []    \n    for sentence in correct:\n        label_correct.append([idx2Label[element] for element in sentence])\n            \n    \n    #print label_pred\n    #print label_correct\n    \n    prec = compute_precision(label_pred, label_correct)\n    rec = compute_precision(label_correct, label_pred)\n    \n    f1 = 0\n    if (rec+prec) > 0:\n        f1 = 2.0 * prec * rec / (prec + rec);\n        \n    return prec, rec, f1\n\ndef compute_precision(guessed_sentences, correct_sentences):\n    assert(len(guessed_sentences) == len(correct_sentences))\n    correctCount = 0\n    count = 0\n    \n    \n    for sentenceIdx in range(len(guessed_sentences)):\n        guessed = guessed_sentences[sentenceIdx]\n        correct = correct_sentences[sentenceIdx]\n        assert(len(guessed) == len(correct))\n        idx = 0\n        while idx < len(guessed):\n            if guessed[idx][0] == 'B': #A new chunk starts\n                count += 1\n                \n                if guessed[idx] == correct[idx]:\n                    idx += 1\n                    correctlyFound = True\n                    \n                    while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n                        if guessed[idx] != correct[idx]:\n                            correctlyFound = False\n                        \n                        idx += 1\n                    \n                    if idx < len(guessed):\n                        if correct[idx][0] == 'I': #The chunk in correct was longer\n                            correctlyFound = False\n                        \n                    \n                    if correctlyFound:\n                        correctCount += 1\n                else:\n                    idx += 1\n            else:  \n                idx += 1\n    \n    precision = 0\n    if count > 0:    \n        precision = float(correctCount) / count\n        \n    return precision"""
