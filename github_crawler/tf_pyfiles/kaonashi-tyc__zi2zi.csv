file_path,api_count,code
export.py,3,"b""# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport argparse\nfrom model.unet import UNet\n\nparser = argparse.ArgumentParser(description='Export generator weights from the checkpoint file')\nparser.add_argument('--model_dir', dest='model_dir', required=True,\n                    help='directory that saves the model checkpoints')\nparser.add_argument('--batch_size', dest='batch_size', type=int, default=16, help='number of examples in batch')\nparser.add_argument('--inst_norm', dest='inst_norm', type=bool, default=False,\n                    help='use conditional instance normalization in your model')\nparser.add_argument('--save_dir', default='save_dir', type=str, help='path to save inferred images')\nargs = parser.parse_args()\n\n\ndef main(_):\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        model = UNet(batch_size=args.batch_size)\n        model.register_session(sess)\n        model.build_model(is_training=False, inst_norm=args.inst_norm)\n        model.export_generator(save_dir=args.save_dir, model_dir=args.model_dir)\n\n\nif __name__ == '__main__':\n    tf.app.run()\n"""
font2img.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport argparse\nimport sys\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nimport json\nimport collections\n\nreload(sys)\nsys.setdefaultencoding(""utf-8"")\n\nCN_CHARSET = None\nCN_T_CHARSET = None\nJP_CHARSET = None\nKR_CHARSET = None\n\nDEFAULT_CHARSET = ""./charset/cjk.json""\n\n\ndef load_global_charset():\n    global CN_CHARSET, JP_CHARSET, KR_CHARSET, CN_T_CHARSET\n    cjk = json.load(open(DEFAULT_CHARSET))\n    CN_CHARSET = cjk[""gbk""]\n    JP_CHARSET = cjk[""jp""]\n    KR_CHARSET = cjk[""kr""]\n    CN_T_CHARSET = cjk[""gb2312_t""]\n\n\ndef draw_single_char(ch, font, canvas_size, x_offset, y_offset):\n    img = Image.new(""RGB"", (canvas_size, canvas_size), (255, 255, 255))\n    draw = ImageDraw.Draw(img)\n    draw.text((x_offset, y_offset), ch, (0, 0, 0), font=font)\n    return img\n\n\ndef draw_example(ch, src_font, dst_font, canvas_size, x_offset, y_offset, filter_hashes):\n    dst_img = draw_single_char(ch, dst_font, canvas_size, x_offset, y_offset)\n    # check the filter example in the hashes or not\n    dst_hash = hash(dst_img.tobytes())\n    if dst_hash in filter_hashes:\n        return None\n    src_img = draw_single_char(ch, src_font, canvas_size, x_offset, y_offset)\n    example_img = Image.new(""RGB"", (canvas_size * 2, canvas_size), (255, 255, 255))\n    example_img.paste(dst_img, (0, 0))\n    example_img.paste(src_img, (canvas_size, 0))\n    return example_img\n\n\ndef filter_recurring_hash(charset, font, canvas_size, x_offset, y_offset):\n    """""" Some characters are missing in a given font, filter them\n    by checking the recurring hashes\n    """"""\n    _charset = charset[:]\n    np.random.shuffle(_charset)\n    sample = _charset[:2000]\n    hash_count = collections.defaultdict(int)\n    for c in sample:\n        img = draw_single_char(c, font, canvas_size, x_offset, y_offset)\n        hash_count[hash(img.tobytes())] += 1\n    recurring_hashes = filter(lambda d: d[1] > 2, hash_count.items())\n    return [rh[0] for rh in recurring_hashes]\n\n\ndef font2img(src, dst, charset, char_size, canvas_size,\n             x_offset, y_offset, sample_count, sample_dir, label=0, filter_by_hash=True):\n    src_font = ImageFont.truetype(src, size=char_size)\n    dst_font = ImageFont.truetype(dst, size=char_size)\n\n    filter_hashes = set()\n    if filter_by_hash:\n        filter_hashes = set(filter_recurring_hash(charset, dst_font, canvas_size, x_offset, y_offset))\n        print(""filter hashes -> %s"" % ("","".join([str(h) for h in filter_hashes])))\n\n    count = 0\n\n    for c in charset:\n        if count == sample_count:\n            break\n        e = draw_example(c, src_font, dst_font, canvas_size, x_offset, y_offset, filter_hashes)\n        if e:\n            e.save(os.path.join(sample_dir, ""%d_%04d.jpg"" % (label, count)))\n            count += 1\n            if count % 100 == 0:\n                print(""processed %d chars"" % count)\n\n\nload_global_charset()\nparser = argparse.ArgumentParser(description=\'Convert font to images\')\nparser.add_argument(\'--src_font\', dest=\'src_font\', required=True, help=\'path of the source font\')\nparser.add_argument(\'--dst_font\', dest=\'dst_font\', required=True, help=\'path of the target font\')\nparser.add_argument(\'--filter\', dest=\'filter\', type=int, default=0, help=\'filter recurring characters\')\nparser.add_argument(\'--charset\', dest=\'charset\', type=str, default=\'CN\',\n                    help=\'charset, can be either: CN, JP, KR or a one line file\')\nparser.add_argument(\'--shuffle\', dest=\'shuffle\', type=int, default=0, help=\'shuffle a charset before processings\')\nparser.add_argument(\'--char_size\', dest=\'char_size\', type=int, default=150, help=\'character size\')\nparser.add_argument(\'--canvas_size\', dest=\'canvas_size\', type=int, default=256, help=\'canvas size\')\nparser.add_argument(\'--x_offset\', dest=\'x_offset\', type=int, default=20, help=\'x offset\')\nparser.add_argument(\'--y_offset\', dest=\'y_offset\', type=int, default=20, help=\'y_offset\')\nparser.add_argument(\'--sample_count\', dest=\'sample_count\', type=int, default=1000, help=\'number of characters to draw\')\nparser.add_argument(\'--sample_dir\', dest=\'sample_dir\', help=\'directory to save examples\')\nparser.add_argument(\'--label\', dest=\'label\', type=int, default=0, help=\'label as the prefix of examples\')\n\nargs = parser.parse_args()\n\nif __name__ == ""__main__"":\n    if args.charset in [\'CN\', \'JP\', \'KR\', \'CN_T\']:\n        charset = locals().get(""%s_CHARSET"" % args.charset)\n    else:\n        charset = [c for c in open(args.charset).readline()[:-1].decode(""utf-8"")]\n    if args.shuffle:\n        np.random.shuffle(charset)\n    font2img(args.src_font, args.dst_font, charset, args.char_size,\n             args.canvas_size, args.x_offset, args.y_offset,\n             args.sample_count, args.sample_dir, args.label, args.filter)\n'"
infer.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport os\nimport argparse\nfrom model.unet import UNet\nfrom model.utils import compile_frames_to_gif\n\n""""""\nPeople are made to have fun and be \xe4\xb8\xad\xe4\xba\x8c sometimes\n                                --Bored Yan LeCun\n""""""\n\nparser = argparse.ArgumentParser(description=\'Inference for unseen data\')\nparser.add_argument(\'--model_dir\', dest=\'model_dir\', required=True,\n                    help=\'directory that saves the model checkpoints\')\nparser.add_argument(\'--batch_size\', dest=\'batch_size\', type=int, default=16, help=\'number of examples in batch\')\nparser.add_argument(\'--source_obj\', dest=\'source_obj\', type=str, required=True, help=\'the source images for inference\')\nparser.add_argument(\'--embedding_ids\', default=\'embedding_ids\', type=str, help=\'embeddings involved\')\nparser.add_argument(\'--save_dir\', default=\'save_dir\', type=str, help=\'path to save inferred images\')\nparser.add_argument(\'--inst_norm\', dest=\'inst_norm\', type=int, default=0,\n                    help=\'use conditional instance normalization in your model\')\nparser.add_argument(\'--interpolate\', dest=\'interpolate\', type=int, default=0,\n                    help=\'interpolate between different embedding vectors\')\nparser.add_argument(\'--steps\', dest=\'steps\', type=int, default=10, help=\'interpolation steps in between vectors\')\nparser.add_argument(\'--output_gif\', dest=\'output_gif\', type=str, default=None, help=\'output name transition gif\')\nparser.add_argument(\'--uroboros\', dest=\'uroboros\', type=int, default=0,\n                    help=\'Sh\xc5\x8dnen yo, you have stepped into uncharted territory\')\nargs = parser.parse_args()\n\n\ndef main(_):\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        model = UNet(batch_size=args.batch_size)\n        model.register_session(sess)\n        model.build_model(is_training=False, inst_norm=args.inst_norm)\n        embedding_ids = [int(i) for i in args.embedding_ids.split("","")]\n        if not args.interpolate:\n            if len(embedding_ids) == 1:\n                embedding_ids = embedding_ids[0]\n            model.infer(model_dir=args.model_dir, source_obj=args.source_obj, embedding_ids=embedding_ids,\n                        save_dir=args.save_dir)\n        else:\n            if len(embedding_ids) < 2:\n                raise Exception(""no need to interpolate yourself unless you are a narcissist"")\n            chains = embedding_ids[:]\n            if args.uroboros:\n                chains.append(chains[0])\n            pairs = list()\n            for i in range(len(chains) - 1):\n                pairs.append((chains[i], chains[i + 1]))\n            for s, e in pairs:\n                model.interpolate(model_dir=args.model_dir, source_obj=args.source_obj, between=[s, e],\n                                  save_dir=args.save_dir, steps=args.steps)\n            if args.output_gif:\n                gif_path = os.path.join(args.save_dir, args.output_gif)\n                compile_frames_to_gif(args.save_dir, gif_path)\n                print(""gif saved at %s"" % gif_path)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
package.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport argparse\nimport glob\nimport os\nimport cPickle as pickle\nimport random\n\n\ndef pickle_examples(paths, train_path, val_path, train_val_split=0.1):\n    """"""\n    Compile a list of examples into pickled format, so during\n    the training, all io will happen in memory\n    """"""\n    with open(train_path, \'wb\') as ft:\n        with open(val_path, \'wb\') as fv:\n            for p in paths:\n                label = int(os.path.basename(p).split(""_"")[0])\n                with open(p, \'rb\') as f:\n                    print(""img %s"" % p, label)\n                    img_bytes = f.read()\n                    r = random.random()\n                    example = (label, img_bytes)\n                    if r < train_val_split:\n                        pickle.dump(example, fv)\n                    else:\n                        pickle.dump(example, ft)\n\n\nparser = argparse.ArgumentParser(description=\'Compile list of images into a pickled object for training\')\nparser.add_argument(\'--dir\', dest=\'dir\', required=True, help=\'path of examples\')\nparser.add_argument(\'--save_dir\', dest=\'save_dir\', required=True, help=\'path to save pickled files\')\nparser.add_argument(\'--split_ratio\', type=float, default=0.1, dest=\'split_ratio\',\n                    help=\'split ratio between train and val\')\nargs = parser.parse_args()\n\nif __name__ == ""__main__"":\n    train_path = os.path.join(args.save_dir, ""train.obj"")\n    val_path = os.path.join(args.save_dir, ""val.obj"")\n    pickle_examples(sorted(glob.glob(os.path.join(args.dir, ""*.jpg""))), train_path=train_path, val_path=val_path,\n                    train_val_split=args.split_ratio)\n'"
train.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport argparse\n\nfrom model.unet import UNet\n\nparser = argparse.ArgumentParser(description=\'Train\')\nparser.add_argument(\'--experiment_dir\', dest=\'experiment_dir\', required=True,\n                    help=\'experiment directory, data, samples,checkpoints,etc\')\nparser.add_argument(\'--experiment_id\', dest=\'experiment_id\', type=int, default=0,\n                    help=\'sequence id for the experiments you prepare to run\')\nparser.add_argument(\'--image_size\', dest=\'image_size\', type=int, default=256,\n                    help=""size of your input and output image"")\nparser.add_argument(\'--L1_penalty\', dest=\'L1_penalty\', type=int, default=100, help=\'weight for L1 loss\')\nparser.add_argument(\'--Lconst_penalty\', dest=\'Lconst_penalty\', type=int, default=15, help=\'weight for const loss\')\nparser.add_argument(\'--Ltv_penalty\', dest=\'Ltv_penalty\', type=float, default=0.0, help=\'weight for tv loss\')\nparser.add_argument(\'--Lcategory_penalty\', dest=\'Lcategory_penalty\', type=float, default=1.0,\n                    help=\'weight for category loss\')\nparser.add_argument(\'--embedding_num\', dest=\'embedding_num\', type=int, default=40,\n                    help=""number for distinct embeddings"")\nparser.add_argument(\'--embedding_dim\', dest=\'embedding_dim\', type=int, default=128, help=""dimension for embedding"")\nparser.add_argument(\'--epoch\', dest=\'epoch\', type=int, default=100, help=\'number of epoch\')\nparser.add_argument(\'--batch_size\', dest=\'batch_size\', type=int, default=16, help=\'number of examples in batch\')\nparser.add_argument(\'--lr\', dest=\'lr\', type=float, default=0.001, help=\'initial learning rate for adam\')\nparser.add_argument(\'--schedule\', dest=\'schedule\', type=int, default=10, help=\'number of epochs to half learning rate\')\nparser.add_argument(\'--resume\', dest=\'resume\', type=int, default=1, help=\'resume from previous training\')\nparser.add_argument(\'--freeze_encoder\', dest=\'freeze_encoder\', type=int, default=0,\n                    help=""freeze encoder weights during training"")\nparser.add_argument(\'--fine_tune\', dest=\'fine_tune\', type=str, default=None,\n                    help=\'specific labels id to be fine tuned\')\nparser.add_argument(\'--inst_norm\', dest=\'inst_norm\', type=int, default=0,\n                    help=\'use conditional instance normalization in your model\')\nparser.add_argument(\'--sample_steps\', dest=\'sample_steps\', type=int, default=10,\n                    help=\'number of batches in between two samples are drawn from validation set\')\nparser.add_argument(\'--checkpoint_steps\', dest=\'checkpoint_steps\', type=int, default=500,\n                    help=\'number of batches in between two checkpoints\')\nparser.add_argument(\'--flip_labels\', dest=\'flip_labels\', type=int, default=None,\n                    help=\'whether flip training data labels or not, in fine tuning\')\nargs = parser.parse_args()\n\n\ndef main(_):\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        model = UNet(args.experiment_dir, batch_size=args.batch_size, experiment_id=args.experiment_id,\n                     input_width=args.image_size, output_width=args.image_size, embedding_num=args.embedding_num,\n                     embedding_dim=args.embedding_dim, L1_penalty=args.L1_penalty, Lconst_penalty=args.Lconst_penalty,\n                     Ltv_penalty=args.Ltv_penalty, Lcategory_penalty=args.Lcategory_penalty)\n        model.register_session(sess)\n        if args.flip_labels:\n            model.build_model(is_training=True, inst_norm=args.inst_norm, no_target_source=True)\n        else:\n            model.build_model(is_training=True, inst_norm=args.inst_norm)\n        fine_tune_list = None\n        if args.fine_tune:\n            ids = args.fine_tune.split("","")\n            fine_tune_list = set([int(i) for i in ids])\n        model.train(lr=args.lr, epoch=args.epoch, resume=args.resume,\n                    schedule=args.schedule, freeze_encoder=args.freeze_encoder, fine_tune=fine_tune_list,\n                    sample_steps=args.sample_steps, checkpoint_steps=args.checkpoint_steps,\n                    flip_labels=args.flip_labels)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
model/__init__.py,0,b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n'
model/dataset.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport cPickle as pickle\nimport numpy as np\nimport random\nimport os\nfrom .utils import pad_seq, bytes_to_file, \\\n    read_split_image, shift_and_resize_image, normalize_image\n\n\nclass PickledImageProvider(object):\n    def __init__(self, obj_path):\n        self.obj_path = obj_path\n        self.examples = self.load_pickled_examples()\n\n    def load_pickled_examples(self):\n        with open(self.obj_path, ""rb"") as of:\n            examples = list()\n            while True:\n                try:\n                    e = pickle.load(of)\n                    examples.append(e)\n                    if len(examples) % 1000 == 0:\n                        print(""processed %d examples"" % len(examples))\n                except EOFError:\n                    break\n                except Exception:\n                    pass\n            print(""unpickled total %d examples"" % len(examples))\n            return examples\n\n\ndef get_batch_iter(examples, batch_size, augment):\n    # the transpose ops requires deterministic\n    # batch size, thus comes the padding\n    padded = pad_seq(examples, batch_size)\n\n    def process(img):\n        img = bytes_to_file(img)\n        try:\n            img_A, img_B = read_split_image(img)\n            if augment:\n                # augment the image by:\n                # 1) enlarge the image\n                # 2) random crop the image back to its original size\n                # NOTE: image A and B needs to be in sync as how much\n                # to be shifted\n                w, h, _ = img_A.shape\n                multiplier = random.uniform(1.00, 1.20)\n                # add an eps to prevent cropping issue\n                nw = int(multiplier * w) + 1\n                nh = int(multiplier * h) + 1\n                shift_x = int(np.ceil(np.random.uniform(0.01, nw - w)))\n                shift_y = int(np.ceil(np.random.uniform(0.01, nh - h)))\n                img_A = shift_and_resize_image(img_A, shift_x, shift_y, nw, nh)\n                img_B = shift_and_resize_image(img_B, shift_x, shift_y, nw, nh)\n            img_A = normalize_image(img_A)\n            img_B = normalize_image(img_B)\n            return np.concatenate([img_A, img_B], axis=2)\n        finally:\n            img.close()\n\n    def batch_iter():\n        for i in range(0, len(padded), batch_size):\n            batch = padded[i: i + batch_size]\n            labels = [e[0] for e in batch]\n            processed = [process(e[1]) for e in batch]\n            # stack into tensor\n            yield labels, np.array(processed).astype(np.float32)\n\n    return batch_iter()\n\n\nclass TrainDataProvider(object):\n    def __init__(self, data_dir, train_name=""train.obj"", val_name=""val.obj"", filter_by=None):\n        self.data_dir = data_dir\n        self.filter_by = filter_by\n        self.train_path = os.path.join(self.data_dir, train_name)\n        self.val_path = os.path.join(self.data_dir, val_name)\n        self.train = PickledImageProvider(self.train_path)\n        self.val = PickledImageProvider(self.val_path)\n        if self.filter_by:\n            print(""filter by label ->"", filter_by)\n            self.train.examples = filter(lambda e: e[0] in self.filter_by, self.train.examples)\n            self.val.examples = filter(lambda e: e[0] in self.filter_by, self.val.examples)\n        print(""train examples -> %d, val examples -> %d"" % (len(self.train.examples), len(self.val.examples)))\n\n    def get_train_iter(self, batch_size, shuffle=True):\n        training_examples = self.train.examples[:]\n        if shuffle:\n            np.random.shuffle(training_examples)\n        return get_batch_iter(training_examples, batch_size, augment=True)\n\n    def get_val_iter(self, batch_size, shuffle=True):\n        """"""\n        Validation iterator runs forever\n        """"""\n        val_examples = self.val.examples[:]\n        if shuffle:\n            np.random.shuffle(val_examples)\n        while True:\n            val_batch_iter = get_batch_iter(val_examples, batch_size, augment=False)\n            for labels, examples in val_batch_iter:\n                yield labels, examples\n\n    def compute_total_batch_num(self, batch_size):\n        """"""Total padded batch num""""""\n        return int(np.ceil(len(self.train.examples) / float(batch_size)))\n\n    def get_all_labels(self):\n        """"""Get all training labels""""""\n        return list({e[0] for e in self.train.examples})\n\n    def get_train_val_path(self):\n        return self.train_path, self.val_path\n\n\nclass InjectDataProvider(object):\n    def __init__(self, obj_path):\n        self.data = PickledImageProvider(obj_path)\n        print(""examples -> %d"" % len(self.data.examples))\n\n    def get_single_embedding_iter(self, batch_size, embedding_id):\n        examples = self.data.examples[:]\n        batch_iter = get_batch_iter(examples, batch_size, augment=False)\n        for _, images in batch_iter:\n            # inject specific embedding style here\n            labels = [embedding_id] * batch_size\n            yield labels, images\n\n    def get_random_embedding_iter(self, batch_size, embedding_ids):\n        examples = self.data.examples[:]\n        batch_iter = get_batch_iter(examples, batch_size, augment=False)\n        for _, images in batch_iter:\n            # inject specific embedding style here\n            labels = [random.choice(embedding_ids) for i in range(batch_size)]\n            yield labels, images\n\n\nclass NeverEndingLoopingProvider(InjectDataProvider):\n    def __init__(self, obj_path):\n        super(NeverEndingLoopingProvider, self).__init__(obj_path)\n\n    def get_random_embedding_iter(self, batch_size, embedding_ids):\n        while True:\n            # np.random.shuffle(self.data.examples)\n            rand_iter = super(NeverEndingLoopingProvider, self) \\\n                .get_random_embedding_iter(batch_size, embedding_ids)\n            for labels, images in rand_iter:\n                yield labels, images\n'"
model/ops.py,30,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport tensorflow as tf\n\n\ndef batch_norm(x, is_training, epsilon=1e-5, decay=0.9, scope=""batch_norm""):\n    return tf.contrib.layers.batch_norm(x, decay=decay, updates_collections=None, epsilon=epsilon,\n                                        scale=True, is_training=is_training, scope=scope)\n\n\ndef conv2d(x, output_filters, kh=5, kw=5, sh=2, sw=2, stddev=0.02, scope=""conv2d""):\n    with tf.variable_scope(scope):\n        shape = x.get_shape().as_list()\n        W = tf.get_variable(\'W\', [kh, kw, shape[-1], output_filters],\n                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        Wconv = tf.nn.conv2d(x, W, strides=[1, sh, sw, 1], padding=\'SAME\')\n\n        biases = tf.get_variable(\'b\', [output_filters], initializer=tf.constant_initializer(0.0))\n        Wconv_plus_b = tf.reshape(tf.nn.bias_add(Wconv, biases), Wconv.get_shape())\n\n        return Wconv_plus_b\n\n\ndef deconv2d(x, output_shape, kh=5, kw=5, sh=2, sw=2, stddev=0.02, scope=""deconv2d""):\n    with tf.variable_scope(scope):\n        # filter : [height, width, output_channels, in_channels]\n        input_shape = x.get_shape().as_list()\n        W = tf.get_variable(\'W\', [kh, kw, output_shape[-1], input_shape[-1]],\n                            initializer=tf.random_normal_initializer(stddev=stddev))\n\n        deconv = tf.nn.conv2d_transpose(x, W, output_shape=output_shape,\n                                        strides=[1, sh, sw, 1])\n\n        biases = tf.get_variable(\'b\', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n        deconv_plus_b = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n\n        return deconv_plus_b\n\n\ndef lrelu(x, leak=0.2):\n    return tf.maximum(x, leak * x)\n\n\ndef fc(x, output_size, stddev=0.02, scope=""fc""):\n    with tf.variable_scope(scope):\n        shape = x.get_shape().as_list()\n        W = tf.get_variable(""W"", [shape[1], output_size], tf.float32,\n                            tf.random_normal_initializer(stddev=stddev))\n        b = tf.get_variable(""b"", [output_size],\n                            initializer=tf.constant_initializer(0.0))\n        return tf.matmul(x, W) + b\n\n\ndef init_embedding(size, dimension, stddev=0.01, scope=""embedding""):\n    with tf.variable_scope(scope):\n        return tf.get_variable(""E"", [size, 1, 1, dimension], tf.float32,\n                               tf.random_normal_initializer(stddev=stddev))\n\n\ndef conditional_instance_norm(x, ids, labels_num, mixed=False, scope=""conditional_instance_norm""):\n    with tf.variable_scope(scope):\n        shape = x.get_shape().as_list()\n        batch_size, output_filters = shape[0], shape[-1]\n        scale = tf.get_variable(""scale"", [labels_num, output_filters], tf.float32, tf.constant_initializer(1.0))\n        shift = tf.get_variable(""shift"", [labels_num, output_filters], tf.float32, tf.constant_initializer(0.0))\n\n        mu, sigma = tf.nn.moments(x, [1, 2], keep_dims=True)\n        norm = (x - mu) / tf.sqrt(sigma + 1e-5)\n\n        batch_scale = tf.reshape(tf.nn.embedding_lookup([scale], ids=ids), [batch_size, 1, 1, output_filters])\n        batch_shift = tf.reshape(tf.nn.embedding_lookup([shift], ids=ids), [batch_size, 1, 1, output_filters])\n\n        z = norm * batch_scale + batch_shift\n        return z\n'"
model/unet.py,75,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc as misc\nimport os\nimport time\nfrom collections import namedtuple\nfrom .ops import conv2d, deconv2d, lrelu, fc, batch_norm, init_embedding, conditional_instance_norm\nfrom .dataset import TrainDataProvider, InjectDataProvider, NeverEndingLoopingProvider\nfrom .utils import scale_back, merge, save_concat_images\n\n# Auxiliary wrapper classes\n# Used to save handles(important nodes in computation graph) for later evaluation\nLossHandle = namedtuple(""LossHandle"", [""d_loss"", ""g_loss"", ""const_loss"", ""l1_loss"",\n                                       ""category_loss"", ""cheat_loss"", ""tv_loss""])\nInputHandle = namedtuple(""InputHandle"", [""real_data"", ""embedding_ids"", ""no_target_data"", ""no_target_ids""])\nEvalHandle = namedtuple(""EvalHandle"", [""encoder"", ""generator"", ""target"", ""source"", ""embedding""])\nSummaryHandle = namedtuple(""SummaryHandle"", [""d_merged"", ""g_merged""])\n\n\nclass UNet(object):\n    def __init__(self, experiment_dir=None, experiment_id=0, batch_size=16, input_width=256, output_width=256,\n                 generator_dim=64, discriminator_dim=64, L1_penalty=100, Lconst_penalty=15, Ltv_penalty=0.0,\n                 Lcategory_penalty=1.0, embedding_num=40, embedding_dim=128, input_filters=3, output_filters=3):\n        self.experiment_dir = experiment_dir\n        self.experiment_id = experiment_id\n        self.batch_size = batch_size\n        self.input_width = input_width\n        self.output_width = output_width\n        self.generator_dim = generator_dim\n        self.discriminator_dim = discriminator_dim\n        self.L1_penalty = L1_penalty\n        self.Lconst_penalty = Lconst_penalty\n        self.Ltv_penalty = Ltv_penalty\n        self.Lcategory_penalty = Lcategory_penalty\n        self.embedding_num = embedding_num\n        self.embedding_dim = embedding_dim\n        self.input_filters = input_filters\n        self.output_filters = output_filters\n        # init all the directories\n        self.sess = None\n        # experiment_dir is needed for training\n        if experiment_dir:\n            self.data_dir = os.path.join(self.experiment_dir, ""data"")\n            self.checkpoint_dir = os.path.join(self.experiment_dir, ""checkpoint"")\n            self.sample_dir = os.path.join(self.experiment_dir, ""sample"")\n            self.log_dir = os.path.join(self.experiment_dir, ""logs"")\n\n            if not os.path.exists(self.checkpoint_dir):\n                os.makedirs(self.checkpoint_dir)\n                print(""create checkpoint directory"")\n            if not os.path.exists(self.log_dir):\n                os.makedirs(self.log_dir)\n                print(""create log directory"")\n            if not os.path.exists(self.sample_dir):\n                os.makedirs(self.sample_dir)\n                print(""create sample directory"")\n\n    def encoder(self, images, is_training, reuse=False):\n        with tf.variable_scope(""generator""):\n            if reuse:\n                tf.get_variable_scope().reuse_variables()\n\n            encode_layers = dict()\n\n            def encode_layer(x, output_filters, layer):\n                act = lrelu(x)\n                conv = conv2d(act, output_filters=output_filters, scope=""g_e%d_conv"" % layer)\n                enc = batch_norm(conv, is_training, scope=""g_e%d_bn"" % layer)\n                encode_layers[""e%d"" % layer] = enc\n                return enc\n\n            e1 = conv2d(images, self.generator_dim, scope=""g_e1_conv"")\n            encode_layers[""e1""] = e1\n            e2 = encode_layer(e1, self.generator_dim * 2, 2)\n            e3 = encode_layer(e2, self.generator_dim * 4, 3)\n            e4 = encode_layer(e3, self.generator_dim * 8, 4)\n            e5 = encode_layer(e4, self.generator_dim * 8, 5)\n            e6 = encode_layer(e5, self.generator_dim * 8, 6)\n            e7 = encode_layer(e6, self.generator_dim * 8, 7)\n            e8 = encode_layer(e7, self.generator_dim * 8, 8)\n\n            return e8, encode_layers\n\n    def decoder(self, encoded, encoding_layers, ids, inst_norm, is_training, reuse=False):\n        with tf.variable_scope(""generator""):\n            if reuse:\n                tf.get_variable_scope().reuse_variables()\n\n            s = self.output_width\n            s2, s4, s8, s16, s32, s64, s128 = int(s / 2), int(s / 4), int(s / 8), int(s / 16), int(s / 32), int(\n                s / 64), int(s / 128)\n\n            def decode_layer(x, output_width, output_filters, layer, enc_layer, dropout=False, do_concat=True):\n                dec = deconv2d(tf.nn.relu(x), [self.batch_size, output_width,\n                                               output_width, output_filters], scope=""g_d%d_deconv"" % layer)\n                if layer != 8:\n                    # IMPORTANT: normalization for last layer\n                    # Very important, otherwise GAN is unstable\n                    # Trying conditional instance normalization to\n                    # overcome the fact that batch normalization offers\n                    # different train/test statistics\n                    if inst_norm:\n                        dec = conditional_instance_norm(dec, ids, self.embedding_num, scope=""g_d%d_inst_norm"" % layer)\n                    else:\n                        dec = batch_norm(dec, is_training, scope=""g_d%d_bn"" % layer)\n                if dropout:\n                    dec = tf.nn.dropout(dec, 0.5)\n                if do_concat:\n                    dec = tf.concat([dec, enc_layer], 3)\n                return dec\n\n            d1 = decode_layer(encoded, s128, self.generator_dim * 8, layer=1, enc_layer=encoding_layers[""e7""],\n                              dropout=True)\n            d2 = decode_layer(d1, s64, self.generator_dim * 8, layer=2, enc_layer=encoding_layers[""e6""], dropout=True)\n            d3 = decode_layer(d2, s32, self.generator_dim * 8, layer=3, enc_layer=encoding_layers[""e5""], dropout=True)\n            d4 = decode_layer(d3, s16, self.generator_dim * 8, layer=4, enc_layer=encoding_layers[""e4""])\n            d5 = decode_layer(d4, s8, self.generator_dim * 4, layer=5, enc_layer=encoding_layers[""e3""])\n            d6 = decode_layer(d5, s4, self.generator_dim * 2, layer=6, enc_layer=encoding_layers[""e2""])\n            d7 = decode_layer(d6, s2, self.generator_dim, layer=7, enc_layer=encoding_layers[""e1""])\n            d8 = decode_layer(d7, s, self.output_filters, layer=8, enc_layer=None, do_concat=False)\n\n            output = tf.nn.tanh(d8)  # scale to (-1, 1)\n            return output\n\n    def generator(self, images, embeddings, embedding_ids, inst_norm, is_training, reuse=False):\n        e8, enc_layers = self.encoder(images, is_training=is_training, reuse=reuse)\n        local_embeddings = tf.nn.embedding_lookup(embeddings, ids=embedding_ids)\n        local_embeddings = tf.reshape(local_embeddings, [self.batch_size, 1, 1, self.embedding_dim])\n        embedded = tf.concat([e8, local_embeddings], 3)\n        output = self.decoder(embedded, enc_layers, embedding_ids, inst_norm, is_training=is_training, reuse=reuse)\n        return output, e8\n\n    def discriminator(self, image, is_training, reuse=False):\n        with tf.variable_scope(""discriminator""):\n            if reuse:\n                tf.get_variable_scope().reuse_variables()\n            h0 = lrelu(conv2d(image, self.discriminator_dim, scope=""d_h0_conv""))\n            h1 = lrelu(batch_norm(conv2d(h0, self.discriminator_dim * 2, scope=""d_h1_conv""),\n                                  is_training, scope=""d_bn_1""))\n            h2 = lrelu(batch_norm(conv2d(h1, self.discriminator_dim * 4, scope=""d_h2_conv""),\n                                  is_training, scope=""d_bn_2""))\n            h3 = lrelu(batch_norm(conv2d(h2, self.discriminator_dim * 8, sh=1, sw=1, scope=""d_h3_conv""),\n                                  is_training, scope=""d_bn_3""))\n            # real or fake binary loss\n            fc1 = fc(tf.reshape(h3, [self.batch_size, -1]), 1, scope=""d_fc1"")\n            # category loss\n            fc2 = fc(tf.reshape(h3, [self.batch_size, -1]), self.embedding_num, scope=""d_fc2"")\n\n            return tf.nn.sigmoid(fc1), fc1, fc2\n\n    def build_model(self, is_training=True, inst_norm=False, no_target_source=False):\n        real_data = tf.placeholder(tf.float32,\n                                   [self.batch_size, self.input_width, self.input_width,\n                                    self.input_filters + self.output_filters],\n                                   name=\'real_A_and_B_images\')\n        embedding_ids = tf.placeholder(tf.int64, shape=None, name=""embedding_ids"")\n        no_target_data = tf.placeholder(tf.float32,\n                                        [self.batch_size, self.input_width, self.input_width,\n                                         self.input_filters + self.output_filters],\n                                        name=\'no_target_A_and_B_images\')\n        no_target_ids = tf.placeholder(tf.int64, shape=None, name=""no_target_embedding_ids"")\n\n        # target images\n        real_B = real_data[:, :, :, :self.input_filters]\n        # source images\n        real_A = real_data[:, :, :, self.input_filters:self.input_filters + self.output_filters]\n\n        embedding = init_embedding(self.embedding_num, self.embedding_dim)\n        fake_B, encoded_real_A = self.generator(real_A, embedding, embedding_ids, is_training=is_training,\n                                                inst_norm=inst_norm)\n        real_AB = tf.concat([real_A, real_B], 3)\n        fake_AB = tf.concat([real_A, fake_B], 3)\n\n        # Note it is not possible to set reuse flag back to False\n        # initialize all variables before setting reuse to True\n        real_D, real_D_logits, real_category_logits = self.discriminator(real_AB, is_training=is_training, reuse=False)\n        fake_D, fake_D_logits, fake_category_logits = self.discriminator(fake_AB, is_training=is_training, reuse=True)\n\n        # encoding constant loss\n        # this loss assume that generated imaged and real image\n        # should reside in the same space and close to each other\n        encoded_fake_B = self.encoder(fake_B, is_training, reuse=True)[0]\n        const_loss = (tf.reduce_mean(tf.square(encoded_real_A - encoded_fake_B))) * self.Lconst_penalty\n\n        # category loss\n        true_labels = tf.reshape(tf.one_hot(indices=embedding_ids, depth=self.embedding_num),\n                                 shape=[self.batch_size, self.embedding_num])\n        real_category_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_category_logits,\n                                                                                    labels=true_labels))\n        fake_category_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_category_logits,\n                                                                                    labels=true_labels))\n        category_loss = self.Lcategory_penalty * (real_category_loss + fake_category_loss)\n\n        # binary real/fake loss\n        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_D_logits,\n                                                                             labels=tf.ones_like(real_D)))\n        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_D_logits,\n                                                                             labels=tf.zeros_like(fake_D)))\n        # L1 loss between real and generated images\n        l1_loss = self.L1_penalty * tf.reduce_mean(tf.abs(fake_B - real_B))\n        # total variation loss\n        width = self.output_width\n        tv_loss = (tf.nn.l2_loss(fake_B[:, 1:, :, :] - fake_B[:, :width - 1, :, :]) / width\n                   + tf.nn.l2_loss(fake_B[:, :, 1:, :] - fake_B[:, :, :width - 1, :]) / width) * self.Ltv_penalty\n\n        # maximize the chance generator fool the discriminator\n        cheat_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_D_logits,\n                                                                            labels=tf.ones_like(fake_D)))\n\n        d_loss = d_loss_real + d_loss_fake + category_loss / 2.0\n        g_loss = cheat_loss + l1_loss + self.Lcategory_penalty * fake_category_loss + const_loss + tv_loss\n\n        if no_target_source:\n            # no_target source are examples that don\'t have the corresponding target images\n            # however, except L1 loss, we can compute category loss, binary loss and constant losses with those examples\n            # it is useful when discriminator get saturated and d_loss drops to near zero\n            # those data could be used as additional source of losses to break the saturation\n            no_target_A = no_target_data[:, :, :, self.input_filters:self.input_filters + self.output_filters]\n            no_target_B, encoded_no_target_A = self.generator(no_target_A, embedding, no_target_ids,\n                                                              is_training=is_training,\n                                                              inst_norm=inst_norm, reuse=True)\n            no_target_labels = tf.reshape(tf.one_hot(indices=no_target_ids, depth=self.embedding_num),\n                                          shape=[self.batch_size, self.embedding_num])\n            no_target_AB = tf.concat([no_target_A, no_target_B], 3)\n            no_target_D, no_target_D_logits, no_target_category_logits = self.discriminator(no_target_AB,\n                                                                                            is_training=is_training,\n                                                                                            reuse=True)\n            encoded_no_target_B = self.encoder(no_target_B, is_training, reuse=True)[0]\n            no_target_const_loss = tf.reduce_mean(\n                tf.square(encoded_no_target_A - encoded_no_target_B)) * self.Lconst_penalty\n            no_target_category_loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=no_target_category_logits,\n                                                        labels=no_target_labels)) * self.Lcategory_penalty\n\n            d_loss_no_target = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=no_target_D_logits,\n                                                                                      labels=tf.zeros_like(\n                                                                                          no_target_D)))\n            cheat_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=no_target_D_logits,\n                                                                                 labels=tf.ones_like(no_target_D)))\n            d_loss = d_loss_real + d_loss_fake + d_loss_no_target + (category_loss + no_target_category_loss) / 3.0\n            g_loss = cheat_loss / 2.0 + l1_loss + \\\n                     (self.Lcategory_penalty * fake_category_loss + no_target_category_loss) / 2.0 + \\\n                     (const_loss + no_target_const_loss) / 2.0 + tv_loss\n\n        d_loss_real_summary = tf.summary.scalar(""d_loss_real"", d_loss_real)\n        d_loss_fake_summary = tf.summary.scalar(""d_loss_fake"", d_loss_fake)\n        category_loss_summary = tf.summary.scalar(""category_loss"", category_loss)\n        cheat_loss_summary = tf.summary.scalar(""cheat_loss"", cheat_loss)\n        l1_loss_summary = tf.summary.scalar(""l1_loss"", l1_loss)\n        fake_category_loss_summary = tf.summary.scalar(""fake_category_loss"", fake_category_loss)\n        const_loss_summary = tf.summary.scalar(""const_loss"", const_loss)\n        d_loss_summary = tf.summary.scalar(""d_loss"", d_loss)\n        g_loss_summary = tf.summary.scalar(""g_loss"", g_loss)\n        tv_loss_summary = tf.summary.scalar(""tv_loss"", tv_loss)\n\n        d_merged_summary = tf.summary.merge([d_loss_real_summary, d_loss_fake_summary,\n                                             category_loss_summary, d_loss_summary])\n        g_merged_summary = tf.summary.merge([cheat_loss_summary, l1_loss_summary,\n                                             fake_category_loss_summary,\n                                             const_loss_summary,\n                                             g_loss_summary, tv_loss_summary])\n\n        # expose useful nodes in the graph as handles globally\n        input_handle = InputHandle(real_data=real_data,\n                                   embedding_ids=embedding_ids,\n                                   no_target_data=no_target_data,\n                                   no_target_ids=no_target_ids)\n\n        loss_handle = LossHandle(d_loss=d_loss,\n                                 g_loss=g_loss,\n                                 const_loss=const_loss,\n                                 l1_loss=l1_loss,\n                                 category_loss=category_loss,\n                                 cheat_loss=cheat_loss,\n                                 tv_loss=tv_loss)\n\n        eval_handle = EvalHandle(encoder=encoded_real_A,\n                                 generator=fake_B,\n                                 target=real_B,\n                                 source=real_A,\n                                 embedding=embedding)\n\n        summary_handle = SummaryHandle(d_merged=d_merged_summary,\n                                       g_merged=g_merged_summary)\n\n        # those operations will be shared, so we need\n        # to make them visible globally\n        setattr(self, ""input_handle"", input_handle)\n        setattr(self, ""loss_handle"", loss_handle)\n        setattr(self, ""eval_handle"", eval_handle)\n        setattr(self, ""summary_handle"", summary_handle)\n\n    def register_session(self, sess):\n        self.sess = sess\n\n    def retrieve_trainable_vars(self, freeze_encoder=False):\n        t_vars = tf.trainable_variables()\n\n        d_vars = [var for var in t_vars if \'d_\' in var.name]\n        g_vars = [var for var in t_vars if \'g_\' in var.name]\n\n        if freeze_encoder:\n            # exclude encoder weights\n            print(""freeze encoder weights"")\n            g_vars = [var for var in g_vars if not (""g_e"" in var.name)]\n\n        return g_vars, d_vars\n\n    def retrieve_generator_vars(self):\n        all_vars = tf.global_variables()\n        generate_vars = [var for var in all_vars if \'embedding\' in var.name or ""g_"" in var.name]\n        return generate_vars\n\n    def retrieve_handles(self):\n        input_handle = getattr(self, ""input_handle"")\n        loss_handle = getattr(self, ""loss_handle"")\n        eval_handle = getattr(self, ""eval_handle"")\n        summary_handle = getattr(self, ""summary_handle"")\n\n        return input_handle, loss_handle, eval_handle, summary_handle\n\n    def get_model_id_and_dir(self):\n        model_id = ""experiment_%d_batch_%d"" % (self.experiment_id, self.batch_size)\n        model_dir = os.path.join(self.checkpoint_dir, model_id)\n        return model_id, model_dir\n\n    def checkpoint(self, saver, step):\n        model_name = ""unet.model""\n        model_id, model_dir = self.get_model_id_and_dir()\n\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n\n        saver.save(self.sess, os.path.join(model_dir, model_name), global_step=step)\n\n    def restore_model(self, saver, model_dir):\n\n        ckpt = tf.train.get_checkpoint_state(model_dir)\n\n        if ckpt:\n            saver.restore(self.sess, ckpt.model_checkpoint_path)\n            print(""restored model %s"" % model_dir)\n        else:\n            print(""fail to restore model %s"" % model_dir)\n\n    def generate_fake_samples(self, input_images, embedding_ids):\n        input_handle, loss_handle, eval_handle, summary_handle = self.retrieve_handles()\n        fake_images, real_images, \\\n        d_loss, g_loss, l1_loss = self.sess.run([eval_handle.generator,\n                                                 eval_handle.target,\n                                                 loss_handle.d_loss,\n                                                 loss_handle.g_loss,\n                                                 loss_handle.l1_loss],\n                                                feed_dict={\n                                                    input_handle.real_data: input_images,\n                                                    input_handle.embedding_ids: embedding_ids,\n                                                    input_handle.no_target_data: input_images,\n                                                    input_handle.no_target_ids: embedding_ids\n                                                })\n        return fake_images, real_images, d_loss, g_loss, l1_loss\n\n    def validate_model(self, val_iter, epoch, step):\n        labels, images = next(val_iter)\n        fake_imgs, real_imgs, d_loss, g_loss, l1_loss = self.generate_fake_samples(images, labels)\n        print(""Sample: d_loss: %.5f, g_loss: %.5f, l1_loss: %.5f"" % (d_loss, g_loss, l1_loss))\n\n        merged_fake_images = merge(scale_back(fake_imgs), [self.batch_size, 1])\n        merged_real_images = merge(scale_back(real_imgs), [self.batch_size, 1])\n        merged_pair = np.concatenate([merged_real_images, merged_fake_images], axis=1)\n\n        model_id, _ = self.get_model_id_and_dir()\n\n        model_sample_dir = os.path.join(self.sample_dir, model_id)\n        if not os.path.exists(model_sample_dir):\n            os.makedirs(model_sample_dir)\n\n        sample_img_path = os.path.join(model_sample_dir, ""sample_%02d_%04d.png"" % (epoch, step))\n        misc.imsave(sample_img_path, merged_pair)\n\n    def export_generator(self, save_dir, model_dir, model_name=""gen_model""):\n        saver = tf.train.Saver()\n        self.restore_model(saver, model_dir)\n\n        gen_saver = tf.train.Saver(var_list=self.retrieve_generator_vars())\n        gen_saver.save(self.sess, os.path.join(save_dir, model_name), global_step=0)\n\n    def infer(self, source_obj, embedding_ids, model_dir, save_dir):\n        source_provider = InjectDataProvider(source_obj)\n\n        if isinstance(embedding_ids, int) or len(embedding_ids) == 1:\n            embedding_id = embedding_ids if isinstance(embedding_ids, int) else embedding_ids[0]\n            source_iter = source_provider.get_single_embedding_iter(self.batch_size, embedding_id)\n        else:\n            source_iter = source_provider.get_random_embedding_iter(self.batch_size, embedding_ids)\n\n        tf.global_variables_initializer().run()\n        saver = tf.train.Saver(var_list=self.retrieve_generator_vars())\n        self.restore_model(saver, model_dir)\n\n        def save_imgs(imgs, count):\n            p = os.path.join(save_dir, ""inferred_%04d.png"" % count)\n            save_concat_images(imgs, img_path=p)\n            print(""generated images saved at %s"" % p)\n\n        count = 0\n        batch_buffer = list()\n        for labels, source_imgs in source_iter:\n            fake_imgs = self.generate_fake_samples(source_imgs, labels)[0]\n            merged_fake_images = merge(scale_back(fake_imgs), [self.batch_size, 1])\n            batch_buffer.append(merged_fake_images)\n            if len(batch_buffer) == 10:\n                save_imgs(batch_buffer, count)\n                batch_buffer = list()\n            count += 1\n        if batch_buffer:\n            # last batch\n            save_imgs(batch_buffer, count)\n\n    def interpolate(self, source_obj, between, model_dir, save_dir, steps):\n        tf.global_variables_initializer().run()\n        saver = tf.train.Saver(var_list=self.retrieve_generator_vars())\n        self.restore_model(saver, model_dir)\n        # new interpolated dimension\n        new_x_dim = steps + 1\n        alphas = np.linspace(0.0, 1.0, new_x_dim)\n\n        def _interpolate_tensor(_tensor):\n            """"""\n            Compute the interpolated tensor here\n            """"""\n\n            x = _tensor[between[0]]\n            y = _tensor[between[1]]\n\n            interpolated = list()\n            for alpha in alphas:\n                interpolated.append(x * (1. - alpha) + alpha * y)\n\n            interpolated = np.asarray(interpolated, dtype=np.float32)\n            return interpolated\n\n        def filter_embedding_vars(var):\n            var_name = var.name\n            if var_name.find(""embedding"") != -1:\n                return True\n            if var_name.find(""inst_norm/shift"") != -1 or var_name.find(""inst_norm/scale"") != -1:\n                return True\n            return False\n\n        embedding_vars = filter(filter_embedding_vars, tf.trainable_variables())\n        # here comes the hack, we overwrite the original tensor\n        # with interpolated ones. Note, the shape might differ\n\n        # this is to restore the embedding at the end\n        embedding_snapshot = list()\n        for e_var in embedding_vars:\n            val = e_var.eval(session=self.sess)\n            embedding_snapshot.append((e_var, val))\n            t = _interpolate_tensor(val)\n            op = tf.assign(e_var, t, validate_shape=False)\n            print(""overwrite %s tensor"" % e_var.name, ""old_shape ->"", e_var.get_shape(), ""new shape ->"", t.shape)\n            self.sess.run(op)\n\n        source_provider = InjectDataProvider(source_obj)\n        input_handle, _, eval_handle, _ = self.retrieve_handles()\n        for step_idx in range(len(alphas)):\n            alpha = alphas[step_idx]\n            print(""interpolate %d -> %.4f + %d -> %.4f"" % (between[0], 1. - alpha, between[1], alpha))\n            source_iter = source_provider.get_single_embedding_iter(self.batch_size, 0)\n            batch_buffer = list()\n            count = 0\n            for _, source_imgs in source_iter:\n                count += 1\n                labels = [step_idx] * self.batch_size\n                generated, = self.sess.run([eval_handle.generator],\n                                           feed_dict={\n                                               input_handle.real_data: source_imgs,\n                                               input_handle.embedding_ids: labels\n                                           })\n                merged_fake_images = merge(scale_back(generated), [self.batch_size, 1])\n                batch_buffer.append(merged_fake_images)\n            if len(batch_buffer):\n                save_concat_images(batch_buffer,\n                                   os.path.join(save_dir, ""frame_%02d_%02d_step_%02d.png"" % (\n                                       between[0], between[1], step_idx)))\n        # restore the embedding variables\n        print(""restore embedding values"")\n        for var, val in embedding_snapshot:\n            op = tf.assign(var, val, validate_shape=False)\n            self.sess.run(op)\n\n    def train(self, lr=0.0002, epoch=100, schedule=10, resume=True, flip_labels=False,\n              freeze_encoder=False, fine_tune=None, sample_steps=50, checkpoint_steps=500):\n        g_vars, d_vars = self.retrieve_trainable_vars(freeze_encoder=freeze_encoder)\n        input_handle, loss_handle, _, summary_handle = self.retrieve_handles()\n\n        if not self.sess:\n            raise Exception(""no session registered"")\n\n        learning_rate = tf.placeholder(tf.float32, name=""learning_rate"")\n        d_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(loss_handle.d_loss, var_list=d_vars)\n        g_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(loss_handle.g_loss, var_list=g_vars)\n        tf.global_variables_initializer().run()\n        real_data = input_handle.real_data\n        embedding_ids = input_handle.embedding_ids\n        no_target_data = input_handle.no_target_data\n        no_target_ids = input_handle.no_target_ids\n\n        # filter by one type of labels\n        data_provider = TrainDataProvider(self.data_dir, filter_by=fine_tune)\n        total_batches = data_provider.compute_total_batch_num(self.batch_size)\n        val_batch_iter = data_provider.get_val_iter(self.batch_size)\n\n        saver = tf.train.Saver(max_to_keep=3)\n        summary_writer = tf.summary.FileWriter(self.log_dir, self.sess.graph)\n\n        if resume:\n            _, model_dir = self.get_model_id_and_dir()\n            self.restore_model(saver, model_dir)\n\n        current_lr = lr\n        counter = 0\n        start_time = time.time()\n\n        for ei in range(epoch):\n            train_batch_iter = data_provider.get_train_iter(self.batch_size)\n\n            if (ei + 1) % schedule == 0:\n                update_lr = current_lr / 2.0\n                # minimum learning rate guarantee\n                update_lr = max(update_lr, 0.0002)\n                print(""decay learning rate from %.5f to %.5f"" % (current_lr, update_lr))\n                current_lr = update_lr\n\n            for bid, batch in enumerate(train_batch_iter):\n                counter += 1\n                labels, batch_images = batch\n                shuffled_ids = labels[:]\n                if flip_labels:\n                    np.random.shuffle(shuffled_ids)\n                # Optimize D\n                _, batch_d_loss, d_summary = self.sess.run([d_optimizer, loss_handle.d_loss,\n                                                            summary_handle.d_merged],\n                                                           feed_dict={\n                                                               real_data: batch_images,\n                                                               embedding_ids: labels,\n                                                               learning_rate: current_lr,\n                                                               no_target_data: batch_images,\n                                                               no_target_ids: shuffled_ids\n                                                           })\n                # Optimize G\n                _, batch_g_loss = self.sess.run([g_optimizer, loss_handle.g_loss],\n                                                feed_dict={\n                                                    real_data: batch_images,\n                                                    embedding_ids: labels,\n                                                    learning_rate: current_lr,\n                                                    no_target_data: batch_images,\n                                                    no_target_ids: shuffled_ids\n                                                })\n                # magic move to Optimize G again\n                # according to https://github.com/carpedm20/DCGAN-tensorflow\n                # collect all the losses along the way\n                _, batch_g_loss, category_loss, cheat_loss, \\\n                const_loss, l1_loss, tv_loss, g_summary = self.sess.run([g_optimizer,\n                                                                         loss_handle.g_loss,\n                                                                         loss_handle.category_loss,\n                                                                         loss_handle.cheat_loss,\n                                                                         loss_handle.const_loss,\n                                                                         loss_handle.l1_loss,\n                                                                         loss_handle.tv_loss,\n                                                                         summary_handle.g_merged],\n                                                                        feed_dict={\n                                                                            real_data: batch_images,\n                                                                            embedding_ids: labels,\n                                                                            learning_rate: current_lr,\n                                                                            no_target_data: batch_images,\n                                                                            no_target_ids: shuffled_ids\n                                                                        })\n                passed = time.time() - start_time\n                log_format = ""Epoch: [%2d], [%4d/%4d] time: %4.4f, d_loss: %.5f, g_loss: %.5f, "" + \\\n                             ""category_loss: %.5f, cheat_loss: %.5f, const_loss: %.5f, l1_loss: %.5f, tv_loss: %.5f""\n                print(log_format % (ei, bid, total_batches, passed, batch_d_loss, batch_g_loss,\n                                    category_loss, cheat_loss, const_loss, l1_loss, tv_loss))\n                summary_writer.add_summary(d_summary, counter)\n                summary_writer.add_summary(g_summary, counter)\n\n                if counter % sample_steps == 0:\n                    # sample the current model states with val data\n                    self.validate_model(val_batch_iter, ei, counter)\n\n                if counter % checkpoint_steps == 0:\n                    print(""Checkpoint: save checkpoint step %d"" % counter)\n                    self.checkpoint(saver, counter)\n        # save the last checkpoint\n        print(""Checkpoint: last checkpoint step %d"" % counter)\n        self.checkpoint(saver, counter)\n'"
model/utils.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport os\nimport glob\n\nimport imageio\nimport scipy.misc as misc\nimport numpy as np\nfrom cStringIO import StringIO\n\n\ndef pad_seq(seq, batch_size):\n    # pad the sequence to be the multiples of batch_size\n    seq_len = len(seq)\n    if seq_len % batch_size == 0:\n        return seq\n    padded = batch_size - (seq_len % batch_size)\n    seq.extend(seq[:padded])\n    return seq\n\n\ndef bytes_to_file(bytes_img):\n    return StringIO(bytes_img)\n\n\ndef normalize_image(img):\n    """"""\n    Make image zero centered and in between (-1, 1)\n    """"""\n    normalized = (img / 127.5) - 1.\n    return normalized\n\n\ndef read_split_image(img):\n    mat = misc.imread(img).astype(np.float)\n    side = int(mat.shape[1] / 2)\n    assert side * 2 == mat.shape[1]\n    img_A = mat[:, :side]  # target\n    img_B = mat[:, side:]  # source\n\n    return img_A, img_B\n\n\ndef shift_and_resize_image(img, shift_x, shift_y, nw, nh):\n    w, h, _ = img.shape\n    enlarged = misc.imresize(img, [nw, nh])\n    return enlarged[shift_x:shift_x + w, shift_y:shift_y + h]\n\n\ndef scale_back(images):\n    return (images + 1.) / 2.\n\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    img = np.zeros((h * size[0], w * size[1], 3))\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx // size[1]\n        img[j * h:j * h + h, i * w:i * w + w, :] = image\n\n    return img\n\n\ndef save_concat_images(imgs, img_path):\n    concated = np.concatenate(imgs, axis=1)\n    misc.imsave(img_path, concated)\n\n\ndef compile_frames_to_gif(frame_dir, gif_file):\n    frames = sorted(glob.glob(os.path.join(frame_dir, ""*.png"")))\n    print(frames)\n    images = [misc.imresize(imageio.imread(f), interp=\'nearest\', size=0.33) for f in frames]\n    imageio.mimsave(gif_file, images, duration=0.1)\n    return gif_file\n'"
